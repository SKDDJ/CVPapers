# Arxiv Papers in cs.CV on 2021-09-19
### Joint Distribution Alignment via Adversarial Learning for Domain Adaptive Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.09033v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09033v2)
- **Published**: 2021-09-19 00:27:08+00:00
- **Updated**: 2022-03-15 17:07:45+00:00
- **Authors**: Bo Zhang, Tao Chen, Bin Wang, Ruoyao Li
- **Comment**: Accepted by IEEE T-MM, 2021, the code is available at
  https://github.com/BOBrown/JADF-caffe
- **Journal**: None
- **Summary**: Unsupervised domain adaptive object detection aims to adapt a well-trained detector from its original source domain with rich labeled data to a new target domain with unlabeled data. Recently, mainstream approaches perform this task through adversarial learning, yet still suffer from two limitations. First, they mainly align marginal distribution by unsupervised cross-domain feature matching, and ignore each feature's categorical and positional information that can be exploited for conditional alignment; Second, they treat all classes as equally important for transferring cross-domain knowledge and ignore that different classes usually have different transferability. In this paper, we propose a joint adaptive detection framework (JADF) to address the above challenges. First, an end-to-end joint adversarial adaptation framework for object detection is proposed, which aligns both marginal and conditional distributions between domains without introducing any extra hyperparameter. Next, to consider the transferability of each object class, a metric for class-wise transferability assessment is proposed, which is incorporated into the JADF objective for domain adaptation. Further, an extended study from unsupervised domain adaptation (UDA) to unsupervised few-shot domain adaptation (UFDA) is conducted, where only a few unlabeled training images are available in unlabeled target domain. Extensive experiments validate that JADF is effective in both the UDA and UFDA settings, achieving significant performance gains over existing state-of-the-art cross-domain detection methods.



### Object Tracking by Jointly Exploiting Frame and Event Domain
- **Arxiv ID**: http://arxiv.org/abs/2109.09052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09052v1)
- **Published**: 2021-09-19 03:13:25+00:00
- **Updated**: 2021-09-19 03:13:25+00:00
- **Authors**: Jiqing Zhang, Xin Yang, Yingkai Fu, Xiaopeng Wei, Baocai Yin, Bo Dong
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by the complementarity between conventional frame-based and bio-inspired event-based cameras, we propose a multi-modal based approach to fuse visual cues from the frame- and event-domain to enhance the single object tracking performance, especially in degraded conditions (e.g., scenes with high dynamic range, low light, and fast-motion objects). The proposed approach can effectively and adaptively combine meaningful information from both domains. Our approach's effectiveness is enforced by a novel designed cross-domain attention schemes, which can effectively enhance features based on self- and cross-domain attention schemes; The adaptiveness is guarded by a specially designed weighting scheme, which can adaptively balance the contribution of the two domains. To exploit event-based visual cues in single-object tracking, we construct a large-scale frame-event-based dataset, which we subsequently employ to train a novel frame-event fusion based model. Extensive experiments show that the proposed approach outperforms state-of-the-art frame-based tracking methods by at least 10.4% and 11.9% in terms of representative success rate and precision rate, respectively. Besides, the effectiveness of each key component of our approach is evidenced by our thorough ablation study.



### A two-step machine learning approach for crop disease detection: an application of GAN and UAV technology
- **Arxiv ID**: http://arxiv.org/abs/2109.11066v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.6; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2109.11066v1)
- **Published**: 2021-09-19 03:51:20+00:00
- **Updated**: 2021-09-19 03:51:20+00:00
- **Authors**: Aaditya Prasad, Nikhil Mehta, Matthew Horak, Wan D. Bae
- **Comment**: 13 pages, 5 figures Preprint of an article submitted for
  consideration in the International Journal on Artificial Intelligence Tools,
  2021, World Scientific Publishing Company,
  https://www.worldscientific.com/worldscinet/ijait
- **Journal**: None
- **Summary**: Automated plant diagnosis is a technology that promises large increases in cost-efficiency for agriculture. However, multiple problems reduce the effectiveness of drones, including the inverse relationship between resolution and speed and the lack of adequate labeled training data. This paper presents a two-step machine learning approach that analyzes low-fidelity and high-fidelity images in sequence, preserving efficiency as well as accuracy. Two data-generators are also used to minimize class imbalance in the high-fidelity dataset and to produce low-fidelity data that is representative of UAV images. The analysis of applications and methods is conducted on a database of high-fidelity apple tree images which are corrupted with class imbalance. The application begins by generating high-fidelity data using generative networks and then uses this novel data alongside the original high-fidelity data to produce low-fidelity images. A machine-learning identifier identifies plants and labels them as potentially diseased or not. A machine learning classifier is then given the potentially diseased plant images and returns actual diagnoses for these plants. The results show an accuracy of 96.3% for the high-fidelity system and a 75.5% confidence level for our low-fidelity system. Our drone technology shows promising results in accuracy when compared to labor-based methods of diagnosis.



### Ontology-based n-ball Concept Embeddings Informing Few-shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2109.09063v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09063v1)
- **Published**: 2021-09-19 05:35:43+00:00
- **Updated**: 2021-09-19 05:35:43+00:00
- **Authors**: Mirantha Jayathilaka, Tingting Mu, Uli Sattler
- **Comment**: None
- **Journal**: The Combination of Symbolic and Sub-symbolic Methods and their
  Applications (CSSA @ ECML PDKK 2021)
- **Summary**: We propose a novel framework named ViOCE that integrates ontology-based background knowledge in the form of $n$-ball concept embeddings into a neural network based vision architecture. The approach consists of two components - converting symbolic knowledge of an ontology into continuous space by learning n-ball embeddings that capture properties of subsumption and disjointness, and guiding the training and inference of a vision model using the learnt embeddings. We evaluate ViOCE using the task of few-shot image classification, where it demonstrates superior performance on two standard benchmarks.



### Simple and Efficient Unpaired Real-world Super-Resolution using Image Statistics
- **Arxiv ID**: http://arxiv.org/abs/2109.09071v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.09071v1)
- **Published**: 2021-09-19 06:10:33+00:00
- **Updated**: 2021-09-19 06:10:33+00:00
- **Authors**: Kwangjin Yoon
- **Comment**: None
- **Journal**: None
- **Summary**: Learning super-resolution (SR) network without the paired low resolution (LR) and high resolution (HR) image is difficult because direct supervision through the corresponding HR counterpart is unavailable. Recently, many real-world SR researches take advantage of the unpaired image-to-image translation technique. That is, they used two or more generative adversarial networks (GANs), each of which translates images from one domain to another domain, \eg, translates images from the HR domain to the LR domain. However, it is not easy to stably learn such a translation with GANs using unpaired data. In this study, we present a simple and efficient method of training of real-world SR network. To stably train the network, we use statistics of an image patch, such as means and variances. Our real-world SR framework consists of two GANs, one for translating HR images to LR images (degradation task) and the other for translating LR to HR (SR task). We argue that the unpaired image translation using GANs can be learned efficiently with our proposed data sampling strategy, namely, variance matching. We test our method on the NTIRE 2020 real-world SR dataset. Our method outperforms the current state-of-the-art method in terms of the SSIM metric as well as produces comparable results on the LPIPS metric.



### Source-Free Domain Adaptive Fundus Image Segmentation with Denoised Pseudo-Labeling
- **Arxiv ID**: http://arxiv.org/abs/2109.09735v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.09735v1)
- **Published**: 2021-09-19 06:38:21+00:00
- **Updated**: 2021-09-19 06:38:21+00:00
- **Authors**: Cheng Chen, Quande Liu, Yueming Jin, Qi Dou, Pheng-Ann Heng
- **Comment**: Accepted to MICCAI 2021
- **Journal**: None
- **Summary**: Domain adaptation typically requires to access source domain data to utilize their distribution information for domain alignment with the target data. However, in many real-world scenarios, the source data may not be accessible during the model adaptation in the target domain due to privacy issue. This paper studies the practical yet challenging source-free unsupervised domain adaptation problem, in which only an existing source model and the unlabeled target data are available for model adaptation. We present a novel denoised pseudo-labeling method for this problem, which effectively makes use of the source model and unlabeled target data to promote model self-adaptation from pseudo labels. Importantly, considering that the pseudo labels generated from source model are inevitably noisy due to domain shift, we further introduce two complementary pixel-level and class-level denoising schemes with uncertainty estimation and prototype estimation to reduce noisy pseudo labels and select reliable ones to enhance the pseudo-labeling efficacy. Experimental results on cross-domain fundus image segmentation show that without using any source images or altering source training, our approach achieves comparable or even higher performance than state-of-the-art source-dependent unsupervised domain adaptation methods.



### Efficient Urban-scale Point Clouds Segmentation with BEV Projection
- **Arxiv ID**: http://arxiv.org/abs/2109.09074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09074v1)
- **Published**: 2021-09-19 06:49:59+00:00
- **Updated**: 2021-09-19 06:49:59+00:00
- **Authors**: Zhenhong Zou, Yizhe Li
- **Comment**: None
- **Journal**: None
- **Summary**: Point clouds analysis has grasped researchers' eyes in recent years, while 3D semantic segmentation remains a problem. Most deep point clouds models directly conduct learning on 3D point clouds, which will suffer from the severe sparsity and extreme data processing load in urban-scale data. To tackle the challenge, we propose to transfer the 3D point clouds to dense bird's-eye-view projection. In this case, the segmentation task is simplified because of class unbalance reduction and the feasibility of leveraging various 2D segmentation methods. We further design an attention-based fusion network that can conduct multi-modal learning on the projected images. Finally, the 2D out are remapped to generate 3D semantic segmentation results. To demonstrate the benefits of our method, we conduct various experiments on the SensatUrban dataset, in which our model presents competitive evaluation results (61.17% mIoU and 91.37% OverallAccuracy). We hope our work can inspire further exploration in point cloud analysis.



### DECORAS: detection and characterization of radio-astronomical sources using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2109.09077v2
- **DOI**: 10.1093/mnras/stab3519
- **Categories**: **astro-ph.IM**, astro-ph.GA, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09077v2)
- **Published**: 2021-09-19 08:03:36+00:00
- **Updated**: 2021-09-21 12:07:42+00:00
- **Authors**: S. Rezaei, J. P. McKean, M. Biehl, A. Javadpour
- **Comment**: submitted to MNRAS
- **Journal**: None
- **Summary**: We present DECORAS, a deep learning based approach to detect both point and extended sources from Very Long Baseline Interferometry (VLBI) observations. Our approach is based on an encoder-decoder neural network architecture that uses a low number of convolutional layers to provide a scalable solution for source detection. In addition, DECORAS performs source characterization in terms of the position, effective radius and peak brightness of the detected sources. We have trained and tested the network with images that are based on realistic Very Long Baseline Array (VLBA) observations at 20 cm. Also, these images have not gone through any prior de-convolution step and are directly related to the visibility data via a Fourier transform. We find that the source catalog generated by DECORAS has a better overall completeness and purity, when compared to a traditional source detection algorithm. DECORAS is complete at the 7.5$\sigma$ level, and has an almost factor of two improvement in reliability at 5.5$\sigma$. We find that DECORAS can recover the position of the detected sources to within 0.61 $\pm$ 0.69 mas, and the effective radius and peak surface brightness are recovered to within 20 per cent for 98 and 94 per cent of the sources, respectively. Overall, we find that DECORAS provides a reliable source detection and characterization solution for future wide-field VLBI surveys.



### Towards robustness under occlusion for face recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.09083v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T45 (Primary) 68T10, 68T07 (Secondary), I.4.9; I.5.4; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2109.09083v1)
- **Published**: 2021-09-19 08:27:57+00:00
- **Updated**: 2021-09-19 08:27:57+00:00
- **Authors**: Tomas M. Borges, Teofilo E. de Campos, Ricardo de Queiroz
- **Comment**: 7 pages, 8 figures
- **Journal**: None
- **Summary**: In this paper, we evaluate the effects of occlusions in the performance of a face recognition pipeline that uses a ResNet backbone. The classifier was trained on a subset of the CelebA-HQ dataset containing 5,478 images from 307 classes, to achieve top-1 error rate of 17.91%. We designed 8 different occlusion masks which were applied to the input images. This caused a significant drop in the classifier performance: its error rate for each mask became at least two times worse than before. In order to increase robustness under occlusions, we followed two approaches. The first is image inpainting using the pre-trained pluralistic image completion network. The second is Cutmix, a regularization strategy consisting of mixing training images and their labels using rectangular patches, making the classifier more robust against input corruptions. Both strategies revealed effective and interesting results were observed. In particular, the Cutmix approach makes the network more robust without requiring additional steps at the application time, though its training time is considerably longer. Our datasets containing the different occlusion masks as well as their inpainted counterparts are made publicly available to promote research on the field.



### Low-resolution Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.09090v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09090v1)
- **Published**: 2021-09-19 09:13:57+00:00
- **Updated**: 2021-09-19 09:13:57+00:00
- **Authors**: Chen Wang, Feng Zhang, Xiatian Zhu, Shuzhi Sam Ge
- **Comment**: 29 pages, 6 figures, under review of Pattern Recognition
- **Journal**: None
- **Summary**: Human pose estimation has achieved significant progress on images with high imaging resolution. However, low-resolution imagery data bring nontrivial challenges which are still under-studied. To fill this gap, we start with investigating existing methods and reveal that the most dominant heatmap-based methods would suffer more severe model performance degradation from low-resolution, and offset learning is an effective strategy. Established on this observation, in this work we propose a novel Confidence-Aware Learning (CAL) method which further addresses two fundamental limitations of existing offset learning methods: inconsistent training and testing, decoupled heatmap and offset learning. Specifically, CAL selectively weighs the learning of heatmap and offset with respect to ground-truth and most confident prediction, whilst capturing the statistical importance of model output in mini-batch learning manner. Extensive experiments conducted on the COCO benchmark show that our method outperforms significantly the state-of-the-art methods for low-resolution human pose estimation.



### HPTQ: Hardware-Friendly Post Training Quantization
- **Arxiv ID**: http://arxiv.org/abs/2109.09113v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.09113v3)
- **Published**: 2021-09-19 12:45:01+00:00
- **Updated**: 2021-11-16 08:44:39+00:00
- **Authors**: Hai Victor Habi, Reuven Peretz, Elad Cohen, Lior Dikstein, Oranit Dror, Idit Diamant, Roy H. Jennings, Arnon Netzer
- **Comment**: None
- **Journal**: None
- **Summary**: Neural network quantization enables the deployment of models on edge devices. An essential requirement for their hardware efficiency is that the quantizers are hardware-friendly: uniform, symmetric, and with power-of-two thresholds. To the best of our knowledge, current post-training quantization methods do not support all of these constraints simultaneously. In this work, we introduce a hardware-friendly post training quantization (HPTQ) framework, which addresses this problem by synergistically combining several known quantization methods. We perform a large-scale study on four tasks: classification, object detection, semantic segmentation and pose estimation over a wide variety of network architectures. Our extensive experiments show that competitive results can be obtained under hardware-friendly constraints.



### ComicGAN: Text-to-Comic Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2109.09120v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09120v1)
- **Published**: 2021-09-19 13:31:32+00:00
- **Updated**: 2021-09-19 13:31:32+00:00
- **Authors**: Ben Proven-Bessel, Zilong Zhao, Lydia Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Drawing and annotating comic illustrations is a complex and difficult process. No existing machine learning algorithms have been developed to create comic illustrations based on descriptions of illustrations, or the dialogue in comics. Moreover, it is not known if a generative adversarial network (GAN) can generate original comics that correspond to the dialogue and/or descriptions. GANs are successful in producing photo-realistic images, but this technology does not necessarily translate to generation of flawless comics. What is more, comic evaluation is a prominent challenge as common metrics such as Inception Score will not perform comparably, as they are designed to work on photos. In this paper: 1. We implement ComicGAN, a novel text-to-comic pipeline based on a text-to-image GAN that synthesizes comics according to text descriptions. 2. We describe an in-depth empirical study of the technical difficulties of comic generation using GAN's. ComicGAN has two novel features: (i) text description creation from labels via permutation and augmentation, and (ii) custom image encoding with Convolutional Neural Networks. We extensively evaluate the proposed ComicGAN in two scenarios, namely image generation from descriptions, and image generation from dialogue. Our results on 1000 Dilbert comic panels and 6000 descriptions show synthetic comic panels from text inputs resemble original Dilbert panels. Novel methods for text description creation and custom image encoding brought improvements to Frechet Inception Distance, detail, and overall image quality over baseline algorithms. Generating illustrations from descriptions provided clear comics including characters and colours that were specified in the descriptions.



### Identifying Autism Spectrum Disorder Based on Individual-Aware Down-Sampling and Multi-Modal Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.09129v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, J.3; I.5.4; I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2109.09129v4)
- **Published**: 2021-09-19 14:22:55+00:00
- **Updated**: 2021-10-25 08:26:54+00:00
- **Authors**: Li Pan, Jundong Liu, Mingqin Shi, Chi Wah Wong, Kei Hang Katie Chan
- **Comment**: for code and support documents, see
  https://github.com/jhonP-Li/ASD_GP_GCN
- **Journal**: None
- **Summary**: Autism Spectrum Disorder(ASD) is a set of neurodevelopmental conditions that affect patients' social abilities. In recent years, many studies have employed deep learning to diagnose this brain dysfunction through functional MRI (fMRI). However, existing approaches solely focused on the abnormal brain functional connections but ignored the impact of regional activities. Due to this biased prior knowledge, previous diagnosis models suffered from inter-site measurement heterogeneity and inter-individual phenotypic differences. To address this issue, we propose a novel feature extraction method for fMRI that can learn a personalized lower-resolution representation of the entire brain networking regarding both the functional connections and regional activities. Specifically, we abstract the brain imaging as a graph structure and straightforwardly downsample it to substructures by hierarchical graph pooling. To further recalibrate the distribution of the extracted features under phenotypic information, we subsequently embed the sparse feature vectors into a population graph, where the hidden inter-subject heterogeneity and homogeneity are explicitly expressed as inter- and intra-community connectivity differences, and utilize Graph Convolutional Networks to learn the node embeddings. By these means, our framework can extract features directly and efficiently from the entire fMRI and be aware of implicit inter-individual variance. We have evaluated our framework on the ABIDE-I dataset with 10-fold cross-validation. The present model has achieved a mean classification accuracy of 87.62\% and a mean AUC of 0.92, better than the state-of-the-art methods.



### RSI-Net: Two-Stream Deep Neural Network for Remote Sensing Imagesbased Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.09148v2
- **DOI**: 10.1109/ACCESS.2022.3163535
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.09148v2)
- **Published**: 2021-09-19 15:57:20+00:00
- **Updated**: 2022-04-01 02:31:26+00:00
- **Authors**: Shuang He, Xia Lu, Jason Gu, Haitong Tang, Qin Yu, Kaiyue Liu, Haozhou Ding, Chunqi Chang, Nizhuan Wang
- **Comment**: 14pages, 8 figures, 4 tables
- **Journal**: IEEE Access,2022,10:1-14
- **Summary**: For semantic segmentation of remote sensing images (RSI), trade-off between representation power and location accuracy is quite important. How to get the trade-off effectively is an open question,where current approaches of utilizing very deep models result in complex models with large memory consumption. In contrast to previous work that utilizes dilated convolutions or deep models, we propose a novel two-stream deep neural network for semantic segmentation of RSI (RSI-Net) to obtain improved performance through modeling and propagating spatial contextual structure effectively and a decoding scheme with image-level and graph-level combination. The first component explicitly models correlations between adjacent land covers and conduct flexible convolution on arbitrarily irregular image regions by using graph convolutional network, while densely connected atrous convolution network (DenseAtrousCNet) with multi-scale atrous convolution can expand the receptive fields and obtain image global information. Extensive experiments are implemented on the Vaihingen, Potsdam and Gaofen RSI datasets, where the comparison results demonstrate the superior performance of RSI-Net in terms of overall accuracy (91.83%, 93.31% and 93.67% on three datasets, respectively), F1 score (90.3%, 91.49% and 89.35% on three datasets, respectively) and kappa coefficient (89.46%, 90.46% and 90.37% on three datasets, respectively) when compared with six state-of-the-art RSI semantic segmentation methods.



### LODE: Deep Local Deblurring and A New Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2109.09149v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09149v1)
- **Published**: 2021-09-19 15:57:28+00:00
- **Updated**: 2021-09-19 15:57:28+00:00
- **Authors**: Zerun Wang, Liuyu Xiang, Fan Yang, Jinzhao Qian, Jie Hu, Haidong Huang, Jungong Han, Yuchen Guo, Guiguang Ding
- **Comment**: None
- **Journal**: None
- **Summary**: While recent deep deblurring algorithms have achieved remarkable progress, most existing methods focus on the global deblurring problem, where the image blur mostly arises from severe camera shake. We argue that the local blur, which is mostly derived from moving objects with a relatively static background, is prevalent but remains under-explored. In this paper, we first lay the data foundation for local deblurring by constructing, for the first time, a LOcal-DEblur (LODE) dataset consisting of 3,700 real-world captured locally blurred images and their corresponding ground-truth. Then, we propose a novel framework, termed BLur-Aware DEblurring network (BladeNet), which contains three components: the Local Blur Synthesis module generates locally blurred training pairs, the Local Blur Perception module automatically captures the locally blurred region and the Blur-guided Spatial Attention module guides the deblurring network with spatial attention. This framework is flexible such that it can be combined with many existing SotA algorithms. We carry out extensive experiments on REDS and LODE datasets showing that BladeNet improves PSNR by 2.5dB over SotAs for local deblurring while keeping comparable performance for global deblurring. We will publish the dataset and codes.



### A Study of the Generalizability of Self-Supervised Representations
- **Arxiv ID**: http://arxiv.org/abs/2109.09150v1
- **DOI**: 10.1016/j.mlwa.2021.100124
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.09150v1)
- **Published**: 2021-09-19 15:57:37+00:00
- **Updated**: 2021-09-19 15:57:37+00:00
- **Authors**: Atharva Tendle, Mohammad Rashedul Hasan
- **Comment**: Journal of Machine Learning With Applications (MLWA)
- **Journal**: Volume 6, 2021, 100124
- **Summary**: Recent advancements in self-supervised learning (SSL) made it possible to learn generalizable visual representations from unlabeled data. The performance of Deep Learning models fine-tuned on pretrained SSL representations is on par with models fine-tuned on the state-of-the-art supervised learning (SL) representations. Irrespective of the progress made in SSL, its generalizability has not been studied extensively. In this article, we perform a deeper analysis of the generalizability of pretrained SSL and SL representations by conducting a domain-based study for transfer learning classification tasks. The representations are learned from the ImageNet source data, which are then fine-tuned using two types of target datasets: similar to the source dataset, and significantly different from the source dataset. We study generalizability of the SSL and SL-based models via their prediction accuracy as well as prediction confidence. In addition to this, we analyze the attribution of the final convolutional layer of these models to understand how they reason about the semantic identity of the data. We show that the SSL representations are more generalizable as compared to the SL representations. We explain the generalizability of the SSL representations by investigating its invariance property, which is shown to be better than that observed in the SL representations.



### CaTGrasp: Learning Category-Level Task-Relevant Grasping in Clutter from Simulation
- **Arxiv ID**: http://arxiv.org/abs/2109.09163v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2109.09163v2)
- **Published**: 2021-09-19 16:48:33+00:00
- **Updated**: 2022-02-25 20:09:44+00:00
- **Authors**: Bowen Wen, Wenzhao Lian, Kostas Bekris, Stefan Schaal
- **Comment**: IEEE International Conference on Robotics and Automation (ICRA) 2022
- **Journal**: None
- **Summary**: Task-relevant grasping is critical for industrial assembly, where downstream manipulation tasks constrain the set of valid grasps. Learning how to perform this task, however, is challenging, since task-relevant grasp labels are hard to define and annotate. There is also yet no consensus on proper representations for modeling or off-the-shelf tools for performing task-relevant grasps. This work proposes a framework to learn task-relevant grasping for industrial objects without the need of time-consuming real-world data collection or manual annotation. To achieve this, the entire framework is trained solely in simulation, including supervised training with synthetic label generation and self-supervised, hand-object interaction. In the context of this framework, this paper proposes a novel, object-centric canonical representation at the category level, which allows establishing dense correspondence across object instances and transferring task-relevant grasps to novel instances. Extensive experiments on task-relevant grasping of densely-cluttered industrial objects are conducted in both simulation and real-world setups, demonstrating the effectiveness of the proposed framework. Code and data are available at https://sites.google.com/view/catgrasp.



### Traffic-Net: 3D Traffic Monitoring Using a Single Camera
- **Arxiv ID**: http://arxiv.org/abs/2109.09165v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09165v2)
- **Published**: 2021-09-19 16:59:01+00:00
- **Updated**: 2022-07-02 23:56:36+00:00
- **Authors**: Mahdi Rezaei, Mohsen Azarmi, Farzam Mohammad Pour Mir
- **Comment**: None
- **Journal**: None
- **Summary**: Computer Vision has played a major role in Intelligent Transportation Systems (ITS) and traffic surveillance. Along with the rapidly growing automated vehicles and crowded cities, the automated and advanced traffic management systems (ATMS) using video surveillance infrastructures have been evolved by the implementation of Deep Neural Networks. In this research, we provide a practical platform for real-time traffic monitoring, including 3D vehicle/pedestrian detection, speed detection, trajectory estimation, congestion detection, as well as monitoring the interaction of vehicles and pedestrians, all using a single CCTV traffic camera. We adapt a custom YOLOv5 deep neural network model for vehicle/pedestrian detection and an enhanced SORT tracking algorithm. For the first time, a hybrid satellite-ground based inverse perspective mapping (SG-IPM) method for camera auto-calibration is also developed which leads to an accurate 3D object detection and visualisation. We also develop a hierarchical traffic modelling solution based on short- and long-term temporal video data stream to understand the traffic flow, bottlenecks, and risky spots for vulnerable road users. Several experiments on real-world scenarios and comparisons with state-of-the-art are conducted using various traffic monitoring datasets, including MIO-TCD, UA-DETRAC and GRAM-RTM collected from highways, intersections, and urban areas under different lighting and weather conditions.



### Unsupervised 3D Pose Estimation for Hierarchical Dance Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.09166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.09166v1)
- **Published**: 2021-09-19 16:59:37+00:00
- **Updated**: 2021-09-19 16:59:37+00:00
- **Authors**: Xiaodan Hu, Narendra Ahuja
- **Comment**: To appear in ICCV2021
- **Journal**: None
- **Summary**: Dance experts often view dance as a hierarchy of information, spanning low-level (raw images, image sequences), mid-levels (human poses and bodypart movements), and high-level (dance genre). We propose a Hierarchical Dance Video Recognition framework (HDVR). HDVR estimates 2D pose sequences, tracks dancers, and then simultaneously estimates corresponding 3D poses and 3D-to-2D imaging parameters, without requiring ground truth for 3D poses. Unlike most methods that work on a single person, our tracking works on multiple dancers, under occlusions. From the estimated 3D pose sequence, HDVR extracts body part movements, and therefrom dance genre. The resulting hierarchical dance representation is explainable to experts. To overcome noise and interframe correspondence ambiguities, we enforce spatial and temporal motion smoothness and photometric continuity over time. We use an LSTM network to extract 3D movement subsequences from which we recognize the dance genre. For experiments, we have identified 154 movement types, of 16 body parts, and assembled a new University of Illinois Dance (UID) Dataset, containing 1143 video clips of 9 genres covering 30 hours, annotated with movement and genre labels. Our experimental results demonstrate that our algorithms outperform the state-of-the-art 3D pose estimation methods, which also enhances our dance recognition performance.



### Unsupervised Domain Adaptation with Semantic Consistency across Heterogeneous Modalities for MRI Prostate Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.09736v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09736v1)
- **Published**: 2021-09-19 17:33:26+00:00
- **Updated**: 2021-09-19 17:33:26+00:00
- **Authors**: Eleni Chiou, Francesco Giganti, Shonit Punwani, Iasonas Kokkinos, Eleftheria Panagiotaki
- **Comment**: Accepted at MICCAI 2021 Workshop on Domain Adaptation and
  Representation Transfer (DART). arXiv admin note: text overlap with
  arXiv:2010.07411
- **Journal**: None
- **Summary**: Any novel medical imaging modality that differs from previous protocols e.g. in the number of imaging channels, introduces a new domain that is heterogeneous from previous ones. This common medical imaging scenario is rarely considered in the domain adaptation literature, which handles shifts across domains of the same dimensionality. In our work we rely on stochastic generative modeling to translate across two heterogeneous domains at pixel space and introduce two new loss functions that promote semantic consistency. Firstly, we introduce a semantic cycle-consistency loss in the source domain to ensure that the translation preserves the semantics. Secondly, we introduce a pseudo-labelling loss, where we translate target data to source, label them by a source-domain network, and use the generated pseudo-labels to supervise the target-domain network. Our results show that this allows us to extract systematically better representations for the target domain. In particular, we address the challenge of enhancing performance on VERDICT-MRI, an advanced diffusion-weighted imaging technique, by exploiting labeled mp-MRI data. When compared to several unsupervised domain adaptation approaches, our approach yields substantial improvements, that consistently carry over to the semi-supervised and supervised learning settings.



### DeepPoint: A Deep Learning Model for 3D Reconstruction in Point Clouds via mmWave Radar
- **Arxiv ID**: http://arxiv.org/abs/2109.09188v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.09188v1)
- **Published**: 2021-09-19 18:28:20+00:00
- **Updated**: 2021-09-19 18:28:20+00:00
- **Authors**: Yue Sun, Honggang Zhang, Zhuoming Huang, Benyuan Liu
- **Comment**: arXiv admin note: text overlap with arXiv:2108.02858
- **Journal**: None
- **Summary**: Recent research has shown that mmWave radar sensing is effective for object detection in low visibility environments, which makes it an ideal technique in autonomous navigation systems such as autonomous vehicles. However, due to the characteristics of radar signals such as sparsity, low resolution, specularity, and high noise, it is still quite challenging to reconstruct 3D object shapes via mmWave radar sensing. Built on our recent proposed 3DRIMR (3D Reconstruction and Imaging via mmWave Radar), we introduce in this paper DeepPoint, a deep learning model that generates 3D objects in point cloud format that significantly outperforms the original 3DRIMR design. The model adopts a conditional Generative Adversarial Network (GAN) based deep neural network architecture. It takes as input the 2D depth images of an object generated by 3DRIMR's Stage 1, and outputs smooth and dense 3D point clouds of the object. The model consists of a novel generator network that utilizes a sequence of DeepPoint blocks or layers to extract essential features of the union of multiple rough and sparse input point clouds of an object when observed from various viewpoints, given that those input point clouds may contain many incorrect points due to the imperfect generation process of 3DRIMR's Stage 1. The design of DeepPoint adopts a deep structure to capture the global features of input point clouds, and it relies on an optimally chosen number of DeepPoint blocks and skip connections to achieve performance improvement over the original 3DRIMR design. Our experiments have demonstrated that this model significantly outperforms the original 3DRIMR and other standard techniques in reconstructing 3D objects.



### Capsule networks with non-iterative cluster routing
- **Arxiv ID**: http://arxiv.org/abs/2109.09213v1
- **DOI**: 10.1016/j.neunet.2021.07.032
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.09213v1)
- **Published**: 2021-09-19 20:14:22+00:00
- **Updated**: 2021-09-19 20:14:22+00:00
- **Authors**: Zhihao Zhao, Samuel Cheng
- **Comment**: None
- **Journal**: Elsevier Neural Networks, Volume 143, November 2021
- **Summary**: Capsule networks use routing algorithms to flow information between consecutive layers. In the existing routing procedures, capsules produce predictions (termed votes) for capsules of the next layer. In a nutshell, the next-layer capsule's input is a weighted sum over all the votes it receives. In this paper, we propose non-iterative cluster routing for capsule networks. In the proposed cluster routing, capsules produce vote clusters instead of individual votes for next-layer capsules, and each vote cluster sends its centroid to a next-layer capsule. Generally speaking, the next-layer capsule's input is a weighted sum over the centroid of each vote cluster it receives. The centroid that comes from a cluster with a smaller variance is assigned a larger weight in the weighted sum process. Compared with the state-of-the-art capsule networks, the proposed capsule networks achieve the best accuracy on the Fashion-MNIST and SVHN datasets with fewer parameters, and achieve the best accuracy on the smallNORB and CIFAR-10 datasets with a moderate number of parameters. The proposed capsule networks also produce capsules with disentangled representation and generalize well to images captured at novel viewpoints. The proposed capsule networks also preserve 2D spatial information of an input image in the capsule channels: if the capsule channels are rotated, the object reconstructed from these channels will be rotated by the same transformation. Codes are available at https://github.com/ZHAOZHIHAO/ClusterRouting.



### Initial Test of "BabyRobot" Behaviour on a Teleoperated Toy Substitution: Improving the Motor Skills of Toddlers
- **Arxiv ID**: http://arxiv.org/abs/2109.09223v3
- **DOI**: 10.5555/3523760.3523860
- **Categories**: **cs.RO**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2109.09223v3)
- **Published**: 2021-09-19 21:00:44+00:00
- **Updated**: 2023-03-16 08:28:00+00:00
- **Authors**: Eric Canas, Alba M. G. Garcia, Anais Garrell, Cecilio Angulo
- **Comment**: None
- **Journal**: Proceedings of the 2022 ACM/IEEE International Conference on
  Human-Robot Interaction (HRI'22). IEEE Press, 708-712
- **Summary**: This article introduces "Baby Robot", a robot aiming to improve motor skills of babies and toddlers. Authors developed a car-like toy that moves autonomously using reinforcement learning and computer vision techniques. The robot behaviour is to escape from a target baby that has been previously recognized, or at least detected, while avoiding obstacles, so that the security of the baby is not compromised. A myriad of commercial toys with a similar mobility improvement purpose are into the market; however, there is no one that bets for an intelligent autonomous movement, as they perform simple yet repetitive trajectories in the best of the cases. Two crawling toys -- one in representation of "Baby Robot" -- were tested in a real environment with respect to regular toys in order to check how they improved the toddlers mobility. These real-life experiments were conducted with our proposed robot in a kindergarten, where a group of children interacted with the toys. Significant improvement in the motion skills of participants were detected.



### Robust Framework for COVID-19 Identification from a Multicenter Dataset of Chest CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2109.09241v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.09241v3)
- **Published**: 2021-09-19 22:32:55+00:00
- **Updated**: 2022-07-28 22:24:48+00:00
- **Authors**: Sadaf Khademi, Shahin Heidarian, Parnian Afshar, Nastaran Enshaei, Farnoosh Naderkhani, Moezedin Javad Rafiee, Anastasia Oikonomou, Akbar Shafiee, Faranak Babaki Fard, Konstantinos N. Plataniotis, Arash Mohammadi
- **Comment**: None
- **Journal**: None
- **Summary**: The objective of this study is to develop a robust deep learning-based framework to distinguish COVID-19, Community-Acquired Pneumonia (CAP), and Normal cases based on chest CT scans acquired in different imaging centers using various protocols, and radiation doses. We showed that while our proposed model is trained on a relatively small dataset acquired from only one imaging center using a specific scanning protocol, the model performs well on heterogeneous test sets obtained by multiple scanners using different technical parameters. We also showed that the model can be updated via an unsupervised approach to cope with the data shift between the train and test sets and enhance the robustness of the model upon receiving a new external dataset from a different center. We adopted an ensemble architecture to aggregate the predictions from multiple versions of the model. For initial training and development purposes, an in-house dataset of 171 COVID-19, 60 CAP, and 76 Normal cases was used, which contained volumetric CT scans acquired from one imaging center using a constant standard radiation dose scanning protocol. To evaluate the model, we collected four different test sets retrospectively to investigate the effects of the shifts in the data characteristics on the model's performance. Among the test cases, there were CT scans with similar characteristics as the train set as well as noisy low-dose and ultra-low dose CT scans. In addition, some test CT scans were obtained from patients with a history of cardiovascular diseases or surgeries. The entire test dataset used in this study contained 51 COVID-19, 28 CAP, and 51 Normal cases. Experimental results indicate that our proposed framework performs well on all test sets achieving total accuracy of 96.15% (95%CI: [91.25-98.74]), COVID-19 sensitivity of 96.08% (95%CI: [86.54-99.5]), CAP sensitivity of 92.86% (95%CI: [76.50-99.19]).



