# Arxiv Papers in cs.CV on 2021-09-28
### Fine-tuning Vision Transformers for the Prediction of State Variables in Ising Models
- **Arxiv ID**: http://arxiv.org/abs/2109.13925v2
- **DOI**: None
- **Categories**: **cs.CV**, cond-mat.stat-mech, cs.LG, physics.comp-ph
- **Links**: [PDF](http://arxiv.org/pdf/2109.13925v2)
- **Published**: 2021-09-28 00:23:31+00:00
- **Updated**: 2021-11-30 04:27:14+00:00
- **Authors**: Onur Kara, Arijit Sehanobish, Hector H Corzo
- **Comment**: Accepted at Ml4Physical Sciences Workshop at Neurips 2021
- **Journal**: None
- **Summary**: Transformers are state-of-the-art deep learning models that are composed of stacked attention and point-wise, fully connected layers designed for handling sequential data. Transformers are not only ubiquitous throughout Natural Language Processing (NLP), but, recently, they have inspired a new wave of Computer Vision (CV) applications research. In this work, a Vision Transformer (ViT) is applied to predict the state variables of 2-dimensional Ising model simulations. Our experiments show that ViT outperform state-of-the-art Convolutional Neural Networks (CNN) when using a small number of microstate images from the Ising model corresponding to various boundary conditions and temperatures. This work opens the possibility of applying ViT to other simulations, and raises interesting research directions on how attention maps can learn about the underlying physics governing different phenomena.



### KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D
- **Arxiv ID**: http://arxiv.org/abs/2109.13410v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13410v2)
- **Published**: 2021-09-28 00:41:29+00:00
- **Updated**: 2022-06-03 07:09:53+00:00
- **Authors**: Yiyi Liao, Jun Xie, Andreas Geiger
- **Comment**: arXiv admin note: text overlap with arXiv:1511.03240
- **Journal**: None
- **Summary**: For the last few decades, several major subfields of artificial intelligence including computer vision, graphics, and robotics have progressed largely independently from each other. Recently, however, the community has realized that progress towards robust intelligent systems such as self-driving cars requires a concerted effort across the different fields. This motivated us to develop KITTI-360, successor of the popular KITTI dataset. KITTI-360 is a suburban driving dataset which comprises richer input modalities, comprehensive semantic instance annotations and accurate localization to facilitate research at the intersection of vision, graphics and robotics. For efficient annotation, we created a tool to label 3D scenes with bounding primitives and developed a model that transfers this information into the 2D image domain, resulting in over 150k images and 1B 3D points with coherent semantic instance annotations across 2D and 3D. Moreover, we established benchmarks and baselines for several tasks relevant to mobile perception, encompassing problems from computer vision, graphics, and robotics on the same dataset, e.g., semantic scene understanding, novel view synthesis and semantic SLAM. KITTI-360 will enable progress at the intersection of these research areas and thus contribute towards solving one of today's grand challenges: the development of fully autonomous self-driving systems.



### Discriminative Attribution from Counterfactuals
- **Arxiv ID**: http://arxiv.org/abs/2109.13412v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.13412v1)
- **Published**: 2021-09-28 00:53:34+00:00
- **Updated**: 2021-09-28 00:53:34+00:00
- **Authors**: Nils Eckstein, Alexander S. Bates, Gregory S. X. E. Jefferis, Jan Funke
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for neural network interpretability by combining feature attribution with counterfactual explanations to generate attribution maps that highlight the most discriminative features between pairs of classes. We show that this method can be used to quantitatively evaluate the performance of feature attribution methods in an objective manner, thus preventing potential observer bias. We evaluate the proposed method on three diverse datasets, including a challenging artificial dataset and real-world biological data. We show quantitatively and qualitatively that the highlighted features are substantially more discriminative than those extracted using conventional attribution methods and argue that this type of explanation is better suited for understanding fine grained class differences as learned by a deep neural network.



### Evaluation of Deep Neural Network Domain Adaptation Techniques for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.13420v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.13420v1)
- **Published**: 2021-09-28 01:22:12+00:00
- **Updated**: 2021-09-28 01:22:12+00:00
- **Authors**: Alan Preciado-Grijalva, Venkata Santosh Sai Ramireddy Muthireddy
- **Comment**: None
- **Journal**: None
- **Summary**: It has been well proved that deep networks are efficient at extracting features from a given (source) labeled dataset. However, it is not always the case that they can generalize well to other (target) datasets which very often have a different underlying distribution. In this report, we evaluate four different domain adaptation techniques for image classification tasks: DeepCORAL, DeepDomainConfusion, CDAN and CDAN+E. These techniques are unsupervised given that the target dataset dopes not carry any labels during training phase. We evaluate model performance on the office-31 dataset. A link to the github repository of this report can be found here: https://github.com/agrija9/Deep-Unsupervised-Domain-Adaptation.



### Weakly Supervised Keypoint Discovery
- **Arxiv ID**: http://arxiv.org/abs/2109.13423v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13423v1)
- **Published**: 2021-09-28 01:26:53+00:00
- **Updated**: 2021-09-28 01:26:53+00:00
- **Authors**: Serim Ryou, Pietro Perona
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a method for keypoint discovery from a 2D image using image-level supervision. Recent works on unsupervised keypoint discovery reliably discover keypoints of aligned instances. However, when the target instances have high viewpoint or appearance variation, the discovered keypoints do not match the semantic correspondences over different images. Our work aims to discover keypoints even when the target instances have high viewpoint and appearance variation by using image-level supervision. Motivated by the weakly-supervised learning approach, our method exploits image-level supervision to identify discriminative parts and infer the viewpoint of the target instance. To discover diverse parts, we adopt a conditional image generation approach using a pair of images with structural deformation. Finally, we enforce a viewpoint-based equivariance constraint using the keypoints from the image-level supervision to resolve the spatial correlation problem that consistently appears in the images taken from various viewpoints. Our approach achieves state-of-the-art performance for the task of keypoint estimation on the limited supervision scenarios. Furthermore, the discovered keypoints are directly applicable to downstream tasks without requiring any keypoint labels.



### Warp-Refine Propagation: Semi-Supervised Auto-labeling via Cycle-consistency
- **Arxiv ID**: http://arxiv.org/abs/2109.13432v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.13432v1)
- **Published**: 2021-09-28 02:04:18+00:00
- **Updated**: 2021-09-28 02:04:18+00:00
- **Authors**: Aditya Ganeshan, Alexis Vallet, Yasunori Kudo, Shin-ichi Maeda, Tommi Kerola, Rares Ambrus, Dennis Park, Adrien Gaidon
- **Comment**: 16 pages, 12 figures, including supplementary material. To be
  published in ICCV 2021
- **Journal**: None
- **Summary**: Deep learning models for semantic segmentation rely on expensive, large-scale, manually annotated datasets. Labelling is a tedious process that can take hours per image. Automatically annotating video sequences by propagating sparsely labeled frames through time is a more scalable alternative. In this work, we propose a novel label propagation method, termed Warp-Refine Propagation, that combines semantic cues with geometric cues to efficiently auto-label videos. Our method learns to refine geometrically-warped labels and infuse them with learned semantic priors in a semi-supervised setting by leveraging cycle consistency across time. We quantitatively show that our method improves label-propagation by a noteworthy margin of 13.1 mIoU on the ApolloScape dataset. Furthermore, by training with the auto-labelled frames, we achieve competitive results on three semantic-segmentation benchmarks, improving the state-of-the-art by a large margin of 1.8 and 3.61 mIoU on NYU-V2 and KITTI, while matching the current best results on Cityscapes.



### Emergent Neural Network Mechanisms for Generalization to Objects in Novel Orientations
- **Arxiv ID**: http://arxiv.org/abs/2109.13445v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2109.13445v2)
- **Published**: 2021-09-28 02:48:00+00:00
- **Updated**: 2023-07-13 04:23:23+00:00
- **Authors**: Avi Cooper, Xavier Boix, Daniel Harari, Spandan Madan, Hanspeter Pfister, Tomotake Sasaki, Pawan Sinha
- **Comment**: None
- **Journal**: None
- **Summary**: The capability of Deep Neural Networks (DNNs) to recognize objects in orientations outside the distribution of the training data is not well understood. We present evidence that DNNs are capable of generalizing to objects in novel orientations by disseminating orientation-invariance obtained from familiar objects seen from many viewpoints. This capability strengthens when training the DNN with an increasing number of familiar objects, but only in orientations that involve 2D rotations of familiar orientations. We show that this dissemination is achieved via neurons tuned to common features between familiar and unfamiliar objects. These results implicate brain-like neural mechanisms for generalization.



### When in Doubt: Improving Classification Performance with Alternating Normalization
- **Arxiv ID**: http://arxiv.org/abs/2109.13449v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.13449v1)
- **Published**: 2021-09-28 02:55:42+00:00
- **Updated**: 2021-09-28 02:55:42+00:00
- **Authors**: Menglin Jia, Austin Reiter, Ser-Nam Lim, Yoav Artzi, Claire Cardie
- **Comment**: Findings of EMNLP 2021
- **Journal**: None
- **Summary**: We introduce Classification with Alternating Normalization (CAN), a non-parametric post-processing step for classification. CAN improves classification accuracy for challenging examples by re-adjusting their predicted class probability distribution using the predicted class distributions of high-confidence validation examples. CAN is easily applicable to any probabilistic classifier, with minimal computation overhead. We analyze the properties of CAN using simulated experiments, and empirically demonstrate its effectiveness across a diverse set of classification tasks.



### SiamEvent: Event-based Object Tracking via Edge-aware Similarity Learning with Siamese Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.13456v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.13456v1)
- **Published**: 2021-09-28 03:14:24+00:00
- **Updated**: 2021-09-28 03:14:24+00:00
- **Authors**: Yujeong Chae, Lin Wang, Kuk-Jin Yoon
- **Comment**: None
- **Journal**: None
- **Summary**: Event cameras are novel sensors that perceive the per-pixel intensity changes and output asynchronous event streams, showing lots of advantages over traditional cameras, such as high dynamic range (HDR) and no motion blur. It has been shown that events alone can be used for object tracking by motion compensation or prediction. However, existing methods assume that the target always moves and is the stand-alone object. Moreover, they fail to track the stopped non-independent moving objects on fixed scenes. In this paper, we propose a novel event-based object tracking framework, called SiamEvent, using Siamese networks via edge-aware similarity learning. Importantly, to find the part having the most similar edge structure of target, we propose to correlate the embedded events at two timestamps to compute the target edge similarity. The Siamese network enables tracking arbitrary target edge by finding the part with the highest similarity score. This extends the possibility of event-based object tracking applied not only for the independent stand-alone moving objects, but also for various settings of the camera and scenes. In addition, target edge initialization and edge detector are also proposed to prevent SiamEvent from the drifting problem. Lastly, we built an open dataset including various synthetic and real scenes to train and evaluate SiamEvent. Extensive experiments demonstrate that SiamEvent achieves up to 15% tracking performance enhancement than the baselines on the real-world scenes and more robust tracking performance in the challenging HDR and motion blur conditions.



### Delve into the Performance Degradation of Differentiable Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2109.13466v1
- **DOI**: 10.1145/3459637.3482248
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.13466v1)
- **Published**: 2021-09-28 03:37:56+00:00
- **Updated**: 2021-09-28 03:37:56+00:00
- **Authors**: Jiuling Zhang, Zhiming Ding
- **Comment**: Accepted as a full paper at the CIKM 2021 conference
- **Journal**: None
- **Summary**: Differentiable architecture search (DARTS) is widely considered to be easy to overfit the validation set which leads to performance degradation. We first employ a series of exploratory experiments to verify that neither high-strength architecture parameters regularization nor warmup training scheme can effectively solve this problem. Based on the insights from the experiments, we conjecture that the performance of DARTS does not depend on the well-trained supernet weights and argue that the architecture parameters should be trained by the gradients which are obtained in the early stage rather than the final stage of training. This argument is then verified by exchanging the learning rate schemes of weights and parameters. Experimental results show that the simple swap of the learning rates can effectively solve the degradation and achieve competitive performance. Further empirical evidence suggests that the degradation is not a simple problem of the validation set overfitting but exhibit some links between the degradation and the operation selection bias within bilevel optimization dynamics. We demonstrate the generalization of this bias and propose to utilize this bias to achieve an operation-magnitude-based selective stop.



### Metal Artifact Reduction in 2D CT Images with Self-supervised Cross-domain Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.13483v1
- **DOI**: 10.1088/1361-6560/ac195c
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2109.13483v1)
- **Published**: 2021-09-28 04:40:57+00:00
- **Updated**: 2021-09-28 04:40:57+00:00
- **Authors**: Lequan Yu, Zhicheng Zhang, Xiaomeng Li, Hongyi Ren, Wei Zhao, Lei Xing
- **Comment**: Accepted by PMB
- **Journal**: None
- **Summary**: The presence of metallic implants often introduces severe metal artifacts in the X-ray CT images, which could adversely influence clinical diagnosis or dose calculation in radiation therapy. In this work, we present a novel deep-learning-based approach for metal artifact reduction (MAR). In order to alleviate the need for anatomically identical CT image pairs (i.e., metal artifact-corrupted CT image and metal artifact-free CT image) for network learning, we propose a self-supervised cross-domain learning framework. Specifically, we train a neural network to restore the metal trace region values in the given metal-free sinogram, where the metal trace is identified by the forward projection of metal masks. We then design a novel FBP reconstruction loss to encourage the network to generate more perfect completion results and a residual-learning-based image refinement module to reduce the secondary artifacts in the reconstructed CT images. To preserve the fine structure details and fidelity of the final MAR image, instead of directly adopting CNN-refined images as output, we incorporate the metal trace replacement into our framework and replace the metal-affected projections of the original sinogram with the prior sinogram generated by the forward projection of the CNN output. We then use the filtered backward projection (FBP) algorithms for final MAR image reconstruction. We conduct an extensive evaluation on simulated and real artifact data to show the effectiveness of our design. Our method produces superior MAR results and outperforms other compelling methods. We also demonstrate the potential of our framework for other organ sites.



### Towards Rotation Invariance in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.13488v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13488v2)
- **Published**: 2021-09-28 04:44:54+00:00
- **Updated**: 2021-10-01 00:59:31+00:00
- **Authors**: Agastya Kalra, Guy Stoppi, Bradley Brown, Rishav Agarwal, Achuta Kadambi
- **Comment**: Accepted ICCV 2021
- **Journal**: None
- **Summary**: Rotation augmentations generally improve a model's invariance/equivariance to rotation - except in object detection. In object detection the shape is not known, therefore rotation creates a label ambiguity. We show that the de-facto method for bounding box label rotation, the Largest Box Method, creates very large labels, leading to poor performance and in many cases worse performance than using no rotation at all. We propose a new method of rotation augmentation that can be implemented in a few lines of code. First, we create a differentiable approximation of label accuracy and show that axis-aligning the bounding box around an ellipse is optimal. We then introduce Rotation Uncertainty (RU) Loss, allowing the model to adapt to the uncertainty of the labels. On five different datasets (including COCO, PascalVOC, and Transparent Object Bin Picking), this approach improves the rotational invariance of both one-stage and two-stage architectures when measured with AP, AP50, and AP75. The code is available at https://github.com/akasha-imaging/ICCV2021.



### Modelling Neighbor Relation in Joint Space-Time Graph for Video Correspondence Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.13499v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13499v1)
- **Published**: 2021-09-28 05:40:01+00:00
- **Updated**: 2021-09-28 05:40:01+00:00
- **Authors**: Zixu Zhao, Yueming Jin, Pheng-Ann Heng
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: This paper presents a self-supervised method for learning reliable visual correspondence from unlabeled videos. We formulate the correspondence as finding paths in a joint space-time graph, where nodes are grid patches sampled from frames, and are linked by two types of edges: (i) neighbor relations that determine the aggregation strength from intra-frame neighbors in space, and (ii) similarity relations that indicate the transition probability of inter-frame paths across time. Leveraging the cycle-consistency in videos, our contrastive learning objective discriminates dynamic objects from both their neighboring views and temporal views. Compared with prior works, our approach actively explores the neighbor relations of central instances to learn a latent association between center-neighbor pairs (e.g., "hand -- arm") across time, thus improving the instance discrimination. Without fine-tuning, our learned representation outperforms the state-of-the-art self-supervised methods on a variety of visual tasks including video object propagation, part propagation, and pose keypoint tracking. Our self-supervised method also surpasses some fully supervised algorithms designed for the specific tasks.



### Random Dilated Shapelet Transform: A New Approach for Time Series Shapelets
- **Arxiv ID**: http://arxiv.org/abs/2109.13514v2
- **DOI**: 10.1007/978-3-031-09037-0_53
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.13514v2)
- **Published**: 2021-09-28 06:30:42+00:00
- **Updated**: 2022-04-21 07:59:49+00:00
- **Authors**: Antoine Guillaume, Christel Vrain, Elloumi Wael
- **Comment**: None
- **Journal**: None
- **Summary**: Shapelet-based algorithms are widely used for time series classification because of their ease of interpretation, but they are currently outperformed by recent state-of-the-art approaches. We present a new formulation of time series shapelets including the notion of dilation, and we introduce a new shapelet feature to enhance their discriminative power for classification. Experiments performed on 112 datasets show that our method improves on the state-of-the-art shapelet algorithm, and achieves comparable accuracy to recent state-of-the-art approaches, without sacrificing neither scalability, nor interpretability.



### Multi-Semantic Image Recognition Model and Evaluating Index for explaining the deep learning models
- **Arxiv ID**: http://arxiv.org/abs/2109.13531v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.13531v1)
- **Published**: 2021-09-28 07:18:05+00:00
- **Updated**: 2021-09-28 07:18:05+00:00
- **Authors**: Qianmengke Zhao, Ye Wang, Qun Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Although deep learning models are powerful among various applications, most deep learning models are still a black box, lacking verifiability and interpretability, which means the decision-making process that human beings cannot understand. Therefore, how to evaluate deep neural networks with explanations is still an urgent task. In this paper, we first propose a multi-semantic image recognition model, which enables human beings to understand the decision-making process of the neural network. Then, we presents a new evaluation index, which can quantitatively assess the model interpretability. We also comprehensively summarize the semantic information that affects the image classification results in the judgment process of neural networks. Finally, this paper also exhibits the relevant baseline performance with current state-of-the-art deep learning models.



### A hierarchical residual network with compact triplet-center loss for sketch recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.13536v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13536v1)
- **Published**: 2021-09-28 07:25:27+00:00
- **Updated**: 2021-09-28 07:25:27+00:00
- **Authors**: Lei Wang, Shihui Zhang, Huan He, Xiaoxiao Zhang, Yu Sang
- **Comment**: None
- **Journal**: None
- **Summary**: With the widespread use of touch-screen devices, it is more and more convenient for people to draw sketches on screen. This results in the demand for automatically understanding the sketches. Thus, the sketch recognition task becomes more significant than before. To accomplish this task, it is necessary to solve the critical issue of improving the distinction of the sketch features. To this end, we have made efforts in three aspects. First, a novel multi-scale residual block is designed. Compared with the conventional basic residual block, it can better perceive multi-scale information and reduce the number of parameters during training. Second, a hierarchical residual structure is built by stacking multi-scale residual blocks in a specific way. In contrast with the single-level residual structure, the learned features from this structure are more sufficient. Last but not least, the compact triplet-center loss is proposed specifically for the sketch recognition task. It can solve the problem that the triplet-center loss does not fully consider too large intra-class space and too small inter-class space in sketch field. By studying the above modules, a hierarchical residual network as a whole is proposed for sketch recognition and evaluated on Tu-Berlin benchmark thoroughly. The experimental results show that the proposed network outperforms most of baseline methods and it is excellent among non-sequential models at present.



### Physical Context and Timing Aware Sequence Generating GANs
- **Arxiv ID**: http://arxiv.org/abs/2110.04077v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2110.04077v1)
- **Published**: 2021-09-28 07:58:53+00:00
- **Updated**: 2021-09-28 07:58:53+00:00
- **Authors**: Hayato Futase, Tomoki Tsujimura, Tetsuya Kajimoto, Hajime Kawarazaki, Toshiyuki Suzuki, Makoto Miwa, Yutaka Sasaki
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have shown remarkable successes in generating realistic images and interpolating changes between images. Existing models, however, do not take into account physical contexts behind images in generating the images, which may cause unrealistic changes. Furthermore, it is difficult to generate the changes at a specific timing and they often do not match with actual changes. This paper proposes a novel GAN, named Physical Context and Timing aware sequence generating GANs (PCTGAN), that generates an image in a sequence at a specific timing between two images with considering physical contexts behind them. Our method consists of three components: an encoder, a generator, and a discriminator. The encoder estimates latent vectors from the beginning and ending images, their timings, and a target timing. The generator generates images and the physical contexts at the beginning, ending, and target timing from the corresponding latent vectors. The discriminator discriminates whether the generated images and contexts are real or not. In the experiments, PCTGAN is applied to a data set of sequential changes of shapes in die forging processes. We show that both timing and physical contexts are effective in generating sequential images.



### A Strong Baseline for the VIPriors Data-Efficient Image Classification Challenge
- **Arxiv ID**: http://arxiv.org/abs/2109.13561v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13561v1)
- **Published**: 2021-09-28 08:45:15+00:00
- **Updated**: 2021-09-28 08:45:15+00:00
- **Authors**: Björn Barz, Lorenzo Brigato, Luca Iocchi, Joachim Denzler
- **Comment**: None
- **Journal**: None
- **Summary**: Learning from limited amounts of data is the hallmark of intelligence, requiring strong generalization and abstraction skills. In a machine learning context, data-efficient methods are of high practical importance since data collection and annotation are prohibitively expensive in many domains. Thus, coordinated efforts to foster progress in this area emerged recently, e.g., in the form of dedicated workshops and competitions. Besides a common benchmark, measuring progress requires strong baselines. We present such a strong baseline for data-efficient image classification on the VIPriors challenge dataset, which is a sub-sampled version of ImageNet-1k with 100 images per class. We do not use any methods tailored to data-efficient classification but only standard models and techniques as well as common competition tricks and thorough hyper-parameter tuning. Our baseline achieves 69.7% accuracy on the VIPriors image classification dataset and outperforms 50% of submissions to the VIPriors 2021 challenge.



### Information Elevation Network for Fast Online Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.13572v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13572v1)
- **Published**: 2021-09-28 09:02:15+00:00
- **Updated**: 2021-09-28 09:02:15+00:00
- **Authors**: Sunah Min, Jinyoung Moon
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Online action detection (OAD) is a task that receives video segments within a streaming video as inputs and identifies ongoing actions within them. It is important to retain past information associated with a current action. However, long short-term memory (LSTM), a popular recurrent unit for modeling temporal information from videos, accumulates past information from the previous hidden and cell states and the extracted visual features at each timestep without considering the relationships between the past and current information. Consequently, the forget gate of the original LSTM can lose the accumulated information relevant to the current action because it determines which information to forget without considering the current action. We introduce a novel information elevation unit (IEU) that lifts up and accumulate the past information relevant to the current action in order to model the past information that is especially relevant to the current action. To the best of our knowledge, our IEN is the first attempt that considers the computational overhead for the practical use of OAD. Through ablation studies, we design an efficient and effective OAD network using IEUs, called an information elevation network (IEN). Our IEN uses visual features extracted by a fast action recognition network taking only RGB frames because extracting optical flows requires heavy computation overhead. On two OAD benchmark datasets, THUMOS-14 and TVSeries, our IEN outperforms state-of-the-art OAD methods using only RGB frames. Furthermore, on the THUMOS-14 dataset, our IEN outperforms the state-of-the-art OAD methods using two-stream features based on RGB frames and optical flows.



### Making Curiosity Explicit in Vision-based RL
- **Arxiv ID**: http://arxiv.org/abs/2109.13588v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML, I.2.6; I.2.8; I.2.9; I.2.10; I.5.0
- **Links**: [PDF](http://arxiv.org/pdf/2109.13588v1)
- **Published**: 2021-09-28 09:50:37+00:00
- **Updated**: 2021-09-28 09:50:37+00:00
- **Authors**: Elie Aljalbout, Maximilian Ulmer, Rudolph Triebel
- **Comment**: ICRA workshop on Curious Robots 2021
- **Journal**: None
- **Summary**: Vision-based reinforcement learning (RL) is a promising technique to solve control tasks involving images as the main observation. State-of-the-art RL algorithms still struggle in terms of sample efficiency, especially when using image observations. This has led to an increased attention on integrating state representation learning (SRL) techniques into the RL pipeline. Work in this field demonstrates a substantial improvement in sample efficiency among other benefits. However, to take full advantage of this paradigm, the quality of samples used for training plays a crucial role. More importantly, the diversity of these samples could affect the sample efficiency of vision-based RL, but also its generalization capability. In this work, we present an approach to improve the sample diversity. Our method enhances the exploration capability of the RL algorithms by taking advantage of the SRL setup. Our experiments show that the presented approach outperforms the baseline for all tested environments. These results are most apparent for environments where the baseline method struggles. Even in simple environments, our method stabilizes the training, reduces the reward variance and boosts sample efficiency.



### Efficient Global-Local Memory for Real-time Instrument Segmentation of Robotic Surgical Video
- **Arxiv ID**: http://arxiv.org/abs/2109.13593v1
- **DOI**: 10.1007/978-3-030-87202-1_33
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.13593v1)
- **Published**: 2021-09-28 10:10:14+00:00
- **Updated**: 2021-09-28 10:10:14+00:00
- **Authors**: Jiacheng Wang, Yueming Jin, Liansheng Wang, Shuntian Cai, Pheng-Ann Heng, Jing Qin
- **Comment**: None
- **Journal**: Medical Image Computing and Computer Assisted Intervention, 2021
- **Summary**: Performing a real-time and accurate instrument segmentation from videos is of great significance for improving the performance of robotic-assisted surgery. We identify two important clues for surgical instrument perception, including local temporal dependency from adjacent frames and global semantic correlation in long-range duration. However, most existing works perform segmentation purely using visual cues in a single frame. Optical flow is just used to model the motion between only two frames and brings heavy computational cost. We propose a novel dual-memory network (DMNet) to wisely relate both global and local spatio-temporal knowledge to augment the current features, boosting the segmentation performance and retaining the real-time prediction capability. We propose, on the one hand, an efficient local memory by taking the complementary advantages of convolutional LSTM and non-local mechanisms towards the relating reception field. On the other hand, we develop an active global memory to gather the global semantic correlation in long temporal range to current one, in which we gather the most informative frames derived from model uncertainty and frame similarity. We have extensively validated our method on two public benchmark surgical video datasets. Experimental results demonstrate that our method largely outperforms the state-of-the-art works on segmentation accuracy while maintaining a real-time speed.



### SafetyNet: Safe planning for real-world self-driving vehicles using machine-learned policies
- **Arxiv ID**: http://arxiv.org/abs/2109.13602v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.13602v1)
- **Published**: 2021-09-28 10:23:46+00:00
- **Updated**: 2021-09-28 10:23:46+00:00
- **Authors**: Matt Vitelli, Yan Chang, Yawei Ye, Maciej Wołczyk, Błażej Osiński, Moritz Niendorf, Hugo Grimmett, Qiangui Huang, Ashesh Jain, Peter Ondruska
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present the first safe system for full control of self-driving vehicles trained from human demonstrations and deployed in challenging, real-world, urban environments. Current industry-standard solutions use rule-based systems for planning. Although they perform reasonably well in common scenarios, the engineering complexity renders this approach incompatible with human-level performance. On the other hand, the performance of machine-learned (ML) planning solutions can be improved by simply adding more exemplar data. However, ML methods cannot offer safety guarantees and sometimes behave unpredictably. To combat this, our approach uses a simple yet effective rule-based fallback layer that performs sanity checks on an ML planner's decisions (e.g. avoiding collision, assuring physical feasibility). This allows us to leverage ML to handle complex situations while still assuring the safety, reducing ML planner-only collisions by 95%. We train our ML planner on 300 hours of expert driving demonstrations using imitation learning and deploy it along with the fallback layer in downtown San Francisco, where it takes complete control of a real vehicle and navigates a wide variety of challenging urban driving scenarios.



### Real-Time Glaucoma Detection from Digital Fundus Images using Self-ONNs
- **Arxiv ID**: http://arxiv.org/abs/2109.13604v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.13604v1)
- **Published**: 2021-09-28 10:27:01+00:00
- **Updated**: 2021-09-28 10:27:01+00:00
- **Authors**: Ozer Can Devecioglu, Junaid Malik, Turker Ince, Serkan Kiranyaz, Eray Atalay, Moncef Gabbouj
- **Comment**: None
- **Journal**: None
- **Summary**: Glaucoma leads to permanent vision disability by damaging the optical nerve that transmits visual images to the brain. The fact that glaucoma does not show any symptoms as it progresses and cannot be stopped at the later stages, makes it critical to be diagnosed in its early stages. Although various deep learning models have been applied for detecting glaucoma from digital fundus images, due to the scarcity of labeled data, their generalization performance was limited along with high computational complexity and special hardware requirements. In this study, compact Self-Organized Operational Neural Networks (Self- ONNs) are proposed for early detection of glaucoma in fundus images and their performance is compared against the conventional (deep) Convolutional Neural Networks (CNNs) over three benchmark datasets: ACRIMA, RIM-ONE, and ESOGU. The experimental results demonstrate that Self-ONNs not only achieve superior detection performance but can also significantly reduce the computational complexity making it a potentially suitable network model for biomedical datasets especially when the data is scarce.



### An Efficient Network Design for Face Video Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2109.13626v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.13626v1)
- **Published**: 2021-09-28 11:28:34+00:00
- **Updated**: 2021-09-28 11:28:34+00:00
- **Authors**: Feng Yu, He Li, Sige Bian, Yongming Tang
- **Comment**: 8 pages, 7 figures, conference workshop
- **Journal**: None
- **Summary**: Face video super-resolution algorithm aims to reconstruct realistic face details through continuous input video sequences. However, existing video processing algorithms usually contain redundant parameters to guarantee different super-resolution scenes. In this work, we focus on super-resolution of face areas in original video scenes, while rest areas are interpolated. This specific super-resolved task makes it possible to cut redundant parameters in general video super-resolution networks. We construct a dataset consisting entirely of face video sequences for network training and evaluation, and conduct hyper-parameter optimization in our experiments. We use three combined strategies to optimize the network parameters with a simultaneous train-evaluation method to accelerate optimization process. Results show that simultaneous train-evaluation method improves the training speed and facilitates the generation of efficient networks. The generated network can reduce at least 52.4% parameters and 20.7% FLOPs, achieve better performance on PSNR, SSIM compared with state-of-art video super-resolution algorithms. When processing 36x36x1x3 input video frame sequences, the efficient network provides 47.62 FPS real-time processing performance. We name our proposal as hyper-parameter optimization for face Video Super-Resolution (HO-FVSR), which is open-sourced at https://github.com/yphone/efficient-network-for-face-VSR.



### Unsupervised Diffeomorphic Surface Registration and Non-Linear Modelling
- **Arxiv ID**: http://arxiv.org/abs/2109.13630v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.13630v1)
- **Published**: 2021-09-28 11:47:12+00:00
- **Updated**: 2021-09-28 11:47:12+00:00
- **Authors**: Balder Croquet, Daan Christiaens, Seth M. Weinberg, Michael Bronstein, Dirk Vandermeulen, Peter Claes
- **Comment**: None
- **Journal**: International Conference on Medical Image Computing and
  Computer-Assisted Intervention (2021) 118-128
- **Summary**: Registration is an essential tool in image analysis. Deep learning based alternatives have recently become popular, achieving competitive performance at a faster speed. However, many contemporary techniques are limited to volumetric representations, despite increased popularity of 3D surface and shape data in medical image analysis. We propose a one-step registration model for 3D surfaces that internalises a lower dimensional probabilistic deformation model (PDM) using conditional variational autoencoders (CVAE). The deformations are constrained to be diffeomorphic using an exponentiation layer. The one-step registration model is benchmarked against iterative techniques, trading in a slightly lower performance in terms of shape fit for a higher compactness. We experiment with two distance metrics, Chamfer distance (CD) and Sinkhorn divergence (SD), as specific distance functions for surface data in real-world registration scenarios. The internalised deformation model is benchmarked against linear principal component analysis (PCA) achieving competitive results and improved generalisability from lower dimensions.



### Fail-Safe Human Detection for Drones Using a Multi-Modal Curriculum Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2109.13666v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.13666v1)
- **Published**: 2021-09-28 12:34:13+00:00
- **Updated**: 2021-09-28 12:34:13+00:00
- **Authors**: Ali Safa, Tim Verbelen, Ilja Ocket, André Bourdoux, Francky Catthoor, Georges G. E. Gielen
- **Comment**: None
- **Journal**: None
- **Summary**: Drones are currently being explored for safety-critical applications where human agents are expected to evolve in their vicinity. In such applications, robust people avoidance must be provided by fusing a number of sensing modalities in order to avoid collisions. Currently however, people detection systems used on drones are solely based on standard cameras besides an emerging number of works discussing the fusion of imaging and event-based cameras. On the other hand, radar-based systems provide up-most robustness towards environmental conditions but do not provide complete information on their own and have mainly been investigated in automotive contexts, not for drones. In order to enable the fusion of radars with both event-based and standard cameras, we present KUL-UAVSAFE, a first-of-its-kind dataset for the study of safety-critical people detection by drones. In addition, we propose a baseline CNN architecture with cross-fusion highways and introduce a curriculum learning strategy for multi-modal data termed SAUL, which greatly enhances the robustness of the system towards hard RGB failures and provides a significant gain of 15% in peak F1 score compared to the use of BlackIn, previously proposed for cross-fusion networks. We demonstrate the real-time performance and feasibility of the approach by implementing the system in an edge-computing unit. We release our dataset and additional material in the project home page.



### Motion Deblurring with Real Events
- **Arxiv ID**: http://arxiv.org/abs/2109.13695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13695v1)
- **Published**: 2021-09-28 13:11:44+00:00
- **Updated**: 2021-09-28 13:11:44+00:00
- **Authors**: Fang Xu, Lei Yu, Bishan Wang, Wen Yang, Gui-Song Xia, Xu Jia, Zhendong Qiao, Jianzhuang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an end-to-end learning framework for event-based motion deblurring in a self-supervised manner, where real-world events are exploited to alleviate the performance degradation caused by data inconsistency. To achieve this end, optical flows are predicted from events, with which the blurry consistency and photometric consistency are exploited to enable self-supervision on the deblurring network with real-world data. Furthermore, a piece-wise linear motion model is proposed to take into account motion non-linearities and thus leads to an accurate model for the physical formation of motion blurs in the real-world scenario. Extensive evaluation on both synthetic and real motion blur datasets demonstrates that the proposed algorithm bridges the gap between simulated and real-world motion blurs and shows remarkable performance for event-based motion deblurring in real-world scenarios.



### CIDEr-R: Robust Consensus-based Image Description Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2109.13701v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2109.13701v1)
- **Published**: 2021-09-28 13:13:21+00:00
- **Updated**: 2021-09-28 13:13:21+00:00
- **Authors**: Gabriel Oliveira dos Santos, Esther Luna Colombini, Sandra Avila
- **Comment**: Paper accepted to the 7th Workshop on Noisy User-generated Text
  (W-NUT). 10 pages, 4 figures, 3 tables
- **Journal**: None
- **Summary**: This paper shows that CIDEr-D, a traditional evaluation metric for image description, does not work properly on datasets where the number of words in the sentence is significantly greater than those in the MS COCO Captions dataset. We also show that CIDEr-D has performance hampered by the lack of multiple reference sentences and high variance of sentence length. To bypass this problem, we introduce CIDEr-R, which improves CIDEr-D, making it more flexible in dealing with datasets with high sentence length variance. We demonstrate that CIDEr-R is more accurate and closer to human judgment than CIDEr-D; CIDEr-R is more robust regarding the number of available references. Our results reveal that using Self-Critical Sequence Training to optimize CIDEr-R generates descriptive captions. In contrast, when CIDEr-D is optimized, the generated captions' length tends to be similar to the reference length. However, the models also repeat several times the same word to increase the sentence length.



### Diffractive lensless imaging with optimized Voronoi-Fresnel phase
- **Arxiv ID**: http://arxiv.org/abs/2109.13703v2
- **DOI**: 10.1364/OE.475004
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2109.13703v2)
- **Published**: 2021-09-28 13:13:58+00:00
- **Updated**: 2022-12-07 07:11:46+00:00
- **Authors**: Qiang Fu, Dong-Ming Yan, Wolfgang Heidrich
- **Comment**: 17 pages, 11 figures, 1 video, 1 video description, and 1
  supplemental document
- **Journal**: Optics Express Vol. 30, Issue 25, pp. 45807-45823 (2022)
- **Summary**: Lensless cameras are a class of imaging devices that shrink the physical dimensions to the very close vicinity of the image sensor by replacing conventional compound lenses with integrated flat optics and computational algorithms. Here we report a diffractive lensless camera with spatially-coded Voronoi-Fresnel phase to achieve superior image quality. We propose a design principle of maximizing the acquired information in optics to facilitate the computational reconstruction. By introducing an easy-to-optimize Fourier domain metric, Modulation Transfer Function volume (MTFv), which is related to the Strehl ratio, we devise an optimization framework to guide the optimization of the diffractive optical element. The resulting Voronoi-Fresnel phase features an irregular array of quasi-Centroidal Voronoi cells containing a base first-order Fresnel phase function. We demonstrate and verify the imaging performance for photography applications with a prototype Voronoi-Fresnel lensless camera on a 1.6-megapixel image sensor in various illumination conditions. Results show that the proposed design outperforms existing lensless cameras, and could benefit the development of compact imaging systems that work in extreme physical conditions.



### A Step Towards Efficient Evaluation of Complex Perception Tasks in Simulation
- **Arxiv ID**: http://arxiv.org/abs/2110.02739v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.02739v2)
- **Published**: 2021-09-28 13:50:21+00:00
- **Updated**: 2021-11-04 18:10:36+00:00
- **Authors**: Jonathan Sadeghi, Blaine Rogers, James Gunn, Thomas Saunders, Sina Samangooei, Puneet Kumar Dokania, John Redford
- **Comment**: To appear in NeurIPS 2021 Workshop on Machine Learning for Autonomous
  Driving (ML4AD)
- **Journal**: None
- **Summary**: There has been increasing interest in characterising the error behaviour of systems which contain deep learning models before deploying them into any safety-critical scenario. However, characterising such behaviour usually requires large-scale testing of the model that can be extremely computationally expensive for complex real-world tasks. For example, tasks involving compute intensive object detectors as one of their components. In this work, we propose an approach that enables efficient large-scale testing using simplified low-fidelity simulators and without the computational cost of executing expensive deep learning models. Our approach relies on designing an efficient surrogate model corresponding to the compute intensive components of the task under test. We demonstrate the efficacy of our methodology by evaluating the performance of an autonomous driving task in the Carla simulator with reduced computational expense by training efficient surrogate models for PIXOR and CenterPoint LiDAR detectors, whilst demonstrating that the accuracy of the simulation is maintained.



### Adaptive Attribute and Structure Subspace Clustering Network
- **Arxiv ID**: http://arxiv.org/abs/2109.13742v3
- **DOI**: 10.1109/TIP.2022.3171421
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.13742v3)
- **Published**: 2021-09-28 14:00:57+00:00
- **Updated**: 2022-04-22 09:08:28+00:00
- **Authors**: Zhihao Peng, Hui Liu, Yuheng Jia, Junhui Hou
- **Comment**: This paper has been accepted by IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Deep self-expressiveness-based subspace clustering methods have demonstrated effectiveness. However, existing works only consider the attribute information to conduct the self-expressiveness, which may limit the clustering performance. In this paper, we propose a novel adaptive attribute and structure subspace clustering network (AASSC-Net) to simultaneously consider the attribute and structure information in an adaptive graph fusion manner. Specifically, we first exploit an auto-encoder to represent input data samples with latent features for the construction of an attribute matrix. We also construct a mixed signed and symmetric structure matrix to capture the local geometric structure underlying data samples. Then, we perform self-expressiveness on the constructed attribute and structure matrices to learn their affinity graphs separately. Finally, we design a novel attention-based fusion module to adaptively leverage these two affinity graphs to construct a more discriminative affinity graph. Extensive experimental results on commonly used benchmark datasets demonstrate that our AASSC-Net significantly outperforms state-of-the-art methods. In addition, we conduct comprehensive ablation studies to discuss the effectiveness of the designed modules. The code will be publicly available at https://github.com/ZhihaoPENG-CityU.



### Improving Autoencoder Training Performance for Hyperspectral Unmixing with Network Reinitialisation
- **Arxiv ID**: http://arxiv.org/abs/2109.13748v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68T07, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2109.13748v3)
- **Published**: 2021-09-28 14:07:24+00:00
- **Updated**: 2022-04-12 08:36:53+00:00
- **Authors**: Kamil Książek, Przemysław Głomb, Michał Romaszewski, Michał Cholewa, Bartosz Grabowski, Krisztián Búza
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks, in particular autoencoders, are one of the most promising solutions for unmixing hyperspectral data, i.e. reconstructing the spectra of observed substances (endmembers) and their relative mixing fractions (abundances), which is needed for effective hyperspectral analysis and classification. However, as we show in this paper, the training of autoencoders for unmixing is highly dependent on weights initialisation; some sets of weights lead to degenerate or low-performance solutions, introducing negative bias in the expected performance. In this work, we experimentally investigate autoencoders stability as well as network reinitialisation methods based on coefficients of neurons' dead activations. We demonstrate that the proposed techniques have a positive effect on autoencoder training in terms of reconstruction, abundances and endmembers errors.



### StereoSpike: Depth Learning with a Spiking Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2109.13751v3
- **DOI**: 10.1109/ACCESS.2022.3226484
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2109.13751v3)
- **Published**: 2021-09-28 14:11:36+00:00
- **Updated**: 2022-11-03 12:35:43+00:00
- **Authors**: Ulysse Rançon, Javier Cuadrado-Anibarro, Benoit R. Cottereau, Timothée Masquelier
- **Comment**: None
- **Journal**: None
- **Summary**: Depth estimation is an important computer vision task, useful in particular for navigation in autonomous vehicles, or for object manipulation in robotics. Here we solved it using an end-to-end neuromorphic approach, combining two event-based cameras and a Spiking Neural Network (SNN) with a slightly modified U-Net-like encoder-decoder architecture, that we named StereoSpike. More specifically, we used the Multi Vehicle Stereo Event Camera Dataset (MVSEC). It provides a depth ground-truth, which was used to train StereoSpike in a supervised manner, using surrogate gradient descent. We propose a novel readout paradigm to obtain a dense analog prediction -- the depth of each pixel -- from the spikes of the decoder. We demonstrate that this architecture generalizes very well, even better than its non-spiking counterparts, leading to state-of-the-art test accuracy. To the best of our knowledge, it is the first time that such a large-scale regression problem is solved by a fully spiking network. Finally, we show that low firing rates (<10%) can be obtained via regularization, with a minimal cost in accuracy. This means that StereoSpike could be efficiently implemented on neuromorphic chips, opening the door for low power and real time embedded systems.



### All-Around Real Label Supervision: Cyclic Prototype Consistency Learning for Semi-supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.13930v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.13930v2)
- **Published**: 2021-09-28 14:34:06+00:00
- **Updated**: 2022-03-15 14:42:34+00:00
- **Authors**: Zhe Xu, Yixin Wang, Donghuan Lu, Lequan Yu, Jiangpeng Yan, Jie Luo, Kai Ma, Yefeng Zheng, Raymond Kai-yu Tong
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Semi-supervised learning has substantially advanced medical image segmentation since it alleviates the heavy burden of acquiring the costly expert-examined annotations. Especially, the consistency-based approaches have attracted more attention for their superior performance, wherein the real labels are only utilized to supervise their paired images via supervised loss while the unlabeled images are exploited by enforcing the perturbation-based \textit{"unsupervised"} consistency without explicit guidance from those real labels. However, intuitively, the expert-examined real labels contain more reliable supervision signals. Observing this, we ask an unexplored but interesting question: can we exploit the unlabeled data via explicit real label supervision for semi-supervised training? To this end, we discard the previous perturbation-based consistency but absorb the essence of non-parametric prototype learning. Based on the prototypical network, we then propose a novel cyclic prototype consistency learning (CPCL) framework, which is constructed by a labeled-to-unlabeled (L2U) prototypical forward process and an unlabeled-to-labeled (U2L) backward process. Such two processes synergistically enhance the segmentation network by encouraging more discriminative and compact features. In this way, our framework turns previous \textit{"unsupervised"} consistency into new \textit{"supervised"} consistency, obtaining the \textit{"all-around real label supervision"} property of our method. Extensive experiments on brain tumor segmentation from MRI and kidney segmentation from CT images show that our CPCL can effectively exploit the unlabeled data and outperform other state-of-the-art semi-supervised medical image segmentation methods.



### PFENet++: Boosting Few-shot Semantic Segmentation with the Noise-filtered Context-aware Prior Mask
- **Arxiv ID**: http://arxiv.org/abs/2109.13788v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13788v1)
- **Published**: 2021-09-28 15:07:43+00:00
- **Updated**: 2021-09-28 15:07:43+00:00
- **Authors**: Xiaoliu Luo, Zhuotao Tian, Taiping Zhang, Bei Yu, Yuan Yan Tang, Jiaya Jia
- **Comment**: 12 pages. The first two authors contribute equally
- **Journal**: None
- **Summary**: In this work, we revisit the prior mask guidance proposed in "Prior Guided Feature Enrichment Network for Few-Shot Segmentation". The prior mask serves as an indicator that highlights the region of interests of unseen categories, and it is effective in achieving better performance on different frameworks of recent studies.   However, the current method directly takes the maximum element-to-element correspondence between the query and support features to indicate the probability of belonging to the target class, thus the broader contextual information is seldom exploited during the prior mask generation. To address this issue, first, we propose the Context-aware Prior Mask (CAPM) that leverages additional nearby semantic cues for better locating the objects in query images. Second, since the maximum correlation value is vulnerable to noisy features, we take one step further by incorporating a lightweight Noise Suppression Module (NSM) to screen out the unnecessary responses, yielding high-quality masks for providing the prior knowledge.   Both two contributions are experimentally shown to have substantial practical merit, and the new model named PFENet++ significantly outperforms the baseline PFENet as well as all other competitors on three challenging benchmarks PASCAL-5$^i$, COCO-20$^i$ and FSS-1000.   The new state-of-the-art performance is achieved without compromising the efficiency, manifesting the potential for being a new strong baseline in few-shot semantic segmentation.   Our code will be available at https://github.com/dvlab-research/PFENet++.



### The VVAD-LRS3 Dataset for Visual Voice Activity Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.13789v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.13789v1)
- **Published**: 2021-09-28 15:08:20+00:00
- **Updated**: 2021-09-28 15:08:20+00:00
- **Authors**: Adrian Lubitz, Matias Valdenegro-Toro, Frank Kirchner
- **Comment**: None
- **Journal**: None
- **Summary**: Robots are becoming everyday devices, increasing their interaction with humans. To make human-machine interaction more natural, cognitive features like Visual Voice Activity Detection (VVAD), which can detect whether a person is speaking or not, given visual input of a camera, need to be implemented. Neural networks are state of the art for tasks in Image Processing, Time Series Prediction, Natural Language Processing and other domains. Those Networks require large quantities of labeled data. Currently there are not many datasets for the task of VVAD. In this work we created a large scale dataset called the VVAD-LRS3 dataset, derived by automatic annotations from the LRS3 dataset. The VVAD-LRS3 dataset contains over 44K samples, over three times the next competitive dataset (WildVVAD). We evaluate different baselines on four kinds of features: facial and lip images, and facial and lip landmark features. With a Convolutional Neural Network Long Short Term Memory (CNN LSTM) on facial images an accuracy of 92% was reached on the test set. A study with humans showed that they reach an accuracy of 87.93% on the test set.



### A framework for quantitative analysis of Computed Tomography images of viral pneumonitis: radiomic features in COVID and non-COVID patients
- **Arxiv ID**: http://arxiv.org/abs/2109.13931v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.13931v1)
- **Published**: 2021-09-28 15:22:24+00:00
- **Updated**: 2021-09-28 15:22:24+00:00
- **Authors**: Giulia Zorzi, Luca Berta, Stefano Carrazza, Alberto Torresin
- **Comment**: 11 pages, 4 figures, preprint
- **Journal**: None
- **Summary**: Purpose: to optimize a pipeline of clinical data gathering and CT images processing implemented during the COVID-19 pandemic crisis and to develop artificial intelligence model for different of viral pneumonia. Methods: 1028 chest CT image of patients with positive swab were segmented automatically for lung extraction. A Gaussian model developed in Python language was applied to calculate quantitative metrics (QM) describing well-aerated and ill portions of the lungs from the histogram distribution of lung CT numbers in both lungs of each image and in four geometrical subdivision. Furthermore, radiomic features (RF) of first and second order were extracted from bilateral lungs using PyRadiomic tools. QM and RF were used to develop 4 different Multi-Layer Perceptron (MLP) classifier to discriminate images of patients with COVID (n=646) and non-COVID (n=382) viral pneumonia. Results: The Gaussian model applied to lung CT histogram correctly described healthy parenchyma 94% of the patients. The resulting accuracy of the models for COVID diagnosis were in the range 0.76-0.87, as the integral of the receiver operating curve. The best diagnostic performances were associated to the model based on RF of first and second order, with 21 relevant features after LASSO regression and an accuracy of 0.81$\pm$0.02 after 4-fold cross validation Conclusions: Despite these results were obtained with CT images from a single center, a platform for extracting useful quantitative metrics from CT images was developed and optimized. Four artificial intelligence-based models for classifying patients with COVID and non-COVID viral pneumonia were developed and compared showing overall good diagnostic performances



### Not Color Blind: AI Predicts Racial Identity from Black and White Retinal Vessel Segmentations
- **Arxiv ID**: http://arxiv.org/abs/2109.13845v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13845v1)
- **Published**: 2021-09-28 16:21:37+00:00
- **Updated**: 2021-09-28 16:21:37+00:00
- **Authors**: Aaron S. Coyner, Praveer Singh, James M. Brown, Susan Ostmo, R. V. Paul Chan, Michael F. Chiang, Jayashree Kalpathy-Cramer, J. Peter Campbell
- **Comment**: 31 pages, 6 figures
- **Journal**: None
- **Summary**: Background: Artificial intelligence (AI) may demonstrate racial bias when skin or choroidal pigmentation is present in medical images. Recent studies have shown that convolutional neural networks (CNNs) can predict race from images that were not previously thought to contain race-specific features. We evaluate whether grayscale retinal vessel maps (RVMs) of patients screened for retinopathy of prematurity (ROP) contain race-specific features.   Methods: 4095 retinal fundus images (RFIs) were collected from 245 Black and White infants. A U-Net generated RVMs from RFIs, which were subsequently thresholded, binarized, or skeletonized. To determine whether RVM differences between Black and White eyes were physiological, CNNs were trained to predict race from color RFIs, raw RVMs, and thresholded, binarized, or skeletonized RVMs. Area under the precision-recall curve (AUC-PR) was evaluated.   Findings: CNNs predicted race from RFIs near perfectly (image-level AUC-PR: 0.999, subject-level AUC-PR: 1.000). Raw RVMs were almost as informative as color RFIs (image-level AUC-PR: 0.938, subject-level AUC-PR: 0.995). Ultimately, CNNs were able to detect whether RFIs or RVMs were from Black or White babies, regardless of whether images contained color, vessel segmentation brightness differences were nullified, or vessel segmentation widths were normalized.   Interpretation: AI can detect race from grayscale RVMs that were not thought to contain racial information. Two potential explanations for these findings are that: retinal vessels physiologically differ between Black and White babies or the U-Net segments the retinal vasculature differently for various fundus pigmentations. Either way, the implications remain the same: AI algorithms have potential to demonstrate racial bias in practice, even when preliminary attempts to remove such information from the underlying images appear to be successful.



### Visually Grounded Reasoning across Languages and Cultures
- **Arxiv ID**: http://arxiv.org/abs/2109.13238v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.13238v2)
- **Published**: 2021-09-28 16:51:38+00:00
- **Updated**: 2021-10-21 18:13:28+00:00
- **Authors**: Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, Desmond Elliott
- **Comment**: EMNLP 2021; Fangyu and Emanuele contributed equally; MaRVL website:
  https://marvl-challenge.github.io
- **Journal**: None
- **Summary**: The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for {M}ulticultur{a}l {R}easoning over {V}ision and {L}anguage (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems.



### 3D Hand Pose and Shape Estimation from RGB Images for Keypoint-Based Hand Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2109.13879v2
- **DOI**: 10.1016/j.patcog.2022.108762
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13879v2)
- **Published**: 2021-09-28 17:07:43+00:00
- **Updated**: 2022-05-09 17:22:19+00:00
- **Authors**: Danilo Avola, Luigi Cinque, Alessio Fagioli, Gian Luca Foresti, Adriano Fragomeni, Daniele Pannone
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating the 3D pose of a hand from a 2D image is a well-studied problem and a requirement for several real-life applications such as virtual reality, augmented reality, and hand gesture recognition. Currently, reasonable estimations can be computed from single RGB images, especially when a multi-task learning approach is used to force the system to consider the shape of the hand when its pose is determined. However, depending on the method used to represent the hand, the performance can drop considerably in real-life tasks, suggesting that stable descriptions are required to achieve satisfactory results. In this paper, we present a keypoint-based end-to-end framework for 3D hand and pose estimation and successfully apply it to the task of hand gesture recognition as a study case. Specifically, after a pre-processing step in which the images are normalized, the proposed pipeline uses a multi-task semantic feature extractor generating 2D heatmaps and hand silhouettes from RGB images, a viewpoint encoder to predict the hand and camera view parameters, a stable hand estimator to produce the 3D hand pose and shape, and a loss function to guide all of the components jointly during the learning phase. Tests were performed on a 3D pose and shape estimation benchmark dataset to assess the proposed framework, which obtained state-of-the-art performance. Our system was also evaluated on two hand-gesture recognition benchmark datasets and significantly outperformed other keypoint-based approaches, indicating that it is an effective solution that is able to generate stable 3D estimates for hand pose and shape.



### Turning old models fashion again: Recycling classical CNN networks using the Lattice Transformation
- **Arxiv ID**: http://arxiv.org/abs/2109.13885v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 65D19, 68T07, I.2; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2109.13885v1)
- **Published**: 2021-09-28 17:24:48+00:00
- **Updated**: 2021-09-28 17:24:48+00:00
- **Authors**: Ana Paula G. S. de Almeida, Flavio de Barros Vidal
- **Comment**: 21 pages, 13 figures
- **Journal**: None
- **Summary**: In the early 1990s, the first signs of life of the CNN era were given: LeCun et al. proposed a CNN model trained by the backpropagation algorithm to classify low-resolution images of handwritten digits. Undoubtedly, it was a breakthrough in the field of computer vision. But with the rise of other classification methods, it fell out fashion. That was until 2012, when Krizhevsky et al. revived the interest in CNNs by exhibiting considerably higher image classification accuracy on the ImageNet challenge. Since then, the complexity of the architectures are exponentially increasing and many structures are rapidly becoming obsolete. Using multistream networks as a base and the feature infusion precept, we explore the proposed LCNN cross-fusion strategy to use the backbones of former state-of-the-art networks on image classification in order to discover if the technique is able to put these designs back in the game. In this paper, we showed that we can obtain an increase of accuracy up to 63.21% on the NORB dataset we comparing with the original structure. However, no technique is definitive. While our goal is to try to reuse previous state-of-the-art architectures with few modifications, we also expose the disadvantages of our explored strategy.



### Image scaling by de la Vallée-Poussin filtered interpolation
- **Arxiv ID**: http://arxiv.org/abs/2109.13897v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA, 68U10, 65D05, 62H35
- **Links**: [PDF](http://arxiv.org/pdf/2109.13897v2)
- **Published**: 2021-09-28 17:48:46+00:00
- **Updated**: 2022-07-08 15:07:47+00:00
- **Authors**: Donatella Occorsio, Giuliana Ramella, Woula Themistoclakis
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new image scaling method both for downscaling and upscaling, running with any scale factor or desired size. The resized image is achieved by sampling a bivariate polynomial which globally interpolates the data at the new scale. The method's particularities lay in both the sampling model and the interpolation polynomial we use. Rather than classical uniform grids, we consider an unusual sampling system based on Chebyshev zeros of the first kind. Such optimal distribution of nodes permits to consider near--best interpolation polynomials defined by a filter of de la Vall\'ee Poussin type. The action ray of this filter provides an additional parameter that can be suitably regulated to improve the approximation. The method has been tested on a significant number of different image datasets. The results are evaluated in qualitative and quantitative terms and compared with other available competitive methods. The perceived quality of the resulting scaled images is such that important details are preserved, and the appearance of artifacts is low. Competitive quality measurement values, good visual quality, limited computational effort, and moderate memory demand make the method suitable for real-world applications.



### A Contrastive Learning Approach to Auroral Identification and Classification
- **Arxiv ID**: http://arxiv.org/abs/2109.13899v2
- **DOI**: 10.1109/ICMLA52953.2021.00128
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.13899v2)
- **Published**: 2021-09-28 17:51:25+00:00
- **Updated**: 2021-09-29 02:08:10+00:00
- **Authors**: Jeremiah W. Johnson, Swathi Hari, Donald Hampton, Hyunju K. Connor, Amy Keesee
- **Comment**: 6 pages, 5 figures, 1 table
- **Journal**: Proceedings of the 20th IEEE International Conference on Machine
  Learning and Applications, Dec. 2021
- **Summary**: Unsupervised learning algorithms are beginning to achieve accuracies comparable to their supervised counterparts on benchmark computer vision tasks, but their utility for practical applications has not yet been demonstrated. In this work, we present a novel application of unsupervised learning to the task of auroral image classification. Specifically, we modify and adapt the Simple framework for Contrastive Learning of Representations (SimCLR) algorithm to learn representations of auroral images in a recently released auroral image dataset constructed using image data from Time History of Events and Macroscale Interactions during Substorms (THEMIS) all-sky imagers. We demonstrate that (a) simple linear classifiers fit to the learned representations of the images achieve state-of-the-art classification performance, improving the classification accuracy by almost 10 percentage points over the current benchmark; and (b) the learned representations naturally cluster into more clusters than exist manually assigned categories, suggesting that existing categorizations are overly coarse and may obscure important connections between auroral types, near-earth solar wind conditions, and geomagnetic disturbances at the earth's surface. Moreover, our model is much lighter than the previous benchmark on this dataset, requiring in the area of fewer than 25\% of the number of parameters. Our approach exceeds an established threshold for operational purposes, demonstrating readiness for deployment and utilization.



### PDC-Net+: Enhanced Probabilistic Dense Correspondence Network
- **Arxiv ID**: http://arxiv.org/abs/2109.13912v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.13912v2)
- **Published**: 2021-09-28 17:56:41+00:00
- **Updated**: 2021-09-29 06:06:24+00:00
- **Authors**: Prune Truong, Martin Danelljan, Radu Timofte, Luc Van Gool
- **Comment**: Code: https://github.com/PruneTruong/DenseMatching. Paper extension
  of PDC-Net. arXiv admin note: substantial text overlap with arXiv:2101.01710
- **Journal**: None
- **Summary**: Establishing robust and accurate correspondences between a pair of images is a long-standing computer vision problem with numerous applications. While classically dominated by sparse methods, emerging dense approaches offer a compelling alternative paradigm that avoids the keypoint detection step. However, dense flow estimation is often inaccurate in the case of large displacements, occlusions, or homogeneous regions. In order to apply dense methods to real-world applications, such as pose estimation, image manipulation, or 3D reconstruction, it is therefore crucial to estimate the confidence of the predicted matches.   We propose the Enhanced Probabilistic Dense Correspondence Network, PDC-Net+, capable of estimating accurate dense correspondences along with a reliable confidence map. We develop a flexible probabilistic approach that jointly learns the flow prediction and its uncertainty. In particular, we parametrize the predictive distribution as a constrained mixture model, ensuring better modelling of both accurate flow predictions and outliers. Moreover, we develop an architecture and an enhanced training strategy tailored for robust and generalizable uncertainty prediction in the context of self-supervised training. Our approach obtains state-of-the-art results on multiple challenging geometric matching and optical flow datasets. We further validate the usefulness of our probabilistic confidence estimation for the tasks of pose estimation, 3D reconstruction, image-based localization, and image retrieval. Code and models are available at https://github.com/PruneTruong/DenseMatching.



### $f$-Cal: Calibrated aleatoric uncertainty estimation from neural networks for robot perception
- **Arxiv ID**: http://arxiv.org/abs/2109.13913v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.13913v1)
- **Published**: 2021-09-28 17:57:58+00:00
- **Updated**: 2021-09-28 17:57:58+00:00
- **Authors**: Dhaivat Bhatt, Kaustubh Mani, Dishank Bansal, Krishna Murthy, Hanju Lee, Liam Paull
- **Comment**: For more details about $f$-Cal, visit https://f-cal.github.io
- **Journal**: None
- **Summary**: While modern deep neural networks are performant perception modules, performance (accuracy) alone is insufficient, particularly for safety-critical robotic applications such as self-driving vehicles. Robot autonomy stacks also require these otherwise blackbox models to produce reliable and calibrated measures of confidence on their predictions. Existing approaches estimate uncertainty from these neural network perception stacks by modifying network architectures, inference procedure, or loss functions. However, in general, these methods lack calibration, meaning that the predictive uncertainties do not faithfully represent the true underlying uncertainties (process noise). Our key insight is that calibration is only achieved by imposing constraints across multiple examples, such as those in a mini-batch; as opposed to existing approaches which only impose constraints per-sample, often leading to overconfident (thus miscalibrated) uncertainty estimates. By enforcing the distribution of outputs of a neural network to resemble a target distribution by minimizing an $f$-divergence, we obtain significantly better-calibrated models compared to prior approaches. Our approach, $f$-Cal, outperforms existing uncertainty calibration approaches on robot perception tasks such as object detection and monocular depth estimation over multiple real-world benchmarks.



### Unsolved Problems in ML Safety
- **Arxiv ID**: http://arxiv.org/abs/2109.13916v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.13916v5)
- **Published**: 2021-09-28 17:59:36+00:00
- **Updated**: 2022-06-16 21:12:42+00:00
- **Authors**: Dan Hendrycks, Nicholas Carlini, John Schulman, Jacob Steinhardt
- **Comment**: Position Paper
- **Journal**: None
- **Summary**: Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards ("Robustness"), identifying hazards ("Monitoring"), reducing inherent model hazards ("Alignment"), and reducing systemic hazards ("Systemic Safety"). Throughout, we clarify each problem's motivation and provide concrete research directions.



### Competence-Aware Path Planning via Introspective Perception
- **Arxiv ID**: http://arxiv.org/abs/2109.13974v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, I.2.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2109.13974v3)
- **Published**: 2021-09-28 18:29:21+00:00
- **Updated**: 2022-01-17 19:38:46+00:00
- **Authors**: Sadegh Rabiee, Connor Basich, Kyle Hollins Wray, Shlomo Zilberstein, Joydeep Biswas
- **Comment**: Accepted by IEEE Robotics and Automation Letters (8 pages, 8 figures)
- **Journal**: None
- **Summary**: Robots deployed in the real world over extended periods of time need to reason about unexpected failures, learn to predict them, and to proactively take actions to avoid future failures. Existing approaches for competence-aware planning are either model-based, requiring explicit enumeration of known failure modes, or purely statistical, using state- and location-specific failure statistics to infer competence. We instead propose a structured model-free approach to competence-aware planning by reasoning about plan execution failures due to errors in perception, without requiring a priori enumeration of failure sources or requiring location-specific failure statistics. We introduce competence-aware path planning via introspective perception (CPIP), a Bayesian framework to iteratively learn and exploit task-level competence in novel deployment environments. CPIP factorizes the competence-aware planning problem into two components. First, perception errors are learned in a model-free and location-agnostic setting via introspective perception prior to deployment in novel environments. Second, during actual deployments, the prediction of task-level failures is learned in a context-aware setting. Experiments in a simulation show that the proposed CPIP approach outperforms the frequentist baseline in multiple mobile robot tasks, and is further validated via real robot experiments in an environment with perceptually challenging obstacles and terrain.



### Self-supervised Point Cloud Prediction Using 3D Spatio-temporal Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.04076v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.04076v2)
- **Published**: 2021-09-28 19:58:13+00:00
- **Updated**: 2021-10-18 15:51:54+00:00
- **Authors**: Benedikt Mersch, Xieyuanli Chen, Jens Behley, Cyrill Stachniss
- **Comment**: Accepted for CoRL 2021
- **Journal**: None
- **Summary**: Exploiting past 3D LiDAR scans to predict future point clouds is a promising method for autonomous mobile systems to realize foresighted state estimation, collision avoidance, and planning. In this paper, we address the problem of predicting future 3D LiDAR point clouds given a sequence of past LiDAR scans. Estimating the future scene on the sensor level does not require any preceding steps as in localization or tracking systems and can be trained self-supervised. We propose an end-to-end approach that exploits a 2D range image representation of each 3D LiDAR scan and concatenates a sequence of range images to obtain a 3D tensor. Based on such tensors, we develop an encoder-decoder architecture using 3D convolutions to jointly aggregate spatial and temporal information of the scene and to predict the future 3D point clouds. We evaluate our method on multiple datasets and the experimental results suggest that our method outperforms existing point cloud prediction architectures and generalizes well to new, unseen environments without additional fine-tuning. Our method operates online and is faster than the common LiDAR frame rate of 10 Hz.



### Y-GAN: Learning Dual Data Representations for Efficient Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.14020v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14020v2)
- **Published**: 2021-09-28 20:17:04+00:00
- **Updated**: 2022-11-11 12:44:29+00:00
- **Authors**: Marija Ivanovska, Vitomir Štruc
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel reconstruction-based model for anomaly detection, called Y-GAN. The model consists of a Y-shaped auto-encoder and represents images in two separate latent spaces. The first captures meaningful image semantics, key for representing (normal) training data, whereas the second encodes low-level residual image characteristics. To ensure the dual representations encode mutually exclusive information, a disentanglement procedure is designed around a latent (proxy) classifier. Additionally, a novel consistency loss is proposed to prevent information leakage between the latent spaces. The model is trained in a one-class learning setting using normal training data only. Due to the separation of semantically-relevant and residual information, Y-GAN is able to derive informative data representations that allow for efficient anomaly detection across a diverse set of anomaly detection tasks. The model is evaluated in comprehensive experiments with several recent anomaly detection models using four popular datasets, i.e., MNIST, FMNIST and CIFAR10, and PlantVillage.



### Prediction of the Facial Growth Direction is Challenging
- **Arxiv ID**: http://arxiv.org/abs/2110.02316v1
- **DOI**: 10.1007/978-3-030-92310-5_77
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.02316v1)
- **Published**: 2021-09-28 20:22:01+00:00
- **Updated**: 2021-09-28 20:22:01+00:00
- **Authors**: Stanisław Kaźmierczak, Zofia Juszka, Vaska Vandevska-Radunovic, Thomas JJ Maal, Piotr Fudalej, Jacek Mańdziuk
- **Comment**: None
- **Journal**: International Conference on Neural Information Processing, ICONIP
  2021, CCIS 1517, 665-673
- **Summary**: Facial dysmorphology or malocclusion is frequently associated with abnormal growth of the face. The ability to predict facial growth (FG) direction would allow clinicians to prepare individualized therapy to increase the chance for successful treatment. Prediction of FG direction is a novel problem in the machine learning (ML) domain. In this paper, we perform feature selection and point the attribute that plays a central role in the abovementioned problem. Then we successfully apply data augmentation (DA) methods and improve the previously reported classification accuracy by 2.81%. Finally, we present the results of two experienced clinicians that were asked to solve a similar task to ours and show how tough is solving this problem for human experts.



### Deep Unrolled Recovery in Sparse Biological Imaging
- **Arxiv ID**: http://arxiv.org/abs/2109.14025v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, cs.LG, eess.SP, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2109.14025v1)
- **Published**: 2021-09-28 20:22:44+00:00
- **Updated**: 2021-09-28 20:22:44+00:00
- **Authors**: Yair Ben Sahel, John P. Bryan, Brian Cleary, Samouil L. Farhi, Yonina C. Eldar
- **Comment**: None
- **Journal**: None
- **Summary**: Deep algorithm unrolling has emerged as a powerful model-based approach to develop deep architectures that combine the interpretability of iterative algorithms with the performance gains of supervised deep learning, especially in cases of sparse optimization. This framework is well-suited to applications in biological imaging, where physics-based models exist to describe the measurement process and the information to be recovered is often highly structured. Here, we review the method of deep unrolling, and show how it improves source localization in several biological imaging settings.



### Learning Perceptual Locomotion on Uneven Terrains using Sparse Visual Observations
- **Arxiv ID**: http://arxiv.org/abs/2109.14026v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.14026v2)
- **Published**: 2021-09-28 20:25:10+00:00
- **Updated**: 2022-05-26 12:26:54+00:00
- **Authors**: Fernando Acero, Kai Yuan, Zhibin Li
- **Comment**: Video summary can be found at https://youtu.be/vtp43jYQ5w4
- **Journal**: None
- **Summary**: To proactively navigate and traverse various terrains, active use of visual perception becomes indispensable. We aim to investigate the feasibility and performance of using sparse visual observations to achieve perceptual locomotion over a range of common terrains (steps, ramps, gaps, and stairs) in human-centered environments. We formulate a selection of sparse visual inputs suitable for locomotion over the terrains of interest, and propose a learning framework to integrate exteroceptive and proprioceptive states. We specifically design the state observations and a training curriculum to learn feedback control policies effectively over a range of different terrains. We extensively validate and benchmark the learned policy in various tasks: omnidirectional walking on flat ground, and forward locomotion over various obstacles, showing high success rate of traversability. Furthermore, we study exteroceptive ablations and evaluate policy generalization by adding various levels of noise and testing on new unseen terrains. We demonstrate the capabilities of autonomous perceptual locomotion that can be achieved by only using sparse visual observations from direct depth measurements, which are easily available from a Lidar or RGB-D sensor, showing robust ascent and descent over high stairs of 20 cm height, i.e., 50% leg length, and robustness against noise and unseen terrains.



### AutoPhaseNN: Unsupervised Physics-aware Deep Learning of 3D Nanoscale Bragg Coherent Diffraction Imaging
- **Arxiv ID**: http://arxiv.org/abs/2109.14053v2
- **DOI**: None
- **Categories**: **physics.app-ph**, cond-mat.mtrl-sci, cs.AI, cs.CV, 68T07, 00A79
- **Links**: [PDF](http://arxiv.org/pdf/2109.14053v2)
- **Published**: 2021-09-28 21:16:34+00:00
- **Updated**: 2022-04-04 15:11:33+00:00
- **Authors**: Yudong Yao, Henry Chan, Subramanian Sankaranarayanan, Prasanna Balaprakash, Ross J. Harder, Mathew J. Cherukara
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of phase retrieval, or the algorithmic recovery of lost phase information from measured intensity alone, underlies various imaging methods from astronomy to nanoscale imaging. Traditional methods of phase retrieval are iterative in nature, and are therefore computationally expensive and time consuming. More recently, deep learning (DL) models have been developed to either provide learned priors to iterative phase retrieval or in some cases completely replace phase retrieval with networks that learn to recover the lost phase information from measured intensity alone. However, such models require vast amounts of labeled data, which can only be obtained through simulation or performing computationally prohibitive phase retrieval on hundreds of or even thousands of experimental datasets. Using a 3D nanoscale X-ray imaging modality (Bragg Coherent Diffraction Imaging or BCDI) as a representative technique, we demonstrate AutoPhaseNN, a DL-based approach which learns to solve the phase problem without labeled data. By incorporating the physics of the imaging technique into the DL model during training, AutoPhaseNN learns to invert 3D BCDI data from reciprocal space to real space in a single shot without ever being shown real space images. Once trained, AutoPhaseNN is about one hundred times faster than traditional iterative phase retrieval methods while providing comparable image quality.



### VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding
- **Arxiv ID**: http://arxiv.org/abs/2109.14084v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2109.14084v2)
- **Published**: 2021-09-28 23:01:51+00:00
- **Updated**: 2021-10-01 15:13:27+00:00
- **Authors**: Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, Christoph Feichtenhofer
- **Comment**: EMNLP 2021
- **Journal**: None
- **Summary**: We present VideoCLIP, a contrastive approach to pre-train a unified model for zero-shot video and text understanding, without using any labels on downstream tasks. VideoCLIP trains a transformer for video and text by contrasting temporally overlapping positive video-text pairs with hard negatives from nearest neighbor retrieval. Our experiments on a diverse series of downstream tasks, including sequence-level text-video retrieval, VideoQA, token-level action localization, and action segmentation reveal state-of-the-art performance, surpassing prior work, and in some cases even outperforming supervised approaches. Code is made available at https://github.com/pytorch/fairseq/tree/main/examples/MMPT.



