# Arxiv Papers in cs.CV on 2021-09-14
### AdaPruner: Adaptive Channel Pruning and Effective Weights Inheritance
- **Arxiv ID**: http://arxiv.org/abs/2109.06397v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.06397v1)
- **Published**: 2021-09-14 01:52:05+00:00
- **Updated**: 2021-09-14 01:52:05+00:00
- **Authors**: Xiangcheng Liu, Jian Cao, Hongyi Yao, Wenyu Sun, Yuan Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Channel pruning is one of the major compression approaches for deep neural networks. While previous pruning methods have mostly focused on identifying unimportant channels, channel pruning is considered as a special case of neural architecture search in recent years. However, existing methods are either complicated or prone to sub-optimal pruning. In this paper, we propose a pruning framework that adaptively determines the number of each layer's channels as well as the wights inheritance criteria for sub-network. Firstly, evaluate the importance of each block in the network based on the mean of the scaling parameters of the BN layers. Secondly, use the bisection method to quickly find the compact sub-network satisfying the budget. Finally, adaptively and efficiently choose the weight inheritance criterion that fits the current architecture and fine-tune the pruned network to recover performance. AdaPruner allows to obtain pruned network quickly, accurately and efficiently, taking into account both the structure and initialization weights. We prune the currently popular CNN models (VGG, ResNet, MobileNetV2) on different image classification datasets, and the experimental results demonstrate the effectiveness of our proposed method. On ImageNet, we reduce 32.8% FLOPs of MobileNetV2 with only 0.62% decrease for top-1 accuracy, which exceeds all previous state-of-the-art channel pruning methods. The code will be released.



### Adaptive Proposal Generation Network for Temporal Sentence Localization in Videos
- **Arxiv ID**: http://arxiv.org/abs/2109.06398v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2109.06398v1)
- **Published**: 2021-09-14 02:02:36+00:00
- **Updated**: 2021-09-14 02:02:36+00:00
- **Authors**: Daizong Liu, Xiaoye Qu, Jianfeng Dong, Pan Zhou
- **Comment**: Accepted as a long paper in the main conference of EMNLP 2021
- **Journal**: None
- **Summary**: We address the problem of temporal sentence localization in videos (TSLV). Traditional methods follow a top-down framework which localizes the target segment with pre-defined segment proposals. Although they have achieved decent performance, the proposals are handcrafted and redundant. Recently, bottom-up framework attracts increasing attention due to its superior efficiency. It directly predicts the probabilities for each frame as a boundary. However, the performance of bottom-up model is inferior to the top-down counterpart as it fails to exploit the segment-level interaction. In this paper, we propose an Adaptive Proposal Generation Network (APGN) to maintain the segment-level interaction while speeding up the efficiency. Specifically, we first perform a foreground-background classification upon the video and regress on the foreground frames to adaptively generate proposals. In this way, the handcrafted proposal design is discarded and the redundant proposals are decreased. Then, a proposal consolidation module is further developed to enhance the semantic of the generated proposals. Finally, we locate the target moments with these generated proposals following the top-down framework. Extensive experiments on three challenging benchmarks show that our proposed APGN significantly outperforms previous state-of-the-art methods.



### Progressively Guide to Attend: An Iterative Alignment Framework for Temporal Sentence Grounding
- **Arxiv ID**: http://arxiv.org/abs/2109.06400v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2109.06400v1)
- **Published**: 2021-09-14 02:08:23+00:00
- **Updated**: 2021-09-14 02:08:23+00:00
- **Authors**: Daizong Liu, Xiaoye Qu, Pan Zhou
- **Comment**: Accepted as a long paper in the main conference of EMNLP 2021
- **Journal**: None
- **Summary**: A key solution to temporal sentence grounding (TSG) exists in how to learn effective alignment between vision and language features extracted from an untrimmed video and a sentence description. Existing methods mainly leverage vanilla soft attention to perform the alignment in a single-step process. However, such single-step attention is insufficient in practice, since complicated relations between inter- and intra-modality are usually obtained through multi-step reasoning. In this paper, we propose an Iterative Alignment Network (IA-Net) for TSG task, which iteratively interacts inter- and intra-modal features within multiple steps for more accurate grounding. Specifically, during the iterative reasoning process, we pad multi-modal features with learnable parameters to alleviate the nowhere-to-attend problem of non-matched frame-word pairs, and enhance the basic co-attention mechanism in a parallel manner. To further calibrate the misaligned attention caused by each reasoning step, we also devise a calibration module following each attention module to refine the alignment knowledge. With such iterative alignment scheme, our IA-Net can robustly capture the fine-grained relations between vision and language domains step-by-step for progressively reasoning the temporal boundaries. Extensive experiments conducted on three challenging benchmarks demonstrate that our proposed model performs better than the state-of-the-arts.



### Camera-Tracklet-Aware Contrastive Learning for Unsupervised Vehicle Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2109.06401v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.06401v1)
- **Published**: 2021-09-14 02:12:54+00:00
- **Updated**: 2021-09-14 02:12:54+00:00
- **Authors**: Jongmin Yu, Junsik Kim, Minkyung Kim, Hyeontaek Oh
- **Comment**: Under review
- **Journal**: None
- **Summary**: Recently, vehicle re-identification methods based on deep learning constitute remarkable achievement. However, this achievement requires large-scale and well-annotated datasets. In constructing the dataset, assigning globally available identities (Ids) to vehicles captured from a great number of cameras is labour-intensive, because it needs to consider their subtle appearance differences or viewpoint variations. In this paper, we propose camera-tracklet-aware contrastive learning (CTACL) using the multi-camera tracklet information without vehicle identity labels. The proposed CTACL divides an unlabelled domain, i.e., entire vehicle images, into multiple camera-level subdomains and conducts contrastive learning within and beyond the subdomains. The positive and negative samples for contrastive learning are defined using tracklet Ids of each camera. Additionally, the domain adaptation across camera networks is introduced to improve the generalisation performance of learnt representations and alleviate the performance degradation resulted from the domain gap between the subdomains. We demonstrate the effectiveness of our approach on video-based and image-based vehicle Re-ID datasets. Experimental results show that the proposed method outperforms the recent state-of-the-art unsupervised vehicle Re-ID methods. The source code for this paper is publicly available on `https://github.com/andreYoo/CTAM-CTACL-VVReID.git'.



### Deep learning-based NLP Data Pipeline for EHR Scanned Document Information Extraction
- **Arxiv ID**: http://arxiv.org/abs/2110.11864v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.11864v1)
- **Published**: 2021-09-14 03:56:56+00:00
- **Updated**: 2021-09-14 03:56:56+00:00
- **Authors**: Enshuo Hsu, Ioannis Malagaris, Yong-Fang Kuo, Rizwana Sultana, Kirk Roberts
- **Comment**: 6 tables, 7 figures
- **Journal**: None
- **Summary**: Scanned documents in electronic health records (EHR) have been a challenge for decades, and are expected to stay in the foreseeable future. Current approaches for processing often include image preprocessing, optical character recognition (OCR), and text mining. However, there is limited work that evaluates the choice of image preprocessing methods, the selection of NLP models, and the role of document layout. The impact of each element remains unknown. We evaluated this method on a use case of two key indicators for sleep apnea, Apnea hypopnea index (AHI) and oxygen saturation (SaO2) values, from scanned sleep study reports. Our data that included 955 manually annotated reports was secondarily utilized from a previous study in the University of Texas Medical Branch. We performed image preprocessing: gray-scaling followed by 1 iteration of dilating and erode, and 20% contrast increasing. The OCR was implemented with the Tesseract OCR engine. A total of seven Bag-of-Words models (Logistic Regression, Ridge Regression, Lasso Regression, Support Vector Machine, k-Nearest Neighbor, Na\"ive Bayes, and Random Forest) and three deep learning-based models (BiLSTM, BERT, and Clinical BERT) were evaluated. We also evaluated the combinations of image preprocessing methods (gray-scaling, dilate & erode, increased contrast by 20%, increased contrast by 60%), and two deep learning architectures (with and without structured input that provides document layout information). Our proposed method using Clinical BERT reached an AUROC of 0.9743 and document accuracy of 94.76% for AHI, and an AUROC of 0.9523, and document accuracy of 91.61% for SaO2. We demonstrated the proper use of image preprocessing and document layout could be beneficial to scanned document processing.



### COVID-Net MLSys: Designing COVID-Net for the Clinical Workflow
- **Arxiv ID**: http://arxiv.org/abs/2109.06421v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.06421v1)
- **Published**: 2021-09-14 04:13:24+00:00
- **Updated**: 2021-09-14 04:13:24+00:00
- **Authors**: Audrey G. Chung, Maya Pavlova, Hayden Gunraj, Naomi Terhljan, Alexander MacLean, Hossein Aboutalebi, Siddharth Surana, Andy Zhao, Saad Abbasi, Alexander Wong
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: As the COVID-19 pandemic continues to devastate globally, one promising field of research is machine learning-driven computer vision to streamline various parts of the COVID-19 clinical workflow. These machine learning methods are typically stand-alone models designed without consideration for the integration necessary for real-world application workflows. In this study, we take a machine learning and systems (MLSys) perspective to design a system for COVID-19 patient screening with the clinical workflow in mind. The COVID-Net system is comprised of the continuously evolving COVIDx dataset, COVID-Net deep neural network for COVID-19 patient detection, and COVID-Net S deep neural networks for disease severity scoring for COVID-19 positive patient cases. The deep neural networks within the COVID-Net system possess state-of-the-art performance, and are designed to be integrated within a user interface (UI) for clinical decision support with automatic report generation to assist clinicians in their treatment decisions.



### Cross-Region Domain Adaptation for Class-level Alignment
- **Arxiv ID**: http://arxiv.org/abs/2109.06422v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.06422v2)
- **Published**: 2021-09-14 04:13:35+00:00
- **Updated**: 2022-10-06 08:15:35+00:00
- **Authors**: Zhijie Wang, Xing Liu, Masanori Suganuma, Takayuki Okatani
- **Comment**: Under review in Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: Semantic segmentation requires a lot of training data, which necessitates costly annotation. There have been many studies on unsupervised domain adaptation (UDA) from one domain to another, e.g., from computer graphics to real images. However, there is still a gap in accuracy between UDA and supervised training on native domain data. It is arguably attributable to class-level misalignment between the source and target domain data. To cope with this, we propose a method that applies adversarial training to align two feature distributions in the target domain. It uses a self-training framework to split the image into two regions (i.e., trusted and untrusted), which form two distributions to align in the feature space. We term this approach cross-region adaptation (CRA) to distinguish from the previous methods of aligning different domain distributions, which we call cross-domain adaptation (CDA). CRA can be applied after any CDA method. Experimental results show that this always improves the accuracy of the combined CDA method, having updated the state-of-the-art.



### Improved Few-shot Segmentation by Redefinition of the Roles of Multi-level CNN Features
- **Arxiv ID**: http://arxiv.org/abs/2109.06432v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.06432v2)
- **Published**: 2021-09-14 04:47:08+00:00
- **Updated**: 2021-09-15 02:48:07+00:00
- **Authors**: Zhijie Wang, Masanori Suganuma, Takayuki Okatani
- **Comment**: None
- **Journal**: None
- **Summary**: This study is concerned with few-shot segmentation, i.e., segmenting the region of an unseen object class in a query image, given support image(s) of its instances. The current methods rely on the pretrained CNN features of the support and query images. The key to good performance depends on the proper fusion of their mid-level and high-level features; the former contains shape-oriented information, while the latter has class-oriented information. Current state-of-the-art methods follow the approach of Tian et al., which gives the mid-level features the primary role and the high-level features the secondary role. In this paper, we reinterpret this widely employed approach by redifining the roles of the multi-level features; we swap the primary and secondary roles. Specifically, we regard that the current methods improve the initial estimate generated from the high-level features using the mid-level features. This reinterpretation suggests a new application of the current methods: to apply the same network multiple times to iteratively update the estimate of the object's region, starting from its initial estimate. Our experiments show that this method is effective and has updated the previous state-of-the-art on COCO-20$^i$ in the 1-shot and 5-shot settings and on PASCAL-5$^i$ in the 1-shot setting.



### Tesla-Rapture: A Lightweight Gesture Recognition System from mmWave Radar Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2109.06448v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.06448v1)
- **Published**: 2021-09-14 05:25:17+00:00
- **Updated**: 2021-09-14 05:25:17+00:00
- **Authors**: Dariush Salami, Ramin Hasibi, Sameera Palipana, Petar Popovski, Tom Michoel, Stephan Sigg
- **Comment**: The paper is submitted to the journal of Transactions on Mobile
  Computing. And it is still under review
- **Journal**: None
- **Summary**: We present Tesla-Rapture, a gesture recognition interface for point clouds generated by mmWave Radars. State of the art gesture recognition models are either too resource consuming or not sufficiently accurate for integration into real-life scenarios using wearable or constrained equipment such as IoT devices (e.g. Raspberry PI), XR hardware (e.g. HoloLens), or smart-phones. To tackle this issue, we developed Tesla, a Message Passing Neural Network (MPNN) graph convolution approach for mmWave radar point clouds. The model outperforms the state of the art on two datasets in terms of accuracy while reducing the computational complexity and, hence, the execution time. In particular, the approach, is able to predict a gesture almost 8 times faster than the most accurate competitor. Our performance evaluation in different scenarios (environments, angles, distances) shows that Tesla generalizes well and improves the accuracy up to 20% in challenging scenarios like a through-wall setting and sensing at extreme angles. Utilizing Tesla, we develop Tesla-Rapture, a real-time implementation using a mmWave Radar on a Raspberry PI 4 and evaluate its accuracy and time-complexity. We also publish the source code, the trained models, and the implementation of the model for embedded devices.



### Spiking Neural Networks for Visual Place Recognition via Weighted Neuronal Assignments
- **Arxiv ID**: http://arxiv.org/abs/2109.06452v2
- **DOI**: 10.1109/LRA.2022.3149030
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.06452v2)
- **Published**: 2021-09-14 05:40:40+00:00
- **Updated**: 2022-02-10 01:54:33+00:00
- **Authors**: Somayeh Hussaini, Michael Milford, Tobias Fischer
- **Comment**: 8 pages, 6 figures, IEEE Robotics and Automation Letters (RA-L), also
  accepted to IEEE International Conference on Robotics and Automation (ICRA
  2022)
- **Journal**: IEEE Robotics and Automation Letters 2022
- **Summary**: Spiking neural networks (SNNs) offer both compelling potential advantages, including energy efficiency and low latencies and challenges including the non-differentiable nature of event spikes. Much of the initial research in this area has converted deep neural networks to equivalent SNNs, but this conversion approach potentially negates some of the advantages of SNN-based approaches developed from scratch. One promising area for high-performance SNNs is template matching and image recognition. This research introduces the first high-performance SNN for the Visual Place Recognition (VPR) task: given a query image, the SNN has to find the closest match out of a list of reference images. At the core of this new system is a novel assignment scheme that implements a form of ambiguity-informed salience, by up-weighting single-place-encoding neurons and down-weighting "ambiguous" neurons that respond to multiple different reference places. In a range of experiments on the challenging Nordland, Oxford RobotCar, SPEDTest, Synthia, and St Lucia datasets, we show that our SNN achieves comparable VPR performance to state-of-the-art and classical techniques, and degrades gracefully in performance with an increasing number of reference places. Our results provide a significant milestone towards SNNs that can provide robust, energy-efficient, and low latency robot localization.



### Dodging Attack Using Carefully Crafted Natural Makeup
- **Arxiv ID**: http://arxiv.org/abs/2109.06467v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.06467v1)
- **Published**: 2021-09-14 06:27:14+00:00
- **Updated**: 2021-09-14 06:27:14+00:00
- **Authors**: Nitzan Guetta, Asaf Shabtai, Inderjeet Singh, Satoru Momiyama, Yuval Elovici
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning face recognition models are used by state-of-the-art surveillance systems to identify individuals passing through public areas (e.g., airports). Previous studies have demonstrated the use of adversarial machine learning (AML) attacks to successfully evade identification by such systems, both in the digital and physical domains. Attacks in the physical domain, however, require significant manipulation to the human participant's face, which can raise suspicion by human observers (e.g. airport security officers). In this study, we present a novel black-box AML attack which carefully crafts natural makeup, which, when applied on a human participant, prevents the participant from being identified by facial recognition models. We evaluated our proposed attack against the ArcFace face recognition model, with 20 participants in a real-world setup that includes two cameras, different shooting angles, and different lighting conditions. The evaluation results show that in the digital domain, the face recognition system was unable to identify all of the participants, while in the physical domain, the face recognition system was able to identify the participants in only 1.22% of the frames (compared to 47.57% without makeup and 33.73% with random natural makeup), which is below a reasonable threshold of a realistic operational environment.



### Space Time Recurrent Memory Network
- **Arxiv ID**: http://arxiv.org/abs/2109.06474v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.06474v2)
- **Published**: 2021-09-14 06:53:51+00:00
- **Updated**: 2022-07-06 23:43:42+00:00
- **Authors**: Hung Nguyen, Chanho Kim, Fuxin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Transformers have recently been popular for learning and inference in the spatial-temporal domain. However, their performance relies on storing and applying attention to the feature tensor of each frame in video. Hence, their space and time complexity increase linearly as the length of video grows, which could be very costly for long videos. We propose a novel visual memory network architecture for the learning and inference problem in the spatial-temporal domain. We maintain a fixed set of memory slots in our memory network and propose an algorithm based on Gumbel-Softmax to learn an adaptive strategy to update this memory. Finally, this architecture is benchmarked on the video object segmentation (VOS) and video prediction problems. We demonstrate that our memory architecture achieves state-of-the-art results, outperforming transformer-based methods on VOS and other recent methods on video prediction while maintaining constant memory capacity independent of the sequence length.



### Image-Based Alignment of 3D Scans
- **Arxiv ID**: http://arxiv.org/abs/2109.06526v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.06526v1)
- **Published**: 2021-09-14 08:48:00+00:00
- **Updated**: 2021-09-14 08:48:00+00:00
- **Authors**: Dolores Messer, Jakob Wilm, Eythor R. Eiriksson, Vedrana A. Dahl, Anders B. Dahl
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: Full 3D scanning can efficiently be obtained using structured light scanning combined with a rotation stage. In this setting it is, however, necessary to reposition the object and scan it in different poses in order to cover the entire object. In this case, correspondence between the scans is lost, since the object was moved. In this paper, we propose a fully automatic method for aligning the scans of an object in two different poses. This is done by matching 2D features between images from two poses and utilizing correspondence between the images and the scanned point clouds. To demonstrate the approach, we present the results of scanning three dissimilar objects.



### 3-Dimensional Deep Learning with Spatial Erasing for Unsupervised Anomaly Segmentation in Brain MRI
- **Arxiv ID**: http://arxiv.org/abs/2109.06540v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.06540v1)
- **Published**: 2021-09-14 09:17:27+00:00
- **Updated**: 2021-09-14 09:17:27+00:00
- **Authors**: Marcel Bengs, Finn Behrendt, Julia Krüger, Roland Opfer, Alexander Schlaefer
- **Comment**: Accepted for publication in the International Journal of Computer
  Assisted Radiology and Surgery (IJCARS)
- **Journal**: None
- **Summary**: Purpose. Brain Magnetic Resonance Images (MRIs) are essential for the diagnosis of neurological diseases. Recently, deep learning methods for unsupervised anomaly detection (UAD) have been proposed for the analysis of brain MRI. These methods rely on healthy brain MRIs and eliminate the requirement of pixel-wise annotated data compared to supervised deep learning. While a wide range of methods for UAD have been proposed, these methods are mostly 2D and only learn from MRI slices, disregarding that brain lesions are inherently 3D and the spatial context of MRI volumes remains unexploited.   Methods. We investigate whether using increased spatial context by using MRI volumes combined with spatial erasing leads to improved unsupervised anomaly segmentation performance compared to learning from slices. We evaluate and compare 2D variational autoencoder (VAE) to their 3D counterpart, propose 3D input erasing, and systemically study the impact of the data set size on the performance.   Results. Using two publicly available segmentation data sets for evaluation, 3D VAE outperform their 2D counterpart, highlighting the advantage of volumetric context. Also, our 3D erasing methods allow for further performance improvements. Our best performing 3D VAE with input erasing leads to an average DICE score of 31.40% compared to 25.76% for the 2D VAE.   Conclusions. We propose 3D deep learning methods for UAD in brain MRI combined with 3D erasing and demonstrate that 3D methods clearly outperform their 2D counterpart for anomaly segmentation. Also, our spatial erasing method allows for further performance improvements and reduces the requirement for large data sets.



### Foreground Object Structure Transfer for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2109.06543v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.06543v2)
- **Published**: 2021-09-14 09:23:27+00:00
- **Updated**: 2022-03-09 16:33:24+00:00
- **Authors**: Jieren Cheng, Le Liu, Xiangyan Tang, Wenxuan Tu, Boyi Liu, Ke Zhou, Qiaobo Da, Yue Yang
- **Comment**: 14 pages,3 figures
- **Journal**: None
- **Summary**: Unsupervised domain adaptation aims to train a classification model from the labeled source domain for the unlabeled target domain. Since the data distributions of the two domains are different, the model often performs poorly on the target domain. Existing methods align the feature distributions of the source and target domains and learn domain-invariant features to improve the performance of the model. However, the features are usually aligned as a whole, and the domain adaptation task fails to serve the classification, which will ignore the class information and lead to misalignment.In this paper, we investigate those features that should be used for domain alignment, introduce prior knowledge to extract foreground features to guide the domain adaptation task for classification tasks, and perform alignment in the local structure of objects. We propose a method called Foreground Object Structure Transfer(FOST). The key to FOST is the new clustering based condition, which combines the relative position relationship of foreground objects. Based on this conditions, FOST makes the data distribution of the same class more compact in geometry. In practice, since the label of the target domain is not available, we use the clustering information of the source domain to assign pseudo labels to the target domain samples, and then according to the source domain data prior knowledge guides those positive features to maximum the inter-class distance between different classes and mimimum the intra-class distance. Extensive experimental results on various benchmarks ($i.e.$ ImageCLEF-DA, Office-31, Office-Home, Visda-2017) under different domain adaptation settings prove that our FOST compares favorably against the existing state-of-the-art domain adaptation methods.



### Semi-Supervised Wide-Angle Portraits Correction by Multi-Scale Transformer
- **Arxiv ID**: http://arxiv.org/abs/2109.08024v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.08024v2)
- **Published**: 2021-09-14 09:40:25+00:00
- **Updated**: 2022-04-02 07:33:04+00:00
- **Authors**: Fushun Zhu, Shan Zhao, Peng Wang, Hao Wang, Hua Yan, Shuaicheng Liu
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: We propose a semi-supervised network for wide-angle portraits correction. Wide-angle images often suffer from skew and distortion affected by perspective distortion, especially noticeable at the face regions. Previous deep learning based approaches need the ground-truth correction flow maps for training guidance. However, such labels are expensive, which can only be obtained manually. In this work, we design a semi-supervised scheme and build a high-quality unlabeled dataset with rich scenarios, allowing us to simultaneously use labeled and unlabeled data to improve performance. Specifically, our semi-supervised scheme takes advantage of the consistency mechanism, with several novel components such as direction and range consistency (DRC) and regression consistency (RC). Furthermore, different from the existing methods, we propose the Multi-Scale Swin-Unet (MS-Unet) based on the multi-scale swin transformer block (MSTB), which can simultaneously learn short-distance and long-distance information to avoid artifacts. Extensive experiments demonstrate that the proposed method is superior to the state-of-the-art methods and other representative baselines. The source code and dataset are available at: https://github.com/megvii-research/Portraits_Correction.



### Multi-Scale Input Strategies for Medulloblastoma Tumor Classification using Deep Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.06547v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.06547v1)
- **Published**: 2021-09-14 09:42:37+00:00
- **Updated**: 2021-09-14 09:42:37+00:00
- **Authors**: Marcel Bengs, Satish Pant, Michael Bockmayr, Ulrich Schüller, Alexander Schlaefer
- **Comment**: Accepted at CURAC 2021
- **Journal**: None
- **Summary**: Medulloblastoma (MB) is a primary central nervous system tumor and the most common malignant brain cancer among children. Neuropathologists perform microscopic inspection of histopathological tissue slides under a microscope to assess the severity of the tumor. This is a time-consuming task and often infused with observer variability. Recently, pre-trained convolutional neural networks (CNN) have shown promising results for MB subtype classification. Typically, high-resolution images are divided into smaller tiles for classification, while the size of the tiles has not been systematically evaluated. We study the impact of tile size and input strategy and classify the two major histopathological subtypes-Classic and Demoplastic/Nodular. To this end, we use recently proposed EfficientNets and evaluate tiles with increasing size combined with various downsampling scales. Our results demonstrate using large input tiles pixels followed by intermediate downsampling and patch cropping significantly improves MB classification performance. Our top-performing method achieves the AUC-ROC value of 90.90\% compared to 84.53\% using the previous approach with smaller input tiles.



### Dense Deep Unfolding Network with 3D-CNN Prior for Snapshot Compressive Imaging
- **Arxiv ID**: http://arxiv.org/abs/2109.06548v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.06548v1)
- **Published**: 2021-09-14 09:42:42+00:00
- **Updated**: 2021-09-14 09:42:42+00:00
- **Authors**: Zhuoyuan Wu, Jian Zhang, Chong Mou
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Snapshot compressive imaging (SCI) aims to record three-dimensional signals via a two-dimensional camera. For the sake of building a fast and accurate SCI recovery algorithm, we incorporate the interpretability of model-based methods and the speed of learning-based ones and present a novel dense deep unfolding network (DUN) with 3D-CNN prior for SCI, where each phase is unrolled from an iteration of Half-Quadratic Splitting (HQS). To better exploit the spatial-temporal correlation among frames and address the problem of information loss between adjacent phases in existing DUNs, we propose to adopt the 3D-CNN prior in our proximal mapping module and develop a novel dense feature map (DFM) strategy, respectively. Besides, in order to promote network robustness, we further propose a dense feature map adaption (DFMA) module to allow inter-phase information to fuse adaptively. All the parameters are learned in an end-to-end fashion. Extensive experiments on simulation data and real data verify the superiority of our method. The source code is available at https://github.com/jianzhangcs/SCI3D.



### Anomaly Attribution of Multivariate Time Series using Counterfactual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2109.06562v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2109.06562v1)
- **Published**: 2021-09-14 10:15:52+00:00
- **Updated**: 2021-09-14 10:15:52+00:00
- **Authors**: Violeta Teodora Trifunov, Maha Shadaydeh, Björn Barz, Joachim Denzler
- **Comment**: ICMLA 2021
- **Journal**: None
- **Summary**: There are numerous methods for detecting anomalies in time series, but that is only the first step to understanding them. We strive to exceed this by explaining those anomalies. Thus we develop a novel attribution scheme for multivariate time series relying on counterfactual reasoning. We aim to answer the counterfactual question of would the anomalous event have occurred if the subset of the involved variables had been more similarly distributed to the data outside of the anomalous interval. Specifically, we detect anomalous intervals using the Maximally Divergent Interval (MDI) algorithm, replace a subset of variables with their in-distribution values within the detected interval and observe if the interval has become less anomalous, by re-scoring it with MDI. We evaluate our method on multivariate temporal and spatio-temporal data and confirm the accuracy of our anomaly attribution of multiple well-understood extreme climate events such as heatwaves and hurricanes.



### The pitfalls of using open data to develop deep learning solutions for COVID-19 detection in chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/2109.08020v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.08020v1)
- **Published**: 2021-09-14 10:59:11+00:00
- **Updated**: 2021-09-14 10:59:11+00:00
- **Authors**: Rachael Harkness, Geoff Hall, Alejandro F Frangi, Nishant Ravikumar, Kieran Zucker
- **Comment**: To be published in MedInfo 21 - 18th World Congress on Medical and
  Health Informatics; 5 pages, 5 figures
- **Journal**: None
- **Summary**: Since the emergence of COVID-19, deep learning models have been developed to identify COVID-19 from chest X-rays. With little to no direct access to hospital data, the AI community relies heavily on public data comprising numerous data sources. Model performance results have been exceptional when training and testing on open-source data, surpassing the reported capabilities of AI in pneumonia-detection prior to the COVID-19 outbreak. In this study impactful models are trained on a widely used open-source data and tested on an external test set and a hospital dataset, for the task of classifying chest X-rays into one of three classes: COVID-19, non-COVID pneumonia and no-pneumonia. Classification performance of the models investigated is evaluated through ROC curves, confusion matrices and standard classification metrics. Explainability modules are implemented to explore the image features most important to classification. Data analysis and model evaluations show that the popular open-source dataset COVIDx is not representative of the real clinical problem and that results from testing on this are inflated. Dependence on open-source data can leave models vulnerable to bias and confounding variables, requiring careful analysis to develop clinically useful/viable AI tools for COVID-19 detection in chest X-rays.



### A Semantic Indexing Structure for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2109.06583v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, 68T20, H.3.1
- **Links**: [PDF](http://arxiv.org/pdf/2109.06583v1)
- **Published**: 2021-09-14 11:12:30+00:00
- **Updated**: 2021-09-14 11:12:30+00:00
- **Authors**: Ying Wang, Tingzhen Liu, Zepeng Bu, Yuhui Huang, Lizhong Gao, Qiao Wang
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: In large-scale image retrieval, many indexing methods have been proposed to narrow down the searching scope of retrieval. The features extracted from images usually are of high dimensions or unfixed sizes due to the existence of key points. Most of existing index structures suffer from the dimension curse, the unfixed feature size and/or the loss of semantic similarity. In this paper a new classification-based indexing structure, called Semantic Indexing Structure (SIS), is proposed, in which we utilize the semantic categories rather than clustering centers to create database partitions, such that the proposed index SIS can be combined with feature extractors without the restriction of dimensions. Besides, it is observed that the size of each semantic partition is positively correlated with the semantic distribution of database. Along this way, we found that when the partition number is normalized to five, the proposed algorithm performed very well in all the tests. Compared with state-of-the-art models, SIS achieves outstanding performance.



### High-Fidelity GAN Inversion for Image Attribute Editing
- **Arxiv ID**: http://arxiv.org/abs/2109.06590v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.06590v3)
- **Published**: 2021-09-14 11:23:48+00:00
- **Updated**: 2022-03-17 08:58:51+00:00
- **Authors**: Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, Qifeng Chen
- **Comment**: CVPR 2022; Project Page is at https://tengfei-wang.github.io/HFGI/
- **Journal**: None
- **Summary**: We present a novel high-fidelity generative adversarial network (GAN) inversion framework that enables attribute editing with image-specific details well-preserved (e.g., background, appearance, and illumination). We first analyze the challenges of high-fidelity GAN inversion from the perspective of lossy data compression. With a low bit-rate latent code, previous works have difficulties in preserving high-fidelity details in reconstructed and edited images. Increasing the size of a latent code can improve the accuracy of GAN inversion but at the cost of inferior editability. To improve image fidelity without compromising editability, we propose a distortion consultation approach that employs a distortion map as a reference for high-fidelity reconstruction. In the distortion consultation inversion (DCI), the distortion map is first projected to a high-rate latent map, which then complements the basic low-rate latent code with more details via consultation fusion. To achieve high-fidelity editing, we propose an adaptive distortion alignment (ADA) module with a self-supervised training scheme, which bridges the gap between the edited and inversion images. Extensive experiments in the face and car domains show a clear improvement in both inversion and editing quality.



### Sampling Network Guided Cross-Entropy Method for Unsupervised Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2109.06619v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.06619v2)
- **Published**: 2021-09-14 12:16:02+00:00
- **Updated**: 2021-09-15 17:21:05+00:00
- **Authors**: Haobo Jiang, Yaqi Shen, Jin Xie, Jun Li, Jianjun Qian, Jian Yang
- **Comment**: Accepted by ICCV-2021
- **Journal**: None
- **Summary**: In this paper, by modeling the point cloud registration task as a Markov decision process, we propose an end-to-end deep model embedded with the cross-entropy method (CEM) for unsupervised 3D registration. Our model consists of a sampling network module and a differentiable CEM module. In our sampling network module, given a pair of point clouds, the sampling network learns a prior sampling distribution over the transformation space. The learned sampling distribution can be used as a "good" initialization of the differentiable CEM module. In our differentiable CEM module, we first propose a maximum consensus criterion based alignment metric as the reward function for the point cloud registration task. Based on the reward function, for each state, we then construct a fused score function to evaluate the sampled transformations, where we weight the current and future rewards of the transformations. Particularly, the future rewards of the sampled transforms are obtained by performing the iterative closest point (ICP) algorithm on the transformed state. By selecting the top-k transformations with the highest scores, we iteratively update the sampling distribution. Furthermore, in order to make the CEM differentiable, we use the sparsemax function to replace the hard top-$k$ selection. Finally, we formulate a Geman-McClure estimator based loss to train our end-to-end registration model. Extensive experimental results demonstrate the good registration performance of our method on benchmark datasets.



### Dynamic Attentive Graph Learning for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2109.06620v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.06620v1)
- **Published**: 2021-09-14 12:19:15+00:00
- **Updated**: 2021-09-14 12:19:15+00:00
- **Authors**: Chong Mou, Jian Zhang, Zhuoyuan Wu
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Non-local self-similarity in natural images has been verified to be an effective prior for image restoration. However, most existing deep non-local methods assign a fixed number of neighbors for each query item, neglecting the dynamics of non-local correlations. Moreover, the non-local correlations are usually based on pixels, prone to be biased due to image degradation. To rectify these weaknesses, in this paper, we propose a dynamic attentive graph learning model (DAGL) to explore the dynamic non-local property on patch level for image restoration. Specifically, we propose an improved graph model to perform patch-wise graph convolution with a dynamic and adaptive number of neighbors for each node. In this way, image content can adaptively balance over-smooth and over-sharp artifacts through the number of its connected neighbors, and the patch-wise non-local correlations can enhance the message passing process. Experimental results on various image restoration tasks: synthetic image denoising, real image denoising, image demosaicing, and compression artifact reduction show that our DAGL can produce state-of-the-art results with superior accuracy and visual quality. The source code is available at https://github.com/jianzhangcs/DAGL.



### Multi-Scale Aligned Distillation for Low-Resolution Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.06875v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.06875v1)
- **Published**: 2021-09-14 12:53:35+00:00
- **Updated**: 2021-09-14 12:53:35+00:00
- **Authors**: Lu Qi, Jason Kuen, Jiuxiang Gu, Zhe Lin, Yi Wang, Yukang Chen, Yanwei Li, Jiaya Jia
- **Comment**: In CVPR 2021
- **Journal**: None
- **Summary**: In instance-level detection tasks (e.g., object detection), reducing input resolution is an easy option to improve runtime efficiency. However, this option traditionally hurts the detection performance much. This paper focuses on boosting the performance of low-resolution models by distilling knowledge from a high- or multi-resolution model. We first identify the challenge of applying knowledge distillation (KD) to teacher and student networks that act on different input resolutions. To tackle it, we explore the idea of spatially aligning feature maps between models of varying input resolutions by shifting feature pyramid positions and introduce aligned multi-scale training to train a multi-scale teacher that can distill its knowledge to a low-resolution student. Further, we propose crossing feature-level fusion to dynamically fuse teacher's multi-resolution features to guide the student better. On several instance-level detection tasks and datasets, the low-resolution models trained via our approach perform competitively with high-resolution models trained via conventional multi-scale training, while outperforming the latter's low-resolution models by 2.1% to 3.6% in terms of mAP. Our code is made publicly available at https://github.com/dvlab-research/MSAD.



### Identifying partial mouse brain microscopy images from Allen reference atlas using a contrastively learned semantic space
- **Arxiv ID**: http://arxiv.org/abs/2109.06662v3
- **DOI**: 10.1007/978-3-031-11203-4_18
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2109.06662v3)
- **Published**: 2021-09-14 13:11:34+00:00
- **Updated**: 2022-07-21 07:53:58+00:00
- **Authors**: Justinas Antanavicius, Roberto Leiras, Raghavendra Selvan
- **Comment**: Published in the Proceedings of International Workshop on Biomedical
  Image Registration (WBIR-2022). Source code available at
  https://github.com/Justinas256/2d-mouse-brain-identification. 12 pages, 6
  figures
- **Journal**: None
- **Summary**: Precise identification of mouse brain microscopy images is a crucial first step when anatomical structures in the mouse brain are to be registered to a reference atlas. Practitioners usually rely on manual comparison of images or tools that assume the presence of complete images. This work explores Siamese Networks as the method for finding corresponding 2D reference atlas plates for given partial 2D mouse brain images. Siamese networks are a class of convolutional neural networks (CNNs) that use weight-shared paths to obtain low dimensional embeddings of pairs of input images. The correspondence between the partial mouse brain image and reference atlas plate is determined based on the distance between low dimensional embeddings of brain slices and atlas plates that are obtained from Siamese networks using contrastive learning. Experiments showed that Siamese CNNs can precisely identify brain slices using the Allen mouse brain atlas when training and testing images come from the same source. They achieved TOP-1 and TOP-5 accuracy of 25% and 100%, respectively, taking only 7.2 seconds to identify 29 images.



### High-Resolution Image Harmonization via Collaborative Dual Transformations
- **Arxiv ID**: http://arxiv.org/abs/2109.06671v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.06671v2)
- **Published**: 2021-09-14 13:18:58+00:00
- **Updated**: 2022-03-24 00:08:21+00:00
- **Authors**: Wenyan Cong, Xinhao Tao, Li Niu, Jing Liang, Xuesong Gao, Qihao Sun, Liqing Zhang
- **Comment**: Accepted by CVPR2022
- **Journal**: None
- **Summary**: Given a composite image, image harmonization aims to adjust the foreground to make it compatible with the background. High-resolution image harmonization is in high demand, but still remains unexplored. Conventional image harmonization methods learn global RGB-to-RGB transformation which could effortlessly scale to high resolution, but ignore diverse local context. Recent deep learning methods learn the dense pixel-to-pixel transformation which could generate harmonious outputs, but are highly constrained in low resolution. In this work, we propose a high-resolution image harmonization network with Collaborative Dual Transformation (CDTNet) to combine pixel-to-pixel transformation and RGB-to-RGB transformation coherently in an end-to-end network. Our CDTNet consists of a low-resolution generator for pixel-to-pixel transformation, a color mapping module for RGB-to-RGB transformation, and a refinement module to take advantage of both. Extensive experiments on high-resolution benchmark dataset and our created high-resolution real composite images demonstrate that our CDTNet strikes a good balance between efficiency and effectiveness. Our used datasets can be found in https://github.com/bcmi/CDTNet-High-Resolution-Image-Harmonization.



### Luminance Attentive Networks for HDR Image and Panorama Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2109.06688v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.06688v1)
- **Published**: 2021-09-14 13:44:34+00:00
- **Updated**: 2021-09-14 13:44:34+00:00
- **Authors**: Hanning Yu, Wentao Liu, Chengjiang Long, Bo Dong, Qin Zou, Chunxia Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: It is very challenging to reconstruct a high dynamic range (HDR) from a low dynamic range (LDR) image as an ill-posed problem. This paper proposes a luminance attentive network named LANet for HDR reconstruction from a single LDR image. Our method is based on two fundamental observations: (1) HDR images stored in relative luminance are scale-invariant, which means the HDR images will hold the same information when multiplied by any positive real number. Based on this observation, we propose a novel normalization method called " HDR calibration " for HDR images stored in relative luminance, calibrating HDR images into a similar luminance scale according to the LDR images. (2) The main difference between HDR images and LDR images is in under-/over-exposed areas, especially those highlighted. Following this observation, we propose a luminance attention module with a two-stream structure for LANet to pay more attention to the under-/over-exposed areas. In addition, we propose an extended network called panoLANet for HDR panorama reconstruction from an LDR panorama and build a dualnet structure for panoLANet to solve the distortion problem caused by the equirectangular panorama. Extensive experiments show that our proposed approach LANet can reconstruct visually convincing HDR images and demonstrate its superiority over state-of-the-art approaches in terms of all metrics in inverse tone mapping. The image-based lighting application with our proposed panoLANet also demonstrates that our method can simulate natural scene lighting using only LDR panorama. Our source code is available at https://github.com/LWT3437/LANet.



### LRWR: Large-Scale Benchmark for Lip Reading in Russian language
- **Arxiv ID**: http://arxiv.org/abs/2109.06692v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.06692v1)
- **Published**: 2021-09-14 13:51:19+00:00
- **Updated**: 2021-09-14 13:51:19+00:00
- **Authors**: Evgeniy Egorov, Vasily Kostyumov, Mikhail Konyk, Sergey Kolesnikov
- **Comment**: None
- **Journal**: None
- **Summary**: Lipreading, also known as visual speech recognition, aims to identify the speech content from videos by analyzing the visual deformations of lips and nearby areas. One of the significant obstacles for research in this field is the lack of proper datasets for a wide variety of languages: so far, these methods have been focused only on English or Chinese. In this paper, we introduce a naturally distributed large-scale benchmark for lipreading in Russian language, named LRWR, which contains 235 classes and 135 speakers. We provide a detailed description of the dataset collection pipeline and dataset statistics. We also present a comprehensive comparison of the current popular lipreading methods on LRWR and conduct a detailed analysis of their performance. The results demonstrate the differences between the benchmarked languages and provide several promising directions for lipreading models finetuning. Thanks to our findings, we also achieved new state-of-the-art results on the LRW benchmark.



### ImUnity: a generalizable VAE-GAN solution for multicenter MR image harmonization
- **Arxiv ID**: http://arxiv.org/abs/2109.06756v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.06756v1)
- **Published**: 2021-09-14 15:21:19+00:00
- **Updated**: 2021-09-14 15:21:19+00:00
- **Authors**: Stenzel Cackowski, Emmanuel L. Barbier, Michel Dojat, Thomas Christen
- **Comment**: 15 pages, 7 Figures
- **Journal**: None
- **Summary**: ImUnity is an original deep-learning model designed for efficient and flexible MR image harmonization. A VAE-GAN network, coupled with a confusion module and an optional biological preservation module, uses multiple 2D-slices taken from different anatomical locations in each subject of the training database, as well as image contrast transformations for its self-supervised training. It eventually generates 'corrected' MR images that can be used for various multi-center population studies. Using 3 open source databases (ABIDE, OASIS and SRPBS), which contain MR images from multiple acquisition scanner types or vendors and a large range of subjects ages, we show that ImUnity: (1) outperforms state-of-the-art methods in terms of quality of images generated using traveling subjects; (2) removes sites or scanner biases while improving patients classification; (3) harmonizes data coming from new sites or scanners without the need for an additional fine-tuning and (4) allows the selection of multiple MR reconstructed images according to the desired applications. Tested here on T1-weighted images, ImUnity could be used to harmonize other types of medical images.



### MotionHint: Self-Supervised Monocular Visual Odometry with Motion Constraints
- **Arxiv ID**: http://arxiv.org/abs/2109.06768v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.06768v3)
- **Published**: 2021-09-14 15:35:08+00:00
- **Updated**: 2022-03-02 08:58:18+00:00
- **Authors**: Cong Wang, Yu-Ping Wang, Dinesh Manocha
- **Comment**: Accepted by ICRA 2022
- **Journal**: None
- **Summary**: We present a novel self-supervised algorithm named MotionHint for monocular visual odometry (VO) that takes motion constraints into account. A key aspect of our approach is to use an appropriate motion model that can help existing self-supervised monocular VO (SSM-VO) algorithms to overcome issues related to the local minima within their self-supervised loss functions. The motion model is expressed with a neural network named PPnet. It is trained to coarsely predict the next pose of the camera and the uncertainty of this prediction. Our self-supervised approach combines the original loss and the motion loss, which is the weighted difference between the prediction and the generated ego-motion. Taking two existing SSM-VO systems as our baseline, we evaluate our MotionHint algorithm on the standard KITTI benchmark. Experimental results show that our MotionHint algorithm can be easily applied to existing open-sourced state-of-the-art SSM-VO systems to greatly improve the performance by reducing the resulting ATE by up to 28.73%.



### A Deep Learning Approach for Masking Fetal Gender in Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/2109.06790v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.06790v1)
- **Published**: 2021-09-14 16:00:29+00:00
- **Updated**: 2021-09-14 16:00:29+00:00
- **Authors**: Amit Borundiya, Arshak Navruzyan, Dennis Igoschev, Feras C. Oughali, Hemanth Pasupuleti, Mike Fuller, Vinay Kanigicherla, T S Aniruddha Kashyap, Rishabh Chaurasia, Sonali Vinod Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Ultrasound (US) imaging is highly effective with regards to both cost and versatility in real-time diagnosis; however, determination of fetal gender by US scan in the early stages of pregnancy is also a cause of sex-selective abortion. This work proposes a deep learning object detection approach to accurately mask fetal gender in US images in order to increase the accessibility of the technology. We demonstrate how the YOLOv5L architecture exhibits superior performance relative to other object detection models on this task. Our model achieves 45.8% AP[0.5:0.95], 92% F1-score and 0.006 False Positive Per Image rate on our test set. Furthermore, we introduce a bounding box delay rule based on frame-to-frame structural similarity to reduce the false negative rate by 85%, further improving masking reliability.



### Automatic hippocampal surface generation via 3D U-net and active shape modeling with hybrid particle swarm optimization
- **Arxiv ID**: http://arxiv.org/abs/2109.06817v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.06817v1)
- **Published**: 2021-09-14 16:55:55+00:00
- **Updated**: 2021-09-14 16:55:55+00:00
- **Authors**: Pinyuan Zhong, Yue Zhang, Xiaoying Tang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we proposed and validated a fully automatic pipeline for hippocampal surface generation via 3D U-net coupled with active shape modeling (ASM). Principally, the proposed pipeline consisted of three steps. In the beginning, for each magnetic resonance image, a 3D U-net was employed to obtain the automatic hippocampus segmentation at each hemisphere. Secondly, ASM was performed on a group of pre-obtained template surfaces to generate mean shape and shape variation parameters through principal component analysis. Ultimately, hybrid particle swarm optimization was utilized to search for the optimal shape variation parameters that best match the segmentation. The hippocampal surface was then generated from the mean shape and the shape variation parameters. The proposed pipeline was observed to provide hippocampal surfaces at both hemispheres with high accuracy, correct anatomical topology, and sufficient smoothness.



### One-Class Meta-Learning: Towards Generalizable Few-Shot Open-Set Classification
- **Arxiv ID**: http://arxiv.org/abs/2109.06859v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.06859v1)
- **Published**: 2021-09-14 17:52:51+00:00
- **Updated**: 2021-09-14 17:52:51+00:00
- **Authors**: Jedrzej Kozerawski, Matthew Turk
- **Comment**: 21 pages, submitted to BMVC 2021
- **Journal**: None
- **Summary**: Real-world classification tasks are frequently required to work in an open-set setting. This is especially challenging for few-shot learning problems due to the small sample size for each known category, which prevents existing open-set methods from working effectively; however, most multiclass few-shot methods are limited to closed-set scenarios. In this work, we address the problem of few-shot open-set classification by first proposing methods for few-shot one-class classification and then extending them to few-shot multiclass open-set classification. We introduce two independent few-shot one-class classification methods: Meta Binary Cross-Entropy (Meta-BCE), which learns a separate feature representation for one-class classification, and One-Class Meta-Learning (OCML), which learns to generate one-class classifiers given standard multiclass feature representation. Both methods can augment any existing few-shot learning method without requiring retraining to work in a few-shot multiclass open-set setting without degrading its closed-set performance. We demonstrate the benefits and drawbacks of both methods in different problem settings and evaluate them on three standard benchmark datasets, miniImageNet, tieredImageNet, and Caltech-UCSD-Birds-200-2011, where they surpass the state-of-the-art methods in the few-shot multiclass open-set and few-shot one-class tasks.



### Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2109.06860v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.06860v1)
- **Published**: 2021-09-14 17:52:55+00:00
- **Updated**: 2021-09-14 17:52:55+00:00
- **Authors**: Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, Kai-Wei Chang
- **Comment**: EMNLP 2021. Code and data are available at
  https://github.com/WadeYin9712/GD-VCR
- **Journal**: None
- **Summary**: Commonsense is defined as the knowledge that is shared by everyone. However, certain types of commonsense knowledge are correlated with culture and geographic locations and they are only shared locally. For example, the scenarios of wedding ceremonies vary across regions due to different customs influenced by historical and religious factors. Such regional characteristics, however, are generally omitted in prior work. In this paper, we construct a Geo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test vision-and-language models' ability to understand cultural and geo-location-specific commonsense. In particular, we study two state-of-the-art Vision-and-Language models, VisualBERT and ViLBERT trained on VCR, a standard multimodal commonsense benchmark with images primarily from Western regions. We then evaluate how well the trained models can generalize to answering the questions in GD-VCR. We find that the performance of both models for non-Western regions including East Asia, South Asia, and Africa is significantly lower than that for Western region. We analyze the reasons behind the performance disparity and find that the performance gap is larger on QA pairs that: 1) are concerned with culture-related scenarios, e.g., weddings, religious activities, and festivals; 2) require high-level geo-diverse commonsense reasoning rather than low-order perception and recognition. Dataset and code are released at https://github.com/WadeYin9712/GD-VCR.



### Focus on Impact: Indoor Exploration with Intrinsic Motivation
- **Arxiv ID**: http://arxiv.org/abs/2109.08521v2
- **DOI**: 10.1109/LRA.2022.3145971
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.08521v2)
- **Published**: 2021-09-14 18:00:07+00:00
- **Updated**: 2022-02-04 14:45:40+00:00
- **Authors**: Roberto Bigazzi, Federico Landi, Silvia Cascianelli, Lorenzo Baraldi, Marcella Cornia, Rita Cucchiara
- **Comment**: Published in IEEE Robotics and Automation Letters. To appear in ICRA
  2022
- **Journal**: IEEE Robotics and Automation Letters (Volume: 7, Issue: 2, April
  2022)
- **Summary**: Exploration of indoor environments has recently experienced a significant interest, also thanks to the introduction of deep neural agents built in a hierarchical fashion and trained with Deep Reinforcement Learning (DRL) on simulated environments. Current state-of-the-art methods employ a dense extrinsic reward that requires the complete a priori knowledge of the layout of the training environment to learn an effective exploration policy. However, such information is expensive to gather in terms of time and resources. In this work, we propose to train the model with a purely intrinsic reward signal to guide exploration, which is based on the impact of the robot's actions on its internal representation of the environment. So far, impact-based rewards have been employed for simple tasks and in procedurally generated synthetic environments with countable states. Since the number of states observable by the agent in realistic indoor environments is non-countable, we include a neural-based density model and replace the traditional count-based regularization with an estimated pseudo-count of previously visited states. The proposed exploration approach outperforms DRL-based competitors relying on intrinsic rewards and surpasses the agents trained with a dense extrinsic reward computed with the environment layouts. We also show that a robot equipped with the proposed approach seamlessly adapts to point-goal navigation and real-world deployment.



### Hardware-aware Real-time Myocardial Segmentation Quality Control in Contrast Echocardiography
- **Arxiv ID**: http://arxiv.org/abs/2109.06909v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2109.06909v1)
- **Published**: 2021-09-14 18:14:53+00:00
- **Updated**: 2021-09-14 18:14:53+00:00
- **Authors**: Dewen Zeng, Yukun Ding, Haiyun Yuan, Meiping Huang, Xiaowei Xu, Jian Zhuang, Jingtong Hu, Yiyu Shi
- **Comment**: 4 pages, DAC'21 invited paper
- **Journal**: None
- **Summary**: Automatic myocardial segmentation of contrast echocardiography has shown great potential in the quantification of myocardial perfusion parameters. Segmentation quality control is an important step to ensure the accuracy of segmentation results for quality research as well as its clinical application. Usually, the segmentation quality control happens after the data acquisition. At the data acquisition time, the operator could not know the quality of the segmentation results. On-the-fly segmentation quality control could help the operator to adjust the ultrasound probe or retake data if the quality is unsatisfied, which can greatly reduce the effort of time-consuming manual correction. However, it is infeasible to deploy state-of-the-art DNN-based models because the segmentation module and quality control module must fit in the limited hardware resource on the ultrasound machine while satisfying strict latency constraints. In this paper, we propose a hardware-aware neural architecture search framework for automatic myocardial segmentation and quality control of contrast echocardiography. We explicitly incorporate the hardware latency as a regularization term into the loss function during training. The proposed method searches the best neural network architecture for the segmentation module and quality prediction module with strict latency.



### A trainable monogenic ConvNet layer robust in front of large contrast changes in image classification
- **Arxiv ID**: http://arxiv.org/abs/2109.06926v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.06926v1)
- **Published**: 2021-09-14 18:50:07+00:00
- **Updated**: 2021-09-14 18:50:07+00:00
- **Authors**: E. Ulises Moya-Sánchez, Sebastiá Xambo-Descamps, Abraham Sánchez, Sebastián Salazar-Colores, Ulises Cortés
- **Comment**: "For associated code, see
  https://gitlab.com/monogenic-layer-m6/monogenic-layer-trainablebio-inspired-cnnlayerforcontrastinvariance"
- **Journal**: None
- **Summary**: Convolutional Neural Networks (ConvNets) at present achieve remarkable performance in image classification tasks. However, current ConvNets cannot guarantee the capabilities of the mammalian visual systems such as invariance to contrast and illumination changes. Some ideas to overcome the illumination and contrast variations usually have to be tuned manually and tend to fail when tested with other types of data degradation. In this context, we present a new bio-inspired {entry} layer, M6, which detects low-level geometric features (lines, edges, and orientations) which are similar to patterns detected by the V1 visual cortex. This new trainable layer is capable of coping with image classification even with large contrast variations. The explanation for this behavior is the monogenic signal geometry, which represents each pixel value in a 3D space using quaternions, a fact that confers a degree of explainability to the networks. We compare M6 with a conventional convolutional layer (C) and a deterministic quaternion local phase layer (Q9). The experimental setup {is designed to evaluate the robustness} of our M6 enriched ConvNet model and includes three architectures, four datasets, three types of contrast degradation (including non-uniform haze degradations). The numerical results reveal that the models with M6 are the most robust in front of any kind of contrast variations. This amounts to a significant enhancement of the C models, which usually have reasonably good performance only when the same training and test degradation are used, except for the case of maximum degradation. Moreover, the Structural Similarity Index Measure (SSIM) is used to analyze and explain the robustness effect of the M6 feature maps under any kind of contrast degradations.



### Multi-modal Wound Classification using Wound Image and Location by Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2109.06969v1
- **DOI**: 10.1038/s41598-022-21813-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.06969v1)
- **Published**: 2021-09-14 21:00:30+00:00
- **Updated**: 2021-09-14 21:00:30+00:00
- **Authors**: D. M. Anisuzzaman, Yash Patel, Behrouz Rostami, Jeffrey Niezgoda, Sandeep Gopalakrishnan, Zeyun Yu
- **Comment**: 30 pages, 10 figures, 15 tables
- **Journal**: Sci Rep 12, 20057 (2022)
- **Summary**: Wound classification is an essential step of wound diagnosis. An efficient classifier can assist wound specialists in classifying wound types with less financial and time costs and help them decide an optimal treatment procedure. This study developed a deep neural network-based multi-modal classifier using wound images and their corresponding locations to categorize wound images into multiple classes, including diabetic, pressure, surgical, and venous ulcers. A body map is also developed to prepare the location data, which can help wound specialists tag wound locations more efficiently. Three datasets containing images and their corresponding location information are designed with the help of wound specialists. The multi-modal network is developed by concatenating the image-based and location-based classifier's outputs with some other modifications. The maximum accuracy on mixed-class classifications (containing background and normal skin) varies from 77.33% to 100% on different experiments. The maximum accuracy on wound-class classifications (containing only diabetic, pressure, surgical, and venous) varies from 72.95% to 98.08% on different experiments. The proposed multi-modal network also shows a significant improvement in results from the previous works of literature.



### SafeAccess+: An Intelligent System to make Smart Home Safer and Americans with Disability Act Compliant
- **Arxiv ID**: http://arxiv.org/abs/2110.09273v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.AI, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2110.09273v1)
- **Published**: 2021-09-14 22:39:58+00:00
- **Updated**: 2021-09-14 22:39:58+00:00
- **Authors**: Shahinur Alam
- **Comment**: None
- **Journal**: None
- **Summary**: Smart homes are becoming ubiquitous, but they are not Americans with Disability Act (ADA) compliant. Smart homes equipped with ADA compliant appliances and services are critical for people with disabilities (i.e., visual impairments and limited mobility) to improve independence, safety, and quality of life. Despite all advancements in smart home technologies, some fundamental design and implementation issues remain. For example, people with disabilities often feel insecure to respond when someone knocks on the door or rings the doorbell. In this paper, we present an intelligent system called "SafeAccess+" to build safer and ADA compliant premises (e.g. smart homes, offices). The key functionalities of the SafeAccess+ are: 1) Monitoring the inside/outside of premises and identifying incoming people; 2) Providing users relevant information to assess incoming threats (e.g., burglary, robbery) and ongoing crimes 3) Allowing users to grant safe access to homes for friends/family members. We have addressed several technical and research challenges: - developing models to detect and recognize person/activity, generating image descriptions, designing ADA compliant end-end system. In addition, we have designed a prototype smart door showcasing the proof-of-concept. The premises are expected to be equipped with cameras placed in strategic locations that facilitate monitoring the premise 24/7 to identify incoming persons and to generate image descriptions. The system generates a pre-structured message from the image description to assess incoming threats and immediately notify the users. The completeness and generalization of models have been ensured through a rigorous quantitative evaluation. The users' satisfaction and reliability of the system has been measured using PYTHEIA scale and was rated excellent (Internal Consistency-Cronbach's alpha is 0.784, Test-retest reliability is 0.939 )



### ZFlow: Gated Appearance Flow-based Virtual Try-on with 3D Priors
- **Arxiv ID**: http://arxiv.org/abs/2109.07001v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.07001v1)
- **Published**: 2021-09-14 22:41:14+00:00
- **Updated**: 2021-09-14 22:41:14+00:00
- **Authors**: Ayush Chopra, Rishabh Jain, Mayur Hemani, Balaji Krishnamurthy
- **Comment**: Accepted at ICCV 2021
- **Journal**: None
- **Summary**: Image-based virtual try-on involves synthesizing perceptually convincing images of a model wearing a particular garment and has garnered significant research interest due to its immense practical applicability. Recent methods involve a two stage process: i) warping of the garment to align with the model ii) texture fusion of the warped garment and target model to generate the try-on output. Issues arise due to the non-rigid nature of garments and the lack of geometric information about the model or the garment. It often results in improper rendering of granular details. We propose ZFlow, an end-to-end framework, which seeks to alleviate these concerns regarding geometric and textural integrity (such as pose, depth-ordering, skin and neckline reproduction) through a combination of gated aggregation of hierarchical flow estimates termed Gated Appearance Flow, and dense structural priors at various stage of the network. ZFlow achieves state-of-the-art results as observed qualitatively, and on quantitative benchmarks of image quality (PSNR, SSIM, and FID). The paper presents extensive comparisons with other existing solutions including a detailed user study and ablation studies to gauge the effect of each of our contributions on multiple datasets.



