# Arxiv Papers in cs.CV on 2021-09-29
### Visually Grounded Concept Composition
- **Arxiv ID**: http://arxiv.org/abs/2109.14115v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.14115v1)
- **Published**: 2021-09-29 00:38:58+00:00
- **Updated**: 2021-09-29 00:38:58+00:00
- **Authors**: Bowen Zhang, Hexiang Hu, Linlu Qiu, Peter Shaw, Fei Sha
- **Comment**: Findings of EMNLP 2021
- **Journal**: None
- **Summary**: We investigate ways to compose complex concepts in texts from primitive ones while grounding them in images. We propose Concept and Relation Graph (CRG), which builds on top of constituency analysis and consists of recursively combined concepts with predicate functions. Meanwhile, we propose a concept composition neural network called Composer to leverage the CRG for visually grounded concept learning. Specifically, we learn the grounding of both primitive and all composed concepts by aligning them to images and show that learning to compose leads to more robust grounding results, measured in text-to-image matching accuracy. Notably, our model can model grounded concepts forming at both the finer-grained sentence level and the coarser-grained intermediate level (or word-level). Composer leads to pronounced improvement in matching accuracy when the evaluation data has significant compound divergence from the training data.



### Comparison of atlas-based and neural-network-based semantic segmentation for DENSE MRI images
- **Arxiv ID**: http://arxiv.org/abs/2109.14116v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2109.14116v1)
- **Published**: 2021-09-29 00:42:43+00:00
- **Updated**: 2021-09-29 00:42:43+00:00
- **Authors**: Elle Buser, Emma Hart, Ben Huenemann
- **Comment**: 23 pages, 14 figures
- **Journal**: None
- **Summary**: Two segmentation methods, one atlas-based and one neural-network-based, were compared to see how well they can each automatically segment the brain stem and cerebellum in Displacement Encoding with Stimulated Echoes Magnetic Resonance Imaging (DENSE-MRI) data. The segmentation is a pre-requisite for estimating the average displacements in these regions, which have recently been proposed as biomarkers in the diagnosis of Chiari Malformation type I (CMI). In numerical experiments, the segmentations of both methods were similar to manual segmentations provided by trained experts. It was found that, overall, the neural-network-based method alone produced more accurate segmentations than the atlas-based method did alone, but that a combination of the two methods -- in which the atlas-based method is used for the segmentation of the brain stem and the neural-network is used for the segmentation of the cerebellum -- may be the most successful.



### Meta Learning on a Sequence of Imbalanced Domains with Difficulty Awareness
- **Arxiv ID**: http://arxiv.org/abs/2109.14120v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14120v1)
- **Published**: 2021-09-29 00:53:09+00:00
- **Updated**: 2021-09-29 00:53:09+00:00
- **Authors**: Zhenyi Wang, Tiehang Duan, Le Fang, Qiuling Suo, Mingchen Gao
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Recognizing new objects by learning from a few labeled examples in an evolving environment is crucial to obtain excellent generalization ability for real-world machine learning systems. A typical setting across current meta learning algorithms assumes a stationary task distribution during meta training. In this paper, we explore a more practical and challenging setting where task distribution changes over time with domain shift. Particularly, we consider realistic scenarios where task distribution is highly imbalanced with domain labels unavailable in nature. We propose a kernel-based method for domain change detection and a difficulty-aware memory management mechanism that jointly considers the imbalanced domain size and domain importance to learn across domains continuously. Furthermore, we introduce an efficient adaptive task sampling method during meta training, which significantly reduces task gradient variance with theoretical guarantees. Finally, we propose a challenging benchmark with imbalanced domain sequences and varied domain difficulty. We have performed extensive evaluations on the proposed benchmark, demonstrating the effectiveness of our method. We made our code publicly available.



### Grouptron: Dynamic Multi-Scale Graph Convolutional Networks for Group-Aware Dense Crowd Trajectory Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2109.14128v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.14128v3)
- **Published**: 2021-09-29 01:22:25+00:00
- **Updated**: 2022-03-04 22:07:37+00:00
- **Authors**: Rui Zhou, Hongyu Zhou, Huidong Gao, Masayoshi Tomizuka, Jiachen Li, Zhuo Xu
- **Comment**: ICRA 2022 Accepted
- **Journal**: None
- **Summary**: Accurate, long-term forecasting of pedestrian trajectories in highly dynamic and interactive scenes is a long-standing challenge. Recent advances in using data-driven approaches have achieved significant improvements in terms of prediction accuracy. However, the lack of group-aware analysis has limited the performance of forecasting models. This is especially nonnegligible in highly crowded scenes, where pedestrians are moving in groups and the interactions between groups are extremely complex and dynamic. In this paper, we present Grouptron, a multi-scale dynamic forecasting framework that leverages pedestrian group detection and utilizes individual-level, group-level and scene-level information for better understanding and representation of the scenes. Our approach employs spatio-temporal clustering algorithms to identify pedestrian groups, creates spatio-temporal graphs at the individual, group, and scene levels. It then uses graph neural networks to encode dynamics at different scales and aggregate the embeddings for trajectory prediction. We conducted extensive comparisons and ablation experiments to demonstrate the effectiveness of our approach. Our method achieves 9.3% decrease in final displacement error (FDE) compared with state-of-the-art methods on ETH/UCY benchmark datasets, and 16.1% decrease in FDE in more crowded scenes where extensive human group interactions are more frequently present.



### Contrastive Video-Language Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.14131v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2109.14131v1)
- **Published**: 2021-09-29 01:40:58+00:00
- **Updated**: 2021-09-29 01:40:58+00:00
- **Authors**: Chen Liang, Yawei Luo, Yu Wu, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: We focus on the problem of segmenting a certain object referred by a natural language sentence in video content, at the core of formulating a pinpoint vision-language relation. While existing attempts mainly construct such relation in an implicit way, i.e., grid-level multi-modal feature fusion, it has been proven problematic to distinguish semantically similar objects under this paradigm. In this work, we propose to interwind the visual and linguistic modalities in an explicit way via the contrastive learning objective, which directly aligns the referred object and the language description and separates the unreferred content apart across frames. Moreover, to remedy for the degradation problem, we present two complementary hard instance mining strategies, i.e., Language-relevant Channel Filter and Relative Hard Instance Construction. They encourage the network to exclude visual-distinguishable feature and to focus on easy-confused objects during the contrastive training. Extensive experiments on two benchmarks, i.e., A2D Sentences and J-HMDB Sentences, quantitatively demonstrate the state-of-the-arts performance of our method and qualitatively show the more accurate distinguishment between semantically similar objects over baselines.



### Fine-Grained Zero-Shot Learning with DNA as Side Information
- **Arxiv ID**: http://arxiv.org/abs/2109.14133v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.14133v1)
- **Published**: 2021-09-29 01:45:22+00:00
- **Updated**: 2021-09-29 01:45:22+00:00
- **Authors**: Sarkhan Badirli, Zeynep Akata, George Mohler, Christine Picard, Murat Dundar
- **Comment**: Accepted to NeurIPS 2021
- **Journal**: None
- **Summary**: Fine-grained zero-shot learning task requires some form of side-information to transfer discriminative information from seen to unseen classes. As manually annotated visual attributes are extremely costly and often impractical to obtain for a large number of classes, in this study we use DNA as side information for the first time for fine-grained zero-shot classification of species. Mitochondrial DNA plays an important role as a genetic marker in evolutionary biology and has been used to achieve near-perfect accuracy in the species classification of living organisms. We implement a simple hierarchical Bayesian model that uses DNA information to establish the hierarchy in the image space and employs local priors to define surrogate classes for unseen ones. On the benchmark CUB dataset, we show that DNA can be equally promising yet in general a more accessible alternative than word vectors as a side information. This is especially important as obtaining robust word representations for fine-grained species names is not a practicable goal when information about these species in free-form text is limited. On a newly compiled fine-grained insect dataset that uses DNA information from over a thousand species, we show that the Bayesian approach outperforms state-of-the-art by a wide margin.



### Improved Xception with Dual Attention Mechanism and Feature Fusion for Face Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.14136v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14136v1)
- **Published**: 2021-09-29 01:54:13+00:00
- **Updated**: 2021-09-29 01:54:13+00:00
- **Authors**: Hao Lin, Weiqi Luo, Kangkang Wei, Minglin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid development of deep learning technology, more and more face forgeries by deepfake are widely spread on social media, causing serious social concern. Face forgery detection has become a research hotspot in recent years, and many related methods have been proposed until now. For those images with low quality and/or diverse sources, however, the detection performances of existing methods are still far from satisfactory. In this paper, we propose an improved Xception with dual attention mechanism and feature fusion for face forgery detection. Different from the middle flow in original Xception model, we try to catch different high-semantic features of the face images using different levels of convolution, and introduce the convolutional block attention module and feature fusion to refine and reorganize those high-semantic features. In the exit flow, we employ the self-attention mechanism and depthwise separable convolution to learn the global information and local information of the fused features separately to improve the classification the ability of the proposed model. Experimental results evaluated on three Deepfake datasets demonstrate that the proposed method outperforms Xception as well as other related methods both in effectiveness and generalization ability.



### Geometry-Entangled Visual Semantic Transformer for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2109.14137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14137v1)
- **Published**: 2021-09-29 01:58:34+00:00
- **Updated**: 2021-09-29 01:58:34+00:00
- **Authors**: Ling Cheng, Wei Wei, Feida Zhu, Yong Liu, Chunyan Miao
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advancements of image captioning have featured Visual-Semantic Fusion or Geometry-Aid attention refinement. However, those fusion-based models, they are still criticized for the lack of geometry information for inter and intra attention refinement. On the other side, models based on Geometry-Aid attention still suffer from the modality gap between visual and semantic information. In this paper, we introduce a novel Geometry-Entangled Visual Semantic Transformer (GEVST) network to realize the complementary advantages of Visual-Semantic Fusion and Geometry-Aid attention refinement. Concretely, a Dense-Cap model proposes some dense captions with corresponding geometry information at first. Then, to empower GEVST with the ability to bridge the modality gap among visual and semantic information, we build four parallel transformer encoders VV(Pure Visual), VS(Semantic fused to Visual), SV(Visual fused to Semantic), SS(Pure Semantic) for final caption generation. Both visual and semantic geometry features are used in the Fusion module and also the Self-Attention module for better attention measurement. To validate our model, we conduct extensive experiments on the MS-COCO dataset, the experimental results show that our GEVST model can obtain promising performance gains.



### Towards Communication-Efficient and Privacy-Preserving Federated Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.14611v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14611v2)
- **Published**: 2021-09-29 02:13:22+00:00
- **Updated**: 2022-01-15 16:02:01+00:00
- **Authors**: Haizhou Shi, Youcai Zhang, Zijin Shen, Siliang Tang, Yaqian Li, Yandong Guo, Yueting Zhuang
- **Comment**: To appear in FL-AAAI-22 workshop, working in progress
- **Journal**: None
- **Summary**: This paper investigates the feasibility of federated representation learning under the constraints of communication cost and privacy protection. Existing works either conduct annotation-guided local training which requires frequent communication or aggregates the client models via weight averaging which has potential risks of privacy exposure. To tackle the above problems, we first identify that self-supervised contrastive local training is robust against the non-identically distributed data, which provides the feasibility of longer local training and thus reduces the communication cost. Then based on the aforementioned robustness, we propose a novel Federated representation Learning framework with Ensemble Similarity Distillation~(FLESD) that utilizes this robustness. At each round of communication, the server first gathers a fraction of the clients' inferred similarity matrices on a public dataset. Then it ensembles the similarity matrices and train the global model via similarity distillation. We verify the effectiveness of FLESD by a series of empirical experiments and show that, despite stricter constraints, it achieves comparable results under multiple settings on multiple datasets.



### Multi-frame Joint Enhancement for Early Interlaced Videos
- **Arxiv ID**: http://arxiv.org/abs/2109.14151v1
- **DOI**: 10.1109/TIP.2022.3207003
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14151v1)
- **Published**: 2021-09-29 02:37:15+00:00
- **Updated**: 2021-09-29 02:37:15+00:00
- **Authors**: Yang Zhao, Yanbo Ma, Yuan Chen, Wei Jia, Ronggang Wang, Xiaoping Liu
- **Comment**: 12 pages, 14 figures
- **Journal**: None
- **Summary**: Early interlaced videos usually contain multiple and interlacing and complex compression artifacts, which significantly reduce the visual quality. Although the high-definition reconstruction technology for early videos has made great progress in recent years, related research on deinterlacing is still lacking. Traditional methods mainly focus on simple interlacing mechanism, and cannot deal with the complex artifacts in real-world early videos. Recent interlaced video reconstruction deep deinterlacing models only focus on single frame, while neglecting important temporal information. Therefore, this paper proposes a multiframe deinterlacing network joint enhancement network for early interlaced videos that consists of three modules, i.e., spatial vertical interpolation module, temporal alignment and fusion module, and final refinement module. The proposed method can effectively remove the complex artifacts in early videos by using temporal redundancy of multi-fields. Experimental results demonstrate that the proposed method can recover high quality results for both synthetic dataset and real-world early interlaced videos.



### Hybrid Dynamic Contrast and Probability Distillation for Unsupervised Person Re-Id
- **Arxiv ID**: http://arxiv.org/abs/2109.14157v1
- **DOI**: 10.1109/TIP.2022.3169693
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14157v1)
- **Published**: 2021-09-29 02:56:45+00:00
- **Updated**: 2021-09-29 02:56:45+00:00
- **Authors**: De Cheng, Jingyu Zhou, Nannan Wang, Xinbo Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised person re-identification (Re-Id) has attracted increasing attention due to its practical application in the read-world video surveillance system. The traditional unsupervised Re-Id are mostly based on the method alternating between clustering and fine-tuning with the classification or metric learning objectives on the grouped clusters. However, since person Re-Id is an open-set problem, the clustering based methods often leave out lots of outlier instances or group the instances into the wrong clusters, thus they can not make full use of the training samples as a whole. To solve these problems, we present the hybrid dynamic cluster contrast and probability distillation algorithm. It formulates the unsupervised Re-Id problem into an unified local-to-global dynamic contrastive learning and self-supervised probability distillation framework. Specifically, the proposed method can make the utmost of the self-supervised signals of all the clustered and un-clustered instances, from both the instances' self-contrastive level and the probability distillation respective, in the memory-based non-parametric manner. Besides, the proposed hybrid local-to-global contrastive learning can take full advantage of the informative and valuable training examples for effective and robust training. Extensive experiment results show that the proposed method achieves superior performances to state-of-the-art methods, under both the purely unsupervised and unsupervised domain adaptation experiment settings.



### Semantic Communications With AI Tasks
- **Arxiv ID**: http://arxiv.org/abs/2109.14170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14170v1)
- **Published**: 2021-09-29 03:33:40+00:00
- **Updated**: 2021-09-29 03:33:40+00:00
- **Authors**: Yang Yang, Caili Guo, Fangfang Liu, Chuanhong Liu, Lunan Sun, Qizheng Sun, Jiujiu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: A radical paradigm shift of wireless networks from ``connected things'' to ``connected intelligence'' undergoes, which coincides with the Shanno and Weaver's envisions: Communications will transform from the technical level to the semantic level. This article proposes a semantic communication method with artificial intelligence tasks (SC-AIT). First, the architecture of SC-AIT is elaborated. Then, based on the proposed architecture, we implement SC-AIT for a image classifications task. A prototype of SC-AIT is also established for surface defect detection, is conducted. Experimental results show that SC-AIT has much lower bandwidth requirements, and can achieve more than $40\%$ classification accuracy gains compared with the communications at the technical level. Future trends and key challenges for semantic communications are also identified.



### Semi-Supervised Segmentation of Radiation-Induced Pulmonary Fibrosis from Lung CT Scans with Multi-Scale Guided Dense Attention
- **Arxiv ID**: http://arxiv.org/abs/2109.14172v1
- **DOI**: 10.1109/TMI.2021.3117564
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14172v1)
- **Published**: 2021-09-29 03:35:50+00:00
- **Updated**: 2021-09-29 03:35:50+00:00
- **Authors**: Guotai Wang, Shuwei Zhai, Giovanni Lasio, Baoshe Zhang, Byong Yi, Shifeng Chen, Thomas J. Macvittie, Dimitris Metaxas, Jinghao Zhou, Shaoting Zhang
- **Comment**: 12 pages, 9 figures. Submitted to IEEE TMI
- **Journal**: None
- **Summary**: Computed Tomography (CT) plays an important role in monitoring radiation-induced Pulmonary Fibrosis (PF), where accurate segmentation of the PF lesions is highly desired for diagnosis and treatment follow-up. However, the task is challenged by ambiguous boundary, irregular shape, various position and size of the lesions, as well as the difficulty in acquiring a large set of annotated volumetric images for training. To overcome these problems, we propose a novel convolutional neural network called PF-Net and incorporate it into a semi-supervised learning framework based on Iterative Confidence-based Refinement And Weighting of pseudo Labels (I-CRAWL). Our PF-Net combines 2D and 3D convolutions to deal with CT volumes with large inter-slice spacing, and uses multi-scale guided dense attention to segment complex PF lesions. For semi-supervised learning, our I-CRAWL employs pixel-level uncertainty-based confidence-aware refinement to improve the accuracy of pseudo labels of unannotated images, and uses image-level uncertainty for confidence-based image weighting to suppress low-quality pseudo labels in an iterative training process. Extensive experiments with CT scans of Rhesus Macaques with radiation-induced PF showed that: 1) PF-Net achieved higher segmentation accuracy than existing 2D, 3D and 2.5D neural networks, and 2) I-CRAWL outperformed state-of-the-art semi-supervised learning methods for the PF lesion segmentation task. Our method has a potential to improve the diagnosis of PF and clinical assessment of side effects of radiotherapy for lung cancers.



### Cross-Camera Human Motion Transfer by Time Series Analysis
- **Arxiv ID**: http://arxiv.org/abs/2109.14174v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14174v2)
- **Published**: 2021-09-29 03:39:01+00:00
- **Updated**: 2022-05-30 13:47:48+00:00
- **Authors**: Yaping Zhao, Guanghan Li, Zhongrui Wang, Edmund Y. Lam
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: Along with advances in optical sensors is the increasingly common practice of building an imaging system with heterogeneous cameras. While high-resolution (HR) video acquisition and analysis benefit from hybrid sensors, the intrinsic characteristics of multiple cameras lead to a challenging motion transfer problem. In this paper, we propose an algorithm using time series analysis for motion transfer among multiple cameras. Specifically, we first identify seasonality in the motion data, and then build an additive time series model to extract patterns that could be transferred across different cameras. Our approach has a complete and clear mathematical formulation, and the algorithm is also efficient and interpretable. Through the experiment on real-world data, we demonstrate the effectiveness of our method. Furthermore, our motion transfer algorithm could combine with and facilitate downstream tasks, e.g., enhancing pose estimation on low-resolution (LR) videos with inherent patterns extracted from HR ones.



### REFLACX, a dataset of reports and eye-tracking data for localization of abnormalities in chest x-rays
- **Arxiv ID**: http://arxiv.org/abs/2109.14187v3
- **DOI**: 10.1038/s41597-022-01441-z
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14187v3)
- **Published**: 2021-09-29 04:14:16+00:00
- **Updated**: 2022-06-29 01:29:34+00:00
- **Authors**: Ricardo Bigolin Lanfredi, Mingyuan Zhang, William F. Auffermann, Jessica Chan, Phuong-Anh T. Duong, Vivek Srikumar, Trafton Drew, Joyce D. Schroeder, Tolga Tasdizen
- **Comment**: Supplementary material included as ancillary files. Update 1: added
  clarifications and a graph showing the time correlation between gaze and
  report. Update 2: This preprint has not undergone peer review or any
  post-submission improvements or corrections. The Version of Record of this
  article is published in Scientific Data, and is available online at
  https://doi.org/10.1038/s41597-022-01441-z
- **Journal**: None
- **Summary**: Deep learning has shown recent success in classifying anomalies in chest x-rays, but datasets are still small compared to natural image datasets. Supervision of abnormality localization has been shown to improve trained models, partially compensating for dataset sizes. However, explicitly labeling these anomalies requires an expert and is very time-consuming. We propose a potentially scalable method for collecting implicit localization data using an eye tracker to capture gaze locations and a microphone to capture a dictation of a report, imitating the setup of a reading room. The resulting REFLACX (Reports and Eye-Tracking Data for Localization of Abnormalities in Chest X-rays) dataset was labeled across five radiologists and contains 3,032 synchronized sets of eye-tracking data and timestamped report transcriptions for 2,616 chest x-rays from the MIMIC-CXR dataset. We also provide auxiliary annotations, including bounding boxes around lungs and heart and validation labels consisting of ellipses localizing abnormalities and image-level labels. Furthermore, a small subset of the data contains readings from all radiologists, allowing for the calculation of inter-rater scores.



### WEDGE: Web-Image Assisted Domain Generalization for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.14196v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.14196v4)
- **Published**: 2021-09-29 05:19:58+00:00
- **Updated**: 2023-05-02 05:59:19+00:00
- **Authors**: Namyup Kim, Taeyoung Son, Jaehyun Pahk, Cuiling Lan, Wenjun Zeng, Suha Kwak
- **Comment**: None
- **Journal**: None
- **Summary**: Domain generalization for semantic segmentation is highly demanded in real applications, where a trained model is expected to work well in previously unseen domains. One challenge lies in the lack of data which could cover the diverse distributions of the possible unseen domains for training. In this paper, we propose a WEb-image assisted Domain GEneralization (WEDGE) scheme, which is the first to exploit the diversity of web-crawled images for generalizable semantic segmentation. To explore and exploit the real-world data distributions, we collect web-crawled images which present large diversity in terms of weather conditions, sites, lighting, camera styles, etc. We also present a method which injects styles of the web-crawled images into training images on-the-fly during training, which enables the network to experience images of diverse styles with reliable labels for effective training. Moreover, we use the web-crawled images with their predicted pseudo labels for training to further enhance the capability of the network. Extensive experiments demonstrate that our method clearly outperforms existing domain generalization techniques.



### Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation
- **Arxiv ID**: http://arxiv.org/abs/2109.14200v1
- **DOI**: 10.34842/w3vw-s84
- **Categories**: **eess.AS**, cs.AI, cs.CL, cs.CV, cs.LG, cs.SD, I.2.0; I.2.6; I.2.7; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2109.14200v1)
- **Published**: 2021-09-29 05:49:46+00:00
- **Updated**: 2021-09-29 05:49:46+00:00
- **Authors**: Khazar Khorrami, Okko Räsänen
- **Comment**: Final manuscript published in Language Development Research under CC
  BY-NC-SA 4.0. Pre-print redistributed through arXiv with permission. Replaces
  corrupted PsyArXiv pre-print repository at https://psyarxiv.com/37zna
- **Journal**: Language Development Research, 2021
- **Summary**: Decades of research has studied how language learning infants learn to discriminate speech sounds, segment words, and associate words with their meanings. While gradual development of such capabilities is unquestionable, the exact nature of these skills and the underlying mental representations yet remains unclear. In parallel, computational studies have shown that basic comprehension of speech can be achieved by statistical learning between speech and concurrent referentially ambiguous visual input. These models can operate without prior linguistic knowledge such as representations of linguistic units, and without learning mechanisms specifically targeted at such units. This has raised the question of to what extent knowledge of linguistic units, such as phone(me)s, syllables, and words, could actually emerge as latent representations supporting the translation between speech and representations in other modalities, and without the units being proximal learning targets for the learner. In this study, we formulate this idea as the so-called latent language hypothesis (LLH), connecting linguistic representation learning to general predictive processing within and across sensory modalities. We review the extent that the audiovisual aspect of LLH is supported by the existing computational studies. We then explore LLH further in extensive learning simulations with different neural network models for audiovisual cross-situational learning, and comparing learning from both synthetic and real speech data. We investigate whether the latent representations learned by the networks reflect phonetic, syllabic, or lexical structure of input speech by utilizing an array of complementary evaluation metrics related to linguistic selectivity and temporal characteristics of the representations. As a result, we find that representations associated...



### Identity-Expression Ambiguity in 3D Morphable Face Models
- **Arxiv ID**: http://arxiv.org/abs/2109.14203v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2109.14203v1)
- **Published**: 2021-09-29 06:11:43+00:00
- **Updated**: 2021-09-29 06:11:43+00:00
- **Authors**: Bernhard Egger, Skylar Sutherland, Safa C. Medin, Joshua Tenenbaum
- **Comment**: IEEE International Conference on Automatic Face and Gesture
  Recognition 2021
- **Journal**: None
- **Summary**: 3D Morphable Models are a class of generative models commonly used to model faces. They are typically applied to ill-posed problems such as 3D reconstruction from 2D data. Several ambiguities in this problem's image formation process have been studied explicitly. We demonstrate that non-orthogonality of the variation in identity and expression can cause identity-expression ambiguity in 3D Morphable Models, and that in practice expression and identity are far from orthogonal and can explain each other surprisingly well. Whilst previously reported ambiguities only arise in an inverse rendering setting, identity-expression ambiguity emerges in the 3D shape generation process itself. We demonstrate this effect with 3D shapes directly as well as through an inverse rendering task, and use two popular models built from high quality 3D scans as well as a model built from a large collection of 2D images and videos. We explore this issue's implications for inverse rendering and observe that it cannot be resolved by a purely statistical prior on identity and expression deformations.



### On Brightness Agnostic Adversarial Examples Against Face Recognition Systems
- **Arxiv ID**: http://arxiv.org/abs/2109.14205v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.14205v1)
- **Published**: 2021-09-29 06:15:17+00:00
- **Updated**: 2021-09-29 06:15:17+00:00
- **Authors**: Inderjeet Singh, Satoru Momiyama, Kazuya Kakizaki, Toshinori Araki
- **Comment**: Accepted at BIOSIG 2021 conference
- **Journal**: LNI Volume: BIOSIG 2021, LNI Volume 315, ISBN 978-3-88579-709-8
- **Summary**: This paper introduces a novel adversarial example generation method against face recognition systems (FRSs). An adversarial example (AX) is an image with deliberately crafted noise to cause incorrect predictions by a target system. The AXs generated from our method remain robust under real-world brightness changes. Our method performs non-linear brightness transformations while leveraging the concept of curriculum learning during the attack generation procedure. We demonstrate that our method outperforms conventional techniques from comprehensive experimental investigations in the digital and physical world. Furthermore, this method enables practical risk assessment of FRSs against brightness agnostic AXs.



### Unsupervised Domain Adaptation in Semantic Segmentation Based on Pixel Alignment and Self-Training
- **Arxiv ID**: http://arxiv.org/abs/2109.14219v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14219v1)
- **Published**: 2021-09-29 06:56:57+00:00
- **Updated**: 2021-09-29 06:56:57+00:00
- **Authors**: Hexin Dong, Fei Yu, Jie Zhao, Bin Dong, Li Zhang
- **Comment**: 6 pages,3 figures,MICCAI 2021 Cross-Modality Domain Adaptation for
  Medical Image Segmentation Challenge
- **Journal**: None
- **Summary**: This paper proposes an unsupervised cross-modality domain adaptation approach based on pixel alignment and self-training. Pixel alignment transfers ceT1 scans to hrT2 modality, helping to reduce domain shift in the training segmentation model. Self-training adapts the decision boundary of the segmentation network to fit the distribution of hrT2 scans. Experiment results show that PAST has outperformed the non-UDA baseline significantly, and it received rank-2 on CrossMoDA validation phase Leaderboard with a mean Dice score of 0.8395.



### Multipath CNN with alpha matte inference for knee tissue segmentation from MRI
- **Arxiv ID**: http://arxiv.org/abs/2109.14249v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14249v1)
- **Published**: 2021-09-29 07:48:47+00:00
- **Updated**: 2021-09-29 07:48:47+00:00
- **Authors**: Sheheryar Khan, Basim Azam, Yongcheng Yao, Weitian Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Precise segmentation of knee tissues from magnetic resonance imaging (MRI) is critical in quantitative imaging and diagnosis. Convolutional neural networks (CNNs), which are state of the art, have limitations owing to the lack of image-specific adaptation, such as low tissue contrasts and structural inhomogeneities, thereby leading to incomplete segmentation results. This paper presents a deep learning based automatic segmentation framework for knee tissue segmentation. A novel multipath CNN-based method is proposed, which consists of an encoder decoder-based segmentation network in combination with a low rank tensor-reconstructed segmentation network. Low rank reconstruction in MRI tensor sub-blocks is introduced to exploit the structural and morphological variations in knee tissues. To further improve the segmentation from CNNs, trimap generation, which effectively utilizes superimposed regions, is proposed for defining high, medium and low confidence regions from the multipath CNNs. The secondary path with low rank reconstructed input mitigates the conditions in which the primary segmentation network can potentially fail and overlook the boundary regions. The outcome of the segmentation is solved as an alpha matting problem by blending the trimap with the source input. Experiments on Osteoarthritis Initiative (OAI) datasets and a self prepared scan validate the effectiveness of the proposed method. We specifically demonstrate the application of the proposed method in a cartilage segmentation based thickness map for diagnosis purposes.



### Road Network Guided Fine-Grained Urban Traffic Flow Inference
- **Arxiv ID**: http://arxiv.org/abs/2109.14251v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14251v2)
- **Published**: 2021-09-29 07:51:49+00:00
- **Updated**: 2023-04-06 06:25:10+00:00
- **Authors**: Lingbo Liu, Mengmeng Liu, Guanbin Li, Ziyi Wu, Junfan Lin, Liang Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate inference of fine-grained traffic flow from coarse-grained one is an emerging yet crucial problem, which can help greatly reduce the number of the required traffic monitoring sensors for cost savings. In this work, we notice that traffic flow has a high correlation with road network, which was either completely ignored or simply treated as an external factor in previous works.To facilitate this problem, we propose a novel Road-Aware Traffic Flow Magnifier (RATFM) that explicitly exploits the prior knowledge of road networks to fully learn the road-aware spatial distribution of fine-grained traffic flow. Specifically, a multi-directional 1D convolutional layer is first introduced to extract the semantic feature of the road network. Subsequently, we incorporate the road network feature and coarse-grained flow feature to regularize the short-range spatial distribution modeling of road-relative traffic flow. Furthermore, we take the road network feature as a query to capture the long-range spatial distribution of traffic flow with a transformer architecture. Benefiting from the road-aware inference mechanism, our method can generate high-quality fine-grained traffic flow maps. Extensive experiments on three real-world datasets show that the proposed RATFM outperforms state-of-the-art models under various scenarios. Our code and datasets are released at {\url{https://github.com/luimoli/RATFM}}.



### Designing Counterfactual Generators using Deep Model Inversion
- **Arxiv ID**: http://arxiv.org/abs/2109.14274v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14274v2)
- **Published**: 2021-09-29 08:40:50+00:00
- **Updated**: 2021-10-05 16:36:30+00:00
- **Authors**: Jayaraman J. Thiagarajan, Vivek Narayanaswamy, Deepta Rajan, Jason Liang, Akshay Chaudhari, Andreas Spanias
- **Comment**: Neurips 2021
- **Journal**: None
- **Summary**: Explanation techniques that synthesize small, interpretable changes to a given image while producing desired changes in the model prediction have become popular for introspecting black-box models. Commonly referred to as counterfactuals, the synthesized explanations are required to contain discernible changes (for easy interpretability) while also being realistic (consistency to the data manifold). In this paper, we focus on the case where we have access only to the trained deep classifier and not the actual training data. While the problem of inverting deep models to synthesize images from the training distribution has been explored, our goal is to develop a deep inversion approach to generate counterfactual explanations for a given query image. Despite their effectiveness in conditional image synthesis, we show that existing deep inversion methods are insufficient for producing meaningful counterfactuals. We propose DISC (Deep Inversion for Synthesizing Counterfactuals) that improves upon deep inversion by utilizing (a) stronger image priors, (b) incorporating a novel manifold consistency objective and (c) adopting a progressive optimization strategy. We find that, in addition to producing visually meaningful explanations, the counterfactuals from DISC are effective at learning classifier decision boundaries and are robust to unknown test-time corruptions.



### Localizing Objects with Self-Supervised Transformers and no Labels
- **Arxiv ID**: http://arxiv.org/abs/2109.14279v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14279v1)
- **Published**: 2021-09-29 09:01:07+00:00
- **Updated**: 2021-09-29 09:01:07+00:00
- **Authors**: Oriane Siméoni, Gilles Puy, Huy V. Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Renaud Marlet, Jean Ponce
- **Comment**: None
- **Journal**: BMVC 2021
- **Summary**: Localizing objects in image collections without supervision can help to avoid expensive annotation campaigns. We propose a simple approach to this problem, that leverages the activation features of a vision transformer pre-trained in a self-supervised manner. Our method, LOST, does not require any external object proposal nor any exploration of the image collection; it operates on a single image. Yet, we outperform state-of-the-art object discovery methods by up to 8 CorLoc points on PASCAL VOC 2012. We also show that training a class-agnostic detector on the discovered objects boosts results by another 7 points. Moreover, we show promising results on the unsupervised object discovery task. The code to reproduce our results can be found at https://github.com/valeoai/LOST.



### Self-Supervised Learning for 3D Medical Image Analysis using 3D SimCLR and Monte Carlo Dropout
- **Arxiv ID**: http://arxiv.org/abs/2109.14288v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14288v2)
- **Published**: 2021-09-29 09:15:41+00:00
- **Updated**: 2021-10-01 08:33:36+00:00
- **Authors**: Yamen Ali, Aiham Taleb, Marina M. -C. Höhne, Christoph Lippert
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning methods can be used to learn meaningful representations from unlabeled data that can be transferred to supervised downstream tasks to reduce the need for labeled data. In this paper, we propose a 3D self-supervised method that is based on the contrastive (SimCLR) method. Additionally, we show that employing Bayesian neural networks (with Monte-Carlo Dropout) during the inference phase can further enhance the results on the downstream tasks. We showcase our models on two medical imaging segmentation tasks: i) Brain Tumor Segmentation from 3D MRI, ii) Pancreas Tumor Segmentation from 3D CT. Our experimental results demonstrate the benefits of our proposed methods in both downstream data-efficiency and performance.



### Three-Stream 3D/1D CNN for Fine-Grained Action Classification and Segmentation in Table Tennis
- **Arxiv ID**: http://arxiv.org/abs/2109.14306v1
- **DOI**: 10.1145/3475722.3482793
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2109.14306v1)
- **Published**: 2021-09-29 09:43:21+00:00
- **Updated**: 2021-09-29 09:43:21+00:00
- **Authors**: Pierre-Etienne Martin, Jenny Benois-Pineau, Renaud Péteri, Julien Morlier
- **Comment**: MMSports '21, October 20, 2021, Virtual Event,, Oct 2021, Chengdu,
  China
- **Journal**: None
- **Summary**: This paper proposes a fusion method of modalities extracted from video through a three-stream network with spatio-temporal and temporal convolutions for fine-grained action classification in sport. It is applied to TTStroke-21 dataset which consists of untrimmed videos of table tennis games. The goal is to detect and classify table tennis strokes in the videos, the first step of a bigger scheme aiming at giving feedback to the players for improving their performance. The three modalities are raw RGB data, the computed optical flow and the estimated pose of the player. The network consists of three branches with attention blocks. Features are fused at the latest stage of the network using bilinear layers. Compared to previous approaches, the use of three modalities allows faster convergence and better performances on both tasks: classification of strokes with known temporal boundaries and joint segmentation and classification. The pose is also further investigated in order to offer richer feedback to the athletes.



### From Beginner to Master: A Survey for Deep Learning-based Single-Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2109.14335v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14335v1)
- **Published**: 2021-09-29 10:41:41+00:00
- **Updated**: 2021-09-29 10:41:41+00:00
- **Authors**: Juncheng Li, Zehua Pei, Tieyong Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: Single-image super-resolution (SISR) is an important task in image processing, which aims to enhance the resolution of imaging systems. Recently, SISR has made a huge leap and has achieved promising results with the help of deep learning (DL). In this survey, we give an overview of DL-based SISR methods and group them according to their targets, such as reconstruction efficiency, reconstruction accuracy, and perceptual accuracy. Specifically, we first introduce the problem definition, research background, and the significance of SISR. Secondly, we introduce some related works, including benchmark datasets, upsampling methods, optimization objectives, and image quality assessment methods. Thirdly, we provide a detailed investigation of SISR and give some domain-specific applications of it. Fourthly, we present the reconstruction results of some classic SISR methods to intuitively know their performance. Finally, we discuss some issues that still exist in SISR and summarize some new trends and future directions. This is an exhaustive survey of SISR, which can help researchers better understand SISR and inspire more exciting research in this field. An investigation project for SISR is provided in https://github.com/CV-JunchengLi/SISR-Survey.



### Does deep learning model calibration improve performance in class-imbalanced medical image classification?
- **Arxiv ID**: http://arxiv.org/abs/2110.00918v3
- **DOI**: 10.1371/journal.pone.0262838
- **Categories**: **cs.LG**, cs.CV, eess.IV, 68T07, I.2
- **Links**: [PDF](http://arxiv.org/pdf/2110.00918v3)
- **Published**: 2021-09-29 12:00:32+00:00
- **Updated**: 2021-10-11 12:37:06+00:00
- **Authors**: Sivaramakrishnan Rajaraman, Prasanth Ganesan, Sameer Antani
- **Comment**: 34 pages, 21 figures, and 10 tables
- **Journal**: None
- **Summary**: In medical image classification tasks, it is common to find that the number of normal samples far exceeds the number of abnormal samples. In such class-imbalanced situations, reliable training of deep neural networks continues to be a major challenge. Under these circumstances, the predicted class probabilities may be biased toward the majority class. Calibration has been suggested to alleviate some of these effects. However, there is insufficient analysis explaining when and whether calibrating a model would be beneficial in improving performance. In this study, we perform a systematic analysis of the effect of model calibration on its performance on two medical image modalities, namely, chest X-rays and fundus images, using various deep learning classifier backbones. For this, we study the following variations: (i) the degree of imbalances in the dataset used for training; (ii) calibration methods; and (iii) two classification thresholds, namely, default decision threshold of 0.5, and optimal threshold from precision-recall curves. Our results indicate that at the default operating threshold of 0.5, the performance achieved through calibration is significantly superior (p < 0.05) to using uncalibrated probabilities. However, at the PR-guided threshold, these gains are not significantly different (p > 0.05). This finding holds for both image modalities and at varying degrees of imbalance.



### Infrared Small-Dim Target Detection with Transformer under Complex Backgrounds
- **Arxiv ID**: http://arxiv.org/abs/2109.14379v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14379v2)
- **Published**: 2021-09-29 12:23:41+00:00
- **Updated**: 2021-11-11 07:15:10+00:00
- **Authors**: Fangcen Liu, Chenqiang Gao, Fang Chen, Deyu Meng, Wangmeng Zuo, Xinbo Gao
- **Comment**: None
- **Journal**: None
- **Summary**: The infrared small-dim target detection is one of the key techniques in the infrared search and tracking system. Since the local regions similar to infrared small-dim targets spread over the whole background, exploring the interaction information amongst image features in large-range dependencies to mine the difference between the target and background is crucial for robust detection. However, existing deep learning-based methods are limited by the locality of convolutional neural networks, which impairs the ability to capture large-range dependencies. Additionally, the small-dim appearance of the infrared target makes the detection model highly possible to miss detection. To this end, we propose a robust and general infrared small-dim target detection method with the transformer. We adopt the self-attention mechanism of the transformer to learn the interaction information of image features in a larger range. Moreover, we design a feature enhancement module to learn discriminative features of small-dim targets to avoid miss detection. After that, to avoid the loss of the target information, we adopt a decoder with the U-Net-like skip connection operation to contain more information of small-dim targets. Finally, we get the detection result by a segmentation head. Extensive experiments on two public datasets show the obvious superiority of the proposed method over state-of-the-art methods and the proposed method has stronger cross-scene generalization and anti-noise performance.



### UFO-ViT: High Performance Linear Vision Transformer without Softmax
- **Arxiv ID**: http://arxiv.org/abs/2109.14382v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14382v2)
- **Published**: 2021-09-29 12:32:49+00:00
- **Updated**: 2021-11-04 17:08:43+00:00
- **Authors**: Jeong-geun Song
- **Comment**: None
- **Journal**: None
- **Summary**: Vision transformers have become one of the most important models for computer vision tasks. Although they outperform prior works, they require heavy computational resources on a scale that is quadratic to $N$. This is a major drawback of the traditional self-attention (SA) algorithm. Here, we propose the Unit Force Operated Vision Transformer (UFO-ViT), a novel SA mechanism that has linear complexity. The main approach of this work is to eliminate nonlinearity from the original SA. We factorize the matrix multiplication of the SA mechanism without complicated linear approximation. By modifying only a few lines of code from the original SA, the proposed models outperform most transformer-based models on image classification and dense prediction tasks on most capacity regimes.



### Neural Knitworks: Patched Neural Implicit Representation Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.14406v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2109.14406v1)
- **Published**: 2021-09-29 13:10:46+00:00
- **Updated**: 2021-09-29 13:10:46+00:00
- **Authors**: Mikolaj Czerkawski, Javier Cardona, Robert Atkinson, Craig Michie, Ivan Andonovic, Carmine Clemente, Christos Tachtatzis
- **Comment**: 11 pages, 9 figures, Rejected from NeurIPS
- **Journal**: None
- **Summary**: Coordinate-based Multilayer Perceptron (MLP) networks, despite being capable of learning neural implicit representations, are not performant for internal image synthesis applications. Convolutional Neural Networks (CNNs) are typically used instead for a variety of internal generative tasks, at the cost of a larger model. We propose Neural Knitwork, an architecture for neural implicit representation learning of natural images that achieves image synthesis by optimizing the distribution of image patches in an adversarial manner and by enforcing consistency between the patch predictions. To the best of our knowledge, this is the first implementation of a coordinate-based MLP tailored for synthesis tasks such as image inpainting, super-resolution, and denoising. We demonstrate the utility of the proposed technique by training on these three tasks. The results show that modeling natural images using patches, rather than pixels, produces results of higher fidelity. The resulting model requires 80% fewer parameters than alternative CNN-based solutions while achieving comparable performance and training time.



### Multi-loss ensemble deep learning for chest X-ray classification
- **Arxiv ID**: http://arxiv.org/abs/2109.14433v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68T07, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2109.14433v3)
- **Published**: 2021-09-29 14:14:04+00:00
- **Updated**: 2021-11-14 14:10:59+00:00
- **Authors**: Sivaramakrishnan Rajaraman, Ghada Zamzmi, Sameer Antani
- **Comment**: 27 pages, 6 figures, 5 tables
- **Journal**: None
- **Summary**: Medical images commonly exhibit multiple abnormalities. Predicting them requires multi-class classifiers whose training and desired reliable performance can be affected by a combination of factors, such as, dataset size, data source, distribution, and the loss function used to train the deep neural networks. Currently, the cross-entropy loss remains the de-facto loss function for training deep learning classifiers. This loss function, however, asserts equal learning from all classes, leading to a bias toward the majority class. In this work, we benchmark various state-of-the-art loss functions that are suitable for multi-class classification, critically analyze model performance, and propose improved loss functions. We select a pediatric chest X-ray (CXR) dataset that includes images with no abnormality (normal), and those exhibiting manifestations consistent with bacterial and viral pneumonia. We construct prediction-level and model-level ensembles, respectively, to improve classification performance. Our results show that compared to the individual models and the state-of-the-art literature, the weighted averaging of the predictions for top-3 and top-5 model-level ensembles delivered significantly superior classification performance (p < 0.05) in terms of MCC (0.9068, 95% confidence interval (0.8839, 0.9297)) metric. Finally, we performed localization studies to interpret model behaviors to visualize and confirm that the individual models and ensembles learned meaningful features and highlighted disease manifestations.



### One Loss for All: Deep Hashing with a Single Cosine Similarity based Learning Objective
- **Arxiv ID**: http://arxiv.org/abs/2109.14449v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.14449v1)
- **Published**: 2021-09-29 14:27:51+00:00
- **Updated**: 2021-09-29 14:27:51+00:00
- **Authors**: Jiun Tian Hoe, Kam Woh Ng, Tianyu Zhang, Chee Seng Chan, Yi-Zhe Song, Tao Xiang
- **Comment**: Accepted at NeurIPS 2021
- **Journal**: None
- **Summary**: A deep hashing model typically has two main learning objectives: to make the learned binary hash codes discriminative and to minimize a quantization error. With further constraints such as bit balance and code orthogonality, it is not uncommon for existing models to employ a large number (>4) of losses. This leads to difficulties in model training and subsequently impedes their effectiveness. In this work, we propose a novel deep hashing model with only a single learning objective. Specifically, we show that maximizing the cosine similarity between the continuous codes and their corresponding binary orthogonal codes can ensure both hash code discriminativeness and quantization error minimization. Further, with this learning objective, code balancing can be achieved by simply using a Batch Normalization (BN) layer and multi-label classification is also straightforward with label smoothing. The result is an one-loss deep hashing model that removes all the hassles of tuning the weights of various losses. Importantly, extensive experiments show that our model is highly effective, outperforming the state-of-the-art multi-loss hashing models on three large-scale instance retrieval benchmarks, often by significant margins. Code is available at https://github.com/kamwoh/orthohash



### Programmable Spectral Filter Arrays using Phase Spatial Light Modulator
- **Arxiv ID**: http://arxiv.org/abs/2109.14450v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2109.14450v2)
- **Published**: 2021-09-29 14:30:55+00:00
- **Updated**: 2022-12-12 00:30:00+00:00
- **Authors**: Vishwanath Saragadam, Vijay Rengarajan, Ryuichi Tadano, Tuo Zhuang, Hideki Oyaizu, Jun Murayama, Aswin C. Sankaranarayanan
- **Comment**: None
- **Journal**: None
- **Summary**: Spatially varying spectral modulation can be implemented using a liquid crystal spatial light modulator (SLM) since it provides an array of liquid crystal cells, each of which can be purposed to act as a programmable spectral filter array. However, such an optical setup suffers from strong optical aberrations due to the unintended phase modulation, precluding spectral modulation at high spatial resolutions. In this work, we propose a novel computational approach for the practical implementation of phase SLMs for implementing spatially varying spectral filters. We provide a careful and systematic analysis of the aberrations arising out of phase SLMs for the purposes of spatially varying spectral modulation. The analysis naturally leads us to a set of "good patterns" that minimize the optical aberrations. We then train a deep network that overcomes any residual aberrations, thereby achieving ideal spectral modulation at high spatial resolution. We show a number of unique operating points with our prototype including dynamic spectral filtering, material classification, and single- and multi-image hyperspectral imaging.



### CCTrans: Simplifying and Improving Crowd Counting with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2109.14483v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14483v1)
- **Published**: 2021-09-29 15:13:10+00:00
- **Updated**: 2021-09-29 15:13:10+00:00
- **Authors**: Ye Tian, Xiangxiang Chu, Hongpeng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Most recent methods used for crowd counting are based on the convolutional neural network (CNN), which has a strong ability to extract local features. But CNN inherently fails in modeling the global context due to the limited receptive fields. However, the transformer can model the global context easily. In this paper, we propose a simple approach called CCTrans to simplify the design pipeline. Specifically, we utilize a pyramid vision transformer backbone to capture the global crowd information, a pyramid feature aggregation (PFA) model to combine low-level and high-level features, an efficient regression head with multi-scale dilated convolution (MDC) to predict density maps. Besides, we tailor the loss functions for our pipeline. Without bells and whistles, extensive experiments demonstrate that our method achieves new state-of-the-art results on several benchmarks both in weakly and fully-supervised crowd counting. Moreover, we currently rank No.1 on the leaderboard of NWPU-Crowd. Our code will be made available.



### Generative Probabilistic Image Colorization
- **Arxiv ID**: http://arxiv.org/abs/2109.14518v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14518v1)
- **Published**: 2021-09-29 16:10:12+00:00
- **Updated**: 2021-09-29 16:10:12+00:00
- **Authors**: Chie Furusawa, Shinya Kitaoka, Michael Li, Yuri Odagiri
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: We propose Generative Probabilistic Image Colorization, a diffusion-based generative process that trains a sequence of probabilistic models to reverse each step of noise corruption. Given a line-drawing image as input, our method suggests multiple candidate colorized images. Therefore, our method accounts for the ill-posed nature of the colorization problem. We conducted comprehensive experiments investigating the colorization of line-drawing images, report the influence of a score-based MCMC approach that corrects the marginal distribution of estimated samples, and further compare different combinations of models and the similarity of their generated images. Despite using only a relatively small training dataset, we experimentally develop a method to generate multiple diverse colorization candidates which avoids mode collapse and does not require any additional constraints, losses, or re-training with alternative training conditions. Our proposed approach performed well not only on color-conditional image generation tasks using biased initial values, but also on some practical image completion and inpainting tasks.



### DRAN: Detailed Region-Adaptive Normalization for Conditional Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2109.14525v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14525v4)
- **Published**: 2021-09-29 16:19:37+00:00
- **Updated**: 2023-06-26 11:16:21+00:00
- **Authors**: Yueming Lyu, Peibin Chen, Jingna Sun, Bo Peng, Xu Wang, Jing Dong
- **Comment**: Accepted by TMM 2023
- **Journal**: None
- **Summary**: In recent years, conditional image synthesis has attracted growing attention due to its controllability in the image generation process. Although recent works have achieved realistic results, most of them have difficulty handling fine-grained styles with subtle details. To address this problem, a novel normalization module, named Detailed Region-Adaptive Normalization~(DRAN), is proposed. It adaptively learns both fine-grained and coarse-grained style representations. Specifically, we first introduce a multi-level structure, Spatiality-aware Pyramid Pooling, to guide the model to learn coarse-to-fine features. Then, to adaptively fuse different levels of styles, we propose Dynamic Gating, making it possible to adaptively fuse different levels of styles according to different spatial regions. Finally, we collect a new makeup dataset (Makeup-Complex dataset) that contains a wide range of complex makeup styles with diverse poses and expressions. To evaluate the effectiveness and show the general use of our method, we conduct a set of experiments on makeup transfer and semantic image synthesis. Quantitative and qualitative experiments show that equipped with DRAN, simple baseline models are able to achieve promising improvements in complex style transfer and detailed texture synthesis. Both the code and the proposed dataset will be available at https://github.com/Yueming6568/DRAN-makeup.git.



### Vision-Guided Quadrupedal Locomotion in the Wild with Multi-Modal Delay Randomization
- **Arxiv ID**: http://arxiv.org/abs/2109.14549v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.14549v2)
- **Published**: 2021-09-29 16:48:05+00:00
- **Updated**: 2022-07-24 01:55:19+00:00
- **Authors**: Chieko Sarah Imai, Minghao Zhang, Yuchen Zhang, Marcin Kierebinski, Ruihan Yang, Yuzhe Qin, Xiaolong Wang
- **Comment**: IROS 2022, Project page: https://mehooz.github.io/mmdr-wild/
- **Journal**: None
- **Summary**: Developing robust vision-guided controllers for quadrupedal robots in complex environments, with various obstacles, dynamical surroundings and uneven terrains, is very challenging. While Reinforcement Learning (RL) provides a promising paradigm for agile locomotion skills with vision inputs in simulation, it is still very challenging to deploy the RL policy in the real world. Our key insight is that aside from the discrepancy in the domain gap, in visual appearance between the simulation and the real world, the latency from the control pipeline is also a major cause of difficulty. In this paper, we propose Multi-Modal Delay Randomization (MMDR) to address this issue when training RL agents. Specifically, we simulate the latency of real hardware by using past observations, sampled with randomized periods, for both proprioception and vision. We train the RL policy for end-to-end control in a physical simulator without any predefined controller or reference motion, and directly deploy it on the real A1 quadruped robot running in the wild. We evaluate our method in different outdoor environments with complex terrains and obstacles. We demonstrate the robot can smoothly maneuver at a high speed, avoid the obstacles, and show significant improvement over the baselines. Our project page with videos is at https://mehooz.github.io/mmdr-wild/.



### Robust Temporal Ensembling for Learning with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2109.14563v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.14563v1)
- **Published**: 2021-09-29 16:59:36+00:00
- **Updated**: 2021-09-29 16:59:36+00:00
- **Authors**: Abel Brown, Benedikt Schifferer, Robert DiPietro
- **Comment**: includes additional baselines and hyperparam references
- **Journal**: None
- **Summary**: Successful training of deep neural networks with noisy labels is an essential capability as most real-world datasets contain some amount of mislabeled data. Left unmitigated, label noise can sharply degrade typical supervised learning approaches. In this paper, we present robust temporal ensembling (RTE), which combines robust loss with semi-supervised regularization methods to achieve noise-robust learning. We demonstrate that RTE achieves state-of-the-art performance across the CIFAR-10, CIFAR-100, ImageNet, WebVision, and Food-101N datasets, while forgoing the recent trend of label filtering and/or fixing. Finally, we show that RTE also retains competitive corruption robustness to unforeseen input noise using CIFAR-10-C, obtaining a mean corruption error (mCE) of 13.50% even in the presence of an 80% noise ratio, versus 26.9% mCE with standard methods on clean data.



### Towards Flexible Blind JPEG Artifacts Removal
- **Arxiv ID**: http://arxiv.org/abs/2109.14573v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 94A08, I.2; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2109.14573v1)
- **Published**: 2021-09-29 17:12:10+00:00
- **Updated**: 2021-09-29 17:12:10+00:00
- **Authors**: Jiaxi Jiang, Kai Zhang, Radu Timofte
- **Comment**: Accepted by ICCV 2021, Code: https://github.com/jiaxi-jiang/FBCNN
- **Journal**: None
- **Summary**: Training a single deep blind model to handle different quality factors for JPEG image artifacts removal has been attracting considerable attention due to its convenience for practical usage. However, existing deep blind methods usually directly reconstruct the image without predicting the quality factor, thus lacking the flexibility to control the output as the non-blind methods. To remedy this problem, in this paper, we propose a flexible blind convolutional neural network, namely FBCNN, that can predict the adjustable quality factor to control the trade-off between artifacts removal and details preservation. Specifically, FBCNN decouples the quality factor from the JPEG image via a decoupler module and then embeds the predicted quality factor into the subsequent reconstructor module through a quality factor attention block for flexible control. Besides, we find existing methods are prone to fail on non-aligned double JPEG images even with only a one-pixel shift, and we thus propose a double JPEG degradation model to augment the training data. Extensive experiments on single JPEG images, more general double JPEG images, and real-world JPEG images demonstrate that our proposed FBCNN achieves favorable performance against state-of-the-art methods in terms of both quantitative metrics and visual quality.



### FathomNet: A global image database for enabling artificial intelligence in the ocean
- **Arxiv ID**: http://arxiv.org/abs/2109.14646v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14646v4)
- **Published**: 2021-09-29 18:08:42+00:00
- **Updated**: 2022-09-07 19:46:22+00:00
- **Authors**: Kakani Katija, Eric Orenstein, Brian Schlining, Lonny Lundsten, Kevin Barnard, Giovanna Sainz, Oceane Boulais, Megan Cromwell, Erin Butler, Benjamin Woodward, Katy Croff Bell
- **Comment**: None
- **Journal**: None
- **Summary**: The ocean is experiencing unprecedented rapid change, and visually monitoring marine biota at the spatiotemporal scales needed for responsible stewardship is a formidable task. As baselines are sought by the research community, the volume and rate of this required data collection rapidly outpaces our abilities to process and analyze them. Recent advances in machine learning enables fast, sophisticated analysis of visual data, but have had limited success in the ocean due to lack of data standardization, insufficient formatting, and demand for large, labeled datasets. To address this need, we built FathomNet, an open-source image database that standardizes and aggregates expertly curated labeled data. FathomNet has been seeded with existing iconic and non-iconic imagery of marine animals, underwater equipment, debris, and other concepts, and allows for future contributions from distributed data sources. We demonstrate how FathomNet data can be used to train and deploy models on other institutional video to reduce annotation effort, and enable automated tracking of underwater concepts when integrated with robotic vehicles. As FathomNet continues to grow and incorporate more labeled data from the community, we can accelerate the processing of visual data to achieve a healthy and sustainable global ocean.



### Uncertainty-aware Mean Teacher for Source-free Unsupervised Domain Adaptive 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.14651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14651v1)
- **Published**: 2021-09-29 18:17:09+00:00
- **Updated**: 2021-09-29 18:17:09+00:00
- **Authors**: Deepti Hegde, Vishwanath Sindagi, Velat Kilic, A. Brinton Cooper, Mark Foster, Vishal Patel
- **Comment**: None
- **Journal**: None
- **Summary**: Pseudo-label based self training approaches are a popular method for source-free unsupervised domain adaptation. However, their efficacy depends on the quality of the labels generated by the source trained model. These labels may be incorrect with high confidence, rendering thresholding methods ineffective. In order to avoid reinforcing errors caused by label noise, we propose an uncertainty-aware mean teacher framework which implicitly filters incorrect pseudo-labels during training. Leveraging model uncertainty allows the mean teacher network to perform implicit filtering by down-weighing losses corresponding uncertain pseudo-labels. Effectively, we perform automatic soft-sampling of pseudo-labeled data while aligning predictions from the student and teacher networks. We demonstrate our method on several domain adaptation scenarios, from cross-dataset to cross-weather conditions, and achieve state-of-the-art performance in these cases, on the KITTI lidar target dataset.



### Understanding Egocentric Hand-Object Interactions from Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2109.14657v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2109.14657v1)
- **Published**: 2021-09-29 18:34:06+00:00
- **Updated**: 2021-09-29 18:34:06+00:00
- **Authors**: Yao Lu, Walterio W. Mayol-Cuevas
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of estimating the hand pose from the egocentric view when the hand is interacting with objects. Specifically, we propose a method to label a dataset Ego-Siam which contains the egocentric images pair-wisely. We also use the collected pairwise data to train our encoder-decoder style network which has been proven efficient in. This could bring extra training efficiency and testing accuracy. Our network is lightweight and can be performed with over 30 FPS with an outdated GPU. We demonstrate that our method outperforms Mueller et al. which is the state of the art work dealing with egocentric hand-object interaction problems on the GANerated dataset. To show the ability to preserve the semantic information of our method, we also report the performance of grasp type classification on GUN-71 dataset and outperforms the benchmark by only using the predicted 3-d hand pose.



### Segmentation of Roads in Satellite Images using specially modified U-Net CNNs
- **Arxiv ID**: http://arxiv.org/abs/2109.14671v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14671v1)
- **Published**: 2021-09-29 19:08:32+00:00
- **Updated**: 2021-09-29 19:08:32+00:00
- **Authors**: Jonas Bokstaller, Yihang She, Zhehan Fu, Tommaso Macrì
- **Comment**: 4 pages, 4 figures
- **Journal**: None
- **Summary**: The image classification problem has been deeply investigated by the research community, with computer vision algorithms and with the help of Neural Networks. The aim of this paper is to build an image classifier for satellite images of urban scenes that identifies the portions of the images in which a road is located, separating these portions from the rest. Unlike conventional computer vision algorithms, convolutional neural networks (CNNs) provide accurate and reliable results on this task. Our novel approach uses a sliding window to extract patches out of the whole image, data augmentation for generating more training/testing data and lastly a series of specially modified U-Net CNNs. This proposed technique outperforms all other baselines tested in terms of mean F-score metric.



### Automatic Estimation of Ulcerative Colitis Severity from Endoscopy Videos using Ordinal Multi-Instance Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.14685v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.14685v1)
- **Published**: 2021-09-29 19:42:51+00:00
- **Updated**: 2021-09-29 19:42:51+00:00
- **Authors**: Evan Schwab, Gabriela Oana Cula, Kristopher Standish, Stephen S. F. Yip, Aleksandar Stojmirovic, Louis Ghanem, Christel Chehoud
- **Comment**: *Co-Senior Authors. Accepted for publication in the Special Issue
  Journal of Computer Methods in Biomechanics and Biomedical Engineering:
  Imaging & Visualization and presented at the MICCAI 2021 AE-CAI $|$ CARE $|$
  OR 2.0 Workshop
- **Journal**: None
- **Summary**: Ulcerative colitis (UC) is a chronic inflammatory bowel disease characterized by relapsing inflammation of the large intestine. The severity of UC is often represented by the Mayo Endoscopic Subscore (MES) which quantifies mucosal disease activity from endoscopy videos. In clinical trials, an endoscopy video is assigned an MES based upon the most severe disease activity observed in the video. For this reason, severe inflammation spread throughout the colon will receive the same MES as an otherwise healthy colon with severe inflammation restricted to a small, localized segment. Therefore, the extent of disease activity throughout the large intestine, and overall response to treatment, may not be completely captured by the MES. In this work, we aim to automatically estimate UC severity for each frame in an endoscopy video to provide a higher resolution assessment of disease activity throughout the colon. Because annotating severity at the frame-level is expensive, labor-intensive, and highly subjective, we propose a novel weakly supervised, ordinal classification method to estimate frame severity from video MES labels alone. Using clinical trial data, we first achieved 0.92 and 0.90 AUC for predicting mucosal healing and remission of UC, respectively. Then, for severity estimation, we demonstrate that our models achieve substantial Cohen's Kappa agreement with ground truth MES labels, comparable to the inter-rater agreement of expert clinicians. These findings indicate that our framework could serve as a foundation for novel clinical endpoints, based on a more localized scoring system, to better evaluate UC drug efficacy in clinical trials.



### Vision-Aided Beam Tracking: Explore the Proper Use of Camera Images with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.14686v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14686v1)
- **Published**: 2021-09-29 19:47:01+00:00
- **Updated**: 2021-09-29 19:47:01+00:00
- **Authors**: Yu Tian, Chenwei Wang
- **Comment**: 5 pages, 4 tables, to appear in IEEE VTC 2021-fall
- **Journal**: None
- **Summary**: We investigate the problem of wireless beam tracking on mmWave bands with the assistance of camera images. In particular, based on the user's beam indices used and camera images taken in the trajectory, we predict the optimal beam indices in the next few time spots. To resolve this problem, we first reformulate the "ViWi" dataset in [1] to get rid of the image repetition problem. Then we develop a deep learning approach and investigate various model components to achieve the best performance. Finally, we explore whether, when, and how to use the image for better beam prediction. To answer this question, we split the dataset into three clusters -- (LOS, light NLOS, serious NLOS)-like -- based on the standard deviation of the beam sequence. With experiments we demonstrate that using the image indeed helps beam tracking especially when the user is in serious NLOS, and the solution relies on carefully-designed dataset for training a model. Generally speaking, including NLOS-like data for training a model does not benefit beam tracking of the user in LOS, but including light NLOS-like data for training a model benefits beam tracking of the user in serious NLOS.



### LR-to-HR Face Hallucination with an Adversarial Progressive Attribute-Induced Network
- **Arxiv ID**: http://arxiv.org/abs/2109.14690v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14690v1)
- **Published**: 2021-09-29 19:50:45+00:00
- **Updated**: 2021-09-29 19:50:45+00:00
- **Authors**: Nitin Balachandran, Jun-Cheng Chen, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Face super-resolution is a challenging and highly ill-posed problem since a low-resolution (LR) face image may correspond to multiple high-resolution (HR) ones during the hallucination process and cause a dramatic identity change for the final super-resolved results. Thus, to address this problem, we propose an end-to-end progressive learning framework incorporating facial attributes and enforcing additional supervision from multi-scale discriminators. By incorporating facial attributes into the learning process and progressively resolving the facial image, the mapping between LR and HR images is constrained more, and this significantly helps to reduce the ambiguity and uncertainty in one-to-many mapping. In addition, we conduct thorough evaluations on the CelebA dataset following the settings of previous works (i.e. super-resolving by a factor of 8x from tiny 16x16 face images.), and the results demonstrate that the proposed approach can yield satisfactory face hallucination images outperforming other state-of-the-art approaches.



### Convolutional Neural Network Compression through Generalized Kronecker Product Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2109.14710v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.14710v2)
- **Published**: 2021-09-29 20:45:08+00:00
- **Updated**: 2022-01-14 15:16:06+00:00
- **Authors**: Marawan Gamal Abdel Hameed, Marzieh S. Tahaei, Ali Mosleh, Vahid Partovi Nia
- **Comment**: None
- **Journal**: None
- **Summary**: Modern Convolutional Neural Network (CNN) architectures, despite their superiority in solving various problems, are generally too large to be deployed on resource constrained edge devices. In this paper, we reduce memory usage and floating-point operations required by convolutional layers in CNNs. We compress these layers by generalizing the Kronecker Product Decomposition to apply to multidimensional tensors, leading to the Generalized Kronecker Product Decomposition (GKPD). Our approach yields a plug-and-play module that can be used as a drop-in replacement for any convolutional layer. Experimental results for image classification on CIFAR-10 and ImageNet datasets using ResNet, MobileNetv2 and SeNet architectures substantiate the effectiveness of our proposed approach. We find that GKPD outperforms state-of-the-art decomposition methods including Tensor-Train and Tensor-Ring as well as other relevant compression methods such as pruning and knowledge distillation.



### USIS: Unsupervised Semantic Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2109.14715v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14715v1)
- **Published**: 2021-09-29 20:48:41+00:00
- **Updated**: 2021-09-29 20:48:41+00:00
- **Authors**: George Eskandar, Mohamed Abdelsamad, Karim Armanious, Bin Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic Image Synthesis (SIS) is a subclass of image-to-image translation where a photorealistic image is synthesized from a segmentation mask. SIS has mostly been addressed as a supervised problem. However, state-of-the-art methods depend on a huge amount of labeled data and cannot be applied in an unpaired setting. On the other hand, generic unpaired image-to-image translation frameworks underperform in comparison, because they color-code semantic layouts and feed them to traditional convolutional networks, which then learn correspondences in appearance instead of semantic content. In this initial work, we propose a new Unsupervised paradigm for Semantic Image Synthesis (USIS) as a first step towards closing the performance gap between paired and unpaired settings. Notably, the framework deploys a SPADE generator that learns to output images with visually separable semantic classes using a self-supervised segmentation loss. Furthermore, in order to match the color and texture distribution of real images without losing high-frequency information, we propose to use whole image wavelet-based discrimination. We test our methodology on 3 challenging datasets and demonstrate its ability to generate multimodal photorealistic images with an improved quality in the unpaired setting.



### Targeted Gradient Descent: A Novel Method for Convolutional Neural Networks Fine-tuning and Online-learning
- **Arxiv ID**: http://arxiv.org/abs/2109.14729v1
- **DOI**: 10.1007/978-3-030-87199-4_3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14729v1)
- **Published**: 2021-09-29 21:22:09+00:00
- **Updated**: 2021-09-29 21:22:09+00:00
- **Authors**: Junyu Chen, Evren Asma, Chung Chan
- **Comment**: MICCAI 2021 (Oral Presentation). Proceedings: LNCS 12903, pp 25-35
- **Journal**: None
- **Summary**: A convolutional neural network (ConvNet) is usually trained and then tested using images drawn from the same distribution. To generalize a ConvNet to various tasks often requires a complete training dataset that consists of images drawn from different tasks. In most scenarios, it is nearly impossible to collect every possible representative dataset as a priori. The new data may only become available after the ConvNet is deployed in clinical practice. ConvNet, however, may generate artifacts on out-of-distribution testing samples. In this study, we present Targeted Gradient Descent (TGD), a novel fine-tuning method that can extend a pre-trained network to a new task without revisiting data from the previous task while preserving the knowledge acquired from previous training. To a further extent, the proposed method also enables online learning of patient-specific data. The method is built on the idea of reusing a pre-trained ConvNet's redundant kernels to learn new knowledge. We compare the performance of TGD to several commonly used training approaches on the task of Positron emission tomography (PET) image denoising. Results from clinical images show that TGD generated results on par with training-from-scratch while significantly reducing data preparation and network training time. More importantly, it enables online learning on the testing study to enhance the network's generalization capability in real-world applications.



### Egocentric Hand-object Interaction Detection and Application
- **Arxiv ID**: http://arxiv.org/abs/2109.14734v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14734v1)
- **Published**: 2021-09-29 21:47:16+00:00
- **Updated**: 2021-09-29 21:47:16+00:00
- **Authors**: Yao Lu, Walterio W. Mayol-Cuevas
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a method to detect the hand-object interaction from an egocentric perspective. In contrast to massive data-driven discriminator based method like \cite{Shan20}, we propose a novel workflow that utilises the cues of hand and object. Specifically, we train networks predicting hand pose, hand mask and in-hand object mask to jointly predict the hand-object interaction status. We compare our method with the most recent work from Shan et al. \cite{Shan20} on selected images from EPIC-KITCHENS \cite{damen2018scaling} dataset and achieve $89\%$ accuracy on HOI (hand-object interaction) detection which is comparative to Shan's ($92\%$). However, for real-time performance, with the same machine, our method can run over $\textbf{30}$ FPS which is much efficient than Shan's ($\textbf{1}\sim\textbf{2}$ FPS). Furthermore, with our approach, we are able to segment script-less activities from where we extract the frames with the HOI status detection. We achieve $\textbf{68.2\%}$ and $\textbf{82.8\%}$ F1 score on GTEA \cite{fathi2011learning} and the UTGrasp \cite{cai2015scalable} dataset respectively which are all comparative to the SOTA methods.



### Unlocking the potential of deep learning for marine ecology: overview, applications, and outlook
- **Arxiv ID**: http://arxiv.org/abs/2109.14737v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14737v1)
- **Published**: 2021-09-29 21:59:16+00:00
- **Updated**: 2021-09-29 21:59:16+00:00
- **Authors**: Morten Goodwin, Kim Tallaksen Halvorsen, Lei Jiao, Kristian Muri Knausgård, Angela Helen Martin, Marta Moyano, Rebekah A. Oomen, Jeppe Have Rasmussen, Tonje Knutsen Sørdalen, Susanna Huneide Thorbjørnsen
- **Comment**: 44 pages, 4 figures
- **Journal**: None
- **Summary**: The deep learning revolution is touching all scientific disciplines and corners of our lives as a means of harnessing the power of big data. Marine ecology is no exception. These new methods provide analysis of data from sensors, cameras, and acoustic recorders, even in real time, in ways that are reproducible and rapid. Off-the-shelf algorithms can find, count, and classify species from digital images or video and detect cryptic patterns in noisy data. Using these opportunities requires collaboration across ecological and data science disciplines, which can be challenging to initiate. To facilitate these collaborations and promote the use of deep learning towards ecosystem-based management of the sea, this paper aims to bridge the gap between marine ecologists and computer scientists. We provide insight into popular deep learning approaches for ecological data analysis in plain language, focusing on the techniques of supervised learning with deep neural networks, and illustrate challenges and opportunities through established and emerging applications of deep learning to marine ecology. We use established and future-looking case studies on plankton, fishes, marine mammals, pollution, and nutrient cycling that involve object detection, classification, tracking, and segmentation of visualized data. We conclude with a broad outlook of the field's opportunities and challenges, including potential technological advances and issues with managing complex data sets.



### The Object at Hand: Automated Editing for Mixed Reality Video Guidance from Hand-Object Interactions
- **Arxiv ID**: http://arxiv.org/abs/2109.14744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14744v1)
- **Published**: 2021-09-29 22:24:25+00:00
- **Updated**: 2021-09-29 22:24:25+00:00
- **Authors**: Yao Lu, Walterio W. Mayol-Cuevas
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we concern with the problem of how to automatically extract the steps that compose real-life hand activities. This is a key competence towards processing, monitoring and providing video guidance in Mixed Reality systems. We use egocentric vision to observe hand-object interactions in real-world tasks and automatically decompose a video into its constituent steps. Our approach combines hand-object interaction (HOI) detection, object similarity measurement and a finite state machine (FSM) representation to automatically edit videos into steps. We use a combination of Convolutional Neural Networks (CNNs) and the FSM to discover, edit cuts and merge segments while observing real hand activities. We evaluate quantitatively and qualitatively our algorithm on two datasets: the GTEA\cite{li2015delving}, and a new dataset we introduce for Chinese Tea making. Results show our method is able to segment hand-object interaction videos into key step segments with high levels of precision.



### Improvising the Learning of Neural Networks on Hyperspherical Manifold
- **Arxiv ID**: http://arxiv.org/abs/2109.14746v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.14746v2)
- **Published**: 2021-09-29 22:39:07+00:00
- **Updated**: 2021-11-06 18:42:22+00:00
- **Authors**: Lalith Bharadwaj Baru, Sai Vardhan Kanumolu, Akshay Patel Shilhora, Madhu G
- **Comment**: LMRL Workshop at NeurIPS 2021
- **Journal**: None
- **Summary**: The impact of convolution neural networks (CNNs) in the supervised settings provided tremendous increment in performance. The representations learned from CNN's operated on hyperspherical manifold led to insightful outcomes in face recognition, face identification, and other supervised tasks. A broad range of activation functions were developed with hypersphere intuition which performs superior to softmax in euclidean space. The main motive of this research is to provide insights. First, the stereographic projection is implied to transform data from Euclidean space ($\mathbb{R}^{n}$) to hyperspherical manifold ($\mathbb{S}^{n}$) to analyze the performance of angular margin losses. Secondly, proving theoretically and practically that decision boundaries constructed on hypersphere using stereographic projection obliges the learning of neural networks. Experiments have demonstrated that applying stereographic projection on existing state-of-the-art angular margin objective functions improved performance for standard image classification data sets (CIFAR-10,100). Further, we ran our experiments on malaria-thin blood smear images, resulting in effective outcomes. The code is publicly available at:https://github.com/barulalithb/stereo-angular-margin.



### MetaHistoSeg: A Python Framework for Meta Learning in Histopathology Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.14754v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14754v1)
- **Published**: 2021-09-29 23:05:04+00:00
- **Updated**: 2021-09-29 23:05:04+00:00
- **Authors**: Zheng Yuan, Andre Esteva, Ran Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning is a standard practice in most deep learning based histopathology image segmentation, given the relatively low number of digitized slides that are generally available. While many models have been developed for domain specific histopathology image segmentation, cross-domain generalization remains a key challenge for properly validating models. Here, tooling and datasets to benchmark model performance across histopathological domains are lacking. To address this limitation, we introduce MetaHistoSeg - a Python framework that implements unique scenarios in both meta learning and instance based transfer learning. Designed for easy extension to customized datasets and task sampling schemes, the framework empowers researchers with the ability of rapid model design and experimentation. We also curate a histopathology meta dataset - a benchmark dataset for training and validating models on out-of-distribution performance across a range of cancer types. In experiments we showcase the usage of MetaHistoSeg with the meta dataset and find that both meta-learning and instance based transfer learning deliver comparable results on average, but in some cases tasks can greatly benefit from one over the other.



### Chest X-Rays Image Classification from beta-Variational Autoencoders Latent Features
- **Arxiv ID**: http://arxiv.org/abs/2109.14760v1
- **DOI**: 10.1109/SSCI50451.2021.9660190
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.14760v1)
- **Published**: 2021-09-29 23:28:09+00:00
- **Updated**: 2021-09-29 23:28:09+00:00
- **Authors**: Leonardo Crespi, Daniele Loiacono, Arturo Chiti
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Chest X-Ray (CXR) is one of the most common diagnostic techniques used in everyday clinical practice all around the world. We hereby present a work which intends to investigate and analyse the use of Deep Learning (DL) techniques to extract information from such images and allow to classify them, trying to keep our methodology as general as possible and possibly also usable in a real world scenario without much effort, in the future. To move in this direction, we trained several beta-Variational Autoencoder (beta-VAE) models on the CheXpert dataset, one of the largest publicly available collection of labeled CXR images; from these models, latent features have been extracted and used to train other Machine Learning models, able to classify the original images from the features extracted by the beta-VAE. Lastly, tree-based models have been combined together in ensemblings to improve the results without the necessity of further training or models engineering. Expecting some drop in pure performance with the respect to state of the art classification specific models, we obtained encouraging results, which show the viability of our approach and the usability of the high level features extracted by the autoencoders for classification tasks.



