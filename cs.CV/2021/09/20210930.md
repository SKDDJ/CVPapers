# Arxiv Papers in cs.CV on 2021-09-30
### A Prior Knowledge Based Tumor and Tumoral Subregion Segmentation Tool for Pediatric Brain Tumors
- **Arxiv ID**: http://arxiv.org/abs/2109.14775v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.14775v1)
- **Published**: 2021-09-30 00:42:54+00:00
- **Updated**: 2021-09-30 00:42:54+00:00
- **Authors**: Silu Zhang, Angela Edwards, Shubo Wang, Zoltan Patay, Asim Bag, Matthew A. Scoggins
- **Comment**: None
- **Journal**: None
- **Summary**: In the past few years, deep learning (DL) models have drawn great attention and shown superior performance on brain tumor and subregion segmentation tasks. However, the success is limited to segmentation of adult gliomas, where sufficient data have been collected, manually labeled, and published for training DL models. It is still challenging to segment pediatric tumors, because the appearances are different from adult gliomas. Hence, directly applying a pretained DL model on pediatric data usually generates unacceptable results. Because pediatric data is very limited, both labeled and unlabeled, we present a brain tumor segmentation model that is based on knowledge rather than learning from data. We also provide segmentation of more subregions for super heterogeneous tumor like atypical teratoid rhabdoid tumor (ATRT). Our proposed approach showed superior performance on both whole tumor and subregion segmentation tasks to DL based models on our pediatric data when training data is not available for transfer learning.



### Automated airway segmentation by learning graphical structure
- **Arxiv ID**: http://arxiv.org/abs/2109.14792v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.14792v1)
- **Published**: 2021-09-30 01:37:31+00:00
- **Updated**: 2021-09-30 01:37:31+00:00
- **Authors**: Yihua Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In this research project, we put forward an advanced method for airway segmentation based on the existent convolutional neural network (CNN) and graph neural network (GNN). The method is originated from the vessel segmentation, but we ameliorate it and enable the novel model to perform better for datasets from computed tomography (CT) scans. Current methods for airway segmentation are considering the regular grid only. No matter what the detailed model is, including the 3-dimensional CNN or 2-dimensional CNN in three directions, the overall graph structures are not taken into consideration. In our model, with the neighbourhoods of airway taken into account, the graph structure is incorporated and the segmentation of airways are improved compared with the traditional CNN methods. We perform experiments on the chest CT scans, where the ground truth segmentation labels are produced manually. The proposed model shows that compared with the CNN-only method, the combination of CNN and GNN has a better performance in that the bronchi in the chest CT scans can be detected in most cases. In addition, the model we propose has a wide extension since the architecture is also utilitarian in fulfilling similar aims in other datasets. Hence, the state-of-the-art model is of great significance and highly applicable in our daily lives.   Keywords: Airway segmentation, Convolutional neural network, Graph neural network



### Unsupervised Landmark Detection Based Spatiotemporal Motion Estimation for 4D Dynamic Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2109.14805v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14805v3)
- **Published**: 2021-09-30 02:06:02+00:00
- **Updated**: 2021-11-08 02:59:47+00:00
- **Authors**: Yuyu Guo, Lei Bi, Dongming Wei, Liyun Chen, Zhengbin Zhu, Dagan Feng, Ruiyan Zhang, Qian Wang, Jinman Kim
- **Comment**: accepted by IEEE Transactions on Cybernetics
- **Journal**: None
- **Summary**: Motion estimation is a fundamental step in dynamic medical image processing for the assessment of target organ anatomy and function. However, existing image-based motion estimation methods, which optimize the motion field by evaluating the local image similarity, are prone to produce implausible estimation, especially in the presence of large motion. In this study, we provide a novel motion estimation framework of Dense-Sparse-Dense (DSD), which comprises two stages. In the first stage, we process the raw dense image to extract sparse landmarks to represent the target organ anatomical topology and discard the redundant information that is unnecessary for motion estimation. For this purpose, we introduce an unsupervised 3D landmark detection network to extract spatially sparse but representative landmarks for the target organ motion estimation. In the second stage, we derive the sparse motion displacement from the extracted sparse landmarks of two images of different time points. Then, we present a motion reconstruction network to construct the motion field by projecting the sparse landmarks displacement back into the dense image domain. Furthermore, we employ the estimated motion field from our two-stage DSD framework as initialization and boost the motion estimation quality in light-weight yet effective iterative optimization. We evaluate our method on two dynamic medical imaging tasks to model cardiac motion and lung respiratory motion, respectively. Our method has produced superior motion estimation accuracy compared to existing comparative methods. Besides, the extensive experimental results demonstrate that our solution can extract well representative anatomical landmarks without any requirement of manual annotation. Our code is publicly available online.



### GT U-Net: A U-Net Like Group Transformer Network for Tooth Root Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.14813v1
- **DOI**: 10.1007/978-3-030-87589-3_40
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14813v1)
- **Published**: 2021-09-30 02:39:07+00:00
- **Updated**: 2021-09-30 02:39:07+00:00
- **Authors**: Yunxiang Li, Shuai Wang, Jun Wang, Guodong Zeng, Wenjun Liu, Qianni Zhang, Qun Jin, Yaqi Wang
- **Comment**: None
- **Journal**: 12th International Workshop, MLMI 2021, Held in Conjunction with
  MICCAI 2021
- **Summary**: To achieve an accurate assessment of root canal therapy, a fundamental step is to perform tooth root segmentation on oral X-ray images, in that the position of tooth root boundary is significant anatomy information in root canal therapy evaluation. However, the fuzzy boundary makes the tooth root segmentation very challenging. In this paper, we propose a novel end-to-end U-Net like Group Transformer Network (GT U-Net) for the tooth root segmentation. The proposed network retains the essential structure of U-Net but each of the encoders and decoders is replaced by a group Transformer, which significantly reduces the computational cost of traditional Transformer architectures by using the grouping structure and the bottleneck structure. In addition, the proposed GT U-Net is composed of a hybrid structure of convolution and Transformer, which makes it independent of pre-training weights. For optimization, we also propose a shape-sensitive Fourier Descriptor (FD) loss function to make use of shape prior knowledge. Experimental results show that our proposed network achieves the state-of-the-art performance on our collected tooth root segmentation dataset and the public retina dataset DRIVE. Code has been released at https://github.com/Kent0n-Li/GT-U-Net.



### Spark in the Dark: Evaluating Encoder-Decoder Pairs for COVID-19 CT's Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.14818v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14818v1)
- **Published**: 2021-09-30 02:54:24+00:00
- **Updated**: 2021-09-30 02:54:24+00:00
- **Authors**: Bruno A. Krinski, Daniel V. Ruiz, Eduardo Todt
- **Comment**: Paper accepted in the 18th IEEE Latin American Robotics Symposium -
  LARS 2021
- **Journal**: None
- **Summary**: With the COVID-19 global pandemic, computerassisted diagnoses of medical images have gained a lot of attention, and robust methods of Semantic Segmentation of Computed Tomography (CT) turned highly desirable. Semantic Segmentation of CT is one of many research fields of automatic detection of Covid-19 and was widely explored since the Covid19 outbreak. In the robotic field, Semantic Segmentation of organs and CTs are widely used in robots developed for surgery tasks. As new methods and new datasets are proposed quickly, it becomes apparent the necessity of providing an extensive evaluation of those methods. To provide a standardized comparison of different architectures across multiple recently proposed datasets, we propose in this paper an extensive benchmark of multiple encoders and decoders with a total of 120 architectures evaluated in five datasets, with each dataset being validated through a five-fold cross-validation strategy, totaling 3.000 experiments. To the best of our knowledge, this is the largest evaluation in number of encoders, decoders, and datasets proposed in the field of Covid-19 CT segmentation.



### Semantic Dense Reconstruction with Consistent Scene Segments
- **Arxiv ID**: http://arxiv.org/abs/2109.14821v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.14821v1)
- **Published**: 2021-09-30 03:01:17+00:00
- **Updated**: 2021-09-30 03:01:17+00:00
- **Authors**: Yingcai Wan, Yanyan Li, Yingxuan You, Cheng Guo, Lijin Fang, Federico Tombari
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a method for dense semantic 3D scene reconstruction from an RGB-D sequence is proposed to solve high-level scene understanding tasks. First, each RGB-D pair is consistently segmented into 2D semantic maps based on a camera tracking backbone that propagates objects' labels with high probabilities from full scans to corresponding ones of partial views. Then a dense 3D mesh model of an unknown environment is incrementally generated from the input RGB-D sequence. Benefiting from 2D consistent semantic segments and the 3D model, a novel semantic projection block (SP-Block) is proposed to extract deep feature volumes from 2D segments of different views. Moreover, the semantic volumes are fused into deep volumes from a point cloud encoder to make the final semantic segmentation. Extensive experimental evaluations on public datasets show that our system achieves accurate 3D dense reconstruction and state-of-the-art semantic prediction performances simultaneously.



### IntentVizor: Towards Generic Query Guided Interactive Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/2109.14834v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14834v2)
- **Published**: 2021-09-30 03:44:02+00:00
- **Updated**: 2022-03-29 06:57:02+00:00
- **Authors**: Guande Wu, Jianzhe Lin, Claudio T. Silva
- **Comment**: 10 pages and 4 figures, CVPR 2022
- **Journal**: None
- **Summary**: The target of automatic video summarization is to create a short skim of the original long video while preserving the major content/events. There is a growing interest in the integration of user queries into video summarization or query-driven video summarization. This video summarization method predicts a concise synopsis of the original video based on the user query, which is commonly represented by the input text. However, two inherent problems exist in this query-driven way. First, the text query might not be enough to describe the exact and diverse needs of the user. Second, the user cannot edit once the summaries are produced, while we assume the needs of the user should be subtle and need to be adjusted interactively. To solve these two problems, we propose IntentVizor, an interactive video summarization framework guided by generic multi-modality queries. The input query that describes the user's needs are not limited to text but also the video snippets. We further represent these multi-modality finer-grained queries as user `intent', which is interpretable, interactable, editable, and can better quantify the user's needs. In this paper, we use a set of the proposed intents to represent the user query and design a new interactive visual analytic interface. Users can interactively control and adjust these mixed-initiative intents to obtain a more satisfying summary through the interface. Also, to improve the summarization quality via video understanding, a novel Granularity-Scalable Ego-Graph Convolutional Networks (GSE-GCN) is proposed. We conduct our experiments on two benchmark datasets. Comparisons with the state-of-the-art methods verify the effectiveness of the proposed framework. Code and dataset are available at https://github.com/jnzs1836/intent-vizor.



### End-to-End Image Compression with Probabilistic Decoding
- **Arxiv ID**: http://arxiv.org/abs/2109.14837v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14837v1)
- **Published**: 2021-09-30 04:07:09+00:00
- **Updated**: 2021-09-30 04:07:09+00:00
- **Authors**: Haichuan Ma, Dong Liu, Cunhui Dong, Li Li, Feng Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Lossy image compression is a many-to-one process, thus one bitstream corresponds to multiple possible original images, especially at low bit rates. However, this nature was seldom considered in previous studies on image compression, which usually chose one possible image as reconstruction, e.g. the one with the maximal a posteriori probability. We propose a learned image compression framework to natively support probabilistic decoding. The compressed bitstream is decoded into a series of parameters that instantiate a pre-chosen distribution; then the distribution is used by the decoder to sample and reconstruct images. The decoder may adopt different sampling strategies and produce diverse reconstructions, among which some have higher signal fidelity and some others have better visual quality. The proposed framework is dependent on a revertible neural network-based transform to convert pixels into coefficients that obey the pre-chosen distribution as much as possible. Our code and models will be made publicly available.



### AffectGAN: Affect-Based Generative Art Driven by Semantics
- **Arxiv ID**: http://arxiv.org/abs/2109.14845v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.14845v1)
- **Published**: 2021-09-30 04:53:25+00:00
- **Updated**: 2021-09-30 04:53:25+00:00
- **Authors**: Theodoros Galanos, Antonios Liapis, Georgios N. Yannakakis
- **Comment**: Published in the "What's Next in Affect Modeling?" workshop at the
  Affective Computing & Intelligent Interaction (ACII) 2021 conference, 7
  pages, 3 figures
- **Journal**: None
- **Summary**: This paper introduces a novel method for generating artistic images that express particular affective states. Leveraging state-of-the-art deep learning methods for visual generation (through generative adversarial networks), semantic models from OpenAI, and the annotated dataset of the visual art encyclopedia WikiArt, our AffectGAN model is able to generate images based on specific or broad semantic prompts and intended affective outcomes. A small dataset of 32 images generated by AffectGAN is annotated by 50 participants in terms of the particular emotion they elicit, as well as their quality and novelty. Results show that for most instances the intended emotion used as a prompt for image generation matches the participants' responses. This small-scale study brings forth a new vision towards blending affective computing with computational creativity, enabling generative systems with intentionality in terms of the emotions they wish their output to elicit.



### HLIC: Harmonizing Optimization Metrics in Learned Image Compression by Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.14863v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14863v1)
- **Published**: 2021-09-30 06:01:57+00:00
- **Updated**: 2021-09-30 06:01:57+00:00
- **Authors**: Baocheng Sun, Meng Gu, Dailan He, Tongda Xu, Yan Wang, Hongwei Qin
- **Comment**: working paper
- **Journal**: None
- **Summary**: Learned image compression is making good progress in recent years. Peak signal-to-noise ratio (PSNR) and multi-scale structural similarity (MS-SSIM) are the two most popular evaluation metrics. As different metrics only reflect certain aspects of human perception, works in this field normally optimize two models using PSNR and MS-SSIM as loss function separately, which is suboptimal and makes it difficult to select the model with best visual quality or overall performance. Towards solving this problem, we propose to Harmonize optimization metrics in Learned Image Compression (HLIC) using online loss function adaptation by reinforcement learning. By doing so, we are able to leverage the advantages of both PSNR and MS-SSIM, achieving better visual quality and higher VMAF score. To our knowledge, our work is the first to explore automatic loss function adaptation for harmonizing optimization metrics in low level vision tasks like learned image compression.



### Robust Segmentation Models using an Uncertainty Slice Sampling Based Annotation Workflow
- **Arxiv ID**: http://arxiv.org/abs/2109.14879v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.14879v1)
- **Published**: 2021-09-30 06:56:11+00:00
- **Updated**: 2021-09-30 06:56:11+00:00
- **Authors**: Grzegorz Chlebus, Andrea Schenk, Horst K. Hahn, Bram van Ginneken, Hans Meine
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Semantic segmentation neural networks require pixel-level annotations in large quantities to achieve a good performance. In the medical domain, such annotations are expensive, because they are time-consuming and require expert knowledge. Active learning optimizes the annotation effort by devising strategies to select cases for labeling that are most informative to the model. In this work, we propose an uncertainty slice sampling (USS) strategy for semantic segmentation of 3D medical volumes that selects 2D image slices for annotation and compare it with various other strategies. We demonstrate the efficiency of USS on a CT liver segmentation task using multi-site data. After five iterations, the training data resulting from USS consisted of 2410 slices (4% of all slices in the data pool) compared to 8121 (13%), 8641 (14%), and 3730 (6%) for uncertainty volume (UVS), random volume (RVS), and random slice (RSS) sampling, respectively. Despite being trained on the smallest amount of data, the model based on the USS strategy evaluated on 234 test volumes significantly outperformed models trained according to other strategies and achieved a mean Dice index of 0.964, a relative volume error of 4.2%, a mean surface distance of 1.35 mm, and a Hausdorff distance of 23.4 mm. This was only slightly inferior to 0.967, 3.8%, 1.18 mm, and 22.9 mm achieved by a model trained on all available data, but the robustness analysis using the 5th percentile of Dice and the 95th percentile of the remaining metrics demonstrated that USS resulted not only in the most robust model compared to other sampling schemes, but also outperformed the model trained on all data according to Dice (0.946 vs. 0.945) and mean surface distance (1.92 mm vs. 2.03 mm).



### CrossCLR: Cross-modal Contrastive Learning For Multi-modal Video Representations
- **Arxiv ID**: http://arxiv.org/abs/2109.14910v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.14910v1)
- **Published**: 2021-09-30 08:12:21+00:00
- **Updated**: 2021-09-30 08:12:21+00:00
- **Authors**: Mohammadreza Zolfaghari, Yi Zhu, Peter Gehler, Thomas Brox
- **Comment**: ICCV 2021, 14 pages, 13 figures
- **Journal**: None
- **Summary**: Contrastive learning allows us to flexibly define powerful losses by contrasting positive pairs from sets of negative samples. Recently, the principle has also been used to learn cross-modal embeddings for video and text, yet without exploiting its full potential. In particular, previous losses do not take the intra-modality similarities into account, which leads to inefficient embeddings, as the same content is mapped to multiple points in the embedding space. With CrossCLR, we present a contrastive loss that fixes this issue. Moreover, we define sets of highly related samples in terms of their input embeddings and exclude them from the negative samples to avoid issues with false negatives. We show that these principles consistently improve the quality of the learned embeddings. The joint embeddings learned with CrossCLR extend the state of the art in video-text retrieval on Youcook2 and LSMDC datasets and in video captioning on Youcook2 dataset by a large margin. We also demonstrate the generality of the concept by learning improved joint embeddings for other pairs of modalities.



### Forming a sparse representation for visual place recognition using a neurorobotic approach
- **Arxiv ID**: http://arxiv.org/abs/2109.14916v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14916v1)
- **Published**: 2021-09-30 08:26:22+00:00
- **Updated**: 2021-09-30 08:26:22+00:00
- **Authors**: Sylvain Colomer, Nicolas Cuperlier, Guillaume Bresson, Olivier Romain
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel unsupervised neural network model for visual information encoding which aims to address the problem of large-scale visual localization. Inspired by the structure of the visual cortex, the model (namely HSD) alternates layers of topologic sparse coding and pooling to build a more compact code of visual information. Intended for visual place recognition (VPR) systems that use local descriptors, the impact of its integration in a bio-inpired model for self-localization (LPMP) is evaluated. Our experimental results on the KITTI dataset show that HSD improves the runtime speed of LPMP by a factor of at least 2 and its localization accuracy by 10%. A comparison with CoHog, a state-of-the-art VPR approach, showed that our method achieves slightly better results.



### A Deep Learning Localization Method for Measuring Abdominal Muscle Dimensions in Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/2109.14919v1
- **DOI**: 10.1109/JBHI.2021.3085019
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14919v1)
- **Published**: 2021-09-30 08:36:50+00:00
- **Updated**: 2021-09-30 08:36:50+00:00
- **Authors**: Alzayat Saleh, Issam H. Laradji, Corey Lammie, David Vazquez, Carol A Flavell, Mostafa Rahimi Azghadi
- **Comment**: 9 pages, 8 figures, 1 tables, Accepted for Publication in the IEEE
  Journal of Biomedical and Health Informatics (J-BHI) 25-May-2021
- **Journal**: None
- **Summary**: Health professionals extensively use Two- Dimensional (2D) Ultrasound (US) videos and images to visualize and measure internal organs for various purposes including evaluation of muscle architectural changes. US images can be used to measure abdominal muscles dimensions for the diagnosis and creation of customized treatment plans for patients with Low Back Pain (LBP), however, they are difficult to interpret. Due to high variability, skilled professionals with specialized training are required to take measurements to avoid low intra-observer reliability. This variability stems from the challenging nature of accurately finding the correct spatial location of measurement endpoints in abdominal US images. In this paper, we use a Deep Learning (DL) approach to automate the measurement of the abdominal muscle thickness in 2D US images. By treating the problem as a localization task, we develop a modified Fully Convolutional Network (FCN) architecture to generate blobs of coordinate locations of measurement endpoints, similar to what a human operator does. We demonstrate that using the TrA400 US image dataset, our network achieves a Mean Absolute Error (MAE) of 0.3125 on the test set, which almost matches the performance of skilled ultrasound technicians. Our approach can facilitate next steps for automating the process of measurements in 2D US images, while reducing inter-observer as well as intra-observer variability for more effective clinical outcomes.



### RED++ : Data-Free Pruning of Deep Neural Networks via Input Splitting and Output Merging
- **Arxiv ID**: http://arxiv.org/abs/2110.01397v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01397v1)
- **Published**: 2021-09-30 09:31:11+00:00
- **Updated**: 2021-09-30 09:31:11+00:00
- **Authors**: Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, Kevin Bailly
- **Comment**: 18 pages, 10 figures
- **Journal**: None
- **Summary**: Pruning Deep Neural Networks (DNNs) is a prominent field of study in the goal of inference runtime acceleration. In this paper, we introduce a novel data-free pruning protocol RED++. Only requiring a trained neural network, and not specific to DNN architecture, we exploit an adaptive data-free scalar hashing which exhibits redundancies among neuron weight values. We study the theoretical and empirical guarantees on the preservation of the accuracy from the hashing as well as the expected pruning ratio resulting from the exploitation of said redundancies. We propose a novel data-free pruning technique of DNN layers which removes the input-wise redundant operations. This algorithm is straightforward, parallelizable and offers novel perspective on DNN pruning by shifting the burden of large computation to efficient memory access and allocation. We provide theoretical guarantees on RED++ performance and empirically demonstrate its superiority over other data-free pruning methods and its competitiveness with data-driven ones on ResNets, MobileNets and EfficientNets.



### Comparative Validation of Machine Learning Algorithms for Surgical Workflow and Skill Analysis with the HeiChole Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2109.14956v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.14956v1)
- **Published**: 2021-09-30 09:34:13+00:00
- **Updated**: 2021-09-30 09:34:13+00:00
- **Authors**: Martin Wagner, Beat-Peter Müller-Stich, Anna Kisilenko, Duc Tran, Patrick Heger, Lars Mündermann, David M Lubotsky, Benjamin Müller, Tornike Davitashvili, Manuela Capek, Annika Reinke, Tong Yu, Armine Vardazaryan, Chinedu Innocent Nwoye, Nicolas Padoy, Xinyang Liu, Eung-Joo Lee, Constantin Disch, Hans Meine, Tong Xia, Fucang Jia, Satoshi Kondo, Wolfgang Reiter, Yueming Jin, Yonghao Long, Meirui Jiang, Qi Dou, Pheng Ann Heng, Isabell Twick, Kadir Kirtac, Enes Hosgor, Jon Lindström Bolmgren, Michael Stenzel, Björn von Siemens, Hannes G. Kenngott, Felix Nickel, Moritz von Frankenberg, Franziska Mathis-Ullrich, Lena Maier-Hein, Stefanie Speidel, Sebastian Bodenstedt
- **Comment**: None
- **Journal**: None
- **Summary**: PURPOSE: Surgical workflow and skill analysis are key technologies for the next generation of cognitive surgical assistance systems. These systems could increase the safety of the operation through context-sensitive warnings and semi-autonomous robotic assistance or improve training of surgeons via data-driven feedback. In surgical workflow analysis up to 91% average precision has been reported for phase recognition on an open data single-center dataset. In this work we investigated the generalizability of phase recognition algorithms in a multi-center setting including more difficult recognition tasks such as surgical action and surgical skill. METHODS: To achieve this goal, a dataset with 33 laparoscopic cholecystectomy videos from three surgical centers with a total operation time of 22 hours was created. Labels included annotation of seven surgical phases with 250 phase transitions, 5514 occurences of four surgical actions, 6980 occurences of 21 surgical instruments from seven instrument categories and 495 skill classifications in five skill dimensions. The dataset was used in the 2019 Endoscopic Vision challenge, sub-challenge for surgical workflow and skill analysis. Here, 12 teams submitted their machine learning algorithms for recognition of phase, action, instrument and/or skill assessment. RESULTS: F1-scores were achieved for phase recognition between 23.9% and 67.7% (n=9 teams), for instrument presence detection between 38.5% and 63.8% (n=8 teams), but for action recognition only between 21.8% and 23.3% (n=5 teams). The average absolute error for skill assessment was 0.78 (n=1 team). CONCLUSION: Surgical workflow and skill analysis are promising technologies to support the surgical team, but are not solved yet, as shown by our comparison of algorithms. This novel benchmark can be used for comparable evaluation and validation of future work.



### Augmented reality navigation system for visual prosthesis
- **Arxiv ID**: http://arxiv.org/abs/2109.14957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14957v1)
- **Published**: 2021-09-30 09:41:40+00:00
- **Updated**: 2021-09-30 09:41:40+00:00
- **Authors**: Melani Sanchez-Garcia, Alejandro Perez-Yus, Ruben Martinez-Cantin, Jose J. Guerrero
- **Comment**: None
- **Journal**: None
- **Summary**: The visual functions of visual prostheses such as field of view, resolution and dynamic range, seriously restrict the person's ability to navigate in unknown environments. Implanted patients still require constant assistance for navigating from one location to another. Hence, there is a need for a system that is able to assist them safely during their journey. In this work, we propose an augmented reality navigation system for visual prosthesis that incorporates a software of reactive navigation and path planning which guides the subject through convenient, obstacle-free route. It consists on four steps: locating the subject on a map, planning the subject trajectory, showing it to the subject and re-planning without obstacles. We have also designed a simulated prosthetic vision environment which allows us to systematically study navigation performance. Twelve subjects participated in the experiment. Subjects were guided by the augmented reality navigation system and their instruction was to navigate through different environments until they reached two goals, cross the door and find an object (bin), as fast and accurately as possible. Results show how our augmented navigation system help navigation performance by reducing the time and distance to reach the goals, even significantly reducing the number of obstacles collisions, compared to other baseline methods.



### Moving Object Detection for Event-based vision using Graph Spectral Clustering
- **Arxiv ID**: http://arxiv.org/abs/2109.14979v3
- **DOI**: 10.1109/ICCVW54120.2021.00103
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2109.14979v3)
- **Published**: 2021-09-30 10:19:22+00:00
- **Updated**: 2021-12-02 14:55:35+00:00
- **Authors**: Anindya Mondal, Shashant R, Jhony H. Giraldo, Thierry Bouwmans, Ananda S. Chowdhury
- **Comment**: Ten pages, five figures, Published in 2021 IEEE/CVF International
  Conference on Computer Vision Workshops (ICCVW), Montreal, BC, Canada
- **Journal**: None
- **Summary**: Moving object detection has been a central topic of discussion in computer vision for its wide range of applications like in self-driving cars, video surveillance, security, and enforcement. Neuromorphic Vision Sensors (NVS) are bio-inspired sensors that mimic the working of the human eye. Unlike conventional frame-based cameras, these sensors capture a stream of asynchronous 'events' that pose multiple advantages over the former, like high dynamic range, low latency, low power consumption, and reduced motion blur. However, these advantages come at a high cost, as the event camera data typically contains more noise and has low resolution. Moreover, as event-based cameras can only capture the relative changes in brightness of a scene, event data do not contain usual visual information (like texture and color) as available in video data from normal cameras. So, moving object detection in event-based cameras becomes an extremely challenging task. In this paper, we present an unsupervised Graph Spectral Clustering technique for Moving Object Detection in Event-based data (GSCEventMOD). We additionally show how the optimum number of moving objects can be automatically determined. Experimental comparisons on publicly available datasets show that the proposed GSCEventMOD algorithm outperforms a number of state-of-the-art techniques by a maximum margin of 30%.



### Revisiting Point Cloud Simplification: A Learnable Feature Preserving Approach
- **Arxiv ID**: http://arxiv.org/abs/2109.14982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.14982v1)
- **Published**: 2021-09-30 10:23:55+00:00
- **Updated**: 2021-09-30 10:23:55+00:00
- **Authors**: Rolandos Alexandros Potamias, Giorgos Bouritsas, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: The recent advances in 3D sensing technology have made possible the capture of point clouds in significantly high resolution. However, increased detail usually comes at the expense of high storage, as well as computational costs in terms of processing and visualization operations. Mesh and Point Cloud simplification methods aim to reduce the complexity of 3D models while retaining visual quality and relevant salient features. Traditional simplification techniques usually rely on solving a time-consuming optimization problem, hence they are impractical for large-scale datasets. In an attempt to alleviate this computational burden, we propose a fast point cloud simplification method by learning to sample salient points. The proposed method relies on a graph neural network architecture trained to select an arbitrary, user-defined, number of points from the input space and to re-arrange their positions so as to minimize the visual perception error. The approach is extensively evaluated on various datasets using several perceptual metrics. Importantly, our method is able to generalize to out-of-distribution shapes, hence demonstrating zero-shot capabilities.



### Adversarial Semantic Contour for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2109.15009v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.15009v1)
- **Published**: 2021-09-30 11:03:06+00:00
- **Updated**: 2021-09-30 11:03:06+00:00
- **Authors**: Yichi Zhang, Zijian Zhu, Xiao Yang, Jun Zhu
- **Comment**: Accepted by ICML2021 Workshop on Adversarial Machine Learning
- **Journal**: None
- **Summary**: Modern object detectors are vulnerable to adversarial examples, which brings potential risks to numerous applications, e.g., self-driving car. Among attacks regularized by $\ell_p$ norm, $\ell_0$-attack aims to modify as few pixels as possible. Nevertheless, the problem is nontrivial since it generally requires to optimize the shape along with the texture simultaneously, which is an NP-hard problem. To address this issue, we propose a novel method of Adversarial Semantic Contour (ASC) guided by object contour as prior. With this prior, we reduce the searching space to accelerate the $\ell_0$ optimization, and also introduce more semantic information which should affect the detectors more. Based on the contour, we optimize the selection of modified pixels via sampling and their colors with gradient descent alternately. Extensive experiments demonstrate that our proposed ASC outperforms the most commonly manually designed patterns (e.g., square patches and grids) on task of disappearing. By modifying no more than 5\% and 3.5\% of the object area respectively, our proposed ASC can successfully mislead the mainstream object detectors including the SSD512, Yolov4, Mask RCNN, Faster RCNN, etc.



### 3D Pose Transfer with Correspondence Learning and Mesh Refinement
- **Arxiv ID**: http://arxiv.org/abs/2109.15025v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.15025v6)
- **Published**: 2021-09-30 11:49:03+00:00
- **Updated**: 2021-12-24 07:48:34+00:00
- **Authors**: Chaoyue Song, Jiacheng Wei, Ruibo Li, Fayao Liu, Guosheng Lin
- **Comment**: None
- **Journal**: None
- **Summary**: 3D pose transfer is one of the most challenging 3D generation tasks. It aims to transfer the pose of a source mesh to a target mesh and keep the identity (e.g., body shape) of the target mesh. Some previous works require key point annotations to build reliable correspondence between the source and target meshes, while other methods do not consider any shape correspondence between sources and targets, which leads to limited generation quality. In this work, we propose a correspondence-refinement network to achieve the 3D pose transfer for both human and animal meshes. The correspondence between source and target meshes is first established by solving an optimal transport problem. Then, we warp the source mesh according to the dense correspondence and obtain a coarse warped mesh. The warped mesh will be better refined with our proposed Elastic Instance Normalization, which is a conditional normalization layer and can help to generate high-quality meshes. Extensive experimental results show that the proposed architecture can effectively transfer the poses from source to target meshes and produce better results with satisfied visual performance than state-of-the-art methods.



### Riedones3D: a celtic coin dataset for registration and fine-grained clustering
- **Arxiv ID**: http://arxiv.org/abs/2109.15033v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.15033v1)
- **Published**: 2021-09-30 11:57:43+00:00
- **Updated**: 2021-09-30 11:57:43+00:00
- **Authors**: Sofiane Horache, Jean-Emmanuel Deschaud, François Goulette, Katherine Gruel, Thierry Lejars, Olivier Masson
- **Comment**: The code and the dataset will be available on the 15th December 2021
- **Journal**: None
- **Summary**: Clustering coins with respect to their die is an important component of numismatic research and crucial for understanding the economic history of tribes (especially when literary production does not exist, in celtic culture). It is a very hard task that requires a lot of times and expertise. To cluster thousands of coins, automatic methods are becoming necessary. Nevertheless, public datasets for coin die clustering evaluation are too rare, though they are very important for the development of new methods. Therefore, we propose a new 3D dataset of 2 070 scans of coins. With this dataset, we propose two benchmarks, one for point cloud registration, essential for coin die recognition, and a benchmark of coin die clustering. We show how we automatically cluster coins to help experts, and perform a preliminary evaluation for these two tasks. The code of the baseline and the dataset will be publicly available at https://www.npm3d.fr/coins-riedones3d and https://www.chronocarto.eu/spip.php?article84&lang=fr



### SPATE-GAN: Improved Generative Modeling of Dynamic Spatio-Temporal Patterns with an Autoregressive Embedding Loss
- **Arxiv ID**: http://arxiv.org/abs/2109.15044v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.15044v1)
- **Published**: 2021-09-30 12:10:05+00:00
- **Updated**: 2021-09-30 12:10:05+00:00
- **Authors**: Konstantin Klemmer, Tianlin Xu, Beatrice Acciaio, Daniel B. Neill
- **Comment**: None
- **Journal**: None
- **Summary**: From ecology to atmospheric sciences, many academic disciplines deal with data characterized by intricate spatio-temporal complexities, the modeling of which often requires specialized approaches. Generative models of these data are of particular interest, as they enable a range of impactful downstream applications like simulation or creating synthetic training data. Recent work has highlighted the potential of generative adversarial nets (GANs) for generating spatio-temporal data. A new GAN algorithm COT-GAN, inspired by the theory of causal optimal transport (COT), was proposed in an attempt to better tackle this challenge. However, the task of learning more complex spatio-temporal patterns requires additional knowledge of their specific data structures. In this study, we propose a novel loss objective combined with COT-GAN based on an autoregressive embedding to reinforce the learning of spatio-temporal dynamics. We devise SPATE (spatio-temporal association), a new metric measuring spatio-temporal autocorrelation by using the deviance of observations from their expected values. We compute SPATE for real and synthetic data samples and use it to compute an embedding loss that considers space-time interactions, nudging the GAN to learn outputs that are faithful to the observed dynamics. We test this new objective on a diverse set of complex spatio-temporal patterns: turbulent flows, log-Gaussian Cox processes and global weather data. We show that our novel embedding loss improves performance without any changes to the architecture of the COT-GAN backbone, highlighting our model's increased capacity for capturing autoregressive structures. We also contextualize our work with respect to recent advances in physics-informed deep learning and interdisciplinary work connecting neural networks with geographic and geophysical sciences.



### Deep Contextual Video Compression
- **Arxiv ID**: http://arxiv.org/abs/2109.15047v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2109.15047v2)
- **Published**: 2021-09-30 12:14:24+00:00
- **Updated**: 2021-12-14 09:28:48+00:00
- **Authors**: Jiahao Li, Bin Li, Yan Lu
- **Comment**: Accepted by NeurIPS 2021, codes are in
  https://github.com/DeepMC-DCVC/DCVC
- **Journal**: None
- **Summary**: Most of the existing neural video compression methods adopt the predictive coding framework, which first generates the predicted frame and then encodes its residue with the current frame. However, as for compression ratio, predictive coding is only a sub-optimal solution as it uses simple subtraction operation to remove the redundancy across frames. In this paper, we propose a deep contextual video compression framework to enable a paradigm shift from predictive coding to conditional coding. In particular, we try to answer the following questions: how to define, use, and learn condition under a deep video compression framework. To tap the potential of conditional coding, we propose using feature domain context as condition. This enables us to leverage the high dimension context to carry rich information to both the encoder and the decoder, which helps reconstruct the high-frequency contents for higher video quality. Our framework is also extensible, in which the condition can be flexibly designed. Experiments show that our method can significantly outperform the previous state-of-the-art (SOTA) deep video compression methods. When compared with x265 using veryslow preset, we can achieve 26.0% bitrate saving for 1080P standard test videos.



### Workflow Augmentation of Video Data for Event Recognition with Time-Sensitive Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.15063v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.15063v1)
- **Published**: 2021-09-30 12:20:54+00:00
- **Updated**: 2021-09-30 12:20:54+00:00
- **Authors**: Andreas Wachter, Werner Nahm
- **Comment**: Submitted to Medical Image Analysis
- **Journal**: None
- **Summary**: Supervised training of neural networks requires large, diverse and well annotated data sets. In the medical field, this is often difficult to achieve due to constraints in time, expert knowledge and prevalence of an event. Artificial data augmentation can help to prevent overfitting and improve the detection of rare events as well as overall performance. However, most augmentation techniques use purely spatial transformations, which are not sufficient for video data with temporal correlations. In this paper, we present a novel methodology for workflow augmentation and demonstrate its benefit for event recognition in cataract surgery. The proposed approach increases the frequency of event alternation by creating artificial videos. The original video is split into event segments and a workflow graph is extracted from the original annotations. Finally, the segments are assembled into new videos based on the workflow graph. Compared to the original videos, the frequency of event alternation in the augmented cataract surgery videos increased by 26%. Further, a 3% higher classification accuracy and a 7.8% higher precision was achieved compared to a state-of-the-art approach. Our approach is particularly helpful to increase the occurrence of rare but important events and can be applied to a large variety of use cases.



### iShape: A First Step Towards Irregular Shape Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.15068v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.15068v1)
- **Published**: 2021-09-30 12:30:16+00:00
- **Updated**: 2021-09-30 12:30:16+00:00
- **Authors**: Lei Yang, Yan Zi Wei, Yisheng HE, Wei Sun, Zhenhang Huang, Haibin Huang, Haoqiang Fan
- **Comment**: Project page: https://ishape.github.io/
- **Journal**: None
- **Summary**: In this paper, we introduce a brand new dataset to promote the study of instance segmentation for objects with irregular shapes. Our key observation is that though irregularly shaped objects widely exist in daily life and industrial scenarios, they received little attention in the instance segmentation field due to the lack of corresponding datasets. To fill this gap, we propose iShape, an irregular shape dataset for instance segmentation. iShape contains six sub-datasets with one real and five synthetics, each represents a scene of a typical irregular shape. Unlike most existing instance segmentation datasets of regular objects, iShape has many characteristics that challenge existing instance segmentation algorithms, such as large overlaps between bounding boxes of instances, extreme aspect ratios, and large numbers of connected components per instance. We benchmark popular instance segmentation methods on iShape and find their performance drop dramatically. Hence, we propose an affinity-based instance segmentation algorithm, called ASIS, as a stronger baseline. ASIS explicitly combines perception and reasoning to solve Arbitrary Shape Instance Segmentation including irregular objects. Experimental results show that ASIS outperforms the state-of-the-art on iShape. Dataset and code are available at https://ishape.github.io



### Deep Homography Estimation in Dynamic Surgical Scenes for Laparoscopic Camera Motion Extraction
- **Arxiv ID**: http://arxiv.org/abs/2109.15098v2
- **DOI**: 10.1080/21681163.2021.2002195
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.15098v2)
- **Published**: 2021-09-30 13:05:37+00:00
- **Updated**: 2021-10-27 12:29:47+00:00
- **Authors**: Martin Huber, Sébastien Ourselin, Christos Bergeles, Tom Vercauteren
- **Comment**: Accepted for publication in 2021 AE-CAI Special Issue of the Computer
  Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization
- **Journal**: None
- **Summary**: Current laparoscopic camera motion automation relies on rule-based approaches or only focuses on surgical tools. Imitation Learning (IL) methods could alleviate these shortcomings, but have so far been applied to oversimplified setups. Instead of extracting actions from oversimplified setups, in this work we introduce a method that allows to extract a laparoscope holder's actions from videos of laparoscopic interventions. We synthetically add camera motion to a newly acquired dataset of camera motion free da Vinci surgery image sequences through a novel homography generation algorithm. The synthetic camera motion serves as a supervisory signal for camera motion estimation that is invariant to object and tool motion. We perform an extensive evaluation of state-of-the-art (SOTA) Deep Neural Networks (DNNs) across multiple compute regimes, finding our method transfers from our camera motion free da Vinci surgery dataset to videos of laparoscopic interventions, outperforming classical homography estimation approaches in both, precision by 41%, and runtime on a CPU by 43%.



### Fake It Till You Make It: Face analysis in the wild using synthetic data alone
- **Arxiv ID**: http://arxiv.org/abs/2109.15102v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.15102v2)
- **Published**: 2021-09-30 13:07:04+00:00
- **Updated**: 2021-10-05 11:44:47+00:00
- **Authors**: Erroll Wood, Tadas Baltrušaitis, Charlie Hewitt, Sebastian Dziadzio, Matthew Johnson, Virginia Estellers, Thomas J. Cashman, Jamie Shotton
- **Comment**: ICCV 2021. Amended acknowledgements
- **Journal**: None
- **Summary**: We demonstrate that it is possible to perform face-related computer vision in the wild using synthetic data alone. The community has long enjoyed the benefits of synthesizing training data with graphics, but the domain gap between real and synthetic data has remained a problem, especially for human faces. Researchers have tried to bridge this gap with data mixing, domain adaptation, and domain-adversarial training, but we show that it is possible to synthesize data with minimal domain gap, so that models trained on synthetic data generalize to real in-the-wild datasets. We describe how to combine a procedurally-generated parametric 3D face model with a comprehensive library of hand-crafted assets to render training images with unprecedented realism and diversity. We train machine learning systems for face-related tasks such as landmark localization and face parsing, showing that synthetic data can both match real data in accuracy as well as open up new approaches where manual labelling would be impossible.



### Motion-aware Contrastive Video Representation Learning via Foreground-background Merging
- **Arxiv ID**: http://arxiv.org/abs/2109.15130v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.15130v3)
- **Published**: 2021-09-30 13:45:26+00:00
- **Updated**: 2022-03-13 17:16:57+00:00
- **Authors**: Shuangrui Ding, Maomao Li, Tianyu Yang, Rui Qian, Haohang Xu, Qingyi Chen, Jue Wang, Hongkai Xiong
- **Comment**: CVPR2022 camera ready
- **Journal**: None
- **Summary**: In light of the success of contrastive learning in the image domain, current self-supervised video representation learning methods usually employ contrastive loss to facilitate video representation learning. When naively pulling two augmented views of a video closer, the model however tends to learn the common static background as a shortcut but fails to capture the motion information, a phenomenon dubbed as background bias. Such bias makes the model suffer from weak generalization ability, leading to worse performance on downstream tasks such as action recognition. To alleviate such bias, we propose \textbf{F}oreground-b\textbf{a}ckground \textbf{Me}rging (FAME) to deliberately compose the moving foreground region of the selected video onto the static background of others. Specifically, without any off-the-shelf detector, we extract the moving foreground out of background regions via the frame difference and color statistics, and shuffle the background regions among the videos. By leveraging the semantic consistency between the original clips and the fused ones, the model focuses more on the motion patterns and is debiased from the background shortcut. Extensive experiments demonstrate that FAME can effectively resist background cheating and thus achieve the state-of-the-art performance on downstream tasks across UCF101, HMDB51, and Diving48 datasets. The code and configurations are released at https://github.com/Mark12Ding/FAME.



### HSVA: Hierarchical Semantic-Visual Adaptation for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2109.15163v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.15163v2)
- **Published**: 2021-09-30 14:27:50+00:00
- **Updated**: 2021-10-08 07:26:51+00:00
- **Authors**: Shiming Chen, Guo-Sen Xie, Yang Liu, Qinmu Peng, Baigui Sun, Hao Li, Xinge You, Ling Shao
- **Comment**: Accepted by NeurIPS 2021
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) tackles the unseen class recognition problem, transferring semantic knowledge from seen classes to unseen ones. Typically, to guarantee desirable knowledge transfer, a common (latent) space is adopted for associating the visual and semantic domains in ZSL. However, existing common space learning methods align the semantic and visual domains by merely mitigating distribution disagreement through one-step adaptation. This strategy is usually ineffective due to the heterogeneous nature of the feature representations in the two domains, which intrinsically contain both distribution and structure variations. To address this and advance ZSL, we propose a novel hierarchical semantic-visual adaptation (HSVA) framework. Specifically, HSVA aligns the semantic and visual domains by adopting a hierarchical two-step adaptation, i.e., structure adaptation and distribution adaptation. In the structure adaptation step, we take two task-specific encoders to encode the source data (visual domain) and the target data (semantic domain) into a structure-aligned common space. To this end, a supervised adversarial discrepancy (SAD) module is proposed to adversarially minimize the discrepancy between the predictions of two task-specific classifiers, thus making the visual and semantic feature manifolds more closely aligned. In the distribution adaptation step, we directly minimize the Wasserstein distance between the latent multivariate Gaussian distributions to align the visual and semantic distributions using a common encoder. Finally, the structure and distribution adaptation are derived in a unified framework under two partially-aligned variational autoencoders. Extensive experiments on four benchmark datasets demonstrate that HSVA achieves superior performance on both conventional and generalized ZSL. The code is available at \url{https://github.com/shiming-chen/HSVA} .



### A Technical Report for ICCV 2021 VIPriors Re-identification Challenge
- **Arxiv ID**: http://arxiv.org/abs/2109.15164v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.15164v1)
- **Published**: 2021-09-30 14:29:31+00:00
- **Updated**: 2021-09-30 14:29:31+00:00
- **Authors**: Cen Liu, Yunbo Peng, Yue Lin
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Person re-identification has always been a hot and challenging task. This paper introduces our solution for the re-identification track in VIPriors Challenge 2021. In this challenge, the difficulty is how to train the model from scratch without any pretrained weight. In our method, we show use state-of-the-art data processing strategies, model designs, and post-processing ensemble methods, it is possible to overcome the difficulty of data shortage and obtain competitive results. (1) Both image augmentation strategy and novel pre-processing method for occluded images can help the model learn more discriminative features. (2) Several strong backbones and multiple loss functions are used to learn more representative features. (3) Post-processing techniques including re-ranking, automatic query expansion, ensemble learning, etc., significantly improve the final performance. The final score of our team (ALONG) is 96.5154% mAP, ranking first in the leaderboard.



### CoSeg: Cognitively Inspired Unsupervised Generic Event Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.15170v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2109.15170v1)
- **Published**: 2021-09-30 14:40:32+00:00
- **Updated**: 2021-09-30 14:40:32+00:00
- **Authors**: Xiao Wang, Jingen Liu, Tao Mei, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Some cognitive research has discovered that humans accomplish event segmentation as a side effect of event anticipation. Inspired by this discovery, we propose a simple yet effective end-to-end self-supervised learning framework for event segmentation/boundary detection. Unlike the mainstream clustering-based methods, our framework exploits a transformer-based feature reconstruction scheme to detect event boundary by reconstruction errors. This is consistent with the fact that humans spot new events by leveraging the deviation between their prediction and what is actually perceived. Thanks to their heterogeneity in semantics, the frames at boundaries are difficult to be reconstructed (generally with large reconstruction errors), which is favorable for event boundary detection. Additionally, since the reconstruction occurs on the semantic feature level instead of pixel level, we develop a temporal contrastive feature embedding module to learn the semantic visual representation for frame feature reconstruction. This procedure is like humans building up experiences with "long-term memory". The goal of our work is to segment generic events rather than localize some specific ones. We focus on achieving accurate event boundaries. As a result, we adopt F1 score (Precision/Recall) as our primary evaluation metric for a fair comparison with previous approaches. Meanwhile, we also calculate the conventional frame-based MoF and IoU metric. We thoroughly benchmark our work on four publicly available datasets and demonstrate much better results.



### You Cannot Easily Catch Me: A Low-Detectable Adversarial Patch for Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2109.15177v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2109.15177v1)
- **Published**: 2021-09-30 14:47:29+00:00
- **Updated**: 2021-09-30 14:47:29+00:00
- **Authors**: Zijian Zhu, Hang Su, Chang Liu, Wenzhao Xiang, Shibao Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Blind spots or outright deceit can bedevil and deceive machine learning models. Unidentified objects such as digital "stickers," also known as adversarial patches, can fool facial recognition systems, surveillance systems and self-driving cars. Fortunately, most existing adversarial patches can be outwitted, disabled and rejected by a simple classification network called an adversarial patch detector, which distinguishes adversarial patches from original images. An object detector classifies and predicts the types of objects within an image, such as by distinguishing a motorcyclist from the motorcycle, while also localizing each object's placement within the image by "drawing" so-called bounding boxes around each object, once again separating the motorcyclist from the motorcycle. To train detectors even better, however, we need to keep subjecting them to confusing or deceitful adversarial patches as we probe for the models' blind spots. For such probes, we came up with a novel approach, a Low-Detectable Adversarial Patch, which attacks an object detector with small and texture-consistent adversarial patches, making these adversaries less likely to be recognized. Concretely, we use several geometric primitives to model the shapes and positions of the patches. To enhance our attack performance, we also assign different weights to the bounding boxes in terms of loss function. Our experiments on the common detection dataset COCO as well as the driving-video dataset D2-City show that LDAP is an effective attack method, and can resist the adversarial patch detector.



### Language-Aligned Waypoint (LAW) Supervision for Vision-and-Language Navigation in Continuous Environments
- **Arxiv ID**: http://arxiv.org/abs/2109.15207v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.15207v1)
- **Published**: 2021-09-30 15:28:24+00:00
- **Updated**: 2021-09-30 15:28:24+00:00
- **Authors**: Sonia Raychaudhuri, Saim Wani, Shivansh Patel, Unnat Jain, Angel X. Chang
- **Comment**: EMNLP 2021
- **Journal**: None
- **Summary**: In the Vision-and-Language Navigation (VLN) task an embodied agent navigates a 3D environment, following natural language instructions. A challenge in this task is how to handle 'off the path' scenarios where an agent veers from a reference path. Prior work supervises the agent with actions based on the shortest path from the agent's location to the goal, but such goal-oriented supervision is often not in alignment with the instruction. Furthermore, the evaluation metrics employed by prior work do not measure how much of a language instruction the agent is able to follow. In this work, we propose a simple and effective language-aligned supervision scheme, and a new metric that measures the number of sub-instructions the agent has completed during navigation.



### Natural Synthetic Anomalies for Self-Supervised Anomaly Detection and Localization
- **Arxiv ID**: http://arxiv.org/abs/2109.15222v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.15222v3)
- **Published**: 2021-09-30 15:50:04+00:00
- **Updated**: 2022-07-24 18:52:59+00:00
- **Authors**: Hannah M. Schlüter, Jeremy Tan, Benjamin Hou, Bernhard Kainz
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: We introduce a simple and intuitive self-supervision task, Natural Synthetic Anomalies (NSA), for training an end-to-end model for anomaly detection and localization using only normal training data. NSA integrates Poisson image editing to seamlessly blend scaled patches of various sizes from separate images. This creates a wide range of synthetic anomalies which are more similar to natural sub-image irregularities than previous data-augmentation strategies for self-supervised anomaly detection. We evaluate the proposed method using natural and medical images. Our experiments with the MVTec AD dataset show that a model trained to localize NSA anomalies generalizes well to detecting real-world a priori unknown types of manufacturing defects. Our method achieves an overall detection AUROC of 97.2 outperforming all previous methods that learn without the use of additional datasets. Code available at https://github.com/hmsch/natural-synthetic-anomalies.



### Real-Time Tactile Grasp Force Sensing Using Fingernail Imaging via Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2109.15231v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.15231v2)
- **Published**: 2021-09-30 16:07:41+00:00
- **Updated**: 2021-10-14 18:07:14+00:00
- **Authors**: Navid Fallahinia, Stephen Mascaro
- **Comment**: pending co-authors permission to publish
- **Journal**: None
- **Summary**: This paper has introduced a novel approach for the real-time estimation of 3D tactile forces exerted by human fingertips via vision only. The introduced approach is entirely monocular vision-based and does not require any physical force sensor. Therefore, it is scalable, non-intrusive, and easily fused with other perception systems such as body pose estimation, making it ideal for HRI applications where force sensing is necessary. The introduced approach consists of three main modules: finger tracking for detection and tracking of each individual finger, image alignment for preserving the spatial information in the images, and the force model for estimating the 3D forces from coloration patterns in the images. The model has been implemented experimentally, and the results have shown a maximum RMS error of 8.4% (for the entire range of force levels) along all three directions. The estimation accuracy is comparable to the offline models in the literature, such as EigneNail, while, this model is capable of performing force estimation at 30 frames per second.



### A novel framework based on deep learning and ANOVA feature selection method for diagnosis of COVID-19 cases from chest X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2110.06340v1
- **DOI**: 10.1155/2022/4694567
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.06340v1)
- **Published**: 2021-09-30 16:10:31+00:00
- **Updated**: 2021-09-30 16:10:31+00:00
- **Authors**: Hamid Nasiri, Seyyed Ali Alavi
- **Comment**: None
- **Journal**: Comput. Intell. Neurosci., vol. 2022, p. 4694567, 2022
- **Summary**: The new coronavirus (known as COVID-19) was first identified in Wuhan and quickly spread worldwide, wreaking havoc on the economy and people's everyday lives. Fever, cough, sore throat, headache, exhaustion, muscular aches, and difficulty breathing are all typical symptoms of COVID-19. A reliable detection technique is needed to identify affected individuals and care for them in the early stages of COVID-19 and reduce the virus's transmission. The most accessible method for COVID-19 identification is RT-PCR; however, due to its time commitment and false-negative results, alternative options must be sought. Indeed, compared to RT-PCR, chest CT scans and chest X-ray images provide superior results. Because of the scarcity and high cost of CT scan equipment, X-ray images are preferable for screening. In this paper, a pre-trained network, DenseNet169, was employed to extract features from X-ray images. Features were chosen by a feature selection method (ANOVA) to reduce computations and time complexity while overcoming the curse of dimensionality to improve predictive accuracy. Finally, selected features were classified by XGBoost. The ChestX-ray8 dataset, which was employed to train and evaluate the proposed method. This method reached 98.72% accuracy for two-class classification (COVID-19, healthy) and 92% accuracy for three-class classification (COVID-19, healthy, pneumonia).



### Transferability Estimation for Semantic Segmentation Task
- **Arxiv ID**: http://arxiv.org/abs/2109.15242v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.15242v2)
- **Published**: 2021-09-30 16:21:17+00:00
- **Updated**: 2021-10-11 03:05:54+00:00
- **Authors**: Yang Tan, Yang Li, Shao-Lun Huang
- **Comment**: It is a very early draft before the formal paper, so there may exist
  many typos or descriptions needing to be further improved. We will keep
  updating this manuscript
- **Journal**: None
- **Summary**: Transferability estimation is a fundamental problem in transfer learning to predict how good the performance is when transferring a source model (or source task) to a target task. With the guidance of transferability score, we can efficiently select the highly transferable source models without performing the real transfer in practice. Recent analytical transferability metrics are mainly designed for image classification problem, and currently there is no specific investigation for the transferability estimation of semantic segmentation task, which is an essential problem in autonomous driving, medical image analysis, etc. Consequently, we further extend the recent analytical transferability metric OTCE (Optimal Transport based Conditional Entropy) score to the semantic segmentation task. The challenge in applying the OTCE score is the high dimensional segmentation output, which is difficult to find the optimal coupling between so many pixels under an acceptable computation cost. Thus we propose to randomly sample N pixels for computing OTCE score and take the expectation over K repetitions as the final transferability score. Experimental evaluation on Cityscapes, BDD100K and GTA5 datasets demonstrates that the OTCE score highly correlates with the transfer performance.



### TöRF: Time-of-Flight Radiance Fields for Dynamic Scene View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2109.15271v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.15271v2)
- **Published**: 2021-09-30 17:12:59+00:00
- **Updated**: 2021-12-06 17:44:32+00:00
- **Authors**: Benjamin Attal, Eliot Laidlaw, Aaron Gokaslan, Changil Kim, Christian Richardt, James Tompkin, Matthew O'Toole
- **Comment**: Accepted to NeurIPS 2021. Web page: https://imaging.cs.cmu.edu/torf/
  NeurIPS camera ready updates -- added quantitative comparisons to new
  methods, visual side-by-side comparisons performed on larger baseline camera
  sequences
- **Journal**: None
- **Summary**: Neural networks can represent and accurately reconstruct radiance fields for static 3D scenes (e.g., NeRF). Several works extend these to dynamic scenes captured with monocular video, with promising performance. However, the monocular setting is known to be an under-constrained problem, and so methods rely on data-driven priors for reconstructing dynamic content. We replace these priors with measurements from a time-of-flight (ToF) camera, and introduce a neural representation based on an image formation model for continuous-wave ToF cameras. Instead of working with processed depth maps, we model the raw ToF sensor measurements to improve reconstruction quality and avoid issues with low reflectance regions, multi-path interference, and a sensor's limited unambiguous depth range. We show that this approach improves robustness of dynamic scene reconstruction to erroneous calibration and large motions, and discuss the benefits and limitations of integrating RGB+ToF sensors that are now available on modern smartphones.



### DAAS: Differentiable Architecture and Augmentation Policy Search
- **Arxiv ID**: http://arxiv.org/abs/2109.15273v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2109.15273v2)
- **Published**: 2021-09-30 17:15:17+00:00
- **Updated**: 2022-01-28 14:56:56+00:00
- **Authors**: Xiaoxing Wang, Xiangxiang Chu, Junchi Yan, Xiaokang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Neural architecture search (NAS) has been an active direction of automatic machine learning (Auto-ML), aiming to explore efficient network structures. The searched architecture is evaluated by training on datasets with fixed data augmentation policies. However, recent works on auto-augmentation show that the suited augmentation policies can vary over different structures. Therefore, this work considers the possible coupling between neural architectures and data augmentation and proposes an effective algorithm jointly searching for them. Specifically, 1) for the NAS task, we adopt a single-path based differentiable method with Gumbel-softmax reparameterization strategy due to its memory efficiency; 2) for the auto-augmentation task, we introduce a novel search method based on policy gradient algorithm, which can significantly reduce the computation complexity. Our approach achieves 97.91% accuracy on CIFAR-10 and 76.6% Top-1 accuracy on ImageNet dataset, showing the outstanding performance of our search algorithm.



### Bend-Net: Bending Loss Regularized Multitask Learning Network for Nuclei Segmentation in Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/2109.15283v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2109.15283v1)
- **Published**: 2021-09-30 17:29:44+00:00
- **Updated**: 2021-09-30 17:29:44+00:00
- **Authors**: Haotian Wang, Aleksandar Vakanski, Changfa Shi, Min Xian
- **Comment**: arXiv admin note: text overlap with arXiv:2002.01020
- **Journal**: None
- **Summary**: Separating overlapped nuclei is a major challenge in histopathology image analysis. Recently published approaches have achieved promising overall performance on nuclei segmentation; however, their performance on separating overlapped nuclei is quite limited. To address the issue, we propose a novel multitask learning network with a bending loss regularizer to separate overlapped nuclei accurately. The newly proposed multitask learning architecture enhances the generalization by learning shared representation from three tasks: instance segmentation, nuclei distance map prediction, and overlapped nuclei distance map prediction. The proposed bending loss defines high penalties to concave contour points with large curvatures, and applies small penalties to convex contour points with small curvatures. Minimizing the bending loss avoids generating contours that encompass multiple nuclei. In addition, two new quantitative metrics, Aggregated Jaccard Index of overlapped nuclei (AJIO) and Accuracy of overlapped nuclei (ACCO), are designed for the evaluation of overlapped nuclei segmentation. We validate the proposed approach on the CoNSeP and MoNuSegv1 datasets using seven quantitative metrics: Aggregate Jaccard Index, Dice, Segmentation Quality, Recognition Quality, Panoptic Quality, AJIO, and ACCO. Extensive experiments demonstrate that the proposed Bend-Net outperforms eight state-of-the-art approaches.



### Unsupervised Domain Adaptation for LiDAR Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2109.15286v2
- **DOI**: 10.1109/LRA.2022.3147326
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2109.15286v2)
- **Published**: 2021-09-30 17:30:43+00:00
- **Updated**: 2022-02-28 16:30:54+00:00
- **Authors**: Borna Bešić, Nikhil Gosala, Daniele Cattaneo, Abhinav Valada
- **Comment**: 8 pages, 3 figures
- **Journal**: IEEE Robotics and Automation Letters, Volume 7, Issue 2, April
  2022, pp. 3404-3411
- **Summary**: Scene understanding is a pivotal task for autonomous vehicles to safely navigate in the environment. Recent advances in deep learning enable accurate semantic reconstruction of the surroundings from LiDAR data. However, these models encounter a large domain gap while deploying them on vehicles equipped with different LiDAR setups which drastically decreases their performance. Fine-tuning the model for every new setup is infeasible due to the expensive and cumbersome process of recording and manually labeling new data. Unsupervised Domain Adaptation (UDA) techniques are thus essential to fill this domain gap and retain the performance of models on new sensor setups without the need for additional data labeling. In this paper, we propose AdaptLPS, a novel UDA approach for LiDAR panoptic segmentation that leverages task-specific knowledge and accounts for variation in the number of scan lines, mounting position, intensity distribution, and environmental conditions. We tackle the UDA task by employing two complementary domain adaptation strategies, data-based and model-based. While data-based adaptations reduce the domain gap by processing the raw LiDAR scans to resemble the scans in the target domain, model-based techniques guide the network in extracting features that are representative for both domains. Extensive evaluations on three pairs of real-world autonomous driving datasets demonstrate that AdaptLPS outperforms existing UDA approaches by up to 6.41 pp in terms of the PQ score.



### Identity-Disentangled Neural Deformation Model for Dynamic Meshes
- **Arxiv ID**: http://arxiv.org/abs/2109.15299v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.15299v2)
- **Published**: 2021-09-30 17:43:06+00:00
- **Updated**: 2021-10-04 22:32:48+00:00
- **Authors**: Binbin Xu, Lingni Ma, Yuting Ye, Tanner Schmidt, Christopher D. Twigg, Steven Lovegrove
- **Comment**: None
- **Journal**: None
- **Summary**: Neural shape models can represent complex 3D shapes with a compact latent space. When applied to dynamically deforming shapes such as the human hands, however, they would need to preserve temporal coherence of the deformation as well as the intrinsic identity of the subject. These properties are difficult to regularize with manually designed loss functions. In this paper, we learn a neural deformation model that disentangles the identity-induced shape variations from pose-dependent deformations using implicit neural functions. We perform template-free unsupervised learning on 3D scans without explicit mesh correspondence or semantic correspondences of shapes across subjects. We can then apply the learned model to reconstruct partial dynamic 4D scans of novel subjects performing unseen actions. We propose two methods to integrate global pose alignment with our neural deformation model. Experiments demonstrate the efficacy of our method in the disentanglement of identities and pose. Our method also outperforms traditional skeleton-driven models in reconstructing surface details such as palm prints or tendons without limitations from a fixed template.



### The Challenge of Appearance-Free Object Tracking with Feedforward Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2110.02772v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2110.02772v1)
- **Published**: 2021-09-30 17:58:53+00:00
- **Updated**: 2021-09-30 17:58:53+00:00
- **Authors**: Girik Malik, Drew Linsley, Thomas Serre, Ennio Mingolla
- **Comment**: Accepted at CVPR Workshop on Dynamic Neural Networks Meet Computer
  Vision
- **Journal**: None
- **Summary**: Nearly all models for object tracking with artificial neural networks depend on appearance features extracted from a "backbone" architecture, designed for object recognition. Indeed, significant progress on object tracking has been spurred by introducing backbones that are better able to discriminate objects by their appearance. However, extensive neurophysiology and psychophysics evidence suggests that biological visual systems track objects using both appearance and motion features. Here, we introduce $\textit{PathTracker}$, a visual challenge inspired by cognitive psychology, which tests the ability of observers to learn to track objects solely by their motion. We find that standard 3D-convolutional deep network models struggle to solve this task when clutter is introduced into the generated scenes, or when objects travel long distances. This challenge reveals that tracing the path of object motion is a blind spot of feedforward neural networks. We expect that strategies for appearance-free object tracking from biological vision can inspire solutions these failures of deep neural networks.



### Unsupervised Few-Shot Action Recognition via Action-Appearance Aligned Meta-Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2109.15317v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2109.15317v2)
- **Published**: 2021-09-30 17:59:17+00:00
- **Updated**: 2021-10-11 05:44:32+00:00
- **Authors**: Jay Patravali, Gaurav Mittal, Ye Yu, Fuxin Li, Mei Chen
- **Comment**: ICCV 2021 (Oral)
- **Journal**: None
- **Summary**: We present MetaUVFS as the first Unsupervised Meta-learning algorithm for Video Few-Shot action recognition. MetaUVFS leverages over 550K unlabeled videos to train a two-stream 2D and 3D CNN architecture via contrastive learning to capture the appearance-specific spatial and action-specific spatio-temporal video features respectively. MetaUVFS comprises a novel Action-Appearance Aligned Meta-adaptation (A3M) module that learns to focus on the action-oriented video features in relation to the appearance features via explicit few-shot episodic meta-learning over unsupervised hard-mined episodes. Our action-appearance alignment and explicit few-shot learner conditions the unsupervised training to mimic the downstream few-shot task, enabling MetaUVFS to significantly outperform all unsupervised methods on few-shot benchmarks. Moreover, unlike previous few-shot action recognition methods that are supervised, MetaUVFS needs neither base-class labels nor a supervised pretrained backbone. Thus, we need to train MetaUVFS just once to perform competitively or sometimes even outperform state-of-the-art supervised methods on popular HMDB51, UCF101, and Kinetics100 few-shot datasets.



### Sensor-Guided Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2109.15321v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2109.15321v1)
- **Published**: 2021-09-30 17:59:57+00:00
- **Updated**: 2021-09-30 17:59:57+00:00
- **Authors**: Matteo Poggi, Filippo Aleotti, Stefano Mattoccia
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: This paper proposes a framework to guide an optical flow network with external cues to achieve superior accuracy either on known or unseen domains. Given the availability of sparse yet accurate optical flow hints from an external source, these are injected to modulate the correlation scores computed by a state-of-the-art optical flow network and guide it towards more accurate predictions. Although no real sensor can provide sparse flow hints, we show how these can be obtained by combining depth measurements from active sensors with geometry and hand-crafted optical flow algorithms, leading to accurate enough hints for our purpose. Experimental results with a state-of-the-art flow network on standard benchmarks support the effectiveness of our framework, both in simulated and real conditions.



### Mining for Strong Gravitational Lenses with Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.00023v2
- **DOI**: 10.3847/1538-4357/ac6d63
- **Categories**: **astro-ph.IM**, astro-ph.CO, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.00023v2)
- **Published**: 2021-09-30 18:00:02+00:00
- **Updated**: 2022-06-21 21:24:28+00:00
- **Authors**: George Stein, Jacqueline Blaum, Peter Harrington, Tomislav Medan, Zarija Lukic
- **Comment**: 24 Pages, 15 figures, published in ApJ, data at
  github.com/georgestein/ssl-legacysurvey
- **Journal**: The Astrophysical Journal, Volume 932, Number 2, 2022
- **Summary**: We employ self-supervised representation learning to distill information from 76 million galaxy images from the Dark Energy Spectroscopic Instrument Legacy Imaging Surveys' Data Release 9. Targeting the identification of new strong gravitational lens candidates, we first create a rapid similarity search tool to discover new strong lenses given only a single labelled example. We then show how training a simple linear classifier on the self-supervised representations, requiring only a few minutes on a CPU, can automatically classify strong lenses with great efficiency. We present 1192 new strong lens candidates that we identified through a brief visual identification campaign, and release an interactive web-based similarity search tool and the top network predictions to facilitate crowd-sourcing rapid discovery of additional strong gravitational lenses and other rare objects: https://github.com/georgestein/ssl-legacysurvey.



### Learning Multi-Site Harmonization of Magnetic Resonance Images Without Traveling Human Phantoms
- **Arxiv ID**: http://arxiv.org/abs/2110.00041v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.00041v1)
- **Published**: 2021-09-30 18:39:28+00:00
- **Updated**: 2021-09-30 18:39:28+00:00
- **Authors**: Siyuan Liu, Pew-Thian Yap
- **Comment**: None
- **Journal**: None
- **Summary**: Harmonization improves data consistency and is central to effective integration of diverse imaging data acquired across multiple sites. Recent deep learning techniques for harmonization are predominantly supervised in nature and hence require imaging data of the same human subjects to be acquired at multiple sites. Data collection as such requires the human subjects to travel across sites and is hence challenging, costly, and impractical, more so when sufficient sample size is needed for reliable network training. Here we show how harmonization can be achieved with a deep neural network that does not rely on traveling human phantom data. Our method disentangles site-specific appearance information and site-invariant anatomical information from images acquired at multiple sites and then employs the disentangled information to generate the image of each subject for any target site. We demonstrate with more than 6,000 multi-site T1- and T2-weighted images that our method is remarkably effective in generating images with realistic site-specific appearances without altering anatomical details. Our method allows retrospective harmonization of data in a wide range of existing modern large-scale imaging studies, conducted via different scanners and protocols, without additional data collection.



### Sparse Quadratic Optimisation over the Stiefel Manifold with Application to Permutation Synchronisation
- **Arxiv ID**: http://arxiv.org/abs/2110.00053v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2110.00053v1)
- **Published**: 2021-09-30 19:17:35+00:00
- **Updated**: 2021-09-30 19:17:35+00:00
- **Authors**: Florian Bernard, Daniel Cremers, Johan Thunberg
- **Comment**: To appear at NeurIPS 2021
- **Journal**: None
- **Summary**: We address the non-convex optimisation problem of finding a sparse matrix on the Stiefel manifold (matrices with mutually orthogonal columns of unit length) that maximises (or minimises) a quadratic objective function. Optimisation problems on the Stiefel manifold occur for example in spectral relaxations of various combinatorial problems, such as graph matching, clustering, or permutation synchronisation. Although sparsity is a desirable property in such settings, it is mostly neglected in spectral formulations since existing solvers, e.g. based on eigenvalue decomposition, are unable to account for sparsity while at the same time maintaining global optimality guarantees. We fill this gap and propose a simple yet effective sparsity-promoting modification of the Orthogonal Iteration algorithm for finding the dominant eigenspace of a matrix. By doing so, we can guarantee that our method finds a Stiefel matrix that is globally optimal with respect to the quadratic objective function, while in addition being sparse. As a motivating application we consider the task of permutation synchronisation, which can be understood as a constrained clustering problem that has particular relevance for matching multiple images or 3D shapes in computer vision, computer graphics, and beyond. We demonstrate that the proposed approach outperforms previous methods in this domain.



### Learning to Predict Trustworthiness with Steep Slope Loss
- **Arxiv ID**: http://arxiv.org/abs/2110.00054v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.00054v2)
- **Published**: 2021-09-30 19:19:09+00:00
- **Updated**: 2021-10-27 21:28:16+00:00
- **Authors**: Yan Luo, Yongkang Wong, Mohan S. Kankanhalli, Qi Zhao
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Understanding the trustworthiness of a prediction yielded by a classifier is critical for the safe and effective use of AI models. Prior efforts have been proven to be reliable on small-scale datasets. In this work, we study the problem of predicting trustworthiness on real-world large-scale datasets, where the task is more challenging due to high-dimensional features, diverse visual concepts, and large-scale samples. In such a setting, we observe that the trustworthiness predictors trained with prior-art loss functions, i.e., the cross entropy loss, focal loss, and true class probability confidence loss, are prone to view both correct predictions and incorrect predictions to be trustworthy. The reasons are two-fold. Firstly, correct predictions are generally dominant over incorrect predictions. Secondly, due to the data complexity, it is challenging to differentiate the incorrect predictions from the correct ones on real-world large-scale datasets. To improve the generalizability of trustworthiness predictors, we propose a novel steep slope loss to separate the features w.r.t. correct predictions from the ones w.r.t. incorrect predictions by two slide-like curves that oppose each other. The proposed loss is evaluated with two representative deep learning models, i.e., Vision Transformer and ResNet, as trustworthiness predictors. We conduct comprehensive experiments and analyses on ImageNet, which show that the proposed loss effectively improves the generalizability of trustworthiness predictors. The code and pre-trained trustworthiness predictors for reproducibility are available at https://github.com/luoyan407/predict_trustworthiness.



### PubTables-1M: Towards comprehensive table extraction from unstructured documents
- **Arxiv ID**: http://arxiv.org/abs/2110.00061v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.00061v3)
- **Published**: 2021-09-30 19:42:07+00:00
- **Updated**: 2021-11-18 20:28:13+00:00
- **Authors**: Brandon Smock, Rohith Pesala, Robin Abraham
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, significant progress has been made applying machine learning to the problem of table structure inference and extraction from unstructured documents. However, one of the greatest challenges remains the creation of datasets with complete, unambiguous ground truth at scale. To address this, we develop a new, more comprehensive dataset for table extraction, called PubTables-1M. PubTables-1M contains nearly one million tables from scientific articles, supports multiple input modalities, and contains detailed header and location information for table structures, making it useful for a wide variety of modeling approaches. It also addresses a significant source of ground truth inconsistency observed in prior datasets called oversegmentation, using a novel canonicalization procedure. We demonstrate that these improvements lead to a significant increase in training performance and a more reliable estimate of model performance at evaluation for table structure recognition. Further, we show that transformer-based object detection models trained on PubTables-1M produce excellent results for all three tasks of detection, structure recognition, and functional analysis without the need for any special customization for these tasks. Data and code will be released at https://github.com/microsoft/table-transformer.



### Noise2Recon: Enabling Joint MRI Reconstruction and Denoising with Semi-Supervised and Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2110.00075v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.00075v2)
- **Published**: 2021-09-30 20:06:43+00:00
- **Updated**: 2022-10-07 08:29:45+00:00
- **Authors**: Arjun D Desai, Batu M Ozturkler, Christopher M Sandino, Robert Boutin, Marc Willis, Shreyas Vasanawala, Brian A Hargreaves, Christopher M Ré, John M Pauly, Akshay S Chaudhari
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning (DL) has shown promise for faster, high quality accelerated MRI reconstruction. However, supervised DL methods depend on extensive amounts of fully-sampled (labeled) data and are sensitive to out-of-distribution (OOD) shifts, particularly low signal-to-noise ratio (SNR) acquisitions. To alleviate this challenge, we propose Noise2Recon, a model-agnostic, consistency training method for joint MRI reconstruction and denoising that can use both fully-sampled (labeled) and undersampled (unlabeled) scans in semi-supervised and self-supervised settings. With limited or no labeled training data, Noise2Recon outperforms compressed sensing and deep learning baselines, including supervised networks, augmentation-based training, fine-tuned denoisers, and self-supervised methods, and matches performance of supervised models, which were trained with 14x more fully-sampled scans. Noise2Recon also outperforms all baselines, including state-of-the-art fine-tuning and augmentation techniques, among low-SNR scans and when generalizing to other OOD factors, such as changes in acceleration factors and different datasets. Augmentation extent and loss weighting hyperparameters had negligible impact on Noise2Recon compared to supervised methods, which may indicate increased training stability. Our code is available at https://github.com/ad12/meddlr.



### Accelerating Inverse Rendering By Using a GPU and Reuse of Light Paths
- **Arxiv ID**: http://arxiv.org/abs/2110.00085v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2110.00085v1)
- **Published**: 2021-09-30 20:53:08+00:00
- **Updated**: 2021-09-30 20:53:08+00:00
- **Authors**: Ido Czerninski, Yoav Y. Schechner
- **Comment**: 31 pages, 21 figures
- **Journal**: None
- **Summary**: Inverse rendering seeks to estimate scene characteristics from a set of data images. The dominant approach is based on differential rendering using Monte-Carlo. Algorithms as such usually rely on a forward model and use an iterative gradient method that requires sampling millions of light paths per iteration. This paper presents an efficient framework that speeds up existing inverse rendering algorithms. This is achieved by tailoring the iterative process of inverse rendering specifically to a GPU architecture. For this cause, we introduce two interleaved steps - Path Sorting and Path Recycling. Path Sorting allows the GPU to deal with light paths of the same size. Path Recycling allows the algorithm to use light paths from previous iterations to better utilize the information they encode. Together, these steps significantly speed up gradient optimization. In this paper, we give the theoretical background for Path Recycling. We demonstrate its efficiency for volumetric scattering tomography and reflectometry (surface reflections).



### Seeing Glass: Joint Point Cloud and Depth Completion for Transparent Objects
- **Arxiv ID**: http://arxiv.org/abs/2110.00087v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2110.00087v1)
- **Published**: 2021-09-30 21:09:09+00:00
- **Updated**: 2021-09-30 21:09:09+00:00
- **Authors**: Haoping Xu, Yi Ru Wang, Sagi Eppel, Alàn Aspuru-Guzik, Florian Shkurti, Animesh Garg
- **Comment**: Accepted for Oral at Conference on Robot Learning (CoRL) 2021;
  Haoping Xu and Yi Ru Wang contributed equally; 8 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: The basis of many object manipulation algorithms is RGB-D input. Yet, commodity RGB-D sensors can only provide distorted depth maps for a wide range of transparent objects due light refraction and absorption. To tackle the perception challenges posed by transparent objects, we propose TranspareNet, a joint point cloud and depth completion method, with the ability to complete the depth of transparent objects in cluttered and complex scenes, even with partially filled fluid contents within the vessels. To address the shortcomings of existing transparent object data collection schemes in literature, we also propose an automated dataset creation workflow that consists of robot-controlled image collection and vision-based automatic annotation. Through this automated workflow, we created Toronto Transparent Objects Depth Dataset (TODD), which consists of nearly 15000 RGB-D images. Our experimental evaluation demonstrates that TranspareNet outperforms existing state-of-the-art depth completion methods on multiple datasets, including ClearGrasp, and that it also handles cluttered scenes when trained on TODD. Code and dataset will be released at https://www.pair.toronto.edu/TranspareNet/



### A Survey of Selected Algorithms Used in Military Applications from the Viewpoints of Dataflow and GaAs
- **Arxiv ID**: http://arxiv.org/abs/2110.01389v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.AR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2110.01389v1)
- **Published**: 2021-09-30 21:57:41+00:00
- **Updated**: 2021-09-30 21:57:41+00:00
- **Authors**: Ilir Capuni, Veljko Milutinovic
- **Comment**: None
- **Journal**: None
- **Summary**: This is a short survey of ten algorithms that are often used for military purposes, followed by analysis of their potential suitability for dataflow and GaAs, which are a specific architecture and technology for supercomputers on a chip, respectively.   Whenever an algorithm or a device is used in military settings, it is natural to assume strict requirements related to speed, reliability, scale, energy, size, and accuracy. The two aforementioned paradigms seem to be promising in fulfilling most of these requirements.



### DeepMCAT: Large-Scale Deep Clustering for Medical Image Categorization
- **Arxiv ID**: http://arxiv.org/abs/2110.00109v1
- **DOI**: 10.1007/978-3-030-88210-5_26
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2110.00109v1)
- **Published**: 2021-09-30 22:39:57+00:00
- **Updated**: 2021-09-30 22:39:57+00:00
- **Authors**: Turkay Kart, Wenjia Bai, Ben Glocker, Daniel Rueckert
- **Comment**: Accepted for the DALI workshop at MICCAI 2021 (full oral)
- **Journal**: None
- **Summary**: In recent years, the research landscape of machine learning in medical imaging has changed drastically from supervised to semi-, weakly- or unsupervised methods. This is mainly due to the fact that ground-truth labels are time-consuming and expensive to obtain manually. Generating labels from patient metadata might be feasible but it suffers from user-originated errors which introduce biases. In this work, we propose an unsupervised approach for automatically clustering and categorizing large-scale medical image datasets, with a focus on cardiac MR images, and without using any labels. We investigated the end-to-end training using both class-balanced and imbalanced large-scale datasets. Our method was able to create clusters with high purity and achieved over 0.99 cluster purity on these datasets. The results demonstrate the potential of the proposed method for categorizing unstructured large medical databases, such as organizing clinical PACS systems in hospitals.



### Deep Learning-based Action Detection in Untrimmed Videos: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2110.00111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.00111v1)
- **Published**: 2021-09-30 22:42:25+00:00
- **Updated**: 2021-09-30 22:42:25+00:00
- **Authors**: Elahe Vahdani, Yingli Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding human behavior and activity facilitates advancement of numerous real-world applications, and is critical for video analysis. Despite the progress of action recognition algorithms in trimmed videos, the majority of real-world videos are lengthy and untrimmed with sparse segments of interest. The task of temporal activity detection in untrimmed videos aims to localize the temporal boundary of actions and classify the action categories. Temporal activity detection task has been investigated in full and limited supervision settings depending on the availability of action annotations. This paper provides an extensive overview of deep learning-based algorithms to tackle temporal action detection in untrimmed videos with different supervision levels including fully-supervised, weakly-supervised, unsupervised, self-supervised, and semi-supervised. In addition, this paper also reviews advances in spatio-temporal action detection where actions are localized in both temporal and spatial dimensions. Moreover, the commonly used action detection benchmark datasets and evaluation metrics are described, and the performance of the state-of-the-art methods are compared. Finally, real-world applications of temporal action detection in untrimmed videos and a set of future directions are discussed.



### HUMBI: A Large Multiview Dataset of Human Body Expressions and Benchmark Challenge
- **Arxiv ID**: http://arxiv.org/abs/2110.00119v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2110.00119v2)
- **Published**: 2021-09-30 23:19:25+00:00
- **Updated**: 2021-12-21 04:31:56+00:00
- **Authors**: Jae Shin Yoon, Zhixuan Yu, Jaesik Park, Hyun Soo Park
- **Comment**: 18 pages; Accepted to TPAMI
- **Journal**: None
- **Summary**: This paper presents a new large multiview dataset called HUMBI for human body expressions with natural clothing. The goal of HUMBI is to facilitate modeling view-specific appearance and geometry of five primary body signals including gaze, face, hand, body, and garment from assorted people. 107 synchronized HD cameras are used to capture 772 distinctive subjects across gender, ethnicity, age, and style. With the multiview image streams, we reconstruct high fidelity body expressions using 3D mesh models, which allows representing view-specific appearance. We demonstrate that HUMBI is highly effective in learning and reconstructing a complete human model and is complementary to the existing datasets of human body expressions with limited views and subjects such as MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets. Based on HUMBI, we formulate a new benchmark challenge of a pose-guided appearance rendering task that aims to substantially extend photorealism in modeling diverse human expressions in 3D, which is the key enabling factor of authentic social tele-presence. HUMBI is publicly available at http://humbi-data.net



### Development of the algorithm for differentiating bone metastases and trauma of the ribs in bone scintigraphy and demonstration of visual evidence of the algorithm -- Using only anterior bone scan view of thorax
- **Arxiv ID**: http://arxiv.org/abs/2110.00130v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph, Primary, I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2110.00130v1)
- **Published**: 2021-09-30 23:55:31+00:00
- **Updated**: 2021-09-30 23:55:31+00:00
- **Authors**: Shigeaki Higashiyama, Yukino Ohta, Yutaka Katayama, Atsushi Yoshida, Joji Kawabe
- **Comment**: 9 pages, 3 figures
- **Journal**: None
- **Summary**: Background: Although there are many studies on the application of artificial intelligence (AI) models to medical imaging, there is no report of an AI model that determines the accumulation of ribs in bone metastases and trauma only using the anterior image of thorax of bone scintigraphy. In recent years, a method for visualizing diagnostic grounds called Gradient-weighted Class Activation Mapping (Grad-CAM) has been proposed in the area of diagnostic images using Deep Convolutional Neural Network (DCNN). As far as we have investigated, there are no reports of visualization of the diagnostic basis in bone scintigraphy. Our aim is to visualize the area of interest of DCNN, in addition to developing an algorithm to classify and diagnose whether RI accumulation on the ribs is bone metastasis or trauma using only anterior bone scan view of thorax. Material and Methods: For this retrospective study, we used 838 patients who underwent bone scintigraphy to search for bone metastases at our institution. A frontal chest image of bone scintigraphy was used to create the algorithm. We used 437 cases with bone metastases on the ribs and 401 cases with abnormal RI accumulation due to trauma. Result: AI model was able to detect bone metastasis lesion with a sensitivity of 90.00% and accuracy of 86.5%. And it was possible to visualize the part that the AI model focused on with Grad-CAM.



