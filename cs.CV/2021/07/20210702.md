# Arxiv Papers in cs.CV on 2021-07-02
### UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.00781v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00781v2)
- **Published**: 2021-07-02 00:56:27+00:00
- **Updated**: 2021-09-28 00:52:51+00:00
- **Authors**: Yunhe Gao, Mu Zhou, Dimitris Metaxas
- **Comment**: Accepted by MICCAI 2021. Code: https://github.com/yhygao/UTNet
- **Journal**: None
- **Summary**: Transformer architecture has emerged to be successful in a number of natural language processing tasks. However, its applications to medical vision remain largely unexplored. In this study, we present UTNet, a simple yet powerful hybrid Transformer architecture that integrates self-attention into a convolutional neural network for enhancing medical image segmentation. UTNet applies self-attention modules in both encoder and decoder for capturing long-range dependency at different scales with minimal overhead. To this end, we propose an efficient self-attention mechanism along with relative position encoding that reduces the complexity of self-attention operation significantly from $O(n^2)$ to approximate $O(n)$. A new self-attention decoder is also proposed to recover fine-grained details from the skipped connections in the encoder. Our approach addresses the dilemma that Transformer requires huge amounts of data to learn vision inductive bias. Our hybrid layer design allows the initialization of Transformer into convolutional networks without a need of pre-training. We have evaluated UTNet on the multi-label, multi-vendor cardiac magnetic resonance imaging cohort. UTNet demonstrates superior segmentation performance and robustness against the state-of-the-art approaches, holding the promise to generalize well on other medical image segmentations.



### Polarized Self-Attention: Towards High-quality Pixel-wise Regression
- **Arxiv ID**: http://arxiv.org/abs/2107.00782v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00782v2)
- **Published**: 2021-07-02 01:03:11+00:00
- **Updated**: 2021-07-08 15:33:31+00:00
- **Authors**: Huajun Liu, Fuqiang Liu, Xinyi Fan, Dong Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Pixel-wise regression is probably the most common problem in fine-grained computer vision tasks, such as estimating keypoint heatmaps and segmentation masks. These regression problems are very challenging particularly because they require, at low computation overheads, modeling long-range dependencies on high-resolution inputs/outputs to estimate the highly nonlinear pixel-wise semantics. While attention mechanisms in Deep Convolutional Neural Networks(DCNNs) has become popular for boosting long-range dependencies, element-specific attention, such as Nonlocal blocks, is highly complex and noise-sensitive to learn, and most of simplified attention hybrids try to reach the best compromise among multiple types of tasks. In this paper, we present the Polarized Self-Attention(PSA) block that incorporates two critical designs towards high-quality pixel-wise regression: (1) Polarized filtering: keeping high internal resolution in both channel and spatial attention computation while completely collapsing input tensors along their counterpart dimensions. (2) Enhancement: composing non-linearity that directly fits the output distribution of typical fine-grained regression, such as the 2D Gaussian distribution (keypoint heatmaps), or the 2D Binormial distribution (binary segmentation masks). PSA appears to have exhausted the representation capacity within its channel-only and spatial-only branches, such that there is only marginal metric differences between its sequential and parallel layouts. Experimental results show that PSA boosts standard baselines by $2-4$ points, and boosts state-of-the-arts by $1-2$ points on 2D pose estimation and semantic segmentation benchmarks.



### Disentangling Transfer and Interference in Multi-Domain Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.05445v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.05445v4)
- **Published**: 2021-07-02 01:30:36+00:00
- **Updated**: 2022-01-14 22:41:18+00:00
- **Authors**: Yipeng Zhang, Tyler L. Hayes, Christopher Kanan
- **Comment**: AAAI 2022 PracticalDL Workshop
- **Journal**: None
- **Summary**: Humans are incredibly good at transferring knowledge from one domain to another, enabling rapid learning of new tasks. Likewise, transfer learning has enabled enormous success in many computer vision problems using pretraining. However, the benefits of transfer in multi-domain learning, where a network learns multiple tasks defined by different datasets, has not been adequately studied. Learning multiple domains could be beneficial, or these domains could interfere with each other given limited network capacity. Understanding how deep neural networks of varied capacity facilitate transfer across inputs from different distributions is a critical step towards open world learning. In this work, we decipher the conditions where interference and knowledge transfer occur in multi-domain learning. We propose new metrics disentangling interference and transfer, set up experimental protocols, and examine the roles of network capacity, task grouping, and dynamic loss weighting in reducing interference and facilitating transfer.



### Case Relation Transformer: A Crossmodal Language Generation Model for Fetching Instructions
- **Arxiv ID**: http://arxiv.org/abs/2107.00789v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00789v1)
- **Published**: 2021-07-02 01:40:33+00:00
- **Updated**: 2021-07-02 01:40:33+00:00
- **Authors**: Motonari Kambara, Komei Sugiura
- **Comment**: Accepted for presentation at IROS2021
- **Journal**: None
- **Summary**: There have been many studies in robotics to improve the communication skills of domestic service robots. Most studies, however, have not fully benefited from recent advances in deep neural networks because the training datasets are not large enough. In this paper, our aim is to augment the datasets based on a crossmodal language generation model. We propose the Case Relation Transformer (CRT), which generates a fetching instruction sentence from an image, such as "Move the blue flip-flop to the lower left box." Unlike existing methods, the CRT uses the Transformer to integrate the visual features and geometry features of objects in the image. The CRT can handle the objects because of the Case Relation Block. We conducted comparison experiments and a human evaluation. The experimental results show the CRT outperforms baseline methods.



### MMF: Multi-Task Multi-Structure Fusion for Hierarchical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.00808v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00808v1)
- **Published**: 2021-07-02 02:53:35+00:00
- **Updated**: 2021-07-02 02:53:35+00:00
- **Authors**: Xiaoni Li, Yucan Zhou, Yu Zhou, Weiping Wang
- **Comment**: Accpeted by ICANN 2021
- **Journal**: None
- **Summary**: Hierarchical classification is significant for complex tasks by providing multi-granular predictions and encouraging better mistakes. As the label structure decides its performance, many existing approaches attempt to construct an excellent label structure for promoting the classification results. In this paper, we consider that different label structures provide a variety of prior knowledge for category recognition, thus fusing them is helpful to achieve better hierarchical classification results. Furthermore, we propose a multi-task multi-structure fusion model to integrate different label structures. It contains two kinds of branches: one is the traditional classification branch to classify the common subclasses, the other is responsible for identifying the heterogeneous superclasses defined by different label structures. Besides the effect of multiple label structures, we also explore the architecture of the deep model for better hierachical classification and adjust the hierarchical evaluation metrics for multiple label structures. Experimental results on CIFAR100 and Car196 show that our method obtains significantly better results than using a flat classifier or a hierarchical classifier with any single label structure.



### Convolutional Neural Bandit for Visual-aware Recommendation
- **Arxiv ID**: http://arxiv.org/abs/2107.07438v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.07438v2)
- **Published**: 2021-07-02 03:02:29+00:00
- **Updated**: 2022-02-09 21:24:48+00:00
- **Authors**: Yikun Ban, Jingrui He
- **Comment**: In submission
- **Journal**: None
- **Summary**: Online recommendation/advertising is ubiquitous in web business. Image displaying is considered as one of the most commonly used formats to interact with customers. Contextual multi-armed bandit has shown success in the application of advertising to solve the exploration-exploitation dilemma existing in the recommendation procedure. Inspired by the visual-aware recommendation, in this paper, we propose a contextual bandit algorithm, where the convolutional neural network (CNN) is utilized to learn the reward function along with an upper confidence bound (UCB) for exploration. We also prove a near-optimal regret bound $\tilde{\mathcal{O}}(\sqrt{T})$ when the network is over-parameterized, and establish strong connections with convolutional neural tangent kernel (CNTK). Finally, we evaluate the empirical performance of the proposed algorithm and show that it outperforms other state-of-the-art UCB-based bandit algorithms on real-world image data sets.



### Target-dependent UNITER: A Transformer-Based Multimodal Language Comprehension Model for Domestic Service Robots
- **Arxiv ID**: http://arxiv.org/abs/2107.00811v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00811v1)
- **Published**: 2021-07-02 03:11:02+00:00
- **Updated**: 2021-07-02 03:11:02+00:00
- **Authors**: Shintaro Ishikawa, Komei Sugiura
- **Comment**: Accepted for presentation at IROS2021
- **Journal**: None
- **Summary**: Currently, domestic service robots have an insufficient ability to interact naturally through language. This is because understanding human instructions is complicated by various ambiguities and missing information. In existing methods, the referring expressions that specify the relationships between objects are insufficiently modeled. In this paper, we propose Target-dependent UNITER, which learns the relationship between the target object and other objects directly by focusing on the relevant regions within an image, rather than the whole image. Our method is an extension of the UNITER-based Transformer that can be pretrained on general-purpose datasets. We extend the UNITER approach by introducing a new architecture for handling the target candidates. Our model is validated on two standard datasets, and the results show that Target-dependent UNITER outperforms the baseline method in terms of classification accuracy.



### 1st Place Solutions for UG2+ Challenge 2021 -- (Semi-)supervised Face detection in the low light condition
- **Arxiv ID**: http://arxiv.org/abs/2107.00818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00818v1)
- **Published**: 2021-07-02 04:12:23+00:00
- **Updated**: 2021-07-02 04:12:23+00:00
- **Authors**: Pengcheng Wang, Lingqiao Ji, Zhilong Ji, Yuan Gao, Xiao Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In this technical report, we briefly introduce the solution of our team "TAL-ai" for (Semi-) supervised Face detection in the low light condition in UG2+ Challenge in CVPR 2021. By conducting several experiments with popular image enhancement methods and image transfer methods, we pulled the low light image and the normal image to a more closer domain. And it is observed that using these data to training can achieve better performance. We also adapt several popular object detection frameworks, e.g., DetectoRS, Cascade-RCNN, and large backbone like Swin-transformer. Finally, we ensemble several models which achieved mAP 74.89 on the testing set, ranking 1st on the final leaderboard.



### Cross-view Geo-localization with Evolving Transformer
- **Arxiv ID**: http://arxiv.org/abs/2107.00842v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00842v2)
- **Published**: 2021-07-02 05:33:14+00:00
- **Updated**: 2021-07-05 02:23:48+00:00
- **Authors**: Hongji Yang, Xiufan Lu, Yingying Zhu
- **Comment**: Under Review
- **Journal**: None
- **Summary**: In this work, we address the problem of cross-view geo-localization, which estimates the geospatial location of a street view image by matching it with a database of geo-tagged aerial images. The cross-view matching task is extremely challenging due to drastic appearance and geometry differences across views. Unlike existing methods that predominantly fall back on CNN, here we devise a novel evolving geo-localization Transformer (EgoTR) that utilizes the properties of self-attention in Transformer to model global dependencies, thus significantly decreasing visual ambiguities in cross-view geo-localization. We also exploit the positional encoding of Transformer to help the EgoTR understand and correspond geometric configurations between ground and aerial images. Compared to state-of-the-art methods that impose strong assumption on geometry knowledge, the EgoTR flexibly learns the positional embeddings through the training objective and hence becomes more practical in many real-world scenarios. Although Transformer is well suited to our task, its vanilla self-attention mechanism independently interacts within image patches in each layer, which overlooks correlations between layers. Instead, this paper propose a simple yet effective self-cross attention mechanism to improve the quality of learned representations. The self-cross attention models global dependencies between adjacent layers, which relates between image patches while modeling how features evolve in the previous layer. As a result, the proposed self-cross attention leads to more stable training, improves the generalization ability and encourages representations to keep evolving as the network goes deeper. Extensive experiments demonstrate that our EgoTR performs favorably against state-of-the-art methods on standard, fine-grained and cross-dataset cross-view geo-localization tasks.



### Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets
- **Arxiv ID**: http://arxiv.org/abs/2107.00860v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00860v1)
- **Published**: 2021-07-02 06:33:59+00:00
- **Updated**: 2021-07-02 06:33:59+00:00
- **Authors**: Hayeon Lee, Eunyoung Hyung, Sung Ju Hwang
- **Comment**: ICLR 2021
- **Journal**: None
- **Summary**: Despite the success of recent Neural Architecture Search (NAS) methods on various tasks which have shown to output networks that largely outperform human-designed networks, conventional NAS methods have mostly tackled the optimization of searching for the network architecture for a single task (dataset), which does not generalize well across multiple tasks (datasets). Moreover, since such task-specific methods search for a neural architecture from scratch for every given task, they incur a large computational cost, which is problematic when the time and monetary budget are limited. In this paper, we propose an efficient NAS framework that is trained once on a database consisting of datasets and pretrained networks and can rapidly search for a neural architecture for a novel dataset. The proposed MetaD2A (Meta Dataset-to-Architecture) model can stochastically generate graphs (architectures) from a given set (dataset) via a cross-modal latent space learned with amortized meta-learning. Moreover, we also propose a meta-performance predictor to estimate and select the best architecture without direct training on target datasets. The experimental results demonstrate that our model meta-learned on subsets of ImageNet-1K and architectures from NAS-Bench 201 search space successfully generalizes to multiple unseen datasets including CIFAR-10 and CIFAR-100, with an average search time of 33 GPU seconds. Even under MobileNetV3 search space, MetaD2A is 5.5K times faster than NSGANetV2, a transferable NAS method, with comparable performance. We believe that the MetaD2A proposes a new research direction for rapid NAS as well as ways to utilize the knowledge from rich databases of datasets and architectures accumulated over the past years. Code is available at https://github.com/HayeonLee/MetaD2A.



### Deep Mesh Prior: Unsupervised Mesh Restoration using Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.02909v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2107.02909v2)
- **Published**: 2021-07-02 07:21:10+00:00
- **Updated**: 2021-11-01 07:17:15+00:00
- **Authors**: Shota Hattori, Tatsuya Yatagawa, Yutaka Ohtake, Hiromasa Suzuki
- **Comment**: 10 pages, 9 figures and 2 tables
- **Journal**: None
- **Summary**: This paper addresses mesh restoration problems, i.e., denoising and completion, by learning self-similarity in an unsupervised manner. For this purpose, the proposed method, which we refer to as Deep Mesh Prior, uses a graph convolutional network on meshes to learn the self-similarity. The network takes a single incomplete mesh as input data and directly outputs the reconstructed mesh without being trained using large-scale datasets. Our method does not use any intermediate representations such as an implicit field because the whole process works on a mesh. We demonstrate that our unsupervised method performs equally well or even better than the state-of-the-art methods using large-scale datasets.



### LensID: A CNN-RNN-Based Framework Towards Lens Irregularity Detection in Cataract Surgery Videos
- **Arxiv ID**: http://arxiv.org/abs/2107.00875v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00875v1)
- **Published**: 2021-07-02 07:27:29+00:00
- **Updated**: 2021-07-02 07:27:29+00:00
- **Authors**: Negin Ghamsarian, Mario Taschwer, Doris Putzgruber-Adamitsch, Stephanie Sarny, Yosuf El-Shabrawi, Klaus Schoeffmann
- **Comment**: 13 pages, 5 figures, accepted at 24th international conference on
  Medical Image Computing & Computer Assisted Intervention (MICCAI 2021)
- **Journal**: None
- **Summary**: A critical complication after cataract surgery is the dislocation of the lens implant leading to vision deterioration and eye trauma. In order to reduce the risk of this complication, it is vital to discover the risk factors during the surgery. However, studying the relationship between lens dislocation and its suspicious risk factors using numerous videos is a time-extensive procedure. Hence, the surgeons demand an automatic approach to enable a larger-scale and, accordingly, more reliable study. In this paper, we propose a novel framework as the major step towards lens irregularity detection. In particular, we propose (I) an end-to-end recurrent neural network to recognize the lens-implantation phase and (II) a novel semantic segmentation network to segment the lens and pupil after the implantation phase. The phase recognition results reveal the effectiveness of the proposed surgical phase recognition approach. Moreover, the segmentation results confirm the proposed segmentation network's effectiveness compared to state-of-the-art rival approaches.



### HO-3D_v3: Improving the Accuracy of Hand-Object Annotations of the HO-3D Dataset
- **Arxiv ID**: http://arxiv.org/abs/2107.00887v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2107.00887v1)
- **Published**: 2021-07-02 08:06:36+00:00
- **Updated**: 2021-07-02 08:06:36+00:00
- **Authors**: Shreyas Hampali, Sayan Deb Sarkar, Vincent Lepetit
- **Comment**: None
- **Journal**: None
- **Summary**: HO-3D is a dataset providing image sequences of various hand-object interaction scenarios annotated with the 3D pose of the hand and the object and was originally introduced as HO-3D_v2. The annotations were obtained automatically using an optimization method, 'HOnnotate', introduced in the original paper. HO-3D_v3 provides more accurate annotations for both the hand and object poses thus resulting in better estimates of contact regions between the hand and the object. In this report, we elaborate on the improvements to the HOnnotate method and provide evaluations to compare the accuracy of HO-3D_v2 and HO-3D_v3. HO-3D_v3 results in 4mm higher accuracy compared to HO-3D_v2 for hand poses while exhibiting higher contact regions with the object surface.



### MSN: Multi-Style Network for Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2107.00932v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00932v5)
- **Published**: 2021-07-02 09:43:59+00:00
- **Updated**: 2023-05-08 07:30:35+00:00
- **Authors**: Conghao Wong, Beihao Xia, Qinmu Peng, Wei Yuan, Xinge You
- **Comment**: Accepted by IEEE Transactions on Intelligent Transportation Systems
- **Journal**: None
- **Summary**: Trajectory prediction aims to forecast agents' possible future locations considering their observations along with the video context. It is strongly needed by many autonomous platforms like tracking, detection, robot navigation, and self-driving cars. Whether it is agents' internal personality factors, interactive behaviors with the neighborhood, or the influence of surroundings, they all impact agents' future planning. However, many previous methods model and predict agents' behaviors with the same strategy or feature distribution, making them challenging to make predictions with sufficient style differences. This paper proposes the Multi-Style Network (MSN), which utilizes style proposal and stylized prediction using two sub-networks, to provide multi-style predictions in a novel categorical way adaptively. The proposed network contains a series of style channels, and each channel is bound to a unique and specific behavior style. We use agents' end-point plannings and their interaction context as the basis for the behavior classification, so as to adaptively learn multiple diverse behavior styles through these channels. Then, we assume that the target agents may plan their future behaviors according to each of these categorized styles, thus utilizing different style channels to make predictions with significant style differences in parallel. Experiments show that the proposed MSN outperforms current state-of-the-art methods up to 10% quantitatively on two widely used datasets, and presents better multi-style characteristics qualitatively.



### Hybrid Supervision Learning for Pathology Whole Slide Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.00934v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00934v3)
- **Published**: 2021-07-02 09:46:06+00:00
- **Updated**: 2021-10-25 06:45:28+00:00
- **Authors**: Jiahui Li, Wen Chen, Xiaodi Huang, Zhiqiang Hu, Qi Duan, Hongsheng Li, Dimitris N. Metaxas, Shaoting Zhang
- **Comment**: Accepted in MICCAI2021
- **Journal**: None
- **Summary**: Weak supervision learning on classification labels has demonstrated high performance in various tasks, while a few pixel-level fine annotations are also affordable. Naturally a question comes to us that whether the combination of pixel-level (e.g., segmentation) and image level (e.g., classification) annotation can introduce further improvement. However in computational pathology this is a difficult task for this reason: High resolution of whole slide images makes it difficult to do end-to-end classification model training, which is challenging to research of weak or hybrid supervision learning in the past. To handle this problem, we propose a hybrid supervision learning framework for this kind of high resolution images with sufficient image-level coarse annotations and a few pixel-level fine labels. This framework, when applied in training patch model, can carefully make use of coarse image-level labels to refine generated pixel-level pseudo labels. Complete strategy is proposed to suppress pixel-level false positives and false negatives. A large hybrid annotated dataset is used to evaluate the effectiveness of hybrid supervision learning. By extracting pixel-level pseudo labels in initially image-level labeled samples, we achieve 5.2% higher specificity than purely training on existing labels while retaining 100% sensitivity, in the task of image-level classification to be positive or negative.



### ResIST: Layer-Wise Decomposition of ResNets for Distributed Training
- **Arxiv ID**: http://arxiv.org/abs/2107.00961v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2107.00961v2)
- **Published**: 2021-07-02 10:48:50+00:00
- **Updated**: 2022-03-14 14:21:25+00:00
- **Authors**: Chen Dun, Cameron R. Wolfe, Christopher M. Jermaine, Anastasios Kyrillidis
- **Comment**: 26 pages, 8 figures, pre-print under review
- **Journal**: None
- **Summary**: We propose ResIST, a novel distributed training protocol for Residual Networks (ResNets). ResIST randomly decomposes a global ResNet into several shallow sub-ResNets that are trained independently in a distributed manner for several local iterations, before having their updates synchronized and aggregated into the global model. In the next round, new sub-ResNets are randomly generated and the process repeats until convergence. By construction, per iteration, ResIST communicates only a small portion of network parameters to each machine and never uses the full model during training. Thus, ResIST reduces the per-iteration communication, memory, and time requirements of ResNet training to only a fraction of the requirements of full-model training. In comparison to common protocols, like data-parallel training and data-parallel training with local SGD, ResIST yields a decrease in communication and compute requirements, while being competitive with respect to model performance.



### Evaluating the Usefulness of Unsupervised monitoring in Cultural Heritage Monuments
- **Arxiv ID**: http://arxiv.org/abs/2107.00964v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.00964v1)
- **Published**: 2021-07-02 10:51:28+00:00
- **Updated**: 2021-07-02 10:51:28+00:00
- **Authors**: Charalampos Zafeiropoulos, Ioannis N. Tzortzis, Ioannis Rallis, Eftychios Protopapadakis, Nikolaos Doulamis, Anastasios Doulamis
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we scrutinize the effectiveness of various clustering techniques, investigating their applicability in Cultural Heritage monitoring applications. In the context of this paper, we detect the level of decomposition and corrosion on the walls of Saint Nicholas fort in Rhodes utilizing hyperspectral images. A total of 6 different clustering approaches have been evaluated over a set of 14 different orthorectified hyperspectral images. Experimental setup in this study involves K-means, Spectral, Meanshift, DBSCAN, Birch and Optics algorithms. For each of these techniques we evaluate its performance by the use of performance metrics such as Calinski-Harabasz, Davies-Bouldin indexes and Silhouette value. In this approach, we evaluate the outcomes of the clustering methods by comparing them with a set of annotated images which denotes the ground truth regarding the decomposition and/or corrosion area of the original images. The results depict that a few clustering techniques applied on the given dataset succeeded decent accuracy, precision, recall and f1 scores. Eventually, it was observed that the deterioration was detected quite accurately.



### Parasitic Egg Detection and Classification in Low-cost Microscopic Images using Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.00968v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.00968v1)
- **Published**: 2021-07-02 11:05:45+00:00
- **Updated**: 2021-07-02 11:05:45+00:00
- **Authors**: Thanaphon Suwannaphong, Sawaphob Chavana, Sahapol Tongsom, Duangdao Palasuwan, Thanarat H. Chalidabhongse, Nantheera Anantrasirichai
- **Comment**: 7 pages, 9 figures, Preprint submitted to Elsevier
- **Journal**: None
- **Summary**: Intestinal parasitic infection leads to several morbidities to humans worldwide, especially in tropical countries. The traditional diagnosis usually relies on manual analysis from microscopic images which is prone to human error due to morphological similarity of different parasitic eggs and abundance of impurities in a sample. Many studies have developed automatic systems for parasite egg detection to reduce human workload. However, they work with high quality microscopes, which unfortunately remain unaffordable in some rural areas. Our work thus exploits a benefit of a low-cost USB microscope. This instrument however provides poor quality of images due to limitation of magnification (10x), causing difficulty in parasite detection and species classification. In this paper, we propose a CNN-based technique using transfer learning strategy to enhance the efficiency of automatic parasite classification in poor-quality microscopic images. The patch-based technique with sliding window is employed to search for location of the eggs. Two networks, AlexNet and ResNet50, are examined with a trade-off between architecture size and classification performance. The results show that our proposed framework outperforms the state-of-the-art object recognition methods. Our system combined with final decision from an expert may improve the real faecal examination with low-cost microscopes.



### Ultrasound Video Transformers for Cardiac Ejection Fraction Estimation
- **Arxiv ID**: http://arxiv.org/abs/2107.00977v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00977v1)
- **Published**: 2021-07-02 11:23:09+00:00
- **Updated**: 2021-07-02 11:23:09+00:00
- **Authors**: Hadrien Reynaud, Athanasios Vlontzos, Benjamin Hou, Arian Beqiri, Paul Leeson, Bernhard Kainz
- **Comment**: Accepted for MICCAI 2021
- **Journal**: None
- **Summary**: Cardiac ultrasound imaging is used to diagnose various heart diseases. Common analysis pipelines involve manual processing of the video frames by expert clinicians. This suffers from intra- and inter-observer variability. We propose a novel approach to ultrasound video analysis using a transformer architecture based on a Residual Auto-Encoder Network and a BERT model adapted for token classification. This enables videos of any length to be processed. We apply our model to the task of End-Systolic (ES) and End-Diastolic (ED) frame detection and the automated computation of the left ventricular ejection fraction. We achieve an average frame distance of 3.36 frames for the ES and 7.17 frames for the ED on videos of arbitrary length. Our end-to-end learnable approach can estimate the ejection fraction with a MAE of 5.95 and $R^2$ of 0.52 in 0.15s per video, showing that segmentation is not the only way to predict ejection fraction. Code and models are available at https://github.com/HReynaud/UVT.



### Blind Image Super-resolution with Elaborate Degradation Modeling on Noise and Kernel
- **Arxiv ID**: http://arxiv.org/abs/2107.00986v4
- **DOI**: None
- **Categories**: **cs.CV**, I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/2107.00986v4)
- **Published**: 2021-07-02 11:55:40+00:00
- **Updated**: 2022-03-16 12:40:58+00:00
- **Authors**: Zongsheng Yue, Qian Zhao, Jianwen Xie, Lei Zhang, Deyu Meng, Kwan-Yee K. Wong
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: While researches on model-based blind single image super-resolution (SISR) have achieved tremendous successes recently, most of them do not consider the image degradation sufficiently. Firstly, they always assume image noise obeys an independent and identically distributed (i.i.d.) Gaussian or Laplacian distribution, which largely underestimates the complexity of real noise. Secondly, previous commonly-used kernel priors (e.g., normalization, sparsity) are not effective enough to guarantee a rational kernel solution, and thus degenerates the performance of subsequent SISR task. To address the above issues, this paper proposes a model-based blind SISR method under the probabilistic framework, which elaborately models image degradation from the perspectives of noise and blur kernel. Specifically, instead of the traditional i.i.d. noise assumption, a patch-based non-i.i.d. noise model is proposed to tackle the complicated real noise, expecting to increase the degrees of freedom of the model for noise representation. As for the blur kernel, we novelly construct a concise yet effective kernel generator, and plug it into the proposed blind SISR method as an explicit kernel prior (EKP). To solve the proposed model, a theoretically grounded Monte Carlo EM algorithm is specifically designed. Comprehensive experiments demonstrate the superiority of our method over current state-of-the-arts on synthetic and real datasets. The source code is available at https://github.com/zsyOAOA/BSRDM.



### Sub-millisecond Video Synchronization of Multiple Android Smartphones
- **Arxiv ID**: http://arxiv.org/abs/2107.00987v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00987v2)
- **Published**: 2021-07-02 11:56:33+00:00
- **Updated**: 2021-08-26 16:56:03+00:00
- **Authors**: Azat Akhmetyanov, Anastasiia Kornilova, Marsel Faizullin, David Pozo, Gonzalo Ferrer
- **Comment**: Accepted to conference IEEE Sensors'2021 as Lecture presentation
- **Journal**: None
- **Summary**: This paper addresses the problem of building an affordable easy-to-setup synchronized multi-view camera system, which is in demand for many Computer Vision and Robotics applications in high-dynamic environments. In our work, we propose a solution for this problem -- a publicly-available Android application for synchronized video recording on multiple smartphones with sub-millisecond accuracy. We present a generalized mathematical model of timestamping for Android smartphones and prove its applicability on 47 different physical devices. Also, we estimate the time drift parameter for those smartphones, which is less than 1.2 msec per minute for most of the considered devices, that makes smartphones' camera system a worthy analog for professional multi-view systems. Finally, we demonstrate Android-app performance on the camera system built from Android smartphones quantitatively on setup with lights and qualitatively -- on panorama stitching task.



### Optical Braille Recognition using Circular Hough Transform
- **Arxiv ID**: http://arxiv.org/abs/2107.00993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00993v1)
- **Published**: 2021-07-02 12:15:24+00:00
- **Updated**: 2021-07-02 12:15:24+00:00
- **Authors**: Zeba Khanam, Atiya Usmani
- **Comment**: None
- **Journal**: None
- **Summary**: Braille has empowered visually challenged community to read and write. But at the same time, it has created a gap due to widespread inability of non-Braille users to understand Braille scripts. This gap has fuelled researchers to propose Optical Braille Recognition techniques to convert Braille documents to natural language. The main motivation of this work is to cement the communication gap at academic institutions by translating personal documents of blind students. This has been accomplished by proposing an economical and effective technique which digitizes Braille documents using a smartphone camera. For any given Braille image, a dot detection mechanism based on Hough transform is proposed which is invariant to skewness, noise and other deterrents. The detected dots are then clustered into Braille cells using distance-based clustering algorithm. In succession, the standard physical parameters of each Braille cells are estimated for feature extraction and classification as natural language characters. The comprehensive evaluation of this technique on the proposed dataset of 54 Braille scripts has yielded into accuracy of 98.71%.



### Magnification-independent Histopathological Image Classification with Similarity-based Multi-scale Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2107.01063v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.01063v2)
- **Published**: 2021-07-02 13:18:45+00:00
- **Updated**: 2022-08-31 07:36:19+00:00
- **Authors**: Yibao Sun, Xingru Huang, Yaqi Wang, Huiyu Zhou, Qianni Zhang
- **Comment**: more experiments and author list updated
- **Journal**: None
- **Summary**: The classification of histopathological images is of great value in both cancer diagnosis and pathological studies. However, multiple reasons, such as variations caused by magnification factors and class imbalance, make it a challenging task where conventional methods that learn from image-label datasets perform unsatisfactorily in many cases. We observe that tumours of the same class often share common morphological patterns. To exploit this fact, we propose an approach that learns similarity-based multi-scale embeddings (SMSE) for magnification-independent histopathological image classification. In particular, a pair loss and a triplet loss are leveraged to learn similarity-based embeddings from image pairs or image triplets. The learned embeddings provide accurate measurements of similarities between images, which are regarded as a more effective form of representation for histopathological morphology than normal image features. Furthermore, in order to ensure the generated models are magnification-independent, images acquired at different magnification factors are simultaneously fed to networks during training for learning multi-scale embeddings. In addition to the SMSE, to eliminate the impact of class imbalance, instead of using the hard sample mining strategy that intuitively discards some easy samples, we introduce a new reinforced focal loss to simultaneously punish hard misclassified samples while suppressing easy well-classified samples. Experimental results show that the SMSE improves the performance for histopathological image classification tasks for both breast and liver cancers by a large margin compared to previous methods. In particular, the SMSE achieves the best performance on the BreakHis benchmark with an improvement ranging from 5% to 18% compared to previous methods using traditional features.



### Cooperative Training and Latent Space Data Augmentation for Robust Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.01079v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2107.01079v1)
- **Published**: 2021-07-02 13:39:13+00:00
- **Updated**: 2021-07-02 13:39:13+00:00
- **Authors**: Chen Chen, Kerstin Hammernik, Cheng Ouyang, Chen Qin, Wenjia Bai, Daniel Rueckert
- **Comment**: MICCAI 2021
- **Journal**: None
- **Summary**: Deep learning-based segmentation methods are vulnerable to unforeseen data distribution shifts during deployment, e.g. change of image appearances or contrasts caused by different scanners, unexpected imaging artifacts etc. In this paper, we present a cooperative framework for training image segmentation models and a latent space augmentation method for generating hard examples. Both contributions improve model generalization and robustness with limited data. The cooperative training framework consists of a fast-thinking network (FTN) and a slow-thinking network (STN). The FTN learns decoupled image features and shape features for image reconstruction and segmentation tasks. The STN learns shape priors for segmentation correction and refinement. The two networks are trained in a cooperative manner. The latent space augmentation generates challenging examples for training by masking the decoupled latent space in both channel-wise and spatial-wise manners. We performed extensive experiments on public cardiac imaging datasets. Using only 10 subjects from a single site for training, we demonstrated improved cross-site segmentation performance and increased robustness against various unforeseen imaging artifacts compared to strong baseline methods. Particularly, cooperative training with latent space data augmentation yields 15% improvement in terms of average Dice score when compared to a standard training method.



### Comparison of end-to-end neural network architectures and data augmentation methods for automatic infant motility assessment using wearable sensors
- **Arxiv ID**: http://arxiv.org/abs/2107.01086v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2107.01086v1)
- **Published**: 2021-07-02 14:02:05+00:00
- **Updated**: 2021-07-02 14:02:05+00:00
- **Authors**: Manu Airaksinen, Sampsa Vanhatalo, Okko Räsänen
- **Comment**: None
- **Journal**: None
- **Summary**: Infant motility assessment using intelligent wearables is a promising new approach for assessment of infant neurophysiological development, and where efficient signal analysis plays a central role. This study investigates the use of different end-to-end neural network architectures for processing infant motility data from wearable sensors. We focus on the performance and computational burden of alternative sensor encoder and time-series modelling modules and their combinations. In addition, we explore the benefits of data augmentation methods in ideal and non-ideal recording conditions. The experiments are conducted using a data-set of multi-sensor movement recordings from 7-month-old infants, as captured by a recently proposed smart jumpsuit for infant motility assessment. Our results indicate that the choice of the encoder module has a major impact on classifier performance. For sensor encoders, the best performance was obtained with parallel 2-dimensional convolutions for intra-sensor channel fusion with shared weights for all sensors. The results also indicate that a relatively compact feature representation is obtainable for within-sensor feature extraction without a drastic loss to classifier performance. Comparison of time-series models revealed that feed-forward dilated convolutions with residual and skip connections outperformed all RNN-based models in performance, training time, and training stability. The experiments also indicate that data augmentation improves model robustness in simulated packet loss or sensor dropout scenarios. In particular, signal- and sensor-dropout-based augmentation strategies provided considerable boosts to performance without negatively affecting the baseline performance. Overall the results provide tangible suggestions on how to optimize end-to-end neural network training for multi-channel movement sensor data.



### On Measuring and Controlling the Spectral Bias of the Deep Image Prior
- **Arxiv ID**: http://arxiv.org/abs/2107.01125v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.01125v3)
- **Published**: 2021-07-02 15:10:42+00:00
- **Updated**: 2021-12-30 07:21:39+00:00
- **Authors**: Zenglin Shi, Pascal Mettes, Subhransu Maji, Cees G. M. Snoek
- **Comment**: IJCV 2022; Spectral bias; Deep image prior; 24 pages
- **Journal**: None
- **Summary**: The deep image prior showed that a randomly initialized network with a suitable architecture can be trained to solve inverse imaging problems by simply optimizing it's parameters to reconstruct a single degraded image. However, it suffers from two practical limitations. First, it remains unclear how to control the prior beyond the choice of the network architecture. Second, training requires an oracle stopping criterion as during the optimization the performance degrades after reaching an optimum value. To address these challenges we introduce a frequency-band correspondence measure to characterize the spectral bias of the deep image prior, where low-frequency image signals are learned faster and better than high-frequency counterparts. Based on our observations, we propose techniques to prevent the eventual performance degradation and accelerate convergence. We introduce a Lipschitz-controlled convolution layer and a Gaussian-controlled upsampling layer as plug-in replacements for layers used in the deep architectures. The experiments show that with these changes the performance does not degrade during optimization, relieving us from the need for an oracle stopping criterion. We further outline a stopping criterion to avoid superfluous computation. Finally, we show that our approach obtains favorable results compared to current approaches across various denoising, deblocking, inpainting, super-resolution and detail enhancement tasks. Code is available at \url{https://github.com/shizenglin/Measure-and-Control-Spectral-Bias}.



### CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2107.03463v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.03463v2)
- **Published**: 2021-07-02 15:16:45+00:00
- **Updated**: 2021-10-26 08:31:24+00:00
- **Authors**: Seyed Mojtaba Marvasti-Zadeh, Javad Khaghani, Li Cheng, Hossein Ghanei-Yakhdan, Shohreh Kasaei
- **Comment**: The first two authors contributed equally to this work. Accepted
  manuscript in BMVC 2021
- **Journal**: None
- **Summary**: A strong visual object tracker nowadays relies on its well-crafted modules, which typically consist of manually-designed network architectures to deliver high-quality tracking results. Not surprisingly, the manual design process becomes a particularly challenging barrier, as it demands sufficient prior experience, enormous effort, intuition, and perhaps some good luck. Meanwhile, neural architecture search has gaining grounds in practical applications as a promising method in tackling the issue of automated search of feasible network structures. In this work, we propose a novel cell-level differentiable architecture search mechanism with early stopping to automate the network design of the tracking module, aiming to adapt backbone features to the objective of Siamese tracking networks during offline training. Besides, the proposed early stopping strategy avoids over-fitting and performance collapse problems leading to generalization improvement. The proposed approach is simple, efficient, and with no need to stack a series of modules to construct a network. Our approach is easy to be incorporated into existing trackers, which is empirically validated using different differentiable architecture search-based methods and tracking objectives. Extensive experimental evaluations demonstrate the superior performance of our approach over five commonly-used benchmarks.



### Ensemble of Loss Functions to Improve Generalizability of Deep Metric Learning methods
- **Arxiv ID**: http://arxiv.org/abs/2107.01130v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, 68T07 (primary), 68T05, 68T45 (secondary), I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2107.01130v2)
- **Published**: 2021-07-02 15:19:46+00:00
- **Updated**: 2022-12-29 15:35:51+00:00
- **Authors**: Davood Zabihzadeh, Zahraa Alitbi, Seyed Jalaleddin Mousavirad
- **Comment**: 27 pages, 12 figures
- **Journal**: None
- **Summary**: Deep Metric Learning (DML) learns a non-linear semantic embedding from input data that brings similar pairs together while keeping dissimilar data away from each other. To this end, many different methods are proposed in the last decade with promising results in various applications. The success of a DML algorithm greatly depends on its loss function. However, no loss function is perfect, and it deals only with some aspects of an optimal similarity embedding. Besides, the generalizability of the DML on unseen categories during the test stage is an important matter that is not considered by existing loss functions. To address these challenges, we propose novel approaches to combine different losses built on top of a shared deep feature extractor. The proposed ensemble of losses enforces the deep model to extract features that are consistent with all losses. Since the selected losses are diverse and each emphasizes different aspects of an optimal semantic embedding, our effective combining methods yield a considerable improvement over any individual loss and generalize well on unseen categories. Here, there is no limitation in choosing loss functions, and our methods can work with any set of existing ones. Besides, they can optimize each loss function as well as its weight in an end-to-end paradigm with no need to adjust any hyper-parameter. We evaluate our methods on some popular datasets from the machine vision domain in conventional Zero-Shot-Learning (ZSL) settings. The results are very encouraging and show that our methods outperform all baseline losses by a large margin in all datasets.



### Collaborative Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/2107.01151v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.01151v2)
- **Published**: 2021-07-02 15:48:16+00:00
- **Updated**: 2021-07-20 12:19:35+00:00
- **Authors**: Haiyang Wang, Wenguan Wang, Xizhou Zhu, Jifeng Dai, Liwei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: As a fundamental problem for Artificial Intelligence, multi-agent system (MAS) is making rapid progress, mainly driven by multi-agent reinforcement learning (MARL) techniques. However, previous MARL methods largely focused on grid-world like or game environments; MAS in visually rich environments has remained less explored. To narrow this gap and emphasize the crucial role of perception in MAS, we propose a large-scale 3D dataset, CollaVN, for multi-agent visual navigation (MAVN). In CollaVN, multiple agents are entailed to cooperatively navigate across photo-realistic environments to reach target locations. Diverse MAVN variants are explored to make our problem more general. Moreover, a memory-augmented communication framework is proposed. Each agent is equipped with a private, external memory to persistently store communication information. This allows agents to make better use of their past communication information, enabling more efficient collaboration and robust long-term planning. In our experiments, several baselines and evaluation metrics are designed. We also empirically verify the efficacy of our proposed MARL approach across different MAVN task settings.



### Simpler, Faster, Stronger: Breaking The log-K Curse On Contrastive Learners With FlatNCE
- **Arxiv ID**: http://arxiv.org/abs/2107.01152v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2107.01152v1)
- **Published**: 2021-07-02 15:50:43+00:00
- **Updated**: 2021-07-02 15:50:43+00:00
- **Authors**: Junya Chen, Zhe Gan, Xuan Li, Qing Guo, Liqun Chen, Shuyang Gao, Tagyoung Chung, Yi Xu, Belinda Zeng, Wenlian Lu, Fan Li, Lawrence Carin, Chenyang Tao
- **Comment**: None
- **Journal**: None
- **Summary**: InfoNCE-based contrastive representation learners, such as SimCLR, have been tremendously successful in recent years. However, these contrastive schemes are notoriously resource demanding, as their effectiveness breaks down with small-batch training (i.e., the log-K curse, whereas K is the batch-size). In this work, we reveal mathematically why contrastive learners fail in the small-batch-size regime, and present a novel simple, non-trivial contrastive objective named FlatNCE, which fixes this issue. Unlike InfoNCE, our FlatNCE no longer explicitly appeals to a discriminative classification goal for contrastive learning. Theoretically, we show FlatNCE is the mathematical dual formulation of InfoNCE, thus bridging the classical literature on energy modeling; and empirically, we demonstrate that, with minimal modification of code, FlatNCE enables immediate performance boost independent of the subject-matter engineering efforts. The significance of this work is furthered by the powerful generalization of contrastive learning techniques, and the introduction of new tools to monitor and diagnose contrastive training. We substantiate our claims with empirical evidence on CIFAR10, ImageNet, and other datasets, where FlatNCE consistently outperforms InfoNCE.



### A Survey on Deep Learning Technique for Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.01153v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.01153v4)
- **Published**: 2021-07-02 15:51:07+00:00
- **Updated**: 2022-11-29 11:08:17+00:00
- **Authors**: Tianfei Zhou, Fatih Porikli, David Crandall, Luc Van Gool, Wenguan Wang
- **Comment**: Accepted by TPAMI. Website: https://github.com/tfzhou/VS-Survey
- **Journal**: None
- **Summary**: Video segmentation -- partitioning video frames into multiple segments or objects -- plays a critical role in a broad range of practical applications, from enhancing visual effects in movie, to understanding scenes in autonomous driving, to creating virtual background in video conferencing. Recently, with the renaissance of connectionism in computer vision, there has been an influx of deep learning based approaches for video segmentation that have delivered compelling performance. In this survey, we comprehensively review two basic lines of research -- generic object segmentation (of unknown categories) in videos, and video semantic segmentation -- by introducing their respective task settings, background concepts, perceived need, development history, and main challenges. We also offer a detailed overview of representative literature on both methods and datasets. We further benchmark the reviewed methods on several well-known datasets. Finally, we point out open issues in this field, and suggest opportunities for further research. We also provide a public website to continuously track developments in this fast advancing field: https://github.com/tfzhou/VS-Survey.



### Compressive Representations of Weather Scenes for Strategic Air Traffic Flow Management
- **Arxiv ID**: http://arxiv.org/abs/2107.06394v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.06394v1)
- **Published**: 2021-07-02 16:16:28+00:00
- **Updated**: 2021-07-02 16:16:28+00:00
- **Authors**: Sandip Roy
- **Comment**: None
- **Journal**: None
- **Summary**: Terse representation of high-dimensional weather scene data is explored, in support of strategic air traffic flow management objectives. Specifically, we consider whether aviation-relevant weather scenes are compressible, in the sense that each scene admits a possibly-different sparse representation in a basis of interest. Here, compression of weather scenes extracted from METAR data (including temperature, flight categories, and visibility profiles for the contiguous United States) is examined, for the graph-spectral basis. The scenes are found to be compressible, with 75-95% of the scene content captured using 0.5-4% of the basis vectors. Further, the dominant basis vectors for each scene are seen to identify time-varying spatial characteristics of the weather, and reconstruction from the compressed representation is demonstrated. Finally, potential uses of the compressive representations in strategic TFM design are briefly scoped.



### Continuous Emotion Recognition with Audio-visual Leader-follower Attentive Fusion
- **Arxiv ID**: http://arxiv.org/abs/2107.01175v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2107.01175v3)
- **Published**: 2021-07-02 16:28:55+00:00
- **Updated**: 2021-08-17 09:31:23+00:00
- **Authors**: Su Zhang, Yi Ding, Ziquan Wei, Cuntai Guan
- **Comment**: 8 pages, 2 figures, 2 tables, accepted to the ICCV 2021: 2nd Workshop
  and Competition on Affective Behavior Analysis in-the-wild (ABAW2)
- **Journal**: None
- **Summary**: We propose an audio-visual spatial-temporal deep neural network with: (1) a visual block containing a pretrained 2D-CNN followed by a temporal convolutional network (TCN); (2) an aural block containing several parallel TCNs; and (3) a leader-follower attentive fusion block combining the audio-visual information. The TCN with large history coverage enables our model to exploit spatial-temporal information within a much larger window length (i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36 or 48). The fusion block emphasizes the visual modality while exploits the noisy aural modality using the inter-modality attention mechanism. To make full use of the data and alleviate over-fitting, cross-validation is carried out on the training and validation set. The concordance correlation coefficient (CCC) centering is used to merge the results from each fold. On the test (validation) set of the Aff-Wild2 database, the achieved CCC is 0.463 (0.469) for valence and 0.492 (0.649) for arousal, which significantly outperforms the baseline method with the corresponding CCC of 0.200 (0.210) and 0.190 (0.230) for valence and arousal, respectively. The code is available at https://github.com/sucv/ABAW2.



### Visual Relationship Forecasting in Videos
- **Arxiv ID**: http://arxiv.org/abs/2107.01181v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.01181v1)
- **Published**: 2021-07-02 16:43:19+00:00
- **Updated**: 2021-07-02 16:43:19+00:00
- **Authors**: Li Mi, Yangjun Ou, Zhenzhong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world scenarios often require the anticipation of object interactions in unknown future, which would assist the decision-making process of both humans and agents. To meet this challenge, we present a new task named Visual Relationship Forecasting (VRF) in videos to explore the prediction of visual relationships in a reasoning manner. Specifically, given a subject-object pair with H existing frames, VRF aims to predict their future interactions for the next T frames without visual evidence. To evaluate the VRF task, we introduce two video datasets named VRF-AG and VRF-VidOR, with a series of spatio-temporally localized visual relation annotations in a video. These two datasets densely annotate 13 and 35 visual relationships in 1923 and 13447 video clips, respectively. In addition, we present a novel Graph Convolutional Transformer (GCT) framework, which captures both object-level and frame-level dependencies by spatio-temporal Graph Convolution Network and Transformer. Experimental results on both VRF-AG and VRF-VidOR datasets demonstrate that GCT outperforms the state-of-the-art sequence modelling methods on visual relationship forecasting.



### NTIRE 2021 Multi-modal Aerial View Object Classification Challenge
- **Arxiv ID**: http://arxiv.org/abs/2107.01189v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.01189v3)
- **Published**: 2021-07-02 16:55:08+00:00
- **Updated**: 2022-04-06 17:11:18+00:00
- **Authors**: Jerrick Liu, Nathan Inkawhich, Oliver Nina, Radu Timofte, Sahil Jain, Bob Lee, Yuru Duan, Wei Wei, Lei Zhang, Songzheng Xu, Yuxuan Sun, Jiaqi Tang, Xueli Geng, Mengru Ma, Gongzhe Li, Xueli Geng, Huanqia Cai, Chengxue Cai, Sol Cummings, Casian Miron, Alexandru Pasarica, Cheng-Yen Yang, Hung-Min Hsu, Jiarui Cai, Jie Mei, Chia-Ying Yeh, Jenq-Neng Hwang, Michael Xin, Zhongkai Shangguan, Zihe Zheng, Xu Yifei, Lehan Yang, Kele Xu, Min Feng
- **Comment**: The paper needs to be withdrawn since it did not properly go through
  the public release process. We will soon release a new version to replace
  this one
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) Workshops, 2021, 588-595
- **Summary**: In this paper, we introduce the first Challenge on Multi-modal Aerial View Object Classification (MAVOC) in conjunction with the NTIRE 2021 workshop at CVPR. This challenge is composed of two different tracks using EO andSAR imagery. Both EO and SAR sensors possess different advantages and drawbacks. The purpose of this competition is to analyze how to use both sets of sensory information in complementary ways. We discuss the top methods submitted for this competition and evaluate their results on our blind test set. Our challenge results show significant improvement of more than 15% accuracy from our current baselines for each track of the competition



### Inter-intra Variant Dual Representations forSelf-supervised Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.01194v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.01194v3)
- **Published**: 2021-07-02 17:03:04+00:00
- **Updated**: 2021-10-23 16:00:54+00:00
- **Authors**: Lin Zhang, Qi She, Zhengyang Shen, Changhu Wang
- **Comment**: Accepted by BMVC 2021
- **Journal**: None
- **Summary**: Contrastive learning applied to self-supervised representation learning has seen a resurgence in deep models. In this paper, we find that existing contrastive learning based solutions for self-supervised video recognition focus on inter-variance encoding but ignore the intra-variance existing in clips within the same video. We thus propose to learn dual representations for each clip which (\romannumeral 1) encode intra-variance through a shuffle-rank pretext task; (\romannumeral 2) encode inter-variance through a temporal coherent contrastive loss. Experiment results show that our method plays an essential role in balancing inter and intra variances and brings consistent performance gains on multiple backbones and contrastive learning frameworks. Integrated with SimCLR and pretrained on Kinetics-400, our method achieves $\textbf{82.0\%}$ and $\textbf{51.2\%}$ downstream classification accuracy on UCF101 and HMDB51 test sets respectively and $\textbf{46.1\%}$ video retrieval accuracy on UCF101, outperforming both pretext-task based and contrastive learning based counterparts. Our code is available at \href{https://github.com/lzhangbj/DualVar}{https://github.com/lzhangbj/DualVar}.



### HandVoxNet++: 3D Hand Shape and Pose Estimation using Voxel-Based Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.01205v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.01205v2)
- **Published**: 2021-07-02 17:59:54+00:00
- **Updated**: 2021-12-05 21:08:53+00:00
- **Authors**: Jameel Malik, Soshi Shimada, Ahmed Elhayek, Sk Aziz Ali, Christian Theobalt, Vladislav Golyanik, Didier Stricker
- **Comment**: 13 pages, 6 tables, 7 figures; project webpage:
  http://4dqv.mpi-inf.mpg.de/HandVoxNet++/. arXiv admin note: text overlap with
  arXiv:2004.01588
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI), 2021
- **Summary**: 3D hand shape and pose estimation from a single depth map is a new and challenging computer vision problem with many applications. Existing methods addressing it directly regress hand meshes via 2D convolutional neural networks, which leads to artefacts due to perspective distortions in the images. To address the limitations of the existing methods, we develop HandVoxNet++, i.e., a voxel-based deep network with 3D and graph convolutions trained in a fully supervised manner. The input to our network is a 3D voxelized-depth-map-based on the truncated signed distance function (TSDF). HandVoxNet++ relies on two hand shape representations. The first one is the 3D voxelized grid of hand shape, which does not preserve the mesh topology and which is the most accurate representation. The second representation is the hand surface that preserves the mesh topology. We combine the advantages of both representations by aligning the hand surface to the voxelized hand shape either with a new neural Graph-Convolutions-based Mesh Registration (GCN-MeshReg) or classical segment-wise Non-Rigid Gravitational Approach (NRGA++) which does not rely on training data. In extensive evaluations on three public benchmarks, i.e., SynHand5M, depth-based HANDS19 challenge and HO-3D, the proposed HandVoxNet++ achieves state-of-the-art performance. In this journal extension of our previous approach presented at CVPR 2020, we gain 41.09% and 13.7% higher shape alignment accuracy on SynHand5M and HANDS19 datasets, respectively. Our method is ranked first on the HANDS19 challenge dataset (Task 1: Depth-Based 3D Hand Pose Estimation) at the moment of the submission of our results to the portal in August 2020.



### Data Uncertainty Guided Noise-aware Preprocessing Of Fingerprints
- **Arxiv ID**: http://arxiv.org/abs/2107.01248v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.01248v1)
- **Published**: 2021-07-02 19:47:58+00:00
- **Updated**: 2021-07-02 19:47:58+00:00
- **Authors**: Indu Joshi, Ayush Utkarsh, Riya Kothari, Vinod K Kurmi, Antitza Dantcheva, Sumantra Dutta Roy, Prem Kumar Kalra
- **Comment**: IJCNN 2021 (Accepted)
- **Journal**: None
- **Summary**: The effectiveness of fingerprint-based authentication systems on good quality fingerprints is established long back. However, the performance of standard fingerprint matching systems on noisy and poor quality fingerprints is far from satisfactory. Towards this, we propose a data uncertainty-based framework which enables the state-of-the-art fingerprint preprocessing models to quantify noise present in the input image and identify fingerprint regions with background noise and poor ridge clarity. Quantification of noise helps the model two folds: firstly, it makes the objective function adaptive to the noise in a particular input fingerprint and consequently, helps to achieve robust performance on noisy and distorted fingerprint regions. Secondly, it provides a noise variance map which indicates noisy pixels in the input fingerprint image. The predicted noise variance map enables the end-users to understand erroneous predictions due to noise present in the input image. Extensive experimental evaluation on 13 publicly available fingerprint databases, across different architectural choices and two fingerprint processing tasks demonstrate effectiveness of the proposed framework.



### Visual Time Series Forecasting: An Image-driven Approach
- **Arxiv ID**: http://arxiv.org/abs/2107.01273v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-fin.ST, q-fin.TR
- **Links**: [PDF](http://arxiv.org/pdf/2107.01273v2)
- **Published**: 2021-07-02 20:59:48+00:00
- **Updated**: 2021-11-15 19:08:33+00:00
- **Authors**: Naftali Cohen, Srijan Sood, Zhen Zeng, Tucker Balch, Manuela Veloso
- **Comment**: This work was intended as a replacement of arXiv:2011.09052 and any
  subsequent updates will appear there
- **Journal**: None
- **Summary**: In this work, we address time-series forecasting as a computer vision task. We capture input data as an image and train a model to produce the subsequent image. This approach results in predicting distributions as opposed to pointwise values. To assess the robustness and quality of our approach, we examine various datasets and multiple evaluation metrics. Our experiments show that our forecasting tool is effective for cyclic data but somewhat less for irregular data such as stock prices. Importantly, when using image-based evaluation metrics, we find our method to outperform various baselines, including ARIMA, and a numerical variation of our deep learning approach.



### A Novel Disaster Image Dataset and Characteristics Analysis using Attention Model
- **Arxiv ID**: http://arxiv.org/abs/2107.01284v1
- **DOI**: 10.1109/ICPR48806.2021.9412504
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.01284v1)
- **Published**: 2021-07-02 21:18:20+00:00
- **Updated**: 2021-07-02 21:18:20+00:00
- **Authors**: Fahim Faisal Niloy, Arif, Abu Bakar Siddik Nayem, Anis Sarker, Ovi Paul, M. Ashraful Amin, Amin Ahsan Ali, Moinul Islam Zaber, AKM Mahbubur Rahman
- **Comment**: ICPR 2020
- **Journal**: None
- **Summary**: The advancement of deep learning technology has enabled us to develop systems that outperform any other classification technique. However, success of any empirical system depends on the quality and diversity of the data available to train the proposed system. In this research, we have carefully accumulated a relatively challenging dataset that contains images collected from various sources for three different disasters: fire, water and land. Besides this, we have also collected images for various damaged infrastructure due to natural or man made calamities and damaged human due to war or accidents. We have also accumulated image data for a class named non-damage that contains images with no such disaster or sign of damage in them. There are 13,720 manually annotated images in this dataset, each image is annotated by three individuals. We are also providing discriminating image class information annotated manually with bounding box for a set of 200 test images. Images are collected from different news portals, social media, and standard datasets made available by other researchers. A three layer attention model (TLAM) is trained and average five fold validation accuracy of 95.88% is achieved. Moreover, on the 200 unseen test images this accuracy is 96.48%. We also generate and compare attention maps for these test images to determine the characteristics of the trained attention model. Our dataset is available at https://niloy193.github.io/Disaster-Dataset



