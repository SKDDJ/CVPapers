# Arxiv Papers in cs.CV on 2021-07-01
### Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?
- **Arxiv ID**: http://arxiv.org/abs/2107.00166v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00166v4)
- **Published**: 2021-07-01 01:27:07+00:00
- **Updated**: 2021-10-26 21:49:29+00:00
- **Authors**: Xiaolong Ma, Geng Yuan, Xuan Shen, Tianlong Chen, Xuxi Chen, Xiaohan Chen, Ning Liu, Minghai Qin, Sijia Liu, Zhangyang Wang, Yanzhi Wang
- **Comment**: NeurIPS 2021 camera ready
- **Journal**: None
- **Summary**: There have been long-standing controversies and inconsistencies over the experiment setup and criteria for identifying the "winning ticket" in literature. To reconcile such, we revisit the definition of lottery ticket hypothesis, with comprehensive and more rigorous conditions. Under our new definition, we show concrete evidence to clarify whether the winning ticket exists across the major DNN architectures and/or applications. Through extensive experiments, we perform quantitative analysis on the correlations between winning tickets and various experimental factors, and empirically study the patterns of our observations. We find that the key training hyperparameters, such as learning rate and training epochs, as well as the architecture characteristics such as capacities and residual connections, are all highly correlated with whether and when the winning tickets can be identified. Based on our analysis, we summarize a guideline for parameter settings in regards of specific architecture characteristics, which we hope to catalyze the research progress on the topic of lottery ticket hypothesis. Our codes are publicly available at: https://github.com/boone891214/sanity-check-LTH.



### Revisiting Knowledge Distillation: An Inheritance and Exploration Framework
- **Arxiv ID**: http://arxiv.org/abs/2107.00181v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00181v1)
- **Published**: 2021-07-01 02:20:56+00:00
- **Updated**: 2021-07-01 02:20:56+00:00
- **Authors**: Zhen Huang, Xu Shen, Jun Xing, Tongliang Liu, Xinmei Tian, Houqiang Li, Bing Deng, Jianqiang Huang, Xian-Sheng Hua
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Knowledge Distillation (KD) is a popular technique to transfer knowledge from a teacher model or ensemble to a student model. Its success is generally attributed to the privileged information on similarities/consistency between the class distributions or intermediate feature representations of the teacher model and the student model. However, directly pushing the student model to mimic the probabilities/features of the teacher model to a large extent limits the student model in learning undiscovered knowledge/features. In this paper, we propose a novel inheritance and exploration knowledge distillation framework (IE-KD), in which a student model is split into two parts - inheritance and exploration. The inheritance part is learned with a similarity loss to transfer the existing learned knowledge from the teacher model to the student model, while the exploration part is encouraged to learn representations different from the inherited ones with a dis-similarity loss. Our IE-KD framework is generic and can be easily combined with existing distillation or mutual learning methods for training deep neural networks. Extensive experiments demonstrate that these two parts can jointly push the student model to learn more diversified and effective representations, and our IE-KD can be a general technique to improve the student network to achieve SOTA performance. Furthermore, by applying our IE-KD to the training of two networks, the performance of both can be improved w.r.t. deep mutual learning. The code and models of IE-KD will be make publicly available at https://github.com/yellowtownhz/IE-KD.



### Unsupervised Model Drift Estimation with Batch Normalization Statistics for Dataset Shift Detection and Model Selection
- **Arxiv ID**: http://arxiv.org/abs/2107.00191v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00191v1)
- **Published**: 2021-07-01 03:04:47+00:00
- **Updated**: 2021-07-01 03:04:47+00:00
- **Authors**: Wonju Lee, Seok-Yong Byun, Jooeun Kim, Minje Park, Kirill Chechil
- **Comment**: 11 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: While many real-world data streams imply that they change frequently in a nonstationary way, most of deep learning methods optimize neural networks on training data, and this leads to severe performance degradation when dataset shift happens. However, it is less possible to annotate or inspect newly streamed data by humans, and thus it is desired to measure model drift at inference time in an unsupervised manner. In this paper, we propose a novel method of model drift estimation by exploiting statistics of batch normalization layer on unlabeled test data. To remedy possible sampling error of streamed input data, we adopt low-rank approximation to each representational layer. We show the effectiveness of our method not only on dataset shift detection but also on model selection when there are multiple candidate models among model zoo or training trajectories in an unsupervised way. We further demonstrate the consistency of our method by comparing model drift scores between different network architectures.



### Few-Shot Learning with a Strong Teacher
- **Arxiv ID**: http://arxiv.org/abs/2107.00197v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.00197v2)
- **Published**: 2021-07-01 03:20:46+00:00
- **Updated**: 2022-07-11 01:37:45+00:00
- **Authors**: Han-Jia Ye, Lu Ming, De-Chuan Zhan, Wei-Lun Chao
- **Comment**: Accepted to publish on IEEE Transactions on Pattern Analysis and
  Machine Intelligence (TPAMI)
- **Journal**: None
- **Summary**: Few-shot learning (FSL) aims to generate a classifier using limited labeled examples. Many existing works take the meta-learning approach, constructing a few-shot learner that can learn from few-shot examples to generate a classifier. Typically, the few-shot learner is constructed or meta-trained by sampling multiple few-shot tasks in turn and optimizing the few-shot learner's performance in generating classifiers for those tasks. The performance is measured by how well the resulting classifiers classify the test (i.e., query) examples of those tasks. In this paper, we point out two potential weaknesses of this approach. First, the sampled query examples may not provide sufficient supervision for meta-training the few-shot learner. Second, the effectiveness of meta-learning diminishes sharply with the increasing number of shots. To resolve these issues, we propose a novel meta-training objective for the few-shot learner, which is to encourage the few-shot learner to generate classifiers that perform like strong classifiers. Concretely, we associate each sampled few-shot task with a strong classifier, which is trained with ample labeled examples. The strong classifiers can be seen as the target classifiers that we hope the few-shot learner to generate given few-shot examples, and we use the strong classifiers to supervise the few-shot learner. We present an efficient way to construct the strong classifier, making our proposed objective an easily plug-and-play term to existing meta-learning based FSL methods. We validate our approach, LastShot, in combinations with many representative meta-learning methods. On several benchmark datasets, our approach leads to a notable improvement across a variety of tasks. More importantly, with our approach, meta-learning based FSL methods can outperform non-meta-learning based methods at different numbers of shots.



### Multi-modal Graph Learning for Disease Prediction
- **Arxiv ID**: http://arxiv.org/abs/2107.00206v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00206v1)
- **Published**: 2021-07-01 03:59:22+00:00
- **Updated**: 2021-07-01 03:59:22+00:00
- **Authors**: Shuai Zheng, Zhenfeng Zhu, Zhizhe Liu, Zhenyu Guo, Yang Liu, Yao Zhao
- **Comment**: 10 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: Benefiting from the powerful expressive capability of graphs, graph-based approaches have achieved impressive performance in various biomedical applications. Most existing methods tend to define the adjacency matrix among samples manually based on meta-features, and then obtain the node embeddings for downstream tasks by Graph Representation Learning (GRL). However, it is not easy for these approaches to generalize to unseen samples. Meanwhile, the complex correlation between modalities is also ignored. As a result, these factors inevitably yield the inadequacy of providing valid information about the patient's condition for a reliable diagnosis. In this paper, we propose an end-to-end Multimodal Graph Learning framework (MMGL) for disease prediction. To effectively exploit the rich information across multi-modality associated with diseases, amodal-attentional multi-modal fusion is proposed to integrate the features of each modality by leveraging the correlation and complementarity between the modalities. Furthermore, instead of defining the adjacency matrix manually as existing methods, the latent graph structure can be captured through a novel way of adaptive graph learning. It could be jointly optimized with the prediction model, thus revealing the intrinsic connections among samples. Unlike the previous transductive methods, our model is also applicable to the scenario of inductive learning for those unseen data. An extensive group of experiments on two disease prediction problems is then carefully designed and presented, demonstrating that MMGL obtains more favorable performances. In addition, we also visualize and analyze the learned graph structure to provide more reliable decision support for doctors in real medical applications and inspiration for disease research.



### Deep auxiliary learning for visual localization using colorization task
- **Arxiv ID**: http://arxiv.org/abs/2107.00222v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.00222v1)
- **Published**: 2021-07-01 05:25:19+00:00
- **Updated**: 2021-07-01 05:25:19+00:00
- **Authors**: Mi Tian, Qiong Nie, Hao Shen, Xiahua Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Visual localization is one of the most important components for robotics and autonomous driving. Recently, inspiring results have been shown with CNN-based methods which provide a direct formulation to end-to-end regress 6-DoF absolute pose. Additional information like geometric or semantic constraints is generally introduced to improve performance. Especially, the latter can aggregate high-level semantic information into localization task, but it usually requires enormous manual annotations. To this end, we propose a novel auxiliary learning strategy for camera localization by introducing scene-specific high-level semantics from self-supervised representation learning task. Viewed as a powerful proxy task, image colorization task is chosen as complementary task that outputs pixel-wise color version of grayscale photograph without extra annotations. In our work, feature representations from colorization network are embedded into localization network by design to produce discriminative features for pose regression. Meanwhile an attention mechanism is introduced for the benefit of localization performance. Extensive experiments show that our model significantly improve localization accuracy over state-of-the-arts on both indoor and outdoor datasets.



### Exponential Lower Bounds for Threshold Circuits of Sub-Linear Depth and Energy
- **Arxiv ID**: http://arxiv.org/abs/2107.00223v2
- **DOI**: None
- **Categories**: **cs.CC**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2107.00223v2)
- **Published**: 2021-07-01 05:37:53+00:00
- **Updated**: 2023-06-28 03:49:37+00:00
- **Authors**: Kei Uchizawa, Haruki Abe
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we investigate computational power of threshold circuits and other theoretical models of neural networks in terms of the following four complexity measures: size (the number of gates), depth, weight and energy. Here the energy complexity of a circuit measures sparsity of their computation, and is defined as the maximum number of gates outputting non-zero values taken over all the input assignments. As our main result, we prove that any threshold circuit $C$ of size $s$, depth $d$, energy $e$ and weight $w$ satisfies $\log (rk(M_C)) \le ed (\log s + \log w + \log n)$, where $rk(M_C)$ is the rank of the communication matrix $M_C$ of a $2n$-variable Boolean function that $C$ computes. Thus, such a threshold circuit $C$ is able to compute only a Boolean function of which communication matrix has rank bounded by a product of logarithmic factors of $s,w$ and linear factors of $d,e$. This implies an exponential lower bound on the size of even sublinear-depth threshold circuit if energy and weight are sufficiently small. For other models of neural networks such as a discretized ReLE circuits and decretized sigmoid circuits, we prove that a similar inequality also holds for a discretized circuit $C$: $rk(M_C) = O(ed(\log s + \log w + \log n)^3)$.



### Scalable Certified Segmentation via Randomized Smoothing
- **Arxiv ID**: http://arxiv.org/abs/2107.00228v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00228v2)
- **Published**: 2021-07-01 05:52:39+00:00
- **Updated**: 2022-07-27 14:19:24+00:00
- **Authors**: Marc Fischer, Maximilian Baader, Martin Vechev
- **Comment**: ICML'21
- **Journal**: None
- **Summary**: We present a new certification method for image and point cloud segmentation based on randomized smoothing. The method leverages a novel scalable algorithm for prediction and certification that correctly accounts for multiple testing, necessary for ensuring statistical guarantees. The key to our approach is reliance on established multiple-testing correction mechanisms as well as the ability to abstain from classifying single pixels or points while still robustly segmenting the overall input. Our experimental evaluation on synthetic data and challenging datasets, such as Pascal Context, Cityscapes, and ShapeNet, shows that our algorithm can achieve, for the first time, competitive accuracy and certification guarantees on real-world segmentation tasks. We provide an implementation at https://github.com/eth-sri/segmentation-smoothing.



### E-DSSR: Efficient Dynamic Surgical Scene Reconstruction with Transformer-based Stereoscopic Depth Perception
- **Arxiv ID**: http://arxiv.org/abs/2107.00229v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.00229v1)
- **Published**: 2021-07-01 05:57:41+00:00
- **Updated**: 2021-07-01 05:57:41+00:00
- **Authors**: Yonghao Long, Zhaoshuo Li, Chi Hang Yee, Chi Fai Ng, Russell H. Taylor, Mathias Unberath, Qi Dou
- **Comment**: Accepted to MICCAI 2021
- **Journal**: None
- **Summary**: Reconstructing the scene of robotic surgery from the stereo endoscopic video is an important and promising topic in surgical data science, which potentially supports many applications such as surgical visual perception, robotic surgery education and intra-operative context awareness. However, current methods are mostly restricted to reconstructing static anatomy assuming no tissue deformation, tool occlusion and de-occlusion, and camera movement. However, these assumptions are not always satisfied in minimal invasive robotic surgeries. In this work, we present an efficient reconstruction pipeline for highly dynamic surgical scenes that runs at 28 fps. Specifically, we design a transformer-based stereoscopic depth perception for efficient depth estimation and a light-weight tool segmentor to handle tool occlusion. After that, a dynamic reconstruction algorithm which can estimate the tissue deformation and camera movement, and aggregate the information over time is proposed for surgical scene reconstruction. We evaluate the proposed pipeline on two datasets, the public Hamlyn Centre Endoscopic Video Dataset and our in-house DaVinci robotic surgery dataset. The results demonstrate that our method can recover the scene obstructed by the surgical tool and handle the movement of camera in realistic surgical scenarios effectively at real-time speed.



### FedMix: Approximation of Mixup under Mean Augmented Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.00233v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2107.00233v1)
- **Published**: 2021-07-01 06:14:51+00:00
- **Updated**: 2021-07-01 06:14:51+00:00
- **Authors**: Tehrim Yoon, Sumin Shin, Sung Ju Hwang, Eunho Yang
- **Comment**: None
- **Journal**: ICLR 2021
- **Summary**: Federated learning (FL) allows edge devices to collectively learn a model without directly sharing data within each device, thus preserving privacy and eliminating the need to store data globally. While there are promising results under the assumption of independent and identically distributed (iid) local data, current state-of-the-art algorithms suffer from performance degradation as the heterogeneity of local data across clients increases. To resolve this issue, we propose a simple framework, Mean Augmented Federated Learning (MAFL), where clients send and receive averaged local data, subject to the privacy requirements of target applications. Under our framework, we propose a new augmentation algorithm, named FedMix, which is inspired by a phenomenal yet simple data augmentation method, Mixup, but does not require local raw data to be directly shared among devices. Our method shows greatly improved performance in the standard benchmark datasets of FL, under highly non-iid federated settings, compared to conventional algorithms.



### Feasibility of Haralick's Texture Features for the Classification of Chromogenic In-situ Hybridization Images
- **Arxiv ID**: http://arxiv.org/abs/2107.00235v1
- **DOI**: 10.1109/BIA50171.2020.9244282
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2107.00235v1)
- **Published**: 2021-07-01 06:18:40+00:00
- **Updated**: 2021-07-01 06:18:40+00:00
- **Authors**: Stoyan Pavlov, Galina Momcheva, Pavlina Burlakova, Simeon Atanasov, Dimo Stoyanov, Martin Ivanov, Anton Tonchev
- **Comment**: 4 pages, 1 figure
- **Journal**: 2020 International Conference on Biomedical Innovations and
  Applications (BIA), 2020, pp. 65-68
- **Summary**: This paper presents a proof of concept for the usefulness of second-order texture features for the qualitative analysis and classification of chromogenic in-situ hybridization whole slide images in high-throughput imaging experiments. The challenge is that currently, the gold standard for gene expression grading in such images is expert assessment. The idea of the research team is to use different approaches in the analysis of these images that will be used for structural segmentation and functional analysis in gene expression. The article presents such perspective idea to select a number of textural features that are going to be used for classification. In our experiment, natural grouping of image samples (tiles) depending on their local texture properties was explored in an unsupervised classification procedure. The features are reduced to two dimensions with fuzzy c-means clustering. The overall conclusion of this experiment is that Haralick features are a viable choice for classification and analysis of chromogenic in-situ hybridization image data. The principal component analysis approach produced slightly more "understandable" from an annotator's point of view classes.



### Generic Event Boundary Detection Challenge at CVPR 2021 Technical Report: Cascaded Temporal Attention Network (CASTANET)
- **Arxiv ID**: http://arxiv.org/abs/2107.00239v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00239v1)
- **Published**: 2021-07-01 06:37:01+00:00
- **Updated**: 2021-07-01 06:37:01+00:00
- **Authors**: Dexiang Hong, Congcong Li, Longyin Wen, Xinyao Wang, Libo Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This report presents the approach used in the submission of Generic Event Boundary Detection (GEBD) Challenge at CVPR21. In this work, we design a Cascaded Temporal Attention Network (CASTANET) for GEBD, which is formed by three parts, the backbone network, the temporal attention module, and the classification module. Specifically, the Channel-Separated Convolutional Network (CSN) is used as the backbone network to extract features, and the temporal attention module is designed to enforce the network to focus on the discriminative features. After that, the cascaded architecture is used in the classification module to generate more accurate boundaries. In addition, the ensemble strategy is used to further improve the performance of the proposed method. The proposed method achieves 83.30% F1 score on Kinetics-GEBD test set, which improves 20.5% F1 score compared to the baseline method. Code is available at https://github.com/DexiangHong/Cascade-PC.



### OPT: Omni-Perception Pre-Trainer for Cross-Modal Understanding and Generation
- **Arxiv ID**: http://arxiv.org/abs/2107.00249v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00249v2)
- **Published**: 2021-07-01 06:59:44+00:00
- **Updated**: 2021-07-06 03:18:27+00:00
- **Authors**: Jing Liu, Xinxin Zhu, Fei Liu, Longteng Guo, Zijia Zhao, Mingzhen Sun, Weining Wang, Hanqing Lu, Shiyu Zhou, Jiajun Zhang, Jinqiao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an Omni-perception Pre-Trainer (OPT) for cross-modal understanding and generation, by jointly modeling visual, text and audio resources. OPT is constructed in an encoder-decoder framework, including three single-modal encoders to generate token-based embeddings for each modality, a cross-modal encoder to encode the correlations among the three modalities, and two cross-modal decoders to generate text and image respectively. For the OPT's pre-training, we design a multi-task pretext learning scheme to model multi-modal resources from three different data granularities, \ie, token-, modality-, and sample-level modeling, through which OPT learns to align and translate among different modalities. The pre-training task is carried out on a large amount of image-text-audio triplets from Open Images. Experimental results show that OPT can learn strong image-text-audio multi-modal representations and achieve promising results on a variety of cross-modal understanding and generation tasks.



### AdaXpert: Adapting Neural Architecture for Growing Data
- **Arxiv ID**: http://arxiv.org/abs/2107.00254v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00254v1)
- **Published**: 2021-07-01 07:22:05+00:00
- **Updated**: 2021-07-01 07:22:05+00:00
- **Authors**: Shuaicheng Niu, Jiaxiang Wu, Guanghui Xu, Yifan Zhang, Yong Guo, Peilin Zhao, Peng Wang, Mingkui Tan
- **Comment**: accepted by ICML 2021
- **Journal**: None
- **Summary**: In real-world applications, data often come in a growing manner, where the data volume and the number of classes may increase dynamically. This will bring a critical challenge for learning: given the increasing data volume or the number of classes, one has to instantaneously adjust the neural model capacity to obtain promising performance. Existing methods either ignore the growing nature of data or seek to independently search an optimal architecture for a given dataset, and thus are incapable of promptly adjusting the architectures for the changed data. To address this, we present a neural architecture adaptation method, namely Adaptation eXpert (AdaXpert), to efficiently adjust previous architectures on the growing data. Specifically, we introduce an architecture adjuster to generate a suitable architecture for each data snapshot, based on the previous architecture and the different extent between current and previous data distributions. Furthermore, we propose an adaptation condition to determine the necessity of adjustment, thereby avoiding unnecessary and time-consuming adjustments. Extensive experiments on two growth scenarios (increasing data volume and number of classes) demonstrate the effectiveness of the proposed method.



### A Survey on Graph-Based Deep Learning for Computational Histopathology
- **Arxiv ID**: http://arxiv.org/abs/2107.00272v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/2107.00272v2)
- **Published**: 2021-07-01 07:50:35+00:00
- **Updated**: 2021-09-27 11:03:00+00:00
- **Authors**: David Ahmedt-Aristizabal, Mohammad Ali Armin, Simon Denman, Clinton Fookes, Lars Petersson
- **Comment**: Preprint submitted to Computerized Medical Imaging and Graphics
- **Journal**: None
- **Summary**: With the remarkable success of representation learning for prediction problems, we have witnessed a rapid expansion of the use of machine learning and deep learning for the analysis of digital pathology and biopsy image patches. However, learning over patch-wise features using convolutional neural networks limits the ability of the model to capture global contextual information and comprehensively model tissue composition. The phenotypical and topological distribution of constituent histological entities play a critical role in tissue diagnosis. As such, graph data representations and deep learning have attracted significant attention for encoding tissue representations, and capturing intra- and inter- entity level interactions. In this review, we provide a conceptual grounding for graph analytics in digital pathology, including entity-graph construction and graph architectures, and present their current success for tumor localization and classification, tumor invasion and staging, image retrieval, and survival prediction. We provide an overview of these methods in a systematic manner organized by the graph representation of the input image, scale, and organ on which they operate. We also outline the limitations of existing techniques, and suggest potential future research directions in this domain.



### DivergentNets: Medical Image Segmentation by Network Ensemble
- **Arxiv ID**: http://arxiv.org/abs/2107.00283v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.00283v1)
- **Published**: 2021-07-01 08:15:00+00:00
- **Updated**: 2021-07-01 08:15:00+00:00
- **Authors**: Vajira Thambawita, Steven A. Hicks, Pål Halvorsen, Michael A. Riegler
- **Comment**: the winning model of the segmentation generalization challenge at
  EndoCV 2021
- **Journal**: Proceedings of the 3rd International Workshop and Challenge on
  Computer Vision in Endoscopy (EndoCV 2021) colocated with with the 17th IEEE
  International Symposium on Biomedical Imaging (ISBI 2021)
- **Summary**: Detection of colon polyps has become a trending topic in the intersecting fields of machine learning and gastrointestinal endoscopy. The focus has mainly been on per-frame classification. More recently, polyp segmentation has gained attention in the medical community. Segmentation has the advantage of being more accurate than per-frame classification or object detection as it can show the affected area in greater detail. For our contribution to the EndoCV 2021 segmentation challenge, we propose two separate approaches. First, a segmentation model named TriUNet composed of three separate UNet models. Second, we combine TriUNet with an ensemble of well-known segmentation models, namely UNet++, FPN, DeepLabv3, and DeepLabv3+, into a model called DivergentNets to produce more generalizable medical image segmentation masks. In addition, we propose a modified Dice loss that calculates loss only for a single class when performing multiclass segmentation, forcing the model to focus on what is most important. Overall, the proposed methods achieved the best average scores for each respective round in the challenge, with TriUNet being the winning model in Round I and DivergentNets being the winning model in Round II of the segmentation generalization challenge at EndoCV 2021. The implementation of our approach is made publicly available on GitHub.



### iMiGUE: An Identity-free Video Dataset for Micro-Gesture Understanding and Emotion Analysis
- **Arxiv ID**: http://arxiv.org/abs/2107.00285v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00285v1)
- **Published**: 2021-07-01 08:15:14+00:00
- **Updated**: 2021-07-01 08:15:14+00:00
- **Authors**: Xin Liu, Henglin Shi, Haoyu Chen, Zitong Yu, Xiaobai Li, Guoying Zhaoz?
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: We introduce a new dataset for the emotional artificial intelligence research: identity-free video dataset for Micro-Gesture Understanding and Emotion analysis (iMiGUE). Different from existing public datasets, iMiGUE focuses on nonverbal body gestures without using any identity information, while the predominant researches of emotion analysis concern sensitive biometric data, like face and speech. Most importantly, iMiGUE focuses on micro-gestures, i.e., unintentional behaviors driven by inner feelings, which are different from ordinary scope of gestures from other gesture datasets which are mostly intentionally performed for illustrative purposes. Furthermore, iMiGUE is designed to evaluate the ability of models to analyze the emotional states by integrating information of recognized micro-gesture, rather than just recognizing prototypes in the sequences separately (or isolatedly). This is because the real need for emotion AI is to understand the emotional states behind gestures in a holistic way. Moreover, to counter for the challenge of imbalanced sample distribution of this dataset, an unsupervised learning method is proposed to capture latent representations from the micro-gesture sequences themselves. We systematically investigate representative methods on this dataset, and comprehensive experimental results reveal several interesting insights from the iMiGUE, e.g., micro-gesture-based analysis can promote emotion understanding. We confirm that the new iMiGUE dataset could advance studies of micro-gesture and emotion AI.



### Explainable Diabetic Retinopathy Detection and Retinal Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2107.00296v1
- **DOI**: 10.1109/JBHI.2021.3110593
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.00296v1)
- **Published**: 2021-07-01 08:30:04+00:00
- **Updated**: 2021-07-01 08:30:04+00:00
- **Authors**: Yuhao Niu, Lin Gu, Yitian Zhao, Feng Lu
- **Comment**: Code is available at https://github.com/zzdyyy/Patho-GAN
- **Journal**: None
- **Summary**: Though deep learning has shown successful performance in classifying the label and severity stage of certain diseases, most of them give few explanations on how to make predictions. Inspired by Koch's Postulates, the foundation in evidence-based medicine (EBM) to identify the pathogen, we propose to exploit the interpretability of deep learning application in medical diagnosis. By determining and isolating the neuron activation patterns on which diabetic retinopathy (DR) detector relies to make decisions, we demonstrate the direct relation between the isolated neuron activation and lesions for a pathological explanation. To be specific, we first define novel pathological descriptors using activated neurons of the DR detector to encode both spatial and appearance information of lesions. Then, to visualize the symptom encoded in the descriptor, we propose Patho-GAN, a new network to synthesize medically plausible retinal images. By manipulating these descriptors, we could even arbitrarily control the position, quantity, and categories of generated lesions. We also show that our synthesized images carry the symptoms directly related to diabetic retinopathy diagnosis. Our generated images are both qualitatively and quantitatively superior to the ones by previous methods. Besides, compared to existing methods that take hours to generate an image, our second level speed endows the potential to be an effective solution for data augmentation.



### Deep Learning for Breast Cancer Classification: Enhanced Tangent Function
- **Arxiv ID**: http://arxiv.org/abs/2108.04663v1
- **DOI**: 10.1111/coin.12476
- **Categories**: **eess.IV**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2108.04663v1)
- **Published**: 2021-07-01 08:36:27+00:00
- **Updated**: 2021-07-01 08:36:27+00:00
- **Authors**: Ashu Thapa, Abeer Alsadoon, P. W. C. Prasad, Simi Bajaj, Omar Hisham Alsadoon, Tarik A. Rashid, Rasha S. Ali, Oday D. Jerew
- **Comment**: 19
- **Journal**: Computational Intelligence, 2021
- **Summary**: Background and Aim: Recently, deep learning using convolutional neural network has been used successfully to classify the images of breast cells accurately. However, the accuracy of manual classification of those histopathological images is comparatively low. This research aims to increase the accuracy of the classification of breast cancer images by utilizing a Patch-Based Classifier (PBC) along with deep learning architecture. Methodology: The proposed system consists of a Deep Convolutional Neural Network (DCNN) that helps in enhancing and increasing the accuracy of the classification process. This is done by the use of the Patch-based Classifier (PBC). CNN has completely different layers where images are first fed through convolutional layers using hyperbolic tangent function together with the max-pooling layer, drop out layers, and SoftMax function for classification. Further, the output obtained is fed to a patch-based classifier that consists of patch-wise classification output followed by majority voting. Results: The results are obtained throughout the classification stage for breast cancer images that are collected from breast-histology datasets. The proposed solution improves the accuracy of classification whether or not the images had normal, benign, in-situ, or invasive carcinoma from 87% to 94% with a decrease in processing time from 0.45 s to 0.2s on average. Conclusion: The proposed solution focused on increasing the accuracy of classifying cancer in the breast by enhancing the image contrast and reducing the vanishing gradient. Finally, this solution for the implementation of the Contrast Limited Adaptive Histogram Equalization (CLAHE) technique and modified tangent function helps in increasing the accuracy.



### Interviewer-Candidate Role Play: Towards Developing Real-World NLP Systems
- **Arxiv ID**: http://arxiv.org/abs/2107.00315v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.00315v1)
- **Published**: 2021-07-01 09:08:43+00:00
- **Updated**: 2021-07-01 09:08:43+00:00
- **Authors**: Neeraj Varshney, Swaroop Mishra, Chitta Baral
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Standard NLP tasks do not incorporate several common real-world scenarios such as seeking clarifications about the question, taking advantage of clues, abstaining in order to avoid incorrect answers, etc. This difference in task formulation hinders the adoption of NLP systems in real-world settings. In this work, we take a step towards bridging this gap and present a multi-stage task that simulates a typical human-human questioner-responder interaction such as an interview. Specifically, the system is provided with question simplifications, knowledge statements, examples, etc. at various stages to improve its prediction when it is not sufficiently confident. We instantiate the proposed task in Natural Language Inference setting where a system is evaluated on both in-domain and out-of-domain (OOD) inputs. We conduct comprehensive experiments and find that the multi-stage formulation of our task leads to OOD generalization performance improvement up to 2.29% in Stage 1, 1.91% in Stage 2, 54.88% in Stage 3, and 72.02% in Stage 4 over the standard unguided prediction. However, our task leaves a significant challenge for NLP researchers to further improve OOD performance at each stage.



### Orthonormal Product Quantization Network for Scalable Face Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2107.00327v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00327v4)
- **Published**: 2021-07-01 09:30:39+00:00
- **Updated**: 2023-05-12 11:56:11+00:00
- **Authors**: Ming Zhang, Xuefei Zhe, Hong Yan
- **Comment**: Published in Pattern Recognition, supplementary material can be found
  in Github project page
- **Journal**: None
- **Summary**: Existing deep quantization methods provided an efficient solution for large-scale image retrieval. However, the significant intra-class variations like pose, illumination, and expressions in face images, still pose a challenge for face image retrieval. In light of this, face image retrieval requires sufficiently powerful learning metrics, which are absent in current deep quantization works. Moreover, to tackle the growing unseen identities in the query stage, face image retrieval drives more demands regarding model generalization and system scalability than general image retrieval tasks. This paper integrates product quantization with orthonormal constraints into an end-to-end deep learning framework to effectively retrieve face images. Specifically, a novel scheme that uses predefined orthonormal vectors as codewords is proposed to enhance the quantization informativeness and reduce codewords' redundancy. A tailored loss function maximizes discriminability among identities in each quantization subspace for both the quantized and original features. An entropy-based regularization term is imposed to reduce the quantization error. Experiments are conducted on four commonly-used face datasets under both seen and unseen identities retrieval settings. Our method outperforms all the compared deep hashing/quantization state-of-the-arts under both settings. Results validate the effectiveness of the proposed orthonormal codewords in improving models' standard retrieval performance and generalization ability. Combing with further experiments on two general image datasets, it demonstrates the broad superiority of our method for scalable image retrieval.



### End-to-end Compression Towards Machine Vision: Network Architecture Design and Optimization
- **Arxiv ID**: http://arxiv.org/abs/2107.00328v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00328v2)
- **Published**: 2021-07-01 09:36:32+00:00
- **Updated**: 2021-11-19 04:06:16+00:00
- **Authors**: Shurun Wang, Zhao Wang, Shiqi Wang, Yan Ye
- **Comment**: None
- **Journal**: None
- **Summary**: The visual signal compression is a long-standing problem. Fueled by the recent advances of deep learning, exciting progress has been made. Despite better compression performance, existing end-to-end compression algorithms are still designed towards better signal quality in terms of rate-distortion optimization. In this paper, we show that the design and optimization of network architecture could be further improved for compression towards machine vision. We propose an inverted bottleneck structure for the encoder of the end-to-end compression towards machine vision, which specifically accounts for efficient representation of the semantic information. Moreover, we quest the capability of optimization by incorporating the analytics accuracy into the optimization process, and the optimality is further explored with generalized rate-accuracy optimization in an iterative manner. We use object detection as a showcase for end-to-end compression towards machine vision, and extensive experiments show that the proposed scheme achieves significant BD-rate savings in terms of analysis performance. Moreover, the promise of the scheme is also demonstrated with strong generalization capability towards other machine vision tasks, due to the enabling of signal-level reconstruction.



### PoliTO-IIT Submission to the EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.00337v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00337v1)
- **Published**: 2021-07-01 10:02:44+00:00
- **Updated**: 2021-07-01 10:02:44+00:00
- **Authors**: Chiara Plizzari, Mirco Planamente, Emanuele Alberti, Barbara Caputo
- **Comment**: 3rd place in the 2021 EPIC-KITCHENS-100 Unsupervised Domain
  Adaptation Challenge for Action Recognition
- **Journal**: None
- **Summary**: In this report, we describe the technical details of our submission to the EPIC-Kitchens-100 Unsupervised Domain Adaptation (UDA) Challenge in Action Recognition. To tackle the domain-shift which exists under the UDA setting, we first exploited a recent Domain Generalization (DG) technique, called Relative Norm Alignment (RNA). It consists in designing a model able to generalize well to any unseen domain, regardless of the possibility to access target data at training time. Then, in a second phase, we extended the approach to work on unlabelled target data, allowing the model to adapt to the target distribution in an unsupervised fashion. For this purpose, we included in our framework existing UDA algorithms, such as Temporal Attentive Adversarial Adaptation Network (TA3N), jointly with new multi-stream consistency losses, namely Temporal Hard Norm Alignment (T-HNA) and Min-Entropy Consistency (MEC). Our submission (entry 'plnet') is visible on the leaderboard and it achieved the 1st position for 'verb', and the 3rd position for both 'noun' and 'action'.



### MASS: Multi-Attentional Semantic Segmentation of LiDAR Data for Dense Top-View Understanding
- **Arxiv ID**: http://arxiv.org/abs/2107.00346v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.00346v2)
- **Published**: 2021-07-01 10:19:32+00:00
- **Updated**: 2022-01-20 15:40:28+00:00
- **Authors**: Kunyu Peng, Juncong Fei, Kailun Yang, Alina Roitberg, Jiaming Zhang, Frank Bieder, Philipp Heidenreich, Christoph Stiller, Rainer Stiefelhagen
- **Comment**: Accepted to IEEE Transactions on Intelligent Transportation Systems
  (T-ITS). Code is publicly available at https://github.com/KPeng9510/MASS
- **Journal**: None
- **Summary**: At the heart of all automated driving systems is the ability to sense the surroundings, e.g., through semantic segmentation of LiDAR sequences, which experienced a remarkable progress due to the release of large datasets such as SemanticKITTI and nuScenes-LidarSeg. While most previous works focus on sparse segmentation of the LiDAR input, dense output masks provide self-driving cars with almost complete environment information. In this paper, we introduce MASS - a Multi-Attentional Semantic Segmentation model specifically built for dense top-view understanding of the driving scenes. Our framework operates on pillar- and occupancy features and comprises three attention-based building blocks: (1) a keypoint-driven graph attention, (2) an LSTM-based attention computed from a vector embedding of the spatial input, and (3) a pillar-based attention, resulting in a dense 360-degree segmentation mask. With extensive experiments on both, SemanticKITTI and nuScenes-LidarSeg, we quantitatively demonstrate the effectiveness of our model, outperforming the state of the art by 19.0% on SemanticKITTI and reaching 30.4% in mIoU on nuScenes-LidarSeg, where MASS is the first work addressing the dense segmentation task. Furthermore, our multi-attention model is shown to be very effective for 3D object detection validated on the KITTI-3D dataset, showcasing its high generalizability to other tasks related to 3D vision.



### Cross-domain Few-shot Learning with Task-specific Adapters
- **Arxiv ID**: http://arxiv.org/abs/2107.00358v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00358v4)
- **Published**: 2021-07-01 10:47:06+00:00
- **Updated**: 2022-05-04 17:02:07+00:00
- **Authors**: Wei-Hong Li, Xialei Liu, Hakan Bilen
- **Comment**: CVPR2022, Code will be available at https://github.com/VICO-UoE/URL
- **Journal**: None
- **Summary**: In this paper, we look at the problem of cross-domain few-shot classification that aims to learn a classifier from previously unseen classes and domains with few labeled samples. Recent approaches broadly solve this problem by parameterizing their few-shot classifiers with task-agnostic and task-specific weights where the former is typically learned on a large training set and the latter is dynamically predicted through an auxiliary network conditioned on a small support set. In this work, we focus on the estimation of the latter, and propose to learn task-specific weights from scratch directly on a small support set, in contrast to dynamically estimating them. In particular, through systematic analysis, we show that task-specific weights through parametric adapters in matrix form with residual connections to multiple intermediate layers of a backbone network significantly improves the performance of the state-of-the-art models in the Meta-Dataset benchmark with minor additional cost.



### Towards Measuring Bias in Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.00360v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2107.00360v1)
- **Published**: 2021-07-01 10:50:39+00:00
- **Updated**: 2021-07-01 10:50:39+00:00
- **Authors**: Nina Schaaf, Omar de Mitri, Hang Beom Kim, Alexander Windberger, Marco F. Huber
- **Comment**: Accepted for publication at the 30th International Conference on
  Artificial Neural Networks (ICANN)
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) have become de fact state-of-the-art for the main computer vision tasks. However, due to the complex underlying structure their decisions are hard to understand which limits their use in some context of the industrial world. A common and hard to detect challenge in machine learning (ML) tasks is data bias. In this work, we present a systematic approach to uncover data bias by means of attribution maps. For this purpose, first an artificial dataset with a known bias is created and used to train intentionally biased CNNs. The networks' decisions are then inspected using attribution maps. Finally, meaningful metrics are used to measure the attribution maps' representativeness with respect to the known bias. The proposed study shows that some attribution map techniques highlight the presence of bias in the data better than others and metrics can support the identification of bias.



### Drone swarm patrolling with uneven coverage requirements
- **Arxiv ID**: http://arxiv.org/abs/2107.00362v1
- **DOI**: 10.1049/iet-cvi.2019.0963
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00362v1)
- **Published**: 2021-07-01 10:58:57+00:00
- **Updated**: 2021-07-01 10:58:57+00:00
- **Authors**: Claudio Piciarelli, Gian Luca Foresti
- **Comment**: This paper has been published on IET Computer Vision. Please cite it
  accordingly (see journal reference below)
- **Journal**: IET Computer Vision, 14: 452-461 (2020)
- **Summary**: Swarms of drones are being more and more used in many practical scenarios, such as surveillance, environmental monitoring, search and rescue in hardly-accessible areas, etc.. While a single drone can be guided by a human operator, the deployment of a swarm of multiple drones requires proper algorithms for automatic task-oriented control. In this paper, we focus on visual coverage optimization with drone-mounted camera sensors. In particular, we consider the specific case in which the coverage requirements are uneven, meaning that different parts of the environment have different coverage priorities. We model these coverage requirements with relevance maps and propose a deep reinforcement learning algorithm to guide the swarm. The paper first defines a proper learning model for a single drone, and then extends it to the case of multiple drones both with greedy and cooperative strategies. Experimental results show the performance of the proposed method, also compared with a standard patrolling algorithm.



### Egocentric Image Captioning for Privacy-Preserved Passive Dietary Intake Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2107.00372v2
- **DOI**: 10.1109/TCYB.2023.3243999
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00372v2)
- **Published**: 2021-07-01 11:16:44+00:00
- **Updated**: 2023-03-01 08:20:17+00:00
- **Authors**: Jianing Qiu, Frank P. -W. Lo, Xiao Gu, Modou L. Jobarteh, Wenyan Jia, Tom Baranowski, Matilda Steiner-Asiedu, Alex K. Anderson, Megan A McCrory, Edward Sazonov, Mingui Sun, Gary Frost, Benny Lo
- **Comment**: None
- **Journal**: IEEE Transactions on Cybernetics, 2023
- **Summary**: Camera-based passive dietary intake monitoring is able to continuously capture the eating episodes of a subject, recording rich visual information, such as the type and volume of food being consumed, as well as the eating behaviours of the subject. However, there currently is no method that is able to incorporate these visual clues and provide a comprehensive context of dietary intake from passive recording (e.g., is the subject sharing food with others, what food the subject is eating, and how much food is left in the bowl). On the other hand, privacy is a major concern while egocentric wearable cameras are used for capturing. In this paper, we propose a privacy-preserved secure solution (i.e., egocentric image captioning) for dietary assessment with passive monitoring, which unifies food recognition, volume estimation, and scene understanding. By converting images into rich text descriptions, nutritionists can assess individual dietary intake based on the captions instead of the original images, reducing the risk of privacy leakage from images. To this end, an egocentric dietary image captioning dataset has been built, which consists of in-the-wild images captured by head-worn and chest-worn cameras in field studies in Ghana. A novel transformer-based architecture is designed to caption egocentric dietary images. Comprehensive experiments have been conducted to evaluate the effectiveness and to justify the design of the proposed architecture for egocentric dietary image captioning. To the best of our knowledge, this is the first work that applies image captioning for dietary intake assessment in real life settings.



### SSC: Semantic Scan Context for Large-Scale Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.00382v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.00382v2)
- **Published**: 2021-07-01 11:51:19+00:00
- **Updated**: 2021-07-10 07:47:00+00:00
- **Authors**: Lin Li, Xin Kong, Xiangrui Zhao, Tianxin Huang, Yong Liu
- **Comment**: 8 pages, Accepted by IROS-2021
- **Journal**: None
- **Summary**: Place recognition gives a SLAM system the ability to correct cumulative errors. Unlike images that contain rich texture features, point clouds are almost pure geometric information which makes place recognition based on point clouds challenging. Existing works usually encode low-level features such as coordinate, normal, reflection intensity, etc., as local or global descriptors to represent scenes. Besides, they often ignore the translation between point clouds when matching descriptors. Different from most existing methods, we explore the use of high-level features, namely semantics, to improve the descriptor's representation ability. Also, when matching descriptors, we try to correct the translation between point clouds to improve accuracy. Concretely, we propose a novel global descriptor, Semantic Scan Context, which explores semantic information to represent scenes more effectively. We also present a two-step global semantic ICP to obtain the 3D pose (x, y, yaw) used to align the point cloud to improve matching performance. Our experiments on the KITTI dataset show that our approach outperforms the state-of-the-art methods with a large margin. Our code is available at: https://github.com/lilin-hitcrt/SSC.



### GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph
- **Arxiv ID**: http://arxiv.org/abs/2107.00395v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00395v1)
- **Published**: 2021-07-01 12:14:05+00:00
- **Updated**: 2021-07-01 12:14:05+00:00
- **Authors**: Yunxin Li, Yu Zhao, Baotian Hu, Qingcai Chen, Yang Xiang, Xiaolong Wang, Yuxin Ding, Lin Ma
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Previous works indicate that the glyph of Chinese characters contains rich semantic information and has the potential to enhance the representation of Chinese characters. The typical method to utilize the glyph features is by incorporating them into the character embedding space. Inspired by previous methods, we innovatively propose a Chinese pre-trained representation model named as GlyphCRM, which abandons the ID-based character embedding method yet solely based on sequential character images. We render each character into a binary grayscale image and design two-channel position feature maps for it. Formally, we first design a two-layer residual convolutional neural network, namely HanGlyph to generate the initial glyph representation of Chinese characters, and subsequently adopt multiple bidirectional encoder Transformer blocks as the superstructure to capture the context-sensitive information. Meanwhile, we feed the glyph features extracted from each layer of the HanGlyph module into the underlying Transformer blocks by skip-connection method to fully exploit the glyph features of Chinese characters. As the HanGlyph module can obtain a sufficient glyph representation of any Chinese character, the long-standing out-of-vocabulary problem could be effectively solved. Extensive experimental results indicate that GlyphCRM substantially outperforms the previous BERT-based state-of-the-art model on 9 fine-tuning tasks, and it has strong transferability and generalization on specialized fields and low-resource tasks. We hope this work could spark further research beyond the realms of well-established representation of Chinese texts.



### MIDV-2020: A Comprehensive Benchmark Dataset for Identity Document Analysis
- **Arxiv ID**: http://arxiv.org/abs/2107.00396v1
- **DOI**: 10.18287/2412-6179-CO-1006
- **Categories**: **cs.CV**, cs.DL, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/2107.00396v1)
- **Published**: 2021-07-01 12:14:17+00:00
- **Updated**: 2021-07-01 12:14:17+00:00
- **Authors**: Konstantin Bulatov, Ekaterina Emelianova, Daniil Tropin, Natalya Skoryukina, Yulia Chernyshova, Alexander Sheshkus, Sergey Usilin, Zuheng Ming, Jean-Christophe Burie, Muhammad Muzzamil Luqman, Vladimir V. Arlazarov
- **Comment**: None
- **Journal**: Computer Optics, volume 46, issue 2, p. 252-270, 2022
- **Summary**: Identity documents recognition is an important sub-field of document analysis, which deals with tasks of robust document detection, type identification, text fields recognition, as well as identity fraud prevention and document authenticity validation given photos, scans, or video frames of an identity document capture. Significant amount of research has been published on this topic in recent years, however a chief difficulty for such research is scarcity of datasets, due to the subject matter being protected by security requirements. A few datasets of identity documents which are available lack diversity of document types, capturing conditions, or variability of document field values. In addition, the published datasets were typically designed only for a subset of document recognition problems, not for a complex identity document analysis. In this paper, we present a dataset MIDV-2020 which consists of 1000 video clips, 2000 scanned images, and 1000 photos of 1000 unique mock identity documents, each with unique text field values and unique artificially generated faces, with rich annotation. For the presented benchmark dataset baselines are provided for such tasks as document location and identification, text fields recognition, and face detection. With 72409 annotated images in total, to the date of publication the proposed dataset is the largest publicly available identity documents dataset with variable artificially generated data, and we believe that it will prove invaluable for advancement of the field of document analysis and recognition. The dataset is available for download at ftp://smartengines.com/midv-2020 and http://l3i-share.univ-lr.fr .



### Lossless Coding of Point Cloud Geometry using a Deep Generative Model
- **Arxiv ID**: http://arxiv.org/abs/2107.00400v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.00400v1)
- **Published**: 2021-07-01 12:20:22+00:00
- **Updated**: 2021-07-01 12:20:22+00:00
- **Authors**: Dat Thanh Nguyen, Maurice Quach, Giuseppe Valenzise, Pierre Duhamel
- **Comment**: This paper has been submitted to the IEEE Transactions on Circuits
  and Systems for Video Technology (TCSVT). arXiv admin note: text overlap with
  arXiv:2011.14700
- **Journal**: None
- **Summary**: This paper proposes a lossless point cloud (PC) geometry compression method that uses neural networks to estimate the probability distribution of voxel occupancy. First, to take into account the PC sparsity, our method adaptively partitions a point cloud into multiple voxel block sizes. This partitioning is signalled via an octree. Second, we employ a deep auto-regressive generative model to estimate the occupancy probability of each voxel given the previously encoded ones. We then employ the estimated probabilities to code efficiently a block using a context-based arithmetic coder. Our context has variable size and can expand beyond the current block to learn more accurate probabilities. We also consider using data augmentation techniques to increase the generalization capability of the learned probability models, in particular in the presence of noise and lower-density point clouds. Experimental evaluation, performed on a variety of point clouds from four different datasets and with diverse characteristics, demonstrates that our method reduces significantly (by up to 30%) the rate for lossless coding compared to the state-of-the-art MPEG codec.



### Quality Metrics for Transparent Machine Learning With and Without Humans In the Loop Are Not Correlated
- **Arxiv ID**: http://arxiv.org/abs/2107.02033v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02033v1)
- **Published**: 2021-07-01 12:30:51+00:00
- **Updated**: 2021-07-01 12:30:51+00:00
- **Authors**: Felix Biessmann, Dionysius Refiano
- **Comment**: Proceedings of the ICML Workshop on Theoretical Foundations,
  Criticism, and Application Trends of Explainable AI held in conjunction with
  the 38th International Conference on Machine Learning (ICML), a
  non-peer-reviewed longer version was previously published as preprint here
  arXiv:1912.05011
- **Journal**: None
- **Summary**: The field explainable artificial intelligence (XAI) has brought about an arsenal of methods to render Machine Learning (ML) predictions more interpretable. But how useful explanations provided by transparent ML methods are for humans remains difficult to assess. Here we investigate the quality of interpretable computer vision algorithms using techniques from psychophysics. In crowdsourced annotation tasks we study the impact of different interpretability approaches on annotation accuracy and task time. We compare these quality metrics with classical XAI, automated quality metrics. Our results demonstrate that psychophysical experiments allow for robust quality assessment of transparency in machine learning. Interestingly the quality metrics computed without humans in the loop did not provide a consistent ranking of interpretability methods nor were they representative for how useful an explanation was for humans. These findings highlight the potential of methods from classical psychophysics for modern machine learning applications. We hope that our results provide convincing arguments for evaluating interpretability in its natural habitat, human-ML interaction, if the goal is to obtain an authentic assessment of interpretability.



### DVS-Attacks: Adversarial Attacks on Dynamic Vision Sensors for Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.00415v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.00415v1)
- **Published**: 2021-07-01 12:56:36+00:00
- **Updated**: 2021-07-01 12:56:36+00:00
- **Authors**: Alberto Marchisio, Giacomo Pira, Maurizio Martina, Guido Masera, Muhammad Shafique
- **Comment**: Accepted for publication at IJCNN 2021
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNNs), despite being energy-efficient when implemented on neuromorphic hardware and coupled with event-based Dynamic Vision Sensors (DVS), are vulnerable to security threats, such as adversarial attacks, i.e., small perturbations added to the input for inducing a misclassification. Toward this, we propose DVS-Attacks, a set of stealthy yet efficient adversarial attack methodologies targeted to perturb the event sequences that compose the input of the SNNs. First, we show that noise filters for DVS can be used as defense mechanisms against adversarial attacks. Afterwards, we implement several attacks and test them in the presence of two types of noise filters for DVS cameras. The experimental results show that the filters can only partially defend the SNNs against our proposed DVS-Attacks. Using the best settings for the noise filters, our proposed Mask Filter-Aware Dash Attack reduces the accuracy by more than 20% on the DVS-Gesture dataset and by more than 65% on the MNIST dataset, compared to the original clean frames. The source code of all the proposed DVS-Attacks and noise filters is released at https://github.com/albertomarchisio/DVS-Attacks.



### Supervised Segmentation with Domain Adaptation for Small Sampled Orbital CT Images
- **Arxiv ID**: http://arxiv.org/abs/2107.00418v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00418v1)
- **Published**: 2021-07-01 13:00:33+00:00
- **Updated**: 2021-07-01 13:00:33+00:00
- **Authors**: Sungho Suh, Sojeong Cheon, Wonseo Choi, Yeon Woong Chung, Won-Kyung Cho, Ji-Sun Paik, Sung Eun Kim, Dong-Jin Chang, Yong Oh Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have been widely used for medical image analysis. However, the lack of access a to large-scale annotated dataset poses a great challenge, especially in the case of rare diseases, or new domains for the research society. Transfer of pre-trained features, from the relatively large dataset is a considerable solution. In this paper, we have explored supervised segmentation using domain adaptation for optic nerve and orbital tumor, when only small sampled CT images are given. Even the lung image database consortium image collection (LIDC-IDRI) is a cross-domain to orbital CT, but the proposed domain adaptation method improved the performance of attention U-Net for the segmentation in public optic nerve dataset and our clinical orbital tumor dataset. The code and dataset are available at https://github.com/cmcbigdata.



### CBNet: A Composite Backbone Network Architecture for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.00420v7
- **DOI**: 10.1109/TIP.2022.3216771
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00420v7)
- **Published**: 2021-07-01 13:05:11+00:00
- **Updated**: 2022-10-18 05:09:09+00:00
- **Authors**: Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang, Zhi Tang, Wei Chu, Jingdong Chen, Haibin Ling
- **Comment**: IEEE Transactions on Image Processing (TIP) camera ready
- **Journal**: None
- **Summary**: Modern top-performing object detectors depend heavily on backbone networks, whose advances bring consistent performance gains through exploring more effective network structures. In this paper, we propose a novel and flexible backbone framework, namely CBNetV2, to construct high-performance detectors using existing open-sourced pre-trained backbones under the pre-training fine-tuning paradigm. In particular, CBNetV2 architecture groups multiple identical backbones, which are connected through composite connections. Specifically, it integrates the high- and low-level features of multiple backbone networks and gradually expands the receptive field to more efficiently perform object detection. We also propose a better training strategy with assistant supervision for CBNet-based detectors. Without additional pre-training of the composite backbone, CBNetV2 can be adapted to various backbones (CNN-based vs. Transformer-based) and head designs of most mainstream detectors (one-stage vs. two-stage, anchor-based vs. anchor-free-based). Experiments provide strong evidence that, compared with simply increasing the depth and width of the network, CBNetV2 introduces a more efficient, effective, and resource-friendly way to build high-performance backbone networks. Particularly, our Dual-Swin-L achieves 59.4% box AP and 51.6% mask AP on COCO test-dev under the single-model and single-scale testing protocol, which is significantly better than the state-of-the-art result (57.7% box AP and 50.2% mask AP) achieved by Swin-L, while the training schedule is reduced by 6$\times$. With multi-scale testing, we push the current best single model result to a new record of 60.1% box AP and 52.3% mask AP without using extra training data. Code is available at https://github.com/VDIGPKU/CBNetV2.



### Generating Synthetic Training Data for Deep Learning-Based UAV Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2107.00422v2
- **DOI**: 10.5220/0010621400003061
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00422v2)
- **Published**: 2021-07-01 13:08:31+00:00
- **Updated**: 2021-11-01 08:52:25+00:00
- **Authors**: Stefan Becker, Ronny Hug, Wolfgang Hübner, Michael Arens, Brendan T. Morris
- **Comment**: Accepted at the International Conference on Robotics, Computer Vision
  and Intelligent Systems (ROBOVIS) 2021
- **Journal**: None
- **Summary**: Deep learning-based models, such as recurrent neural networks (RNNs), have been applied to various sequence learning tasks with great success. Following this, these models are increasingly replacing classic approaches in object tracking applications for motion prediction. On the one hand, these models can capture complex object dynamics with less modeling required, but on the other hand, they depend on a large amount of training data for parameter tuning. Towards this end, we present an approach for generating synthetic trajectory data of unmanned-aerial-vehicles (UAVs) in image space. Since UAVs, or rather quadrotors are dynamical systems, they can not follow arbitrary trajectories. With the prerequisite that UAV trajectories fulfill a smoothness criterion corresponding to a minimal change of higher-order motion, methods for planning aggressive quadrotors flights can be utilized to generate optimal trajectories through a sequence of 3D waypoints. By projecting these maneuver trajectories, which are suitable for controlling quadrotors, to image space, a versatile trajectory data set is realized. To demonstrate the applicability of the synthetic trajectory data, we show that an RNN-based prediction model solely trained on the generated data can outperform classic reference models on a real-world UAV tracking dataset. The evaluation is done on the publicly available ANTI-UAV dataset.



### Language-Level Semantics Conditioned 3D Point Cloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.00430v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00430v3)
- **Published**: 2021-07-01 13:21:49+00:00
- **Updated**: 2022-03-30 03:01:34+00:00
- **Authors**: Bo Liu, Shuang Deng, Qiulei Dong, Zhanyi Hu
- **Comment**: 13 pages, 7 figures
- **Journal**: None
- **Summary**: In this work, a language-level Semantics Conditioned framework for 3D Point cloud segmentation, called SeCondPoint, is proposed, where language-level semantics are introduced to condition the modeling of point feature distribution as well as the pseudo-feature generation, and a feature-geometry-based mixup approach is further proposed to facilitate the distribution learning. To our knowledge, this is the first attempt in literature to introduce language-level semantics to the 3D point cloud segmentation task. Since a large number of point features could be generated from the learned distribution thanks to the semantics conditioned modeling, any existing segmentation network could be embedded into the proposed framework to boost its performance. In addition, the proposed framework has the inherent advantage of dealing with novel classes, which seems an impossible feat for the current segmentation networks. Extensive experimental results on two public datasets demonstrate that three typical segmentation networks could achieve significant improvements over their original performances after enhancement by the proposed framework in the conventional 3D segmentation task. Two benchmarks are also introduced for a newly introduced zero-shot 3D segmentation task, and the results also validate the proposed framework.



### Learning to Disambiguate Strongly Interacting Hands via Probabilistic Per-pixel Part Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.00434v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00434v2)
- **Published**: 2021-07-01 13:28:02+00:00
- **Updated**: 2021-11-28 11:13:05+00:00
- **Authors**: Zicong Fan, Adrian Spurr, Muhammed Kocabas, Siyu Tang, Michael J. Black, Otmar Hilliges
- **Comment**: None
- **Journal**: None
- **Summary**: In natural conversation and interaction, our hands often overlap or are in contact with each other. Due to the homogeneous appearance of hands, this makes estimating the 3D pose of interacting hands from images difficult. In this paper we demonstrate that self-similarity, and the resulting ambiguities in assigning pixel observations to the respective hands and their parts, is a major cause of the final 3D pose error. Motivated by this insight, we propose DIGIT, a novel method for estimating the 3D poses of two interacting hands from a single monocular image. The method consists of two interwoven branches that process the input imagery into a per-pixel semantic part segmentation mask and a visual feature volume. In contrast to prior work, we do not decouple the segmentation from the pose estimation stage, but rather leverage the per-pixel probabilities directly in the downstream pose estimation task. To do so, the part probabilities are merged with the visual features and processed via fully-convolutional layers. We experimentally show that the proposed approach achieves new state-of-the-art performance on the InterHand2.6M dataset. We provide detailed ablation studies to demonstrate the efficacy of our method and to provide insights into how the modelling of pixel ownership affects 3D hand pose estimation.



### Overhead-MNIST: Machine Learning Baselines for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.00436v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.00436v2)
- **Published**: 2021-07-01 13:30:39+00:00
- **Updated**: 2021-10-19 20:31:26+00:00
- **Authors**: Erik Larsen, David Noever, Korey MacVittie, John Lilly
- **Comment**: 6 pages; 8 figures, 2 tables
- **Journal**: None
- **Summary**: Twenty-three machine learning algorithms were trained then scored to establish baseline comparison metrics and to select an image classification algorithm worthy of embedding into mission-critical satellite imaging systems. The Overhead-MNIST dataset is a collection of satellite images similar in style to the ubiquitous MNIST hand-written digits found in the machine learning literature. The CatBoost classifier, Light Gradient Boosting Machine, and Extreme Gradient Boosting models produced the highest accuracies, Areas Under the Curve (AUC), and F1 scores in a PyCaret general comparison. Separate evaluations showed that a deep convolutional architecture was the most promising. We present results for the overall best performing algorithm as a baseline for edge deployability and future performance improvement: a convolutional neural network (CNN) scoring 0.965 categorical accuracy on unseen test data.



### VideoLightFormer: Lightweight Action Recognition using Transformers
- **Arxiv ID**: http://arxiv.org/abs/2107.00451v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.00451v2)
- **Published**: 2021-07-01 13:55:52+00:00
- **Updated**: 2021-11-18 11:53:30+00:00
- **Authors**: Raivo Koot, Haiping Lu
- **Comment**: Rejected at NeurIPS 2021. Paper withdrawn from arxiv
- **Journal**: None
- **Summary**: Efficient video action recognition remains a challenging problem. One large model after another takes the place of the state-of-the-art on the Kinetics dataset, but real-world efficiency evaluations are often lacking. In this work, we fill this gap and investigate the use of transformers for efficient action recognition. We propose a novel, lightweight action recognition architecture, VideoLightFormer. In a factorized fashion, we carefully extend the 2D convolutional Temporal Segment Network with transformers, while maintaining spatial and temporal video structure throughout the entire model. Existing methods often resort to one of the two extremes, where they either apply huge transformers to video features, or minimal transformers on highly pooled video features. Our method differs from them by keeping the transformer models small, but leveraging full spatiotemporal feature structure. We evaluate VideoLightFormer in a high-efficiency setting on the temporally-demanding EPIC-KITCHENS-100 and Something-Something-V2 (SSV2) datasets and find that it achieves a better mix of efficiency and accuracy than existing state-of-the-art models, apart from the Temporal Shift Module on SSV2.



### On the detection-to-track association for online multi-object tracking
- **Arxiv ID**: http://arxiv.org/abs/2107.00500v1
- **DOI**: 10.1016/j.patrec.2021.03.022
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00500v1)
- **Published**: 2021-07-01 14:44:12+00:00
- **Updated**: 2021-07-01 14:44:12+00:00
- **Authors**: Xufeng Lin, Chang-Tsun Li, Victor Sanchez, Carsten Maple
- **Comment**: None
- **Journal**: Pattern Recognition Letters 146 (2021) 200-207
- **Summary**: Driven by recent advances in object detection with deep neural networks, the tracking-by-detection paradigm has gained increasing prevalence in the research community of multi-object tracking (MOT). It has long been known that appearance information plays an essential role in the detection-to-track association, which lies at the core of the tracking-by-detection paradigm. While most existing works consider the appearance distances between the detections and the tracks, they ignore the statistical information implied by the historical appearance distance records in the tracks, which can be particularly useful when a detection has similar distances with two or more tracks. In this work, we propose a hybrid track association (HTA) algorithm that models the historical appearance distances of a track with an incremental Gaussian mixture model (IGMM) and incorporates the derived statistical information into the calculation of the detection-to-track association cost. Experimental results on three MOT benchmarks confirm that HTA effectively improves the target identification performance with a small compromise to the tracking speed. Additionally, compared to many state-of-the-art trackers, the DeepSORT tracker equipped with HTA achieves better or comparable performance in terms of the balance of tracking quality and speed.



### Improving Human Motion Prediction Through Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.00544v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00544v1)
- **Published**: 2021-07-01 15:34:41+00:00
- **Updated**: 2021-07-01 15:34:41+00:00
- **Authors**: Mohammad Samin Yasar, Tariq Iqbal
- **Comment**: None
- **Journal**: None
- **Summary**: Human motion prediction is an essential component for enabling closer human-robot collaboration. The task of accurately predicting human motion is non-trivial. It is compounded by the variability of human motion, both at a skeletal level due to the varying size of humans and at a motion level due to individual movement's idiosyncrasies. These variables make it challenging for learning algorithms to obtain a general representation that is robust to the diverse spatio-temporal patterns of human motion. In this work, we propose a modular sequence learning approach that allows end-to-end training while also having the flexibility of being fine-tuned. Our approach relies on the diversity of training samples to first learn a robust representation, which can then be fine-tuned in a continual learning setup to predict the motion of new subjects. We evaluated the proposed approach by comparing its performance against state-of-the-art baselines. The results suggest that our approach outperforms other methods over all the evaluated temporal horizons, using a small amount of data for fine-tuning. The improved performance of our approach opens up the possibility of using continual learning for personalized and reliable motion prediction.



### Inter Extreme Points Geodesics for End-to-End Weakly Supervised Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.00583v3
- **DOI**: 10.1007/978-3-030-87196-3_57
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00583v3)
- **Published**: 2021-07-01 16:16:50+00:00
- **Updated**: 2021-11-04 18:33:40+00:00
- **Authors**: Reuben Dorent, Samuel Joutard, Jonathan Shapey, Aaron Kujawa, Marc Modat, Sebastien Ourselin, Tom Vercauteren
- **Comment**: Early accept at MICCAI 2021 - code available at:
  https://github.com/ReubenDo/InExtremIS
- **Journal**: MICCAI 2021 pp 615-624
- **Summary**: We introduce $\textit{InExtremIS}$, a weakly supervised 3D approach to train a deep image segmentation network using particularly weak train-time annotations: only 6 extreme clicks at the boundary of the objects of interest. Our fully-automatic method is trained end-to-end and does not require any test-time annotations. From the extreme points, 3D bounding boxes are extracted around objects of interest. Then, deep geodesics connecting extreme points are generated to increase the amount of "annotated" voxels within the bounding boxes. Finally, a weakly supervised regularised loss derived from a Conditional Random Field formulation is used to encourage prediction consistency over homogeneous regions. Extensive experiments are performed on a large open dataset for Vestibular Schwannoma segmentation. $\textit{InExtremIS}$ obtained competitive performance, approaching full supervision and outperforming significantly other weakly supervised techniques based on bounding boxes. Moreover, given a fixed annotation time budget, $\textit{InExtremIS}$ outperforms full supervision. Our code and data are available online.



### 3D Iterative Spatiotemporal Filtering for Classification of Multitemporal Satellite Data Sets
- **Arxiv ID**: http://arxiv.org/abs/2107.00590v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00590v1)
- **Published**: 2021-07-01 16:26:52+00:00
- **Updated**: 2021-07-01 16:26:52+00:00
- **Authors**: Hessah Albanwan, Rongjun Qin, Xiaohu Lu, Mao Li, Desheng Liu, Jean-Michel Guldmann
- **Comment**: None
- **Journal**: None
- **Summary**: The current practice in land cover/land use change analysis relies heavily on the individually classified maps of the multitemporal data set. Due to varying acquisition conditions (e.g., illumination, sensors, seasonal differences), the classification maps yielded are often inconsistent through time for robust statistical analysis. 3D geometric features have been shown to be stable for assessing differences across the temporal data set. Therefore, in this article we investigate he use of a multitemporal orthophoto and digital surface model derived from satellite data for spatiotemporal classification. Our approach consists of two major steps: generating per-class probability distribution maps using the random-forest classifier with limited training samples, and making spatiotemporal inferences using an iterative 3D spatiotemporal filter operating on per-class probability maps. Our experimental results demonstrate that the proposed methods can consistently improve the individual classification results by 2%-6% and thus can be an important postclassification refinement approach.



### Individual Tree Detection and Crown Delineation with 3D Information from Multi-view Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2107.00592v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00592v1)
- **Published**: 2021-07-01 16:28:43+00:00
- **Updated**: 2021-07-01 16:28:43+00:00
- **Authors**: Changlin Xiao, Rongjun Qin, Xiao Xie, Xu Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Individual tree detection and crown delineation (ITDD) are critical in forest inventory management and remote sensing based forest surveys are largely carried out through satellite images. However, most of these surveys only use 2D spectral information which normally has not enough clues for ITDD. To fully explore the satellite images, we propose a ITDD method using the orthophoto and digital surface model (DSM) derived from the multi-view satellite data. Our algorithm utilizes the top-hat morphological operation to efficiently extract the local maxima from DSM as treetops, and then feed them to a modi-fied superpixel segmentation that combines both 2D and 3D information for tree crown delineation. In subsequent steps, our method incorporates the biological characteristics of the crowns through plant allometric equation to falsify potential outliers. Experiments against manually marked tree plots on three representative regions have demonstrated promising results - the best overall detection accuracy can be 89%.



### A Unified Framework of Bundle Adjustment and Feature Matching for High-Resolution Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2107.00598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00598v1)
- **Published**: 2021-07-01 16:40:25+00:00
- **Updated**: 2021-07-01 16:40:25+00:00
- **Authors**: Xiao Ling, Xu Huang, Rongjun Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Bundle adjustment (BA) is a technique for refining sensor orientations of satellite images, while adjustment accuracy is correlated with feature matching results. Feature match-ing often contains high uncertainties in weak/repeat textures, while BA results are helpful in reducing these uncertainties. To compute more accurate orientations, this article incorpo-rates BA and feature matching in a unified framework and formulates the union as the optimization of a global energy function so that the solutions of the BA and feature matching are constrained with each other. To avoid a degeneracy in the optimization, we propose a comprised solution by breaking the optimization of the global energy function into two-step suboptimizations and compute the local minimums of each suboptimization in an incremental manner. Experiments on multi-view high-resolution satellite images show that our proposed method outperforms state-of-the-art orientation techniques with or without accurate least-squares matching.



### Action Transformer: A Self-Attention Model for Short-Time Pose-Based Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.00606v6
- **DOI**: 10.1016/j.patcog.2021.108487
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.00606v6)
- **Published**: 2021-07-01 16:53:16+00:00
- **Updated**: 2022-01-10 08:42:16+00:00
- **Authors**: Vittorio Mazzia, Simone Angarano, Francesco Salvetti, Federico Angelini, Marcello Chiaberge
- **Comment**: Published by Pattern Recognition, Elsevier
- **Journal**: Pattern Recognition, Volume 124, April 2022, 108487
- **Summary**: Deep neural networks based purely on attention have been successful across several domains, relying on minimal architectural priors from the designer. In Human Action Recognition (HAR), attention mechanisms have been primarily adopted on top of standard convolutional or recurrent layers, improving the overall generalization capability. In this work, we introduce Action Transformer (AcT), a simple, fully self-attentional architecture that consistently outperforms more elaborated networks that mix convolutional, recurrent and attentive layers. In order to limit computational and energy requests, building on previous human action recognition research, the proposed approach exploits 2D pose representations over small temporal windows, providing a low latency solution for accurate and effective real-time performance. Moreover, we open-source MPOSE2021, a new large-scale dataset, as an attempt to build a formal training and evaluation benchmark for real-time, short-time HAR. The proposed methodology was extensively tested on MPOSE2021 and compared to several state-of-the-art architectures, proving the effectiveness of the AcT model and laying the foundations for future work on HAR.



### Semi-Sparsity for Smoothing Filters
- **Arxiv ID**: http://arxiv.org/abs/2107.00627v3
- **DOI**: 10.1109/TIP.2023.3247181
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00627v3)
- **Published**: 2021-07-01 17:31:42+00:00
- **Updated**: 2023-03-07 10:34:32+00:00
- **Authors**: Junqing Huang, Haihui Wang, Xuechao Wang, Michael Ruzhansky
- **Comment**: Final version but delete the graphic processing part
- **Journal**: IEEE Transactions on Image Processing, 2023
- **Summary**: In this paper, we propose an interesting semi-sparsity smoothing algorithm based on a novel sparsity-inducing optimization framework. This method is derived from the multiple observations that semi-sparsity prior knowledge is more universally applicable, especially in areas where sparsity is not fully admitted, such as polynomial-smoothing surfaces. We illustrate that this semi-sparsity can be identified into a generalized $L_0$-norm minimization in higher-order gradient domains, thereby giving rise to a new "feature-aware" filtering method with a powerful simultaneous-fitting ability in both sparse features (singularities and sharpening edges) and non-sparse regions (polynomial-smoothing surfaces). Notice that a direct solver is always unavailable due to the non-convexity and combinatorial nature of $L_0$-norm minimization. Instead, we solve the model based on an efficient half-quadratic splitting minimization with fast Fourier transforms (FFTs) for acceleration. We finally demonstrate its versatility and many benefits to a series of signal/image processing and computer vision applications.



### Generalization and Robustness Implications in Object-Centric Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.00637v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2107.00637v3)
- **Published**: 2021-07-01 17:51:11+00:00
- **Updated**: 2022-06-09 16:34:33+00:00
- **Authors**: Andrea Dittadi, Samuele Papa, Michele De Vita, Bernhard Schölkopf, Ole Winther, Francesco Locatello
- **Comment**: Published at ICML 2022
- **Journal**: None
- **Summary**: The idea behind object-centric representation learning is that natural scenes can better be modeled as compositions of objects and their relations as opposed to distributed representations. This inductive bias can be injected into neural networks to potentially improve systematic generalization and performance of downstream tasks in scenes with multiple objects. In this paper, we train state-of-the-art unsupervised models on five common multi-object datasets and evaluate segmentation metrics and downstream object property prediction. In addition, we study generalization and robustness by investigating the settings where either a single object is out of distribution -- e.g., having an unseen color, texture, or shape -- or global properties of the scene are altered -- e.g., by occlusions, cropping, or increasing the number of objects. From our experimental study, we find object-centric representations to be useful for downstream tasks and generally robust to most distribution shifts affecting objects. However, when the distribution shift affects the input in a less structured manner, robustness in terms of segmentation and downstream task performance may vary significantly across models and distribution shifts.



### Focal Self-attention for Local-Global Interactions in Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2107.00641v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.00641v1)
- **Published**: 2021-07-01 17:56:09+00:00
- **Updated**: 2021-07-01 17:56:09+00:00
- **Authors**: Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, Jianfeng Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Vision Transformer and its variants have shown great promise on various computer vision tasks. The ability of capturing short- and long-range visual dependencies through self-attention is arguably the main source for the success. But it also brings challenges due to quadratic computational overhead, especially for the high-resolution vision tasks (e.g., object detection). In this paper, we present focal self-attention, a new mechanism that incorporates both fine-grained local and coarse-grained global interactions. Using this new mechanism, each token attends the closest surrounding tokens at fine granularity but the tokens far away at coarse granularity, and thus can capture both short- and long-range visual dependencies efficiently and effectively. With focal self-attention, we propose a new variant of Vision Transformer models, called Focal Transformer, which achieves superior performance over the state-of-the-art vision Transformers on a range of public image classification and object detection benchmarks. In particular, our Focal Transformer models with a moderate size of 51.1M and a larger size of 89.8M achieve 83.5 and 83.8 Top-1 accuracy, respectively, on ImageNet classification at 224x224 resolution. Using Focal Transformers as the backbones, we obtain consistent and substantial improvements over the current state-of-the-art Swin Transformers for 6 different object detection methods trained with standard 1x and 3x schedules. Our largest Focal Transformer yields 58.7/58.9 box mAPs and 50.9/51.3 mask mAPs on COCO mini-val/test-dev, and 55.4 mIoU on ADE20K for semantic segmentation, creating new SoTA on three of the most challenging computer vision tasks.



### Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.00644v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.00644v2)
- **Published**: 2021-07-01 17:58:05+00:00
- **Updated**: 2021-12-09 16:25:39+00:00
- **Authors**: Nicklas Hansen, Hao Su, Xiaolong Wang
- **Comment**: Code and videos are available at https://nicklashansen.github.io/SVEA
- **Journal**: None
- **Summary**: While agents trained by Reinforcement Learning (RL) can solve increasingly challenging tasks directly from visual observations, generalizing learned skills to novel environments remains very challenging. Extensive use of data augmentation is a promising technique for improving generalization in RL, but it is often found to decrease sample efficiency and can even lead to divergence. In this paper, we investigate causes of instability when using data augmentation in common off-policy RL algorithms. We identify two problems, both rooted in high-variance Q-targets. Based on our findings, we propose a simple yet effective technique for stabilizing this class of algorithms under augmentation. We perform extensive empirical evaluation of image-based RL using both ConvNets and Vision Transformers (ViT) on a family of benchmarks based on DeepMind Control Suite, as well as in robotic manipulation tasks. Our method greatly improves stability and sample efficiency of ConvNets under augmentation, and achieves generalization results competitive with state-of-the-art methods for image-based RL in environments with unseen visuals. We further show that our method scales to RL with ViT-based architectures, and that data augmentation may be especially important in this setting.



### Global Filter Networks for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.00645v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.00645v2)
- **Published**: 2021-07-01 17:58:16+00:00
- **Updated**: 2021-10-26 13:21:45+00:00
- **Authors**: Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, Jie Zhou
- **Comment**: Accepted to NeurIPS 2021. Project page:
  https://gfnet.ivg-research.xyz/
- **Journal**: None
- **Summary**: Recent advances in self-attention and pure multi-layer perceptrons (MLP) models for vision have shown great potential in achieving promising performance with fewer inductive biases. These models are generally based on learning interaction among spatial locations from raw data. The complexity of self-attention and MLP grows quadratically as the image size increases, which makes these models hard to scale up when high-resolution features are required. In this paper, we present the Global Filter Network (GFNet), a conceptually simple yet computationally efficient architecture, that learns long-term spatial dependencies in the frequency domain with log-linear complexity. Our architecture replaces the self-attention layer in vision transformers with three key operations: a 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and a 2D inverse Fourier transform. We exhibit favorable accuracy/complexity trade-offs of our models on both ImageNet and downstream tasks. Our results demonstrate that GFNet can be a very competitive alternative to transformer-style models and CNNs in efficiency, generalization ability and robustness. Code is available at https://github.com/raoyongming/GFNet



### Learning to See before Learning to Act: Visual Pre-training for Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2107.00646v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00646v1)
- **Published**: 2021-07-01 17:58:37+00:00
- **Updated**: 2021-07-01 17:58:37+00:00
- **Authors**: Lin Yen-Chen, Andy Zeng, Shuran Song, Phillip Isola, Tsung-Yi Lin
- **Comment**: Accepted to ICRA 2020. Porject page:
  http://yenchenlin.me/vision2action/
- **Journal**: None
- **Summary**: Does having visual priors (e.g. the ability to detect objects) facilitate learning to perform vision-based manipulation (e.g. picking up objects)? We study this problem under the framework of transfer learning, where the model is first trained on a passive vision task, and adapted to perform an active manipulation task. We find that pre-training on vision tasks significantly improves generalization and sample efficiency for learning to manipulate objects. However, realizing these gains requires careful selection of which parts of the model to transfer. Our key insight is that outputs of standard vision models highly correlate with affordance maps commonly used in manipulation. Therefore, we explore directly transferring model parameters from vision networks to affordance prediction networks, and show that this can result in successful zero-shot adaptation, where a robot can pick up certain objects with zero robotic experience. With just a small amount of robotic experience, we can further fine-tune the affordance model to achieve better results. With just 10 minutes of suction experience or 1 hour of grasping experience, our method achieves ~80% success rate at picking up novel objects.



### Deep Orthogonal Fusion: Multimodal Prognostic Biomarker Discovery Integrating Radiology, Pathology, Genomic, and Clinical Data
- **Arxiv ID**: http://arxiv.org/abs/2107.00648v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, q-bio.GN, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2107.00648v1)
- **Published**: 2021-07-01 17:59:01+00:00
- **Updated**: 2021-07-01 17:59:01+00:00
- **Authors**: Nathaniel Braman, Jacob W. H. Gordon, Emery T. Goossens, Caleb Willis, Martin C. Stumpe, Jagadish Venkataraman
- **Comment**: Accepted for presentation at MICCAI 2021
- **Journal**: None
- **Summary**: Clinical decision-making in oncology involves multimodal data such as radiology scans, molecular profiling, histopathology slides, and clinical factors. Despite the importance of these modalities individually, no deep learning framework to date has combined them all to predict patient prognosis. Here, we predict the overall survival (OS) of glioma patients from diverse multimodal data with a Deep Orthogonal Fusion (DOF) model. The model learns to combine information from multiparametric MRI exams, biopsy-based modalities (such as H&E slide images and/or DNA sequencing), and clinical variables into a comprehensive multimodal risk score. Prognostic embeddings from each modality are learned and combined via attention-gated tensor fusion. To maximize the information gleaned from each modality, we introduce a multimodal orthogonalization (MMO) loss term that increases model performance by incentivizing constituent embeddings to be more complementary. DOF predicts OS in glioma patients with a median C-index of 0.788 +/- 0.067, significantly outperforming (p=0.023) the best performing unimodal model with a median C-index of 0.718 +/- 0.064. The prognostic model significantly stratifies glioma patients by OS within clinical subsets, adding further granularity to prognostic clinical grading and molecular subtyping.



### On the Practicality of Deterministic Epistemic Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2107.00649v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00649v3)
- **Published**: 2021-07-01 17:59:07+00:00
- **Updated**: 2022-07-05 07:39:17+00:00
- **Authors**: Janis Postels, Mattia Segu, Tao Sun, Luca Sieber, Luc Van Gool, Fisher Yu, Federico Tombari
- **Comment**: International Conference on Machine Learning 2022
- **Journal**: None
- **Summary**: A set of novel approaches for estimating epistemic uncertainty in deep neural networks with a single forward pass has recently emerged as a valid alternative to Bayesian Neural Networks. On the premise of informative representations, these deterministic uncertainty methods (DUMs) achieve strong performance on detecting out-of-distribution (OOD) data while adding negligible computational costs at inference time. However, it remains unclear whether DUMs are well calibrated and can seamlessly scale to real-world applications - both prerequisites for their practical deployment. To this end, we first provide a taxonomy of DUMs, and evaluate their calibration under continuous distributional shifts. Then, we extend them to semantic segmentation. We find that, while DUMs scale to realistic vision tasks and perform well on OOD detection, the practicality of current methods is undermined by poor calibration under distributional shifts.



### CLIP-It! Language-Guided Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/2107.00650v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2107.00650v2)
- **Published**: 2021-07-01 17:59:27+00:00
- **Updated**: 2021-12-08 01:30:47+00:00
- **Authors**: Medhini Narasimhan, Anna Rohrbach, Trevor Darrell
- **Comment**: Neurips 2021. Website at https://medhini.github.io/clip_it/
- **Journal**: Thirty-Fifth Conference on Neural Information Processing Systems.
  2021
- **Summary**: A generic video summary is an abridged version of a video that conveys the whole story and features the most important scenes. Yet the importance of scenes in a video is often subjective, and users should have the option of customizing the summary by using natural language to specify what is important to them. Further, existing models for fully automatic generic summarization have not exploited available language models, which can serve as an effective prior for saliency. This work introduces CLIP-It, a single framework for addressing both generic and query-focused video summarization, typically approached separately in the literature. We propose a language-guided multimodal transformer that learns to score frames in a video based on their importance relative to one another and their correlation with a user-defined query (for query-focused summarization) or an automatically generated dense video caption (for generic video summarization). Our model can be extended to the unsupervised setting by training without ground-truth supervision. We outperform baselines and prior work by a significant margin on both standard video summarization datasets (TVSum and SumMe) and a query-focused video summarization dataset (QFVS). Particularly, we achieve large improvements in the transfer setting, attesting to our method's strong generalization capabilities.



### AutoFormer: Searching Transformers for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.00651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00651v1)
- **Published**: 2021-07-01 17:59:30+00:00
- **Updated**: 2021-07-01 17:59:30+00:00
- **Authors**: Minghao Chen, Houwen Peng, Jianlong Fu, Haibin Ling
- **Comment**: Github: https://github.com/microsoft/AutoML
- **Journal**: None
- **Summary**: Recently, pure transformer-based models have shown great potentials for vision tasks such as image classification and detection. However, the design of transformer networks is challenging. It has been observed that the depth, embedding dimension, and number of heads can largely affect the performance of vision transformers. Previous models configure these dimensions based upon manual crafting. In this work, we propose a new one-shot architecture search framework, namely AutoFormer, dedicated to vision transformer search. AutoFormer entangles the weights of different blocks in the same layers during supernet training. Benefiting from the strategy, the trained supernet allows thousands of subnets to be very well-trained. Specifically, the performance of these subnets with weights inherited from the supernet is comparable to those retrained from scratch. Besides, the searched models, which we refer to AutoFormers, surpass the recent state-of-the-arts such as ViT and DeiT. In particular, AutoFormer-tiny/small/base achieve 74.7%/81.7%/82.4% top-1 accuracy on ImageNet with 5.7M/22.9M/53.7M parameters, respectively. Lastly, we verify the transferability of AutoFormer by providing the performance on downstream benchmarks and distillation experiments. Code and models are available at https://github.com/microsoft/AutoML.



### CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows
- **Arxiv ID**: http://arxiv.org/abs/2107.00652v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.00652v3)
- **Published**: 2021-07-01 17:59:56+00:00
- **Updated**: 2022-01-09 05:49:30+00:00
- **Authors**: Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, Baining Guo
- **Comment**: None
- **Journal**: None
- **Summary**: We present CSWin Transformer, an efficient and effective Transformer-based backbone for general-purpose vision tasks. A challenging issue in Transformer design is that global self-attention is very expensive to compute whereas local self-attention often limits the field of interactions of each token. To address this issue, we develop the Cross-Shaped Window self-attention mechanism for computing self-attention in the horizontal and vertical stripes in parallel that form a cross-shaped window, with each stripe obtained by splitting the input feature into stripes of equal width. We provide a mathematical analysis of the effect of the stripe width and vary the stripe width for different layers of the Transformer network which achieves strong modeling capability while limiting the computation cost. We also introduce Locally-enhanced Positional Encoding (LePE), which handles the local positional information better than existing encoding schemes. LePE naturally supports arbitrary input resolutions, and is thus especially effective and friendly for downstream tasks. Incorporated with these designs and a hierarchical structure, CSWin Transformer demonstrates competitive performance on common vision tasks. Specifically, it achieves 85.4\% Top-1 accuracy on ImageNet-1K without any extra training data or label, 53.9 box AP and 46.4 mask AP on the COCO detection task, and 52.2 mIOU on the ADE20K semantic segmentation task, surpassing previous state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and +2.0 respectively under the similar FLOPs setting. By further pretraining on the larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K and high segmentation performance on ADE20K with 55.7 mIoU. The code and models are available at https://github.com/microsoft/CSWin-Transformer.



### Aerial Map-Based Navigation Using Semantic Segmentation and Pattern Matching
- **Arxiv ID**: http://arxiv.org/abs/2107.00689v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.00689v3)
- **Published**: 2021-07-01 18:31:42+00:00
- **Updated**: 2022-05-30 17:55:17+00:00
- **Authors**: Youngjoo Kim
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: This paper proposes a novel approach to map-based navigation system for unmanned aircraft. The proposed system attempts label-to-label matching, not image-to-image matching, between aerial images and a map database. The ground objects can be labelled by deep learning approaches and the configuration of the objects is used to find the corresponding location in the map database. The use of the deep learning technique as a tool for extracting high-level features reduces the image-based localization problem to a pattern matching problem. This paper proposes a pattern matching algorithm that does not require altitude information or a camera model to estimate the absolute horizontal position. The feasibility analysis with simulated images shows the proposed map-based navigation can be realized with the proposed pattern matching algorithm and it is able to provide positions given the labelled objects.



### Unsupervised Image Segmentation by Mutual Information Maximization and Adversarial Regularization
- **Arxiv ID**: http://arxiv.org/abs/2107.00691v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00691v1)
- **Published**: 2021-07-01 18:36:27+00:00
- **Updated**: 2021-07-01 18:36:27+00:00
- **Authors**: S. Ehsan Mirsadeghi, Ali Royat, Hamid Rezatofighi
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters (RA-L 2021) & IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS 2021)
- **Summary**: Semantic segmentation is one of the basic, yet essential scene understanding tasks for an autonomous agent. The recent developments in supervised machine learning and neural networks have enjoyed great success in enhancing the performance of the state-of-the-art techniques for this task. However, their superior performance is highly reliant on the availability of a large-scale annotated dataset. In this paper, we propose a novel fully unsupervised semantic segmentation method, the so-called Information Maximization and Adversarial Regularization Segmentation (InMARS). Inspired by human perception which parses a scene into perceptual groups, rather than analyzing each pixel individually, our proposed approach first partitions an input image into meaningful regions (also known as superpixels). Next, it utilizes Mutual-Information-Maximization followed by an adversarial training strategy to cluster these regions into semantically meaningful classes. To customize an adversarial training scheme for the problem, we incorporate adversarial pixel noise along with spatial perturbations to impose photometrical and geometrical invariance on the deep neural network. Our experiments demonstrate that our method achieves the state-of-the-art performance on two commonly used unsupervised semantic segmentation datasets, COCO-Stuff, and Potsdam.



### Intrinsic Image Transfer for Illumination Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2107.00704v2
- **DOI**: 10.1109/TPAMI.2022.3224253
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00704v2)
- **Published**: 2021-07-01 19:12:24+00:00
- **Updated**: 2022-12-24 20:17:21+00:00
- **Authors**: Junqing Huang, Michael Ruzhansky, Qianying Zhang, Haihui Wang
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (T-PAMI)
- **Journal**: None
- **Summary**: This paper presents a novel intrinsic image transfer (IIT) algorithm for illumination manipulation, which creates a local image translation between two illumination surfaces. This model is built on an optimization-based framework consisting of three photo-realistic losses defined on the sub-layers factorized by an intrinsic image decomposition. We illustrate that all losses can be reduced without the necessity of taking an intrinsic image decomposition under the well-known spatial-varying illumination illumination-invariant reflectance prior knowledge. Moreover, with a series of relaxations, all of them can be directly defined on images, giving a closed-form solution for image illumination manipulation. This new paradigm differs from the prevailing Retinex-based algorithms, as it provides an implicit way to deal with the per-pixel image illumination. We finally demonstrate its versatility and benefits to the illumination-related tasks such as illumination compensation, image enhancement, and high dynamic range (HDR) image compression, and show the high-quality results on natural image datasets.



### Blind Image Super-Resolution via Contrastive Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.00708v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00708v1)
- **Published**: 2021-07-01 19:34:23+00:00
- **Updated**: 2021-07-01 19:34:23+00:00
- **Authors**: Jiahui Zhang, Shijian Lu, Fangneng Zhan, Yingchen Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Image super-resolution (SR) research has witnessed impressive progress thanks to the advance of convolutional neural networks (CNNs) in recent years. However, most existing SR methods are non-blind and assume that degradation has a single fixed and known distribution (e.g., bicubic) which struggle while handling degradation in real-world data that usually follows a multi-modal, spatially variant, and unknown distribution. The recent blind SR studies address this issue via degradation estimation, but they do not generalize well to multi-source degradation and cannot handle spatially variant degradation. We design CRL-SR, a contrastive representation learning network that focuses on blind SR of images with multi-modal and spatially variant distributions. CRL-SR addresses the blind SR challenges from two perspectives. The first is contrastive decoupling encoding which introduces contrastive learning to extract resolution-invariant embedding and discard resolution-variant embedding under the guidance of a bidirectional contrastive loss. The second is contrastive feature refinement which generates lost or corrupted high-frequency details under the guidance of a conditional contrastive loss. Extensive experiments on synthetic datasets and real images show that the proposed CRL-SR can handle multi-modal and spatially variant degradation effectively under blind settings and it also outperforms state-of-the-art SR methods qualitatively and quantitatively.



### Long-Short Ensemble Network for Bipolar Manic-Euthymic State Recognition Based on Wrist-worn Sensors
- **Arxiv ID**: http://arxiv.org/abs/2107.00710v3
- **DOI**: 10.1109/MPRV.2022.3155728
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00710v3)
- **Published**: 2021-07-01 19:35:54+00:00
- **Updated**: 2022-04-06 13:11:38+00:00
- **Authors**: Ulysse Côté-Allard, Petter Jakobsen, Andrea Stautland, Tine Nordgreen, Ole Bernt Fasmer, Ketil Joachim Oedegaard, Jim Torresen
- **Comment**: Published in IEEE Pervasive Computing in 2022. 12 pages + 2. 2
  Figures and 3 tables
- **Journal**: IEEE Pervasive Computing (2022) 1-12
- **Summary**: Manic episodes of bipolar disorder can lead to uncritical behaviour and delusional psychosis, often with destructive consequences for those affected and their surroundings. Early detection and intervention of a manic episode are crucial to prevent escalation, hospital admission and premature death. However, people with bipolar disorder may not recognize that they are experiencing a manic episode and symptoms such as euphoria and increased productivity can also deter affected individuals from seeking help. This work proposes to perform user-independent, automatic mood-state detection based on actigraphy and electrodermal activity acquired from a wrist-worn device during mania and after recovery (euthymia). This paper proposes a new deep learning-based ensemble method leveraging long (20h) and short (5 minutes) time-intervals to discriminate between the mood-states. When tested on 47 bipolar patients, the proposed classification scheme achieves an average accuracy of 91.59% in euthymic/manic mood-state recognition.



### Passing a Non-verbal Turing Test: Evaluating Gesture Animations Generated from Speech
- **Arxiv ID**: http://arxiv.org/abs/2107.00712v2
- **DOI**: 10.1109/VR50410.2021.00082
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.00712v2)
- **Published**: 2021-07-01 19:38:43+00:00
- **Updated**: 2021-08-21 20:03:52+00:00
- **Authors**: Manuel Rebol, Christian Gütl, Krzysztof Pietroszek
- **Comment**: None
- **Journal**: 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
- **Summary**: People communicate using both speech and non-verbal signals such as gestures, face expression or body pose. Non-verbal signals impact the meaning of the spoken utterance in an abundance of ways. An absence of non-verbal signals impoverishes the process of communication. Yet, when users are represented as avatars, it is difficult to translate non-verbal signals along with the speech into the virtual world without specialized motion-capture hardware. In this paper, we propose a novel, data-driven technique for generating gestures directly from speech. Our approach is based on the application of Generative Adversarial Neural Networks (GANs) to model the correlation rather than causation between speech and gestures. This approach approximates neuroscience findings on how non-verbal communication and speech are correlated. We create a large dataset which consists of speech and corresponding gestures in a 3D human pose format from which our model learns the speaker-specific correlation. We evaluate the proposed technique in a user study that is inspired by the Turing test. For the study, we animate the generated gestures on a virtual character. We find that users are not able to distinguish between the generated and the recorded gestures. Moreover, users are able to identify our synthesized gestures as related or not related to a given utterance. Code and videos are available at https://github.com/mrebol/Gestures-From-Speech



### SIMILAR: Submodular Information Measures Based Active Learning In Realistic Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2107.00717v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00717v2)
- **Published**: 2021-07-01 19:49:44+00:00
- **Updated**: 2021-11-03 19:00:28+00:00
- **Authors**: Suraj Kothawade, Nathan Beck, Krishnateja Killamsetty, Rishabh Iyer
- **Comment**: To Appear In Thirty-fifth Conference on Neural Information Processing
  Systems, NeurIPS 2021
- **Journal**: None
- **Summary**: Active learning has proven to be useful for minimizing labeling costs by selecting the most informative samples. However, existing active learning methods do not work well in realistic scenarios such as imbalance or rare classes, out-of-distribution data in the unlabeled set, and redundancy. In this work, we propose SIMILAR (Submodular Information Measures based actIve LeARning), a unified active learning framework using recently proposed submodular information measures (SIM) as acquisition functions. We argue that SIMILAR not only works in standard active learning, but also easily extends to the realistic settings considered above and acts as a one-stop solution for active learning that is scalable to large real-world datasets. Empirically, we show that SIMILAR significantly outperforms existing active learning algorithms by as much as ~5% - 18% in the case of rare classes and ~5% - 10% in the case of out-of-distribution data on several image classification tasks like CIFAR-10, MNIST, and ImageNet. SIMILAR is available as a part of the DISTIL toolkit: "https://github.com/decile-team/distil".



### Mitigating Uncertainty of Classifier for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2107.00727v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.00727v1)
- **Published**: 2021-07-01 20:08:15+00:00
- **Updated**: 2021-07-01 20:08:15+00:00
- **Authors**: Shanu Kumar, Vinod Kumar Kurmi, Praphul Singh, Vinay P Namboodiri
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding unsupervised domain adaptation has been an important task that has been well explored. However, the wide variety of methods have not analyzed the role of a classifier's performance in detail. In this paper, we thoroughly examine the role of a classifier in terms of matching source and target distributions. We specifically investigate the classifier ability by matching a) the distribution of features, b) probabilistic uncertainty for samples and c) certainty activation mappings. Our analysis suggests that using these three distributions does result in a consistently improved performance on all the datasets. Our work thus extends present knowledge on the role of the various distributions obtained from the classifier towards solving unsupervised domain adaptation.



### Enhancing Multi-Robot Perception via Learned Data Association
- **Arxiv ID**: http://arxiv.org/abs/2107.00769v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2107.00769v1)
- **Published**: 2021-07-01 22:45:26+00:00
- **Updated**: 2021-07-01 22:45:26+00:00
- **Authors**: Nathaniel Glaser, Yen-Cheng Liu, Junjiao Tian, Zsolt Kira
- **Comment**: Accepted to ICRA 2020 Workshop on "Emerging Learning and Algorithmic
  Methods for Data Association in Robotics"; associated spotlight talk
  available at https://www.youtube.com/watch?v=-lEVvtsfz0I&t=16743s
- **Journal**: None
- **Summary**: In this paper, we address the multi-robot collaborative perception problem, specifically in the context of multi-view infilling for distributed semantic segmentation. This setting entails several real-world challenges, especially those relating to unregistered multi-agent image data. Solutions must effectively leverage multiple, non-static, and intermittently-overlapping RGB perspectives. To this end, we propose the Multi-Agent Infilling Network: an extensible neural architecture that can be deployed (in a distributed manner) to each agent in a robotic swarm. Specifically, each robot is in charge of locally encoding and decoding visual information, and an extensible neural mechanism allows for an uncertainty-aware and context-based exchange of intermediate features. We demonstrate improved performance on a realistic multi-robot AirSim dataset.



### Overcoming Obstructions via Bandwidth-Limited Multi-Agent Spatial Handshaking
- **Arxiv ID**: http://arxiv.org/abs/2107.00771v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2107.00771v1)
- **Published**: 2021-07-01 22:56:47+00:00
- **Updated**: 2021-07-01 22:56:47+00:00
- **Authors**: Nathaniel Glaser, Yen-Cheng Liu, Junjiao Tian, Zsolt Kira
- **Comment**: Accepted to IROS 2021
- **Journal**: None
- **Summary**: In this paper, we address bandwidth-limited and obstruction-prone collaborative perception, specifically in the context of multi-agent semantic segmentation. This setting presents several key challenges, including processing and exchanging unregistered robotic swarm imagery. To be successful, solutions must effectively leverage multiple non-static and intermittently-overlapping RGB perspectives, while heeding bandwidth constraints and overcoming unwanted foreground obstructions. As such, we propose an end-to-end learn-able Multi-Agent Spatial Handshaking network (MASH) to process, compress, and propagate visual information across a robotic swarm. Our distributed communication module operates directly (and exclusively) on raw image data, without additional input requirements such as pose, depth, or warping data. We demonstrate superior performance of our model compared against several baselines in a photo-realistic multi-robot AirSim environment, especially in the presence of image occlusions. Our method achieves an absolute 11% IoU improvement over strong baselines.



