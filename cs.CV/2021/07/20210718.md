# Arxiv Papers in cs.CV on 2021-07-18
### Attention-based Multi-scale Gated Recurrent Encoder with Novel Correlation Loss for COVID-19 Progression Prediction
- **Arxiv ID**: http://arxiv.org/abs/2107.08330v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.08330v1)
- **Published**: 2021-07-18 00:37:55+00:00
- **Updated**: 2021-07-18 00:37:55+00:00
- **Authors**: Aishik Konwer, Joseph Bae, Gagandeep Singh, Rishabh Gattu, Syed Ali, Jeremy Green, Tej Phatak, Prateek Prasanna
- **Comment**: The paper is early accepted to MICCAI 2021
- **Journal**: None
- **Summary**: COVID-19 image analysis has mostly focused on diagnostic tasks using single timepoint scans acquired upon disease presentation or admission. We present a deep learning-based approach to predict lung infiltrate progression from serial chest radiographs (CXRs) of COVID-19 patients. Our method first utilizes convolutional neural networks (CNNs) for feature extraction from patches within the concerned lung zone, and also from neighboring and remote boundary regions. The framework further incorporates a multi-scale Gated Recurrent Unit (GRU) with a correlation module for effective predictions. The GRU accepts CNN feature vectors from three different areas as input and generates a fused representation. The correlation module attempts to minimize the correlation loss between hidden representations of concerned and neighboring area feature vectors, while maximizing the loss between the same from concerned and remote regions. Further, we employ an attention module over the output hidden states of each encoder timepoint to generate a context vector. This vector is used as an input to a decoder module to predict patch severity grades at a future timepoint. Finally, we ensemble the patch classification scores to calculate patient-wise grades. Specifically, our framework predicts zone-wise disease severity for a patient on a given day by learning representations from the previous temporal CXRs. Our novel multi-institutional dataset comprises sequential CXR scans from N=93 patients. Our approach outperforms transfer learning and radiomic feature-based baseline approaches on this dataset.



### Federated Action Recognition on Heterogeneous Embedded Devices
- **Arxiv ID**: http://arxiv.org/abs/2107.12147v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.12147v1)
- **Published**: 2021-07-18 02:33:24+00:00
- **Updated**: 2021-07-18 02:33:24+00:00
- **Authors**: Pranjal Jain, Shreyas Goenka, Saurabh Bagchi, Biplab Banerjee, Somali Chaterji
- **Comment**: 13 pages, 12 figures
- **Journal**: IEEE Transactions on Artificial Intelligence 2021
- **Summary**: Federated learning allows a large number of devices to jointly learn a model without sharing data. In this work, we enable clients with limited computing power to perform action recognition, a computationally heavy task. We first perform model compression at the central server through knowledge distillation on a large dataset. This allows the model to learn complex features and serves as an initialization for model fine-tuning. The fine-tuning is required because the limited data present in smaller datasets is not adequate for action recognition models to learn complex spatio-temporal features. Because the clients present are often heterogeneous in their computing resources, we use an asynchronous federated optimization and we further show a convergence bound. We compare our approach to two baseline approaches: fine-tuning at the central server (no clients) and fine-tuning using (heterogeneous) clients using synchronous federated averaging. We empirically show on a testbed of heterogeneous embedded devices that we can perform action recognition with comparable accuracy to the two baselines above, while our asynchronous learning strategy reduces the training time by 40%, relative to synchronous learning.



### Fully Polarimetric SAR and Single-Polarization SAR Image Fusion Network
- **Arxiv ID**: http://arxiv.org/abs/2107.08355v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.08355v1)
- **Published**: 2021-07-18 03:51:04+00:00
- **Updated**: 2021-07-18 03:51:04+00:00
- **Authors**: Liupeng Lin, Jie Li, Huanfeng Shen, Lingli Zhao, Qiangqiang Yuan, Xinghua Li
- **Comment**: None
- **Journal**: None
- **Summary**: The data fusion technology aims to aggregate the characteristics of different data and obtain products with multiple data advantages. To solves the problem of reduced resolution of PolSAR images due to system limitations, we propose a fully polarimetric synthetic aperture radar (PolSAR) images and single-polarization synthetic aperture radar SAR (SinSAR) images fusion network to generate high-resolution PolSAR (HR-PolSAR) images. To take advantage of the polarimetric information of the low-resolution PolSAR (LR-PolSAR) image and the spatial information of the high-resolution single-polarization SAR (HR-SinSAR) image, we propose a fusion framework for joint LR-PolSAR image and HR-SinSAR image and design a cross-attention mechanism to extract features from the joint input data. Besides, based on the physical imaging mechanism, we designed the PolSAR polarimetric loss function for constrained network training. The experimental results confirm the superiority of fusion network over traditional algorithms. The average PSNR is increased by more than 3.6db, and the average MAE is reduced to less than 0.07. Experiments on polarimetric decomposition and polarimetric signature show that it maintains polarimetric information well.



### Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.08369v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.08369v4)
- **Published**: 2021-07-18 05:42:10+00:00
- **Updated**: 2021-10-25 15:49:50+00:00
- **Authors**: Sayak Paul, Siddha Ganju
- **Comment**: Equal authorship. Accepted to the Tackling Climate Change with
  Machine Learning workshop at NeurIPS 2021. Code and models are available at
  https://git.io/JW3P8
- **Journal**: None
- **Summary**: Floods wreak havoc throughout the world, causing billions of dollars in damages, and uprooting communities, ecosystems and economies. The NASA Impact Flood Detection competition tasked participants with predicting flooded pixels after training with synthetic aperture radar (SAR) images in a supervised setting. We propose a semi-supervised learning pseudo-labeling scheme that derives confidence estimates from U-Net ensembles, progressively improving accuracy. Concretely, we use a cyclical approach involving multiple stages (1) training an ensemble model of multiple U-Net architectures with the provided high confidence hand-labeled data and, generated pseudo labels or low confidence labels on the entire unlabeled test dataset, and then, (2) filter out quality generated labels and, (3) combine the generated labels with the previously available high confidence hand-labeled dataset. This assimilated dataset is used for the next round of training ensemble models and the cyclical process is repeated until the performance improvement plateaus. We post process our results with Conditional Random Fields. Our approach sets a new state-of-the-art on the Sentinel-1 dataset with 0.7654 IoU, an impressive improvement over the 0.60 IoU baseline. Our method, which we release with all the code and models, can also be used as an open science benchmark for the Sentinel-1 dataset.



### An Experimental Study of Data Heterogeneity in Federated Learning Methods for Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2107.08371v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.08371v1)
- **Published**: 2021-07-18 05:47:48+00:00
- **Updated**: 2021-07-18 05:47:48+00:00
- **Authors**: Liangqiong Qu, Niranjan Balachandar, Daniel L Rubin
- **Comment**: None
- **Journal**: None
- **Summary**: Federated learning enables multiple institutions to collaboratively train machine learning models on their local data in a privacy-preserving way. However, its distributed nature often leads to significant heterogeneity in data distributions across institutions. In this paper, we investigate the deleterious impact of a taxonomy of data heterogeneity regimes on federated learning methods, including quantity skew, label distribution skew, and imaging acquisition skew. We show that the performance degrades with the increasing degrees of data heterogeneity. We present several mitigation strategies to overcome performance drops from data heterogeneity, including weighted average for data quantity skew, weighted loss and batch normalization averaging for label distribution skew. The proposed optimizations to federated learning methods improve their capability of handling heterogeneity across institutions, which provides valuable guidance for the deployment of federated learning in real clinical applications.



### A High-Performance Adaptive Quantization Approach for Edge CNN Applications
- **Arxiv ID**: http://arxiv.org/abs/2107.08382v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.08382v1)
- **Published**: 2021-07-18 07:49:18+00:00
- **Updated**: 2021-07-18 07:49:18+00:00
- **Authors**: Hsu-Hsun Chin, Ren-Song Tsay, Hsin-I Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent convolutional neural network (CNN) development continues to advance the state-of-the-art model accuracy for various applications. However, the enhanced accuracy comes at the cost of substantial memory bandwidth and storage requirements and demanding computational resources. Although in the past the quantization methods have effectively reduced the deployment cost for edge devices, it suffers from significant information loss when processing the biased activations of contemporary CNNs. In this paper, we hence introduce an adaptive high-performance quantization method to resolve the issue of biased activation by dynamically adjusting the scaling and shifting factors based on the task loss. Our proposed method has been extensively evaluated on image classification models (ResNet-18/34/50, MobileNet-V2, EfficientNet-B0) with ImageNet dataset, object detection model (YOLO-V4) with COCO dataset, and language models with PTB dataset. The results show that our 4-bit integer (INT4) quantization models achieve better accuracy than the state-of-the-art 4-bit models, and in some cases, even surpass the golden full-precision models. The final designs have been successfully deployed onto extremely resource-constrained edge devices for many practical applications.



### AS-MLP: An Axial Shifted MLP Architecture for Vision
- **Arxiv ID**: http://arxiv.org/abs/2107.08391v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08391v2)
- **Published**: 2021-07-18 08:56:34+00:00
- **Updated**: 2022-03-17 06:59:03+00:00
- **Authors**: Dongze Lian, Zehao Yu, Xing Sun, Shenghua Gao
- **Comment**: Accepted by ICLR2022
- **Journal**: None
- **Summary**: An Axial Shifted MLP architecture (AS-MLP) is proposed in this paper. Different from MLP-Mixer, where the global spatial feature is encoded for information flow through matrix transposition and one token-mixing MLP, we pay more attention to the local features interaction. By axially shifting channels of the feature map, AS-MLP is able to obtain the information flow from different axial directions, which captures the local dependencies. Such an operation enables us to utilize a pure MLP architecture to achieve the same local receptive field as CNN-like architecture. We can also design the receptive field size and dilation of blocks of AS-MLP, etc, in the same spirit of convolutional neural networks. With the proposed AS-MLP architecture, our model obtains 83.3% Top-1 accuracy with 88M parameters and 15.2 GFLOPs on the ImageNet-1K dataset. Such a simple yet effective architecture outperforms all MLP-based architectures and achieves competitive performance compared to the transformer-based architectures (e.g., Swin Transformer) even with slightly lower FLOPs. In addition, AS-MLP is also the first MLP-based architecture to be applied to the downstream tasks (e.g., object detection and semantic segmentation). The experimental results are also impressive. Our proposed AS-MLP obtains 51.5 mAP on the COCO validation set and 49.5 MS mIoU on the ADE20K dataset, which is competitive compared to the transformer-based architectures. Our AS-MLP establishes a strong baseline of MLP-based architecture. Code is available at https://github.com/svip-lab/AS-MLP.



### Dynamic Convolution for 3D Point Cloud Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.08392v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08392v3)
- **Published**: 2021-07-18 09:05:16+00:00
- **Updated**: 2022-10-14 06:08:24+00:00
- **Authors**: Tong He, Chunhua Shen, Anton van den Hengel
- **Comment**: Accepted to IEEE Trans. Pattern Analysis and Machine Intelligence.
  Extended version of arXiv:2011.13328
- **Journal**: None
- **Summary**: We propose an approach to instance segmentation from 3D point clouds based on dynamic convolution. This enables it to adapt, at inference, to varying feature and object scales. Doing so avoids some pitfalls of bottom up approaches, including a dependence on hyper-parameter tuning and heuristic post-processing pipelines to compensate for the inevitable variability in object sizes, even within a single scene. The representation capability of the network is greatly improved by gathering homogeneous points that have identical semantic categories and close votes for the geometric centroids. Instances are then decoded via several simple convolution layers, where the parameters are generated conditioned on the input. The proposed approach is proposal-free, and instead exploits a convolution process that adapts to the spatial and semantic characteristics of each instance. A light-weight transformer, built on the bottleneck layer, allows the model to capture long-range dependencies, with limited computational overhead. The result is a simple, efficient, and robust approach that yields strong performance on various datasets: ScanNetV2, S3DIS, and PartNet. The consistent improvements on both voxel- and point-based architectures imply the effectiveness of the proposed method. Code is available at: https://git.io/DyCo3D



### A Positive/Unlabeled Approach for the Segmentation of Medical Sequences using Point-Wise Supervision
- **Arxiv ID**: http://arxiv.org/abs/2107.08394v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08394v1)
- **Published**: 2021-07-18 09:13:33+00:00
- **Updated**: 2021-07-18 09:13:33+00:00
- **Authors**: Laurent Lejeune, Raphael Sznitman
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to quickly annotate medical imaging data plays a critical role in training deep learning frameworks for segmentation. Doing so for image volumes or video sequences is even more pressing as annotating these is particularly burdensome. To alleviate this problem, this work proposes a new method to efficiently segment medical imaging volumes or videos using point-wise annotations only. This allows annotations to be collected extremely quickly and remains applicable to numerous segmentation tasks. Our approach trains a deep learning model using an appropriate Positive/Unlabeled objective function using sparse point-wise annotations. While most methods of this kind assume that the proportion of positive samples in the data is known a-priori, we introduce a novel self-supervised method to estimate this prior efficiently by combining a Bayesian estimation framework and new stopping criteria. Our method iteratively estimates appropriate class priors and yields high segmentation quality for a variety of object types and imaging modalities. In addition, by leveraging a spatio-temporal tracking framework, we regularize our predictions by leveraging the complete data volume. We show experimentally that our approach outperforms state-of-the-art methods tailored to the same problem.



### A Miniature Biological Eagle-Eye Vision System for Small Target Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.08406v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2107.08406v1)
- **Published**: 2021-07-18 09:51:57+00:00
- **Updated**: 2021-07-18 09:51:57+00:00
- **Authors**: Shutai Wang, Qiang Fu, Yinhao Hu, Chunhua Zhang, Wei He
- **Comment**: submitted to 2021 Chinese Automation Congress (CAC 2021)
- **Journal**: None
- **Summary**: Small target detection is known to be a challenging problem. Inspired by the structural characteristics and physiological mechanism of eagle-eye, a miniature vision system is designed for small target detection in this paper. First, a hardware platform is established, which consists of a pan-tilt, a short-focus camera and a long-focus camera. Then, based on the visual attention mechanism of eagle-eye, the cameras with different focal lengths are controlled cooperatively to achieve small target detection. Experimental results show that the designed biological eagle-eye vision system can accurately detect small targets, which has a strong adaptive ability.



### Feature Mining: A Novel Training Strategy for Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2107.08421v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08421v1)
- **Published**: 2021-07-18 12:14:42+00:00
- **Updated**: 2021-07-18 12:14:42+00:00
- **Authors**: Tianshu Xie, Xuan Cheng, Xiaomin Wang, Minghui Liu, Jiali Deng, Ming Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel training strategy for convolutional neural network(CNN) named Feature Mining, that aims to strengthen the network's learning of the local feature. Through experiments, we find that semantic contained in different parts of the feature is different, while the network will inevitably lose the local information during feedforward propagation. In order to enhance the learning of local feature, Feature Mining divides the complete feature into two complementary parts and reuse these divided feature to make the network learn more local information, we call the two steps as feature segmentation and feature reusing. Feature Mining is a parameter-free method and has plug-and-play nature, and can be applied to any CNN models. Extensive experiments demonstrate the wide applicability, versatility, and compatibility of our method.



### YOLOX: Exceeding YOLO Series in 2021
- **Arxiv ID**: http://arxiv.org/abs/2107.08430v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08430v2)
- **Published**: 2021-07-18 12:55:11+00:00
- **Updated**: 2021-08-06 03:22:14+00:00
- **Authors**: Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: In this report, we present some experienced improvements to YOLO series, forming a new high-performance detector -- YOLOX. We switch the YOLO detector to an anchor-free manner and conduct other advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art results across a large scale range of models: For YOLO-Nano with only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing NanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in industry, we boost it to 47.3% AP on COCO, outperforming the current best practice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as YOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on Tesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model. We hope this report can provide useful experience for developers and researchers in practical scenes, and we also provide deploy versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at https://github.com/Megvii-BaseDetection/YOLOX.



### Fully Automated Machine Learning Pipeline for Echocardiogram Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.08440v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08440v2)
- **Published**: 2021-07-18 13:15:46+00:00
- **Updated**: 2021-10-28 08:34:38+00:00
- **Authors**: Hang Duong Thi Thuy, Tuan Nguyen Minh, Phi Nguyen Van, Long Tran Quoc
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, cardiac diagnosis largely depends on left ventricular function assessment. With the help of the segmentation deep learning model, the assessment of the left ventricle becomes more accessible and accurate. However, deep learning technique still faces two main obstacles: the difficulty in acquiring sufficient training data and time-consuming in developing quality models. In the ordinary data acquisition process, the dataset was selected randomly from a large pool of unlabeled images for labeling, leading to massive labor time to annotate those images. Besides that, hand-designed model development is strenuous and also costly. This paper introduces a pipeline that relies on Active Learning to ease the labeling work and utilizes Neural Architecture Search's idea to design the adequate deep learning model automatically. We called this Fully automated machine learning pipeline for echocardiogram segmentation. The experiment results show that our method obtained the same IOU accuracy with only two-fifths of the original training dataset, and the searched model got the same accuracy as the hand-designed model given the same training dataset.



### ANFIC: Image Compression Using Augmented Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/2107.08470v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.08470v2)
- **Published**: 2021-07-18 15:02:31+00:00
- **Updated**: 2021-10-25 15:52:46+00:00
- **Authors**: Yung-Han Ho, Chih-Chun Chan, Wen-Hsiao Peng, Hsueh-Ming Hang, Marek Domanski
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces an end-to-end learned image compression system, termed ANFIC, based on Augmented Normalizing Flows (ANF). ANF is a new type of flow model, which stacks multiple variational autoencoders (VAE) for greater model expressiveness. The VAE-based image compression has gone mainstream, showing promising compression performance. Our work presents the first attempt to leverage VAE-based compression in a flow-based framework. ANFIC advances further compression efficiency by stacking and extending hierarchically multiple VAE's. The invertibility of ANF, together with our training strategies, enables ANFIC to support a wide range of quality levels without changing the encoding and decoding networks. Extensive experimental results show that in terms of PSNR-RGB, ANFIC performs comparably to or better than the state-of-the-art learned image compression. Moreover, it performs close to VVC intra coding, from low-rate compression up to nearly-lossless compression. In particular, ANFIC achieves the state-of-the-art performance, when extended with conditional convolution for variable rate compression with a single model.



### A stepped sampling method for video detection using LSTM
- **Arxiv ID**: http://arxiv.org/abs/2107.08471v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.08471v2)
- **Published**: 2021-07-18 15:04:13+00:00
- **Updated**: 2021-08-28 06:53:50+00:00
- **Authors**: Dengshan Li, Rujing Wang, Chengjun Xie
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Artificial neural networks that simulate human achieves great successes. From the perspective of simulating human memory method, we propose a stepped sampler based on the "repeated input". We repeatedly inputted data to the LSTM model stepwise in a batch. The stepped sampler is used to strengthen the ability of fusing the temporal information in LSTM. We tested the stepped sampler on the LSTM built-in in PyTorch. Compared with the traditional sampler of PyTorch, such as sequential sampler, batch sampler, the training loss of the proposed stepped sampler converges faster in the training of the model, and the training loss after convergence is more stable. Meanwhile, it can maintain a higher test accuracy. We quantified the algorithm of the stepped sampler. We assume that, the artificial neural networks have human-like characteristics, and human learning method could be used for machine learning.



### Multi-Modal Temporal Convolutional Network for Anticipating Actions in Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/2107.09504v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09504v1)
- **Published**: 2021-07-18 16:21:35+00:00
- **Updated**: 2021-07-18 16:21:35+00:00
- **Authors**: Olga Zatsarynna, Yazan Abu Farha, Juergen Gall
- **Comment**: CVPR Precognition Workshop
- **Journal**: None
- **Summary**: Anticipating human actions is an important task that needs to be addressed for the development of reliable intelligent agents, such as self-driving cars or robot assistants. While the ability to make future predictions with high accuracy is crucial for designing the anticipation approaches, the speed at which the inference is performed is not less important. Methods that are accurate but not sufficiently fast would introduce a high latency into the decision process. Thus, this will increase the reaction time of the underlying system. This poses a problem for domains such as autonomous driving, where the reaction time is crucial. In this work, we propose a simple and effective multi-modal architecture based on temporal convolutions. Our approach stacks a hierarchy of temporal convolutional layers and does not rely on recurrent layers to ensure a fast prediction. We further introduce a multi-modal fusion mechanism that captures the pairwise interactions between RGB, flow, and object modalities. Results on two large-scale datasets of egocentric videos, EPIC-Kitchens-55 and EPIC-Kitchens-100, show that our approach achieves comparable performance to the state-of-the-art approaches while being significantly faster.



### Zero-Shot Domain Adaptation in CT Segmentation by Filtered Back Projection Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.08543v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.08543v4)
- **Published**: 2021-07-18 21:46:49+00:00
- **Updated**: 2022-06-08 18:49:35+00:00
- **Authors**: Talgat Saparov, Anvar Kurmukov, Boris Shirokikh, Mikhail Belyaev
- **Comment**: table fixed
- **Journal**: None
- **Summary**: Domain shift is one of the most salient challenges in medical computer vision. Due to immense variability in scanners' parameters and imaging protocols, even images obtained from the same person and the same scanner could differ significantly. We address variability in computed tomography (CT) images caused by different convolution kernels used in the reconstruction process, the critical domain shift factor in CT. The choice of a convolution kernel affects pixels' granularity, image smoothness, and noise level. We analyze a dataset of paired CT images, where smooth and sharp images were reconstructed from the same sinograms with different kernels, thus providing identical anatomy but different style. Though identical predictions are desired, we show that the consistency, measured as the average Dice between predictions on pairs, is just 0.54. We propose Filtered Back-Projection Augmentation (FBPAug), a simple and surprisingly efficient approach to augment CT images in sinogram space emulating reconstruction with different kernels. We apply the proposed method in a zero-shot domain adaptation setup and show that the consistency boosts from 0.54 to 0.92 outperforming other augmentation approaches. Neither specific preparation of source domain data nor target domain data is required, so our publicly released FBPAug can be used as a plug-and-play module for zero-shot domain adaptation in any CT-based task.



