# Arxiv Papers in cs.CV on 2021-07-08
### Staying in Shape: Learning Invariant Shape Representations using Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.03552v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03552v1)
- **Published**: 2021-07-08 00:53:24+00:00
- **Updated**: 2021-07-08 00:53:24+00:00
- **Authors**: Jeffrey Gu, Serena Yeung
- **Comment**: None
- **Journal**: None
- **Summary**: Creating representations of shapes that are invari-ant to isometric or almost-isometric transforma-tions has long been an area of interest in shape anal-ysis, since enforcing invariance allows the learningof more effective and robust shape representations.Most existing invariant shape representations arehandcrafted, and previous work on learning shaperepresentations do not focus on producing invariantrepresentations. To solve the problem of learningunsupervised invariant shape representations, weuse contrastive learning, which produces discrimi-native representations through learning invarianceto user-specified data augmentations. To producerepresentations that are specifically isometry andalmost-isometry invariant, we propose new dataaugmentations that randomly sample these transfor-mations. We show experimentally that our methodoutperforms previous unsupervised learning ap-proaches in both effectiveness and robustness.



### Automated Object Behavioral Feature Extraction for Potential Risk Analysis based on Video Sensor
- **Arxiv ID**: http://arxiv.org/abs/2107.03554v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2107.03554v3)
- **Published**: 2021-07-08 01:11:31+00:00
- **Updated**: 2021-10-26 01:12:08+00:00
- **Authors**: Byeongjoon Noh, Dongho Ka, Wonjun Noh, Hwasoo Yeo
- **Comment**: 6 pages, 9 figures
- **Journal**: None
- **Summary**: Pedestrians are exposed to risk of death or serious injuries on roads, especially unsignalized crosswalks, for a variety of reasons. To date, an extensive variety of studies have reported on vision based traffic safety system. However, many studies required manual inspection of the volumes of traffic video to reliably obtain traffic related objects behavioral factors. In this paper, we propose an automated and simpler system for effectively extracting object behavioral features from video sensors deployed on the road. We conduct basic statistical analysis on these features, and show how they can be useful for monitoring the traffic behavior on the road. We confirm the feasibility of the proposed system by applying our prototype to two unsignalized crosswalks in Osan city, South Korea. To conclude, we compare behaviors of vehicles and pedestrians in those two areas by simple statistical analysis. This study demonstrates the potential for a network of connected video sensors to provide actionable data for smart cities to improve pedestrian safety in dangerous road environments.



### Uncertainty-aware Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2107.03575v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03575v1)
- **Published**: 2021-07-08 03:09:01+00:00
- **Updated**: 2021-07-08 03:09:01+00:00
- **Authors**: Pengxiang Ding, Jianqin Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Human motion prediction is essential for tasks such as human motion analysis and human-robot interactions. Most existing approaches have been proposed to realize motion prediction. However, they ignore an important task, the evaluation of the quality of the predicted result. It is far more enough for current approaches in actual scenarios because people can't know how to interact with the machine without the evaluation of prediction, and unreliable predictions may mislead the machine to harm the human. Hence, we propose an uncertainty-aware framework for human motion prediction (UA-HMP). Concretely, we first design an uncertainty-aware predictor through Gaussian modeling to achieve the value and the uncertainty of predicted motion. Then, an uncertainty-guided learning scheme is proposed to quantitate the uncertainty and reduce the negative effect of the noisy samples during optimization for better performance. Our proposed framework is easily combined with current SOTA baselines to overcome their weakness in uncertainty modeling with slight parameters increment. Extensive experiments also show that they can achieve better performance in both short and long-term predictions in H3.6M, CMU-Mocap.



### Rethinking of Pedestrian Attribute Recognition: A Reliable Evaluation under Zero-Shot Pedestrian Identity Setting
- **Arxiv ID**: http://arxiv.org/abs/2107.03576v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03576v2)
- **Published**: 2021-07-08 03:12:24+00:00
- **Updated**: 2021-11-29 08:37:11+00:00
- **Authors**: Jian Jia, Houjing Huang, Xiaotang Chen, Kaiqi Huang
- **Comment**: 13 pages, 6 figures, journal version of arXiv:2005.11909, submitted
  to TIP. Code is available https://github.com/valencebond/Rethinking_of_PAR
- **Journal**: None
- **Summary**: Pedestrian attribute recognition aims to assign multiple attributes to one pedestrian image captured by a video surveillance camera. Although numerous methods are proposed and make tremendous progress, we argue that it is time to step back and analyze the status quo of the area. We review and rethink the recent progress from three perspectives. First, given that there is no explicit and complete definition of pedestrian attribute recognition, we formally define and distinguish pedestrian attribute recognition from other similar tasks. Second, based on the proposed definition, we expose the limitations of the existing datasets, which violate the academic norm and are inconsistent with the essential requirement of practical industry application. Thus, we propose two datasets, PETA\textsubscript{$ZS$} and RAP\textsubscript{$ZS$}, constructed following the zero-shot settings on pedestrian identity. In addition, we also introduce several realistic criteria for future pedestrian attribute dataset construction. Finally, we reimplement existing state-of-the-art methods and introduce a strong baseline method to give reliable evaluations and fair comparisons. Experiments are conducted on four existing datasets and two proposed datasets to measure progress on pedestrian attribute recognition.



### Video 3D Sampling for Self-supervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.03578v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03578v1)
- **Published**: 2021-07-08 03:22:06+00:00
- **Updated**: 2021-07-08 03:22:06+00:00
- **Authors**: Wei Li, Dezhao Luo, Bo Fang, Yu Zhou, Weiping Wang
- **Comment**: 9 pages, 5 figures, 6 tables
- **Journal**: None
- **Summary**: Most of the existing video self-supervised methods mainly leverage temporal signals of videos, ignoring that the semantics of moving objects and environmental information are all critical for video-related tasks. In this paper, we propose a novel self-supervised method for video representation learning, referred to as Video 3D Sampling (V3S). In order to sufficiently utilize the information (spatial and temporal) provided in videos, we pre-process a video from three dimensions (width, height, time). As a result, we can leverage the spatial information (the size of objects), temporal information (the direction and magnitude of motions) as our learning target. In our implementation, we combine the sampling of the three dimensions and propose the scale and projection transformations in space and time respectively. The experimental results show that, when applied to action recognition, video retrieval and action similarity labeling, our approach improves the state-of-the-arts with significant margins.



### Relation-Based Associative Joint Location for Human Pose Estimation in Videos
- **Arxiv ID**: http://arxiv.org/abs/2107.03591v3
- **DOI**: 10.1109/TIP.2022.3177959
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03591v3)
- **Published**: 2021-07-08 04:05:23+00:00
- **Updated**: 2023-06-30 09:52:30+00:00
- **Authors**: Yonghao Dang, Jianqin Yin, Shaojie Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Video-based human pose estimation (VHPE) is a vital yet challenging task. While deep learning methods have made significant progress for the VHPE, most approaches to this task implicitly model the long-range interaction between joints by enlarging the receptive field of the convolution. Unlike prior methods, we design a lightweight and plug-and-play joint relation extractor (JRE) to model the associative relationship between joints explicitly and automatically. The JRE takes the pseudo heatmaps of joints as input and calculates the similarity between pseudo heatmaps. In this way, the JRE flexibly learns the relationship between any two joints, allowing it to learn the rich spatial configuration of human poses. Moreover, the JRE can infer invisible joints according to the relationship between joints, which is beneficial for the model to locate occluded joints. Then, combined with temporal semantic continuity modeling, we propose a Relation-based Pose Semantics Transfer Network (RPSTN) for video-based human pose estimation. Specifically, to capture the temporal dynamics of poses, the pose semantic information of the current frame is transferred to the next with a joint relation guided pose semantics propagator (JRPSP). The proposed model can transfer the pose semantic features from the non-occluded frame to the occluded frame, making our method robust to the occlusion. Furthermore, the proposed JRE module is also suitable for image-based human pose estimation. The proposed RPSTN achieves state-of-the-art results on the video-based Penn Action dataset, Sub-JHMDB dataset, and PoseTrack2018 dataset. Moreover, the proposed JRE improves the performance of backbones on the image-based COCO2017 dataset. Code is available at https://github.com/YHDang/pose-estimation.



### Affect Expression Behaviour Analysis in the Wild using Consensual Collaborative Training
- **Arxiv ID**: http://arxiv.org/abs/2107.05736v2
- **DOI**: 10.14445/22315381/IJETT-V69I7P231
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05736v2)
- **Published**: 2021-07-08 04:28:21+00:00
- **Updated**: 2021-07-24 05:28:32+00:00
- **Authors**: Darshan Gera, S Balasubramanian
- **Comment**: 7 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:2009.14440. substantial text overlap with arXiv:2107.04746
- **Journal**: International Journal of Engineering Trends and Technology
  69.7(2021):244-254
- **Summary**: Facial expression recognition (FER) in the wild is crucial for building reliable human-computer interactive systems. However, annotations of large scale datasets in FER has been a key challenge as these datasets suffer from noise due to various factors like crowd sourcing, subjectivity of annotators, poor quality of images, automatic labelling based on key word search etc. Such noisy annotations impede the performance of FER due to the memorization ability of deep networks. During early learning stage, deep networks fit on clean data. Then, eventually, they start overfitting on noisy labels due to their memorization ability, which limits FER performance. This report presents Consensual Collaborative Training (CCT) framework used in our submission to expression recognition track of the Affective Behaviour Analysis in-the-wild (ABAW) 2021 competition. CCT co-trains three networks jointly using a convex combination of supervision loss and consistency loss, without making any assumption about the noise distribution. A dynamic transition mechanism is used to move from supervision loss in early learning to consistency loss for consensus of predictions among networks in the later stage. Co-training reduces overall error, and consistency loss prevents overfitting to noisy samples. The performance of the model is validated on challenging Aff-Wild2 dataset for categorical expression classification. Our code is made publicly available at https://github.com/1980x/ABAW2021DMACS.



### Superpoint-guided Semi-supervised Semantic Segmentation of 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2107.03601v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03601v3)
- **Published**: 2021-07-08 04:43:21+00:00
- **Updated**: 2021-09-09 11:18:18+00:00
- **Authors**: Shuang Deng, Qiulei Dong, Bo Liu, Zhanyi Hu
- **Comment**: None
- **Journal**: IEEE Conference on Robotics and Automation (ICRA), 2022
- **Summary**: 3D point cloud semantic segmentation is a challenging topic in the computer vision field. Most of the existing methods in literature require a large amount of fully labeled training data, but it is extremely time-consuming to obtain these training data by manually labeling massive point clouds. Addressing this problem, we propose a superpoint-guided semi-supervised segmentation network for 3D point clouds, which jointly utilizes a small portion of labeled scene point clouds and a large number of unlabeled point clouds for network training. The proposed network is iteratively updated with its predicted pseudo labels, where a superpoint generation module is introduced for extracting superpoints from 3D point clouds, and a pseudo-label optimization module is explored for automatically assigning pseudo labels to the unlabeled points under the constraint of the extracted superpoints. Additionally, there are some 3D points without pseudo-label supervision. We propose an edge prediction module to constrain features of edge points. A superpoint feature aggregation module and a superpoint feature consistency loss function are introduced to smooth superpoint features. Extensive experimental results on two 3D public datasets demonstrate that our method can achieve better performance than several state-of-the-art point cloud segmentation networks and several popular semi-supervised segmentation methods with few labeled scenes.



### Case-based Similar Image Retrieval for Weakly Annotated Large Histopathological Images of Malignant Lymphoma Using Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.03602v4
- **DOI**: None
- **Categories**: **cs.CV**, H.3.3; I.2.1; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2107.03602v4)
- **Published**: 2021-07-08 04:50:37+00:00
- **Updated**: 2023-01-27 05:31:13+00:00
- **Authors**: Noriaki Hashimoto, Yusuke Takagi, Hiroki Masuda, Hiroaki Miyoshi, Kei Kohno, Miharu Nagaishi, Kensaku Sato, Mai Takeuchi, Takuya Furuta, Keisuke Kawamoto, Kyohei Yamada, Mayuko Moritsubo, Kanako Inoue, Yasumasa Shimasaki, Yusuke Ogura, Teppei Imamoto, Tatsuzo Mishina, Ken Tanaka, Yoshino Kawaguchi, Shigeo Nakamura, Koichi Ohshima, Hidekata Hontani, Ichiro Takeuchi
- **Comment**: None
- **Journal**: None
- **Summary**: In the present study, we propose a novel case-based similar image retrieval (SIR) method for hematoxylin and eosin (H&E)-stained histopathological images of malignant lymphoma. When a whole slide image (WSI) is used as an input query, it is desirable to be able to retrieve similar cases by focusing on image patches in pathologically important regions such as tumor cells. To address this problem, we employ attention-based multiple instance learning, which enables us to focus on tumor-specific regions when the similarity between cases is computed. Moreover, we employ contrastive distance metric learning to incorporate immunohistochemical (IHC) staining patterns as useful supervised information for defining appropriate similarity between heterogeneous malignant lymphoma cases. In the experiment with 249 malignant lymphoma patients, we confirmed that the proposed method exhibited higher evaluation measures than the baseline case-based SIR methods. Furthermore, the subjective evaluation by pathologists revealed that our similarity measure using IHC staining patterns is appropriate for representing the similarity of H&E-stained tissue images for malignant lymphoma.



### 4D Attention: Comprehensive Framework for Spatio-Temporal Gaze Mapping
- **Arxiv ID**: http://arxiv.org/abs/2107.03606v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.03606v1)
- **Published**: 2021-07-08 04:55:18+00:00
- **Updated**: 2021-07-08 04:55:18+00:00
- **Authors**: Shuji Oishi, Kenji Koide, Masashi Yokozuka, Atsuhiko Banno
- **Comment**: None
- **Journal**: None
- **Summary**: This study presents a framework for capturing human attention in the spatio-temporal domain using eye-tracking glasses. Attention mapping is a key technology for human perceptual activity analysis or Human-Robot Interaction (HRI) to support human visual cognition; however, measuring human attention in dynamic environments is challenging owing to the difficulty in localizing the subject and dealing with moving objects. To address this, we present a comprehensive framework, 4D Attention, for unified gaze mapping onto static and dynamic objects. Specifically, we estimate the glasses pose by leveraging a loose coupling of direct visual localization and Inertial Measurement Unit (IMU) values. Further, by installing reconstruction components into our framework, dynamic objects not captured in the 3D environment map are instantiated based on the input images. Finally, a scene rendering component synthesizes a first-person view with identification (ID) textures and performs direct 2D-3D gaze association. Quantitative evaluations showed the effectiveness of our framework. Additionally, we demonstrated the applications of 4D Attention through experiments in real situations.



### Multi-frame Collaboration for Effective Endoscopic Video Polyp Detection via Spatial-Temporal Feature Transformation
- **Arxiv ID**: http://arxiv.org/abs/2107.03609v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03609v1)
- **Published**: 2021-07-08 05:17:30+00:00
- **Updated**: 2021-07-08 05:17:30+00:00
- **Authors**: Lingyun Wu, Zhiqiang Hu, Yuanfeng Ji, Ping Luo, Shaoting Zhang
- **Comment**: Accepted by MICCAI2021
- **Journal**: None
- **Summary**: Precise localization of polyp is crucial for early cancer screening in gastrointestinal endoscopy. Videos given by endoscopy bring both richer contextual information as well as more challenges than still images. The camera-moving situation, instead of the common camera-fixed-object-moving one, leads to significant background variation between frames. Severe internal artifacts (e.g. water flow in the human body, specular reflection by tissues) can make the quality of adjacent frames vary considerately. These factors hinder a video-based model to effectively aggregate features from neighborhood frames and give better predictions. In this paper, we present Spatial-Temporal Feature Transformation (STFT), a multi-frame collaborative framework to address these issues. Spatially, STFT mitigates inter-frame variations in the camera-moving situation with feature alignment by proposal-guided deformable convolutions. Temporally, STFT proposes a channel-aware attention module to simultaneously estimate the quality and correlation of adjacent frames for adaptive feature aggregation. Empirical studies and superior results demonstrate the effectiveness and stability of our method. For example, STFT improves the still image baseline FCOS by 10.6% and 20.6% on the comprehensive F1-score of the polyp localization task in CVC-Clinic and ASUMayo datasets, respectively, and outperforms the state-of-the-art video-based method by 3.6% and 8.0%, respectively. Code is available at \url{https://github.com/lingyunwu14/STFT}.



### NccFlow: Unsupervised Learning of Optical Flow With Non-occlusion from Geometry
- **Arxiv ID**: http://arxiv.org/abs/2107.03610v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03610v1)
- **Published**: 2021-07-08 05:19:54+00:00
- **Updated**: 2021-07-08 05:19:54+00:00
- **Authors**: Guangming Wang, Shuaiqi Ren, Hesheng Wang
- **Comment**: 10 pages, 7 figures, under review
- **Journal**: None
- **Summary**: Optical flow estimation is a fundamental problem of computer vision and has many applications in the fields of robot learning and autonomous driving. This paper reveals novel geometric laws of optical flow based on the insight and detailed definition of non-occlusion. Then, two novel loss functions are proposed for the unsupervised learning of optical flow based on the geometric laws of non-occlusion. Specifically, after the occlusion part of the images are masked, the flowing process of pixels is carefully considered and geometric constraints are conducted based on the geometric laws of optical flow. First, neighboring pixels in the first frame will not intersect during the pixel displacement to the second frame. Secondly, when the cluster containing adjacent four pixels in the first frame moves to the second frame, no other pixels will flow into the quadrilateral formed by them. According to the two geometrical constraints, the optical flow non-intersection loss and the optical flow non-blocking loss in the non-occlusion regions are proposed. Two loss functions punish the irregular and inexact optical flows in the non-occlusion regions. The experiments on datasets demonstrated that the proposed unsupervised losses of optical flow based on the geometric laws in non-occlusion regions make the estimated optical flow more refined in detail, and improve the performance of unsupervised learning of optical flow. In addition, the experiments training on synthetic data and evaluating on real data show that the generalization ability of optical flow network is improved by our proposed unsupervised approach.



### A Dataset and Method for Hallux Valgus Angle Estimation Based on Deep Learing
- **Arxiv ID**: http://arxiv.org/abs/2107.03640v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.4.7; I.2.10; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2107.03640v1)
- **Published**: 2021-07-08 07:11:12+00:00
- **Updated**: 2021-07-08 07:11:12+00:00
- **Authors**: Ningyuan Xu, Jiayan Zhuang, Yaojun Wu, Jiangjian Xiao
- **Comment**: 7pages, 12 figures
- **Journal**: None
- **Summary**: Angular measurements is essential to make a resonable treatment for Hallux valgus (HV), a common forefoot deformity. However, it still depends on manual labeling and measurement, which is time-consuming and sometimes unreliable. Automating this process is a thing of concern. However, it lack of dataset and the keypoints based method which made a great success in pose estimation is not suitable for this field.To solve the problems, we made a dataset and developed an algorithm based on deep learning and linear regression. It shows great fitting ability to the ground truth.



### Image restoration quality assessment based on regional differential information entropy
- **Arxiv ID**: http://arxiv.org/abs/2107.03642v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.03642v2)
- **Published**: 2021-07-08 07:12:55+00:00
- **Updated**: 2022-11-26 08:56:34+00:00
- **Authors**: Zhiyu Wang, Jiayan Zhuang, Ningyuan Xu, Sichao Ye, Jiangjian Xiao, Chengbin Peng
- **Comment**: 14 pages, 8 figures, 5 tables
- **Journal**: None
- **Summary**: With the development of image recovery models,especially those based on adversarial and perceptual losses,the detailed texture portions of images are being recovered more naturally.However,these restored images are similar but not identical in detail texture to their reference images.With traditional image quality assessment methods,results with better subjective perceived quality often score lower in objective scoring.Assessment methods suffer from subjective and objective inconsistencies.This paper proposes a regional differential information entropy (RDIE) method for image quality assessment to address this problem.This approach allows better assessment of similar but not identical textural details and achieves good agreement with perceived quality.Neural networks are used to reshape the process of calculating information entropy,improving the speed and efficiency of the operation. Experiments conducted with this study image quality assessment dataset and the PIPAL dataset show that the proposed RDIE method yields a high degree of agreement with people average opinion scores compared to other image quality assessment metrics,proving that RDIE can better quantify the perceived quality of images.



### Deep Learning Based Image Retrieval in the JPEG Compressed Domain
- **Arxiv ID**: http://arxiv.org/abs/2107.03648v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.03648v1)
- **Published**: 2021-07-08 07:30:03+00:00
- **Updated**: 2021-07-08 07:30:03+00:00
- **Authors**: Shrikant Temburwar, Bulla Rajesh, Mohammed Javed
- **Comment**: Accepted in MISP2021
- **Journal**: None
- **Summary**: Content-based image retrieval (CBIR) systems on pixel domain use low-level features, such as colour, texture and shape, to retrieve images. In this context, two types of image representations i.e. local and global image features have been studied in the literature. Extracting these features from pixel images and comparing them with images from the database is very time-consuming. Therefore, in recent years, there has been some effort to accomplish image analysis directly in the compressed domain with lesser computations. Furthermore, most of the images in our daily transactions are stored in the JPEG compressed format. Therefore, it would be ideal if we could retrieve features directly from the partially decoded or compressed data and use them for retrieval. Here, we propose a unified model for image retrieval which takes DCT coefficients as input and efficiently extracts global and local features directly in the JPEG compressed domain for accurate image retrieval. The experimental findings indicate that our proposed model performed similarly to the current DELG model which takes RGB features as an input with reference to mean average precision while having a faster training and retrieval speed.



### Elastic deformation of optical coherence tomography images of diabetic macular edema for deep-learning models training: how far to go?
- **Arxiv ID**: http://arxiv.org/abs/2107.03651v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.03651v2)
- **Published**: 2021-07-08 07:35:34+00:00
- **Updated**: 2021-07-13 07:43:21+00:00
- **Authors**: Daniel Bar-David, Laura Bar-David, Yinon Shapira, Rina Leibu, Dalia Dori, Ronit Schneor, Anath Fischer, Shiri Soudry
- **Comment**: None
- **Journal**: None
- **Summary**: To explore the clinical validity of elastic deformation of optical coherence tomography (OCT) images for data augmentation in the development of deep-learning model for detection of diabetic macular edema (DME).



### Crowd Counting via Perspective-Guided Fractional-Dilation Convolution
- **Arxiv ID**: http://arxiv.org/abs/2107.03665v1
- **DOI**: 10.1109/TMM.2021.3086709
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03665v1)
- **Published**: 2021-07-08 07:57:00+00:00
- **Updated**: 2021-07-08 07:57:00+00:00
- **Authors**: Zhaoyi Yan, Ruimao Zhang, Hongzhi Zhang, Qingfu Zhang, Wangmeng Zuo
- **Comment**: Accepted by T-MM 2021
- **Journal**: T-MM 2021
- **Summary**: Crowd counting is critical for numerous video surveillance scenarios. One of the main issues in this task is how to handle the dramatic scale variations of pedestrians caused by the perspective effect. To address this issue, this paper proposes a novel convolution neural network-based crowd counting method, termed Perspective-guided Fractional-Dilation Network (PFDNet). By modeling the continuous scale variations, the proposed PFDNet is able to select the proper fractional dilation kernels for adapting to different spatial locations. It significantly improves the flexibility of the state-of-the-arts that only consider the discrete representative scales. In addition, by avoiding the multi-scale or multi-column architecture that used in other methods, it is computationally more efficient. In practice, the proposed PFDNet is constructed by stacking multiple Perspective-guided Fractional-Dilation Convolutions (PFC) on a VGG16-BN backbone. By introducing a novel generalized dilation convolution operation, the PFC can handle fractional dilation ratios in the spatial domain under the guidance of perspective annotations, achieving continuous scales modeling of pedestrians. To deal with the problem of unavailable perspective information in some cases, we further introduce an effective perspective estimation branch to the proposed PFDNet, which can be trained in either supervised or weakly-supervised setting once the branch has been pre-trained. Extensive experiments show that the proposed PFDNet outperforms state-of-the-art methods on ShanghaiTech A, ShanghaiTech B, WorldExpo'10, UCF-QNRF, UCF_CC_50 and TRANCOS dataset, achieving MAE 53.8, 6.5, 6.8, 84.3, 205.8, and 3.06 respectively.



### Feature Pyramid Network for Multi-task Affective Analysis
- **Arxiv ID**: http://arxiv.org/abs/2107.03670v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03670v3)
- **Published**: 2021-07-08 08:10:04+00:00
- **Updated**: 2021-07-19 06:38:32+00:00
- **Authors**: Ruian He, Zhen Xing, Weimin Tan, Bo Yan
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Affective Analysis is not a single task, and the valence-arousal value, expression class, and action unit can be predicted at the same time. Previous researches did not pay enough attention to the entanglement and hierarchical relation of these three facial attributes. We propose a novel model named feature pyramid networks for multi-task affect analysis. The hierarchical features are extracted to predict three labels and we apply a teacher-student training strategy to learn from pretrained single-task models. Extensive experiment results demonstrate the proposed model outperforms other models. This is a submission to The 2nd Workshop and Competition on Affective Behavior Analysis in the wild (ABAW). The code and model are available for research purposes at https://github.com/ryanhe312/ABAW2-FPNMAA.



### End-to-end Malaria Diagnosis and 3D Cell Rendering with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.04220v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.04220v1)
- **Published**: 2021-07-08 08:13:11+00:00
- **Updated**: 2021-07-08 08:13:11+00:00
- **Authors**: Vignav Ramesh
- **Comment**: 7 pages, 2 figures
- **Journal**: None
- **Summary**: Malaria is a parasitic infection that poses a significant burden on global health. It kills one child every 30 seconds and over one million people annually. If diagnosed in a timely manner, however, most people can be effectively treated with antimalarial therapy. Several deaths due to malaria are byproducts of disparities in the social determinants of health; the current gold standard for diagnosing malaria requires microscopes, reagents, and other equipment that most patients of low socioeconomic brackets do not have access to. In this paper, we propose a convolutional neural network (CNN) architecture that allows for rapid automated diagnosis of malaria (achieving a high classification accuracy of 98%), as well as a deep neural network (DNN) based three-dimensional (3D) modeling algorithm that renders 3D models of parasitic cells in augmented reality (AR). This creates an opportunity to optimize the current workflow for malaria diagnosis and demonstrates potential for deep learning models to improve telemedicine practices and patient health literacy on a global scale.



### An Embedded Iris Recognition System Optimization using Dynamically ReconfigurableDecoder with LDPC Codes
- **Arxiv ID**: http://arxiv.org/abs/2107.03688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03688v1)
- **Published**: 2021-07-08 09:04:11+00:00
- **Updated**: 2021-07-08 09:04:11+00:00
- **Authors**: Longyu Ma, Chiu-Wing Sham, Chun Yan Lo, Xinchao Zhong
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Extracting and analyzing iris textures for biometric recognition has been extensively studied. As the transition of iris recognition from lab technology to nation-scale applications, most systems are facing high complexity in either time or space, leading to unfitness for embedded devices. In this paper, the proposed design includes a minimal set of computer vision modules and multi-mode QC-LDPC decoder which can alleviate variability and noise caused by iris acquisition and follow-up process. Several classes of QC-LDPC code from IEEE 802.16 are tested for the validity of accuracy improvement. Some of the codes mentioned above are used for further QC-LDPC decoder quantization, validation and comparison to each other. We show that we can apply Dynamic Partial Reconfiguration technology to implement the multi-mode QC-LDPC decoder for the iris recognition system. The results show that the implementation is power-efficient and good for edge applications.



### Complete Scanning Application Using OpenCv
- **Arxiv ID**: http://arxiv.org/abs/2107.03700v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.03700v1)
- **Published**: 2021-07-08 09:21:57+00:00
- **Updated**: 2021-07-08 09:21:57+00:00
- **Authors**: Ayushe Gangal, Peeyush Kumar, Sunita Kumari
- **Comment**: 10 pages, 14 figures
- **Journal**: None
- **Summary**: In the following paper, we have combined the various basic functionalities provided by the NumPy library and OpenCv library, which is an open source for Computer Vision applications, like conversion of colored images to grayscale, calculating threshold, finding contours and using those contour points to take perspective transform of the image inputted by the user, using Python version 3.7. Additional features include cropping, rotating and saving as well. All these functions and features, when implemented step by step, results in a complete scanning application. The applied procedure involves the following steps: Finding contours, applying Perspective transform and brightening the image, Adaptive Thresholding and applying filters for noise cancellation, and Rotation features and perspective transform for a special cropping algorithm. The described technique is implemented on various samples.



### Prior Aided Streaming Network for Multi-task Affective Recognitionat the 2nd ABAW2 Competition
- **Arxiv ID**: http://arxiv.org/abs/2107.03708v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03708v1)
- **Published**: 2021-07-08 09:35:08+00:00
- **Updated**: 2021-07-08 09:35:08+00:00
- **Authors**: Wei Zhang, Zunhu Guo, Keyu Chen, Lincheng Li, Zhimeng Zhang, Yu Ding
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic affective recognition has been an important research topic in human computer interaction (HCI) area. With recent development of deep learning techniques and large scale in-the-wild annotated datasets, the facial emotion analysis is now aimed at challenges in the real world settings. In this paper, we introduce our submission to the 2nd Affective Behavior Analysis in-the-wild (ABAW2) Competition. In dealing with different emotion representations, including Categorical Emotions (CE), Action Units (AU), and Valence Arousal (VA), we propose a multi-task streaming network by a heuristic that the three representations are intrinsically associated with each other. Besides, we leverage an advanced facial expression embedding as prior knowledge, which is capable of capturing identity-invariant expression features while preserving the expression similarities, to aid the down-streaming recognition tasks. The extensive quantitative evaluations as well as ablation studies on the Aff-Wild2 dataset prove the effectiveness of our proposed prior aided streaming network approach.



### Grid Partitioned Attention: Efficient TransformerApproximation with Inductive Bias for High Resolution Detail Generation
- **Arxiv ID**: http://arxiv.org/abs/2107.03742v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.03742v1)
- **Published**: 2021-07-08 10:37:23+00:00
- **Updated**: 2021-07-08 10:37:23+00:00
- **Authors**: Nikolay Jetchev, Gökhan Yildirim, Christian Bracher, Roland Vollgraf
- **Comment**: code available at https://github.com/zalandoresearch/gpa
- **Journal**: None
- **Summary**: Attention is a general reasoning mechanism than can flexibly deal with image information, but its memory requirements had made it so far impractical for high resolution image generation. We present Grid Partitioned Attention (GPA), a new approximate attention algorithm that leverages a sparse inductive bias for higher computational and memory efficiency in image domains: queries attend only to few keys, spatially close queries attend to close keys due to correlations. Our paper introduces the new attention layer, analyzes its complexity and how the trade-off between memory usage and model power can be tuned by the hyper-parameters.We will show how such attention enables novel deep learning architectures with copying modules that are especially useful for conditional image generation tasks like pose morphing. Our contributions are (i) algorithm and code1of the novel GPA layer, (ii) a novel deep attention-copying architecture, and (iii) new state-of-the art experimental results in human pose morphing generation benchmarks.



### Exploiting the relationship between visual and textual features in social networks for image classification with zero-shot deep learning
- **Arxiv ID**: http://arxiv.org/abs/2107.03751v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.03751v1)
- **Published**: 2021-07-08 10:54:59+00:00
- **Updated**: 2021-07-08 10:54:59+00:00
- **Authors**: Luis Lucas, David Tomas, Jose Garcia-Rodriguez
- **Comment**: None
- **Journal**: None
- **Summary**: One of the main issues related to unsupervised machine learning is the cost of processing and extracting useful information from large datasets. In this work, we propose a classifier ensemble based on the transferable learning capabilities of the CLIP neural network architecture in multimodal environments (image and text) from social media. For this purpose, we used the InstaNY100K dataset and proposed a validation approach based on sampling techniques. Our experiments, based on image classification tasks according to the labels of the Places dataset, are performed by first considering only the visual part, and then adding the associated texts as support. The results obtained demonstrated that trained neural networks such as CLIP can be successfully applied to image classification with little fine-tuning, and considering the associated texts to the images can help to improve the accuracy depending on the goal. The results demonstrated what seems to be a promising research direction.



### Investigate the Essence of Long-Tailed Recognition from a Unified Perspective
- **Arxiv ID**: http://arxiv.org/abs/2107.03758v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03758v2)
- **Published**: 2021-07-08 11:08:40+00:00
- **Updated**: 2022-08-17 01:27:01+00:00
- **Authors**: Lei Liu, Li Liu
- **Comment**: unfinished version
- **Journal**: None
- **Summary**: As the data scale grows, deep recognition models often suffer from long-tailed data distributions due to the heavy imbalanced sample number across categories. Indeed, real-world data usually exhibit some similarity relation among different categories (e.g., pigeons and sparrows), called category similarity in this work. It is doubly difficult when the imbalance occurs between such categories with similar appearances. However, existing solutions mainly focus on the sample number to re-balance data distribution. In this work, we systematically investigate the essence of the long-tailed problem from a unified perspective. Specifically, we demonstrate that long-tailed recognition suffers from both sample number and category similarity. Intuitively, using a toy example, we first show that sample number is not the unique influence factor for performance dropping of long-tailed recognition. Theoretically, we demonstrate that (1) category similarity, as an inevitable factor, would also influence the model learning under long-tailed distribution via similar samples, (2) using more discriminative representation methods (e.g., self-supervised learning) for similarity reduction, the classifier bias can be further alleviated with greatly improved performance. Extensive experiments on several long-tailed datasets verify the rationality of our theoretical analysis, and show that based on existing state-of-the-arts (SOTAs), the performance could be further improved by similarity reduction. Our investigations highlight the essence behind the long-tailed problem, and claim several feasible directions for future work.



### Susceptibility to Image Resolution in Face Recognition and Trainings Strategies
- **Arxiv ID**: http://arxiv.org/abs/2107.03769v2
- **DOI**: 10.4230/LITES.8.1.1
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.03769v2)
- **Published**: 2021-07-08 11:30:27+00:00
- **Updated**: 2022-11-25 11:09:56+00:00
- **Authors**: Martin Knoche, Stefan Hörmann, Gerhard Rigoll
- **Comment**: 19 pages, 15 figures, 2 tables
- **Journal**: None
- **Summary**: Face recognition approaches often rely on equal image resolution for verifying faces on two images. However, in practical applications, those image resolutions are usually not in the same range due to different image capture mechanisms or sources. In this work, we first analyze the impact of image resolutions on face verification performance with a state-of-the-art face recognition model. For images synthetically reduced to $5\,\times\,5$ px resolution, the verification performance drops from $99.23\%$ increasingly down to almost $55\%$. Especially for cross-resolution image pairs (one high- and one low-resolution image), the verification accuracy decreases even further. We investigate this behavior more in-depth by looking at the feature distances for every 2-image test pair. To tackle this problem, we propose the following two methods: 1) Train a state-of-the-art face-recognition model straightforwardly with $50\%$ low-resolution images directly within each batch. 2) Train a siamese-network structure and add a cosine distance feature loss between high- and low-resolution features. Both methods show an improvement for cross-resolution scenarios and can increase the accuracy at very low resolution to approximately $70\%$. However, a disadvantage is that a specific model needs to be trained for every resolution pair. Thus, we extend the aforementioned methods by training them with multiple image resolutions at once. The performances for particular testing image resolutions are slightly worse, but the advantage is that this model can be applied to arbitrary resolution images and achieves an overall better performance ($97.72\%$ compared to $96.86\%$). Due to the lack of a benchmark for arbitrary resolution images for the cross-resolution and equal-resolution task, we propose an evaluation protocol for five well-known datasets, focusing on high, mid, and low-resolution images.



### Optimizing Data Processing in Space for Object Detection in Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2107.03774v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.03774v1)
- **Published**: 2021-07-08 11:37:24+00:00
- **Updated**: 2021-07-08 11:37:24+00:00
- **Authors**: Martina Lofqvist, José Cano
- **Comment**: Published as a workshop paper at SmallSat 2021 - The 35th Annual
  Small Satellite Conference. 9 pages, 10 figures. arXiv admin note: text
  overlap with arXiv:2007.11089
- **Journal**: None
- **Summary**: There is a proliferation in the number of satellites launched each year, resulting in downlinking of terabytes of data each day. The data received by ground stations is often unprocessed, making this an expensive process considering the large data sizes and that not all of the data is useful. This, coupled with the increasing demand for real-time data processing, has led to a growing need for on-orbit processing solutions. In this work, we investigate the performance of CNN-based object detectors on constrained devices by applying different image compression techniques to satellite data. We examine the capabilities of the NVIDIA Jetson Nano and NVIDIA Jetson AGX Xavier; low-power, high-performance computers, with integrated GPUs, small enough to fit on-board a nanosatellite. We take a closer look at object detection networks, including the Single Shot MultiBox Detector (SSD) and Region-based Fully Convolutional Network (R-FCN) models that are pre-trained on DOTA - a Large Scale Dataset for Object Detection in Aerial Images. The performance is measured in terms of execution time, memory consumption, and accuracy, and are compared against a baseline containing a server with two powerful GPUs. The results show that by applying image compression techniques, we are able to improve the execution time and memory consumption, achieving a fully runnable dataset. A lossless compression technique achieves roughly a 10% reduction in execution time and about a 3% reduction in memory consumption, with no impact on the accuracy. While a lossy compression technique improves the execution time by up to 144% and the memory consumption is reduced by as much as 97%. However, it has a significant impact on accuracy, varying depending on the compression ratio. Thus the application and ratio of these compression techniques may differ depending on the required level of accuracy for a particular task.



### Use of Affective Visual Information for Summarization of Human-Centric Videos
- **Arxiv ID**: http://arxiv.org/abs/2107.03783v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2107.03783v1)
- **Published**: 2021-07-08 11:46:04+00:00
- **Updated**: 2021-07-08 11:46:04+00:00
- **Authors**: Berkay Köprü, Engin Erzin
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Increasing volume of user-generated human-centric video content and their applications, such as video retrieval and browsing, require compact representations that are addressed by the video summarization literature. Current supervised studies formulate video summarization as a sequence-to-sequence learning problem and the existing solutions often neglect the surge of human-centric view, which inherently contains affective content. In this study, we investigate the affective-information enriched supervised video summarization task for human-centric videos. First, we train a visual input-driven state-of-the-art continuous emotion recognition model (CER-NET) on the RECOLA dataset to estimate emotional attributes. Then, we integrate the estimated emotional attributes and the high-level representations from the CER-NET with the visual information to define the proposed affective video summarization architectures (AVSUM). In addition, we investigate the use of attention to improve the AVSUM architectures and propose two new architectures based on temporal attention (TA-AVSUM) and spatial attention (SA-AVSUM). We conduct video summarization experiments on the TvSum database. The proposed AVSUM-GRU architecture with an early fusion of high level GRU embeddings and the temporal attention based TA-AVSUM architecture attain competitive video summarization performances by bringing strong performance improvements for the human-centric videos compared to the state-of-the-art in terms of F-score and self-defined face recall metrics.



### Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs
- **Arxiv ID**: http://arxiv.org/abs/2107.03815v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.03815v2)
- **Published**: 2021-07-08 12:44:41+00:00
- **Updated**: 2022-07-19 14:29:13+00:00
- **Authors**: Yikang Zhang, Zhuo Chen, Zhao Zhong
- **Comment**: ICML
- **Journal**: None
- **Summary**: In this paper, we propose a Collaboration of Experts (CoE) framework to pool together the expertise of multiple networks towards a common aim. Each expert is an individual network with expertise on a unique portion of the dataset, which enhances the collective capacity. Given a sample, an expert is selected by the delegator, which simultaneously outputs a rough prediction to support early termination. To fulfill this framework, we propose three modules to impel each model to play its role, namely weight generation module (WGM), label generation module (LGM) and variance calculation module (VCM). Our method achieves the state-of-the-art performance on ImageNet, 80.7% top-1 accuracy with 194M FLOPs. Combined with PWLU activation function and CondConv, CoE further achieves the accuracy of 80.0% with only 100M FLOPs for the first time. More importantly, our method is hardware friendly and achieves a 3-6x speedup compared with some existing conditional computation approaches.



### Instance-Level Relative Saliency Ranking with Graph Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2107.03824v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03824v1)
- **Published**: 2021-07-08 13:10:42+00:00
- **Updated**: 2021-07-08 13:10:42+00:00
- **Authors**: Nian Liu, Long Li, Wangbo Zhao, Junwei Han, Ling Shao
- **Comment**: TPAMI under review
- **Journal**: None
- **Summary**: Conventional salient object detection models cannot differentiate the importance of different salient objects. Recently, two works have been proposed to detect saliency ranking by assigning different degrees of saliency to different objects. However, one of these models cannot differentiate object instances and the other focuses more on sequential attention shift order inference. In this paper, we investigate a practical problem setting that requires simultaneously segment salient instances and infer their relative saliency rank order. We present a novel unified model as the first end-to-end solution, where an improved Mask R-CNN is first used to segment salient instances and a saliency ranking branch is then added to infer the relative saliency. For relative saliency ranking, we build a new graph reasoning module by combining four graphs to incorporate the instance interaction relation, local contrast, global contrast, and a high-level semantic prior, respectively. A novel loss function is also proposed to effectively train the saliency ranking branch. Besides, a new dataset and an evaluation metric are proposed for this task, aiming at pushing forward this field of research. Finally, experimental results demonstrate that our proposed model is more effective than previous methods. We also show an example of its practical usage on adaptive image retargeting.



### Label-set Loss Functions for Partial Supervision: Application to Fetal Brain 3D MRI Parcellation
- **Arxiv ID**: http://arxiv.org/abs/2107.03846v2
- **DOI**: 10.1007/978-3-030-87196-3_60
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.03846v2)
- **Published**: 2021-07-08 13:53:56+00:00
- **Updated**: 2021-07-09 15:44:06+00:00
- **Authors**: Lucas Fidon, Michael Aertsen, Doaa Emam, Nada Mufti, Frédéric Guffens, Thomas Deprest, Philippe Demaerel, Anna L. David, Andrew Melbourne, Sébastien Ourselin, Jan Deprest, Tom Vercauteren
- **Comment**: Accepted at MICCAI 2021
- **Journal**: None
- **Summary**: Deep neural networks have increased the accuracy of automatic segmentation, however, their accuracy depends on the availability of a large number of fully segmented images. Methods to train deep neural networks using images for which some, but not all, regions of interest are segmented are necessary to make better use of partially annotated datasets. In this paper, we propose the first axiomatic definition of label-set loss functions that are the loss functions that can handle partially segmented images. We prove that there is one and only one method to convert a classical loss function for fully segmented images into a proper label-set loss function. Our theory also allows us to define the leaf-Dice loss, a label-set generalization of the Dice loss particularly suited for partial supervision with only missing labels. Using the leaf-Dice loss, we set a new state of the art in partially supervised learning for fetal brain 3D MRI segmentation. We achieve a deep neural network able to segment white matter, ventricles, cerebellum, extra-ventricular CSF, cortical gray matter, deep gray matter, brainstem, and corpus callosum based on fetal brain 3D MRI of anatomically normal fetuses or with open spina bifida. Our implementation of the proposed label-set loss functions is available at https://github.com/LucasFidon/label-set-loss-functions



### Causal affect prediction model using a facial image sequence
- **Arxiv ID**: http://arxiv.org/abs/2107.03886v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2107.03886v1)
- **Published**: 2021-07-08 15:13:50+00:00
- **Updated**: 2021-07-08 15:13:50+00:00
- **Authors**: Geesung Oh, Euiseok Jeong, Sejoon Lim
- **Comment**: None
- **Journal**: None
- **Summary**: Among human affective behavior research, facial expression recognition research is improving in performance along with the development of deep learning. However, for improved performance, not only past images but also future images should be used along with corresponding facial images, but there are obstacles to the application of this technique to real-time environments. In this paper, we propose the causal affect prediction network (CAPNet), which uses only past facial images to predict corresponding affective valence and arousal. We train CAPNet to learn causal inference between past images and corresponding affective valence and arousal through supervised learning by pairing the sequence of past images with the current label using the Aff-Wild2 dataset. We show through experiments that the well-trained CAPNet outperforms the baseline of the second challenge of the Affective Behavior Analysis in-the-wild (ABAW2) Competition by predicting affective valence and arousal only with past facial images one-third of a second earlier. Therefore, in real-time application, CAPNet can reliably predict affective valence and arousal only with past data.



### Joint Motion Correction and Super Resolution for Cardiac Segmentation via Latent Optimisation
- **Arxiv ID**: http://arxiv.org/abs/2107.03887v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.03887v1)
- **Published**: 2021-07-08 15:14:00+00:00
- **Updated**: 2021-07-08 15:14:00+00:00
- **Authors**: Shuo Wang, Chen Qin, Nicolo Savioli, Chen Chen, Declan O'Regan, Stuart Cook, Yike Guo, Daniel Rueckert, Wenjia Bai
- **Comment**: The paper is early accepted to MICCAI 2021. The codes are available
  at https://github.com/shuowang26/SRHeart
- **Journal**: None
- **Summary**: In cardiac magnetic resonance (CMR) imaging, a 3D high-resolution segmentation of the heart is essential for detailed description of its anatomical structures. However, due to the limit of acquisition duration and respiratory/cardiac motion, stacks of multi-slice 2D images are acquired in clinical routine. The segmentation of these images provides a low-resolution representation of cardiac anatomy, which may contain artefacts caused by motion. Here we propose a novel latent optimisation framework that jointly performs motion correction and super resolution for cardiac image segmentations. Given a low-resolution segmentation as input, the framework accounts for inter-slice motion in cardiac MR imaging and super-resolves the input into a high-resolution segmentation consistent with input. A multi-view loss is incorporated to leverage information from both short-axis view and long-axis view of cardiac imaging. To solve the inverse problem, iterative optimisation is performed in a latent space, which ensures the anatomical plausibility. This alleviates the need of paired low-resolution and high-resolution images for supervised learning. Experiments on two cardiac MR datasets show that the proposed framework achieves high performance, comparable to state-of-the-art super-resolution approaches and with better cross-domain generalisability and anatomical plausibility.



### Uncertainty-Aware Camera Pose Estimation from Points and Lines
- **Arxiv ID**: http://arxiv.org/abs/2107.03890v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03890v1)
- **Published**: 2021-07-08 15:19:36+00:00
- **Updated**: 2021-07-08 15:19:36+00:00
- **Authors**: Alexander Vakhitov, Luis Ferraz Colomina, Antonio Agudo, Francesc Moreno-Noguer
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Perspective-n-Point-and-Line (P$n$PL) algorithms aim at fast, accurate, and robust camera localization with respect to a 3D model from 2D-3D feature correspondences, being a major part of modern robotic and AR/VR systems. Current point-based pose estimation methods use only 2D feature detection uncertainties, and the line-based methods do not take uncertainties into account. In our setup, both 3D coordinates and 2D projections of the features are considered uncertain. We propose PnP(L) solvers based on EPnP and DLS for the uncertainty-aware pose estimation. We also modify motion-only bundle adjustment to take 3D uncertainties into account. We perform exhaustive synthetic and real experiments on two different visual odometry datasets. The new PnP(L) methods outperform the state-of-the-art on real data in isolation, showing an increase in mean translation accuracy by 18% on a representative subset of KITTI, while the new uncertain refinement improves pose accuracy for most of the solvers, e.g. decreasing mean translation error for the EPnP by 16% compared to the standard refinement on the same dataset. The code is available at https://alexandervakhitov.github.io/uncertain-pnp/.



### Technical Report for Valence-Arousal Estimation in ABAW2 Challenge
- **Arxiv ID**: http://arxiv.org/abs/2107.03891v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03891v1)
- **Published**: 2021-07-08 15:21:38+00:00
- **Updated**: 2021-07-08 15:21:38+00:00
- **Authors**: Hong-Xia Xie, I-Hsuan Li, Ling Lo, Hong-Han Shuai, Wen-Huang Cheng
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2105.01502
- **Journal**: None
- **Summary**: In this work, we describe our method for tackling the valence-arousal estimation challenge from ABAW2 ICCV-2021 Competition. The competition organizers provide an in-the-wild Aff-Wild2 dataset for participants to analyze affective behavior in real-life settings. We use a two stream model to learn emotion features from appearance and action respectively. To solve data imbalanced problem, we apply label distribution smoothing (LDS) to re-weight labels. Our proposed method achieves Concordance Correlation Coefficient (CCC) of 0.591 and 0.617 for valence and arousal on the validation set of Aff-wild2 dataset.



### A hybrid deep learning framework for Covid-19 detection via 3D Chest CT Images
- **Arxiv ID**: http://arxiv.org/abs/2107.03904v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.03904v2)
- **Published**: 2021-07-08 15:37:46+00:00
- **Updated**: 2021-07-09 11:16:14+00:00
- **Authors**: Shuang Liang
- **Comment**: 5 pages, 1 figure, 2 tables
- **Journal**: None
- **Summary**: In this paper, we present a hybrid deep learning framework named CTNet which combines convolutional neural network and transformer together for the detection of COVID-19 via 3D chest CT images. It consists of a CNN feature extractor module with SE attention to extract sufficient features from CT scans, together with a transformer model to model the discriminative features of the 3D CT scans. Compared to previous works, CTNet provides an effective and efficient method to perform COVID-19 diagnosis via 3D CT scans with data resampling strategy. Advanced results on a large and public benchmarks, COV19-CT-DB database was achieved by the proposed CTNet, over the state-of-the-art baseline approachproposed together with the dataset.



### Weight Reparametrization for Budget-Aware Network Pruning
- **Arxiv ID**: http://arxiv.org/abs/2107.03909v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03909v2)
- **Published**: 2021-07-08 15:40:16+00:00
- **Updated**: 2021-07-27 14:15:03+00:00
- **Authors**: Robin Dupont, Hichem Sahbi, Guillaume Michel
- **Comment**: Accepted at International Conference on Image Processing (ICIP 2021)
- **Journal**: None
- **Summary**: Pruning seeks to design lightweight architectures by removing redundant weights in overparameterized networks. Most of the existing techniques first remove structured sub-networks (filters, channels,...) and then fine-tune the resulting networks to maintain a high accuracy. However, removing a whole structure is a strong topological prior and recovering the accuracy, with fine-tuning, is highly cumbersome. In this paper, we introduce an "end-to-end" lightweight network design that achieves training and pruning simultaneously without fine-tuning. The design principle of our method relies on reparametrization that learns not only the weights but also the topological structure of the lightweight sub-network. This reparametrization acts as a prior (or regularizer) that defines pruning masks implicitly from the weights of the underlying network, without increasing the number of training parameters. Sparsity is induced with a budget loss that provides an accurate pruning. Extensive experiments conducted on the CIFAR10 and the TinyImageNet datasets, using standard architectures (namely Conv4, VGG19 and ResNet18), show compelling results without fine-tuning.



### Task Fingerprinting for Meta Learning in Biomedical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2107.03949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03949v1)
- **Published**: 2021-07-08 16:20:28+00:00
- **Updated**: 2021-07-08 16:20:28+00:00
- **Authors**: Patrick Godau, Lena Maier-Hein
- **Comment**: Medical Image Computing and Computer Assisted Interventions (MICCAI)
  2021
- **Journal**: None
- **Summary**: Shortage of annotated data is one of the greatest bottlenecks in biomedical image analysis. Meta learning studies how learning systems can increase in efficiency through experience and could thus evolve as an important concept to overcome data sparsity. However, the core capability of meta learning-based approaches is the identification of similar previous tasks given a new task - a challenge largely unexplored in the biomedical imaging domain. In this paper, we address the problem of quantifying task similarity with a concept that we refer to as task fingerprinting. The concept involves converting a given task, represented by imaging data and corresponding labels, to a fixed-length vector representation. In fingerprint space, different tasks can be directly compared irrespective of their data set sizes, types of labels or specific resolutions. An initial feasibility study in the field of surgical data science (SDS) with 26 classification tasks from various medical and non-medical domains suggests that task fingerprinting could be leveraged for both (1) selecting appropriate data sets for pretraining and (2) selecting appropriate architectures for a new task. Task fingerprinting could thus become an important tool for meta learning in SDS and other fields of biomedical image analysis.



### Classifying Component Function in Product Assemblies with Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.07042v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.07042v1)
- **Published**: 2021-07-08 16:27:23+00:00
- **Updated**: 2021-07-08 16:27:23+00:00
- **Authors**: Vincenzo Ferrero, Kaveh Hassani, Daniele Grandi, Bryony DuPont
- **Comment**: None
- **Journal**: None
- **Summary**: Function is defined as the ensemble of tasks that enable the product to complete the designed purpose. Functional tools, such as functional modeling, offer decision guidance in the early phase of product design, where explicit design decisions are yet to be made. Function-based design data is often sparse and grounded in individual interpretation. As such, function-based design tools can benefit from automatic function classification to increase data fidelity and provide function representation models that enable function-based intelligent design agents. Function-based design data is commonly stored in manually generated design repositories. These design repositories are a collection of expert knowledge and interpretations of function in product design bounded by function-flow and component taxonomies. In this work, we represent a structured taxonomy-based design repository as assembly-flow graphs, then leverage a graph neural network (GNN) model to perform automatic function classification. We support automated function classification by learning from repository data to establish the ground truth of component function assignment. Experimental results show that our GNN model achieves a micro-average F${_1}$-score of 0.832 for tier 1 (broad), 0.756 for tier 2, and 0.783 for tier 3 (specific) functions. Given the imbalance of data features, the results are encouraging. Our efforts in this paper can be a starting point for more sophisticated applications in knowledge-based CAD systems and Design-for-X consideration in function-based design.



### Enhancing Video Analytics Accuracy via Real-time Automated Camera Parameter Tuning
- **Arxiv ID**: http://arxiv.org/abs/2107.03964v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.03964v4)
- **Published**: 2021-07-08 16:43:02+00:00
- **Updated**: 2022-09-15 20:47:47+00:00
- **Authors**: Sibendu Paul, Kunal Rao, Giuseppe Coviello, Murugan Sankaradas, Oliver Po, Y. Charlie Hu, Srimat T. Chakradhar
- **Comment**: None
- **Journal**: None
- **Summary**: In Video Analytics Pipelines (VAP), Analytics Units (AUs) such as object detection and face recognition running on remote servers critically rely on surveillance cameras to capture high-quality video streams in order to achieve high accuracy. Modern IP cameras come with a large number of camera parameters that directly affect the quality of the video stream capture. While a few of such parameters, e.g., exposure, focus, white balance are automatically adjusted by the camera internally, the remaining ones are not. We denote such camera parameters as non-automated (NAUTO) parameters. In this paper, we first show that environmental condition changes can have significant adverse effect on the accuracy of insights from the AUs, but such adverse impact can potentially be mitigated by dynamically adjusting NAUTO camera parameters in response to changes in environmental conditions. We then present CamTuner, to our knowledge, the first framework that dynamically adapts NAUTO camera parameters to optimize the accuracy of AUs in a VAP in response to adverse changes in environmental conditions. CamTuner is based on SARSA reinforcement learning and it incorporates two novel components: a light-weight analytics quality estimator and a virtual camera that drastically speed up offline RL training. Our controlled experiments and real-world VAP deployment show that compared to a VAP using the default camera setting, CamTuner enhances VAP accuracy by detecting 15.9% additional persons and 2.6%-4.2% additional cars (without any false positives) in a large enterprise parking lot and 9.7% additional cars in a 5G smart traffic intersection scenario, which enables a new usecase of accurate and reliable automatic vehicle collision prediction (AVCP). CamTuner opens doors for new ways to significantly enhance video analytics accuracy beyond incremental improvements from refining deep-learning models.



### EEG-ConvTransformer for Single-Trial EEG based Visual Stimuli Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.03983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03983v1)
- **Published**: 2021-07-08 17:22:04+00:00
- **Updated**: 2021-07-08 17:22:04+00:00
- **Authors**: Subhranil Bagchi, Deepti R. Bathula
- **Comment**: Preprint and Supplementary material. 17 pages, 13 figures and 4
  tables
- **Journal**: None
- **Summary**: Different categories of visual stimuli activate different responses in the human brain. These signals can be captured with EEG for utilization in applications such as Brain-Computer Interface (BCI). However, accurate classification of single-trial data is challenging due to low signal-to-noise ratio of EEG. This work introduces an EEG-ConvTranformer network that is based on multi-headed self-attention. Unlike other transformers, the model incorporates self-attention to capture inter-region interactions. It further extends to adjunct convolutional filters with multi-head attention as a single module to learn temporal patterns. Experimental results demonstrate that EEG-ConvTransformer achieves improved classification accuracy over the state-of-the-art techniques across five different visual stimuli classification tasks. Finally, quantitative analysis of inter-head diversity also shows low similarity in representational subspaces, emphasizing the implicit diversity of multi-head attention.



### Atlas-Based Segmentation of Intracochlear Anatomy in Metal Artifact Affected CT Images of the Ear with Co-trained Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.03987v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.03987v2)
- **Published**: 2021-07-08 17:26:19+00:00
- **Updated**: 2021-07-09 15:00:52+00:00
- **Authors**: Jianing Wang, Dingjie Su, Yubo Fan, Srijata Chakravorti, Jack H. Noble, Benoit M. Dawant
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: We propose an atlas-based method to segment the intracochlear anatomy (ICA) in the post-implantation CT (Post-CT) images of cochlear implant (CI) recipients that preserves the point-to-point correspondence between the meshes in the atlas and the segmented volumes. To solve this problem, which is challenging because of the strong artifacts produced by the implant, we use a pair of co-trained deep networks that generate dense deformation fields (DDFs) in opposite directions. One network is tasked with registering an atlas image to the Post-CT images and the other network is tasked with registering the Post-CT images to the atlas image. The networks are trained using loss functions based on voxel-wise labels, image content, fiducial registration error, and cycle-consistency constraint. The segmentation of the ICA in the Post-CT images is subsequently obtained by transferring the predefined segmentation meshes of the ICA in the atlas image to the Post-CT images using the corresponding DDFs generated by the trained registration networks. Our model can learn the underlying geometric features of the ICA even though they are obscured by the metal artifacts. We show that our end-to-end network produces results that are comparable to the current state of the art (SOTA) that relies on a two-steps approach that first uses conditional generative adversarial networks to synthesize artifact-free images from the Post-CT images and then uses an active shape model-based method to segment the ICA in the synthetic images. Our method requires a fraction of the time needed by the SOTA, which is important for end-user acceptance.



### Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers
- **Arxiv ID**: http://arxiv.org/abs/2107.03996v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.03996v3)
- **Published**: 2021-07-08 17:41:55+00:00
- **Updated**: 2022-05-26 04:39:01+00:00
- **Authors**: Ruihan Yang, Minghao Zhang, Nicklas Hansen, Huazhe Xu, Xiaolong Wang
- **Comment**: Our project page with videos is at
  https://RchalYang.github.io/LocoTransformer
- **Journal**: None
- **Summary**: We propose to address quadrupedal locomotion tasks using Reinforcement Learning (RL) with a Transformer-based model that learns to combine proprioceptive information and high-dimensional depth sensor inputs. While learning-based locomotion has made great advances using RL, most methods still rely on domain randomization for training blind agents that generalize to challenging terrains. Our key insight is that proprioceptive states only offer contact measurements for immediate reaction, whereas an agent equipped with visual sensory observations can learn to proactively maneuver environments with obstacles and uneven terrain by anticipating changes in the environment many steps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL method that leverages both proprioceptive states and visual observations for locomotion control. We evaluate our method in challenging simulated environments with different obstacles and uneven terrain. We transfer our learned policy from simulation to a real robot by running it indoors and in the wild with unseen obstacles and terrain. Our method not only significantly improves over baselines, but also achieves far better generalization performance, especially when transferred to the real robot. Our project page with videos is at https://rchalyang.github.io/LocoTransformer/ .



### Active Safety Envelopes using Light Curtains with Probabilistic Guarantees
- **Arxiv ID**: http://arxiv.org/abs/2107.04000v1
- **DOI**: 10.15607/rss.2021.xvii.045
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.04000v1)
- **Published**: 2021-07-08 17:46:05+00:00
- **Updated**: 2021-07-08 17:46:05+00:00
- **Authors**: Siddharth Ancha, Gaurav Pathak, Srinivasa G. Narasimhan, David Held
- **Comment**: 18 pages, Published at Robotics: Science and Systems (RSS) 2021
- **Journal**: None
- **Summary**: To safely navigate unknown environments, robots must accurately perceive dynamic obstacles. Instead of directly measuring the scene depth with a LiDAR sensor, we explore the use of a much cheaper and higher resolution sensor: programmable light curtains. Light curtains are controllable depth sensors that sense only along a surface that a user selects. We use light curtains to estimate the safety envelope of a scene: a hypothetical surface that separates the robot from all obstacles. We show that generating light curtains that sense random locations (from a particular distribution) can quickly discover the safety envelope for scenes with unknown objects. Importantly, we produce theoretical safety guarantees on the probability of detecting an obstacle using random curtains. We combine random curtains with a machine learning based model that forecasts and tracks the motion of the safety envelope efficiently. Our method accurately estimates safety envelopes while providing probabilistic safety guarantees that can be used to certify the efficacy of a robot perception system to detect and avoid dynamic obstacles. We evaluate our approach in a simulated urban driving environment and a real-world environment with moving pedestrians using a light curtain device and show that we can estimate safety envelopes efficiently and effectively. Project website: https://siddancha.github.io/projects/active-safety-envelopes-with-guarantees



### 3D Neural Scene Representations for Visuomotor Control
- **Arxiv ID**: http://arxiv.org/abs/2107.04004v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.04004v2)
- **Published**: 2021-07-08 17:49:37+00:00
- **Updated**: 2021-11-11 20:16:59+00:00
- **Authors**: Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal, Antonio Torralba
- **Comment**: Accepted to Conference on Robot Learning (CoRL 2021) as Oral
  Presentation. The first two authors contributed equally. Project Page:
  https://3d-representation-learning.github.io/nerf-dy/
- **Journal**: None
- **Summary**: Humans have a strong intuitive understanding of the 3D environment around us. The mental model of the physics in our brain applies to objects of different materials and enables us to perform a wide range of manipulation tasks that are far beyond the reach of current robots. In this work, we desire to learn models for dynamic 3D scenes purely from 2D visual observations. Our model combines Neural Radiance Fields (NeRF) and time contrastive learning with an autoencoding framework, which learns viewpoint-invariant 3D-aware scene representations. We show that a dynamics model, constructed over the learned representation space, enables visuomotor control for challenging manipulation tasks involving both rigid bodies and fluids, where the target is specified in a viewpoint different from what the robot operates on. When coupled with an auto-decoding framework, it can even support goal specification from camera viewpoints that are outside the training distribution. We further demonstrate the richness of the learned 3D dynamics model by performing future prediction and novel view synthesis. Finally, we provide detailed ablation studies regarding different system designs and qualitative analysis of the learned representations.



### Malware Classification Using Deep Boosted Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.04008v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.04008v1)
- **Published**: 2021-07-08 17:53:33+00:00
- **Updated**: 2021-07-08 17:53:33+00:00
- **Authors**: Muhammad Asam, Saddam Hussain Khan, Tauseef Jamal, Umme Zahoora, Asifullah Khan
- **Comment**: None
- **Journal**: None
- **Summary**: Malicious activities in cyberspace have gone further than simply hacking machines and spreading viruses. It has become a challenge for a nations survival and hence has evolved to cyber warfare. Malware is a key component of cyber-crime, and its analysis is the first line of defence against attack. This work proposes a novel deep boosted hybrid learning-based malware classification framework and named as Deep boosted Feature Space-based Malware classification (DFS-MC). In the proposed framework, the discrimination power is enhanced by fusing the feature spaces of the best performing customized CNN architectures models and its discrimination by an SVM for classification. The discrimination capacity of the proposed classification framework is assessed by comparing it against the standard customized CNNs. The customized CNN models are implemented in two ways: softmax classifier and deep hybrid learning-based malware classification. In the hybrid learning, Deep features are extracted from customized CNN architectures and fed into the conventional machine learning classifier to improve the classification performance. We also introduced the concept of transfer learning in a customized CNN architecture based malware classification framework through fine-tuning. The performance of the proposed malware classification approaches are validated on the MalImg malware dataset using the hold-out cross-validation technique. Experimental comparisons were conducted by employing innovative, customized CNN, trained from scratch and fine-tuning the customized CNN using transfer learning. The proposed classification framework DFS-MC showed improved results, Accuracy: 98.61%, F-score: 0.96, Precision: 0.96, and Recall: 0.96.



### Multi-Modality Task Cascade for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.04013v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.04013v1)
- **Published**: 2021-07-08 17:55:01+00:00
- **Updated**: 2021-07-08 17:55:01+00:00
- **Authors**: Jinhyung Park, Xinshuo Weng, Yunze Man, Kris Kitani
- **Comment**: None
- **Journal**: None
- **Summary**: Point clouds and RGB images are naturally complementary modalities for 3D visual understanding - the former provides sparse but accurate locations of points on objects, while the latter contains dense color and texture information. Despite this potential for close sensor fusion, many methods train two models in isolation and use simple feature concatenation to represent 3D sensor data. This separated training scheme results in potentially sub-optimal performance and prevents 3D tasks from being used to benefit 2D tasks that are often useful on their own. To provide a more integrated approach, we propose a novel Multi-Modality Task Cascade network (MTC-RCNN) that leverages 3D box proposals to improve 2D segmentation predictions, which are then used to further refine the 3D boxes. We show that including a 2D network between two stages of 3D modules significantly improves both 2D and 3D task performance. Moreover, to prevent the 3D module from over-relying on the overfitted 2D predictions, we propose a dual-head 2D segmentation training and inference scheme, allowing the 2nd 3D module to learn to interpret imperfect 2D segmentation predictions. Evaluating our model on the challenging SUN RGB-D dataset, we improve upon state-of-the-art results of both single modality and fusion networks by a large margin ($\textbf{+3.8}$ mAP@0.5). Code will be released $\href{https://github.com/Divadi/MTC_RCNN}{\text{here.}}$



### TGHop: An Explainable, Efficient and Lightweight Method for Texture Generation
- **Arxiv ID**: http://arxiv.org/abs/2107.04020v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04020v1)
- **Published**: 2021-07-08 17:56:58+00:00
- **Updated**: 2021-07-08 17:56:58+00:00
- **Authors**: Xuejing Lei, Ganning Zhao, Kaitai Zhang, C. -C. Jay Kuo
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2009.01376
- **Journal**: None
- **Summary**: An explainable, efficient and lightweight method for texture generation, called TGHop (an acronym of Texture Generation PixelHop), is proposed in this work. Although synthesis of visually pleasant texture can be achieved by deep neural networks, the associated models are large in size, difficult to explain in theory, and computationally expensive in training. In contrast, TGHop is small in its model size, mathematically transparent, efficient in training and inference, and able to generate high quality texture. Given an exemplary texture, TGHop first crops many sample patches out of it to form a collection of sample patches called the source. Then, it analyzes pixel statistics of samples from the source and obtains a sequence of fine-to-coarse subspaces for these patches by using the PixelHop++ framework. To generate texture patches with TGHop, we begin with the coarsest subspace, which is called the core, and attempt to generate samples in each subspace by following the distribution of real samples. Finally, texture patches are stitched to form texture images of a large size. It is demonstrated by experimental results that TGHop can generate texture images of superior quality with a small model size and at a fast speed.



### Adiabatic Quantum Graph Matching with Permutation Matrix Constraints
- **Arxiv ID**: http://arxiv.org/abs/2107.04032v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04032v1)
- **Published**: 2021-07-08 17:59:55+00:00
- **Updated**: 2021-07-08 17:59:55+00:00
- **Authors**: Marcel Seelbach Benkner, Vladislav Golyanik, Christian Theobalt, Michael Moeller
- **Comment**: 18 pages, 14 figures, 2 tables; project webpage:
  http://gvv.mpi-inf.mpg.de/projects/QGM/
- **Journal**: Published at 3DV 2020
- **Summary**: Matching problems on 3D shapes and images are challenging as they are frequently formulated as combinatorial quadratic assignment problems (QAPs) with permutation matrix constraints, which are NP-hard. In this work, we address such problems with emerging quantum computing technology and propose several reformulations of QAPs as unconstrained problems suitable for efficient execution on quantum hardware. We investigate several ways to inject permutation matrix constraints in a quadratic unconstrained binary optimization problem which can be mapped to quantum hardware. We focus on obtaining a sufficient spectral gap, which further increases the probability to measure optimal solutions and valid permutation matrices in a single run. We perform our experiments on the quantum computer D-Wave 2000Q (2^11 qubits, adiabatic). Despite the observed discrepancy between simulated adiabatic quantum computing and execution on real quantum hardware, our reformulation of permutation matrix constraints increases the robustness of the numerical computations over other penalty approaches in our experiments. The proposed algorithm has the potential to scale to higher dimensions on future quantum computing architectures, which opens up multiple new directions for solving matching problems in 3D computer vision and graphics.



### RMA: Rapid Motor Adaptation for Legged Robots
- **Arxiv ID**: http://arxiv.org/abs/2107.04034v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.04034v1)
- **Published**: 2021-07-08 17:59:59+00:00
- **Updated**: 2021-07-08 17:59:59+00:00
- **Authors**: Ashish Kumar, Zipeng Fu, Deepak Pathak, Jitendra Malik
- **Comment**: RSS 2021. Webpage at https://ashish-kmr.github.io/rma-legged-robots/
- **Journal**: None
- **Summary**: Successful real-world deployment of legged robots would require them to adapt in real-time to unseen scenarios like changing terrains, changing payloads, wear and tear. This paper presents Rapid Motor Adaptation (RMA) algorithm to solve this problem of real-time online adaptation in quadruped robots. RMA consists of two components: a base policy and an adaptation module. The combination of these components enables the robot to adapt to novel situations in fractions of a second. RMA is trained completely in simulation without using any domain knowledge like reference trajectories or predefined foot trajectory generators and is deployed on the A1 robot without any fine-tuning. We train RMA on a varied terrain generator using bioenergetics-inspired rewards and deploy it on a variety of difficult terrains including rocky, slippery, deformable surfaces in environments with grass, long vegetation, concrete, pebbles, stairs, sand, etc. RMA shows state-of-the-art performance across diverse real-world as well as simulation experiments. Video results at https://ashish-kmr.github.io/rma-legged-robots/



### 3D RegNet: Deep Learning Model for COVID-19 Diagnosis on Chest CT Image
- **Arxiv ID**: http://arxiv.org/abs/2107.04055v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.04055v1)
- **Published**: 2021-07-08 18:10:07+00:00
- **Updated**: 2021-07-08 18:10:07+00:00
- **Authors**: Haibo Qi, Yuhan Wang, Xinyu Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a 3D-RegNet-based neural network is proposed for diagnosing the physical condition of patients with coronavirus (Covid-19) infection. In the application of clinical medicine, lung CT images are utilized by practitioners to determine whether a patient is infected with coronavirus. However, there are some laybacks can be considered regarding to this diagnostic method, such as time consuming and low accuracy. As a relatively large organ of human body, important spatial features would be lost if the lungs were diagnosed utilizing two dimensional slice image. Therefore, in this paper, a deep learning model with 3D image was designed. The 3D image as input data was comprised of two-dimensional pulmonary image sequence and from which relevant coronavirus infection 3D features were extracted and classified. The results show that the test set of the 3D model, the result: f1 score of 0.8379 and AUC value of 0.8807 have been achieved.



### Comparison of 2D vs. 3D U-Net Organ Segmentation in abdominal 3D CT images
- **Arxiv ID**: http://arxiv.org/abs/2107.04062v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.04062v1)
- **Published**: 2021-07-08 18:35:15+00:00
- **Updated**: 2021-07-08 18:35:15+00:00
- **Authors**: Nico Zettler, Andre Mastmeyer
- **Comment**: 9 pages, 6 figure, 2 tables
- **Journal**: International Conference on Computer Graphics, Visualization and
  Computer Vision 2021 - WSCG
- **Summary**: A two-step concept for 3D segmentation on 5 abdominal organs inside volumetric CT images is presented. First each relevant organ's volume of interest is extracted as bounding box. The extracted volume acts as input for a second stage, wherein two compared U-Nets with different architectural dimensions re-construct an organ segmentation as label mask. In this work, we focus on comparing 2D U-Nets vs. 3D U-Net counterparts. Our initial results indicate Dice improvements of about 6\% at maximum. In this study to our surprise, liver and kidneys for instance were tackled significantly better using the faster and GPU-memory saving 2D U-Nets. For other abdominal key organs, there were no significant differences, but we observe highly significant advantages for the 2D U-Net in terms of GPU computational efforts for all organs under study.



### CASPIANET++: A Multidimensional Channel-Spatial Asymmetric Attention Network with Noisy Student Curriculum Learning Paradigm for Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.04099v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04099v1)
- **Published**: 2021-07-08 20:35:17+00:00
- **Updated**: 2021-07-08 20:35:17+00:00
- **Authors**: Andrea Liew, Chun Cheng Lee, Boon Leong Lan, Maxine Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have been used quite successfully for semantic segmentation of brain tumors. However, current CNNs and attention mechanisms are stochastic in nature and neglect the morphological indicators used by radiologists to manually annotate regions of interest. In this paper, we introduce a channel and spatial wise asymmetric attention (CASPIAN) by leveraging the inherent structure of tumors to detect regions of saliency. To demonstrate the efficacy of our proposed layer, we integrate this into a well-established convolutional neural network (CNN) architecture to achieve higher Dice scores, with less GPU resources. Also, we investigate the inclusion of auxiliary multiscale and multiplanar attention branches to increase the spatial context crucial in semantic segmentation tasks. The resulting architecture is the new CASPIANET++, which achieves Dice Scores of 91.19% whole tumor, 87.6% for tumor core and 81.03% for enhancing tumor. Furthermore, driven by the scarcity of brain tumor data, we investigate the Noisy Student method for segmentation tasks. Our new Noisy Student Curriculum Learning paradigm, which infuses noise incrementally to increase the complexity of the training images exposed to the network, further boosts the enhancing tumor region to 81.53%. Additional validation performed on the BraTS2020 data shows that the Noisy Student Curriculum Learning method works well without any additional training or finetuning.



### Multitask Multi-database Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.04127v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.04127v2)
- **Published**: 2021-07-08 21:57:58+00:00
- **Updated**: 2021-07-12 15:36:55+00:00
- **Authors**: Manh Tu Vu, Marie Beurton-Aimar
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we introduce our submission to the 2nd Affective Behavior Analysis in-the-wild (ABAW) 2021 competition. We train a unified deep learning model on multi-databases to perform two tasks: seven basic facial expressions prediction and valence-arousal estimation. Since these databases do not contains labels for all the two tasks, we have applied the distillation knowledge technique to train two networks: one teacher and one student model. The student model will be trained using both ground truth labels and soft labels derived from the pretrained teacher model. During the training, we add one more task, which is the combination of the two mentioned tasks, for better exploiting inter-task correlations. We also exploit the sharing videos between the two tasks of the AffWild2 database that is used in the competition, to further improve the performance of the network. Experiment results shows that the network have achieved promising results on the validation set of the AffWild2 database. Code and pretrained model are publicly available at https://github.com/glmanhtu/multitask-abaw-2021



### Effectiveness of State-of-the-Art Super Resolution Algorithms in Surveillance Environment
- **Arxiv ID**: http://arxiv.org/abs/2107.04133v1
- **DOI**: 10.1007/978-3-030-74728-2_8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04133v1)
- **Published**: 2021-07-08 22:28:48+00:00
- **Updated**: 2021-07-08 22:28:48+00:00
- **Authors**: Muhammad Ali Farooq, Ammar Ali Khan, Ansar Ahmad, Rana Hammad Raza
- **Comment**: None
- **Journal**: Springer, Part of the Advances in Intelligent Systems and
  Computing book series (AISC, volume 1376), 2021
- **Summary**: Image Super Resolution (SR) finds applications in areas where images need to be closely inspected by the observer to extract enhanced information. One such focused application is an offline forensic analysis of surveillance feeds. Due to the limitations of camera hardware, camera pose, limited bandwidth, varying illumination conditions, and occlusions, the quality of the surveillance feed is significantly degraded at times, thereby compromising monitoring of behavior, activities, and other sporadic information in the scene. For the proposed research work, we have inspected the effectiveness of four conventional yet effective SR algorithms and three deep learning-based SR algorithms to seek the finest method that executes well in a surveillance environment with limited training data op-tions. These algorithms generate an enhanced resolution output image from a sin-gle low-resolution (LR) input image. For performance analysis, a subset of 220 images from six surveillance datasets has been used, consisting of individuals with varying distances from the camera, changing illumination conditions, and complex backgrounds. The performance of these algorithms has been evaluated and compared using both qualitative and quantitative metrics. These SR algo-rithms have also been compared based on face detection accuracy. By analyzing and comparing the performance of all the algorithms, a Convolutional Neural Network (CNN) based SR technique using an external dictionary proved to be best by achieving robust face detection accuracy and scoring optimal quantitative metric results under different surveillance conditions. This is because the CNN layers progressively learn more complex features using an external dictionary.



### Does Form Follow Function? An Empirical Exploration of the Impact of Deep Neural Network Architecture Design on Hardware-Specific Acceleration
- **Arxiv ID**: http://arxiv.org/abs/2107.04144v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04144v1)
- **Published**: 2021-07-08 23:05:39+00:00
- **Updated**: 2021-07-08 23:05:39+00:00
- **Authors**: Saad Abbasi, Mohammad Javad Shafiee, Ellick Chan, Alexander Wong
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: The fine-grained relationship between form and function with respect to deep neural network architecture design and hardware-specific acceleration is one area that is not well studied in the research literature, with form often dictated by accuracy as opposed to hardware function. In this study, a comprehensive empirical exploration is conducted to investigate the impact of deep neural network architecture design on the degree of inference speedup that can be achieved via hardware-specific acceleration. More specifically, we empirically study the impact of a variety of commonly used macro-architecture design patterns across different architectural depths through the lens of OpenVINO microprocessor-specific and GPU-specific acceleration. Experimental results showed that while leveraging hardware-specific acceleration achieved an average inference speed-up of 380%, the degree of inference speed-up varied drastically depending on the macro-architecture design pattern, with the greatest speedup achieved on the depthwise bottleneck convolution design pattern at 550%. Furthermore, we conduct an in-depth exploration of the correlation between FLOPs requirement, level 3 cache efficacy, and network latency with increasing architectural depth and width. Finally, we analyze the inference time reductions using hardware-specific acceleration when compared to native deep learning frameworks across a wide variety of hand-crafted deep convolutional neural network architecture designs as well as ones found via neural architecture search strategies. We found that the DARTS-derived architecture to benefit from the greatest improvement from hardware-specific software acceleration (1200%) while the depthwise bottleneck convolution-based MobileNet-V2 to have the lowest overall inference time of around 2.4 ms.



