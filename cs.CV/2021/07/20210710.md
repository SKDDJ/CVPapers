# Arxiv Papers in cs.CV on 2021-07-10
### Using Causal Analysis for Conceptual Deep Learning Explanation
- **Arxiv ID**: http://arxiv.org/abs/2107.06098v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.06098v1)
- **Published**: 2021-07-10 00:01:45+00:00
- **Updated**: 2021-07-10 00:01:45+00:00
- **Authors**: Sumedha Singla, Stephen Wallace, Sofia Triantafillou, Kayhan Batmanghelich
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Model explainability is essential for the creation of trustworthy Machine Learning models in healthcare. An ideal explanation resembles the decision-making process of a domain expert and is expressed using concepts or terminology that is meaningful to the clinicians. To provide such an explanation, we first associate the hidden units of the classifier to clinically relevant concepts. We take advantage of radiology reports accompanying the chest X-ray images to define concepts. We discover sparse associations between concepts and hidden units using a linear sparse logistic regression. To ensure that the identified units truly influence the classifier's outcome, we adopt tools from Causal Inference literature and, more specifically, mediation analysis through counterfactual interventions. Finally, we construct a low-depth decision tree to translate all the discovered concepts into a straightforward decision rule, expressed to the radiologist. We evaluated our approach on a large chest x-ray dataset, where our model produces a global explanation consistent with clinical knowledge.



### Local-to-Global Self-Attention in Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2107.04735v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04735v1)
- **Published**: 2021-07-10 02:34:55+00:00
- **Updated**: 2021-07-10 02:34:55+00:00
- **Authors**: Jinpeng Li, Yichao Yan, Shengcai Liao, Xiaokang Yang, Ling Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Transformers have demonstrated great potential in computer vision tasks. To avoid dense computations of self-attentions in high-resolution visual data, some recent Transformer models adopt a hierarchical design, where self-attentions are only computed within local windows. This design significantly improves the efficiency but lacks global feature reasoning in early stages. In this work, we design a multi-path structure of the Transformer, which enables local-to-global reasoning at multiple granularities in each stage. The proposed framework is computationally efficient and highly effective. With a marginal increasement in computational overhead, our model achieves notable improvements in both image classification and semantic segmentation. Code is available at https://github.com/ljpadam/LG-Transformer



### Consensual Collaborative Training And Knowledge Distillation Based Facial Expression Recognition Under Noisy Annotations
- **Arxiv ID**: http://arxiv.org/abs/2107.04746v1
- **DOI**: 10.14445/22315381/IJETT-V69I7P231
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04746v1)
- **Published**: 2021-07-10 03:37:06+00:00
- **Updated**: 2021-07-10 03:37:06+00:00
- **Authors**: Darshan Gera, S. Balasubramanian
- **Comment**: 11 pages, 6 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT), Codes:
  https://github.com/1980x/CCT
- **Journal**: International Journal of Engineering Trends and Technology
  69.7(2021):244-254
- **Summary**: Presence of noise in the labels of large scale facial expression datasets has been a key challenge towards Facial Expression Recognition (FER) in the wild. During early learning stage, deep networks fit on clean data. Then, eventually, they start overfitting on noisy labels due to their memorization ability, which limits FER performance. This work proposes an effective training strategy in the presence of noisy labels, called as Consensual Collaborative Training (CCT) framework. CCT co-trains three networks jointly using a convex combination of supervision loss and consistency loss, without making any assumption about the noise distribution. A dynamic transition mechanism is used to move from supervision loss in early learning to consistency loss for consensus of predictions among networks in the later stage. Inference is done using a single network based on a simple knowledge distillation scheme. Effectiveness of the proposed framework is demonstrated on synthetic as well as real noisy FER datasets. In addition, a large test subset of around 5K images is annotated from the FEC dataset using crowd wisdom of 16 different annotators and reliable labels are inferred. CCT is also validated on it. State-of-the-art performance is reported on the benchmark FER datasets RAFDB (90.84%) FERPlus (89.99%) and AffectNet (66%). Our codes are available at https://github.com/1980x/CCT.



### Resilience of Autonomous Vehicle Object Category Detection to Universal Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2107.04749v1
- **DOI**: 10.1109/IEMTRONICS52119.2021.9422616
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04749v1)
- **Published**: 2021-07-10 03:40:25+00:00
- **Updated**: 2021-07-10 03:40:25+00:00
- **Authors**: Mohammad Nayeem Teli, Seungwon Oh
- **Comment**: None
- **Journal**: 2021 IEEE International IOT, Electronics and Mechatronics
  Conference (IEMTRONICS), 2021, pp. 1-6
- **Summary**: Due to the vulnerability of deep neural networks to adversarial examples, numerous works on adversarial attacks and defenses have been burgeoning over the past several years. However, there seem to be some conventional views regarding adversarial attacks and object detection approaches that most researchers take for granted. In this work, we bring a fresh perspective on those procedures by evaluating the impact of universal perturbations on object detection at a class-level. We apply it to a carefully curated data set related to autonomous driving. We use Faster-RCNN object detector on images of five different categories: person, car, truck, stop sign and traffic light from the COCO data set, while carefully perturbing the images using Universal Dense Object Suppression algorithm. Our results indicate that person, car, traffic light, truck and stop sign are resilient in that order (most to least) to universal perturbations. To the best of our knowledge, this is the first time such a ranking has been established which is significant for the security of the data sets pertaining to autonomous vehicles and object detection in general.



### Anomaly Detection in Residential Video Surveillance on Edge Devices in IoT Framework
- **Arxiv ID**: http://arxiv.org/abs/2107.04767v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.04767v2)
- **Published**: 2021-07-10 05:52:15+00:00
- **Updated**: 2021-08-09 09:24:00+00:00
- **Authors**: Mayur R. Parate, Kishor M. Bhurchandi, Ashwin G. Kothari
- **Comment**: 7 Pages, 7 Figures and 3 Tables
- **Journal**: None
- **Summary**: Intelligent resident surveillance is one of the most essential smart community services. The increasing demand for security needs surveillance systems to be able to detect anomalies in surveillance scenes. Employing high-capacity computational devices for intelligent surveillance in residential societies is costly and not feasible. Therefore, we propose anomaly detection for intelligent surveillance using CPU-only edge devices. A modular framework to capture object-level inferences and tracking is developed. To cope with partial occlusions, posture deformations, and complex scenes, we employed feature encoding and trajectory association governed by two metrices complementing to each other. The elements of an anomaly detection framework are optimized to run on CPU-only edge devices with sufficient frames per second (FPS). The experimental results indicate the proposed method is feasible and achieves satisfactory results in real-life scenarios.



### DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2107.04768v1
- **DOI**: 10.1109/TMM.2021.3097171
- **Categories**: **cs.MM**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04768v1)
- **Published**: 2021-07-10 06:08:15+00:00
- **Updated**: 2021-07-10 06:08:15+00:00
- **Authors**: Jianyu Wang, Bing-Kun Bao, Changsheng Xu
- **Comment**: 12 pages, 12 figures
- **Journal**: IEEE Transactions on Multimedia 2021
- **Summary**: Video question answering is a challenging task, which requires agents to be able to understand rich video contents and perform spatial-temporal reasoning. However, existing graph-based methods fail to perform multi-step reasoning well, neglecting two properties of VideoQA: (1) Even for the same video, different questions may require different amount of video clips or objects to infer the answer with relational reasoning; (2) During reasoning, appearance and motion features have complicated interdependence which are correlated and complementary to each other. Based on these observations, we propose a Dual-Visual Graph Reasoning Unit (DualVGR) which reasons over videos in an end-to-end fashion. The first contribution of our DualVGR is the design of an explainable Query Punishment Module, which can filter out irrelevant visual features through multiple cycles of reasoning. The second contribution is the proposed Video-based Multi-view Graph Attention Network, which captures the relations between appearance and motion features. Our DualVGR network achieves state-of-the-art performance on the benchmark MSVD-QA and SVQA datasets, and demonstrates competitive results on benchmark MSRVTT-QA datasets. Our code is available at https://github.com/MMIR/DualVGR-VideoQA.



### TA2N: Two-Stage Action Alignment Network for Few-shot Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.04782v4
- **DOI**: 10.1609/aaai.v36i2.20029
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04782v4)
- **Published**: 2021-07-10 07:22:49+00:00
- **Updated**: 2022-12-22 08:40:02+00:00
- **Authors**: Shuyuan Li, Huabin Liu, Rui Qian, Yuxi Li, John See, Mengjuan Fei, Xiaoyuan Yu, Weiyao Lin
- **Comment**: Published in AAAI 2022
- **Journal**: None
- **Summary**: Few-shot action recognition aims to recognize novel action classes (query) using just a few samples (support). The majority of current approaches follow the metric learning paradigm, which learns to compare the similarity between videos. Recently, it has been observed that directly measuring this similarity is not ideal since different action instances may show distinctive temporal distribution, resulting in severe misalignment issues across query and support videos. In this paper, we arrest this problem from two distinct aspects -- action duration misalignment and action evolution misalignment. We address them sequentially through a Two-stage Action Alignment Network (TA2N). The first stage locates the action by learning a temporal affine transform, which warps each video feature to its action duration while dismissing the action-irrelevant feature (e.g. background). Next, the second stage coordinates query feature to match the spatial-temporal action evolution of support by performing temporally rearrange and spatially offset prediction. Extensive experiments on benchmark datasets show the potential of the proposed method in achieving state-of-the-art performance for few-shot action recognition.The code of this project can be found at https://github.com/R00Kie-Liu/TA2N



### Merging Tasks for Video Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2108.04223v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.04223v1)
- **Published**: 2021-07-10 08:46:42+00:00
- **Updated**: 2021-07-10 08:46:42+00:00
- **Authors**: Jake Rap, Panagiotis Meletis
- **Comment**: Bachelor Thesis
- **Journal**: None
- **Summary**: In this paper, the task of video panoptic segmentation is studied and two different methods to solve the task will be proposed. Video panoptic segmentation (VPS) is a recently introduced computer vision task that requires classifying and tracking every pixel in a given video. The nature of this task makes the cost of annotating datasets for it prohibiting. To understand video panoptic segmentation, first, earlier introduced constituent tasks that focus on semantics and tracking separately will be researched. Thereafter, two data-driven approaches which do not require training on a tailored VPS dataset will be selected to solve it. The first approach will show how a model for video panoptic segmentation can be built by heuristically fusing the outputs of a pre-trained semantic segmentation model and a pre-trained multi-object tracking model. This can be desired if one wants to easily extend the capabilities of either model. The second approach will counter some of the shortcomings of the first approach by building on top of a shared neural network backbone with task-specific heads. This network is designed for panoptic segmentation and will be extended by a mask propagation module to link instance masks across time, yielding the video panoptic segmentation format.



### Semi-Supervised Learning with Multi-Head Co-Training
- **Arxiv ID**: http://arxiv.org/abs/2107.04795v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04795v2)
- **Published**: 2021-07-10 08:53:14+00:00
- **Updated**: 2021-09-13 02:19:10+00:00
- **Authors**: Mingcai Chen, Yuntao Du, Yi Zhang, Shuwei Qian, Chongjun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Co-training, extended from self-training, is one of the frameworks for semi-supervised learning. Without natural split of features, single-view co-training works at the cost of training extra classifiers, where the algorithm should be delicately designed to prevent individual classifiers from collapsing into each other. To remove these obstacles which deter the adoption of single-view co-training, we present a simple and efficient algorithm Multi-Head Co-Training. By integrating base learners into a multi-head structure, the model is in a minimal amount of extra parameters. Every classification head in the unified model interacts with its peers through a "Weak and Strong Augmentation" strategy, in which the diversity is naturally brought by the strong data augmentation. Therefore, the proposed method facilitates single-view co-training by 1). promoting diversity implicitly and 2). only requiring a small extra computational overhead. The effectiveness of Multi-Head Co-Training is demonstrated in an empirical study on standard semi-supervised learning benchmarks.



### Few-Shot Domain Adaptation with Polymorphic Transformers
- **Arxiv ID**: http://arxiv.org/abs/2107.04805v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04805v1)
- **Published**: 2021-07-10 10:08:57+00:00
- **Updated**: 2021-07-10 10:08:57+00:00
- **Authors**: Shaohua Li, Xiuchao Sui, Jie Fu, Huazhu Fu, Xiangde Luo, Yangqin Feng, Xinxing Xu, Yong Liu, Daniel Ting, Rick Siow Mong Goh
- **Comment**: MICCAI'2021 camera ready
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) trained on one set of medical images often experience severe performance drop on unseen test images, due to various domain discrepancy between the training images (source domain) and the test images (target domain), which raises a domain adaptation issue. In clinical settings, it is difficult to collect enough annotated target domain data in a short period. Few-shot domain adaptation, i.e., adapting a trained model with a handful of annotations, is highly practical and useful in this case. In this paper, we propose a Polymorphic Transformer (Polyformer), which can be incorporated into any DNN backbones for few-shot domain adaptation. Specifically, after the polyformer layer is inserted into a model trained on the source domain, it extracts a set of prototype embeddings, which can be viewed as a "basis" of the source-domain features. On the target domain, the polyformer layer adapts by only updating a projection layer which controls the interactions between image features and the prototype embeddings. All other model weights (except BatchNorm parameters) are frozen during adaptation. Thus, the chance of overfitting the annotations is greatly reduced, and the model can perform robustly on the target domain after being trained on a few annotated images. We demonstrate the effectiveness of Polyformer on two medical segmentation tasks (i.e., optic disc/cup segmentation, and polyp segmentation). The source code of Polyformer is released at https://github.com/askerlee/segtran.



### Speech2Video: Cross-Modal Distillation for Speech to Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2107.04806v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04806v1)
- **Published**: 2021-07-10 10:27:26+00:00
- **Updated**: 2021-07-10 10:27:26+00:00
- **Authors**: Shijing Si, Jianzong Wang, Xiaoyang Qu, Ning Cheng, Wenqi Wei, Xinghua Zhu, Jing Xiao
- **Comment**: Accepted by InterSpeech2021
- **Journal**: None
- **Summary**: This paper investigates a novel task of talking face video generation solely from speeches. The speech-to-video generation technique can spark interesting applications in entertainment, customer service, and human-computer-interaction industries. Indeed, the timbre, accent and speed in speeches could contain rich information relevant to speakers' appearance. The challenge mainly lies in disentangling the distinct visual attributes from audio signals. In this article, we propose a light-weight, cross-modal distillation method to extract disentangled emotional and identity information from unlabelled video inputs. The extracted features are then integrated by a generative adversarial network into talking face video clips. With carefully crafted discriminators, the proposed framework achieves realistic generation results. Experiments with observed individuals demonstrated that the proposed framework captures the emotional expressions solely from speeches, and produces spontaneous facial motion in the video output. Compared to the baseline method where speeches are combined with a static image of the speaker, the results of the proposed framework is almost indistinguishable. User studies also show that the proposed method outperforms the existing algorithms in terms of emotion expression in the generated videos.



### COVID Detection in Chest CTs: Improving the Baseline on COV19-CT-DB
- **Arxiv ID**: http://arxiv.org/abs/2107.04808v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04808v2)
- **Published**: 2021-07-10 10:39:18+00:00
- **Updated**: 2021-07-13 08:45:22+00:00
- **Authors**: Radu Miron, Cosmin Moisii, Sergiu Dinu, Mihaela Breaban
- **Comment**: None
- **Journal**: None
- **Summary**: The paper presents a comparative analysis of three distinct approaches based on deep learning for COVID-19 detection in chest CTs. The first approach is a volumetric one, involving 3D convolutions, while the other two approaches perform at first slice-wise classification and then aggregate the results at the volume level. The experiments are carried on the COV19-CT-DB dataset, with the aim of addressing the challenge raised by the MIA-COV19D Competition within ICCV 2021. Our best results on the validation subset reach a macro-F1 score of 0.92, which improves considerably the baseline score of 0.70 set by the organizers.



### Not End-to-End: Explore Multi-Stage Architecture for Online Surgical Phase Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.04810v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.04810v1)
- **Published**: 2021-07-10 11:00:38+00:00
- **Updated**: 2021-07-10 11:00:38+00:00
- **Authors**: Fangqiu Yi, Tingting Jiang
- **Comment**: Not accepted by M2CAI2021
- **Journal**: None
- **Summary**: Surgical phase recognition is of particular interest to computer assisted surgery systems, in which the goal is to predict what phase is occurring at each frame for a surgery video. Networks with multi-stage architecture have been widely applied in many computer vision tasks with rich patterns, where a predictor stage first outputs initial predictions and an additional refinement stage operates on the initial predictions to perform further refinement. Existing works show that surgical video contents are well ordered and contain rich temporal patterns, making the multi-stage architecture well suited for the surgical phase recognition task. However, we observe that when simply applying the multi-stage architecture to the surgical phase recognition task, the end-to-end training manner will make the refinement ability fall short of its wishes. To address the problem, we propose a new non end-to-end training strategy and explore different designs of multi-stage architecture for surgical phase recognition task. For the non end-to-end training strategy, the refinement stage is trained separately with proposed two types of disturbed sequences. Meanwhile, we evaluate three different choices of refinement models to show that our analysis and solution are robust to the choices of specific multi-stage models. We conduct experiments on two public benchmarks, the M2CAI16 Workflow Challenge, and the Cholec80 dataset. Results show that multi-stage architecture trained with our strategy largely boosts the performance of the current state-of-the-art single-stage model. Code is available at \url{https://github.com/ChinaYi/casual_tcn}.



### Detection of Plant Leaf Disease Directly in the JPEG Compressed Domain using Transfer Learning Technique
- **Arxiv ID**: http://arxiv.org/abs/2107.04813v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04813v1)
- **Published**: 2021-07-10 11:10:28+00:00
- **Updated**: 2021-07-10 11:10:28+00:00
- **Authors**: Atul Sharma, Bulla Rajesh, Mohammed Javed
- **Comment**: Accepted in MISP 2021 3rd International Conference On Machine
  Intelligence And Signal Processing
- **Journal**: None
- **Summary**: Plant leaf diseases pose a significant danger to food security and they cause depletion in quality and volume of production. Therefore accurate and timely detection of leaf disease is very important to check the loss of the crops and meet the growing food demand of the people. Conventional techniques depend on lab investigation and human skills which are generally costly and inaccessible. Recently, Deep Neural Networks have been exceptionally fruitful in image classification. In this research paper, plant leaf disease detection employing transfer learning is explored in the JPEG compressed domain. Here, the JPEG compressed stream consisting of DCT coefficients is, directly fed into the Neural Network to improve the efficiency of classification. The experimental results on JPEG compressed leaf dataset demonstrate the efficacy of the proposed model.



### A Weakly-Supervised Depth Estimation Network Using Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2107.04819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04819v1)
- **Published**: 2021-07-10 11:40:05+00:00
- **Updated**: 2021-07-10 11:40:05+00:00
- **Authors**: Fang Gao, Jiabao Wang, Jun Yu, Yaoxiong Wang, Feng Shuang
- **Comment**: 8 pages, 8 figures, IJCAI-WSRL
- **Journal**: None
- **Summary**: Monocular depth estimation (MDE) is a fundamental task in many applications such as scene understanding and reconstruction. However, most of the existing methods rely on accurately labeled datasets. A weakly-supervised framework based on attention nested U-net (ANU) named as ANUW is introduced in this paper for cases with wrong labels. The ANUW is trained end-to-end to convert an input single RGB image into a depth image. It consists of a dense residual network structure, an adaptive weight channel attention (AWCA) module, a patch second non-local (PSNL) module and a soft label generation method. The dense residual network is the main body of the network to encode and decode the input. The AWCA module can adaptively adjust the channel weights to extract important features. The PSNL module implements the spatial attention mechanism through a second-order non-local method. The proposed soft label generation method uses the prior knowledge of the dataset to produce soft labels to replace false ones. The proposed ANUW is trained on a defective monocular depth dataset and the trained model is tested on three public datasets, and the results demonstrate the superiority of ANUW in comparison with the state-of-the-art MDE methods.



### BSDA-Net: A Boundary Shape and Distance Aware Joint Learning Framework for Segmenting and Classifying OCTA Images
- **Arxiv ID**: http://arxiv.org/abs/2107.04823v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04823v2)
- **Published**: 2021-07-10 12:16:23+00:00
- **Updated**: 2021-07-13 15:34:19+00:00
- **Authors**: Li Lin, Zhonghua Wang, Jiewei Wu, Yijin Huang, Junyan Lyu, Pujin Cheng, Jiong Wu, Xiaoying Tang
- **Comment**: 12 pages, 4 figures, MICCAI2021 [Student Travel Award]
- **Journal**: None
- **Summary**: Optical coherence tomography angiography (OCTA) is a novel non-invasive imaging technique that allows visualizations of vasculature and foveal avascular zone (FAZ) across retinal layers. Clinical researches suggest that the morphology and contour irregularity of FAZ are important biomarkers of various ocular pathologies. Therefore, precise segmentation of FAZ has great clinical interest. Also, there is no existing research reporting that FAZ features can improve the performance of deep diagnostic classification networks. In this paper, we propose a novel multi-level boundary shape and distance aware joint learning framework, named BSDA-Net, for FAZ segmentation and diagnostic classification from OCTA images. Two auxiliary branches, namely boundary heatmap regression and signed distance map reconstruction branches, are constructed in addition to the segmentation branch to improve the segmentation performance, resulting in more accurate FAZ contours and fewer outliers. Moreover, both low-level and high-level features from the aforementioned three branches, including shape, size, boundary, and signed directional distance map of FAZ, are fused hierarchically with features from the diagnostic classifier. Through extensive experiments, the proposed BSDA-Net is found to yield state-of-the-art segmentation and classification results on the OCTA-500, OCTAGON, and FAZID datasets.



### Identifying Layers Susceptible to Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2107.04827v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04827v2)
- **Published**: 2021-07-10 12:38:49+00:00
- **Updated**: 2021-10-29 00:26:34+00:00
- **Authors**: Shoaib Ahmed Siddiqui, Thomas Breuel
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we investigate the use of pretraining with adversarial networks, with the objective of discovering the relationship between network depth and robustness. For this purpose, we selectively retrain different portions of VGG and ResNet architectures on CIFAR-10, Imagenette, and ImageNet using non-adversarial and adversarial data. Experimental results show that susceptibility to adversarial samples is associated with low-level feature extraction layers. Therefore, retraining of high-level layers is insufficient for achieving robustness. Furthermore, adversarial attacks yield outputs from early layers that differ statistically from features for non-adversarial samples and do not permit consistent classification by subsequent layers. This supports common hypotheses regarding the association of robustness with the feature extractor, insufficiency of deeper layers in providing robustness, and large differences in adversarial and non-adversarial feature vectors.



### CSL-YOLO: A New Lightweight Object Detection System for Edge Computing
- **Arxiv ID**: http://arxiv.org/abs/2107.04829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04829v1)
- **Published**: 2021-07-10 12:56:46+00:00
- **Updated**: 2021-07-10 12:56:46+00:00
- **Authors**: Yu-Ming Zhang, Chun-Chieh Lee, Jun-Wei Hsieh, Kuo-Chin Fan
- **Comment**: None
- **Journal**: None
- **Summary**: The development of lightweight object detectors is essential due to the limited computation resources. To reduce the computation cost, how to generate redundant features plays a significant role. This paper proposes a new lightweight Convolution method Cross-Stage Lightweight (CSL) Module, to generate redundant features from cheap operations. In the intermediate expansion stage, we replaced Pointwise Convolution with Depthwise Convolution to produce candidate features. The proposed CSL-Module can reduce the computation cost significantly. Experiments conducted at MS-COCO show that the proposed CSL-Module can approximate the fitting ability of Convolution-3x3. Finally, we use the module to construct a lightweight detector CSL-YOLO, achieving better detection performance with only 43% FLOPs and 52% parameters than Tiny-YOLOv4.



### Bayesian Convolutional Neural Networks for Seven Basic Facial Expression Classifications
- **Arxiv ID**: http://arxiv.org/abs/2107.04834v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04834v2)
- **Published**: 2021-07-10 13:09:13+00:00
- **Updated**: 2021-07-13 13:05:36+00:00
- **Authors**: Yuan Tai, Yihua Tan, Wei Gong, Hailan Huang
- **Comment**: None
- **Journal**: None
- **Summary**: The seven basic facial expression classifications are a basic way to express complex human emotions and are an important part of artificial intelligence research. Based on the traditional Bayesian neural network framework, the ResNet18_BNN network constructed in this paper has been improved in the following three aspects: (1) A new objective function is proposed, which is composed of the KL loss of uncertain parameters and the intersection of specific parameters. Entropy loss composition. (2) Aiming at a special objective function, a training scheme for alternately updating these two parameters is proposed. (3) Only model the parameters of the last convolution group. Through testing on the FER2013 test set, we achieved 71.5% and 73.1% accuracy in PublicTestSet and PrivateTestSet, respectively. Compared with traditional Bayesian neural networks, our method brings the highest classification accuracy gain.



### Weaving Attention U-net: A Novel Hybrid CNN and Attention-based Method for Organs-at-risk Segmentation in Head and Neck CT Images
- **Arxiv ID**: http://arxiv.org/abs/2107.04847v2
- **DOI**: 10.1002/mp.15287
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04847v2)
- **Published**: 2021-07-10 14:27:46+00:00
- **Updated**: 2021-09-22 22:58:11+00:00
- **Authors**: Zhuangzhuang Zhang, Tianyu Zhao, Hiram Gay, Weixiong Zhang, Baozhou Sun
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: In radiotherapy planning, manual contouring is labor-intensive and time-consuming. Accurate and robust automated segmentation models improve the efficiency and treatment outcome. We aim to develop a novel hybrid deep learning approach, combining convolutional neural networks (CNNs) and the self-attention mechanism, for rapid and accurate multi-organ segmentation on head and neck computed tomography (CT) images. Head and neck CT images with manual contours of 115 patients were retrospectively collected and used. We set the training/validation/testing ratio to 81/9/25 and used the 10-fold cross-validation strategy to select the best model parameters. The proposed hybrid model segmented ten organs-at-risk (OARs) altogether for each case. The performance of the model was evaluated by three metrics, i.e., the Dice Similarity Coefficient (DSC), Hausdorff distance 95% (HD95), and mean surface distance (MSD). We also tested the performance of the model on the Head and Neck 2015 challenge dataset and compared it against several state-of-the-art automated segmentation algorithms. The proposed method generated contours that closely resemble the ground truth for ten OARs. Our results of the new Weaving Attention U-net demonstrate superior or similar performance on the segmentation of head and neck CT images.



### SynPick: A Dataset for Dynamic Bin Picking Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/2107.04852v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04852v1)
- **Published**: 2021-07-10 14:58:43+00:00
- **Updated**: 2021-07-10 14:58:43+00:00
- **Authors**: Arul Selvam Periyasamy, Max Schwarz, Sven Behnke
- **Comment**: Accepted for 17th IEEE International Conference on Automation Science
  and Engineering (CASE), Lyon, France, August 2021
- **Journal**: None
- **Summary**: We present SynPick, a synthetic dataset for dynamic scene understanding in bin-picking scenarios. In contrast to existing datasets, our dataset is both situated in a realistic industrial application domain -- inspired by the well-known Amazon Robotics Challenge (ARC) -- and features dynamic scenes with authentic picking actions as chosen by our picking heuristic developed for the ARC 2017. The dataset is compatible with the popular BOP dataset format. We describe the dataset generation process in detail, including object arrangement generation and manipulation simulation using the NVIDIA PhysX physics engine. To cover a large action space, we perform untargeted and targeted picking actions, as well as random moving actions. To establish a baseline for object perception, a state-of-the-art pose estimation approach is evaluated on the dataset. We demonstrate the usefulness of tracking poses during manipulation instead of single-shot estimation even with a naive filtering approach. The generator source code and dataset are publicly available.



### Learning 3D Dense Correspondence via Canonical Point Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2107.04867v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04867v1)
- **Published**: 2021-07-10 15:54:48+00:00
- **Updated**: 2021-07-10 15:54:48+00:00
- **Authors**: An-Chieh Cheng, Xueting Li, Min Sun, Ming-Hsuan Yang, Sifei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a canonical point autoencoder (CPAE) that predicts dense correspondences between 3D shapes of the same category. The autoencoder performs two key functions: (a) encoding an arbitrarily ordered point cloud to a canonical primitive, e.g., a sphere, and (b) decoding the primitive back to the original input instance shape. As being placed in the bottleneck, this primitive plays a key role to map all the unordered point clouds on the canonical surface and to be reconstructed in an ordered fashion. Once trained, points from different shape instances that are mapped to the same locations on the primitive surface are determined to be a pair of correspondence. Our method does not require any form of annotation or self-supervised part segmentation network and can handle unaligned input point clouds. Experimental results on 3D semantic keypoint transfer and part segmentation transfer show that our model performs favorably against state-of-the-art correspondence learning methods.



### Out of Distribution Detection and Adversarial Attacks on Deep Neural Networks for Robust Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2107.04882v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04882v1)
- **Published**: 2021-07-10 18:00:40+00:00
- **Updated**: 2021-07-10 18:00:40+00:00
- **Authors**: Anisie Uwimana1, Ransalu Senanayake
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models have become a popular choice for medical image analysis. However, the poor generalization performance of deep learning models limits them from being deployed in the real world as robustness is critical for medical applications. For instance, the state-of-the-art Convolutional Neural Networks (CNNs) fail to detect adversarial samples or samples drawn statistically far away from the training distribution. In this work, we experimentally evaluate the robustness of a Mahalanobis distance-based confidence score, a simple yet effective method for detecting abnormal input samples, in classifying malaria parasitized cells and uninfected cells. Results indicated that the Mahalanobis confidence score detector exhibits improved performance and robustness of deep learning models, and achieves stateof-the-art performance on both out-of-distribution (OOD) and adversarial samples.



### Hierarchical Self-Supervised Learning for Medical Image Segmentation Based on Multi-Domain Data Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2107.04886v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04886v1)
- **Published**: 2021-07-10 18:17:57+00:00
- **Updated**: 2021-07-10 18:17:57+00:00
- **Authors**: Hao Zheng, Jun Han, Hongxiao Wang, Lin Yang, Zhuo Zhao, Chaoli Wang, Danny Z. Chen
- **Comment**: Accepted to MICCAI 2021
- **Journal**: None
- **Summary**: A large labeled dataset is a key to the success of supervised deep learning, but for medical image segmentation, it is highly challenging to obtain sufficient annotated images for model training. In many scenarios, unannotated images are abundant and easy to acquire. Self-supervised learning (SSL) has shown great potentials in exploiting raw data information and representation learning. In this paper, we propose Hierarchical Self-Supervised Learning (HSSL), a new self-supervised framework that boosts medical image segmentation by making good use of unannotated data. Unlike the current literature on task-specific self-supervised pretraining followed by supervised fine-tuning, we utilize SSL to learn task-agnostic knowledge from heterogeneous data for various medical image segmentation tasks. Specifically, we first aggregate a dataset from several medical challenges, then pre-train the network in a self-supervised manner, and finally fine-tune on labeled data. We develop a new loss function by combining contrastive loss and classification loss and pretrain an encoder-decoder architecture for segmentation tasks. Our extensive experiments show that multi-domain joint pre-training benefits downstream segmentation tasks and outperforms single-domain pre-training significantly. Compared to learning from scratch, our new method yields better performance on various tasks (e.g., +0.69% to +18.60% in Dice scores with 5% of annotated data). With limited amounts of training data, our method can substantially bridge the performance gap w.r.t. denser annotations (e.g., 10% vs.~100% of annotated data).



### Industry and Academic Research in Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2107.04902v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04902v3)
- **Published**: 2021-07-10 20:09:52+00:00
- **Updated**: 2021-07-17 23:30:52+00:00
- **Authors**: Iuliia Kotseruba, Manos Papagelis, John K. Tsotsos
- **Comment**: 8 pages, 9 Figures, 2 Tables
- **Journal**: None
- **Summary**: This work aims to study the dynamic between research in the industry and academia in computer vision. The results are demonstrated on a set of top-5 vision conferences that are representative of the field. Since data for such analysis was not readily available, significant effort was spent on gathering and processing meta-data from the original publications. First, this study quantifies the share of industry-sponsored research. Specifically, it shows that the proportion of papers published by industry-affiliated researchers is increasing and that more academics join companies or collaborate with them. Next, the possible impact of industry presence is further explored, namely in the distribution of research topics and citation patterns. The results indicate that the distribution of the research topics is similar in industry and academic papers. However, there is a strong preference towards citing industry papers. Finally, possible reasons for citation bias, such as code availability and influence, are investigated.



### Anatomy of Domain Shift Impact on U-Net Layers in MRI Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.04914v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04914v1)
- **Published**: 2021-07-10 21:13:55+00:00
- **Updated**: 2021-07-10 21:13:55+00:00
- **Authors**: Ivan Zakazov, Boris Shirokikh, Alexey Chernyavskiy, Mikhail Belyaev
- **Comment**: Accepted for MICCAI-2021 conference
- **Journal**: None
- **Summary**: Domain Adaptation (DA) methods are widely used in medical image segmentation tasks to tackle the problem of differently distributed train (source) and test (target) data. We consider the supervised DA task with a limited number of annotated samples from the target domain. It corresponds to one of the most relevant clinical setups: building a sufficiently accurate model on the minimum possible amount of annotated data. Existing methods mostly fine-tune specific layers of the pretrained Convolutional Neural Network (CNN). However, there is no consensus on which layers are better to fine-tune, e.g. the first layers for images with low-level domain shift or the deeper layers for images with high-level domain shift. To this end, we propose SpotTUnet - a CNN architecture that automatically chooses the layers which should be optimally fine-tuned. More specifically, on the target domain, our method additionally learns the policy that indicates whether a specific layer should be fine-tuned or reused from the pretrained network. We show that our method performs at the same level as the best of the nonflexible fine-tuning methods even under the extreme scarcity of annotated data. Secondly, we show that SpotTUnet policy provides a layer-wise visualization of the domain shift impact on the network, which could be further used to develop robust domain generalization methods. In order to extensively evaluate SpotTUnet performance, we use a publicly available dataset of brain MR images (CC359), characterized by explicit domain shift. We release a reproducible experimental pipeline.



### Feature-based Event Stereo Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/2107.04921v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04921v1)
- **Published**: 2021-07-10 22:36:49+00:00
- **Updated**: 2021-07-10 22:36:49+00:00
- **Authors**: Antea Hadviger, Igor Cvišić, Ivan Marković, Sacha Vražić, Ivan Petrović
- **Comment**: Submitted to Accepted to European Conference on Mobile Robots (ECMR)
  2021
- **Journal**: None
- **Summary**: Event-based cameras are biologically inspired sensors that output events, i.e., asynchronous pixel-wise brightness changes in the scene. Their high dynamic range and temporal resolution of a microsecond makes them more reliable than standard cameras in environments of challenging illumination and in high-speed scenarios, thus developing odometry algorithms based solely on event cameras offers exciting new possibilities for autonomous systems and robots. In this paper, we propose a novel stereo visual odometry method for event cameras based on feature detection and matching with careful feature management, while pose estimation is done by reprojection error minimization. We evaluate the performance of the proposed method on two publicly available datasets: MVSEC sequences captured by an indoor flying drone and DSEC outdoor driving sequences. MVSEC offers accurate ground truth from motion capture, while for DSEC, which does not offer ground truth, in order to obtain a reference trajectory on the standard camera frames we used our SOFT visual odometry, one of the highest ranking algorithms on the KITTI scoreboards. We compared our method to the ESVO method, which is the first and still the only stereo event odometry method, showing on par performance on the MVSEC sequences, while on the DSEC dataset ESVO, unlike our method, was unable to handle outdoor driving scenario with default parameters. Furthermore, two important advantages of our method over ESVO are that it adapts tracking frequency to the asynchronous event rate and does not require initialization.



### TeliNet: Classifying CT scan images for COVID-19 diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2107.04930v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04930v2)
- **Published**: 2021-07-10 23:46:14+00:00
- **Updated**: 2021-08-17 18:59:25+00:00
- **Authors**: Mohammad Nayeem Teli
- **Comment**: 7 pages, 4 figures, ICCVW
- **Journal**: ICCVW 2021
- **Summary**: COVID-19 has led to hundreds of millions of cases and millions of deaths worldwide since its onset. The fight against this pandemic is on-going on multiple fronts. While vaccinations are picking up speed, there are still billions of unvaccinated people. In this fight against the virus, diagnosis of the disease and isolation of the patients to prevent any spread play a huge role. Machine Learning approaches have assisted in the diagnosis of COVID-19 cases by analyzing chest X-rays and CT-scan images of patients. To push algorithm development and research in this direction of radiological diagnosis, a challenge to classify CT-scan series was organized in conjunction with ICCV, 2021. In this research we present a simple and shallow Convolutional Neural Network based approach, TeliNet, to classify these CT-scan images of COVID-19 patients presented as part of this competition. Our results outperform the F1 `macro' score of the competition benchmark and VGGNet approaches. Our proposed solution is also more lightweight in comparison to the other methods.



