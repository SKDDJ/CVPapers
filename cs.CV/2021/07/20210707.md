# Arxiv Papers in cs.CV on 2021-07-07
### End-to-End Simultaneous Learning of Single-particle Orientation and 3D Map Reconstruction from Cryo-electron Microscopy Data
- **Arxiv ID**: http://arxiv.org/abs/2107.02958v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2107.02958v1)
- **Published**: 2021-07-07 00:39:58+00:00
- **Updated**: 2021-07-07 00:39:58+00:00
- **Authors**: Youssef S. G. Nashed, Frederic Poitevin, Harshit Gupta, Geoffrey Woollard, Michael Kagan, Chuck Yoon, Daniel Ratner
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: Cryogenic electron microscopy (cryo-EM) provides images from different copies of the same biomolecule in arbitrary orientations. Here, we present an end-to-end unsupervised approach that learns individual particle orientations from cryo-EM data while reconstructing the average 3D map of the biomolecule, starting from a random initialization. The approach relies on an auto-encoder architecture where the latent space is explicitly interpreted as orientations used by the decoder to form an image according to the linear projection model. We evaluate our method on simulated data and show that it is able to reconstruct 3D particle maps from noisy- and CTF-corrupted 2D projection images of unknown particle orientations.



### GLiT: Neural Architecture Search for Global and Local Image Transformer
- **Arxiv ID**: http://arxiv.org/abs/2107.02960v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02960v3)
- **Published**: 2021-07-07 00:48:09+00:00
- **Updated**: 2021-08-17 11:01:05+00:00
- **Authors**: Boyu Chen, Peixia Li, Chuming Li, Baopu Li, Lei Bai, Chen Lin, Ming Sun, Junjie yan, Wanli Ouyang
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: We introduce the first Neural Architecture Search (NAS) method to find a better transformer architecture for image recognition. Recently, transformers without CNN-based backbones are found to achieve impressive performance for image recognition. However, the transformer is designed for NLP tasks and thus could be sub-optimal when directly used for image recognition. In order to improve the visual representation ability for transformers, we propose a new search space and searching algorithm. Specifically, we introduce a locality module that models the local correlations in images explicitly with fewer computational cost. With the locality module, our search space is defined to let the search algorithm freely trade off between global and local information as well as optimizing the low-level design choice in each module. To tackle the problem caused by huge search space, a hierarchical neural architecture search method is proposed to search the optimal vision transformer from two levels separately with the evolutionary algorithm. Extensive experiments on the ImageNet dataset demonstrate that our method can find more discriminative and efficient transformer variants than the ResNet family (e.g., ResNet101) and the baseline ViT for image classification.



### Disentangle Your Dense Object Detector
- **Arxiv ID**: http://arxiv.org/abs/2107.02963v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02963v2)
- **Published**: 2021-07-07 00:52:16+00:00
- **Updated**: 2021-07-27 11:15:13+00:00
- **Authors**: Zehui Chen, Chenhongyi Yang, Qiaofei Li, Feng Zhao, Zheng-Jun Zha, Feng Wu
- **Comment**: ACM MM2021
- **Journal**: None
- **Summary**: Deep learning-based dense object detectors have achieved great success in the past few years and have been applied to numerous multimedia applications such as video understanding. However, the current training pipeline for dense detectors is compromised to lots of conjunctions that may not hold. In this paper, we investigate three such important conjunctions: 1) only samples assigned as positive in classification head are used to train the regression head; 2) classification and regression share the same input feature and computational fields defined by the parallel head architecture; and 3) samples distributed in different feature pyramid layers are treated equally when computing the loss. We first carry out a series of pilot experiments to show disentangling such conjunctions can lead to persistent performance improvement. Then, based on these findings, we propose Disentangled Dense Object Detector (DDOD), in which simple and effective disentanglement mechanisms are designed and integrated into the current state-of-the-art dense object detectors. Extensive experiments on MS COCO benchmark show that our approach can lead to 2.0 mAP, 2.4 mAP and 2.2 mAP absolute improvements on RetinaNet, FCOS, and ATSS baselines with negligible extra overhead. Notably, our best model reaches 55.0 mAP on the COCO test-dev set and 93.5 AP on the hard subset of WIDER FACE, achieving new state-of-the-art performance on these two competitive benchmarks. Code is available at https://github.com/zehuichen123/DDOD.



### E-PixelHop: An Enhanced PixelHop Method for Object Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.02966v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02966v1)
- **Published**: 2021-07-07 01:22:12+00:00
- **Updated**: 2021-07-07 01:22:12+00:00
- **Authors**: Yijing Yang, Vasileios Magoulianitis, C. -C. Jay Kuo
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Based on PixelHop and PixelHop++, which are recently developed using the successive subspace learning (SSL) framework, we propose an enhanced solution for object classification, called E-PixelHop, in this work. E-PixelHop consists of the following steps. First, to decouple the color channels for a color image, we apply principle component analysis and project RGB three color channels onto two principle subspaces which are processed separately for classification. Second, to address the importance of multi-scale features, we conduct pixel-level classification at each hop with various receptive fields. Third, to further improve pixel-level classification accuracy, we develop a supervised label smoothing (SLS) scheme to ensure prediction consistency. Forth, pixel-level decisions from each hop and from each color subspace are fused together for image-level decision. Fifth, to resolve confusing classes for further performance boosting, we formulate E-PixelHop as a two-stage pipeline. In the first stage, multi-class classification is performed to get a soft decision for each class, where the top 2 classes with the highest probabilities are called confusing classes. Then,we conduct a binary classification in the second stage. The main contributions lie in Steps 1, 3 and 5.We use the classification of the CIFAR-10 dataset as an example to demonstrate the effectiveness of the above-mentioned key components of E-PixelHop.



### Edge-aware Bidirectional Diffusion for Dense Depth Estimation from Light Fields
- **Arxiv ID**: http://arxiv.org/abs/2107.02967v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02967v1)
- **Published**: 2021-07-07 01:26:25+00:00
- **Updated**: 2021-07-07 01:26:25+00:00
- **Authors**: Numair Khan, Min H. Kim, James Tompkin
- **Comment**: Project webpage: http://visual.cs.brown.edu/lightfielddepth
- **Journal**: None
- **Summary**: We present an algorithm to estimate fast and accurate depth maps from light fields via a sparse set of depth edges and gradients. Our proposed approach is based around the idea that true depth edges are more sensitive than texture edges to local constraints, and so they can be reliably disambiguated through a bidirectional diffusion process. First, we use epipolar-plane images to estimate sub-pixel disparity at a sparse set of pixels. To find sparse points efficiently, we propose an entropy-based refinement approach to a line estimate from a limited set of oriented filter banks. Next, to estimate the diffusion direction away from sparse points, we optimize constraints at these points via our bidirectional diffusion method. This resolves the ambiguity of which surface the edge belongs to and reliably separates depth from texture edges, allowing us to diffuse the sparse set in a depth-edge and occlusion-aware manner to obtain accurate dense depth maps.



### GAN-based Data Augmentation for Chest X-ray Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.02970v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2107.02970v1)
- **Published**: 2021-07-07 01:36:48+00:00
- **Updated**: 2021-07-07 01:36:48+00:00
- **Authors**: Shobhita Sundaram, Neha Hulkund
- **Comment**: Spotlight Talk at KDD 2021 - Applied Data Science for Healthcare
  Workshop
- **Journal**: None
- **Summary**: A common problem in computer vision -- particularly in medical applications -- is a lack of sufficiently diverse, large sets of training data. These datasets often suffer from severe class imbalance. As a result, networks often overfit and are unable to generalize to novel examples. Generative Adversarial Networks (GANs) offer a novel method of synthetic data augmentation. In this work, we evaluate the use of GAN- based data augmentation to artificially expand the CheXpert dataset of chest radiographs. We compare performance to traditional augmentation and find that GAN-based augmentation leads to higher downstream performance for underrepresented classes. Furthermore, we see that this result is pronounced in low data regimens. This suggests that GAN-based augmentation a promising area of research to improve network performance when data collection is prohibitively expensive.



### Learn to Learn Metric Space for Few-Shot Segmentation of 3D Shapes
- **Arxiv ID**: http://arxiv.org/abs/2107.02972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02972v1)
- **Published**: 2021-07-07 01:47:00+00:00
- **Updated**: 2021-07-07 01:47:00+00:00
- **Authors**: Xiang Li, Lingjing Wang, Yi Fang
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Recent research has seen numerous supervised learning-based methods for 3D shape segmentation and remarkable performance has been achieved on various benchmark datasets. These supervised methods require a large amount of annotated data to train deep neural networks to ensure the generalization ability on the unseen test set. In this paper, we introduce a meta-learning-based method for few-shot 3D shape segmentation where only a few labeled samples are provided for the unseen classes. To achieve this, we treat the shape segmentation as a point labeling problem in the metric space. Specifically, we first design a meta-metric learner to transform input shapes into embedding space and our model learns to learn a proper metric space for each object class based on point embeddings. Then, for each class, we design a metric learner to extract part-specific prototype representations from a few support shapes and our model performs per-point segmentation over the query shapes by matching each point to its nearest prototype in the learned metric space. A metric-based loss function is used to dynamically modify distances between point embeddings thus maximizes in-part similarity while minimizing inter-part similarity. A dual segmentation branch is adopted to make full use of the support information and implicitly encourages consistency between the support and query prototypes. We demonstrate the superior performance of our proposed on the ShapeNet part dataset under the few-shot scenario, compared with well-established baseline and state-of-the-art semi-supervised methods.



### RAM-VO: Less is more in Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/2107.02974v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02974v1)
- **Published**: 2021-07-07 01:48:16+00:00
- **Updated**: 2021-07-07 01:48:16+00:00
- **Authors**: Iury Cleveston, Esther L. Colombini
- **Comment**: None
- **Journal**: None
- **Summary**: Building vehicles capable of operating without human supervision requires the determination of the agent's pose. Visual Odometry (VO) algorithms estimate the egomotion using only visual changes from the input images. The most recent VO methods implement deep-learning techniques using convolutional neural networks (CNN) extensively, which add a substantial cost when dealing with high-resolution images. Furthermore, in VO tasks, more input data does not mean a better prediction; on the contrary, the architecture may filter out useless information. Therefore, the implementation of computationally efficient and lightweight architectures is essential. In this work, we propose the RAM-VO, an extension of the Recurrent Attention Model (RAM) for visual odometry tasks. RAM-VO improves the visual and temporal representation of information and implements the Proximal Policy Optimization (PPO) algorithm to learn robust policies. The results indicate that RAM-VO can perform regressions with six degrees of freedom from monocular input images using approximately 3 million parameters. In addition, experiments on the KITTI dataset demonstrate that RAM-VO achieves competitive results using only 5.7% of the available visual information.



### VIN: Voxel-based Implicit Network for Joint 3D Object Detection and Segmentation for Lidars
- **Arxiv ID**: http://arxiv.org/abs/2107.02980v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02980v2)
- **Published**: 2021-07-07 02:16:20+00:00
- **Updated**: 2021-11-13 03:22:20+00:00
- **Authors**: Yuanxin Zhong, Minghan Zhu, Huei Peng
- **Comment**: To be presented at BMVC 2021
- **Journal**: None
- **Summary**: A unified neural network structure is presented for joint 3D object detection and point cloud segmentation in this paper. We leverage rich supervision from both detection and segmentation labels rather than using just one of them. In addition, an extension based on single-stage object detectors is proposed based on the implicit function widely used in 3D scene and object understanding. The extension branch takes the final feature map from the object detection module as input, and produces an implicit function that generates semantic distribution for each point for its corresponding voxel center. We demonstrated the performance of our structure on nuScenes-lidarseg, a large-scale outdoor dataset. Our solution achieves competitive results against state-of-the-art methods in both 3D object detection and point cloud segmentation with little additional computation load compared with object detection solutions. The capability of efficient weakly supervision semantic segmentation of the proposed method is also validated by experiments.



### Deep Convolutional Correlation Iterative Particle Filter for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2107.02984v2
- **DOI**: 10.1016/j.cviu.2022.103479
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02984v2)
- **Published**: 2021-07-07 02:44:43+00:00
- **Updated**: 2023-01-03 20:31:29+00:00
- **Authors**: Reza Jalil Mozhdehi, Henry Medeiros
- **Comment**: 25 pages, 10 figures, 1 table
- **Journal**: Computer Vision and Image Understanding (ELSEVIER), Volume 222,
  103479, 2022
- **Summary**: This work proposes a novel framework for visual tracking based on the integration of an iterative particle filter, a deep convolutional neural network, and a correlation filter. The iterative particle filter enables the particles to correct themselves and converge to the correct target position. We employ a novel strategy to assess the likelihood of the particles after the iterations by applying K-means clustering. Our approach ensures a consistent support for the posterior distribution. Thus, we do not need to perform resampling at every video frame, improving the utilization of prior distribution information. Experimental results on two different benchmark datasets show that our tracker performs favorably against state-of-the-art methods.



### SpectralFormer: Rethinking Hyperspectral Image Classification with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2107.02988v2
- **DOI**: 10.1109/TGRS.2021.3130716
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.02988v2)
- **Published**: 2021-07-07 02:59:21+00:00
- **Updated**: 2021-11-20 01:26:16+00:00
- **Authors**: Danfeng Hong, Zhu Han, Jing Yao, Lianru Gao, Bing Zhang, Antonio Plaza, Jocelyn Chanussot
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral (HS) images are characterized by approximately contiguous spectral information, enabling the fine identification of materials by capturing subtle spectral discrepancies. Owing to their excellent locally contextual modeling ability, convolutional neural networks (CNNs) have been proven to be a powerful feature extractor in HS image classification. However, CNNs fail to mine and represent the sequence attributes of spectral signatures well due to the limitations of their inherent network backbone. To solve this issue, we rethink HS image classification from a sequential perspective with transformers, and propose a novel backbone network called \ul{SpectralFormer}. Beyond band-wise representations in classic transformers, SpectralFormer is capable of learning spectrally local sequence information from neighboring bands of HS images, yielding group-wise spectral embeddings. More significantly, to reduce the possibility of losing valuable information in the layer-wise propagation process, we devise a cross-layer skip connection to convey memory-like components from shallow to deep layers by adaptively learning to fuse "soft" residuals across layers. It is worth noting that the proposed SpectralFormer is a highly flexible backbone network, which can be applicable to both pixel- and patch-wise inputs. We evaluate the classification performance of the proposed SpectralFormer on three HS datasets by conducting extensive experiments, showing the superiority over classic transformers and achieving a significant improvement in comparison with state-of-the-art backbone networks. The codes of this work will be available at https://github.com/danfenghong/IEEE_TGRS_SpectralFormer for the sake of reproducibility.



### PoseRN: A 2D pose refinement network for bias-free multi-view 3D human pose estimation
- **Arxiv ID**: http://arxiv.org/abs/2107.03000v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03000v1)
- **Published**: 2021-07-07 03:49:36+00:00
- **Updated**: 2021-07-07 03:49:36+00:00
- **Authors**: Akihiko Sayo, Diego Thomas, Hiroshi Kawasaki, Yuta Nakashima, Katsushi Ikeuchi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new 2D pose refinement network that learns to predict the human bias in the estimated 2D pose. There are biases in 2D pose estimations that are due to differences between annotations of 2D joint locations based on annotators' perception and those defined by motion capture (MoCap) systems. These biases are crafted into publicly available 2D pose datasets and cannot be removed with existing error reduction approaches. Our proposed pose refinement network allows us to efficiently remove the human bias in the estimated 2D poses and achieve highly accurate multi-view 3D human pose estimation.



### Structured Denoising Diffusion Models in Discrete State-Spaces
- **Arxiv ID**: http://arxiv.org/abs/2107.03006v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.03006v3)
- **Published**: 2021-07-07 04:11:00+00:00
- **Updated**: 2023-02-22 16:05:48+00:00
- **Authors**: Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, Rianne van den Berg
- **Comment**: 10 pages plus references and appendices. First two authors
  contributed equally
- **Journal**: None
- **Summary**: Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.



### Learning Invariant Representation with Consistency and Diversity for Semi-supervised Source Hypothesis Transfer
- **Arxiv ID**: http://arxiv.org/abs/2107.03008v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03008v2)
- **Published**: 2021-07-07 04:14:24+00:00
- **Updated**: 2021-07-20 02:37:34+00:00
- **Authors**: Xiaodong Wang, Junbao Zhuo, Shuhao Cui, Shuhui Wang
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Semi-supervised domain adaptation (SSDA) aims to solve tasks in target domain by utilizing transferable information learned from the available source domain and a few labeled target data. However, source data is not always accessible in practical scenarios, which restricts the application of SSDA in real world circumstances. In this paper, we propose a novel task named Semi-supervised Source Hypothesis Transfer (SSHT), which performs domain adaptation based on source trained model, to generalize well in target domain with a few supervisions. In SSHT, we are facing two challenges: (1) The insufficient labeled target data may result in target features near the decision boundary, with the increased risk of mis-classification; (2) The data are usually imbalanced in source domain, so the model trained with these data is biased. The biased model is prone to categorize samples of minority categories into majority ones, resulting in low prediction diversity. To tackle the above issues, we propose Consistency and Diversity Learning (CDL), a simple but effective framework for SSHT by facilitating prediction consistency between two randomly augmented unlabeled data and maintaining the prediction diversity when adapting model to target domain. Encouraging consistency regularization brings difficulty to memorize the few labeled target data and thus enhances the generalization ability of the learned model. We further integrate Batch Nuclear-norm Maximization into our method to enhance the discriminability and diversity. Experimental results show that our method outperforms existing SSDA methods and unsupervised model adaptation methods on DomainNet, Office-Home and Office-31 datasets. The code is available at https://github.com/Wang-xd1899/SSHT.



### Multi-modal Affect Analysis using standardized data within subjects in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2107.03009v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03009v3)
- **Published**: 2021-07-07 04:18:28+00:00
- **Updated**: 2021-07-10 05:33:54+00:00
- **Authors**: Sachihiro Youoku, Takahisa Yamamoto, Junya Saito, Akiyoshi Uchida, Xiaoyu Mi, Ziqiang Shi, Liu Liu, Zhongling Liu, Osafumi Nakayama, Kentaro Murase
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Human affective recognition is an important factor in human-computer interaction. However, the method development with in-the-wild data is not yet accurate enough for practical usage. In this paper, we introduce the affective recognition method focusing on facial expression (EXP) and valence-arousal calculation that was submitted to the Affective Behavior Analysis in-the-wild (ABAW) 2021 Contest.   When annotating facial expressions from a video, we thought that it would be judged not only from the features common to all people, but also from the relative changes in the time series of individuals. Therefore, after learning the common features for each frame, we constructed a facial expression estimation model and valence-arousal model using time-series data after combining the common features and the standardized features for each video. Furthermore, the above features were learned using multi-modal data such as image features, AU, Head pose, and Gaze. In the validation set, our model achieved a facial expression score of 0.546. These verification results reveal that our proposed framework can improve estimation accuracy and robustness effectively.



### Visual Odometry with an Event Camera Using Continuous Ray Warping and Volumetric Contrast Maximization
- **Arxiv ID**: http://arxiv.org/abs/2107.03011v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03011v1)
- **Published**: 2021-07-07 04:32:57+00:00
- **Updated**: 2021-07-07 04:32:57+00:00
- **Authors**: Yifu Wang, Jiaqi Yang, Xin Peng, Peng Wu, Ling Gao, Kun Huang, Jiaben Chen, Laurent Kneip
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new solution to tracking and mapping with an event camera. The motion of the camera contains both rotation and translation, and the displacements happen in an arbitrarily structured environment. As a result, the image matching may no longer be represented by a low-dimensional homographic warping, thus complicating an application of the commonly used Image of Warped Events (IWE). We introduce a new solution to this problem by performing contrast maximization in 3D. The 3D location of the rays cast for each event is smoothly varied as a function of a continuous-time motion parametrization, and the optimal parameters are found by maximizing the contrast in a volumetric ray density field. Our method thus performs joint optimization over motion and structure. The practical validity of our approach is supported by an application to AGV motion estimation and 3D reconstruction with a single vehicle-mounted event camera. The method approaches the performance obtained with regular cameras, and eventually outperforms in challenging visual conditions.



### Bi-level Feature Alignment for Versatile Image Translation and Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2107.03021v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03021v2)
- **Published**: 2021-07-07 05:26:29+00:00
- **Updated**: 2022-07-21 22:29:00+00:00
- **Authors**: Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang, Kaiwen Cui, Aoran Xiao, Shijian Lu, Chunyan Miao
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have achieved great success in image translation and manipulation. However, high-fidelity image generation with faithful style control remains a grand challenge in computer vision. This paper presents a versatile image translation and manipulation framework that achieves accurate semantic and style guidance in image generation by explicitly building a correspondence. To handle the quadratic complexity incurred by building the dense correspondences, we introduce a bi-level feature alignment strategy that adopts a top-$k$ operation to rank block-wise features followed by dense attention between block features which reduces memory cost substantially. As the top-$k$ operation involves index swapping which precludes the gradient propagation, we approximate the non-differentiable top-$k$ operation with a regularized earth mover's problem so that its gradient can be effectively back-propagated. In addition, we design a novel semantic position encoding mechanism that builds up coordinate for each individual semantic region to preserve texture structures while building correspondences. Further, we design a novel confidence feature injection module which mitigates mismatch problem by fusing features adaptively according to the reliability of built correspondences. Extensive experiments show that our method achieves superior performance qualitatively and quantitatively as compared with the state-of-the-art.



### Rethinking Sampling Strategies for Unsupervised Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2107.03024v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03024v3)
- **Published**: 2021-07-07 05:39:58+00:00
- **Updated**: 2023-03-16 07:43:08+00:00
- **Authors**: Xumeng Han, Xuehui Yu, Guorong Li, Jian Zhao, Gang Pan, Qixiang Ye, Jianbin Jiao, Zhenjun Han
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised person re-identification (re-ID) remains a challenging task. While extensive research has focused on the framework design and loss function, this paper shows that sampling strategy plays an equally important role. We analyze the reasons for the performance differences between various sampling strategies under the same framework and loss function. We suggest that deteriorated over-fitting is an important factor causing poor performance, and enhancing statistical stability can rectify this problem. Inspired by that, a simple yet effective approach is proposed, termed group sampling, which gathers samples from the same class into groups. The model is thereby trained using normalized group samples, which helps alleviate the negative impact of individual samples. Group sampling updates the pipeline of pseudo-label generation by guaranteeing that samples are more efficiently classified into the correct classes. It regulates the representation learning process, enhancing statistical stability for feature representation in a progressive fashion. Extensive experiments on Market-1501, DukeMTMC-reID and MSMT17 show that group sampling achieves performance comparable to state-of-the-art methods and outperforms the current techniques under purely camera-agnostic settings. Code has been available at https://github.com/ucas-vg/GroupSampling.



### A convolutional neural network for teeth margin detection on 3-dimensional dental meshes
- **Arxiv ID**: http://arxiv.org/abs/2107.03030v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2107.03030v1)
- **Published**: 2021-07-07 06:16:17+00:00
- **Updated**: 2021-07-07 06:16:17+00:00
- **Authors**: Hu Chen, Hong Li, Bifu Hu, Kenan Ma, Yuchun Sun
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: We proposed a convolutional neural network for vertex classification on 3-dimensional dental meshes, and used it to detect teeth margins. An expanding layer was constructed to collect statistic values of neighbor vertex features and compute new features for each vertex with convolutional neural networks. An end-to-end neural network was proposed to take vertex features, including coordinates, curvatures and distance, as input and output each vertex classification label. Several network structures with different parameters of expanding layers and a base line network without expanding layers were designed and trained by 1156 dental meshes. The accuracy, recall and precision were validated on 145 dental meshes to rate the best network structures, which were finally tested on another 144 dental meshes. All networks with our expanding layers performed better than baseline, and the best one achieved an accuracy of 0.877 both on validation dataset and test dataset.



### Transformer Network for Significant Stenosis Detection in CCTA of Coronary Arteries
- **Arxiv ID**: http://arxiv.org/abs/2107.03035v3
- **DOI**: 10.1007/978-3-030-87231-1_50
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.03035v3)
- **Published**: 2021-07-07 06:27:52+00:00
- **Updated**: 2021-09-27 08:38:07+00:00
- **Authors**: Xinghua Ma, Gongning Luo, Wei Wang, Kuanquan Wang
- **Comment**: None
- **Journal**: Medical Image Computing and Computer Assisted Intervention -
  MICCAI 2021
- **Summary**: Coronary artery disease (CAD) has posed a leading threat to the lives of cardiovascular disease patients worldwide for a long time. Therefore, automated diagnosis of CAD has indispensable significance in clinical medicine. However, the complexity of coronary artery plaques that cause CAD makes the automatic detection of coronary artery stenosis in Coronary CT angiography (CCTA) a difficult task. In this paper, we propose a Transformer network (TR-Net) for the automatic detection of significant stenosis (i.e. luminal narrowing > 50%) while practically completing the computer-assisted diagnosis of CAD. The proposed TR-Net introduces a novel Transformer, and tightly combines convolutional layers and Transformer encoders, allowing their advantages to be demonstrated in the task. By analyzing semantic information sequences, TR-Net can fully understand the relationship between image information in each position of a multiplanar reformatted (MPR) image, and accurately detect significant stenosis based on both local and global information. We evaluate our TR-Net on a dataset of 76 patients from different patients annotated by experienced radiologists. Experimental results illustrate that our TR-Net has achieved better results in ACC (0.92), Spec (0.96), PPV (0.84), F1 (0.79) and MCC (0.74) indicators compared with the state-of-the-art methods. The source code is publicly available from the link (https://github.com/XinghuaMa/TR-Net).



### Maintaining a Reliable World Model using Action-aware Perceptual Anchoring
- **Arxiv ID**: http://arxiv.org/abs/2107.03038v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.03038v1)
- **Published**: 2021-07-07 06:35:14+00:00
- **Updated**: 2021-07-07 06:35:14+00:00
- **Authors**: Ying Siu Liang, Dongkyu Choi, Kenneth Kwok
- **Comment**: 7 pages, 3 figures
- **Journal**: 2021 International Conference on Robotics and Automation (ICRA
  2021)
- **Summary**: Reliable perception is essential for robots that interact with the world. But sensors alone are often insufficient to provide this capability, and they are prone to errors due to various conditions in the environment. Furthermore, there is a need for robots to maintain a model of its surroundings even when objects go out of view and are no longer visible. This requires anchoring perceptual information onto symbols that represent the objects in the environment. In this paper, we present a model for action-aware perceptual anchoring that enables robots to track objects in a persistent manner. Our rule-based approach considers inductive biases to perform high-level reasoning over the results from low-level object detection, and it improves the robot's perceptual capability for complex tasks. We evaluate our model against existing baseline models for object permanence and show that it outperforms these on a snitch localisation task using a dataset of 1,371 videos. We also integrate our action-aware perceptual anchoring in the context of a cognitive architecture and demonstrate its benefits in a realistic gearbox assembly task on a Universal Robot.



### Controlled Caption Generation for Images Through Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2107.03050v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.03050v1)
- **Published**: 2021-07-07 07:22:41+00:00
- **Updated**: 2021-07-07 07:22:41+00:00
- **Authors**: Nayyer Aafaq, Naveed Akhtar, Wei Liu, Mubarak Shah, Ajmal Mian
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning is found to be vulnerable to adversarial examples. However, its adversarial susceptibility in image caption generation is under-explored. We study adversarial examples for vision and language models, which typically adopt an encoder-decoder framework consisting of two major components: a Convolutional Neural Network (i.e., CNN) for image feature extraction and a Recurrent Neural Network (RNN) for caption generation. In particular, we investigate attacks on the visual encoder's hidden layer that is fed to the subsequent recurrent network. The existing methods either attack the classification layer of the visual encoder or they back-propagate the gradients from the language model. In contrast, we propose a GAN-based algorithm for crafting adversarial examples for neural image captioning that mimics the internal representation of the CNN such that the resulting deep features of the input image enable a controlled incorrect caption generation through the recurrent network. Our contribution provides new insights for understanding adversarial attacks on vision systems with language component. The proposed method employs two strategies for a comprehensive evaluation. The first examines if a neural image captioning system can be misled to output targeted image captions. The second analyzes the possibility of keywords into the predicted captions. Experiments show that our algorithm can craft effective adversarial images based on the CNN hidden layers to fool captioning framework. Moreover, we discover the proposed attack to be highly transferable. Our work leads to new robustness implications for neural image captioning.



### Blind Image Super-Resolution: A Survey and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2107.03055v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03055v1)
- **Published**: 2021-07-07 07:38:14+00:00
- **Updated**: 2021-07-07 07:38:14+00:00
- **Authors**: Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, Chao Dong
- **Comment**: None
- **Journal**: None
- **Summary**: Blind image super-resolution (SR), aiming to super-resolve low-resolution images with unknown degradation, has attracted increasing attention due to its significance in promoting real-world applications. Many novel and effective solutions have been proposed recently, especially with the powerful deep learning techniques. Despite years of efforts, it still remains as a challenging research problem. This paper serves as a systematic review on recent progress in blind image SR, and proposes a taxonomy to categorize existing methods into three different classes according to their ways of degradation modelling and the data used for solving the SR model. This taxonomy helps summarize and distinguish among existing methods. We hope to provide insights into current research states, as well as to reveal novel research directions worth exploring. In addition, we make a summary on commonly used datasets and previous competitions related to blind image SR. Last but not least, a comparison among different methods is provided with detailed analysis on their merits and demerits using both synthetic and real testing images.



### Video-Based Camera Localization Using Anchor View Detection and Recursive 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2107.03068v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03068v1)
- **Published**: 2021-07-07 08:13:33+00:00
- **Updated**: 2021-07-07 08:13:33+00:00
- **Authors**: Hajime Taira, Koki Onbe, Naoyuki Miyashita, Masatoshi Okutomi
- **Comment**: This paper have been accepted and will be appeared in the proceedings
  of 17th International Conference on Machine Vision Applications (MVA2021)
- **Journal**: None
- **Summary**: In this paper we introduce a new camera localization strategy designed for image sequences captured in challenging industrial situations such as industrial parts inspection. To deal with peculiar appearances that hurt standard 3D reconstruction pipeline, we exploit pre-knowledge of the scene by selecting key frames in the sequence (called as anchors) which are roughly connected to a certain location. Our method then seek the location of each frame in time-order, while recursively updating an augmented 3D model which can provide current camera location and surrounding 3D structure. In an experiment on a practical industrial situation, our method can localize over 99% frames in the input sequence, whereas standard localization methods fail to reconstruct a complete camera trajectory.



### Learning Stixel-based Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.03070v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03070v1)
- **Published**: 2021-07-07 08:16:42+00:00
- **Updated**: 2021-07-07 08:16:42+00:00
- **Authors**: Monty Santarossa, Lukas Schneider, Claudius Zelenka, Lars Schmarje, Reinhard Koch, Uwe Franke
- **Comment**: Accepted for publication in IEEE Intelligent Vehicles Symposium
- **Journal**: None
- **Summary**: Stixels have been successfully applied to a wide range of vision tasks in autonomous driving, recently including instance segmentation. However, due to their sparse occurrence in the image, until now Stixels seldomly served as input for Deep Learning algorithms, restricting their utility for such approaches. In this work we present StixelPointNet, a novel method to perform fast instance segmentation directly on Stixels. By regarding the Stixel representation as unstructured data similar to point clouds, architectures like PointNet are able to learn features from Stixels. We use a bounding box detector to propose candidate instances, for which the relevant Stixels are extracted from the input image. On these Stixels, a PointNet models learns binary segmentations, which we then unify throughout the whole image in a final selection step. StixelPointNet achieves state-of-the-art performance on Stixel-level, is considerably faster than pixel-based segmentation methods, and shows that with our approach the Stixel domain can be introduced to many new 3D Deep Learning tasks.



### WeClick: Weakly-Supervised Video Semantic Segmentation with Click Annotations
- **Arxiv ID**: http://arxiv.org/abs/2107.03088v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2107.03088v2)
- **Published**: 2021-07-07 09:12:46+00:00
- **Updated**: 2021-08-04 15:50:15+00:00
- **Authors**: Peidong Liu, Zibin He, Xiyu Yan, Yong Jiang, Shutao Xia, Feng Zheng, Maowei Hu
- **Comment**: Accepted by ACM MM2021 Oral
- **Journal**: None
- **Summary**: Compared with tedious per-pixel mask annotating, it is much easier to annotate data by clicks, which costs only several seconds for an image. However, applying clicks to learn video semantic segmentation model has not been explored before. In this work, we propose an effective weakly-supervised video semantic segmentation pipeline with click annotations, called WeClick, for saving laborious annotating effort by segmenting an instance of the semantic class with only a single click. Since detailed semantic information is not captured by clicks, directly training with click labels leads to poor segmentation predictions. To mitigate this problem, we design a novel memory flow knowledge distillation strategy to exploit temporal information (named memory flow) in abundant unlabeled video frames, by distilling the neighboring predictions to the target frame via estimated motion. Moreover, we adopt vanilla knowledge distillation for model compression. In this case, WeClick learns compact video semantic segmentation models with the low-cost click annotations during the training phase yet achieves real-time and accurate models during the inference period. Experimental results on Cityscapes and Camvid show that WeClick outperforms the state-of-the-art methods, increases performance by 10.24% mIoU than baseline, and achieves real-time execution.



### Greedy Offset-Guided Keypoint Grouping for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2107.03098v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03098v2)
- **Published**: 2021-07-07 09:32:01+00:00
- **Updated**: 2021-09-06 06:18:12+00:00
- **Authors**: Jia Li, Linhua Xiang, Jiwei Chen, Zengfu Wang
- **Comment**: 5 pages, 2 figures, code available at
  https://github.com/hellojialee/OffsetGuided
- **Journal**: None
- **Summary**: We propose a simple yet reliable bottom-up approach with a good trade-off between accuracy and efficiency for the problem of multi-person pose estimation. Given an image, we employ an Hourglass Network to infer all the keypoints from different persons indiscriminately as well as the guiding offsets connecting the adjacent keypoints belonging to the same persons. Then, we greedily group the candidate keypoints into multiple human poses (if any), utilizing the predicted guiding offsets. And we refer to this process as greedy offset-guided keypoint grouping (GOG). Moreover, we revisit the encoding-decoding method for the multi-person keypoint coordinates and reveal some important facts affecting accuracy. Experiments have demonstrated the obvious performance improvements brought by the introduced components. Our approach is comparable to the state of the art on the challenging COCO dataset under fair conditions. The source code and our pre-trained model are publicly available online.



### GA-NET: Global Attention Network for Point Cloud Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.03101v1
- **DOI**: 10.1109/LSP.2021.3082851
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03101v1)
- **Published**: 2021-07-07 09:35:59+00:00
- **Updated**: 2021-07-07 09:35:59+00:00
- **Authors**: Shuang Deng, Qiulei Dong
- **Comment**: None
- **Journal**: IEEE Signal Processing Letter, Vol. 28, pp. 1300-1304, 2021
- **Summary**: How to learn long-range dependencies from 3D point clouds is a challenging problem in 3D point cloud analysis. Addressing this problem, we propose a global attention network for point cloud semantic segmentation, named as GA-Net, consisting of a point-independent global attention module and a point-dependent global attention module for obtaining contextual information of 3D point clouds in this paper. The point-independent global attention module simply shares a global attention map for all 3D points. In the point-dependent global attention module, for each point, a novel random cross attention block using only two randomly sampled subsets is exploited to learn the contextual information of all the points. Additionally, we design a novel point-adaptive aggregation block to replace linear skip connection for aggregating more discriminate features. Extensive experimental results on three 3D public datasets demonstrate that our method outperforms state-of-the-art methods in most cases.



### Rotation Transformation Network: Learning View-Invariant Point Cloud for Classification and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.03105v1
- **DOI**: 10.1109/ICME51207.2021.9428265
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03105v1)
- **Published**: 2021-07-07 09:45:22+00:00
- **Updated**: 2021-07-07 09:45:22+00:00
- **Authors**: Shuang Deng, Bo Liu, Qiulei Dong, Zhanyi Hu
- **Comment**: None
- **Journal**: IEEE International Conference on Multimedia and Expo (ICME), 2021
- **Summary**: Many recent works show that a spatial manipulation module could boost the performances of deep neural networks (DNNs) for 3D point cloud analysis. In this paper, we aim to provide an insight into spatial manipulation modules. Firstly, we find that the smaller the rotational degree of freedom (RDF) of objects is, the more easily these objects are handled by these DNNs. Then, we investigate the effect of the popular T-Net module and find that it could not reduce the RDF of objects. Motivated by the above two issues, we propose a rotation transformation network for point cloud analysis, called RTN, which could reduce the RDF of input 3D objects to 0. The RTN could be seamlessly inserted into many existing DNNs for point cloud analysis. Extensive experimental results on 3D point cloud classification and segmentation tasks demonstrate that the proposed RTN could improve the performances of several state-of-the-art methods significantly.



### Self-supervised Outdoor Scene Relighting
- **Arxiv ID**: http://arxiv.org/abs/2107.03106v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2107.03106v1)
- **Published**: 2021-07-07 09:46:19+00:00
- **Updated**: 2021-07-07 09:46:19+00:00
- **Authors**: Ye Yu, Abhimitra Meka, Mohamed Elgharib, Hans-Peter Seidel, Christian Theobalt, William A. P. Smith
- **Comment**: Published in ECCV '20,
  http://gvv.mpi-inf.mpg.de/projects/SelfRelight/
- **Journal**: None
- **Summary**: Outdoor scene relighting is a challenging problem that requires good understanding of the scene geometry, illumination and albedo. Current techniques are completely supervised, requiring high quality synthetic renderings to train a solution. Such renderings are synthesized using priors learned from limited data. In contrast, we propose a self-supervised approach for relighting. Our approach is trained only on corpora of images collected from the internet without any user-supervision. This virtually endless source of training data allows training a general relighting solution. Our approach first decomposes an image into its albedo, geometry and illumination. A novel relighting is then produced by modifying the illumination parameters. Our solution capture shadow using a dedicated shadow prediction map, and does not rely on accurate geometry estimation. We evaluate our technique subjectively and objectively using a new dataset with ground-truth relighting. Results show the ability of our technique to produce photo-realistic and physically plausible results, that generalizes to unseen scenes.



### Learning Vision Transformer with Squeeze and Excitation for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.03107v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03107v4)
- **Published**: 2021-07-07 09:49:01+00:00
- **Updated**: 2021-07-16 07:49:19+00:00
- **Authors**: Mouath Aouayeb, Wassim Hamidouche, Catherine Soladie, Kidiyo Kpalma, Renaud Seguier
- **Comment**: None
- **Journal**: None
- **Summary**: As various databases of facial expressions have been made accessible over the last few decades, the Facial Expression Recognition (FER) task has gotten a lot of interest. The multiple sources of the available databases raised several challenges for facial recognition task. These challenges are usually addressed by Convolution Neural Network (CNN) architectures. Different from CNN models, a Transformer model based on attention mechanism has been presented recently to address vision tasks. One of the major issue with Transformers is the need of a large data for training, while most FER databases are limited compared to other vision applications. Therefore, we propose in this paper to learn a vision Transformer jointly with a Squeeze and Excitation (SE) block for FER task. The proposed method is evaluated on different publicly available FER databases including CK+, JAFFE,RAF-DB and SFEW. Experiments demonstrate that our model outperforms state-of-the-art methods on CK+ and SFEW and achieves competitive results on JAFFE and RAF-DB.



### Egocentric Videoconferencing
- **Arxiv ID**: http://arxiv.org/abs/2107.03109v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.03109v1)
- **Published**: 2021-07-07 09:49:39+00:00
- **Updated**: 2021-07-07 09:49:39+00:00
- **Authors**: Mohamed Elgharib, Mohit Mendiratta, Justus Thies, Matthias Nießner, Hans-Peter Seidel, Ayush Tewari, Vladislav Golyanik, Christian Theobalt
- **Comment**: Mohamed Elgharib and Mohit Mendiratta contributed equally to this
  work. http://gvv.mpi-inf.mpg.de/projects/EgoChat/
- **Journal**: ACM Transactions on Graphics, volume = 39, number = 6, articleno =
  268, year = 2020
- **Summary**: We introduce a method for egocentric videoconferencing that enables hands-free video calls, for instance by people wearing smart glasses or other mixed-reality devices. Videoconferencing portrays valuable non-verbal communication and face expression cues, but usually requires a front-facing camera. Using a frontal camera in a hands-free setting when a person is on the move is impractical. Even holding a mobile phone camera in the front of the face while sitting for a long duration is not convenient. To overcome these issues, we propose a low-cost wearable egocentric camera setup that can be integrated into smart glasses. Our goal is to mimic a classical video call, and therefore, we transform the egocentric perspective of this camera into a front facing video. To this end, we employ a conditional generative adversarial neural network that learns a transition from the highly distorted egocentric views to frontal views common in videoconferencing. Our approach learns to transfer expression details directly from the egocentric view without using a complex intermediate parametric expressions model, as it is used by related face reenactment methods. We successfully handle subtle expressions, not easily captured by parametric blendshape-based solutions, e.g., tongue movement, eye movements, eye blinking, strong expressions and depth varying movements. To get control over the rigid head movements in the target view, we condition the generator on synthetic renderings of a moving neutral face. This allows us to synthesis results at different head poses. Our technique produces temporally smooth video-realistic renderings in real-time using a video-to-video translation network in conjunction with a temporal discriminator. We demonstrate the improved capabilities of our technique by comparing against related state-of-the art approaches.



### Cross-View Exocentric to Egocentric Video Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2107.03120v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2107.03120v1)
- **Published**: 2021-07-07 10:00:52+00:00
- **Updated**: 2021-07-07 10:00:52+00:00
- **Authors**: Gaowen Liu, Hao Tang, Hugo Latapie, Jason Corso, Yan Yan
- **Comment**: ACM MM 2021
- **Journal**: None
- **Summary**: Cross-view video synthesis task seeks to generate video sequences of one view from another dramatically different view. In this paper, we investigate the exocentric (third-person) view to egocentric (first-person) view video generation task. This is challenging because egocentric view sometimes is remarkably different from the exocentric view. Thus, transforming the appearances across the two different views is a non-trivial task. Particularly, we propose a novel Bi-directional Spatial Temporal Attention Fusion Generative Adversarial Network (STA-GAN) to learn both spatial and temporal information to generate egocentric video sequences from the exocentric view. The proposed STA-GAN consists of three parts: temporal branch, spatial branch, and attention fusion. First, the temporal and spatial branches generate a sequence of fake frames and their corresponding features. The fake frames are generated in both downstream and upstream directions for both temporal and spatial branches. Next, the generated four different fake frames and their corresponding features (spatial and temporal branches in two directions) are fed into a novel multi-generation attention fusion module to produce the final video sequence. Meanwhile, we also propose a novel temporal and spatial dual-discriminator for more robust network optimization. Extensive experiments on the Side2Ego and Top2Ego datasets show that the proposed STA-GAN significantly outperforms the existing methods.



### Action Units Recognition Using Improved Pairwise Deep Architecture
- **Arxiv ID**: http://arxiv.org/abs/2107.03143v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03143v2)
- **Published**: 2021-07-07 11:10:13+00:00
- **Updated**: 2021-07-08 11:45:51+00:00
- **Authors**: Junya Saito, Xiaoyu Mi, Akiyoshi Uchida, Sachihiro Youoku, Takahisa Yamamoto, Kentaro Murase, Osafumi Nakayama
- **Comment**: None
- **Journal**: None
- **Summary**: Facial Action Units (AUs) represent a set of facial muscular activities and various combinations of AUs can represent a wide range of emotions. AU recognition is often used in many applications, including marketing, healthcare, education, and so forth. Although a lot of studies have developed various methods to improve recognition accuracy, it still remains a major challenge for AU recognition. In the Affective Behavior Analysis in-the-wild (ABAW) 2020 competition, we proposed a new automatic Action Units (AUs) recognition method using a pairwise deep architecture to derive the Pseudo-Intensities of each AU and then convert them into predicted intensities. This year, we introduced a new technique to last year's framework to further reduce AU recognition errors due to temporary face occlusion such as hands on face or large face orientation. We obtained a score of 0.65 in the validation data set for this year's competition.



### A Deep Residual Star Generative Adversarial Network for multi-domain Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2107.03145v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.03145v1)
- **Published**: 2021-07-07 11:15:17+00:00
- **Updated**: 2021-07-07 11:15:17+00:00
- **Authors**: Rao Muhammad Umer, Asad Munir, Christian Micheloni
- **Comment**: 5 pages, 6th International Conference on Smart and Sustainable
  Technologies 2021. arXiv admin note: text overlap with arXiv:2009.03693,
  arXiv:2005.00953
- **Journal**: None
- **Summary**: Recently, most of state-of-the-art single image super-resolution (SISR) methods have attained impressive performance by using deep convolutional neural networks (DCNNs). The existing SR methods have limited performance due to a fixed degradation settings, i.e. usually a bicubic downscaling of low-resolution (LR) image. However, in real-world settings, the LR degradation process is unknown which can be bicubic LR, bilinear LR, nearest-neighbor LR, or real LR. Therefore, most SR methods are ineffective and inefficient in handling more than one degradation settings within a single network. To handle the multiple degradation, i.e. refers to multi-domain image super-resolution, we propose a deep Super-Resolution Residual StarGAN (SR2*GAN), a novel and scalable approach that super-resolves the LR images for the multiple LR domains using only a single model. The proposed scheme is trained in a StarGAN like network topology with a single generator and discriminator networks. We demonstrate the effectiveness of our proposed approach in quantitative and qualitative experiments compared to other state-of-the-art methods.



### Mitigating Generation Shifts for Generalized Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.03163v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03163v1)
- **Published**: 2021-07-07 11:43:59+00:00
- **Updated**: 2021-07-07 11:43:59+00:00
- **Authors**: Zhi Chen, Yadan Luo, Sen Wang, Ruihong Qiu, Jingjing Li, Zi Huang
- **Comment**: ACM Multimedia 2021
- **Journal**: None
- **Summary**: Generalized Zero-Shot Learning (GZSL) is the task of leveraging semantic information (e.g., attributes) to recognize the seen and unseen samples, where unseen classes are not observable during training. It is natural to derive generative models and hallucinate training samples for unseen classes based on the knowledge learned from the seen samples. However, most of these models suffer from the `generation shifts', where the synthesized samples may drift from the real distribution of unseen data. In this paper, we conduct an in-depth analysis on this issue and propose a novel Generation Shifts Mitigating Flow (GSMFlow) framework, which is comprised of multiple conditional affine coupling layers for learning unseen data synthesis efficiently and effectively. In particular, we identify three potential problems that trigger the generation shifts, i.e., semantic inconsistency, variance decay, and structural permutation and address them respectively. First, to reinforce the correlations between the generated samples and the respective attributes, we explicitly embed the semantic information into the transformations in each of the coupling layers. Second, to recover the intrinsic variance of the synthesized unseen features, we introduce a visual perturbation strategy to diversify the intra-class variance of generated data and hereby help adjust the decision boundary of the classifier. Third, to avoid structural permutation in the semantic space, we propose a relative positioning strategy to manipulate the attribute embeddings, guiding which to fully preserve the inter-class geometric structure. Experimental results demonstrate that GSMFlow achieves state-of-the-art recognition performance in both conventional and generalized zero-shot settings. Our code is available at: https://github.com/uqzhichen/GSMFlow



### FBC-GAN: Diverse and Flexible Image Synthesis via Foreground-Background Composition
- **Arxiv ID**: http://arxiv.org/abs/2107.03166v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03166v1)
- **Published**: 2021-07-07 11:49:14+00:00
- **Updated**: 2021-07-07 11:49:14+00:00
- **Authors**: Kaiwen Cui, Gongjie Zhang, Fangneng Zhan, Jiaxing Huang, Shijian Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have become the de-facto standard in image synthesis. However, without considering the foreground-background decomposition, existing GANs tend to capture excessive content correlation between foreground and background, thus constraining the diversity in image generation. This paper presents a novel Foreground-Background Composition GAN (FBC-GAN) that performs image generation by generating foreground objects and background scenes concurrently and independently, followed by composing them with style and geometrical consistency. With this explicit design, FBC-GAN can generate images with foregrounds and backgrounds that are mutually independent in contents, thus lifting the undesirably learned content correlation constraint and achieving superior diversity. It also provides excellent flexibility by allowing the same foreground object with different background scenes, the same background scene with varying foreground objects, or the same foreground object and background scene with different object positions, sizes and poses. It can compose foreground objects and background scenes sampled from different datasets as well. Extensive experiments over multiple datasets show that FBC-GAN achieves competitive visual realism and superior diversity as compared with state-of-the-art methods.



### Trans4Trans: Efficient Transformer for Transparent Object Segmentation to Help Visually Impaired People Navigate in the Real World
- **Arxiv ID**: http://arxiv.org/abs/2107.03172v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.03172v2)
- **Published**: 2021-07-07 12:06:27+00:00
- **Updated**: 2021-08-20 15:00:45+00:00
- **Authors**: Jiaming Zhang, Kailun Yang, Angela Constantinescu, Kunyu Peng, Karin Müller, Rainer Stiefelhagen
- **Comment**: Accepted to ICCV 2021 ACVR Workshop. Code will be made publicly
  available at https://github.com/jamycheung/Trans4Trans
- **Journal**: None
- **Summary**: Common fully glazed facades and transparent objects present architectural barriers and impede the mobility of people with low vision or blindness, for instance, a path detected behind a glass door is inaccessible unless it is correctly perceived and reacted. However, segmenting these safety-critical objects is rarely covered by conventional assistive technologies. To tackle this issue, we construct a wearable system with a novel dual-head Transformer for Transparency (Trans4Trans) model, which is capable of segmenting general and transparent objects and performing real-time wayfinding to assist people walking alone more safely. Especially, both decoders created by our proposed Transformer Parsing Module (TPM) enable effective joint learning from different datasets. Besides, the efficient Trans4Trans model composed of symmetric transformer-based encoder and decoder, requires little computational expenses and is readily deployed on portable GPUs. Our Trans4Trans model outperforms state-of-the-art methods on the test sets of Stanford2D3D and Trans10K-v2 datasets and obtains mIoU of 45.13% and 75.14%, respectively. Through various pre-tests and a user study conducted in indoor and outdoor scenarios, the usability and reliability of our assistive system have been extensively verified.



### Deep Learning for Embodied Vision Navigation: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2108.04097v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.04097v4)
- **Published**: 2021-07-07 12:09:04+00:00
- **Updated**: 2021-10-11 08:48:18+00:00
- **Authors**: Fengda Zhu, Yi Zhu, Vincent CS Lee, Xiaodan Liang, Xiaojun Chang
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: "Embodied visual navigation" problem requires an agent to navigate in a 3D environment mainly rely on its first-person observation. This problem has attracted rising attention in recent years due to its wide application in autonomous driving, vacuum cleaner, and rescue robot. A navigation agent is supposed to have various intelligent skills, such as visual perceiving, mapping, planning, exploring and reasoning, etc. Building such an agent that observes, thinks, and acts is a key to real intelligence. The remarkable learning ability of deep learning methods empowered the agents to accomplish embodied visual navigation tasks. Despite this, embodied visual navigation is still in its infancy since a lot of advanced skills are required, including perceiving partially observed visual input, exploring unseen areas, memorizing and modeling seen scenarios, understanding cross-modal instructions, and adapting to a new environment, etc. Recently, embodied visual navigation has attracted rising attention of the community, and numerous works has been proposed to learn these skills. This paper attempts to establish an outline of the current works in the field of embodied visual navigation by providing a comprehensive literature survey. We summarize the benchmarks and metrics, review different methods, analysis the challenges, and highlight the state-of-the-art methods. Finally, we discuss unresolved challenges in the field of embodied visual navigation and give promising directions in pursuing future research.



### HIDA: Towards Holistic Indoor Understanding for the Visually Impaired via Semantic Instance Segmentation with a Wearable Solid-State LiDAR Sensor
- **Arxiv ID**: http://arxiv.org/abs/2107.03180v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.03180v1)
- **Published**: 2021-07-07 12:23:53+00:00
- **Updated**: 2021-07-07 12:23:53+00:00
- **Authors**: Huayao Liu, Ruiping Liu, Kailun Yang, Jiaming Zhang, Kunyu Peng, Rainer Stiefelhagen
- **Comment**: 10 figures, 5 tables
- **Journal**: None
- **Summary**: Independently exploring unknown spaces or finding objects in an indoor environment is a daily but challenging task for visually impaired people. However, common 2D assistive systems lack depth relationships between various objects, resulting in difficulty to obtain accurate spatial layout and relative positions of objects. To tackle these issues, we propose HIDA, a lightweight assistive system based on 3D point cloud instance segmentation with a solid-state LiDAR sensor, for holistic indoor detection and avoidance. Our entire system consists of three hardware components, two interactive functions~(obstacle avoidance and object finding) and a voice user interface. Based on voice guidance, the point cloud from the most recent state of the changing indoor environment is captured through an on-site scanning performed by the user. In addition, we design a point cloud segmentation model with dual lightweight decoders for semantic and offset predictions, which satisfies the efficiency of the whole system. After the 3D instance segmentation, we post-process the segmented point cloud by removing outliers and projecting all points onto a top-view 2D map representation. The system integrates the information above and interacts with users intuitively by acoustic feedback. The proposed 3D instance segmentation model has achieved state-of-the-art performance on ScanNet v2 dataset. Comprehensive field tests with various tasks in a user study verify the usability and effectiveness of our system for assisting visually impaired people in holistic indoor understanding, obstacle avoidance and object search.



### Urban Tree Species Classification Using Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/2107.03182v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.03182v1)
- **Published**: 2021-07-07 12:30:22+00:00
- **Updated**: 2021-07-07 12:30:22+00:00
- **Authors**: Emily Waters, Mahdi Maktabdar Oghaz, Lakshmi Babu Saheer
- **Comment**: International Conference on Machine Learning (ICML 2021), Workshop on
  Tackling Climate Change with Machine Learning
- **Journal**: None
- **Summary**: Urban trees help regulate temperature, reduce energy consumption, improve urban air quality, reduce wind speeds, and mitigating the urban heat island effect. Urban trees also play a key role in climate change mitigation and global warming by capturing and storing atmospheric carbon-dioxide which is the largest contributor to greenhouse gases. Automated tree detection and species classification using aerial imagery can be a powerful tool for sustainable forest and urban tree management. Hence, This study first offers a pipeline for generating labelled dataset of urban trees using Google Map's aerial images and then investigates how state of the art deep Convolutional Neural Network models such as VGG and ResNet handle the classification problem of urban tree aerial images under different parameters. Experimental results show our best model achieves an average accuracy of 60% over 6 tree species.



### eRAKI: Fast Robust Artificial neural networks for K-space Interpolation (RAKI) with Coil Combination and Joint Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2108.04218v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04218v1)
- **Published**: 2021-07-07 12:52:13+00:00
- **Updated**: 2021-07-07 12:52:13+00:00
- **Authors**: Heng Yu, Zijing Dong, Yamin Arefeen, Congyu Liao, Kawin Setsompop, Berkin Bilgic
- **Comment**: accepted by ISMRM2021 as an oral abstract
- **Journal**: None
- **Summary**: RAKI can perform database-free MRI reconstruction by training models using only auto-calibration signal (ACS) from each specific scan. As it trains a separate model for each individual coil, learning and inference with RAKI can be computationally prohibitive, particularly for large 3D datasets. In this abstract, we accelerate RAKI more than 200 times by directly learning a coil-combined target and further improve the reconstruction performance using joint reconstruction across multiple echoes together with an elliptical-CAIPI sampling approach. We further deploy these improvements in quantitative imaging and rapidly obtain T2 and T2* parameter maps from a fast EPTI scan.



### Hierarchical Semantic Segmentation using Psychometric Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.03212v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.03212v2)
- **Published**: 2021-07-07 13:38:33+00:00
- **Updated**: 2021-12-16 21:31:18+00:00
- **Authors**: Lu Yin, Vlado Menkovski, Shiwei Liu, Mykola Pechenizkiy
- **Comment**: Accepted by the 13th Asian Conference on Machine Learning (ACML
  2021), 17 pages, 12 figures
- **Journal**: None
- **Summary**: Assigning meaning to parts of image data is the goal of semantic image segmentation. Machine learning methods, specifically supervised learning is commonly used in a variety of tasks formulated as semantic segmentation. One of the major challenges in the supervised learning approaches is expressing and collecting the rich knowledge that experts have with respect to the meaning present in the image data. Towards this, typically a fixed set of labels is specified and experts are tasked with annotating the pixels, patches or segments in the images with the given labels. In general, however, the set of classes does not fully capture the rich semantic information present in the images. For example, in medical imaging such as histology images, the different parts of cells could be grouped and sub-grouped based on the expertise of the pathologist.   To achieve such a precise semantic representation of the concepts in the image, we need access to the full depth of knowledge of the annotator. In this work, we develop a novel approach to collect segmentation annotations from experts based on psychometric testing. Our method consists of the psychometric testing procedure, active query selection, query enhancement, and a deep metric learning model to achieve a patch-level image embedding that allows for semantic segmentation of images. We show the merits of our method with evaluation on the synthetically generated image, aerial image and histology image.



### FasterPose: A Faster Simple Baseline for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2107.03215v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.8
- **Links**: [PDF](http://arxiv.org/pdf/2107.03215v1)
- **Published**: 2021-07-07 13:39:08+00:00
- **Updated**: 2021-07-07 13:39:08+00:00
- **Authors**: Hanbin Dai, Hailin Shi, Wu Liu, Linfang Wang, Yinglu Liu, Tao Mei
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: The performance of human pose estimation depends on the spatial accuracy of keypoint localization. Most existing methods pursue the spatial accuracy through learning the high-resolution (HR) representation from input images. By the experimental analysis, we find that the HR representation leads to a sharp increase of computational cost, while the accuracy improvement remains marginal compared with the low-resolution (LR) representation. In this paper, we propose a design paradigm for cost-effective network with LR representation for efficient pose estimation, named FasterPose. Whereas the LR design largely shrinks the model complexity, yet how to effectively train the network with respect to the spatial accuracy is a concomitant challenge. We study the training behavior of FasterPose, and formulate a novel regressive cross-entropy (RCE) loss function for accelerating the convergence and promoting the accuracy. The RCE loss generalizes the ordinary cross-entropy loss from the binary supervision to a continuous range, thus the training of pose estimation network is able to benefit from the sigmoid function. By doing so, the output heatmap can be inferred from the LR features without loss of spatial accuracy, while the computational cost and model size has been significantly reduced. Compared with the previously dominant network of pose estimation, our method reduces 58% of the FLOPs and simultaneously gains 1.3% improvement of accuracy. Extensive experiments show that FasterPose yields promising results on the common benchmarks, i.e., COCO and MPII, consistently validating the effectiveness and efficiency for practical utilization, especially the low-latency and low-energy-budget applications in the non-GPU scenarios.



### MuVAM: A Multi-View Attention-based Model for Medical Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2107.03216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03216v1)
- **Published**: 2021-07-07 13:40:25+00:00
- **Updated**: 2021-07-07 13:40:25+00:00
- **Authors**: Haiwei Pan, Shuning He, Kejia Zhang, Bo Qu, Chunling Chen, Kun Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Medical Visual Question Answering (VQA) is a multi-modal challenging task widely considered by research communities of the computer vision and natural language processing. Since most current medical VQA models focus on visual content, ignoring the importance of text, this paper proposes a multi-view attention-based model(MuVAM) for medical visual question answering which integrates the high-level semantics of medical images on the basis of text description. Firstly, different methods are utilized to extract the features of the image and the question for the two modalities of vision and text. Secondly, this paper proposes a multi-view attention mechanism that include Image-to-Question (I2Q) attention and Word-to-Text (W2T) attention. Multi-view attention can correlate the question with image and word in order to better analyze the question and get an accurate answer. Thirdly, a composite loss is presented to predict the answer accurately after multi-modal feature fusion and improve the similarity between visual and textual cross-modal features. It consists of classification loss and image-question complementary (IQC) loss. Finally, for data errors and missing labels in the VQA-RAD dataset, we collaborate with medical experts to correct and complete this dataset and then construct an enhanced dataset, VQA-RADPh. The experiments on these two datasets show that the effectiveness of MuVAM surpasses the state-of-the-art method.



### Categorical Relation-Preserving Contrastive Knowledge Distillation for Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.03225v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03225v1)
- **Published**: 2021-07-07 13:56:38+00:00
- **Updated**: 2021-07-07 13:56:38+00:00
- **Authors**: Xiaohan Xing, Yuenan Hou, Hang Li, Yixuan Yuan, Hongsheng Li, Max Q. -H. Meng
- **Comment**: None
- **Journal**: None
- **Summary**: The amount of medical images for training deep classification models is typically very scarce, making these deep models prone to overfit the training data. Studies showed that knowledge distillation (KD), especially the mean-teacher framework which is more robust to perturbations, can help mitigate the over-fitting effect. However, directly transferring KD from computer vision to medical image classification yields inferior performance as medical images suffer from higher intra-class variance and class imbalance. To address these issues, we propose a novel Categorical Relation-preserving Contrastive Knowledge Distillation (CRCKD) algorithm, which takes the commonly used mean-teacher model as the supervisor. Specifically, we propose a novel Class-guided Contrastive Distillation (CCD) module to pull closer positive image pairs from the same class in the teacher and student models, while pushing apart negative image pairs from different classes. With this regularization, the feature distribution of the student model shows higher intra-class similarity and inter-class variance. Besides, we propose a Categorical Relation Preserving (CRP) loss to distill the teacher's relational knowledge in a robust and class-balanced manner. With the contribution of the CCD and CRP, our CRCKD algorithm can distill the relational knowledge more comprehensively. Extensive experiments on the HAM10000 and APTOS datasets demonstrate the superiority of the proposed CRCKD method.



### Scalable Data Balancing for Unlabeled Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2107.03227v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.03227v1)
- **Published**: 2021-07-07 13:58:15+00:00
- **Updated**: 2021-07-07 13:58:15+00:00
- **Authors**: Deep Patel, Erin Gao, Anirudh Koul, Siddha Ganju, Meher Anand Kasam
- **Comment**: Accepted to COSPAR 2021 Workshop on Machine Learning for Space
  Sciences. 5 pages, 9 figures
- **Journal**: None
- **Summary**: Data imbalance is a ubiquitous problem in machine learning. In large scale collected and annotated datasets, data imbalance is either mitigated manually by undersampling frequent classes and oversampling rare classes, or planned for with imputation and augmentation techniques. In both cases balancing data requires labels. In other words, only annotated data can be balanced. Collecting fully annotated datasets is challenging, especially for large scale satellite systems such as the unlabeled NASA's 35 PB Earth Imagery dataset. Although the NASA Earth Imagery dataset is unlabeled, there are implicit properties of the data source that we can rely on to hypothesize about its imbalance, such as distribution of land and water in the case of the Earth's imagery. We present a new iterative method to balance unlabeled data. Our method utilizes image embeddings as a proxy for image labels that can be used to balance data, and ultimately when trained increases overall accuracy.



### Introducing the structural bases of typicality effects in deep learning
- **Arxiv ID**: http://arxiv.org/abs/2107.03279v1
- **DOI**: 10.1016/j.imavis.2021.104249
- **Categories**: **cs.CV**, cs.AI, math.RT, 68T07 (Primary) 68Q55 (Secondary), I.2.4; I.2.6; I.2.10; I.4.8; I.4.10; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2107.03279v1)
- **Published**: 2021-07-07 15:15:43+00:00
- **Updated**: 2021-07-07 15:15:43+00:00
- **Authors**: Omar Vidal Pino, Erickson Rangel Nascimento, Mario Fernando Montenegro Campos
- **Comment**: 14 pages (12 + 2 reference); 13 Figures and 2 Tables. arXiv admin
  note: text overlap with arXiv:1906.03365
- **Journal**: None
- **Summary**: In this paper, we hypothesize that the effects of the degree of typicality in natural semantic categories can be generated based on the structure of artificial categories learned with deep learning models. Motivated by the human approach to representing natural semantic categories and based on the Prototype Theory foundations, we propose a novel Computational Prototype Model (CPM) to represent the internal structure of semantic categories. Unlike other prototype learning approaches, our mathematical framework proposes a first approach to provide deep neural networks with the ability to model abstract semantic concepts such as category central semantic meaning, typicality degree of an object's image, and family resemblance relationship. We proposed several methodologies based on the typicality's concept to evaluate our CPM-model in image semantic processing tasks such as image classification, a global semantic description, and transfer learning. Our experiments on different image datasets, such as ImageNet and Coco, showed that our approach might be an admissible proposition in the effort to endow machines with greater power of abstraction for the semantic representation of objects' categories.



### Bone Surface Reconstruction and Clinical Features Estimation from Sparse Landmarks and Statistical Shape Models: A feasibility study on the femur
- **Arxiv ID**: http://arxiv.org/abs/2107.03292v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2107.03292v1)
- **Published**: 2021-07-07 15:27:30+00:00
- **Updated**: 2021-07-07 15:27:30+00:00
- **Authors**: Alireza Asvadi, Guillaume Dardenne, Jocelyne Troccaz, Valerie Burdin
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, we investigated a method allowing the determination of the femur bone surface as well as its mechanical axis from some easy-to-identify bony landmarks. The reconstruction of the whole femur is therefore performed from these landmarks using a Statistical Shape Model (SSM). The aim of this research is therefore to assess the impact of the number, the position, and the accuracy of the landmarks for the reconstruction of the femur and the determination of its related mechanical axis, an important clinical parameter to consider for the lower limb analysis. Two statistical femur models were created from our in-house dataset and a publicly available dataset. Both were evaluated in terms of average point-to-point surface distance error and through the mechanical axis of the femur. Furthermore, the clinical impact of using landmarks on the skin in replacement of bony landmarks is investigated. The predicted proximal femurs from bony landmarks were more accurate compared to on-skin landmarks while both had less than 3.5 degrees mechanical axis angle deviation error. The results regarding the non-invasive determination of the mechanical axis are very encouraging and could open very interesting clinical perspectives for the analysis of the lower limb either for orthopedics or functional rehabilitation.



### Predicting with Confidence on Unseen Distributions
- **Arxiv ID**: http://arxiv.org/abs/2107.03315v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2107.03315v2)
- **Published**: 2021-07-07 15:50:18+00:00
- **Updated**: 2021-08-19 20:26:13+00:00
- **Authors**: Devin Guillory, Vaishaal Shankar, Sayna Ebrahimi, Trevor Darrell, Ludwig Schmidt
- **Comment**: ICCV Camera ready; new scatter plots in supplementary material
- **Journal**: None
- **Summary**: Recent work has shown that the performance of machine learning models can vary substantially when models are evaluated on data drawn from a distribution that is close to but different from the training distribution. As a result, predicting model performance on unseen distributions is an important challenge. Our work connects techniques from domain adaptation and predictive uncertainty literature, and allows us to predict model accuracy on challenging unseen distributions without access to labeled data. In the context of distribution shift, distributional distances are often used to adapt models and improve their performance on new domains, however accuracy estimation, or other forms of predictive uncertainty, are often neglected in these investigations. Through investigating a wide range of established distributional distances, such as Frechet distance or Maximum Mean Discrepancy, we determine that they fail to induce reliable estimates of performance under distribution shift. On the other hand, we find that the difference of confidences (DoC) of a classifier's predictions successfully estimates the classifier's performance change over a variety of shifts. We specifically investigate the distinction between synthetic and natural distribution shifts and observe that despite its simplicity DoC consistently outperforms other quantifications of distributional difference. $DoC$ reduces predictive error by almost half ($46\%$) on several realistic and challenging distribution shifts, e.g., on the ImageNet-Vid-Robust and ImageNet-Rendition datasets.



### AGD-Autoencoder: Attention Gated Deep Convolutional Autoencoder for Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.03323v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.03323v1)
- **Published**: 2021-07-07 16:01:24+00:00
- **Updated**: 2021-07-07 16:01:24+00:00
- **Authors**: Tim Cvetko
- **Comment**: 8 pages, 2 figures
- **Journal**: None
- **Summary**: Brain tumor segmentation is a challenging problem in medical image analysis. The endpoint is to generate the salient masks that accurately identify brain tumor regions in an fMRI screening. In this paper, we propose a novel attention gate (AG model) for brain tumor segmentation that utilizes both the edge detecting unit and the attention gated network to highlight and segment the salient regions from fMRI images. This feature enables us to eliminate the necessity of having to explicitly point towards the damaged area(external tissue localization) and classify(classification) as per classical computer vision techniques. AGs can easily be integrated within the deep convolutional neural networks(CNNs). Minimal computional overhead is required while the AGs increase the sensitivity scores significantly. We show that the edge detector along with an attention gated mechanism provide a sufficient enough method for brain segmentation reaching an IOU of 0.78



### KOALA: A Kalman Optimization Algorithm with Loss Adaptivity
- **Arxiv ID**: http://arxiv.org/abs/2107.03331v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2107.03331v2)
- **Published**: 2021-07-07 16:13:57+00:00
- **Updated**: 2021-12-16 13:11:53+00:00
- **Authors**: Aram Davtyan, Sepehr Sameni, Llukman Cerkezi, Givi Meishvilli, Adam Bielski, Paolo Favaro
- **Comment**: Accepted to AAAI2022
- **Journal**: None
- **Summary**: Optimization is often cast as a deterministic problem, where the solution is found through some iterative procedure such as gradient descent. However, when training neural networks the loss function changes over (iteration) time due to the randomized selection of a subset of the samples. This randomization turns the optimization problem into a stochastic one. We propose to consider the loss as a noisy observation with respect to some reference optimum. This interpretation of the loss allows us to adopt Kalman filtering as an optimizer, as its recursive formulation is designed to estimate unknown parameters from noisy measurements. Moreover, we show that the Kalman Filter dynamical model for the evolution of the unknown parameters can be used to capture the gradient dynamics of advanced methods such as Momentum and Adam. We call this stochastic optimization method KOALA, which is short for Kalman Optimization Algorithm with Loss Adaptivity. KOALA is an easy to implement, scalable, and efficient method to train neural networks. We provide convergence analysis and show experimentally that it yields parameter estimates that are on par with or better than existing state of the art optimization algorithms across several neural network architectures and machine learning tasks, such as computer vision and language modeling.



### SimCC: a Simple Coordinate Classification Perspective for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2107.03332v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03332v3)
- **Published**: 2021-07-07 16:20:12+00:00
- **Updated**: 2022-07-05 16:13:26+00:00
- **Authors**: Yanjie Li, Sen Yang, Peidong Liu, Shoukui Zhang, Yunxiao Wang, Zhicheng Wang, Wankou Yang, Shu-Tao Xia
- **Comment**: None
- **Journal**: None
- **Summary**: The 2D heatmap-based approaches have dominated Human Pose Estimation (HPE) for years due to high performance. However, the long-standing quantization error problem in the 2D heatmap-based methods leads to several well-known drawbacks: 1) The performance for the low-resolution inputs is limited; 2) To improve the feature map resolution for higher localization precision, multiple costly upsampling layers are required; 3) Extra post-processing is adopted to reduce the quantization error. To address these issues, we aim to explore a brand new scheme, called \textit{SimCC}, which reformulates HPE as two classification tasks for horizontal and vertical coordinates. The proposed SimCC uniformly divides each pixel into several bins, thus achieving \emph{sub-pixel} localization precision and low quantization error. Benefiting from that, SimCC can omit additional refinement post-processing and exclude upsampling layers under certain settings, resulting in a more simple and effective pipeline for HPE. Extensive experiments conducted over COCO, CrowdPose, and MPII datasets show that SimCC outperforms heatmap-based counterparts, especially in low-resolution settings by a large margin.



### Samplets: A new paradigm for data compression
- **Arxiv ID**: http://arxiv.org/abs/2107.03337v3
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.LG, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/2107.03337v3)
- **Published**: 2021-07-07 16:25:12+00:00
- **Updated**: 2021-11-16 15:30:56+00:00
- **Authors**: Helmut Harbrecht, Michael Multerer
- **Comment**: None
- **Journal**: None
- **Summary**: In this article, we introduce the concept of samplets by transferring the construction of Tausch-White wavelets to the realm of data. This way we obtain a multilevel representation of discrete data which directly enables data compression, detection of singularities and adaptivity. Applying samplets to represent kernel matrices, as they arise in kernel based learning or Gaussian process regression, we end up with quasi-sparse matrices. By thresholding small entries, these matrices are compressible to O(N log N) relevant entries, where N is the number of data points. This feature allows for the use of fill-in reducing reorderings to obtain a sparse factorization of the compressed matrices. Besides the comprehensive introduction to samplets and their properties, we present extensive numerical studies to benchmark the approach. Our results demonstrate that samplets mark a considerable step in the direction of making large data sets accessible for analysis.



### IntraLoss: Further Margin via Gradient-Enhancing Term for Deep Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.03352v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03352v1)
- **Published**: 2021-07-07 16:53:45+00:00
- **Updated**: 2021-07-07 16:53:45+00:00
- **Authors**: Chengzhi Jiang, Yanzhou Su, Wen Wang, Haiwei Bai, Haijun Liu, Jian Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Existing classification-based face recognition methods have achieved remarkable progress, introducing large margin into hypersphere manifold to learn discriminative facial representations. However, the feature distribution is ignored. Poor feature distribution will wipe out the performance improvement brought about by margin scheme. Recent studies focus on the unbalanced inter-class distribution and form a equidistributed feature representations by penalizing the angle between identity and its nearest neighbor. But the problem is more than that, we also found the anisotropy of intra-class distribution. In this paper, we propose the `gradient-enhancing term' that concentrates on the distribution characteristics within the class. This method, named IntraLoss, explicitly performs gradient enhancement in the anisotropic region so that the intra-class distribution continues to shrink, resulting in isotropic and more compact intra-class distribution and further margin between identities. The experimental results on LFW, YTF and CFP-FP show that our outperforms state-of-the-art methods by gradient enhancement, demonstrating the superiority of our method. In addition, our method has intuitive geometric interpretation and can be easily combined with existing methods to solve the previously ignored problems.



### Novel Visual Category Discovery with Dual Ranking Statistics and Mutual Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2107.03358v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03358v2)
- **Published**: 2021-07-07 17:14:40+00:00
- **Updated**: 2022-01-03 06:53:51+00:00
- **Authors**: Bingchen Zhao, Kai Han
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of novel visual category discovery, i.e., grouping unlabelled images from new classes into different semantic partitions by leveraging a labelled dataset that contains images from other different but relevant categories. This is a more realistic and challenging setting than conventional semi-supervised learning. We propose a two-branch learning framework for this problem, with one branch focusing on local part-level information and the other branch focusing on overall characteristics. To transfer knowledge from the labelled data to the unlabelled, we propose using dual ranking statistics on both branches to generate pseudo labels for training on the unlabelled data. We further introduce a mutual knowledge distillation method to allow information exchange and encourage agreement between the two branches for discovering new categories, allowing our model to enjoy the benefits of global and local features. We comprehensively evaluate our method on public benchmarks for generic object classification, as well as the more challenging datasets for fine-grained visual recognition, achieving state-of-the-art performance.



### Partial 3D Object Retrieval using Local Binary QUICCI Descriptors and Dissimilarity Tree Indexing
- **Arxiv ID**: http://arxiv.org/abs/2107.03368v1
- **DOI**: 10.1016/j.cag.2021.07.018
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03368v1)
- **Published**: 2021-07-07 17:30:47+00:00
- **Updated**: 2021-07-07 17:30:47+00:00
- **Authors**: Bart Iver van Blokland, Theoharis Theoharis
- **Comment**: 19 pages, 17 figures, to be published in Computers & Graphics
- **Journal**: None
- **Summary**: A complete pipeline is presented for accurate and efficient partial 3D object retrieval based on Quick Intersection Count Change Image (QUICCI) binary local descriptors and a novel indexing tree. It is shown how a modification to the QUICCI query descriptor makes it ideal for partial retrieval. An indexing structure called Dissimilarity Tree is proposed which can significantly accelerate searching the large space of local descriptors; this is applicable to QUICCI and other binary descriptors. The index exploits the distribution of bits within descriptors for efficient retrieval. The retrieval pipeline is tested on the artificial part of SHREC'16 dataset with near-ideal retrieval results.



### Differentiable Architecture Pruning for Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.03375v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2107.03375v1)
- **Published**: 2021-07-07 17:44:59+00:00
- **Updated**: 2021-07-07 17:44:59+00:00
- **Authors**: Nicolo Colombo, Yang Gao
- **Comment**: 19 pages (main + appendix), 7 figures and 1 table, Workshop @ ICML
  2021, 24th July 2021
- **Journal**: None
- **Summary**: We propose a new gradient-based approach for extracting sub-architectures from a given large model. Contrarily to existing pruning methods, which are unable to disentangle the network architecture and the corresponding weights, our architecture-pruning scheme produces transferable new structures that can be successfully retrained to solve different tasks. We focus on a transfer-learning setup where architectures can be trained on a large data set but very few data points are available for fine-tuning them on new tasks. We define a new gradient-based algorithm that trains architectures of arbitrarily low complexity independently from the attached weights. Given a search space defined by an existing large neural model, we reformulate the architecture search task as a complexity-penalized subset-selection problem and solve it through a two-temperature relaxation scheme. We provide theoretical convergence guarantees and validate the proposed transfer-learning strategy on real data.



### Pragmatic Image Compression for Human-in-the-Loop Decision-Making
- **Arxiv ID**: http://arxiv.org/abs/2108.04219v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04219v1)
- **Published**: 2021-07-07 17:45:53+00:00
- **Updated**: 2021-07-07 17:45:53+00:00
- **Authors**: Siddharth Reddy, Anca D. Dragan, Sergey Levine
- **Comment**: None
- **Journal**: None
- **Summary**: Standard lossy image compression algorithms aim to preserve an image's appearance, while minimizing the number of bits needed to transmit it. However, the amount of information actually needed by a user for downstream tasks -- e.g., deciding which product to click on in a shopping website -- is likely much lower. To achieve this lower bitrate, we would ideally only transmit the visual features that drive user behavior, while discarding details irrelevant to the user's decisions. We approach this problem by training a compression model through human-in-the-loop learning as the user performs tasks with the compressed images. The key insight is to train the model to produce a compressed image that induces the user to take the same action that they would have taken had they seen the original image. To approximate the loss function for this model, we train a discriminator that tries to distinguish whether a user's action was taken in response to the compressed image or the original. We evaluate our method through experiments with human participants on four tasks: reading handwritten digits, verifying photos of faces, browsing an online shopping catalogue, and playing a car racing video game. The results show that our method learns to match the user's actions with and without compression at lower bitrates than baseline methods, and adapts the compression model to the user's behavior: it preserves the digit number and randomizes handwriting style in the digit reading task, preserves hats and eyeglasses while randomizing faces in the photo verification task, preserves the perceived price of an item while randomizing its color and background in the online shopping task, and preserves upcoming bends in the road in the car racing game.



### Long Short-Term Transformer for Online Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.03377v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03377v3)
- **Published**: 2021-07-07 17:49:51+00:00
- **Updated**: 2021-12-22 18:01:50+00:00
- **Authors**: Mingze Xu, Yuanjun Xiong, Hao Chen, Xinyu Li, Wei Xia, Zhuowen Tu, Stefano Soatto
- **Comment**: NeurIPS 2021 Spotlight
- **Journal**: None
- **Summary**: We present Long Short-term TRansformer (LSTR), a temporal modeling algorithm for online action detection, which employs a long- and short-term memory mechanism to model prolonged sequence data. It consists of an LSTR encoder that dynamically leverages coarse-scale historical information from an extended temporal window (e.g., 2048 frames spanning of up to 8 minutes), together with an LSTR decoder that focuses on a short time window (e.g., 32 frames spanning 8 seconds) to model the fine-scale characteristics of the data. Compared to prior work, LSTR provides an effective and efficient method to model long videos with fewer heuristics, which is validated by extensive empirical analysis. LSTR achieves state-of-the-art performance on three standard online action detection benchmarks, THUMOS'14, TVSeries, and HACS Segment. Code has been made available at: https://xumingze0308.github.io/projects/lstr



### Tensor Methods in Computer Vision and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.03436v1
- **DOI**: 10.1109/JPROC.2021.3074329
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03436v1)
- **Published**: 2021-07-07 18:42:45+00:00
- **Updated**: 2021-07-07 18:42:45+00:00
- **Authors**: Yannis Panagakis, Jean Kossaifi, Grigorios G. Chrysos, James Oldfield, Mihalis A. Nicolaou, Anima Anandkumar, Stefanos Zafeiriou
- **Comment**: Proceedings of the IEEE (2021)
- **Journal**: None
- **Summary**: Tensors, or multidimensional arrays, are data structures that can naturally represent visual data of multiple dimensions. Inherently able to efficiently capture structured, latent semantic spaces and high-order interactions, tensors have a long history of applications in a wide span of computer vision problems. With the advent of the deep learning paradigm shift in computer vision, tensors have become even more fundamental. Indeed, essential ingredients in modern deep learning architectures, such as convolutions and attention mechanisms, can readily be considered as tensor mappings. In effect, tensor methods are increasingly finding significant applications in deep learning, including the design of memory and compute efficient network architectures, improving robustness to random noise and adversarial attacks, and aiding the theoretical understanding of deep networks.   This article provides an in-depth and practical review of tensors and tensor methods in the context of representation learning and deep learning, with a particular focus on visual data analysis and computer vision applications. Concretely, besides fundamental work in tensor-based visual data analysis methods, we focus on recent developments that have brought on a gradual increase of tensor methods, especially in deep learning architectures, and their implications in computer vision applications. To further enable the newcomer to grasp such concepts quickly, we provide companion Python notebooks, covering key aspects of the paper and implementing them, step-by-step with TensorLy.



### LanguageRefer: Spatial-Language Model for 3D Visual Grounding
- **Arxiv ID**: http://arxiv.org/abs/2107.03438v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.03438v3)
- **Published**: 2021-07-07 18:55:03+00:00
- **Updated**: 2021-11-04 19:38:48+00:00
- **Authors**: Junha Roh, Karthik Desingh, Ali Farhadi, Dieter Fox
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: For robots to understand human instructions and perform meaningful tasks in the near future, it is important to develop learned models that comprehend referential language to identify common objects in real-world 3D scenes. In this paper, we introduce a spatial-language model for a 3D visual grounding problem. Specifically, given a reconstructed 3D scene in the form of point clouds with 3D bounding boxes of potential object candidates, and a language utterance referring to a target object in the scene, our model successfully identifies the target object from a set of potential candidates. Specifically, LanguageRefer uses a transformer-based architecture that combines spatial embedding from bounding boxes with fine-tuned language embeddings from DistilBert to predict the target object. We show that it performs competitively on visio-linguistic datasets proposed by ReferIt3D. Further, we analyze its spatial reasoning task performance decoupled from perception noise, the accuracy of view-dependent utterances, and viewpoint annotations for potential robotics applications.



### Modality Completion via Gaussian Process Prior Variational Autoencoders for Multi-Modal Glioma Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.03442v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.03442v1)
- **Published**: 2021-07-07 19:06:34+00:00
- **Updated**: 2021-07-07 19:06:34+00:00
- **Authors**: Mohammad Hamghalam, Alejandro F. Frangi, Baiying Lei, Amber L. Simpson
- **Comment**: Accepted in MICCAI 2021
- **Journal**: None
- **Summary**: In large studies involving multi protocol Magnetic Resonance Imaging (MRI), it can occur to miss one or more sub-modalities for a given patient owing to poor quality (e.g. imaging artifacts), failed acquisitions, or hallway interrupted imaging examinations. In some cases, certain protocols are unavailable due to limited scan time or to retrospectively harmonise the imaging protocols of two independent studies. Missing image modalities pose a challenge to segmentation frameworks as complementary information contributed by the missing scans is then lost. In this paper, we propose a novel model, Multi-modal Gaussian Process Prior Variational Autoencoder (MGP-VAE), to impute one or more missing sub-modalities for a patient scan. MGP-VAE can leverage the Gaussian Process (GP) prior on the Variational Autoencoder (VAE) to utilize the subjects/patients and sub-modalities correlations. Instead of designing one network for each possible subset of present sub-modalities or using frameworks to mix feature maps, missing data can be generated from a single model based on all the available samples. We show the applicability of MGP-VAE on brain tumor segmentation where either, two, or three of four sub-modalities may be missing. Our experiments against competitive segmentation baselines with missing sub-modality on BraTS'19 dataset indicate the effectiveness of the MGP-VAE model for segmentation tasks.



### $S^3$: Sign-Sparse-Shift Reparametrization for Effective Training of Low-bit Shift Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.03453v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.03453v2)
- **Published**: 2021-07-07 19:33:02+00:00
- **Updated**: 2021-12-06 20:13:03+00:00
- **Authors**: Xinlin Li, Bang Liu, Yaoliang Yu, Wulong Liu, Chunjing Xu, Vahid Partovi Nia
- **Comment**: None
- **Journal**: None
- **Summary**: Shift neural networks reduce computation complexity by removing expensive multiplication operations and quantizing continuous weights into low-bit discrete values, which are fast and energy efficient compared to conventional neural networks. However, existing shift networks are sensitive to the weight initialization, and also yield a degraded performance caused by vanishing gradient and weight sign freezing problem. To address these issues, we propose S low-bit re-parameterization, a novel technique for training low-bit shift networks. Our method decomposes a discrete parameter in a sign-sparse-shift 3-fold manner. In this way, it efficiently learns a low-bit network with a weight dynamics similar to full-precision networks and insensitive to weight initialization. Our proposed training method pushes the boundaries of shift neural networks and shows 3-bit shift networks out-performs their full-precision counterparts in terms of top-1 accuracy on ImageNet.



### Comparing Machine Learning based Segmentation Models on Jet Fire Radiation Zones
- **Arxiv ID**: http://arxiv.org/abs/2107.03461v3
- **DOI**: 10.1007/978-3-030-89817-5_12
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03461v3)
- **Published**: 2021-07-07 19:52:52+00:00
- **Updated**: 2021-11-01 23:22:53+00:00
- **Authors**: Carmina Pérez-Guerrero, Adriana Palacios, Gilberto Ochoa-Ruiz, Christian Mata, Miguel Gonzalez-Mendoza, Luis Eduardo Falcón-Morales
- **Comment**: None
- **Journal**: Advances in Computational Intelligence. MICAI 2021. Lecture Notes
  in Computer Science, 13067 (2021), 161-172
- **Summary**: Risk assessment is relevant in any workplace, however there is a degree of unpredictability when dealing with flammable or hazardous materials so that detection of fire accidents by itself may not be enough. An example of this is the impingement of jet fires, where the heat fluxes of the flame could reach nearby equipment and dramatically increase the probability of a domino effect with catastrophic results. Because of this, the characterization of such fire accidents is important from a risk management point of view. One such characterization would be the segmentation of different radiation zones within the flame, so this paper presents an exploratory research regarding several traditional computer vision and Deep Learning segmentation approaches to solve this specific problem. A data set of propane jet fires is used to train and evaluate the different approaches and given the difference in the distribution of the zones and background of the images, different loss functions, that seek to alleviate data imbalance, are also explored. Additionally, different metrics are correlated to a manual ranking performed by experts to make an evaluation that closely resembles the expert's criteria. The Hausdorff Distance and Adjusted Random Index were the metrics with the highest correlation and the best results were obtained from the UNet architecture with a Weighted Cross-Entropy Loss. These results can be used in future research to extract more geometric information from the segmentation masks or could even be implemented on other types of fire accidents.



### An audiovisual and contextual approach for categorical and continuous emotion recognition in-the-wild
- **Arxiv ID**: http://arxiv.org/abs/2107.03465v3
- **DOI**: 10.1109/ICCVW54120.2021.00407
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.03465v3)
- **Published**: 2021-07-07 20:13:17+00:00
- **Updated**: 2021-08-13 16:39:08+00:00
- **Authors**: Panagiotis Antoniadis, Ioannis Pikoulis, Panagiotis P. Filntisis, Petros Maragos
- **Comment**: 7 pages, 1 figure, 3 tables, accepted to the 2nd Workshop and
  Competition on Affective Behavior Analysis In-the-Wild (ABAW2)
- **Journal**: 2021 IEEE/CVF International Conference on Computer Vision
  Workshops (ICCVW)
- **Summary**: In this work we tackle the task of video-based audio-visual emotion recognition, within the premises of the 2nd Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW2). Poor illumination conditions, head/body orientation and low image resolution constitute factors that can potentially hinder performance in case of methodologies that solely rely on the extraction and analysis of facial features. In order to alleviate this problem, we leverage both bodily and contextual features, as part of a broader emotion recognition framework. We choose to use a standard CNN-RNN cascade as the backbone of our proposed model for sequence-to-sequence (seq2seq) learning. Apart from learning through the RGB input modality, we construct an aural stream which operates on sequences of extracted mel-spectrograms. Our extensive experiments on the challenging and newly assembled Aff-Wild2 dataset verify the validity of our intuitive multi-stream and multi-modal approach towards emotion recognition in-the-wild. Emphasis is being laid on the the beneficial influence of the human body and scene context, as aspects of the emotion recognition process that have been left relatively unexplored up to this point. All the code was implemented using PyTorch and is publicly available.



