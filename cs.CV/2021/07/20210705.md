# Arxiv Papers in cs.CV on 2021-07-05
### Continual Contrastive Learning for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.01776v4
- **DOI**: 10.1109/ICME52920.2022.9859995
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.01776v4)
- **Published**: 2021-07-05 03:53:42+00:00
- **Updated**: 2022-11-02 08:42:10+00:00
- **Authors**: Zhiwei Lin, Yongtao Wang, Hongxiang Lin
- **Comment**: Accepted in ICME2022
- **Journal**: None
- **Summary**: Recently, self-supervised representation learning gives further development in multimedia technology. Most existing self-supervised learning methods are applicable to packaged data. However, when it comes to streamed data, they are suffering from a catastrophic forgetting problem, which is not studied extensively. In this paper, we make the first attempt to tackle the catastrophic forgetting problem in the mainstream self-supervised methods, i.e., contrastive learning methods. Specifically, we first develop a rehearsal-based framework combined with a novel sampling strategy and a self-supervised knowledge distillation to transfer information over time efficiently. Then, we propose an extra sample queue to help the network separate the feature representations of old and new data in the embedding space. Experimental results show that compared with the naive self-supervised baseline, which learns tasks one by one without taking any technique, we improve the image classification accuracy by 1.60% on CIFAR-100, 2.86% on ImageNet-Sub, and 1.29% on ImageNet-Full under 10 incremental steps setting. Our code will be available at https://github.com/VDIGPKU/ContinualContrastiveLearning.



### Depth Quality-Inspired Feature Manipulation for Efficient RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.01779v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.01779v2)
- **Published**: 2021-07-05 04:03:02+00:00
- **Updated**: 2021-07-06 05:07:19+00:00
- **Authors**: Wenbo Zhang, Ge-Peng Ji, Zhuo Wang, Keren Fu, Qijun Zhao
- **Comment**: accepted in ACM MM 2021
- **Journal**: None
- **Summary**: RGB-D salient object detection (SOD) recently has attracted increasing research interest by benefiting conventional RGB SOD with extra depth information. However, existing RGB-D SOD models often fail to perform well in terms of both efficiency and accuracy, which hinders their potential applications on mobile devices and real-world problems. An underlying challenge is that the model accuracy usually degrades when the model is simplified to have few parameters. To tackle this dilemma and also inspired by the fact that depth quality is a key factor influencing the accuracy, we propose a novel depth quality-inspired feature manipulation (DQFM) process, which is efficient itself and can serve as a gating mechanism for filtering depth features to greatly boost the accuracy. DQFM resorts to the alignment of low-level RGB and depth features, as well as holistic attention of the depth stream to explicitly control and enhance cross-modal fusion. We embed DQFM to obtain an efficient light-weight model called DFM-Net, where we also design a tailored depth backbone and a two-stage decoder for further efficiency consideration. Extensive experimental results demonstrate that our DFM-Net achieves state-of-the-art accuracy when comparing to existing non-efficient models, and meanwhile runs at 140ms on CPU (2.2$\times$ faster than the prior fastest efficient model) with only $\sim$8.5Mb model size (14.9% of the prior lightest). Our code will be available at https://github.com/zwbx/DFM-Net.



### A contextual analysis of multi-layer perceptron models in classifying hand-written digits and letters: limited resources
- **Arxiv ID**: http://arxiv.org/abs/2107.01782v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.01782v1)
- **Published**: 2021-07-05 04:30:37+00:00
- **Updated**: 2021-07-05 04:30:37+00:00
- **Authors**: Tidor-Vlad Pricope
- **Comment**: None
- **Journal**: None
- **Summary**: Classifying hand-written digits and letters has taken a big leap with the introduction of ConvNets. However, on very constrained hardware the time necessary to train such models would be high. Our main contribution is twofold. First, we extensively test an end-to-end vanilla neural network (MLP) approach in pure numpy without any pre-processing or feature extraction done beforehand. Second, we show that basic data mining operations can significantly improve the performance of the models in terms of computational time, without sacrificing much accuracy. We illustrate our claims on a simpler variant of the Extended MNIST dataset, called Balanced EMNIST dataset. Our experiments show that, without any data mining, we get increased generalization performance when using more hidden layers and regularization techniques, the best model achieving 84.83% accuracy on a test dataset. Using dimensionality reduction done by PCA we were able to increase that figure to 85.08% with only 10% of the original feature space, reducing the memory size needed by 64%. Finally, adding methods to remove possibly harmful training samples like deviation from the mean helped us to still achieve over 84% test accuracy but with only 32.8% of the original memory size for the training set. This compares favorably to the majority of literature results obtained through similar architectures. Although this approach gets outshined by state-of-the-art models, it does scale to some (AlexNet, VGGNet) trained on 50% of the same dataset.



### Learning a Model for Inferring a Spatial Road Lane Network Graph using Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2107.01784v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, I.2.10; I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2107.01784v1)
- **Published**: 2021-07-05 04:34:51+00:00
- **Updated**: 2021-07-05 04:34:51+00:00
- **Authors**: Robin Karlsson, David Robert Wong, Simon Thompson, Kazuya Takeda
- **Comment**: Accepted for IEEE ITSC 2021
- **Journal**: None
- **Summary**: Interconnected road lanes are a central concept for navigating urban roads. Currently, most autonomous vehicles rely on preconstructed lane maps as designing an algorithmic model is difficult. However, the generation and maintenance of such maps is costly and hinders large-scale adoption of autonomous vehicle technology. This paper presents the first self-supervised learning method to train a model to infer a spatially grounded lane-level road network graph based on a dense segmented representation of the road scene generated from onboard sensors. A formal road lane network model is presented and proves that any structured road scene can be represented by a directed acyclic graph of at most depth three while retaining the notion of intersection regions, and that this is the most compressed representation. The formal model is implemented by a hybrid neural and search-based model, utilizing a novel barrier function loss formulation for robust learning from partial labels. Experiments are conducted for all common road intersection layouts. Results show that the model can generalize to new road layouts, unlike previous approaches, demonstrating its potential for real-world application as a practical learning-based lane-level map generator.



### Multi-View Correlation Distillation for Incremental Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.01787v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.01787v1)
- **Published**: 2021-07-05 04:36:33+00:00
- **Updated**: 2021-07-05 04:36:33+00:00
- **Authors**: Dongbao Yang, Yu Zhou, Weiping Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In real applications, new object classes often emerge after the detection model has been trained on a prepared dataset with fixed classes. Due to the storage burden and the privacy of old data, sometimes it is impractical to train the model from scratch with both old and new data. Fine-tuning the old model with only new data will lead to a well-known phenomenon of catastrophic forgetting, which severely degrades the performance of modern object detectors. In this paper, we propose a novel \textbf{M}ulti-\textbf{V}iew \textbf{C}orrelation \textbf{D}istillation (MVCD) based incremental object detection method, which explores the correlations in the feature space of the two-stage object detector (Faster R-CNN). To better transfer the knowledge learned from the old classes and maintain the ability to learn new classes, we design correlation distillation losses from channel-wise, point-wise and instance-wise views to regularize the learning of the incremental model. A new metric named Stability-Plasticity-mAP is proposed to better evaluate both the stability for old classes and the plasticity for new classes in incremental object detection. The extensive experiments conducted on VOC2007 and COCO demonstrate that MVCD can effectively learn to detect objects of new classes and mitigate the problem of catastrophic forgetting.



### Web-Scale Generic Object Detection at Microsoft Bing
- **Arxiv ID**: http://arxiv.org/abs/2107.01814v1
- **DOI**: 10.1145/3447548.3467122
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.01814v1)
- **Published**: 2021-07-05 06:46:09+00:00
- **Updated**: 2021-07-05 06:46:09+00:00
- **Authors**: Stephen Xi Chen, Saurajit Mukherjee, Unmesh Phadke, Tingting Wang, Junwon Park, Ravi Theja Yada
- **Comment**: In Proceedings of the 27th ACM SIGKDD International Conference on
  Knowledge Discovery & Data Mining (KDD) 2021, Virtual Event, Singapore
- **Journal**: None
- **Summary**: In this paper, we present Generic Object Detection (GenOD), one of the largest object detection systems deployed to a web-scale general visual search engine that can detect over 900 categories for all Microsoft Bing Visual Search queries in near real-time. It acts as a fundamental visual query understanding service that provides object-centric information and shows gains in multiple production scenarios, improving upon domain-specific models. We discuss the challenges of collecting data, training, deploying and updating such a large-scale object detection model with multiple dependencies. We discuss a data collection pipeline that reduces per-bounding box labeling cost by 81.5% and latency by 61.2% while improving on annotation quality. We show that GenOD can improve weighted average precision by over 20% compared to multiple domain-specific models. We also improve the model update agility by nearly 2 times with the proposed disjoint detector training compared to joint fine-tuning. Finally we demonstrate how GenOD benefits visual search applications by significantly improving object-level search relevance by 54.9% and user engagement by 59.9%.



### Exploring Data Pipelines through the Process Lens: a Reference Model forComputer Vision
- **Arxiv ID**: http://arxiv.org/abs/2107.01824v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.01824v1)
- **Published**: 2021-07-05 07:15:57+00:00
- **Updated**: 2021-07-05 07:15:57+00:00
- **Authors**: Agathe Balayn, Bogdan Kulynych, Seda Guerses
- **Comment**: Presented at the CVPR workshop 2021 Beyond Fair Computer Vision
- **Journal**: None
- **Summary**: Researchers have identified datasets used for training computer vision (CV) models as an important source of hazardous outcomes, and continue to examine popular CV datasets to expose their harms. These works tend to treat datasets as objects, or focus on particular steps in data production pipelines. We argue here that we could further systematize our analysis of harms by examining CV data pipelines through a process-oriented lens that captures the creation, the evolution and use of these datasets. As a step towards cultivating a process-oriented lens, we embarked on an empirical study of CV data pipelines informed by the field of method engineering. We present here a preliminary result: a reference model of CV data pipelines. Besides exploring the questions that this endeavor raises, we discuss how the process lens could support researchers in discovering understudied issues, and could help practitioners in making their processes more transparent.



### GraspME -- Grasp Manifold Estimator
- **Arxiv ID**: http://arxiv.org/abs/2107.01836v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.01836v1)
- **Published**: 2021-07-05 07:49:12+00:00
- **Updated**: 2021-07-05 07:49:12+00:00
- **Authors**: Janik Hager, Ruben Bauer, Marc Toussaint, Jim Mainprice
- **Comment**: Accepted to RoMan 2021
- **Journal**: None
- **Summary**: In this paper, we introduce a Grasp Manifold Estimator (GraspME) to detect grasp affordances for objects directly in 2D camera images. To perform manipulation tasks autonomously it is crucial for robots to have such graspability models of the surrounding objects. Grasp manifolds have the advantage of providing continuously infinitely many grasps, which is not the case when using other grasp representations such as predefined grasp points. For instance, this property can be leveraged in motion optimization to define goal sets as implicit surface constraints in the robot configuration space. In this work, we restrict ourselves to the case of estimating possible end-effector positions directly from 2D camera images. To this extend, we define grasp manifolds via a set of key points and locate them in images using a Mask R-CNN backbone. Using learned features allows generalizing to different view angles, with potentially noisy images, and objects that were not part of the training set. We rely on simulation data only and perform experiments on simple and complex objects, including unseen ones. Our framework achieves an inference speed of 11.5 fps on a GPU, an average precision for keypoint estimation of 94.5% and a mean pixel distance of only 1.29. This shows that we can estimate the objects very well via bounding boxes and segmentation masks as well as approximate the correct grasp manifold's keypoint coordinates.



### Towards Better Adversarial Synthesis of Human Images from Text
- **Arxiv ID**: http://arxiv.org/abs/2107.01869v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.01869v1)
- **Published**: 2021-07-05 08:47:51+00:00
- **Updated**: 2021-07-05 08:47:51+00:00
- **Authors**: Rania Briq, Pratika Kochar, Juergen Gall
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an approach that generates multiple 3D human meshes from text. The human shapes are represented by 3D meshes based on the SMPL model. The model's performance is evaluated on the COCO dataset, which contains challenging human shapes and intricate interactions between individuals. The model is able to capture the dynamics of the scene and the interactions between individuals based on text. We further show how using such a shape as input to image synthesis frameworks helps to constrain the network to synthesize humans with realistic human shapes.



### Parts2Words: Learning Joint Embedding of Point Clouds and Texts by Bidirectional Matching between Parts and Words
- **Arxiv ID**: http://arxiv.org/abs/2107.01872v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.01872v2)
- **Published**: 2021-07-05 08:55:34+00:00
- **Updated**: 2023-04-02 02:12:38+00:00
- **Authors**: Chuan Tang, Xi Yang, Bojian Wu, Zhizhong Han, Yi Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Shape-Text matching is an important task of high-level shape understanding. Current methods mainly represent a 3D shape as multiple 2D rendered views, which obviously can not be understood well due to the structural ambiguity caused by self-occlusion in the limited number of views. To resolve this issue, we directly represent 3D shapes as point clouds, and propose to learn joint embedding of point clouds and texts by bidirectional matching between parts from shapes and words from texts. Specifically, we first segment the point clouds into parts, and then leverage optimal transport method to match parts and words in an optimized feature space, where each part is represented by aggregating features of all points within it and each word is abstracted by its contextual information. We optimize the feature space in order to enlarge the similarities between the paired training samples, while simultaneously maximizing the margin between the unpaired ones. Experiments demonstrate that our method achieves a significant improvement in accuracy over the SOTAs on multi-modal retrieval tasks under the Text2Shape dataset. Codes are available at https://github.com/JLUtangchuan/Parts2Words.



### Faster-LTN: a neuro-symbolic, end-to-end object detection architecture
- **Arxiv ID**: http://arxiv.org/abs/2107.01877v1
- **DOI**: 10.1007/978-3-030-86340-1_4
- **Categories**: **cs.CV**, cs.AI, cs.LO
- **Links**: [PDF](http://arxiv.org/pdf/2107.01877v1)
- **Published**: 2021-07-05 09:09:20+00:00
- **Updated**: 2021-07-05 09:09:20+00:00
- **Authors**: Francesco Manigrasso, Filomeno Davide Miro, Lia Morra, Fabrizio Lamberti
- **Comment**: accepted for presentation at ICANN 2021
- **Journal**: None
- **Summary**: The detection of semantic relationships between objects represented in an image is one of the fundamental challenges in image interpretation. Neural-Symbolic techniques, such as Logic Tensor Networks (LTNs), allow the combination of semantic knowledge representation and reasoning with the ability to efficiently learn from examples typical of neural networks. We here propose Faster-LTN, an object detector composed of a convolutional backbone and an LTN. To the best of our knowledge, this is the first attempt to combine both frameworks in an end-to-end training setting. This architecture is trained by optimizing a grounded theory which combines labelled examples with prior knowledge, in the form of logical axioms. Experimental comparisons show competitive performance with respect to the traditional Faster R-CNN architecture.



### Self-Contrastive Learning with Hard Negative Sampling for Self-supervised Point Cloud Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.01886v2
- **DOI**: 10.1145/3474085.3475458
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.01886v2)
- **Published**: 2021-07-05 09:17:45+00:00
- **Updated**: 2021-08-29 18:10:15+00:00
- **Authors**: Bi'an Du, Xiang Gao, Wei Hu, Xin Li
- **Comment**: Accepted to ACM MM 2021
- **Journal**: None
- **Summary**: Point clouds have attracted increasing attention. Significant progress has been made in methods for point cloud analysis, which often requires costly human annotation as supervision. To address this issue, we propose a novel self-contrastive learning for self-supervised point cloud representation learning, aiming to capture both local geometric patterns and nonlocal semantic primitives based on the nonlocal self-similarity of point clouds. The contributions are two-fold: on the one hand, instead of contrasting among different point clouds as commonly employed in contrastive learning, we exploit self-similar point cloud patches within a single point cloud as positive samples and otherwise negative ones to facilitate the task of contrastive learning. On the other hand, we actively learn hard negative samples that are close to positive samples for discriminative feature learning. Experimental results show that the proposed method achieves state-of-the-art performance on widely used benchmark datasets for self-supervised point cloud segmentation and transfer learning for classification.



### OPA: Object Placement Assessment Dataset
- **Arxiv ID**: http://arxiv.org/abs/2107.01889v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.01889v3)
- **Published**: 2021-07-05 09:23:53+00:00
- **Updated**: 2022-06-21 01:39:21+00:00
- **Authors**: Liu Liu, Zhenchen Liu, Bo Zhang, Jiangtong Li, Li Niu, Qingyang Liu, Liqing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Image composition aims to generate realistic composite image by inserting an object from one image into another background image, where the placement (e.g., location, size, occlusion) of inserted object may be unreasonable, which would significantly degrade the quality of the composite image. Although some works attempted to learn object placement to create realistic composite images, they did not focus on assessing the plausibility of object placement. In this paper, we focus on object placement assessment task, which verifies whether a composite image is plausible in terms of the object placement. To accomplish this task, we construct the first Object Placement Assessment (OPA) dataset consisting of composite images and their rationality labels. We also propose a simple yet effective baseline for this task. Dataset is available at https://github.com/bcmi/Object-Placement-Assessment-Dataset-OPA.



### Ray-ONet: Efficient 3D Reconstruction From A Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/2107.01899v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.01899v2)
- **Published**: 2021-07-05 09:45:57+00:00
- **Updated**: 2021-10-22 14:12:41+00:00
- **Authors**: Wenjing Bian, Zirui Wang, Kejie Li, Victor Adrian Prisacariu
- **Comment**: accepted in BMVC 2021
- **Journal**: None
- **Summary**: We propose Ray-ONet to reconstruct detailed 3D models from monocular images efficiently. By predicting a series of occupancy probabilities along a ray that is back-projected from a pixel in the camera coordinate, our method Ray-ONet improves the reconstruction accuracy in comparison with Occupancy Networks (ONet), while reducing the network inference complexity to O($N^2$). As a result, Ray-ONet achieves state-of-the-art performance on the ShapeNet benchmark with more than 20$\times$ speed-up at $128^3$ resolution and maintains a similar memory footprint during inference.



### On The Distribution of Penultimate Activations of Classification Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.01900v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.01900v2)
- **Published**: 2021-07-05 09:47:10+00:00
- **Updated**: 2021-07-06 01:10:24+00:00
- **Authors**: Minkyo Seo, Yoonho Lee, Suha Kwak
- **Comment**: 8 pages, UAI 2021, The first two authors equally contributed
- **Journal**: None
- **Summary**: This paper studies probability distributions of penultimate activations of classification networks. We show that, when a classification network is trained with the cross-entropy loss, its final classification layer forms a Generative-Discriminative pair with a generative classifier based on a specific distribution of penultimate activations. More importantly, the distribution is parameterized by the weights of the final fully-connected layer, and can be considered as a generative model that synthesizes the penultimate activations without feeding input data. We empirically demonstrate that this generative model enables stable knowledge distillation in the presence of domain shift, and can transfer knowledge from a classifier to variational autoencoders and generative adversarial networks for class-conditional image generation.



### SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2107.01903v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.01903v1)
- **Published**: 2021-07-05 09:53:08+00:00
- **Updated**: 2021-07-05 09:53:08+00:00
- **Authors**: Haocong Rao, Xiping Hu, Jun Cheng, Bin Hu
- **Comment**: Accepted at ACMMM 2021 Main Track. Sole copyright holder is ACMMM.
  Codes are available at https://github.com/Kali-Hac/SM-SGE
- **Journal**: None
- **Summary**: Person re-identification via 3D skeletons is an emerging topic with great potential in security-critical applications. Existing methods typically learn body and motion features from the body-joint trajectory, whereas they lack a systematic way to model body structure and underlying relations of body components beyond the scale of body joints. In this paper, we for the first time propose a Self-supervised Multi-scale Skeleton Graph Encoding (SM-SGE) framework that comprehensively models human body, component relations, and skeleton dynamics from unlabeled skeleton graphs of various scales to learn an effective skeleton representation for person Re-ID. Specifically, we first devise multi-scale skeleton graphs with coarse-to-fine human body partitions, which enables us to model body structure and skeleton dynamics at multiple levels. Second, to mine inherent correlations between body components in skeletal motion, we propose a multi-scale graph relation network to learn structural relations between adjacent body-component nodes and collaborative relations among nodes of different scales, so as to capture more discriminative skeleton graph features. Last, we propose a novel multi-scale skeleton reconstruction mechanism to enable our framework to encode skeleton dynamics and high-level semantics from unlabeled skeleton graphs, which encourages learning a discriminative skeleton representation for person Re-ID. Extensive experiments show that SM-SGE outperforms most state-of-the-art skeleton-based methods. We further demonstrate its effectiveness on 3D skeleton data estimated from large-scale RGB videos. Our codes are open at https://github.com/Kali-Hac/SM-SGE.



### Gaze Estimation with an Ensemble of Four Architectures
- **Arxiv ID**: http://arxiv.org/abs/2107.01980v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.01980v1)
- **Published**: 2021-07-05 12:40:26+00:00
- **Updated**: 2021-07-05 12:40:26+00:00
- **Authors**: Xin Cai, Boyu Chen, Jiabei Zeng, Jiajun Zhang, Yunjia Sun, Xiao Wang, Zhilong Ji, Xiao Liu, Xilin Chen, Shiguang Shan
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a method for gaze estimation according to face images. We train several gaze estimators adopting four different network architectures, including an architecture designed for gaze estimation (i.e.,iTracker-MHSA) and three originally designed for general computer vision tasks(i.e., BoTNet, HRNet, ResNeSt). Then, we select the best six estimators and ensemble their predictions through a linear combination. The method ranks the first on the leader-board of ETH-XGaze Competition, achieving an average angular error of $3.11^{\circ}$ on the ETH-XGaze test set.



### Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models
- **Arxiv ID**: http://arxiv.org/abs/2108.02562v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.02562v1)
- **Published**: 2021-07-05 12:54:05+00:00
- **Updated**: 2021-07-05 12:54:05+00:00
- **Authors**: Khazar Khorrami, Okko Räsänen
- **Comment**: To be published in Proc. Interspeech-2021, Brno, Czech Republic
- **Journal**: None
- **Summary**: Systems that can find correspondences between multiple modalities, such as between speech and images, have great potential to solve different recognition and data analysis tasks in an unsupervised manner. This work studies multimodal learning in the context of visually grounded speech (VGS) models, and focuses on their recently demonstrated capability to extract spatiotemporal alignments between spoken words and the corresponding visual objects without ever been explicitly trained for object localization or word recognition. As the main contributions, we formalize the alignment problem in terms of an audiovisual alignment tensor that is based on earlier VGS work, introduce systematic metrics for evaluating model performance in aligning visual objects and spoken words, and propose a new VGS model variant for the alignment task utilizing cross-modal attention layer. We test our model and a previously proposed model in the alignment task using SPEECH-COCO captions coupled with MSCOCO images. We compare the alignment performance using our proposed evaluation metrics to the semantic retrieval task commonly used to evaluate VGS models. We show that cross-modal attention layer not only helps the model to achieve higher semantic cross-modal retrieval performance, but also leads to substantial improvements in the alignment performance between image object and spoken words.



### UCSL : A Machine Learning Expectation-Maximization framework for Unsupervised Clustering driven by Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.01988v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.01988v1)
- **Published**: 2021-07-05 12:55:13+00:00
- **Updated**: 2021-07-05 12:55:13+00:00
- **Authors**: Robin Louiset, Pietro Gori, Benoit Dufumier, Josselin Houenou, Antoine Grigis, Edouard Duchesnay
- **Comment**: ECML/PKDD 2021
- **Journal**: ECML/PKDD 2021
- **Summary**: Subtype Discovery consists in finding interpretable and consistent sub-parts of a dataset, which are also relevant to a certain supervised task. From a mathematical point of view, this can be defined as a clustering task driven by supervised learning in order to uncover subgroups in line with the supervised prediction. In this paper, we propose a general Expectation-Maximization ensemble framework entitled UCSL (Unsupervised Clustering driven by Supervised Learning). Our method is generic, it can integrate any clustering method and can be driven by both binary classification and regression. We propose to construct a non-linear model by merging multiple linear estimators, one per cluster. Each hyperplane is estimated so that it correctly discriminates - or predict - only one cluster. We use SVC or Logistic Regression for classification and SVR for regression. Furthermore, to perform cluster analysis within a more suitable space, we also propose a dimension-reduction algorithm that projects the data onto an orthonormal space relevant to the supervised task. We analyze the robustness and generalization capability of our algorithm using synthetic and experimental datasets. In particular, we validate its ability to identify suitable consistent sub-types by conducting a psychiatric-diseases cluster analysis with known ground-truth labels. The gain of the proposed method over previous state-of-the-art techniques is about +1.9 points in terms of balanced accuracy. Finally, we make codes and examples available in a scikit-learn-compatible Python package at https://github.com/neurospin-projects/2021_rlouiset_ucsl



### FINT: Field-aware INTeraction Neural Network For CTR Prediction
- **Arxiv ID**: http://arxiv.org/abs/2107.01999v3
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.01999v3)
- **Published**: 2021-07-05 13:17:54+00:00
- **Updated**: 2022-07-11 01:40:48+00:00
- **Authors**: Zhishan Zhao, Sen Yang, Guohui Liu, Dawei Feng, Kele Xu
- **Comment**: 5 pages, accepted by ICASSP 2022
- **Journal**: None
- **Summary**: As a critical component for online advertising and marking, click-through rate (CTR) prediction has draw lots of attentions from both industry and academia field. Recently, the deep learning has become the mainstream methodological choice for CTR. Despite of sustainable efforts have been made, existing approaches still pose several challenges. On the one hand, high-order interaction between the features is under-explored. On the other hand, high-order interactions may neglect the semantic information from the low-order fields. In this paper, we proposed a novel prediction method, named FINT, that employs the Field-aware INTeraction layer which captures high-order feature interactions while retaining the low-order field information. To empirically investigate the effectiveness and robustness of the FINT, we perform extensive experiments on the three realistic databases: KDD2012, Criteo and Avazu. The obtained results demonstrate that the FINT can significantly improve the performance compared to the existing methods, without increasing the amount of computation required. Moreover, the proposed method brought about 2.72\% increase to the advertising revenue of a big online video app through A/B testing. To better promote the research in CTR field, we released our code as well as reference implementation at: https://github.com/zhishan01/FINT.



### Improving a neural network model by explanation-guided training for glioma classification based on MRI data
- **Arxiv ID**: http://arxiv.org/abs/2107.02008v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02008v2)
- **Published**: 2021-07-05 13:27:28+00:00
- **Updated**: 2023-04-16 20:17:22+00:00
- **Authors**: Frantisek Sefcik, Wanda Benesova
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: In recent years, artificial intelligence (AI) systems have come to the forefront. These systems, mostly based on Deep learning (DL), achieve excellent results in areas such as image processing, natural language processing, or speech recognition. Despite the statistically high accuracy of deep learning models, their output is often a decision of "black box". Thus, Interpretability methods have become a popular way to gain insight into the decision-making process of deep learning models. Explanation of a deep learning model is desirable in the medical domain since the experts have to justify their judgments to the patient. In this work, we proposed a method for explanation-guided training that uses a Layer-wise relevance propagation (LRP) technique to force the model to focus only on the relevant part of the image. We experimentally verified our method on a convolutional neural network (CNN) model for low-grade and high-grade glioma classification problems. Our experiments show promising results in a way to use interpretation techniques in the model training process.



### Fast and Scalable Optimal Transport for Brain Tractograms
- **Arxiv ID**: http://arxiv.org/abs/2107.02010v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2107.02010v1)
- **Published**: 2021-07-05 13:28:41+00:00
- **Updated**: 2021-07-05 13:28:41+00:00
- **Authors**: Jean Feydy, Pierre Roussillon, Alain Trouvé, Pietro Gori
- **Comment**: MICCAI 2019
- **Journal**: MICCAI 2019
- **Summary**: We present a new multiscale algorithm for solving regularized Optimal Transport problems on the GPU, with a linear memory footprint. Relying on Sinkhorn divergences which are convex, smooth and positive definite loss functions, this method enables the computation of transport plans between millions of points in a matter of minutes. We show the effectiveness of this approach on brain tractograms modeled either as bundles of fibers or as track density maps. We use the resulting smooth assignments to perform label transfer for atlas-based segmentation of fiber tractograms. The parameters -- blur and reach -- of our method are meaningful, defining the minimum and maximum distance at which two fibers are compared with each other. They can be set according to anatomical knowledge. Furthermore, we also propose to estimate a probabilistic atlas of a population of track density maps as a Wasserstein barycenter. Our CUDA implementation is endowed with a user-friendly PyTorch interface, freely available on the PyPi repository (pip install geomloss) and at www.kernel-operations.io/geomloss.



### FFR_FD: Effective and Fast Detection of DeepFakes Based on Feature Point Defects
- **Arxiv ID**: http://arxiv.org/abs/2107.02016v2
- **DOI**: 10.1016/j.ins.2022.03.026
- **Categories**: **cs.CV**, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2107.02016v2)
- **Published**: 2021-07-05 13:35:39+00:00
- **Updated**: 2021-08-26 07:42:41+00:00
- **Authors**: Gaojian Wang, Qian Jiang, Xin Jin, Xiaohui Cui
- **Comment**: 14 pages, 11 figures
- **Journal**: Information Sciences, Volume 596, 2022, Pages 472-488
- **Summary**: The internet is filled with fake face images and videos synthesized by deep generative models. These realistic DeepFakes pose a challenge to determine the authenticity of multimedia content. As countermeasures, artifact-based detection methods suffer from insufficiently fine-grained features that lead to limited detection performance. DNN-based detection methods are not efficient enough, given that a DeepFake can be created easily by mobile apps and DNN-based models require high computational resources. For the first time, we show that DeepFake faces have fewer feature points than real ones, especially in certain facial regions. Inspired by feature point detector-descriptors to extract discriminative features at the pixel level, we propose the Fused Facial Region_Feature Descriptor (FFR_FD) for effective and fast DeepFake detection. FFR_FD is only a vector extracted from the face, and it can be constructed from any feature point detector-descriptors. We train a random forest classifier with FFR_FD and conduct extensive experiments on six large-scale DeepFake datasets, whose results demonstrate that our method is superior to most state of the art DNN-based models.



### A topological solution to object segmentation and tracking
- **Arxiv ID**: http://arxiv.org/abs/2107.02036v1
- **DOI**: 10.1073/pnas.2204248119
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02036v1)
- **Published**: 2021-07-05 13:52:57+00:00
- **Updated**: 2021-07-05 13:52:57+00:00
- **Authors**: Thomas Tsao, Doris Y. Tsao
- **Comment**: 21 pages, 6 main figures, 3 supplemental figures, and supplementary
  material containing mathematical proofs
- **Journal**: None
- **Summary**: The world is composed of objects, the ground, and the sky. Visual perception of objects requires solving two fundamental challenges: segmenting visual input into discrete units, and tracking identities of these units despite appearance changes due to object deformation, changing perspective, and dynamic occlusion. Current computer vision approaches to segmentation and tracking that approach human performance all require learning, raising the question: can objects be segmented and tracked without learning? Here, we show that the mathematical structure of light rays reflected from environment surfaces yields a natural representation of persistent surfaces, and this surface representation provides a solution to both the segmentation and tracking problems. We describe how to generate this surface representation from continuous visual input, and demonstrate that our approach can segment and invariantly track objects in cluttered synthetic video despite severe appearance changes, without requiring learning.



### No-Reference Quality Assessment for 3D Colored Point Cloud and Mesh Models
- **Arxiv ID**: http://arxiv.org/abs/2107.02041v6
- **DOI**: 10.1109/TCSVT.2022.3186894
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02041v6)
- **Published**: 2021-07-05 14:03:15+00:00
- **Updated**: 2022-05-02 06:15:32+00:00
- **Authors**: Zicheng Zhang, Wei Sun, Xiongkuo Min, Tao Wang, Wei Lu, Guangtao Zhai
- **Comment**: None
- **Journal**: None
- **Summary**: To improve the viewer's Quality of Experience (QoE) and optimize computer graphics applications, 3D model quality assessment (3D-QA) has become an important task in the multimedia area. Point cloud and mesh are the two most widely used digital representation formats of 3D models, the visual quality of which is quite sensitive to lossy operations like simplification and compression. Therefore, many related studies such as point cloud quality assessment (PCQA) and mesh quality assessment (MQA) have been carried out to measure the visual quality degradations of 3D models. However, a large part of previous studies utilize full-reference (FR) metrics, which indicates they can not predict the quality level with the absence of the reference 3D model. Furthermore, few 3D-QA metrics consider color information, which significantly restricts their effectiveness and scope of application. In this paper, we propose a no-reference (NR) quality assessment metric for colored 3D models represented by both point cloud and mesh. First, we project the 3D models from 3D space into quality-related geometry and color feature domains. Then, the 3D natural scene statistics (3D-NSS) and entropy are utilized to extract quality-aware features. Finally, machine learning is employed to regress the quality-aware features into visual quality scores. Our method is validated on the colored point cloud quality assessment database (SJTU-PCQA), the Waterloo point cloud assessment database (WPC), and the colored mesh quality assessment database (CMDM). The experimental results show that the proposed method outperforms most compared NR 3D-QA metrics with competitive computational resources and greatly reduces the performance gap with the state-of-the-art FR 3D-QA metrics. The code of the proposed model is publicly available now to facilitate further research.



### Understanding the Security of Deepfake Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.02045v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02045v3)
- **Published**: 2021-07-05 14:18:21+00:00
- **Updated**: 2021-10-06 18:46:22+00:00
- **Authors**: Xiaoyu Cao, Neil Zhenqiang Gong
- **Comment**: To appear in ICDF2C 2021
- **Journal**: None
- **Summary**: Deepfakes pose growing challenges to the trust of information on the Internet. Thus, detecting deepfakes has attracted increasing attentions from both academia and industry. State-of-the-art deepfake detection methods consist of two key components, i.e., face extractor and face classifier, which extract the face region in an image and classify it to be real/fake, respectively. Existing studies mainly focused on improving the detection performance in non-adversarial settings, leaving security of deepfake detection in adversarial settings largely unexplored. In this work, we aim to bridge the gap. In particular, we perform a systematic measurement study to understand the security of the state-of-the-art deepfake detection methods in adversarial settings. We use two large-scale public deepfakes data sources including FaceForensics++ and Facebook Deepfake Detection Challenge, where the deepfakes are fake face images; and we train state-of-the-art deepfake detection methods. These detection methods can achieve 0.94--0.99 accuracies in non-adversarial settings on these datasets. However, our measurement results uncover multiple security limitations of the deepfake detection methods in adversarial settings. First, we find that an attacker can evade a face extractor, i.e., the face extractor fails to extract the correct face regions, via adding small Gaussian noise to its deepfake images. Second, we find that a face classifier trained using deepfakes generated by one method cannot detect deepfakes generated by another method, i.e., an attacker can evade detection via generating deepfakes using a new method. Third, we find that an attacker can leverage backdoor attacks developed by the adversarial machine learning community to evade a face classifier. Our results highlight that deepfake detection should consider the adversarial nature of the problem.



### MixStyle Neural Networks for Domain Generalization and Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2107.02053v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02053v1)
- **Published**: 2021-07-05 14:29:19+00:00
- **Updated**: 2021-07-05 14:29:19+00:00
- **Authors**: Kaiyang Zhou, Yongxin Yang, Yu Qiao, Tao Xiang
- **Comment**: Extension of https://openreview.net/forum?id=6xHJ37MVxxp. Code
  available at https://github.com/KaiyangZhou/mixstyle-release
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) often have poor generalization performance under domain shift. One way to improve domain generalization is to collect diverse source data from multiple relevant domains so that a CNN model is allowed to learn more domain-invariant, and hence generalizable representations. In this work, we address domain generalization with MixStyle, a plug-and-play, parameter-free module that is simply inserted to shallow CNN layers and requires no modification to training objectives. Specifically, MixStyle probabilistically mixes feature statistics between instances. This idea is inspired by the observation that visual domains can often be characterized by image styles which are in turn encapsulated within instance-level feature statistics in shallow CNN layers. Therefore, inserting MixStyle modules in effect synthesizes novel domains albeit in an implicit way. MixStyle is not only simple and flexible, but also versatile -- it can be used for problems whereby unlabeled images are available, such as semi-supervised domain generalization and unsupervised domain adaptation, with a simple extension to mix feature statistics between labeled and pseudo-labeled instances. We demonstrate through extensive experiments that MixStyle can significantly boost the out-of-distribution generalization performance across a wide range of tasks including object recognition, instance retrieval, and reinforcement learning.



### 6D Object Pose Estimation using Keypoints and Part Affinity Fields
- **Arxiv ID**: http://arxiv.org/abs/2107.02057v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.02057v2)
- **Published**: 2021-07-05 14:41:19+00:00
- **Updated**: 2021-07-22 13:22:07+00:00
- **Authors**: Moritz Zappel, Simon Bultmann, Sven Behnke
- **Comment**: In: Proceedings of 24th RoboCup International Symposium, June 2021,
  12 pages, 6 figures
- **Journal**: Proceedings of 24th RoboCup International Symposium, June 2021
- **Summary**: The task of 6D object pose estimation from RGB images is an important requirement for autonomous service robots to be able to interact with the real world. In this work, we present a two-step pipeline for estimating the 6 DoF translation and orientation of known objects. Keypoints and Part Affinity Fields (PAFs) are predicted from the input image adopting the OpenPose CNN architecture from human pose estimation. Object poses are then calculated from 2D-3D correspondences between detected and model keypoints via the PnP-RANSAC algorithm. The proposed approach is evaluated on the YCB-Video dataset and achieves accuracy on par with recent methods from the literature. Using PAFs to assemble detected keypoints into object instances proves advantageous over only using heatmaps. Models trained to predict keypoints of a single object class perform significantly better than models trained for several classes.



### Distance-based Hyperspherical Classification for Multi-source Open-Set Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2107.02067v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02067v3)
- **Published**: 2021-07-05 14:56:57+00:00
- **Updated**: 2021-10-20 13:23:48+00:00
- **Authors**: Silvia Bucci, Francesco Cappio Borlino, Barbara Caputo, Tatiana Tommasi
- **Comment**: accepted at WACV 2022
- **Journal**: None
- **Summary**: Vision systems trained in closed-world scenarios fail when presented with new environmental conditions, new data distributions, and novel classes at deployment time. How to move towards open-world learning is a long-standing research question. The existing solutions mainly focus on specific aspects of the problem (single domain Open-Set, multi-domain Closed-Set), or propose complex strategies which combine several losses and manually tuned hyperparameters. In this work, we tackle multi-source Open-Set domain adaptation by introducing HyMOS: a straightforward model that exploits the power of contrastive learning and the properties of its hyperspherical feature space to correctly predict known labels on the target, while rejecting samples belonging to any unknown class. HyMOS includes style transfer among the instance transformations of contrastive learning to get domain invariance while avoiding the risk of negative-transfer. A self-paced threshold is defined on the basis of the observed data distribution and updates online during training, allowing to handle the known-unknown separation. We validate our method over three challenging datasets. The obtained results show that HyMOS outperforms several competitors, defining the new state-of-the-art. Our code is available at https://github.com/silvia1993/HyMOS.



### Application of artificial intelligence techniques for automated detection of myocardial infarction: A review
- **Arxiv ID**: http://arxiv.org/abs/2107.06179v2
- **DOI**: 10.1088/1361-6579/ac7fd9
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.06179v2)
- **Published**: 2021-07-05 15:15:06+00:00
- **Updated**: 2022-02-21 09:06:53+00:00
- **Authors**: Javad Hassannataj Joloudari, Sanaz Mojrian, Issa Nodehi, Amir Mashmool, Zeynab Kiani Zadegan, Sahar Khanjani Shirkharkolaie, Roohallah Alizadehsani, Tahereh Tamadon, Samiyeh Khosravi, Mitra Akbari Kohnehshari, Edris Hassannatajjeloudari, Danial Sharifrazi, Amir Mosavi, Hui Wen Loh, Ru-San Tan, U Rajendra Acharya
- **Comment**: 16 pages, 8 figures
- **Journal**: None
- **Summary**: Myocardial infarction (MI) results in heart muscle injury due to receiving insufficient blood flow. MI is the most common cause of mortality in middle-aged and elderly individuals around the world. To diagnose MI, clinicians need to interpret electrocardiography (ECG) signals, which requires expertise and is subject to observer bias. Artificial intelligence-based methods can be utilized to screen for or diagnose MI automatically using ECG signals. In this work, we conducted a comprehensive assessment of artificial intelligence-based approaches for MI detection based on ECG as well as other biophysical signals, including machine learning (ML) and deep learning (DL) models. The performance of traditional ML methods relies on handcrafted features and manual selection of ECG signals, whereas DL models can automate these tasks. The review observed that deep convolutional neural networks (DCNNs) yielded excellent classification performance for MI diagnosis, which explains why they have become prevalent in recent years. To our knowledge, this is the first comprehensive survey of artificial intelligence techniques employed for MI diagnosis using ECG and other biophysical signals.



### One-Cycle Pruning: Pruning ConvNets Under a Tight Training Budget
- **Arxiv ID**: http://arxiv.org/abs/2107.02086v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.02086v3)
- **Published**: 2021-07-05 15:27:07+00:00
- **Updated**: 2022-07-03 18:12:46+00:00
- **Authors**: Nathan Hubens, Matei Mancas, Bernard Gosselin, Marius Preda, Titus Zaharia
- **Comment**: Accepted at Sparsity in Neural Networks (SNN 2021)
- **Journal**: None
- **Summary**: Introducing sparsity in a neural network has been an efficient way to reduce its complexity while keeping its performance almost intact. Most of the time, sparsity is introduced using a three-stage pipeline: 1) train the model to convergence, 2) prune the model according to some criterion, 3) fine-tune the pruned model to recover performance. The last two steps are often performed iteratively, leading to reasonable results but also to a time-consuming and complex process. In our work, we propose to get rid of the first step of the pipeline and to combine the two other steps in a single pruning-training cycle, allowing the model to jointly learn for the optimal weights while being pruned. We do this by introducing a novel pruning schedule, named One-Cycle Pruning, which starts pruning from the beginning of the training, and until its very end. Adopting such a schedule not only leads to better performing pruned models but also drastically reduces the training budget required to prune a model. Experiments are conducted on a variety of architectures (VGG-16 and ResNet-18) and datasets (CIFAR-10, CIFAR-100 and Caltech-101), and for relatively high sparsity values (80%, 90%, 95% of weights removed). Our results show that One-Cycle Pruning consistently outperforms commonly used pruning schedules such as One-Shot Pruning, Iterative Pruning and Automated Gradual Pruning, on a fixed training budget.



### Are standard Object Segmentation models sufficient for Learning Affordance Segmentation?
- **Arxiv ID**: http://arxiv.org/abs/2107.02095v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02095v1)
- **Published**: 2021-07-05 15:34:20+00:00
- **Updated**: 2021-07-05 15:34:20+00:00
- **Authors**: Hugo Caselles-Dupré, Michael Garcia-Ortiz, David Filliat
- **Comment**: None
- **Journal**: None
- **Summary**: Affordances are the possibilities of actions the environment offers to the individual. Ordinary objects (hammer, knife) usually have many affordances (grasping, pounding, cutting), and detecting these allow artificial agents to understand what are their possibilities in the environment, with obvious application in Robotics. Proposed benchmarks and state-of-the-art prediction models for supervised affordance segmentation are usually modifications of popular object segmentation models such as Mask R-CNN. We observe that theoretically, these popular object segmentation methods should be sufficient for detecting affordances masks. So we ask the question: is it necessary to tailor new architectures to the problem of learning affordances? We show that applying the out-of-the-box Mask R-CNN to the problem of affordances segmentation outperforms the current state-of-the-art. We conclude that the problem of supervised affordance segmentation is included in the problem of object segmentation and argue that better benchmarks for affordance learning should include action capacities.



### RATCHET: Medical Transformer for Chest X-ray Diagnosis and Reporting
- **Arxiv ID**: http://arxiv.org/abs/2107.02104v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02104v2)
- **Published**: 2021-07-05 15:58:48+00:00
- **Updated**: 2021-09-15 12:42:45+00:00
- **Authors**: Benjamin Hou, Georgios Kaissis, Ronald Summers, Bernhard Kainz
- **Comment**: None
- **Journal**: None
- **Summary**: Chest radiographs are one of the most common diagnostic modalities in clinical routine. It can be done cheaply, requires minimal equipment, and the image can be diagnosed by every radiologists. However, the number of chest radiographs obtained on a daily basis can easily overwhelm the available clinical capacities. We propose RATCHET: RAdiological Text Captioning for Human Examined Thoraces. RATCHET is a CNN-RNN-based medical transformer that is trained end-to-end. It is capable of extracting image features from chest radiographs, and generates medically accurate text reports that fit seamlessly into clinical work flows. The model is evaluated for its natural language generation ability using common metrics from NLP literature, as well as its medically accuracy through a surrogate report classification task. The model is available for download at: http://www.github.com/farrell236/RATCHET.



### Can Super Resolution be used to improve Human Pose Estimation in Low Resolution Scenarios?
- **Arxiv ID**: http://arxiv.org/abs/2107.02108v2
- **DOI**: 10.5220/0010863700003124
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02108v2)
- **Published**: 2021-07-05 16:06:55+00:00
- **Updated**: 2021-10-19 11:28:07+00:00
- **Authors**: Peter Hardy, Srinandan Dasmahapatra, Hansung Kim
- **Comment**: None
- **Journal**: Proceedings of the 17th International Joint Conference on Computer
  Vision, Imaging and Computer Graphics Theory and Applications - Volume 4:
  VISAPP (2022)
- **Summary**: The results obtained from state of the art human pose estimation (HPE) models degrade rapidly when evaluating people of a low resolution, but can super resolution (SR) be used to help mitigate this effect? By using various SR approaches we enhanced two low resolution datasets and evaluated the change in performance of both an object and keypoint detector as well as end-to-end HPE results. We remark the following observations. First we find that for people who were originally depicted at a low resolution (segmentation area in pixels), their keypoint detection performance would improve once SR was applied. Second, the keypoint detection performance gained is dependent on that persons pixel count in the original image prior to any application of SR; keypoint detection performance was improved when SR was applied to people with a small initial segmentation area, but degrades as this becomes larger. To address this we introduced a novel Mask-RCNN approach, utilising a segmentation area threshold to decide when to use SR during the keypoint detection step. This approach achieved the best results on our low resolution datasets for each HPE performance metrics.



### Recovering the Unbiased Scene Graphs from the Biased Ones
- **Arxiv ID**: http://arxiv.org/abs/2107.02112v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2107.02112v1)
- **Published**: 2021-07-05 16:10:41+00:00
- **Updated**: 2021-07-05 16:10:41+00:00
- **Authors**: Meng-Jiun Chiou, Henghui Ding, Hanshu Yan, Changhu Wang, Roger Zimmermann, Jiashi Feng
- **Comment**: Accepted by ACMMM 2021. Source code will be available at
  https://github.com/coldmanck/recovering-unbiased-scene-graphs
- **Journal**: None
- **Summary**: Given input images, scene graph generation (SGG) aims to produce comprehensive, graphical representations describing visual relationships among salient objects. Recently, more efforts have been paid to the long tail problem in SGG; however, the imbalance in the fraction of missing labels of different classes, or reporting bias, exacerbating the long tail is rarely considered and cannot be solved by the existing debiasing methods. In this paper we show that, due to the missing labels, SGG can be viewed as a "Learning from Positive and Unlabeled data" (PU learning) problem, where the reporting bias can be removed by recovering the unbiased probabilities from the biased ones by utilizing label frequencies, i.e., the per-class fraction of labeled, positive examples in all the positive examples. To obtain accurate label frequency estimates, we propose Dynamic Label Frequency Estimation (DLFE) to take advantage of training-time data augmentation and average over multiple training iterations to introduce more valid examples. Extensive experiments show that DLFE is more effective in estimating label frequencies than a naive variant of the traditional estimate, and DLFE significantly alleviates the long tail and achieves state-of-the-art debiasing performance on the VG dataset. We also show qualitatively that SGG models with DLFE produce prominently more balanced and unbiased scene graphs.



### Semi-supervised Learning for Dense Object Detection in Retail Scenes
- **Arxiv ID**: http://arxiv.org/abs/2107.02114v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02114v1)
- **Published**: 2021-07-05 16:12:38+00:00
- **Updated**: 2021-07-05 16:12:38+00:00
- **Authors**: Jaydeep Chauhan, Srikrishna Varadarajan, Muktabh Mayank Srivastava
- **Comment**: None
- **Journal**: None
- **Summary**: Retail scenes usually contain densely packed high number of objects in each image. Standard object detection techniques use fully supervised training methodology. This is highly costly as annotating a large dense retail object detection dataset involves an order of magnitude more effort compared to standard datasets. Hence, we propose semi-supervised learning to effectively use the large amount of unlabeled data available in the retail domain. We adapt a popular self supervised method called noisy student initially proposed for object classification to the task of dense object detection. We show that using unlabeled data with the noisy student training methodology, we can improve the state of the art on precise detection of objects in densely packed retail scenes. We also show that performance of the model increases as you increase the amount of unlabeled data.



### Multi-modality Deep Restoration of Extremely Compressed Face Videos
- **Arxiv ID**: http://arxiv.org/abs/2107.05548v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2107.05548v2)
- **Published**: 2021-07-05 16:29:02+00:00
- **Updated**: 2022-03-04 05:28:57+00:00
- **Authors**: Xi Zhang, Xiaolin Wu
- **Comment**: Accepted by TPAMI. Extension of DAVD-Net in CVPR 2020
- **Journal**: None
- **Summary**: Arguably the most common and salient object in daily video communications is the talking head, as encountered in social media, virtual classrooms, teleconferences, news broadcasting, talk shows, etc. When communication bandwidth is limited by network congestions or cost effectiveness, compression artifacts in talking head videos are inevitable. The resulting video quality degradation is highly visible and objectionable due to high acuity of human visual system to faces. To solve this problem, we develop a multi-modality deep convolutional neural network method for restoring face videos that are aggressively compressed. The main innovation is a new DCNN architecture that incorporates known priors of multiple modalities: the video-synchronized speech signal and semantic elements of the compression code stream, including motion vectors, code partition map and quantization parameters. These priors strongly correlate with the latent video and hence they are able to enhance the capability of deep learning to remove compression artifacts. Ample empirical evidences are presented to validate the superior performance of the proposed DCNN method on face videos over the existing state-of-the-art methods.



### Test-Time Personalization with a Transformer for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2107.02133v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02133v3)
- **Published**: 2021-07-05 16:48:34+00:00
- **Updated**: 2021-11-08 01:22:00+00:00
- **Authors**: Yizhuo Li, Miao Hao, Zonglin Di, Nitesh B. Gundavarapu, Xiaolong Wang
- **Comment**: Project page: http://liyz15.github.io/TTP/
- **Journal**: None
- **Summary**: We propose to personalize a human pose estimator given a set of test images of a person without using any manual annotations. While there is a significant advancement in human pose estimation, it is still very challenging for a model to generalize to different unknown environments and unseen persons. Instead of using a fixed model for every test case, we adapt our pose estimator during test time to exploit person-specific information. We first train our model on diverse data with both a supervised and a self-supervised pose estimation objectives jointly. We use a Transformer model to build a transformation between the self-supervised keypoints and the supervised keypoints. During test time, we personalize and adapt our model by fine-tuning with the self-supervised objective. The pose is then improved by transforming the updated self-supervised keypoints. We experiment with multiple datasets and show significant improvements on pose estimations with our self-supervised personalization.



### Do Different Tracking Tasks Require Different Appearance Models?
- **Arxiv ID**: http://arxiv.org/abs/2107.02156v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.02156v2)
- **Published**: 2021-07-05 17:40:17+00:00
- **Updated**: 2021-12-01 09:21:09+00:00
- **Authors**: Zhongdao Wang, Hengshuang Zhao, Ya-Li Li, Shengjin Wang, Philip H. S. Torr, Luca Bertinetto
- **Comment**: To appear at NeurIPS 2021
- **Journal**: None
- **Summary**: Tracking objects of interest in a video is one of the most popular and widely applicable problems in computer vision. However, with the years, a Cambrian explosion of use cases and benchmarks has fragmented the problem in a multitude of different experimental setups. As a consequence, the literature has fragmented too, and now novel approaches proposed by the community are usually specialised to fit only one specific setup. To understand to what extent this specialisation is necessary, in this work we present UniTrack, a solution to address five different tasks within the same framework. UniTrack consists of a single and task-agnostic appearance model, which can be learned in a supervised or self-supervised fashion, and multiple ``heads'' that address individual tasks and do not require training. We show how most tracking tasks can be solved within this framework, and that the same appearance model can be successfully used to obtain results that are competitive against specialised methods for most of the tasks considered. The framework also allows us to analyse appearance models obtained with the most recent self-supervised methods, thus extending their evaluation and comparison to a larger variety of important problems.



### Conditional Identity Disentanglement for Differential Face Morph Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.02162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02162v1)
- **Published**: 2021-07-05 17:44:52+00:00
- **Updated**: 2021-07-05 17:44:52+00:00
- **Authors**: Sudipta Banerjee, Arun Ross
- **Comment**: None
- **Journal**: None
- **Summary**: We present the task of differential face morph attack detection using a conditional generative network (cGAN). To determine whether a face image in an identification document, such as a passport, is morphed or not, we propose an algorithm that learns to implicitly disentangle identities from the morphed image conditioned on the trusted reference image using the cGAN. Furthermore, the proposed method can also recover some underlying information about the second subject used in generating the morph. We performed experiments on AMSL face morph, MorGAN, and EMorGAN datasets to demonstrate the effectiveness of the proposed method. We also conducted cross-dataset and cross-attack detection experiments. We obtained promising results of 3% BPCER @ 10% APCER on intra-dataset evaluation, which is comparable to existing methods; and 4.6% BPCER @ 10% APCER on cross-dataset evaluation, which outperforms state-of-the-art methods by at least 13.9%.



### On Model Calibration for Long-Tailed Object Detection and Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.02170v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02170v2)
- **Published**: 2021-07-05 17:57:20+00:00
- **Updated**: 2021-11-29 19:17:26+00:00
- **Authors**: Tai-Yu Pan, Cheng Zhang, Yandong Li, Hexiang Hu, Dong Xuan, Soravit Changpinyo, Boqing Gong, Wei-Lun Chao
- **Comment**: Accepted to NeurIPS 2021
- **Journal**: None
- **Summary**: Vanilla models for object detection and instance segmentation suffer from the heavy bias toward detecting frequent objects in the long-tailed setting. Existing methods address this issue mostly during training, e.g., by re-sampling or re-weighting. In this paper, we investigate a largely overlooked approach -- post-processing calibration of confidence scores. We propose NorCal, Normalized Calibration for long-tailed object detection and instance segmentation, a simple and straightforward recipe that reweighs the predicted scores of each class by its training sample size. We show that separately handling the background class and normalizing the scores over classes for each proposal are keys to achieving superior performance. On the LVIS dataset, NorCal can effectively improve nearly all the baseline models not only on rare classes but also on common and frequent classes. Finally, we conduct extensive analysis and ablation studies to offer insights into various modeling choices and mechanisms of our approach. Our code is publicly available at https://github.com/tydpan/NorCal/.



### What Makes for Hierarchical Vision Transformer?
- **Arxiv ID**: http://arxiv.org/abs/2107.02174v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02174v2)
- **Published**: 2021-07-05 17:59:35+00:00
- **Updated**: 2021-09-10 03:04:13+00:00
- **Authors**: Yuxin Fang, Xinggang Wang, Rui Wu, Wenyu Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies indicate that hierarchical Vision Transformer with a macro architecture of interleaved non-overlapped window-based self-attention \& shifted-window operation is able to achieve state-of-the-art performance in various visual recognition tasks, and challenges the ubiquitous convolutional neural networks (CNNs) using densely slid kernels. Most follow-up works attempt to replace the shifted-window operation with other kinds of cross-window communication paradigms, while treating self-attention as the de-facto standard for window-based information aggregation. In this manuscript, we question whether self-attention is the only choice for hierarchical Vision Transformer to attain strong performance, and the effects of different kinds of cross-window communication. To this end, we replace self-attention layers with embarrassingly simple linear mapping layers, and the resulting proof-of-concept architecture termed as LinMapper can achieve very strong performance in ImageNet-1k image recognition. Moreover, we find that LinMapper is able to better leverage the pre-trained representations from image recognition and demonstrates excellent transfer learning properties on downstream dense prediction tasks such as object detection and instance segmentation. We also experiment with other alternatives to self-attention for content aggregation inside each non-overlapped window under different cross-window communication approaches, which all give similar competitive results. Our study reveals that the \textbf{macro architecture} of Swin model families, other than specific aggregation layers or specific means of cross-window communication, may be more responsible for its strong performance and is the real challenger to the ubiquitous CNN's dense sliding window paradigm. Code and models will be publicly available to facilitate future research.



### Label noise in segmentation networks : mitigation must deal with bias
- **Arxiv ID**: http://arxiv.org/abs/2107.02189v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02189v1)
- **Published**: 2021-07-05 18:00:07+00:00
- **Updated**: 2021-07-05 18:00:07+00:00
- **Authors**: Eugene Vorontsov, Samuel Kadoury
- **Comment**: None
- **Journal**: None
- **Summary**: Imperfect labels limit the quality of predictions learned by deep neural networks. This is particularly relevant in medical image segmentation, where reference annotations are difficult to collect and vary significantly even across expert annotators. Prior work on mitigating label noise focused on simple models of mostly uniform noise. In this work, we explore biased and unbiased errors artificially introduced to brain tumour annotations on MRI data. We found that supervised and semi-supervised segmentation methods are robust or fairly robust to unbiased errors but sensitive to biased errors. It is therefore important to identify the sorts of errors expected in medical image labels and especially mitigate the biased errors.



### TransformerFusion: Monocular RGB Scene Reconstruction using Transformers
- **Arxiv ID**: http://arxiv.org/abs/2107.02191v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02191v1)
- **Published**: 2021-07-05 18:00:11+00:00
- **Updated**: 2021-07-05 18:00:11+00:00
- **Authors**: Aljaž Božič, Pablo Palafox, Justus Thies, Angela Dai, Matthias Nießner
- **Comment**: Video: https://youtu.be/LIpTKYfKSqw
- **Journal**: None
- **Summary**: We introduce TransformerFusion, a transformer-based 3D scene reconstruction approach. From an input monocular RGB video, the video frames are processed by a transformer network that fuses the observations into a volumetric feature grid representing the scene; this feature grid is then decoded into an implicit 3D scene representation. Key to our approach is the transformer architecture that enables the network to learn to attend to the most relevant image frames for each 3D location in the scene, supervised only by the scene reconstruction task. Features are fused in a coarse-to-fine fashion, storing fine-level features only where needed, requiring lower memory storage and enabling fusion at interactive rates. The feature grid is then decoded to a higher-resolution scene reconstruction, using an MLP-based surface occupancy prediction from interpolated coarse-to-fine 3D features. Our approach results in an accurate surface reconstruction, outperforming state-of-the-art multi-view stereo depth estimation methods, fully-convolutional 3D reconstruction approaches, and approaches using LSTM- or GRU-based recurrent networks for video sequence fusion.



### Long-Short Transformer: Efficient Transformers for Language and Vision
- **Arxiv ID**: http://arxiv.org/abs/2107.02192v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2107.02192v3)
- **Published**: 2021-07-05 18:00:14+00:00
- **Updated**: 2021-12-07 07:23:24+00:00
- **Authors**: Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, Bryan Catanzaro
- **Comment**: Published at NeurIPS 2021
- **Journal**: None
- **Summary**: Transformers have achieved success in both language and vision domains. However, it is prohibitively expensive to scale them to long sequences such as long documents or high-resolution images, because self-attention mechanism has quadratic time and memory complexities with respect to the input sequence length. In this paper, we propose Long-Short Transformer (Transformer-LS), an efficient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks. It aggregates a novel long-range attention with dynamic projection to model distant correlations and a short-term attention to capture fine-grained local correlations. We propose a dual normalization strategy to account for the scale mismatch between the two attention mechanisms. Transformer-LS can be applied to both autoregressive and bidirectional models without additional complexity. Our method outperforms the state-of-the-art models on multiple tasks in language and vision domains, including the Long Range Arena benchmark, autoregressive language modeling, and ImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on enwik8 using half the number of parameters than previous method, while being faster and is able to handle 3x as long sequences compared to its full-attention version on the same hardware. On ImageNet, it can obtain the state-of-the-art results (e.g., a moderate size of 55.8M model solely trained on 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more scalable on high-resolution images. The source code and models are released at https://github.com/NVIDIA/transformer-ls .



### Automated age-related macular degeneration area estimation -- first results
- **Arxiv ID**: http://arxiv.org/abs/2107.02211v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68T07, 68T05, 68T45, 92C55, I.2.6; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2107.02211v1)
- **Published**: 2021-07-05 18:29:56+00:00
- **Updated**: 2021-07-05 18:29:56+00:00
- **Authors**: Rokas Pečiulis, Mantas Lukoševičius, Algimantas Kriščiukaitis, Robertas Petrolis, Dovilė Buteikienė
- **Comment**: None
- **Journal**: Proceedings of the 26th International Conference on Information
  Society and University Studies (IVUS 2021), pp. 141-149, CEUR, 2021
- **Summary**: This work aims to research an automatic method for detecting Age-related Macular Degeneration (AMD) lesions in RGB eye fundus images. For this, we align invasively obtained eye fundus contrast images (the "golden standard" diagnostic) to the RGB ones and use them to hand-annotate the lesions. This is done using our custom-made tool. Using the data, we train and test five different convolutional neural networks: a custom one to classify healthy and AMD-affected eye fundi, and four well-known networks: ResNet50, ResNet101, MobileNetV3, and UNet to segment (localize) the AMD lesions in the affected eye fundus images. We achieve 93.55% accuracy or 69.71% Dice index as the preliminary best results in segmentation with MobileNetV3.



### Graph Convolution for Re-ranking in Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2107.02220v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02220v2)
- **Published**: 2021-07-05 18:40:43+00:00
- **Updated**: 2022-01-28 18:00:06+00:00
- **Authors**: Yuqi Zhang, Qian Qi, Chong Liu, Weihua Chen, Fan Wang, Hao Li, Rong Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, deep learning is widely applied to extract features for similarity computation in person re-identification (re-ID) and have achieved great success. However, due to the non-overlapping between training and testing IDs, the difference between the data used for model training and the testing data makes the performance of learned feature degraded during testing. Hence, re-ranking is proposed to mitigate this issue and various algorithms have been developed. However, most of existing re-ranking methods focus on replacing the Euclidean distance with sophisticated distance metrics, which are not friendly to downstream tasks and hard to be used for fast retrieval of massive data in real applications. In this work, we propose a graph-based re-ranking method to improve learned features while still keeping Euclidean distance as the similarity metric. Inspired by graph convolution networks, we develop an operator to propagate features over an appropriate graph. Since graph is the essential key for the propagation, two important criteria are considered for designing the graph, and three different graphs are explored accordingly. Furthermore, a simple yet effective method is proposed to generate a profile vector for each tracklet in videos, which helps extend our method to video re-ID. Extensive experiments on three benchmark data sets, e.g., Market-1501, Duke, and MARS, demonstrate the effectiveness of our proposed approach.



### Vision Xformers: Efficient Attention for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.02239v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CC, cs.LG, I.4.0; I.4.1; I.4.7; I.4.8; I.4.9; I.4.10; I.2.10; I.5.1; I.5.2;
  I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2107.02239v4)
- **Published**: 2021-07-05 19:24:23+00:00
- **Updated**: 2021-10-01 15:08:54+00:00
- **Authors**: Pranav Jeevan, Amit Sethi
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Although transformers have become the neural architectures of choice for natural language processing, they require orders of magnitude more training data, GPU memory, and computations in order to compete with convolutional neural networks for computer vision. The attention mechanism of transformers scales quadratically with the length of the input sequence, and unrolled images have long sequence lengths. Plus, transformers lack an inductive bias that is appropriate for images. We tested three modifications to vision transformer (ViT) architectures that address these shortcomings. Firstly, we alleviate the quadratic bottleneck by using linear attention mechanisms, called X-formers (such that, X in {Performer, Linformer, Nystr\"omformer}), thereby creating Vision X-formers (ViXs). This resulted in up to a seven times reduction in the GPU memory requirement. We also compared their performance with FNet and multi-layer perceptron mixers, which further reduced the GPU memory requirement. Secondly, we introduced an inductive bias for images by replacing the initial linear embedding layer by convolutional layers in ViX, which significantly increased classification accuracy without increasing the model size. Thirdly, we replaced the learnable 1D position embeddings in ViT with Rotary Position Embedding (RoPE), which increases the classification accuracy for the same model size. We believe that incorporating such changes can democratize transformers by making them accessible to those with limited data and computing resources.



### GuavaNet: A deep neural network architecture for automatic sensory evaluation to predict degree of acceptability for Guava by a consumer
- **Arxiv ID**: http://arxiv.org/abs/2108.02563v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.02563v1)
- **Published**: 2021-07-05 20:13:37+00:00
- **Updated**: 2021-07-05 20:13:37+00:00
- **Authors**: Vipul Mehra
- **Comment**: None
- **Journal**: None
- **Summary**: This thesis is divided into two parts:Part I: Analysis of Fruits, Vegetables, Cheese and Fish based on Image Processing using Computer Vision and Deep Learning: A Review. It consists of a comprehensive review of image processing, computer vision and deep learning techniques applied to carry out analysis of fruits, vegetables, cheese and fish.This part also serves as a literature review for Part II.Part II: GuavaNet: A deep neural network architecture for automatic sensory evaluation to predict degree of acceptability for Guava by a consumer. This part introduces to an end-to-end deep neural network architecture that can predict the degree of acceptability by the consumer for a guava based on sensory evaluation.



### VolNet: Estimating Human Body Part Volumes from a Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/2107.02259v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02259v1)
- **Published**: 2021-07-05 20:38:44+00:00
- **Updated**: 2021-07-05 20:38:44+00:00
- **Authors**: Fabian Leinen, Vittorio Cozzolino, Torsten Schön
- **Comment**: None
- **Journal**: None
- **Summary**: Human body volume estimation from a single RGB image is a challenging problem despite minimal attention from the research community. However VolNet, an architecture leveraging 2D and 3D pose estimation, body part segmentation and volume regression extracted from a single 2D RGB image combined with the subject's body height can be used to estimate the total body volume. VolNet is designed to predict the 2D and 3D pose as well as the body part segmentation in intermediate tasks. We generated a synthetic, large-scale dataset of photo-realistic images of human bodies with a wide range of body shapes and realistic poses called SURREALvols. By using Volnet and combining multiple stacked hourglass networks together with ResNeXt, our model correctly predicted the volume in ~82% of cases with a 10% tolerance threshold. This is a considerable improvement compared to state-of-the-art solutions such as BodyNet with only a ~38% success rate.



### Beyond the Hausdorff Metric in Digital Topology
- **Arxiv ID**: http://arxiv.org/abs/2108.03114v2
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV, math.MG, 54B20
- **Links**: [PDF](http://arxiv.org/pdf/2108.03114v2)
- **Published**: 2021-07-05 21:24:25+00:00
- **Updated**: 2021-09-23 20:55:33+00:00
- **Authors**: Laurence Boxer
- **Comment**: None
- **Journal**: None
- **Summary**: Two objects may be close in the Hausdorff metric, yet have very different geometric and topological properties. We examine other methods of comparing digital images such that objects close in each of these measures have some similar geometric or topological property. Such measures may be combined with the Hausdorff metric to yield a metric in which close images are similar with respect to multiple properties.



### Morphological Classification of Galaxies in S-PLUS using an Ensemble of Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.02287v1
- **DOI**: None
- **Categories**: **astro-ph.GA**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02287v1)
- **Published**: 2021-07-05 21:51:19+00:00
- **Updated**: 2021-07-05 21:51:19+00:00
- **Authors**: N. M. Cardoso, G. B. O. Schwarz, L. O. Dias, C. R. Bom, L. Sodré Jr., C. Mendes de Oliveira
- **Comment**: 18 pages, 13 figures, codes and data available at
  https://natanael.net, text in portuguese
- **Journal**: None
- **Summary**: The universe is composed of galaxies that have diverse shapes. Once the structure of a galaxy is determined, it is possible to obtain important information about its formation and evolution. Morphologically classifying galaxies means cataloging them according to their visual appearance and the classification is linked to the physical properties of the galaxy. A morphological classification made through visual inspection is subject to biases introduced by subjective observations made by human volunteers. For this reason, systematic, objective and easily reproducible classification of galaxies has been gaining importance since the astronomer Edwin Hubble created his famous classification method. In this work, we combine accurate visual classifications of the Galaxy Zoo project with \emph {Deep Learning} methods. The goal is to find an efficient technique at human performance level classification, but in a systematic and automatic way, for classification of elliptical and spiral galaxies. For this, a neural network model was created through an Ensemble of four other convolutional models, allowing a greater accuracy in the classification than what would be obtained with any one individual. Details of the individual models and improvements made are also described. The present work is entirely based on the analysis of images (not parameter tables) from DR1 (www.datalab.noao.edu) of the Southern Photometric Local Universe Survey (S-PLUS). In terms of classification, we achieved, with the Ensemble, an accuracy of $\approx 99 \%$ in the test sample (using pre-trained networks).



### Histogram of Cell Types: Deep Learning for Automated Bone Marrow Cytology
- **Arxiv ID**: http://arxiv.org/abs/2107.02293v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02293v2)
- **Published**: 2021-07-05 21:55:00+00:00
- **Updated**: 2021-07-08 16:11:28+00:00
- **Authors**: Rohollah Moosavi Tayebi, Youqing Mu, Taher Dehkharghanian, Catherine Ross, Monalisa Sur, Ronan Foley, Hamid R. Tizhoosh, Clinton JV Campbell
- **Comment**: None
- **Journal**: None
- **Summary**: Bone marrow cytology is required to make a hematological diagnosis, influencing critical clinical decision points in hematology. However, bone marrow cytology is tedious, limited to experienced reference centers and associated with high inter-observer variability. This may lead to a delayed or incorrect diagnosis, leaving an unmet need for innovative supporting technologies. We have developed the first ever end-to-end deep learning-based technology for automated bone marrow cytology. Starting with a bone marrow aspirate digital whole slide image, our technology rapidly and automatically detects suitable regions for cytology, and subsequently identifies and classifies all bone marrow cells in each region. This collective cytomorphological information is captured in a novel representation called Histogram of Cell Types (HCT) quantifying bone marrow cell class probability distribution and acting as a cytological "patient fingerprint". The approach achieves high accuracy in region detection (0.97 accuracy and 0.99 ROC AUC), and cell detection and cell classification (0.75 mAP, 0.78 F1-score, Log-average miss rate of 0.31). HCT has potential to revolutionize hematopathology diagnostic workflows, leading to more cost-effective, accurate diagnosis and opening the door to precision medicine.



### LightFuse: Lightweight CNN based Dual-exposure Fusion
- **Arxiv ID**: http://arxiv.org/abs/2107.02299v5
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02299v5)
- **Published**: 2021-07-05 22:10:29+00:00
- **Updated**: 2022-09-22 20:24:19+00:00
- **Authors**: Ziyi Liu, Jie Yang, Svetlana Yanushkevich, Orly Yadid-Pecht
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNNs) have aided high dynamic range (HDR) imaging recently and have received a lot of attention. The quality of DCNN-generated HDR images has overperformed the traditional counterparts. However, DCNNs are prone to be computationally intensive and power-hungry, and hence cannot be implemented on various embedded computing platforms with limited power and hardware resources. Embedded systems have a huge market, and utilizing DCNNs' powerful functionality into them will further reduce human intervention. To address the challenge, we propose LightFuse, a lightweight CNN-based algorithm for extreme dual-exposure image fusion, which achieves better functionality than a conventional DCNN and can be deployed in embedded systems. Two sub-networks are utilized: a GlobalNet (G) and a DetailNet (D). The goal of G is to learn the global illumination information on the spatial dimension, whereas D aims to enhance local details on the channel dimension. Both G and D are based solely on depthwise convolution (D_Conv) and pointwise convolution (P_Conv) to reduce required parameters and computations. Experimental results show that this proposed technique could generate HDR images in extremely exposed regions with sufficient details to be legible. Our model outperforms other state-of-the-art approaches in peak signal-to-noise ratio (PSNR) score by 0.9 to 8.7 while achieving 16.7 to 306.2 times parameter reduction.



### Connectivity Matters: Neural Network Pruning Through the Lens of Effective Sparsity
- **Arxiv ID**: http://arxiv.org/abs/2107.02306v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02306v2)
- **Published**: 2021-07-05 22:36:57+00:00
- **Updated**: 2023-04-08 01:04:39+00:00
- **Authors**: Artem Vysogorets, Julia Kempe
- **Comment**: None
- **Journal**: None
- **Summary**: Neural network pruning is a fruitful area of research with surging interest in high sparsity regimes. Benchmarking in this domain heavily relies on faithful representation of the sparsity of subnetworks, which has been traditionally computed as the fraction of removed connections (direct sparsity). This definition, however, fails to recognize unpruned parameters that detached from input or output layers of underlying subnetworks, potentially underestimating actual effective sparsity: the fraction of inactivated connections. While this effect might be negligible for moderately pruned networks (up to 10-100 compression rates), we find that it plays an increasing role for thinner subnetworks, greatly distorting comparison between different pruning algorithms. For example, we show that effective compression of a randomly pruned LeNet-300-100 can be orders of magnitude larger than its direct counterpart, while no discrepancy is ever observed when using SynFlow for pruning [Tanaka et al., 2020]. In this work, we adopt the lens of effective sparsity to reevaluate several recent pruning algorithms on common benchmark architectures (e.g., LeNet-300-100, VGG-19, ResNet-18) and discover that their absolute and relative performance changes dramatically in this new and more appropriate framework. To aim for effective, rather than direct, sparsity, we develop a low-cost extension to most pruning algorithms. Further, equipped with effective sparsity as a reference frame, we partially reconfirm that random pruning with appropriate sparsity allocation across layers performs as well or better than more sophisticated algorithms for pruning at initialization [Su et al., 2020]. In response to this observation, using a simple analogy of pressure distribution in coupled cylinders from physics, we design novel layerwise sparsity quotas that outperform all existing baselines in the context of random pruning.



### A visual introduction to Gaussian Belief Propagation
- **Arxiv ID**: http://arxiv.org/abs/2107.02308v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.02308v1)
- **Published**: 2021-07-05 22:43:27+00:00
- **Updated**: 2021-07-05 22:43:27+00:00
- **Authors**: Joseph Ortiz, Talfan Evans, Andrew J. Davison
- **Comment**: See online version of this article: https://gaussianbp.github.io/
- **Journal**: None
- **Summary**: In this article, we present a visual introduction to Gaussian Belief Propagation (GBP), an approximate probabilistic inference algorithm that operates by passing messages between the nodes of arbitrarily structured factor graphs. A special case of loopy belief propagation, GBP updates rely only on local information and will converge independently of the message schedule. Our key argument is that, given recent trends in computing hardware, GBP has the right computational properties to act as a scalable distributed probabilistic inference framework for future machine learning systems.



### The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.02314v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02314v2)
- **Published**: 2021-07-05 23:12:06+00:00
- **Updated**: 2021-09-12 20:26:52+00:00
- **Authors**: Ujjwal Baid, Satyam Ghodasara, Suyash Mohan, Michel Bilello, Evan Calabrese, Errol Colak, Keyvan Farahani, Jayashree Kalpathy-Cramer, Felipe C. Kitamura, Sarthak Pati, Luciano M. Prevedello, Jeffrey D. Rudie, Chiharu Sako, Russell T. Shinohara, Timothy Bergquist, Rong Chai, James Eddy, Julia Elliott, Walter Reade, Thomas Schaffter, Thomas Yu, Jiaxin Zheng, Ahmed W. Moawad, Luiz Otavio Coelho, Olivia McDonnell, Elka Miller, Fanny E. Moron, Mark C. Oswood, Robert Y. Shih, Loizos Siakallis, Yulia Bronstein, James R. Mason, Anthony F. Miller, Gagandeep Choudhary, Aanchal Agarwal, Cristina H. Besada, Jamal J. Derakhshan, Mariana C. Diogo, Daniel D. Do-Dai, Luciano Farage, John L. Go, Mohiuddin Hadi, Virginia B. Hill, Michael Iv, David Joyner, Christie Lincoln, Eyal Lotan, Asako Miyakoshi, Mariana Sanchez-Montano, Jaya Nath, Xuan V. Nguyen, Manal Nicolas-Jilwan, Johanna Ortiz Jimenez, Kerem Ozturk, Bojan D. Petrovic, Chintan Shah, Lubdha M. Shah, Manas Sharma, Onur Simsek, Achint K. Singh, Salil Soman, Volodymyr Statsevych, Brent D. Weinberg, Robert J. Young, Ichiro Ikuta, Amit K. Agarwal, Sword C. Cambron, Richard Silbergleit, Alexandru Dusoi, Alida A. Postma, Laurent Letourneau-Guillon, Gloria J. Guzman Perez-Carrillo, Atin Saha, Neetu Soni, Greg Zaharchuk, Vahe M. Zohrabian, Yingming Chen, Milos M. Cekic, Akm Rahman, Juan E. Small, Varun Sethi, Christos Davatzikos, John Mongan, Christopher Hess, Soonmee Cha, Javier Villanueva-Meyer, John B. Freymann, Justin S. Kirby, Benedikt Wiestler, Priscila Crivellaro, Rivka R. Colen, Aikaterini Kotrotsou, Daniel Marcus, Mikhail Milchenko, Arash Nazeri, Hassan Fathallah-Shaykh, Roland Wiest, Andras Jakab, Marc-Andre Weber, Abhishek Mahajan, Bjoern Menze, Adam E. Flanders, Spyridon Bakas
- **Comment**: 19 pages, 2 figures, 1 table
- **Journal**: None
- **Summary**: The BraTS 2021 challenge celebrates its 10th anniversary and is jointly organized by the Radiological Society of North America (RSNA), the American Society of Neuroradiology (ASNR), and the Medical Image Computing and Computer Assisted Interventions (MICCAI) society. Since its inception, BraTS has been focusing on being a common benchmarking venue for brain glioma segmentation algorithms, with well-curated multi-institutional multi-parametric magnetic resonance imaging (mpMRI) data. Gliomas are the most common primary malignancies of the central nervous system, with varying degrees of aggressiveness and prognosis. The RSNA-ASNR-MICCAI BraTS 2021 challenge targets the evaluation of computational algorithms assessing the same tumor compartmentalization, as well as the underlying tumor's molecular characterization, in pre-operative baseline mpMRI data from 2,040 patients. Specifically, the two tasks that BraTS 2021 focuses on are: a) the segmentation of the histologically distinct brain tumor sub-regions, and b) the classification of the tumor's O[6]-methylguanine-DNA methyltransferase (MGMT) promoter methylation status. The performance evaluation of all participating algorithms in BraTS 2021 will be conducted through the Sage Bionetworks Synapse platform (Task 1) and Kaggle (Task 2), concluding in distributing to the top ranked participants monetary awards of $60,000 collectively.



### Exploring Deep Learning Methods for Real-Time Surgical Instrument Segmentation in Laparoscopy
- **Arxiv ID**: http://arxiv.org/abs/2107.02319v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02319v2)
- **Published**: 2021-07-05 23:32:05+00:00
- **Updated**: 2021-08-03 17:12:51+00:00
- **Authors**: Debesh Jha, Sharib Ali, Nikhil Kumar Tomar, Michael A. Riegler, Dag Johansen, Håvard D. Johansen, Pål Halvorsen
- **Comment**: None
- **Journal**: BHI 2021
- **Summary**: Minimally invasive surgery is a surgical intervention used to examine the organs inside the abdomen and has been widely used due to its effectiveness over open surgery. Due to the hardware improvements such as high definition cameras, this procedure has significantly improved and new software methods have demonstrated potential for computer-assisted procedures. However, there exists challenges and requirements to improve detection and tracking of the position of the instruments during these surgical procedures. To this end, we evaluate and compare some popular deep learning methods that can be explored for the automated segmentation of surgical instruments in laparoscopy, an important step towards tool tracking. Our experimental results exhibit that the Dual decoder attention network (DDANet) produces a superior result compared to other recent deep learning methods. DDANet yields a Dice coefficient of 0.8739 and mean intersection-over-union of 0.8183 for the Robust Medical Instrument Segmentation (ROBUST-MIS) Challenge 2019 dataset, at a real-time speed of 101.36 frames-per-second that is critical for such procedures.



