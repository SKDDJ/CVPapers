# Arxiv Papers in cs.CV on 2021-07-20
### A Comparison of Supervised and Unsupervised Deep Learning Methods for Anomaly Detection in Images
- **Arxiv ID**: http://arxiv.org/abs/2107.09204v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.09204v1)
- **Published**: 2021-07-20 00:14:12+00:00
- **Updated**: 2021-07-20 00:14:12+00:00
- **Authors**: Vincent Wilmet, Sauraj Verma, Tabea Redl, HÃ¥kon Sandaker, Zhenning Li
- **Comment**: 8 pages, for FML
- **Journal**: None
- **Summary**: Anomaly detection in images plays a significant role for many applications across all industries, such as disease diagnosis in healthcare or quality assurance in manufacturing. Manual inspection of images, when extended over a monotonously repetitive period of time is very time consuming and can lead to anomalies being overlooked.Artificial neural networks have proven themselves very successful on simple, repetitive tasks, in some cases even outperforming humans. Therefore, in this paper we investigate different methods of deep learning, including supervised and unsupervised learning, for anomaly detection applied to a quality assurance use case. We utilize the MVTec anomaly dataset and develop three different models, a CNN for supervised anomaly detection, KD-CAE for autoencoder anomaly detection, NI-CAE for noise induced anomaly detection and a DCGAN for generating reconstructed images. By experiments, we found that KD-CAE performs better on the anomaly datasets compared to CNN and NI-CAE, with NI-CAE performing the best on the Transistor dataset. We also implemented a DCGAN for the creation of new training data but due to computational limitation and lack of extrapolating the mechanics of AnoGAN, we restricted ourselves just to the generation of GAN based images. We conclude that unsupervised methods are more powerful for anomaly detection in images, especially in a setting where only a small amount of anomalous data is available, or the data is unlabeled.



### Understanding Gender and Racial Disparities in Image Recognition Models
- **Arxiv ID**: http://arxiv.org/abs/2107.09211v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09211v1)
- **Published**: 2021-07-20 01:05:31+00:00
- **Updated**: 2021-07-20 01:05:31+00:00
- **Authors**: Rohan Mahadev, Anindya Chakravarti
- **Comment**: None
- **Journal**: None
- **Summary**: Large scale image classification models trained on top of popular datasets such as Imagenet have shown to have a distributional skew which leads to disparities in prediction accuracies across different subsections of population demographics. A lot of approaches have been made to solve for this distributional skew using methods that alter the model pre, post and during training. We investigate one such approach - which uses a multi-label softmax loss with cross-entropy as the loss function instead of a binary cross-entropy on a multi-label classification problem on the Inclusive Images dataset which is a subset of the OpenImages V6 dataset. We use the MR2 dataset, which contains images of people with self-identified gender and race attributes to evaluate the fairness in the model outcomes and try to interpret the mistakes by looking at model activations and suggest possible fixes.



### Discriminator-Free Generative Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/2107.09225v1
- **DOI**: 10.1145/3474085.3475290
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09225v1)
- **Published**: 2021-07-20 01:55:21+00:00
- **Updated**: 2021-07-20 01:55:21+00:00
- **Authors**: Shaohao Lu, Yuqiao Xian, Ke Yan, Yi Hu, Xing Sun, Xiaowei Guo, Feiyue Huang, Wei-Shi Zheng
- **Comment**: 9 pages, 6 figures, 4 tables
- **Journal**: None
- **Summary**: The Deep Neural Networks are vulnerable toadversarial exam-ples(Figure 1), making the DNNs-based systems collapsed byadding the inconspicuous perturbations to the images. Most of the existing works for adversarial attack are gradient-based and suf-fer from the latency efficiencies and the load on GPU memory. Thegenerative-based adversarial attacks can get rid of this limitation,and some relative works propose the approaches based on GAN.However, suffering from the difficulty of the convergence of train-ing a GAN, the adversarial examples have either bad attack abilityor bad visual quality. In this work, we find that the discriminatorcould be not necessary for generative-based adversarial attack, andpropose theSymmetric Saliency-based Auto-Encoder (SSAE)to generate the perturbations, which is composed of the saliencymap module and the angle-norm disentanglement of the featuresmodule. The advantage of our proposed method lies in that it is notdepending on discriminator, and uses the generative saliency map to pay more attention to label-relevant regions. The extensive exper-iments among the various tasks, datasets, and models demonstratethat the adversarial examples generated by SSAE not only make thewidely-used models collapse, but also achieves good visual quality.The code is available at https://github.com/BravoLu/SSAE.



### Attention-Guided NIR Image Colorization via Adaptive Fusion of Semantic and Texture Clues
- **Arxiv ID**: http://arxiv.org/abs/2107.09237v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09237v2)
- **Published**: 2021-07-20 03:00:51+00:00
- **Updated**: 2022-01-15 14:19:25+00:00
- **Authors**: Xingxing Yang, Jie Chen, Zaifeng Yang, Zhenghua Chen
- **Comment**: We have a significant change of contents to the existing version
- **Journal**: None
- **Summary**: Near infrared (NIR) imaging has been widely applied in low-light imaging scenarios; however, it is difficult for human and algorithms to perceive the real scene in the colorless NIR domain. While Generative Adversarial Network (GAN) has been widely employed in various image colorization tasks, it is challenging for a direct mapping mechanism, such as a conventional GAN, to transform an image from the NIR to the RGB domain with correct semantic reasoning, well-preserved textures, and vivid color combinations concurrently. In this work, we propose a novel Attention-based NIR image colorization framework via Adaptive Fusion of Semantic and Texture clues, aiming at achieving these goals within the same framework. The tasks of texture transfer and semantic reasoning are carried out in two separate network blocks. Specifically, the Texture Transfer Block (TTB) aims at extracting texture features from the NIR image's Laplacian component and transferring them for subsequent color fusion. The Semantic Reasoning Block (SRB) extracts semantic clues and maps the NIR pixel values to the RGB domain. Finally, a Fusion Attention Block (FAB) is proposed to adaptively fuse the features from the two branches and generate an optimized colorization result. In order to enhance the network's learning capacity in semantic reasoning as well as mapping precision in texture transfer, we have proposed the Residual Coordinate Attention Block (RCAB), which incorporates coordinate attention into a residual learning framework, enabling the network to capture long-range dependencies along the channel direction and meanwhile precise positional information can be preserved along spatial directions. RCAB is also incorporated into FAB to facilitate accurate texture alignment during fusion. Both quantitative and qualitative evaluations show that the proposed method outperforms state-of-the-art NIR image colorization methods.



### Generative Video Transformer: Can Objects be the Words?
- **Arxiv ID**: http://arxiv.org/abs/2107.09240v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.09240v1)
- **Published**: 2021-07-20 03:08:39+00:00
- **Updated**: 2021-07-20 03:08:39+00:00
- **Authors**: Yi-Fu Wu, Jaesik Yoon, Sungjin Ahn
- **Comment**: Published in ICML 2021
- **Journal**: None
- **Summary**: Transformers have been successful for many natural language processing tasks. However, applying transformers to the video domain for tasks such as long-term video generation and scene understanding has remained elusive due to the high computational complexity and the lack of natural tokenization. In this paper, we propose the Object-Centric Video Transformer (OCVT) which utilizes an object-centric approach for decomposing scenes into tokens suitable for use in a generative video transformer. By factoring the video into objects, our fully unsupervised model is able to learn complex spatio-temporal dynamics of multiple interacting objects in a scene and generate future frames of the video. Our model is also significantly more memory-efficient than pixel-based models and thus able to train on videos of length up to 70 frames with a single 48GB GPU. We compare our model with previous RNN-based approaches as well as other possible video transformer baselines. We demonstrate OCVT performs well when compared to baselines in generating future frames. OCVT also develops useful representations for video reasoning, achieving start-of-the-art performance on the CATER task.



### Boosting Few-Shot Classification with View-Learnable Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.09242v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09242v2)
- **Published**: 2021-07-20 03:13:33+00:00
- **Updated**: 2021-07-30 17:55:20+00:00
- **Authors**: Xu Luo, Yuxuan Chen, Liangjian Wen, Lili Pan, Zenglin Xu
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: The goal of few-shot classification is to classify new categories with few labeled examples within each class. Nowadays, the excellent performance in handling few-shot classification problems is shown by metric-based meta-learning methods. However, it is very hard for previous methods to discriminate the fine-grained sub-categories in the embedding space without fine-grained labels. This may lead to unsatisfactory generalization to fine-grained subcategories, and thus affects model interpretation. To tackle this problem, we introduce the contrastive loss into few-shot classification for learning latent fine-grained structure in the embedding space. Furthermore, to overcome the drawbacks of random image transformation used in current contrastive learning in producing noisy and inaccurate image pairs (i.e., views), we develop a learning-to-learn algorithm to automatically generate different views of the same image. Extensive experiments on standard few-shot learning benchmarks demonstrate the superiority of our method.



### S2Looking: A Satellite Side-Looking Dataset for Building Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.09244v3
- **DOI**: 10.3390/rs13245094
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.09244v3)
- **Published**: 2021-07-20 03:31:00+00:00
- **Updated**: 2022-01-11 06:54:03+00:00
- **Authors**: Li Shen, Yao Lu, Hao Chen, Hao Wei, Donghai Xie, Jiabao Yue, Rui Chen, Shouye Lv, Bitao Jiang
- **Comment**: None
- **Journal**: Remote Sens. 2021, 13, 5094
- **Summary**: Building-change detection underpins many important applications, especially in the military and crisis-management domains. Recent methods used for change detection have shifted towards deep learning, which depends on the quality of its training data. The assembly of large-scale annotated satellite imagery datasets is therefore essential for global building-change surveillance. Existing datasets almost exclusively offer near-nadir viewing angles. This limits the range of changes that can be detected. By offering larger observation ranges, the scroll imaging mode of optical satellites presents an opportunity to overcome this restriction. This paper therefore introduces S2Looking, a building-change-detection dataset that contains large-scale side-looking satellite images captured at various off-nadir angles. The dataset consists of 5000 bitemporal image pairs of rural areas and more than 65,920 annotated instances of changes throughout the world. The dataset can be used to train deep-learning-based change-detection algorithms. It expands upon existing datasets by providing (1) larger viewing angles; (2) large illumination variances; and (3) the added complexity of rural images. To facilitate {the} use of the dataset, a benchmark task has been established, and preliminary tests suggest that deep-learning algorithms find the dataset significantly more challenging than the closest-competing near-nadir dataset, LEVIR-CD+. S2Looking may therefore promote important advances in existing building-change-detection algorithms. The dataset is available at https://github.com/S2Looking/.



### Self-Supervised Aggregation of Diverse Experts for Test-Agnostic Long-Tailed Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.09249v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09249v4)
- **Published**: 2021-07-20 04:10:31+00:00
- **Updated**: 2022-10-27 08:14:58+00:00
- **Authors**: Yifan Zhang, Bryan Hooi, Lanqing Hong, Jiashi Feng
- **Comment**: NeurIPS 2022. Source code: https://github.com/Vanint/SADE-AgnosticLT
- **Journal**: None
- **Summary**: Existing long-tailed recognition methods, aiming to train class-balanced models from long-tailed data, generally assume the models would be evaluated on the uniform test class distribution. However, practical test class distributions often violate this assumption (e.g., being either long-tailed or even inversely long-tailed), which may lead existing methods to fail in real applications. In this paper, we study a more practical yet challenging task, called test-agnostic long-tailed recognition, where the training class distribution is long-tailed while the test class distribution is agnostic and not necessarily uniform. In addition to the issue of class imbalance, this task poses another challenge: the class distribution shift between the training and test data is unknown. To tackle this task, we propose a novel approach, called Self-supervised Aggregation of Diverse Experts, which consists of two strategies: (i) a new skill-diverse expert learning strategy that trains multiple experts from a single and stationary long-tailed dataset to separately handle different class distributions; (ii) a novel test-time expert aggregation strategy that leverages self-supervision to aggregate the learned multiple experts for handling unknown test class distributions. We theoretically show that our self-supervised strategy has a provable ability to simulate test-agnostic class distributions. Promising empirical results demonstrate the effectiveness of our method on both vanilla and test-agnostic long-tailed recognition. Code is available at \url{https://github.com/Vanint/SADE-AgnosticLT}.



### Monocular Visual Analysis for Electronic Line Calling of Tennis Games
- **Arxiv ID**: http://arxiv.org/abs/2107.09255v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09255v1)
- **Published**: 2021-07-20 04:23:11+00:00
- **Updated**: 2021-07-20 04:23:11+00:00
- **Authors**: Yuanzhou Chen, Shaobo Cai, Yuxin Wang, Junchi Yan
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: Electronic Line Calling is an auxiliary referee system used for tennis matches based on binocular vision technology. While ELC has been widely used, there are still many problems, such as complex installation and maintenance, high cost and etc. We propose a monocular vision technology based ELC method. The method has the following steps. First, locate the tennis ball's trajectory. We propose a multistage tennis ball positioning approach combining background subtraction and color area filtering. Then we propose a bouncing point prediction method by minimizing the fitting loss of the uncertain point. Finally, we find out whether the bouncing point of the ball is out of bounds or not according to the relative position between the bouncing point and the court side line in the two dimensional image. We collected and tagged 394 samples with an accuracy rate of 99.4%, and 81.8% of the 11 samples with bouncing points.The experimental results show that our method is feasible to judge if a ball is out of the court with monocular vision and significantly reduce complex installation and costs of ELC system with binocular vision.



### FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos
- **Arxiv ID**: http://arxiv.org/abs/2107.09262v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.MM, cs.SD, 68T10 (primary) 68T07, 68U10(secondary), I.5.4; I.2.10; J.5
- **Links**: [PDF](http://arxiv.org/pdf/2107.09262v1)
- **Published**: 2021-07-20 04:59:26+00:00
- **Updated**: 2021-07-20 04:59:26+00:00
- **Authors**: Sanchita Ghose, John J. Prevost
- **Comment**: This article is under review in IEEE Transaction on Multimedia. It
  contains total 12 pages, 6 figures, 4 tables
- **Journal**: None
- **Summary**: Deep learning based visual to sound generation systems essentially need to be developed particularly considering the synchronicity aspects of visual and audio features with time. In this research we introduce a novel task of guiding a class conditioned generative adversarial network with the temporal visual information of a video input for visual to sound generation task adapting the synchronicity traits between audio-visual modalities. Our proposed FoleyGAN model is capable of conditioning action sequences of visual events leading towards generating visually aligned realistic sound tracks. We expand our previously proposed Automatic Foley dataset to train with FoleyGAN and evaluate our synthesized sound through human survey that shows noteworthy (on average 81\%) audio-visual synchronicity performance. Our approach also outperforms in statistical experiments compared with other baseline models and audio-visual datasets.



### Locality-aware Channel-wise Dropout for Occluded Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.09270v1
- **DOI**: 10.1109/TIP.2021.3132827
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09270v1)
- **Published**: 2021-07-20 05:53:14+00:00
- **Updated**: 2021-07-20 05:53:14+00:00
- **Authors**: Mingjie He, Jie Zhang, Shiguang Shan, Xiao Liu, Zhongqin Wu, Xilin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition remains a challenging task in unconstrained scenarios, especially when faces are partially occluded. To improve the robustness against occlusion, augmenting the training images with artificial occlusions has been proved as a useful approach. However, these artificial occlusions are commonly generated by adding a black rectangle or several object templates including sunglasses, scarfs and phones, which cannot well simulate the realistic occlusions. In this paper, based on the argument that the occlusion essentially damages a group of neurons, we propose a novel and elegant occlusion-simulation method via dropping the activations of a group of neurons in some elaborately selected channel. Specifically, we first employ a spatial regularization to encourage each feature channel to respond to local and different face regions. In this way, the activations affected by an occlusion in a local region are more likely to be located in a single feature channel. Then, the locality-aware channel-wise dropout (LCD) is designed to simulate the occlusion by dropping out the entire feature channel. Furthermore, by randomly dropping out several feature channels, our method can well simulate the occlusion of larger area. The proposed LCD can encourage its succeeding layers to minimize the intra-class feature variance caused by occlusions, thus leading to improved robustness against occlusion. In addition, we design an auxiliary spatial attention module by learning a channel-wise attention vector to reweight the feature channels, which improves the contributions of non-occluded regions. Extensive experiments on various benchmarks show that the proposed method outperforms state-of-the-art methods with a remarkable improvement.



### ReSSL: Relational Self-Supervised Learning with Weak Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.09282v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.09282v2)
- **Published**: 2021-07-20 06:53:07+00:00
- **Updated**: 2021-07-23 14:24:31+00:00
- **Authors**: Mingkai Zheng, Shan You, Fei Wang, Chen Qian, Changshui Zhang, Xiaogang Wang, Chang Xu
- **Comment**: fixed several typos
- **Journal**: None
- **Summary**: Self-supervised Learning (SSL) including the mainstream contrastive learning has achieved great success in learning visual representations without data annotations. However, most of methods mainly focus on the instance level information (\ie, the different augmented images of the same instance should have the same feature or cluster into the same class), but there is a lack of attention on the relationships between different instances. In this paper, we introduced a novel SSL paradigm, which we term as relational self-supervised learning (ReSSL) framework that learns representations by modeling the relationship between different instances. Specifically, our proposed method employs sharpened distribution of pairwise similarities among different instances as \textit{relation} metric, which is thus utilized to match the feature embeddings of different augmentations. Moreover, to boost the performance, we argue that weak augmentations matter to represent a more reliable relation, and leverage momentum strategy for practical efficiency. Experimental results show that our proposed ReSSL significantly outperforms the previous state-of-the-art algorithms in terms of both performance and training efficiency. Code is available at \url{https://github.com/KyleZheng1997/ReSSL}.



### Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking and Steganography
- **Arxiv ID**: http://arxiv.org/abs/2107.09287v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09287v3)
- **Published**: 2021-07-20 07:03:23+00:00
- **Updated**: 2023-04-19 05:09:08+00:00
- **Authors**: Zihan Wang, Olivia Byrnes, Hu Wang, Ruoxi Sun, Congbo Ma, Huaming Chen, Qi Wu, Minhui Xue
- **Comment**: None
- **Journal**: None
- **Summary**: The advancement of secure communication and identity verification fields has significantly increased through the use of deep learning techniques for data hiding. By embedding information into a noise-tolerant signal such as audio, video, or images, digital watermarking and steganography techniques can be used to protect sensitive intellectual property and enable confidential communication, ensuring that the information embedded is only accessible to authorized parties. This survey provides an overview of recent developments in deep learning techniques deployed for data hiding, categorized systematically according to model architectures and noise injection methods. The objective functions, evaluation metrics, and datasets used for training these data hiding models are comprehensively summarised. Additionally, potential future research directions that unite digital watermarking and steganography on software engineering to enhance security and mitigate risks are suggested and deliberated. This contribution furthers the creation of a more trustworthy digital world and advances Responsible AI.



### Cell Detection from Imperfect Annotation by Pseudo Label Selection Using P-classification
- **Arxiv ID**: http://arxiv.org/abs/2107.09289v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09289v2)
- **Published**: 2021-07-20 07:08:05+00:00
- **Updated**: 2021-07-21 04:28:59+00:00
- **Authors**: Kazuma Fujii, Daiki Suehiro, Kazuya Nishimura, Ryoma Bise
- **Comment**: 10 pages, 3 figures, Accepted in MICCAI2021
- **Journal**: None
- **Summary**: Cell detection is an essential task in cell image analysis. Recent deep learning-based detection methods have achieved very promising results. In general, these methods require exhaustively annotating the cells in an entire image. If some of the cells are not annotated (imperfect annotation), the detection performance significantly degrades due to noisy labels. This often occurs in real collaborations with biologists and even in public data-sets. Our proposed method takes a pseudo labeling approach for cell detection from imperfect annotated data. A detection convolutional neural network (CNN) trained using such missing labeled data often produces over-detection. We treat partially labeled cells as positive samples and the detected positions except for the labeled cell as unlabeled samples. Then we select reliable pseudo labels from unlabeled data using recent machine learning techniques; positive-and-unlabeled (PU) learning and P-classification. Experiments using microscopy images for five different conditions demonstrate the effectiveness of the proposed method.



### Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion
- **Arxiv ID**: http://arxiv.org/abs/2107.09293v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2107.09293v1)
- **Published**: 2021-07-20 07:22:42+00:00
- **Updated**: 2021-07-20 07:22:42+00:00
- **Authors**: Suzhen Wang, Lincheng Li, Yu Ding, Changjie Fan, Xin Yu
- **Comment**: None
- **Journal**: IJCAI 2021
- **Summary**: We propose an audio-driven talking-head method to generate photo-realistic talking-head videos from a single reference image. In this work, we tackle two key challenges: (i) producing natural head motions that match speech prosody, and (ii) maintaining the appearance of a speaker in a large head motion while stabilizing the non-face regions. We first design a head pose predictor by modeling rigid 6D head movements with a motion-aware recurrent neural network (RNN). In this way, the predicted head poses act as the low-frequency holistic movements of a talking head, thus allowing our latter network to focus on detailed facial movement generation. To depict the entire image motions arising from audio, we exploit a keypoint based dense motion field representation. Then, we develop a motion field generator to produce the dense motion fields from input audio, head poses, and a reference image. As this keypoint based representation models the motions of facial regions, head, and backgrounds integrally, our method can better constrain the spatial and temporal consistency of the generated videos. Finally, an image generation network is employed to render photo-realistic talking-head videos from the estimated keypoint based motion fields and the input reference image. Extensive experiments demonstrate that our method produces videos with plausible head motions, synchronized facial expressions, and stable backgrounds and outperforms the state-of-the-art.



### Follow Your Path: a Progressive Method for Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2107.09305v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.09305v1)
- **Published**: 2021-07-20 07:44:33+00:00
- **Updated**: 2021-07-20 07:44:33+00:00
- **Authors**: Wenxian Shi, Yuxuan Song, Hao Zhou, Bohan Li, Lei Li
- **Comment**: Accepted by ECML-PKDD 2021
- **Journal**: None
- **Summary**: Deep neural networks often have a huge number of parameters, which posts challenges in deployment in application scenarios with limited memory and computation capacity. Knowledge distillation is one approach to derive compact models from bigger ones. However, it has been observed that a converged heavy teacher model is strongly constrained for learning a compact student network and could make the optimization subject to poor local optima. In this paper, we propose ProKT, a new model-agnostic method by projecting the supervision signals of a teacher model into the student's parameter space. Such projection is implemented by decomposing the training objective into local intermediate targets with an approximate mirror descent technique. The proposed method could be less sensitive with the quirks during optimization which could result in a better local optimum. Experiments on both image and text datasets show that our proposed ProKT consistently achieves superior performance compared to other existing knowledge distillation methods.



### SynthTIGER: Synthetic Text Image GEneratoR Towards Better Text Recognition Models
- **Arxiv ID**: http://arxiv.org/abs/2107.09313v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09313v1)
- **Published**: 2021-07-20 08:03:45+00:00
- **Updated**: 2021-07-20 08:03:45+00:00
- **Authors**: Moonbin Yim, Yoonsik Kim, Han-Cheol Cho, Sungrae Park
- **Comment**: Accepted at ICDAR 2021, 16 pages, 6 figures
- **Journal**: None
- **Summary**: For successful scene text recognition (STR) models, synthetic text image generators have alleviated the lack of annotated text images from the real world. Specifically, they generate multiple text images with diverse backgrounds, font styles, and text shapes and enable STR models to learn visual patterns that might not be accessible from manually annotated data. In this paper, we introduce a new synthetic text image generator, SynthTIGER, by analyzing techniques used for text image synthesis and integrating effective ones under a single algorithm. Moreover, we propose two techniques that alleviate the long-tail problem in length and character distributions of training data. In our experiments, SynthTIGER achieves better STR performance than the combination of synthetic datasets, MJSynth (MJ) and SynthText (ST). Our ablation study demonstrates the benefits of using sub-components of SynthTIGER and the guideline on generating synthetic text images for STR models. Our implementation is publicly available at https://github.com/clovaai/synthtiger.



### Protecting Semantic Segmentation Models by Using Block-wise Image Encryption with Secret Key from Unauthorized Access
- **Arxiv ID**: http://arxiv.org/abs/2107.09362v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.09362v1)
- **Published**: 2021-07-20 09:31:15+00:00
- **Updated**: 2021-07-20 09:31:15+00:00
- **Authors**: Hiroki Ito, MaungMaung AprilPyone, Hitoshi Kiya
- **Comment**: To appear in 2021 International Workshop on Smart Info-Media Systems
  in Asia (SISA 2021)
- **Journal**: None
- **Summary**: Since production-level trained deep neural networks (DNNs) are of a great business value, protecting such DNN models against copyright infringement and unauthorized access is in a rising demand. However, conventional model protection methods focused only the image classification task, and these protection methods were never applied to semantic segmentation although it has an increasing number of applications. In this paper, we propose to protect semantic segmentation models from unauthorized access by utilizing block-wise transformation with a secret key for the first time. Protected models are trained by using transformed images. Experiment results show that the proposed protection method allows rightful users with the correct key to access the model to full capacity and deteriorate the performance for unauthorized users. However, protected models slightly drop the segmentation performance compared to non-protected models.



### Self-Supervised Domain Adaptation for Diabetic Retinopathy Grading using Vessel Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2107.09372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09372v1)
- **Published**: 2021-07-20 09:44:07+00:00
- **Updated**: 2021-07-20 09:44:07+00:00
- **Authors**: Duy M. H. Nguyen, Truong T. N. Mai, Ngoc T. T. Than, Alexander Prange, Daniel Sonntag
- **Comment**: None
- **Journal**: None
- **Summary**: This paper investigates the problem of domain adaptation for diabetic retinopathy (DR) grading. We learn invariant target-domain features by defining a novel self-supervised task based on retinal vessel image reconstructions, inspired by medical domain knowledge. Then, a benchmark of current state-of-the-art unsupervised domain adaptation methods on the DR problem is provided. It can be shown that our approach outperforms existing domain adaption strategies. Furthermore, when utilizing entire training data in the target domain, we are able to compete with several state-of-the-art approaches in final classification accuracy just by applying standard network architectures and using image-level labels.



### Image-Hashing-Based Anomaly Detection for Privacy-Preserving Online Proctoring
- **Arxiv ID**: http://arxiv.org/abs/2107.09373v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2107.09373v1)
- **Published**: 2021-07-20 09:45:05+00:00
- **Updated**: 2021-07-20 09:45:05+00:00
- **Authors**: Waheeb Yaqub, Manoranjan Mohanty, Basem Suleiman
- **Comment**: None
- **Journal**: None
- **Summary**: Online proctoring has become a necessity in online teaching. Video-based crowd-sourced online proctoring solutions are being used, where an exam-taking student's video is monitored by third parties, leading to privacy concerns. In this paper, we propose a privacy-preserving online proctoring system. The proposed image-hashing-based system can detect the student's excessive face and body movement (i.e., anomalies) that is resulted when the student tries to cheat in the exam. The detection can be done even if the student's face is blurred or masked in video frames. Experiment with an in-house dataset shows the usability of the proposed system.



### Built-in Elastic Transformations for Improved Robustness
- **Arxiv ID**: http://arxiv.org/abs/2107.09391v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.09391v3)
- **Published**: 2021-07-20 10:16:38+00:00
- **Updated**: 2021-10-22 10:23:47+00:00
- **Authors**: Sadaf Gulshad, Ivan Sosnovik, Arnold Smeulders
- **Comment**: None
- **Journal**: None
- **Summary**: We focus on building robustness in the convolutions of neural visual classifiers, especially against natural perturbations like elastic deformations, occlusions and Gaussian noise. Existing CNNs show outstanding performance on clean images, but fail to tackle naturally occurring perturbations. In this paper, we start from elastic perturbations, which approximate (local) view-point changes of the object. We present elastically-augmented convolutions (EAConv) by parameterizing filters as a combination of fixed elastically-perturbed bases functions and trainable weights for the purpose of integrating unseen viewpoints in the CNN. We show on CIFAR-10 and STL-10 datasets that the general robustness of our method on unseen occlusion, zoom, rotation, image cut and Gaussian perturbations improves, while significantly improving the performance on clean images without any data augmentation.



### DeepSMILE: Contrastive self-supervised pre-training benefits MSI and HRD classification directly from H&E whole-slide images in colorectal and breast cancer
- **Arxiv ID**: http://arxiv.org/abs/2107.09405v3
- **DOI**: 10.1016/j.media.2022.102464
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.09405v3)
- **Published**: 2021-07-20 11:00:16+00:00
- **Updated**: 2023-06-28 13:52:29+00:00
- **Authors**: Yoni Schirris, Efstratios Gavves, Iris Nederlof, Hugo Mark Horlings, Jonas Teuwen
- **Comment**: Main paper: 14 pages, 2 tables, 1 algorithm, 3 figures. Supplementary
  material: 3 pages
- **Journal**: Medical Image Analysis Volume 79, July 2022, 102464
- **Summary**: We propose a Deep learning-based weak label learning method for analyzing whole slide images (WSIs) of Hematoxylin and Eosin (H&E) stained tumor tissue not requiring pixel-level or tile-level annotations using Self-supervised pre-training and heterogeneity-aware deep Multiple Instance LEarning (DeepSMILE). We apply DeepSMILE to the task of Homologous recombination deficiency (HRD) and microsatellite instability (MSI) prediction. We utilize contrastive self-supervised learning to pre-train a feature extractor on histopathology tiles of cancer tissue. Additionally, we use variability-aware deep multiple instance learning to learn the tile feature aggregation function while modeling tumor heterogeneity. For MSI prediction in a tumor-annotated and color normalized subset of TCGA-CRC (n=360 patients), contrastive self-supervised learning improves the tile supervision baseline from 0.77 to 0.87 AUROC, on par with our proposed DeepSMILE method. On TCGA-BC (n=1041 patients) without any manual annotations, DeepSMILE improves HRD classification performance from 0.77 to 0.81 AUROC compared to tile supervision with either a self-supervised or ImageNet pre-trained feature extractor. Our proposed methods reach the baseline performance using only 40% of the labeled data on both datasets. These improvements suggest we can use standard self-supervised learning techniques combined with multiple instance learning in the histopathology domain to improve genomic label classification performance with fewer labeled data.



### RankSRGAN: Super Resolution Generative Adversarial Networks with Learning to Rank
- **Arxiv ID**: http://arxiv.org/abs/2107.09427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09427v1)
- **Published**: 2021-07-20 11:42:18+00:00
- **Updated**: 2021-07-20 11:42:18+00:00
- **Authors**: Wenlong Zhang, Yihao Liu, Chao Dong, Yu Qiao
- **Comment**: IEEE PAMI accepted. arXiv admin note: substantial text overlap with
  arXiv:1908.06382
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GAN) have demonstrated the potential to recover realistic details for single image super-resolution (SISR). To further improve the visual quality of super-resolved results, PIRM2018-SR Challenge employed perceptual metrics to assess the perceptual quality, such as PI, NIQE, and Ma. However, existing methods cannot directly optimize these indifferentiable perceptual metrics, which are shown to be highly correlated with human ratings. To address the problem, we propose Super-Resolution Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator in the direction of different perceptual metrics. Specifically, we first train a Ranker which can learn the behaviour of perceptual metrics and then introduce a novel rank-content loss to optimize the perceptual quality. The most appealing part is that the proposed method can combine the strengths of different SR methods to generate better results. Furthermore, we extend our method to multiple Rankers to provide multi-dimension constraints for the generator. Extensive experiments show that RankSRGAN achieves visually pleasing results and reaches state-of-the-art performance in perceptual metrics and quality. Project page: https://wenlongzhang0517.github.io/Projects/RankSRGAN



### Automated Segmentation and Volume Measurement of Intracranial Carotid Artery Calcification on Non-Contrast CT
- **Arxiv ID**: http://arxiv.org/abs/2107.09442v1
- **DOI**: 10.1148/ryai.2021200226
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.09442v1)
- **Published**: 2021-07-20 12:21:45+00:00
- **Updated**: 2021-07-20 12:21:45+00:00
- **Authors**: Gerda Bortsova, Daniel Bos, Florian Dubost, Meike W. Vernooij, M. Kamran Ikram, Gijs van Tulder, Marleen de Bruijne
- **Comment**: Accepted for publication in Radiology: Artificial Intelligence
  (https://pubs.rsna.org/journal/ai), which is published by the Radiological
  Society of North America (RSNA)
- **Journal**: None
- **Summary**: Purpose: To evaluate a fully-automated deep-learning-based method for assessment of intracranial carotid artery calcification (ICAC). Methods: Two observers manually delineated ICAC in non-contrast CT scans of 2,319 participants (mean age 69 (SD 7) years; 1154 women) of the Rotterdam Study, prospectively collected between 2003 and 2006. These data were used to retrospectively develop and validate a deep-learning-based method for automated ICAC delineation and volume measurement. To evaluate the method, we compared manual and automatic assessment (computed using ten-fold cross-validation) with respect to 1) the agreement with an independent observer's assessment (available in a random subset of 47 scans); 2) the accuracy in delineating ICAC as judged via blinded visual comparison by an expert; 3) the association with first stroke incidence from the scan date until 2012. All method performance metrics were computed using 10-fold cross-validation. Results: The automated delineation of ICAC reached sensitivity of 83.8% and positive predictive value (PPV) of 88%. The intraclass correlation between automatic and manual ICAC volume measures was 0.98 (95% CI: 0.97, 0.98; computed in the entire dataset). Measured between the assessments of independent observers, sensitivity was 73.9%, PPV was 89.5%, and intraclass correlation was 0.91 (95% CI: 0.84, 0.95; computed in the 47-scan subset). In the blinded visual comparisons, automatic delineations were more accurate than manual ones (p-value = 0.01). The association of ICAC volume with incident stroke was similarly strong for both automated (hazard ratio, 1.38 (95% CI: 1.12, 1.75) and manually measured volumes (hazard ratio, 1.48 (95% CI: 1.20, 1.87)). Conclusions: The developed model was capable of automated segmentation and volume quantification of ICAC with accuracy comparable to human experts.



### Critic Guided Segmentation of Rewarding Objects in First-Person Views
- **Arxiv ID**: http://arxiv.org/abs/2107.09540v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.09540v1)
- **Published**: 2021-07-20 14:54:43+00:00
- **Updated**: 2021-07-20 14:54:43+00:00
- **Authors**: Andrew Melnik, Augustin Harter, Christian Limberg, Krishan Rana, Niko Suenderhauf, Helge Ritter
- **Comment**: None
- **Journal**: None
- **Summary**: This work discusses a learning approach to mask rewarding objects in images using sparse reward signals from an imitation learning dataset. For that, we train an Hourglass network using only feedback from a critic model. The Hourglass network learns to produce a mask to decrease the critic's score of a high score image and increase the critic's score of a low score image by swapping the masked areas between these two images. We trained the model on an imitation learning dataset from the NeurIPS 2020 MineRL Competition Track, where our model learned to mask rewarding objects in a complex interactive 3D environment with a sparse reward signal. This approach was part of the 1st place winning solution in this competition. Video demonstration and code: https://rebrand.ly/critic-guided-segmentation



### Data synthesis and adversarial networks: A review and meta-analysis in cancer imaging
- **Arxiv ID**: http://arxiv.org/abs/2107.09543v2
- **DOI**: 10.1016/j.media.2022.102704
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.09543v2)
- **Published**: 2021-07-20 14:57:51+00:00
- **Updated**: 2022-11-27 18:51:19+00:00
- **Authors**: Richard Osuala, Kaisar Kushibar, Lidia Garrucho, Akis Linardos, Zuzanna Szafranowska, Stefan Klein, Ben Glocker, Oliver Diaz, Karim Lekadir
- **Comment**: v2, 51 pages, 15 Figures, 9 Tables, accepted for publication in
  Medical Image Analysis
- **Journal**: Medical Image Analysis (2022)
- **Summary**: Despite technological and medical advances, the detection, interpretation, and treatment of cancer based on imaging data continue to pose significant challenges. These include inter-observer variability, class imbalance, dataset shifts, inter- and intra-tumour heterogeneity, malignancy determination, and treatment effect uncertainty. Given the recent advancements in Generative Adversarial Networks (GANs), data synthesis, and adversarial training, we assess the potential of these technologies to address a number of key challenges of cancer imaging. We categorise these challenges into (a) data scarcity and imbalance, (b) data access and privacy, (c) data annotation and segmentation, (d) cancer detection and diagnosis, and (e) tumour profiling, treatment planning and monitoring. Based on our analysis of 164 publications that apply adversarial training techniques in the context of cancer imaging, we highlight multiple underexplored solutions with research potential. We further contribute the Synthesis Study Trustworthiness Test (SynTRUST), a meta-analysis framework for assessing the validation rigour of medical image synthesis studies. SynTRUST is based on 26 concrete measures of thoroughness, reproducibility, usefulness, scalability, and tenability. Based on SynTRUST, we analyse 16 of the most promising cancer imaging challenge solutions and observe a high validation rigour in general, but also several desirable improvements. With this work, we strive to bridge the gap between the needs of the clinical cancer imaging community and the current and prospective research on data synthesis and adversarial networks in the artificial intelligence community.



### SynthSeg: Segmentation of brain MRI scans of any contrast and resolution without retraining
- **Arxiv ID**: http://arxiv.org/abs/2107.09559v4
- **DOI**: 10.1016/j.media.2023.102789
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.09559v4)
- **Published**: 2021-07-20 15:22:16+00:00
- **Updated**: 2023-02-28 15:46:52+00:00
- **Authors**: Benjamin Billot, Douglas N. Greve, Oula Puonti, Axel Thielscher, Koen Van Leemput, Bruce Fischl, Adrian V. Dalca, Juan Eugenio Iglesias
- **Comment**: None
- **Journal**: Medical Image Analysis (83), May 2023, pp 102789
- **Summary**: Despite advances in data augmentation and transfer learning, convolutional neural networks (CNNs) difficultly generalise to unseen domains. When segmenting brain scans, CNNs are highly sensitive to changes in resolution and contrast: even within the same MRI modality, performance can decrease across datasets. Here we introduce SynthSeg, the first segmentation CNN robust against changes in contrast and resolution. SynthSeg is trained with synthetic data sampled from a generative model conditioned on segmentations. Crucially, we adopt a domain randomisation strategy where we fully randomise the contrast and resolution of the synthetic training data. Consequently, SynthSeg can segment real scans from a wide range of target domains without retraining or fine-tuning, which enables straightforward analysis of huge amounts of heterogeneous clinical data. Because SynthSeg only requires segmentations to be trained (no images), it can learn from labels obtained by automated methods on diverse populations (e.g., ageing and diseased), thus achieving robustness to a wide range of morphological variability. We demonstrate SynthSeg on 5,000 scans of six modalities (including CT) and ten resolutions, where it exhibits unparalleled generalisation compared with supervised CNNs, state-of-the-art domain adaptation, and Bayesian segmentation. Finally, we demonstrate the generalisability of SynthSeg by applying it to cardiac MRI and CT scans.



### Characterizing Generalization under Out-Of-Distribution Shifts in Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.09562v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.09562v2)
- **Published**: 2021-07-20 15:26:09+00:00
- **Updated**: 2021-11-29 12:56:08+00:00
- **Authors**: Timo Milbich, Karsten Roth, Samarth Sinha, Ludwig Schmidt, Marzyeh Ghassemi, BjÃ¶rn Ommer
- **Comment**: 35th Conference on Neural Information Processing Systems (NeurIPS
  2021)
- **Journal**: None
- **Summary**: Deep Metric Learning (DML) aims to find representations suitable for zero-shot transfer to a priori unknown test distributions. However, common evaluation protocols only test a single, fixed data split in which train and test classes are assigned randomly. More realistic evaluations should consider a broad spectrum of distribution shifts with potentially varying degree and difficulty. In this work, we systematically construct train-test splits of increasing difficulty and present the ooDML benchmark to characterize generalization under out-of-distribution shifts in DML. ooDML is designed to probe the generalization performance on much more challenging, diverse train-to-test distribution shifts. Based on our new benchmark, we conduct a thorough empirical analysis of state-of-the-art DML methods. We find that while generalization tends to consistently degrade with difficulty, some methods are better at retaining performance as the distribution shift increases. Finally, we propose few-shot DML as an efficient way to consistently improve generalization in response to unknown test shifts presented in ooDML. Code available here: https://github.com/CompVis/Characterizing_Generalization_in_DML.



### Active 3D Shape Reconstruction from Vision and Touch
- **Arxiv ID**: http://arxiv.org/abs/2107.09584v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.09584v2)
- **Published**: 2021-07-20 15:56:52+00:00
- **Updated**: 2021-10-26 14:15:32+00:00
- **Authors**: Edward J. Smith, David Meger, Luis Pineda, Roberto Calandra, Jitendra Malik, Adriana Romero, Michal Drozdzal
- **Comment**: None
- **Journal**: Published at Neurips 2021
- **Summary**: Humans build 3D understandings of the world through active object exploration, using jointly their senses of vision and touch. However, in 3D shape reconstruction, most recent progress has relied on static datasets of limited sensory data such as RGB images, depth maps or haptic readings, leaving the active exploration of the shape largely unexplored. Inactive touch sensing for 3D reconstruction, the goal is to actively select the tactile readings that maximize the improvement in shape reconstruction accuracy. However, the development of deep learning-based active touch models is largely limited by the lack of frameworks for shape exploration. In this paper, we focus on this problem and introduce a system composed of: 1) a haptic simulator leveraging high spatial resolution vision-based tactile sensors for active touching of 3D objects; 2)a mesh-based 3D shape reconstruction model that relies on tactile or visuotactile signals; and 3) a set of data-driven solutions with either tactile or visuotactile priors to guide the shape exploration. Our framework enables the development of the first fully data-driven solutions to active touch on top of learned models for object understanding. Our experiments show the benefits of such solutions in the task of 3D shape understanding where our models consistently outperform natural baselines. We provide our framework as a tool to foster future research in this direction.



### DSP: Dual Soft-Paste for Unsupervised Domain Adaptive Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.09600v3
- **DOI**: 10.1145/3474085.3475186
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.09600v3)
- **Published**: 2021-07-20 16:22:40+00:00
- **Updated**: 2021-07-27 15:41:13+00:00
- **Authors**: Li Gao, Jing Zhang, Lefei Zhang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) for semantic segmentation aims to adapt a segmentation model trained on the labeled source domain to the unlabeled target domain. Existing methods try to learn domain invariant features while suffering from large domain gaps that make it difficult to correctly align discrepant features, especially in the initial training phase. To address this issue, we propose a novel Dual Soft-Paste (DSP) method in this paper. Specifically, DSP selects some classes from a source domain image using a long-tail class first sampling strategy and softly pastes the corresponding image patch on both the source and target training images with a fusion weight. Technically, we adopt the mean teacher framework for domain adaptation, where the pasted source and target images go through the student network while the original target image goes through the teacher network. Output-level alignment is carried out by aligning the probability maps of the target fused image from both networks using a weighted cross-entropy loss. In addition, feature-level alignment is carried out by aligning the feature maps of the source and target images from student network using a weighted maximum mean discrepancy loss. DSP facilitates the model learning domain-invariant features from the intermediate domains, leading to faster convergence and better performance. Experiments on two challenging benchmarks demonstrate the superiority of DSP over state-of-the-art methods. Code is available at \url{https://github.com/GaoLii/DSP}.



### QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries
- **Arxiv ID**: http://arxiv.org/abs/2107.09609v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2107.09609v2)
- **Published**: 2021-07-20 16:42:58+00:00
- **Updated**: 2021-11-29 18:35:51+00:00
- **Authors**: Jie Lei, Tamara L. Berg, Mohit Bansal
- **Comment**: Accepted to NeurIPS 2021
- **Journal**: None
- **Summary**: Detecting customized moments and highlights from videos given natural language (NL) user queries is an important but under-studied topic. One of the challenges in pursuing this direction is the lack of annotated data. To address this issue, we present the Query-based Video Highlights (QVHIGHLIGHTS) dataset. It consists of over 10,000 YouTube videos, covering a wide range of topics, from everyday activities and travel in lifestyle vlog videos to social and political activities in news videos. Each video in the dataset is annotated with: (1) a human-written free-form NL query, (2) relevant moments in the video w.r.t. the query, and (3) five-point scale saliency scores for all query-relevant clips. This comprehensive annotation enables us to develop and evaluate systems that detect relevant moments as well as salient highlights for diverse, flexible user queries. We also present a strong baseline for this task, Moment-DETR, a transformer encoder-decoder model that views moment retrieval as a direct set prediction problem, taking extracted video and query representations as inputs and predicting moment coordinates and saliency scores end-to-end. While our model does not utilize any human prior, we show that it performs competitively when compared to well-engineered architectures. With weakly supervised pretraining using ASR captions, MomentDETR substantially outperforms previous methods. Lastly, we present several ablations and visualizations of Moment-DETR. Data and code is publicly available at https://github.com/jayleicn/moment_detr



### Saliency for free: Saliency prediction as a side-effect of object recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.09628v1
- **DOI**: 10.1016/j.patrec.2021.05.015
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09628v1)
- **Published**: 2021-07-20 17:17:28+00:00
- **Updated**: 2021-07-20 17:17:28+00:00
- **Authors**: Carola Figueroa-Flores, David Berga, Joost van der Weijer, Bogdan Raducanu
- **Comment**: Paper published to Pattern Recognition Letter
- **Journal**: 2021
- **Summary**: Saliency is the perceptual capacity of our visual system to focus our attention (i.e. gaze) on relevant objects. Neural networks for saliency estimation require ground truth saliency maps for training which are usually achieved via eyetracking experiments. In the current paper, we demonstrate that saliency maps can be generated as a side-effect of training an object recognition deep neural network that is endowed with a saliency branch. Such a network does not require any ground-truth saliency maps for training.Extensive experiments carried out on both real and synthetic saliency datasets demonstrate that our approach is able to generate accurate saliency maps, achieving competitive results on both synthetic and real datasets when compared to methods that do require ground truth data.



### Towards Privacy-preserving Explanations in Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2107.09652v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09652v1)
- **Published**: 2021-07-20 17:35:36+00:00
- **Updated**: 2021-07-20 17:35:36+00:00
- **Authors**: H. Montenegro, W. Silva, J. S. Cardoso
- **Comment**: 7 pages, 5 figures, accepted at Workshop on Interpretable ML in
  Healthcare at ICML2021
- **Journal**: None
- **Summary**: The use of Deep Learning in the medical field is hindered by the lack of interpretability. Case-based interpretability strategies can provide intuitive explanations for deep learning models' decisions, thus, enhancing trust. However, the resulting explanations threaten patient privacy, motivating the development of privacy-preserving methods compatible with the specifics of medical data. In this work, we analyze existing privacy-preserving methods and their respective capacity to anonymize medical data while preserving disease-related semantic features. We find that the PPRL-VGAN deep learning method was the best at preserving the disease-related semantic features while guaranteeing a high level of privacy among the compared state-of-the-art methods. Nevertheless, we emphasize the need to improve privacy-preserving methods for medical imaging, as we identified relevant drawbacks in all existing privacy-preserving approaches.



### 3D-StyleGAN: A Style-Based Generative Adversarial Network for Generative Modeling of Three-Dimensional Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2107.09700v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68T07 (Primary) 68T01 (Secondary), I.2; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2107.09700v1)
- **Published**: 2021-07-20 18:08:27+00:00
- **Updated**: 2021-07-20 18:08:27+00:00
- **Authors**: Sungmin Hong, Razvan Marinescu, Adrian V. Dalca, Anna K. Bonkhoff, Martin Bretzner, Natalia S. Rost, Polina Golland
- **Comment**: 11 pages, 6 figures, 2 tables. Provisionally Accepted at DGM4MICCAI
  workshop in MICCAI 2021
- **Journal**: None
- **Summary**: Image synthesis via Generative Adversarial Networks (GANs) of three-dimensional (3D) medical images has great potential that can be extended to many medical applications, such as, image enhancement and disease progression modeling. However, current GAN technologies for 3D medical image synthesis need to be significantly improved to be readily adapted to real-world medical problems. In this paper, we extend the state-of-the-art StyleGAN2 model, which natively works with two-dimensional images, to enable 3D image synthesis. In addition to the image synthesis, we investigate the controllability and interpretability of the 3D-StyleGAN via style vectors inherited form the original StyleGAN2 that are highly suitable for medical applications: (i) the latent space projection and reconstruction of unseen real images, and (ii) style mixing. We demonstrate the 3D-StyleGAN's performance and feasibility with ~12,000 three-dimensional full brain MR T1 images, although it can be applied to any 3D volumetric images. Furthermore, we explore different configurations of hyperparameters to investigate potential improvement of the image synthesis with larger networks. The codes and pre-trained networks are available online: https://github.com/sh4174/3DStyleGAN.



### Registration of 3D Point Sets Using Correntropy Similarity Matrix
- **Arxiv ID**: http://arxiv.org/abs/2107.09725v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09725v1)
- **Published**: 2021-07-20 18:56:22+00:00
- **Updated**: 2021-07-20 18:56:22+00:00
- **Authors**: Ashutosh Singandhupe, Hung La, Trung Dung Ngo, Van Ho
- **Comment**: None
- **Journal**: None
- **Summary**: This work focuses on Registration or Alignment of 3D point sets. Although the Registration problem is a well established problem and it's solved using multiple variants of Iterative Closest Point (ICP) Algorithm, most of the approaches in the current state of the art still suffers from misalignment when the \textit{Source} and the \textit{Target} point sets are separated by large rotations and translation. In this work, we propose a variant of the Standard ICP algorithm, where we introduce a Correntropy Relationship Matrix in the computation of rotation and translation component which attempts to solve the large rotation and translation problem between \textit{Source} and \textit{Target} point sets. This matrix is created through correntropy criterion which is updated in every iteration. The correntropy criterion defined in this approach maintains the relationship between the points in the \textit{Source} dataset and the \textit{Target} dataset. Through our experiments and validation we verify that our approach has performed well under various rotation and translation in comparison to the other well-known state of the art methods available in the Point Cloud Library (PCL) as well as other methods available as open source. We have uploaded our code in the github repository for the readers to validate and verify our approach https://github.com/aralab-unr/CoSM-ICP.



### Unsupervised Domain Adaptation in LiDAR Semantic Segmentation with Self-Supervision and Gated Adapters
- **Arxiv ID**: http://arxiv.org/abs/2107.09783v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09783v3)
- **Published**: 2021-07-20 21:57:18+00:00
- **Updated**: 2022-03-21 21:23:32+00:00
- **Authors**: Mrigank Rochan, Shubhra Aich, Eduardo R. Corral-Soto, Amir Nabatchian, Bingbing Liu
- **Comment**: Accepted to the 2022 IEEE International Conference on Robotics and
  Automation (ICRA)
- **Journal**: None
- **Summary**: In this paper, we focus on a less explored, but more realistic and complex problem of domain adaptation in LiDAR semantic segmentation. There is a significant drop in performance of an existing segmentation model when training (source domain) and testing (target domain) data originate from different LiDAR sensors. To overcome this shortcoming, we propose an unsupervised domain adaptation framework that leverages unlabeled target domain data for self-supervision, coupled with an unpaired mask transfer strategy to mitigate the impact of domain shifts. Furthermore, we introduce the gated adapter module with a small number of parameters into the network to account for target domain-specific information. Experiments adapting from both real-to-real and synthetic-to-real LiDAR semantic segmentation benchmarks demonstrate the significant improvement over prior arts.



