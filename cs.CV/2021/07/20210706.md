# Arxiv Papers in cs.CV on 2021-07-06
### Polarized skylight orientation determination artificial neural network
- **Arxiv ID**: http://arxiv.org/abs/2107.02328v1
- **DOI**: 10.1364/AO.453177
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.02328v1)
- **Published**: 2021-07-06 00:19:22+00:00
- **Updated**: 2021-07-06 00:19:22+00:00
- **Authors**: Huaju Liang, Hongyang Bai, Ke Hu, Xinbo Lv
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an artificial neural network to determine orientation using polarized skylight. This neural network has specific dilated convolution, which can extract light intensity information of different polarization directions. Then, the degree of polarization (DOP) and angle of polarization (AOP) are directly extracted in the network. In addition, the exponential function encoding of orientation is designed as the network output, which can better reflect the insect's encoding of polarization information, and improve the accuracy of orientation determination. Finally, training and testing were conducted on a public polarized skylight navigation dataset, and the experimental results proved the stability and effectiveness of the network.



### Confidence Conditioned Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2107.06993v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.06993v1)
- **Published**: 2021-07-06 00:33:25+00:00
- **Updated**: 2021-07-06 00:33:25+00:00
- **Authors**: Sourav Mishra, Suresh Sundaram
- **Comment**: 31 pages, 41 references, 5 figures, 9 tables
- **Journal**: None
- **Summary**: In this paper, a novel confidence conditioned knowledge distillation (CCKD) scheme for transferring the knowledge from a teacher model to a student model is proposed. Existing state-of-the-art methods employ fixed loss functions for this purpose and ignore the different levels of information that need to be transferred for different samples. In addition to that, these methods are also inefficient in terms of data usage. CCKD addresses these issues by leveraging the confidence assigned by the teacher model to the correct class to devise sample-specific loss functions (CCKD-L formulation) and targets (CCKD-T formulation). Further, CCKD improves the data efficiency by employing self-regulation to stop those samples from participating in the distillation process on which the student model learns faster. Empirical evaluations on several benchmark datasets show that CCKD methods achieve at least as much generalization performance levels as other state-of-the-art methods while being data efficient in the process. Student models trained through CCKD methods do not retain most of the misclassifications commited by the teacher model on the training set. Distillation through CCKD methods improves the resilience of the student models against adversarial attacks compared to the conventional KD method. Experiments show at least 3% increase in performance against adversarial attacks for the MNIST and the Fashion MNIST datasets, and at least 6% increase for the CIFAR10 dataset.



### Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2107.02331v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02331v1)
- **Published**: 2021-07-06 00:52:11+00:00
- **Updated**: 2021-07-06 00:52:11+00:00
- **Authors**: Siddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, Christopher D. Manning
- **Comment**: Accepted at ACL-IJCNLP 2021. 17 pages, 16 Figures
- **Journal**: None
- **Summary**: Active learning promises to alleviate the massive data needs of supervised machine learning: it has successfully improved sample efficiency by an order of magnitude on traditional tasks like topic classification and object recognition. However, we uncover a striking contrast to this promise: across 5 models and 4 datasets on the task of visual question answering, a wide variety of active learning approaches fail to outperform random selection. To understand this discrepancy, we profile 8 active learning methods on a per-example basis, and identify the problem as collective outliers -- groups of examples that active learning methods prefer to acquire but models fail to learn (e.g., questions that ask about text in images or require external knowledge). Through systematic ablation experiments and qualitative visualizations, we verify that collective outliers are a general phenomenon responsible for degrading pool-based active learning. Notably, we show that active learning sample efficiency increases significantly as the number of collective outliers in the active learning pool decreases. We conclude with a discussion and prescriptive recommendations for mitigating the effects of these outliers in future work.



### Impact of deep learning-based image super-resolution on binary signal detection
- **Arxiv ID**: http://arxiv.org/abs/2107.02338v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2107.02338v1)
- **Published**: 2021-07-06 01:27:32+00:00
- **Updated**: 2021-07-06 01:27:32+00:00
- **Authors**: Xiaohui Zhang, Varun A. Kelkar, Jason Granstedt, Hua Li, Mark A. Anastasio
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based image super-resolution (DL-SR) has shown great promise in medical imaging applications. To date, most of the proposed methods for DL-SR have only been assessed by use of traditional measures of image quality (IQ) that are commonly employed in the field of computer vision. However, the impact of these methods on objective measures of image quality that are relevant to medical imaging tasks remains largely unexplored. In this study, we investigate the impact of DL-SR methods on binary signal detection performance. Two popular DL-SR methods, the super-resolution convolutional neural network (SRCNN) and the super-resolution generative adversarial network (SRGAN), were trained by use of simulated medical image data. Binary signal-known-exactly with background-known-statistically (SKE/BKS) and signal-known-statistically with background-known-statistically (SKS/BKS) detection tasks were formulated. Numerical observers, which included a neural network-approximated ideal observer and common linear numerical observers, were employed to assess the impact of DL-SR on task performance. The impact of the complexity of the DL-SR network architectures on task-performance was quantified. In addition, the utility of DL-SR for improving the task-performance of sub-optimal observers was investigated. Our numerical experiments confirmed that, as expected, DL-SR could improve traditional measures of IQ. However, for many of the study designs considered, the DL-SR methods provided little or no improvement in task performance and could even degrade it. It was observed that DL-SR could improve the task-performance of sub-optimal observers under certain conditions. The presented study highlights the urgent need for the objective assessment of DL-SR methods and suggests avenues for improving their efficacy in medical imaging applications.



### Feature Fusion Vision Transformer for Fine-Grained Visual Categorization
- **Arxiv ID**: http://arxiv.org/abs/2107.02341v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02341v3)
- **Published**: 2021-07-06 01:48:43+00:00
- **Updated**: 2022-02-28 20:31:54+00:00
- **Authors**: Jun Wang, Xiaohan Yu, Yongsheng Gao
- **Comment**: This paper was accepted by BMVC2021
- **Journal**: None
- **Summary**: The core for tackling the fine-grained visual categorization (FGVC) is to learn subtle yet discriminative features. Most previous works achieve this by explicitly selecting the discriminative parts or integrating the attention mechanism via CNN-based approaches.However, these methods enhance the computational complexity and make the modeldominated by the regions containing the most of the objects. Recently, vision trans-former (ViT) has achieved SOTA performance on general image recognition tasks. Theself-attention mechanism aggregates and weights the information from all patches to the classification token, making it perfectly suitable for FGVC. Nonetheless, the classifi-cation token in the deep layer pays more attention to the global information, lacking the local and low-level features that are essential for FGVC. In this work, we proposea novel pure transformer-based framework Feature Fusion Vision Transformer (FFVT)where we aggregate the important tokens from each transformer layer to compensate thelocal, low-level and middle-level information. We design a novel token selection mod-ule called mutual attention weight selection (MAWS) to guide the network effectively and efficiently towards selecting discriminative tokens without introducing extra param-eters. We verify the effectiveness of FFVT on three benchmarks where FFVT achieves the state-of-the-art performance.



### Domain Adaptation via CycleGAN for Retina Segmentation in Optical Coherence Tomography
- **Arxiv ID**: http://arxiv.org/abs/2107.02345v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2107.02345v1)
- **Published**: 2021-07-06 02:07:53+00:00
- **Updated**: 2021-07-06 02:07:53+00:00
- **Authors**: Ricky Chen, Timothy T. Yu, Gavin Xu, Da Ma, Marinko V. Sarunic, Mirza Faisal Beg
- **Comment**: 10 pages, 6 figures, 1 table
- **Journal**: None
- **Summary**: With the FDA approval of Artificial Intelligence (AI) for point-of-care clinical diagnoses, model generalizability is of the utmost importance as clinical decision-making must be domain-agnostic. A method of tackling the problem is to increase the dataset to include images from a multitude of domains; while this technique is ideal, the security requirements of medical data is a major limitation. Additionally, researchers with developed tools benefit from the addition of open-sourced data, but are limited by the difference in domains. Herewith, we investigated the implementation of a Cycle-Consistent Generative Adversarial Networks (CycleGAN) for the domain adaptation of Optical Coherence Tomography (OCT) volumes. This study was done in collaboration with the Biomedical Optics Research Group and Functional & Anatomical Imaging & Shape Analysis Lab at Simon Fraser University. In this study, we investigated a learning-based approach of adapting the domain of a publicly available dataset, UK Biobank dataset (UKB). To evaluate the performance of domain adaptation, we utilized pre-existing retinal layer segmentation tools developed on a different set of RETOUCH OCT data. This study provides insight on state-of-the-art tools for domain adaptation compared to traditional processing techniques as well as a pipeline for adapting publicly available retinal data to the domains previously used by our collaborators.



### An Ensemble Noise-Robust K-fold Cross-Validation Selection Method for Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2107.02347v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02347v1)
- **Published**: 2021-07-06 02:14:52+00:00
- **Updated**: 2021-07-06 02:14:52+00:00
- **Authors**: Yong Wen, Marcus Kalander, Chanfei Su, Lujia Pan
- **Comment**: Accepted by the IJCAI2021 Weakly Supervised Representation Learning
  (WSRL) Workshop
- **Journal**: None
- **Summary**: We consider the problem of training robust and accurate deep neural networks (DNNs) when subject to various proportions of noisy labels. Large-scale datasets tend to contain mislabeled samples that can be memorized by DNNs, impeding the performance. With appropriate handling, this degradation can be alleviated. There are two problems to consider: how to distinguish clean samples and how to deal with noisy samples. In this paper, we present Ensemble Noise-robust K-fold Cross-Validation Selection (E-NKCVS) to effectively select clean samples from noisy data, solving the first problem. For the second problem, we create a new pseudo label for any sample determined to have an uncertain or likely corrupt label. E-NKCVS obtains multiple predicted labels for each sample and the entropy of these labels is used to tune the weight given to the pseudo label and the given label. Theoretical analysis and extensive verification of the algorithms in the noisy label setting are provided. We evaluate our approach on various image and text classification tasks where the labels have been manually corrupted with different noise ratios. Additionally, two large real-world noisy datasets are also used, Clothing-1M and WebVision. E-NKCVS is empirically shown to be highly tolerant to considerable proportions of label noise and has a consistent improvement over state-of-the-art methods. Especially on more difficult datasets with higher noise ratios, we can achieve a significant improvement over the second-best model. Moreover, our proposed approach can easily be integrated into existing DNN methods to improve their robustness against label noise.



### UACANet: Uncertainty Augmented Context Attention for Polyp Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.02368v3
- **DOI**: 10.1145/3474085.3475375
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02368v3)
- **Published**: 2021-07-06 03:11:12+00:00
- **Updated**: 2021-07-22 00:20:25+00:00
- **Authors**: Taehun Kim, Hyemin Lee, Daijin Kim
- **Comment**: 9 pages, 6 figures, 4 tables. To appear in the Proceedings of the
  29th ACM International Conference on Multimedia (ACM MM '21), October 20-24,
  2021, Chengdu, China. DOI will be added soon
- **Journal**: None
- **Summary**: We propose Uncertainty Augmented Context Attention network (UACANet) for polyp segmentation which consider a uncertain area of the saliency map. We construct a modified version of U-Net shape network with additional encoder and decoder and compute a saliency map in each bottom-up stream prediction module and propagate to the next prediction module. In each prediction module, previously predicted saliency map is utilized to compute foreground, background and uncertain area map and we aggregate the feature map with three area maps for each representation. Then we compute the relation between each representation and each pixel in the feature map. We conduct experiments on five popular polyp segmentation benchmarks, Kvasir, CVC-ClinicDB, ETIS, CVC-ColonDB and CVC-300, and achieve state-of-the-art performance. Especially, we achieve 76.6% mean Dice on ETIS dataset which is 13.8% improvement compared to the previous state-of-the-art method. Source code is publicly available at https://github.com/plemeri/UACANet



### Learning Disentangled Representation Implicitly via Transformer for Occluded Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2107.02380v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02380v1)
- **Published**: 2021-07-06 04:24:10+00:00
- **Updated**: 2021-07-06 04:24:10+00:00
- **Authors**: Mengxi Jia, Xinhua Cheng, Shijian Lu, Jian Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (re-ID) under various occlusions has been a long-standing challenge as person images with different types of occlusions often suffer from misalignment in image matching and ranking. Most existing methods tackle this challenge by aligning spatial features of body parts according to external semantic cues or feature similarities but this alignment approach is complicated and sensitive to noises. We design DRL-Net, a disentangled representation learning network that handles occluded re-ID without requiring strict person image alignment or any additional supervision. Leveraging transformer architectures, DRL-Net achieves alignment-free re-ID via global reasoning of local features of occluded person images. It measures image similarity by automatically disentangling the representation of undefined semantic components, e.g., human body parts or obstacles, under the guidance of semantic preference object queries in the transformer. In addition, we design a decorrelation constraint in the transformer decoder and impose it over object queries for better focus on different semantic components. To better eliminate interference from occlusions, we design a contrast feature learning technique (CFL) for better separation of occlusion features and discriminative ID features. Extensive experiments over occluded and holistic re-ID benchmarks (Occluded-DukeMTMC, Market1501 and DukeMTMC) show that the DRL-Net achieves superior re-ID performance consistently and outperforms the state-of-the-art by large margins for Occluded-DukeMTMC.



### Learning Semantic Segmentation of Large-Scale Point Clouds with Random Sampling
- **Arxiv ID**: http://arxiv.org/abs/2107.02389v1
- **DOI**: 10.1109/TPAMI.2021.3083288
- **Categories**: **cs.CV**, cs.AI, cs.RO, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2107.02389v1)
- **Published**: 2021-07-06 05:08:34+00:00
- **Updated**: 2021-07-06 05:08:34+00:00
- **Authors**: Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, Andrew Markham
- **Comment**: IEEE TPAMI 2021. arXiv admin note: substantial text overlap with
  arXiv:1911.11236
- **Journal**: None
- **Summary**: We study the problem of efficient semantic segmentation of large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre/post-processing steps, most existing approaches are only able to be trained and operate over small-scale point clouds. In this paper, we introduce RandLA-Net, an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. The key to our approach is to use random point sampling instead of more complex point selection approaches. Although remarkably computation and memory efficient, random sampling can discard key features by chance. To overcome this, we introduce a novel local feature aggregation module to progressively increase the receptive field for each 3D point, thereby effectively preserving geometric details. Comparative experiments show that our RandLA-Net can process 1 million points in a single pass up to 200x faster than existing approaches. Moreover, extensive experiments on five large-scale point cloud datasets, including Semantic3D, SemanticKITTI, Toronto3D, NPM3D and S3DIS, demonstrate the state-of-the-art semantic segmentation performance of our RandLA-Net.



### MSE Loss with Outlying Label for Imbalanced Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.02393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02393v1)
- **Published**: 2021-07-06 05:17:00+00:00
- **Updated**: 2021-07-06 05:17:00+00:00
- **Authors**: Sota Kato, Kazuhiro Hotta
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose mean squared error (MSE) loss with outlying label for class imbalanced classification. Cross entropy (CE) loss, which is widely used for image recognition, is learned so that the probability value of true class is closer to one by back propagation. However, for imbalanced datasets, the learning is insufficient for the classes with a small number of samples. Therefore, we propose a novel classification method using the MSE loss that can be learned the relationships of all classes no matter which image is input. Unlike CE loss, MSE loss is possible to equalize the number of back propagation for all classes and to learn the feature space considering the relationships between classes as metric learning. Furthermore, instead of the usual one-hot teacher label, we use a novel teacher label that takes the number of class samples into account. This induces the outlying label which depends on the number of samples in each class, and the class with a small number of samples has outlying margin in a feature space. It is possible to create the feature space for separating high-difficulty classes and low-difficulty classes. By the experiments on imbalanced classification and semantic segmentation, we confirmed that the proposed method was much improved in comparison with standard CE loss and conventional methods, even though only the loss and teacher labels were changed.



### Semi-TCL: Semi-Supervised Track Contrastive Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.02396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02396v1)
- **Published**: 2021-07-06 05:23:30+00:00
- **Updated**: 2021-07-06 05:23:30+00:00
- **Authors**: Wei Li, Yuanjun Xiong, Shuo Yang, Mingze Xu, Yongxin Wang, Wei Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Online tracking of multiple objects in videos requires strong capacity of modeling and matching object appearances. Previous methods for learning appearance embedding mostly rely on instance-level matching without considering the temporal continuity provided by videos. We design a new instance-to-track matching objective to learn appearance embedding that compares a candidate detection to the embedding of the tracks persisted in the tracker. It enables us to learn not only from videos labeled with complete tracks, but also unlabeled or partially labeled videos. We implement this learning objective in a unified form following the spirit of constrastive loss. Experiments on multiple object tracking datasets demonstrate that our method can effectively learning discriminative appearance embeddings in a semi-supervised fashion and outperform state of the art methods on representative benchmarks.



### From General to Specific: Online Updating for Blind Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2107.02398v2
- **DOI**: 10.1016/j.patcog.2022.108613
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02398v2)
- **Published**: 2021-07-06 05:25:16+00:00
- **Updated**: 2022-03-15 06:58:02+00:00
- **Authors**: Shang Li, Guixuan Zhang, Zhengxiong Luo, Jie Liu, Zhi Zeng, Shuwu Zhang
- **Comment**: Accepted by Pattern Recognition
- **Journal**: Pattern Recognition, Volume 127, July 2022, 108613
- **Summary**: Most deep learning-based super-resolution (SR) methods are not image-specific: 1) They are trained on samples synthesized by predefined degradations (e.g. bicubic downsampling), regardless of the domain gap between training and testing data. 2) During testing, they super-resolve all images by the same set of model weights, ignoring the degradation variety. As a result, most previous methods may suffer a performance drop when the degradations of test images are unknown and various (i.e. the case of blind SR). To address these issues, we propose an online SR (ONSR) method. It does not rely on predefined degradations and allows the model weights to be updated according to the degradation of the test image. Specifically, ONSR consists of two branches, namely internal branch (IB) and external branch (EB). IB could learn the specific degradation of the given test LR image, and EB could learn to super resolve images degraded by the learned degradation. In this way, ONSR could customize a specific model for each test image, and thus get more robust to various degradations. Extensive experiments on both synthesized and real-world images show that ONSR can generate more visually favorable SR results and achieve state-of-the-art performance in blind SR.



### NRST: Non-rigid Surface Tracking from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/2107.02407v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02407v2)
- **Published**: 2021-07-06 06:06:45+00:00
- **Updated**: 2021-07-12 08:55:46+00:00
- **Authors**: Marc Habermann, Weipeng Xu, Helge Rhodin, Michael Zollhoefer, Gerard Pons-Moll, Christian Theobalt
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an efficient method for non-rigid surface tracking from monocular RGB videos. Given a video and a template mesh, our algorithm sequentially registers the template non-rigidly to each frame. We formulate the per-frame registration as an optimization problem that includes a novel texture term specifically tailored towards tracking objects with uniform texture but fine-scale structure, such as the regular micro-structural patterns of fabric. Our texture term exploits the orientation information in the micro-structures of the objects, e.g., the yarn patterns of fabrics. This enables us to accurately track uniformly colored materials that have these high frequency micro-structures, for which traditional photometric terms are usually less effective. The results demonstrate the effectiveness of our method on both general textured non-rigid objects and monochromatic fabrics.



### CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation
- **Arxiv ID**: http://arxiv.org/abs/2107.02408v3
- **DOI**: 10.1145/3474085.3475535
- **Categories**: **cs.CV**, cs.CR, cs.LG, cs.MM, I.4.9; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2107.02408v3)
- **Published**: 2021-07-06 06:07:17+00:00
- **Updated**: 2021-08-05 05:53:57+00:00
- **Authors**: Minha Kim, Shahroz Tariq, Simon S. Woo
- **Comment**: 13 pages, 7 Figures, 13 Tables, Accepted for publication in the 29th
  ACM International Conference on Multimedia (ACMMM '21)
- **Journal**: None
- **Summary**: Over the last few decades, artificial intelligence research has made tremendous strides, but it still heavily relies on fixed datasets in stationary environments. Continual learning is a growing field of research that examines how AI systems can learn sequentially from a continuous stream of linked data in the same way that biological systems do. Simultaneously, fake media such as deepfakes and synthetic face images have emerged as significant to current multimedia technologies. Recently, numerous method has been proposed which can detect deepfakes with high accuracy. However, they suffer significantly due to their reliance on fixed datasets in limited evaluation settings. Therefore, in this work, we apply continuous learning to neural networks' learning dynamics, emphasizing its potential to increase data efficiency significantly. We propose Continual Representation using Distillation (CoReD) method that employs the concept of Continual Learning (CL), Representation Learning (RL), and Knowledge Distillation (KD). We design CoReD to perform sequential domain adaptation tasks on new deepfake and GAN-generated synthetic face datasets, while effectively minimizing the catastrophic forgetting in a teacher-student model setting. Our extensive experimental results demonstrate that our method is efficient at domain adaptation to detect low-quality deepfakes videos and GAN-generated images from several datasets, outperforming the-state-of-art baseline methods.



### Adapting Vehicle Detector to Target Domain by Adversarial Prediction Alignment
- **Arxiv ID**: http://arxiv.org/abs/2107.02411v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02411v1)
- **Published**: 2021-07-06 06:21:39+00:00
- **Updated**: 2021-07-06 06:21:39+00:00
- **Authors**: Yohei Koga, Hiroyuki Miyazaki, Ryosuke Shibasaki
- **Comment**: The accepted version of the article in IEEE International Geoscience
  and Remote Sensing Symposium (IGARSS) 2021. Copyright 2021 IEEE. Code
  available: https://github.com/monotaro3/vd_pred_align
- **Journal**: None
- **Summary**: While recent advancement of domain adaptation techniques is significant, most of methods only align a feature extractor and do not adapt a classifier to target domain, which would be a cause of performance degradation. We propose novel domain adaptation technique for object detection that aligns prediction output space. In addition to feature alignment, we aligned predictions of locations and class confidences of our vehicle detector for satellite images by adversarial training. The proposed method significantly improved AP score by over 5%, which shows effectivity of our method for object detection tasks in satellite images.



### Double-Uncertainty Guided Spatial and Temporal Consistency Regularization Weighting for Learning-based Abdominal Registration
- **Arxiv ID**: http://arxiv.org/abs/2107.02433v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02433v3)
- **Published**: 2021-07-06 07:19:49+00:00
- **Updated**: 2022-03-02 18:13:23+00:00
- **Authors**: Zhe Xu, Jie Luo, Donghuan Lu, Jiangpeng Yan, Sarah Frisken, Jayender Jagadeesan, William Wells III, Xiu Li, Yefeng Zheng, Raymond Tong
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: In order to tackle the difficulty associated with the ill-posed nature of the image registration problem, regularization is often used to constrain the solution space. For most learning-based registration approaches, the regularization usually has a fixed weight and only constrains the spatial transformation. Such convention has two limitations: (i) Besides the laborious grid search for the optimal fixed weight, the regularization strength of a specific image pair should be associated with the content of the images, thus the "one value fits all" training scheme is not ideal; (ii) Only spatially regularizing the transformation may neglect some informative clues related to the ill-posedness. In this study, we propose a mean-teacher based registration framework, which incorporates an additional temporal consistency regularization term by encouraging the teacher model's prediction to be consistent with that of the student model. More importantly, instead of searching for a fixed weight, the teacher enables automatically adjusting the weights of the spatial regularization and the temporal consistency regularization by taking advantage of the transformation uncertainty and appearance uncertainty. Extensive experiments on the challenging abdominal CT-MRI registration show that our training strategy can promisingly advance the original learning-based method in terms of efficient hyperparameter tuning and a better tradeoff between accuracy and smoothness.



### Self-Adversarial Training incorporating Forgery Attention for Image Forgery Localization
- **Arxiv ID**: http://arxiv.org/abs/2107.02434v2
- **DOI**: 10.1109/TIFS.2022.3152362
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2107.02434v2)
- **Published**: 2021-07-06 07:20:08+00:00
- **Updated**: 2022-02-03 04:14:46+00:00
- **Authors**: Long Zhuo, Shunquan Tan, Bin Li, Jiwu Huang
- **Comment**: accepted by TIFS
- **Journal**: None
- **Summary**: Image editing techniques enable people to modify the content of an image without leaving visual traces and thus may cause serious security risks. Hence the detection and localization of these forgeries become quite necessary and challenging. Furthermore, unlike other tasks with extensive data, there is usually a lack of annotated forged images for training due to annotation difficulties. In this paper, we propose a self-adversarial training strategy and a reliable coarse-to-fine network that utilizes a self-attention mechanism to localize forged regions in forgery images. The self-attention module is based on a Channel-Wise High Pass Filter block (CW-HPF). CW-HPF leverages inter-channel relationships of features and extracts noise features by high pass filters. Based on the CW-HPF, a self-attention mechanism, called forgery attention, is proposed to capture rich contextual dependencies of intrinsic inconsistency extracted from tampered regions. Specifically, we append two types of attention modules on top of CW-HPF respectively to model internal interdependencies in spatial dimension and external dependencies among channels. We exploit a coarse-to-fine network to enhance the noise inconsistency between original and tampered regions. More importantly, to address the issue of insufficient training data, we design a self-adversarial training strategy that expands training data dynamically to achieve more robust performance. Specifically, in each training iteration, we perform adversarial attacks against our network to generate adversarial examples and train our model on them. Extensive experimental results demonstrate that our proposed algorithm steadily outperforms state-of-the-art methods by a clear margin in different benchmark datasets.



### A Hierarchical Dual Model of Environment- and Place-Specific Utility for Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.02440v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02440v1)
- **Published**: 2021-07-06 07:38:47+00:00
- **Updated**: 2021-07-06 07:38:47+00:00
- **Authors**: Nikhil Varma Keetha, Michael Milford, Sourav Garg
- **Comment**: Accepted to IEEE Robotics and Automation Letters (RA-L) and IROS 2021
- **Journal**: None
- **Summary**: Visual Place Recognition (VPR) approaches have typically attempted to match places by identifying visual cues, image regions or landmarks that have high ``utility'' in identifying a specific place. But this concept of utility is not singular - rather it can take a range of forms. In this paper, we present a novel approach to deduce two key types of utility for VPR: the utility of visual cues `specific' to an environment, and to a particular place. We employ contrastive learning principles to estimate both the environment- and place-specific utility of Vector of Locally Aggregated Descriptors (VLAD) clusters in an unsupervised manner, which is then used to guide local feature matching through keypoint selection. By combining these two utility measures, our approach achieves state-of-the-art performance on three challenging benchmark datasets, while simultaneously reducing the required storage and compute time. We provide further analysis demonstrating that unsupervised cluster selection results in semantically meaningful results, that finer grained categorization often has higher utility for VPR than high level semantic categorization (e.g. building, road), and characterise how these two utility measures vary across different places and environments. Source code is made publicly available at https://github.com/Nik-V9/HEAPUtil.



### End-To-End Data-Dependent Routing in Multi-Path Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.02450v3
- **DOI**: None
- **Categories**: **cs.CV**, 68T10, I.2; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2107.02450v3)
- **Published**: 2021-07-06 07:58:07+00:00
- **Updated**: 2023-02-28 06:59:49+00:00
- **Authors**: Dumindu Tissera, Rukshan Wijessinghe, Kasun Vithanage, Alex Xavier, Subha Fernando, Ranga Rodrigo
- **Comment**: Neural Computing and Applications 2023
- **Journal**: None
- **Summary**: Neural networks are known to give better performance with increased depth due to their ability to learn more abstract features. Although the deepening of networks has been well established, there is still room for efficient feature extraction within a layer which would reduce the need for mere parameter increment. The conventional widening of networks by having more filters in each layer introduces a quadratic increment of parameters. Having multiple parallel convolutional/dense operations in each layer solves this problem, but without any context-dependent allocation of resources among these operations: the parallel computations tend to learn similar features making the widening process less effective. Therefore, we propose the use of multi-path neural networks with data-dependent resource allocation among parallel computations within layers, which also lets an input to be routed end-to-end through these parallel paths. To do this, we first introduce a cross-prediction based algorithm between parallel tensors of subsequent layers. Second, we further reduce the routing overhead by introducing feature-dependent cross-connections between parallel tensors of successive layers. Our multi-path networks show superior performance to existing widening and adaptive feature extraction, and even ensembles, and deeper networks at similar complexity in the image recognition task.



### Integrating Large Circular Kernels into CNNs through Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2107.02451v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.02451v4)
- **Published**: 2021-07-06 07:59:36+00:00
- **Updated**: 2022-04-16 03:38:06+00:00
- **Authors**: Kun He, Chao Li, Yixiao Yang, Gao Huang, John E. Hopcroft
- **Comment**: 20 pages, 10 figures, submitted to a conference
- **Journal**: None
- **Summary**: The square kernel is a standard unit for contemporary CNNs, as it fits well on the tensor computation for convolution operation. However, the retinal ganglion cells in the biological visual system have approximately concentric receptive fields. Motivated by this observation, we propose to use circular kernel with a concentric and isotropic receptive field as an option for the convolution operation. We first propose a simple yet efficient implementation of the convolution using circular kernels, and empirically show the significant advantages of large circular kernels over the counterpart square kernels. We then expand the operation space of several typical Neural Architecture Search (NAS) methods with the convolutions of large circular kernels. The searched new neural architectures do contain large circular kernels and outperform the original searched models considerably. Our additional analysis also reveals that large circular kernels could help the model to be more robust to the rotated or sheared images due to their better rotation invariance. Our work shows the potential of designing new convolutional kernels for CNNs, bringing up the prospect of expanding the search space of NAS with new variants of convolutions.



### Neural Mixture Models with Expectation-Maximization for End-to-end Deep Clustering
- **Arxiv ID**: http://arxiv.org/abs/2107.02453v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, 68T10, 62H30, I.2; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2107.02453v2)
- **Published**: 2021-07-06 08:00:58+00:00
- **Updated**: 2022-10-02 09:54:17+00:00
- **Authors**: Dumindu Tissera, Kasun Vithanage, Rukshan Wijesinghe, Alex Xavier, Sanath Jayasena, Subha Fernando, Ranga Rodrigo
- **Comment**: Accepted and published at Neurocomputing 2022
- **Journal**: None
- **Summary**: Any clustering algorithm must synchronously learn to model the clusters and allocate data to those clusters in the absence of labels. Mixture model-based methods model clusters with pre-defined statistical distributions and allocate data to those clusters based on the cluster likelihoods. They iteratively refine those distribution parameters and member assignments following the Expectation-Maximization (EM) algorithm. However, the cluster representability of such hand-designed distributions that employ a limited amount of parameters is not adequate for most real-world clustering tasks. In this paper, we realize mixture model-based clustering with a neural network where the final layer neurons, with the aid of an additional transformation, approximate cluster distribution outputs. The network parameters pose as the parameters of those distributions. The result is an elegant, much-generalized representation of clusters than a restricted mixture of hand-designed distributions. We train the network end-to-end via batch-wise EM iterations where the forward pass acts as the E-step and the backward pass acts as the M-step. In image clustering, the mixture-based EM objective can be used as the clustering objective along with existing representation learning methods. In particular, we show that when mixture-EM optimization is fused with consistency optimization, it improves the sole consistency optimization performance in clustering. Our trained networks outperform single-stage deep clustering methods that still depend on k-means, with unsupervised classification accuracy of 63.8% in STL10, 58% in CIFAR10, 25.9% in CIFAR100, and 98.9% in MNIST.



### FloorLevel-Net: Recognizing Floor-Level Lines with Height-Attention-Guided Multi-task Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.02462v1
- **DOI**: 10.1109/TIP.2021.3096090
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02462v1)
- **Published**: 2021-07-06 08:17:59+00:00
- **Updated**: 2021-07-06 08:17:59+00:00
- **Authors**: Mengyang Wu, Wei Zeng, Chi-Wing Fu
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to recognize the position and order of the floor-level lines that divide adjacent building floors can benefit many applications, for example, urban augmented reality (AR). This work tackles the problem of locating floor-level lines in street-view images, using a supervised deep learning approach. Unfortunately, very little data is available for training such a network $-$ current street-view datasets contain either semantic annotations that lack geometric attributes, or rectified facades without perspective priors. To address this issue, we first compile a new dataset and develop a new data augmentation scheme to synthesize training samples by harassing (i) the rich semantics of existing rectified facades and (ii) perspective priors of buildings in diverse street views. Next, we design FloorLevel-Net, a multi-task learning network that associates explicit features of building facades and implicit floor-level lines, along with a height-attention mechanism to help enforce a vertical ordering of floor-level lines. The generated segmentations are then passed to a second-stage geometry post-processing to exploit self-constrained geometric priors for plausible and consistent reconstruction of floor-level lines. Quantitative and qualitative evaluations conducted on assorted facades in existing datasets and street views from Google demonstrate the effectiveness of our approach. Also, we present context-aware image overlay results and show the potentials of our approach in enriching AR-related applications.



### A new smart-cropping pipeline for prostate segmentation using deep learning networks
- **Arxiv ID**: http://arxiv.org/abs/2107.02476v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02476v2)
- **Published**: 2021-07-06 08:42:48+00:00
- **Updated**: 2021-07-07 07:03:09+00:00
- **Authors**: Dimitrios G. Zaridis, Eugenia Mylona, Nikolaos S. Tachos, Kostas Marias, Nikolaos Papanikolaou, Manolis Tsiknakis, Dimitrios I. Fotiadis
- **Comment**: 8 pages, 6 figures, 1 table
- **Journal**: None
- **Summary**: Prostate segmentation from magnetic resonance imaging (MRI) is a challenging task. In recent years, several network architectures have been proposed to automate this process and alleviate the burden of manual annotation. Although the performance of these models has achieved promising results, there is still room for improvement before these models can be used safely and effectively in clinical practice. One of the major challenges in prostate MR image segmentation is the presence of class imbalance in the image labels where the background pixels dominate over the prostate. In the present work we propose a DL-based pipeline for cropping the region around the prostate from MRI images to produce a more balanced distribution of the foreground pixels (prostate) and the background pixels and improve segmentation accuracy. The effect of DL-cropping for improving the segmentation performance compared to standard center-cropping is assessed using five popular DL networks for prostate segmentation, namely U-net, U-net+, Res Unet++, Bridge U-net and Dense U-net. The proposed smart-cropping outperformed the standard center cropping in terms of segmentation accuracy for all the evaluated prostate segmentation networks. In terms of Dice score, the highest improvement was achieved for the U-net+ and ResU-net++ architectures corresponding to 8.9% and 8%, respectively.



### A Linkage-based Doubly Imbalanced Graph Learning Framework for Face Clustering
- **Arxiv ID**: http://arxiv.org/abs/2107.02477v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02477v3)
- **Published**: 2021-07-06 08:45:26+00:00
- **Updated**: 2022-12-29 11:46:35+00:00
- **Authors**: Huafeng Yang, Qijie Shen, Xingjian Chen, Fangyi Zhang, Rong Du
- **Comment**: 9 pages, accepted by SIAM International Conference on Data Mining
  (SDM) 2023
- **Journal**: None
- **Summary**: In recent years, benefiting from the expressive power of Graph Convolutional Networks (GCNs), significant breakthroughs have been made in face clustering area. However, rare attention has been paid to GCN-based clustering on imbalanced data. Although imbalance problem has been extensively studied, the impact of imbalanced data on GCN- based linkage prediction task is quite different, which would cause problems in two aspects: imbalanced linkage labels and biased graph representations. The former is similar to that in classic image classification task, but the latter is a particular problem in GCN-based clustering via linkage prediction. Significantly biased graph representations in training can cause catastrophic over-fitting of a GCN model. To tackle these challenges, we propose a linkage-based doubly imbalanced graph learning framework for face clustering. In this framework, we evaluate the feasibility of those existing methods for imbalanced image classification problem on GCNs, and present a new method to alleviate the imbalanced labels and also augment graph representations using a Reverse-Imbalance Weighted Sampling (RIWS) strategy. With the RIWS strategy, probability-based class balancing weights could ensure the overall distribution of positive and negative samples; in addition, weighted random sampling provides diverse subgraph structures, which effectively alleviates the over-fitting problem and improves the representation ability of GCNs. Extensive experiments on series of imbalanced benchmark datasets synthesized from MS-Celeb-1M and DeepFashion demonstrate the effectiveness and generality of our proposed method. Our implementation and the synthesized datasets will be openly available on https://github.com/espectre/GCNs_on_imbalanced_datasets.



### On Robustness of Lane Detection Models to Physical-World Adversarial Attacks in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2107.02488v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2107.02488v1)
- **Published**: 2021-07-06 09:04:47+00:00
- **Updated**: 2021-07-06 09:04:47+00:00
- **Authors**: Takami Sato, Qi Alfred Chen
- **Comment**: None
- **Journal**: None
- **Summary**: After the 2017 TuSimple Lane Detection Challenge, its evaluation based on accuracy and F1 score has become the de facto standard to measure the performance of lane detection methods. In this work, we conduct the first large-scale empirical study to evaluate the robustness of state-of-the-art lane detection methods under physical-world adversarial attacks in autonomous driving. We evaluate 4 major types of lane detection approaches with the conventional evaluation and end-to-end evaluation in autonomous driving scenarios and then discuss the security proprieties of each lane detection model. We demonstrate that the conventional evaluation fails to reflect the robustness in end-to-end autonomous driving scenarios. Our results show that the most robust model on the conventional metrics is the least robust in the end-to-end evaluation. Although the competition dataset and its metrics have played a substantial role in developing performant lane detection methods along with the rapid development of deep neural networks, the conventional evaluation is becoming obsolete and the gap between the metrics and practicality is critical. We hope that our study will help the community make further progress in building a more comprehensive framework to evaluate lane detection models.



### Neighbor-Vote: Improving Monocular 3D Object Detection through Neighbor Distance Voting
- **Arxiv ID**: http://arxiv.org/abs/2107.02493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02493v1)
- **Published**: 2021-07-06 09:18:33+00:00
- **Updated**: 2021-07-06 09:18:33+00:00
- **Authors**: Xiaomeng Chu, Jiajun Deng, Yao Li, Zhenxun Yuan, Yanyong Zhang, Jianmin Ji, Yu Zhang
- **Comment**: Accepted by ACM Multimedia 2021
- **Journal**: None
- **Summary**: As cameras are increasingly deployed in new application domains such as autonomous driving, performing 3D object detection on monocular images becomes an important task for visual scene understanding. Recent advances on monocular 3D object detection mainly rely on the ``pseudo-LiDAR'' generation, which performs monocular depth estimation and lifts the 2D pixels to pseudo 3D points. However, depth estimation from monocular images, due to its poor accuracy, leads to inevitable position shift of pseudo-LiDAR points within the object. Therefore, the predicted bounding boxes may suffer from inaccurate location and deformed shape. In this paper, we present a novel neighbor-voting method that incorporates neighbor predictions to ameliorate object detection from severely deformed pseudo-LiDAR point clouds. Specifically, each feature point around the object forms their own predictions, and then the ``consensus'' is achieved through voting. In this way, we can effectively combine the neighbors' predictions with local prediction and achieve more accurate 3D detection. To further enlarge the difference between the foreground region of interest (ROI) pseudo-LiDAR points and the background points, we also encode the ROI prediction scores of 2D foreground pixels into the corresponding pseudo-LiDAR points. We conduct extensive experiments on the KITTI benchmark to validate the merits of our proposed method. Our results on the bird's eye view detection outperform the state-of-the-art performance by a large margin, especially for the ``hard'' level detection.



### Independent Encoder for Deep Hierarchical Unsupervised Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2107.02494v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02494v1)
- **Published**: 2021-07-06 09:18:59+00:00
- **Updated**: 2021-07-06 09:18:59+00:00
- **Authors**: Kai Ye, Yinru Ye, Minqiang Yang, Bin Hu
- **Comment**: None
- **Journal**: None
- **Summary**: The main challenges of image-to-image (I2I) translation are to make the translated image realistic and retain as much information from the source domain as possible. To address this issue, we propose a novel architecture, termed as IEGAN, which removes the encoder of each network and introduces an encoder that is independent of other networks. Compared with previous models, it embodies three advantages of our model: Firstly, it is more directly and comprehensively to grasp image information since the encoder no longer receives loss from generator and discriminator. Secondly, the independent encoder allows each network to focus more on its own goal which makes the translated image more realistic. Thirdly, the reduction in the number of encoders performs more unified image representation. However, when the independent encoder applies two down-sampling blocks, it's hard to extract semantic information. To tackle this problem, we propose deep and shallow information space containing characteristic and semantic information, which can guide the model to translate high-quality images under the task with significant shape or texture change. We compare IEGAN with other previous models, and conduct researches on semantic information consistency and component ablation at the same time. These experiments show the superiority and effectiveness of our architecture. Our code is published on: https://github.com/Elvinky/IEGAN.



### Generalizing Nucleus Recognition Model in Multi-source Images via Pruning
- **Arxiv ID**: http://arxiv.org/abs/2107.02500v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02500v1)
- **Published**: 2021-07-06 09:36:34+00:00
- **Updated**: 2021-07-06 09:36:34+00:00
- **Authors**: Jiatong Cai, Chenglu Zhu, Can Cui, Honglin Li, Tong Wu, Shichuan Zhang, Lin Yang
- **Comment**: Accepted by MICCAI2021
- **Journal**: None
- **Summary**: Ki67 is a significant biomarker in the diagnosis and prognosis of cancer, whose index can be evaluated by quantifying its expression in Ki67 immunohistochemistry (IHC) stained images. However, quantitative analysis on multi-source Ki67 images is yet a challenging task in practice due to cross-domain distribution differences, which result from imaging variation, staining styles, and lesion types. Many recent studies have made some efforts on domain generalization (DG), whereas there are still some noteworthy limitations. Specifically in the case of Ki67 images, learning invariant representation is at the mercy of the insufficient number of domains and the cell categories mismatching in different domains. In this paper, we propose a novel method to improve DG by searching the domain-agnostic subnetwork in a domain merging scenario. Partial model parameters are iteratively pruned according to the domain gap, which is caused by the data converting from a single domain into merged domains during training. In addition, the model is optimized by fine-tuning on merged domains to eliminate the interference of class mismatching among various domains. Furthermore, an appropriate implementation is attained by applying the pruning method to different parts of the framework. Compared with known DG methods, our method yields excellent performance in multiclass nucleus recognition of Ki67 IHC images, especially in the lost category cases. Moreover, our competitive results are also evaluated on the public dataset over the state-of-the-art DG methods.



### Memory-aware curriculum federated learning for breast cancer classification
- **Arxiv ID**: http://arxiv.org/abs/2107.02504v2
- **DOI**: 10.1016/j.cmpb.2022.107318
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02504v2)
- **Published**: 2021-07-06 09:50:20+00:00
- **Updated**: 2023-01-06 15:04:36+00:00
- **Authors**: Amelia Jiménez-Sánchez, Mickael Tardy, Miguel A. González Ballester, Diana Mateus, Gemma Piella
- **Comment**: Computer Methods and Programs in Biomedicine
- **Journal**: None
- **Summary**: For early breast cancer detection, regular screening with mammography imaging is recommended. Routinary examinations result in datasets with a predominant amount of negative samples. A potential solution to such class-imbalance is joining forces across multiple institutions. Developing a collaborative computer-aided diagnosis system is challenging in different ways. Patient privacy and regulations need to be carefully respected. Data across institutions may be acquired from different devices or imaging protocols, leading to heterogeneous non-IID data. Also, for learning-based methods, new optimization strategies working on distributed data are required. Recently, federated learning has emerged as an effective tool for collaborative learning. In this setting, local models perform computation on their private data to update the global model. The order and the frequency of local updates influence the final global model. Hence, the order in which samples are locally presented to the optimizers plays an important role. In this work, we define a memory-aware curriculum learning method for the federated setting. Our curriculum controls the order of the training samples paying special attention to those that are forgotten after the deployment of the global model. Our approach is combined with unsupervised domain adaptation to deal with domain shift while preserving data privacy. We evaluate our method with three clinical datasets from different vendors. Our results verify the effectiveness of federated adversarial learning for the multi-site breast cancer classification. Moreover, we show that our proposed memory-aware curriculum method is beneficial to further improve classification performance. Our code is publicly available at: https://github.com/ameliajimenez/curriculum-federated-learning.



### Depth-Aware Multi-Grid Deep Homography Estimation with Contextual Correlation
- **Arxiv ID**: http://arxiv.org/abs/2107.02524v2
- **DOI**: 10.1109/TCSVT.2021.3125736
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.02524v2)
- **Published**: 2021-07-06 10:33:12+00:00
- **Updated**: 2021-11-03 09:54:01+00:00
- **Authors**: Lang Nie, Chunyu Lin, Kang Liao, Shuaicheng Liu, Yao Zhao
- **Comment**: Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT)
- **Journal**: None
- **Summary**: Homography estimation is an important task in computer vision applications, such as image stitching, video stabilization, and camera calibration. Traditional homography estimation methods heavily depend on the quantity and distribution of feature correspondences, leading to poor robustness in low-texture scenes. The learning solutions, on the contrary, try to learn robust deep features but demonstrate unsatisfying performance in the scenes with low overlap rates. In this paper, we address these two problems simultaneously by designing a contextual correlation layer (CCL). The CCL can efficiently capture the long-range correlation within feature maps and can be flexibly used in a learning framework. In addition, considering that a single homography can not represent the complex spatial transformation in depth-varying images with parallax, we propose to predict multi-grid homography from global to local. Moreover, we equip our network with a depth perception capability, by introducing a novel depth-aware shape-preserved loss. Extensive experiments demonstrate the superiority of our method over state-of-the-art solutions in the synthetic benchmark dataset and real-world dataset. The codes and models will be available at https://github.com/nie-lang/Multi-Grid-Deep-Homography.



### Semantic Segmentation Alternative Technique: Segmentation Domain Generation
- **Arxiv ID**: http://arxiv.org/abs/2107.02525v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02525v1)
- **Published**: 2021-07-06 10:34:55+00:00
- **Updated**: 2021-07-06 10:34:55+00:00
- **Authors**: Ana-Cristina Rogoz, Radu Muntean, Stefan Cobeli
- **Comment**: Accepted contribution at EEML2021 with poster presentation
- **Journal**: None
- **Summary**: Detecting objects of interest in images was always a compelling task to automate. In recent years this task was more and more explored using deep learning techniques, mostly using region-based convolutional networks. In this project we propose an alternative semantic segmentation technique making use of Generative Adversarial Networks. We consider semantic segmentation to be a domain transfer problem. Thus, we train a feed forward network (FFNN) to receive as input a seed real image and generate as output its segmentation mask.



### A Deep Learning-based Multimodal Depth-Aware Dynamic Hand Gesture Recognition System
- **Arxiv ID**: http://arxiv.org/abs/2107.02543v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02543v2)
- **Published**: 2021-07-06 11:18:53+00:00
- **Updated**: 2021-11-05 17:57:37+00:00
- **Authors**: Hasan Mahmud, Mashrur M. Morshed, Md. Kamrul Hasan
- **Comment**: None
- **Journal**: None
- **Summary**: The dynamic hand gesture recognition task has seen studies on various unimodal and multimodal methods. Previously, researchers have explored depth and 2D-skeleton-based multimodal fusion CRNNs (Convolutional Recurrent Neural Networks) but have had limitations in getting expected recognition results. In this paper, we revisit this approach to hand gesture recognition and suggest several improvements. We observe that raw depth images possess low contrast in the hand regions of interest (ROI). They do not highlight important fine details, such as finger orientation, overlap between the finger and palm, or overlap between multiple fingers. We thus propose quantizing the depth values into several discrete regions, to create a higher contrast between several key parts of the hand. In addition, we suggest several ways to tackle the high variance problem in existing multimodal fusion CRNN architectures. We evaluate our method on two benchmarks: the DHG-14/28 dataset and the SHREC'17 track dataset. Our approach shows a significant improvement in accuracy and parameter efficiency over previous similar multimodal methods, with a comparable result to the state-of-the-art.



### A Theory of the Distortion-Perception Tradeoff in Wasserstein Space
- **Arxiv ID**: http://arxiv.org/abs/2107.02555v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02555v1)
- **Published**: 2021-07-06 11:53:36+00:00
- **Updated**: 2021-07-06 11:53:36+00:00
- **Authors**: Dror Freirich, Tomer Michaeli, Ron Meir
- **Comment**: None
- **Journal**: None
- **Summary**: The lower the distortion of an estimator, the more the distribution of its outputs generally deviates from the distribution of the signals it attempts to estimate. This phenomenon, known as the perception-distortion tradeoff, has captured significant attention in image restoration, where it implies that fidelity to ground truth images comes at the expense of perceptual quality (deviation from statistics of natural images). However, despite the increasing popularity of performing comparisons on the perception-distortion plane, there remains an important open question: what is the minimal distortion that can be achieved under a given perception constraint? In this paper, we derive a closed form expression for this distortion-perception (DP) function for the mean squared-error (MSE) distortion and the Wasserstein-2 perception index. We prove that the DP function is always quadratic, regardless of the underlying distribution. This stems from the fact that estimators on the DP curve form a geodesic in Wasserstein space. In the Gaussian setting, we further provide a closed form expression for such estimators. For general distributions, we show how these estimators can be constructed from the estimators at the two extremes of the tradeoff: The global MSE minimizer, and a minimizer of the MSE under a perfect perceptual quality constraint. The latter can be obtained as a stochastic transformation of the former.



### Coarse-to-fine Semantic Localization with HD Map for Autonomous Driving in Structural Scenes
- **Arxiv ID**: http://arxiv.org/abs/2107.02557v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02557v1)
- **Published**: 2021-07-06 11:58:55+00:00
- **Updated**: 2021-07-06 11:58:55+00:00
- **Authors**: Chengcheng Guo, Minjie Lin, Heyang Guo, Pengpeng Liang, Erkang Cheng
- **Comment**: The IEEE/RSJ International Conference on Intelligent Robots and
  Systems, IROS 2021
- **Journal**: None
- **Summary**: Robust and accurate localization is an essential component for robotic navigation and autonomous driving. The use of cameras for localization with high definition map (HD Map) provides an affordable localization sensor set. Existing methods suffer from pose estimation failure due to error prone data association or initialization with accurate initial pose requirement. In this paper, we propose a cost-effective vehicle localization system with HD map for autonomous driving that uses cameras as primary sensors. To this end, we formulate vision-based localization as a data association problem that maps visual semantics to landmarks in HD map. Specifically, system initialization is finished in a coarse to fine manner by combining coarse GPS (Global Positioning System) measurement and fine pose searching. In tracking stage, vehicle pose is refined by implicitly aligning the semantic segmentation result between image and landmarks in HD maps with photometric consistency. Finally, vehicle pose is computed by pose graph optimization in a sliding window fashion. We evaluate our method on two datasets and demonstrate that the proposed approach yields promising localization results in different driving scenarios. Additionally, our approach is suitable for both monocular camera and multi-cameras that provides flexibility and improves robustness for the localization system.



### ROPUST: Improving Robustness through Fine-tuning with Photonic Processors and Synthetic Gradients
- **Arxiv ID**: http://arxiv.org/abs/2108.04217v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04217v1)
- **Published**: 2021-07-06 12:03:36+00:00
- **Updated**: 2021-07-06 12:03:36+00:00
- **Authors**: Alessandro Cappelli, Julien Launay, Laurent Meunier, Ruben Ohana, Iacopo Poli
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Robustness to adversarial attacks is typically obtained through expensive adversarial training with Projected Gradient Descent. Here we introduce ROPUST, a remarkably simple and efficient method to leverage robust pre-trained models and further increase their robustness, at no cost in natural accuracy. Our technique relies on the use of an Optical Processing Unit (OPU), a photonic co-processor, and a fine-tuning step performed with Direct Feedback Alignment, a synthetic gradient training scheme. We test our method on nine different models against four attacks in RobustBench, consistently improving over state-of-the-art performance. We perform an ablation study on the single components of our defense, showing that robustness arises from parameter obfuscation and the alternative training method. We also introduce phase retrieval attacks, specifically designed to increase the threat level of attackers against our own defense. We show that even with state-of-the-art phase retrieval techniques, ROPUST remains an effective defense.



### Rethinking Positional Encoding
- **Arxiv ID**: http://arxiv.org/abs/2107.02561v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02561v3)
- **Published**: 2021-07-06 12:04:04+00:00
- **Updated**: 2021-10-11 22:39:59+00:00
- **Authors**: Jianqiao Zheng, Sameera Ramasinghe, Simon Lucey
- **Comment**: None
- **Journal**: None
- **Summary**: It is well noted that coordinate based MLPs benefit -- in terms of preserving high-frequency information -- through the encoding of coordinate positions as an array of Fourier features. Hitherto, the rationale for the effectiveness of these positional encodings has been solely studied through a Fourier lens. In this paper, we strive to broaden this understanding by showing that alternative non-Fourier embedding functions can indeed be used for positional encoding. Moreover, we show that their performance is entirely determined by a trade-off between the stable rank of the embedded matrix and the distance preservation between embedded coordinates. We further establish that the now ubiquitous Fourier feature mapping of position is a special case that fulfills these conditions. Consequently, we present a more general theory to analyze positional encoding in terms of shifted basis functions. To this end, we develop the necessary theoretical formulae and empirically verify that our theoretical claims hold in practice. Codes available at https://github.com/osiriszjq/Rethinking-positional-encoding.



### Confidence-based Out-of-Distribution Detection: A Comparative Study and Analysis
- **Arxiv ID**: http://arxiv.org/abs/2107.02568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02568v1)
- **Published**: 2021-07-06 12:10:09+00:00
- **Updated**: 2021-07-06 12:10:09+00:00
- **Authors**: Christoph Berger, Magdalini Paschali, Ben Glocker, Konstantinos Kamnitsas
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: Image classification models deployed in the real world may receive inputs outside the intended data distribution. For critical applications such as clinical decision making, it is important that a model can detect such out-of-distribution (OOD) inputs and express its uncertainty. In this work, we assess the capability of various state-of-the-art approaches for confidence-based OOD detection through a comparative study and in-depth analysis. First, we leverage a computer vision benchmark to reproduce and compare multiple OOD detection methods. We then evaluate their capabilities on the challenging task of disease classification using chest X-rays. Our study shows that high performance in a computer vision task does not directly translate to accuracy in a medical imaging task. We analyse factors that affect performance of the methods between the two tasks. Our results provide useful insights for developing the next generation of OOD detection methods.



### Unsupervised Knowledge-Transfer for Learned Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2107.02572v2
- **DOI**: 10.1088/1361-6420/ac8a91
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02572v2)
- **Published**: 2021-07-06 12:19:16+00:00
- **Updated**: 2022-07-21 15:05:18+00:00
- **Authors**: Riccardo Barbano, Zeljko Kereta, Andreas Hauptmann, Simon R. Arridge, Bangti Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based image reconstruction approaches have demonstrated impressive empirical performance in many imaging modalities. These approaches usually require a large amount of high-quality paired training data, which is often not available in medical imaging. To circumvent this issue we develop a novel unsupervised knowledge-transfer paradigm for learned reconstruction within a Bayesian framework. The proposed approach learns a reconstruction network in two phases. The first phase trains a reconstruction network with a set of ordered pairs comprising of ground truth images of ellipses and the corresponding simulated measurement data. The second phase fine-tunes the pretrained network to more realistic measurement data without supervision. By construction, the framework is capable of delivering predictive uncertainty information over the reconstructed image. We present extensive experimental results on low-dose and sparse-view computed tomography showing that the approach is competitive with several state-of-the-art supervised and unsupervised reconstruction techniques. Moreover, for test data distributed differently from the training data, the proposed framework can significantly improve reconstruction quality not only visually, but also quantitatively in terms of PSNR and SSIM, when compared with learned methods trained on the synthetic dataset only.



### Contrastive Multimodal Fusion with TupleInfoNCE
- **Arxiv ID**: http://arxiv.org/abs/2107.02575v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02575v1)
- **Published**: 2021-07-06 12:26:58+00:00
- **Updated**: 2021-07-06 12:26:58+00:00
- **Authors**: Yunze Liu, Qingnan Fan, Shanghang Zhang, Hao Dong, Thomas Funkhouser, Li Yi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a method for representation learning of multimodal data using contrastive losses. A traditional approach is to contrast different modalities to learn the information shared between them. However, that approach could fail to learn the complementary synergies between modalities that might be useful for downstream tasks. Another approach is to concatenate all the modalities into a tuple and then contrast positive and negative tuple correspondences. However, that approach could consider only the stronger modalities while ignoring the weaker ones. To address these issues, we propose a novel contrastive learning objective, TupleInfoNCE. It contrasts tuples based not only on positive and negative correspondences but also by composing new negative tuples using modalities describing different scenes. Training with these additional negatives encourages the learning model to examine the correspondences among modalities in the same tuple, ensuring that weak modalities are not ignored. We provide a theoretical justification based on mutual information for why this approach works, and we propose a sample optimization algorithm to generate positive and negative samples to maximize training efficacy. We find that TupleInfoNCE significantly outperforms the previous state of the arts on three different downstream tasks.



### Point Cloud Registration using Representative Overlapping Points
- **Arxiv ID**: http://arxiv.org/abs/2107.02583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02583v1)
- **Published**: 2021-07-06 12:52:22+00:00
- **Updated**: 2021-07-06 12:52:22+00:00
- **Authors**: Lifa Zhu, Dongrui Liu, Changwei Lin, Rui Yan, Francisco Gómez-Fernández, Ninghua Yang, Ziyong Feng
- **Comment**: None
- **Journal**: None
- **Summary**: 3D point cloud registration is a fundamental task in robotics and computer vision. Recently, many learning-based point cloud registration methods based on correspondences have emerged. However, these methods heavily rely on such correspondences and meet great challenges with partial overlap. In this paper, we propose ROPNet, a new deep learning model using Representative Overlapping Points with discriminative features for registration that transforms partial-to-partial registration into partial-to-complete registration. Specifically, we propose a context-guided module which uses an encoder to extract global features for predicting point overlap score. To better find representative overlapping points, we use the extracted global features for coarse alignment. Then, we introduce a Transformer to enrich point features and remove non-representative points based on point overlap score and feature matching. A similarity matrix is built in a partial-to-complete mode, and finally, weighted SVD is adopted to estimate a transformation matrix. Extensive experiments over ModelNet40 using noisy and partially overlapping point clouds show that the proposed method outperforms traditional and learning-based methods, achieving state-of-the-art performance. The code is available at https://github.com/zhulf0804/ROPNet.



### Differentially private federated deep learning for multi-site medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.02586v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02586v1)
- **Published**: 2021-07-06 12:57:32+00:00
- **Updated**: 2021-07-06 12:57:32+00:00
- **Authors**: Alexander Ziller, Dmitrii Usynin, Nicolas Remerscheid, Moritz Knolle, Marcus Makowski, Rickmer Braren, Daniel Rueckert, Georgios Kaissis
- **Comment**: Submitted to the Journal of Machine Learning in Biomedical Imaging
  (MELBA)
- **Journal**: None
- **Summary**: Collaborative machine learning techniques such as federated learning (FL) enable the training of models on effectively larger datasets without data transfer. Recent initiatives have demonstrated that segmentation models trained with FL can achieve performance similar to locally trained models. However, FL is not a fully privacy-preserving technique and privacy-centred attacks can disclose confidential patient data. Thus, supplementing FL with privacy-enhancing technologies (PTs) such as differential privacy (DP) is a requirement for clinical applications in a multi-institutional setting. The application of PTs to FL in medical imaging and the trade-offs between privacy guarantees and model utility, the ramifications on training performance and the susceptibility of the final models to attacks have not yet been conclusively investigated. Here we demonstrate the first application of differentially private gradient descent-based FL on the task of semantic segmentation in computed tomography. We find that high segmentation performance is possible under strong privacy guarantees with an acceptable training time penalty. We furthermore demonstrate the first successful gradient-based model inversion attack on a semantic segmentation model and show that the application of DP prevents it from divulging sensitive image features.



### Stateless actor-critic for instance segmentation with high-level priors
- **Arxiv ID**: http://arxiv.org/abs/2107.02600v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02600v2)
- **Published**: 2021-07-06 13:20:14+00:00
- **Updated**: 2023-03-21 15:11:44+00:00
- **Authors**: Paul Hilt, Maedeh Zarvandi, Edgar Kaziakhmedov, Sourabh Bhide, Maria Leptin, Constantin Pape, Anna Kreshuk
- **Comment**: None
- **Journal**: None
- **Summary**: Instance segmentation is an important computer vision problem which remains challenging despite impressive recent advances due to deep learning-based methods. Given sufficient training data, fully supervised methods can yield excellent performance, but annotation of ground-truth data remains a major bottleneck, especially for biomedical applications where it has to be performed by domain experts. The amount of labels required can be drastically reduced by using rules derived from prior knowledge to guide the segmentation. However, these rules are in general not differentiable and thus cannot be used with existing methods. Here, we relax this requirement by using stateless actor critic reinforcement learning, which enables non-differentiable rewards. We formulate the instance segmentation problem as graph partitioning and the actor critic predicts the edge weights driven by the rewards, which are based on the conformity of segmented instances to high-level priors on object shape, position or size. The experiments on toy and real datasets demonstrate that we can achieve excellent performance without any direct supervision based only on a rich set of priors.



### Combining EfficientNet and Vision Transformers for Video Deepfake Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.02612v2
- **DOI**: 10.1007/978-3-031-06433-3_19
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02612v2)
- **Published**: 2021-07-06 13:35:11+00:00
- **Updated**: 2022-01-20 14:35:11+00:00
- **Authors**: Davide Coccomini, Nicola Messina, Claudio Gennaro, Fabrizio Falchi
- **Comment**: None
- **Journal**: None
- **Summary**: Deepfakes are the result of digital manipulation to forge realistic yet fake imagery. With the astonishing advances in deep generative models, fake images or videos are nowadays obtained using variational autoencoders (VAEs) or Generative Adversarial Networks (GANs). These technologies are becoming more accessible and accurate, resulting in fake videos that are very difficult to be detected. Traditionally, Convolutional Neural Networks (CNNs) have been used to perform video deepfake detection, with the best results obtained using methods based on EfficientNet B7. In this study, we focus on video deep fake detection on faces, given that most methods are becoming extremely accurate in the generation of realistic human faces. Specifically, we combine various types of Vision Transformers with a convolutional EfficientNet B0 used as a feature extractor, obtaining comparable results with some very recent methods that use Vision Transformers. Differently from the state-of-the-art approaches, we use neither distillation nor ensemble methods. Furthermore, we present a straightforward inference procedure based on a simple voting scheme for handling multiple faces in the same video shot. The best model achieved an AUC of 0.951 and an F1 score of 88.0%, very close to the state-of-the-art on the DeepFake Detection Challenge (DFDC).



### Detecting Outliers with Poisson Image Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2107.02622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02622v1)
- **Published**: 2021-07-06 13:53:17+00:00
- **Updated**: 2021-07-06 13:53:17+00:00
- **Authors**: Jeremy Tan, Benjamin Hou, Thomas Day, John Simpson, Daniel Rueckert, Bernhard Kainz
- **Comment**: MICCAI 2021
- **Journal**: None
- **Summary**: Supervised learning of every possible pathology is unrealistic for many primary care applications like health screening. Image anomaly detection methods that learn normal appearance from only healthy data have shown promising results recently. We propose an alternative to image reconstruction-based and image embedding-based methods and propose a new self-supervised method to tackle pathological anomaly detection. Our approach originates in the foreign patch interpolation (FPI) strategy that has shown superior performance on brain MRI and abdominal CT data. We propose to use a better patch interpolation strategy, Poisson image interpolation (PII), which makes our method suitable for applications in challenging data regimes. PII outperforms state-of-the-art methods by a good margin when tested on surrogate tasks like identifying common lung anomalies in chest X-rays or hypo-plastic left heart syndrome in prenatal, fetal cardiac ultrasound images. Code available at https://github.com/jemtan/PII.



### Embracing the Dark Knowledge: Domain Generalization Using Regularized Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2107.02629v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.02629v1)
- **Published**: 2021-07-06 14:08:54+00:00
- **Updated**: 2021-07-06 14:08:54+00:00
- **Authors**: Yufei Wang, Haoliang Li, Lap-pui Chau, Alex C. Kot
- **Comment**: Accepted by ACM MM, 2021
- **Journal**: None
- **Summary**: Though convolutional neural networks are widely used in different tasks, lack of generalization capability in the absence of sufficient and representative data is one of the challenges that hinder their practical application. In this paper, we propose a simple, effective, and plug-and-play training strategy named Knowledge Distillation for Domain Generalization (KDDG) which is built upon a knowledge distillation framework with the gradient filter as a novel regularization term. We find that both the ``richer dark knowledge" from the teacher network, as well as the gradient filter we proposed, can reduce the difficulty of learning the mapping which further improves the generalization ability of the model. We also conduct experiments extensively to show that our framework can significantly improve the generalization capability of deep neural networks in different tasks including image classification, segmentation, reinforcement learning by comparing our method with existing state-of-the-art domain generalization techniques. Last but not the least, we propose to adopt two metrics to analyze our proposed method in order to better understand how our proposed method benefits the generalization capability of deep neural networks.



### Hyperspectral Pansharpening Based on Improved Deep Image Prior and Residual Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2107.02630v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02630v1)
- **Published**: 2021-07-06 14:11:03+00:00
- **Updated**: 2021-07-06 14:11:03+00:00
- **Authors**: Wele Gedara Chaminda Bandara, Jeya Maria Jose Valanarasu, Vishal M. Patel
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral pansharpening aims to synthesize a low-resolution hyperspectral image (LR-HSI) with a registered panchromatic image (PAN) to generate an enhanced HSI with high spectral and spatial resolution. Recently proposed HS pansharpening methods have obtained remarkable results using deep convolutional networks (ConvNets), which typically consist of three steps: (1) up-sampling the LR-HSI, (2) predicting the residual image via a ConvNet, and (3) obtaining the final fused HSI by adding the outputs from first and second steps. Recent methods have leveraged Deep Image Prior (DIP) to up-sample the LR-HSI due to its excellent ability to preserve both spatial and spectral information, without learning from large data sets. However, we observed that the quality of up-sampled HSIs can be further improved by introducing an additional spatial-domain constraint to the conventional spectral-domain energy function. We define our spatial-domain constraint as the $L_1$ distance between the predicted PAN image and the actual PAN image. To estimate the PAN image of the up-sampled HSI, we also propose a learnable spectral response function (SRF). Moreover, we noticed that the residual image between the up-sampled HSI and the reference HSI mainly consists of edge information and very fine structures. In order to accurately estimate fine information, we propose a novel over-complete network, called HyperKite, which focuses on learning high-level features by constraining the receptive from increasing in the deep layers. We perform experiments on three HSI datasets to demonstrate the superiority of our DIP-HyperKite over the state-of-the-art pansharpening methods. The deployment codes, pre-trained models, and final fusion outputs of our DIP-HyperKite and the methods used for the comparisons will be publicly made available at https://github.com/wgcban/DIP-HyperKite.git.



### DocSynth: A Layout Guided Approach for Controllable Document Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2107.02638v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02638v1)
- **Published**: 2021-07-06 14:24:30+00:00
- **Updated**: 2021-07-06 14:24:30+00:00
- **Authors**: Sanket Biswas, Pau Riba, Josep Lladós, Umapada Pal
- **Comment**: Accepted by ICDAR 2021
- **Journal**: None
- **Summary**: Despite significant progress on current state-of-the-art image generation models, synthesis of document images containing multiple and complex object layouts is a challenging task. This paper presents a novel approach, called DocSynth, to automatically synthesize document images based on a given layout. In this work, given a spatial layout (bounding boxes with object categories) as a reference by the user, our proposed DocSynth model learns to generate a set of realistic document images consistent with the defined layout. Also, this framework has been adapted to this work as a superior baseline model for creating synthetic document image datasets for augmenting real data during training for document layout analysis tasks. Different sets of learning objectives have been also used to improve the model performance. Quantitatively, we also compare the generated results of our model with real data using standard evaluation metrics. The results highlight that our model can successfully generate realistic and diverse document images with multiple objects. We also present a comprehensive qualitative analysis summary of the different scopes of synthetic image generation tasks. Lastly, to our knowledge this is the first work of its kind.



### Detecting Hypo-plastic Left Heart Syndrome in Fetal Ultrasound via Disease-specific Atlas Maps
- **Arxiv ID**: http://arxiv.org/abs/2107.02643v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02643v1)
- **Published**: 2021-07-06 14:31:19+00:00
- **Updated**: 2021-07-06 14:31:19+00:00
- **Authors**: Samuel Budd, Matthew Sinclair, Thomas Day, Athanasios Vlontzos, Jeremy Tan, Tianrui Liu, Jaqueline Matthew, Emily Skelton, John Simpson, Reza Razavi, Ben Glocker, Daniel Rueckert, Emma C. Robinson, Bernhard Kainz
- **Comment**: MICCAI'21 Main Conference
- **Journal**: None
- **Summary**: Fetal ultrasound screening during pregnancy plays a vital role in the early detection of fetal malformations which have potential long-term health impacts. The level of skill required to diagnose such malformations from live ultrasound during examination is high and resources for screening are often limited. We present an interpretable, atlas-learning segmentation method for automatic diagnosis of Hypo-plastic Left Heart Syndrome (HLHS) from a single `4 Chamber Heart' view image. We propose to extend the recently introduced Image-and-Spatial Transformer Networks (Atlas-ISTN) into a framework that enables sensitising atlas generation to disease. In this framework we can jointly learn image segmentation, registration, atlas construction and disease prediction while providing a maximum level of clinical interpretability compared to direct image classification methods. As a result our segmentation allows diagnoses competitive with expert-derived manual diagnosis and yields an AUC-ROC of 0.978 (1043 cases for training, 260 for validation and 325 for testing).



### Automatic size and pose homogenization with spatial transformer network to improve and accelerate pediatric segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.02655v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2107.02655v1)
- **Published**: 2021-07-06 14:50:03+00:00
- **Updated**: 2021-07-06 14:50:03+00:00
- **Authors**: Giammarco La Barbera, Pietro Gori, Haithem Boussaid, Bruno Belucci, Alessandro Delmonte, Jeanne Goulin, Sabine Sarnacki, Laurence Rouet, Isabelle Bloch
- **Comment**: ISBI 2021
- **Journal**: ISBI 2021
- **Summary**: Due to a high heterogeneity in pose and size and to a limited number of available data, segmentation of pediatric images is challenging for deep learning methods. In this work, we propose a new CNN architecture that is pose and scale invariant thanks to the use of Spatial Transformer Network (STN). Our architecture is composed of three sequential modules that are estimated together during training: (i) a regression module to estimate a similarity matrix to normalize the input image to a reference one; (ii) a differentiable module to find the region of interest to segment; (iii) a segmentation module, based on the popular UNet architecture, to delineate the object. Unlike the original UNet, which strives to learn a complex mapping, including pose and scale variations, from a finite training dataset, our segmentation module learns a simpler mapping focusing on images with normalized pose and size. Furthermore, the use of an automatic bounding box detection through STN allows saving time and especially memory, while keeping similar performance. We test the proposed method in kidney and renal tumor segmentation on abdominal pediatric CT scanners. Results indicate that the estimated STN homogenization of size and pose accelerates the segmentation (25h), compared to standard data-augmentation (33h), while obtaining a similar quality for the kidney (88.01\% of Dice score) and improving the renal tumor delineation (from 85.52\% to 87.12\%).



### HybrUR: A Hybrid Physical-Neural Solution for Unsupervised Underwater Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2107.02660v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02660v1)
- **Published**: 2021-07-06 15:00:30+00:00
- **Updated**: 2021-07-06 15:00:30+00:00
- **Authors**: Shuaizheng Yan, Xingyu Chen, Zhengxing Wu, Jian Wang, Yue Lu, Min Tan, Junzhi Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Robust vision restoration for an underwater image remains a challenging problem. For the lack of aligned underwater-terrestrial image pairs, the unsupervised method is more suited to this task. However, the pure data-driven unsupervised method usually has difficulty in achieving realistic color correction for lack of optical constraint. In this paper, we propose a data- and physics-driven unsupervised architecture that learns underwater vision restoration from unpaired underwater-terrestrial images. For sufficient domain transformation and detail preservation, the underwater degeneration needs to be explicitly constructed based on the optically unambiguous physics law. Thus, we employ the Jaffe-McGlamery degradation theory to design the generation models, and use neural networks to describe the process of underwater degradation. Furthermore, to overcome the problem of invalid gradient when optimizing the hybrid physical-neural model, we fully investigate the intrinsic correlation between the scene depth and the degradation factors for the backscattering estimation, to improve the restoration performance through physical constraints. Our experimental results show that the proposed method is able to perform high-quality restoration for unconstrained underwater images without any supervision. On multiple benchmarks, we outperform several state-of-the-art supervised and unsupervised approaches. We also demonstrate that our methods yield encouraging results on real-world applications.



### COVID-19 Pneumonia Severity Prediction using Hybrid Convolution-Attention Neural Architectures
- **Arxiv ID**: http://arxiv.org/abs/2107.02672v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02672v2)
- **Published**: 2021-07-06 15:26:07+00:00
- **Updated**: 2021-07-07 17:59:00+00:00
- **Authors**: Nam Nguyen, J. Morris Chang
- **Comment**: None
- **Journal**: None
- **Summary**: This study proposed a novel framework for COVID-19 severity prediction, which is a combination of data-centric and model-centric approaches. First, we propose a data-centric pre-training for extremely scare data scenarios of the investigating dataset. Second, we propose two hybrid convolution-attention neural architectures that leverage the self-attention from the Transformer and the Dense Associative Memory (Modern Hopfield networks). Our proposed approach achieves significant improvement from the conventional baseline approach. The best model from our proposed approach achieves $R^2 = 0.85 \pm 0.05$ and Pearson correlation coefficient $\rho = 0.92 \pm 0.02$ in geographic extend and $R^2 = 0.72 \pm 0.09, \rho = 0.85\pm 0.06$ in opacity prediction.



### Attention-based Adversarial Appearance Learning of Augmented Pedestrians
- **Arxiv ID**: http://arxiv.org/abs/2107.02673v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02673v1)
- **Published**: 2021-07-06 15:27:00+00:00
- **Updated**: 2021-07-06 15:27:00+00:00
- **Authors**: Kevin Strauss, Artem Savkin, Federico Tombari
- **Comment**: None
- **Journal**: None
- **Summary**: Synthetic data became already an essential component of machine learning-based perception in the field of autonomous driving. Yet it still cannot replace real data completely due to the sim2real domain shift. In this work, we propose a method that leverages the advantages of the augmentation process and adversarial training to synthesize realistic data for the pedestrian recognition task. Our approach utilizes an attention mechanism driven by an adversarial loss to learn domain discrepancies and improve sim2real adaptation. Our experiments confirm that the proposed adaptation method is robust to such discrepancies and reveals both visual realism and semantic consistency. Furthermore, we evaluate our data generation pipeline on the task of pedestrian recognition and demonstrate that generated data resemble properties of the real domain.



### Real-time Pose Estimation from Images for Multiple Humanoid Robots
- **Arxiv ID**: http://arxiv.org/abs/2107.02675v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02675v1)
- **Published**: 2021-07-06 15:33:57+00:00
- **Updated**: 2021-07-06 15:33:57+00:00
- **Authors**: Arash Amini, Hafez Farazi, Sven Behnke
- **Comment**: None
- **Journal**: None
- **Summary**: Pose estimation commonly refers to computer vision methods that recognize people's body postures in images or videos. With recent advancements in deep learning, we now have compelling models to tackle the problem in real-time. Since these models are usually designed for human images, one needs to adapt existing models to work on other creatures, including robots. This paper examines different state-of-the-art pose estimation models and proposes a lightweight model that can work in real-time on humanoid robots in the RoboCup Humanoid League environment. Additionally, we present a novel dataset called the HumanoidRobotPose dataset. The results of this work have the potential to enable many advanced behaviors for soccer-playing robots.



### VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2107.02681v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02681v2)
- **Published**: 2021-07-06 15:41:32+00:00
- **Updated**: 2021-10-19 17:14:00+00:00
- **Authors**: Zineng Tang, Jaemin Cho, Hao Tan, Mohit Bansal
- **Comment**: NeurIPS 2021 (19 pages)
- **Journal**: None
- **Summary**: Since visual perception can give rich information beyond text descriptions for world understanding, there has been increasing interest in leveraging visual grounding for language learning. Recently, vokenization (Tan and Bansal, 2020) has attracted attention by using the predictions of a text-to-image retrieval model as labels for language model supervision. Despite its success, the method suffers from approximation error of using finite image labels and the lack of vocabulary diversity of a small image-text dataset. To overcome these limitations, we present VidLanKD, a video-language knowledge distillation method for improving language understanding. We train a multi-modal teacher model on a video-text dataset, and then transfer its knowledge to a student language model with a text dataset. To avoid approximation error, we propose to use different knowledge distillation objectives. In addition, the use of a large-scale video-text dataset helps learn diverse and richer vocabularies. In our experiments, VidLanKD achieves consistent improvements over text-only language models and vokenization models, on several downstream language understanding tasks including GLUE, SQuAD, and SWAG. We also demonstrate the improved world knowledge, physical reasoning, and temporal reasoning capabilities of our model by evaluating on the GLUE-diagnostics, PIQA, and TRACIE datasets. Lastly, we present comprehensive ablation studies as well as visualizations of the learned text-to-video grounding results of our teacher and student language models. Our code and models are available at: https://github.com/zinengtang/VidLanKD



### Spatiotemporal Fusion in Remote Sensing
- **Arxiv ID**: http://arxiv.org/abs/2107.02701v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02701v1)
- **Published**: 2021-07-06 16:04:04+00:00
- **Updated**: 2021-07-06 16:04:04+00:00
- **Authors**: Hessah Albanwan, Rongjun Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Remote sensing images and techniques are powerful tools to investigate earth surface. Data quality is the key to enhance remote sensing applications and obtaining a clear and noise-free set of data is very difficult in most situations due to the varying acquisition (e.g., atmosphere and season), sensor, and platform (e.g., satellite angles and sensor characteristics) conditions. With the increasing development of satellites, nowadays Terabytes of remote sensing images can be acquired every day. Therefore, information and data fusion can be particularly important in the remote sensing community. The fusion integrates data from various sources acquired asynchronously for information extraction, analysis, and quality improvement. In this chapter, we aim to discuss the theory of spatiotemporal fusion by investigating previous works, in addition to describing the basic concepts and some of its applications by summarizing our prior and ongoing works.



### Unsupervised learning of MRI tissue properties using MRI physics models
- **Arxiv ID**: http://arxiv.org/abs/2107.02704v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.NC, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2107.02704v1)
- **Published**: 2021-07-06 16:07:14+00:00
- **Updated**: 2021-07-06 16:07:14+00:00
- **Authors**: Divya Varadarajan, Katherine L. Bouman, Andre van der Kouwe, Bruce Fischl, Adrian V. Dalca
- **Comment**: 11 Pages, 9 figures
- **Journal**: None
- **Summary**: In neuroimaging, MRI tissue properties characterize underlying neurobiology, provide quantitative biomarkers for neurological disease detection and analysis, and can be used to synthesize arbitrary MRI contrasts. Estimating tissue properties from a single scan session using a protocol available on all clinical scanners promises to reduce scan time and cost, enable quantitative analysis in routine clinical scans and provide scan-independent biomarkers of disease. However, existing tissue properties estimation methods - most often $\mathbf{T_1}$ relaxation, $\mathbf{T_2^*}$ relaxation, and proton density ($\mathbf{PD}$) - require data from multiple scan sessions and cannot estimate all properties from a single clinically available MRI protocol such as the multiecho MRI scan. In addition, the widespread use of non-standard acquisition parameters across clinical imaging sites require estimation methods that can generalize across varying scanner parameters. However, existing learning methods are acquisition protocol specific and cannot estimate from heterogenous clinical data from different imaging sites. In this work we propose an unsupervised deep-learning strategy that employs MRI physics to estimate all three tissue properties from a single multiecho MRI scan session, and generalizes across varying acquisition parameters. The proposed strategy optimizes accurate synthesis of new MRI contrasts from estimated latent tissue properties, enabling unsupervised training, we also employ random acquisition parameters during training to achieve acquisition generalization. We provide the first demonstration of estimating all tissue properties from a single multiecho scan session. We demonstrate improved accuracy and generalizability for tissue property estimation and MRI synthesis.



### Predicate correlation learning for scene graph generation
- **Arxiv ID**: http://arxiv.org/abs/2107.02713v1
- **DOI**: 10.1109/TIP.2022.3181511
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02713v1)
- **Published**: 2021-07-06 16:24:33+00:00
- **Updated**: 2021-07-06 16:24:33+00:00
- **Authors**: Leitian Tao, Li Mi, Nannan Li, Xianhang Cheng, Yaosi Hu, Zhenzhong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: For a typical Scene Graph Generation (SGG) method, there is often a large gap in the performance of the predicates' head classes and tail classes. This phenomenon is mainly caused by the semantic overlap between different predicates as well as the long-tailed data distribution. In this paper, a Predicate Correlation Learning (PCL) method for SGG is proposed to address the above two problems by taking the correlation between predicates into consideration. To describe the semantic overlap between strong-correlated predicate classes, a Predicate Correlation Matrix (PCM) is defined to quantify the relationship between predicate pairs, which is dynamically updated to remove the matrix's long-tailed bias. In addition, PCM is integrated into a Predicate Correlation Loss function ($L_{PC}$) to reduce discouraging gradients of unannotated classes. The proposed method is evaluated on Visual Genome benchmark, where the performance of the tail classes is significantly improved when built on the existing methods.



### Finding Significant Features for Few-Shot Learning using Dimensionality Reduction
- **Arxiv ID**: http://arxiv.org/abs/2107.06992v1
- **DOI**: 10.1007/978-3-030-89817-5_10
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.06992v1)
- **Published**: 2021-07-06 16:36:57+00:00
- **Updated**: 2021-07-06 16:36:57+00:00
- **Authors**: Mauricio Mendez-Ruiz, Ivan Garcia Jorge Gonzalez-Zapata, Gilberto Ochoa-Ruiz, Andres Mendez-Vazquez
- **Comment**: This paper is currently under review for the Mexican International
  Conference on Artificial Intelligence (MICAI) 2021
- **Journal**: None
- **Summary**: Few-shot learning is a relatively new technique that specializes in problems where we have little amounts of data. The goal of these methods is to classify categories that have not been seen before with just a handful of samples. Recent approaches, such as metric learning, adopt the meta-learning strategy in which we have episodic tasks conformed by support (training) data and query (test) data. Metric learning methods have demonstrated that simple models can achieve good performance by learning a similarity function to compare the support and the query data. However, the feature space learned by a given metric learning approach may not exploit the information given by a specific few-shot task. In this work, we explore the use of dimension reduction techniques as a way to find task-significant features helping to make better predictions. We measure the performance of the reduced features by assigning a score based on the intra-class and inter-class distance, and selecting a feature reduction method in which instances of different classes are far away and instances of the same class are close. This module helps to improve the accuracy performance by allowing the similarity function, given by the metric learning method, to have more discriminative features for the classification. Our method outperforms the metric learning baselines in the miniImageNet dataset by around 2% in accuracy performance.



### Foreground-Aware Stylization and Consensus Pseudo-Labeling for Domain Adaptation of First-Person Hand Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.02718v4
- **DOI**: 10.1109/ACCESS.2021.3094052
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02718v4)
- **Published**: 2021-07-06 16:39:06+00:00
- **Updated**: 2022-03-27 12:10:35+00:00
- **Authors**: Takehiko Ohkawa, Takuma Yagi, Atsushi Hashimoto, Yoshitaka Ushiku, Yoichi Sato
- **Comment**: Accepted to IEEE Access 2021
- **Journal**: None
- **Summary**: Hand segmentation is a crucial task in first-person vision. Since first-person images exhibit strong bias in appearance among different environments, adapting a pre-trained segmentation model to a new domain is required in hand segmentation. Here, we focus on appearance gaps for hand regions and backgrounds separately. We propose (i) foreground-aware image stylization and (ii) consensus pseudo-labeling for domain adaptation of hand segmentation. We stylize source images independently for the foreground and background using target images as style. To resolve the domain shift that the stylization has not addressed, we apply careful pseudo-labeling by taking a consensus between the models trained on the source and stylized source images. We validated our method on domain adaptation of hand segmentation from real and simulation images. Our method achieved state-of-the-art performance in both settings. We also demonstrated promising results in challenging multi-target domain adaptation and domain generalization settings. Code is available at https://github.com/ut-vision/FgSty-CPL.



### Shapes as Product Differentiation: Neural Network Embedding in the Analysis of Markets for Fonts
- **Arxiv ID**: http://arxiv.org/abs/2107.02739v1
- **DOI**: None
- **Categories**: **econ.EM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02739v1)
- **Published**: 2021-07-06 17:12:27+00:00
- **Updated**: 2021-07-06 17:12:27+00:00
- **Authors**: Sukjin Han, Eric H. Schulman, Kristen Grauman, Santhosh Ramakrishnan
- **Comment**: None
- **Journal**: None
- **Summary**: Many differentiated products have key attributes that are unstructured and thus high-dimensional (e.g., design, text). Instead of treating unstructured attributes as unobservables in economic models, quantifying them can be important to answer interesting economic questions. To propose an analytical framework for this type of products, this paper considers one of the simplest design products -- fonts -- and investigates merger and product differentiation using an original dataset from the world's largest online marketplace for fonts. We quantify font shapes by constructing embeddings from a deep convolutional neural network. Each embedding maps a font's shape onto a low-dimensional vector. In the resulting product space, designers are assumed to engage in Hotelling-type spatial competition. From the image embeddings, we construct two alternative measures that capture the degree of design differentiation. We then study the causal effects of a merger on the merging firm's creative decisions using the constructed measures in a synthetic control method. We find that the merger causes the merging firm to increase the visual variety of font design. Notably, such effects are not captured when using traditional measures for product offerings (e.g., specifications and the number of products) constructed from structured data.



### Anomaly Detection using Edge Computing in Video Surveillance System: Review
- **Arxiv ID**: http://arxiv.org/abs/2107.02778v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02778v2)
- **Published**: 2021-07-06 17:41:56+00:00
- **Updated**: 2021-08-07 09:20:43+00:00
- **Authors**: Devashree R. Patrikar, Mayur Rajram Parate
- **Comment**: 27 pages, 6 figures, 5 Tables
- **Journal**: None
- **Summary**: The current concept of Smart Cities influences urban planners and researchers to provide modern, secured and sustainable infrastructure and give a decent quality of life to its residents. To fulfill this need video surveillance cameras have been deployed to enhance the safety and well-being of the citizens. Despite technical developments in modern science, abnormal event detection in surveillance video systems is challenging and requires exhaustive human efforts. In this paper, we surveyed various methodologies developed to detect anomalies in intelligent video surveillance. Firstly, we revisit the surveys on anomaly detection in the last decade. We then present a systematic categorization of methodologies developed for ease of understanding. Considering the notion of anomaly depends on context, we identify different objects-of-interest and publicly available datasets in anomaly detection. Since anomaly detection is considered a time-critical application of computer vision, our emphasis is on anomaly detection using edge devices and approaches explicitly designed for them. Further, we discuss the challenges and opportunities involved in anomaly detection at the edge.



### iPOKE: Poking a Still Image for Controlled Stochastic Video Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2107.02790v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02790v2)
- **Published**: 2021-07-06 17:57:55+00:00
- **Updated**: 2021-10-06 05:45:28+00:00
- **Authors**: Andreas Blattmann, Timo Milbich, Michael Dorkenwald, Björn Ommer
- **Comment**: ICCV 2021, Project page is available at https://bit.ly/3dJN4Lf
- **Journal**: None
- **Summary**: How would a static scene react to a local poke? What are the effects on other parts of an object if you could locally push it? There will be distinctive movement, despite evident variations caused by the stochastic nature of our world. These outcomes are governed by the characteristic kinematics of objects that dictate their overall motion caused by a local interaction. Conversely, the movement of an object provides crucial information about its underlying distinctive kinematics and the interdependencies between its parts. This two-way relation motivates learning a bijective mapping between object kinematics and plausible future image sequences. Therefore, we propose iPOKE -- invertible Prediction of Object Kinematics -- that, conditioned on an initial frame and a local poke, allows to sample object kinematics and establishes a one-to-one correspondence to the corresponding plausible videos, thereby providing a controlled stochastic video synthesis. In contrast to previous works, we do not generate arbitrary realistic videos, but provide efficient control of movements, while still capturing the stochastic nature of our environment and the diversity of plausible outcomes it entails. Moreover, our approach can transfer kinematics onto novel object instances and is not confined to particular object classes. Our project page is available at https://bit.ly/3dJN4Lf.



### Depth-supervised NeRF: Fewer Views and Faster Training for Free
- **Arxiv ID**: http://arxiv.org/abs/2107.02791v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02791v2)
- **Published**: 2021-07-06 17:58:35+00:00
- **Updated**: 2022-04-29 19:06:52+00:00
- **Authors**: Kangle Deng, Andrew Liu, Jun-Yan Zhu, Deva Ramanan
- **Comment**: Project page: http://www.cs.cmu.edu/~dsnerf/ GitHub:
  https://github.com/dunbar12138/DSNeRF
- **Journal**: None
- **Summary**: A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting incorrect geometries when given an insufficient number of input views. One potential reason is that standard volumetric rendering does not enforce the constraint that most of a scene's geometry consist of empty space and opaque surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning radiance fields that takes advantage of readily-available depth supervision. We leverage the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as "free" depth supervision during training: we add a loss to encourage the distribution of a ray's terminating depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can render better images given fewer training views while training 2-3x faster. Further, we show that our loss is compatible with other recently proposed NeRF methods, demonstrating that depth is a cheap and easily digestible supervisory signal. And finally, we find that DS-NeRF can support other types of depth supervision such as scanned depth sensors and RGB-D reconstruction outputs.



### Learned Visual Navigation for Under-Canopy Agricultural Robots
- **Arxiv ID**: http://arxiv.org/abs/2107.02792v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.02792v1)
- **Published**: 2021-07-06 17:59:02+00:00
- **Updated**: 2021-07-06 17:59:02+00:00
- **Authors**: Arun Narenthiran Sivakumar, Sahil Modi, Mateus Valverde Gasparino, Che Ellis, Andres Eduardo Baquero Velasquez, Girish Chowdhary, Saurabh Gupta
- **Comment**: RSS 2021. Project website with data and videos:
  https://ansivakumar.github.io/learned-visual-navigation/
- **Journal**: None
- **Summary**: We describe a system for visually guided autonomous navigation of under-canopy farm robots. Low-cost under-canopy robots can drive between crop rows under the plant canopy and accomplish tasks that are infeasible for over-the-canopy drones or larger agricultural equipment. However, autonomously navigating them under the canopy presents a number of challenges: unreliable GPS and LiDAR, high cost of sensing, challenging farm terrain, clutter due to leaves and weeds, and large variability in appearance over the season and across crop types. We address these challenges by building a modular system that leverages machine learning for robust and generalizable perception from monocular RGB images from low-cost cameras, and model predictive control for accurate control in challenging terrain. Our system, CropFollow, is able to autonomously drive 485 meters per intervention on average, outperforming a state-of-the-art LiDAR based system (286 meters per intervention) in extensive field testing spanning over 25 km.



### Deep Learning for Micro-expression Recognition: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2107.02823v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2107.02823v5)
- **Published**: 2021-07-06 18:05:52+00:00
- **Updated**: 2022-10-10 11:20:14+00:00
- **Authors**: Yante Li, Jinsheng Wei, Yang Liu, Janne Kauttonen, Guoying Zhao
- **Comment**: 20 pages, 8 figures
- **Journal**: None
- **Summary**: Micro-expressions (MEs) are involuntary facial movements revealing people's hidden feelings in high-stake situations and have practical importance in medical treatment, national security, interrogations and many human-computer interaction systems. Early methods for MER mainly based on traditional appearance and geometry features. Recently, with the success of deep learning (DL) in various fields, neural networks have received increasing interests in MER. Different from macro-expressions, MEs are spontaneous, subtle, and rapid facial movements, leading to difficult data collection, thus have small-scale datasets. DL based MER becomes challenging due to above ME characters. To date, various DL approaches have been proposed to solve the ME issues and improve MER performance. In this survey, we provide a comprehensive review of deep micro-expression recognition (MER), including datasets, deep MER pipeline, and the bench-marking of most influential methods. This survey defines a new taxonomy for the field, encompassing all aspects of MER based on DL. For each aspect, the basic approaches and advanced developments are summarized and discussed. In addition, we conclude the remaining challenges and and potential directions for the design of robust deep MER systems. To the best of our knowledge, this is the first survey of deep MER methods, and this survey can serve as a reference point for future MER research.



### Plot2Spectra: an Automatic Spectra Extraction Tool
- **Arxiv ID**: http://arxiv.org/abs/2107.02827v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02827v1)
- **Published**: 2021-07-06 18:17:28+00:00
- **Updated**: 2021-07-06 18:17:28+00:00
- **Authors**: Weixin Jiang, Eric Schwenker, Trevor Spreadbury, Kai Li, Maria K. Y. Chan, Oliver Cossairt
- **Comment**: None
- **Journal**: None
- **Summary**: Different types of spectroscopies, such as X-ray absorption near edge structure (XANES) and Raman spectroscopy, play a very important role in analyzing the characteristics of different materials. In scientific literature, XANES/Raman data are usually plotted in line graphs which is a visually appropriate way to represent the information when the end-user is a human reader. However, such graphs are not conducive to direct programmatic analysis due to the lack of automatic tools. In this paper, we develop a plot digitizer, named Plot2Spectra, to extract data points from spectroscopy graph images in an automatic fashion, which makes it possible for large scale data acquisition and analysis. Specifically, the plot digitizer is a two-stage framework. In the first axis alignment stage, we adopt an anchor-free detector to detect the plot region and then refine the detected bounding boxes with an edge-based constraint to locate the position of two axes. We also apply scene text detector to extract and interpret all tick information below the x-axis. In the second plot data extraction stage, we first employ semantic segmentation to separate pixels belonging to plot lines from the background, and from there, incorporate optical flow constraints to the plot line pixels to assign them to the appropriate line (data instance) they encode. Extensive experiments are conducted to validate the effectiveness of the proposed plot digitizer, which shows that such a tool could help accelerate the discovery and machine learning of materials properties.



### Poly-NL: Linear Complexity Non-local Layers with Polynomials
- **Arxiv ID**: http://arxiv.org/abs/2107.02859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.02859v1)
- **Published**: 2021-07-06 19:51:37+00:00
- **Updated**: 2021-07-06 19:51:37+00:00
- **Authors**: Francesca Babiloni, Ioannis Marras, Filippos Kokkinos, Jiankang Deng, Grigorios Chrysos, Stefanos Zafeiriou
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Spatial self-attention layers, in the form of Non-Local blocks, introduce long-range dependencies in Convolutional Neural Networks by computing pairwise similarities among all possible positions. Such pairwise functions underpin the effectiveness of non-local layers, but also determine a complexity that scales quadratically with respect to the input size both in space and time. This is a severely limiting factor that practically hinders the applicability of non-local blocks to even moderately sized inputs. Previous works focused on reducing the complexity by modifying the underlying matrix operations, however in this work we aim to retain full expressiveness of non-local layers while keeping complexity linear. We overcome the efficiency limitation of non-local blocks by framing them as special cases of 3rd order polynomial functions. This fact enables us to formulate novel fast Non-Local blocks, capable of reducing the complexity from quadratic to linear with no loss in performance, by replacing any direct computation of pairwise similarities with element-wise multiplications. The proposed method, which we dub as "Poly-NL", is competitive with state-of-the-art performance across image recognition, instance segmentation, and face detection tasks, while having considerably less computational overhead.



### Image Complexity Guided Network Compression for Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.02927v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.02927v1)
- **Published**: 2021-07-06 22:28:10+00:00
- **Updated**: 2021-07-06 22:28:10+00:00
- **Authors**: Suraj Mishra, Danny Z. Chen, X. Sharon Hu
- **Comment**: ACM JETC
- **Journal**: None
- **Summary**: Compression is a standard procedure for making convolutional neural networks (CNNs) adhere to some specific computing resource constraints. However, searching for a compressed architecture typically involves a series of time-consuming training/validation experiments to determine a good compromise between network size and performance accuracy. To address this, we propose an image complexity-guided network compression technique for biomedical image segmentation. Given any resource constraints, our framework utilizes data complexity and network architecture to quickly estimate a compressed model which does not require network training. Specifically, we map the dataset complexity to the target network accuracy degradation caused by compression. Such mapping enables us to predict the final accuracy for different network sizes, based on the computed dataset complexity. Thus, one may choose a solution that meets both the network size and segmentation accuracy requirements. Finally, the mapping is used to determine the convolutional layer-wise multiplicative factor for generating a compressed network. We conduct experiments using 5 datasets, employing 3 commonly-used CNN architectures for biomedical image segmentation as representative networks. Our proposed framework is shown to be effective for generating compressed segmentation networks, retaining up to $\approx 95\%$ of the full-sized network segmentation accuracy, and at the same time, utilizing $\approx 32x$ fewer network trainable weights (average reduction) of the full-sized networks.



