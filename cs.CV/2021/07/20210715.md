# Arxiv Papers in cs.CV on 2021-07-15
### Learning Sparse Interaction Graphs of Partially Detected Pedestrians for Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2107.07056v3
- **DOI**: 10.1109/LRA.2021.3138547
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.07056v3)
- **Published**: 2021-07-15 00:45:11+00:00
- **Updated**: 2022-02-03 02:13:35+00:00
- **Authors**: Zhe Huang, Ruohua Li, Kazuki Shin, Katherine Driggs-Campbell
- **Comment**: 8 pages, 6 figures, Accepted by RA-L with ICRA 2022 presentation
  option
- **Journal**: None
- **Summary**: Multi-pedestrian trajectory prediction is an indispensable element of autonomous systems that safely interact with crowds in unstructured environments. Many recent efforts in trajectory prediction algorithms have focused on understanding social norms behind pedestrian motions. Yet we observe these works usually hold two assumptions, which prevent them from being smoothly applied to robot applications: (1) positions of all pedestrians are consistently tracked, and (2) the target agent pays attention to all pedestrians in the scene. The first assumption leads to biased interaction modeling with incomplete pedestrian data. The second assumption introduces aggregation of redundant surrounding information, and the target agent may be affected by unimportant neighbors or present overly conservative motion. Thus, we propose Gumbel Social Transformer, in which an Edge Gumbel Selector samples a sparse interaction graph of partially detected pedestrians at each time step. A Node Transformer Encoder and a Masked LSTM encode pedestrian features with sampled sparse graphs to predict trajectories. We demonstrate that our model overcomes potential problems caused by the aforementioned assumptions, and our approach outperforms related works in trajectory prediction benchmarks. Code is available at \url{https://github.com/tedhuang96/gst}.



### A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing
- **Arxiv ID**: http://arxiv.org/abs/2107.07058v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.07058v4)
- **Published**: 2021-07-15 00:55:27+00:00
- **Updated**: 2021-08-04 05:36:22+00:00
- **Authors**: Wei Liu, Pingping Zhang, Yinjie Lei, Xiaolin Huang, Jie Yang, Michael Ng
- **Comment**: This work is accepted by TPAMI. The code is available at
  https://github.com/wliusjtu/Generalized-Smoothing-Framework. arXiv admin
  note: substantial text overlap with arXiv:1907.09642
- **Journal**: None
- **Summary**: Image smoothing is a fundamental procedure in applications of both computer vision and graphics. The required smoothing properties can be different or even contradictive among different tasks. Nevertheless, the inherent smoothing nature of one smoothing operator is usually fixed and thus cannot meet the various requirements of different applications. In this paper, we first introduce the truncated Huber penalty function which shows strong flexibility under different parameter settings. A generalized framework is then proposed with the introduced truncated Huber penalty function. When combined with its strong flexibility, our framework is able to achieve diverse smoothing natures where contradictive smoothing behaviors can even be achieved. It can also yield the smoothing behavior that can seldom be achieved by previous methods, and superior performance is thus achieved in challenging cases. These together enable our framework capable of a range of applications and able to outperform the state-of-the-art approaches in several tasks, such as image detail enhancement, clip-art compression artifacts removal, guided depth map restoration, image texture removal, etc. In addition, an efficient numerical solution is provided and its convergence is theoretically guaranteed even the optimization framework is non-convex and non-smooth. A simple yet effective approach is further proposed to reduce the computational cost of our method while maintaining its performance. The effectiveness and superior performance of our approach are validated through comprehensive experiments in a range of applications. Our code is available at https://github.com/wliusjtu/Generalized-Smoothing-Framework.



### MeNToS: Tracklets Association with a Space-Time Memory Network
- **Arxiv ID**: http://arxiv.org/abs/2107.07067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.07067v1)
- **Published**: 2021-07-15 01:33:21+00:00
- **Updated**: 2021-07-15 01:33:21+00:00
- **Authors**: Mehdi Miah, Guillaume-Alexandre Bilodeau, Nicolas Saunier
- **Comment**: Presented at the "Robust Video Scene Understanding: Tracking and
  Video Segmentation" workshop (CVPR-W 2021)
- **Journal**: None
- **Summary**: We propose a method for multi-object tracking and segmentation (MOTS) that does not require fine-tuning or per benchmark hyperparameter selection. The proposed method addresses particularly the data association problem. Indeed, the recently introduced HOTA metric, that has a better alignment with the human visual assessment by evenly balancing detections and associations quality, has shown that improvements are still needed for data association. After creating tracklets using instance segmentation and optical flow, the proposed method relies on a space-time memory network (STM) developed for one-shot video object segmentation to improve the association of tracklets with temporal gaps. To the best of our knowledge, our method, named MeNToS, is the first to use the STM network to track object masks for MOTS. We took the 4th place in the RobMOTS challenge. The project page is https://mehdimiah.com/mentos.html.



### STAR: Sparse Transformer-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.07089v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.07089v1)
- **Published**: 2021-07-15 02:53:11+00:00
- **Updated**: 2021-07-15 02:53:11+00:00
- **Authors**: Feng Shi, Chonghan Lee, Liang Qiu, Yizhou Zhao, Tianyi Shen, Shivran Muralidhar, Tian Han, Song-Chun Zhu, Vijaykrishnan Narayanan
- **Comment**: None
- **Journal**: None
- **Summary**: The cognitive system for human action and behavior has evolved into a deep learning regime, and especially the advent of Graph Convolution Networks has transformed the field in recent years. However, previous works have mainly focused on over-parameterized and complex models based on dense graph convolution networks, resulting in low efficiency in training and inference. Meanwhile, the Transformer architecture-based model has not yet been well explored for cognitive application in human action and behavior estimation. This work proposes a novel skeleton-based human action recognition model with sparse attention on the spatial dimension and segmented linear attention on the temporal dimension of data. Our model can also process the variable length of video clips grouped as a single batch. Experiments show that our model can achieve comparable performance while utilizing much less trainable parameters and achieve high speed in training and inference. Experiments show that our model achieves 4~18x speedup and 1/7~1/15 model size compared with the baseline models at competitive accuracy.



### Applying the Case Difference Heuristic to Learn Adaptations from Deep Network Features
- **Arxiv ID**: http://arxiv.org/abs/2107.07095v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.07095v1)
- **Published**: 2021-07-15 03:11:56+00:00
- **Updated**: 2021-07-15 03:11:56+00:00
- **Authors**: Xiaomeng Ye, Ziwei Zhao, David Leake, Xizi Wang, David Crandall
- **Comment**: 7 pages, 2 figures, 1 table. To be published in the IJCAI-21 Workshop
  on Deep Learning, Case-Based Reasoning, and AutoML: Present and Future
  Synergies
- **Journal**: None
- **Summary**: The case difference heuristic (CDH) approach is a knowledge-light method for learning case adaptation knowledge from the case base of a case-based reasoning system. Given a pair of cases, the CDH approach attributes the difference in their solutions to the difference in the problems they solve, and generates adaptation rules to adjust solutions accordingly when a retrieved case and new query have similar problem differences. As an alternative to learning adaptation rules, several researchers have applied neural networks to learn to predict solution differences from problem differences. Previous work on such approaches has assumed that the feature set describing problems is predefined. This paper investigates a two-phase process combining deep learning for feature extraction and neural network based adaptation learning from extracted features. Its performance is demonstrated in a regression task on an image data: predicting age given the image of a face. Results show that the combined process can successfully learn adaptation knowledge applicable to nonsymbolic differences in cases. The CBR system achieves slightly lower performance overall than a baseline deep network regressor, but better performance than the baseline on novel queries.



### Compact and Optimal Deep Learning with Recurrent Parameter Generators
- **Arxiv ID**: http://arxiv.org/abs/2107.07110v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.07110v3)
- **Published**: 2021-07-15 04:23:59+00:00
- **Updated**: 2022-10-26 23:56:01+00:00
- **Authors**: Jiayun Wang, Yubei Chen, Stella X. Yu, Brian Cheung, Yann LeCun
- **Comment**: None
- **Journal**: WACV 2023
- **Summary**: Deep learning has achieved tremendous success by training increasingly large models, which are then compressed for practical deployment. We propose a drastically different approach to compact and optimal deep learning: We decouple the Degrees of freedom (DoF) and the actual number of parameters of a model, optimize a small DoF with predefined random linear constraints for a large model of arbitrary architecture, in one-stage end-to-end learning. Specifically, we create a recurrent parameter generator (RPG), which repeatedly fetches parameters from a ring and unpacks them onto a large model with random permutation and sign flipping to promote parameter decorrelation. We show that gradient descent can automatically find the best model under constraints with faster convergence. Our extensive experimentation reveals a log-linear relationship between model DoF and accuracy. Our RPG demonstrates remarkable DoF reduction and can be further pruned and quantized for additional run-time performance gain. For example, in terms of top-1 accuracy on ImageNet, RPG achieves $96\%$ of ResNet18's performance with only $18\%$ DoF (the equivalent of one convolutional layer) and $52\%$ of ResNet34's performance with only $0.25\%$ DoF! Our work shows a significant potential of constrained neural optimization in compact and optimal deep learning.



### What Image Features Boost Housing Market Predictions?
- **Arxiv ID**: http://arxiv.org/abs/2107.07148v1
- **DOI**: 10.1109/TMM.2020.2966890
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.07148v1)
- **Published**: 2021-07-15 06:32:10+00:00
- **Updated**: 2021-07-15 06:32:10+00:00
- **Authors**: Zona Kostic, Aleksandar Jevremovic
- **Comment**: None
- **Journal**: None
- **Summary**: The attractiveness of a property is one of the most interesting, yet challenging, categories to model. Image characteristics are used to describe certain attributes, and to examine the influence of visual factors on the price or timeframe of the listing. In this paper, we propose a set of techniques for the extraction of visual features for efficient numerical inclusion in modern-day predictive algorithms. We discuss techniques such as Shannon's entropy, calculating the center of gravity, employing image segmentation, and using Convolutional Neural Networks. After comparing these techniques as applied to a set of property-related images (indoor, outdoor, and satellite), we conclude the following: (i) the entropy is the most efficient single-digit visual measure for housing price prediction; (ii) image segmentation is the most important visual feature for the prediction of housing lifespan; and (iii) deep image features can be used to quantify interior characteristics and contribute to captivation modeling. The set of 40 image features selected here carries a significant amount of predictive power and outperforms some of the strongest metadata predictors. Without any need to replace a human expert in a real-estate appraisal process, we conclude that the techniques presented in this paper can efficiently describe visible characteristics, thus introducing perceived attractiveness as a quantitative measure into the predictive modeling of housing.



### Semantic Image Cropping
- **Arxiv ID**: http://arxiv.org/abs/2107.07153v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.07153v1)
- **Published**: 2021-07-15 06:54:42+00:00
- **Updated**: 2021-07-15 06:54:42+00:00
- **Authors**: Oriol Corcoll
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic image cropping techniques are commonly used to enhance the aesthetic quality of an image; they do it by detecting the most beautiful or the most salient parts of the image and removing the unwanted content to have a smaller image that is more visually pleasing. In this thesis, I introduce an additional dimension to the problem of cropping, semantics. I argue that image cropping can also enhance the image's relevancy for a given entity by using the semantic information contained in the image. I call this problem, Semantic Image Cropping. To support my argument, I provide a new dataset containing 100 images with at least two different entities per image and four ground truth croppings collected using Amazon Mechanical Turk. I use this dataset to show that state-of-the-art cropping algorithms that only take into account aesthetics do not perform well in the problem of semantic image cropping. Additionally, I provide a new deep learning system that takes not just aesthetics but also semantics into account to generate image croppings, and I evaluate its performance using my new semantic cropping dataset, showing that using the semantic information of an image can help to produce better croppings.



### What and When to Look?: Temporal Span Proposal Network for Video Relation Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.07154v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.07154v2)
- **Published**: 2021-07-15 07:01:26+00:00
- **Updated**: 2022-10-05 09:41:51+00:00
- **Authors**: Sangmin Woo, Junhyug Noh, Kangil Kim
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Identifying relations between objects is central to understanding the scene. While several works have been proposed for relation modeling in the image domain, there have been many constraints in the video domain due to challenging dynamics of spatio-temporal interactions (e.g., between which objects are there an interaction? when do relations start and end?). To date, two representative methods have been proposed to tackle Video Visual Relation Detection (VidVRD): segment-based and window-based. We first point out limitations of these methods and propose a novel approach named Temporal Span Proposal Network (TSPN). TSPN tells what to look: it sparsifies relation search space by scoring relationness of object pair, i.e., measuring how probable a relation exist. TSPN tells when to look: it simultaneously predicts start-end timestamps (i.e., temporal spans) and categories of the all possible relations by utilizing full video context. These two designs enable a win-win scenario: it accelerates training by 2X or more than existing methods and achieves competitive performance on two VidVRD benchmarks (ImageNet-VidVDR and VidOR). Moreover, comprehensive ablative experiments demonstrate the effectiveness of our approach. Codes are available at https://github.com/sangminwoo/Temporal-Span-Proposal-Network-VidVRD.



### An Efficient and Small Convolutional Neural Network for Pest Recognition -- ExquisiteNet
- **Arxiv ID**: http://arxiv.org/abs/2107.07167v1
- **DOI**: 10.1109/ECICE50847.2020.9301938
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.07167v1)
- **Published**: 2021-07-15 07:34:45+00:00
- **Updated**: 2021-07-15 07:34:45+00:00
- **Authors**: Shi-Yao Zhou, Chung-Yen Su
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: Nowadays, due to the rapid population expansion, food shortage has become a critical issue. In order to stabilizing the food source production, preventing crops from being attacked by pests is very important. In generally, farmers use pesticides to kill pests, however, improperly using pesticides will also kill some insects which is beneficial to crops, such as bees. If the number of bees is too few, the supplement of food in the world will be in short. Besides, excessive pesticides will seriously pollute the environment. Accordingly, farmers need a machine which can automatically recognize the pests. Recently, deep learning is popular because its effectiveness in the field of image classification. In this paper, we propose a small and efficient model called ExquisiteNet to complete the task of recognizing the pests and we expect to apply our model on mobile devices. ExquisiteNet mainly consists of two blocks. One is double fusion with squeeze-and-excitation-bottleneck block (DFSEB block), and the other is max feature expansion block (ME block). ExquisiteNet only has 0.98M parameters and its computing speed is very fast almost the same as SqueezeNet. In order to evaluate our model's performance, we test our model on a benchmark pest dataset called IP102. Compared to many state-of-the-art models, such as ResNet101, ShuffleNetV2, MobileNetV3-large and EfficientNet etc., our model achieves higher accuracy, that is, 52.32% on the test set of IP102 without any data augmentation.



### Deep Learning based Food Instance Segmentation using Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2107.07191v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.07191v2)
- **Published**: 2021-07-15 08:36:54+00:00
- **Updated**: 2021-07-20 13:42:37+00:00
- **Authors**: D. Park, J. Lee, J. Lee, K. Lee
- **Comment**: Accepted by UR2021(Ubiquitous Robots 2021) conference
- **Journal**: None
- **Summary**: In the process of intelligently segmenting foods in images using deep neural networks for diet management, data collection and labeling for network training are very important but labor-intensive tasks. In order to solve the difficulties of data collection and annotations, this paper proposes a food segmentation method applicable to real-world through synthetic data. To perform food segmentation on healthcare robot systems, such as meal assistance robot arm, we generate synthetic data using the open-source 3D graphics software Blender placing multiple objects on meal plate and train Mask R-CNN for instance segmentation. Also, we build a data collection system and verify our segmentation model on real-world food data. As a result, on our real-world dataset, the model trained only synthetic data is available to segment food instances that are not trained with 52.2% mask AP@all, and improve performance by +6.4%p after fine-tuning comparing to the model trained from scratch. In addition, we also confirm the possibility and performance improvement on the public dataset for fair analysis. Our code and pre-trained weights are avaliable online at: https://github.com/gist-ailab/Food-Instance-Segmentation



### Incorporating Lambertian Priors into Surface Normals Measurement
- **Arxiv ID**: http://arxiv.org/abs/2107.07192v1
- **DOI**: 10.1109/TIM.2021.3096282
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.07192v1)
- **Published**: 2021-07-15 08:40:55+00:00
- **Updated**: 2021-07-15 08:40:55+00:00
- **Authors**: Yakun Ju, Muwei Jian, Shaoxiang Guo, Yingyu Wang, Huiyu Zhou, Junyu Dong
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of photometric stereo is to measure the precise surface normal of a 3D object from observations with various shading cues. However, non-Lambertian surfaces influence the measurement accuracy due to irregular shading cues. Despite deep neural networks have been employed to simulate the performance of non-Lambertian surfaces, the error in specularities, shadows, and crinkle regions is hard to be reduced. In order to address this challenge, we here propose a photometric stereo network that incorporates Lambertian priors to better measure the surface normal. In this paper, we use the initial normal under the Lambertian assumption as the prior information to refine the normal measurement, instead of solely applying the observed shading cues to deriving the surface normal. Our method utilizes the Lambertian information to reparameterize the network weights and the powerful fitting ability of deep neural networks to correct these errors caused by general reflectance properties. Our explorations include: the Lambertian priors (1) reduce the learning hypothesis space, making our method learn the mapping in the same surface normal space and improving the accuracy of learning, and (2) provides the differential features learning, improving the surfaces reconstruction of details. Extensive experiments verify the effectiveness of the proposed Lambertian prior photometric stereo network in accurate surface normal measurement, on the challenging benchmark dataset.



### Neighbor-view Enhanced Model for Vision and Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2107.07201v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.07201v3)
- **Published**: 2021-07-15 09:11:02+00:00
- **Updated**: 2021-07-24 08:03:02+00:00
- **Authors**: Dong An, Yuankai Qi, Yan Huang, Qi Wu, Liang Wang, Tieniu Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Vision and Language Navigation (VLN) requires an agent to navigate to a target location by following natural language instructions. Most of existing works represent a navigation candidate by the feature of the corresponding single view where the candidate lies in. However, an instruction may mention landmarks out of the single view as references, which might lead to failures of textual-visual matching of existing methods. In this work, we propose a multi-module Neighbor-View Enhanced Model (NvEM) to adaptively incorporate visual contexts from neighbor views for better textual-visual matching. Specifically, our NvEM utilizes a subject module and a reference module to collect contexts from neighbor views. The subject module fuses neighbor views at a global level, and the reference module fuses neighbor objects at a local level. Subjects and references are adaptively determined via attention me'chanisms. Our model also includes an action module to utilize the strong orientation guidance (e.g., "turn left") in instructions. Each module predicts navigation action separately and their weighted sum is used for predicting the final action. Extensive experimental results demonstrate the effectiveness of the proposed method on the R2R and R4R benchmarks against several state-of-the-art navigators, and NvEM even beats some pre-training ones. Our code is available at https://github.com/MarSaKi/NvEM.



### StyleVideoGAN: A Temporal Generative Model using a Pretrained StyleGAN
- **Arxiv ID**: http://arxiv.org/abs/2107.07224v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.07224v2)
- **Published**: 2021-07-15 09:58:15+00:00
- **Updated**: 2021-11-30 11:58:21+00:00
- **Authors**: Gereon Fox, Ayush Tewari, Mohamed Elgharib, Christian Theobalt
- **Comment**: Final draft
- **Journal**: None
- **Summary**: Generative adversarial models (GANs) continue to produce advances in terms of the visual quality of still images, as well as the learning of temporal correlations. However, few works manage to combine these two interesting capabilities for the synthesis of video content: Most methods require an extensive training dataset to learn temporal correlations, while being rather limited in the resolution and visual quality of their output. We present a novel approach to the video synthesis problem that helps to greatly improve visual quality and drastically reduce the amount of training data and resources necessary for generating videos. Our formulation separates the spatial domain, in which individual frames are synthesized, from the temporal domain, in which motion is generated. For the spatial domain we use a pre-trained StyleGAN network, the latent space of which allows control over the appearance of the objects it was trained for. The expressive power of this model allows us to embed our training videos in the StyleGAN latent space. Our temporal architecture is then trained not on sequences of RGB frames, but on sequences of StyleGAN latent codes. The advantageous properties of the StyleGAN space simplify the discovery of temporal correlations. We demonstrate that it suffices to train our temporal architecture on only 10 minutes of footage of 1 subject for about 6 hours. After training, our model can not only generate new portrait videos for the training subject, but also for any random subject which can be embedded in the StyleGAN space.



### COAST: COntrollable Arbitrary-Sampling NeTwork for Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/2107.07225v1
- **DOI**: 10.1109/TIP.2021.3091834
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.07225v1)
- **Published**: 2021-07-15 10:05:00+00:00
- **Updated**: 2021-07-15 10:05:00+00:00
- **Authors**: Di You, Jian Zhang, Jingfen Xie, Bin Chen, Siwei Ma
- **Comment**: Published in IEEE Transactions on Image Processing, 2021
- **Journal**: IEEE Transactions on Image Processing, vol. 30, pp. 6066-6080,
  2021
- **Summary**: Recent deep network-based compressive sensing (CS) methods have achieved great success. However, most of them regard different sampling matrices as different independent tasks and need to train a specific model for each target sampling matrix. Such practices give rise to inefficiency in computing and suffer from poor generalization ability. In this paper, we propose a novel COntrollable Arbitrary-Sampling neTwork, dubbed COAST, to solve CS problems of arbitrary-sampling matrices (including unseen sampling matrices) with one single model. Under the optimization-inspired deep unfolding framework, our COAST exhibits good interpretability. In COAST, a random projection augmentation (RPA) strategy is proposed to promote the training diversity in the sampling space to enable arbitrary sampling, and a controllable proximal mapping module (CPMM) and a plug-and-play deblocking (PnP-D) strategy are further developed to dynamically modulate the network features and effectively eliminate the blocking artifacts, respectively. Extensive experiments on widely used benchmark datasets demonstrate that our proposed COAST is not only able to handle arbitrary sampling matrices with one single model but also to achieve state-of-the-art performance with fast speed. The source code is available on https://github.com/jianzhangcs/COAST.



### Deep Automatic Natural Image Matting
- **Arxiv ID**: http://arxiv.org/abs/2107.07235v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.07235v1)
- **Published**: 2021-07-15 10:29:01+00:00
- **Updated**: 2021-07-15 10:29:01+00:00
- **Authors**: Jizhizi Li, Jing Zhang, Dacheng Tao
- **Comment**: Accepted to IJCAI-21, code and dataset available at
  https://github.com/JizhiziLi/AIM
- **Journal**: None
- **Summary**: Automatic image matting (AIM) refers to estimating the soft foreground from an arbitrary natural image without any auxiliary input like trimap, which is useful for image editing. Prior methods try to learn semantic features to aid the matting process while being limited to images with salient opaque foregrounds such as humans and animals. In this paper, we investigate the difficulties when extending them to natural images with salient transparent/meticulous foregrounds or non-salient foregrounds. To address the problem, a novel end-to-end matting network is proposed, which can predict a generalized trimap for any image of the above types as a unified semantic representation. Simultaneously, the learned semantic features guide the matting network to focus on the transition areas via an attention mechanism. We also construct a test set AIM-500 that contains 500 diverse natural images covering all types along with manually labeled alpha mattes, making it feasible to benchmark the generalization ability of AIM models. Results of the experiments demonstrate that our network trained on available composite matting datasets outperforms existing methods both objectively and subjectively. The source code and dataset are available at https://github.com/JizhiziLi/AIM.



### VILENS: Visual, Inertial, Lidar, and Leg Odometry for All-Terrain Legged Robots
- **Arxiv ID**: http://arxiv.org/abs/2107.07243v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.07243v2)
- **Published**: 2021-07-15 11:05:00+00:00
- **Updated**: 2022-02-21 10:02:24+00:00
- **Authors**: David Wisth, Marco Camurri, Maurice Fallon
- **Comment**: Video: https://youtu.be/NG4pkjJKhus
- **Journal**: None
- **Summary**: We present VILENS (Visual Inertial Lidar Legged Navigation System), an odometry system for legged robots based on factor graphs. The key novelty is the tight fusion of four different sensor modalities to achieve reliable operation when the individual sensors would otherwise produce degenerate estimation. To minimize leg odometry drift, we extend the robot's state with a linear velocity bias term which is estimated online. This bias is observable because of the tight fusion of this preintegrated velocity factor with vision, lidar, and IMU factors. Extensive experimental validation on different ANYmal quadruped robots is presented, for a total duration of 2 h and 1.8 km traveled. The experiments involved dynamic locomotion over loose rocks, slopes, and mud which caused challenges like slippage and terrain deformation. Perceptual challenges included dark and dusty underground caverns, and open and feature-deprived areas. We show an average improvement of 62% translational and 51% rotational errors compared to a state-of-the-art loosely coupled approach. To demonstrate its robustness, VILENS was also integrated with a perceptive controller and a local path planner.



### Single-image Full-body Human Relighting
- **Arxiv ID**: http://arxiv.org/abs/2107.07259v1
- **DOI**: 10.2312/sr.20211300
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2107.07259v1)
- **Published**: 2021-07-15 11:34:03+00:00
- **Updated**: 2021-07-15 11:34:03+00:00
- **Authors**: Manuel Lagunas, Xin Sun, Jimei Yang, Ruben Villegas, Jianming Zhang, Zhixin Shu, Belen Masia, Diego Gutierrez
- **Comment**: 11 pages, 12 figures
- **Journal**: Eurographics Symposium on Rendering (EGSR), 2021
- **Summary**: We present a single-image data-driven method to automatically relight images with full-body humans in them. Our framework is based on a realistic scene decomposition leveraging precomputed radiance transfer (PRT) and spherical harmonics (SH) lighting. In contrast to previous work, we lift the assumptions on Lambertian materials and explicitly model diffuse and specular reflectance in our data. Moreover, we introduce an additional light-dependent residual term that accounts for errors in the PRT-based image reconstruction. We propose a new deep learning architecture, tailored to the decomposition performed in PRT, that is trained using a combination of L1, logarithmic, and rendering losses. Our model outperforms the state of the art for full-body human relighting both with synthetic images and photographs.



### Multi-Channel Auto-Encoders and a Novel Dataset for Learning Domain Invariant Representations of Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/2107.07271v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.07271v1)
- **Published**: 2021-07-15 11:56:41+00:00
- **Updated**: 2021-07-15 11:56:41+00:00
- **Authors**: Andrew Moyes, Richard Gault, Kun Zhang, Ji Ming, Danny Crookes, Jing Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Domain shift is a problem commonly encountered when developing automated histopathology pipelines. The performance of machine learning models such as convolutional neural networks within automated histopathology pipelines is often diminished when applying them to novel data domains due to factors arising from differing staining and scanning protocols. The Dual-Channel Auto-Encoder (DCAE) model was previously shown to produce feature representations that are less sensitive to appearance variation introduced by different digital slide scanners. In this work, the Multi-Channel Auto-Encoder (MCAE) model is presented as an extension to DCAE which learns from more than two domains of data. Additionally, a synthetic dataset is generated using CycleGANs that contains aligned tissue images that have had their appearance synthetically modified. Experimental results show that the MCAE model produces feature representations that are less sensitive to inter-domain variations than the comparative StaNoSA method when tested on the novel synthetic data. Additionally, the MCAE and StaNoSA models are tested on a novel tissue classification task. The results of this experiment show the MCAE model out performs the StaNoSA model by 5 percentage-points in the f1-score. These results show that the MCAE model is able to generalise better to novel data and tasks than existing approaches by actively learning normalised feature representations.



### Training for temporal sparsity in deep neural networks, application in video processing
- **Arxiv ID**: http://arxiv.org/abs/2107.07305v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.07305v1)
- **Published**: 2021-07-15 13:17:11+00:00
- **Updated**: 2021-07-15 13:17:11+00:00
- **Authors**: Amirreza Yousefzadeh, Manolis Sifalakis
- **Comment**: None
- **Journal**: None
- **Summary**: Activation sparsity improves compute efficiency and resource utilization in sparsity-aware neural network accelerators. As the predominant operation in DNNs is multiply-accumulate (MAC) of activations with weights to compute inner products, skipping operations where (at least) one of the two operands is zero can make inference more efficient in terms of latency and power. Spatial sparsification of activations is a popular topic in DNN literature and several methods have already been established to bias a DNN for it. On the other hand, temporal sparsity is an inherent feature of bio-inspired spiking neural networks (SNNs), which neuromorphic processing exploits for hardware efficiency. Introducing and exploiting spatio-temporal sparsity, is a topic much less explored in DNN literature, but in perfect resonance with the trend in DNN, to shift from static signal processing to more streaming signal processing. Towards this goal, in this paper we introduce a new DNN layer (called Delta Activation Layer), whose sole purpose is to promote temporal sparsity of activations during training. A Delta Activation Layer casts temporal sparsity into spatial activation sparsity to be exploited when performing sparse tensor multiplications in hardware. By employing delta inference and ``the usual'' spatial sparsification heuristics during training, the resulting model learns to exploit not only spatial but also temporal activation sparsity (for a given input data distribution). One may use the Delta Activation Layer either during vanilla training or during a refinement phase. We have implemented Delta Activation Layer as an extension of the standard Tensoflow-Keras library, and applied it to train deep neural networks on the Human Action Recognition (UCF101) dataset. We report an almost 3x improvement of activation sparsity, with recoverable loss of model accuracy after longer training.



### Variational Topic Inference for Chest X-Ray Report Generation
- **Arxiv ID**: http://arxiv.org/abs/2107.07314v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.07314v1)
- **Published**: 2021-07-15 13:34:38+00:00
- **Updated**: 2021-07-15 13:34:38+00:00
- **Authors**: Ivona Najdenkoska, Xiantong Zhen, Marcel Worring, Ling Shao
- **Comment**: To be published in the International Conference on Medical Image
  Computing and Computer Assisted Intervention 2021
- **Journal**: None
- **Summary**: Automating report generation for medical imaging promises to reduce workload and assist diagnosis in clinical practice. Recent work has shown that deep learning models can successfully caption natural images. However, learning from medical data is challenging due to the diversity and uncertainty inherent in the reports written by different radiologists with discrepant expertise and experience. To tackle these challenges, we propose variational topic inference for automatic report generation. Specifically, we introduce a set of topics as latent variables to guide sentence generation by aligning image and language modalities in a latent space. The topics are inferred in a conditional variational inference framework, with each topic governing the generation of a sentence in the report. Further, we adopt a visual attention module that enables the model to attend to different locations in the image and generate more informative descriptions. We conduct extensive experiments on two benchmarks, namely Indiana U. Chest X-rays and MIMIC-CXR. The results demonstrate that our proposed variational topic inference method can generate novel reports rather than mere copies of reports used in training, while still achieving comparable performance to state-of-the-art methods in terms of standard language generation criteria.



### DynaDog+T: A Parametric Animal Model for Synthetic Canine Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2107.07330v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.07330v2)
- **Published**: 2021-07-15 13:53:10+00:00
- **Updated**: 2021-07-20 14:07:12+00:00
- **Authors**: Jake Deane, Sinead Kearney, Kwang In Kim, Darren Cosker
- **Comment**: CV4Animals Workshop in CVPR 2021. Update to correct minor spelling
  and grammer mistakes in supplementary material
- **Journal**: None
- **Summary**: Synthetic data is becoming increasingly common for training computer vision models for a variety of tasks. Notably, such data has been applied in tasks related to humans such as 3D pose estimation where data is either difficult to create or obtain in realistic settings. Comparatively, there has been less work into synthetic animal data and it's uses for training models. Consequently, we introduce a parametric canine model, DynaDog+T, for generating synthetic canine images and data which we use for a common computer vision task, binary segmentation, which would otherwise be difficult due to the lack of available data.



### Unsupervised Anomaly Instance Segmentation for Baggage Threat Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.07333v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.07333v2)
- **Published**: 2021-07-15 13:56:55+00:00
- **Updated**: 2021-07-16 18:24:14+00:00
- **Authors**: Taimur Hassan, Samet Akcay, Mohammed Bennamoun, Salman Khan, Naoufel Werghi
- **Comment**: Accepted in J-AIHC, Source Code is available at
  https://github.com/taimurhassan/anomaly
- **Journal**: None
- **Summary**: Identifying potential threats concealed within the baggage is of prime concern for the security staff. Many researchers have developed frameworks that can detect baggage threats from X-ray scans. However, to the best of our knowledge, all of these frameworks require extensive training on large-scale and well-annotated datasets, which are hard to procure in the real world. This paper presents a novel unsupervised anomaly instance segmentation framework that recognizes baggage threats, in X-ray scans, as anomalies without requiring any ground truth labels. Furthermore, thanks to its stylization capacity, the framework is trained only once, and at the inference stage, it detects and extracts contraband items regardless of their scanner specifications. Our one-staged approach initially learns to reconstruct normal baggage content via an encoder-decoder network utilizing a proposed stylization loss function. The model subsequently identifies the abnormal regions by analyzing the disparities within the original and the reconstructed scans. The anomalous regions are then clustered and post-processed to fit a bounding box for their localization. In addition, an optional classifier can also be appended with the proposed framework to recognize the categories of these extracted anomalies. A thorough evaluation of the proposed system on four public baggage X-ray datasets, without any re-training, demonstrates that it achieves competitive performance as compared to the conventional fully supervised methods (i.e., the mean average precision score of 0.7941 on SIXray, 0.8591 on GDXray, 0.7483 on OPIXray, and 0.5439 on COMPASS-XP dataset) while outperforming state-of-the-art semi-supervised and unsupervised baggage threat detection frameworks by 67.37%, 32.32%, 47.19%, and 45.81% in terms of F1 score across SIXray, GDXray, OPIXray, and COMPASS-XP datasets, respectively.



### Level generation and style enhancement -- deep learning for game development overview
- **Arxiv ID**: http://arxiv.org/abs/2107.07397v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4.3; J.5
- **Links**: [PDF](http://arxiv.org/pdf/2107.07397v1)
- **Published**: 2021-07-15 15:24:43+00:00
- **Updated**: 2021-07-15 15:24:43+00:00
- **Authors**: Piotr Migdał, Bartłomiej Olechno, Błażej Podgórski
- **Comment**: 16 pages, 10 figures, submitted to the 52nd International Simulation
  and Gaming Association (ISAGA) Conference 2021
- **Journal**: None
- **Summary**: We present practical approaches of using deep learning to create and enhance level maps and textures for video games -- desktop, mobile, and web. We aim to present new possibilities for game developers and level artists. The task of designing levels and filling them with details is challenging. It is both time-consuming and takes effort to make levels rich, complex, and with a feeling of being natural. Fortunately, recent progress in deep learning provides new tools to accompany level designers and visual artists. Moreover, they offer a way to generate infinite worlds for game replayability and adjust educational games to players' needs. We present seven approaches to create level maps, each using statistical methods, machine learning, or deep learning. In particular, we include:   - Generative Adversarial Networks for creating new images from existing examples (e.g. ProGAN).   - Super-resolution techniques for upscaling images while preserving crisp detail (e.g. ESRGAN).   - Neural style transfer for changing visual themes.   - Image translation - turning semantic maps into images (e.g. GauGAN).   - Semantic segmentation for turning images into semantic masks (e.g. U-Net).   - Unsupervised semantic segmentation for extracting semantic features (e.g. Tile2Vec).   - Texture synthesis - creating large patterns based on a smaller sample (e.g. InGAN).



### High carbon stock mapping at large scale with optical satellite imagery and spaceborne LIDAR
- **Arxiv ID**: http://arxiv.org/abs/2107.07431v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.07431v1)
- **Published**: 2021-07-15 16:21:21+00:00
- **Updated**: 2021-07-15 16:21:21+00:00
- **Authors**: Nico Lang, Konrad Schindler, Jan Dirk Wegner
- **Comment**: None
- **Journal**: None
- **Summary**: The increasing demand for commodities is leading to changes in land use worldwide. In the tropics, deforestation, which causes high carbon emissions and threatens biodiversity, is often linked to agricultural expansion. While the need for deforestation-free global supply chains is widely recognized, making progress in practice remains a challenge. Here, we propose an automated approach that aims to support conservation and sustainable land use planning decisions by mapping tropical landscapes at large scale and high spatial resolution following the High Carbon Stock (HCS) approach. A deep learning approach is developed that estimates canopy height for each 10 m Sentinel-2 pixel by learning from sparse GEDI LIDAR reference data, achieving an overall RMSE of 6.3 m. We show that these wall-to-wall maps of canopy top height are predictive for classifying HCS forests and degraded areas with an overall accuracy of 86 % and produce a first high carbon stock map for Indonesia, Malaysia, and the Philippines.



### FastSHAP: Real-Time Shapley Value Estimation
- **Arxiv ID**: http://arxiv.org/abs/2107.07436v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.07436v3)
- **Published**: 2021-07-15 16:34:45+00:00
- **Updated**: 2022-03-22 19:49:03+00:00
- **Authors**: Neil Jethani, Mukund Sudarshan, Ian Covert, Su-In Lee, Rajesh Ranganath
- **Comment**: ICLR 2022 Camera Ready, 20 pages, 10 figures, 3 tables
- **Journal**: None
- **Summary**: Shapley values are widely used to explain black-box models, but they are costly to calculate because they require many model evaluations. We introduce FastSHAP, a method for estimating Shapley values in a single forward pass using a learned explainer model. FastSHAP amortizes the cost of explaining many inputs via a learning approach inspired by the Shapley value's weighted least squares characterization, and it can be trained using standard stochastic gradient optimization. We compare FastSHAP to existing estimation approaches, revealing that it generates high-quality explanations with orders of magnitude speedup.



### StyleFusion: A Generative Model for Disentangling Spatial Segments
- **Arxiv ID**: http://arxiv.org/abs/2107.07437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.07437v1)
- **Published**: 2021-07-15 16:35:21+00:00
- **Updated**: 2021-07-15 16:35:21+00:00
- **Authors**: Omer Kafri, Or Patashnik, Yuval Alaluf, Daniel Cohen-Or
- **Comment**: Code is available at: https://github.com/OmerKafri/StyleFusion
- **Journal**: None
- **Summary**: We present StyleFusion, a new mapping architecture for StyleGAN, which takes as input a number of latent codes and fuses them into a single style code. Inserting the resulting style code into a pre-trained StyleGAN generator results in a single harmonized image in which each semantic region is controlled by one of the input latent codes. Effectively, StyleFusion yields a disentangled representation of the image, providing fine-grained control over each region of the generated image. Moreover, to help facilitate global control over the generated image, a special input latent code is incorporated into the fused representation. StyleFusion operates in a hierarchical manner, where each level is tasked with learning to disentangle a pair of image regions (e.g., the car body and wheels). The resulting learned disentanglement allows one to modify both local, fine-grained semantics (e.g., facial features) as well as more global features (e.g., pose and background), providing improved flexibility in the synthesis process. As a natural extension, StyleFusion enables one to perform semantically-aware cross-image mixing of regions that are not necessarily aligned. Finally, we demonstrate how StyleFusion can be paired with existing editing techniques to more faithfully constrain the edit to the user's region of interest.



### Adversarial Attacks on Multi-task Visual Perception for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2107.07449v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.07449v2)
- **Published**: 2021-07-15 16:53:48+00:00
- **Updated**: 2021-11-07 23:17:22+00:00
- **Authors**: Ibrahim Sobh, Ahmed Hamed, Varun Ravi Kumar, Senthil Yogamani
- **Comment**: Accepted for publication at Journal of Imaging Science and Technology
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have accomplished impressive success in various applications, including autonomous driving perception tasks, in recent years. On the other hand, current deep neural networks are easily fooled by adversarial attacks. This vulnerability raises significant concerns, particularly in safety-critical applications. As a result, research into attacking and defending DNNs has gained much coverage. In this work, detailed adversarial attacks are applied on a diverse multi-task visual perception deep network across distance estimation, semantic segmentation, motion detection, and object detection. The experiments consider both white and black box attacks for targeted and un-targeted cases, while attacking a task and inspecting the effect on all the others, in addition to inspecting the effect of applying a simple defense method. We conclude this paper by comparing and discussing the experimental results, proposing insights and future work. The visualizations of the attacks are available at https://youtu.be/6AixN90budY.



### Amodal segmentation just like doing a jigsaw
- **Arxiv ID**: http://arxiv.org/abs/2107.07464v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.07464v1)
- **Published**: 2021-07-15 17:08:53+00:00
- **Updated**: 2021-07-15 17:08:53+00:00
- **Authors**: Xunli Zeng, Jianqin Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Amodal segmentation is a new direction of instance segmentation while considering the segmentation of the visible and occluded parts of the instance. The existing state-of-the-art method uses multi-task branches to predict the amodal part and the visible part separately and subtract the visible part from the amodal part to obtain the occluded part. However, the amodal part contains visible information. Therefore, the separated prediction method will generate duplicate information. Different from this method, we propose a method of amodal segmentation based on the idea of the jigsaw. The method uses multi-task branches to predict the two naturally decoupled parts of visible and occluded, which is like getting two matching jigsaw pieces. Then put the two jigsaw pieces together to get the amodal part. This makes each branch focus on the modeling of the object. And we believe that there are certain rules in the occlusion relationship in the real world. This is a kind of occlusion context information. This jigsaw method can better model the occlusion relationship and use the occlusion context information, which is important for amodal segmentation. Experiments on two widely used amodally annotated datasets prove that our method exceeds existing state-of-the-art methods. The source code of this work will be made public soon.



### A modular U-Net for automated segmentation of X-ray tomography images in composite materials
- **Arxiv ID**: http://arxiv.org/abs/2107.07468v1
- **DOI**: 10.3389/fmats.2021.761229
- **Categories**: **eess.IV**, cs.CV, 68T07 (Primary) 68T45 (Secondary), I.4.6; I.2.10; I.5.4; J.2
- **Links**: [PDF](http://arxiv.org/pdf/2107.07468v1)
- **Published**: 2021-07-15 17:15:24+00:00
- **Updated**: 2021-07-15 17:15:24+00:00
- **Authors**: João P C Bertoldo, Etienne Decencière, David Ryckelynck, Henry Proudhon
- **Comment**: Submitted to Nature Machine Intelligence
- **Journal**: Front. Mater., 25 November 2021 Sec. Computational Materials
  Science
- **Summary**: X-ray Computed Tomography (XCT) techniques have evolved to a point that high-resolution data can be acquired so fast that classic segmentation methods are prohibitively cumbersome, demanding automated data pipelines capable of dealing with non-trivial 3D images. Deep learning has demonstrated success in many image processing tasks, including material science applications, showing a promising alternative for a humanfree segmentation pipeline. In this paper a modular interpretation of UNet (Modular U-Net) is proposed and trained to segment 3D tomography images of a three-phased glass fiber-reinforced Polyamide 66. We compare 2D and 3D versions of our model, finding that the former is slightly better than the latter. We observe that human-comparable results can be achievied even with only 10 annotated layers and using a shallow U-Net yields better results than a deeper one. As a consequence, Neural Network (NN) show indeed a promising venue to automate XCT data processing pipelines needing no human, adhoc intervention.



### Context-Conditional Adaptation for Recognizing Unseen Classes in Unseen Domains
- **Arxiv ID**: http://arxiv.org/abs/2107.07497v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.07497v1)
- **Published**: 2021-07-15 17:51:16+00:00
- **Updated**: 2021-07-15 17:51:16+00:00
- **Authors**: Puneet Mangla, Shivam Chandhok, Vineeth N Balasubramanian, Fahad Shahbaz Khan
- **Comment**: None
- **Journal**: None
- **Summary**: Recent progress towards designing models that can generalize to unseen domains (i.e domain generalization) or unseen classes (i.e zero-shot learning) has embarked interest towards building models that can tackle both domain-shift and semantic shift simultaneously (i.e zero-shot domain generalization). For models to generalize to unseen classes in unseen domains, it is crucial to learn feature representation that preserves class-level (domain-invariant) as well as domain-specific information. Motivated from the success of generative zero-shot approaches, we propose a feature generative framework integrated with a COntext COnditional Adaptive (COCOA) Batch-Normalization to seamlessly integrate class-level semantic and domain-specific information. The generated visual features better capture the underlying data distribution enabling us to generalize to unseen classes and domains at test-time. We thoroughly evaluate and analyse our approach on established large-scale benchmark - DomainNet and demonstrate promising performance over baselines and state-of-art methods.



### Recommending best course of treatment based on similarities of prognostic markers
- **Arxiv ID**: http://arxiv.org/abs/2107.07500v2
- **DOI**: 10.1007/978-3-030-92270-2_34
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.07500v2)
- **Published**: 2021-07-15 17:52:12+00:00
- **Updated**: 2021-07-19 07:39:23+00:00
- **Authors**: Sudhanshu, Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali Agarwal
- **Comment**: None
- **Journal**: None
- **Summary**: With the advancement in the technology sector spanning over every field, a huge influx of information is inevitable. Among all the opportunities that the advancements in the technology have brought, one of them is to propose efficient solutions for data retrieval. This means that from an enormous pile of data, the retrieval methods should allow the users to fetch the relevant and recent data over time. In the field of entertainment and e-commerce, recommender systems have been functioning to provide the aforementioned. Employing the same systems in the medical domain could definitely prove to be useful in variety of ways. Following this context, the goal of this paper is to propose collaborative filtering based recommender system in the healthcare sector to recommend remedies based on the symptoms experienced by the patients. Furthermore, a new dataset is developed consisting of remedies concerning various diseases to address the limited availability of the data. The proposed recommender system accepts the prognostic markers of a patient as the input and generates the best remedy course. With several experimental trials, the proposed model achieved promising results in recommending the possible remedy for given prognostic markers.



### MultiBench: Multiscale Benchmarks for Multimodal Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.07502v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2107.07502v2)
- **Published**: 2021-07-15 17:54:36+00:00
- **Updated**: 2021-11-10 07:31:56+00:00
- **Authors**: Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Chen, Peter Wu, Michelle A. Lee, Yuke Zhu, Ruslan Salakhutdinov, Louis-Philippe Morency
- **Comment**: NeurIPS 2021 Datasets and Benchmarks Track. Code:
  https://github.com/pliang279/MultiBench and Website:
  https://cmu-multicomp-lab.github.io/multibench/
- **Journal**: None
- **Summary**: Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. It is a challenging yet crucial area with numerous real-world applications in multimedia, affective computing, robotics, finance, human-computer interaction, and healthcare. Unfortunately, multimodal research has seen limited resources to study (1) generalization across domains and modalities, (2) complexity during training and inference, and (3) robustness to noisy and missing modalities. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiBench, a systematic and unified large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. MultiBench provides an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, MultiBench offers a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench introduces impactful challenges for future research, including scalability to large-scale multimodal datasets and robustness to realistic imperfections. To accompany this benchmark, we also provide a standardized implementation of 20 core approaches in multimodal learning. Simply applying methods proposed in different research areas can improve the state-of-the-art performance on 9/15 datasets. Therefore, MultiBench presents a milestone in unifying disjoint efforts in multimodal research and paves the way towards a better understanding of the capabilities and limitations of multimodal models, all the while ensuring ease of use, accessibility, and reproducibility. MultiBench, our standardized code, and leaderboards are publicly available, will be regularly updated, and welcomes inputs from the community.



### Self-supervised 3D Human Mesh Recovery from Noisy Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2107.07539v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.07539v2)
- **Published**: 2021-07-15 18:07:47+00:00
- **Updated**: 2021-11-26 20:39:34+00:00
- **Authors**: Xinxin Zuo, Sen Wang, Qiang Sun, Minglun Gong, Li Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel self-supervised approach to reconstruct human shape and pose from noisy point cloud data. Relying on large amount of dataset with ground-truth annotations, recent learning-based approaches predict correspondences for every vertice on the point cloud; Chamfer distance is usually used to minimize the distance between a deformed template model and the input point cloud. However, Chamfer distance is quite sensitive to noise and outliers, thus could be unreliable to assign correspondences. To address these issues, we model the probability distribution of the input point cloud as generated from a parametric human model under a Gaussian Mixture Model. Instead of explicitly aligning correspondences, we treat the process of correspondence search as an implicit probabilistic association by updating the posterior probability of the template model given the input. A novel self-supervised loss is further derived which penalizes the discrepancy between the deformed template and the input point cloud conditioned on the posterior probability. Our approach is very flexible, which works with both complete point cloud and incomplete ones including even a single depth image as input. Compared to previous self-supervised methods, our method shows the capability to deal with substantial noise and outliers. Extensive experiments conducted on various public synthetic datasets as well as a very noisy real dataset (i.e. CMU Panoptic) demonstrate the superior performance of our approach over the state-of-the-art methods.



### OdoViz: A 3D Odometry Visualization and Processing Tool
- **Arxiv ID**: http://arxiv.org/abs/2107.07557v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.07557v1)
- **Published**: 2021-07-15 18:37:19+00:00
- **Updated**: 2021-07-15 18:37:19+00:00
- **Authors**: Saravanabalagi Ramachandran, John McDonald
- **Comment**: Accepted, ITSC 2021
- **Journal**: None
- **Summary**: OdoViz is a reactive web-based tool for 3D visualization and processing of autonomous vehicle datasets designed to support common tasks in visual place recognition research. The system includes functionality for loading, inspecting, visualizing, and processing GPS/INS poses, point clouds and camera images. It supports a number of commonly used driving datasets and can be adapted to load custom datasets with minimal effort. OdoViz's design consists of a slim server to serve the datasets coupled with a rich client frontend. This design supports multiple deployment configurations including single user stand-alone installations, research group installations serving datasets internally across a lab, or publicly accessible web-frontends for providing online interfaces for exploring and interacting with datasets. The tool allows viewing complete vehicle trajectories traversed at multiple different time periods simultaneously, facilitating tasks such as sub-sampling, comparing and finding pose correspondences both across and within sequences. This significantly reduces the effort required in creating subsets of data from existing datasets for machine learning tasks. Further to the above, the system also supports adding custom extensions and plugins to extend the capabilities of the software for other potential data management, visualization and processing tasks. The platform has been open-sourced to promote its use and encourage further contributions from the research community.



### Real-Time Face Recognition System for Remote Employee Tracking
- **Arxiv ID**: http://arxiv.org/abs/2107.07576v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.07576v2)
- **Published**: 2021-07-15 19:21:37+00:00
- **Updated**: 2021-10-13 03:55:01+00:00
- **Authors**: Mohammad Sabik Irbaz, MD Abdullah Al Nasim, Refat E Ferdous
- **Comment**: Accepted in International Conference on Big Data, IoT and Machine
  Learning (BIM 2021)
- **Journal**: None
- **Summary**: During the COVID-19 pandemic, most of the human-to-human interactions have been stopped. To mitigate the spread of deadly coronavirus, many offices took the initiative so that the employees can work from home. But, tracking the employees and finding out if they are really performing what they were supposed to turn out to be a serious challenge for all the companies and organizations who are facilitating "Work From Home". To deal with the challenge effectively, we came up with a solution to track the employees with face recognition. We have been testing this system experimentally for our office. To train the face recognition module, we used FaceNet with KNN using the Labeled Faces in the Wild (LFW) dataset and achieved 97.8\% accuracy. We integrated the trained model into our central system, where the employees log their time. In this paper, we discuss in brief the system we have been experimenting with and the pros and cons of the system.



### Real-Time Violence Detection Using CNN-LSTM
- **Arxiv ID**: http://arxiv.org/abs/2107.07578v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.07578v1)
- **Published**: 2021-07-15 19:37:41+00:00
- **Updated**: 2021-07-15 19:37:41+00:00
- **Authors**: Mann Patel
- **Comment**: 5 pages, 9 figures
- **Journal**: None
- **Summary**: Violence rates however have been brought down about 57% during the span of the past 4 decades yet it doesn't change the way that the demonstration of violence actually happens, unseen by the law. Violence can be mass controlled sometimes by higher authorities, however, to hold everything in line one must "Microgovern" over each movement occurring in every road of each square. To address the butterfly effects impact in our setting, I made a unique model and a theorized system to handle the issue utilizing deep learning. The model takes the input of the CCTV video feeds and after drawing inference, recognizes if a violent movement is going on. And hypothesized architecture aims towards probability-driven computation of video feeds and reduces overhead from naively computing for every CCTV video feeds.



### Depth Estimation from Monocular Images and Sparse radar using Deep Ordinal Regression Network
- **Arxiv ID**: http://arxiv.org/abs/2107.07596v1
- **DOI**: 10.1109/ICIP42928.2021.9506550
- **Categories**: **eess.IV**, cs.AI, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2107.07596v1)
- **Published**: 2021-07-15 20:17:48+00:00
- **Updated**: 2021-07-15 20:17:48+00:00
- **Authors**: Chen-Chou Lo, Patrick Vandewalle
- **Comment**: Accepted to ICIP2021
- **Journal**: None
- **Summary**: We integrate sparse radar data into a monocular depth estimation model and introduce a novel preprocessing method for reducing the sparseness and limited field of view provided by radar. We explore the intrinsic error of different radar modalities and show our proposed method results in more data points with reduced error. We further propose a novel method for estimating dense depth maps from monocular 2D images and sparse radar measurements using deep learning based on the deep ordinal regression network by Fu et al. Radar data are integrated by first converting the sparse 2D points to a height-extended 3D measurement and then including it into the network using a late fusion approach. Experiments are conducted on the nuScenes dataset. Our experiments demonstrate state-of-the-art performance in both day and night scenes.



### Multi-Level Contrastive Learning for Few-Shot Problems
- **Arxiv ID**: http://arxiv.org/abs/2107.07608v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.07608v1)
- **Published**: 2021-07-15 21:00:02+00:00
- **Updated**: 2021-07-15 21:00:02+00:00
- **Authors**: Qing Chen, Jian Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive learning is a discriminative approach that aims at grouping similar samples closer and diverse samples far from each other. It it an efficient technique to train an encoder generating distinguishable and informative representations, and it may even increase the encoder's transferability. Most current applications of contrastive learning benefit only a single representation from the last layer of an encoder.In this paper, we propose a multi-level contrasitive learning approach which applies contrastive losses at different layers of an encoder to learn multiple representations from the encoder. Afterward, an ensemble can be constructed to take advantage of the multiple representations for the downstream tasks. We evaluated the proposed method on few-shot learning problems and conducted experiments using the mini-ImageNet and the tiered-ImageNet datasets. Our model achieved the new state-of-the-art results for both datasets, comparing to previous regular, ensemble, and contrastive learing (single-level) based approaches.



### An Energy-Efficient Edge Computing Paradigm for Convolution-based Image Upsampling
- **Arxiv ID**: http://arxiv.org/abs/2107.07647v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, cs.DC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.07647v2)
- **Published**: 2021-07-15 23:49:37+00:00
- **Updated**: 2021-07-26 05:34:59+00:00
- **Authors**: Ian Colbert, Ken Kreutz-Delgado, Srinjoy Das
- **Comment**: None
- **Journal**: None
- **Summary**: A novel energy-efficient edge computing paradigm is proposed for real-time deep learning-based image upsampling applications. State-of-the-art deep learning solutions for image upsampling are currently trained using either resize or sub-pixel convolution to learn kernels that generate high fidelity images with minimal artifacts. However, performing inference with these learned convolution kernels requires memory-intensive feature map transformations that dominate time and energy costs in real-time applications. To alleviate this pressure on memory bandwidth, we confine the use of resize or sub-pixel convolution to training in the cloud by transforming learned convolution kernels to deconvolution kernels before deploying them for inference as a functionally equivalent deconvolution. These kernel transformations, intended as a one-time cost when shifting from training to inference, enable a systems designer to use each algorithm in their optimal context by preserving the image fidelity learned when training in the cloud while minimizing data transfer penalties during inference at the edge. We also explore existing variants of deconvolution inference algorithms and introduce a novel variant for consideration. We analyze and compare the inference properties of convolution-based upsampling algorithms using a quantitative model of incurred time and energy costs and show that using deconvolution for inference at the edge improves both system latency and energy efficiency when compared to their sub-pixel or resize convolution counterparts.



