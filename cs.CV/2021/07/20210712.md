# Arxiv Papers in cs.CV on 2021-07-12
### Spatial and Temporal Networks for Facial Expression Recognition in the Wild Videos
- **Arxiv ID**: http://arxiv.org/abs/2107.05160v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.05160v1)
- **Published**: 2021-07-12 01:41:23+00:00
- **Updated**: 2021-07-12 01:41:23+00:00
- **Authors**: Shuyi Mao, Xinqi Fan, Xiaojiang Peng
- **Comment**: None
- **Journal**: None
- **Summary**: The paper describes our proposed methodology for the seven basic expression classification track of Affective Behavior Analysis in-the-wild (ABAW) Competition 2021. In this task, facial expression recognition (FER) methods aim to classify the correct expression category from a diverse background, but there are several challenges. First, to adapt the model to in-the-wild scenarios, we use the knowledge from pre-trained large-scale face recognition data. Second, we propose an ensemble model with a convolution neural network (CNN), a CNN-recurrent neural network (CNN-RNN), and a CNN-Transformer (CNN-Transformer), to incorporate both spatial and temporal information. Our ensemble model achieved F1 as 0.4133, accuracy as 0.6216 and final metric as 0.4821 on the validation set.



### Zero-Shot Compositional Concept Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.05176v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2107.05176v1)
- **Published**: 2021-07-12 03:31:56+00:00
- **Updated**: 2021-07-12 03:31:56+00:00
- **Authors**: Guangyue Xu, Parisa Kordjamshidi, Joyce Y. Chai
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the problem of recognizing compositional attribute-object concepts within the zero-shot learning (ZSL) framework. We propose an episode-based cross-attention (EpiCA) network which combines merits of cross-attention mechanism and episode-based training strategy to recognize novel compositional concepts. Firstly, EpiCA bases on cross-attention to correlate concept-visual information and utilizes the gated pooling layer to build contextualized representations for both images and concepts. The updated representations are used for a more in-depth multi-modal relevance calculation for concept recognition. Secondly, a two-phase episode training strategy, especially the transductive phase, is adopted to utilize unlabeled test examples to alleviate the low-resource learning problem. Experiments on two widely-used zero-shot compositional learning (ZSCL) benchmarks have demonstrated the effectiveness of the model compared with recent approaches on both conventional and generalized ZSCL settings.



### Early warning of pedestrians and cyclists
- **Arxiv ID**: http://arxiv.org/abs/2107.05186v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05186v1)
- **Published**: 2021-07-12 04:02:57+00:00
- **Updated**: 2021-07-12 04:02:57+00:00
- **Authors**: Joerg Christian Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art motor vehicles are able to break for pedestrians in an emergency. We investigate what it would take to issue an early warning to the driver so he/she has time to react. We have identified that predicting the intention of a pedestrian reliably by position is a particularly hard challenge. This paper describes an early pedestrian warning demonstration system.



### Improvement of image classification by multiple optical scattering
- **Arxiv ID**: http://arxiv.org/abs/2107.14051v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.14051v1)
- **Published**: 2021-07-12 04:12:41+00:00
- **Updated**: 2021-07-12 04:12:41+00:00
- **Authors**: Xinyu Gao, Yi Li, Yanqing Qiu, Bangning Mao, Miaogen Chen, Yanlong Meng, Chunliu Zhao, Juan Kang, Yong Guo, Changyu Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple optical scattering occurs when light propagates in a non-uniform medium. During the multiple scattering, images were distorted and the spatial information they carried became scrambled. However, the image information is not lost but presents in the form of speckle patterns (SPs). In this study, we built up an optical random scattering system based on an LCD and an RGB laser source. We found that the image classification can be improved by the help of random scattering which is considered as a feedforward neural network to extracts features from image. Along with the ridge classification deployed on computer, we achieved excellent classification accuracy higher than 94%, for a variety of data sets covering medical, agricultural, environmental protection and other fields. In addition, the proposed optical scattering system has the advantages of high speed, low power consumption, and miniaturization, which is suitable for deploying in edge computing applications.



### TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.05188v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.05188v1)
- **Published**: 2021-07-12 04:17:39+00:00
- **Updated**: 2021-07-12 04:17:39+00:00
- **Authors**: Yao Chang, Hu Menghan, Zhai Guangtao, Zhang Xiao-Ping
- **Comment**: 8 page, 3 figures
- **Journal**: None
- **Summary**: In recent years, computer-aided diagnosis has become an increasingly popular topic. Methods based on convolutional neural networks have achieved good performance in medical image segmentation and classification. Due to the limitations of the convolution operation, the long-term spatial features are often not accurately obtained. Hence, we propose a TransClaw U-Net network structure, which combines the convolution operation with the transformer operation in the encoding part. The convolution part is applied for extracting the shallow spatial features to facilitate the recovery of the image resolution after upsampling. The transformer part is used to encode the patches, and the self-attention mechanism is used to obtain global information between sequences. The decoding part retains the bottom upsampling structure for better detail segmentation performance. The experimental results on Synapse Multi-organ Segmentation Datasets show that the performance of TransClaw U-Net is better than other network structures. The ablation experiments also prove the generalization performance of TransClaw U-Net.



### Deep-learning-based Hyperspectral imaging through a RGB camera
- **Arxiv ID**: http://arxiv.org/abs/2107.05190v1
- **DOI**: 10.1117/1.JEI.30.5.053014
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.05190v1)
- **Published**: 2021-07-12 04:23:25+00:00
- **Updated**: 2021-07-12 04:23:25+00:00
- **Authors**: Xinyu Gao, Tianlang Wang, Jing Yang, Jinchao Tao, Yanqing Qiu, Yanlong Meng, Banging Mao, Pengwei Zhou, Yi Li
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral image (HSI) contains both spatial pattern and spectral information which has been widely used in food safety, remote sensing, and medical detection. However, the acquisition of hyperspectral images is usually costly due to the complicated apparatus for the acquisition of optical spectrum. Recently, it has been reported that HSI can be reconstructed from single RGB image using convolution neural network (CNN) algorithms. Compared with the traditional hyperspectral cameras, the method based on CNN algorithms is simple, portable and low cost. In this study, we focused on the influence of the RGB camera spectral sensitivity (CSS) on the HSI. A Xenon lamp incorporated with a monochromator were used as the standard light source to calibrate the CSS. And the experimental results show that the CSS plays a significant role in the reconstruction accuracy of an HSI. In addition, we proposed a new HSI reconstruction network where the dimensional structure of the original hyperspectral datacube was modified by 3D matrix transpose to improve the reconstruction accuracy.



### Delta Sampling R-BERT for limited data and low-light action recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.05202v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05202v1)
- **Published**: 2021-07-12 05:35:51+00:00
- **Updated**: 2021-07-12 05:35:51+00:00
- **Authors**: Sanchit Hira, Ritwik Das, Abhinav Modi, Daniil Pakhomov
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach to perform supervised action recognition in the dark. In this work, we present our results on the ARID dataset. Most previous works only evaluate performance on large, well illuminated datasets like Kinetics and HMDB51. We demonstrate that our work is able to achieve a very low error rate while being trained on a much smaller dataset of dark videos. We also explore a variety of training and inference strategies including domain transfer methodologies and also propose a simple but useful frame selection strategy. Our empirical results demonstrate that we beat previously published baseline models by 11%.



### Split, embed and merge: An accurate table structure recognizer
- **Arxiv ID**: http://arxiv.org/abs/2107.05214v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05214v3)
- **Published**: 2021-07-12 06:26:19+00:00
- **Updated**: 2022-01-30 07:25:38+00:00
- **Authors**: Zhenrong Zhang, Jianshu Zhang, Jun Du
- **Comment**: None
- **Journal**: None
- **Summary**: Table structure recognition is an essential part for making machines understand tables. Its main task is to recognize the internal structure of a table. However, due to the complexity and diversity in their structure and style, it is very difficult to parse the tabular data into the structured format which machines can understand easily, especially for complex tables. In this paper, we introduce Split, Embed and Merge (SEM), an accurate table structure recognizer. Our model takes table images as input and can correctly recognize the structure of tables, whether they are simple or a complex tables. SEM is mainly composed of three parts, splitter, embedder and merger. In the first stage, we apply the splitter to predict the potential regions of the table row (column) separators, and obtain the fine grid structure of the table. In the second stage, by taking a full consideration of the textual information in the table, we fuse the output features for each table grid from both vision and language modalities. Moreover, we achieve a higher precision in our experiments through adding additional semantic features. Finally, we process the merging of these basic table grids in a self-regression manner. The correspondent merging results is learned through the attention mechanism. In our experiments, SEM achieves an average F1-Measure of 97.11% on the SciTSR dataset which outperforms other methods by a large margin. We also won the first place in the complex table and third place in all tables in ICDAR 2021 Competition on Scientific Literature Parsing, Task-B. Extensive experiments on other publicly available datasets demonstrate that our model achieves state-of-the-art.



### Training of deep cross-modality conversion models with a small dataset, and their application in megavoltage CT to kilovoltage CT conversion
- **Arxiv ID**: http://arxiv.org/abs/2107.05238v2
- **DOI**: 10.1002/mp.15626
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2107.05238v2)
- **Published**: 2021-07-12 07:43:41+00:00
- **Updated**: 2022-04-05 09:33:23+00:00
- **Authors**: Sho Ozaki, Shizuo Kaji, Kanabu Nawa, Toshikazu Imae, Atsushi Aoki, Takahiro Nakamoto, Takeshi Ohta, Yuki Nozawa, Hideomi Yamashita, Akihiro Haga, Keiichi Nakagawa
- **Comment**: 3+27 pages, 13 figures, version published in Medical Physics
- **Journal**: None
- **Summary**: In recent years, deep-learning-based image processing has emerged as a valuable tool for medical imaging owing to its high performance. However, the quality of deep-learning-based methods heavily relies on the amount of training data; the high cost of acquiring a large dataset is a limitation to their utilization in medical fields. Herein, based on deep learning, we developed a computed tomography (CT) modality conversion method requiring only a few unsupervised images. The proposed method is based on CycleGAN with several extensions tailored for CT images, which aims at preserving the structure in the processed images and reducing the amount of training data. This method was applied to realize the conversion of megavoltage computed tomography (MVCT) to kilovoltage computed tomography (kVCT) images. Training was conducted using several datasets acquired from patients with head and neck cancer. The size of the datasets ranged from 16 slices (two patients) to 2745 slices (137 patients) for MVCT and 2824 slices (98 patients) for kVCT. The required size of the training data was found to be as small as a few hundred slices. By statistical and visual evaluations, the quality improvement and structure preservation of the MVCT images converted by the proposed model were investigated. As a clinical benefit, it was observed by medical doctors that the converted images enhanced the precision of contouring. We developed an MVCT to kVCT conversion model based on deep learning, which can be trained using only a few hundred unpaired images. The stability of the model against changes in data size was demonstrated. This study promotes the reliable use of deep learning in clinical medicine by partially answering commonly asked questions, such as "Is our data sufficient?" and "How much data should we acquire?"



### Prb-GAN: A Probabilistic Framework for GAN Modelling
- **Arxiv ID**: http://arxiv.org/abs/2107.05241v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2107.05241v1)
- **Published**: 2021-07-12 08:04:13+00:00
- **Updated**: 2021-07-12 08:04:13+00:00
- **Authors**: Blessen George, Vinod K. Kurmi, Vinay P. Namboodiri
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) are very popular to generate realistic images, but they often suffer from the training instability issues and the phenomenon of mode loss. In order to attain greater diversity in GAN synthesized data, it is critical to solving the problem of mode loss. Our work explores probabilistic approaches to GAN modelling that could allow us to tackle these issues. We present Prb-GANs, a new variation that uses dropout to create a distribution over the network parameters with the posterior learnt using variational inference. We describe theoretically and validate experimentally using simple and complex datasets the benefits of such an approach. We look into further improvements using the concept of uncertainty measures. Through a set of further modifications to the loss functions for each network of the GAN, we are able to get results that show the improvement of GAN performance. Our methods are extremely simple and require very little modification to existing GAN architecture.



### Modeling Explicit Concerning States for Reinforcement Learning in Visual Dialogue
- **Arxiv ID**: http://arxiv.org/abs/2107.05250v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05250v2)
- **Published**: 2021-07-12 08:15:35+00:00
- **Updated**: 2021-11-26 07:03:03+00:00
- **Authors**: Zipeng Xu, Fandong Meng, Xiaojie Wang, Duo Zheng, Chenxu Lv, Jie Zhou
- **Comment**: In BMVC 2021
- **Journal**: None
- **Summary**: To encourage AI agents to conduct meaningful Visual Dialogue (VD), the use of Reinforcement Learning has been proven potential. In Reinforcement Learning, it is crucial to represent states and assign rewards based on the action-caused transitions of states. However, the state representation in previous Visual Dialogue works uses the textual information only and its transitions are implicit. In this paper, we propose Explicit Concerning States (ECS) to represent what visual contents are concerned at each round and what have been concerned throughout the Visual Dialogue. ECS is modeled from multimodal information and is represented explicitly. Based on ECS, we formulate two intuitive and interpretable rewards to encourage the Visual Dialogue agents to converse on diverse and informative visual information. Experimental results on the VisDial v1.0 dataset show our method enables the Visual Dialogue agents to generate more visual coherent, less repetitive and more visual informative dialogues compared with previous methods, according to multiple automatic metrics, human study and qualitative analysis.



### AutoFB: Automating Fetal Biometry Estimation from Standard Ultrasound Planes
- **Arxiv ID**: http://arxiv.org/abs/2107.05255v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.05255v1)
- **Published**: 2021-07-12 08:42:31+00:00
- **Updated**: 2021-07-12 08:42:31+00:00
- **Authors**: Sophia Bano, Brian Dromey, Francisco Vasconcelos, Raffaele Napolitano, Anna L. David, Donald M. Peebles, Danail Stoyanov
- **Comment**: Accepted at MICCAI 2021
- **Journal**: None
- **Summary**: During pregnancy, ultrasound examination in the second trimester can assess fetal size according to standardized charts. To achieve a reproducible and accurate measurement, a sonographer needs to identify three standard 2D planes of the fetal anatomy (head, abdomen, femur) and manually mark the key anatomical landmarks on the image for accurate biometry and fetal weight estimation. This can be a time-consuming operator-dependent task, especially for a trainee sonographer. Computer-assisted techniques can help in automating the fetal biometry computation process. In this paper, we present a unified automated framework for estimating all measurements needed for the fetal weight assessment. The proposed framework semantically segments the key fetal anatomies using state-of-the-art segmentation models, followed by region fitting and scale recovery for the biometry estimation. We present an ablation study of segmentation algorithms to show their robustness through 4-fold cross-validation on a dataset of 349 ultrasound standard plane images from 42 pregnancies. Moreover, we show that the network with the best segmentation performance tends to be more accurate for biometry estimation. Furthermore, we demonstrate that the error between clinically measured and predicted fetal biometry is lower than the permissible error during routine clinical measurements.



### Learned super resolution ultrasound for improved breast lesion characterization
- **Arxiv ID**: http://arxiv.org/abs/2107.05270v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.05270v1)
- **Published**: 2021-07-12 09:04:20+00:00
- **Updated**: 2021-07-12 09:04:20+00:00
- **Authors**: Or Bar-Shira, Ahuva Grubstein, Yael Rapson, Dror Suhami, Eli Atar, Keren Peri-Hanania, Ronnie Rosen, Yonina C. Eldar
- **Comment**: to be published in MICCAI 2021 proceedings
- **Journal**: None
- **Summary**: Breast cancer is the most common malignancy in women. Mammographic findings such as microcalcifications and masses, as well as morphologic features of masses in sonographic scans, are the main diagnostic targets for tumor detection. However, improved specificity of these imaging modalities is required. A leading alternative target is neoangiogenesis. When pathological, it contributes to the development of numerous types of tumors, and the formation of metastases. Hence, demonstrating neoangiogenesis by visualization of the microvasculature may be of great importance. Super resolution ultrasound localization microscopy enables imaging of the microvasculature at the capillary level. Yet, challenges such as long reconstruction time, dependency on prior knowledge of the system Point Spread Function (PSF), and separability of the Ultrasound Contrast Agents (UCAs), need to be addressed for translation of super-resolution US into the clinic. In this work we use a deep neural network architecture that makes effective use of signal structure to address these challenges. We present in vivo human results of three different breast lesions acquired with a clinical US scanner. By leveraging our trained network, the microvasculature structure is recovered in a short time, without prior PSF knowledge, and without requiring separability of the UCAs. Each of the recoveries exhibits a different structure that corresponds with the known histological structure. This study demonstrates the feasibility of in vivo human super resolution, based on a clinical scanner, to increase US specificity for different breast lesions and promotes the use of US in the diagnosis of breast pathologies.



### TransAttUnet: Multi-level Attention-guided U-Net with Transformer for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.05274v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.05274v2)
- **Published**: 2021-07-12 09:17:06+00:00
- **Updated**: 2022-07-09 03:28:13+00:00
- **Authors**: Bingzhi Chen, Yishu Liu, Zheng Zhang, Guangming Lu, Adams Wai Kin Kong
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate segmentation of organs or lesions from medical images is crucial for reliable diagnosis of diseases and organ morphometry. In recent years, convolutional encoder-decoder solutions have achieved substantial progress in the field of automatic medical image segmentation. Due to the inherent bias in the convolution operations, prior models mainly focus on local visual cues formed by the neighboring pixels, but fail to fully model the long-range contextual dependencies. In this paper, we propose a novel Transformer-based Attention Guided Network called TransAttUnet, in which the multi-level guided attention and multi-scale skip connection are designed to jointly enhance the performance of the semantical segmentation architecture. Inspired by Transformer, the self-aware attention (SAA) module with Transformer Self Attention (TSA) and Global Spatial Attention (GSA) is incorporated into TransAttUnet to effectively learn the non-local interactions among encoder features. Moreover, we also use additional multi-scale skip connections between decoder blocks to aggregate the upsampled features with different semantic scales. In this way, the representation ability of multi-scale context information is strengthened to generate discriminative features. Benefitting from these complementary components, the proposed TransAttUnet can effectively alleviate the loss of fine details caused by the stacking of convolution layers and the consecutive sampling operations, finally improving the segmentation quality of medical images. Extensive experiments on multiple medical image segmentation datasets from different imaging modalities demonstrate that the proposed method consistently outperforms the state-of-the-art baselines. Our code and pre-trained models are available at: https://github.com/YishuLiu/TransAttUnet.



### Geographical Knowledge-driven Representation Learning for Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2107.05276v1
- **DOI**: 10.1109/TGRS.2021.3115569
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05276v1)
- **Published**: 2021-07-12 09:23:15+00:00
- **Updated**: 2021-07-12 09:23:15+00:00
- **Authors**: Wenyuan Li, Keyan Chen, Hao Chen, Zhenwei Shi
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: The proliferation of remote sensing satellites has resulted in a massive amount of remote sensing images. However, due to human and material resource constraints, the vast majority of remote sensing images remain unlabeled. As a result, it cannot be applied to currently available deep learning methods. To fully utilize the remaining unlabeled images, we propose a Geographical Knowledge-driven Representation learning method for remote sensing images (GeoKR), improving network performance and reduce the demand for annotated data. The global land cover products and geographical location associated with each remote sensing image are regarded as geographical knowledge to provide supervision for representation learning and network pre-training. An efficient pre-training framework is proposed to eliminate the supervision noises caused by imaging times and resolutions difference between remote sensing images and geographical knowledge. A large scale pre-training dataset Levir-KR is proposed to support network pre-training. It contains 1,431,950 remote sensing images from Gaofen series satellites with various resolutions. Experimental results demonstrate that our proposed method outperforms ImageNet pre-training and self-supervised representation learning methods and significantly reduces the burden of data annotation on downstream tasks such as scene classification, semantic segmentation, object detection, and cloud / snow detection. It demonstrates that our proposed method can be used as a novel paradigm for pre-training neural networks. Codes will be available on https://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning.



### ICDAR 2021 Competition on Integrated Circuit Text Spotting and Aesthetic Assessment
- **Arxiv ID**: http://arxiv.org/abs/2107.05279v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05279v1)
- **Published**: 2021-07-12 09:29:02+00:00
- **Updated**: 2021-07-12 09:29:02+00:00
- **Authors**: Chun Chet Ng, Akmalul Khairi Bin Nazaruddin, Yeong Khang Lee, Xinyu Wang, Yuliang Liu, Chee Seng Chan, Lianwen Jin, Yipeng Sun, Lixin Fan
- **Comment**: Technical report of ICDAR 2021 Competition on Integrated Circuit Text
  Spotting and Aesthetic Assessment
- **Journal**: International Conference on Document Analysis and Recognition
  (ICDAR) 2021
- **Summary**: With hundreds of thousands of electronic chip components are being manufactured every day, chip manufacturers have seen an increasing demand in seeking a more efficient and effective way of inspecting the quality of printed texts on chip components. The major problem that deters this area of research is the lacking of realistic text on chips datasets to act as a strong foundation. Hence, a text on chips dataset, ICText is used as the main target for the proposed Robust Reading Challenge on Integrated Circuit Text Spotting and Aesthetic Assessment (RRC-ICText) 2021 to encourage the research on this problem. Throughout the entire competition, we have received a total of 233 submissions from 10 unique teams/individuals. Details of the competition and submission results are presented in this report.



### End-to-end Trainable Deep Neural Network for Robotic Grasp Detection and Semantic Segmentation from RGB
- **Arxiv ID**: http://arxiv.org/abs/2107.05287v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.05287v2)
- **Published**: 2021-07-12 09:45:13+00:00
- **Updated**: 2022-02-11 10:06:12+00:00
- **Authors**: Stefan Ainetter, Friedrich Fraundorfer
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we introduce a novel, end-to-end trainable CNN-based architecture to deliver high quality results for grasp detection suitable for a parallel-plate gripper, and semantic segmentation. Utilizing this, we propose a novel refinement module that takes advantage of previously calculated grasp detection and semantic segmentation and further increases grasp detection accuracy. Our proposed network delivers state-of-the-art accuracy on two popular grasp dataset, namely Cornell and Jacquard. As additional contribution, we provide a novel dataset extension for the OCID dataset, making it possible to evaluate grasp detection in highly challenging scenes. Using this dataset, we show that semantic segmentation can additionally be used to assign grasp candidates to object classes, which can be used to pick specific objects in the scene.



### Real-Time Super-Resolution System of 4K-Video Based on Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.05307v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.05307v2)
- **Published**: 2021-07-12 10:35:05+00:00
- **Updated**: 2021-07-14 14:42:34+00:00
- **Authors**: Yanpeng Cao, Chengcheng Wang, Changjun Song, Yongming Tang, He Li
- **Comment**: 8 pages, 7 figures, ASAP
- **Journal**: None
- **Summary**: Video super-resolution (VSR) technology excels in reconstructing low-quality video, avoiding unpleasant blur effect caused by interpolation-based algorithms. However, vast computation complexity and memory occupation hampers the edge of deplorability and the runtime inference in real-life applications, especially for large-scale VSR task. This paper explores the possibility of real-time VSR system and designs an efficient and generic VSR network, termed EGVSR. The proposed EGVSR is based on spatio-temporal adversarial learning for temporal coherence. In order to pursue faster VSR processing ability up to 4K resolution, this paper tries to choose lightweight network structure and efficient upsampling method to reduce the computation required by EGVSR network under the guarantee of high visual quality. Besides, we implement the batch normalization computation fusion, convolutional acceleration algorithm and other neural network acceleration techniques on the actual hardware platform to optimize the inference process of EGVSR network. Finally, our EGVSR achieves the real-time processing capacity of 4K@29.61FPS. Compared with TecoGAN, the most advanced VSR network at present, we achieve 85.04% reduction of computation density and 7.92x performance speedups. In terms of visual quality, the proposed EGVSR tops the list of most metrics (such as LPIPS, tOF, tLP, etc.) on the public test dataset Vid4 and surpasses other state-of-the-art methods in overall performance score. The source code of this project can be found on https://github.com/Thmen/EGVSR.



### R3L: Connecting Deep Reinforcement Learning to Recurrent Neural Networks for Image Denoising via Residual Recovery
- **Arxiv ID**: http://arxiv.org/abs/2107.05318v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.05318v1)
- **Published**: 2021-07-12 11:12:49+00:00
- **Updated**: 2021-07-12 11:12:49+00:00
- **Authors**: Rongkai Zhang, Jiang Zhu, Zhiyuan Zha, Justin Dauwels, Bihan Wen
- **Comment**: Accepted by ICIP 2021
- **Journal**: None
- **Summary**: State-of-the-art image denoisers exploit various types of deep neural networks via deterministic training. Alternatively, very recent works utilize deep reinforcement learning for restoring images with diverse or unknown corruptions. Though deep reinforcement learning can generate effective policy networks for operator selection or architecture search in image restoration, how it is connected to the classic deterministic training in solving inverse problems remains unclear. In this work, we propose a novel image denoising scheme via Residual Recovery using Reinforcement Learning, dubbed R3L. We show that R3L is equivalent to a deep recurrent neural network that is trained using a stochastic reward, in contrast to many popular denoisers using supervised learning with deterministic losses. To benchmark the effectiveness of reinforcement learning in R3L, we train a recurrent neural network with the same architecture for residual recovery using the deterministic loss, thus to analyze how the two different training strategies affect the denoising performance. With such a unified benchmarking system, we demonstrate that the proposed R3L has better generalizability and robustness in image denoising when the estimated noise level varies, comparing to its counterparts using deterministic training, as well as various state-of-the-art image denoising algorithms.



### Human-like Relational Models for Activity Recognition in Video
- **Arxiv ID**: http://arxiv.org/abs/2107.05319v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05319v2)
- **Published**: 2021-07-12 11:13:17+00:00
- **Updated**: 2022-01-11 10:47:07+00:00
- **Authors**: Joseph Chrol-Cannon, Andrew Gilbert, Ranko Lazic, Adithya Madhusoodanan, Frank Guerin
- **Comment**: None
- **Journal**: None
- **Summary**: Video activity recognition by deep neural networks is impressive for many classes. However, it falls short of human performance, especially for challenging to discriminate activities. Humans differentiate these complex activities by recognising critical spatio-temporal relations among explicitly recognised objects and parts, for example, an object entering the aperture of a container. Deep neural networks can struggle to learn such critical relationships effectively. Therefore we propose a more human-like approach to activity recognition, which interprets a video in sequential temporal phases and extracts specific relationships among objects and hands in those phases. Random forest classifiers are learnt from these extracted relationships. We apply the method to a challenging subset of the something-something dataset and achieve a more robust performance against neural network baselines on challenging activities.



### Visual Transformer with Statistical Test for COVID-19 Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.05334v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.05334v1)
- **Published**: 2021-07-12 11:48:33+00:00
- **Updated**: 2021-07-12 11:48:33+00:00
- **Authors**: Chih-Chung Hsu, Guan-Lin Chen, Mei-Hsuan Wu
- **Comment**: this is a draft for MIA-Competition/ICCV2021
- **Journal**: None
- **Summary**: With the massive damage in the world caused by Coronavirus Disease 2019 SARS-CoV-2 (COVID-19), many related research topics have been proposed in the past two years. The Chest Computed Tomography (CT) scans are the most valuable materials to diagnose the COVID-19 symptoms. However, most schemes for COVID-19 classification of Chest CT scan is based on a single-slice level, implying that the most critical CT slice should be selected from the original CT scan volume manually. We simultaneously propose 2-D and 3-D models to predict the COVID-19 of CT scan to tickle this issue. In our 2-D model, we introduce the Deep Wilcoxon signed-rank test (DWCC) to determine the importance of each slice of a CT scan to overcome the issue mentioned previously. Furthermore, a Convolutional CT scan-Aware Transformer (CCAT) is proposed to discover the context of the slices fully. The frame-level feature is extracted from each CT slice based on any backbone network and followed by feeding the features to our within-slice-Transformer (WST) to discover the context information in the pixel dimension. The proposed Between-Slice-Transformer (BST) is used to aggregate the extracted spatial-context features of every CT slice. A simple classifier is then used to judge whether the Spatio-temporal features are COVID-19 or non-COVID-19. The extensive experiments demonstrated that the proposed CCAT and DWCC significantly outperform the state-of-the-art methods.



### EndoUDA: A modality independent segmentation approach for endoscopy imaging
- **Arxiv ID**: http://arxiv.org/abs/2107.05342v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.05342v1)
- **Published**: 2021-07-12 11:57:33+00:00
- **Updated**: 2021-07-12 11:57:33+00:00
- **Authors**: Numan Celik, Sharib Ali, Soumya Gupta, Barbara Braden, Jens Rittscher
- **Comment**: 10 pages, 3 figures, 3 tables. Accepted for MICCAI 2021
- **Journal**: None
- **Summary**: Gastrointestinal (GI) cancer precursors require frequent monitoring for risk stratification of patients. Automated segmentation methods can help to assess risk areas more accurately, and assist in therapeutic procedures or even removal. In clinical practice, addition to the conventional white-light imaging (WLI), complimentary modalities such as narrow-band imaging (NBI) and fluorescence imaging are used. While, today most segmentation approaches are supervised and only concentrated on a single modality dataset, this work exploits to use a target-independent unsupervised domain adaptation (UDA) technique that is capable to generalize to an unseen target modality. In this context, we propose a novel UDA-based segmentation method that couples the variational autoencoder and U-Net with a common EfficientNet-B4 backbone, and uses a joint loss for latent-space optimization for target samples. We show that our model can generalize to unseen target NBI (target) modality when trained using only WLI (source) modality. Our experiments on both upper and lower GI endoscopy data show the effectiveness of our approach compared to naive supervised approach and state-of-the-art UDA segmentation methods.



### Fine-Grained AutoAugmentation for Multi-Label Classification
- **Arxiv ID**: http://arxiv.org/abs/2107.05384v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.05384v2)
- **Published**: 2021-07-12 12:47:16+00:00
- **Updated**: 2021-07-13 07:38:08+00:00
- **Authors**: Ya Wang, Hesen Chen, Fangyi Zhang, Yaohua Wang, Xiuyu Sun, Ming Lin, Hao Li
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation is a commonly used approach to improving the generalization of deep learning models. Recent works show that learned data augmentation policies can achieve better generalization than hand-crafted ones. However, most of these works use unified augmentation policies for all samples in a dataset, which is observed not necessarily beneficial for all labels in multi-label classification tasks, i.e., some policies may have negative impacts on some labels while benefitting the others. To tackle this problem, we propose a novel Label-Based AutoAugmentation (LB-Aug) method for multi-label scenarios, where augmentation policies are generated with respect to labels by an augmentation-policy network. The policies are learned via reinforcement learning using policy gradient methods, providing a mapping from instance labels to their optimal augmentation policies. Numerical experiments show that our LB-Aug outperforms previous state-of-the-art augmentation methods by large margins in multiple benchmarks on image and video classification.



### Transfer Learning from Synthetic to Real LiDAR Point Cloud for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.05399v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05399v2)
- **Published**: 2021-07-12 12:51:08+00:00
- **Updated**: 2021-12-02 04:52:43+00:00
- **Authors**: Aoran Xiao, Jiaxing Huang, Dayan Guan, Fangneng Zhan, Shijian Lu
- **Comment**: Accepted by AAAI 2022
- **Journal**: None
- **Summary**: Knowledge transfer from synthetic to real data has been widely studied to mitigate data annotation constraints in various computer vision tasks such as semantic segmentation. However, the study focused on 2D images and its counterpart in 3D point clouds segmentation lags far behind due to the lack of large-scale synthetic datasets and effective transfer methods. We address this issue by collecting SynLiDAR, a large-scale synthetic LiDAR dataset that contains point-wise annotated point clouds with accurate geometric shapes and comprehensive semantic classes. SynLiDAR was collected from multiple virtual environments with rich scenes and layouts which consists of over 19 billion points of 32 semantic classes. In addition, we design PCT, a novel point cloud translator that effectively mitigates the gap between synthetic and real point clouds. Specifically, we decompose the synthetic-to-real gap into an appearance component and a sparsity component and handle them separately which improves the point cloud translation greatly. We conducted extensive experiments over three transfer learning setups including data augmentation, semi-supervised domain adaptation and unsupervised domain adaptation. Extensive experiments show that SynLiDAR provides a high-quality data source for studying 3D transfer and the proposed PCT achieves superior point cloud translation consistently across the three setups. SynLiDAR project page: \url{https://github.com/xiaoaoran/SynLiDAR}



### Source-Free Adaptation to Measurement Shift via Bottom-Up Feature Restoration
- **Arxiv ID**: http://arxiv.org/abs/2107.05446v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2107.05446v3)
- **Published**: 2021-07-12 14:21:14+00:00
- **Updated**: 2022-03-17 10:40:17+00:00
- **Authors**: Cian Eastwood, Ian Mason, Christopher K. I. Williams, Bernhard Schölkopf
- **Comment**: ICLR 2022 (Spotlight)
- **Journal**: None
- **Summary**: Source-free domain adaptation (SFDA) aims to adapt a model trained on labelled data in a source domain to unlabelled data in a target domain without access to the source-domain data during adaptation. Existing methods for SFDA leverage entropy-minimization techniques which: (i) apply only to classification; (ii) destroy model calibration; and (iii) rely on the source model achieving a good level of feature-space class-separation in the target domain. We address these issues for a particularly pervasive type of domain shift called measurement shift which can be resolved by restoring the source features rather than extracting new ones. In particular, we propose Feature Restoration (FR) wherein we: (i) store a lightweight and flexible approximation of the feature distribution under the source data; and (ii) adapt the feature-extractor such that the approximate feature distribution under the target data realigns with that saved on the source. We additionally propose a bottom-up training scheme which boosts performance, which we call Bottom-Up Feature Restoration (BUFR). On real and synthetic data, we demonstrate that BUFR outperforms existing SFDA methods in terms of accuracy, calibration, and data efficiency, while being less reliant on the performance of the source model in the target domain.



### Scenes and Surroundings: Scene Graph Generation using Relation Transformer
- **Arxiv ID**: http://arxiv.org/abs/2107.05448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05448v1)
- **Published**: 2021-07-12 14:22:20+00:00
- **Updated**: 2021-07-12 14:22:20+00:00
- **Authors**: Rajat Koner, Poulami Sinhamahapatra, Volker Tresp
- **Comment**: arXiv admin note: text overlap with arXiv:2004.06193
- **Journal**: None
- **Summary**: Identifying objects in an image and their mutual relationships as a scene graph leads to a deep understanding of image content. Despite the recent advancement in deep learning, the detection and labeling of visual object relationships remain a challenging task. This work proposes a novel local-context aware architecture named relation transformer, which exploits complex global objects to object and object to edge (relation) interactions. Our hierarchical multi-head attention-based approach efficiently captures contextual dependencies between objects and predicts their relationships. In comparison to state-of-the-art approaches, we have achieved an overall mean \textbf{4.85\%} improvement and a new benchmark across all the scene graph generation tasks on the Visual Genome dataset.



### AxonEM Dataset: 3D Axon Instance Segmentation of Brain Cortical Regions
- **Arxiv ID**: http://arxiv.org/abs/2107.05451v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05451v1)
- **Published**: 2021-07-12 14:24:03+00:00
- **Updated**: 2021-07-12 14:24:03+00:00
- **Authors**: Donglai Wei, Kisuk Lee, Hanyu Li, Ran Lu, J. Alexander Bae, Zequan Liu, Lifu Zhang, Márcia dos Santos, Zudi Lin, Thomas Uram, Xueying Wang, Ignacio Arganda-Carreras, Brian Matejek, Narayanan Kasthuri, Jeff Lichtman, Hanspeter Pfister
- **Comment**: The two first authors contributed equally. To be published in the
  proceedings of MICCAI 2021
- **Journal**: None
- **Summary**: Electron microscopy (EM) enables the reconstruction of neural circuits at the level of individual synapses, which has been transformative for scientific discoveries. However, due to the complex morphology, an accurate reconstruction of cortical axons has become a major challenge. Worse still, there is no publicly available large-scale EM dataset from the cortex that provides dense ground truth segmentation for axons, making it difficult to develop and evaluate large-scale axon reconstruction methods. To address this, we introduce the AxonEM dataset, which consists of two 30x30x30 um^3 EM image volumes from the human and mouse cortex, respectively. We thoroughly proofread over 18,000 axon instances to provide dense 3D axon instance segmentation, enabling large-scale evaluation of axon reconstruction methods. In addition, we densely annotate nine ground truth subvolumes for training, per each data volume. With this, we reproduce two published state-of-the-art methods and provide their evaluation results as a baseline. We publicly release our code and data at https://connectomics-bazaar.github.io/proj/AxonEM/index.html to foster the development of advanced methods.



### Visual-Tactile Cross-Modal Data Generation using Residue-Fusion GAN with Feature-Matching and Perceptual Losses
- **Arxiv ID**: http://arxiv.org/abs/2107.05468v1
- **DOI**: 10.1109/LRA.2021.3095925
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.05468v1)
- **Published**: 2021-07-12 14:36:16+00:00
- **Updated**: 2021-07-12 14:36:16+00:00
- **Authors**: Shaoyu Cai, Kening Zhu, Yuki Ban, Takuji Narumi
- **Comment**: 8 pages, 6 figures, Accepted by IEEE Robotics and Automation Letters
- **Journal**: None
- **Summary**: Existing psychophysical studies have revealed that the cross-modal visual-tactile perception is common for humans performing daily activities. However, it is still challenging to build the algorithmic mapping from one modality space to another, namely the cross-modal visual-tactile data translation/generation, which could be potentially important for robotic operation. In this paper, we propose a deep-learning-based approach for cross-modal visual-tactile data generation by leveraging the framework of the generative adversarial networks (GANs). Our approach takes the visual image of a material surface as the visual data, and the accelerometer signal induced by the pen-sliding movement on the surface as the tactile data. We adopt the conditional-GAN (cGAN) structure together with the residue-fusion (RF) module, and train the model with the additional feature-matching (FM) and perceptual losses to achieve the cross-modal data generation. The experimental results show that the inclusion of the RF module, and the FM and the perceptual losses significantly improves cross-modal data generation performance in terms of the classification accuracy upon the generated data and the visual similarity between the ground-truth and the generated data.



### The Power of Proxy Data and Proxy Networks for Hyper-Parameter Optimization in Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.05471v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.05471v1)
- **Published**: 2021-07-12 14:37:08+00:00
- **Updated**: 2021-07-12 14:37:08+00:00
- **Authors**: Vishwesh Nath, Dong Yang, Ali Hatamizadeh, Anas A. Abidin, Andriy Myronenko, Holger Roth, Daguang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models for medical image segmentation are primarily data-driven. Models trained with more data lead to improved performance and generalizability. However, training is a computationally expensive process because multiple hyper-parameters need to be tested to find the optimal setting for best performance. In this work, we focus on accelerating the estimation of hyper-parameters by proposing two novel methodologies: proxy data and proxy networks. Both can be useful for estimating hyper-parameters more efficiently. We test the proposed techniques on CT and MR imaging modalities using well-known public datasets. In both cases using one dataset for building proxy data and another data source for external evaluation. For CT, the approach is tested on spleen segmentation with two datasets. The first dataset is from the medical segmentation decathlon (MSD), where the proxy data is constructed, the secondary dataset is utilized as an external validation dataset. Similarly, for MR, the approach is evaluated on prostate segmentation where the first dataset is from MSD and the second dataset is PROSTATEx. First, we show higher correlation to using full data for training when testing on the external validation set using smaller proxy data than a random selection of the proxy data. Second, we show that a high correlation exists for proxy networks when compared with the full network on validation Dice score. Third, we show that the proposed approach of utilizing a proxy network can speed up an AutoML framework for hyper-parameter search by 3.3x, and by 4.4x if proxy data and proxy network are utilized together.



### GiT: Graph Interactive Transformer for Vehicle Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2107.05475v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05475v3)
- **Published**: 2021-07-12 14:43:44+00:00
- **Updated**: 2023-01-11 03:25:22+00:00
- **Authors**: Fei Shen, Yi Xie, Jianqing Zhu, Xiaobin Zhu, Huanqiang Zeng
- **Comment**: Accepted in IEEE TIP 2023
- **Journal**: None
- **Summary**: Transformers are more and more popular in computer vision, which treat an image as a sequence of patches and learn robust global features from the sequence. However, pure transformers are not entirely suitable for vehicle re-identification because vehicle re-identification requires both robust global features and discriminative local features. For that, a graph interactive transformer (GiT) is proposed in this paper. In the macro view, a list of GiT blocks are stacked to build a vehicle re-identification model, in where graphs are to extract discriminative local features within patches and transformers are to extract robust global features among patches. In the micro view, graphs and transformers are in an interactive status, bringing effective cooperation between local and global features. Specifically, one current graph is embedded after the former level's graph and transformer, while the current transform is embedded after the current graph and the former level's transformer. In addition to the interaction between graphs and transforms, the graph is a newly-designed local correction graph, which learns discriminative local features within a patch by exploring nodes' relationships. Extensive experiments on three large-scale vehicle re-identification datasets demonstrate that our GiT method is superior to state-of-the-art vehicle re-identification approaches.



### Anatomy-Constrained Contrastive Learning for Synthetic Segmentation without Ground-truth
- **Arxiv ID**: http://arxiv.org/abs/2107.05482v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.05482v1)
- **Published**: 2021-07-12 14:54:04+00:00
- **Updated**: 2021-07-12 14:54:04+00:00
- **Authors**: Bo Zhou, Chi Liu, James S. Duncan
- **Comment**: Accepted at MICCAI 2021
- **Journal**: None
- **Summary**: A large amount of manual segmentation is typically required to train a robust segmentation network so that it can segment objects of interest in a new imaging modality. The manual efforts can be alleviated if the manual segmentation in one imaging modality (e.g., CT) can be utilized to train a segmentation network in another imaging modality (e.g., CBCT/MRI/PET). In this work, we developed an anatomy-constrained contrastive synthetic segmentation network (AccSeg-Net) to train a segmentation network for a target imaging modality without using its ground truth. Specifically, we proposed to use anatomy-constraint and patch contrastive learning to ensure the anatomy fidelity during the unsupervised adaptation, such that the segmentation network can be trained on the adapted image with correct anatomical structure/content. The training data for our AccSeg-Net consists of 1) imaging data paired with segmentation ground-truth in source modality, and 2) unpaired source and target modality imaging data. We demonstrated successful applications on CBCT, MRI, and PET imaging data, and showed superior segmentation performances as compared to previous methods.



### Synthesizing Multi-Tracer PET Images for Alzheimer's Disease Patients using a 3D Unified Anatomy-aware Cyclic Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2107.05491v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.05491v1)
- **Published**: 2021-07-12 15:10:29+00:00
- **Updated**: 2021-07-12 15:10:29+00:00
- **Authors**: Bo Zhou, Rui Wang, Ming-Kai Chen, Adam P. Mecca, Ryan S. O'Dell, Christopher H. Van Dyck, Richard E. Carson, James S. Duncan, Chi Liu
- **Comment**: Accepted at MICCAI 2021
- **Journal**: None
- **Summary**: Positron Emission Tomography (PET) is an important tool for studying Alzheimer's disease (AD). PET scans can be used as diagnostics tools, and to provide molecular characterization of patients with cognitive disorders. However, multiple tracers are needed to measure glucose metabolism (18F-FDG), synaptic vesicle protein (11C-UCB-J), and $\beta$-amyloid (11C-PiB). Administering multiple tracers to patient will lead to high radiation dose and cost. In addition, access to PET scans using new or less-available tracers with sophisticated production methods and short half-life isotopes may be very limited. Thus, it is desirable to develop an efficient multi-tracer PET synthesis model that can generate multi-tracer PET from single-tracer PET. Previous works on medical image synthesis focus on one-to-one fixed domain translations, and cannot simultaneously learn the feature from multi-tracer domains. Given 3 or more tracers, relying on previous methods will also create a heavy burden on the number of models to be trained. To tackle these issues, we propose a 3D unified anatomy-aware cyclic adversarial network (UCAN) for translating multi-tracer PET volumes with one unified generative model, where MR with anatomical information is incorporated. Evaluations on a multi-tracer PET dataset demonstrate the feasibility that our UCAN can generate high-quality multi-tracer PET volumes, with NMSE less than 15% for all PET tracers.



### Multi-view Image-based Hand Geometry Refinement using Differentiable Monte Carlo Ray Tracing
- **Arxiv ID**: http://arxiv.org/abs/2107.05509v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05509v3)
- **Published**: 2021-07-12 15:35:20+00:00
- **Updated**: 2022-01-07 14:28:21+00:00
- **Authors**: Giorgos Karvounas, Nikolaos Kyriazis, Iason Oikonomidis, Aggeliki Tsoli, Antonis A. Argyros
- **Comment**: British Machine Vision Conference (BMVC) 2021
- **Journal**: None
- **Summary**: The amount and quality of datasets and tools available in the research field of hand pose and shape estimation act as evidence to the significant progress that has been made.However, even the datasets of the highest quality, reported to date, have shortcomings in annotation. We propose a refinement approach, based on differentiable ray tracing,and demonstrate how a high-quality publicly available, multi-camera dataset of hands(InterHand2.6M) can become an even better dataset, with respect to annotation quality. Differentiable ray tracing has not been employed so far to relevant problems and is hereby shown to be superior to the approximative alternatives that have been employed in the past. To tackle the lack of reliable ground truth, as far as quantitative evaluation is concerned, we resort to realistic synthetic data, to show that the improvement we induce is indeed significant. The same becomes evident in real data through visual evaluation.



### Context-aware virtual adversarial training for anatomically-plausible segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.05532v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05532v2)
- **Published**: 2021-07-12 16:01:27+00:00
- **Updated**: 2021-07-13 13:36:51+00:00
- **Authors**: Ping Wang, Jizong Peng, Marco Pedersoli, Yuanfeng Zhou, Caiming Zhang, Christian Desrosiers
- **Comment**: This paper is accepted at MICCAI2021
- **Journal**: None
- **Summary**: Despite their outstanding accuracy, semi-supervised segmentation methods based on deep neural networks can still yield predictions that are considered anatomically impossible by clinicians, for instance, containing holes or disconnected regions. To solve this problem, we present a Context-aware Virtual Adversarial Training (CaVAT) method for generating anatomically plausible segmentation. Unlike approaches focusing solely on accuracy, our method also considers complex topological constraints like connectivity which cannot be easily modeled in a differentiable loss function. We use adversarial training to generate examples violating the constraints, so the network can learn to avoid making such incorrect predictions on new examples, and employ the Reinforce algorithm to handle non-differentiable segmentation constraints. The proposed method offers a generic and efficient way to add any constraint on top of any segmentation network. Experiments on two clinically-relevant datasets show our method to produce segmentations that are both accurate and anatomically-plausible in terms of region connectivity.



### Deformation-Compensated Learning for Image Reconstruction without Ground Truth
- **Arxiv ID**: http://arxiv.org/abs/2107.05533v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.05533v2)
- **Published**: 2021-07-12 16:01:45+00:00
- **Updated**: 2021-12-17 22:04:10+00:00
- **Authors**: Weijie Gan, Yu Sun, Cihat Eldeniz, Jiaming Liu, Hongyu An, Ulugbek S. Kamilov
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks for medical image reconstruction are traditionally trained using high-quality ground-truth images as training targets. Recent work on Noise2Noise (N2N) has shown the potential of using multiple noisy measurements of the same object as an alternative to having a ground-truth. However, existing N2N-based methods are not suitable for learning from the measurements of an object undergoing nonrigid deformation. This paper addresses this issue by proposing the deformation-compensated learning (DeCoLearn) method for training deep reconstruction networks by compensating for object deformations. A key component of DeCoLearn is a deep registration module, which is jointly trained with the deep reconstruction network without any ground-truth supervision. We validate DeCoLearn on both simulated and experimentally collected magnetic resonance imaging (MRI) data and show that it significantly improves imaging quality.



### 1st Place Solution for ICDAR 2021 Competition on Mathematical Formula Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.05534v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.05534v1)
- **Published**: 2021-07-12 16:03:16+00:00
- **Updated**: 2021-07-12 16:03:16+00:00
- **Authors**: Yuxiang Zhong, Xianbiao Qi, Shanjun Li, Dengyi Gu, Yihao Chen, Peiyang Ning, Rong Xiao
- **Comment**: 1st Place Solution for ICDAR 2021 Competition on Mathematical Formula
  Detection. http://transcriptorium.eu/~htrcontest/MathsICDAR2021/
- **Journal**: None
- **Summary**: In this technical report, we present our 1st place solution for the ICDAR 2021 competition on mathematical formula detection (MFD). The MFD task has three key challenges including a large scale span, large variation of the ratio between height and width, and rich character set and mathematical expressions. Considering these challenges, we used Generalized Focal Loss (GFL), an anchor-free method, instead of the anchor-based method, and prove the Adaptive Training Sampling Strategy (ATSS) and proper Feature Pyramid Network (FPN) can well solve the important issue of scale variation. Meanwhile, we also found some tricks, e.g., Deformable Convolution Network (DCN), SyncBN, and Weighted Box Fusion (WBF), were effective in MFD task. Our proposed method ranked 1st in the final 15 teams.



### Few-shot Learning with Global Relatedness Decoupled-Distillation
- **Arxiv ID**: http://arxiv.org/abs/2107.05583v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05583v3)
- **Published**: 2021-07-12 17:01:11+00:00
- **Updated**: 2022-12-14 03:11:01+00:00
- **Authors**: Yuan Zhou, Yanrong Guo, Shijie Hao, Richang Hong, Zhengjun Zha, Meng Wang
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Despite the success that metric learning based approaches have achieved in few-shot learning, recent works reveal the ineffectiveness of their episodic training mode. In this paper, we point out two potential reasons for this problem: 1) the random episodic labels can only provide limited supervision information, while the relatedness information between the query and support samples is not fully exploited; 2) the meta-learner is usually constrained by the limited contextual information of the local episode. To overcome these problems, we propose a new Global Relatedness Decoupled-Distillation (GRDD) method using the global category knowledge and the Relatedness Decoupled-Distillation (RDD) strategy. Our GRDD learns new visual concepts quickly by imitating the habit of humans, i.e. learning from the deep knowledge distilled from the teacher. More specifically, we first train a global learner on the entire base subset using category labels as supervision to leverage the global context information of the categories. Then, the well-trained global learner is used to simulate the query-support relatedness in global dependencies. Finally, the distilled global query-support relatedness is explicitly used to train the meta-learner using the RDD strategy, with the goal of making the meta-learner more discriminative. The RDD strategy aims to decouple the dense query-support relatedness into the groups of sparse decoupled relatedness. Moreover, only the relatedness of a single support sample with other query samples is considered in each group. By distilling the sparse decoupled relatedness group by group, sharper relatedness can be effectively distilled to the meta-learner, thereby facilitating the learning of a discriminative meta-learner. We conduct extensive experiments on the miniImagenet and CIFAR-FS datasets, which show the state-of-the-art performance of our GRDD method.



### DDCNet-Multires: Effective Receptive Field Guided Multiresolution CNN for Dense Prediction
- **Arxiv ID**: http://arxiv.org/abs/2107.05634v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.05634v1)
- **Published**: 2021-07-12 17:28:08+00:00
- **Updated**: 2021-07-12 17:28:08+00:00
- **Authors**: Ali Salehi, Madhusudhanan Balasubramanian
- **Comment**: 27 pages, 10 figures, 2 tables. arXiv admin note: text overlap with
  arXiv:2107.04715
- **Journal**: None
- **Summary**: Dense optical flow estimation is challenging when there are large displacements in a scene with heterogeneous motion dynamics, occlusion, and scene homogeneity. Traditional approaches to handle these challenges include hierarchical and multiresolution processing methods. Learning-based optical flow methods typically use a multiresolution approach with image warping when a broad range of flow velocities and heterogeneous motion is present. Accuracy of such coarse-to-fine methods is affected by the ghosting artifacts when images are warped across multiple resolutions and by the vanishing problem in smaller scene extents with higher motion contrast. Previously, we devised strategies for building compact dense prediction networks guided by the effective receptive field (ERF) characteristics of the network (DDCNet). The DDCNet design was intentionally simple and compact allowing it to be used as a building block for designing more complex yet compact networks. In this work, we extend the DDCNet strategies to handle heterogeneous motion dynamics by cascading DDCNet based sub-nets with decreasing extents of their ERF. Our DDCNet with multiresolution capability (DDCNet-Multires) is compact without any specialized network layers. We evaluate the performance of the DDCNet-Multires network using standard optical flow benchmark datasets. Our experiments demonstrate that DDCNet-Multires improves over the DDCNet-B0 and -B1 and provides optical flow estimates with accuracy comparable to similar lightweight learning-based methods.



### Interpretable Mammographic Image Classification using Case-Based Reasoning and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.05605v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.6; I.4.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2107.05605v2)
- **Published**: 2021-07-12 17:42:09+00:00
- **Updated**: 2021-10-05 00:43:13+00:00
- **Authors**: Alina Jade Barnett, Fides Regina Schwartz, Chaofan Tao, Chaofan Chen, Yinhao Ren, Joseph Y. Lo, Cynthia Rudin
- **Comment**: 10 pages, 6 figures, accepted for oral presentation at the IJCAI-21
  Workshop on Deep Learning, Case-Based Reasoning, and AutoML: Present and
  Future Synergies. arXiv admin note: substantial text overlap with
  arXiv:2103.12308
- **Journal**: None
- **Summary**: When we deploy machine learning models in high-stakes medical settings, we must ensure these models make accurate predictions that are consistent with known medical science. Inherently interpretable networks address this need by explaining the rationale behind each decision while maintaining equal or higher accuracy compared to black-box models. In this work, we present a novel interpretable neural network algorithm that uses case-based reasoning for mammography. Designed to aid a radiologist in their decisions, our network presents both a prediction of malignancy and an explanation of that prediction using known medical features. In order to yield helpful explanations, the network is designed to mimic the reasoning processes of a radiologist: our network first detects the clinically relevant semantic features of each image by comparing each new image with a learned set of prototypical image parts from the training images, then uses those clinical features to predict malignancy. Compared to other methods, our model detects clinical features (mass margins) with equal or higher accuracy, provides a more detailed explanation of its prediction, and is better able to differentiate the classification-relevant parts of the image.



### A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution
- **Arxiv ID**: http://arxiv.org/abs/2107.05612v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.05612v3)
- **Published**: 2021-07-12 17:47:19+00:00
- **Updated**: 2021-11-28 19:33:23+00:00
- **Authors**: Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, Yoav Artzi
- **Comment**: Presented at CoRL 2021
- **Journal**: None
- **Summary**: Natural language provides an accessible and expressive interface to specify long-term tasks for robotic agents. However, non-experts are likely to specify such tasks with high-level instructions, which abstract over specific robot actions through several layers of abstraction. We propose that key to bridging this gap between language and robot actions over long execution horizons are persistent representations. We propose a persistent spatial semantic representation method, and show how it enables building an agent that performs hierarchical reasoning to effectively execute long-term tasks. We evaluate our approach on the ALFRED benchmark and achieve state-of-the-art results, despite completely avoiding the commonly used step-by-step instructions.



### Let's Play for Action: Recognizing Activities of Daily Living by Learning from Life Simulation Video Games
- **Arxiv ID**: http://arxiv.org/abs/2107.05617v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.05617v1)
- **Published**: 2021-07-12 17:53:38+00:00
- **Updated**: 2021-07-12 17:53:38+00:00
- **Authors**: Alina Roitberg, David Schneider, Aulia Djamal, Constantin Seibold, Simon Reiß, Rainer Stiefelhagen
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing Activities of Daily Living (ADL) is a vital process for intelligent assistive robots, but collecting large annotated datasets requires time-consuming temporal labeling and raises privacy concerns, e.g., if the data is collected in a real household. In this work, we explore the concept of constructing training examples for ADL recognition by playing life simulation video games and introduce the SIMS4ACTION dataset created with the popular commercial game THE SIMS 4. We build Sims4Action by specifically executing actions-of-interest in a "top-down" manner, while the gaming circumstances allow us to freely switch between environments, camera angles and subject appearances. While ADL recognition on gaming data is interesting from the theoretical perspective, the key challenge arises from transferring it to the real-world applications, such as smart-homes or assistive robotics. To meet this requirement, Sims4Action is accompanied with a GamingToReal benchmark, where the models are evaluated on real videos derived from an existing ADL dataset. We integrate two modern algorithms for video-based activity recognition in our framework, revealing the value of life simulation video games as an inexpensive and far less intrusive source of training data. However, our results also indicate that tasks involving a mixture of gaming and real data are challenging, opening a new research direction. We will make our dataset publicly available at https://github.com/aroitberg/sims4action.



### Structured Latent Embeddings for Recognizing Unseen Classes in Unseen Domains
- **Arxiv ID**: http://arxiv.org/abs/2107.05622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05622v1)
- **Published**: 2021-07-12 17:57:46+00:00
- **Updated**: 2021-07-12 17:57:46+00:00
- **Authors**: Shivam Chandhok, Sanath Narayan, Hisham Cholakkal, Rao Muhammad Anwer, Vineeth N Balasubramanian, Fahad Shahbaz Khan, Ling Shao
- **Comment**: None
- **Journal**: None
- **Summary**: The need to address the scarcity of task-specific annotated data has resulted in concerted efforts in recent years for specific settings such as zero-shot learning (ZSL) and domain generalization (DG), to separately address the issues of semantic shift and domain shift, respectively. However, real-world applications often do not have constrained settings and necessitate handling unseen classes in unseen domains -- a setting called Zero-shot Domain Generalization, which presents the issues of domain and semantic shifts simultaneously. In this work, we propose a novel approach that learns domain-agnostic structured latent embeddings by projecting images from different domains as well as class-specific semantic text-based representations to a common latent space. In particular, our method jointly strives for the following objectives: (i) aligning the multimodal cues from visual and text-based semantic concepts; (ii) partitioning the common latent space according to the domain-agnostic class-level semantic concepts; and (iii) learning a domain invariance w.r.t the visual-semantic joint distribution for generalizing to unseen classes in unseen domains. Our experiments on the challenging DomainNet and DomainNet-LS benchmarks show the superiority of our approach over existing methods, with significant gains on difficult domains like quickdraw and sketch.



### End-to-end Multi-modal Video Temporal Grounding
- **Arxiv ID**: http://arxiv.org/abs/2107.05624v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05624v2)
- **Published**: 2021-07-12 17:58:10+00:00
- **Updated**: 2021-10-28 18:14:01+00:00
- **Authors**: Yi-Wen Chen, Yi-Hsuan Tsai, Ming-Hsuan Yang
- **Comment**: Accepted in NeurIPS 2021. Project page at
  https://github.com/wenz116/DRFT
- **Journal**: None
- **Summary**: We address the problem of text-guided video temporal grounding, which aims to identify the time interval of a certain event based on a natural language description. Different from most existing methods that only consider RGB images as visual features, we propose a multi-modal framework to extract complementary information from videos. Specifically, we adopt RGB images for appearance, optical flow for motion, and depth maps for image structure. While RGB images provide abundant visual cues of certain events, the performance may be affected by background clutters. Therefore, we use optical flow to focus on large motion and depth maps to infer the scene configuration when the action is related to objects recognizable with their shapes. To integrate the three modalities more effectively and enable inter-modal learning, we design a dynamic fusion scheme with transformers to model the interactions between modalities. Furthermore, we apply intra-modal self-supervised learning to enhance feature representations across videos for each modality, which also facilitates multi-modal learning. We conduct extensive experiments on the Charades-STA and ActivityNet Captions datasets, and show that the proposed method performs favorably against state-of-the-art approaches.



### Hierarchical Neural Dynamic Policies
- **Arxiv ID**: http://arxiv.org/abs/2107.05627v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2107.05627v1)
- **Published**: 2021-07-12 17:59:58+00:00
- **Updated**: 2021-07-12 17:59:58+00:00
- **Authors**: Shikhar Bahl, Abhinav Gupta, Deepak Pathak
- **Comment**: Accepted at RSS 2021. Videos and code at
  https://shikharbahl.github.io/hierarchical-ndps/
- **Journal**: None
- **Summary**: We tackle the problem of generalization to unseen configurations for dynamic tasks in the real world while learning from high-dimensional image input. The family of nonlinear dynamical system-based methods have successfully demonstrated dynamic robot behaviors but have difficulty in generalizing to unseen configurations as well as learning from image inputs. Recent works approach this issue by using deep network policies and reparameterize actions to embed the structure of dynamical systems but still struggle in domains with diverse configurations of image goals, and hence, find it difficult to generalize. In this paper, we address this dichotomy by leveraging embedding the structure of dynamical systems in a hierarchical deep policy learning framework, called Hierarchical Neural Dynamical Policies (H-NDPs). Instead of fitting deep dynamical systems to diverse data directly, H-NDPs form a curriculum by learning local dynamical system-based policies on small regions in state-space and then distill them into a global dynamical system-based policy that operates only from high-dimensional images. H-NDPs additionally provide smooth trajectories, a strong safety benefit in the real world. We perform extensive experiments on dynamic tasks both in the real world (digit writing, scooping, and pouring) and simulation (catching, throwing, picking). We show that H-NDPs are easily integrated with both imitation as well as reinforcement learning setups and achieve state-of-the-art results. Video results are at https://shikharbahl.github.io/hierarchical-ndps/



### Locally Enhanced Self-Attention: Combining Self-Attention and Convolution as Local and Context Terms
- **Arxiv ID**: http://arxiv.org/abs/2107.05637v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05637v3)
- **Published**: 2021-07-12 18:00:00+00:00
- **Updated**: 2021-11-29 00:44:09+00:00
- **Authors**: Chenglin Yang, Siyuan Qiao, Adam Kortylewski, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Self-Attention has become prevalent in computer vision models. Inspired by fully connected Conditional Random Fields (CRFs), we decompose self-attention into local and context terms. They correspond to the unary and binary terms in CRF and are implemented by attention mechanisms with projection matrices. We observe that the unary terms only make small contributions to the outputs, and meanwhile standard CNNs that rely solely on the unary terms achieve great performances on a variety of tasks. Therefore, we propose Locally Enhanced Self-Attention (LESA), which enhances the unary term by incorporating it with convolutions, and utilizes a fusion module to dynamically couple the unary and binary operations. In our experiments, we replace the self-attention modules with LESA. The results on ImageNet and COCO show the superiority of LESA over convolution and self-attention baselines for the tasks of image recognition, object detection, and instance segmentation. The code is made publicly available.



### Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions
- **Arxiv ID**: http://arxiv.org/abs/2107.05680v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2107.05680v2)
- **Published**: 2021-07-12 18:33:49+00:00
- **Updated**: 2022-03-21 17:38:44+00:00
- **Authors**: Arda Sahiner, Tolga Ergen, Batu Ozturkler, Burak Bartan, John Pauly, Morteza Mardani, Mert Pilanci
- **Comment**: Published as paper in ICLR 2022. First two authors contributed
  equally to this work; 34 pages, 11 figures
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are commonly used for modeling complex distributions of data. Both the generators and discriminators of GANs are often modeled by neural networks, posing a non-transparent optimization problem which is non-convex and non-concave over the generator and discriminator, respectively. Such networks are often heuristically optimized with gradient descent-ascent (GDA), but it is unclear whether the optimization problem contains any saddle points, or whether heuristic methods can find them in practice. In this work, we analyze the training of Wasserstein GANs with two-layer neural network discriminators through the lens of convex duality, and for a variety of generators expose the conditions under which Wasserstein GANs can be solved exactly with convex optimization approaches, or can be represented as convex-concave games. Using this convex duality interpretation, we further demonstrate the impact of different activation functions of the discriminator. Our observations are verified with numerical results demonstrating the power of the convex interpretation, with applications in progressive training of convex architectures corresponding to linear generators and quadratic-activation discriminators for CelebA image generation. The code for our experiments is available at https://github.com/ardasahiner/ProCoGAN.



### LANA: Latency Aware Network Acceleration
- **Arxiv ID**: http://arxiv.org/abs/2107.10624v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.10624v2)
- **Published**: 2021-07-12 18:46:34+00:00
- **Updated**: 2021-11-18 18:55:13+00:00
- **Authors**: Pavlo Molchanov, Jimmy Hall, Hongxu Yin, Jan Kautz, Nicolo Fusi, Arash Vahdat
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce latency-aware network acceleration (LANA) - an approach that builds on neural architecture search techniques and teacher-student distillation to accelerate neural networks. LANA consists of two phases: in the first phase, it trains many alternative operations for every layer of the teacher network using layer-wise feature map distillation. In the second phase, it solves the combinatorial selection of efficient operations using a novel constrained integer linear optimization (ILP) approach. ILP brings unique properties as it (i) performs NAS within a few seconds to minutes, (ii) easily satisfies budget constraints, (iii) works on the layer-granularity, (iv) supports a huge search space $O(10^{100})$, surpassing prior search approaches in efficacy and efficiency. In extensive experiments, we show that LANA yields efficient and accurate models constrained by a target latency budget, while being significantly faster than other techniques. We analyze three popular network architectures: EfficientNetV1, EfficientNetV2 and ResNeST, and achieve accuracy improvement for all models (up to $3.0\%$) when compressing larger models to the latency level of smaller models. LANA achieves significant speed-ups (up to $5\times$) with minor to no accuracy drop on GPU and CPU. The code will be shared soon.



### Bayesian Atlas Building with Hierarchical Priors for Subject-specific Regularization
- **Arxiv ID**: http://arxiv.org/abs/2107.05698v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, I.2.10; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2107.05698v1)
- **Published**: 2021-07-12 19:32:09+00:00
- **Updated**: 2021-07-12 19:32:09+00:00
- **Authors**: Jian Wang, Miaomiao Zhang
- **Comment**: 11 pages, 2 figures
- **Journal**: International Conference on Medical Image Computing and Computer
  Assisted Intervention (MICCAI 2021)
- **Summary**: This paper presents a novel hierarchical Bayesian model for unbiased atlas building with subject-specific regularizations of image registration. We develop an atlas construction process that automatically selects parameters to control the smoothness of diffeomorphic transformation according to individual image data. To achieve this, we introduce a hierarchical prior distribution on regularization parameters that allows multiple penalties on images with various degrees of geometric transformations. We then treat the regularization parameters as latent variables and integrate them out from the model by using the Monte Carlo Expectation Maximization (MCEM) algorithm. Another advantage of our algorithm is that it eliminates the need for manual parameter tuning, which can be tedious and infeasible. We demonstrate the effectiveness of our model on 3D brain MR images. Experimental results show that our model provides a sharper atlas compared to the current atlas building algorithms with single-penalty regularizations. Our code is publicly available at https://github.com/jw4hv/HierarchicalBayesianAtlasBuild.



### SoftHebb: Bayesian Inference in Unsupervised Hebbian Soft Winner-Take-All Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.05747v4
- **DOI**: 10.1088/2634-4386/aca710
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2107.05747v4)
- **Published**: 2021-07-12 21:34:45+00:00
- **Updated**: 2022-11-12 21:37:57+00:00
- **Authors**: Timoleon Moraitis, Dmitry Toichkin, Adrien Journé, Yansong Chua, Qinghai Guo
- **Comment**: None
- **Journal**: Neuromorphic Computing and Engineering, 2(4), 044017 (2022)
- **Summary**: Hebbian plasticity in winner-take-all (WTA) networks is highly attractive for neuromorphic on-chip learning, owing to its efficient, local, unsupervised, and on-line nature. Moreover, its biological plausibility may help overcome important limitations of artificial algorithms, such as their susceptibility to adversarial attacks, and their high demands for training-example quantity and repetition. However, Hebbian WTA learning has found little use in machine learning (ML), likely because it has been missing an optimization theory compatible with deep learning (DL). Here we show rigorously that WTA networks constructed by standard DL elements, combined with a Hebbian-like plasticity that we derive, maintain a Bayesian generative model of the data. Importantly, without any supervision, our algorithm, SoftHebb, minimizes cross-entropy, i.e. a common loss function in supervised DL. We show this theoretically and in practice. The key is a "soft" WTA where there is no absolute "hard" winner neuron. Strikingly, in shallow-network comparisons with backpropagation (BP), SoftHebb shows advantages beyond its Hebbian efficiency. Namely, it converges in fewer iterations, and is significantly more robust to noise and adversarial attacks. Notably, attacks that maximally confuse SoftHebb are also confusing to the human eye, potentially linking human perceptual robustness, with Hebbian WTA circuits of cortex. Finally, SoftHebb can generate synthetic objects as interpolations of real object classes. All in all, Hebbian efficiency, theoretical underpinning, cross-entropy-minimization, and surprising empirical advantages, suggest that SoftHebb may inspire highly neuromorphic and radically different, but practical and advantageous learning algorithms and hardware accelerators.



### EvoBA: An Evolution Strategy as a Strong Baseline forBlack-Box Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2107.05754v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.05754v1)
- **Published**: 2021-07-12 21:55:01+00:00
- **Updated**: 2021-07-12 21:55:01+00:00
- **Authors**: Andrei Ilie, Marius Popescu, Alin Stefanescu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work has shown how easily white-box adversarial attacks can be applied to state-of-the-art image classifiers. However, real-life scenarios resemble more the black-box adversarial conditions, lacking transparency and usually imposing natural, hard constraints on the query budget.   We propose $\textbf{EvoBA}$, a black-box adversarial attack based on a surprisingly simple evolutionary search strategy. $\textbf{EvoBA}$ is query-efficient, minimizes $L_0$ adversarial perturbations, and does not require any form of training.   $\textbf{EvoBA}$ shows efficiency and efficacy through results that are in line with much more complex state-of-the-art black-box attacks such as $\textbf{AutoZOOM}$. It is more query-efficient than $\textbf{SimBA}$, a simple and powerful baseline black-box attack, and has a similar level of complexity. Therefore, we propose it both as a new strong baseline for black-box adversarial attacks and as a fast and general tool for gaining empirical insight into how robust image classifiers are with respect to $L_0$ adversarial perturbations.   There exist fast and reliable $L_2$ black-box attacks, such as $\textbf{SimBA}$, and $L_{\infty}$ black-box attacks, such as $\textbf{DeepSearch}$. We propose $\textbf{EvoBA}$ as a query-efficient $L_0$ black-box adversarial attack which, together with the aforementioned methods, can serve as a generic tool to assess the empirical robustness of image classifiers. The main advantages of such methods are that they run fast, are query-efficient, and can easily be integrated in image classifiers development pipelines.   While our attack minimises the $L_0$ adversarial perturbation, we also report $L_2$, and notice that we compare favorably to the state-of-the-art $L_2$ black-box attack, $\textbf{AutoZOOM}$, and of the $L_2$ strong baseline, $\textbf{SimBA}$.



### Combiner: Full Attention Transformer with Sparse Computation Cost
- **Arxiv ID**: http://arxiv.org/abs/2107.05768v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.05768v2)
- **Published**: 2021-07-12 22:43:11+00:00
- **Updated**: 2021-10-28 05:17:27+00:00
- **Authors**: Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, Bo Dai
- **Comment**: NeurIPS 2021 spotlight
- **Journal**: None
- **Summary**: Transformers provide a class of expressive architectures that are extremely effective for sequence modeling. However, the key limitation of transformers is their quadratic memory and time complexity $\mathcal{O}(L^2)$ with respect to the sequence length in attention layers, which restricts application in extremely long sequences. Most existing approaches leverage sparsity or low-rank assumptions in the attention matrix to reduce cost, but sacrifice expressiveness. Instead, we propose Combiner, which provides full attention capability in each attention head while maintaining low computation and memory complexity. The key idea is to treat the self-attention mechanism as a conditional expectation over embeddings at each location, and approximate the conditional distribution with a structured factorization. Each location can attend to all other locations, either via direct attention, or through indirect attention to abstractions, which are again conditional expectations of embeddings from corresponding local regions. We show that most sparse attention patterns used in existing sparse transformers are able to inspire the design of such factorization for full attention, resulting in the same sub-quadratic cost ($\mathcal{O}(L\log(L))$ or $\mathcal{O}(L\sqrt{L})$). Combiner is a drop-in replacement for attention layers in existing transformers and can be easily implemented in common frameworks. An experimental evaluation on both autoregressive and bidirectional sequence tasks demonstrates the effectiveness of this approach, yielding state-of-the-art results on several image and text modeling tasks.



### Fast and Explicit Neural View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2107.05775v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.05775v2)
- **Published**: 2021-07-12 23:24:53+00:00
- **Updated**: 2021-12-08 22:47:48+00:00
- **Authors**: Pengsheng Guo, Miguel Angel Bautista, Alex Colburn, Liang Yang, Daniel Ulbricht, Joshua M. Susskind, Qi Shan
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of novel view synthesis from sparse source observations of a scene comprised of 3D objects. We propose a simple yet effective approach that is neither continuous nor implicit, challenging recent trends on view synthesis. Our approach explicitly encodes observations into a volumetric representation that enables amortized rendering. We demonstrate that although continuous radiance field representations have gained a lot of attention due to their expressive power, our simple approach obtains comparable or even better novel view reconstruction quality comparing with state-of-the-art baselines while increasing rendering speed by over 400x. Our model is trained in a category-agnostic manner and does not require scene-specific optimization. Therefore, it is able to generalize novel view synthesis to object categories not seen during training. In addition, we show that with our simple formulation, we can use view synthesis as a self-supervision signal for efficient learning of 3D geometry without explicit 3D supervision.



### Detect and Defense Against Adversarial Examples in Deep Learning using Natural Scene Statistics and Adaptive Denoising
- **Arxiv ID**: http://arxiv.org/abs/2107.05780v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.05780v1)
- **Published**: 2021-07-12 23:45:44+00:00
- **Updated**: 2021-07-12 23:45:44+00:00
- **Authors**: Anouar Kherchouche, Sid Ahmed Fezza, Wassim Hamidouche
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the enormous performance of deepneural networks (DNNs), recent studies have shown theirvulnerability to adversarial examples (AEs), i.e., care-fully perturbed inputs designed to fool the targetedDNN. Currently, the literature is rich with many ef-fective attacks to craft such AEs. Meanwhile, many de-fenses strategies have been developed to mitigate thisvulnerability. However, these latter showed their effec-tiveness against specific attacks and does not general-ize well to different attacks. In this paper, we proposea framework for defending DNN classifier against ad-versarial samples. The proposed method is based on atwo-stage framework involving a separate detector anda denoising block. The detector aims to detect AEs bycharacterizing them through the use of natural scenestatistic (NSS), where we demonstrate that these statis-tical features are altered by the presence of adversarialperturbations. The denoiser is based on block matching3D (BM3D) filter fed by an optimum threshold valueestimated by a convolutional neural network (CNN) toproject back the samples detected as AEs into theirdata manifold. We conducted a complete evaluation onthree standard datasets namely MNIST, CIFAR-10 andTiny-ImageNet. The experimental results show that theproposed defense method outperforms the state-of-the-art defense techniques by improving the robustnessagainst a set of attacks under black-box, gray-box and white-box settings. The source code is available at: https://github.com/kherchouche-anouar/2DAE



