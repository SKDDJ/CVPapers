# Arxiv Papers in cs.CV on 2021-07-19
### Learning point embedding for 3D data processing
- **Arxiv ID**: http://arxiv.org/abs/2107.08565v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08565v2)
- **Published**: 2021-07-19 00:25:28+00:00
- **Updated**: 2021-08-10 15:51:47+00:00
- **Authors**: Zhenpeng Chen, Yuan li
- **Comment**: None
- **Journal**: None
- **Summary**: Among 2D convolutional networks on point clouds, point-based approaches consume point clouds of fixed size directly. By analysis of PointNet, a pioneer in introducing deep learning into point sets, we reveal that current point-based methods are essentially spatial relationship processing networks. In this paper, we take a different approach. Our architecture, named PE-Net, learns the representation of point clouds in high-dimensional space, and encodes the unordered input points to feature vectors, which standard 2D CNNs can be applied to. The recommended network can adapt to changes in the number of input points which is the limit of current methods. Experiments show that in the tasks of classification and part segmentation, PE-Net achieves the state-of-the-art performance in multiple challenging datasets, such as ModelNet and ShapeNetPart.



### Action Forecasting with Feature-wise Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/2107.08579v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08579v1)
- **Published**: 2021-07-19 01:55:30+00:00
- **Updated**: 2021-07-19 01:55:30+00:00
- **Authors**: Yan Bin Ng, Basura Fernando
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new architecture for human action forecasting from videos. A temporal recurrent encoder captures temporal information of input videos while a self-attention model is used to attend on relevant feature dimensions of the input space. To handle temporal variations in observed video data, a feature masking techniques is employed. We classify observed actions accurately using an auxiliary classifier which helps to understand what has happened so far. Then the decoder generates actions for the future based on the output of the recurrent encoder and the self-attention model. Experimentally, we validate each component of our architecture where we see that the impact of self-attention to identify relevant feature dimensions, temporal masking, and observed auxiliary classifier. We evaluate our method on two standard action forecasting benchmarks and obtain state-of-the-art results.



### UNIK: A Unified Framework for Real-world Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.08580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08580v1)
- **Published**: 2021-07-19 02:00:28+00:00
- **Updated**: 2021-07-19 02:00:28+00:00
- **Authors**: Di Yang, Yaohui Wang, Antitza Dantcheva, Lorenzo Garattoni, Gianpiero Francesca, Francois Bremond
- **Comment**: Code is available at: https://github.com/YangDi666/UNIK
- **Journal**: None
- **Summary**: Action recognition based on skeleton data has recently witnessed increasing attention and progress. State-of-the-art approaches adopting Graph Convolutional networks (GCNs) can effectively extract features on human skeletons relying on the pre-defined human topology. Despite associated progress, GCN-based methods have difficulties to generalize across domains, especially with different human topological structures. In this context, we introduce UNIK, a novel skeleton-based action recognition method that is not only effective to learn spatio-temporal features on human skeleton sequences but also able to generalize across datasets. This is achieved by learning an optimal dependency matrix from the uniform distribution based on a multi-head attention mechanism. Subsequently, to study the cross-domain generalizability of skeleton-based action recognition in real-world videos, we re-evaluate state-of-the-art approaches as well as the proposed UNIK in light of a novel Posetics dataset. This dataset is created from Kinetics-400 videos by estimating, refining and filtering poses. We provide an analysis on how much performance improves on smaller benchmark datasets after pre-training on Posetics for the action classification task. Experimental results show that the proposed UNIK, with pre-training on Posetics, generalizes well and outperforms state-of-the-art when transferred onto four target action classification datasets: Toyota Smarthome, Penn Action, NTU-RGB+D 60 and NTU-RGB+D 120.



### A Systematical Solution for Face De-identification
- **Arxiv ID**: http://arxiv.org/abs/2107.08581v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.08581v1)
- **Published**: 2021-07-19 02:02:51+00:00
- **Updated**: 2021-07-19 02:02:51+00:00
- **Authors**: Songlin Yang, Wei Wang, Yuehua Cheng, Jing Dong
- **Comment**: accepted by the 15th Chinese Conference on Biometrics Recognition
  (CCBR2021)
- **Journal**: None
- **Summary**: With the identity information in face data more closely related to personal credit and property security, people pay increasing attention to the protection of face data privacy. In different tasks, people have various requirements for face de-identification (De-ID), so we propose a systematical solution compatible for these De-ID operations. Firstly, an attribute disentanglement and generative network is constructed to encode two parts of the face, which are the identity (facial features like mouth, nose and eyes) and expression (including expression, pose and illumination). Through face swapping, we can remove the original ID completely. Secondly, we add an adversarial vector mapping network to perturb the latent code of the face image, different from previous traditional adversarial methods. Through this, we can construct unrestricted adversarial image to decrease ID similarity recognized by model. Our method can flexibly de-identify the face data in various ways and the processed images have high image quality.



### Non-binary deep transfer learning for image classification
- **Arxiv ID**: http://arxiv.org/abs/2107.08585v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/2107.08585v2)
- **Published**: 2021-07-19 02:34:38+00:00
- **Updated**: 2021-08-30 02:59:40+00:00
- **Authors**: Jo Plested, Xuyang Shen, Tom Gedeon
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: The current standard for a variety of computer vision tasks using smaller numbers of labelled training examples is to fine-tune from weights pre-trained on a large image classification dataset such as ImageNet. The application of transfer learning and transfer learning methods tends to be rigidly binary. A model is either pre-trained or not pre-trained. Pre-training a model either increases performance or decreases it, the latter being defined as negative transfer. Application of L2-SP regularisation that decays the weights towards their pre-trained values is either applied or all weights are decayed towards 0. This paper re-examines these assumptions. Our recommendations are based on extensive empirical evaluation that demonstrate the application of a non-binary approach to achieve optimal results. (1) Achieving best performance on each individual dataset requires careful adjustment of various transfer learning hyperparameters not usually considered, including number of layers to transfer, different learning rates for different layers and different combinations of L2SP and L2 regularization. (2) Best practice can be achieved using a number of measures of how well the pre-trained weights fit the target dataset to guide optimal hyperparameters. We present methods for non-binary transfer learning including combining L2SP and L2 regularization and performing non-traditional fine-tuning hyperparameter searches. Finally we suggest heuristics for determining the optimal transfer learning hyperparameters. The benefits of using a non-binary approach are supported by final results that come close to or exceed state of the art performance on a variety of tasks that have traditionally been more difficult for transfer learning.



### Double Similarity Distillation for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.08591v1
- **DOI**: 10.1109/TIP.2021.3083113
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08591v1)
- **Published**: 2021-07-19 02:45:13+00:00
- **Updated**: 2021-07-19 02:45:13+00:00
- **Authors**: Yingchao Feng, Xian Sun, Wenhui Diao, Jihao Li, Xin Gao
- **Comment**: Published in IEEE Transaction on Image Processing (TIP) 2021, Volume:
  30
- **Journal**: None
- **Summary**: The balance between high accuracy and high speed has always been a challenging task in semantic image segmentation. Compact segmentation networks are more widely used in the case of limited resources, while their performances are constrained. In this paper, motivated by the residual learning and global aggregation, we propose a simple yet general and effective knowledge distillation framework called double similarity distillation (DSD) to improve the classification accuracy of all existing compact networks by capturing the similarity knowledge in pixel and category dimensions, respectively. Specifically, we propose a pixel-wise similarity distillation (PSD) module that utilizes residual attention maps to capture more detailed spatial dependencies across multiple layers. Compared with exiting methods, the PSD module greatly reduces the amount of calculation and is easy to expand. Furthermore, considering the differences in characteristics between semantic segmentation task and other computer vision tasks, we propose a category-wise similarity distillation (CSD) module, which can help the compact segmentation network strengthen the global category correlation by constructing the correlation matrix. Combining these two modules, DSD framework has no extra parameters and only a minimal increase in FLOPs. Extensive experiments on four challenging datasets, including Cityscapes, CamVid, ADE20K, and Pascal VOC 2012, show that DSD outperforms current state-of-the-art methods, proving its effectiveness and generality. The code and models will be publicly available.



### Face.evoLVe: A High-Performance Face Recognition Library
- **Arxiv ID**: http://arxiv.org/abs/2107.08621v4
- **DOI**: 10.1016/j.neucom.2022.04.118
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2107.08621v4)
- **Published**: 2021-07-19 05:38:50+00:00
- **Updated**: 2022-05-05 06:30:07+00:00
- **Authors**: Qingzhong Wang, Pengfei Zhang, Haoyi Xiong, Jian Zhao
- **Comment**: A short verson is accepted by NeuroComputing
  (https://www.sciencedirect.com/science/article/pii/S0925231222005057?via%3Dihub).
  Primary corresponding author is Dr. Jian Zhao
- **Journal**: NeuroComputing 494 (2022) 443-445
- **Summary**: In this paper, we develop face.evoLVe -- a comprehensive library that collects and implements a wide range of popular deep learning-based methods for face recognition. First of all, face.evoLVe is composed of key components that cover the full process of face analytics, including face alignment, data processing, various backbones, losses, and alternatives with bags of tricks for improving performance. Later, face.evoLVe supports multi-GPU training on top of different deep learning platforms, such as PyTorch and PaddlePaddle, which facilitates researchers to work on both large-scale datasets with millions of images and low-shot counterparts with limited well-annotated data. More importantly, along with face.evoLVe, images before & after alignment in the common benchmark datasets are released with source codes and trained models provided. All these efforts lower the technical burdens in reproducing the existing methods for comparison, while users of our library could focus on developing advanced approaches more efficiently. Last but not least, face.evoLVe is well designed and vibrantly evolving, so that new face recognition approaches can be easily plugged into our framework. Note that we have used face.evoLVe to participate in a number of face recognition competitions and secured the first place. The version that supports PyTorch is publicly available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch and the PaddlePaddle version is available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch/tree/master/paddle. Face.evoLVe has been widely used for face analytics, receiving 2.4K stars and 622 forks.



### LeViT-UNet: Make Faster Encoders with Transformer for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.08623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08623v1)
- **Published**: 2021-07-19 05:48:51+00:00
- **Updated**: 2021-07-19 05:48:51+00:00
- **Authors**: Guoping Xu, Xingrong Wu, Xuan Zhang, Xinwei He
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image segmentation plays an essential role in developing computer-assisted diagnosis and therapy systems, yet still faces many challenges. In the past few years, the popular encoder-decoder architectures based on CNNs (e.g., U-Net) have been successfully applied in the task of medical image segmentation. However, due to the locality of convolution operations, they demonstrate limitations in learning global context and long-range spatial relations. Recently, several researchers try to introduce transformers to both the encoder and decoder components with promising results, but the efficiency requires further improvement due to the high computational complexity of transformers. In this paper, we propose LeViT-UNet, which integrates a LeViT Transformer module into the U-Net architecture, for fast and accurate medical image segmentation. Specifically, we use LeViT as the encoder of the LeViT-UNet, which better trades off the accuracy and efficiency of the Transformer block. Moreover, multi-scale feature maps from transformer blocks and convolutional blocks of LeViT are passed into the decoder via skip-connection, which can effectively reuse the spatial information of the feature maps. Our experiments indicate that the proposed LeViT-UNet achieves better performance comparing to various competing methods on several challenging medical image segmentation benchmarks including Synapse and ACDC. Code and models will be publicly available at https://github.com/apple1986/LeViT_UNet.



### Generative Adversarial Neural Cellular Automata
- **Arxiv ID**: http://arxiv.org/abs/2108.04328v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, I.2.m
- **Links**: [PDF](http://arxiv.org/pdf/2108.04328v1)
- **Published**: 2021-07-19 06:23:11+00:00
- **Updated**: 2021-07-19 06:23:11+00:00
- **Authors**: Maximilian Otte, Quentin Delfosse, Johannes Czech, Kristian Kersting
- **Comment**: 8 pages with 12 figures
- **Journal**: None
- **Summary**: Motivated by the interaction between cells, the recently introduced concept of Neural Cellular Automata shows promising results in a variety of tasks. So far, this concept was mostly used to generate images for a single scenario. As each scenario requires a new model, this type of generation seems contradictory to the adaptability of cells in nature. To address this contradiction, we introduce a concept using different initial environments as input while using a single Neural Cellular Automata to produce several outputs. Additionally, we introduce GANCA, a novel algorithm that combines Neural Cellular Automata with Generative Adversarial Networks, allowing for more generalization through adversarial training. The experiments show that a single model is capable of learning several images when presented with different inputs, and that the adversarially trained model improves drastically on out-of-distribution data compared to a supervised trained model.



### Semi-supervised Cell Detection in Time-lapse Images Using Temporal Consistency
- **Arxiv ID**: http://arxiv.org/abs/2107.08639v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08639v1)
- **Published**: 2021-07-19 06:40:47+00:00
- **Updated**: 2021-07-19 06:40:47+00:00
- **Authors**: Kazuya Nishimura, Hyeonwoo Cho, Ryoma Bise
- **Comment**: 11 pages, 5 figures, Accepted in MICCAI2021
- **Journal**: None
- **Summary**: Cell detection is the task of detecting the approximate positions of cell centroids from microscopy images. Recently, convolutional neural network-based approaches have achieved promising performance. However, these methods require a certain amount of annotation for each imaging condition. This annotation is a time-consuming and labor-intensive task. To overcome this problem, we propose a semi-supervised cell-detection method that effectively uses a time-lapse sequence with one labeled image and the other images unlabeled. First, we train a cell-detection network with a one-labeled image and estimate the unlabeled images with the trained network. We then select high-confidence positions from the estimations by tracking the detected cells from the labeled frame to those far from it. Next, we generate pseudo-labels from the tracking results and train the network by using pseudo-labels. We evaluated our method for seven conditions of public datasets, and we achieved the best results relative to other semi-supervised methods. Our code is available at https://github.com/naivete5656/SCDTC



### Facial Expressions Recognition with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.08640v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.08640v1)
- **Published**: 2021-07-19 06:41:00+00:00
- **Updated**: 2021-07-19 06:41:00+00:00
- **Authors**: Subodh Lonkar
- **Comment**: None
- **Journal**: None
- **Summary**: Over the centuries, humans have developed and acquired a number of ways to communicate. But hardly any of them can be as natural and instinctive as facial expressions. On the other hand, neural networks have taken the world by storm. And no surprises, that the area of Computer Vision and the problem of facial expressions recognitions hasn't remained untouched. Although a wide range of techniques have been applied, achieving extremely high accuracies and preparing highly robust FER systems still remains a challenge due to heterogeneous details in human faces. In this paper, we will be deep diving into implementing a system for recognition of facial expressions (FER) by leveraging neural networks, and more specifically, Convolutional Neural Networks (CNNs). We adopt the fundamental concepts of deep learning and computer vision with various architectures, fine-tune it's hyperparameters and experiment with various optimization methods and demonstrate a state-of-the-art single-network-accuracy of 70.10% on the FER2013 dataset without using any additional training data.



### Video Crowd Localization with Multi-focus Gaussian Neighborhood Attention and a Large-Scale Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2107.08645v4
- **DOI**: 10.1109/TIP.2022.3205210
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08645v4)
- **Published**: 2021-07-19 06:59:27+00:00
- **Updated**: 2022-08-08 04:22:04+00:00
- **Authors**: Haopeng Li, Lingbo Liu, Kunlin Yang, Shinan Liu, Junyu Gao, Bin Zhao, Rui Zhang, Jun Hou
- **Comment**: None
- **Journal**: None
- **Summary**: Video crowd localization is a crucial yet challenging task, which aims to estimate exact locations of human heads in the given crowded videos. To model spatial-temporal dependencies of human mobility, we propose a multi-focus Gaussian neighborhood attention (GNA), which can effectively exploit long-range correspondences while maintaining the spatial topological structure of the input videos. In particular, our GNA can also capture the scale variation of human heads well using the equipped multi-focus mechanism. Based on the multi-focus GNA, we develop a unified neural network called GNANet to accurately locate head centers in video clips by fully aggregating spatial-temporal information via a scene modeling module and a context cross-attention module. Moreover, to facilitate future researches in this field, we introduce a large-scale crowd video benchmark named VSCrowd, which consists of 60K+ frames captured in various surveillance scenarios and 2M+ head annotations. Finally, we conduct extensive experiments on three datasets including our SenseCrowd, and the experiment results show that the proposed method is capable to achieve state-of-the-art performance for both video crowd localization and counting.



### Compound Figure Separation of Biomedical Images with Side Loss
- **Arxiv ID**: http://arxiv.org/abs/2107.08650v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08650v1)
- **Published**: 2021-07-19 07:16:32+00:00
- **Updated**: 2021-07-19 07:16:32+00:00
- **Authors**: Tianyuan Yao, Chang Qu, Quan Liu, Ruining Deng, Yuanhan Tian, Jiachen Xu, Aadarsh Jha, Shunxing Bao, Mengyang Zhao, Agnes B. Fogo, Bennett A. Landman, Catie Chang, Haichun Yang, Yuankai Huo
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised learning algorithms (e.g., self-supervised learning, auto-encoder, contrastive learning) allow deep learning models to learn effective image representations from large-scale unlabeled data. In medical image analysis, even unannotated data can be difficult to obtain for individual labs. Fortunately, national-level efforts have been made to provide efficient access to obtain biomedical image data from previous scientific publications. For instance, NIH has launched the Open-i search engine that provides a large-scale image database with free access. However, the images in scientific publications consist of a considerable amount of compound figures with subplots. To extract and curate individual subplots, many different compound figure separation approaches have been developed, especially with the recent advances in deep learning. However, previous approaches typically required resource extensive bounding box annotation to train detection models. In this paper, we propose a simple compound figure separation (SimCFS) framework that uses weak classification annotations from individual images. Our technical contribution is three-fold: (1) we introduce a new side loss that is designed for compound figure separation; (2) we introduce an intra-class image augmentation method to simulate hard cases; (3) the proposed framework enables an efficient deployment to new classes of images, without requiring resource extensive bounding box annotations. From the results, the SimCFS achieved a new state-of-the-art performance on the ImageCLEF 2016 Compound Figure Separation Database. The source code of SimCFS is made publicly available at https://github.com/hrlblab/ImageSeperation.



### Cell Detection in Domain Shift Problem Using Pseudo-Cell-Position Heatmap
- **Arxiv ID**: http://arxiv.org/abs/2107.08653v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08653v1)
- **Published**: 2021-07-19 07:22:10+00:00
- **Updated**: 2021-07-19 07:22:10+00:00
- **Authors**: Hyeonwoo Cho, Kazuya Nishimura, Kazuhide Watanabe, Ryoma Bise
- **Comment**: 10 pages, 4 figures, Accepted in MICCAI 2021
- **Journal**: None
- **Summary**: The domain shift problem is an important issue in automatic cell detection. A detection network trained with training data under a specific condition (source domain) may not work well in data under other conditions (target domain). We propose an unsupervised domain adaptation method for cell detection using the pseudo-cell-position heatmap, where a cell centroid becomes a peak with a Gaussian distribution in the map. In the prediction result for the target domain, even if a peak location is correct, the signal distribution around the peak often has anon-Gaussian shape. The pseudo-cell-position heatmap is re-generated using the peak positions in the predicted heatmap to have a clear Gaussian shape. Our method selects confident pseudo-cell-position heatmaps using a Bayesian network and adds them to the training data in the next iteration. The method can incrementally extend the domain from the source domain to the target domain in a semi-supervised manner. In the experiments using 8 combinations of domains, the proposed method outperformed the existing domain adaptation methods.



### Input Agnostic Deep Learning for Alzheimer's Disease Classification Using Multimodal MRI Images
- **Arxiv ID**: http://arxiv.org/abs/2107.08673v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.08673v1)
- **Published**: 2021-07-19 08:19:34+00:00
- **Updated**: 2021-07-19 08:19:34+00:00
- **Authors**: Aidana Massalimova, Huseyin Atakan Varol
- **Comment**: 4 pages, submitted to EMBC 2021
- **Journal**: None
- **Summary**: Alzheimer's disease (AD) is a progressive brain disorder that causes memory and functional impairments. The advances in machine learning and publicly available medical datasets initiated multiple studies in AD diagnosis. In this work, we utilize a multi-modal deep learning approach in classifying normal cognition, mild cognitive impairment and AD classes on the basis of structural MRI and diffusion tensor imaging (DTI) scans from the OASIS-3 dataset. In addition to a conventional multi-modal network, we also present an input agnostic architecture that allows diagnosis with either sMRI or DTI scan, which distinguishes our method from previous multi-modal machine learning-based methods. The results show that the input agnostic model achieves 0.96 accuracy when both structural MRI and DTI scans are provided as inputs.



### Exploring Set Similarity for Dense Self-supervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.08712v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08712v2)
- **Published**: 2021-07-19 09:38:27+00:00
- **Updated**: 2022-03-14 18:46:56+00:00
- **Authors**: Zhaoqing Wang, Qiang Li, Guoxin Zhang, Pengfei Wan, Wen Zheng, Nannan Wang, Mingming Gong, Tongliang Liu
- **Comment**: 10 pages, 4 figures, Accepted by CVPR2022
- **Journal**: None
- **Summary**: By considering the spatial correspondence, dense self-supervised representation learning has achieved superior performance on various dense prediction tasks. However, the pixel-level correspondence tends to be noisy because of many similar misleading pixels, e.g., backgrounds. To address this issue, in this paper, we propose to explore \textbf{set} \textbf{sim}ilarity (SetSim) for dense self-supervised representation learning. We generalize pixel-wise similarity learning to set-wise one to improve the robustness because sets contain more semantic and structure information. Specifically, by resorting to attentional features of views, we establish corresponding sets, thus filtering out noisy backgrounds that may cause incorrect correspondences. Meanwhile, these attentional features can keep the coherence of the same image across different views to alleviate semantic inconsistency. We further search the cross-view nearest neighbours of sets and employ the structured neighbourhood information to enhance the robustness. Empirical evaluations demonstrate that SetSim is superior to state-of-the-art methods on object detection, keypoint detection, instance segmentation, and semantic segmentation.



### RECIST-Net: Lesion detection via grouping keypoints on RECIST-based annotation
- **Arxiv ID**: http://arxiv.org/abs/2107.08715v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08715v1)
- **Published**: 2021-07-19 09:41:13+00:00
- **Updated**: 2021-07-19 09:41:13+00:00
- **Authors**: Cong Xie, Shilei Cao, Dong Wei, Hongyu Zhou, Kai Ma, Xianli Zhang, Buyue Qian, Liansheng Wang, Yefeng Zheng
- **Comment**: 5 pages, 3 figures, IEEE ISBI 2021
- **Journal**: None
- **Summary**: Universal lesion detection in computed tomography (CT) images is an important yet challenging task due to the large variations in lesion type, size, shape, and appearance. Considering that data in clinical routine (such as the DeepLesion dataset) are usually annotated with a long and a short diameter according to the standard of Response Evaluation Criteria in Solid Tumors (RECIST) diameters, we propose RECIST-Net, a new approach to lesion detection in which the four extreme points and center point of the RECIST diameters are detected. By detecting a lesion as keypoints, we provide a more conceptually straightforward formulation for detection, and overcome several drawbacks (e.g., requiring extensive effort in designing data-appropriate anchors and losing shape information) of existing bounding-box-based methods while exploring a single-task, one-stage approach compared to other RECIST-based approaches. Experiments show that RECIST-Net achieves a sensitivity of 92.49% at four false positives per image, outperforming other recent methods including those using multi-task learning.



### Joint Implicit Image Function for Guided Depth Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2107.08717v2
- **DOI**: 10.1145/3474085.3475584
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08717v2)
- **Published**: 2021-07-19 09:42:18+00:00
- **Updated**: 2021-07-23 09:54:31+00:00
- **Authors**: Jiaxiang Tang, Xiaokang Chen, Gang Zeng
- **Comment**: Accepted by ACM MM 2021
- **Journal**: None
- **Summary**: Guided depth super-resolution is a practical task where a low-resolution and noisy input depth map is restored to a high-resolution version, with the help of a high-resolution RGB guide image. Existing methods usually view this task as a generalized guided filtering problem that relies on designing explicit filters and objective functions, or a dense regression problem that directly predicts the target image via deep neural networks. These methods suffer from either model capability or interpretability. Inspired by the recent progress in implicit neural representation, we propose to formulate the guided super-resolution as a neural implicit image interpolation problem, where we take the form of a general image interpolation but use a novel Joint Implicit Image Function (JIIF) representation to learn both the interpolation weights and values. JIIF represents the target image domain with spatially distributed local latent codes extracted from the input image and the guide image, and uses a graph attention mechanism to learn the interpolation weights at the same time in one unified deep implicit function. We demonstrate the effectiveness of our JIIF representation on guided depth super-resolution task, significantly outperforming state-of-the-art methods on three public benchmarks. Code can be found at \url{https://git.io/JC2sU}.



### Synthesizing Human Faces using Latent Space Factorization and Local Weights (Extended Version)
- **Arxiv ID**: http://arxiv.org/abs/2107.08737v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.08737v1)
- **Published**: 2021-07-19 10:17:30+00:00
- **Updated**: 2021-07-19 10:17:30+00:00
- **Authors**: Minyoung Kim, Young J. Kim
- **Comment**: Extended version of the paper to will be published in Computer
  Graphics International 2021 (LNCS Proceeding Papers)
- **Journal**: None
- **Summary**: We propose a 3D face generative model with local weights to increase the model's variations and expressiveness. The proposed model allows partial manipulation of the face while still learning the whole face mesh. For this purpose, we address an effective way to extract local facial features from the entire data and explore a way to manipulate them during a holistic generation. First, we factorize the latent space of the whole face to the subspace indicating different parts of the face. In addition, local weights generated by non-negative matrix factorization are applied to the factorized latent space so that the decomposed part space is semantically meaningful. We experiment with our model and observe that effective facial part manipulation is possible and that the model's expressiveness is improved.



### Adversarial Continual Learning for Multi-Domain Hippocampal Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.08751v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.08751v4)
- **Published**: 2021-07-19 10:55:21+00:00
- **Updated**: 2021-07-25 14:48:14+00:00
- **Authors**: Marius Memmel, Camila Gonzalez, Anirban Mukhopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning for medical imaging suffers from temporal and privacy-related restrictions on data availability. To still obtain viable models, continual learning aims to train in sequential order, as and when data is available. The main challenge that continual learning methods face is to prevent catastrophic forgetting, i.e., a decrease in performance on the data encountered earlier. This issue makes continuous training of segmentation models for medical applications extremely difficult. Yet, often, data from at least two different domains is available which we can exploit to train the model in a way that it disregards domain-specific information. We propose an architecture that leverages the simultaneous availability of two or more datasets to learn a disentanglement between the content and domain in an adversarial fashion. The domain-invariant content representation then lays the base for continual semantic segmentation. Our approach takes inspiration from domain adaptation and combines it with continual learning for hippocampal segmentation in brain MRI. We showcase that our method reduces catastrophic forgetting and outperforms state-of-the-art continual learning methods.



### Attribution of Predictive Uncertainties in Classification Models
- **Arxiv ID**: http://arxiv.org/abs/2107.08756v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2107.08756v3)
- **Published**: 2021-07-19 11:07:34+00:00
- **Updated**: 2022-06-08 16:10:21+00:00
- **Authors**: Iker Perez, Piotr Skalski, Alec Barns-Graham, Jason Wong, David Sutton
- **Comment**: None
- **Journal**: Proceedings of the Thirty-Eighth Conference on Uncertainty in
  Artificial Intelligence, PMLR 180:1582-1591, 2022
- **Summary**: Predictive uncertainties in classification tasks are often a consequence of model inadequacy or insufficient training data. In popular applications, such as image processing, we are often required to scrutinise these uncertainties by meaningfully attributing them to input features. This helps to improve interpretability assessments. However, there exist few effective frameworks for this purpose. Vanilla forms of popular methods for the provision of saliency masks, such as SHAP or integrated gradients, adapt poorly to target measures of uncertainty. Thus, state-of-the-art tools instead proceed by creating counterfactual or adversarial feature vectors, and assign attributions by direct comparison to original images. In this paper, we present a novel framework that combines path integrals, counterfactual explanations and generative models, in order to procure attributions that contain few observable artefacts or noise. We evidence that this outperforms existing alternatives through quantitative evaluations with popular benchmarking methods and data sets of varying complexity.



### VisDrone-CC2020: The Vision Meets Drone Crowd Counting Challenge Results
- **Arxiv ID**: http://arxiv.org/abs/2107.08766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08766v1)
- **Published**: 2021-07-19 11:48:29+00:00
- **Updated**: 2021-07-19 11:48:29+00:00
- **Authors**: Dawei Du, Longyin Wen, Pengfei Zhu, Heng Fan, Qinghua Hu, Haibin Ling, Mubarak Shah, Junwen Pan, Ali Al-Ali, Amr Mohamed, Bakour Imene, Bin Dong, Binyu Zhang, Bouchali Hadia Nesma, Chenfeng Xu, Chenzhen Duan, Ciro Castiello, Corrado Mencar, Dingkang Liang, Florian Krüger, Gennaro Vessio, Giovanna Castellano, Jieru Wang, Junyu Gao, Khalid Abualsaud, Laihui Ding, Lei Zhao, Marco Cianciotta, Muhammad Saqib, Noor Almaadeed, Omar Elharrouss, Pei Lyu, Qi Wang, Shidong Liu, Shuang Qiu, Siyang Pan, Somaya Al-Maadeed, Sultan Daud Khan, Tamer Khattab, Tao Han, Thomas Golda, Wei Xu, Xiang Bai, Xiaoqing Xu, Xuelong Li, Yanyun Zhao, Ye Tian, Yingnan Lin, Yongchao Xu, Yuehan Yao, Zhenyu Xu, Zhijian Zhao, Zhipeng Luo, Zhiwei Wei, Zhiyuan Zhao
- **Comment**: The method description of A7 Mutil-Scale Aware based SFANet
  (M-SFANet) is updated and missing references are added
- **Journal**: European Conference on Computer Vision. Springer, Cham, 2020:
  675-691
- **Summary**: Crowd counting on the drone platform is an interesting topic in computer vision, which brings new challenges such as small object inference, background clutter and wide viewpoint. However, there are few algorithms focusing on crowd counting on the drone-captured data due to the lack of comprehensive datasets. To this end, we collect a large-scale dataset and organize the Vision Meets Drone Crowd Counting Challenge (VisDrone-CC2020) in conjunction with the 16th European Conference on Computer Vision (ECCV 2020) to promote the developments in the related fields. The collected dataset is formed by $3,360$ images, including $2,460$ images for training, and $900$ images for testing. Specifically, we manually annotate persons with points in each video frame. There are $14$ algorithms from $15$ institutes submitted to the VisDrone-CC2020 Challenge. We provide a detailed analysis of the evaluation results and conclude the challenge. More information can be found at the website: \url{http://www.aiskyeye.com/}.



### Improving Interpretability of Deep Neural Networks in Medical Diagnosis by Investigating the Individual Units
- **Arxiv ID**: http://arxiv.org/abs/2107.08767v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.08767v1)
- **Published**: 2021-07-19 11:49:31+00:00
- **Updated**: 2021-07-19 11:49:31+00:00
- **Authors**: Woo-Jeoung Nam, Seong-Whan Lee
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: As interpretability has been pointed out as the obstacle to the adoption of Deep Neural Networks (DNNs), there is an increasing interest in solving a transparency issue to guarantee the impressive performance. In this paper, we demonstrate the efficiency of recent attribution techniques to explain the diagnostic decision by visualizing the significant factors in the input image. By utilizing the characteristics of objectness that DNNs have learned, fully decomposing the network prediction visualizes clear localization of target lesion. To verify our work, we conduct our experiments on Chest X-ray diagnosis with publicly accessible datasets. As an intuitive assessment metric for explanations, we report the performance of intersection of Union between visual explanation and bounding box of lesions. Experiment results show that recently proposed attribution methods visualize the more accurate localization for the diagnostic decision compared to the traditionally used CAM. Furthermore, we analyze the inconsistency of intentions between humans and DNNs, which is easily obscured by high performance. By visualizing the relevant factors, it is possible to confirm that the criterion for decision is in line with the learning strategy. Our analysis of unmasking machine intelligence represents the necessity of explainability in the medical diagnostic decision.



### Precise Aerial Image Matching based on Deep Homography Estimation
- **Arxiv ID**: http://arxiv.org/abs/2107.08768v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08768v1)
- **Published**: 2021-07-19 11:52:52+00:00
- **Updated**: 2021-07-19 11:52:52+00:00
- **Authors**: Myeong-Seok Oh, Yong-Ju Lee, Seong-Whan Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Aerial image registration or matching is a geometric process of aligning two aerial images captured in different environments. Estimating the precise transformation parameters is hindered by various environments such as time, weather, and viewpoints. The characteristics of the aerial images are mainly composed of a straight line owing to building and road. Therefore, the straight lines are distorted when estimating homography parameters directly between two images. In this paper, we propose a deep homography alignment network to precisely match two aerial images by progressively estimating the various transformation parameters. The proposed network is possible to train the matching network with a higher degree of freedom by progressively analyzing the transformation parameters. The precision matching performances have been increased by applying homography transformation. In addition, we introduce a method that can effectively learn the difficult-to-learn homography estimation network. Since there is no published learning data for aerial image registration, in this paper, a pair of images to which random homography transformation is applied within a certain range is used for learning. Hence, we could confirm that the deep homography alignment network shows high precision matching performance compared with conventional works.



### Joint Dermatological Lesion Classification and Confidence Modeling with Uncertainty Estimation
- **Arxiv ID**: http://arxiv.org/abs/2107.08770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08770v1)
- **Published**: 2021-07-19 11:54:37+00:00
- **Updated**: 2021-07-19 11:54:37+00:00
- **Authors**: Gun-Hee Lee, Han-Bin Ko, Seong-Whan Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has played a major role in the interpretation of dermoscopic images for detecting skin defects and abnormalities. However, current deep learning solutions for dermatological lesion analysis are typically limited in providing probabilistic predictions which highlights the importance of concerning uncertainties. This concept of uncertainty can provide a confidence level for each feature which prevents overconfident predictions with poor generalization on unseen data. In this paper, we propose an overall framework that jointly considers dermatological classification and uncertainty estimation together. The estimated confidence of each feature to avoid uncertain feature and undesirable shift, which are caused by environmental difference of input image, in the latent space is pooled from confidence network. Our qualitative results show that modeling uncertainties not only helps to quantify model confidence for each prediction but also helps classification layers to focus on confident features, therefore, improving the accuracy for dermatological lesion classification. We demonstrate the potential of the proposed approach in two state-of-the-art dermoscopic datasets (ISIC 2018 and ISIC 2019).



### Automatic and explainable grading of meningiomas from histopathology images
- **Arxiv ID**: http://arxiv.org/abs/2107.08850v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68T99
- **Links**: [PDF](http://arxiv.org/pdf/2107.08850v1)
- **Published**: 2021-07-19 13:05:51+00:00
- **Updated**: 2021-07-19 13:05:51+00:00
- **Authors**: Jonathan Ganz, Tobias Kirsch, Lucas Hoffmann, Christof A. Bertram, Christoph Hoffmann, Andreas Maier, Katharina Breininger, Ingmar Blümcke, Samir Jabari, Marc Aubreville
- **Comment**: 12 pages, 3 figures
- **Journal**: None
- **Summary**: Meningioma is one of the most prevalent brain tumors in adults. To determine its malignancy, it is graded by a pathologist into three grades according to WHO standards. This grade plays a decisive role in treatment, and yet may be subject to inter-rater discordance. In this work, we present and compare three approaches towards fully automatic meningioma grading from histology whole slide images. All approaches are following a two-stage paradigm, where we first identify a region of interest based on the detection of mitotic figures in the slide using a state-of-the-art object detection deep learning network. This region of highest mitotic rate is considered characteristic for biological tumor behavior. In the second stage, we calculate a score corresponding to tumor malignancy based on information contained in this region using three different settings. In a first approach, image patches are sampled from this region and regression is based on morphological features encoded by a ResNet-based network. We compare this to learning a logistic regression from the determined mitotic count, an approach which is easily traceable and explainable. Lastly, we combine both approaches in a single network. We trained the pipeline on 951 slides from 341 patients and evaluated them on a separate set of 141 slides from 43 patients. All approaches yield a high correlation to the WHO grade. The logistic regression and the combined approach had the best results in our experiments, yielding correct predictions in 32 and 33 of all cases, respectively, with the image-based approach only predicting 25 cases correctly. Spearman's correlation was 0.716, 0.792 and 0.790 respectively. It may seem counterintuitive at first that morphological features provided by image patches do not improve model performance. Yet, this mirrors the criteria of the grading scheme, where mitotic count is the only unequivocal parameter.



### Disentangling and Vectorization: A 3D Visual Perception Approach for Autonomous Driving Based on Surround-View Fisheye Cameras
- **Arxiv ID**: http://arxiv.org/abs/2107.08862v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08862v1)
- **Published**: 2021-07-19 13:24:21+00:00
- **Updated**: 2021-07-19 13:24:21+00:00
- **Authors**: Zizhang Wu, Wenkai Zhang, Jizheng Wang, Man Wang, Yuanzhu Gan, Xinchao Gou, Muqing Fang, Jing Song
- **Comment**: Accepted by IROS 2021
- **Journal**: None
- **Summary**: The 3D visual perception for vehicles with the surround-view fisheye camera system is a critical and challenging task for low-cost urban autonomous driving. While existing monocular 3D object detection methods perform not well enough on the fisheye images for mass production, partly due to the lack of 3D datasets of such images. In this paper, we manage to overcome and avoid the difficulty of acquiring the large scale of accurate 3D labeled truth data, by breaking down the 3D object detection task into some sub-tasks, such as vehicle's contact point detection, type classification, re-identification and unit assembling, etc. Particularly, we propose the concept of Multidimensional Vector to include the utilizable information generated in different dimensions and stages, instead of the descriptive approach for the bird's eye view (BEV) or a cube of eight points. The experiments of real fisheye images demonstrate that our solution achieves state-of-the-art accuracy while being real-time in practice.



### Unsupervised Embedding Learning from Uncertainty Momentum Modeling
- **Arxiv ID**: http://arxiv.org/abs/2107.08892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08892v1)
- **Published**: 2021-07-19 14:06:19+00:00
- **Updated**: 2021-07-19 14:06:19+00:00
- **Authors**: Jiahuan Zhou, Yansong Tang, Bing Su, Ying Wu
- **Comment**: 14 pages, in submission
- **Journal**: None
- **Summary**: Existing popular unsupervised embedding learning methods focus on enhancing the instance-level local discrimination of the given unlabeled images by exploring various negative data. However, the existed sample outliers which exhibit large intra-class divergences or small inter-class variations severely limit their learning performance. We justify that the performance limitation is caused by the gradient vanishing on these sample outliers. Moreover, the shortage of positive data and disregard for global discrimination consideration also pose critical issues for unsupervised learning but are always ignored by existing methods. To handle these issues, we propose a novel solution to explicitly model and directly explore the uncertainty of the given unlabeled learning samples. Instead of learning a deterministic feature point for each sample in the embedding space, we propose to represent a sample by a stochastic Gaussian with the mean vector depicting its space localization and covariance vector representing the sample uncertainty. We leverage such uncertainty modeling as momentum to the learning which is helpful to tackle the outliers. Furthermore, abundant positive candidates can be readily drawn from the learned instance-specific distributions which are further adopted to mitigate the aforementioned issues. Thorough rationale analyses and extensive experiments are presented to verify our superiority.



### Self-Promoted Prototype Refinement for Few-Shot Class-Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.08918v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08918v1)
- **Published**: 2021-07-19 14:31:33+00:00
- **Updated**: 2021-07-19 14:31:33+00:00
- **Authors**: Kai Zhu, Yang Cao, Wei Zhai, Jie Cheng, Zheng-Jun Zha
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Few-shot class-incremental learning is to recognize the new classes given few samples and not forget the old classes. It is a challenging task since representation optimization and prototype reorganization can only be achieved under little supervision. To address this problem, we propose a novel incremental prototype learning scheme. Our scheme consists of a random episode selection strategy that adapts the feature representation to various generated incremental episodes to enhance the corresponding extensibility, and a self-promoted prototype refinement mechanism which strengthens the expression ability of the new classes by explicitly considering the dependencies among different classes. Particularly, a dynamic relation projection module is proposed to calculate the relation matrix in a shared embedding space and leverage it as the factor for bootstrapping the update of prototypes. Extensive experiments on three benchmark datasets demonstrate the above-par incremental performance, outperforming state-of-the-art methods by a margin of 13%, 17% and 11%, respectively.



### DHNet: Double MPEG-4 Compression Detection via Multiple DCT Histograms
- **Arxiv ID**: http://arxiv.org/abs/2107.08939v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.08939v2)
- **Published**: 2021-07-19 14:53:17+00:00
- **Updated**: 2022-04-15 12:56:40+00:00
- **Authors**: Seung-Hun Nam, Wonhyuk Ahn, Myung-Joon Kwon, Jihyeon Kang, In-Jae Yu
- **Comment**: Accepted to IEEE MultiMedia
- **Journal**: None
- **Summary**: In this article, we aim to detect the double compression of MPEG-4, a universal video codec that is built into surveillance systems and shooting devices. Double compression is accompanied by various types of video manipulation, and its traces can be exploited to determine whether a video is a forgery. To this end, we present a neural network-based approach with discriminant features for capturing peculiar artifacts in the discrete cosine transform (DCT) domain caused by double MPEG-4 compression. By analyzing the intra-coding process of MPEG-4, which performs block-DCT-based quantization, we exploit multiple DCT histograms as features to focus on the statistical properties of DCT coefficients on multiresolution blocks. Furthermore, we improve detection performance using a vectorized feature of the quantization table on dense layers as auxiliary information. Compared with neural network-based approaches suitable for exploring subtle manipulations, the experimental results reveal that this work achieves high performance.



### GenRadar: Self-supervised Probabilistic Camera Synthesis based on Radar Frequencies
- **Arxiv ID**: http://arxiv.org/abs/2107.08948v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.08948v1)
- **Published**: 2021-07-19 15:00:28+00:00
- **Updated**: 2021-07-19 15:00:28+00:00
- **Authors**: Carsten Ditzel, Klaus Dietmayer
- **Comment**: concurrently submitted to IEEE Access
- **Journal**: None
- **Summary**: Autonomous systems require a continuous and dependable environment perception for navigation and decision-making, which is best achieved by combining different sensor types. Radar continues to function robustly in compromised circumstances in which cameras become impaired, guaranteeing a steady inflow of information. Yet, camera images provide a more intuitive and readily applicable impression of the world. This work combines the complementary strengths of both sensor types in a unique self-learning fusion approach for a probabilistic scene reconstruction in adverse surrounding conditions. After reducing the memory requirements of both high-dimensional measurements through a decoupled stochastic self-supervised compression technique, the proposed algorithm exploits similarities and establishes correspondences between both domains at different feature levels during training. Then, at inference time, relying exclusively on radio frequencies, the model successively predicts camera constituents in an autoregressive and self-contained process. These discrete tokens are finally transformed back into an instructive view of the respective surrounding, allowing to visually perceive potential dangers for important tasks downstream.



### Class dependency based learning using Bi-LSTM coupled with the transfer learning of VGG16 for the diagnosis of Tuberculosis from chest x-rays
- **Arxiv ID**: http://arxiv.org/abs/2108.04329v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.04329v1)
- **Published**: 2021-07-19 15:13:46+00:00
- **Updated**: 2021-07-19 15:13:46+00:00
- **Authors**: G Jignesh Chowdary, Suganya G, Premalatha M, Karunamurthy K
- **Comment**: None
- **Journal**: None
- **Summary**: Tuberculosis is an infectious disease that is leading to the death of millions of people across the world. The mortality rate of this disease is high in patients suffering from immuno-compromised disorders. The early diagnosis of this disease can save lives and can avoid further complications. But the diagnosis of TB is a very complex task. The standard diagnostic tests still rely on traditional procedures developed in the last century. These procedures are slow and expensive. So this paper presents an automatic approach for the diagnosis of TB from posteroanterior chest x-rays. This is a two-step approach, where in the first step the lung regions are segmented from the chest x-rays using the graph cut method, and then in the second step the transfer learning of VGG16 combined with Bi-directional LSTM is used for extracting high-level discriminative features from the segmented lung regions and then classification is performed using a fully connected layer. The proposed model is evaluated using data from two publicly available databases namely Montgomery Country set and Schezien set. The proposed model achieved accuracy and sensitivity of 97.76%, 97.01% and 96.42%, 94.11% on Schezien and Montgomery county datasets. This model enhanced the diagnostic accuracy of TB by 0.7% and 11.68% on Schezien and Montgomery county datasets.



### Frequency-Supervised MR-to-CT Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2107.08962v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.08962v1)
- **Published**: 2021-07-19 15:18:36+00:00
- **Updated**: 2021-07-19 15:18:36+00:00
- **Authors**: Zenglin Shi, Pascal Mettes, Guoyan Zheng, Cees Snoek
- **Comment**: MICCAI workshop on Deep Generative Models, 2021
- **Journal**: None
- **Summary**: This paper strives to generate a synthetic computed tomography (CT) image from a magnetic resonance (MR) image. The synthetic CT image is valuable for radiotherapy planning when only an MR image is available. Recent approaches have made large strides in solving this challenging synthesis problem with convolutional neural networks that learn a mapping from MR inputs to CT outputs. In this paper, we find that all existing approaches share a common limitation: reconstruction breaks down in and around the high-frequency parts of CT images. To address this common limitation, we introduce frequency-supervised deep networks to explicitly enhance high-frequency MR-to-CT image reconstruction. We propose a frequency decomposition layer that learns to decompose predicted CT outputs into low- and high-frequency components, and we introduce a refinement module to improve high-frequency reconstruction through high-frequency adversarial learning. Experimental results on a new dataset with 45 pairs of 3D MR-CT brain images show the effectiveness and potential of the proposed approach. Code is available at \url{https://github.com/shizenglin/Frequency-Supervised-MR-to-CT-Image-Synthesis}.



### Transductive image segmentation: Self-training and effect of uncertainty estimation
- **Arxiv ID**: http://arxiv.org/abs/2107.08964v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08964v3)
- **Published**: 2021-07-19 15:26:07+00:00
- **Updated**: 2021-08-02 20:28:27+00:00
- **Authors**: Konstantinos Kamnitsas, Stefan Winzeck, Evgenios N. Kornaropoulos, Daniel Whitehouse, Cameron Englman, Poe Phyu, Norman Pao, David K. Menon, Daniel Rueckert, Tilak Das, Virginia F. J. Newcombe, Ben Glocker
- **Comment**: Published at Domain Adaptation and Representation Transfer (DART)
  wshop at MICCAI 2021. This version improves methods' names and adds 1
  experiment in Tab.3a
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL) uses unlabeled data during training to learn better models. Previous studies on SSL for medical image segmentation focused mostly on improving model generalization to unseen data. In some applications, however, our primary interest is not generalization but to obtain optimal predictions on a specific unlabeled database that is fully available during model development. Examples include population studies for extracting imaging phenotypes. This work investigates an often overlooked aspect of SSL, transduction. It focuses on the quality of predictions made on the unlabeled data of interest when they are included for optimization during training, rather than improving generalization. We focus on the self-training framework and explore its potential for transduction. We analyze it through the lens of Information Gain and reveal that learning benefits from the use of calibrated or under-confident models. Our extensive experiments on a large MRI database for multi-class segmentation of traumatic brain lesions shows promising results when comparing transductive with inductive predictions. We believe this study will inspire further research on transductive learning, a well-suited paradigm for medical image analysis.



### LAPNet: Non-rigid Registration derived in k-space for Magnetic Resonance Imaging
- **Arxiv ID**: http://arxiv.org/abs/2107.09060v1
- **DOI**: 10.1109/TMI.2021.3096131
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.09060v1)
- **Published**: 2021-07-19 15:39:23+00:00
- **Updated**: 2021-07-19 15:39:23+00:00
- **Authors**: Thomas Küstner, Jiazhen Pan, Haikun Qi, Gastao Cruz, Christopher Gilliam, Thierry Blu, Bin Yang, Sergios Gatidis, René Botnar, Claudia Prieto
- **Comment**: None
- **Journal**: None
- **Summary**: Physiological motion, such as cardiac and respiratory motion, during Magnetic Resonance (MR) image acquisition can cause image artifacts. Motion correction techniques have been proposed to compensate for these types of motion during thoracic scans, relying on accurate motion estimation from undersampled motion-resolved reconstruction. A particular interest and challenge lie in the derivation of reliable non-rigid motion fields from the undersampled motion-resolved data. Motion estimation is usually formulated in image space via diffusion, parametric-spline, or optical flow methods. However, image-based registration can be impaired by remaining aliasing artifacts due to the undersampled motion-resolved reconstruction. In this work, we describe a formalism to perform non-rigid registration directly in the sampled Fourier space, i.e. k-space. We propose a deep-learning based approach to perform fast and accurate non-rigid registration from the undersampled k-space data. The basic working principle originates from the Local All-Pass (LAP) technique, a recently introduced optical flow-based registration. The proposed LAPNet is compared against traditional and deep learning image-based registrations and tested on fully-sampled and highly-accelerated (with two undersampling strategies) 3D respiratory motion-resolved MR images in a cohort of 40 patients with suspected liver or lung metastases and 25 healthy subjects. The proposed LAPNet provided consistent and superior performance to image-based approaches throughout different sampling trajectories and acceleration factors.



### OODformer: Out-Of-Distribution Detection Transformer
- **Arxiv ID**: http://arxiv.org/abs/2107.08976v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08976v2)
- **Published**: 2021-07-19 15:46:38+00:00
- **Updated**: 2021-12-06 16:47:07+00:00
- **Authors**: Rajat Koner, Poulami Sinhamahapatra, Karsten Roscher, Stephan Günnemann, Volker Tresp
- **Comment**: None
- **Journal**: BMVC,2021
- **Summary**: A serious problem in image classification is that a trained model might perform well for input data that originates from the same distribution as the data available for model training, but performs much worse for out-of-distribution (OOD) samples. In real-world safety-critical applications, in particular, it is important to be aware if a new data point is OOD. To date, OOD detection is typically addressed using either confidence scores, auto-encoder based reconstruction, or by contrastive learning. However, the global image context has not yet been explored to discriminate the non-local objectness between in-distribution and OOD samples. This paper proposes a first-of-its-kind OOD detection architecture named OODformer that leverages the contextualization capabilities of the transformer. Incorporating the trans\-former as the principal feature extractor allows us to exploit the object concepts and their discriminate attributes along with their co-occurrence via visual attention. Using the contextualised embedding, we demonstrate OOD detection using both class-conditioned latent space similarity and a network confidence score. Our approach shows improved generalizability across various datasets. We have achieved a new state-of-the-art result on CIFAR-10/-100 and ImageNet30.



### InsPose: Instance-Aware Networks for Single-Stage Multi-Person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2107.08982v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.08982v3)
- **Published**: 2021-07-19 15:56:09+00:00
- **Updated**: 2022-04-22 07:20:30+00:00
- **Authors**: Dahu Shi, Xing Wei, Xiaodong Yu, Wenming Tan, Ye Ren, Shiliang Pu
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-person pose estimation is an attractive and challenging task. Existing methods are mostly based on two-stage frameworks, which include top-down and bottom-up methods. Two-stage methods either suffer from high computational redundancy for additional person detectors or they need to group keypoints heuristically after predicting all the instance-agnostic keypoints. The single-stage paradigm aims to simplify the multi-person pose estimation pipeline and receives a lot of attention. However, recent single-stage methods have the limitation of low performance due to the difficulty of regressing various full-body poses from a single feature vector. Different from previous solutions that involve complex heuristic designs, we present a simple yet effective solution by employing instance-aware dynamic networks. Specifically, we propose an instance-aware module to adaptively adjust (part of) the network parameters for each instance. Our solution can significantly increase the capacity and adaptive-ability of the network for recognizing various poses, while maintaining a compact end-to-end trainable pipeline. Extensive experiments on the MS-COCO dataset demonstrate that our method achieves significant improvement over existing single-stage methods, and makes a better balance of accuracy and efficiency compared to the state-of-the-art two-stage approaches. The code and models are available at \url{https://github.com/hikvision-research/opera}.



### A Benchmark for Gait Recognition under Occlusion Collected by Multi-Kinect SDAS
- **Arxiv ID**: http://arxiv.org/abs/2107.08990v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T01, I.2.10; I.5.1; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2107.08990v1)
- **Published**: 2021-07-19 16:01:18+00:00
- **Updated**: 2021-07-19 16:01:18+00:00
- **Authors**: Na Li, Xinbo Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Human gait is one of important biometric characteristics for human identification at a distance. In practice, occlusion usually occurs and seriously affects accuracy of gait recognition. However, there is no available database to support in-depth research of this problem, and state-of-arts gait recognition methods have not paid enough attention to it, thus this paper focuses on gait recognition under occlusion. We collect a new gait recognition database called OG RGB+D database, which breaks through the limitation of other gait databases and includes multimodal gait data of various occlusions (self-occlusion, active occlusion, and passive occlusion) by our multiple synchronous Azure Kinect DK sensors data acquisition system (multi-Kinect SDAS) that can be also applied in security situations. Because Azure Kinect DK can simultaneously collect multimodal data to support different types of gait recognition algorithms, especially enables us to effectively obtain camera-centric multi-person 3D poses, and multi-view is better to deal with occlusion than single-view. In particular, the OG RGB+D database provides accurate silhouettes and the optimized human 3D joints data (OJ) by fusing data collected by multi-Kinects which are more accurate in human pose representation under occlusion. We also use the OJ data to train an advanced 3D multi-person pose estimation model to improve its accuracy of pose estimation under occlusion for universality. Besides, as human pose is less sensitive to occlusion than human appearance, we propose a novel gait recognition method SkeletonGait based on human dual skeleton model using a framework of siamese spatio-temporal graph convolutional networks (siamese ST-GCN). The evaluation results demonstrate that SkeletonGait has competitive performance compared with state-of-art gait recognition methods on OG RGB+D database and popular CAISA-B database.



### CodeMapping: Real-Time Dense Mapping for Sparse SLAM using Compact Scene Representations
- **Arxiv ID**: http://arxiv.org/abs/2107.08994v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.08994v1)
- **Published**: 2021-07-19 16:13:18+00:00
- **Updated**: 2021-07-19 16:13:18+00:00
- **Authors**: Hidenobu Matsuki, Raluca Scona, Jan Czarnowski, Andrew J. Davison
- **Comment**: Accepted to IEEE Robotics and Automation Letters (RA-L) 2021
- **Journal**: None
- **Summary**: We propose a novel dense mapping framework for sparse visual SLAM systems which leverages a compact scene representation. State-of-the-art sparse visual SLAM systems provide accurate and reliable estimates of the camera trajectory and locations of landmarks. While these sparse maps are useful for localization, they cannot be used for other tasks such as obstacle avoidance or scene understanding. In this paper we propose a dense mapping framework to complement sparse visual SLAM systems which takes as input the camera poses, keyframes and sparse points produced by the SLAM system and predicts a dense depth image for every keyframe. We build on CodeSLAM and use a variational autoencoder (VAE) which is conditioned on intensity, sparse depth and reprojection error images from sparse SLAM to predict an uncertainty-aware dense depth map. The use of a VAE then enables us to refine the dense depth images through multi-view optimization which improves the consistency of overlapping frames. Our mapper runs in a separate thread in parallel to the SLAM system in a loosely coupled manner. This flexible design allows for integration with arbitrary metric sparse SLAM systems without delaying the main SLAM process. Our dense mapper can be used not only for local mapping but also globally consistent dense 3D reconstruction through TSDF fusion. We demonstrate our system running with ORB-SLAM3 and show accurate dense depth estimation which could enable applications such as robotics and augmented reality.



### Image Fusion Transformer
- **Arxiv ID**: http://arxiv.org/abs/2107.09011v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09011v4)
- **Published**: 2021-07-19 16:42:49+00:00
- **Updated**: 2022-12-04 22:25:02+00:00
- **Authors**: Vibashan VS, Jeya Maria Jose Valanarasu, Poojan Oza, Vishal M. Patel
- **Comment**: Accepted at ICIP 2022
- **Journal**: None
- **Summary**: In image fusion, images obtained from different sensors are fused to generate a single image with enhanced information. In recent years, state-of-the-art methods have adopted Convolution Neural Networks (CNNs) to encode meaningful features for image fusion. Specifically, CNN-based methods perform image fusion by fusing local features. However, they do not consider long-range dependencies that are present in the image. Transformer-based models are designed to overcome this by modeling the long-range dependencies with the help of self-attention mechanism. This motivates us to propose a novel Image Fusion Transformer (IFT) where we develop a transformer-based multi-scale fusion strategy that attends to both local and long-range information (or global context). The proposed method follows a two-stage training approach. In the first stage, we train an auto-encoder to extract deep features at multiple scales. In the second stage, multi-scale features are fused using a Spatio-Transformer (ST) fusion strategy. The ST fusion blocks are comprised of a CNN and a transformer branch which capture local and long-range features, respectively. Extensive experiments on multiple benchmark datasets show that the proposed method performs better than many competitive fusion algorithms. Furthermore, we show the effectiveness of the proposed ST fusion strategy with an ablation analysis. The source code is available at: https://github.com/Vibashan/Image-Fusion-Transformer.



### Playful Interactions for Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.09046v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.09046v1)
- **Published**: 2021-07-19 17:54:48+00:00
- **Updated**: 2021-07-19 17:54:48+00:00
- **Authors**: Sarah Young, Jyothish Pari, Pieter Abbeel, Lerrel Pinto
- **Comment**: None
- **Journal**: None
- **Summary**: One of the key challenges in visual imitation learning is collecting large amounts of expert demonstrations for a given task. While methods for collecting human demonstrations are becoming easier with teleoperation methods and the use of low-cost assistive tools, we often still require 100-1000 demonstrations for every task to learn a visual representation and policy. To address this, we turn to an alternate form of data that does not require task-specific demonstrations -- play. Playing is a fundamental method children use to learn a set of skills and behaviors and visual representations in early learning. Importantly, play data is diverse, task-agnostic, and relatively cheap to obtain. In this work, we propose to use playful interactions in a self-supervised manner to learn visual representations for downstream tasks. We collect 2 hours of playful data in 19 diverse environments and use self-predictive learning to extract visual representations. Given these representations, we train policies using imitation learning for two downstream tasks: Pushing and Stacking. We demonstrate that our visual representations generalize better than standard behavior cloning and can achieve similar performance with only half the number of required demonstrations. Our representations, which are trained from scratch, compare favorably against ImageNet pretrained representations. Finally, we provide an experimental analysis on the effects of different pretraining modes on downstream task learning.



### Know Thyself: Transferable Visual Control Policies Through Robot-Awareness
- **Arxiv ID**: http://arxiv.org/abs/2107.09047v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.09047v3)
- **Published**: 2021-07-19 17:56:04+00:00
- **Updated**: 2022-10-17 17:55:44+00:00
- **Authors**: Edward S. Hu, Kun Huang, Oleh Rybkin, Dinesh Jayaraman
- **Comment**: Updated to ICLR22 version
- **Journal**: None
- **Summary**: Training visual control policies from scratch on a new robot typically requires generating large amounts of robot-specific data. How might we leverage data previously collected on another robot to reduce or even completely remove this need for robot-specific data? We propose a "robot-aware control" paradigm that achieves this by exploiting readily available knowledge about the robot. We then instantiate this in a robot-aware model-based RL policy by training modular dynamics models that couple a transferable, robot-aware world dynamics module with a robot-specific, potentially analytical, robot dynamics module. This also enables us to set up visual planning costs that separately consider the robot agent and the world. Our experiments on tabletop manipulation tasks with simulated and real robots demonstrate that these plug-in improvements dramatically boost the transferability of visual model-based RL policies, even permitting zero-shot transfer of visual manipulation skills onto new robots. Project website: https://www.seas.upenn.edu/~hued/rac



### Learning a Joint Embedding of Multiple Satellite Sensors: A Case Study for Lake Ice Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2107.09092v2
- **DOI**: 10.1109/TGRS.2022.3211184
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.09092v2)
- **Published**: 2021-07-19 18:11:55+00:00
- **Updated**: 2022-11-29 15:17:32+00:00
- **Authors**: Manu Tom, Yuchang Jiang, Emmanuel Baltsavias, Konrad Schindler
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing 2022
- **Summary**: Fusing satellite imagery acquired with different sensors has been a long-standing challenge of Earth observation, particularly across different modalities such as optical and Synthetic Aperture Radar (SAR) images. Here, we explore the joint analysis of imagery from different sensors in the light of representation learning: we propose to learn a joint embedding of multiple satellite sensors within a deep neural network. Our application problem is the monitoring of lake ice on Alpine lakes. To reach the temporal resolution requirement of the Swiss Global Climate Observing System (GCOS) office, we combine three image sources: Sentinel-1 SAR (S1-SAR), Terra MODIS, and Suomi-NPP VIIRS. The large gaps between the optical and SAR domains and between the sensor resolutions make this a challenging instance of the sensor fusion problem. Our approach can be classified as a late fusion that is learned in a data-driven manner. The proposed network architecture has separate encoding branches for each image sensor, which feed into a single latent embedding. I.e., a common feature representation shared by all inputs, such that subsequent processing steps deliver comparable output irrespective of which sort of input image was used. By fusing satellite data, we map lake ice at a temporal resolution of < 1.5 days. The network produces spatially explicit lake ice maps with pixel-wise accuracies > 91% (respectively, mIoU scores > 60%) and generalises well across different lakes and winters. Moreover, it sets a new state-of-the-art for determining the important ice-on and ice-off dates for the target lakes, in many cases meeting the GCOS requirement.



### Accelerating deep neural networks for efficient scene understanding in automotive cyber-physical systems
- **Arxiv ID**: http://arxiv.org/abs/2107.09101v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.09101v1)
- **Published**: 2021-07-19 18:43:17+00:00
- **Updated**: 2021-07-19 18:43:17+00:00
- **Authors**: Stavros Nousias, Erion-Vasilis Pikoulis, Christos Mavrokefalidis, Aris S. Lalos
- **Comment**: None
- **Journal**: None
- **Summary**: Automotive Cyber-Physical Systems (ACPS) have attracted a significant amount of interest in the past few decades, while one of the most critical operations in these systems is the perception of the environment. Deep learning and, especially, the use of Deep Neural Networks (DNNs) provides impressive results in analyzing and understanding complex and dynamic scenes from visual data. The prediction horizons for those perception systems are very short and inference must often be performed in real time, stressing the need of transforming the original large pre-trained networks into new smaller models, by utilizing Model Compression and Acceleration (MCA) techniques. Our goal in this work is to investigate best practices for appropriately applying novel weight sharing techniques, optimizing the available variables and the training procedures towards the significant acceleration of widely adopted DNNs. Extensive evaluation studies carried out using various state-of-the-art DNN models in object detection and tracking experiments, provide details about the type of errors that manifest after the application of weight sharing techniques, resulting in significant acceleration gains with negligible accuracy losses.



### Separating Skills and Concepts for Novel Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2107.09106v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.09106v1)
- **Published**: 2021-07-19 18:55:10+00:00
- **Updated**: 2021-07-19 18:55:10+00:00
- **Authors**: Spencer Whitehead, Hui Wu, Heng Ji, Rogerio Feris, Kate Saenko
- **Comment**: Paper at CVPR 2021. 14 pages, 7 figures
- **Journal**: None
- **Summary**: Generalization to out-of-distribution data has been a problem for Visual Question Answering (VQA) models. To measure generalization to novel questions, we propose to separate them into "skills" and "concepts". "Skills" are visual tasks, such as counting or attribute recognition, and are applied to "concepts" mentioned in the question, such as objects and people. VQA methods should be able to compose skills and concepts in novel ways, regardless of whether the specific composition has been seen in training, yet we demonstrate that existing models have much to improve upon towards handling new compositions. We present a novel method for learning to compose skills and concepts that separates these two factors implicitly within a model by learning grounded concept representations and disentangling the encoding of skills from that of concepts. We enforce these properties with a novel contrastive learning procedure that does not rely on external annotations and can be learned from unlabeled image-question pairs. Experiments demonstrate the effectiveness of our approach for improving compositional and grounding performance.



### Confidence Aware Neural Networks for Skin Cancer Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.09118v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.09118v2)
- **Published**: 2021-07-19 19:21:57+00:00
- **Updated**: 2021-07-24 15:20:54+00:00
- **Authors**: Donya Khaledyan, AmirReza Tajally, Ali Sarkhosh, Afshar Shamsi, Hamzeh Asgharnezhad, Abbas Khosravi, Saeid Nahavandi
- **Comment**: 21 Pages, 7 Figures, 2 Tables
- **Journal**: None
- **Summary**: Deep learning (DL) models have received particular attention in medical imaging due to their promising pattern recognition capabilities. However, Deep Neural Networks (DNNs) require a huge amount of data, and because of the lack of sufficient data in this field, transfer learning can be a great solution. DNNs used for disease diagnosis meticulously concentrate on improving the accuracy of predictions without providing a figure about their confidence of predictions. Knowing how much a DNN model is confident in a computer-aided diagnosis model is necessary for gaining clinicians' confidence and trust in DL-based solutions. To address this issue, this work presents three different methods for quantifying uncertainties for skin cancer detection from images. It also comprehensively evaluates and compares performance of these DNNs using novel uncertainty-related metrics. The obtained results reveal that the predictive uncertainty estimation methods are capable of flagging risky and erroneous predictions with a high uncertainty estimate. We also demonstrate that ensemble approaches are more reliable in capturing uncertainties through inference.



### Examining the Human Perceptibility of Black-Box Adversarial Attacks on Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.09126v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.09126v1)
- **Published**: 2021-07-19 19:45:44+00:00
- **Updated**: 2021-07-19 19:45:44+00:00
- **Authors**: Benjamin Spetter-Goldstein, Nataniel Ruiz, Sarah Adel Bargal
- **Comment**: 5 pages, 5 figures, submitted to AdvML @ ICML 2021
- **Journal**: None
- **Summary**: The modern open internet contains billions of public images of human faces across the web, especially on social media websites used by half the world's population. In this context, Face Recognition (FR) systems have the potential to match faces to specific names and identities, creating glaring privacy concerns. Adversarial attacks are a promising way to grant users privacy from FR systems by disrupting their capability to recognize faces. Yet, such attacks can be perceptible to human observers, especially under the more challenging black-box threat model. In the literature, the justification for the imperceptibility of such attacks hinges on bounding metrics such as $\ell_p$ norms. However, there is not much research on how these norms match up with human perception. Through examining and measuring both the effectiveness of recent black-box attacks in the face recognition setting and their corresponding human perceptibility through survey data, we demonstrate the trade-offs in perceptibility that occur as attacks become more aggressive. We also show how the $\ell_2$ norm and other metrics do not correlate with human perceptibility in a linear fashion, thus making these norms suboptimal at measuring adversarial attack perceptibility.



### Convolutional module for heart localization and segmentation in MRI
- **Arxiv ID**: http://arxiv.org/abs/2107.09134v1
- **DOI**: 10.1007/978-3-031-06427-2_37
- **Categories**: **eess.IV**, cs.AI, cs.CV, 68T07, 92C55, 92B20, I.2.10; I.4.6; I.5.1; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2107.09134v1)
- **Published**: 2021-07-19 20:19:40+00:00
- **Updated**: 2021-07-19 20:19:40+00:00
- **Authors**: Daniel Lima, Catharine Graves, Marco Gutierrez, Bruno Brandoli, Jose Rodrigues-Jr
- **Comment**: Submitted to CMIG
- **Journal**: ICIAP 2022, Part I, LNCS 13231
- **Summary**: Magnetic resonance imaging (MRI) is a widely known medical imaging technique used to assess the heart function. Deep learning (DL) models perform several tasks in cardiac MRI (CMR) images with good efficacy, such as segmentation, estimation, and detection of diseases. Many DL models based on convolutional neural networks (CNN) were improved by detecting regions-of-interest (ROI) either automatically or by hand. In this paper we describe Visual-Motion-Focus (VMF), a module that detects the heart motion in the 4D MRI sequence, and highlights ROIs by focusing a Radial Basis Function (RBF) on the estimated motion field. We experimented and evaluated VMF on three CMR datasets, observing that the proposed ROIs cover 99.7% of data labels (Recall score), improved the CNN segmentation (mean Dice score) by 1.7 (p < .001) after the ROI extraction, and improved the overall training speed by 2.5 times (+150%).



### Quality and Complexity Assessment of Learning-Based Image Compression Solutions
- **Arxiv ID**: http://arxiv.org/abs/2107.09136v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.09136v1)
- **Published**: 2021-07-19 20:20:38+00:00
- **Updated**: 2021-07-19 20:20:38+00:00
- **Authors**: João Dick, Brunno Abreu, Mateus Grellert, Sergio Bampi
- **Comment**: Paper accepted at ICIP2021
- **Journal**: None
- **Summary**: This work presents an analysis of state-of-the-art learning-based image compression techniques. We compare 8 models available in the Tensorflow Compression package in terms of visual quality metrics and processing time, using the KODAK data set. The results are compared with the Better Portable Graphics (BPG) and the JPEG2000 codecs. Results show that JPEG2000 has the lowest execution times compared with the fastest learning-based model, with a speedup of 1.46x in compression and 30x in decompression. However, the learning-based models achieved improvements over JPEG2000 in terms of quality, specially for lower bitrates. Our findings also show that BPG is more efficient in terms of PSNR, but the learning models are better for other quality metrics, and sometimes even faster. The results indicate that learning-based techniques are promising solutions towards a future mainstream compression method.



### DeepSocNav: Social Navigation by Imitating Human Behaviors
- **Arxiv ID**: http://arxiv.org/abs/2107.09170v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.09170v1)
- **Published**: 2021-07-19 21:51:06+00:00
- **Updated**: 2021-07-19 21:51:06+00:00
- **Authors**: Juan Pablo de Vicente, Alvaro Soto
- **Comment**: 6 pages, Accepted paper at the RSS Workshop on Social Robot
  Navigation 2021
- **Journal**: None
- **Summary**: Current datasets to train social behaviors are usually borrowed from surveillance applications that capture visual data from a bird's-eye perspective. This leaves aside precious relationships and visual cues that could be captured through a first-person view of a scene. In this work, we propose a strategy to exploit the power of current game engines, such as Unity, to transform pre-existing bird's-eye view datasets into a first-person view, in particular, a depth view. Using this strategy, we are able to generate large volumes of synthetic data that can be used to pre-train a social navigation model. To test our ideas, we present DeepSocNav, a deep learning based model that takes advantage of the proposed approach to generate synthetic data. Furthermore, DeepSocNav includes a self-supervised strategy that is included as an auxiliary task. This consists of predicting the next depth frame that the agent will face. Our experiments show the benefits of the proposed model that is able to outperform relevant baselines in terms of social navigation scores.



### OSLO: On-the-Sphere Learning for Omnidirectional images and its application to 360-degree image compression
- **Arxiv ID**: http://arxiv.org/abs/2107.09179v2
- **DOI**: 10.1109/TIP.2022.3202357
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.09179v2)
- **Published**: 2021-07-19 22:14:30+00:00
- **Updated**: 2022-08-21 21:03:59+00:00
- **Authors**: Navid Mahmoudian Bidgoli, Roberto G. de A. Azevedo, Thomas Maugey, Aline Roumy, Pascal Frossard
- **Comment**: Accepted for publication in IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: State-of-the-art 2D image compression schemes rely on the power of convolutional neural networks (CNNs). Although CNNs offer promising perspectives for 2D image compression, extending such models to omnidirectional images is not straightforward. First, omnidirectional images have specific spatial and statistical properties that can not be fully captured by current CNN models. Second, basic mathematical operations composing a CNN architecture, e.g., translation and sampling, are not well-defined on the sphere. In this paper, we study the learning of representation models for omnidirectional images and propose to use the properties of HEALPix uniform sampling of the sphere to redefine the mathematical tools used in deep learning models for omnidirectional images. In particular, we: i) propose the definition of a new convolution operation on the sphere that keeps the high expressiveness and the low complexity of a classical 2D convolution; ii) adapt standard CNN techniques such as stride, iterative aggregation, and pixel shuffling to the spherical domain; and then iii) apply our new framework to the task of omnidirectional image compression. Our experiments show that our proposed on-the-sphere solution leads to a better compression gain that can save 13.7% of the bit rate compared to similar learned models applied to equirectangular images. Also, compared to learning models based on graph convolutional networks, our solution supports more expressive filters that can preserve high frequencies and provide a better perceptual quality of the compressed images. Such results demonstrate the efficiency of the proposed framework, which opens new research venues for other omnidirectional vision tasks to be effectively implemented on the sphere manifold.



### Directly Training Joint Energy-Based Models for Conditional Synthesis and Calibrated Prediction of Multi-Attribute Data
- **Arxiv ID**: http://arxiv.org/abs/2108.04227v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.04227v1)
- **Published**: 2021-07-19 22:19:41+00:00
- **Updated**: 2021-07-19 22:19:41+00:00
- **Authors**: Jacob Kelly, Richard Zemel, Will Grathwohl
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-attribute classification generalizes classification, presenting new challenges for making accurate predictions and quantifying uncertainty. We build upon recent work and show that architectures for multi-attribute prediction can be reinterpreted as energy-based models (EBMs). While existing EBM approaches achieve strong discriminative performance, they are unable to generate samples conditioned on novel attribute combinations. We propose a simple extension which expands the capabilities of EBMs to generating accurate conditional samples. Our approach, combined with newly developed techniques in energy-based model training, allows us to directly maximize the likelihood of data and labels under the unnormalized joint distribution. We evaluate our proposed approach on high-dimensional image data with high-dimensional binary attribute labels. We find our models are capable of both accurate, calibrated predictions and high-quality conditional synthesis of novel attribute combinations.



