# Arxiv Papers in cs.CV on 2021-07-11
### Aligning Correlation Information for Domain Adaptation in Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.04932v2
- **DOI**: 10.1109/TNNLS.2022.3212909
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.04932v2)
- **Published**: 2021-07-11 00:13:36+00:00
- **Updated**: 2022-12-08 11:13:12+00:00
- **Authors**: Yuecong Xu, Jianfei Yang, Haozhi Cao, Kezhi Mao, Jianxiong Yin, Simon See
- **Comment**: The dataset HMDB-ARID is available at
  https://xuyu0010.github.io/vuda.html.Camera-ready version of this paper
  accepted at IEEE TNNLS. Correction made for Figure 1 of the Camera-ready
  version
- **Journal**: None
- **Summary**: Domain adaptation (DA) approaches address domain shift and enable networks to be applied to different scenarios. Although various image DA approaches have been proposed in recent years, there is limited research towards video DA. This is partly due to the complexity in adapting the different modalities of features in videos, which includes the correlation features extracted as long-term dependencies of pixels across spatiotemporal dimensions. The correlation features are highly associated with action classes and proven their effectiveness in accurate video feature extraction through the supervised action recognition task. Yet correlation features of the same action would differ across domains due to domain shift. Therefore we propose a novel Adversarial Correlation Adaptation Network (ACAN) to align action videos by aligning pixel correlations. ACAN aims to minimize the distribution of correlation information, termed as Pixel Correlation Discrepancy (PCD). Additionally, video DA research is also limited by the lack of cross-domain video datasets with larger domain shifts. We, therefore, introduce a novel HMDB-ARID dataset with a larger domain shift caused by a larger statistical difference between domains. This dataset is built in an effort to leverage current datasets for dark video classification. Empirical results demonstrate the state-of-the-art performance of our proposed ACAN for both existing and the new video DA datasets.



### A Spatial Guided Self-supervised Clustering Network for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.04934v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04934v1)
- **Published**: 2021-07-11 00:40:40+00:00
- **Updated**: 2021-07-11 00:40:40+00:00
- **Authors**: Euijoon Ahn, Dagan Feng, Jinman Kim
- **Comment**: Accepted at Medical Image Computing and Computer Assisted
  Interventions (MICCAI) 2021
- **Journal**: None
- **Summary**: The segmentation of medical images is a fundamental step in automated clinical decision support systems. Existing medical image segmentation methods based on supervised deep learning, however, remain problematic because of their reliance on large amounts of labelled training data. Although medical imaging data repositories continue to expand, there has not been a commensurate increase in the amount of annotated data. Hence, we propose a new spatial guided self-supervised clustering network (SGSCN) for medical image segmentation, where we introduce multiple loss functions designed to aid in grouping image pixels that are spatially connected and have similar feature representations. It iteratively learns feature representations and clustering assignment of each pixel in an end-to-end fashion from a single image. We also propose a context-based consistency loss that better delineates the shape and boundaries of image regions. It enforces all the pixels belonging to a cluster to be spatially close to the cluster centre. We evaluated our method on 2 public medical image datasets and compared it to existing conventional and self-supervised clustering methods. Experimental results show that our method was most accurate for medical image segmentation.



### BEV-MODNet: Monocular Camera based Bird's Eye View Moving Object Detection for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2107.04937v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.04937v1)
- **Published**: 2021-07-11 01:11:58+00:00
- **Updated**: 2021-07-11 01:11:58+00:00
- **Authors**: Hazem Rashed, Mariam Essam, Maha Mohamed, Ahmad El Sallab, Senthil Yogamani
- **Comment**: Accepted for Oral Presentation at IEEE Intelligent Transportation
  Systems Conference (ITSC) 2021
- **Journal**: None
- **Summary**: Detection of moving objects is a very important task in autonomous driving systems. After the perception phase, motion planning is typically performed in Bird's Eye View (BEV) space. This would require projection of objects detected on the image plane to top view BEV plane. Such a projection is prone to errors due to lack of depth information and noisy mapping in far away areas. CNNs can leverage the global context in the scene to project better. In this work, we explore end-to-end Moving Object Detection (MOD) on the BEV map directly using monocular images as input. To the best of our knowledge, such a dataset does not exist and we create an extended KITTI-raw dataset consisting of 12.9k images with annotations of moving object masks in BEV space for five classes. The dataset is intended to be used for class agnostic motion cue based object detection and classes are provided as meta-data for better tuning. We design and implement a two-stream RGB and optical flow fusion architecture which outputs motion segmentation directly in BEV space. We compare it with inverse perspective mapping of state-of-the-art motion segmentation predictions on the image plane. We observe a significant improvement of 13% in mIoU using the simple baseline implementation. This demonstrates the ability to directly learn motion segmentation output in BEV space. Qualitative results of our baseline and the dataset annotations can be found in https://sites.google.com/view/bev-modnet.



### Deep Fiber Clustering: Anatomically Informed Unsupervised Deep Learning for Fast and Effective White Matter Parcellation
- **Arxiv ID**: http://arxiv.org/abs/2107.04938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04938v1)
- **Published**: 2021-07-11 01:36:57+00:00
- **Updated**: 2021-07-11 01:36:57+00:00
- **Authors**: Yuqian Chen, Chaoyi Zhang, Yang Song, Nikos Makris, Yogesh Rathi, Weidong Cai, Fan Zhang, Lauren J. O'Donnell
- **Comment**: MICCAI 2021
- **Journal**: None
- **Summary**: White matter fiber clustering (WMFC) enables parcellation of white matter tractography for applications such as disease classification and anatomical tract segmentation. However, the lack of ground truth and the ambiguity of fiber data (the points along a fiber can equivalently be represented in forward or reverse order) pose challenges to this task. We propose a novel WMFC framework based on unsupervised deep learning. We solve the unsupervised clustering problem as a self-supervised learning task. Specifically, we use a convolutional neural network to learn embeddings of input fibers, using pairwise fiber distances as pseudo annotations. This enables WMFC that is insensitive to fiber point ordering. In addition, anatomical coherence of fiber clusters is improved by incorporating brain anatomical segmentation data. The proposed framework enables outlier removal in a natural way by rejecting fibers with low cluster assignment probability. We train and evaluate our method using 200 datasets from the Human Connectome Project. Results demonstrate superior performance and efficiency of the proposed approach.



### Partial Video Domain Adaptation with Partial Adversarial Temporal Attentive Network
- **Arxiv ID**: http://arxiv.org/abs/2107.04941v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.04941v1)
- **Published**: 2021-07-11 02:17:29+00:00
- **Updated**: 2021-07-11 02:17:29+00:00
- **Authors**: Yuecong Xu, Jianfei Yang, Haozhi Cao, Qi Li, Kezhi Mao, Zhenghua Chen
- **Comment**: The new datasets for PVDA: HMDB-ARID(partial), MiniKinetics-UCF,
  HMDB-ARID(partial) can be downloaded from
  https://xuyu0010.github.io/pvda.html
- **Journal**: None
- **Summary**: Partial Domain Adaptation (PDA) is a practical and general domain adaptation scenario, which relaxes the fully shared label space assumption such that the source label space subsumes the target one. The key challenge of PDA is the issue of negative transfer caused by source-only classes. For videos, such negative transfer could be triggered by both spatial and temporal features, which leads to a more challenging Partial Video Domain Adaptation (PVDA) problem. In this paper, we propose a novel Partial Adversarial Temporal Attentive Network (PATAN) to address the PVDA problem by utilizing both spatial and temporal features for filtering source-only classes. Besides, PATAN constructs effective overall temporal features by attending to local temporal features that contribute more toward the class filtration process. We further introduce new benchmarks to facilitate research on PVDA problems, covering a wide range of PVDA scenarios. Empirical results demonstrate the state-of-the-art performance of our proposed PATAN across the multiple PVDA benchmarks.



### Deep Geometric Distillation Network for Compressive Sensing MRI
- **Arxiv ID**: http://arxiv.org/abs/2107.04943v2
- **DOI**: 10.1109/BHI50953.2021.9508565
- **Categories**: **eess.IV**, cs.CV, 68T05, 68T20, 68T09, 68W25, F.2.2; I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2107.04943v2)
- **Published**: 2021-07-11 02:24:55+00:00
- **Updated**: 2021-08-27 07:40:50+00:00
- **Authors**: Xiaohong Fan, Yin Yang, Jianping Zhang
- **Comment**: Accepted by IEEE-EMBS International Conference on Biomedical and
  Health Informatics (BHI), 2021
- **Journal**: None
- **Summary**: Compressed sensing (CS) is an efficient method to reconstruct MR image from small sampled data in $k$-space and accelerate the acquisition of MRI. In this work, we propose a novel deep geometric distillation network which combines the merits of model-based and deep learning-based CS-MRI methods, it can be theoretically guaranteed to improve geometric texture details of a linear reconstruction. Firstly, we unfold the model-based CS-MRI optimization problem into two sub-problems that consist of image linear approximation and image geometric compensation. Secondly, geometric compensation sub-problem for distilling lost texture details in approximation stage can be expanded by Taylor expansion to design a geometric distillation module fusing features of different geometric characteristic domains. Additionally, we use a learnable version with adaptive initialization of the step-length parameter, which allows model more flexibility that can lead to convergent smoothly. Numerical experiments verify its superiority over other state-of-the-art CS-MRI reconstruction approaches. The source code will be available at \url{https://github.com/fanxiaohong/Deep-Geometric-Distillation-Network-for-CS-MRI}



### Learn from Anywhere: Rethinking Generalized Zero-Shot Learning with Limited Supervision
- **Arxiv ID**: http://arxiv.org/abs/2107.04952v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.04952v2)
- **Published**: 2021-07-11 03:23:20+00:00
- **Updated**: 2021-07-14 01:28:32+00:00
- **Authors**: Gaurav Bhatt, Shivam Chandhok, Vineeth N Balasubramanian
- **Comment**: Accepted at IJCAI'21 workshop on Weakly Supervised Representation
  Learning
- **Journal**: None
- **Summary**: A common problem with most zero and few-shot learning approaches is they suffer from bias towards seen classes resulting in sub-optimal performance. Existing efforts aim to utilize unlabeled images from unseen classes (i.e transductive zero-shot) during training to enable generalization. However, this limits their use in practical scenarios where data from target unseen classes is unavailable or infeasible to collect. In this work, we present a practical setting of inductive zero and few-shot learning, where unlabeled images from other out-of-data classes, that do not belong to seen or unseen categories, can be used to improve generalization in any-shot learning. We leverage a formulation based on product-of-experts and introduce a new AUD module that enables us to use unlabeled samples from out-of-data classes which are usually easily available and practically entail no annotation cost. In addition, we also demonstrate the applicability of our model to address a more practical and challenging, Generalized Zero-shot under a limited supervision setting, where even base seen classes do not have sufficient annotated samples.



### Non-linear Visual Knowledge Discovery with Elliptic Paired Coordinates
- **Arxiv ID**: http://arxiv.org/abs/2107.04974v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2107.04974v1)
- **Published**: 2021-07-11 05:53:38+00:00
- **Updated**: 2021-07-11 05:53:38+00:00
- **Authors**: Rose McDonald, Boris Kovalerchuk
- **Comment**: 29 pages, 29 figures, 12 tables
- **Journal**: None
- **Summary**: It is challenging for humans to enable visual knowledge discovery in data with more than 2-3 dimensions with a naked eye. This chapter explores the efficiency of discovering predictive machine learning models interactively using new Elliptic Paired coordinates (EPC) visualizations. It is shown that EPC are capable to visualize multidimensional data and support visual machine learning with preservation of multidimensional information in 2-D. Relative to parallel and radial coordinates, EPC visualization requires only a half of the visual elements for each n-D point. An interactive software system EllipseVis, which is developed in this work, processes high-dimensional datasets, creates EPC visualizations, and produces predictive classification models by discovering dominance rules in EPC. By using interactive and automatic processes it discovers zones in EPC with a high dominance of a single class. The EPC methodology has been successful in discovering non-linear predictive models with high coverage and precision in the computational experiments. This can benefit multiple domains by producing visually appealing dominance rules. This chapter presents results of successful testing the EPC non-linear methodology in experiments using real and simulated data, EPC generalized to the Dynamic Elliptic Paired Coordinates (DEPC), incorporation of the weights of coordinates to optimize the visual discovery, introduction of an alternative EPC design and introduction of the concept of incompact machine learning methodology based on EPC/DEPC.



### Leveraging Domain Adaptation for Low-Resource Geospatial Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.04983v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04983v1)
- **Published**: 2021-07-11 06:47:20+00:00
- **Updated**: 2021-07-11 06:47:20+00:00
- **Authors**: Jack Lynch, Sam Wookey
- **Comment**: Tackling Climate Change with Machine Learning Workshop at ICML 2021
- **Journal**: None
- **Summary**: Machine learning in remote sensing has matured alongside a proliferation in availability and resolution of geospatial imagery, but its utility is bottlenecked by the need for labeled data. What's more, many labeled geospatial datasets are specific to certain regions, instruments, or extreme weather events. We investigate the application of modern domain-adaptation to multiple proposed geospatial benchmarks, uncovering unique challenges and proposing solutions to them.



### Prediction Surface Uncertainty Quantification in Object Detection Models for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2107.04991v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.04991v1)
- **Published**: 2021-07-11 08:31:15+00:00
- **Updated**: 2021-07-11 08:31:15+00:00
- **Authors**: Ferhat Ozgur Catak, Tao Yue, Shaukat Ali
- **Comment**: Accepted in AITest 2021, The Third IEEE International Conference On
  Artificial Intelligence Testing
- **Journal**: None
- **Summary**: Object detection in autonomous cars is commonly based on camera images and Lidar inputs, which are often used to train prediction models such as deep artificial neural networks for decision making for object recognition, adjusting speed, etc. A mistake in such decision making can be damaging; thus, it is vital to measure the reliability of decisions made by such prediction models via uncertainty measurement. Uncertainty, in deep learning models, is often measured for classification problems. However, deep learning models in autonomous driving are often multi-output regression models. Hence, we propose a novel method called PURE (Prediction sURface uncErtainty) for measuring prediction uncertainty of such regression models. We formulate the object recognition problem as a regression model with more than one outputs for finding object locations in a 2-dimensional camera view. For evaluation, we modified three widely-applied object recognition models (i.e., YoLo, SSD300 and SSD512) and used the KITTI, Stanford Cars, Berkeley DeepDrive, and NEXET datasets. Results showed the statistically significant negative correlation between prediction surface uncertainty and prediction accuracy suggesting that uncertainty significantly impacts the decisions made by autonomous driving.



### Towards Accurate Localization by Instance Search
- **Arxiv ID**: http://arxiv.org/abs/2107.05005v2
- **DOI**: 10.1145/3474085.3475530
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2107.05005v2)
- **Published**: 2021-07-11 10:03:31+00:00
- **Updated**: 2021-08-07 14:58:34+00:00
- **Authors**: Yi-Geng Hong, Hui-Chu Xiao, Wan-Lei Zhao
- **Comment**: Accepted by ACM MM 2021 as Oral
- **Journal**: None
- **Summary**: Visual object localization is the key step in a series of object detection tasks. In the literature, high localization accuracy is achieved with the mainstream strongly supervised frameworks. However, such methods require object-level annotations and are unable to detect objects of unknown categories. Weakly supervised methods face similar difficulties. In this paper, a self-paced learning framework is proposed to achieve accurate object localization on the rank list returned by instance search. The proposed framework mines the target instance gradually from the queries and their corresponding top-ranked search results. Since a common instance is shared between the query and the images in the rank list, the target visual instance can be accurately localized even without knowing what the object category is. In addition to performing localization on instance search, the issue of few-shot object detection is also addressed under the same framework. Superior performance over state-of-the-art methods is observed on both tasks.



### NeoUNet: Towards accurate colon polyp segmentation and neoplasm detection
- **Arxiv ID**: http://arxiv.org/abs/2107.05023v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.05023v1)
- **Published**: 2021-07-11 11:10:12+00:00
- **Updated**: 2021-07-11 11:10:12+00:00
- **Authors**: Phan Ngoc Lan, Nguyen Sy An, Dao Viet Hang, Dao Van Long, Tran Quang Trung, Nguyen Thi Thuy, Dinh Viet Sang
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic polyp segmentation has proven to be immensely helpful for endoscopy procedures, reducing the missing rate of adenoma detection for endoscopists while increasing efficiency. However, classifying a polyp as being neoplasm or not and segmenting it at the pixel level is still a challenging task for doctors to perform in a limited time. In this work, we propose a fine-grained formulation for the polyp segmentation problem. Our formulation aims to not only segment polyp regions, but also identify those at high risk of malignancy with high accuracy. In addition, we present a UNet-based neural network architecture called NeoUNet, along with a hybrid loss function to solve this problem. Experiments show highly competitive results for NeoUNet on our benchmark dataset compared to existing polyp segmentation models.



### Similarity Guided Deep Face Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2107.05025v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2107.05025v1)
- **Published**: 2021-07-11 11:32:04+00:00
- **Updated**: 2021-07-11 11:32:04+00:00
- **Authors**: Young Kyun Jang, Nam Ik Cho
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: Face image retrieval, which searches for images of the same identity from the query input face image, is drawing more attention as the size of the image database increases rapidly. In order to conduct fast and accurate retrieval, a compact hash code-based methods have been proposed, and recently, deep face image hashing methods with supervised classification training have shown outstanding performance. However, classification-based scheme has a disadvantage in that it cannot reveal complex similarities between face images into the hash code learning. In this paper, we attempt to improve the face image retrieval quality by proposing a Similarity Guided Hashing (SGH) method, which gently considers self and pairwise-similarity simultaneously. SGH employs various data augmentations designed to explore elaborate similarities between face images, solving both intra and inter identity-wise difficulties. Extensive experimental results on the protocols with existing benchmarks and an additionally proposed large scale higher resolution face image dataset demonstrate that our SGH delivers state-of-the-art retrieval performance.



### Semi-Supervised Object Detection with Adaptive Class-Rebalancing Self-Training
- **Arxiv ID**: http://arxiv.org/abs/2107.05031v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.05031v1)
- **Published**: 2021-07-11 12:14:42+00:00
- **Updated**: 2021-07-11 12:14:42+00:00
- **Authors**: Fangyuan Zhang, Tianxiang Pan, Bin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: This study delves into semi-supervised object detection (SSOD) to improve detector performance with additional unlabeled data. State-of-the-art SSOD performance has been achieved recently by self-training, in which training supervision consists of ground truths and pseudo-labels. In current studies, we observe that class imbalance in SSOD severely impedes the effectiveness of self-training. To address the class imbalance, we propose adaptive class-rebalancing self-training (ACRST) with a novel memory module called CropBank. ACRST adaptively rebalances the training data with foreground instances extracted from the CropBank, thereby alleviating the class imbalance. Owing to the high complexity of detection tasks, we observe that both self-training and data-rebalancing suffer from noisy pseudo-labels in SSOD. Therefore, we propose a novel two-stage filtering algorithm to generate accurate pseudo-labels. Our method achieves satisfactory improvements on MS-COCO and VOC benchmarks. When using only 1\% labeled data in MS-COCO, our method achieves 17.02 mAP improvement over supervised baselines, and 5.32 mAP improvement compared with state-of-the-art methods.



### Blending Pruning Criteria for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.05033v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.05033v1)
- **Published**: 2021-07-11 12:34:19+00:00
- **Updated**: 2021-07-11 12:34:19+00:00
- **Authors**: Wei He, Zhongzhan Huang, Mingfu Liang, Senwei Liang, Haizhao Yang
- **Comment**: None
- **Journal**: None
- **Summary**: The advancement of convolutional neural networks (CNNs) on various vision applications has attracted lots of attention. Yet the majority of CNNs are unable to satisfy the strict requirement for real-world deployment. To overcome this, the recent popular network pruning is an effective method to reduce the redundancy of the models. However, the ranking of filters according to their "importance" on different pruning criteria may be inconsistent. One filter could be important according to a certain criterion, while it is unnecessary according to another one, which indicates that each criterion is only a partial view of the comprehensive "importance". From this motivation, we propose a novel framework to integrate the existing filter pruning criteria by exploring the criteria diversity. The proposed framework contains two stages: Criteria Clustering and Filters Importance Calibration. First, we condense the pruning criteria via layerwise clustering based on the rank of "importance" score. Second, within each cluster, we propose a calibration factor to adjust their significance for each selected blending candidates and search for the optimal blending criterion via Evolutionary Algorithm. Quantitative results on the CIFAR-100 and ImageNet benchmarks show that our framework outperforms the state-of-the-art baselines, regrading to the compact model performance after pruning.



### BCNet: A Deep Convolutional Neural Network for Breast Cancer Grading
- **Arxiv ID**: http://arxiv.org/abs/2107.05037v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.05037v1)
- **Published**: 2021-07-11 12:55:33+00:00
- **Updated**: 2021-07-11 12:55:33+00:00
- **Authors**: Pouya Hallaj Zavareh, Atefeh Safayari, Hamidreza Bolhasani
- **Comment**: None
- **Journal**: None
- **Summary**: Breast cancer has become one of the most prevalent cancers by which people all over the world are affected and is posed serious threats to human beings, in a particular woman. In order to provide effective treatment or prevention of this cancer, disease diagnosis in the early stages would be of high importance. There have been various methods to detect this disorder in which using images have to play a dominant role. Deep learning has been recently adopted widely in different areas of science, especially medicine. In breast cancer detection problems, some diverse deep learning techniques have been developed on different datasets and resulted in good accuracy. In this article, we aimed to present a deep neural network model to classify histopathological images from the Databiox image dataset as the first application on this image database. Our proposed model named BCNet has taken advantage of the transfer learning approach in which VGG16 is selected from available pertained models as a feature extractor. Furthermore, to address the problem of insufficient data, we employed the data augmentation technique to expand the input dataset. All implementations in this research, ranging from pre-processing actions to depicting the diagram of the model architecture, have been carried out using tf.keras API. As a consequence of the proposed model execution, the significant validation accuracy of 88% and evaluation accuracy of 72% obtained.



### A Projector-Camera System Using Hybrid Pixels with Projection and Capturing Capabilities
- **Arxiv ID**: http://arxiv.org/abs/2107.05043v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2107.05043v1)
- **Published**: 2021-07-11 13:27:25+00:00
- **Updated**: 2021-07-11 13:27:25+00:00
- **Authors**: Kenta Yamamoto, Daisuke Iwai, Kosuke Sato
- **Comment**: Author's version of a paper published at IDW (International Display
  Workshops) 2020
- **Journal**: In Proceedings of the International Display Workshops, pp.
  655-658, 2020
- **Summary**: We propose a novel projector-camera system (ProCams) in which each pixel has both projection and capturing capabilities. Our proposed ProCams solves the difficulty of obtaining precise pixel correspondence between the projector and the camera. We implemented a proof-of-concept ProCams prototype and demonstrated its applicability to a dynamic projection mapping.



### One Map Does Not Fit All: Evaluating Saliency Map Explanation on Multi-Modal Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2107.05047v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.05047v1)
- **Published**: 2021-07-11 13:43:02+00:00
- **Updated**: 2021-07-11 13:43:02+00:00
- **Authors**: Weina Jin, Xiaoxiao Li, Ghassan Hamarneh
- **Comment**: None
- **Journal**: None
- **Summary**: Being able to explain the prediction to clinical end-users is a necessity to leverage the power of AI models for clinical decision support. For medical images, saliency maps are the most common form of explanation. The maps highlight important features for AI model's prediction. Although many saliency map methods have been proposed, it is unknown how well they perform on explaining decisions on multi-modal medical images, where each modality/channel carries distinct clinical meanings of the same underlying biomedical phenomenon. Understanding such modality-dependent features is essential for clinical users' interpretation of AI decisions. To tackle this clinically important but technically ignored problem, we propose the MSFI (Modality-Specific Feature Importance) metric to examine whether saliency maps can highlight modality-specific important features. MSFI encodes the clinical requirements on modality prioritization and modality-specific feature localization. Our evaluations on 16 commonly used saliency map methods, including a clinician user study, show that although most saliency map methods captured modality importance information in general, most of them failed to highlight modality-specific important features consistently and precisely. The evaluation results guide the choices of saliency map methods and provide insights to propose new ones targeting clinical applications.



### Contrast R-CNN for Continual Learning in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2108.04224v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04224v1)
- **Published**: 2021-07-11 14:09:10+00:00
- **Updated**: 2021-07-11 14:09:10+00:00
- **Authors**: Kai Zheng, Cen Chen
- **Comment**: MSc Thesis
- **Journal**: None
- **Summary**: The continual learning problem has been widely studied in image classification, while rare work has been explored in object detection. Some recent works apply knowledge distillation to constrain the model to retain old knowledge, but this rigid constraint is detrimental for learning new knowledge. In our paper, we propose a new scheme for continual learning of object detection, namely Contrast R-CNN, an approach strikes a balance between retaining the old knowledge and learning the new knowledge. Furthermore, we design a Proposal Contrast to eliminate the ambiguity between old and new instance to make the continual learning more robust. Extensive evaluation on the PASCAL VOC dataset demonstrates the effectiveness of our approach.



### Locality Relationship Constrained Multi-view Clustering Framework
- **Arxiv ID**: http://arxiv.org/abs/2107.05073v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.05073v1)
- **Published**: 2021-07-11 15:45:10+00:00
- **Updated**: 2021-07-11 15:45:10+00:00
- **Authors**: Xiangzhu Meng, Wei Wei, Wenzhe Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In most practical applications, it's common to utilize multiple features from different views to represent one object. Among these works, multi-view subspace-based clustering has gained extensive attention from many researchers, which aims to provide clustering solutions to multi-view data. However, most existing methods fail to take full use of the locality geometric structure and similarity relationship among samples under the multi-view scenario. To solve these issues, we propose a novel multi-view learning method with locality relationship constraint to explore the problem of multi-view clustering, called Locality Relationship Constrained Multi-view Clustering Framework (LRC-MCF). LRC-MCF aims to explore the diversity, geometric, consensus and complementary information among different views, by capturing the locality relationship information and the common similarity relationships among multiple views. Moreover, LRC-MCF takes sufficient consideration to weights of different views in finding the common-view locality structure and straightforwardly produce the final clusters. To effectually reduce the redundancy of the learned representations, the low-rank constraint on the common similarity matrix is considered additionally. To solve the minimization problem of LRC-MCF, an Alternating Direction Minimization (ADM) method is provided to iteratively calculate all variables LRC-MCF. Extensive experimental results on seven benchmark multi-view datasets validate the effectiveness of the LRC-MCF method.



### A Cloud-Edge-Terminal Collaborative System for Temperature Measurement in COVID-19 Prevention
- **Arxiv ID**: http://arxiv.org/abs/2107.05078v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2107.05078v1)
- **Published**: 2021-07-11 16:15:15+00:00
- **Updated**: 2021-07-11 16:15:15+00:00
- **Authors**: Zheyi Ma, Hao Li, Wen Fang, Qingwen Liu, Bin Zhou, Zhiyong Bu
- **Comment**: 6 pages, 8 figures, INFOCOMW ICCN 2021
- **Journal**: None
- **Summary**: To prevent the spread of coronavirus disease 2019 (COVID-19), preliminary temperature measurement and mask detection in public areas are conducted. However, the existing temperature measurement methods face the problems of safety and deployment. In this paper, to realize safe and accurate temperature measurement even when a person's face is partially obscured, we propose a cloud-edge-terminal collaborative system with a lightweight infrared temperature measurement model. A binocular camera with an RGB lens and a thermal lens is utilized to simultaneously capture image pairs. Then, a mobile detection model based on a multi-task cascaded convolutional network (MTCNN) is proposed to realize face alignment and mask detection on the RGB images. For accurate temperature measurement, we transform the facial landmarks on the RGB images to the thermal images by an affine transformation and select a more accurate temperature measurement area on the forehead. The collected information is uploaded to the cloud in real time for COVID-19 prevention. Experiments show that the detection model is only 6.1M and the average detection speed is 257ms. At a distance of 1m, the error of indoor temperature measurement is about 3%. That is, the proposed system can realize real-time temperature measurement in public areas.



### Zero-Shot Scene Graph Relation Prediction through Commonsense Knowledge Integration
- **Arxiv ID**: http://arxiv.org/abs/2107.05080v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4.8; I.2.4; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2107.05080v1)
- **Published**: 2021-07-11 16:22:45+00:00
- **Updated**: 2021-07-11 16:22:45+00:00
- **Authors**: Xuan Kan, Hejie Cui, Carl Yang
- **Comment**: This paper has been accepted for presentation in the Research Track
  of ECML-PKDD 2021
- **Journal**: None
- **Summary**: Relation prediction among entities in images is an important step in scene graph generation (SGG), which further impacts various visual understanding and reasoning tasks. Existing SGG frameworks, however, require heavy training yet are incapable of modeling unseen (i.e.,zero-shot) triplets. In this work, we stress that such incapability is due to the lack of commonsense reasoning,i.e., the ability to associate similar entities and infer similar relations based on general understanding of the world. To fill this gap, we propose CommOnsense-integrAted sCenegrapHrElation pRediction (COACHER), a framework to integrate commonsense knowledge for SGG, especially for zero-shot relation prediction. Specifically, we develop novel graph mining pipelines to model the neighborhoods and paths around entities in an external commonsense knowledge graph, and integrate them on top of state-of-the-art SGG frameworks. Extensive quantitative evaluations and qualitative case studies on both original and manipulated datasets from Visual Genome demonstrate the effectiveness of our proposed approach.



### Effect of Input Size on the Classification of Lung Nodules Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.05085v1
- **DOI**: 10.1109/SIU.2018.8404659
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.05085v1)
- **Published**: 2021-07-11 16:52:30+00:00
- **Updated**: 2021-07-11 16:52:30+00:00
- **Authors**: Gorkem Polat, Yesim Dogrusoz Serinagaoglu, Ugur Halici
- **Comment**: 4 pages, in Turkish language, 2018 26th Signal Processing and
  Communications Applications Conference (SIU)
- **Journal**: None
- **Summary**: Recent studies have shown that lung cancer screening using annual low-dose computed tomography (CT) reduces lung cancer mortality by 20% compared to traditional chest radiography. Therefore, CT lung screening has started to be used widely all across the world. However, analyzing these images is a serious burden for radiologists. The number of slices in a CT scan can be up to 600. Therefore, computer-aided-detection (CAD) systems are very important for faster and more accurate assessment of the data. In this study, we proposed a framework that analyzes CT lung screenings using convolutional neural networks (CNNs) to reduce false positives. We trained our model with different volume sizes and showed that volume size plays a critical role in the performance of the system. We also used different fusions in order to show their power and effect on the overall accuracy. 3D CNNs were preferred over 2D CNNs because 2D convolutional operations applied to 3D data could result in information loss. The proposed framework has been tested on the dataset provided by the LUNA16 Challenge and resulted in a sensitivity of 0.831 at 1 false positive per scan.



### Remote Blood Oxygen Estimation From Videos Using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.05087v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.05087v2)
- **Published**: 2021-07-11 16:59:49+00:00
- **Updated**: 2022-05-05 21:09:44+00:00
- **Authors**: Joshua Mathew, Xin Tian, Min Wu, Chau-Wai Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory functionality and is receiving increasing attention during the COVID-19 pandemic. Clinical findings show that it is possible for COVID-19 patients to have significantly low SpO$_2$ before any obvious symptoms. The prevalence of cameras has motivated researchers to investigate methods for monitoring SpO$_2$ using videos. Most prior schemes involving smartphones are contact-based: They require a fingertip to cover the phone's camera and the nearby light source to capture re-emitted light from the illuminated tissue. In this paper, we propose the first convolutional neural network based noncontact SpO$_2$ estimation scheme using smartphone cameras. The scheme analyzes the videos of a participant's hand for physiological sensing, which is convenient and comfortable, and can protect their privacy and allow for keeping face masks on. We design our neural network architectures inspired by the optophysiological models for SpO$_2$ measurement and demonstrate the explainability by visualizing the weights for channel combination. Our proposed models outperform the state-of-the-art model that is designed for contact-based SpO$_2$ measurement, showing the potential of our proposed method to contribute to public health. We also analyze the impact of skin type and the side of a hand on SpO$_2$ estimation performance.



### SE-PSNet: Silhouette-based Enhancement Feature for Panoptic Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/2107.05093v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.05093v1)
- **Published**: 2021-07-11 17:20:32+00:00
- **Updated**: 2021-07-11 17:20:32+00:00
- **Authors**: Shuo-En Chang, Yi-Cheng Yang, En-Ting Lin, Pei-Yung Hsiao, Li-Chen Fu
- **Comment**: Technical report
- **Journal**: None
- **Summary**: Recently, there has been a panoptic segmentation task combining semantic and instance segmentation, in which the goal is to classify each pixel with the corresponding instance ID. In this work, we propose a solution to tackle the panoptic segmentation task. The overall structure combines the bottom-up method and the top-down method. Therefore, not only can there be better performance, but also the execution speed can be maintained. The network mainly pays attention to the quality of the mask. In the previous work, we can see that the uneven contour of the object is more likely to appear, resulting in low-quality prediction. Accordingly, we propose enhancement features and corresponding loss functions for the silhouette of objects and backgrounds to improve the mask. Meanwhile, we use the new proposed confidence score to solve the occlusion problem and make the network tend to use higher quality masks as prediction results. To verify our research, we used the COCO dataset and CityScapes dataset to do experiments and obtained competitive results with fast inference time.



### BrainNNExplainer: An Interpretable Graph Neural Network Framework for Brain Network based Disease Analysis
- **Arxiv ID**: http://arxiv.org/abs/2107.05097v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, q-bio.NC, 68T07, 68T45, 68T20, I.2.6; I.2.10; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2107.05097v1)
- **Published**: 2021-07-11 17:33:02+00:00
- **Updated**: 2021-07-11 17:33:02+00:00
- **Authors**: Hejie Cui, Wei Dai, Yanqiao Zhu, Xiaoxiao Li, Lifang He, Carl Yang
- **Comment**: This paper has been accepted to ICML 2021 Workshop on Interpretable
  Machine Learning in Healthcare
- **Journal**: None
- **Summary**: Interpretable brain network models for disease prediction are of great value for the advancement of neuroscience. GNNs are promising to model complicated network data, but they are prone to overfitting and suffer from poor interpretability, which prevents their usage in decision-critical scenarios like healthcare. To bridge this gap, we propose BrainNNExplainer, an interpretable GNN framework for brain network analysis. It is mainly composed of two jointly learned modules: a backbone prediction model that is specifically designed for brain networks and an explanation generator that highlights disease-specific prominent brain network connections. Extensive experimental results with visualizations on two challenging disease prediction datasets demonstrate the unique interpretability and outstanding performance of BrainNNExplainer.



### LiveView: Dynamic Target-Centered MPI for View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2107.05113v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05113v1)
- **Published**: 2021-07-11 19:01:56+00:00
- **Updated**: 2021-07-11 19:01:56+00:00
- **Authors**: Sushobhan Ghosh, Zhaoyang Lv, Nathan Matsuda, Lei Xiao, Andrew Berkovich, Oliver Cossairt
- **Comment**: None
- **Journal**: None
- **Summary**: Existing Multi-Plane Image (MPI) based view-synthesis methods generate an MPI aligned with the input view using a fixed number of planes in one forward pass. These methods produce fast, high-quality rendering of novel views, but rely on slow and computationally expensive MPI generation methods unsuitable for real-time applications. In addition, most MPI techniques use fixed depth/disparity planes which cannot be modified once the training is complete, hence offering very little flexibility at run-time.   We propose LiveView - a novel MPI generation and rendering technique that produces high-quality view synthesis in real-time. Our method can also offer the flexibility to select scene-dependent MPI planes (number of planes and spacing between them) at run-time. LiveView first warps input images to target view (target-centered) and then learns to generate a target view centered MPI, one depth plane at a time (dynamically). The method generates high-quality renderings, while also enabling fast MPI generation and novel view synthesis. As a result, LiveView enables real-time view synthesis applications where an MPI needs to be updated frequently based on a video stream of input views. We demonstrate that LiveView improves the quality of view synthesis while being 70 times faster at run-time compared to state-of-the-art MPI-based methods.



### Details Preserving Deep Collaborative Filtering-Based Method for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2107.05115v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.05115v2)
- **Published**: 2021-07-11 19:02:36+00:00
- **Updated**: 2021-07-14 16:53:08+00:00
- **Authors**: Basit O. Alawode, Mudassir Masood, Tarig Ballal, Tareq Al-Naffouri
- **Comment**: None
- **Journal**: None
- **Summary**: In spite of the improvements achieved by the several denoising algorithms over the years, many of them still fail at preserving the fine details of the image after denoising. This is as a result of the smooth-out effect they have on the images. Most neural network-based algorithms have achieved better quantitative performance than the classical denoising algorithms. However, they also suffer from qualitative (visual) performance as a result of the smooth-out effect. In this paper, we propose an algorithm to address this shortcoming. We propose a deep collaborative filtering-based (Deep-CoFiB) algorithm for image denoising. This algorithm performs collaborative denoising of image patches in the sparse domain using a set of optimized neural network models. This results in a fast algorithm that is able to excellently obtain a trade-off between noise removal and details preservation. Extensive experiments show that the DeepCoFiB performed quantitatively (in terms of PSNR and SSIM) and qualitatively (visually) better than many of the state-of-the-art denoising algorithms.



### eGHWT: The Extended Generalized Haar-Walsh Transform
- **Arxiv ID**: http://arxiv.org/abs/2107.05121v3
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.IT, cs.NA, math.CO, math.IT, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2107.05121v3)
- **Published**: 2021-07-11 19:39:19+00:00
- **Updated**: 2021-12-26 11:29:45+00:00
- **Authors**: Naoki Saito, Yiqun Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Extending computational harmonic analysis tools from the classical setting of regular lattices to the more general setting of graphs and networks is very important and much research has been done recently. The Generalized Haar-Walsh Transform (GHWT) developed by Irion and Saito (2014) is a multiscale transform for signals on graphs, which is a generalization of the classical Haar and Walsh-Hadamard Transforms. We propose the extended Generalized Haar-Walsh Transform (eGHWT), which is a generalization of the adapted time-frequency tilings of Thiele and Villemoes (1996). The eGHWT examines not only the efficiency of graph-domain partitions but also that of "sequency-domain" partitions simultaneously. Consequently, the eGHWT and its associated best-basis selection algorithm for graph signals significantly improve the performance of the previous GHWT with the similar computational cost, $O(N \log N)$, where $N$ is the number of nodes of an input graph. While the GHWT best-basis algorithm seeks the most suitable orthonormal basis for a given task among more than $(1.5)^N$ possible orthonormal bases in $\mathbb{R}^N$, the eGHWT best-basis algorithm can find a better one by searching through more than $0.618\cdot(1.84)^N$ possible orthonormal bases in $\mathbb{R}^N$. This article describes the details of the eGHWT best-basis algorithm and demonstrates its superiority using several examples including genuine graph signals as well as conventional digital images viewed as graph signals. Furthermore, we also show how the eGHWT can be extended to 2D signals and matrix-form data by viewing them as a tensor product of graphs generated from their columns and rows and demonstrate its effectiveness on applications such as image approximation.



### Interpretable Deep Feature Propagation for Early Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.05122v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05122v1)
- **Published**: 2021-07-11 19:40:19+00:00
- **Updated**: 2021-07-11 19:40:19+00:00
- **Authors**: He Zhao, Richard P. Wildes
- **Comment**: None
- **Journal**: None
- **Summary**: Early action recognition (action prediction) from limited preliminary observations plays a critical role for streaming vision systems that demand real-time inference, as video actions often possess elongated temporal spans which cause undesired latency. In this study, we address action prediction by investigating how action patterns evolve over time in a spatial feature space. There are three key components to our system. First, we work with intermediate-layer ConvNet features, which allow for abstraction from raw data, while retaining spatial layout. Second, instead of propagating features per se, we propagate their residuals across time, which allows for a compact representation that reduces redundancy. Third, we employ a Kalman filter to combat error build-up and unify across prediction start times. Extensive experimental results on multiple benchmarks show that our approach leads to competitive performance in action prediction. Notably, we investigate the learned components of our system to shed light on their otherwise opaque natures in two ways. First, we document that our learned feature propagation module works as a spatial shifting mechanism under convolution to propagate current observations into the future. Thus, it captures flow-based image motion information. Second, the learned Kalman filter adaptively updates prior estimation to aid the sequence learning process.



### Review of Video Predictive Understanding: Early Action Recognition and Future Action Prediction
- **Arxiv ID**: http://arxiv.org/abs/2107.05140v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05140v2)
- **Published**: 2021-07-11 22:46:52+00:00
- **Updated**: 2021-07-16 19:34:34+00:00
- **Authors**: He Zhao, Richard P. Wildes
- **Comment**: None
- **Journal**: None
- **Summary**: Video predictive understanding encompasses a wide range of efforts that are concerned with the anticipation of the unobserved future from the current as well as historical video observations. Action prediction is a major sub-area of video predictive understanding and is the focus of this review. This sub-area has two major subdivisions: early action recognition and future action prediction. Early action recognition is concerned with recognizing an ongoing action as soon as possible. Future action prediction is concerned with the anticipation of actions that follow those previously observed. In either case, the \textbf{\textit{causal}} relationship between the past, current, and potential future information is the main focus. Various mathematical tools such as Markov Chains, Gaussian Processes, Auto-Regressive modeling, and Bayesian recursive filtering are widely adopted jointly with computer vision techniques for these two tasks. However, these approaches face challenges such as the curse of dimensionality, poor generalization, and constraints from domain-specific knowledge. Recently, structures that rely on deep convolutional neural networks and recurrent neural networks have been extensively proposed for improving the performance of existing vision tasks, in general, and action prediction tasks, in particular. However, they have their own shortcomings, \eg reliance on massive training data and lack of strong theoretical underpinnings. In this survey, we start by introducing the major sub-areas of the broad area of video predictive understanding, which recently have received intensive attention and proven to have practical value. Next, a thorough review of various early action recognition and future action prediction algorithms are provided with suitably organized divisions. Finally, we conclude our discussion with future research directions.



### CFTrack: Center-based Radar and Camera Fusion for 3D Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2107.05150v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.05150v1)
- **Published**: 2021-07-11 23:56:53+00:00
- **Updated**: 2021-07-11 23:56:53+00:00
- **Authors**: Ramin Nabati, Landon Harris, Hairong Qi
- **Comment**: 2021 IEEE Intelligent Vehicles Symposium, 3D-Deep Learning for
  Autonomous Driving Workshop (WS15)
- **Journal**: None
- **Summary**: 3D multi-object tracking is a crucial component in the perception system of autonomous driving vehicles. Tracking all dynamic objects around the vehicle is essential for tasks such as obstacle avoidance and path planning. Autonomous vehicles are usually equipped with different sensor modalities to improve accuracy and reliability. While sensor fusion has been widely used in object detection networks in recent years, most existing multi-object tracking algorithms either rely on a single input modality, or do not fully exploit the information provided by multiple sensing modalities. In this work, we propose an end-to-end network for joint object detection and tracking based on radar and camera sensor fusion. Our proposed method uses a center-based radar-camera fusion algorithm for object detection and utilizes a greedy algorithm for object association. The proposed greedy algorithm uses the depth, velocity and 2D displacement of the detected objects to associate them through time. This makes our tracking algorithm very robust to occluded and overlapping objects, as the depth and velocity information can help the network in distinguishing them. We evaluate our method on the challenging nuScenes dataset, where it achieves 20.0 AMOTA and outperforms all vision-based 3D tracking methods in the benchmark, as well as the baseline LiDAR-based method. Our method is online with a runtime of 35ms per image, making it very suitable for autonomous driving applications.



