# Arxiv Papers in cs.CV on 2021-07-23
### Compositional Models: Multi-Task Learning and Knowledge Transfer with Modular Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.10963v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.10963v1)
- **Published**: 2021-07-23 00:05:55+00:00
- **Updated**: 2021-07-23 00:05:55+00:00
- **Authors**: Andrey Zhmoginov, Dina Bashkirova, Mark Sandler
- **Comment**: None
- **Journal**: None
- **Summary**: Conditional computation and modular networks have been recently proposed for multitask learning and other problems as a way to decompose problem solving into multiple reusable computational blocks. We propose a new approach for learning modular networks based on the isometric version of ResNet with all residual blocks having the same configuration and the same number of parameters. This architectural choice allows adding, removing and changing the order of residual blocks. In our method, the modules can be invoked repeatedly and allow knowledge transfer to novel tasks by adjusting the order of computation. This allows soft weight sharing between tasks with only a small increase in the number of parameters. We show that our method leads to interpretable self-organization of modules in case of multi-task learning, transfer learning and domain adaptation while achieving competitive results on those tasks. From practical perspective, our approach allows to: (a) reuse existing modules for learning new task by adjusting the computation order, (b) use it for unsupervised multi-source domain adaptation to illustrate that adaptation to unseen data can be achieved by only manipulating the order of pretrained modules, (c) show how our approach can be used to increase accuracy of existing architectures for image classification tasks such as ImageNet, without any parameter increase, by reusing the same block multiple times.



### Score-Based Point Cloud Denoising (Learning Gradient Fields for Point Cloud Denoising)
- **Arxiv ID**: http://arxiv.org/abs/2107.10981v4
- **DOI**: 10.1109/ICCV48922.2021.00454
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.10981v4)
- **Published**: 2021-07-23 01:13:03+00:00
- **Updated**: 2022-07-31 09:35:41+00:00
- **Authors**: Shitong Luo, Wei Hu
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Point clouds acquired from scanning devices are often perturbed by noise, which affects downstream tasks such as surface reconstruction and analysis. The distribution of a noisy point cloud can be viewed as the distribution of a set of noise-free samples $p(x)$ convolved with some noise model $n$, leading to $(p * n)(x)$ whose mode is the underlying clean surface. To denoise a noisy point cloud, we propose to increase the log-likelihood of each point from $p * n$ via gradient ascent -- iteratively updating each point's position. Since $p * n$ is unknown at test-time, and we only need the score (i.e., the gradient of the log-probability function) to perform gradient ascent, we propose a neural network architecture to estimate the score of $p * n$ given only noisy point clouds as input. We derive objective functions for training the network and develop a denoising algorithm leveraging on the estimated scores. Experiments demonstrate that the proposed model outperforms state-of-the-art methods under a variety of noise models, and shows the potential to be applied in other tasks such as point cloud upsampling. The code is available at \url{https://github.com/luost26/score-denoise}.



### Human Pose Transfer with Augmented Disentangled Feature Consistency
- **Arxiv ID**: http://arxiv.org/abs/2107.10984v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.10984v3)
- **Published**: 2021-07-23 01:25:07+00:00
- **Updated**: 2022-02-09 06:56:52+00:00
- **Authors**: Kun Wu, Chengxiang Yin, Zhengping Che, Bo Jiang, Jian Tang, Zheng Guan, Gangyi Ding
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: Deep generative models have made great progress in synthesizing images with arbitrary human poses and transferring poses of one person to others. Though many different methods have been proposed to generate images with high visual fidelity, the main challenge remains and comes from two fundamental issues: pose ambiguity and appearance inconsistency. To alleviate the current limitations and improve the quality of the synthesized images, we propose a pose transfer network with augmented Disentangled Feature Consistency (DFC-Net) to facilitate human pose transfer. Given a pair of images containing the source and target person, DFC-Net extracts pose and static information from the source and target respectively, then synthesizes an image of the target person with the desired pose from the source. Moreover, DFC-Net leverages disentangled feature consistency losses in the adversarial training to strengthen the transfer coherence and integrates a keypoint amplifier to enhance the pose feature extraction. With the help of the disentangled feature consistency losses, we further propose a novel data augmentation scheme that introduces unpaired support data with the augmented consistency constraints to improve the generality and robustness of DFC-Net. Extensive experimental results on Mixamo-Pose and EDN-10k have demonstrated DFC-Net achieves state-of-the-art performance on pose transfer.



### Detail Preserving Residual Feature Pyramid Modules for Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2107.10990v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.10990v1)
- **Published**: 2021-07-23 01:53:04+00:00
- **Updated**: 2021-07-23 01:53:04+00:00
- **Authors**: Libo Long, Jochen Lang
- **Comment**: None
- **Journal**: None
- **Summary**: Feature pyramids and iterative refinement have recently led to great progress in optical flow estimation. However, downsampling in feature pyramids can cause blending of foreground objects with the background, which will mislead subsequent decisions in the iterative processing. The results are missing details especially in the flow of thin and of small structures. We propose a novel Residual Feature Pyramid Module (RFPM) which retains important details in the feature map without changing the overall iterative refinement design of the optical flow estimation. RFPM incorporates a residual structure between multiple feature pyramids into a downsampling module that corrects the blending of objects across boundaries. We demonstrate how to integrate our module with two state-of-the-art iterative refinement architectures. Results show that our RFPM visibly reduces flow errors and improves state-of-art performance in the clean pass of Sintel, and is one of the top-performing methods in KITTI. According to the particular modular structure of RFPM, we introduce a special transfer learning approach that can dramatically decrease the training time compared to a typical full optical flow training schedule on multiple datasets.



### Resource Efficient Mountainous Skyline Extraction using Shallow Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.10997v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2107.10997v1)
- **Published**: 2021-07-23 02:14:17+00:00
- **Updated**: 2021-07-23 02:14:17+00:00
- **Authors**: Touqeer Ahmad, Ebrahim Emami, Martin Čadík, George Bebis
- **Comment**: Accepted at International Joint Conference on Neural Networks, 2021
- **Journal**: None
- **Summary**: Skyline plays a pivotal role in mountainous visual geo-localization and localization/navigation of planetary rovers/UAVs and virtual/augmented reality applications. We present a novel mountainous skyline detection approach where we adapt a shallow learning approach to learn a set of filters to discriminate between edges belonging to sky-mountain boundary and others coming from different regions. Unlike earlier approaches, which either rely on extraction of explicit feature descriptors and their classification, or fine-tuning general scene parsing deep networks for sky segmentation, our approach learns linear filters based on local structure analysis. At test time, for every candidate edge pixel, a single filter is chosen from the set of learned filters based on pixel's structure tensor, and then applied to the patch around it. We then employ dynamic programming to solve the shortest path problem for the resultant multistage graph to get the sky-mountain boundary. The proposed approach is computationally faster than earlier methods while providing comparable performance and is more suitable for resource constrained platforms e.g., mobile devices, planetary rovers and UAVs. We compare our proposed approach against earlier skyline detection methods using four different data sets. Our code is available at \url{https://github.com/TouqeerAhmad/skyline_detection}.



### Pruning Ternary Quantization
- **Arxiv ID**: http://arxiv.org/abs/2107.10998v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.10998v5)
- **Published**: 2021-07-23 02:18:00+00:00
- **Updated**: 2023-07-14 22:37:31+00:00
- **Authors**: Dan Liu, Xi Chen, Jie Fu, Chen Ma, Xue Liu
- **Comment**: Merged with Hyperspherical Quantization: Toward Smaller and More
  Accurate Models (arXiv:2212.12653.)
- **Journal**: None
- **Summary**: Inference time, model size, and accuracy are three key factors in deep model compression. Most of the existing work addresses these three key factors separately as it is difficult to optimize them all at the same time. For example, low-bit quantization aims at obtaining a faster model; weight sharing quantization aims at improving compression ratio and accuracy; and mixed-precision quantization aims at balancing accuracy and inference time. To simultaneously optimize bit-width, model size, and accuracy, we propose pruning ternary quantization (PTQ): a simple, effective, symmetric ternary quantization method. We integrate L2 normalization, pruning, and the weight decay term to reduce the weight discrepancy in the gradient estimator during quantization, thus producing highly compressed ternary weights. Our method brings the highest test accuracy and the highest compression ratio. For example, it produces a 939kb (49$\times$) 2bit ternary ResNet-18 model with only 4\% accuracy drop on the ImageNet dataset. It compresses 170MB Mask R-CNN to 5MB (34$\times$) with only 2.8\% average precision drop. Our method is verified on image classification, object detection/segmentation tasks with different network structures such as ResNet-18, ResNet-50, and MobileNetV2.



### Photon-Starved Scene Inference using Single Photon Cameras
- **Arxiv ID**: http://arxiv.org/abs/2107.11001v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.11001v3)
- **Published**: 2021-07-23 02:27:03+00:00
- **Updated**: 2021-08-16 22:56:24+00:00
- **Authors**: Bhavya Goyal, Mohit Gupta
- **Comment**: International Conference on Computer Vision (ICCV), 2021 - Camera
  Ready
- **Journal**: None
- **Summary**: Scene understanding under low-light conditions is a challenging problem. This is due to the small number of photons captured by the camera and the resulting low signal-to-noise ratio (SNR). Single-photon cameras (SPCs) are an emerging sensing modality that are capable of capturing images with high sensitivity. Despite having minimal read-noise, images captured by SPCs in photon-starved conditions still suffer from strong shot noise, preventing reliable scene inference. We propose photon scale-space a collection of high-SNR images spanning a wide range of photons-per-pixel (PPP) levels (but same scene content) as guides to train inference model on low photon flux images. We develop training techniques that push images with different illumination levels closer to each other in feature representation space. The key idea is that having a spectrum of different brightness levels during training enables effective guidance, and increases robustness to shot noise even in extreme noise cases. Based on the proposed approach, we demonstrate, via simulations and real experiments with a SPAD camera, high-performance on various inference tasks such as image classification and monocular depth estimation under ultra low-light, down to < 1 PPP.



### Domain Adaptive Video Segmentation via Temporal Consistency Regularization
- **Arxiv ID**: http://arxiv.org/abs/2107.11004v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.11004v1)
- **Published**: 2021-07-23 02:50:42+00:00
- **Updated**: 2021-07-23 02:50:42+00:00
- **Authors**: Dayan Guan, Jiaxing Huang, Aoran Xiao, Shijian Lu
- **Comment**: Accepted to ICCV 2021. Code is available at
  https://github.com/Dayan-Guan/DA-VSN
- **Journal**: None
- **Summary**: Video semantic segmentation is an essential task for the analysis and understanding of videos. Recent efforts largely focus on supervised video segmentation by learning from fully annotated data, but the learnt models often experience clear performance drop while applied to videos of a different domain. This paper presents DA-VSN, a domain adaptive video segmentation network that addresses domain gaps in videos by temporal consistency regularization (TCR) for consecutive frames of target-domain videos. DA-VSN consists of two novel and complementary designs. The first is cross-domain TCR that guides the prediction of target frames to have similar temporal consistency as that of source frames (learnt from annotated source data) via adversarial learning. The second is intra-domain TCR that guides unconfident predictions of target frames to have similar temporal consistency as confident predictions of target frames. Extensive experiments demonstrate the superiority of our proposed domain adaptive video segmentation network which outperforms multiple baselines consistently by large margins.



### Dynamic Proximal Unrolling Network for Compressive Imaging
- **Arxiv ID**: http://arxiv.org/abs/2107.11007v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.11007v2)
- **Published**: 2021-07-23 03:04:44+00:00
- **Updated**: 2021-10-25 14:34:36+00:00
- **Authors**: Yixiao Yang, Ran Tao, Kaixuan Wei, Ying Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Compressive imaging aims to recover a latent image from under-sampled measurements, suffering from a serious ill-posed inverse problem. Recently, deep neural networks have been applied to this problem with superior results, owing to the learned advanced image priors. These approaches, however, require training separate models for different imaging modalities and sampling ratios, leading to overfitting to specific settings. In this paper, a dynamic proximal unrolling network (dubbed DPUNet) was proposed, which can handle a variety of measurement matrices via one single model without retraining. Specifically, DPUNet can exploit both the embedded observation model via gradient descent and imposed image priors by learned dynamic proximal operators, achieving joint reconstruction. A key component of DPUNet is a dynamic proximal mapping module, whose parameters can be dynamically adjusted at the inference stage and make it adapt to different imaging settings. Experimental results demonstrate that the proposed DPUNet can effectively handle multiple compressive imaging modalities under varying sampling ratios and noise levels via only one trained model, and outperform the state-of-the-art approaches.



### SuperCaustics: Real-time, open-source simulation of transparent objects for deep learning applications
- **Arxiv ID**: http://arxiv.org/abs/2107.11008v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.11008v2)
- **Published**: 2021-07-23 03:11:47+00:00
- **Updated**: 2021-10-11 16:14:15+00:00
- **Authors**: Mehdi Mousavi, Rolando Estrada
- **Comment**: None
- **Journal**: None
- **Summary**: Transparent objects are a very challenging problem in computer vision. They are hard to segment or classify due to their lack of precise boundaries, and there is limited data available for training deep neural networks. As such, current solutions for this problem employ rigid synthetic datasets, which lack flexibility and lead to severe performance degradation when deployed on real-world scenarios. In particular, these synthetic datasets omit features such as refraction, dispersion and caustics due to limitations in the rendering pipeline. To address this issue, we present SuperCaustics, a real-time, open-source simulation of transparent objects designed for deep learning applications. SuperCaustics features extensive modules for stochastic environment creation; uses hardware ray-tracing to support caustics, dispersion, and refraction; and enables generating massive datasets with multi-modal, pixel-perfect ground truth annotations. To validate our proposed system, we trained a deep neural network from scratch to segment transparent objects in difficult lighting scenarios. Our neural network achieved performance comparable to the state-of-the-art on a real-world dataset using only 10% of the training data and in a fraction of the training time. Further experiments show that a model trained with SuperCaustics can segment different types of caustics, even in images with multiple overlapping transparent objects. To the best of our knowledge, this is the first such result for a model trained on synthetic data. Both our open-source code and experimental data are freely available online.



### 3D Brain Reconstruction by Hierarchical Shape-Perception Network from a Single Incomplete Image
- **Arxiv ID**: http://arxiv.org/abs/2107.11010v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.11010v2)
- **Published**: 2021-07-23 03:20:42+00:00
- **Updated**: 2021-10-12 03:19:45+00:00
- **Authors**: Bowen Hu, Baiying Lei, Shuqiang Wang, Yong Liu, Bingchuan Wang, Min Gan, Yanyan Shen
- **Comment**: None
- **Journal**: None
- **Summary**: 3D shape reconstruction is essential in the navigation of minimally-invasive and auto robot-guided surgeries whose operating environments are indirect and narrow, and there have been some works that focused on reconstructing the 3D shape of the surgical organ through limited 2D information available. However, the lack and incompleteness of such information caused by intraoperative emergencies (such as bleeding) and risk control conditions have not been considered. In this paper, a novel hierarchical shape-perception network (HSPN) is proposed to reconstruct the 3D point clouds (PCs) of specific brains from one single incomplete image with low latency. A branching predictor and several hierarchical attention pipelines are constructed to generate point clouds that accurately describe the incomplete images and then complete these point clouds with high quality. Meanwhile, attention gate blocks (AGBs) are designed to efficiently aggregate geometric local features of incomplete PCs transmitted by hierarchical attention pipelines and internal features of reconstructing point clouds. With the proposed HSPN, 3D shape perception and completion can be achieved spontaneously. Comprehensive results measured by Chamfer distance and PC-to-PC error demonstrate that the performance of the proposed HSPN outperforms other competitive methods in terms of qualitative displays, quantitative experiment, and classification evaluation.



### AD-GAN: End-to-end Unsupervised Nuclei Segmentation with Aligned Disentangling Training
- **Arxiv ID**: http://arxiv.org/abs/2107.11022v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.11022v2)
- **Published**: 2021-07-23 04:08:44+00:00
- **Updated**: 2022-03-10 07:02:21+00:00
- **Authors**: Kai Yao, Kaizhu Huang, Jie Sun, Curran Jude
- **Comment**: None
- **Journal**: None
- **Summary**: We consider unsupervised cell nuclei segmentation in this paper. Exploiting the recently-proposed unpaired image-to-image translation between cell nuclei images and randomly synthetic masks, existing approaches, e.g., CycleGAN, have achieved encouraging results. However, these methods usually take a two-stage pipeline and fail to learn end-to-end in cell nuclei images. More seriously, they could lead to the lossy transformation problem, i.e., the content inconsistency between the original images and the corresponding segmentation output. To address these limitations, we propose a novel end-to-end unsupervised framework called Aligned Disentangling Generative Adversarial Network (AD-GAN). Distinctively, AD-GAN introduces representation disentanglement to separate content representation (the underling spatial structure) from style representation (the rendering of the structure). With this framework, spatial structure can be preserved explicitly, enabling a significant reduction of macro-level lossy transformation. We also propose a novel training algorithm able to align the disentangled content in the latent space to reduce micro-level lossy transformation. Evaluations on real-world 2D and 3D datasets show that AD-GAN substantially outperforms the other comparison methods and the professional software both quantitatively and qualitatively. Specifically, the proposed AD-GAN leads to significant improvement over the current best unsupervised methods by an average 17.8% relatively (w.r.t. the metric DICE) on four cell nuclei datasets. As an unsupervised method, AD-GAN even performs competitive with the best supervised models, taking a further leap towards end-to-end unsupervised nuclei segmentation.



### A Deep Signed Directional Distance Function for Object Shape Representation
- **Arxiv ID**: http://arxiv.org/abs/2107.11024v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.11024v2)
- **Published**: 2021-07-23 04:11:59+00:00
- **Updated**: 2021-12-05 01:56:03+00:00
- **Authors**: Ehsan Zobeidi, Nikolay Atanasov
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks that map 3D coordinates to signed distance function (SDF) or occupancy values have enabled high-fidelity implicit representations of object shape. This paper develops a new shape model that allows synthesizing novel distance views by optimizing a continuous signed directional distance function (SDDF). Similar to deep SDF models, our SDDF formulation can represent whole categories of shapes and complete or interpolate across shapes from partial input data. Unlike an SDF, which measures distance to the nearest surface in any direction, an SDDF measures distance in a given direction. This allows training an SDDF model without 3D shape supervision, using only distance measurements, readily available from depth camera or Lidar sensors. Our model also removes post-processing steps like surface extraction or rendering by directly predicting distance at arbitrary locations and viewing directions. Unlike deep view-synthesis techniques, such as Neural Radiance Fields, which train high-capacity black-box models, our model encodes by construction the property that SDDF values decrease linearly along the viewing direction. This structure constraint not only results in dimensionality reduction but also provides analytical confidence about the accuracy of SDDF predictions, regardless of the distance to the object surface.



### WaveFill: A Wavelet-based Generation Network for Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2107.11027v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.11027v1)
- **Published**: 2021-07-23 04:44:40+00:00
- **Updated**: 2021-07-23 04:44:40+00:00
- **Authors**: Yingchen Yu, Fangneng Zhan, Shijian Lu, Jianxiong Pan, Feiying Ma, Xuansong Xie, Chunyan Miao
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Image inpainting aims to complete the missing or corrupted regions of images with realistic contents. The prevalent approaches adopt a hybrid objective of reconstruction and perceptual quality by using generative adversarial networks. However, the reconstruction loss and adversarial loss focus on synthesizing contents of different frequencies and simply applying them together often leads to inter-frequency conflicts and compromised inpainting. This paper presents WaveFill, a wavelet-based inpainting network that decomposes images into multiple frequency bands and fills the missing regions in each frequency band separately and explicitly. WaveFill decomposes images by using discrete wavelet transform (DWT) that preserves spatial information naturally. It applies L1 reconstruction loss to the decomposed low-frequency bands and adversarial loss to high-frequency bands, hence effectively mitigate inter-frequency conflicts while completing images in spatial domain. To address the inpainting inconsistency in different frequency bands and fuse features with distinct statistics, we design a novel normalization scheme that aligns and fuses the multi-frequency features effectively. Extensive experiments over multiple datasets show that WaveFill achieves superior image inpainting qualitatively and quantitatively.



### Early Diagnosis of Lung Cancer Using Computer Aided Detection via Lung Segmentation Approach
- **Arxiv ID**: http://arxiv.org/abs/2107.12205v1
- **DOI**: 10.14445/22315381/IJETT-V69I5P213
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.12205v1)
- **Published**: 2021-07-23 05:46:06+00:00
- **Updated**: 2021-07-23 05:46:06+00:00
- **Authors**: Abhir Bhandary, Ananth Prabhu G, Mustafa Basthikodi, Chaitra K M
- **Comment**: 9 pages, 10 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT)
- **Journal**: International Journal of Engineering Trends and Technology
  69.5(2021):85-93
- **Summary**: Lung cancer begins in the lungs and leading to the reason of cancer demise amid population in the creation. According to the American Cancer Society, which estimates about 27% of the deaths because of cancer. In the early phase of its evolution, lung cancer does not cause any symptoms usually. Many of the patients have been diagnosed in a developed phase where symptoms become more prominent, that results in poor curative treatment and high mortality rate. Computer Aided Detection systems are used to achieve greater accuracies for the lung cancer diagnosis. In this research exertion, we proposed a novel methodology for lung Segmentation on the basis of Fuzzy C-Means Clustering, Adaptive Thresholding, and Segmentation of Active Contour Model. The experimental results are analysed and presented.



### 3D Radar Velocity Maps for Uncertain Dynamic Environments
- **Arxiv ID**: http://arxiv.org/abs/2107.11039v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, 68T45, I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2107.11039v1)
- **Published**: 2021-07-23 06:03:16+00:00
- **Updated**: 2021-07-23 06:03:16+00:00
- **Authors**: Ransalu Senanayake, Kyle Beltran Hatch, Jason Zheng, Mykel J. Kochenderfer
- **Comment**: Accepted to IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS), 2021
- **Journal**: None
- **Summary**: Future urban transportation concepts include a mixture of ground and air vehicles with varying degrees of autonomy in a congested environment. In such dynamic environments, occupancy maps alone are not sufficient for safe path planning. Safe and efficient transportation requires reasoning about the 3D flow of traffic and properly modeling uncertainty. Several different approaches can be taken for developing 3D velocity maps. This paper explores a Bayesian approach that captures our uncertainty in the map given training data. The approach involves projecting spatial coordinates into a high-dimensional feature space and then applying Bayesian linear regression to make predictions and quantify uncertainty in our estimates. On a collection of air and ground datasets, we demonstrate that this approach is effective and more scalable than several alternative approaches.



### RewriteNet: Reliable Scene Text Editing with Implicit Decomposition of Text Contents and Styles
- **Arxiv ID**: http://arxiv.org/abs/2107.11041v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.11041v2)
- **Published**: 2021-07-23 06:32:58+00:00
- **Updated**: 2022-05-02 11:30:26+00:00
- **Authors**: Junyeop Lee, Yoonsik Kim, Seonghyeon Kim, Moonbin Yim, Seung Shin, Gayoung Lee, Sungrae Park
- **Comment**: CVPRW 2022 - AI for Content Creation Workshop
- **Journal**: None
- **Summary**: Scene text editing (STE), which converts a text in a scene image into the desired text while preserving an original style, is a challenging task due to a complex intervention between text and style. In this paper, we propose a novel STE model, referred to as RewriteNet, that decomposes text images into content and style features and re-writes a text in the original image. Specifically, RewriteNet implicitly distinguishes the content from the style by introducing scene text recognition. Additionally, independent of the exact supervisions with synthetic examples, we propose a self-supervised training scheme for unlabeled real-world images, which bridges the domain gap between synthetic and real data. Our experiments present that RewriteNet achieves better generation performances than other comparisons. Further analysis proves the feature decomposition of RewriteNet and demonstrates the reliability and robustness through diverse experiments. Our implementation is publicly available at \url{https://github.com/clovaai/rewritenet}



### Integrating Deep Learning and Augmented Reality to Enhance Situational Awareness in Firefighting Environments
- **Arxiv ID**: http://arxiv.org/abs/2107.11043v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.11043v2)
- **Published**: 2021-07-23 06:35:13+00:00
- **Updated**: 2021-11-08 22:36:33+00:00
- **Authors**: Manish Bhattarai
- **Comment**: PhD Dissertation
- **Journal**: None
- **Summary**: We present a new four-pronged approach to build firefighter's situational awareness for the first time in the literature. We construct a series of deep learning frameworks built on top of one another to enhance the safety, efficiency, and successful completion of rescue missions conducted by firefighters in emergency first response settings. First, we used a deep Convolutional Neural Network (CNN) system to classify and identify objects of interest from thermal imagery in real-time. Next, we extended this CNN framework for object detection, tracking, segmentation with a Mask RCNN framework, and scene description with a multimodal natural language processing(NLP) framework. Third, we built a deep Q-learning-based agent, immune to stress-induced disorientation and anxiety, capable of making clear navigation decisions based on the observed and stored facts in live-fire environments. Finally, we used a low computational unsupervised learning technique called tensor decomposition to perform meaningful feature extraction for anomaly detection in real-time. With these ad-hoc deep learning structures, we built the artificial intelligence system's backbone for firefighters' situational awareness. To bring the designed system into usage by firefighters, we designed a physical structure where the processed results are used as inputs in the creation of an augmented reality capable of advising firefighters of their location and key features around them, which are vital to the rescue operation at hand, as well as a path planning feature that acts as a virtual guide to assist disoriented first responders in getting back to safety. When combined, these four approaches present a novel approach to information understanding, transfer, and synthesis that could dramatically improve firefighter response and efficacy and reduce life loss.



### Unrealistic Feature Suppression for Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.11047v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.11047v1)
- **Published**: 2021-07-23 06:43:14+00:00
- **Updated**: 2021-07-23 06:43:14+00:00
- **Authors**: Sanghun Kim, SeungKyu Lee
- **Comment**: 8 pages, 10 figures
- **Journal**: None
- **Summary**: Due to the unstable nature of minimax game between generator and discriminator, improving the performance of GANs is a challenging task. Recent studies have shown that selected high-quality samples in training improve the performance of GANs. However, sampling approaches which discard samples show limitations in some aspects such as the speed of training and optimality of the networks. In this paper we propose unrealistic feature suppression (UFS) module that keeps high-quality features and suppresses unrealistic features. UFS module keeps the training stability of networks and improves the quality of generated images. We demonstrate the effectiveness of UFS module on various models such as WGAN-GP, SNGAN, and BigGAN. By using UFS module, we achieved better Frechet inception distance and inception score compared to various baseline models. We also visualize how effectively our UFS module suppresses unrealistic features through class activation maps.



### Sexing Caucasian 2D footprints using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2108.01554v1
- **DOI**: 10.1371/journal.pone.0255630
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2108.01554v1)
- **Published**: 2021-07-23 06:56:34+00:00
- **Updated**: 2021-07-23 06:56:34+00:00
- **Authors**: Marcin Budka, Matthew R. Bennet, Sally Reynolds, Shelby Barefoot, Sarah Reel, Selina Reidy, Jeremy Walker
- **Comment**: None
- **Journal**: None
- **Summary**: Footprints are left, or obtained, in a variety of scenarios from crime scenes to anthropological investigations. Determining the sex of a footprint can be useful in screening such impressions and attempts have been made to do so using single or multi landmark distances, shape analyses and via the density of friction ridges. Here we explore the relative importance of different components in sexing two-dimensional foot impressions namely, size, shape and texture. We use a machine learning approach and compare this to more traditional methods of discrimination. Two datasets are used, a pilot data set collected from students at Bournemouth University (N=196) and a larger data set collected by podiatrists at Sheffield NHS Teaching Hospital (N=2677). Our convolutional neural network can sex a footprint with accuracy of around 90% on a test set of N=267 footprint images using all image components, which is better than an expert can achieve. However, the quality of the impressions impacts on this success rate, but the results are promising and in time it may be possible to create an automated screening algorithm in which practitioners of whatever sort (medical or forensic) can obtain a first order sexing of a two-dimensional footprint.



### MCDAL: Maximum Classifier Discrepancy for Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.11049v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.11049v2)
- **Published**: 2021-07-23 06:57:08+00:00
- **Updated**: 2022-02-14 07:27:04+00:00
- **Authors**: Jae Won Cho, Dong-Jin Kim, Yunjae Jung, In So Kweon
- **Comment**: 11 pages, Accepted to IEEE-TNNLS
- **Journal**: None
- **Summary**: Recent state-of-the-art active learning methods have mostly leveraged Generative Adversarial Networks (GAN) for sample acquisition; however, GAN is usually known to suffer from instability and sensitivity to hyper-parameters. In contrast to these methods, we propose in this paper a novel active learning framework that we call Maximum Classifier Discrepancy for Active Learning (MCDAL) which takes the prediction discrepancies between multiple classifiers. In particular, we utilize two auxiliary classification layers that learn tighter decision boundaries by maximizing the discrepancies among them. Intuitively, the discrepancies in the auxiliary classification layers' predictions indicate the uncertainty in the prediction. In this regard, we propose a novel method to leverage the classifier discrepancies for the acquisition function for active learning. We also provide an interpretation of our idea in relation to existing GAN based active learning methods and domain adaptation frameworks. Moreover, we empirically demonstrate the utility of our approach where the performance of our approach exceeds the state-of-the-art methods on several image classification and semantic segmentation datasets in active learning setups.



### Unsupervised Domain Adaptation for Video Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.11052v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.11052v2)
- **Published**: 2021-07-23 07:18:20+00:00
- **Updated**: 2021-09-13 16:16:55+00:00
- **Authors**: Inkyu Shin, Kwanyong Park, Sanghyun Woo, In So Kweon
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation for semantic segmentation has gained immense popularity since it can transfer knowledge from simulation to real (Sim2Real) by largely cutting out the laborious per pixel labeling efforts at real. In this work, we present a new video extension of this task, namely Unsupervised Domain Adaptation for Video Semantic Segmentation. As it became easy to obtain large-scale video labels through simulation, we believe attempting to maximize Sim2Real knowledge transferability is one of the promising directions for resolving the fundamental data-hungry issue in the video. To tackle this new problem, we present a novel two-phase adaptation scheme. In the first step, we exhaustively distill source domain knowledge using supervised loss functions. Simultaneously, video adversarial training (VAT) is employed to align the features from source to target utilizing video context. In the second step, we apply video self-training (VST), focusing only on the target data. To construct robust pseudo labels, we exploit the temporal information in the video, which has been rarely explored in the previous image-based self-training approaches. We set strong baseline scores on 'VIPER to CityscapeVPS' adaptation scenario. We show that our proposals significantly outperform previous image-based UDA methods both on image-level (mIoU) and video-level (VPQ) evaluation metrics.



### Transporting Causal Mechanisms for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2107.11055v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.11055v2)
- **Published**: 2021-07-23 07:25:15+00:00
- **Updated**: 2021-07-28 12:07:42+00:00
- **Authors**: Zhongqi Yue, Qianru Sun, Xian-Sheng Hua, Hanwang Zhang
- **Comment**: ICCV 2021 Oral
- **Journal**: None
- **Summary**: Existing Unsupervised Domain Adaptation (UDA) literature adopts the covariate shift and conditional shift assumptions, which essentially encourage models to learn common features across domains. However, due to the lack of supervision in the target domain, they suffer from the semantic loss: the feature will inevitably lose non-discriminative semantics in source domain, which is however discriminative in target domain. We use a causal view -- transportability theory -- to identify that such loss is in fact a confounding effect, which can only be removed by causal intervention. However, the theoretical solution provided by transportability is far from practical for UDA, because it requires the stratification and representation of the unobserved confounder that is the cause of the domain gap. To this end, we propose a practical solution: Transporting Causal Mechanisms (TCM), to identify the confounder stratum and representations by using the domain-invariant disentangled causal mechanisms, which are discovered in an unsupervised fashion. Our TCM is both theoretically and empirically grounded. Extensive experiments show that TCM achieves state-of-the-art performance on three challenging UDA benchmarks: ImageCLEF-DA, Office-Home, and VisDA-2017. Codes are available in Appendix.



### Improving the Generalization of Meta-learning on Unseen Domains via Adversarial Shift
- **Arxiv ID**: http://arxiv.org/abs/2107.11056v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.11056v1)
- **Published**: 2021-07-23 07:29:30+00:00
- **Updated**: 2021-07-23 07:29:30+00:00
- **Authors**: Pinzhuo Tian, Yao Gao
- **Comment**: v1
- **Journal**: None
- **Summary**: Meta-learning provides a promising way for learning to efficiently learn and achieves great success in many applications. However, most meta-learning literature focuses on dealing with tasks from a same domain, making it brittle to generalize to tasks from the other unseen domains. In this work, we address this problem by simulating tasks from the other unseen domains to improve the generalization and robustness of meta-learning method. Specifically, we propose a model-agnostic shift layer to learn how to simulate the domain shift and generate pseudo tasks, and develop a new adversarial learning-to-learn mechanism to train it. Based on the pseudo tasks, the meta-learning model can learn cross-domain meta-knowledge, which can generalize well on unseen domains. We conduct extensive experiments under the domain generalization setting. Experimental results demonstrate that the proposed shift layer is applicable to various meta-learning frameworks. Moreover, our method also leads to state-of-the-art performance on different cross-domain few-shot classification benchmarks and produces good results on cross-domain few-shot regression.



### Label Distribution Amendment with Emotional Semantic Correlations for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.11061v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.11061v1)
- **Published**: 2021-07-23 07:46:14+00:00
- **Updated**: 2021-07-23 07:46:14+00:00
- **Authors**: Shasha Mao, Guanghui Shi, Licheng Jiao, Shuiping Gou, Yangyang Li, Lin Xiong, Boxin Shi
- **Comment**: None
- **Journal**: None
- **Summary**: By utilizing label distribution learning, a probability distribution is assigned for a facial image to express a compound emotion, which effectively improves the problem of label uncertainties and noises occurred in one-hot labels. In practice, it is observed that correlations among emotions are inherently different, such as surprised and happy emotions are more possibly synchronized than surprised and neutral. It indicates the correlation may be crucial for obtaining a reliable label distribution. Based on this, we propose a new method that amends the label distribution of each facial image by leveraging correlations among expressions in the semantic space. Inspired by inherently diverse correlations among word2vecs, the topological information among facial expressions is firstly explored in the semantic space, and each image is embedded into the semantic space. Specially, a class-relation graph is constructed to transfer the semantic correlation among expressions into the task space. By comparing semantic and task class-relation graphs of each image, the confidence of its label distribution is evaluated. Based on the confidence, the label distribution is amended by enhancing samples with higher confidence and weakening samples with lower confidence. Experimental results demonstrate the proposed method is more effective than compared state-of-the-art methods.



### Reservoir Computing Approach for Gray Images Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.11077v3
- **DOI**: 10.1109/INISTA55318.2022.9894221
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.11077v3)
- **Published**: 2021-07-23 08:37:24+00:00
- **Updated**: 2022-10-06 13:41:47+00:00
- **Authors**: Petia Koprinkova-Hristova
- **Comment**: 12 pages, 7 figures
- **Journal**: 2022 International Conference on INnovations in Intelligent
  SysTems and Applications (INISTA)
- **Summary**: The paper proposes a novel approach for gray scale images segmentation. It is based on multiple features extraction from single feature per image pixel, namely its intensity value, using Echo state network. The newly extracted features - reservoir equilibrium states - reveal hidden image characteristics that improve its segmentation via a clustering algorithm. Moreover, it was demonstrated that the intrinsic plasticity tuning of reservoir fits its equilibrium states to the original image intensity distribution thus allowing for its better segmentation. The proposed approach is tested on the benchmark image Lena.



### Data-driven deep density estimation
- **Arxiv ID**: http://arxiv.org/abs/2107.11085v1
- **DOI**: 10.1007/s00521-021-06281-3
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.11085v1)
- **Published**: 2021-07-23 08:53:46+00:00
- **Updated**: 2021-07-23 08:53:46+00:00
- **Authors**: Patrik Puchert, Pedro Hermosilla, Tobias Ritschel, Timo Ropinski
- **Comment**: 35 pages, 25 figures. Puplished in Neural Computing and Applications
  (2021). The method described is available as python pip pachage
  deep_density_estimation and on github https://github.com/trikpachu/ DDE
- **Journal**: None
- **Summary**: Density estimation plays a crucial role in many data analysis tasks, as it infers a continuous probability density function (PDF) from discrete samples. Thus, it is used in tasks as diverse as analyzing population data, spatial locations in 2D sensor readings, or reconstructing scenes from 3D scans. In this paper, we introduce a learned, data-driven deep density estimation (DDE) to infer PDFs in an accurate and efficient manner, while being independent of domain dimensionality or sample size. Furthermore, we do not require access to the original PDF during estimation, neither in parametric form, nor as priors, or in the form of many samples. This is enabled by training an unstructured convolutional neural network on an infinite stream of synthetic PDFs, as unbound amounts of synthetic training data generalize better across a deck of natural PDFs than any natural finite training data will do. Thus, we hope that our publicly available DDE method will be beneficial in many areas of data analysis, where continuous models are to be estimated from discrete observations.



### Class-Incremental Domain Adaptation with Smoothing and Calibration for Surgical Report Generation
- **Arxiv ID**: http://arxiv.org/abs/2107.11091v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.11091v1)
- **Published**: 2021-07-23 09:08:26+00:00
- **Updated**: 2021-07-23 09:08:26+00:00
- **Authors**: Mengya Xu, Mobarakol Islam, Chwee Ming Lim, Hongliang Ren
- **Comment**: Accepted in MICCAI 2021
- **Journal**: None
- **Summary**: Generating surgical reports aimed at surgical scene understanding in robot-assisted surgery can contribute to documenting entry tasks and post-operative analysis. Despite the impressive outcome, the deep learning model degrades the performance when applied to different domains encountering domain shifts. In addition, there are new instruments and variations in surgical tissues appeared in robotic surgery. In this work, we propose class-incremental domain adaptation (CIDA) with a multi-layer transformer-based model to tackle the new classes and domain shift in the target domain to generate surgical reports during robotic surgery. To adapt incremental classes and extract domain invariant features, a class-incremental (CI) learning method with supervised contrastive (SupCon) loss is incorporated with a feature extractor. To generate caption from the extracted feature, curriculum by one-dimensional gaussian smoothing (CBS) is integrated with a multi-layer transformer-based caption prediction model. CBS smoothes the features embedding using anti-aliasing and helps the model to learn domain invariant features. We also adopt label smoothing (LS) to calibrate prediction probability and obtain better feature representation with both feature extractor and captioning model. The proposed techniques are empirically evaluated by using the datasets of two surgical domains, such as nephrectomy operations and transoral robotic surgery. We observe that domain invariant feature learning and the well-calibrated network improves the surgical report generation performance in both source and target domain under domain shift and unseen classes in the manners of one-shot and few-shot learning. The code is publicly available at https://github.com/XuMengyaAmy/CIDACaptioning.



### RGB Image Classification with Quantum Convolutional Ansaetze
- **Arxiv ID**: http://arxiv.org/abs/2107.11099v2
- **DOI**: 10.1007/s11128-022-03442-8
- **Categories**: **quant-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.11099v2)
- **Published**: 2021-07-23 09:38:59+00:00
- **Updated**: 2022-02-23 04:17:38+00:00
- **Authors**: Yu Jing, Xiaogang Li, Yang Yang, Chonghang Wu, Wenbing Fu, Wei Hu, Yuanyuan Li, Hua Xu
- **Comment**: https://link.springer.com/article/10.1007/s11128-022-03442-8
- **Journal**: Quantum Inf Process 21, 101 (2022)
- **Summary**: With the rapid growth of qubit numbers and coherence times in quantum hardware technology, implementing shallow neural networks on the so-called Noisy Intermediate-Scale Quantum (NISQ) devices has attracted a lot of interest. Many quantum (convolutional) circuit ansaetze are proposed for grayscale images classification tasks with promising empirical results. However, when applying these ansaetze on RGB images, the intra-channel information that is useful for vision tasks is not extracted effectively. In this paper, we propose two types of quantum circuit ansaetze to simulate convolution operations on RGB images, which differ in the way how inter-channel and intra-channel information are extracted. To the best of our knowledge, this is the first work of a quantum convolutional circuit to deal with RGB images effectively, with a higher test accuracy compared to the purely classical CNNs. We also investigate the relationship between the size of quantum circuit ansatz and the learnability of the hybrid quantum-classical convolutional neural network. Through experiments based on CIFAR-10 and MNIST datasets, we demonstrate that a larger size of the quantum circuit ansatz improves predictive performance in multiclass classification tasks, providing useful insights for near term quantum algorithm developments.



### Cardiac CT segmentation based on distance regularized level set
- **Arxiv ID**: http://arxiv.org/abs/2107.11119v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.11119v1)
- **Published**: 2021-07-23 10:13:31+00:00
- **Updated**: 2021-07-23 10:13:31+00:00
- **Authors**: Xinyang Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Before analy z ing the CT image, it is very important to segment the heart image, and the left ve ntricular (LV) inner and outer membrane segmentation is one of the most important contents. However, manual segmentation is tedious and time consuming. In order to facilitate doctors to focus on high tech tasks such as disease analysis and diagnosis, it is crucial to develop a fast and accurate segmentation method [1]. In view of this phenomenon, this paper uses distance regularized level set (DRL SE) to explore the segmentation effect of epicardium and endocardium 2 ]], which includes a distance regula riz ed t erm and an external energy term. Finally, five CT images are used to verify the proposed method, and image quality evaluation indexes such as dice score and Hausdorff distance are used to evaluate the segmentation effect. The results showed that the me tho d could separate the inner and outer membrane very well (endocardium dice = 0.9253, Hausdorff = 7.8740; epicardium Hausdorff = 0.9687, Hausdorff = 6 .



### Learning Discriminative Representations for Multi-Label Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.11159v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.11159v1)
- **Published**: 2021-07-23 12:10:46+00:00
- **Updated**: 2021-07-23 12:10:46+00:00
- **Authors**: Mohammed Hassanin, Ibrahim Radwan, Salman Khan, Murat Tahtali
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-label recognition is a fundamental, and yet is a challenging task in computer vision. Recently, deep learning models have achieved great progress towards learning discriminative features from input images. However, conventional approaches are unable to model the inter-class discrepancies among features in multi-label images, since they are designed to work for image-level feature discrimination. In this paper, we propose a unified deep network to learn discriminative features for the multi-label task. Given a multi-label image, the proposed method first disentangles features corresponding to different classes. Then, it discriminates between these classes via increasing the inter-class distance while decreasing the intra-class differences in the output space. By regularizing the whole network with the proposed loss, the performance of applying the wellknown ResNet-101 is improved significantly. Extensive experiments have been performed on COCO-2014, VOC2007 and VOC2012 datasets, which demonstrate that the proposed method outperforms state-of-the-art approaches by a significant margin of 3:5% on large-scale COCO dataset. Moreover, analysis of the discriminative feature learning approach shows that it can be plugged into various types of multi-label methods as a general module.



### Bias Loss for Mobile Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.11170v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.11170v3)
- **Published**: 2021-07-23 12:37:56+00:00
- **Updated**: 2021-08-10 14:00:57+00:00
- **Authors**: Lusine Abrahamyan, Valentin Ziatchin, Yiming Chen, Nikos Deligiannis
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Compact convolutional neural networks (CNNs) have witnessed exceptional improvements in performance in recent years. However, they still fail to provide the same predictive power as CNNs with a large number of parameters. The diverse and even abundant features captured by the layers is an important characteristic of these successful CNNs. However, differences in this characteristic between large CNNs and their compact counterparts have rarely been investigated. In compact CNNs, due to the limited number of parameters, abundant features are unlikely to be obtained, and feature diversity becomes an essential characteristic. Diverse features present in the activation maps derived from a data point during model inference may indicate the presence of a set of unique descriptors necessary to distinguish between objects of different classes. In contrast, data points with low feature diversity may not provide a sufficient amount of unique descriptors to make a valid prediction; we refer to them as random predictions. Random predictions can negatively impact the optimization process and harm the final performance. This paper proposes addressing the problem raised by random predictions by reshaping the standard cross-entropy to make it biased toward data points with a limited number of unique descriptive features. Our novel Bias Loss focuses the training on a set of valuable data points and prevents the vast number of samples with poor learning features from misleading the optimization process. Furthermore, to show the importance of diversity, we present a family of SkipNet models whose architectures are brought to boost the number of unique descriptors in the last layers. Our Skipnet-M can achieve 1% higher classification accuracy than MobileNetV3 Large.



### Developing efficient transfer learning strategies for robust scene recognition in mobile robotics using pre-trained convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2107.11187v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.11187v1)
- **Published**: 2021-07-23 12:48:56+00:00
- **Updated**: 2021-07-23 12:48:56+00:00
- **Authors**: Hermann Baumgartl, Ricardo Buettner
- **Comment**: 18 pages, 1 figures, 10 tables. Submitted to IEEE Transactions on
  Robotics (T-RO)
- **Journal**: None
- **Summary**: We present four different robust transfer learning and data augmentation strategies for robust mobile scene recognition. By training three mobile-ready (EfficientNetB0, MobileNetV2, MobileNetV3) and two large-scale baseline (VGG16, ResNet50) convolutional neural network architectures on the widely available Event8, Scene15, Stanford40, and MIT67 datasets, we show the generalization ability of our transfer learning strategies. Furthermore, we tested the robustness of our transfer learning strategies under viewpoint and lighting changes using the KTH-Idol2 database. Also, the impact of inference optimization techniques on the general performance and the robustness under different transfer learning strategies is evaluated. Experimental results show that when employing transfer learning, Fine-Tuning in combination with extensive data augmentation improves the general accuracy and robustness in mobile scene recognition. We achieved state-of-the-art results using various baseline convolutional neural networks and showed the robustness against lighting and viewpoint changes in challenging mobile robot place recognition.



### Multi-Modal Pedestrian Detection with Large Misalignment Based on Modal-Wise Regression and Multi-Modal IoU
- **Arxiv ID**: http://arxiv.org/abs/2107.11196v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.11196v1)
- **Published**: 2021-07-23 12:58:41+00:00
- **Updated**: 2021-07-23 12:58:41+00:00
- **Authors**: Napat Wanchaitanawong, Masayuki Tanaka, Takashi Shibata, Masatoshi Okutomi
- **Comment**: Accepted by MVA2021
- **Journal**: None
- **Summary**: The combined use of multiple modalities enables accurate pedestrian detection under poor lighting conditions by using the high visibility areas from these modalities together. The vital assumption for the combination use is that there is no or only a weak misalignment between the two modalities. In general, however, this assumption often breaks in actual situations. Due to this assumption's breakdown, the position of the bounding boxes does not match between the two modalities, resulting in a significant decrease in detection accuracy, especially in regions where the amount of misalignment is large. In this paper, we propose a multi-modal Faster-RCNN that is robust against large misalignment. The keys are 1) modal-wise regression and 2) multi-modal IoU for mini-batch sampling. To deal with large misalignment, we perform bounding box regression for both the RPN and detection-head with both modalities. We also propose a new sampling strategy called "multi-modal mini-batch sampling" that integrates the IoU for both modalities. We demonstrate that the proposed method's performance is much better than that of the state-of-the-art methods for data with large misalignment through actual image experiments.



### A3GC-IP: Attention-Oriented Adjacency Adaptive Recurrent Graph Convolutions for Human Pose Estimation from Sparse Inertial Measurements
- **Arxiv ID**: http://arxiv.org/abs/2107.11214v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.11214v4)
- **Published**: 2021-07-23 13:23:10+00:00
- **Updated**: 2022-12-22 12:06:43+00:00
- **Authors**: Patrik Puchert, Timo Ropinski
- **Comment**: Preprint, in submission
- **Journal**: None
- **Summary**: Conventional methods for human pose estimation either require a high degree of instrumentation, by relying on many inertial measurement units (IMUs), or constraint the recording space, by relying on extrinsic cameras. These deficits are tackled through the approach of human pose estimation from sparse IMU data. We define attention-oriented adjacency adaptive graph convolutional long-short term memory networks (A3GC-LSTM), to tackle human pose estimation based on six IMUs, through incorporating the human body graph structure directly into the network. The A3GC-LSTM combines both spatial and temporal dependency in a single network operation, more memory efficiently than previous approaches. The recurrent graph learning on arbitrarily long sequences is made possible by equipping graph convolutions with adjacency adaptivity, which eliminates the problem of information loss in deep or recurrent graph networks, while it also allows for learning unknown dependencies between the human body joints. To further boost accuracy, a spatial attention formalism is incorporated into the recurrent LSTM cell. With our presented approach, we are able to utilize the inherent graph nature of the human body, and thus can outperform the state of the art for human pose estimation from sparse IMU data.



### Exploring Deep Registration Latent Spaces
- **Arxiv ID**: http://arxiv.org/abs/2107.11238v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.11238v1)
- **Published**: 2021-07-23 13:54:21+00:00
- **Updated**: 2021-07-23 13:54:21+00:00
- **Authors**: Théo Estienne, Maria Vakalopoulou, Stergios Christodoulidis, Enzo Battistella, Théophraste Henry, Marvin Lerousseau, Amaury Leroy, Guillaume Chassagnon, Marie-Pierre Revel, Nikos Paragios, Eric Deutsch
- **Comment**: 13 pages, 5 figures + 3 figures in supplementary materials Accepted
  to DART 2021 workshop
- **Journal**: None
- **Summary**: Explainability of deep neural networks is one of the most challenging and interesting problems in the field. In this study, we investigate the topic focusing on the interpretability of deep learning-based registration methods. In particular, with the appropriate model architecture and using a simple linear projection, we decompose the encoding space, generating a new basis, and we empirically show that this basis captures various decomposed anatomically aware geometrical transformations. We perform experiments using two different datasets focusing on lungs and hippocampus MRI. We show that such an approach can decompose the highly convoluted latent spaces of registration pipelines in an orthogonal space with several interesting properties. We hope that this work could shed some light on a better understanding of deep learning-based registration methods.



### Adversarial Reinforced Instruction Attacker for Robust Vision-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2107.11252v1
- **DOI**: 10.1109/TPAMI.2021.3097435
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2107.11252v1)
- **Published**: 2021-07-23 14:11:31+00:00
- **Updated**: 2021-07-23 14:11:31+00:00
- **Authors**: Bingqian Lin, Yi Zhu, Yanxin Long, Xiaodan Liang, Qixiang Ye, Liang Lin
- **Comment**: Accepted by TPAMI 2021
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2021
- **Summary**: Language instruction plays an essential role in the natural language grounded navigation tasks. However, navigators trained with limited human-annotated instructions may have difficulties in accurately capturing key information from the complicated instruction at different timesteps, leading to poor navigation performance. In this paper, we exploit to train a more robust navigator which is capable of dynamically extracting crucial factors from the long instruction, by using an adversarial attacking paradigm. Specifically, we propose a Dynamic Reinforced Instruction Attacker (DR-Attacker), which learns to mislead the navigator to move to the wrong target by destroying the most instructive information in instructions at different timesteps. By formulating the perturbation generation as a Markov Decision Process, DR-Attacker is optimized by the reinforcement learning algorithm to generate perturbed instructions sequentially during the navigation, according to a learnable attack score. Then, the perturbed instructions, which serve as hard samples, are used for improving the robustness of the navigator with an effective adversarial training strategy and an auxiliary self-supervised reasoning task. Experimental results on both Vision-and-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks show the superiority of our proposed method over state-of-the-art methods. Moreover, the visualization analysis shows the effectiveness of the proposed DR-Attacker, which can successfully attack crucial information in the instructions at different timesteps. Code is available at https://github.com/expectorlin/DR-Attacker.



### Image-to-Image Translation with Low Resolution Conditioning
- **Arxiv ID**: http://arxiv.org/abs/2107.11262v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.11262v1)
- **Published**: 2021-07-23 14:22:12+00:00
- **Updated**: 2021-07-23 14:22:12+00:00
- **Authors**: Mohamed Abderrahmen Abid, Ihsen Hedhli, Jean-François Lalonde, Christian Gagne
- **Comment**: None
- **Journal**: None
- **Summary**: Most image-to-image translation methods focus on learning mappings across domains with the assumption that images share content (e.g., pose) but have their own domain-specific information known as style. When conditioned on a target image, such methods aim to extract the style of the target and combine it with the content of the source image. In this work, we consider the scenario where the target image has a very low resolution. More specifically, our approach aims at transferring fine details from a high resolution (HR) source image to fit a coarse, low resolution (LR) image representation of the target. We therefore generate HR images that share features from both HR and LR inputs. This differs from previous methods that focus on translating a given image style into a target content, our translation approach being able to simultaneously imitate the style and merge the structural information of the LR target. Our approach relies on training the generative model to produce HR target images that both 1) share distinctive information of the associated source image; 2) correctly match the LR target image when downscaled. We validate our method on the CelebA-HQ and AFHQ datasets by demonstrating improvements in terms of visual quality, diversity and coverage. Qualitative and quantitative results show that when dealing with intra-domain image translation, our method generates more realistic samples compared to state-of-the-art methods such as Stargan-v2



### Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.11264v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.11264v4)
- **Published**: 2021-07-23 14:25:02+00:00
- **Updated**: 2021-10-11 11:10:41+00:00
- **Authors**: Sanghun Jung, Jungsoo Lee, Daehoon Gwak, Sungha Choi, Jaegul Choo
- **Comment**: Accepted to ICCV 2021 (Oral Presentation)
- **Journal**: None
- **Summary**: Identifying unexpected objects on roads in semantic segmentation (e.g., identifying dogs on roads) is crucial in safety-critical applications. Existing approaches use images of unexpected objects from external datasets or require additional training (e.g., retraining segmentation networks or training an extra network), which necessitate a non-trivial amount of labor intensity or lengthy inference time. One possible alternative is to use prediction scores of a pre-trained network such as the max logits (i.e., maximum values among classes before the final softmax layer) for detecting such objects. However, the distribution of max logits of each predicted class is significantly different from each other, which degrades the performance of identifying unexpected objects in urban-scene segmentation. To address this issue, we propose a simple yet effective approach that standardizes the max logits in order to align the different distributions and reflect the relative meanings of max logits within each predicted class. Moreover, we consider the local regions from two different perspectives based on the intuition that neighboring pixels share similar semantic information. In contrast to previous approaches, our method does not utilize any external datasets or require additional training, which makes our method widely applicable to existing pre-trained segmentation models. Such a straightforward approach achieves a new state-of-the-art performance on the publicly available Fishyscapes Lost & Found leaderboard with a large margin. Our code is publicly available at this $\href{https://github.com/shjung13/Standardized-max-logits}{link}$.



### Dense Supervision Propagation for Weakly Supervised Semantic Segmentation on 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2107.11267v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.11267v2)
- **Published**: 2021-07-23 14:34:57+00:00
- **Updated**: 2021-09-03 09:42:00+00:00
- **Authors**: Jiacheng Wei, Guosheng Lin, Kim-Hui Yap, Fayao Liu, Tzu-Yi Hung
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation on 3D point clouds is an important task for 3D scene understanding. While dense labeling on 3D data is expensive and time-consuming, only a few works address weakly supervised semantic point cloud segmentation methods to relieve the labeling cost by learning from simpler and cheaper labels. Meanwhile, there are still huge performance gaps between existing weakly supervised methods and state-of-the-art fully supervised methods. In this paper, we train a semantic point cloud segmentation network with only a small portion of points being labeled. We argue that we can better utilize the limited supervision information as we densely propagate the supervision signal from the labeled points to other points within and across the input samples. Specifically, we propose a cross-sample feature reallocating module to transfer similar features and therefore re-route the gradients across two samples with common classes and an intra-sample feature redistribution module to propagate supervision signals on unlabeled points across and within point cloud samples. We conduct extensive experiments on public datasets S3DIS and ScanNet. Our weakly supervised method with only 10\% and 1\% of labels can produce compatible results with the fully supervised counterpart.



### Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation: A Baseline Investigation
- **Arxiv ID**: http://arxiv.org/abs/2107.11279v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.11279v2)
- **Published**: 2021-07-23 14:45:14+00:00
- **Updated**: 2021-07-26 06:11:54+00:00
- **Authors**: Ruifei He, Jihan Yang, Xiaojuan Qi
- **Comment**: ICCV 2021 (oral)
- **Journal**: None
- **Summary**: While self-training has advanced semi-supervised semantic segmentation, it severely suffers from the long-tailed class distribution on real-world semantic segmentation datasets that make the pseudo-labeled data bias toward majority classes. In this paper, we present a simple and yet effective Distribution Alignment and Random Sampling (DARS) method to produce unbiased pseudo labels that match the true class distribution estimated from the labeled data. Besides, we also contribute a progressive data augmentation and labeling strategy to facilitate model training with pseudo-labeled data. Experiments on both Cityscapes and PASCAL VOC 2012 datasets demonstrate the effectiveness of our approach. Albeit simple, our method performs favorably in comparison with state-of-the-art approaches. Code will be available at https://github.com/CVMI-Lab/DARS.



### Human Pose Regression with Residual Log-likelihood Estimation
- **Arxiv ID**: http://arxiv.org/abs/2107.11291v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.11291v3)
- **Published**: 2021-07-23 15:06:31+00:00
- **Updated**: 2021-07-31 09:21:16+00:00
- **Authors**: Jiefeng Li, Siyuan Bian, Ailing Zeng, Can Wang, Bo Pang, Wentao Liu, Cewu Lu
- **Comment**: ICCV 2021 Oral
- **Journal**: None
- **Summary**: Heatmap-based methods dominate in the field of human pose estimation by modelling the output distribution through likelihood heatmaps. In contrast, regression-based methods are more efficient but suffer from inferior performance. In this work, we explore maximum likelihood estimation (MLE) to develop an efficient and effective regression-based methods. From the perspective of MLE, adopting different regression losses is making different assumptions about the output density function. A density function closer to the true distribution leads to a better regression performance. In light of this, we propose a novel regression paradigm with Residual Log-likelihood Estimation (RLE) to capture the underlying output distribution. Concretely, RLE learns the change of the distribution instead of the unreferenced underlying distribution to facilitate the training process. With the proposed reparameterization design, our method is compatible with off-the-shelf flow models. The proposed method is effective, efficient and flexible. We show its potential in various human pose estimation tasks with comprehensive experiments. Compared to the conventional regression paradigm, regression with RLE bring 12.4 mAP improvement on MSCOCO without any test-time overhead. Moreover, for the first time, especially on multi-person pose estimation, our regression method is superior to the heatmap-based methods. Our code is available at https://github.com/Jeff-sjtu/res-loglikelihood-regression



### SurfaceNet: Adversarial SVBRDF Estimation from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2107.11298v1
- **DOI**: 10.1109/ICCV48922.2021
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.11298v1)
- **Published**: 2021-07-23 15:18:54+00:00
- **Updated**: 2021-07-23 15:18:54+00:00
- **Authors**: Giuseppe Vecchio, Simone Palazzo, Concetto Spampinato
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present SurfaceNet, an approach for estimating spatially-varying bidirectional reflectance distribution function (SVBRDF) material properties from a single image. We pose the problem as an image translation task and propose a novel patch-based generative adversarial network (GAN) that is able to produce high-quality, high-resolution surface reflectance maps. The employment of the GAN paradigm has a twofold objective: 1) allowing the model to recover finer details than standard translation models; 2) reducing the domain shift between synthetic and real data distributions in an unsupervised way. An extensive evaluation, carried out on a public benchmark of synthetic and real images under different illumination conditions, shows that SurfaceNet largely outperforms existing SVBRDF reconstruction methods, both quantitatively and qualitatively. Furthermore, SurfaceNet exhibits a remarkable ability in generating high-quality maps from real samples without any supervision at training time.



### Provident Vehicle Detection at Night for Advanced Driver Assistance Systems
- **Arxiv ID**: http://arxiv.org/abs/2107.11302v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.11302v3)
- **Published**: 2021-07-23 15:27:17+00:00
- **Updated**: 2022-04-08 07:21:58+00:00
- **Authors**: Lukas Ewecker, Ebubekir Asan, Lars Ohnemus, Sascha Saralajew
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, computer vision algorithms have become more powerful. However, current algorithms mainly share one limitation: They rely on directly visible objects. This is a significant drawback compared to human behavior, where visual cues caused by objects (e.g., shadows) are already used intuitively to retrieve information or anticipate occurring objects. While driving at night, this performance deficit becomes even more obvious: Humans already process the light artifacts caused by the headlamps of oncoming vehicles to estimate where they appear, whereas current object detection systems require that the oncoming vehicle is directly visible before it can be detected. Based on previous work on this subject, in this paper, we present a complete system that can detect light artifacts caused by the headlights of oncoming vehicles so that it detects that a vehicle is approaching providently. For that, an entire algorithm architecture is investigated, including the detection in the image space, the three-dimensional localization, and the tracking of light artifacts. To demonstrate the usefulness of such an algorithm, the proposed algorithm is deployed in a test vehicle to use the detected light artifacts to control the glare-free high beam system proactively. Using this experimental setting, the provident vehicle detection system's time benefit compared to an in-production computer vision system is quantified. Additionally, the glare-free high beam use case provides a real-time and real-world visualization interface of the detection results by considering the adaptive headlamps as projectors. With this investigation of provident vehicle detection, we want to put awareness on the unconventional sensing task of detecting objects providently and further close the performance gap between human behavior and computer vision algorithms to bring autonomous and automated driving a step forward.



### Mixed SIGNals: Sign Language Production via a Mixture of Motion Primitives
- **Arxiv ID**: http://arxiv.org/abs/2107.11317v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.11317v2)
- **Published**: 2021-07-23 15:53:11+00:00
- **Updated**: 2021-07-26 09:13:33+00:00
- **Authors**: Ben Saunders, Necati Cihan Camgoz, Richard Bowden
- **Comment**: None
- **Journal**: International Conference of Computer Vision (ICCV 2021)
- **Summary**: It is common practice to represent spoken languages at their phonetic level. However, for sign languages, this implies breaking motion into its constituent motion primitives. Avatar based Sign Language Production (SLP) has traditionally done just this, building up animation from sequences of hand motions, shapes and facial expressions. However, more recent deep learning based solutions to SLP have tackled the problem using a single network that estimates the full skeletal structure.   We propose splitting the SLP task into two distinct jointly-trained sub-tasks. The first translation sub-task translates from spoken language to a latent sign language representation, with gloss supervision. Subsequently, the animation sub-task aims to produce expressive sign language sequences that closely resemble the learnt spatio-temporal representation. Using a progressive transformer for the translation sub-task, we propose a novel Mixture of Motion Primitives (MoMP) architecture for sign language animation. A set of distinct motion primitives are learnt during training, that can be temporally combined at inference to animate continuous sign language sequences.   We evaluate on the challenging RWTH-PHOENIX-Weather-2014T(PHOENIX14T) dataset, presenting extensive ablation studies and showing that MoMP outperforms baselines in user evaluations. We achieve state-of-the-art back translation performance with an 11% improvement over competing results. Importantly, and for the first time, we showcase stronger performance for a full translation pipeline going from spoken language to sign, than from gloss to sign.



### Tackling the Overestimation of Forest Carbon with Deep Learning and Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/2107.11320v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.11320v2)
- **Published**: 2021-07-23 15:59:52+00:00
- **Updated**: 2021-08-19 12:04:58+00:00
- **Authors**: Gyri Reiersen, David Dao, Björn Lütjens, Konstantin Klemmer, Xiaoxiang Zhu, Ce Zhang
- **Comment**: Spotlight talk at the Tackling Climate Change with Machine Learning
  workshop at the ICML 2021 https://www.climatechange.ai/papers/icml2021/79
- **Journal**: None
- **Summary**: Forest carbon offsets are increasingly popular and can play a significant role in financing climate mitigation, forest conservation, and reforestation. Measuring how much carbon is stored in forests is, however, still largely done via expensive, time-consuming, and sometimes unaccountable field measurements. To overcome these limitations, many verification bodies are leveraging machine learning (ML) algorithms to estimate forest carbon from satellite or aerial imagery. Aerial imagery allows for tree species or family classification, which improves the satellite imagery-based forest type classification. However, aerial imagery is significantly more expensive to collect and it is unclear by how much the higher resolution improves the forest carbon estimation. This proposal paper describes the first systematic comparison of forest carbon estimation from aerial imagery, satellite imagery, and ground-truth field measurements via deep learning-based algorithms for a tropical reforestation project. Our initial results show that forest carbon estimates from satellite imagery can overestimate above-ground biomass by up to 10-times for tropical reforestation projects. The significant difference between aerial and satellite-derived forest carbon measurements shows the potential for aerial imagery-based ML algorithms and raises the importance to extend this study to a global benchmark between options for carbon measurements.



### Continuous Non-Invasive Eye Tracking In Intensive Care
- **Arxiv ID**: http://arxiv.org/abs/2108.01439v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.01439v1)
- **Published**: 2021-07-23 16:19:35+00:00
- **Updated**: 2021-07-23 16:19:35+00:00
- **Authors**: Ahmed Al-Hindawi, Marcela Paula Vizcaychipi, Yiannis Demiris
- **Comment**: 4 pages, 4 figures. Accepted into EMBC
- **Journal**: None
- **Summary**: Delirium, an acute confusional state, is a common occurrence in Intensive Care Units (ICUs). Patients who develop delirium have globally worse outcomes than those who do not and thus the diagnosis of delirium is of importance. Current diagnostic methods have several limitations leading to the suggestion of eye-tracking for its diagnosis through in-attention. To ascertain the requirements for an eye-tracking system in an adult ICU, measurements were carried out at Chelsea & Westminster Hospital NHS Foundation Trust. Clinical criteria guided empirical requirements of invasiveness and calibration methods while accuracy and precision were measured. A non-invasive system was then developed utilising a patient-facing RGB-camera and a scene-facing RGBD-camera. The system's performance was measured in a replicated laboratory environment with healthy volunteers revealing an accuracy and precision that outperforms what is required while simultaneously being non-invasive and calibration-free The system was then deployed as part CONfuSED, a clinical feasibility study where we report aggregated data from 5 patients as well as the acceptability of the system to bedside nursing staff. The system is the first eye-tracking system to be deployed in an ICU.



### Unsupervised Domain Adaptive 3D Detection with Multi-Level Consistency
- **Arxiv ID**: http://arxiv.org/abs/2107.11355v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.11355v2)
- **Published**: 2021-07-23 17:19:23+00:00
- **Updated**: 2021-08-18 03:25:24+00:00
- **Authors**: Zhipeng Luo, Zhongang Cai, Changqing Zhou, Gongjie Zhang, Haiyu Zhao, Shuai Yi, Shijian Lu, Hongsheng Li, Shanghang Zhang, Ziwei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based 3D object detection has achieved unprecedented success with the advent of large-scale autonomous driving datasets. However, drastic performance degradation remains a critical challenge for cross-domain deployment. In addition, existing 3D domain adaptive detection methods often assume prior access to the target domain annotations, which is rarely feasible in the real world. To address this challenge, we study a more realistic setting, unsupervised 3D domain adaptive detection, which only utilizes source domain annotations. 1) We first comprehensively investigate the major underlying factors of the domain gap in 3D detection. Our key insight is that geometric mismatch is the key factor of domain shift. 2) Then, we propose a novel and unified framework, Multi-Level Consistency Network (MLC-Net), which employs a teacher-student paradigm to generate adaptive and reliable pseudo-targets. MLC-Net exploits point-, instance- and neural statistics-level consistency to facilitate cross-domain transfer. Extensive experiments demonstrate that MLC-Net outperforms existing state-of-the-art methods (including those using additional target domain information) on standard benchmarks. Notably, our approach is detector-agnostic, which achieves consistent gains on both single- and two-stage 3D detectors.



### Robust Explainability: A Tutorial on Gradient-Based Attribution Methods for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.11400v4
- **DOI**: 10.1109/MSP.2022.3142719
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.11400v4)
- **Published**: 2021-07-23 18:06:29+00:00
- **Updated**: 2022-01-13 19:58:42+00:00
- **Authors**: Ian E. Nielsen, Dimah Dera, Ghulam Rasool, Nidhal Bouaynaya, Ravi P. Ramachandran
- **Comment**: 23 pages, 4 figures
- **Journal**: None
- **Summary**: With the rise of deep neural networks, the challenge of explaining the predictions of these networks has become increasingly recognized. While many methods for explaining the decisions of deep neural networks exist, there is currently no consensus on how to evaluate them. On the other hand, robustness is a popular topic for deep learning research; however, it is hardly talked about in explainability until very recently. In this tutorial paper, we start by presenting gradient-based interpretability methods. These techniques use gradient signals to assign the burden of the decision on the input features. Later, we discuss how gradient-based methods can be evaluated for their robustness and the role that adversarial robustness plays in having meaningful explanations. We also discuss the limitations of gradient-based methods. Finally, we present the best practices and attributes that should be examined before choosing an explainability method. We conclude with the future directions for research in the area at the convergence of robustness and explainability.



### Estimation of excess air coefficient on coal combustion processes via gauss model and artificial neural network
- **Arxiv ID**: http://arxiv.org/abs/2108.04180v1
- **DOI**: 10.1016/j.aej.2021.06.022
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.04180v1)
- **Published**: 2021-07-23 18:47:56+00:00
- **Updated**: 2021-07-23 18:47:56+00:00
- **Authors**: Sedat Golgiyaz, Muhammed Fatih Talu, Mahmut Daskin, Cem Onat
- **Comment**: None
- **Journal**: None
- **Summary**: It is no doubt that the most important contributing cause of global efficiency of coal fired thermal systems is combustion efficiency. In this study, the relationship between the flame image obtained by a CCD camera and the excess air coefficient ({\lambda}) has been modelled. The model has been obtained with a three-stage approach: 1) Data collection and synchronization: Obtaining the flame images by means of a CCD camera mounted on a 10 cm diameter observation port, {\lambda} data has been coordinately measured and recorded by the flue gas analyzer. 2) Feature extraction: Gridding the flame image, it is divided into small pieces. The uniformity of each piece to the optimal flame image has been calculated by means of modelling with single and multivariable Gaussian, calculating of color probabilities and Gauss mixture approach. 3) Matching and testing: A multilayer artificial neural network (ANN) has been used for the matching of feature-{\lambda}.



### Compressing Neural Networks: Towards Determining the Optimal Layer-wise Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2107.11442v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.11442v2)
- **Published**: 2021-07-23 20:01:30+00:00
- **Updated**: 2021-11-18 19:59:50+00:00
- **Authors**: Lucas Liebenwein, Alaa Maalouf, Oren Gal, Dan Feldman, Daniela Rus
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: We present a novel global compression framework for deep neural networks that automatically analyzes each layer to identify the optimal per-layer compression ratio, while simultaneously achieving the desired overall compression. Our algorithm hinges on the idea of compressing each convolutional (or fully-connected) layer by slicing its channels into multiple groups and decomposing each group via low-rank decomposition. At the core of our algorithm is the derivation of layer-wise error bounds from the Eckart Young Mirsky theorem. We then leverage these bounds to frame the compression problem as an optimization problem where we wish to minimize the maximum compression error across layers and propose an efficient algorithm towards a solution. Our experiments indicate that our method outperforms existing low-rank compression approaches across a wide range of networks and data sets. We believe that our results open up new avenues for future research into the global performance-size trade-offs of modern neural networks. Our code is available at https://github.com/lucaslie/torchprune.



### Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation
- **Arxiv ID**: http://arxiv.org/abs/2107.11443v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.11443v2)
- **Published**: 2021-07-23 20:04:01+00:00
- **Updated**: 2021-08-17 18:41:00+00:00
- **Authors**: Jiabo Huang, Yang Liu, Shaogang Gong, Hailin Jin
- **Comment**: International Conference on Computer Vision (ICCV'21)
- **Journal**: None
- **Summary**: Video activity localisation has recently attained increasing attention due to its practical values in automatically localising the most salient visual segments corresponding to their language descriptions (sentences) from untrimmed and unstructured videos. For supervised model training, a temporal annotation of both the start and end time index of each video segment for a sentence (a video moment) must be given. This is not only very expensive but also sensitive to ambiguity and subjective annotation bias, a much harder task than image labelling. In this work, we develop a more accurate weakly-supervised solution by introducing Cross-Sentence Relations Mining (CRM) in video moment proposal generation and matching when only a paragraph description of activities without per-sentence temporal annotation is available. Specifically, we explore two cross-sentence relational constraints: (1) Temporal ordering and (2) semantic consistency among sentences in a paragraph description of video activities. Existing weakly-supervised techniques only consider within-sentence video segment correlations in training without considering cross-sentence paragraph context. This can mislead due to ambiguous expressions of individual sentences with visually indiscriminate video moment proposals in isolation. Experiments on two publicly available activity localisation datasets show the advantages of our approach over the state-of-the-art weakly supervised methods, especially so when the video activity descriptions become more complex.



### Deep Learning Based Cardiac MRI Segmentation: Do We Need Experts?
- **Arxiv ID**: http://arxiv.org/abs/2107.11447v1
- **DOI**: 10.3390/a14070212
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.11447v1)
- **Published**: 2021-07-23 20:10:58+00:00
- **Updated**: 2021-07-23 20:10:58+00:00
- **Authors**: Youssef Skandarani, Pierre-Marc Jodoin, Alain Lalande
- **Comment**: None
- **Journal**: Algorithms 2021, 14(7), 212
- **Summary**: Deep learning methods are the de-facto solutions to a multitude of medical image analysis tasks. Cardiac MRI segmentation is one such application which, like many others, requires a large number of annotated data so a trained network can generalize well. Unfortunately, the process of having a large number of manually curated images by medical experts is both slow and utterly expensive. In this paper, we set out to explore whether expert knowledge is a strict requirement for the creation of annotated datasets that machine learning can successfully train on. To do so, we gauged the performance of three segmentation models, namely U-Net, Attention U-Net, and ENet, trained with different loss functions on expert and non-expert groundtruth for cardiac cine-MRI segmentation. Evaluation was done with classic segmentation metrics (Dice index and Hausdorff distance) as well as clinical measurements, such as the ventricular ejection fractions and the myocardial mass. Results reveal that generalization performances of a segmentation neural network trained on non-expert groundtruth data is, to all practical purposes, as good as on expert groundtruth data, in particular when the non-expert gets a decent level of training, highlighting an opportunity for the efficient and cheap creation of annotations for cardiac datasets.



### Using a Cross-Task Grid of Linear Probes to Interpret CNN Model Predictions On Retinal Images
- **Arxiv ID**: http://arxiv.org/abs/2107.11468v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.11468v1)
- **Published**: 2021-07-23 21:30:27+00:00
- **Updated**: 2021-07-23 21:30:27+00:00
- **Authors**: Katy Blumer, Subhashini Venugopalan, Michael P. Brenner, Jon Kleinberg
- **Comment**: Extended abstract at Interpretable Machine Learning in Healthcare
  (IMLH) workshop at ICML 2021
- **Journal**: None
- **Summary**: We analyze a dataset of retinal images using linear probes: linear regression models trained on some "target" task, using embeddings from a deep convolutional (CNN) model trained on some "source" task as input. We use this method across all possible pairings of 93 tasks in the UK Biobank dataset of retinal images, leading to ~164k different models. We analyze the performance of these linear probes by source and target task and by layer depth. We observe that representations from the middle layers of the network are more generalizable. We find that some target tasks are easily predicted irrespective of the source task, and that some other target tasks are more accurately predicted from correlated source tasks than from embeddings trained on the same task.



### Multi-Echo LiDAR for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.11470v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.11470v1)
- **Published**: 2021-07-23 21:43:09+00:00
- **Updated**: 2021-07-23 21:43:09+00:00
- **Authors**: Yunze Man, Xinshuo Weng, Prasanna Kumar Sivakuma, Matthew O'Toole, Kris Kitani
- **Comment**: None
- **Journal**: None
- **Summary**: LiDAR sensors can be used to obtain a wide range of measurement signals other than a simple 3D point cloud, and those signals can be leveraged to improve perception tasks like 3D object detection. A single laser pulse can be partially reflected by multiple objects along its path, resulting in multiple measurements called echoes. Multi-echo measurement can provide information about object contours and semi-transparent surfaces which can be used to better identify and locate objects. LiDAR can also measure surface reflectance (intensity of laser pulse return), as well as ambient light of the scene (sunlight reflected by objects). These signals are already available in commercial LiDAR devices but have not been used in most LiDAR-based detection models. We present a 3D object detection model which leverages the full spectrum of measurement signals provided by LiDAR. First, we propose a multi-signal fusion (MSF) module to combine (1) the reflectance and ambient features extracted with a 2D CNN, and (2) point cloud features extracted using a 3D graph neural network (GNN). Second, we propose a multi-echo aggregation (MEA) module to combine the information encoded in different set of echo points. Compared with traditional single echo point cloud methods, our proposed Multi-Signal LiDAR Detector (MSLiD) extracts richer context information from a wider range of sensing measurements and achieves more accurate 3D object detection. Experiments show that by incorporating the multi-modality of LiDAR, our method outperforms the state-of-the-art by up to 9.1%.



### Clipped Hyperbolic Classifiers Are Super-Hyperbolic Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2107.11472v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.11472v4)
- **Published**: 2021-07-23 22:10:16+00:00
- **Updated**: 2022-05-13 18:55:50+00:00
- **Authors**: Yunhui Guo, Xudong Wang, Yubei Chen, Stella X. Yu
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Hyperbolic space can naturally embed hierarchies, unlike Euclidean space. Hyperbolic Neural Networks (HNNs) exploit such representational power by lifting Euclidean features into hyperbolic space for classification, outperforming Euclidean neural networks (ENNs) on datasets with known semantic hierarchies. However, HNNs underperform ENNs on standard benchmarks without clear hierarchies, greatly restricting HNNs' applicability in practice.   Our key insight is that HNNs' poorer general classification performance results from vanishing gradients during backpropagation, caused by their hybrid architecture connecting Euclidean features to a hyperbolic classifier. We propose an effective solution by simply clipping the Euclidean feature magnitude while training HNNs.   Our experiments demonstrate that clipped HNNs become super-hyperbolic classifiers: They are not only consistently better than HNNs which already outperform ENNs on hierarchical data, but also on-par with ENNs on MNIST, CIFAR10, CIFAR100 and ImageNet benchmarks, with better adversarial robustness and out-of-distribution detection.



