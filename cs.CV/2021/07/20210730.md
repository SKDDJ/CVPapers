# Arxiv Papers in cs.CV on 2021-07-30
### Exploring Low-light Object Detection Techniques
- **Arxiv ID**: http://arxiv.org/abs/2107.14382v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.14382v1)
- **Published**: 2021-07-30 01:11:11+00:00
- **Updated**: 2021-07-30 01:11:11+00:00
- **Authors**: Winston Chen, Tejas Shah
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: Images acquired by computer vision systems under low light conditions have multiple characteristics like high noise, lousy illumination, reflectance, and bad contrast, which make object detection tasks difficult. Much work has been done to enhance images using various pixel manipulation techniques, as well as deep neural networks - some focused on improving the illumination, while some on reducing the noise. Similarly, considerable research has been done in object detection neural network models. In our work, we break down the problem into two phases: 1)First, we explore which image enhancement algorithm is more suited for object detection tasks, where accurate feature retrieval is more important than good image quality. Specifically, we look at basic histogram equalization techniques and unpaired image translation techniques. 2)In the second phase, we explore different object detection models that can be applied to the enhanced image. We conclude by comparing all results, calculating mean average precisions (mAP), and giving some directions for future work.



### Real-time Streaming Perception System for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2107.14388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.14388v1)
- **Published**: 2021-07-30 01:32:44+00:00
- **Updated**: 2021-07-30 01:32:44+00:00
- **Authors**: Yongxiang Gu, Qianlei Wang, Xiaolin Qin
- **Comment**: 6 pages,6 figures
- **Journal**: None
- **Summary**: Nowadays, plenty of deep learning technologies are being applied to all aspects of autonomous driving with promising results. Among them, object detection is the key to improve the ability of an autonomous agent to perceive its environment so that it can (re)act. However, previous vision-based object detectors cannot achieve satisfactory performance under real-time driving scenarios. To remedy this, we present the real-time steaming perception system in this paper, which is also the 2nd Place solution of Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) for the detection-only track. Unlike traditional object detection challenges, which focus mainly on the absolute performance, streaming perception task requires achieving a balance of accuracy and latency, which is crucial for real-time autonomous driving. We adopt YOLOv5 as our basic framework, data augmentation, Bag-of-Freebies, and Transformer are adopted to improve streaming object detection performance with negligible extra inference cost. On the Argoverse-HD test set, our method achieves 33.2 streaming AP (34.6 streaming AP verified by the organizer) under the required hardware. Its performance significantly surpasses the fixed baseline of 13.6 (host team), demonstrating the potentiality of application.



### DarkLighter: Light Up the Darkness for UAV Tracking
- **Arxiv ID**: http://arxiv.org/abs/2107.14389v2
- **DOI**: 10.1109/IROS51168.2021.9636680
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.14389v2)
- **Published**: 2021-07-30 01:37:24+00:00
- **Updated**: 2022-03-27 07:09:17+00:00
- **Authors**: Junjie Ye, Changhong Fu, Guangze Zheng, Ziang Cao, Bowen Li
- **Comment**: accepted by IROS2021
- **Journal**: None
- **Summary**: Recent years have witnessed the fast evolution and promising performance of the convolutional neural network (CNN)-based trackers, which aim at imitating biological visual systems. However, current CNN-based trackers can hardly generalize well to low-light scenes that are commonly lacked in the existing training set. In indistinguishable night scenarios frequently encountered in unmanned aerial vehicle (UAV) tracking-based applications, the robustness of the state-of-the-art (SOTA) trackers drops significantly. To facilitate aerial tracking in the dark through a general fashion, this work proposes a low-light image enhancer namely DarkLighter, which dedicates to alleviate the impact of poor illumination and noise iteratively. A lightweight map estimation network, i.e., ME-Net, is trained to efficiently estimate illumination maps and noise maps jointly. Experiments are conducted with several SOTA trackers on numerous UAV dark tracking scenes. Exhaustive evaluations demonstrate the reliability and universality of DarkLighter, with high efficiency. Moreover, DarkLighter has further been implemented on a typical UAV system. Real-world tests at night scenes have verified its practicability and dependability.



### From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.14391v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.14391v1)
- **Published**: 2021-07-30 02:00:06+00:00
- **Updated**: 2021-07-30 02:00:06+00:00
- **Authors**: Jiajun Deng, Wengang Zhou, Yanyong Zhang, Houqiang Li
- **Comment**: This paper has been accepted by IEEE TCSVT
- **Journal**: None
- **Summary**: As an emerging data modal with precise distance sensing, LiDAR point clouds have been placed great expectations on 3D scene understanding. However, point clouds are always sparsely distributed in the 3D space, and with unstructured storage, which makes it difficult to represent them for effective 3D object detection. To this end, in this work, we regard point clouds as hollow-3D data and propose a new architecture, namely Hallucinated Hollow-3D R-CNN ($\text{H}^2$3D R-CNN), to address the problem of 3D object detection. In our approach, we first extract the multi-view features by sequentially projecting the point clouds into the perspective view and the bird-eye view. Then, we hallucinate the 3D representation by a novel bilaterally guided multi-view fusion block. Finally, the 3D objects are detected via a box refinement module with a novel Hierarchical Voxel RoI Pooling operation. The proposed $\text{H}^2$3D R-CNN provides a new angle to take full advantage of complementary information in the perspective view and the bird-eye view with an efficient framework. We evaluate our approach on the public KITTI Dataset and Waymo Open Dataset. Extensive experiments demonstrate the superiority of our method over the state-of-the-art algorithms with respect to both effectiveness and efficiency. The code will be made available at \url{https://github.com/djiajunustc/H-23D_R-CNN}.



### Self-Supervised Regional and Temporal Auxiliary Tasks for Facial Action Unit Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.14399v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.14399v1)
- **Published**: 2021-07-30 02:39:45+00:00
- **Updated**: 2021-07-30 02:39:45+00:00
- **Authors**: Jingwei Yan, Jingjing Wang, Qiang Li, Chunmao Wang, Shiliang Pu
- **Comment**: The first two authors contributed equally to this work. Accepted for
  publication in the 29th ACM International Conference on Multimedia (ACMMM
  '21)
- **Journal**: None
- **Summary**: Automatic facial action unit (AU) recognition is a challenging task due to the scarcity of manual annotations. To alleviate this problem, a large amount of efforts has been dedicated to exploiting various methods which leverage numerous unlabeled data. However, many aspects with regard to some unique properties of AUs, such as the regional and relational characteristics, are not sufficiently explored in previous works. Motivated by this, we take the AU properties into consideration and propose two auxiliary AU related tasks to bridge the gap between limited annotations and the model performance in a self-supervised manner via the unlabeled data. Specifically, to enhance the discrimination of regional features with AU relation embedding, we design a task of RoI inpainting to recover the randomly cropped AU patches. Meanwhile, a single image based optical flow estimation task is proposed to leverage the dynamic change of facial muscles and encode the motion information into the global feature representation. Based on these two self-supervised auxiliary tasks, local features, mutual relation and motion cues of AUs are better captured in the backbone network with the proposed regional and temporal based auxiliary task learning (RTATL) framework. Extensive experiments on BP4D and DISFA demonstrate the superiority of our method and new state-of-the-art performances are achieved.



### Enhancing Social Relation Inference with Concise Interaction Graph and Discriminative Scene Representation
- **Arxiv ID**: http://arxiv.org/abs/2107.14425v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.14425v1)
- **Published**: 2021-07-30 04:20:13+00:00
- **Updated**: 2021-07-30 04:20:13+00:00
- **Authors**: Xiaotian Yu, Hanling Yi, Yi Yu, Ling Xing, Shiliang Zhang, Xiaoyu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: There has been a recent surge of research interest in attacking the problem of social relation inference based on images. Existing works classify social relations mainly by creating complicated graphs of human interactions, or learning the foreground and/or background information of persons and objects, but ignore holistic scene context. The holistic scene refers to the functionality of a place in images, such as dinning room, playground and office. In this paper, by mimicking human understanding on images, we propose an approach of \textbf{PR}actical \textbf{I}nference in \textbf{S}ocial r\textbf{E}lation (PRISE), which concisely learns interactive features of persons and discriminative features of holistic scenes. Technically, we develop a simple and fast relational graph convolutional network to capture interactive features of all persons in one image. To learn the holistic scene feature, we elaborately design a contrastive learning task based on image scene classification. To further boost the performance in social relation inference, we collect and distribute a new large-scale dataset, which consists of about 240 thousand unlabeled images. The extensive experimental results show that our novel learning framework significantly beats the state-of-the-art methods, e.g., PRISE achieves 6.8$\%$ improvement for domain classification in PIPA dataset.



### Dynamic Neural Representational Decoders for High-Resolution Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.14428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.14428v1)
- **Published**: 2021-07-30 04:50:56+00:00
- **Updated**: 2021-07-30 04:50:56+00:00
- **Authors**: Bowen Zhang, Yifan Liu, Zhi Tian, Chunhua Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation requires per-pixel prediction for a given image. Typically, the output resolution of a segmentation network is severely reduced due to the downsampling operations in the CNN backbone. Most previous methods employ upsampling decoders to recover the spatial resolution. Various decoders were designed in the literature. Here, we propose a novel decoder, termed dynamic neural representational decoder (NRD), which is simple yet significantly more efficient. As each location on the encoder's output corresponds to a local patch of the semantic labels, in this work, we represent these local patches of labels with compact neural networks. This neural representation enables our decoder to leverage the smoothness prior in the semantic label space, and thus makes our decoder more efficient. Furthermore, these neural representations are dynamically generated and conditioned on the outputs of the encoder networks. The desired semantic labels can be efficiently decoded from the neural representations, resulting in high-resolution semantic segmentation predictions. We empirically show that our proposed decoder can outperform the decoder in DeeplabV3+ with only 30% computational complexity, and achieve competitive performance with the methods using dilated encoders with only 15% computation. Experiments on the Cityscapes, ADE20K, and PASCAL Context datasets demonstrate the effectiveness and efficiency of our proposed method.



### Single image deep defocus estimation and its applications
- **Arxiv ID**: http://arxiv.org/abs/2107.14443v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.14443v2)
- **Published**: 2021-07-30 06:18:16+00:00
- **Updated**: 2021-12-14 03:26:16+00:00
- **Authors**: Fernando J. Galetto, Guang Deng
- **Comment**: None
- **Journal**: None
- **Summary**: Depth information is useful in many image processing applications. However, since taking a picture is a process of projection of a 3D scene onto a 2D imaging sensor, the depth information is embedded in the image. Extracting the depth information from the image is a challenging task. A guiding principle is that the level of blurriness due to defocus is related to the distance between the object and the focal plane. Based on this principle and the widely used assumption that Gaussian blur is a good model for defocus blur, we formulate the problem of estimating the spatially varying defocus blurriness as a Gaussian blur classification problem. We solved the problem by training a deep neural network to classify image patches into one of the 20 levels of blurriness. We have created a dataset of more than 500000 image patches of size $32\times32$ which are used to train and test several well-known network models. We find that MobileNetV2 is suitable for this application due to its low memory requirement and high accuracy. The trained model is used to determine the patch blurriness which is then refined by applying an iterative weighted guided filter. The result is a defocus map that carries the information of the degree of blurriness for each pixel. We compare the proposed method with state-of-the-art techniques and we demonstrate its successful applications in adaptive image enhancement, defocus magnification, and multi-focus image fusion.



### Manipulating Identical Filter Redundancy for Efficient Pruning on Deep and Complicated CNN
- **Arxiv ID**: http://arxiv.org/abs/2107.14444v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.14444v1)
- **Published**: 2021-07-30 06:18:19+00:00
- **Updated**: 2021-07-30 06:18:19+00:00
- **Authors**: Xiaohan Ding, Tianxiang Hao, Jungong Han, Yuchen Guo, Guiguang Ding
- **Comment**: Extension of the CVPR-2019 paper
  (https://openaccess.thecvf.com/content_CVPR_2019/papers/Ding_Centripetal_SGD_for_Pruning_Very_Deep_Convolutional_Networks_With_Complicated_CVPR_2019_paper.pdf).
  arXiv admin note: substantial text overlap with arXiv:1904.03837
- **Journal**: None
- **Summary**: The existence of redundancy in Convolutional Neural Networks (CNNs) enables us to remove some filters/channels with acceptable performance drops. However, the training objective of CNNs usually tends to minimize an accuracy-related loss function without any attention paid to the redundancy, making the redundancy distribute randomly on all the filters, such that removing any of them may trigger information loss and accuracy drop, necessitating a following finetuning step for recovery. In this paper, we propose to manipulate the redundancy during training to facilitate network pruning. To this end, we propose a novel Centripetal SGD (C-SGD) to make some filters identical, resulting in ideal redundancy patterns, as such filters become purely redundant due to their duplicates; hence removing them does not harm the network. As shown on CIFAR and ImageNet, C-SGD delivers better performance because the redundancy is better organized, compared to the existing methods. The efficiency also characterizes C-SGD because it is as fast as regular SGD, requires no finetuning, and can be conducted simultaneously on all the layers even in very deep CNNs. Besides, C-SGD can improve the accuracy of CNNs by first training a model with the same architecture but wider layers then squeezing it into the original width.



### T-SVDNet: Exploring High-Order Prototypical Correlations for Multi-Source Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2107.14447v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.14447v1)
- **Published**: 2021-07-30 06:33:05+00:00
- **Updated**: 2021-07-30 06:33:05+00:00
- **Authors**: Ruihuang Li, Xu Jia, Jianzhong He, Shuaijun Chen, Qinghua Hu
- **Comment**: 11 pages, 6 figures, International Conference on Computer Vision
- **Journal**: None
- **Summary**: Most existing domain adaptation methods focus on adaptation from only one source domain, however, in practice there are a number of relevant sources that could be leveraged to help improve performance on target domain. We propose a novel approach named T-SVDNet to address the task of Multi-source Domain Adaptation (MDA), which is featured by incorporating Tensor Singular Value Decomposition (T-SVD) into a neural network's training pipeline. Overall, high-order correlations among multiple domains and categories are fully explored so as to better bridge the domain gap. Specifically, we impose Tensor-Low-Rank (TLR) constraint on a tensor obtained by stacking up a group of prototypical similarity matrices, aiming at capturing consistent data structure across different domains. Furthermore, to avoid negative transfer brought by noisy source data, we propose a novel uncertainty-aware weighting strategy to adaptively assign weights to different source domains and samples based on the result of uncertainty estimation. Extensive experiments conducted on public benchmarks demonstrate the superiority of our model in addressing the task of MDA compared to state-of-the-art methods.



### Synth-by-Reg (SbR): Contrastive learning for synthesis-based registration of paired images
- **Arxiv ID**: http://arxiv.org/abs/2107.14449v3
- **DOI**: 10.1007/978-3-030-87592-3_5
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.14449v3)
- **Published**: 2021-07-30 06:40:58+00:00
- **Updated**: 2023-01-27 08:26:30+00:00
- **Authors**: Adrià Casamitjana, Matteo Mancini, Juan Eugenio Iglesias
- **Comment**: Best paper award in Simulation and Synthesis in Medical Imaging
  (SASHIMI) workshop
- **Journal**: Simulation and Synthesis in Medical Imaging. SASHIMI 2021. Lecture
  Notes in Computer Science
- **Summary**: Nonlinear inter-modality registration is often challenging due to the lack of objective functions that are good proxies for alignment. Here we propose a synthesis-by-registration method to convert this problem into an easier intra-modality task. We introduce a registration loss for weakly supervised image translation between domains that does not require perfectly aligned training data. This loss capitalises on a registration U-Net with frozen weights, to drive a synthesis CNN towards the desired translation. We complement this loss with a structure preserving constraint based on contrastive learning, which prevents blurring and content shifts due to overfitting. We apply this method to the registration of histological sections to MRI slices, a key step in 3D histology reconstruction. Results on two different public datasets show improvements over registration based on mutual information (13% reduction in landmark error) and synthesis-based algorithms such as CycleGAN (11% reduction), and are comparable to a registration CNN with label supervision. Code and data are publicly available at \url{https://github.com/acasamitjana/SynthByReg}



### DPT: Deformable Patch-based Transformer for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.14467v1
- **DOI**: 10.1145/3474085.3475467
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.14467v1)
- **Published**: 2021-07-30 07:33:17+00:00
- **Updated**: 2021-07-30 07:33:17+00:00
- **Authors**: Zhiyang Chen, Yousong Zhu, Chaoyang Zhao, Guosheng Hu, Wei Zeng, Jinqiao Wang, Ming Tang
- **Comment**: In Proceedings of the 29th ACM International Conference on Multimedia
  (MM '21)
- **Journal**: None
- **Summary**: Transformer has achieved great success in computer vision, while how to split patches in an image remains a problem. Existing methods usually use a fixed-size patch embedding which might destroy the semantics of objects. To address this problem, we propose a new Deformable Patch (DePatch) module which learns to adaptively split the images into patches with different positions and scales in a data-driven way rather than using predefined fixed patches. In this way, our method can well preserve the semantics in patches. The DePatch module can work as a plug-and-play module, which can easily be incorporated into different transformers to achieve an end-to-end training. We term this DePatch-embedded transformer as Deformable Patch-based Transformer (DPT) and conduct extensive evaluations of DPT on image classification and object detection. Results show DPT can achieve 81.9% top-1 accuracy on ImageNet classification, and 43.7% box mAP with RetinaNet, 44.3% with Mask R-CNN on MSCOCO object detection. Code has been made available at: https://github.com/CASIA-IVA-Lab/DPT .



### Medical Instrument Segmentation in 3D US by Hybrid Constrained Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.14476v1
- **DOI**: 10.1109/JBHI.2021.3101872
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.14476v1)
- **Published**: 2021-07-30 07:59:45+00:00
- **Updated**: 2021-07-30 07:59:45+00:00
- **Authors**: Hongxu Yang, Caifeng Shan, R. Arthur Bouwman, Lukas R. C. Dekker, Alexander F. Kolen, Peter H. N. de With
- **Comment**: Accepted by IEEE JBHI
- **Journal**: None
- **Summary**: Medical instrument segmentation in 3D ultrasound is essential for image-guided intervention. However, to train a successful deep neural network for instrument segmentation, a large number of labeled images are required, which is expensive and time-consuming to obtain. In this article, we propose a semi-supervised learning (SSL) framework for instrument segmentation in 3D US, which requires much less annotation effort than the existing methods. To achieve the SSL learning, a Dual-UNet is proposed to segment the instrument. The Dual-UNet leverages unlabeled data using a novel hybrid loss function, consisting of uncertainty and contextual constraints. Specifically, the uncertainty constraints leverage the uncertainty estimation of the predictions of the UNet, and therefore improve the unlabeled information for SSL training. In addition, contextual constraints exploit the contextual information of the training images, which are used as the complementary information for voxel-wise uncertainty estimation. Extensive experiments on multiple ex-vivo and in-vivo datasets show that our proposed method achieves Dice score of about 68.6%-69.1% and the inference time of about 1 sec. per volume. These results are better than the state-of-the-art SSL methods and the inference time is comparable to the supervised approaches.



### OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery Detection And Segmentation In-The-Wild
- **Arxiv ID**: http://arxiv.org/abs/2107.14480v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.14480v1)
- **Published**: 2021-07-30 08:15:41+00:00
- **Updated**: 2021-07-30 08:15:41+00:00
- **Authors**: Trung-Nghia Le, Huy H. Nguyen, Junichi Yamagishi, Isao Echizen
- **Comment**: Accepted to ICCV 2021. Project page:
  https://sites.google.com/view/ltnghia/research/openforensics
- **Journal**: None
- **Summary**: The proliferation of deepfake media is raising concerns among the public and relevant authorities. It has become essential to develop countermeasures against forged faces in social media. This paper presents a comprehensive study on two new countermeasure tasks: multi-face forgery detection and segmentation in-the-wild. Localizing forged faces among multiple human faces in unrestricted natural scenes is far more challenging than the traditional deepfake recognition task. To promote these new tasks, we have created the first large-scale dataset posing a high level of challenges that is designed with face-wise rich annotations explicitly for face forgery detection and segmentation, namely OpenForensics. With its rich annotations, our OpenForensics dataset has great potentials for research in both deepfake prevention and general human face detection. We have also developed a suite of benchmarks for these tasks by conducting an extensive evaluation of state-of-the-art instance detection and segmentation methods on our newly constructed dataset in various scenarios. The dataset, benchmark results, codes, and supplementary materials will be publicly available on our project page: https://sites.google.com/view/ltnghia/research/openforensics



### ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale Demonstrations
- **Arxiv ID**: http://arxiv.org/abs/2107.14483v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.14483v5)
- **Published**: 2021-07-30 08:20:22+00:00
- **Updated**: 2021-11-04 12:11:21+00:00
- **Authors**: Tongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Yang, Xuanlin Li, Stone Tao, Zhiao Huang, Zhiwei Jia, Hao Su
- **Comment**: NeurIPS 2021 Track on Datasets and Benchmarks; code:
  https://github.com/haosulab/ManiSkill
- **Journal**: None
- **Summary**: Object manipulation from 3D visual inputs poses many challenges on building generalizable perception and policy models. However, 3D assets in existing benchmarks mostly lack the diversity of 3D shapes that align with real-world intra-class complexity in topology and geometry. Here we propose SAPIEN Manipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over diverse objects in a full-physics simulator. 3D assets in ManiSkill include large intra-class topological and geometric variations. Tasks are carefully chosen to cover distinct types of manipulation challenges. Latest progress in 3D vision also makes us believe that we should customize the benchmark so that the challenge is inviting to researchers working on 3D deep learning. To this end, we simulate a moving panoramic camera that returns ego-centric point clouds or RGB-D images. In addition, we would like ManiSkill to serve a broad set of researchers interested in manipulation research. Besides supporting the learning of policies from interactions, we also support learning-from-demonstrations (LfD) methods, by providing a large number of high-quality demonstrations (~36,000 successful trajectories, ~1.5M point cloud/RGB-D frames in total). We provide baselines using 3D deep learning and LfD algorithms. All code of our benchmark (simulator, environment, SDK, and baselines) is open-sourced, and a challenge facing interdisciplinary researchers will be held based on the benchmark.



### Pix2Point: Learning Outdoor 3D Using Sparse Point Clouds and Optimal Transport
- **Arxiv ID**: http://arxiv.org/abs/2107.14498v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.5; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2107.14498v1)
- **Published**: 2021-07-30 09:03:39+00:00
- **Updated**: 2021-07-30 09:03:39+00:00
- **Authors**: Rémy Leroy, Pauline Trouvé-Peloux, Frédéric Champagnat, Bertrand Le Saux, Marcela Carvalho
- **Comment**: 5 pages, 2 figures, to be published in 2021 International Conference
  on Machine Vision Applications
- **Journal**: None
- **Summary**: Good quality reconstruction and comprehension of a scene rely on 3D estimation methods. The 3D information was usually obtained from images by stereo-photogrammetry, but deep learning has recently provided us with excellent results for monocular depth estimation. Building up a sufficiently large and rich training dataset to achieve these results requires onerous processing. In this paper, we address the problem of learning outdoor 3D point cloud from monocular data using a sparse ground-truth dataset. We propose Pix2Point, a deep learning-based approach for monocular 3D point cloud prediction, able to deal with complete and challenging outdoor scenes. Our method relies on a 2D-3D hybrid neural network architecture, and a supervised end-to-end minimisation of an optimal transport divergence between point clouds. We show that, when trained on sparse point clouds, our simple promising approach achieves a better coverage of 3D outdoor scenes than efficient monocular depth methods.



### Fourier Series Expansion Based Filter Parametrization for Equivariant Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2107.14519v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.14519v2)
- **Published**: 2021-07-30 10:01:52+00:00
- **Updated**: 2023-03-09 09:13:47+00:00
- **Authors**: Qi Xie, Qian Zhao, Zongben Xu, Deyu Meng
- **Comment**: 15 pages, 8 figures
- **Journal**: IEEE transactions on pattern analysis and machine intelligence
  2022
- **Summary**: It has been shown that equivariant convolution is very helpful for many types of computer vision tasks. Recently, the 2D filter parametrization technique plays an important role when designing equivariant convolutions. However, the current filter parametrization method still has its evident drawbacks, where the most critical one lies in the accuracy problem of filter representation. Against this issue, in this paper we modify the classical Fourier series expansion for 2D filters, and propose a new set of atomic basis functions for filter parametrization. The proposed filter parametrization method not only finely represents 2D filters with zero error when the filter is not rotated, but also substantially alleviates the fence-effect-caused quality degradation when the filter is rotated. Accordingly, we construct a new equivariant convolution method based on the proposed filter parametrization method, named F-Conv. We prove that the equivariance of the proposed F-Conv is exact in the continuous domain, which becomes approximate only after discretization. Extensive experiments show the superiority of the proposed method. Particularly, we adopt rotation equivariant convolution methods to image super-resolution task, and F-Conv evidently outperforms previous filter parametrization based method in this task, reflecting its intrinsic capability of faithfully preserving rotation symmetries in local image features.



### The Minimum Edit Arborescence Problem and Its Use in Compressing Graph Collections [Extended Version]
- **Arxiv ID**: http://arxiv.org/abs/2107.14525v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/2107.14525v1)
- **Published**: 2021-07-30 10:18:57+00:00
- **Updated**: 2021-07-30 10:18:57+00:00
- **Authors**: Lucas Gnecco, Nicolas Boria, Sébastien Bougleux, Florian Yger, David B. Blumenthal
- **Comment**: None
- **Journal**: None
- **Summary**: The inference of minimum spanning arborescences within a set of objects is a general problem which translates into numerous application-specific unsupervised learning tasks. We introduce a unified and generic structure called edit arborescence that relies on edit paths between data in a collection, as well as the Min Edit Arborescence Problem, which asks for an edit arborescence that minimizes the sum of costs of its inner edit paths. Through the use of suitable cost functions, this generic framework allows to model a variety of problems. In particular, we show that by introducing encoding size preserving edit costs, it can be used as an efficient method for compressing collections of labeled graphs. Experiments on various graph datasets, with comparisons to standard compression tools, show the potential of our method.



### Recognizing Emotions evoked by Movies using Multitask Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.14529v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.14529v1)
- **Published**: 2021-07-30 10:21:40+00:00
- **Updated**: 2021-07-30 10:21:40+00:00
- **Authors**: Hassan Hayat, Carles Ventura, Agata Lapedriza
- **Comment**: Accepted to the International Conference on Affective Computing and
  Intelligent Interaction (ACII) 2021
- **Journal**: None
- **Summary**: Understanding the emotional impact of movies has become important for affective movie analysis, ranking, and indexing. Methods for recognizing evoked emotions are usually trained on human annotated data. Concretely, viewers watch video clips and have to manually annotate the emotions they experienced while watching the videos. Then, the common practice is to aggregate the different annotations, by computing average scores or majority voting, and train and test models on these aggregated annotations. With this procedure a single aggregated evoked emotion annotation is obtained per each video. However, emotions experienced while watching a video are subjective: different individuals might experience different emotions. In this paper, we model the emotions evoked by videos in a different manner: instead of modeling the aggregated value we jointly model the emotions experienced by each viewer and the aggregated value using a multi-task learning approach. Concretely, we propose two deep learning architectures: a Single-Task (ST) architecture and a Multi-Task (MT) architecture. Our results show that the MT approach can more accurately model each viewer and the aggregated annotation when compared to methods that are directly trained on the aggregated annotations. Furthermore, our approach outperforms the current state-of-the-art results on the COGNIMUSE benchmark.



### Topological Similarity Index and Loss Function for Blood Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.14531v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.2.10; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2107.14531v1)
- **Published**: 2021-07-30 10:24:47+00:00
- **Updated**: 2021-07-30 10:24:47+00:00
- **Authors**: R. J. Araújo, J. S. Cardoso, H. P. Oliveira
- **Comment**: 10 pages, 1 table, 8 images, to be submitted to IEEE-TMI
- **Journal**: None
- **Summary**: Blood vessel segmentation is one of the most studied topics in computer vision, due to its relevance in daily clinical practice. Despite the evolution the field has been facing, especially after the dawn of deep learning, important challenges are still not solved. One of them concerns the consistency of the topological properties of the vascular trees, given that the best performing methodologies do not directly penalize mistakes such as broken segments and end up producing predictions with disconnected trees. This is particularly relevant in graph-like structures, such as blood vessel trees, given that it puts at risk the characterization steps that follow the segmentation task. In this paper, we propose a similarity index which captures the topological consistency of the predicted segmentations having as reference the ground truth. We also design a novel loss function based on the morphological closing operator and show how it allows to learn deep neural network models which produce more topologically coherent masks. Our experiments target well known retinal benchmarks and a coronary angiogram database.



### Shadow Art Revisited: A Differentiable Rendering Based Approach
- **Arxiv ID**: http://arxiv.org/abs/2107.14539v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.14539v1)
- **Published**: 2021-07-30 10:43:48+00:00
- **Updated**: 2021-07-30 10:43:48+00:00
- **Authors**: Kaustubh Sadekar, Ashish Tiwari, Shanmuganathan Raman
- **Comment**: Accepted in WACV 2022
- **Journal**: None
- **Summary**: While recent learning based methods have been observed to be superior for several vision-related applications, their potential in generating artistic effects has not been explored much. One such interesting application is Shadow Art - a unique form of sculptural art where 2D shadows cast by a 3D sculpture produce artistic effects. In this work, we revisit shadow art using differentiable rendering based optimization frameworks to obtain the 3D sculpture from a set of shadow (binary) images and their corresponding projection information. Specifically, we discuss shape optimization through voxel as well as mesh-based differentiable renderers. Our choice of using differentiable rendering for generating shadow art sculptures can be attributed to its ability to learn the underlying 3D geometry solely from image data, thus reducing the dependence on 3D ground truth. The qualitative and quantitative results demonstrate the potential of the proposed framework in generating complex 3D sculptures that go beyond those seen in contemporary art pieces using just a set of shadow images as input. Further, we demonstrate the generation of 3D sculptures to cast shadows of faces, animated movie characters, and applicability of the framework to sketch-based 3D reconstruction of underlying shapes.



### Iterative, Deep, and Unsupervised Synthetic Aperture Sonar Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.14563v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.14563v1)
- **Published**: 2021-07-30 11:37:33+00:00
- **Updated**: 2021-07-30 11:37:33+00:00
- **Authors**: Yung-Chen Sun, Isaac D. Gerg, Vishal Monga
- **Comment**: IEEE OCEANS 2021
- **Journal**: None
- **Summary**: Deep learning has not been routinely employed for semantic segmentation of seabed environment for synthetic aperture sonar (SAS) imagery due to the implicit need of abundant training data such methods necessitate. Abundant training data, specifically pixel-level labels for all images, is usually not available for SAS imagery due to the complex logistics (e.g., diver survey, chase boat, precision position information) needed for obtaining accurate ground-truth. Many hand-crafted feature based algorithms have been proposed to segment SAS in an unsupervised fashion. However, there is still room for improvement as the feature extraction step of these methods is fixed. In this work, we present a new iterative unsupervised algorithm for learning deep features for SAS image segmentation. Our proposed algorithm alternates between clustering superpixels and updating the parameters of a convolutional neural network (CNN) so that the feature extraction for image segmentation can be optimized. We demonstrate the efficacy of our method on a realistic benchmark dataset. Our results show that the performance of our proposed method is considerably better than current state-of-the-art methods in SAS image segmentation.



### Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-modal Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2107.14572v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.14572v2)
- **Published**: 2021-07-30 12:11:24+00:00
- **Updated**: 2021-08-09 14:58:36+00:00
- **Authors**: Xunlin Zhan, Yangxin Wu, Xiao Dong, Yunchao Wei, Minlong Lu, Yichi Zhang, Hang Xu, Xiaodan Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, customer's demands for E-commerce are more diversified, which introduces more complications to the product retrieval industry. Previous methods are either subject to single-modal input or perform supervised image-level product retrieval, thus fail to accommodate real-life scenarios where enormous weakly annotated multi-modal data are present. In this paper, we investigate a more realistic setting that aims to perform weakly-supervised multi-modal instance-level product retrieval among fine-grained product categories. To promote the study of this challenging task, we contribute Product1M, one of the largest multi-modal cosmetic datasets for real-world instance-level retrieval. Notably, Product1M contains over 1 million image-caption pairs and consists of two sample types, i.e., single-product and multi-product samples, which encompass a wide variety of cosmetics brands. In addition to the great diversity, Product1M enjoys several appealing characteristics including fine-grained categories, complex combinations, and fuzzy correspondence that well mimic the real-world scenes. Moreover, we propose a novel model named Cross-modal contrAstive Product Transformer for instance-level prodUct REtrieval (CAPTURE), that excels in capturing the potential synergy between multi-modal inputs via a hybrid-stream transformer in a self-supervised manner.CAPTURE generates discriminative instance features via masked multi-modal learning as well as cross-modal contrastive pretraining and it outperforms several SOTA cross-modal baselines. Extensive ablation studies well demonstrate the effectiveness and the generalization capacity of our model. Dataset and codes are available at https: //github.com/zhanxlin/Product1M.



### SNE-RoadSeg+: Rethinking Depth-Normal Translation and Deep Supervision for Freespace Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.14599v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.14599v2)
- **Published**: 2021-07-30 12:50:05+00:00
- **Updated**: 2021-09-19 08:00:34+00:00
- **Authors**: Hengli Wang, Rui Fan, Peide Cai, Ming Liu
- **Comment**: Fix a mistake in Equation 3. 7 pages, 5 figures and 2 tables. This
  paper is accepted by IROS 2021
- **Journal**: None
- **Summary**: Freespace detection is a fundamental component of autonomous driving perception. Recently, deep convolutional neural networks (DCNNs) have achieved impressive performance for this task. In particular, SNE-RoadSeg, our previously proposed method based on a surface normal estimator (SNE) and a data-fusion DCNN (RoadSeg), has achieved impressive performance in freespace detection. However, SNE-RoadSeg is computationally intensive, and it is difficult to execute in real time. To address this problem, we introduce SNE-RoadSeg+, an upgraded version of SNE-RoadSeg. SNE-RoadSeg+ consists of 1) SNE+, a module for more accurate surface normal estimation, and 2) RoadSeg+, a data-fusion DCNN that can greatly minimize the trade-off between accuracy and efficiency with the use of deep supervision. Extensive experimental results have demonstrated the effectiveness of our SNE+ for surface normal estimation and the superior performance of our SNE-RoadSeg+ over all other freespace detection approaches. Specifically, our SNE-RoadSeg+ runs in real time, and meanwhile, achieves the state-of-the-art performance on the KITTI road benchmark. Our project page is at https://www.sne-roadseg.site/sne-roadseg-plus.



### Automatic Vocabulary and Graph Verification for Accurate Loop Closure Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.14611v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.14611v1)
- **Published**: 2021-07-30 13:19:33+00:00
- **Updated**: 2021-07-30 13:19:33+00:00
- **Authors**: Haosong Yue, Jinyu Miao, Weihai Chen, Wei Wang, Fanghong Guo, Zhengguo Li
- **Comment**: 11 pages, 9 figures
- **Journal**: None
- **Summary**: Localizing pre-visited places during long-term simultaneous localization and mapping, i.e. loop closure detection (LCD), is a crucial technique to correct accumulated inconsistencies. As one of the most effective and efficient solutions, Bag-of-Words (BoW) builds a visual vocabulary to associate features and then detect loops. Most existing approaches that build vocabularies off-line determine scales of the vocabulary by trial-and-error, which often results in unreasonable feature association. Moreover, the accuracy of the algorithm usually declines due to perceptual aliasing, as the BoW-based method ignores the positions of visual features. To overcome these disadvantages, we propose a natural convergence criterion based on the comparison between the radii of nodes and the drifts of feature descriptors, which is then utilized to build the optimal vocabulary automatically. Furthermore, we present a novel topological graph verification method for validating candidate loops so that geometrical positions of the words can be involved with a negligible increase in complexity, which can significantly improve the accuracy of LCD. Experiments on various public datasets and comparisons against several state-of-the-art algorithms verify the performance of our proposed approach.



### Instant Visual Odometry Initialization for Mobile AR
- **Arxiv ID**: http://arxiv.org/abs/2107.14659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.14659v1)
- **Published**: 2021-07-30 14:25:40+00:00
- **Updated**: 2021-07-30 14:25:40+00:00
- **Authors**: Alejo Concha, Michael Burri, Jesús Briales, Christian Forster, Luc Oth
- **Comment**: None
- **Journal**: ISMAR, TVCG 2021
- **Summary**: Mobile AR applications benefit from fast initialization to display world-locked effects instantly. However, standard visual odometry or SLAM algorithms require motion parallax to initialize (see Figure 1) and, therefore, suffer from delayed initialization. In this paper, we present a 6-DoF monocular visual odometry that initializes instantly and without motion parallax. Our main contribution is a pose estimator that decouples estimating the 5-DoF relative rotation and translation direction from the 1-DoF translation magnitude. While scale is not observable in a monocular vision-only setting, it is still paramount to estimate a consistent scale over the whole trajectory (even if not physically accurate) to avoid AR effects moving erroneously along depth. In our approach, we leverage the fact that depth errors are not perceivable to the user during rotation-only motion. However, as the user starts translating the device, depth becomes perceivable and so does the capability to estimate consistent scale. Our proposed algorithm naturally transitions between these two modes. We perform extensive validations of our contributions with both a publicly available dataset and synthetic data. We show that the proposed pose estimator outperforms the classical approaches for 6-DoF pose estimation used in the literature in low-parallax configurations. We release a dataset for the relative pose problem using real data to facilitate the comparison with future solutions for the relative pose problem. Our solution is either used as a full odometry or as a preSLAM component of any supported SLAM system (ARKit, ARCore) in world-locked AR effects on platforms such as Instagram and Facebook.



### Can non-specialists provide high quality gold standard labels in challenging modalities?
- **Arxiv ID**: http://arxiv.org/abs/2107.14682v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.14682v1)
- **Published**: 2021-07-30 15:03:15+00:00
- **Updated**: 2021-07-30 15:03:15+00:00
- **Authors**: Samuel Budd, Thomas Day, John Simpson, Karen Lloyd, Jacqueline Matthew, Emily Skelton, Reza Razavi, Bernhard Kainz
- **Comment**: Accepted at the FAIR workshop in conjunction with MICCAI'21
- **Journal**: None
- **Summary**: Probably yes. -- Supervised Deep Learning dominates performance scores for many computer vision tasks and defines the state-of-the-art. However, medical image analysis lags behind natural image applications. One of the many reasons is the lack of well annotated medical image data available to researchers. One of the first things researchers are told is that we require significant expertise to reliably and accurately interpret and label such data. We see significant inter- and intra-observer variability between expert annotations of medical images. Still, it is a widely held assumption that novice annotators are unable to provide useful annotations for use by clinical Deep Learning models. In this work we challenge this assumption and examine the implications of using a minimally trained novice labelling workforce to acquire annotations for a complex medical image dataset. We study the time and cost implications of using novice annotators, the raw performance of novice annotators compared to gold-standard expert annotators, and the downstream effects on a trained Deep Learning segmentation model's performance for detecting a specific congenital heart disease (hypoplastic left heart syndrome) in fetal ultrasound imaging.



### High-Resolution Depth Maps Based on TOF-Stereo Fusion
- **Arxiv ID**: http://arxiv.org/abs/2107.14688v1
- **DOI**: 10.1109/ICRA.2012.6224771
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.14688v1)
- **Published**: 2021-07-30 15:11:42+00:00
- **Updated**: 2021-07-30 15:11:42+00:00
- **Authors**: Vineet Gandhi, Jan Cech, Radu Horaud
- **Comment**: IEEE International Conference on Robotics and Automation, 2012
- **Journal**: None
- **Summary**: The combination of range sensors with color cameras can be very useful for robot navigation, semantic perception, manipulation, and telepresence. Several methods of combining range- and color-data have been investigated and successfully used in various robotic applications. Most of these systems suffer from the problems of noise in the range-data and resolution mismatch between the range sensor and the color cameras, since the resolution of current range sensors is much less than the resolution of color cameras. High-resolution depth maps can be obtained using stereo matching, but this often fails to construct accurate depth maps of weakly/repetitively textured scenes, or if the scene exhibits complex self-occlusions. Range sensors provide coarse depth information regardless of presence/absence of texture. The use of a calibrated system, composed of a time-of-flight (TOF) camera and of a stereoscopic camera pair, allows data fusion thus overcoming the weaknesses of both individual sensors. We propose a novel TOF-stereo fusion method based on an efficient seed-growing algorithm which uses the TOF data projected onto the stereo image pair as an initial set of correspondences. These initial "seeds" are then propagated based on a Bayesian model which combines an image similarity score with rough depth priors computed from the low-resolution range data. The overall result is a dense and accurate depth map at the resolution of the color cameras at hand. We show that the proposed algorithm outperforms 2D image-based stereo algorithms and that the results are of higher resolution than off-the-shelf color-range sensors, e.g., Kinect. Moreover, the algorithm potentially exhibits real-time performance on a single CPU.



### Seeing poverty from space, how much can it be tuned?
- **Arxiv ID**: http://arxiv.org/abs/2107.14700v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.14700v1)
- **Published**: 2021-07-30 15:23:54+00:00
- **Updated**: 2021-07-30 15:23:54+00:00
- **Authors**: Tomas Sako, Arturo Jr M. Martinez
- **Comment**: 19 pages
- **Journal**: None
- **Summary**: Since the United Nations launched the Sustainable Development Goals (SDG) in 2015, numerous universities, NGOs and other organizations have attempted to develop tools for monitoring worldwide progress in achieving them. Led by advancements in the fields of earth observation techniques, data sciences and the emergence of artificial intelligence, a number of research teams have developed innovative tools for highlighting areas of vulnerability and tracking the implementation of SDG targets. In this paper we demonstrate that individuals with no organizational affiliation and equipped only with common hardware, publicly available datasets and cloud-based computing services can participate in the improvement of predicting machine-learning-based approaches to predicting local poverty levels in a given agro-ecological environment. The approach builds upon several pioneering efforts over the last five years related to mapping poverty by deep learning to process satellite imagery and "ground-truth" data from the field to link features with incidence of poverty in a particular context. The approach employs new methods for object identification in order to optimize the modeled results and achieve significantly high accuracy. A key goal of the project was to intentionally keep costs as low as possible - by using freely available resources - so that citizen scientists, students and organizations could replicate the method in other areas of interest. Moreover, for simplicity, the input data used were derived from just a handful of sources (involving only earth observation and population headcounts). The results of the project could therefore certainly be strengthened further through the integration of proprietary data from social networks, mobile phone providers, and other sources.



### When Deep Learners Change Their Mind: Learning Dynamics for Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.14707v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.14707v1)
- **Published**: 2021-07-30 15:30:17+00:00
- **Updated**: 2021-07-30 15:30:17+00:00
- **Authors**: Javad Zolfaghari Bengar, Bogdan Raducanu, Joost van de Weijer
- **Comment**: Accepted in International Conference on Computer Analysis of Images
  and Patterns (CAIP) 2021
- **Journal**: None
- **Summary**: Active learning aims to select samples to be annotated that yield the largest performance improvement for the learning algorithm. Many methods approach this problem by measuring the informativeness of samples and do this based on the certainty of the network predictions for samples. However, it is well-known that neural networks are overly confident about their prediction and are therefore an untrustworthy source to assess sample informativeness. In this paper, we propose a new informativeness-based active learning method. Our measure is derived from the learning dynamics of a neural network. More precisely we track the label assignment of the unlabeled data pool during the training of the algorithm. We capture the learning dynamics with a metric called label-dispersion, which is low when the network consistently assigns the same label to the sample during the training of the network and high when the assigned label changes frequently. We show that label-dispersion is a promising predictor of the uncertainty of the network, and show on two benchmark datasets that an active learning algorithm based on label-dispersion obtains excellent results.



### Sparse-to-dense Feature Matching: Intra and Inter domain Cross-modal Learning in Domain Adaptation for 3D Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.14724v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.14724v5)
- **Published**: 2021-07-30 15:55:55+00:00
- **Updated**: 2021-08-08 01:57:48+00:00
- **Authors**: Duo Peng, Yinjie Lei, Wen Li, Pingping Zhang, Yulan Guo
- **Comment**: 10 pages,6 figures,accepted at ICCV 2021
- **Journal**: None
- **Summary**: Domain adaptation is critical for success when confronting with the lack of annotations in a new domain. As the huge time consumption of labeling process on 3D point cloud, domain adaptation for 3D semantic segmentation is of great expectation. With the rise of multi-modal datasets, large amount of 2D images are accessible besides 3D point clouds. In light of this, we propose to further leverage 2D data for 3D domain adaptation by intra and inter domain cross modal learning. As for intra-domain cross modal learning, most existing works sample the dense 2D pixel-wise features into the same size with sparse 3D point-wise features, resulting in the abandon of numerous useful 2D features. To address this problem, we propose Dynamic sparse-to-dense Cross Modal Learning (DsCML) to increase the sufficiency of multi-modality information interaction for domain adaptation. For inter-domain cross modal learning, we further advance Cross Modal Adversarial Learning (CMAL) on 2D and 3D data which contains different semantic content aiming to promote high-level modal complementarity. We evaluate our model under various multi-modality domain adaptation settings including day-to-night, country-to-country and dataset-to-dataset, brings large improvements over both uni-modal and multi-modal domain adaptation methods on all settings.



### Neural Relighting and Expression Transfer On Video Portraits
- **Arxiv ID**: http://arxiv.org/abs/2107.14735v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2107.14735v4)
- **Published**: 2021-07-30 16:20:45+00:00
- **Updated**: 2021-12-09 06:13:47+00:00
- **Authors**: Youjia Wang, Taotao Zhou, Minzhang Li, Teng Xu, Minye Wu, Lan Xu, Jingyi Yu
- **Comment**: Project Page
  https://miaoing.github.io/Neural-Relighting-and-Expression-Transfer-on-Video-Portraits/
- **Journal**: None
- **Summary**: Photo-realistic video portrait reenactment benefits virtual production and numerous VR/AR experiences. The task remains challenging as the reenacted expression should match the source while the lighting should be adjustable to new environments. We present a neural relighting and expression transfer technique to transfer the facial expressions from a source performer to a portrait video of a target performer while enabling dynamic relighting. Our approach employs 4D reflectance field learning, model-based facial performance capture and target-aware neural rendering. Specifically, given a short sequence of the target performer's OLAT, we apply a rendering-to-video translation network to first synthesize the OLAT result of new sequences with unseen expressions. We then design a semantic-aware facial normalization scheme along with a multi-frame multi-task learning strategy to encode the content, segmentation, and motion flows for reliably inferring the reflectance field. This allows us to simultaneously control facial expression and apply virtual relighting. Extensive experiments demonstrate that our technique can robustly handle challenging expressions and lighting environments and produce results at a cinematographic quality.



### On the Efficacy of Small Self-Supervised Contrastive Models without Distillation Signals
- **Arxiv ID**: http://arxiv.org/abs/2107.14762v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.14762v2)
- **Published**: 2021-07-30 17:10:05+00:00
- **Updated**: 2021-12-13 15:22:08+00:00
- **Authors**: Haizhou Shi, Youcai Zhang, Siliang Tang, Wenjie Zhu, Yaqian Li, Yandong Guo, Yueting Zhuang
- **Comment**: Accepted by AAAI'22
- **Journal**: None
- **Summary**: It is a consensus that small models perform quite poorly under the paradigm of self-supervised contrastive learning. Existing methods usually adopt a large off-the-shelf model to transfer knowledge to the small one via distillation. Despite their effectiveness, distillation-based methods may not be suitable for some resource-restricted scenarios due to the huge computational expenses of deploying a large model. In this paper, we study the issue of training self-supervised small models without distillation signals. We first evaluate the representation spaces of the small models and make two non-negligible observations: (i) the small models can complete the pretext task without overfitting despite their limited capacity and (ii) they universally suffer the problem of over clustering. Then we verify multiple assumptions that are considered to alleviate the over-clustering phenomenon. Finally, we combine the validated techniques and improve the baseline performances of five small architectures with considerable margins, which indicates that training small self-supervised contrastive models is feasible even without distillation signals. The code is available at \textit{https://github.com/WOWNICE/ssl-small}.



### Out-of-Core Surface Reconstruction via Global $TGV$ Minimization
- **Arxiv ID**: http://arxiv.org/abs/2107.14790v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.GR, I.4.8; I.3.5; C.2.4
- **Links**: [PDF](http://arxiv.org/pdf/2107.14790v2)
- **Published**: 2021-07-30 17:48:22+00:00
- **Updated**: 2021-08-14 16:45:58+00:00
- **Authors**: Nikolai Poliarnyi
- **Comment**: Accepted to the 2021 IEEE/CVF International Conference on Computer
  Vision (ICCV 2021)
- **Journal**: None
- **Summary**: We present an out-of-core variational approach for surface reconstruction from a set of aligned depth maps. Input depth maps are supposed to be reconstructed from regular photos or/and can be a representation of terrestrial LIDAR point clouds. Our approach is based on surface reconstruction via total generalized variation minimization ($TGV$) because of its strong visibility-based noise-filtering properties and GPU-friendliness. Our main contribution is an out-of-core OpenCL-accelerated adaptation of this numerical algorithm which can handle arbitrarily large real-world scenes with scale diversity.



### Perceiver IO: A General Architecture for Structured Inputs & Outputs
- **Arxiv ID**: http://arxiv.org/abs/2107.14795v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2107.14795v3)
- **Published**: 2021-07-30 17:53:34+00:00
- **Updated**: 2022-03-15 22:37:19+00:00
- **Authors**: Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, Joāo Carreira
- **Comment**: ICLR 2022 camera ready. Code: https://dpmd.ai/perceiver-code
- **Journal**: None
- **Summary**: A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain & task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.



### Multi-Head Self-Attention via Vision Transformer for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.00045v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.00045v1)
- **Published**: 2021-07-30 19:08:44+00:00
- **Updated**: 2021-07-30 19:08:44+00:00
- **Authors**: Faisal Alamri, Anjan Dutta
- **Comment**: Irish Machine Vision and Image Processing Conference (IMVIP) 2021
- **Journal**: None
- **Summary**: Zero-Shot Learning (ZSL) aims to recognise unseen object classes, which are not observed during the training phase. The existing body of works on ZSL mostly relies on pretrained visual features and lacks the explicit attribute localisation mechanism on images. In this work, we propose an attention-based model in the problem settings of ZSL to learn attributes useful for unseen class recognition. Our method uses an attention mechanism adapted from Vision Transformer to capture and learn discriminative attributes by splitting images into small patches. We conduct experiments on three popular ZSL benchmarks (i.e., AWA2, CUB and SUN) and set new state-of-the-art harmonic mean results {on all the three datasets}, which illustrate the effectiveness of our proposed method.



### Controlling Weather Field Synthesis Using Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2108.00048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.00048v1)
- **Published**: 2021-07-30 19:17:30+00:00
- **Updated**: 2021-07-30 19:17:30+00:00
- **Authors**: Dario Augusto Borges Oliveira, Jorge Guevara Diaz, Bianca Zadrozny, Campbell Watson
- **Comment**: ICML Climate Change AI Workshop
- **Journal**: None
- **Summary**: One of the consequences of climate change is anobserved increase in the frequency of extreme cli-mate events. That poses a challenge for weatherforecast and generation algorithms, which learnfrom historical data but should embed an often un-certain bias to create correct scenarios. This paperinvestigates how mapping climate data to a knowndistribution using variational autoencoders mighthelp explore such biases and control the synthesisof weather fields towards more extreme climatescenarios. We experimented using a monsoon-affected precipitation dataset from southwest In-dia, which should give a roughly stable pattern ofrainy days and ease our investigation. We reportcompelling results showing that mapping complexweather data to a known distribution implementsan efficient control for weather field synthesis to-wards more (or less) extreme scenarios.



### Object-aware Contrastive Learning for Debiased Scene Representation
- **Arxiv ID**: http://arxiv.org/abs/2108.00049v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.00049v2)
- **Published**: 2021-07-30 19:24:07+00:00
- **Updated**: 2021-10-26 19:00:12+00:00
- **Authors**: Sangwoo Mo, Hyunwoo Kang, Kihyuk Sohn, Chun-Liang Li, Jinwoo Shin
- **Comment**: NeurIPS 2021. First two authors contributed equally
- **Journal**: None
- **Summary**: Contrastive self-supervised learning has shown impressive results in learning visual representations from unlabeled images by enforcing invariance against different data augmentations. However, the learned representations are often contextually biased to the spurious scene correlations of different objects or object and background, which may harm their generalization on the downstream tasks. To tackle the issue, we develop a novel object-aware contrastive learning framework that first (a) localizes objects in a self-supervised manner and then (b) debias scene correlations via appropriate data augmentations considering the inferred object locations. For (a), we propose the contrastive class activation map (ContraCAM), which finds the most discriminative regions (e.g., objects) in the image compared to the other images using the contrastively trained models. We further improve the ContraCAM to detect multiple objects and entire shapes via an iterative refinement procedure. For (b), we introduce two data augmentations based on ContraCAM, object-aware random crop and background mixup, which reduce contextual and background biases during contrastive self-supervised learning, respectively. Our experiments demonstrate the effectiveness of our representation learning framework, particularly when trained under multi-object images or evaluated under the background (and distribution) shifted images.



### MTVR: Multilingual Moment Retrieval in Videos
- **Arxiv ID**: http://arxiv.org/abs/2108.00061v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.00061v1)
- **Published**: 2021-07-30 20:01:03+00:00
- **Updated**: 2021-07-30 20:01:03+00:00
- **Authors**: Jie Lei, Tamara L. Berg, Mohit Bansal
- **Comment**: ACL 2021 (9 pages, 4 figures)
- **Journal**: None
- **Summary**: We introduce mTVR, a large-scale multilingual video moment retrieval dataset, containing 218K English and Chinese queries from 21.8K TV show video clips. The dataset is collected by extending the popular TVR dataset (in English) with paired Chinese queries and subtitles. Compared to existing moment retrieval datasets, mTVR is multilingual, larger, and comes with diverse annotations. We further propose mXML, a multilingual moment retrieval model that learns and operates on data from both languages, via encoder parameter sharing and language neighborhood constraints. We demonstrate the effectiveness of mXML on the newly collected MTVR dataset, where mXML outperforms strong monolingual baselines while using fewer parameters. In addition, we also provide detailed dataset analyses and model ablations. Data and code are publicly available at https://github.com/jayleicn/mTVRetrieval



### A New Semi-supervised Learning Benchmark for Classifying View and Diagnosing Aortic Stenosis from Echocardiograms
- **Arxiv ID**: http://arxiv.org/abs/2108.00080v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2108.00080v1)
- **Published**: 2021-07-30 21:08:12+00:00
- **Updated**: 2021-07-30 21:08:12+00:00
- **Authors**: Zhe Huang, Gary Long, Benjamin Wessler, Michael C. Hughes
- **Comment**: To appear in the Proceedings of the Machine Learning for Healthcare
  (MLHC) conference, 2021. 20 pages (including 7 tables & 3 figures). 13
  additional pages of references and supplementary material
- **Journal**: None
- **Summary**: Semi-supervised image classification has shown substantial progress in learning from limited labeled data, but recent advances remain largely untested for clinical applications. Motivated by the urgent need to improve timely diagnosis of life-threatening heart conditions, especially aortic stenosis, we develop a benchmark dataset to assess semi-supervised approaches to two tasks relevant to cardiac ultrasound (echocardiogram) interpretation: view classification and disease severity classification. We find that a state-of-the-art method called MixMatch achieves promising gains in heldout accuracy on both tasks, learning from a large volume of truly unlabeled images as well as a labeled set collected at great expense to achieve better performance than is possible with the labeled set alone. We further pursue patient-level diagnosis prediction, which requires aggregating across hundreds of images of diverse view types, most of which are irrelevant, to make a coherent prediction. The best patient-level performance is achieved by new methods that prioritize diagnosis predictions from images that are predicted to be clinically-relevant views and transfer knowledge from the view task to the diagnosis task. We hope our released Tufts Medical Echocardiogram Dataset and evaluation framework inspire further improvements in multi-task semi-supervised learning for clinical applications.



### Thermal Image Super-Resolution Using Second-Order Channel Attention with Varying Receptive Fields
- **Arxiv ID**: http://arxiv.org/abs/2108.00094v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2108.00094v1)
- **Published**: 2021-07-30 22:17:51+00:00
- **Updated**: 2021-07-30 22:17:51+00:00
- **Authors**: Nolan B. Gutierrez, William J. Beksi
- **Comment**: To be published in the 2021 13th International Conference on Computer
  Vision Systems (ICVS)
- **Journal**: None
- **Summary**: Thermal images model the long-infrared range of the electromagnetic spectrum and provide meaningful information even when there is no visible illumination. Yet, unlike imagery that represents radiation from the visible continuum, infrared images are inherently low-resolution due to hardware constraints. The restoration of thermal images is critical for applications that involve safety, search and rescue, and military operations. In this paper, we introduce a system to efficiently reconstruct thermal images. Specifically, we explore how to effectively attend to contrasting receptive fields (RFs) where increasing the RFs of a network can be computationally expensive. For this purpose, we introduce a deep attention to varying receptive fields network (AVRFN). We supply a gated convolutional layer with higher-order information extracted from disparate RFs, whereby an RF is parameterized by a dilation rate. In this way, the dilation rate can be tuned to use fewer parameters thus increasing the efficacy of AVRFN. Our experimental results show an improvement over the state of the art when compared against competing thermal image super-resolution methods.



### Deep Feature Tracker: A Novel Application for Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2108.00105v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2108.00105v1)
- **Published**: 2021-07-30 23:24:29+00:00
- **Updated**: 2021-07-30 23:24:29+00:00
- **Authors**: Mostafa Parchami, Saif Iftekar Sayed
- **Comment**: None
- **Journal**: None
- **Summary**: Feature tracking is the building block of many applications such as visual odometry, augmented reality, and target tracking. Unfortunately, the state-of-the-art vision-based tracking algorithms fail in surgical images due to the challenges imposed by the nature of such environments. In this paper, we proposed a novel and unified deep learning-based approach that can learn how to track features reliably as well as learn how to detect such reliable features for tracking purposes. The proposed network dubbed as Deep-PT, consists of a tracker network which is a convolutional neural network simulating cross-correlation in terms of deep learning and two fully connected networks that operate on the output of intermediate layers of the tracker to detect features and predict trackability of the detected points. The ability to detect features based on the capabilities of the tracker distinguishes the proposed method from previous algorithms used in this area and improves the robustness of the algorithms against dynamics of the scene. The network is trained using multiple datasets due to the lack of specialized dataset for feature tracking datasets and extensive comparisons are conducted to compare the accuracy of Deep-PT against recent pixel tracking algorithms. As the experiments suggest, the proposed deep architecture deliberately learns what to track and how to track and outperforms the state-of-the-art methods.



### Comparing object recognition in humans and deep convolutional neural networks -- An eye tracking study
- **Arxiv ID**: http://arxiv.org/abs/2108.00107v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.00107v2)
- **Published**: 2021-07-30 23:32:05+00:00
- **Updated**: 2021-09-21 13:49:26+00:00
- **Authors**: Leonard E. van Dyck, Roland Kwitt, Sebastian J. Denzler, Walter R. Gruber
- **Comment**: 25 pages, 10 figures, 1 table
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNNs) and the ventral visual pathway share vast architectural and functional similarities in visual challenges such as object recognition. Recent insights have demonstrated that both hierarchical cascades can be compared in terms of both exerted behavior and underlying activation. However, these approaches ignore key differences in spatial priorities of information processing. In this proof-of-concept study, we demonstrate a comparison of human observers (N = 45) and three feedforward DCNNs through eye tracking and saliency maps. The results reveal fundamentally different resolutions in both visualization methods that need to be considered for an insightful comparison. Moreover, we provide evidence that a DCNN with biologically plausible receptive field sizes called vNet reveals higher agreement with human viewing behavior as contrasted with a standard ResNet architecture. We find that image-specific factors such as category, animacy, arousal, and valence have a direct link to the agreement of spatial object recognition priorities in humans and DCNNs, while other measures such as difficulty and general image properties do not. With this approach, we try to open up new perspectives at the intersection of biological and computer vision research.



