# Arxiv Papers in cs.CV on 2021-07-09
### Computer-Aided Diagnosis of Low Grade Endometrial Stromal Sarcoma (LGESS)
- **Arxiv ID**: http://arxiv.org/abs/2107.05426v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.05426v1)
- **Published**: 2021-07-09 00:41:18+00:00
- **Updated**: 2021-07-09 00:41:18+00:00
- **Authors**: Xinxin Yang, Mark Stamp
- **Comment**: None
- **Journal**: None
- **Summary**: Low grade endometrial stromal sarcoma (LGESS) is rare form of cancer, accounting for about 0.2% of all uterine cancer cases. Approximately 75% of LGESS patients are initially misdiagnosed with leiomyoma, which is a type of benign tumor, also known as fibroids. In this research, uterine tissue biopsy images of potential LGESS patients are preprocessed using segmentation and staining normalization algorithms. A variety of classic machine learning and leading deep learning models are then applied to classify tissue images as either benign or cancerous. For the classic techniques considered, the highest classification accuracy we attain is about 0.85, while our best deep learning model achieves an accuracy of approximately 0.87. These results indicate that properly trained learning algorithms can play a useful role in the diagnosis of LGESS.



### EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy Communication in Noisy Environments
- **Arxiv ID**: http://arxiv.org/abs/2107.04174v2
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2107.04174v2)
- **Published**: 2021-07-09 02:00:47+00:00
- **Updated**: 2021-10-18 22:37:53+00:00
- **Authors**: Jacob Donley, Vladimir Tourbabin, Jung-Suk Lee, Mark Broyles, Hao Jiang, Jie Shen, Maja Pantic, Vamsi Krishna Ithapu, Ravish Mehra
- **Comment**: Dataset is available at:
  https://github.com/facebookresearch/EasyComDataset
- **Journal**: None
- **Summary**: Augmented Reality (AR) as a platform has the potential to facilitate the reduction of the cocktail party effect. Future AR headsets could potentially leverage information from an array of sensors spanning many different modalities. Training and testing signal processing and machine learning algorithms on tasks such as beam-forming and speech enhancement require high quality representative data. To the best of the author's knowledge, as of publication there are no available datasets that contain synchronized egocentric multi-channel audio and video with dynamic movement and conversations in a noisy environment. In this work, we describe, evaluate and release a dataset that contains over 5 hours of multi-modal data useful for training and testing algorithms for the application of improving conversations for an AR glasses wearer. We provide speech intelligibility, quality and signal-to-noise ratio improvement results for a baseline method and show improvements across all tested metrics. The dataset we are releasing contains AR glasses egocentric multi-channel microphone array audio, wide field-of-view RGB video, speech source pose, headset microphone audio, annotated voice activity, speech transcriptions, head bounding boxes, target of speech and source identification labels. We have created and are releasing this dataset to facilitate research in multi-modal AR solutions to the cocktail party problem.



### A Multi-modal and Multi-task Learning Method for Action Unit and Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2107.04187v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04187v2)
- **Published**: 2021-07-09 03:28:17+00:00
- **Updated**: 2021-07-15 11:14:15+00:00
- **Authors**: Yue Jin, Tianqing Zheng, Chao Gao, Guoqiang Xu
- **Comment**: 5 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: Analyzing human affect is vital for human-computer interaction systems. Most methods are developed in restricted scenarios which are not practical for in-the-wild settings. The Affective Behavior Analysis in-the-wild (ABAW) 2021 Contest provides a benchmark for this in-the-wild problem. In this paper, we introduce a multi-modal and multi-task learning method by using both visual and audio information. We use both AU and expression annotations to train the model and apply a sequence model to further extract associations between video frames. We achieve an AU score of 0.712 and an expression score of 0.477 on the validation set. These results demonstrate the effectiveness of our approach in improving model performance.



### Emotion Recognition with Incomplete Labels Using Modified Multi-task Learning Technique
- **Arxiv ID**: http://arxiv.org/abs/2107.04192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04192v1)
- **Published**: 2021-07-09 03:43:53+00:00
- **Updated**: 2021-07-09 03:43:53+00:00
- **Authors**: Phan Tran Dac Thinh, Hoang Manh Hung, Hyung-Jeong Yang, Soo-Hyung Kim, Guee-Sang Lee
- **Comment**: None
- **Journal**: None
- **Summary**: The task of predicting affective information in the wild such as seven basic emotions or action units from human faces has gradually become more interesting due to the accessibility and availability of massive annotated datasets. In this study, we propose a method that utilizes the association between seven basic emotions and twelve action units from the AffWild2 dataset. The method based on the architecture of ResNet50 involves the multi-task learning technique for the incomplete labels of the two tasks. By combining the knowledge for two correlated tasks, both performances are improved by a large margin compared to those with the model employing only one kind of label.



### Deep Learning models for benign and malign Ocular Tumor Growth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2107.04220v1
- **DOI**: 10.1016/j.compmedimag.2021.101986
- **Categories**: **eess.IV**, cs.CV, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/2107.04220v1)
- **Published**: 2021-07-09 05:40:25+00:00
- **Updated**: 2021-07-09 05:40:25+00:00
- **Authors**: Mayank Goswami
- **Comment**: 22 Page, 10 Figures
- **Journal**: None
- **Summary**: Relatively abundant availability of medical imaging data has provided significant support in the development and testing of Neural Network based image processing methods. Clinicians often face issues in selecting suitable image processing algorithm for medical imaging data. A strategy for the selection of a proper model is presented here. The training data set comprises optical coherence tomography (OCT) and angiography (OCT-A) images of 50 mice eyes with more than 100 days follow-up. The data contains images from treated and untreated mouse eyes. Four deep learning variants are tested for automatic (a) differentiation of tumor region with healthy retinal layer and (b) segmentation of 3D ocular tumor volumes. Exhaustive sensitivity analysis of deep learning models is performed with respect to the number of training and testing images using 8 eight performance indices to study accuracy, reliability/reproducibility, and speed. U-net with UVgg16 is best for malign tumor data set with treatment (having considerable variation) and U-net with Inception backbone for benign tumor data (with minor variation). Loss value and root mean square error (R.M.S.E.) are found most and least sensitive performance indices, respectively. The performance (via indices) is found to be exponentially improving regarding a number of training images. The segmented OCT-Angiography data shows that neovascularization drives the tumor volume. Image analysis shows that photodynamic imaging-assisted tumor treatment protocol is transforming an aggressively growing tumor into a cyst. An empirical expression is obtained to help medical professionals to choose a particular model given the number of images and types of characteristics. We recommend that the presented exercise should be taken as standard practice before employing a particular deep learning model for biomedical image analysis.



### A Multi-task Mean Teacher for Semi-supervised Facial Affective Behavior Analysis
- **Arxiv ID**: http://arxiv.org/abs/2107.04225v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2107.04225v3)
- **Published**: 2021-07-09 05:48:22+00:00
- **Updated**: 2021-08-13 07:10:05+00:00
- **Authors**: Lingfeng Wang, Shisen Wang, Jin Qi, Kenji Suzuki
- **Comment**: None
- **Journal**: None
- **Summary**: Affective Behavior Analysis is an important part in human-computer interaction. Existing multi-task affective behavior recognition methods suffer from the problem of incomplete labeled datasets. To tackle this problem, this paper presents a semi-supervised model with a mean teacher framework to leverage additional unlabeled data. To be specific, a multi-task model is proposed to learn three different kinds of facial affective representations simultaneously. After that, the proposed model is assigned to be student and teacher networks. When training with unlabeled data, the teacher network is employed to predict pseudo labels for student network training, which allows it to learn from unlabeled data. Experimental results showed that our proposed method achieved much better performance than baseline model and ranked 4th in both competition track 1 and track 2, and 6th in track 3, which verifies that the proposed network can effectively learn from incomplete datasets.



### Activated Gradients for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.04228v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.04228v1)
- **Published**: 2021-07-09 06:00:55+00:00
- **Updated**: 2021-07-09 06:00:55+00:00
- **Authors**: Mei Liu, Liangming Chen, Xiaohao Du, Long Jin, Mingsheng Shang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks often suffer from poor performance or even training failure due to the ill-conditioned problem, the vanishing/exploding gradient problem, and the saddle point problem. In this paper, a novel method by acting the gradient activation function (GAF) on the gradient is proposed to handle these challenges. Intuitively, the GAF enlarges the tiny gradients and restricts the large gradient. Theoretically, this paper gives conditions that the GAF needs to meet, and on this basis, proves that the GAF alleviates the problems mentioned above. In addition, this paper proves that the convergence rate of SGD with the GAF is faster than that without the GAF under some assumptions. Furthermore, experiments on CIFAR, ImageNet, and PASCAL visual object classes confirm the GAF's effectiveness. The experimental results also demonstrate that the proposed method is able to be adopted in various deep neural networks to improve their performance. The source code is publicly available at https://github.com/LongJin-lab/Activated-Gradients-for-Deep-Neural-Networks.



### Exploring Dropout Discriminator for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2107.04231v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04231v1)
- **Published**: 2021-07-09 06:11:34+00:00
- **Updated**: 2021-07-09 06:11:34+00:00
- **Authors**: Vinod K Kurmi, Venkatesh K Subramanian, Vinay P. Namboodiri
- **Comment**: This work is an extension of our BMVC-2019 paper (arXiv:1907.10628)
- **Journal**: None
- **Summary**: Adaptation of a classifier to new domains is one of the challenging problems in machine learning. This has been addressed using many deep and non-deep learning based methods. Among the methodologies used, that of adversarial learning is widely applied to solve many deep learning problems along with domain adaptation. These methods are based on a discriminator that ensures source and target distributions are close. However, here we suggest that rather than using a point estimate obtaining by a single discriminator, it would be useful if a distribution based on ensembles of discriminators could be used to bridge this gap. This could be achieved using multiple classifiers or using traditional ensemble methods. In contrast, we suggest that a Monte Carlo dropout based ensemble discriminator could suffice to obtain the distribution based discriminator. Specifically, we propose a curriculum based dropout discriminator that gradually increases the variance of the sample based distribution and the corresponding reverse gradients are used to align the source and target feature representations. An ensemble of discriminators helps the model to learn the data distribution efficiently. It also provides a better gradient estimates to train the feature extractor. The detailed results and thorough ablation analysis show that our model outperforms state-of-the-art results.



### Deep Image Synthesis from Intuitive User Input: A Review and Perspectives
- **Arxiv ID**: http://arxiv.org/abs/2107.04240v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.04240v2)
- **Published**: 2021-07-09 06:31:47+00:00
- **Updated**: 2021-09-30 19:41:05+00:00
- **Authors**: Yuan Xue, Yuan-Chen Guo, Han Zhang, Tao Xu, Song-Hai Zhang, Xiaolei Huang
- **Comment**: 26 pages, 7 figures, 1 table
- **Journal**: Computational Visual Media 2021
- **Summary**: In many applications of computer graphics, art and design, it is desirable for a user to provide intuitive non-image input, such as text, sketch, stroke, graph or layout, and have a computer system automatically generate photo-realistic images that adhere to the input content. While classic works that allow such automatic image content generation have followed a framework of image retrieval and composition, recent advances in deep generative models such as generative adversarial networks (GANs), variational autoencoders (VAEs), and flow-based methods have enabled more powerful and versatile image generation tasks. This paper reviews recent works for image synthesis given intuitive user input, covering advances in input versatility, image generation methodology, benchmark datasets, and evaluation metrics. This motivates new perspectives on input representation and interactivity, cross pollination between major image generation paradigms, and evaluation and comparison of generation methods.



### Unity Perception: Generate Synthetic Data for Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2107.04259v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04259v2)
- **Published**: 2021-07-09 07:09:00+00:00
- **Updated**: 2021-07-19 19:12:32+00:00
- **Authors**: Steve Borkman, Adam Crespi, Saurav Dhakad, Sujoy Ganguly, Jonathan Hogins, You-Cyuan Jhang, Mohsen Kamalzadeh, Bowen Li, Steven Leal, Pete Parisi, Cesar Romero, Wesley Smith, Alex Thaman, Samuel Warren, Nupur Yadav
- **Comment**: We corrected tasks supported by NVISII platform. For the Unity
  perception package, see
  https://github.com/Unity-Technologies/com.unity.perception
- **Journal**: None
- **Summary**: We introduce the Unity Perception package which aims to simplify and accelerate the process of generating synthetic datasets for computer vision tasks by offering an easy-to-use and highly customizable toolset. This open-source package extends the Unity Editor and engine components to generate perfectly annotated examples for several common computer vision tasks. Additionally, it offers an extensible Randomization framework that lets the user quickly construct and configure randomized simulation parameters in order to introduce variation into the generated datasets. We provide an overview of the provided tools and how they work, and demonstrate the value of the generated synthetic datasets by training a 2D object detection model. The model trained with mostly synthetic data outperforms the model trained using only real data.



### Wavelet Transform-assisted Adaptive Generative Modeling for Colorization
- **Arxiv ID**: http://arxiv.org/abs/2107.04261v2
- **DOI**: 10.1109/TMM.2022.3177933
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04261v2)
- **Published**: 2021-07-09 07:12:39+00:00
- **Updated**: 2022-08-18 06:05:18+00:00
- **Authors**: Jin Li, Wanyun Li, Zichen Xu, Yuhao Wang, Qiegen Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised deep learning has recently demonstrated the promise of producing high-quality samples. While it has tremendous potential to promote the image colorization task, the performance is limited owing to the high-dimension of data manifold and model capability. This study presents a novel scheme that exploits the score-based generative model in wavelet domain to address the issues. By taking advantage of the multi-scale and multi-channel representation via wavelet transform, the proposed model learns the richer priors from stacked coarse and detailed wavelet coefficient components jointly and effectively. This strategy also reduces the dimension of the original manifold and alleviates the curse of dimensionality, which is beneficial for estimation and sampling. Moreover, dual consistency terms in the wavelet domain, namely data-consistency and structure-consistency are devised to leverage colorization task better. Specifically, in the training phase, a set of multi-channel tensors consisting of wavelet coefficients is used as the input to train the network with denoising score matching. In the inference phase, samples are iteratively generated via annealed Langevin dynamics with data and structure consistencies. Experiments demonstrated remarkable improvements of the proposed method on both generation and colorization quality, particularly in colorization robustness and diversity.



### Towards Robust General Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.04263v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04263v1)
- **Published**: 2021-07-09 07:17:05+00:00
- **Updated**: 2021-07-09 07:17:05+00:00
- **Authors**: Laura Daza, Juan C. Pérez, Pablo Arbeláez
- **Comment**: Accepted at MICCAI 2021
- **Journal**: None
- **Summary**: The reliability of Deep Learning systems depends on their accuracy but also on their robustness against adversarial perturbations to the input data. Several attacks and defenses have been proposed to improve the performance of Deep Neural Networks under the presence of adversarial noise in the natural image domain. However, robustness in computer-aided diagnosis for volumetric data has only been explored for specific tasks and with limited attacks. We propose a new framework to assess the robustness of general medical image segmentation systems. Our contributions are two-fold: (i) we propose a new benchmark to evaluate robustness in the context of the Medical Segmentation Decathlon (MSD) by extending the recent AutoAttack natural image classification framework to the domain of volumetric data segmentation, and (ii) we present a novel lattice architecture for RObust Generic medical image segmentation (ROG). Our results show that ROG is capable of generalizing across different tasks of the MSD and largely surpasses the state-of-the-art under sophisticated adversarial attacks.



### Prior-Guided Multi-View 3D Head Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2107.04277v2
- **DOI**: 10.1109/TMM.2021.3111485
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04277v2)
- **Published**: 2021-07-09 07:43:56+00:00
- **Updated**: 2021-10-02 01:21:22+00:00
- **Authors**: Xueying Wang, Yudong Guo, Zhongqi Yang, Juyong Zhang
- **Comment**: 13 pages, 14 figures
- **Journal**: Received by IEEE Transactions on Multimedia (2021)
- **Summary**: Recovery of a 3D head model including the complete face and hair regions is still a challenging problem in computer vision and graphics. In this paper, we consider this problem using only a few multi-view portrait images as input. Previous multi-view stereo methods that have been based, either on optimization strategies or deep learning techniques, suffer from low-frequency geometric structures such as unclear head structures and inaccurate reconstruction in hair regions. To tackle this problem, we propose a prior-guided implicit neural rendering network. Specifically, we model the head geometry with a learnable signed distance field (SDF) and optimize it via an implicit differentiable renderer with the guidance of some human head priors, including the facial prior knowledge, head semantic segmentation information and 2D hair orientation maps. The utilization of these priors can improve the reconstruction accuracy and robustness, leading to a high-quality integrated 3D head model. Extensive ablation studies and comparisons with state-of-the-art methods demonstrate that our method can generate high-fidelity 3D head geometries with the guidance of these priors.



### Fast Pixel-Matching for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.04279v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04279v1)
- **Published**: 2021-07-09 07:46:46+00:00
- **Updated**: 2021-07-09 07:46:46+00:00
- **Authors**: Siyue Yu, Jimin Xiao, BingFeng Zhang, Eng Gee Lim
- **Comment**: Accepted by Signal Processing: Image Communication
- **Journal**: None
- **Summary**: Video object segmentation, aiming to segment the foreground objects given the annotation of the first frame, has been attracting increasing attentions. Many state-of-the-art approaches have achieved great performance by relying on online model updating or mask-propagation techniques. However, most online models require high computational cost due to model fine-tuning during inference. Most mask-propagation based models are faster but with relatively low performance due to failure to adapt to object appearance variation. In this paper, we are aiming to design a new model to make a good balance between speed and performance. We propose a model, called NPMCA-net, which directly localizes foreground objects based on mask-propagation and non-local technique by matching pixels in reference and target frames. Since we bring in information of both first and previous frames, our network is robust to large object appearance variation, and can better adapt to occlusions. Extensive experiments show that our approach can achieve a new state-of-the-art performance with a fast speed at the same time (86.5% IoU on DAVIS-2016 and 72.2% IoU on DAVIS-2017, with speed of 0.11s per frame) under the same level comparison. Source code is available at https://github.com/siyueyu/NPMCA-net.



### JPGNet: Joint Predictive Filtering and Generative Network for Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2107.04281v3
- **DOI**: 10.1145/3474085.3475170
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04281v3)
- **Published**: 2021-07-09 07:49:52+00:00
- **Updated**: 2021-10-18 07:27:03+00:00
- **Authors**: Qing Guo, Xiaoguang Li, Felix Juefei-Xu, Hongkai Yu, Yang Liu, Song wang
- **Comment**: This work has been accepted to ACM-MM 2021
- **Journal**: None
- **Summary**: Image inpainting aims to restore the missing regions of corrupted images and make the recovery result identical to the originally complete image, which is different from the common generative task emphasizing the naturalness or realism of generated images. Nevertheless, existing works usually regard it as a pure generation problem and employ cutting-edge deep generative techniques to address it. The generative networks can fill the main missing parts with realistic contents but usually distort the local structures or introduce obvious artifacts. In this paper, for the first time, we formulate image inpainting as a mix of two problems, predictive filtering and deep generation. Predictive filtering is good at preserving local structures and removing artifacts but falls short to complete the large missing regions. The deep generative network can fill the numerous missing pixels based on the understanding of the whole scene but hardly restores the details identical to the original ones. To make use of their respective advantages, we propose the joint predictive filtering and generative network (JPGNet) that contains three branches: predictive filtering & uncertainty network (PFUNet), deep generative network, and uncertainty-aware fusion network (UAFNet). The PFUNet can adaptively predict pixel-wise kernels for filtering-based inpainting according to the input image and output an uncertainty map. This map indicates the pixels should be processed by filtering or generative networks, which is further fed to the UAFNet for a smart combination between filtering and generative results. Note that, our method as a novel inpainting framework can benefit any existing generation-based methods. We validate our method on three public datasets, Dunhuang, Places2, and CelebA, and demonstrate that our method can enhance three state-of-the-art generative methods significantly with slightly extra time costs.



### LIFE: A Generalizable Autodidactic Pipeline for 3D OCT-A Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2107.04282v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04282v1)
- **Published**: 2021-07-09 07:51:33+00:00
- **Updated**: 2021-07-09 07:51:33+00:00
- **Authors**: Dewei Hu, Can Cui, Hao Li, Kathleen E. Larson, Yuankai K. Tao, Ipek Oguz
- **Comment**: Accepted by International Conference on Medical Image Computing and
  Computer Assisted Intervention (MICCAI) 2021
- **Journal**: None
- **Summary**: Optical coherence tomography (OCT) is a non-invasive imaging technique widely used for ophthalmology. It can be extended to OCT angiography (OCT-A), which reveals the retinal vasculature with improved contrast. Recent deep learning algorithms produced promising vascular segmentation results; however, 3D retinal vessel segmentation remains difficult due to the lack of manually annotated training data. We propose a learning-based method that is only supervised by a self-synthesized modality named local intensity fusion (LIF). LIF is a capillary-enhanced volume computed directly from the input OCT-A. We then construct the local intensity fusion encoder (LIFE) to map a given OCT-A volume and its LIF counterpart to a shared latent space. The latent space of LIFE has the same dimensions as the input data and it contains features common to both modalities. By binarizing this latent space, we obtain a volumetric vessel segmentation. Our method is evaluated in a human fovea OCT-A and three zebrafish OCT-A volumes with manual labels. It yields a Dice score of 0.7736 on human data and 0.8594 +/- 0.0275 on zebrafish data, a dramatic improvement over existing unsupervised algorithms.



### Capturing, Reconstructing, and Simulating: the UrbanScene3D Dataset
- **Arxiv ID**: http://arxiv.org/abs/2107.04286v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2107.04286v3)
- **Published**: 2021-07-09 07:56:46+00:00
- **Updated**: 2022-07-19 05:20:26+00:00
- **Authors**: Liqiang Lin, Yilin Liu, Yue Hu, Xingguang Yan, Ke Xie, Hui Huang
- **Comment**: ECCV 2022 camera ready; Project page: https://vcc.tech/UrbanScene3D/
- **Journal**: None
- **Summary**: We present UrbanScene3D, a large-scale data platform for research of urban scene perception and reconstruction. UrbanScene3D contains over 128k high-resolution images covering 16 scenes including large-scale real urban regions and synthetic cities with 136 km^2 area in total. The dataset also contains high-precision LiDAR scans and hundreds of image sets with different observation patterns, which provide a comprehensive benchmark to design and evaluate aerial path planning and 3D reconstruction algorithms. In addition, the dataset, which is built on Unreal Engine and Airsim simulator together with the manually annotated unique instance label for each building in the dataset, enables the generation of all kinds of data, e.g., 2D depth maps, 2D/3D bounding boxes, and 3D point cloud/mesh segmentations, etc. The simulator with physical engine and lighting system not only produce variety of data but also enable users to simulate cars or drones in the proposed urban environment for future research.



### Retinal OCT Denoising with Pseudo-Multimodal Fusion Network
- **Arxiv ID**: http://arxiv.org/abs/2107.04288v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04288v1)
- **Published**: 2021-07-09 08:00:20+00:00
- **Updated**: 2021-07-09 08:00:20+00:00
- **Authors**: Dewei Hu, Joseph D. Malone, Yigit Atay, Yuankai K. Tao, Ipek Oguz
- **Comment**: Accepted by International Workshop on Ophthalmic Medical Image
  Analysis (OMIA) 2020
- **Journal**: None
- **Summary**: Optical coherence tomography (OCT) is a prevalent imaging technique for retina. However, it is affected by multiplicative speckle noise that can degrade the visibility of essential anatomical structures, including blood vessels and tissue layers. Although averaging repeated B-scan frames can significantly improve the signal-to-noise-ratio (SNR), this requires longer acquisition time, which can introduce motion artifacts and cause discomfort to patients. In this study, we propose a learning-based method that exploits information from the single-frame noisy B-scan and a pseudo-modality that is created with the aid of the self-fusion method. The pseudo-modality provides good SNR for layers that are barely perceptible in the noisy B-scan but can over-smooth fine features such as small vessels. By using a fusion network, desired features from each modality can be combined, and the weight of their contribution is adjustable. Evaluated by intensity-based and structural metrics, the result shows that our method can effectively suppress the speckle noise and enhance the contrast between retina layers while the overall structure and small blood vessels are preserved. Compared to the single modality network, our method improves the structural similarity with low noise B-scan from 0.559 +\- 0.033 to 0.576 +\- 0.031.



### Task-Aware Sampling Layer for Point-Wise Analysis
- **Arxiv ID**: http://arxiv.org/abs/2107.04291v3
- **DOI**: 10.1109/TVCG.2022.3171794
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04291v3)
- **Published**: 2021-07-09 08:08:44+00:00
- **Updated**: 2022-04-27 03:00:54+00:00
- **Authors**: Yiqun Lin, Lichang Chen, Haibin Huang, Chongyang Ma, Xiaoguang Han, Shuguang Cui
- **Comment**: 14 pages, 13 figures and 14 tables
- **Journal**: None
- **Summary**: Sampling, grouping, and aggregation are three important components in the multi-scale analysis of point clouds. In this paper, we present a novel data-driven sampler learning strategy for point-wise analysis tasks. Unlike the widely used sampling technique, Farthest Point Sampling (FPS), we propose to learn sampling and downstream applications jointly. Our key insight is that uniform sampling methods like FPS are not always optimal for different tasks: sampling more points around boundary areas can make the point-wise classification easier for segmentation. Towards this end, we propose a novel sampler learning strategy that learns sampling point displacement supervised by task-related ground truth information and can be trained jointly with the underlying tasks. We further demonstrate our methods in various point-wise analysis tasks, including semantic part segmentation, point cloud completion, and keypoint detection. Our experiments show that jointly learning of the sampler and task brings better performance than using FPS in various point-based networks.



### Differentially private training of neural networks with Langevin dynamics for calibrated predictive uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2107.04296v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04296v2)
- **Published**: 2021-07-09 08:14:45+00:00
- **Updated**: 2021-08-04 09:02:27+00:00
- **Authors**: Moritz Knolle, Alexander Ziller, Dmitrii Usynin, Rickmer Braren, Marcus R. Makowski, Daniel Rueckert, Georgios Kaissis
- **Comment**: Accepted to the ICML 2021 Theory and Practice of Differential Privacy
  Workshop
- **Journal**: None
- **Summary**: We show that differentially private stochastic gradient descent (DP-SGD) can yield poorly calibrated, overconfident deep learning models. This represents a serious issue for safety-critical applications, e.g. in medical diagnosis. We highlight and exploit parallels between stochastic gradient Langevin dynamics, a scalable Bayesian inference technique for training deep neural networks, and DP-SGD, in order to train differentially private, Bayesian neural networks with minor adjustments to the original (DP-SGD) algorithm. Our approach provides considerably more reliable uncertainty estimates than DP-SGD, as demonstrated empirically by a reduction in expected calibration error (MNIST $\sim{5}$-fold, Pediatric Pneumonia Dataset $\sim{2}$-fold).



### Hepatocellular Carcinoma Segmentation from Digital Subtraction Angiography Videos using Learnable Temporal Difference
- **Arxiv ID**: http://arxiv.org/abs/2107.04306v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04306v3)
- **Published**: 2021-07-09 08:35:37+00:00
- **Updated**: 2021-09-17 03:37:57+00:00
- **Authors**: Wenting Jiang, Yicheng Jiang, Lu Zhang, Changmiao Wang, Xiaoguang Han, Shuixing Zhang, Xiang Wan, Shuguang Cui
- **Comment**: 10 pages; accepted to MICCAI 2021
- **Journal**: None
- **Summary**: Automatic segmentation of hepatocellular carcinoma (HCC) in Digital Subtraction Angiography (DSA) videos can assist radiologists in efficient diagnosis of HCC and accurate evaluation of tumors in clinical practice. Few studies have investigated HCC segmentation from DSA videos. It shows great challenging due to motion artifacts in filming, ambiguous boundaries of tumor regions and high similarity in imaging to other anatomical tissues. In this paper, we raise the problem of HCC segmentation in DSA videos, and build our own DSA dataset. We also propose a novel segmentation network called DSA-LTDNet, including a segmentation sub-network, a temporal difference learning (TDL) module and a liver region segmentation (LRS) sub-network for providing additional guidance. DSA-LTDNet is preferable for learning the latent motion information from DSA videos proactively and boosting segmentation performance. All of experiments are conducted on our self-collected dataset. Experimental results show that DSA-LTDNet increases the DICE score by nearly 4% compared to the U-Net baseline.



### ABD-Net: Attention Based Decomposition Network for 3D Point Cloud Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2108.04221v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2108.04221v1)
- **Published**: 2021-07-09 08:39:30+00:00
- **Updated**: 2021-07-09 08:39:30+00:00
- **Authors**: Siddharth Katageri, Shashidhar V Kudari, Akshaykumar Gunari, Ramesh Ashok Tabib, Uma Mudenagudi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose Attention Based Decomposition Network (ABD-Net), for point cloud decomposition into basic geometric shapes namely, plane, sphere, cone and cylinder. We show improved performance of 3D object classification using attention features based on primitive shapes in point clouds. Point clouds, being the simple and compact representation of 3D objects have gained increasing popularity. They demand robust methods for feature extraction due to unorderness in point sets. In ABD-Net the proposed Local Proximity Encapsulator captures the local geometric variations along with spatial encoding around each point from the input point sets. The encapsulated local features are further passed to proposed Attention Feature Encoder to learn basic shapes in point cloud. Attention Feature Encoder models geometric relationship between the neighborhoods of all the points resulting in capturing global point cloud information. We demonstrate the results of our proposed ABD-Net on ANSI mechanical component and ModelNet40 datasets. We also demonstrate the effectiveness of ABD-Net over the acquired attention features by improving the performance of 3D object classification on ModelNet40 benchmark dataset and compare them with state-of-the-art techniques.



### Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset
- **Arxiv ID**: http://arxiv.org/abs/2107.04313v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04313v1)
- **Published**: 2021-07-09 09:04:05+00:00
- **Updated**: 2021-07-09 09:04:05+00:00
- **Authors**: Hannah Rose Kirk, Yennie Jun, Paulius Rauba, Gal Wachtel, Ruining Li, Xingjian Bai, Noah Broestl, Martin Doff-Sotta, Aleksandar Shtedritski, Yuki M. Asano
- **Comment**: Accepted paper at ACL WOAH 2021
- **Journal**: None
- **Summary**: Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text- and visual-modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to `memes in the wild'. In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that memes in the wild differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than `traditional memes', including screenshots of conversations or text on a plain background. This paper thus serves as a reality check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.



### Mutually-aware Sub-Graphs Differentiable Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2107.04324v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04324v3)
- **Published**: 2021-07-09 09:31:31+00:00
- **Updated**: 2021-11-06 01:29:35+00:00
- **Authors**: Haoxian Tan, Sheng Guo, Yujie Zhong, Matthew R. Scott, Weilin Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Differentiable architecture search is prevalent in the field of NAS because of its simplicity and efficiency, where two paradigms, multi-path algorithms and single-path methods, are dominated. Multi-path framework (e.g. DARTS) is intuitive but suffers from memory usage and training collapse. Single-path methods (e.g.GDAS and ProxylessNAS) mitigate the memory issue and shrink the gap between searching and evaluation but sacrifice the performance. In this paper, we propose a conceptually simple yet efficient method to bridge these two paradigms, referred as Mutually-aware Sub-Graphs Differentiable Architecture Search (MSG-DAS). The core of our framework is a differentiable Gumbel-TopK sampler that produces multiple mutually exclusive single-path sub-graphs. To alleviate the severer skip-connect issue brought by multiple sub-graphs setting, we propose a Dropblock-Identity module to stabilize the optimization. To make best use of the available models (super-net and sub-graphs), we introduce a memory-efficient super-net guidance distillation to improve training. The proposed framework strikes a balance between flexible memory usage and searching quality. We demonstrate the effectiveness of our methods on ImageNet and CIFAR10, where the searched models show a comparable performance as the most recent approaches.



### Semantic Segmentation on Multiple Visual Domains
- **Arxiv ID**: http://arxiv.org/abs/2107.04326v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04326v1)
- **Published**: 2021-07-09 09:34:51+00:00
- **Updated**: 2021-07-09 09:34:51+00:00
- **Authors**: Floris Naber
- **Comment**: Graduation project report
- **Journal**: None
- **Summary**: Semantic segmentation models only perform well on the domain they are trained on and datasets for training are scarce and often have a small label-spaces, because the pixel level annotations required are expensive to make. Thus training models on multiple existing domains is desired to increase the output label-space. Current research shows that there is potential to improve accuracy across datasets by using multi-domain training, but this has not yet been successfully extended to datasets of three different non-overlapping domains without manual labelling. In this paper a method for this is proposed for the datasets Cityscapes, SUIM and SUN RGB-D, by creating a label-space that spans all classes of the datasets. Duplicate classes are merged and discrepant granularity is solved by keeping classes separate. Results show that accuracy of the multi-domain model has higher accuracy than all baseline models together, if hardware performance is equalized, as resources are not limitless, showing that models benefit from additional data even from domains that have nothing in common.



### Score refinement for confidence-based 3D multi-object tracking
- **Arxiv ID**: http://arxiv.org/abs/2107.04327v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.04327v1)
- **Published**: 2021-07-09 09:40:07+00:00
- **Updated**: 2021-07-09 09:40:07+00:00
- **Authors**: Nuri Benbarka, Jona Schröder, Andreas Zell
- **Comment**: Accepted at IROS 2021
- **Journal**: None
- **Summary**: Multi-object tracking is a critical component in autonomous navigation, as it provides valuable information for decision-making. Many researchers tackled the 3D multi-object tracking task by filtering out the frame-by-frame 3D detections; however, their focus was mainly on finding useful features or proper matching metrics. Our work focuses on a neglected part of the tracking system: score refinement and tracklet termination. We show that manipulating the scores depending on time consistency while terminating the tracklets depending on the tracklet score improves tracking results. We do this by increasing the matched tracklets' score with score update functions and decreasing the unmatched tracklets' score. Compared to count-based methods, our method consistently produces better AMOTA and MOTA scores when utilizing various detectors and filtering algorithms on different datasets. The improvements in AMOTA score went up to 1.83 and 2.96 in MOTA. We also used our method as a late-fusion ensembling method, and it performed better than voting-based ensemble methods by a solid margin. It achieved an AMOTA score of 67.6 on nuScenes test evaluation, which is comparable to other state-of-the-art trackers. Code is publicly available at: \url{https://github.com/cogsys-tuebingen/CBMOT}.



### StyleCariGAN: Caricature Generation via StyleGAN Feature Map Modulation
- **Arxiv ID**: http://arxiv.org/abs/2107.04331v1
- **DOI**: 10.1145/3450626.3459860
- **Categories**: **cs.CV**, cs.GR, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2107.04331v1)
- **Published**: 2021-07-09 09:49:31+00:00
- **Updated**: 2021-07-09 09:49:31+00:00
- **Authors**: Wonjong Jang, Gwangjin Ju, Yucheol Jung, Jiaolong Yang, Xin Tong, Seungyong Lee
- **Comment**: Accepted to SIGGRAPH 2021. For supplementary material, see
  http://cg.postech.ac.kr/papers/2021_StyleCariGAN_supp.zip
- **Journal**: ACM Trans. Graph., Vol. 40, No. 4, Article 116. Publication date:
  August 2021
- **Summary**: We present a caricature generation framework based on shape and style manipulation using StyleGAN. Our framework, dubbed StyleCariGAN, automatically creates a realistic and detailed caricature from an input photo with optional controls on shape exaggeration degree and color stylization type. The key component of our method is shape exaggeration blocks that are used for modulating coarse layer feature maps of StyleGAN to produce desirable caricature shape exaggerations. We first build a layer-mixed StyleGAN for photo-to-caricature style conversion by swapping fine layers of the StyleGAN for photos to the corresponding layers of the StyleGAN trained to generate caricatures. Given an input photo, the layer-mixed model produces detailed color stylization for a caricature but without shape exaggerations. We then append shape exaggeration blocks to the coarse layers of the layer-mixed model and train the blocks to create shape exaggerations while preserving the characteristic appearances of the input. Experimental results show that our StyleCariGAN generates realistic and detailed caricatures compared to the current state-of-the-art methods. We demonstrate StyleCariGAN also supports other StyleGAN-based image manipulations, such as facial expression control.



### Graph-based Deep Generative Modelling for Document Layout Generation
- **Arxiv ID**: http://arxiv.org/abs/2107.04357v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.04357v1)
- **Published**: 2021-07-09 10:49:49+00:00
- **Updated**: 2021-07-09 10:49:49+00:00
- **Authors**: Sanket Biswas, Pau Riba, Josep Lladós, Umapada Pal
- **Comment**: Accepted by ICDAR Workshops-GLESDO 2021
- **Journal**: None
- **Summary**: One of the major prerequisites for any deep learning approach is the availability of large-scale training data. When dealing with scanned document images in real world scenarios, the principal information of its content is stored in the layout itself. In this work, we have proposed an automated deep generative model using Graph Neural Networks (GNNs) to generate synthetic data with highly variable and plausible document layouts that can be used to train document interpretation systems, in this case, specially in digital mailroom applications. It is also the first graph-based approach for document layout generation task experimented on administrative document images, in this case, invoices.



### RGB Stream Is Enough for Temporal Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2107.04362v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.04362v1)
- **Published**: 2021-07-09 11:10:11+00:00
- **Updated**: 2021-07-09 11:10:11+00:00
- **Authors**: Chenhao Wang, Hongxiang Cai, Yuxin Zou, Yichao Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art temporal action detectors to date are based on two-stream input including RGB frames and optical flow. Although combining RGB frames and optical flow boosts performance significantly, optical flow is a hand-designed representation which not only requires heavy computation, but also makes it methodologically unsatisfactory that two-stream methods are often not learned end-to-end jointly with the flow. In this paper, we argue that optical flow is dispensable in high-accuracy temporal action detection and image level data augmentation (ILDA) is the key solution to avoid performance degradation when optical flow is removed. To evaluate the effectiveness of ILDA, we design a simple yet efficient one-stage temporal action detector based on single RGB stream named DaoTAD. Our results show that when trained with ILDA, DaoTAD has comparable accuracy with all existing state-of-the-art two-stream detectors while surpassing the inference speed of previous methods by a large margin and the inference speed is astounding 6668 fps on GeForce GTX 1080 Ti. Code is available at \url{https://github.com/Media-Smart/vedatad}.



### Segmentation of VHR EO Images using Unsupervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2108.04222v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2108.04222v2)
- **Published**: 2021-07-09 11:42:48+00:00
- **Updated**: 2021-08-10 08:55:26+00:00
- **Authors**: Sudipan Saha, Lichao Mou, Muhammad Shahzad, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation is a crucial step in many Earth observation tasks. Large quantity of pixel-level annotation is required to train deep networks for semantic segmentation. Earth observation techniques are applied to varieties of applications and since classes vary widely depending on the applications, therefore, domain knowledge is often required to label Earth observation images, impeding availability of labeled training data in many Earth observation applications. To tackle these challenges, in this paper we propose an unsupervised semantic segmentation method that can be trained using just a single unlabeled scene. Remote sensing scenes are generally large. The proposed method exploits this property to sample smaller patches from the larger scene and uses deep clustering and contrastive learning to refine the weights of a lightweight deep model composed of a series of the convolution layers along with an embedded channel attention. After unsupervised training on the target image/scene, the model automatically segregates the major classes present in the scene and produces the segmentation map. Experimental results on the Vaihingen dataset demonstrate the efficacy of the proposed method.



### Joint Matrix Decomposition for Deep Convolutional Neural Networks Compression
- **Arxiv ID**: http://arxiv.org/abs/2107.04386v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.04386v3)
- **Published**: 2021-07-09 12:32:10+00:00
- **Updated**: 2022-10-26 14:35:41+00:00
- **Authors**: Shaowu Chen, Jiahao Zhou, Weize Sun, Lei Huang
- **Comment**: Code is publicly available at: https://github.com/ShaowuChen/JointSVD
- **Journal**: Published in Neurocomputing,Volume 516, 2023, Pages 11-26
- **Summary**: Deep convolutional neural networks (CNNs) with a large number of parameters require intensive computational resources, and thus are hard to be deployed in resource-constrained platforms. Decomposition-based methods, therefore, have been utilized to compress CNNs in recent years. However, since the compression factor and performance are negatively correlated, the state-of-the-art works either suffer from severe performance degradation or have relatively low compression factors. To overcome this problem, we propose to compress CNNs and alleviate performance degradation via joint matrix decomposition, which is different from existing works that compressed layers separately. The idea is inspired by the fact that there are lots of repeated modules in CNNs. By projecting weights with the same structures into the same subspace, networks can be jointly compressed with larger ranks. In particular, three joint matrix decomposition schemes are developed, and the corresponding optimization approaches based on Singular Value Decomposition are proposed. Extensive experiments are conducted across three challenging compact CNNs for different benchmark data sets to demonstrate the superior performance of our proposed algorithms. As a result, our methods can compress the size of ResNet-34 by 22X with slighter accuracy degradation compared with several state-of-the-art methods.



### Hoechst Is All You Need: Lymphocyte Classification with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.04388v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.04388v2)
- **Published**: 2021-07-09 12:33:22+00:00
- **Updated**: 2021-07-16 13:43:59+00:00
- **Authors**: Jessica Cooper, In Hwa Um, Ognjen Arandjelović, David J Harrison
- **Comment**: 15 pages, 4 figures
- **Journal**: None
- **Summary**: Multiplex immunofluorescence and immunohistochemistry benefit patients by allowing cancer pathologists to identify several proteins expressed on the surface of cells, enabling cell classification, better understanding of the tumour micro-environment, more accurate diagnoses, prognoses, and tailored immunotherapy based on the immune status of individual patients. However, they are expensive and time consuming processes which require complex staining and imaging techniques by expert technicians. Hoechst staining is much cheaper and easier to perform, but is not typically used in this case as it binds to DNA rather than to the proteins targeted by immunofluorescent techniques, and it was not previously thought possible to differentiate cells expressing these proteins based only on DNA morphology. In this work we show otherwise, training a deep convolutional neural network to identify cells expressing three proteins (T lymphocyte markers CD3 and CD8, and the B lymphocyte marker CD20) with greater than 90% precision and recall, from Hoechst 33342 stained tissue only. Our model learns previously unknown morphological features associated with expression of these proteins which can be used to accurately differentiate lymphocyte subtypes for use in key prognostic metrics such as assessment of immune cell infiltration,and thereby predict and improve patient outcomes without the need for costly multiplex immunofluorescence.



### Action Unit Detection with Joint Adaptive Attention and Graph Relation
- **Arxiv ID**: http://arxiv.org/abs/2107.04389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04389v1)
- **Published**: 2021-07-09 12:33:38+00:00
- **Updated**: 2021-07-09 12:33:38+00:00
- **Authors**: Chenggong Zhang, Juan Song, Qingyang Zhang, Weilong Dong, Ruomeng Ding, Zhilei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes an approach to the facial action unit (AU) detection. In this work, we present our submission to the Field Affective Behavior Analysis (ABAW) 2021 competition. The proposed method uses the pre-trained JAA model as the feature extractor, and extracts global features, face alignment features and AU local features on the basis of multi-scale features. We take the AU local features as the input of the graph convolution to further consider the correlation between AU, and finally use the fused features to classify AU. The detected accuracy was evaluated by 0.5*accuracy + 0.5*F1. Our model achieves 0.674 on the challenging Aff-Wild2 database.



### Multi-Modal Association based Grouping for Form Structure Extraction
- **Arxiv ID**: http://arxiv.org/abs/2107.04396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04396v1)
- **Published**: 2021-07-09 12:49:34+00:00
- **Updated**: 2021-07-09 12:49:34+00:00
- **Authors**: Milan Aggarwal, Mausoom Sarkar, Hiresh Gupta, Balaji Krishnamurthy
- **Comment**: This work has been accepted and presented at WACV 2020
- **Journal**: None
- **Summary**: Document structure extraction has been a widely researched area for decades. Recent work in this direction has been deep learning-based, mostly focusing on extracting structure using fully convolution NN through semantic segmentation. In this work, we present a novel multi-modal approach for form structure extraction. Given simple elements such as textruns and widgets, we extract higher-order structures such as TextBlocks, Text Fields, Choice Fields, and Choice Groups, which are essential for information collection in forms. To achieve this, we obtain a local image patch around each low-level element (reference) by identifying candidate elements closest to it. We process textual and spatial representation of candidates sequentially through a BiLSTM to obtain context-aware representations and fuse them with image patch features obtained by processing it through a CNN. Subsequently, the sequential decoder takes this fused feature vector to predict the association type between reference and candidates. These predicted associations are utilized to determine larger structures through connected components analysis. Experimental results show the effectiveness of our approach achieving a recall of 90.29%, 73.80%, 83.12%, and 52.72% for the above structures, respectively, outperforming semantic segmentation baselines significantly. We show the efficacy of our method through ablations, comparing it against using individual modalities. We also introduce our new rich human-annotated Forms Dataset.



### Learning to Detect Adversarial Examples Based on Class Scores
- **Arxiv ID**: http://arxiv.org/abs/2107.04435v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04435v1)
- **Published**: 2021-07-09 13:29:54+00:00
- **Updated**: 2021-07-09 13:29:54+00:00
- **Authors**: Tobias Uelwer, Felix Michels, Oliver De Candido
- **Comment**: Accepted at the 44th German Conference on Artificial Intelligence (KI
  2021)
- **Journal**: None
- **Summary**: Given the increasing threat of adversarial attacks on deep neural networks (DNNs), research on efficient detection methods is more important than ever. In this work, we take a closer look at adversarial attack detection based on the class scores of an already trained classification model. We propose to train a support vector machine (SVM) on the class scores to detect adversarial examples. Our method is able to detect adversarial examples generated by various attacks, and can be easily adopted to a plethora of deep classification models. We show that our approach yields an improved detection rate compared to an existing method, whilst being easy to implement. We perform an extensive empirical analysis on different deep classification models, investigating various state-of-the-art adversarial attacks. Moreover, we observe that our proposed method is better at detecting a combination of adversarial attacks. This work indicates the potential of detecting various adversarial attacks simply by using the class scores of an already trained classification model.



### A Deep Discontinuity-Preserving Image Registration Network
- **Arxiv ID**: http://arxiv.org/abs/2107.04440v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04440v1)
- **Published**: 2021-07-09 13:35:59+00:00
- **Updated**: 2021-07-09 13:35:59+00:00
- **Authors**: Xiang Chen, Nishant Ravikumar, Yan Xia, Alejandro F Frangi
- **Comment**: Provisional accepted in MICCAI 2021
- **Journal**: None
- **Summary**: Image registration aims to establish spatial correspondence across pairs, or groups of images, and is a cornerstone of medical image computing and computer-assisted-interventions. Currently, most deep learning-based registration methods assume that the desired deformation fields are globally smooth and continuous, which is not always valid for real-world scenarios, especially in medical image registration (e.g. cardiac imaging and abdominal imaging). Such a global constraint can lead to artefacts and increased errors at discontinuous tissue interfaces. To tackle this issue, we propose a weakly-supervised Deep Discontinuity-preserving Image Registration network (DDIR), to obtain better registration performance and realistic deformation fields. We demonstrate that our method achieves significant improvements in registration accuracy and predicts more realistic deformations, in registration experiments on cardiac magnetic resonance (MR) images from UK Biobank Imaging Study (UKBB), than state-of-the-art approaches.



### Multimodal Icon Annotation For Mobile Applications
- **Arxiv ID**: http://arxiv.org/abs/2107.04452v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2107.04452v1)
- **Published**: 2021-07-09 13:57:37+00:00
- **Updated**: 2021-07-09 13:57:37+00:00
- **Authors**: Xiaoxue Zang, Ying Xu, Jindong Chen
- **Comment**: 11 pages, MobileHCI 2021
- **Journal**: None
- **Summary**: Annotating user interfaces (UIs) that involves localization and classification of meaningful UI elements on a screen is a critical step for many mobile applications such as screen readers and voice control of devices. Annotating object icons, such as menu, search, and arrow backward, is especially challenging due to the lack of explicit labels on screens, their similarity to pictures, and their diverse shapes. Existing studies either use view hierarchy or pixel based methods to tackle the task. Pixel based approaches are more popular as view hierarchy features on mobile platforms are often incomplete or inaccurate, however it leaves out instructional information in the view hierarchy such as resource-ids or content descriptions. We propose a novel deep learning based multi-modal approach that combines the benefits of both pixel and view hierarchy features as well as leverages the state-of-the-art object detection techniques. In order to demonstrate the utility provided, we create a high quality UI dataset by manually annotating the most commonly used 29 icons in Rico, a large scale mobile design dataset consisting of 72k UI screenshots. The experimental results indicate the effectiveness of our multi-modal approach. Our model not only outperforms a widely used object classification baseline but also pixel based object detection models. Our study sheds light on how to combine view hierarchy with pixel features for annotating UI elements.



### Understanding the Distributions of Aggregation Layers in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.04458v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04458v1)
- **Published**: 2021-07-09 14:23:57+00:00
- **Updated**: 2021-07-09 14:23:57+00:00
- **Authors**: Eng-Jon Ong, Sameed Husain, Miroslaw Bober
- **Comment**: None
- **Journal**: None
- **Summary**: The process of aggregation is ubiquitous in almost all deep nets models. It functions as an important mechanism for consolidating deep features into a more compact representation, whilst increasing robustness to overfitting and providing spatial invariance in deep nets. In particular, the proximity of global aggregation layers to the output layers of DNNs mean that aggregated features have a direct influence on the performance of a deep net. A better understanding of this relationship can be obtained using information theoretic methods. However, this requires the knowledge of the distributions of the activations of aggregation layers. To achieve this, we propose a novel mathematical formulation for analytically modelling the probability distributions of output values of layers involved with deep feature aggregation. An important outcome is our ability to analytically predict the KL-divergence of output nodes in a DNN. We also experimentally verify our theoretical predictions against empirical observations across a range of different classification tasks and datasets.



### On the Challenges of Open World Recognitionunder Shifting Visual Domains
- **Arxiv ID**: http://arxiv.org/abs/2107.04461v1
- **DOI**: 10.1109/LRA.2020.3047777
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.04461v1)
- **Published**: 2021-07-09 14:25:45+00:00
- **Updated**: 2021-07-09 14:25:45+00:00
- **Authors**: Dario Fontanel, Fabio Cermelli, Massimiliano Mancini, Barbara Caputo
- **Comment**: RAL/ICRA 2021
- **Journal**: None
- **Summary**: Robotic visual systems operating in the wild must act in unconstrained scenarios, under different environmental conditions while facing a variety of semantic concepts, including unknown ones. To this end, recent works tried to empower visual object recognition methods with the capability to i) detect unseen concepts and ii) extended their knowledge over time, as images of new semantic classes arrive. This setting, called Open World Recognition (OWR), has the goal to produce systems capable of breaking the semantic limits present in the initial training set. However, this training set imposes to the system not only its own semantic limits, but also environmental ones, due to its bias toward certain acquisition conditions that do not necessarily reflect the high variability of the real-world. This discrepancy between training and test distribution is called domain-shift. This work investigates whether OWR algorithms are effective under domain-shift, presenting the first benchmark setup for assessing fairly the performances of OWR algorithms, with and without domain-shift. We then use this benchmark to conduct analyses in various scenarios, showing how existing OWR algorithms indeed suffer a severe performance degradation when train and test distributions differ. Our analysis shows that this degradation is only slightly mitigated by coupling OWR with domain generalization techniques, indicating that the mere plug-and-play of existing algorithms is not enough to recognize new and unknown categories in unseen domains. Our results clearly point toward open issues and future research directions, that need to be investigated for building robot visual systems able to function reliably under these challenging yet very real conditions. Code available at https://github.com/DarioFontanel/OWR-VisualDomains



### Interpretable Compositional Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.04474v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04474v1)
- **Published**: 2021-07-09 15:01:24+00:00
- **Updated**: 2021-07-09 15:01:24+00:00
- **Authors**: Wen Shen, Zhihua Wei, Shikun Huang, Binbin Zhang, Jiaqi Fan, Ping Zhao, Quanshi Zhang
- **Comment**: IJCAI2021
- **Journal**: None
- **Summary**: The reasonable definition of semantic interpretability presents the core challenge in explainable AI. This paper proposes a method to modify a traditional convolutional neural network (CNN) into an interpretable compositional CNN, in order to learn filters that encode meaningful visual patterns in intermediate convolutional layers. In a compositional CNN, each filter is supposed to consistently represent a specific compositional object part or image region with a clear meaning. The compositional CNN learns from image labels for classification without any annotations of parts or regions for supervision. Our method can be broadly applied to different types of CNNs. Experiments have demonstrated the effectiveness of our method.



### MutualEyeContact: A conversation analysis tool with focus on eye contact
- **Arxiv ID**: http://arxiv.org/abs/2107.04476v1
- **DOI**: 10.1145/3379156.3391340
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04476v1)
- **Published**: 2021-07-09 15:05:53+00:00
- **Updated**: 2021-07-09 15:05:53+00:00
- **Authors**: Alexander Schäfer, Tomoko Isomura, Gerd Reis, Katsumi Watanabe, Didier Stricker
- **Comment**: None
- **Journal**: None
- **Summary**: Eye contact between individuals is particularly important for understanding human behaviour. To further investigate the importance of eye contact in social interactions, portable eye tracking technology seems to be a natural choice. However, the analysis of available data can become quite complex. Scientists need data that is calculated quickly and accurately. Additionally, the relevant data must be automatically separated to save time. In this work, we propose a tool called MutualEyeContact which excels in those tasks and can help scientists to understand the importance of (mutual) eye contact in social interactions. We combine state-of-the-art eye tracking with face recognition based on machine learning and provide a tool for analysis and visualization of social interaction sessions. This work is a joint collaboration of computer scientists and cognitive scientists. It combines the fields of social and behavioural science with computer vision and deep learning.



### Semantic and Geometric Unfolding of StyleGAN Latent Space
- **Arxiv ID**: http://arxiv.org/abs/2107.04481v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04481v1)
- **Published**: 2021-07-09 15:12:55+00:00
- **Updated**: 2021-07-09 15:12:55+00:00
- **Authors**: Mustafa Shukor, Xu Yao, Bharath Bhushan Damodaran, Pierre Hellier
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have proven to be surprisingly efficient for image editing by inverting and manipulating the latent code corresponding to a natural image. This property emerges from the disentangled nature of the latent space. In this paper, we identify two geometric limitations of such latent space: (a) euclidean distances differ from image perceptual distance, and (b) disentanglement is not optimal and facial attribute separation using linear model is a limiting hypothesis. We thus propose a new method to learn a proxy latent representation using normalizing flows to remedy these limitations, and show that this leads to a more efficient space for face image editing.



### Hacking VMAF and VMAF NEG: vulnerability to different preprocessing methods
- **Arxiv ID**: http://arxiv.org/abs/2107.04510v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2107.04510v2)
- **Published**: 2021-07-09 15:53:47+00:00
- **Updated**: 2021-08-16 07:04:36+00:00
- **Authors**: Maksim Siniukov, Anastasia Antsiferova, Dmitriy Kulikov, Dmitriy Vatolin
- **Comment**: None
- **Journal**: None
- **Summary**: Video-quality measurement plays a critical role in the development of video-processing applications. In this paper, we show how video preprocessing can artificially increase the popular quality metric VMAF and its tuning-resistant version, VMAF NEG. We propose a pipeline that tunes processing-algorithm parameters to increase VMAF by up to 218.8%. A subjective comparison revealed that for most preprocessing methods, a video's visual quality drops or stays unchanged. We also show that some preprocessing methods can increase VMAF NEG scores by up to 23.6%.



### Gradient-Based Quantification of Epistemic Uncertainty for Deep Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2107.04517v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04517v2)
- **Published**: 2021-07-09 16:04:11+00:00
- **Updated**: 2022-03-17 20:42:48+00:00
- **Authors**: Tobias Riedlinger, Matthias Rottmann, Marius Schubert, Hanno Gottschalk
- **Comment**: 26 pages, 11 figures, 14 tables
- **Journal**: None
- **Summary**: The vast majority of uncertainty quantification methods for deep object detectors such as variational inference are based on the network output. Here, we study gradient-based epistemic uncertainty metrics for deep object detectors to obtain reliable confidence estimates. We show that they contain predictive information and that they capture information orthogonal to that of common, output-based uncertainty estimation methods like Monte-Carlo dropout and deep ensembles. To this end, we use meta classification and meta regression to produce confidence estimates using gradient metrics and other baselines for uncertainty quantification which are in principle applicable to any object detection architecture. Specifically, we employ false positive detection and prediction of localization quality to investigate uncertainty content of our metrics and compute the calibration errors of meta classifiers. Moreover, we use them as a post-processing filter mechanism to the object detection pipeline and compare object detection performance. Our results show that gradient-based uncertainty is itself on par with output-based methods across different detectors and datasets. More significantly, combined meta classifiers based on gradient and output-based metrics outperform the standalone models. Based on this result, we conclude that gradient uncertainty adds orthogonal information to output-based methods. This suggests that variational inference may be supplemented by gradient-based uncertainty to obtain improved confidence measures, contributing to down-stream applications of deep object detectors and improving their probabilistic reliability.



### Learning Cascaded Detection Tasks with Weakly-Supervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2107.04523v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04523v1)
- **Published**: 2021-07-09 16:18:12+00:00
- **Updated**: 2021-07-09 16:18:12+00:00
- **Authors**: Niklas Hanselmann, Nick Schneider, Benedikt Ortelt, Andreas Geiger
- **Comment**: Accepted to IEEE IV 2021
- **Journal**: None
- **Summary**: In order to handle the challenges of autonomous driving, deep learning has proven to be crucial in tackling increasingly complex tasks, such as 3D detection or instance segmentation. State-of-the-art approaches for image-based detection tasks tackle this complexity by operating in a cascaded fashion: they first extract a 2D bounding box based on which additional attributes, e.g. instance masks, are inferred. While these methods perform well, a key challenge remains the lack of accurate and cheap annotations for the growing variety of tasks. Synthetic data presents a promising solution but, despite the effort in domain adaptation research, the gap between synthetic and real data remains an open problem. In this work, we propose a weakly supervised domain adaptation setting which exploits the structure of cascaded detection tasks. In particular, we learn to infer the attributes solely from the source domain while leveraging 2D bounding boxes as weak labels in both domains to explain the domain shift. We further encourage domain-invariant features through class-wise feature alignment using ground-truth class information, which is not available in the unsupervised setting. As our experiments demonstrate, the approach is competitive with fully supervised settings while outperforming unsupervised adaptation approaches by a large margin.



### Event-Based Feature Tracking in Continuous Time with Sliding Window Optimization
- **Arxiv ID**: http://arxiv.org/abs/2107.04536v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04536v1)
- **Published**: 2021-07-09 16:41:20+00:00
- **Updated**: 2021-07-09 16:41:20+00:00
- **Authors**: Jason Chui, Simon Klenk, Daniel Cremers
- **Comment**: 9 pages, 4 figures, 1 table
- **Journal**: None
- **Summary**: We propose a novel method for continuous-time feature tracking in event cameras. To this end, we track features by aligning events along an estimated trajectory in space-time such that the projection on the image plane results in maximally sharp event patch images. The trajectory is parameterized by $n^{th}$ order B-splines, which are continuous up to $(n-2)^{th}$ derivative. In contrast to previous work, we optimize the curve parameters in a sliding window fashion. On a public dataset we experimentally confirm that the proposed sliding-window B-spline optimization leads to longer and more accurate feature tracks than in previous work.



### Modality specific U-Net variants for biomedical image segmentation: A survey
- **Arxiv ID**: http://arxiv.org/abs/2107.04537v4
- **DOI**: 10.1007/s10462-022-10152-1
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04537v4)
- **Published**: 2021-07-09 16:41:40+00:00
- **Updated**: 2022-01-27 12:07:14+00:00
- **Authors**: Narinder Singh Punn, Sonali Agarwal
- **Comment**: None
- **Journal**: Artificial Intelligence Review (2022)
- **Summary**: With the advent of advancements in deep learning approaches, such as deep convolution neural network, residual neural network, adversarial network; U-Net architectures are most widely utilized in biomedical image segmentation to address the automation in identification and detection of the target regions or sub-regions. In recent studies, U-Net based approaches have illustrated state-of-the-art performance in different applications for the development of computer-aided diagnosis systems for early diagnosis and treatment of diseases such as brain tumor, lung cancer, alzheimer, breast cancer, etc., using various modalities. This article contributes in presenting the success of these approaches by describing the U-Net framework, followed by the comprehensive analysis of the U-Net variants by performing 1) inter-modality, and 2) intra-modality categorization to establish better insights into the associated challenges and solutions. Besides, this article also highlights the contribution of U-Net based frameworks in the ongoing pandemic, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) also known as COVID-19. Finally, the strengths and similarities of these U-Net variants are analysed along with the challenges involved in biomedical image segmentation to uncover promising future research directions in this area.



### Cross-modal Attention for MRI and Ultrasound Volume Registration
- **Arxiv ID**: http://arxiv.org/abs/2107.04548v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04548v2)
- **Published**: 2021-07-09 17:04:02+00:00
- **Updated**: 2021-07-12 01:46:43+00:00
- **Authors**: Xinrui Song, Hengtao Guo, Xuanang Xu, Hanqing Chao, Sheng Xu, Baris Turkbey, Bradford J. Wood, Ge Wang, Pingkun Yan
- **Comment**: This paper has been accepted by MICCAI 2021
- **Journal**: None
- **Summary**: Prostate cancer biopsy benefits from accurate fusion of transrectal ultrasound (TRUS) and magnetic resonance (MR) images. In the past few years, convolutional neural networks (CNNs) have been proved powerful in extracting image features crucial for image registration. However, challenging applications and recent advances in computer vision suggest that CNNs are quite limited in its ability to understand spatial correspondence between features, a task in which the self-attention mechanism excels. This paper aims to develop a self-attention mechanism specifically for cross-modal image registration. Our proposed cross-modal attention block effectively maps each of the features in one volume to all features in the corresponding volume. Our experimental results demonstrate that a CNN network designed with the cross-modal attention block embedded outperforms an advanced CNN network 10 times of its size. We also incorporated visualization techniques to improve the interpretability of our network. The source code of our work is available at https://github.com/DIAL-RPI/Attention-Reg .



### White-Box Cartoonization Using An Extended GAN Framework
- **Arxiv ID**: http://arxiv.org/abs/2107.04551v1
- **DOI**: 10.33564/IJEAST.2021.v05i12.049
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.04551v1)
- **Published**: 2021-07-09 17:09:19+00:00
- **Updated**: 2021-07-09 17:09:19+00:00
- **Authors**: Amey Thakur, Hasan Rizvi, Mega Satish
- **Comment**: 5 pages, 6 figures. International Journal of Engineering Applied
  Sciences and Technology, 2021
- **Journal**: None
- **Summary**: In the present study, we propose to implement a new framework for estimating generative models via an adversarial process to extend an existing GAN framework and develop a white-box controllable image cartoonization, which can generate high-quality cartooned images/videos from real-world photos and videos. The learning purposes of our system are based on three distinct representations: surface representation, structure representation, and texture representation. The surface representation refers to the smooth surface of the images. The structure representation relates to the sparse colour blocks and compresses generic content. The texture representation shows the texture, curves, and features in cartoon images. Generative Adversarial Network (GAN) framework decomposes the images into different representations and learns from them to generate cartoon images. This decomposition makes the framework more controllable and flexible which allows users to make changes based on the required output. This approach overcomes any previous system in terms of maintaining clarity, colours, textures, shapes of images yet showing the characteristics of cartoon images.



### Seven Basic Expression Recognition Using ResNet-18
- **Arxiv ID**: http://arxiv.org/abs/2107.04569v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04569v1)
- **Published**: 2021-07-09 17:40:57+00:00
- **Updated**: 2021-07-09 17:40:57+00:00
- **Authors**: Satnam Singh, Doris Schicker
- **Comment**: None
- **Journal**: None
- **Summary**: We propose to use a ResNet-18 architecture that was pre-trained on the FER+ dataset for tackling the problem of affective behavior analysis in-the-wild (ABAW) for classification of the seven basic expressions, namely, neutral, anger, disgust, fear, happiness, sadness and surprise. As part of the second workshop and competition on affective behavior analysis in-the-wild (ABAW2), a database consisting of 564 videos with around 2.8M frames is provided along with labels for these seven basic expressions. We resampled the dataset to counter class-imbalances by under-sampling the over-represented classes and over-sampling the under-represented classes along with class-wise weights. To avoid overfitting we performed data-augmentation and used L2 regularisation. Our classifier reaches an ABAW2 score of 0.4 and therefore exceeds the baseline results provided by the hosts of the competition.



### ANCER: Anisotropic Certification via Sample-wise Volume Maximization
- **Arxiv ID**: http://arxiv.org/abs/2107.04570v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04570v4)
- **Published**: 2021-07-09 17:42:38+00:00
- **Updated**: 2022-08-31 13:16:19+00:00
- **Authors**: Francisco Eiras, Motasem Alfarra, M. Pawan Kumar, Philip H. S. Torr, Puneet K. Dokania, Bernard Ghanem, Adel Bibi
- **Comment**: First two authors and the last one contributed equally to this work
- **Journal**: None
- **Summary**: Randomized smoothing has recently emerged as an effective tool that enables certification of deep neural network classifiers at scale. All prior art on randomized smoothing has focused on isotropic $\ell_p$ certification, which has the advantage of yielding certificates that can be easily compared among isotropic methods via $\ell_p$-norm radius. However, isotropic certification limits the region that can be certified around an input to worst-case adversaries, i.e., it cannot reason about other "close", potentially large, constant prediction safe regions. To alleviate this issue, (i) we theoretically extend the isotropic randomized smoothing $\ell_1$ and $\ell_2$ certificates to their generalized anisotropic counterparts following a simplified analysis. Moreover, (ii) we propose evaluation metrics allowing for the comparison of general certificates - a certificate is superior to another if it certifies a superset region - with the quantification of each certificate through the volume of the certified region. We introduce ANCER, a framework for obtaining anisotropic certificates for a given test set sample via volume maximization. We achieve it by generalizing memory-based certification of data-dependent classifiers. Our empirical results demonstrate that ANCER achieves state-of-the-art $\ell_1$ and $\ell_2$ certified accuracy on CIFAR-10 and ImageNet in the data-dependence setting, while certifying larger regions in terms of volume, highlighting the benefits of moving away from isotropic analysis. Our code is available in https://github.com/MotasemAlfarra/ANCER.



### ViTGAN: Training GANs with Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2107.04589v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04589v1)
- **Published**: 2021-07-09 17:59:30+00:00
- **Updated**: 2021-07-09 17:59:30+00:00
- **Authors**: Kwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang, Zhuowen Tu, Ce Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Vision Transformers (ViTs) have shown competitive performance on image recognition while requiring less vision-specific inductive biases. In this paper, we investigate if such observation can be extended to image generation. To this end, we integrate the ViT architecture into generative adversarial networks (GANs). We observe that existing regularization methods for GANs interact poorly with self-attention, causing serious instability during training. To resolve this issue, we introduce novel regularization techniques for training GANs with ViTs. Empirically, our approach, named ViTGAN, achieves comparable performance to state-of-the-art CNN-based StyleGAN2 on CIFAR-10, CelebA, and LSUN bedroom datasets.



### Optimal Triangulation Method is Not Really Optimal
- **Arxiv ID**: http://arxiv.org/abs/2107.04618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04618v1)
- **Published**: 2021-07-09 18:14:36+00:00
- **Updated**: 2021-07-09 18:14:36+00:00
- **Authors**: Seyed-Mahdi Nasiri, Reshad Hosseini, Hadi Moradi
- **Comment**: 9 pages, 13 figures
- **Journal**: None
- **Summary**: Triangulation refers to the problem of finding a 3D point from its 2D projections on multiple camera images. For solving this problem, it is the common practice to use so-called optimal triangulation method, which we call the L2 method in this paper. But, the method can be optimal only if we assume no uncertainty in the camera parameters. Through extensive comparison on synthetic and real data, we observed that the L2 method is actually not the best choice when there is uncertainty in the camera parameters. Interestingly, it can be observed that the simple mid-point method outperforms other methods. Apart from its high performance, the mid-point method has a simple closed formed solution for multiple camera images while the L2 method is hard to be used for more than two camera images. Therefore, in contrast to the common practice, we argue that the simple mid-point method should be used in structure-from-motion applications where there is uncertainty in camera parameters.



### Diverse Video Generation using a Gaussian Process Trigger
- **Arxiv ID**: http://arxiv.org/abs/2107.04619v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2107.04619v1)
- **Published**: 2021-07-09 18:15:16+00:00
- **Updated**: 2021-07-09 18:15:16+00:00
- **Authors**: Gaurav Shrivastava, Abhinav Shrivastava
- **Comment**: International Conference on Learning Representations, 2021
- **Journal**: None
- **Summary**: Generating future frames given a few context (or past) frames is a challenging task. It requires modeling the temporal coherence of videos and multi-modality in terms of diversity in the potential future states. Current variational approaches for video generation tend to marginalize over multi-modal future outcomes. Instead, we propose to explicitly model the multi-modality in the future outcomes and leverage it to sample diverse futures. Our approach, Diverse Video Generator, uses a Gaussian Process (GP) to learn priors on future states given the past and maintains a probability distribution over possible futures given a particular sample. In addition, we leverage the changes in this distribution over time to control the sampling of diverse future states by estimating the end of ongoing sequences. That is, we use the variance of GP over the output function space to trigger a change in an action sequence. We achieve state-of-the-art results on diverse future frame generation in terms of reconstruction quality and diversity of the generated sequences.



### Cumulative Assessment for Urban 3D Modeling
- **Arxiv ID**: http://arxiv.org/abs/2107.04622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04622v1)
- **Published**: 2021-07-09 18:29:50+00:00
- **Updated**: 2021-07-09 18:29:50+00:00
- **Authors**: Shea Hagstrom, Hee Won Pak, Stephanie Ku, Sean Wang, Gregory Hager, Myron Brown
- **Comment**: Published in IEEE International Geoscience and Remote Sensing
  Symposium (IGARSS), 2021
- **Journal**: None
- **Summary**: Urban 3D modeling from satellite images requires accurate semantic segmentation to delineate urban features, multiple view stereo for 3D reconstruction of surface heights, and 3D model fitting to produce compact models with accurate surface slopes. In this work, we present a cumulative assessment metric that succinctly captures error contributions from each of these components. We demonstrate our approach by providing challenging public datasets and extending two open source projects to provide an end-to-end 3D modeling baseline solution to stimulate further research and evaluation with a public leaderboard.



### Self-Supervised Generative Adversarial Network for Depth Estimation in Laparoscopic Images
- **Arxiv ID**: http://arxiv.org/abs/2107.04644v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04644v1)
- **Published**: 2021-07-09 19:40:20+00:00
- **Updated**: 2021-07-09 19:40:20+00:00
- **Authors**: Baoru Huang, Jianqing Zheng, Anh Nguyen, David Tuch, Kunal Vyas, Stamatia Giannarou, Daniel S. Elson
- **Comment**: None
- **Journal**: None
- **Summary**: Dense depth estimation and 3D reconstruction of a surgical scene are crucial steps in computer assisted surgery. Recent work has shown that depth estimation from a stereo images pair could be solved with convolutional neural networks. However, most recent depth estimation models were trained on datasets with per-pixel ground truth. Such data is especially rare for laparoscopic imaging, making it hard to apply supervised depth estimation to real surgical applications. To overcome this limitation, we propose SADepth, a new self-supervised depth estimation method based on Generative Adversarial Networks. It consists of an encoder-decoder generator and a discriminator to incorporate geometry constraints during training. Multi-scale outputs from the generator help to solve the local minima caused by the photometric reprojection loss, while the adversarial learning improves the framework generation quality. Extensive experiments on two public datasets show that SADepth outperforms recent state-of-the-art unsupervised methods by a large margin, and reduces the gap between supervised and unsupervised depth estimation in laparoscopic images.



### Efficient Real-Time Image Recognition Using Collaborative Swarm of UAVs and Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.04648v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2107.04648v1)
- **Published**: 2021-07-09 19:47:02+00:00
- **Updated**: 2021-07-09 19:47:02+00:00
- **Authors**: Marwan Dhuheir, Emna Baccour, Aiman Erbad, Sinan Sabeeh, Mounir Hamdi
- **Comment**: conference paper accepted and presented at 17th Int. Wireless
  Communications & Mobile Computing Conference - IWCMC 2021, Harbin, China
- **Journal**: None
- **Summary**: Unmanned Aerial Vehicles (UAVs) have recently attracted significant attention due to their outstanding ability to be used in different sectors and serve in difficult and dangerous areas. Moreover, the advancements in computer vision and artificial intelligence have increased the use of UAVs in various applications and solutions, such as forest fires detection and borders monitoring. However, using deep neural networks (DNNs) with UAVs introduces several challenges of processing deeper networks and complex models, which restricts their on-board computation. In this work, we present a strategy aiming at distributing inference requests to a swarm of resource-constrained UAVs that classifies captured images on-board and finds the minimum decision-making latency. We formulate the model as an optimization problem that minimizes the latency between acquiring images and making the final decisions. The formulated optimization solution is an NP-hard problem. Hence it is not adequate for online resource allocation. Therefore, we introduce an online heuristic solution, namely DistInference, to find the layers placement strategy that gives the best latency among the available UAVs. The proposed approach is general enough to be used for different low decision-latency applications as well as for all CNN types organized into the pipeline of layers (e.g., VGG) or based on residual blocks (e.g., ResNet).



### Scaled-Time-Attention Robust Edge Network
- **Arxiv ID**: http://arxiv.org/abs/2107.04688v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T05
- **Links**: [PDF](http://arxiv.org/pdf/2107.04688v1)
- **Published**: 2021-07-09 21:24:49+00:00
- **Updated**: 2021-07-09 21:24:49+00:00
- **Authors**: Richard Lau, Lihan Yao, Todd Huster, William Johnson, Stephen Arleth, Justin Wong, Devin Ridge, Michael Fletcher, William C. Headley
- **Comment**: 20 pages, 22 figures, 9 tables, Darpa Distribution Statement A.
  Approved for public release. Distribution Unlimited
- **Journal**: None
- **Summary**: This paper describes a systematic approach towards building a new family of neural networks based on a delay-loop version of a reservoir neural network. The resulting architecture, called Scaled-Time-Attention Robust Edge (STARE) network, exploits hyper dimensional space and non-multiply-and-add computation to achieve a simpler architecture, which has shallow layers, is simple to train, and is better suited for Edge applications, such as Internet of Things (IoT), over traditional deep neural networks. STARE incorporates new AI concepts such as Attention and Context, and is best suited for temporal feature extraction and classification. We demonstrate that STARE is applicable to a variety of applications with improved performance and lower implementation complexity. In particular, we showed a novel way of applying a dual-loop configuration to detection and identification of drone vs bird in a counter Unmanned Air Systems (UAS) detection application by exploiting both spatial (video frame) and temporal (trajectory) information. We also demonstrated that the STARE performance approaches that of a State-of-the-Art deep neural network in classifying RF modulations, and outperforms Long Short-term Memory (LSTM) in a special case of Mackey Glass time series prediction. To demonstrate hardware efficiency, we designed and developed an FPGA implementation of the STARE algorithm to demonstrate its low-power and high-throughput operations. In addition, we illustrate an efficient structure for integrating a massively parallel implementation of the STARE algorithm for ASIC implementation.



### Lifelong Teacher-Student Network Learning
- **Arxiv ID**: http://arxiv.org/abs/2107.04689v1
- **DOI**: 10.1109/TPAMI.2021.3092677
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04689v1)
- **Published**: 2021-07-09 21:25:56+00:00
- **Updated**: 2021-07-09 21:25:56+00:00
- **Authors**: Fei Ye, Adrian G. Bors
- **Comment**: 18 pages, 18 figures. in IEEE Transactions on Pattern Analysis and
  Machine Intelligence
- **Journal**: None
- **Summary**: A unique cognitive capability of humans consists in their ability to acquire new knowledge and skills from a sequence of experiences. Meanwhile, artificial intelligence systems are good at learning only the last given task without being able to remember the databases learnt in the past. We propose a novel lifelong learning methodology by employing a Teacher-Student network framework. While the Student module is trained with a new given database, the Teacher module would remind the Student about the information learnt in the past. The Teacher, implemented by a Generative Adversarial Network (GAN), is trained to preserve and replay past knowledge corresponding to the probabilistic representations of previously learn databases. Meanwhile, the Student module is implemented by a Variational Autoencoder (VAE) which infers its latent variable representation from both the output of the Teacher module as well as from the newly available database. Moreover, the Student module is trained to capture both continuous and discrete underlying data representations across different domains. The proposed lifelong learning framework is applied in supervised, semi-supervised and unsupervised training. The code is available~: \url{https://github.com/dtuzi123/Lifelong-Teacher-Student-Network-Learning}



### Lifelong Mixture of Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2107.04694v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04694v1)
- **Published**: 2021-07-09 22:07:39+00:00
- **Updated**: 2021-07-09 22:07:39+00:00
- **Authors**: Fei Ye, Adrian G. Bors
- **Comment**: Accepted by IEEE Transactions on Neural Networks and Learning Systems
- **Journal**: None
- **Summary**: In this paper, we propose an end-to-end lifelong learning mixture of experts. Each expert is implemented by a Variational Autoencoder (VAE). The experts in the mixture system are jointly trained by maximizing a mixture of individual component evidence lower bounds (MELBO) on the log-likelihood of the given training samples. The mixing coefficients in the mixture, control the contributions of each expert in the goal representation. These are sampled from a Dirichlet distribution whose parameters are determined through non-parametric estimation during lifelong learning. The model can learn new tasks fast when these are similar to those previously learnt. The proposed Lifelong mixture of VAE (L-MVAE) expands its architecture with new components when learning a completely new task. After the training, our model can automatically determine the relevant expert to be used when fed with new data samples. This mechanism benefits both the memory efficiency and the required computational cost as only one expert is used during the inference. The L-MVAE inference model is able to perform interpolation in the joint latent space across the data domains associated with different tasks and is shown to be efficient for disentangled learning representation.



### InfoVAEGAN : learning joint interpretable representations by information maximization and maximum likelihood
- **Arxiv ID**: http://arxiv.org/abs/2107.04705v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04705v1)
- **Published**: 2021-07-09 22:38:10+00:00
- **Updated**: 2021-07-09 22:38:10+00:00
- **Authors**: Fei Ye, Adrian G. Bors
- **Comment**: Accepted at International Conference on Image Processing (ICIP 2021)
- **Journal**: None
- **Summary**: Learning disentangled and interpretable representations is an important step towards accomplishing comprehensive data representations on the manifold. In this paper, we propose a novel representation learning algorithm which combines the inference abilities of Variational Autoencoders (VAE) with the generalization capability of Generative Adversarial Networks (GAN). The proposed model, called InfoVAEGAN, consists of three networks~: Encoder, Generator and Discriminator. InfoVAEGAN aims to jointly learn discrete and continuous interpretable representations in an unsupervised manner by using two different data-free log-likelihood functions onto the variables sampled from the generator's distribution. We propose a two-stage algorithm for optimizing the inference network separately from the generator training. Moreover, we enforce the learning of interpretable representations through the maximization of the mutual information between the existing latent variables and those created through generative and inference processes.



### Lifelong Twin Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2107.04708v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2107.04708v1)
- **Published**: 2021-07-09 22:52:49+00:00
- **Updated**: 2021-07-09 22:52:49+00:00
- **Authors**: Fei Ye, Adrian G. Bors
- **Comment**: Accepted at International Conference on Image Processing (ICIP 2021)
- **Journal**: None
- **Summary**: In this paper, we propose a new continuously learning generative model, called the Lifelong Twin Generative Adversarial Networks (LT-GANs). LT-GANs learns a sequence of tasks from several databases and its architecture consists of three components: two identical generators, namely the Teacher and Assistant, and one Discriminator. In order to allow for the LT-GANs to learn new concepts without forgetting, we introduce a new lifelong training approach, namely Lifelong Adversarial Knowledge Distillation (LAKD), which encourages the Teacher and Assistant to alternately teach each other, while learning a new database. This training approach favours transferring knowledge from a more knowledgeable player to another player which knows less information about a previously given task.



### Automated Graph Learning via Population Based Self-Tuning GCN
- **Arxiv ID**: http://arxiv.org/abs/2107.04713v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2107.04713v1)
- **Published**: 2021-07-09 23:05:21+00:00
- **Updated**: 2021-07-09 23:05:21+00:00
- **Authors**: Ronghang Zhu, Zhiqiang Tao, Yaliang Li, Sheng Li
- **Comment**: This manuscript has been accepted by the SIGIR2021
- **Journal**: None
- **Summary**: Owing to the remarkable capability of extracting effective graph embeddings, graph convolutional network (GCN) and its variants have been successfully applied to a broad range of tasks, such as node classification, link prediction, and graph classification. Traditional GCN models suffer from the issues of overfitting and oversmoothing, while some recent techniques like DropEdge could alleviate these issues and thus enable the development of deep GCN. However, training GCN models is non-trivial, as it is sensitive to the choice of hyperparameters such as dropout rate and learning weight decay, especially for deep GCN models. In this paper, we aim to automate the training of GCN models through hyperparameter optimization. To be specific, we propose a self-tuning GCN approach with an alternate training algorithm, and further extend our approach by incorporating the population based training scheme. Experimental results on three benchmark datasets demonstrate the effectiveness of our approaches on optimizing multi-layer GCN, compared with several representative baselines.



### A Topological-Framework to Improve Analysis of Machine Learning Model Performance
- **Arxiv ID**: http://arxiv.org/abs/2107.04714v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.GN
- **Links**: [PDF](http://arxiv.org/pdf/2107.04714v1)
- **Published**: 2021-07-09 23:11:13+00:00
- **Updated**: 2021-07-09 23:11:13+00:00
- **Authors**: Henry Kvinge, Colby Wight, Sarah Akers, Scott Howland, Woongjo Choi, Xiaolong Ma, Luke Gosink, Elizabeth Jurrus, Keerti Kappagantula, Tegan H. Emerson
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: As both machine learning models and the datasets on which they are evaluated have grown in size and complexity, the practice of using a few summary statistics to understand model performance has become increasingly problematic. This is particularly true in real-world scenarios where understanding model failure on certain subpopulations of the data is of critical importance. In this paper we propose a topological framework for evaluating machine learning models in which a dataset is treated as a "space" on which a model operates. This provides us with a principled way to organize information about model performance at both the global level (over the entire test set) and also the local level (on specific subpopulations). Finally, we describe a topological data structure, presheaves, which offer a convenient way to store and analyze model performance between different subpopulations.



### DDCNet: Deep Dilated Convolutional Neural Network for Dense Prediction
- **Arxiv ID**: http://arxiv.org/abs/2107.04715v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2107.04715v1)
- **Published**: 2021-07-09 23:15:34+00:00
- **Updated**: 2021-07-09 23:15:34+00:00
- **Authors**: Ali Salehi, Madhusudhanan Balasubramanian
- **Comment**: 32 pages, 13 figures, 3 tables
- **Journal**: None
- **Summary**: Dense pixel matching problems such as optical flow and disparity estimation are among the most challenging tasks in computer vision. Recently, several deep learning methods designed for these problems have been successful. A sufficiently larger effective receptive field (ERF) and a higher resolution of spatial features within a network are essential for providing higher-resolution dense estimates. In this work, we present a systemic approach to design network architectures that can provide a larger receptive field while maintaining a higher spatial feature resolution. To achieve a larger ERF, we utilized dilated convolutional layers. By aggressively increasing dilation rates in the deeper layers, we were able to achieve a sufficiently larger ERF with a significantly fewer number of trainable parameters. We used optical flow estimation problem as the primary benchmark to illustrate our network design strategy. The benchmark results (Sintel, KITTI, and Middlebury) indicate that our compact networks can achieve comparable performance in the class of lightweight networks.



### U-Net with Hierarchical Bottleneck Attention for Landmark Detection in Fundus Images of the Degenerated Retina
- **Arxiv ID**: http://arxiv.org/abs/2107.04721v1
- **DOI**: 10.1007/978-3-030-87000-3_7
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2107.04721v1)
- **Published**: 2021-07-09 23:57:51+00:00
- **Updated**: 2021-07-09 23:57:51+00:00
- **Authors**: Shuyun Tang, Ziming Qi, Jacob Granley, Michael Beyeler
- **Comment**: None
- **Journal**: Ophthalmic Medical Image Analysis 2021
- **Summary**: Fundus photography has routinely been used to document the presence and severity of retinal degenerative diseases such as age-related macular degeneration (AMD), glaucoma, and diabetic retinopathy (DR) in clinical practice, for which the fovea and optic disc (OD) are important retinal landmarks. However, the occurrence of lesions, drusen, and other retinal abnormalities during retinal degeneration severely complicates automatic landmark detection and segmentation. Here we propose HBA-U-Net: a U-Net backbone enriched with hierarchical bottleneck attention. The network consists of a novel bottleneck attention block that combines and refines self-attention, channel attention, and relative-position attention to highlight retinal abnormalities that may be important for fovea and OD segmentation in the degenerated retina. HBA-U-Net achieved state-of-the-art results on fovea detection across datasets and eye conditions (ADAM: Euclidean Distance (ED) of 25.4 pixels, REFUGE: 32.5 pixels, IDRiD: 32.1 pixels), on OD segmentation for AMD (ADAM: Dice Coefficient (DC) of 0.947), and on OD detection for DR (IDRiD: ED of 20.5 pixels). Our results suggest that HBA-U-Net may be well suited for landmark detection in the presence of a variety of retinal degenerative diseases.



