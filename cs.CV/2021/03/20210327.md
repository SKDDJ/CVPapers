# Arxiv Papers in cs.CV on 2021-03-27
### A Comprehensive Review of the Video-to-Text Problem
- **Arxiv ID**: http://arxiv.org/abs/2103.14785v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2103.14785v3)
- **Published**: 2021-03-27 02:12:28+00:00
- **Updated**: 2021-11-30 20:18:30+00:00
- **Authors**: Jesus Perez-Martin, Benjamin Bustos, Silvio Jamil F. Guimarães, Ivan Sipiran, Jorge Pérez, Grethel Coello Said
- **Comment**: 66 pages, 6 figures. Accepted by Artificial Intelligence Review
- **Journal**: None
- **Summary**: Research in the Vision and Language area encompasses challenging topics that seek to connect visual and textual information. When the visual information is related to videos, this takes us into Video-Text Research, which includes several challenging tasks such as video question answering, video summarization with natural language, and video-to-text and text-to-video conversion. This paper reviews the video-to-text problem, in which the goal is to associate an input video with its textual description. This association can be mainly made by retrieving the most relevant descriptions from a corpus or generating a new one given a context video. These two ways represent essential tasks for Computer Vision and Natural Language Processing communities, called text retrieval from video task and video captioning/description task. These two tasks are substantially more complex than predicting or retrieving a single sentence from an image. The spatiotemporal information present in videos introduces diversity and complexity regarding the visual content and the structure of associated language descriptions. This review categorizes and describes the state-of-the-art techniques for the video-to-text problem. It covers the main video-to-text methods and the ways to evaluate their performance. We analyze twenty-six benchmark datasets, showing their drawbacks and strengths for the problem requirements. We also show the progress that researchers have made on each dataset, we cover the challenges in the field, and we discuss future research directions.



### CalibDNN: Multimodal Sensor Calibration for Perception Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.14793v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14793v1)
- **Published**: 2021-03-27 02:43:37+00:00
- **Updated**: 2021-03-27 02:43:37+00:00
- **Authors**: Ganning Zhao, Jiesi Hu, Suya You, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: Current perception systems often carry multimodal imagers and sensors such as 2D cameras and 3D LiDAR sensors. To fuse and utilize the data for downstream perception tasks, robust and accurate calibration of the multimodal sensor data is essential. We propose a novel deep learning-driven technique (CalibDNN) for accurate calibration among multimodal sensor, specifically LiDAR-Camera pairs. The key innovation of the proposed work is that it does not require any specific calibration targets or hardware assistants, and the entire processing is fully automatic with a single model and single iteration. Results comparison among different methods and extensive experiments on different datasets demonstrates the state-of-the-art performance.



### Learning Efficient Photometric Feature Transform for Multi-view Stereo
- **Arxiv ID**: http://arxiv.org/abs/2103.14794v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2103.14794v1)
- **Published**: 2021-03-27 02:53:15+00:00
- **Updated**: 2021-03-27 02:53:15+00:00
- **Authors**: Kaizhang Kang, Cihui Xie, Ruisheng Zhu, Xiaohe Ma, Ping Tan, Hongzhi Wu, Kun Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel framework to learn to convert the perpixel photometric information at each view into spatially distinctive and view-invariant low-level features, which can be plugged into existing multi-view stereo pipeline for enhanced 3D reconstruction. Both the illumination conditions during acquisition and the subsequent per-pixel feature transform can be jointly optimized in a differentiable fashion. Our framework automatically adapts to and makes efficient use of the geometric information available in different forms of input data. High-quality 3D reconstructions of a variety of challenging objects are demonstrated on the data captured with an illumination multiplexing device, as well as a point light. Our results compare favorably with state-of-the-art techniques.



### Ensemble-in-One: Learning Ensemble within Random Gated Networks for Enhanced Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2103.14795v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.14795v1)
- **Published**: 2021-03-27 03:13:03+00:00
- **Updated**: 2021-03-27 03:13:03+00:00
- **Authors**: Yi Cai, Xuefei Ning, Huazhong Yang, Yu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial attacks have rendered high security risks on modern deep learning systems. Adversarial training can significantly enhance the robustness of neural network models by suppressing the non-robust features. However, the models often suffer from significant accuracy loss on clean data. Ensemble training methods have emerged as promising solutions for defending against adversarial attacks by diversifying the vulnerabilities among the sub-models, simultaneously maintaining comparable accuracy as standard training. However, existing ensemble methods are with poor scalability, owing to the rapid complexity increase when including more sub-models in the ensemble. Moreover, in real-world applications, it is difficult to deploy an ensemble with multiple sub-models, owing to the tight hardware resource budget and latency requirement. In this work, we propose ensemble-in-one (EIO), a simple but efficient way to train an ensemble within one random gated network (RGN). EIO augments the original model by replacing the parameterized layers with multi-path random gated blocks (RGBs) to construct a RGN. By diversifying the vulnerability of the numerous paths within the RGN, better robustness can be achieved. It provides high scalability because the paths within an EIO network exponentially increase with the network depth. Our experiments demonstrate that EIO consistently outperforms previous ensemble training methods with even less computational overhead.



### A Survey of Orthogonal Moments for Image Representation: Theory, Implementation, and Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2103.14799v3
- **DOI**: 10.1145/3479428
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14799v3)
- **Published**: 2021-03-27 03:41:08+00:00
- **Updated**: 2021-11-24 12:20:28+00:00
- **Authors**: Shuren Qi, Yushu Zhang, Chao Wang, Jiantao Zhou, Xiaochun Cao
- **Comment**: ACM Computing Surveys, Volume 55, Issue 1, January 2023, Article No
  1, pp 1-35, https://doi.org/10.1145/3479428
- **Journal**: None
- **Summary**: Image representation is an important topic in computer vision and pattern recognition. It plays a fundamental role in a range of applications towards understanding visual contents. Moment-based image representation has been reported to be effective in satisfying the core conditions of semantic description due to its beneficial mathematical properties, especially geometric invariance and independence. This paper presents a comprehensive survey of the orthogonal moments for image representation, covering recent advances in fast/accurate calculation, robustness/invariance optimization, definition extension, and application. We also create a software package for a variety of widely-used orthogonal moments and evaluate such methods in a same base. The presented theory analysis, software implementation, and evaluation results can support the community, particularly in developing novel techniques and promoting real-world applications.



### Face Transformer for Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.14803v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14803v2)
- **Published**: 2021-03-27 03:53:29+00:00
- **Updated**: 2021-04-13 02:55:36+00:00
- **Authors**: Yaoyao Zhong, Weihong Deng
- **Comment**: None
- **Journal**: None
- **Summary**: Recently there has been a growing interest in Transformer not only in NLP but also in computer vision. We wonder if transformer can be used in face recognition and whether it is better than CNNs. Therefore, we investigate the performance of Transformer models in face recognition. Considering the original Transformer may neglect the inter-patch information, we modify the patch generation process and make the tokens with sliding patches which overlaps with each others. The models are trained on CASIA-WebFace and MS-Celeb-1M databases, and evaluated on several mainstream benchmarks, including LFW, SLLFW, CALFW, CPLFW, TALFW, CFP-FP, AGEDB and IJB-C databases. We demonstrate that Face Transformer models trained on a large-scale database, MS-Celeb-1M, achieve comparable performance as CNN with similar number of parameters and MACs. To facilitate further researches, Face Transformer models and codes are available at https://github.com/zhongyy/Face-Transformer.



### SelfGait: A Spatiotemporal Representation Learning Method for Self-supervised Gait Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.14811v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14811v1)
- **Published**: 2021-03-27 05:15:39+00:00
- **Updated**: 2021-03-27 05:15:39+00:00
- **Authors**: Yiqun Liu, Yi Zeng, Jian Pu, Hongming Shan, Peiyang He, Junping Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Gait recognition plays a vital role in human identification since gait is a unique biometric feature that can be perceived at a distance. Although existing gait recognition methods can learn gait features from gait sequences in different ways, the performance of gait recognition suffers from insufficient labeled data, especially in some practical scenarios associated with short gait sequences or various clothing styles. It is unpractical to label the numerous gait data. In this work, we propose a self-supervised gait recognition method, termed SelfGait, which takes advantage of the massive, diverse, unlabeled gait data as a pre-training process to improve the representation abilities of spatiotemporal backbones. Specifically, we employ the horizontal pyramid mapping (HPM) and micro-motion template builder (MTB) as our spatiotemporal backbones to capture the multi-scale spatiotemporal representations. Experiments on CASIA-B and OU-MVLP benchmark gait datasets demonstrate the effectiveness of the proposed SelfGait compared with four state-of-the-art gait recognition methods. The source code has been released at https://github.com/EchoItLiu/SelfGait.



### Looking Beyond Two Frames: End-to-End Multi-Object Tracking Using Spatial and Temporal Transformers
- **Arxiv ID**: http://arxiv.org/abs/2103.14829v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14829v4)
- **Published**: 2021-03-27 07:23:38+00:00
- **Updated**: 2022-10-07 16:58:44+00:00
- **Authors**: Tianyu Zhu, Markus Hiller, Mahsa Ehsanpour, Rongkai Ma, Tom Drummond, Ian Reid, Hamid Rezatofighi
- **Comment**: This paper has been accepted as a Regular Paper in an upcoming issue
  of the Transactions on Pattern Analysis and Machine Intelligence (Tpami)
- **Journal**: None
- **Summary**: Tracking a time-varying indefinite number of objects in a video sequence over time remains a challenge despite recent advances in the field. Most existing approaches are not able to properly handle multi-object tracking challenges such as occlusion, in part because they ignore long-term temporal information. To address these shortcomings, we present MO3TR: a truly end-to-end Transformer-based online multi-object tracking (MOT) framework that learns to handle occlusions, track initiation and termination without the need for an explicit data association module or any heuristics. MO3TR encodes object interactions into long-term temporal embeddings using a combination of spatial and temporal Transformers, and recursively uses the information jointly with the input data to estimate the states of all tracked objects over time. The spatial attention mechanism enables our framework to learn implicit representations between all the objects and the objects to the measurements, while the temporal attention mechanism focuses on specific parts of past information, allowing our approach to resolve occlusions over multiple frames. Our experiments demonstrate the potential of this new approach, achieving results on par with or better than the current state-of-the-art on multiple MOT metrics for several popular multi-object tracking benchmarks.



### LiBRe: A Practical Bayesian Approach to Adversarial Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.14835v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.14835v2)
- **Published**: 2021-03-27 07:48:58+00:00
- **Updated**: 2021-05-31 06:42:21+00:00
- **Authors**: Zhijie Deng, Xiao Yang, Shizhen Xu, Hang Su, Jun Zhu
- **Comment**: IEEE/ CVF International Conference on Computer Vision and Pattern
  Recognition (CVPR), 2021
- **Journal**: None
- **Summary**: Despite their appealing flexibility, deep neural networks (DNNs) are vulnerable against adversarial examples. Various adversarial defense strategies have been proposed to resolve this problem, but they typically demonstrate restricted practicability owing to unsurmountable compromise on universality, effectiveness, or efficiency. In this work, we propose a more practical approach, Lightweight Bayesian Refinement (LiBRe), in the spirit of leveraging Bayesian neural networks (BNNs) for adversarial detection. Empowered by the task and attack agnostic modeling under Bayes principle, LiBRe can endow a variety of pre-trained task-dependent DNNs with the ability of defending heterogeneous adversarial attacks at a low cost. We develop and integrate advanced learning techniques to make LiBRe appropriate for adversarial detection. Concretely, we build the few-layer deep ensemble variational and adopt the pre-training & fine-tuning workflow to boost the effectiveness and efficiency of LiBRe. We further provide a novel insight to realise adversarial detection-oriented uncertainty quantification without inefficiently crafting adversarial examples during training. Extensive empirical studies covering a wide range of scenarios verify the practicability of LiBRe. We also conduct thorough ablation studies to evidence the superiority of our modeling and learning strategies.



### From Synthetic to Real: Unsupervised Domain Adaptation for Animal Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2103.14843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14843v1)
- **Published**: 2021-03-27 08:39:43+00:00
- **Updated**: 2021-03-27 08:39:43+00:00
- **Authors**: Chen Li, Gim Hee Lee
- **Comment**: CVPR2021
- **Journal**: None
- **Summary**: Animal pose estimation is an important field that has received increasing attention in the recent years. The main challenge for this task is the lack of labeled data. Existing works circumvent this problem with pseudo labels generated from data of other easily accessible domains such as synthetic data. However, these pseudo labels are noisy even with consistency check or confidence-based filtering due to the domain shift in the data. To solve this problem, we design a multi-scale domain adaptation module (MDAM) to reduce the domain gap between the synthetic and real data. We further introduce an online coarse-to-fine pseudo label updating strategy. Specifically, we propose a self-distillation module in an inner coarse-update loop and a mean-teacher in an outer fine-update loop to generate new pseudo labels that gradually replace the old ones. Consequently, our model is able to learn from the old pseudo labels at the early stage, and gradually switch to the new pseudo labels to prevent overfitting in the later stage. We evaluate our approach on the TigDog and VisDA 2019 datasets, where we outperform existing approaches by a large margin. We also demonstrate the generalization ability of our model by testing extensively on both unseen domains and unseen animal categories. Our code is available at the project website.



### Deep Ensemble Collaborative Learning by using Knowledge-transfer Graph for Fine-grained Object Classification
- **Arxiv ID**: http://arxiv.org/abs/2103.14845v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14845v1)
- **Published**: 2021-03-27 08:56:00+00:00
- **Updated**: 2021-03-27 08:56:00+00:00
- **Authors**: Naoki Okamoto, Soma Minami, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Mutual learning, in which multiple networks learn by sharing their knowledge, improves the performance of each network. However, the performance of ensembles of networks that have undergone mutual learning does not improve significantly from that of normal ensembles without mutual learning, even though the performance of each network has improved significantly. This may be due to the relationship between the knowledge in mutual learning and the individuality of the networks in the ensemble. In this study, we propose an ensemble method using knowledge transfer to improve the accuracy of ensembles by introducing a loss design that promotes diversity among networks in mutual learning. We use an attention map as knowledge, which represents the probability distribution and information in the middle layer of a network. There are many ways to combine networks and loss designs for knowledge transfer methods. Therefore, we use the automatic optimization of knowledge-transfer graphs to consider a variety of knowledge-transfer methods by graphically representing conventional mutual-learning and distillation methods and optimizing each element through hyperparameter search. The proposed method consists of a mechanism for constructing an ensemble in a knowledge-transfer graph, attention loss, and a loss design that promotes diversity among networks. We explore optimal ensemble learning by optimizing a knowledge-transfer graph to maximize ensemble accuracy. From exploration of graphs and evaluation experiments using the datasets of Stanford Dogs, Stanford Cars, and CUB-200-2011, we confirm that the proposed method is more accurate than a conventional ensemble method.



### AR Mapping: Accurate and Efficient Mapping for Augmented Reality
- **Arxiv ID**: http://arxiv.org/abs/2103.14846v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.14846v1)
- **Published**: 2021-03-27 08:57:48+00:00
- **Updated**: 2021-03-27 08:57:48+00:00
- **Authors**: Rui Huang, Chuan Fang, Kejie Qiu, Le Cui, Zilong Dong, Siyu Zhu, Ping Tan
- **Comment**: 8 pages, 14 figures
- **Journal**: None
- **Summary**: Augmented reality (AR) has gained increasingly attention from both research and industry communities. By overlaying digital information and content onto the physical world, AR enables users to experience the world in a more informative and efficient manner. As a major building block for AR systems, localization aims at determining the device's pose from a pre-built "map" consisting of visual and depth information in a known environment. While the localization problem has been widely studied in the literature, the "map" for AR systems is rarely discussed. In this paper, we introduce the AR Map for a specific scene to be composed of 1) color images with 6-DOF poses; 2) dense depth maps for each image and 3) a complete point cloud map. We then propose an efficient end-to-end solution to generating and evaluating AR Maps. Firstly, for efficient data capture, a backpack scanning device is presented with a unified calibration pipeline. Secondly, we propose an AR mapping pipeline which takes the input from the scanning device and produces accurate AR Maps. Finally, we present an approach to evaluating the accuracy of AR Maps with the help of the highly accurate reconstruction result from a high-end laser scanner. To the best of our knowledge, it is the first time to present an end-to-end solution to efficient and accurate mapping for AR applications.



### Video Rescaling Networks with Joint Optimization Strategies for Downscaling and Upscaling
- **Arxiv ID**: http://arxiv.org/abs/2103.14858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14858v1)
- **Published**: 2021-03-27 09:35:38+00:00
- **Updated**: 2021-03-27 09:35:38+00:00
- **Authors**: Yan-Cheng Huang, Yi-Hsin Chen, Cheng-You Lu, Hui-Po Wang, Wen-Hsiao Peng, Ching-Chun Huang
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: This paper addresses the video rescaling task, which arises from the needs of adapting the video spatial resolution to suit individual viewing devices. We aim to jointly optimize video downscaling and upscaling as a combined task. Most recent studies focus on image-based solutions, which do not consider temporal information. We present two joint optimization approaches based on invertible neural networks with coupling layers. Our Long Short-Term Memory Video Rescaling Network (LSTM-VRN) leverages temporal information in the low-resolution video to form an explicit prediction of the missing high-frequency information for upscaling. Our Multi-input Multi-output Video Rescaling Network (MIMO-VRN) proposes a new strategy for downscaling and upscaling a group of video frames simultaneously. Not only do they outperform the image-based invertible model in terms of quantitative and qualitative results, but also show much improved upscaling quality than the video rescaling methods without joint optimization. To our best knowledge, this work is the first attempt at the joint optimization of video downscaling and upscaling.



### TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2103.14862v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14862v5)
- **Published**: 2021-03-27 09:43:16+00:00
- **Updated**: 2021-08-04 02:11:13+00:00
- **Authors**: Wei Gao, Fang Wan, Xingjia Pan, Zhiliang Peng, Qi Tian, Zhenjun Han, Bolei Zhou, Qixiang Ye
- **Comment**: Accepted by ICCV2021 (poster)
- **Journal**: None
- **Summary**: Weakly supervised object localization (WSOL) is a challenging problem when given image category labels but requires to learn object localization models. Optimizing a convolutional neural network (CNN) for classification tends to activate local discriminative regions while ignoring complete object extent, causing the partial activation issue. In this paper, we argue that partial activation is caused by the intrinsic characteristics of CNN, where the convolution operations produce local receptive fields and experience difficulty to capture long-range feature dependency among pixels. We introduce the token semantic coupled attention map (TS-CAM) to take full advantage of the self-attention mechanism in visual transformer for long-range dependency extraction. TS-CAM first splits an image into a sequence of patch tokens for spatial embedding, which produce attention maps of long-range visual dependency to avoid partial activation. TS-CAM then re-allocates category-related semantics for patch tokens, enabling each of them to be aware of object categories. TS-CAM finally couples the patch tokens with the semantic-agnostic attention map to achieve semantic-aware localization. Experiments on the ILSVRC/CUB-200-2011 datasets show that TS-CAM outperforms its CNN-CAM counterparts by 7.1%/27.1% for WSOL, achieving state-of-the-art performance.



### Instance segmentation with the number of clusters incorporated in embedding learning
- **Arxiv ID**: http://arxiv.org/abs/2103.14869v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14869v1)
- **Published**: 2021-03-27 10:03:19+00:00
- **Updated**: 2021-03-27 10:03:19+00:00
- **Authors**: Jianfeng Cao, Hong Yan
- **Comment**: Accepted by ICASSP2021
- **Journal**: None
- **Summary**: Semantic and instance segmentation algorithms are two general yet distinct image segmentation solutions powered by Convolution Neural Network. While semantic segmentation benefits extensively from the end-to-end training strategy, instance segmentation is frequently framed as a multi-stage task, supported by learning-based discrimination and post-process clustering. Independent optimizations on substages instigate the accumulation of segmentation errors. In this work, we propose to embed prior clustering information into an embedding learning framework FCRNet, stimulating the one-stage instance segmentation. FCRNet relieves the complexity of post process by incorporating the number of clustering groups into the embedding space. The superior performance of FCRNet is verified and compared with other methods on the nucleus dataset BBBC006.



### Deep Learning Techniques for In-Crop Weed Identification: A Review
- **Arxiv ID**: http://arxiv.org/abs/2103.14872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14872v1)
- **Published**: 2021-03-27 10:08:41+00:00
- **Updated**: 2021-03-27 10:08:41+00:00
- **Authors**: Kun Hu, Zhiyong Wang, Guy Coleman, Asher Bender, Tingting Yao, Shan Zeng, Dezhen Song, Arnold Schumann, Michael Walsh
- **Comment**: None
- **Journal**: None
- **Summary**: Weeds are a significant threat to the agricultural productivity and the environment. The increasing demand for sustainable agriculture has driven innovations in accurate weed control technologies aimed at reducing the reliance on herbicides. With the great success of deep learning in various vision tasks, many promising image-based weed detection algorithms have been developed. This paper reviews recent developments of deep learning techniques in the field of image-based weed detection. The review begins with an introduction to the fundamentals of deep learning related to weed detection. Next, recent progresses on deep weed detection are reviewed with the discussion of the research materials including public weed datasets. Finally, the challenges of developing practically deployable weed detection methods are summarized, together with the discussions of the opportunities for future research.We hope that this review will provide a timely survey of the field and attract more researchers to address this inter-disciplinary research problem.



### Few-shot Semantic Image Synthesis Using StyleGAN Prior
- **Arxiv ID**: http://arxiv.org/abs/2103.14877v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2103.14877v2)
- **Published**: 2021-03-27 11:04:22+00:00
- **Updated**: 2021-05-12 09:37:56+00:00
- **Authors**: Yuki Endo, Yoshihiro Kanamori
- **Comment**: The source codes are available at
  https://github.com/endo-yuki-t/Fewshot-SMIS
- **Journal**: None
- **Summary**: This paper tackles a challenging problem of generating photorealistic images from semantic layouts in few-shot scenarios where annotated training pairs are hardly available but pixel-wise annotation is quite costly. We present a training strategy that performs pseudo labeling of semantic masks using the StyleGAN prior. Our key idea is to construct a simple mapping between the StyleGAN feature and each semantic class from a few examples of semantic masks. With such mappings, we can generate an unlimited number of pseudo semantic masks from random noise to train an encoder for controlling a pre-trained StyleGAN generator. Although the pseudo semantic masks might be too coarse for previous approaches that require pixel-aligned masks, our framework can synthesize high-quality images from not only dense semantic masks but also sparse inputs such as landmarks and scribbles. Qualitative and quantitative results with various datasets demonstrate improvement over previous approaches with respect to layout fidelity and visual quality in as few as one- or five-shot settings.



### COVID-19 personal protective equipment detection using real-time deep learning methods
- **Arxiv ID**: http://arxiv.org/abs/2103.14878v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.14878v1)
- **Published**: 2021-03-27 11:07:11+00:00
- **Updated**: 2021-03-27 11:07:11+00:00
- **Authors**: Shayan Khosravipour, Erfan Taghvaei, Nasrollah Moghadam Charkari
- **Comment**: None
- **Journal**: None
- **Summary**: The exponential spread of COVID-19 in over 215 countries has led WHO to recommend face masks and gloves for a safe return to school or work. We used artificial intelligence and deep learning algorithms for automatic face masks and gloves detection in public areas. We investigated and assessed the efficacy of two popular deep learning algorithms of YOLO (You Only Look Once) and SSD MobileNet for the detection and proper wearing of face masks and gloves trained over a data set of 8250 images imported from the internet. YOLOv3 is implemented using the DarkNet framework, and the SSD MobileNet algorithm is applied for the development of accurate object detection. The proposed models have been developed to provide accurate multi-class detection (Mask vs. No-Mask vs. Gloves vs. No-Gloves vs. Improper). When people wear their masks improperly, the method detects them as an improper class. The introduced models provide accuracies of (90.6% for YOLO and 85.5% for SSD) for multi-class detection. The systems' results indicate the efficiency and validity of detecting people who do not wear masks and gloves in public.



### Continuous Conditional Generative Adversarial Networks (cGAN) with Generator Regularization
- **Arxiv ID**: http://arxiv.org/abs/2103.14884v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.14884v1)
- **Published**: 2021-03-27 12:01:56+00:00
- **Updated**: 2021-03-27 12:01:56+00:00
- **Authors**: Yufeng Zheng, Yunkai Zhang, Zeyu Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Conditional Generative Adversarial Networks are known to be difficult to train, especially when the conditions are continuous and high-dimensional. To partially alleviate this difficulty, we propose a simple generator regularization term on the GAN generator loss in the form of Lipschitz penalty. Thus, when the generator is fed with neighboring conditions in the continuous space, the regularization term will leverage the neighbor information and push the generator to generate samples that have similar conditional distributions for each neighboring condition. We analyze the effect of the proposed regularization term and demonstrate its robust performance on a range of synthetic and real-world tasks.



### An Efficiently Coupled Shape and Appearance Prior for Active Contour Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.14887v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14887v2)
- **Published**: 2021-03-27 12:14:04+00:00
- **Updated**: 2021-03-31 00:45:20+00:00
- **Authors**: Martin Mueller, Navdeep Dahiya, Anthony Yezzi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel training model based on shape and appearance features for object segmentation in images and videos. Whereas most such models rely on two-dimensional appearance templates or a finite set of descriptors, our appearance-based feature is a one-dimensional function, which is efficiently coupled with the object's shape by integrating intensities along the object's iso-contours. Joint PCA training on these shape and appearance features further exploits shape-appearance correlations and the resulting training model is incorporated in an active-contour-type energy functional for recognition-segmentation tasks. Experiments on synthetic and infrared images demonstrate how this shape and appearance training model improves accuracy compared to methods based on the Chan-Vese energy.



### Representation, Analysis of Bayesian Refinement Approximation Network: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2103.14896v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14896v1)
- **Published**: 2021-03-27 12:55:09+00:00
- **Updated**: 2021-03-27 12:55:09+00:00
- **Authors**: Ningbo Zhu, Fei Yang
- **Comment**: 6 pages, 8 figures
- **Journal**: None
- **Summary**: After an artificial model background subtraction, the pixels have been labelled as foreground and background. Previous approaches to secondary processing the output for denoising usually use traditional methods such as the Bayesian refinement method. In this paper, we focus on using a modified U-Net model to approximate the result of the Bayesian refinement method and improve the result. In our modified U-Net model, the result of background subtraction from other models will be combined with the source image as input for learning the statistical distribution. Thus, the losing information caused by the background subtraction model can be restored from the source image. Moreover, since the part of the input image is already the output of the other background subtraction model, the feature extraction should be convenient, it only needs to change the labels of the noise pixels. Compare with traditional methods, using deep learning methods superiority in keeping details.



### SceneGraphFusion: Incremental 3D Scene Graph Prediction from RGB-D Sequences
- **Arxiv ID**: http://arxiv.org/abs/2103.14898v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14898v3)
- **Published**: 2021-03-27 13:00:36+00:00
- **Updated**: 2021-03-31 08:05:08+00:00
- **Authors**: Shun-Cheng Wu, Johanna Wald, Keisuke Tateno, Nassir Navab, Federico Tombari
- **Comment**: Proceedings IEEE Computer Vision and Pattern Recognition (CVPR), 2021
- **Journal**: None
- **Summary**: Scene graphs are a compact and explicit representation successfully used in a variety of 2D scene understanding tasks. This work proposes a method to incrementally build up semantic scene graphs from a 3D environment given a sequence of RGB-D frames. To this end, we aggregate PointNet features from primitive scene components by means of a graph neural network. We also propose a novel attention mechanism well suited for partial and missing graph data present in such an incremental reconstruction scenario. Although our proposed method is designed to run on submaps of the scene, we show it also transfers to entire 3D scenes. Experiments show that our approach outperforms 3D scene graph prediction methods by a large margin and its accuracy is on par with other 3D semantic and panoptic segmentation methods while running at 35 Hz.



### CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2103.14899v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14899v2)
- **Published**: 2021-03-27 13:03:17+00:00
- **Updated**: 2021-08-22 18:53:59+00:00
- **Authors**: Chun-Fu Chen, Quanfu Fan, Rameswar Panda
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: The recently developed vision transformer (ViT) has achieved promising results on image classification compared to convolutional neural networks. Inspired by this, in this paper, we study how to learn multi-scale feature representations in transformer models for image classification. To this end, we propose a dual-branch transformer to combine image patches (i.e., tokens in a transformer) of different sizes to produce stronger image features. Our approach processes small-patch and large-patch tokens with two separate branches of different computational complexity and these tokens are then fused purely by attention multiple times to complement each other. Furthermore, to reduce computation, we develop a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches. Our proposed cross-attention only requires linear time for both computational and memory complexity instead of quadratic time otherwise. Extensive experiments demonstrate that our approach performs better than or on par with several concurrent works on vision transformer, in addition to efficient CNN models. For example, on the ImageNet1K dataset, with some architectural changes, our approach outperforms the recent DeiT by a large margin of 2\% with a small to moderate increase in FLOPs and model parameters. Our source codes and models are available at \url{https://github.com/IBM/CrossViT}.



### Frequency-specific segregation and integration of human cerebral cortex: an intrinsic functional atlas
- **Arxiv ID**: http://arxiv.org/abs/2103.14907v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.14907v1)
- **Published**: 2021-03-27 13:26:33+00:00
- **Updated**: 2021-03-27 13:26:33+00:00
- **Authors**: Zhiguo Luo, Ling-Li Zeng, Hui Shen, Dewen Hu
- **Comment**: 43 pages, 14 figures
- **Journal**: None
- **Summary**: The frequency-specific coupling mechanism of the functional human brain networks underpins its complex cognitive and behavioral functions. Nevertheless, it is not well unveiled what are the frequency-specific subdivisions and network topologies of the human brain. In this study, we estimated functional connectivity of the human cerebral cortex using spectral connection, and conducted frequency-specific parcellation using eigen-clustering and gradient-based methods, and then explored their topological structures. 7T fMRI data of 184 subjects in the HCP dataset were used for parcellation and exploring the topological properties of the functional networks, and 3T fMRI data of another 890 subjects were used to confirm the stability of the frequency-specific topologies. Seven to ten functional networks were stably integrated by two to four dissociable hub categories at specific frequencies, and we proposed an intrinsic functional atlas containing 456 parcels according to the parcellations across frequencies. The results revealed that the functional networks contained stable frequency-specific topologies, which may imply more abundant roles of the functional units and more complex interactions among them.



### Embedding Transfer with Label Relaxation for Improved Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.14908v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14908v1)
- **Published**: 2021-03-27 13:35:03+00:00
- **Updated**: 2021-03-27 13:35:03+00:00
- **Authors**: Sungyeon Kim, Dongwon Kim, Minsu Cho, Suha Kwak
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: This paper presents a novel method for embedding transfer, a task of transferring knowledge of a learned embedding model to another. Our method exploits pairwise similarities between samples in the source embedding space as the knowledge, and transfers them through a loss used for learning target embedding models. To this end, we design a new loss called relaxed contrastive loss, which employs the pairwise similarities as relaxed labels for inter-sample relations. Our loss provides a rich supervisory signal beyond class equivalence, enables more important pairs to contribute more to training, and imposes no restriction on manifolds of target embedding spaces. Experiments on metric learning benchmarks demonstrate that our method largely improves performance, or reduces sizes and output dimensions of target models effectively. We further show that it can be also used to enhance quality of self-supervised representation and performance of classification models. In all the experiments, our method clearly outperforms existing embedding transfer techniques.



### MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2103.14910v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14910v3)
- **Published**: 2021-03-27 13:41:00+00:00
- **Updated**: 2021-07-30 06:26:32+00:00
- **Authors**: Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang, Gim Hee Lee
- **Comment**: ICCV 2021. Main paper and supplementary materials
- **Journal**: None
- **Summary**: In this paper, we propose MINE to perform novel view synthesis and depth estimation via dense 3D reconstruction from a single image. Our approach is a continuous depth generalization of the Multiplane Images (MPI) by introducing the NEural radiance fields (NeRF). Given a single image as input, MINE predicts a 4-channel image (RGB and volume density) at arbitrary depth values to jointly reconstruct the camera frustum and fill in occluded contents. The reconstructed and inpainted frustum can then be easily rendered into novel RGB or depth views using differentiable rendering. Extensive experiments on RealEstate10K, KITTI and Flowers Light Fields show that our MINE outperforms state-of-the-art by a large margin in novel view synthesis. We also achieve competitive results in depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our source code is available at https://github.com/vincentfung13/MINE



### IoU Attack: Towards Temporally Coherent Black-Box Adversarial Attack for Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2103.14938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14938v1)
- **Published**: 2021-03-27 16:20:32+00:00
- **Updated**: 2021-03-27 16:20:32+00:00
- **Authors**: Shuai Jia, Yibing Song, Chao Ma, Xiaokang Yang
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Adversarial attack arises due to the vulnerability of deep neural networks to perceive input samples injected with imperceptible perturbations. Recently, adversarial attack has been applied to visual object tracking to evaluate the robustness of deep trackers. Assuming that the model structures of deep trackers are known, a variety of white-box attack approaches to visual tracking have demonstrated promising results. However, the model knowledge about deep trackers is usually unavailable in real applications. In this paper, we propose a decision-based black-box attack method for visual object tracking. In contrast to existing black-box adversarial attack methods that deal with static images for image classification, we propose IoU attack that sequentially generates perturbations based on the predicted IoU scores from both current and historical frames. By decreasing the IoU scores, the proposed attack method degrades the accuracy of temporal coherent bounding boxes (i.e., object motions) accordingly. In addition, we transfer the learned perturbations to the next few frames to initialize temporal motion attack. We validate the proposed IoU attack on state-of-the-art deep trackers (i.e., detection based, correlation filter based, and long-term trackers). Extensive experiments on the benchmark datasets indicate the effectiveness of the proposed IoU attack method. The source code is available at https://github.com/VISION-SJTU/IoUattack.



### HDR Video Reconstruction: A Coarse-to-fine Network and A Real-world Benchmark Dataset
- **Arxiv ID**: http://arxiv.org/abs/2103.14943v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14943v2)
- **Published**: 2021-03-27 16:40:05+00:00
- **Updated**: 2021-08-21 11:17:14+00:00
- **Authors**: Guanying Chen, Chaofeng Chen, Shi Guo, Zhetong Liang, Kwan-Yee K. Wong, Lei Zhang
- **Comment**: ICCV 2021: http://guanyingc.github.io/DeepHDRVideo/
- **Journal**: None
- **Summary**: High dynamic range (HDR) video reconstruction from sequences captured with alternating exposures is a very challenging problem. Existing methods often align low dynamic range (LDR) input sequence in the image space using optical flow, and then merge the aligned images to produce HDR output. However, accurate alignment and fusion in the image space are difficult due to the missing details in the over-exposed regions and noise in the under-exposed regions, resulting in unpleasing ghosting artifacts. To enable more accurate alignment and HDR fusion, we introduce a coarse-to-fine deep learning framework for HDR video reconstruction. Firstly, we perform coarse alignment and pixel blending in the image space to estimate the coarse HDR video. Secondly, we conduct more sophisticated alignment and temporal fusion in the feature space of the coarse HDR video to produce better reconstruction. Considering the fact that there is no publicly available dataset for quantitative and comprehensive evaluation of HDR video reconstruction methods, we collect such a benchmark dataset, which contains $97$ sequences of static scenes and 184 testing pairs of dynamic scenes. Extensive experiments show that our method outperforms previous state-of-the-art methods. Our dataset, code and model will be made publicly available.



### Automated Backend-Aware Post-Training Quantization
- **Arxiv ID**: http://arxiv.org/abs/2103.14949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14949v1)
- **Published**: 2021-03-27 17:12:32+00:00
- **Updated**: 2021-03-27 17:12:32+00:00
- **Authors**: Ziheng Jiang, Animesh Jain, Andrew Liu, Josh Fromm, Chengqian Ma, Tianqi Chen, Luis Ceze
- **Comment**: None
- **Journal**: None
- **Summary**: Quantization is a key technique to reduce the resource requirement and improve the performance of neural network deployment. However, different hardware backends such as x86 CPU, NVIDIA GPU, ARM CPU, and accelerators may demand different implementations for quantized networks. This diversity calls for specialized post-training quantization pipelines to built for each hardware target, an engineering effort that is often too large for developers to keep up with. We tackle this problem with an automated post-training quantization framework called HAGO. HAGO provides a set of general quantization graph transformations based on a user-defined hardware specification and implements a search mechanism to find the optimal quantization strategy while satisfying hardware constraints for any model. We observe that HAGO achieves speedups of 2.09x, 1.97x, and 2.48x on Intel Xeon Cascade Lake CPUs, NVIDIA Tesla T4 GPUs, ARM Cortex-A CPUs on Raspberry Pi4 relative to full precision respectively, while maintaining the highest reported post-training quantization accuracy in each case.



### OLED: One-Class Learned Encoder-Decoder Network with Adversarial Context Masking for Novelty Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.14953v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.14953v3)
- **Published**: 2021-03-27 17:59:40+00:00
- **Updated**: 2021-11-10 04:14:34+00:00
- **Authors**: John Taylor Jewell, Vahid Reza Khazaie, Yalda Mohsenzadeh
- **Comment**: accepted by WACV 2022
- **Journal**: None
- **Summary**: Novelty detection is the task of recognizing samples that do not belong to the distribution of the target class. During training, the novelty class is absent, preventing the use of traditional classification approaches. Deep autoencoders have been widely used as a base of many unsupervised novelty detection methods. In particular, context autoencoders have been successful in the novelty detection task because of the more effective representations they learn by reconstructing original images from randomly masked images. However, a significant drawback of context autoencoders is that random masking fails to consistently cover important structures of the input image, leading to suboptimal representations - especially for the novelty detection task. In this paper, to optimize input masking, we have designed a framework consisting of two competing networks, a Mask Module and a Reconstructor. The Mask Module is a convolutional autoencoder that learns to generate optimal masks that cover the most important parts of images. Alternatively, the Reconstructor is a convolutional encoder-decoder that aims to reconstruct unperturbed images from masked images. The networks are trained in an adversarial manner in which the Mask Module generates masks that are applied to images given to the Reconstructor. In this way, the Mask Module seeks to maximize the reconstruction error that the Reconstructor is minimizing. When applied to novelty detection, the proposed approach learns semantically richer representations compared to context autoencoders and enhances novelty detection at test time through more optimal masking. Novelty detection experiments on the MNIST and CIFAR-10 image datasets demonstrate the proposed approach's superiority over cutting-edge methods. In a further experiment on the UCSD video dataset for novelty detection, the proposed approach achieves state-of-the-art results.



### Improving prostate whole gland segmentation in t2-weighted MRI with synthetically generated data
- **Arxiv ID**: http://arxiv.org/abs/2103.14955v1
- **DOI**: 10.1109/ISBI48211.2021.9433793
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14955v1)
- **Published**: 2021-03-27 18:04:11+00:00
- **Updated**: 2021-03-27 18:04:11+00:00
- **Authors**: Alvaro Fernandez-Quilez, Steinar Valle Larsen, Morten Goodwin, Thor Ole Gulsurd, Svein Reidar Kjosavik, Ketil Oppedal
- **Comment**: 5 pages. Accepted as a full paper at the International Symposium on
  Biomedical Imaging (ISBI) 2021
- **Journal**: None
- **Summary**: Whole gland (WG) segmentation of the prostate plays a crucial role in detection, staging and treatment planning of prostate cancer (PCa). Despite promise shown by deep learning (DL) methods, they rely on the availability of a considerable amount of annotated data. Augmentation techniques such as translation and rotation of images present an alternative to increase data availability. Nevertheless, the amount of information provided by the transformed data is limited due to the correlation between the generated data and the original. Based on the recent success of generative adversarial networks (GAN) in producing synthetic images for other domains as well as in the medical domain, we present a pipeline to generate WG segmentation masks and synthesize T2-weighted MRI of the prostate based on a publicly available multi-center dataset. Following, we use the generated data as a form of data augmentation. Results show an improvement in the quality of the WG segmentation when compared to standard augmentation techniques.



### Panoptic-PolarNet: Proposal-free LiDAR Point Cloud Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.14962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14962v1)
- **Published**: 2021-03-27 18:31:40+00:00
- **Updated**: 2021-03-27 18:31:40+00:00
- **Authors**: Zixiang Zhou, Yang Zhang, Hassan Foroosh
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Panoptic segmentation presents a new challenge in exploiting the merits of both detection and segmentation, with the aim of unifying instance segmentation and semantic segmentation in a single framework. However, an efficient solution for panoptic segmentation in the emerging domain of LiDAR point cloud is still an open research problem and is very much under-explored. In this paper, we present a fast and robust LiDAR point cloud panoptic segmentation framework, referred to as Panoptic-PolarNet. We learn both semantic segmentation and class-agnostic instance clustering in a single inference network using a polar Bird's Eye View (BEV) representation, enabling us to circumvent the issue of occlusion among instances in urban street scenes. To improve our network's learnability, we also propose an adapted instance augmentation technique and a novel adversarial point cloud pruning method. Our experiments show that Panoptic-PolarNet outperforms the baseline methods on SemanticKITTI and nuScenes datasets with an almost real-time inference speed. Panoptic-PolarNet achieved 54.1% PQ in the public SemanticKITTI panoptic segmentation leaderboard and leading performance for the validation set of nuScenes.



### Particle Filter Bridge Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2103.14963v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14963v1)
- **Published**: 2021-03-27 18:33:00+00:00
- **Updated**: 2021-03-27 18:33:00+00:00
- **Authors**: Adam Lindhe, Carl Ringqvist, Henrik Hult
- **Comment**: None
- **Journal**: None
- **Summary**: Auto encoding models have been extensively studied in recent years. They provide an efficient framework for sample generation, as well as for analysing feature learning. Furthermore, they are efficient in performing interpolations between data-points in semantically meaningful ways. In this paper, we build further on a previously introduced method for generating canonical, dimension independent, stochastic interpolations. Here, the distribution of interpolation paths is represented as the distribution of a bridge process constructed from an artificial random data generating process in the latent space, having the prior distribution as its invariant distribution. As a result the stochastic interpolation paths tend to reside in regions of the latent space where the prior has high mass. This is a desirable feature since, generally, such areas produce semantically meaningful samples. In this paper, we extend the bridge process method by introducing a discriminator network that accurately identifies areas of high latent representation density. The discriminator network is incorporated as a change of measure of the underlying bridge process and sampling of interpolation paths is implemented using sequential Monte Carlo. The resulting sampling procedure allows for greater variability in interpolation paths and stronger drift towards areas of high data density.



### Labels4Free: Unsupervised Segmentation using StyleGAN
- **Arxiv ID**: http://arxiv.org/abs/2103.14968v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14968v1)
- **Published**: 2021-03-27 18:59:22+00:00
- **Updated**: 2021-03-27 18:59:22+00:00
- **Authors**: Rameen Abdal, Peihao Zhu, Niloy Mitra, Peter Wonka
- **Comment**: "Project Page: https://rameenabdal.github.io/Labels4Free/"
- **Journal**: ICCV 2021
- **Summary**: We propose an unsupervised segmentation framework for StyleGAN generated objects. We build on two main observations. First, the features generated by StyleGAN hold valuable information that can be utilized towards training segmentation networks. Second, the foreground and background can often be treated to be largely independent and be composited in different ways. For our solution, we propose to augment the StyleGAN2 generator architecture with a segmentation branch and to split the generator into a foreground and background network. This enables us to generate soft segmentation masks for the foreground object in an unsupervised fashion. On multiple object classes, we report comparable results against state-of-the-art supervised segmentation networks, while against the best unsupervised segmentation approach we demonstrate a clear improvement, both in qualitative and quantitative metrics.



### Catalyzing Clinical Diagnostic Pipelines Through Volumetric Medical Image Segmentation Using Deep Neural Networks: Past, Present, & Future
- **Arxiv ID**: http://arxiv.org/abs/2103.14969v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14969v2)
- **Published**: 2021-03-27 19:05:11+00:00
- **Updated**: 2021-05-13 00:35:48+00:00
- **Authors**: Teofilo E. Zosa
- **Comment**: Review paper written for the UCSD PhD Research Mastery Exam; June 7,
  2019
- **Journal**: None
- **Summary**: Deep learning has made a remarkable impact in the field of natural image processing over the past decade. Consequently, there is a great deal of interest in replicating this success across unsolved tasks in related domains, such as medical image analysis. Core to medical image analysis is the task of semantic segmentation which enables various clinical workflows. Due to the challenges inherent in manual segmentation, many decades of research have been devoted to discovering extensible, automated, expert-level segmentation techniques. Given the groundbreaking performance demonstrated by recent neural network-based techniques, deep learning seems poised to achieve what classic methods have historically been unable. This paper will briefly overview some of the state-of-the-art (SoTA) neural network-based segmentation algorithms with a particular emphasis on the most recent architectures, comparing and contrasting the contributions and characteristics of each network topology. Using ultrasonography as a motivating example, it will also demonstrate important clinical implications of effective deep learning-based solutions, articulate challenges unique to the modality, and discuss novel approaches developed in response to those challenges, concluding with the proposal of future directions in the field. Given the generally observed ephemerality of the best deep learning approaches (i.e. the extremely quick succession of the SoTA), the main contributions of the paper are its contextualization of modern deep learning architectures with historical background and the elucidation of the current trajectory of volumetric medical image segmentation research.



### Going Deeper Into Face Detection: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2103.14983v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14983v2)
- **Published**: 2021-03-27 20:18:00+00:00
- **Updated**: 2021-04-13 18:50:21+00:00
- **Authors**: Shervin Minaee, Ping Luo, Zhe Lin, Kevin Bowyer
- **Comment**: None
- **Journal**: None
- **Summary**: Face detection is a crucial first step in many facial recognition and face analysis systems. Early approaches for face detection were mainly based on classifiers built on top of hand-crafted features extracted from local image regions, such as Haar Cascades and Histogram of Oriented Gradients. However, these approaches were not powerful enough to achieve a high accuracy on images of from uncontrolled environments. With the breakthrough work in image classification using deep neural networks in 2012, there has been a huge paradigm shift in face detection. Inspired by the rapid progress of deep learning in computer vision, many deep learning based frameworks have been proposed for face detection over the past few years, achieving significant improvements in accuracy. In this work, we provide a detailed overview of some of the most representative deep learning based face detection methods by grouping them into a few major categories, and present their core architectural designs and accuracies on popular benchmarks. We also describe some of the most popular face detection datasets. Finally, we discuss some current challenges in the field, and suggest potential future research directions.



### Realistic face animation generation from videos
- **Arxiv ID**: http://arxiv.org/abs/2103.14984v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.14984v1)
- **Published**: 2021-03-27 20:18:14+00:00
- **Updated**: 2021-03-27 20:18:14+00:00
- **Authors**: Zihao Jian, Minshan Xie
- **Comment**: None
- **Journal**: None
- **Summary**: 3D face reconstruction and face alignment are two fundamental and highly related topics in computer vision. Recently, some works start to use deep learning models to estimate the 3DMM coefficients to reconstruct 3D face geometry. However, the performance is restricted due to the limitation of the pre-defined face templates. To address this problem, some end-to-end methods, which can completely bypass the calculation of 3DMM coefficients, are proposed and attract much attention. In this report, we introduce and analyse three state-of-the-art methods in 3D face reconstruction and face alignment. Some potential improvement on PRN are proposed to further enhance its accuracy and speed.



### H-GAN: the power of GANs in your Hands
- **Arxiv ID**: http://arxiv.org/abs/2103.15017v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.15017v2)
- **Published**: 2021-03-27 23:46:27+00:00
- **Updated**: 2021-04-21 16:16:41+00:00
- **Authors**: Sergiu Oprea, Giorgos Karvounas, Pablo Martinez-Gonzalez, Nikolaos Kyriazis, Sergio Orts-Escolano, Iason Oikonomidis, Alberto Garcia-Garcia, Aggeliki Tsoli, Jose Garcia-Rodriguez, Antonis Argyros
- **Comment**: Paper accepted at The International Joint Conference on Neural
  Networks (IJCNN) 2021
- **Journal**: None
- **Summary**: We present HandGAN (H-GAN), a cycle-consistent adversarial learning approach implementing multi-scale perceptual discriminators. It is designed to translate synthetic images of hands to the real domain. Synthetic hands provide complete ground-truth annotations, yet they are not representative of the target distribution of real-world data. We strive to provide the perfect blend of a realistic hand appearance with synthetic annotations. Relying on image-to-image translation, we improve the appearance of synthetic hands to approximate the statistical distribution underlying a collection of real images of hands. H-GAN tackles not only the cross-domain tone mapping but also structural differences in localized areas such as shading discontinuities. Results are evaluated on a qualitative and quantitative basis improving previous works. Furthermore, we relied on the hand classification task to claim our generated hands are statistically similar to the real domain of hands.



