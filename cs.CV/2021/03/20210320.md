# Arxiv Papers in cs.CV on 2021-03-20
### Automatic Quantification of Facial Asymmetry using Facial Landmarks
- **Arxiv ID**: http://arxiv.org/abs/2103.11059v1
- **DOI**: 10.1109/WNYIPW.2019.8923078
- **Categories**: **cs.CV**, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2103.11059v1)
- **Published**: 2021-03-20 00:08:37+00:00
- **Updated**: 2021-03-20 00:08:37+00:00
- **Authors**: Abu Md Niamul Taufique, Andreas Savakis, Jonathan Leckenby
- **Comment**: 5 pages, 4 figures
- **Journal**: 2019 IEEE Western New York Image and Signal Processing Workshop
  (WNYISPW)
- **Summary**: One-sided facial paralysis causes uneven movements of facial muscles on the sides of the face. Physicians currently assess facial asymmetry in a subjective manner based on their clinical experience. This paper proposes a novel method to provide an objective and quantitative asymmetry score for frontal faces. Our metric has the potential to help physicians for diagnosis as well as monitoring the rehabilitation of patients with one-sided facial paralysis. A deep learning based landmark detection technique is used to estimate style invariant facial landmark points and dense optical flow is used to generate motion maps from a short sequence of frames. Six face regions are considered corresponding to the left and right parts of the forehead, eyes, and mouth. Motion is computed and compared between the left and the right parts of each region of interest to estimate the symmetry score. For testing, asymmetric sequences are synthetically generated from a facial expression dataset. A score equation is developed to quantify symmetry in both symmetric and asymmetric face sequences.



### Visualization of Deep Transfer Learning In SAR Imagery
- **Arxiv ID**: http://arxiv.org/abs/2103.11061v1
- **DOI**: 10.1109/IGARSS39084.2020.9324490
- **Categories**: **cs.CV**, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2103.11061v1)
- **Published**: 2021-03-20 00:16:15+00:00
- **Updated**: 2021-03-20 00:16:15+00:00
- **Authors**: Abu Md Niamul Taufique, Navya Nagananda, Andreas Savakis
- **Comment**: 4 pages, 5 figures
- **Journal**: IGARSS 2020 - 2020 IEEE International Geoscience and Remote
  Sensing Symposium
- **Summary**: Synthetic Aperture Radar (SAR) imagery has diverse applications in land and marine surveillance. Unlike electro-optical (EO) systems, these systems are not affected by weather conditions and can be used in the day and night times. With the growing importance of SAR imagery, it would be desirable if models trained on widely available EO datasets can also be used for SAR images. In this work, we consider transfer learning to leverage deep features from a network trained on an EO ships dataset and generate predictions on SAR imagery. Furthermore, by exploring the network activations in the form of class-activation maps (CAMs), we visualize the transfer learning process to SAR imagery and gain insight on how a deep network interprets a new modality.



### Stereo CenterNet based 3D Object Detection for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2103.11071v3
- **DOI**: 10.1016/j.neucom.2021.11.048
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11071v3)
- **Published**: 2021-03-20 02:18:49+00:00
- **Updated**: 2021-09-23 08:50:58+00:00
- **Authors**: Yuguang Shi, Yu Guo, Zhenqiang Mi, Xinjie Li
- **Comment**: None
- **Journal**: Published by Neurocomputing,Volume 471, 30 January 2022, Pages
  219-229
- **Summary**: Recently, three-dimensional (3D) detection based on stereo images has progressed remarkably; however, most advanced methods adopt anchor-based two-dimensional (2D) detection or depth estimation to address this problem. Nevertheless, high computational cost inhibits these methods from achieving real-time performance. In this study, we propose a 3D object detection method, Stereo CenterNet (SC), using geometric information in stereo imagery. SC predicts the four semantic key points of the 3D bounding box of the object in space and utilizes 2D left and right boxes, 3D dimension, orientation, and key points to restore the bounding box of the object in the 3D space. Subsequently, we adopt an improved photometric alignment module to further optimize the position of the 3D bounding box. Experiments conducted on the KITTI dataset indicate that the proposed SC exhibits the best speed-accuracy trade-off among advanced methods without using extra data.



### AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2103.11078v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11078v3)
- **Published**: 2021-03-20 02:58:13+00:00
- **Updated**: 2021-08-19 02:58:41+00:00
- **Authors**: Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun Bao, Juyong Zhang
- **Comment**: Project: https://yudongguo.github.io/ADNeRF/ Code:
  https://github.com/YudongGuo/AD-NeRF
- **Journal**: None
- **Summary**: Generating high-fidelity talking head video by fitting with the input audio sequence is a challenging problem that receives considerable attentions recently. In this paper, we address this problem with the aid of neural scene representation networks. Our method is completely different from existing methods that rely on intermediate representations like 2D landmarks or 3D face models to bridge the gap between audio input and video output. Specifically, the feature of input audio signal is directly fed into a conditional implicit function to generate a dynamic neural radiance field, from which a high-fidelity talking-head video corresponding to the audio signal is synthesized using volume rendering. Another advantage of our framework is that not only the head (with hair) region is synthesized as previous methods did, but also the upper body is generated via two individual neural radiance fields. Experimental results demonstrate that our novel framework can (1) produce high-fidelity and natural results, and (2) support free adjustment of audio signals, viewing directions, and background images. Code is available at https://github.com/YudongGuo/AD-NeRF.



### 3DMNDT:3D multi-view registration method based on the normal distributions transform
- **Arxiv ID**: http://arxiv.org/abs/2103.11084v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.11084v1)
- **Published**: 2021-03-20 03:20:31+00:00
- **Updated**: 2021-03-20 03:20:31+00:00
- **Authors**: Jihua Zhu, Di Wang, Jiaxi Mu, Huimin Lu, Zhiqiang Tian, Zhongyu Li
- **Comment**: None
- **Journal**: None
- **Summary**: The normal distributions transform (NDT) is an effective paradigm for the point set registration. This method is originally designed for pair-wise registration and it will suffer from great challenges when applied to multi-view registration. Under the NDT framework, this paper proposes a novel multi-view registration method, named 3D multi-view registration based on the normal distributions transform (3DMNDT), which integrates the K-means clustering and Lie algebra solver to achieve multi-view registration. More specifically, the multi-view registration is cast into the problem of maximum likelihood estimation. Then, the K-means algorithm is utilized to divide all data points into different clusters, where a normal distribution is computed to locally models the probability of measuring a data point in each cluster. Subsequently, the registration problem is formulated by the NDT-based likelihood function. To maximize this likelihood function, the Lie algebra solver is developed to sequentially optimize each rigid transformation. The proposed method alternately implements data point clustering, NDT computing, and likelihood maximization until desired registration results are obtained. Experimental results tested on benchmark data sets illustrate that the proposed method can achieve state-of-the-art performance for multi-view registration.



### Pathological Image Segmentation with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2104.02602v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.02602v1)
- **Published**: 2021-03-20 03:36:06+00:00
- **Updated**: 2021-03-20 03:36:06+00:00
- **Authors**: Li Xiao, Yinhao Li, Luxi Qv, Xinxia Tian, Yijie Peng, S. Kevin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation of pathological images is essential for accurate disease diagnosis. The quality of manual labels plays a critical role in segmentation accuracy; yet, in practice, the labels between pathologists could be inconsistent, thus confusing the training process. In this work, we propose a novel label re-weighting framework to account for the reliability of different experts' labels on each pixel according to its surrounding features. We further devise a new attention heatmap, which takes roughness as prior knowledge to guide the model to focus on important regions. Our approach is evaluated on the public Gleason 2019 datasets. The results show that our approach effectively improves the model's robustness against noisy labels and outperforms state-of-the-art approaches.



### Exploring The Effect of High-frequency Components in GANs Training
- **Arxiv ID**: http://arxiv.org/abs/2103.11093v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11093v2)
- **Published**: 2021-03-20 04:37:06+00:00
- **Updated**: 2021-11-30 03:14:47+00:00
- **Authors**: Ziqiang Li, Pengfei Xia, Xue Rui, Bin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have the ability to generate images that are visually indistinguishable from real images. However, recent studies have revealed that generated and real images share significant differences in the frequency domain. In this paper, we explore the effect of high-frequency components in GANs training. According to our observation, during the training of most GANs, severe high-frequency differences make the discriminator focus on high-frequency components excessively, which hinders the generator from fitting the low-frequency components that are important for learning images' content. Then, we propose two simple yet effective frequency operations for eliminating the side effects caused by high-frequency differences in GANs training: High-Frequency Confusion (HFC) and High-Frequency Filter (HFF). The proposed operations are general and can be applied to most existing GANs with a fraction of the cost. The advanced performance of the proposed operations is verified on multiple loss functions, network architectures, and datasets. Specifically, the proposed HFF achieves significant improvements of $42.5\%$ FID on CelebA (128*128) unconditional generation based on SNGAN, $30.2\%$ FID on CelebA unconditional generation based on SSGAN, and $69.3\%$ FID on CelebA unconditional generation based on InfoMAXGAN.



### Local Patch AutoAugment with Multi-Agent Collaboration
- **Arxiv ID**: http://arxiv.org/abs/2103.11099v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11099v2)
- **Published**: 2021-03-20 05:10:05+00:00
- **Updated**: 2021-10-18 08:52:45+00:00
- **Authors**: Shiqi Lin, Tao Yu, Ruoyu Feng, Xin Li, Xin Jin, Zhibo Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation (DA) plays a critical role in improving the generalization of deep learning models. Recent works on automatically searching for DA policies from data have achieved great success. However, existing automated DA methods generally perform the search at the image level, which limits the exploration of diversity in local regions. In this paper, we propose a more fine-grained automated DA approach, dubbed Patch AutoAugment, to divide an image into a grid of patches and search for the joint optimal augmentation policies for the patches. We formulate it as a multi-agent reinforcement learning (MARL) problem, where each agent learns an augmentation policy for each patch based on its content together with the semantics of the whole image. The agents cooperate with each other to achieve the optimal augmentation effect of the entire image by sharing a team reward. We show the effectiveness of our method on multiple benchmark datasets of image classification and fine-grained image recognition (e.g., CIFAR-10, CIFAR-100, ImageNet, CUB-200-2011, Stanford Cars and FGVC-Aircraft). Extensive experiments demonstrate that our method outperforms the state-of-the-art DA methods while requiring fewer computational resources.



### A Novel Upsampling and Context Convolution for Image Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.11110v1
- **DOI**: 10.3390/s21062170
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11110v1)
- **Published**: 2021-03-20 06:16:42+00:00
- **Updated**: 2021-03-20 06:16:42+00:00
- **Authors**: Khwaja Monib Sediqi, Hyo Jong Lee
- **Comment**: 11 pages, published in sensors journal
- **Journal**: Sensors 2021, 21, 2170
- **Summary**: Semantic segmentation, which refers to pixel-wise classification of an image, is a fundamental topic in computer vision owing to its growing importance in robot vision and autonomous driving industries. It provides rich information about objects in the scene such as object boundary, category, and location. Recent methods for semantic segmentation often employ an encoder-decoder structure using deep convolutional neural networks. The encoder part extracts feature of the image using several filters and pooling operations, whereas the decoder part gradually recovers the low-resolution feature maps of the encoder into a full input resolution feature map for pixel-wise prediction. However, the encoder-decoder variants for semantic segmentation suffer from severe spatial information loss, caused by pooling operations or convolutions with stride, and does not consider the context in the scene. In this paper, we propose a dense upsampling convolution method based on guided filtering to effectively preserve the spatial information of the image in the network. We further propose a novel local context convolution method that not only covers larger-scale objects in the scene but covers them densely for precise object boundary delineation. Theoretical analyses and experimental results on several benchmark datasets verify the effectiveness of our method. Qualitatively, our approach delineates object boundaries at a level of accuracy that is beyond the current excellent methods. Quantitatively, we report a new record of 82.86% and 81.62% of pixel accuracy on ADE20K and Pascal-Context benchmark datasets, respectively. In comparison with the state-of-the-art methods, the proposed method offers promising improvements.



### Classifier Crafting: Turn Your ConvNet into a Zero-Shot Learner!
- **Arxiv ID**: http://arxiv.org/abs/2103.11112v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11112v1)
- **Published**: 2021-03-20 06:26:29+00:00
- **Updated**: 2021-03-20 06:26:29+00:00
- **Authors**: Jacopo Cavazza
- **Comment**: 8 pages (excluding references), 9 figures
- **Journal**: None
- **Summary**: In Zero-shot learning (ZSL), we classify unseen categories using textual descriptions about their expected appearance when observed (class embeddings) and a disjoint pool of seen classes, for which annotated visual data are accessible. We tackle ZSL by casting a "vanilla" convolutional neural network (e.g. AlexNet, ResNet-101, DenseNet-201 or DarkNet-53) into a zero-shot learner. We do so by crafting the softmax classifier: we freeze its weights using fixed seen classification rules, either semantic (seen class embeddings) or visual (seen class prototypes). Then, we learn a data-driven and ZSL-tailored feature representation on seen classes only to match these fixed classification rules. Given that the latter seamlessly generalize towards unseen classes, while requiring not actual unseen data to be computed, we can perform ZSL inference by augmenting the pool of classification rules at test time while keeping the very same representation we learnt: nowhere re-training or fine-tuning on unseen data is performed. The combination of semantic and visual crafting (by simply averaging softmax scores) improves prior state-of-the-art methods in benchmark datasets for standard, inductive ZSL. After rebalancing predictions to better handle the joint inference over seen and unseen classes, we outperform prior generalized, inductive ZSL methods as well. Also, we gain interpretability at no additional cost, by using neural attention methods (e.g., grad-CAM) as they are. Code will be made publicly available.



### A novel multimodal fusion network based on a joint coding model for lane line segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.11114v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11114v1)
- **Published**: 2021-03-20 06:47:58+00:00
- **Updated**: 2021-03-20 06:47:58+00:00
- **Authors**: Zhenhong Zou, Xinyu Zhang, Huaping Liu, Zhiwei Li, Amir Hussain, Jun Li
- **Comment**: None
- **Journal**: None
- **Summary**: There has recently been growing interest in utilizing multimodal sensors to achieve robust lane line segmentation. In this paper, we introduce a novel multimodal fusion architecture from an information theory perspective, and demonstrate its practical utility using Light Detection and Ranging (LiDAR) camera fusion networks. In particular, we develop, for the first time, a multimodal fusion network as a joint coding model, where each single node, layer, and pipeline is represented as a channel. The forward propagation is thus equal to the information transmission in the channels. Then, we can qualitatively and quantitatively analyze the effect of different fusion approaches. We argue the optimal fusion architecture is related to the essential capacity and its allocation based on the source and channel. To test this multimodal fusion hypothesis, we progressively determine a series of multimodal models based on the proposed fusion methods and evaluate them on the KITTI and the A2D2 datasets. Our optimal fusion network achieves 85%+ lane line accuracy and 98.7%+ overall. The performance gap among the models will inform continuing future research into development of optimal fusion algorithms for the deep multimodal learning community.



### Adaptive Feature Fusion Network for Gaze Tracking in Mobile Tablets
- **Arxiv ID**: http://arxiv.org/abs/2103.11119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11119v1)
- **Published**: 2021-03-20 07:16:10+00:00
- **Updated**: 2021-03-20 07:16:10+00:00
- **Authors**: Yiwei Bao, Yihua Cheng, Yunfei Liu, Feng Lu
- **Comment**: Accepted at International Conference on Pattern Recognition 2020
  (ICPR)
- **Journal**: None
- **Summary**: Recently, many multi-stream gaze estimation methods have been proposed. They estimate gaze from eye and face appearances and achieve reasonable accuracy. However, most of the methods simply concatenate the features extracted from eye and face appearance. The feature fusion process has been ignored. In this paper, we propose a novel Adaptive Feature Fusion Network (AFF-Net), which performs gaze tracking task in mobile tablets. We stack two-eye feature maps and utilize Squeeze-and-Excitation layers to adaptively fuse two-eye features according to their similarity on appearance. Meanwhile, we also propose Adaptive Group Normalization to recalibrate eye features with the guidance of facial feature. Extensive experiments on both GazeCapture and MPIIFaceGaze datasets demonstrate consistently superior performance of the proposed method.



### MetaHDR: Model-Agnostic Meta-Learning for HDR Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2103.12545v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.12545v1)
- **Published**: 2021-03-20 07:56:45+00:00
- **Updated**: 2021-03-20 07:56:45+00:00
- **Authors**: Edwin Pan, Anthony Vento
- **Comment**: 7 pages, 6 figures, 2 tables
- **Journal**: None
- **Summary**: Capturing scenes with a high dynamic range is crucial to reproducing images that appear similar to those seen by the human visual system. Despite progress in developing data-driven deep learning approaches for converting low dynamic range images to high dynamic range images, existing approaches are limited by the assumption that all conversions are governed by the same nonlinear mapping. To address this problem, we propose "Model-Agnostic Meta-Learning for HDR Image Reconstruction" (MetaHDR), which applies meta-learning to the LDR-to-HDR conversion problem using existing HDR datasets. Our key novelty is the reinterpretation of LDR-to-HDR conversion scenes as independently sampled tasks from a common LDR-to-HDR conversion task distribution. Naturally, we use a meta-learning framework that learns a set of meta-parameters which capture the common structure consistent across all LDR-to-HDR conversion tasks. Finally, we perform experimentation with MetaHDR to demonstrate its capacity to tackle challenging LDR-to-HDR image conversions. Code and pretrained models are available at https://github.com/edwin-pan/MetaHDR.



### High Resolution Face Editing with Masked GAN Latent Code Optimization
- **Arxiv ID**: http://arxiv.org/abs/2103.11135v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11135v3)
- **Published**: 2021-03-20 08:39:41+00:00
- **Updated**: 2023-02-06 16:34:27+00:00
- **Authors**: Martin Pernuš, Vitomir Štruc, Simon Dobrišek
- **Comment**: Final ArXiv version. The paper has been accepted for publication in
  IEEE Transactions on Image Processing journal and will be published in 2023
- **Journal**: None
- **Summary**: Face editing represents a popular research topic within the computer vision and image processing communities. While significant progress has been made recently in this area, existing solutions: (i) are still largely focused on low-resolution images, (ii) often generate editing results with visual artefacts, or (iii) lack fine-grained control and alter multiple (entangled) attributes at once, when trying to generate the desired facial semantics. In this paper, we aim to address these issues though a novel attribute editing approach called MaskFaceGAN that focuses on local attribute editing. The proposed approach is based on an optimization procedure that directly optimizes the latent code of a pre-trained (state-of-the-art) Generative Adversarial Network (i.e., StyleGAN2) with respect to several constraints that ensure: (i) preservation of relevant image content, (ii) generation of the targeted facial attributes, and (iii) spatially--selective treatment of local image areas. The constraints are enforced with the help of an (differentiable) attribute classifier and face parser that provide the necessary reference information for the optimization procedure. MaskFaceGAN is evaluated in extensive experiments on the CelebA-HQ, Helen and SiblingsDB-HQf datasets and in comparison with several state-of-the-art techniques from the literature, i.e., StarGAN, AttGAN, STGAN, and two versions of InterFaceGAN. Our experimental results show that the proposed approach is able to edit face images with respect to several local facial attributes with unprecedented image quality and at high-resolutions (1024x1024), while exhibiting considerably less problems with attribute entanglement than competing solutions. The source code is made freely available from: https://github.com/MartinPernus/MaskFaceGAN.



### MogFace: Towards a Deeper Appreciation on Face Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.11139v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.11139v5)
- **Published**: 2021-03-20 09:17:04+00:00
- **Updated**: 2022-03-29 07:00:26+00:00
- **Authors**: Yang Liu, Fei Wang, Jiankang Deng, Zhipeng Zhou, Baigui Sun, Hao Li
- **Comment**: None
- **Journal**: None
- **Summary**: Benefiting from the pioneering design of generic object detectors, significant achievements have been made in the field of face detection. Typically, the architectures of the backbone, feature pyramid layer, and detection head module within the face detector all assimilate the excellent experience from general object detectors. However, several effective methods, including label assignment and scale-level data augmentation strategy, fail to maintain consistent superiority when applying on the face detector directly. Concretely, the former strategy involves a vast body of hyper-parameters and the latter one suffers from the challenge of scale distribution bias between different detection tasks, which both limit their generalization abilities. Furthermore, in order to provide accurate face bounding boxes for facial down-stream tasks, the face detector imperatively requires the elimination of false alarms. As a result, practical solutions on label assignment, scale-level data augmentation, and reducing false alarms are necessary for advancing face detectors. In this paper, we focus on resolving three aforementioned challenges that exiting methods are difficult to finish off and present a novel face detector, termed MogFace. In our Mogface, three key components, Adaptive Online Incremental Anchor Mining Strategy, Selective Scale Enhancement Strategy and Hierarchical Context-Aware Module, are separately proposed to boost the performance of face detectors. Finally, to the best of our knowledge, our MogFace is the best face detector on the Wider Face leader-board, achieving all champions across different testing scenarios. The code is available at \url{https://github.com/damo-cv/MogFace}.



### Overprotective Training Environments Fall Short at Testing Time: Let Models Contribute to Their Own Training
- **Arxiv ID**: http://arxiv.org/abs/2103.11145v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.11145v2)
- **Published**: 2021-03-20 09:55:50+00:00
- **Updated**: 2021-03-30 19:16:52+00:00
- **Authors**: Alberto Testoni, Raffaella Bernardi
- **Comment**: This paper has been published in the Proceedings of the Seventh
  Italian Conference on Computational Linguistics, CLiC-it 2020
- **Journal**: None
- **Summary**: Despite important progress, conversational systems often generate dialogues that sound unnatural to humans. We conjecture that the reason lies in their different training and testing conditions: agents are trained in a controlled "lab" setting but tested in the "wild". During training, they learn to generate an utterance given the human dialogue history. On the other hand, during testing, they must interact with each other, and hence deal with noisy data. We propose to fill this gap by training the model with mixed batches containing both samples of human and machine-generated dialogues. We assess the validity of the proposed method on GuessWhat?!, a visual referential game.



### MonteFloor: Extending MCTS for Reconstructing Accurate Large-Scale Floor Plans
- **Arxiv ID**: http://arxiv.org/abs/2103.11161v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.11161v2)
- **Published**: 2021-03-20 11:36:49+00:00
- **Updated**: 2021-09-13 12:35:02+00:00
- **Authors**: Sinisa Stekovic, Mahdi Rad, Friedrich Fraundorfer, Vincent Lepetit
- **Comment**: Accepted for oral presentation at ICCV 2021
- **Journal**: None
- **Summary**: We propose a novel method for reconstructing floor plans from noisy 3D point clouds. Our main contribution is a principled approach that relies on the Monte Carlo Tree Search (MCTS) algorithm to maximize a suitable objective function efficiently despite the complexity of the problem. Like previous work, we first project the input point cloud to a top view to create a density map and extract room proposals from it. Our method selects and optimizes the polygonal shapes of these room proposals jointly to fit the density map and outputs an accurate vectorized floor map even for large complex scenes. To do this, we adapted MCTS, an algorithm originally designed to learn to play games, to select the room proposals by maximizing an objective function combining the fitness with the density map as predicted by a deep network and regularizing terms on the room shapes. We also introduce a refinement step to MCTS that adjusts the shape of the room proposals. For this step, we propose a novel differentiable method for rendering the polygonal shapes of these proposals. We evaluate our method on the recent and challenging Structured3D and Floor-SP datasets and show a significant improvement over the state-of-the-art, without imposing any hard constraints nor assumptions on the floor plan configurations.



### Efficient Subsampling of Realistic Images From GANs Conditional on a Class or a Continuous Variable
- **Arxiv ID**: http://arxiv.org/abs/2103.11166v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2103.11166v5)
- **Published**: 2021-03-20 12:19:18+00:00
- **Updated**: 2022-04-20 15:36:36+00:00
- **Authors**: Xin Ding, Yongwei Wang, Z. Jane Wang, William J. Welch
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, subsampling or refining images generated from unconditional GANs has been actively studied to improve the overall image quality. Unfortunately, these methods are often observed less effective or inefficient in handling conditional GANs (cGANs) -- conditioning on a class (aka class-conditional GANs) or a continuous variable (aka continuous cGANs or CcGANs). In this work, we introduce an effective and efficient subsampling scheme, named conditional density ratio-guided rejection sampling (cDR-RS), to sample high-quality images from cGANs. Specifically, we first develop a novel conditional density ratio estimation method, termed cDRE-F-cSP, by proposing the conditional Softplus (cSP) loss and an improved feature extraction mechanism. We then derive the error bound of a density ratio model trained with the cSP loss. Finally, we accept or reject a fake image in terms of its estimated conditional density ratio. A filtering scheme is also developed to increase fake images' label consistency without losing diversity when sampling from CcGANs. We extensively test the effectiveness and efficiency of cDR-RS in sampling from both class-conditional GANs and CcGANs on five benchmark datasets. When sampling from class-conditional GANs, cDR-RS outperforms modern state-of-the-art methods by a large margin (except DRE-F-SP+RS) in terms of effectiveness. Although the effectiveness of cDR-RS is often comparable to that of DRE-F-SP+RS, cDR-RS is substantially more efficient. When sampling from CcGANs, the superiority of cDR-RS is even more noticeable in terms of both effectiveness and efficiency. Notably, with the consumption of reasonable computational resources, cDR-RS can substantially reduce Label Score without decreasing the diversity of CcGAN-generated images, while other methods often need to trade much diversity for slightly improved Label Score.



### Your Classifier can Secretly Suffice Multi-Source Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2103.11169v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.11169v1)
- **Published**: 2021-03-20 12:44:13+00:00
- **Updated**: 2021-03-20 12:44:13+00:00
- **Authors**: Naveen Venkat, Jogendra Nath Kundu, Durgesh Kumar Singh, Ambareesh Revanur, R. Venkatesh Babu
- **Comment**: NeurIPS 2020. Project page: https://sites.google.com/view/simpal
- **Journal**: None
- **Summary**: Multi-Source Domain Adaptation (MSDA) deals with the transfer of task knowledge from multiple labeled source domains to an unlabeled target domain, under a domain-shift. Existing methods aim to minimize this domain-shift using auxiliary distribution alignment objectives. In this work, we present a different perspective to MSDA wherein deep models are observed to implicitly align the domains under label supervision. Thus, we aim to utilize implicit alignment without additional training objectives to perform adaptation. To this end, we use pseudo-labeled target samples and enforce a classifier agreement on the pseudo-labels, a process called Self-supervised Implicit Alignment (SImpAl). We find that SImpAl readily works even under category-shift among the source domains. Further, we propose classifier agreement as a cue to determine the training convergence, resulting in a simple training algorithm. We provide a thorough evaluation of our approach on five benchmarks, along with detailed insights into each component of our approach.



### 3M: Multi-style image caption generation using Multi-modality features under Multi-UPDOWN model
- **Arxiv ID**: http://arxiv.org/abs/2103.11186v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11186v1)
- **Published**: 2021-03-20 14:12:13+00:00
- **Updated**: 2021-03-20 14:12:13+00:00
- **Authors**: Chengxi Li, Brent Harrison
- **Comment**: To be published at FLAIRS-34
- **Journal**: None
- **Summary**: In this paper, we build a multi-style generative model for stylish image captioning which uses multi-modality image features, ResNeXt features and text features generated by DenseCap. We propose the 3M model, a Multi-UPDOWN caption model that encodes multi-modality features and decode them to captions. We demonstrate the effectiveness of our model on generating human-like captions by examining its performance on two datasets, the PERSONALITY-CAPTIONS dataset and the FlickrStyle10K dataset. We compare against a variety of state-of-the-art baselines on various automatic NLP metrics such as BLEU, ROUGE-L, CIDEr, SPICE, etc. A qualitative study has also been done to verify our 3M model can be used for generating different stylized captions.



### Efficient Spatialtemporal Context Modeling for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.11190v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.11190v2)
- **Published**: 2021-03-20 14:48:12+00:00
- **Updated**: 2021-04-06 04:40:12+00:00
- **Authors**: Congqi Cao, Yue Lu, Yifan Zhang, Dongmei Jiang, Yanning Zhang
- **Comment**: 16 pages, 7 figures
- **Journal**: None
- **Summary**: Contextual information plays an important role in action recognition. Local operations have difficulty to model the relation between two elements with a long-distance interval. However, directly modeling the contextual information between any two points brings huge cost in computation and memory, especially for action recognition, where there is an additional temporal dimension. Inspired from 2D criss-cross attention used in segmentation task, we propose a recurrent 3D criss-cross attention (RCCA-3D) module to model the dense long-range spatiotemporal contextual information in video for action recognition. The global context is factorized into sparse relation maps. We model the relationship between points in the same line along the direction of horizon, vertical and depth at each time, which forms a 3D criss-cross structure, and duplicate the same operation with recurrent mechanism to transmit the relation between points in a line to a plane finally to the whole spatiotemporal space. Compared with the non-local method, the proposed RCCA-3D module reduces the number of parameters and FLOPs by 25% and 30% for video context modeling. We evaluate the performance of RCCA-3D with two latest action recognition networks on three datasets and make a thorough analysis of the architecture, obtaining the optimal way to factorize and fuse the relation maps. Comparisons with other state-of-the-art methods demonstrate the effectiveness and efficiency of our model.



### Self-Supervised Steering Angle Prediction for Vehicle Control Using Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/2103.11204v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11204v1)
- **Published**: 2021-03-20 16:29:01+00:00
- **Updated**: 2021-03-20 16:29:01+00:00
- **Authors**: Qadeer Khan, Patrick Wenzel, Daniel Cremers
- **Comment**: Accepted at International Conference on Artificial Intelligence and
  Statistics (AISTATS), 2021
- **Journal**: None
- **Summary**: Vision-based learning methods for self-driving cars have primarily used supervised approaches that require a large number of labels for training. However, those labels are usually difficult and expensive to obtain. In this paper, we demonstrate how a model can be trained to control a vehicle's trajectory using camera poses estimated through visual odometry methods in an entirely self-supervised fashion. We propose a scalable framework that leverages trajectory information from several different runs using a camera setup placed at the front of a car. Experimental results on the CARLA simulator demonstrate that our proposed approach performs at par with the model trained with supervision.



### Efficient Global Optimization of Non-differentiable, Symmetric Objectives for Multi Camera Placement
- **Arxiv ID**: http://arxiv.org/abs/2103.11210v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MA, math.OC, 90-05, 90C26, 90C30, 90C56, 90C59, I.4.8; I.6; I.2.10; I.2.11; I.2.8; G.1.6; G.1.1
- **Links**: [PDF](http://arxiv.org/pdf/2103.11210v1)
- **Published**: 2021-03-20 17:01:15+00:00
- **Updated**: 2021-03-20 17:01:15+00:00
- **Authors**: Maria L. Hänel, Carola-B. Schönlieb
- **Comment**: Submitted to be reviewed, 10 pages, 6 figures, 2 tables, 3 algorithms
- **Journal**: None
- **Summary**: We propose a novel iterative method for optimally placing and orienting multiple cameras in a 3D scene. Sample applications include improving the accuracy of 3D reconstruction, maximizing the covered area for surveillance, or improving the coverage in multi-viewpoint pedestrian tracking. Our algorithm is based on a block-coordinate ascent combined with a surrogate function and an exclusion area technique. This allows to flexibly handle difficult objective functions that are often expensive and quantized or non-differentiable. The solver is globally convergent and easily parallelizable. We show how to accelerate the optimization by exploiting special properties of the objective function, such as symmetry. Additionally, we discuss the trade-off between non-optimal stationary points and the cost reduction when optimizing the viewpoints consecutively.



### Multi Camera Placement via Z-buffer Rendering for the Optimization of the Coverage and the Visual Hull
- **Arxiv ID**: http://arxiv.org/abs/2103.11211v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, math.OC, 65D19, 65D18, 68T45, 68T42, 90-05, 90C26, 90C30, 90C56, I.4.8; I.6.4; I.2.10; I.2.11; I.2.8; G.1.6; B.8.2; J.7
- **Links**: [PDF](http://arxiv.org/pdf/2103.11211v1)
- **Published**: 2021-03-20 17:04:00+00:00
- **Updated**: 2021-03-20 17:04:00+00:00
- **Authors**: Maria L. Hänel, Johannes Völkel, Dominik Henrich
- **Comment**: 8 pages, 9 figures, not reviewed yet
- **Journal**: None
- **Summary**: We can only allow human-robot-cooperation in a common work cell if the human integrity is guaranteed. A surveillance system with multiple cameras can detect collisions without contact to the human collaborator. A failure safe system needs to optimally cover the important areas of the robot work cell with safety overlap. We propose an efficient algorithm for optimally placing and orienting the cameras in a 3D CAD model of the work cell. In order to evaluate the quality of the camera constellation in each step, our method simulates the vision system using a z-buffer rendering technique for image acquisition, a voxel space for the overlap and a refined visual hull method for a conservative human reconstruction. The simulation allows to evaluate the quality with respect to the distortion of images and advanced image analysis in the presence of static and dynamic visual obstacles such as tables, racks, walls, robots and people. Our method is ideally suited for maximizing the coverage of multiple cameras or minimizing an error made by the visual hull and can be extended to probabilistic space carving.



### Preprocessing power weighted shortest path data using a s-Well Separated Pair Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2103.11216v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.DS, cs.LG, 68T10, 62H30, 52B05, 60F15, 94A08
- **Links**: [PDF](http://arxiv.org/pdf/2103.11216v2)
- **Published**: 2021-03-20 17:38:13+00:00
- **Updated**: 2021-05-16 20:27:09+00:00
- **Authors**: Gurpreet S. Kalsi, Steven B. Damelin
- **Comment**: None
- **Journal**: None
- **Summary**: For $s$ $>$ 0, we consider an algorithm that computes all $s$-well separated pairs in certain point sets in $\mathbb{R}^{n}$, $n$ $>1$. For an integer $K$ $>1$, we also consider an algorithm that is a permutation of Dijkstra's algorithm, that computes $K$-nearest neighbors using a certain power weighted shortest path metric in $\mathbb{R}^{n}$, $n$ $>$ $1$. We describe each algorithm and their respective dependencies on the input data. We introduce a way to combine both algorithms into a fused algorithm. Several open problems are given for future research.



### Artificial intelligence for detection and quantification of rust and leaf miner in coffee crop
- **Arxiv ID**: http://arxiv.org/abs/2103.11241v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.2; I.4; I.5; K.6.3
- **Links**: [PDF](http://arxiv.org/pdf/2103.11241v2)
- **Published**: 2021-03-20 20:52:11+00:00
- **Updated**: 2021-04-01 22:41:10+00:00
- **Authors**: Alvaro Leandro Cavalcante Carneiro, Lucas de Brito Silva, Marisa Silveira Almeida Renaud Faulin
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Pest and disease control plays a key role in agriculture since the damage caused by these agents are responsible for a huge economic loss every year. Based on this assumption, we create an algorithm capable of detecting rust (Hemileia vastatrix) and leaf miner (Leucoptera coffeella) in coffee leaves (Coffea arabica) and quantify disease severity using a mobile application as a high-level interface for the model inferences. We used different convolutional neural network architectures to create the object detector, besides the OpenCV library, k-means, and three treatments: the RGB and value to quantification, and the AFSoft software, in addition to the analysis of variance, where we compare the three methods. The results show an average precision of 81,5% in the detection and that there was no significant statistical difference between treatments to quantify the severity of coffee leaves, proposing a computationally less costly method. The application, together with the trained model, can detect the pest and disease over different image conditions and infection stages and also estimate the disease infection stage.



### Paying Attention to Multiscale Feature Maps in Multimodal Image Matching
- **Arxiv ID**: http://arxiv.org/abs/2103.11247v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.11247v1)
- **Published**: 2021-03-20 21:14:24+00:00
- **Updated**: 2021-03-20 21:14:24+00:00
- **Authors**: Aviad Moreshet, Yosi Keller
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an attention-based approach for multimodal image patch matching using a Transformer encoder attending to the feature maps of a multiscale Siamese CNN. Our encoder is shown to efficiently aggregate multiscale image embeddings while emphasizing task-specific appearance-invariant image cues. We also introduce an attention-residual architecture, using a residual connection bypassing the encoder. This additional learning signal facilitates end-to-end training from scratch. Our approach is experimentally shown to achieve new state-of-the-art accuracy on both multimodal and single modality benchmarks, illustrating its general applicability. To the best of our knowledge, this is the first successful implementation of the Transformer encoder architecture to the multimodal image patch matching task.



### Robust Models Are More Interpretable Because Attributions Look Normal
- **Arxiv ID**: http://arxiv.org/abs/2103.11257v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.11257v3)
- **Published**: 2021-03-20 22:36:39+00:00
- **Updated**: 2021-10-06 02:21:53+00:00
- **Authors**: Zifan Wang, Matt Fredrikson, Anupam Datta
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work has found that adversarially-robust deep networks used for image classification are more interpretable: their feature attributions tend to be sharper, and are more concentrated on the objects associated with the image's ground-truth class. We show that smooth decision boundaries play an important role in this enhanced interpretability, as the model's input gradients around data points will more closely align with boundaries' normal vectors when they are smooth. Thus, because robust models have smoother boundaries, the results of gradient-based attribution methods, like Integrated Gradients and DeepLift, will capture more accurate information about nearby decision boundaries. This understanding of robust interpretability leads to our second contribution: \emph{boundary attributions}, which aggregate information about the normal vectors of local decision boundaries to explain a classification outcome. We show that by leveraging the key factors underpinning robust interpretability, boundary attributions produce sharper, more concentrated visual explanations -- even on non-robust models. Any example implementation can be found at \url{https://github.com/zifanw/boundary}.



### Temporally-Weighted Hierarchical Clustering for Unsupervised Action Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.11264v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.11264v4)
- **Published**: 2021-03-20 23:30:01+00:00
- **Updated**: 2021-03-27 16:40:25+00:00
- **Authors**: M. Saquib Sarfraz, Naila Murray, Vivek Sharma, Ali Diba, Luc Van Gool, Rainer Stiefelhagen
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Action segmentation refers to inferring boundaries of semantically consistent visual concepts in videos and is an important requirement for many video understanding tasks. For this and other video understanding tasks, supervised approaches have achieved encouraging performance but require a high volume of detailed frame-level annotations. We present a fully automatic and unsupervised approach for segmenting actions in a video that does not require any training. Our proposal is an effective temporally-weighted hierarchical clustering algorithm that can group semantically consistent frames of the video. Our main finding is that representing a video with a 1-nearest neighbor graph by taking into account the time progression is sufficient to form semantically and temporally consistent clusters of frames where each cluster may represent some action in the video. Additionally, we establish strong unsupervised baselines for action segmentation and show significant performance improvements over published unsupervised methods on five challenging action segmentation datasets. Our code is available at https://github.com/ssarfraz/FINCH-Clustering/tree/master/TW-FINCH



