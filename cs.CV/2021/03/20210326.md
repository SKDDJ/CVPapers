# Arxiv Papers in cs.CV on 2021-03-26
### Self-Attentive 3D Human Pose and Shape Estimation from Videos
- **Arxiv ID**: http://arxiv.org/abs/2103.14182v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14182v2)
- **Published**: 2021-03-26 00:02:19+00:00
- **Updated**: 2021-09-07 01:28:20+00:00
- **Authors**: Yun-Chun Chen, Marco Piccirilli, Robinson Piramuthu, Ming-Hsuan Yang
- **Comment**: This paper is under consideration at Computer Vision and Image
  Understanding
- **Journal**: None
- **Summary**: We consider the task of estimating 3D human pose and shape from videos. While existing frame-based approaches have made significant progress, these methods are independently applied to each image, thereby often leading to inconsistent predictions. In this work, we present a video-based learning algorithm for 3D human pose and shape estimation. The key insights of our method are two-fold. First, to address the inconsistent temporal prediction issue, we exploit temporal information in videos and propose a self-attention module that jointly considers short-range and long-range dependencies across frames, resulting in temporally coherent estimations. Second, we model human motion with a forecasting module that allows the transition between adjacent frames to be smooth. We evaluate our method on the 3DPW, MPI-INF-3DHP, and Human3.6M datasets. Extensive experimental results show that our algorithm performs favorably against the state-of-the-art methods.



### Deformable Linear Object Prediction Using Locally Linear Latent Dynamics
- **Arxiv ID**: http://arxiv.org/abs/2103.14184v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14184v1)
- **Published**: 2021-03-26 00:29:31+00:00
- **Updated**: 2021-03-26 00:29:31+00:00
- **Authors**: Wenbo Zhang, Karl Schmeckpeper, Pratik Chaudhari, Kostas Daniilidis
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a framework for deformable linear object prediction. Prediction of deformable objects (e.g., rope) is challenging due to their non-linear dynamics and infinite-dimensional configuration spaces. By mapping the dynamics from a non-linear space to a linear space, we can use the good properties of linear dynamics for easier learning and more efficient prediction. We learn a locally linear, action-conditioned dynamics model that can be used to predict future latent states. Then, we decode the predicted latent state into the predicted state. We also apply a sampling-based optimization algorithm to select the optimal control action. We empirically demonstrate that our approach can predict the rope state accurately up to ten steps into the future and that our algorithm can find the optimal action given an initial state and a goal state.



### Exploiting Playbacks in Unsupervised Domain Adaptation for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.14198v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14198v2)
- **Published**: 2021-03-26 01:18:11+00:00
- **Updated**: 2022-07-10 21:51:55+00:00
- **Authors**: Yurong You, Carlos Andres Diaz-Ruiz, Yan Wang, Wei-Lun Chao, Bharath Hariharan, Mark Campbell, Kilian Q Weinberger
- **Comment**: Accepted by ICRA 2022
- **Journal**: None
- **Summary**: Self-driving cars must detect other vehicles and pedestrians in 3D to plan safe routes and avoid collisions. State-of-the-art 3D object detectors, based on deep learning, have shown promising accuracy but are prone to over-fit to domain idiosyncrasies, making them fail in new environments -- a serious problem if autonomous vehicles are meant to operate freely. In this paper, we propose a novel learning approach that drastically reduces this gap by fine-tuning the detector on pseudo-labels in the target domain, which our method generates while the vehicle is parked, based on replays of previously recorded driving sequences. In these replays, objects are tracked over time, and detections are interpolated and extrapolated -- crucially, leveraging future information to catch hard cases. We show, on five autonomous driving datasets, that fine-tuning the object detector on these pseudo-labels substantially reduces the domain gap to new driving environments, yielding drastic improvements in accuracy and detection reliability.



### Image2Reverb: Cross-Modal Reverb Impulse Response Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2103.14201v2
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2103.14201v2)
- **Published**: 2021-03-26 01:25:58+00:00
- **Updated**: 2021-08-13 18:48:16+00:00
- **Authors**: Nikhil Singh, Jeff Mentch, Jerry Ng, Matthew Beveridge, Iddo Drori
- **Comment**: ICCV 2021. Project page:
  https://web.media.mit.edu/~nsingh1/image2reverb/
- **Journal**: None
- **Summary**: Measuring the acoustic characteristics of a space is often done by capturing its impulse response (IR), a representation of how a full-range stimulus sound excites it. This work generates an IR from a single image, which can then be applied to other signals using convolution, simulating the reverberant characteristics of the space shown in the image. Recording these IRs is both time-intensive and expensive, and often infeasible for inaccessible locations. We use an end-to-end neural network architecture to generate plausible audio impulse responses from single images of acoustic environments. We evaluate our method both by comparisons to ground truth data and by human expert evaluation. We demonstrate our approach by generating plausible impulse responses from diverse settings and formats including well known places, musical halls, rooms in paintings, images from animations and computer games, synthetic environments generated from text, panoramic images, and video conference backgrounds.



### Towards a Unified Approach to Single Image Deraining and Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2103.14204v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14204v1)
- **Published**: 2021-03-26 01:35:43+00:00
- **Updated**: 2021-03-26 01:35:43+00:00
- **Authors**: Xiaohong Liu, Yongrui Ma, Zhihao Shi, Linhui Dai, Jun Chen
- **Comment**: 10 pages, 11 figures
- **Journal**: None
- **Summary**: We develop a new physical model for the rain effect and show that the well-known atmosphere scattering model (ASM) for the haze effect naturally emerges as its homogeneous continuous limit. Via depth-aware fusion of multi-layer rain streaks according to the camera imaging mechanism, the new model can better capture the sophisticated non-deterministic degradation patterns commonly seen in real rainy images. We also propose a Densely Scale-Connected Attentive Network (DSCAN) that is suitable for both deraining and dehazing tasks. Our design alleviates the bottleneck issue existent in conventional multi-scale networks and enables more effective information exchange and aggregation. Extensive experimental results demonstrate that the proposed DSCAN is able to deliver superior derained/dehazed results on both synthetic and real images as compared to the state-of-the-art. Moreover, it is shown that for our DSCAN, the synthetic dataset built using the new physical model yields better generalization performance on real images in comparison with the existing datasets based on over-simplified models.



### Leaning Compact and Representative Features for Cross-Modality Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2103.14210v2
- **DOI**: 10.1007/s11280-022-01014-5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14210v2)
- **Published**: 2021-03-26 01:53:16+00:00
- **Updated**: 2022-02-09 16:11:48+00:00
- **Authors**: Guangwei Gao, Hao Shao, Fei Wu, Meng Yang, Yi Yu
- **Comment**: World Wide Web Journal, 20 pages, 6 figures
- **Journal**: None
- **Summary**: This paper pays close attention to the cross-modality visible-infrared person re-identification (VI Re-ID) task, which aims to match pedestrian samples between visible and infrared modes. In order to reduce the modality-discrepancy between samples from different cameras, most existing works usually use constraints based on Euclidean metric. Because of the Euclidean based distance metric strategy cannot effectively measure the internal angles between the embedded vectors, the existing solutions cannot learn the angularly discriminative feature embedding. Since the most important factor affecting the classification task based on embedding vector is whether there is an angularly discriminative feature space, in this paper, we present a new loss function called Enumerate Angular Triplet (EAT) loss. Also, motivated by the knowledge distillation, to narrow down the features between different modalities before feature embedding, we further present a novel Cross-Modality Knowledge Distillation (CMKD) loss. Benefit from the above two considerations, the embedded features are discriminative enough in a way to tackle modality-discrepancy problem. The experimental results on RegDB and SYSU-MM01 datasets have demonstrated that the proposed method is superior to the other most advanced methods in terms of impressive performance. Code is available at https://github.com/IVIPLab/LCCRF.



### MagDR: Mask-guided Detection and Reconstruction for Defending Deepfakes
- **Arxiv ID**: http://arxiv.org/abs/2103.14211v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14211v1)
- **Published**: 2021-03-26 01:57:04+00:00
- **Updated**: 2021-03-26 01:57:04+00:00
- **Authors**: Zhikai Chen, Lingxi Xie, Shanmin Pang, Yong He, Bo Zhang
- **Comment**: Accepted to CVPR2021
- **Journal**: None
- **Summary**: Deepfakes raised serious concerns on the authenticity of visual contents. Prior works revealed the possibility to disrupt deepfakes by adding adversarial perturbations to the source data, but we argue that the threat has not been eliminated yet. This paper presents MagDR, a mask-guided detection and reconstruction pipeline for defending deepfakes from adversarial attacks. MagDR starts with a detection module that defines a few criteria to judge the abnormality of the output of deepfakes, and then uses it to guide a learnable reconstruction procedure. Adaptive masks are extracted to capture the change in local facial regions. In experiments, MagDR defends three main tasks of deepfakes, and the learned reconstruction pipeline transfers across input data, showing promising performance in defending both black-box and white-box attacks.



### Synthesize-It-Classifier: Learning a Generative Classifier through RecurrentSelf-analysis
- **Arxiv ID**: http://arxiv.org/abs/2103.14212v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14212v1)
- **Published**: 2021-03-26 02:00:29+00:00
- **Updated**: 2021-03-26 02:00:29+00:00
- **Authors**: Arghya Pal, Rapha Phan, KokSheik Wong
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we show the generative capability of an image classifier network by synthesizing high-resolution, photo-realistic, and diverse images at scale. The overall methodology, called Synthesize-It-Classifier (STIC), does not require an explicit generator network to estimate the density of the data distribution and sample images from that, but instead uses the classifier's knowledge of the boundary to perform gradient ascent w.r.t. class logits and then synthesizes images using Gram Matrix Metropolis Adjusted Langevin Algorithm (GRMALA) by drawing on a blank canvas. During training, the classifier iteratively uses these synthesized images as fake samples and re-estimates the class boundary in a recurrent fashion to improve both the classification accuracy and quality of synthetic images. The STIC shows the mixing of the hard fake samples (i.e. those synthesized by the one hot class conditioning), and the soft fake samples (which are synthesized as a convex combination of classes, i.e. a mixup of classes) improves class interpolation. We demonstrate an Attentive-STIC network that shows an iterative drawing of synthesized images on the ImageNet dataset that has thousands of classes. In addition, we introduce the synthesis using a class conditional score classifier (Score-STIC) instead of a normal image classifier and show improved results on several real-world datasets, i.e. ImageNet, LSUN, and CIFAR 10.



### Which Parts Determine the Impression of the Font?
- **Arxiv ID**: http://arxiv.org/abs/2103.14216v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14216v3)
- **Published**: 2021-03-26 02:13:24+00:00
- **Updated**: 2021-06-21 03:55:31+00:00
- **Authors**: Masaya Ueda, Akisato Kimura, Seiichi Uchida
- **Comment**: Accepted at ICDAR 2021
- **Journal**: None
- **Summary**: Various fonts give different impressions, such as legible, rough, and comic-text.This paper aims to analyze the correlation between the local shapes, or parts, and the impression of fonts. By focusing on local shapes instead of the whole letter shape, we can realize letter-shape independent and more general analysis. The analysis is performed by newly combining SIFT and DeepSets, to extract an arbitrary number of essential parts from a particular font and aggregate them to infer the font impressions by nonlinear regression. Our qualitative and quantitative analyses prove that (1)fonts with similar parts have similar impressions, (2)many impressions, such as legible and rough, largely depend on specific parts, (3)several impressions are very irrelevant to parts.



### Adversarial Attacks are Reversible with Natural Supervision
- **Arxiv ID**: http://arxiv.org/abs/2103.14222v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14222v3)
- **Published**: 2021-03-26 02:21:40+00:00
- **Updated**: 2021-09-09 02:58:44+00:00
- **Authors**: Chengzhi Mao, Mia Chiquier, Hao Wang, Junfeng Yang, Carl Vondrick
- **Comment**: None
- **Journal**: None
- **Summary**: We find that images contain intrinsic structure that enables the reversal of many adversarial attacks. Attack vectors cause not only image classifiers to fail, but also collaterally disrupt incidental structure in the image. We demonstrate that modifying the attacked image to restore the natural structure will reverse many types of attacks, providing a defense. Experiments demonstrate significantly improved robustness for several state-of-the-art models across the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Our results show that our defense is still effective even if the attacker is aware of the defense mechanism. Since our defense is deployed during inference instead of training, it is compatible with pre-trained networks as well as most other defenses. Our results suggest deep networks are vulnerable to adversarial examples partly because their representations do not enforce the natural structure of images.



### Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution
- **Arxiv ID**: http://arxiv.org/abs/2103.14230v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14230v2)
- **Published**: 2021-03-26 02:42:18+00:00
- **Updated**: 2021-05-14 01:47:55+00:00
- **Authors**: Chi Zhang, Baoxiong Jia, Song-Chun Zhu, Yixin Zhu
- **Comment**: CVPR 2021 paper. Supplementary:
  http://wellyzhang.github.io/attach/cvpr21zhang_prae_supp.pdf Project:
  http://wellyzhang.github.io/project/prae.html
- **Journal**: None
- **Summary**: Spatial-temporal reasoning is a challenging task in Artificial Intelligence (AI) due to its demanding but unique nature: a theoretic requirement on representing and reasoning based on spatial-temporal knowledge in mind, and an applied requirement on a high-level cognitive system capable of navigating and acting in space and time. Recent works have focused on an abstract reasoning task of this kind -- Raven's Progressive Matrices (RPM). Despite the encouraging progress on RPM that achieves human-level performance in terms of accuracy, modern approaches have neither a treatment of human-like reasoning on generalization, nor a potential to generate answers. To fill in this gap, we propose a neuro-symbolic Probabilistic Abduction and Execution (PrAE) learner; central to the PrAE learner is the process of probabilistic abduction and execution on a probabilistic scene representation, akin to the mental manipulation of objects. Specifically, we disentangle perception and reasoning from a monolithic model. The neural visual perception frontend predicts objects' attributes, later aggregated by a scene inference engine to produce a probabilistic scene representation. In the symbolic logical reasoning backend, the PrAE learner uses the representation to abduce the hidden rules. An answer is predicted by executing the rules on the probabilistic representation. The entire system is trained end-to-end in an analysis-by-synthesis manner without any visual attribute annotations. Extensive experiments demonstrate that the PrAE learner improves cross-configuration generalization and is capable of rendering an answer, in contrast to prior works that merely make a categorical choice from candidates.



### Congestion-aware Multi-agent Trajectory Prediction for Collision Avoidance
- **Arxiv ID**: http://arxiv.org/abs/2103.14231v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14231v1)
- **Published**: 2021-03-26 02:42:33+00:00
- **Updated**: 2021-03-26 02:42:33+00:00
- **Authors**: Xu Xie, Chi Zhang, Yixin Zhu, Ying Nian Wu, Song-Chun Zhu
- **Comment**: ICRA 2021 paper. Project:
  https://xuxie1031.github.io/projects/GTA/GTAProj.html
- **Journal**: None
- **Summary**: Predicting agents' future trajectories plays a crucial role in modern AI systems, yet it is challenging due to intricate interactions exhibited in multi-agent systems, especially when it comes to collision avoidance. To address this challenge, we propose to learn congestion patterns as contextual cues explicitly and devise a novel "Sense--Learn--Reason--Predict" framework by exploiting advantages of three different doctrines of thought, which yields the following desirable benefits: (i) Representing congestion as contextual cues via latent factors subsumes the concept of social force commonly used in physics-based approaches and implicitly encodes the distance as a cost, similar to the way a planning-based method models the environment. (ii) By decomposing the learning phases into two stages, a "student" can learn contextual cues from a "teacher" while generating collision-free trajectories. To make the framework computationally tractable, we formulate it as an optimization problem and derive an upper bound by leveraging the variational parametrization. In experiments, we demonstrate that the proposed model is able to generate collision-free trajectory predictions in a synthetic dataset designed for collision avoidance evaluation and remains competitive on the commonly used NGSIM US-101 highway dataset.



### ACRE: Abstract Causal REasoning Beyond Covariation
- **Arxiv ID**: http://arxiv.org/abs/2103.14232v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14232v1)
- **Published**: 2021-03-26 02:42:38+00:00
- **Updated**: 2021-03-26 02:42:38+00:00
- **Authors**: Chi Zhang, Baoxiong Jia, Mark Edmonds, Song-Chun Zhu, Yixin Zhu
- **Comment**: CVPR 2021 paper. Supplementary:
  http://wellyzhang.github.io/attach/cvpr21zhang_acre_supp.pdf Project:
  http://wellyzhang.github.io/project/acre.html
- **Journal**: None
- **Summary**: Causal induction, i.e., identifying unobservable mechanisms that lead to the observable relations among variables, has played a pivotal role in modern scientific discovery, especially in scenarios with only sparse and limited data. Humans, even young toddlers, can induce causal relationships surprisingly well in various settings despite its notorious difficulty. However, in contrast to the commonplace trait of human cognition is the lack of a diagnostic benchmark to measure causal induction for modern Artificial Intelligence (AI) systems. Therefore, in this work, we introduce the Abstract Causal REasoning (ACRE) dataset for systematic evaluation of current vision systems in causal induction. Motivated by the stream of research on causal discovery in Blicket experiments, we query a visual reasoning system with the following four types of questions in either an independent scenario or an interventional scenario: direct, indirect, screening-off, and backward-blocking, intentionally going beyond the simple strategy of inducing causal relationships by covariation. By analyzing visual reasoning architectures on this testbed, we notice that pure neural models tend towards an associative strategy under their chance-level performance, whereas neuro-symbolic combinations struggle in backward-blocking reasoning. These deficiencies call for future research in models with a more comprehensive capability of causal induction.



### Learning from Pixel-Level Label Noise: A New Perspective for Semi-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.14242v1
- **DOI**: 10.1109/TIP.2021.3134142
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14242v1)
- **Published**: 2021-03-26 03:23:21+00:00
- **Updated**: 2021-03-26 03:23:21+00:00
- **Authors**: Rumeng Yi, Yaping Huang, Qingji Guan, Mengyang Pu, Runsheng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses semi-supervised semantic segmentation by exploiting a small set of images with pixel-level annotations (strong supervisions) and a large set of images with only image-level annotations (weak supervisions). Most existing approaches aim to generate accurate pixel-level labels from weak supervisions. However, we observe that those generated labels still inevitably contain noisy labels. Motivated by this observation, we present a novel perspective and formulate this task as a problem of learning with pixel-level label noise. Existing noisy label methods, nevertheless, mainly aim at image-level tasks, which can not capture the relationship between neighboring labels in one image. Therefore, we propose a graph based label noise detection and correction framework to deal with pixel-level noisy labels. In particular, for the generated pixel-level noisy labels from weak supervisions by Class Activation Map (CAM), we train a clean segmentation model with strong supervisions to detect the clean labels from these noisy labels according to the cross-entropy loss. Then, we adopt a superpixel-based graph to represent the relations of spatial adjacency and semantic similarity between pixels in one image. Finally we correct the noisy labels using a Graph Attention Network (GAT) supervised by detected clean labels. We comprehensively conduct experiments on PASCAL VOC 2012, PASCAL-Context and MS-COCO datasets. The experimental results show that our proposed semi supervised method achieves the state-of-the-art performances and even outperforms the fully-supervised models on PASCAL VOC 2012 and MS-COCO datasets in some cases.



### Super-Resolving Compressed Video in Coding Chain
- **Arxiv ID**: http://arxiv.org/abs/2103.14247v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.14247v1)
- **Published**: 2021-03-26 03:39:54+00:00
- **Updated**: 2021-03-26 03:39:54+00:00
- **Authors**: Dewang Hou, Yang Zhao, Yuyao Ye, Jiayu Yang, Jian Zhang, Ronggang Wang
- **Comment**: Technical report
- **Journal**: None
- **Summary**: Scaling and lossy coding are widely used in video transmission and storage. Previous methods for enhancing the resolution of such videos often ignore the inherent interference between resolution loss and compression artifacts, which compromises perceptual video quality. To address this problem, we present a mixed-resolution coding framework, which cooperates with a reference-based DCNN. In this novel coding chain, the reference-based DCNN learns the direct mapping from low-resolution (LR) compressed video to their high-resolution (HR) clean version at the decoder side. We further improve reconstruction quality by devising an efficient deformable alignment module with receptive field block to handle various motion distances and introducing a disentangled loss that helps networks distinguish the artifact patterns from texture. Extensive experiments demonstrate the effectiveness of proposed innovations by comparing with state-of-the-art single image, video and reference-based restoration methods.



### Marine Snow Removal Benchmarking Dataset
- **Arxiv ID**: http://arxiv.org/abs/2103.14249v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.14249v2)
- **Published**: 2021-03-26 03:54:43+00:00
- **Updated**: 2021-03-29 22:34:11+00:00
- **Authors**: Yuya Sato, Takumi Ueda, Yuichi Tanaka
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a new benchmarking dataset for marine snow removal of underwater images. Marine snow is one of the main degradation sources of underwater images that are caused by small particles, e.g., organic matter and sand, between the underwater scene and photosensors. We mathematically model two typical types of marine snow from the observations of real underwater images. The modeled artifacts are synthesized with underwater images to construct large-scale pairs of ground-truth and degraded images to calculate objective qualities for marine snow removal and to train a deep neural network. We propose two marine snow removal tasks using the dataset and show the first benchmarking results of marine snow removal. The Marine Snow Removal Benchmarking Dataset is publicly available online.



### Mixing-AdaSIN: Constructing a De-biased Dataset using Adaptive Structural Instance Normalization and Texture Mixing
- **Arxiv ID**: http://arxiv.org/abs/2103.14255v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.14255v2)
- **Published**: 2021-03-26 04:40:14+00:00
- **Updated**: 2021-07-31 23:42:55+00:00
- **Authors**: Myeongkyun Kang, Philip Chikontwe, Miguel Luna, Kyung Soo Hong, June Hong Ahn, Sang Hyun Park
- **Comment**: None
- **Journal**: None
- **Summary**: Following the pandemic outbreak, several works have proposed to diagnose COVID-19 with deep learning in computed tomography (CT); reporting performance on-par with experts. However, models trained/tested on the same in-distribution data may rely on the inherent data biases for successful prediction, failing to generalize on out-of-distribution samples or CT with different scanning protocols. Early attempts have partly addressed bias-mitigation and generalization through augmentation or re-sampling, but are still limited by collection costs and the difficulty of quantifying bias in medical images. In this work, we propose Mixing-AdaSIN; a bias mitigation method that uses a generative model to generate de-biased images by mixing texture information between different labeled CT scans with semantically similar features. Here, we use Adaptive Structural Instance Normalization (AdaSIN) to enhance de-biasing generation quality and guarantee structural consistency. Following, a classifier trained with the generated images learns to correctly predict the label without bias and generalizes better. To demonstrate the efficacy of our method, we construct a biased COVID-19 vs. bacterial pneumonia dataset based on CT protocols and compare with existing state-of-the-art de-biasing methods. Our experiments show that classifiers trained with de-biased generated images report improved in-distribution performance and generalization on an external COVID-19 dataset.



### Learning to Track with Object Permanence
- **Arxiv ID**: http://arxiv.org/abs/2103.14258v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14258v2)
- **Published**: 2021-03-26 04:43:04+00:00
- **Updated**: 2021-09-30 18:02:23+00:00
- **Authors**: Pavel Tokmakov, Jie Li, Wolfram Burgard, Adrien Gaidon
- **Comment**: None
- **Journal**: None
- **Summary**: Tracking by detection, the dominant approach for online multi-object tracking, alternates between localization and association steps. As a result, it strongly depends on the quality of instantaneous observations, often failing when objects are not fully visible. In contrast, tracking in humans is underlined by the notion of object permanence: once an object is recognized, we are aware of its physical existence and can approximately localize it even under full occlusions. In this work, we introduce an end-to-end trainable approach for joint object detection and tracking that is capable of such reasoning. We build on top of the recent CenterTrack architecture, which takes pairs of frames as input, and extend it to videos of arbitrary length. To this end, we augment the model with a spatio-temporal, recurrent memory module, allowing it to reason about object locations and identities in the current frame using all the previous history. It is, however, not obvious how to train such an approach. We study this question on a new, large-scale, synthetic dataset for multi-object tracking, which provides ground truth annotations for invisible objects, and propose several approaches for supervising tracking behind occlusions. Our model, trained jointly on synthetic and real data, outperforms the state of the art on KITTI and MOT17 datasets thanks to its robustness to occlusions.



### OTA: Optimal Transport Assignment for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.14259v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14259v1)
- **Published**: 2021-03-26 04:45:12+00:00
- **Updated**: 2021-03-26 04:45:12+00:00
- **Authors**: Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, Jian Sun
- **Comment**: CVPR2021
- **Journal**: None
- **Summary**: Recent advances in label assignment in object detection mainly seek to independently define positive/negative training samples for each ground-truth (gt) object. In this paper, we innovatively revisit the label assignment from a global perspective and propose to formulate the assigning procedure as an Optimal Transport (OT) problem -- a well-studied topic in Optimization Theory. Concretely, we define the unit transportation cost between each demander (anchor) and supplier (gt) pair as the weighted summation of their classification and regression losses. After formulation, finding the best assignment solution is converted to solve the optimal transport plan at minimal transportation costs, which can be solved via Sinkhorn-Knopp Iteration. On COCO, a single FCOS-ResNet-50 detector equipped with Optimal Transport Assignment (OTA) can reach 40.7% mAP under 1X scheduler, outperforming all other existing assigning methods. Extensive experiments conducted on COCO and CrowdHuman further validate the effectiveness of our proposed OTA, especially its superiority in crowd scenarios. The code is available at https://github.com/Megvii-BaseDetection/OTA.



### Contrastive Learning based Hybrid Networks for Long-Tailed Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2103.14267v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14267v1)
- **Published**: 2021-03-26 05:22:36+00:00
- **Updated**: 2021-03-26 05:22:36+00:00
- **Authors**: Peng Wang, Kai Han, Xiu-Shen Wei, Lei Zhang, Lei Wang
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Learning discriminative image representations plays a vital role in long-tailed image classification because it can ease the classifier learning in imbalanced cases. Given the promising performance contrastive learning has shown recently in representation learning, in this work, we explore effective supervised contrastive learning strategies and tailor them to learn better image representations from imbalanced data in order to boost the classification accuracy thereon. Specifically, we propose a novel hybrid network structure being composed of a supervised contrastive loss to learn image representations and a cross-entropy loss to learn classifiers, where the learning is progressively transited from feature learning to the classifier learning to embody the idea that better features make better classifiers. We explore two variants of contrastive loss for feature learning, which vary in the forms but share a common idea of pulling the samples from the same class together in the normalized embedding space and pushing the samples from different classes apart. One of them is the recently proposed supervised contrastive (SC) loss, which is designed on top of the state-of-the-art unsupervised contrastive loss by incorporating positive samples from the same class. The other is a prototypical supervised contrastive (PSC) learning strategy which addresses the intensive memory consumption in standard SC loss and thus shows more promise under limited memory budget. Extensive experiments on three long-tailed classification datasets demonstrate the advantage of the proposed contrastive learning based hybrid networks in long-tailed classification.



### Confluent Vessel Trees with Accurate Bifurcations
- **Arxiv ID**: http://arxiv.org/abs/2103.14268v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14268v1)
- **Published**: 2021-03-26 05:22:56+00:00
- **Updated**: 2021-03-26 05:22:56+00:00
- **Authors**: Zhongwen Zhang, Dmitrii Marin, Maria Drangova, Yuri Boykov
- **Comment**: 13 pages, 14 figures, CVPR2021
- **Journal**: None
- **Summary**: We are interested in unsupervised reconstruction of complex near-capillary vasculature with thousands of bifurcations where supervision and learning are infeasible. Unsupervised methods can use many structural constraints, e.g. topology, geometry, physics. Common techniques use variants of MST on geodesic tubular graphs minimizing symmetric pairwise costs, i.e. distances. We show limitations of such standard undirected tubular graphs producing typical errors at bifurcations where flow "directedness" is critical. We introduce a new general concept of confluence for continuous oriented curves forming vessel trees and show how to enforce it on discrete tubular graphs. While confluence is a high-order property, we present an efficient practical algorithm for reconstructing confluent vessel trees using minimum arborescence on a directed graph enforcing confluence via simple flow-extrapolating arc construction. Empirical tests on large near-capillary sub-voxel vasculature volumes demonstrate significantly improved reconstruction accuracy at bifurcations. Our code has also been made publicly available.



### Input-Output Balanced Framework for Long-tailed LiDAR Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.14269v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14269v1)
- **Published**: 2021-03-26 05:42:11+00:00
- **Updated**: 2021-03-26 05:42:11+00:00
- **Authors**: Peishan Cong, Xinge Zhu, Yuexin Ma
- **Comment**: Accepted by ICME 2021
- **Journal**: None
- **Summary**: A thorough and holistic scene understanding is crucial for autonomous vehicles, where LiDAR semantic segmentation plays an indispensable role. However, most existing methods focus on the network design while neglecting the inherent difficulty, imbalanced data distribution in the realistic dataset (also named long-tailed distribution), which narrows down the capability of state-of-the-art methods. In this paper, we propose an input-output balanced framework to handle the issue of long-tailed distribution. Specifically, for the input space, we synthesize these tailed instances from mesh models and well simulate the position and density distribution of LiDAR scan, which enhances the input data balance and improves the data diversity. For the output space, a multi-head block is proposed to group different categories based on their shapes and instance amounts, which alleviates the biased representation of dominating category during the feature learning. We evaluate the proposed model on two large-scale datasets, SemanticKITTI and nuScenes, where state-of-the-art results demonstrate its effectiveness. The proposed new modules can also be used as a plug-and-play, and we apply them on various backbones and datasets, showing its good generalization ability.



### LightSAL: Lightweight Sign Agnostic Learning for Implicit Surface Representation
- **Arxiv ID**: http://arxiv.org/abs/2103.14273v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14273v2)
- **Published**: 2021-03-26 05:50:14+00:00
- **Updated**: 2021-09-09 12:33:58+00:00
- **Authors**: Abol Basher, Muhammad Sarmad, Jani Boutellier
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, several works have addressed modeling of 3D shapes using deep neural networks to learn implicit surface representations. Up to now, the majority of works have concentrated on reconstruction quality, paying little or no attention to model size or training time. This work proposes LightSAL, a novel deep convolutional architecture for learning 3D shapes; the proposed work concentrates on efficiency both in network training time and resulting model size. We build on the recent concept of Sign Agnostic Learning for training the proposed network, relying on signed distance fields, with unsigned distance as ground truth. In the experimental section of the paper, we demonstrate that the proposed architecture outperforms previous work in model size and number of required training iterations, while achieving equivalent accuracy. Experiments are based on the D-Faust dataset that contains 41k 3D scans of human shapes. The proposed model has been implemented in PyTorch.



### DDR-Net: Learning Multi-Stage Multi-View Stereo With Dynamic Depth Range
- **Arxiv ID**: http://arxiv.org/abs/2103.14275v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14275v1)
- **Published**: 2021-03-26 05:52:38+00:00
- **Updated**: 2021-03-26 05:52:38+00:00
- **Authors**: Puyuan Yi, Shengkun Tang, Jian Yao
- **Comment**: None
- **Journal**: None
- **Summary**: To obtain high-resolution depth maps, some previous learning-based multi-view stereo methods build a cost volume pyramid in a coarse-to-fine manner. These approaches leverage fixed depth range hypotheses to construct cascaded plane sweep volumes. However, it is inappropriate to set identical range hypotheses for each pixel since the uncertainties of previous per-pixel depth predictions are spatially varying. Distinct from these approaches, we propose a Dynamic Depth Range Network (DDR-Net) to determine the depth range hypotheses dynamically by applying a range estimation module (REM) to learn the uncertainties of range hypotheses in the former stages. Specifically, in our DDR-Net, we first build an initial depth map at the coarsest resolution of an image across the entire depth range. Then the range estimation module (REM) leverages the probability distribution information of the initial depth to estimate the depth range hypotheses dynamically for the following stages. Moreover, we develop a novel loss strategy, which utilizes learned dynamic depth ranges to generate refined depth maps, to keep the ground truth value of each pixel covered in the range hypotheses of the next stage. Extensive experimental results show that our method achieves superior performance over other state-of-the-art methods on the DTU benchmark and obtains comparable results on the Tanks and Temples benchmark. The code is available at https://github.com/Tangshengku/DDR-Net.



### OmniHang: Learning to Hang Arbitrary Objects using Contact Point Correspondences and Neural Collision Estimation
- **Arxiv ID**: http://arxiv.org/abs/2103.14283v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14283v1)
- **Published**: 2021-03-26 06:11:05+00:00
- **Updated**: 2021-03-26 06:11:05+00:00
- **Authors**: Yifan You, Lin Shao, Toki Migimatsu, Jeannette Bohg
- **Comment**: Accepted to IEEE International Conference on Robotics and Automation
  (ICRA) 2021
- **Journal**: None
- **Summary**: In this paper, we explore whether a robot can learn to hang arbitrary objects onto a diverse set of supporting items such as racks or hooks. Endowing robots with such an ability has applications in many domains such as domestic services, logistics, or manufacturing. Yet, it is a challenging manipulation task due to the large diversity of geometry and topology of everyday objects. In this paper, we propose a system that takes partial point clouds of an object and a supporting item as input and learns to decide where and how to hang the object stably. Our system learns to estimate the contact point correspondences between the object and supporting item to get an estimated stable pose. We then run a deep reinforcement learning algorithm to refine the predicted stable pose. Then, the robot needs to find a collision-free path to move the object from its initial pose to stable hanging pose. To this end, we train a neural network based collision estimator that takes as input partial point clouds of the object and supporting item. We generate a new and challenging, large-scale, synthetic dataset annotated with stable poses of objects hung on various supporting items and their contact point correspondences. In this dataset, we show that our system is able to achieve a 68.3% success rate of predicting stable object poses and has a 52.1% F1 score in terms of finding feasible paths. Supplemental material and videos are available on our project webpage.



### IMU Data Processing For Inertial Aided Navigation: A Recurrent Neural Network Based Approach
- **Arxiv ID**: http://arxiv.org/abs/2103.14286v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.14286v1)
- **Published**: 2021-03-26 06:21:37+00:00
- **Updated**: 2021-03-26 06:21:37+00:00
- **Authors**: Ming Zhang, Mingming Zhang, Yiming Chen, Mingyang Li
- **Comment**: IEEE International Conference on Robotics and Automation (ICRA),
  Xi'an, China, 2021
- **Journal**: None
- **Summary**: In this work, we propose a novel method for performing inertial aided navigation, by using deep neural networks (DNNs). To date, most DNN inertial navigation methods focus on the task of inertial odometry, by taking gyroscope and accelerometer readings as input and regressing for integrated IMU poses (i.e., position and orientation). While this design has been successfully applied on a number of applications, it is not of theoretical performance guarantee unless patterned motion is involved. This inevitably leads to significantly reduced accuracy and robustness in certain use cases. To solve this problem, we design a framework to compute observable IMU integration terms using DNNs, followed by the numerical pose integration and sensor fusion to achieve the performance gain. Specifically, we perform detailed analysis on the motion terms in IMU kinematic equations, propose a dedicated network design, loss functions, and training strategies for the IMU data processing, and conduct extensive experiments. The results show that our method is generally applicable and outperforms both traditional and DNN methods by wide margins.



### Evaluation of Preprocessing Techniques for U-Net Based Automated Liver Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.14301v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14301v1)
- **Published**: 2021-03-26 07:31:25+00:00
- **Updated**: 2021-03-26 07:31:25+00:00
- **Authors**: Muhammad Islam, Kaleem Nawaz Khan, Muhammad Salman Khan
- **Comment**: None
- **Journal**: None
- **Summary**: To extract liver from medical images is a challenging task due to similar intensity values of liver with adjacent organs, various contrast levels, various noise associated with medical images and irregular shape of liver. To address these issues, it is important to preprocess the medical images, i.e., computerized tomography (CT) and magnetic resonance imaging (MRI) data prior to liver analysis and quantification. This paper investigates the impact of permutation of various preprocessing techniques for CT images, on the automated liver segmentation using deep learning, i.e., U-Net architecture. The study focuses on Hounsfield Unit (HU) windowing, contrast limited adaptive histogram equalization (CLAHE), z-score normalization, median filtering and Block-Matching and 3D (BM3D) filtering. The segmented results show that combination of three techniques; HU-windowing, median filtering and z-score normalization achieve optimal performance with Dice coefficient of 96.93%, 90.77% and 90.84% for training, validation and testing respectively.



### Exploiting Temporal Contexts with Strided Transformer for 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2103.14304v8
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14304v8)
- **Published**: 2021-03-26 07:35:08+00:00
- **Updated**: 2022-01-11 02:41:57+00:00
- **Authors**: Wenhao Li, Hong Liu, Runwei Ding, Mengyuan Liu, Pichao Wang, Wenming Yang
- **Comment**: Accepted by IEEE Transactions on Multimedia. Open sourced
- **Journal**: None
- **Summary**: Despite the great progress in 3D human pose estimation from videos, it is still an open problem to take full advantage of a redundant 2D pose sequence to learn representative representations for generating one 3D pose. To this end, we propose an improved Transformer-based architecture, called Strided Transformer, which simply and effectively lifts a long sequence of 2D joint locations to a single 3D pose. Specifically, a Vanilla Transformer Encoder (VTE) is adopted to model long-range dependencies of 2D pose sequences. To reduce the redundancy of the sequence, fully-connected layers in the feed-forward network of VTE are replaced with strided convolutions to progressively shrink the sequence length and aggregate information from local contexts. The modified VTE is termed as Strided Transformer Encoder (STE), which is built upon the outputs of VTE. STE not only effectively aggregates long-range information to a single-vector representation in a hierarchical global and local fashion, but also significantly reduces the computation cost. Furthermore, a full-to-single supervision scheme is designed at both full sequence and single target frame scales applied to the outputs of VTE and STE, respectively. This scheme imposes extra temporal smoothness constraints in conjunction with the single target frame supervision and hence helps produce smoother and more accurate 3D poses. The proposed Strided Transformer is evaluated on two challenging benchmark datasets, Human3.6M and HumanEva-I, and achieves state-of-the-art results with fewer parameters. Code and models are available at \url{https://github.com/Vegetebird/StridedTransformer-Pose3D}.



### City-scale Scene Change Detection using Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2103.14314v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14314v1)
- **Published**: 2021-03-26 08:04:13+00:00
- **Updated**: 2021-03-26 08:04:13+00:00
- **Authors**: Zi Jian Yew, Gim Hee Lee
- **Comment**: 8 pages, 10 figures. To be presented at ICRA2021
- **Journal**: None
- **Summary**: We propose a method for detecting structural changes in a city using images captured from vehicular mounted cameras over traversals at two different times. We first generate 3D point clouds for each traversal from the images and approximate GNSS/INS readings using Structure-from-Motion (SfM). A direct comparison of the two point clouds for change detection is not ideal due to inaccurate geo-location information and possible drifts in the SfM. To circumvent this problem, we propose a deep learning-based non-rigid registration on the point clouds which allows us to compare the point clouds for structural change detection in the scene. Furthermore, we introduce a dual thresholding check and post-processing step to enhance the robustness of our method. We collect two datasets for the evaluation of our approach. Experiments show that our method is able to detect scene changes effectively, even in the presence of viewpoint and illumination differences.



### Bidirectional Projection Network for Cross Dimension Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/2103.14326v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14326v1)
- **Published**: 2021-03-26 08:31:39+00:00
- **Updated**: 2021-03-26 08:31:39+00:00
- **Authors**: Wenbo Hu, Hengshuang Zhao, Li Jiang, Jiaya Jia, Tien-Tsin Wong
- **Comment**: CVPR 2021 (Oral)
- **Journal**: CVPR 2021
- **Summary**: 2D image representations are in regular grids and can be processed efficiently, whereas 3D point clouds are unordered and scattered in 3D space. The information inside these two visual domains is well complementary, e.g., 2D images have fine-grained texture while 3D point clouds contain plentiful geometry information. However, most current visual recognition systems process them individually. In this paper, we present a \emph{bidirectional projection network (BPNet)} for joint 2D and 3D reasoning in an end-to-end manner. It contains 2D and 3D sub-networks with symmetric architectures, that are connected by our proposed \emph{bidirectional projection module (BPM)}. Via the \emph{BPM}, complementary 2D and 3D information can interact with each other in multiple architectural levels, such that advantages in these two visual domains can be combined for better scene recognition. Extensive quantitative and qualitative experimental evaluations show that joint reasoning over 2D and 3D visual domains can benefit both 2D and 3D scene understanding simultaneously. Our \emph{BPNet} achieves top performance on the ScanNetV2 benchmark for both 2D and 3D semantic segmentation. Code is available at \url{https://github.com/wbhu/BPNet}.



### Building Reliable Explanations of Unreliable Neural Networks: Locally Smoothing Perspective of Model Interpretation
- **Arxiv ID**: http://arxiv.org/abs/2103.14332v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14332v2)
- **Published**: 2021-03-26 08:52:11+00:00
- **Updated**: 2021-03-29 01:59:56+00:00
- **Authors**: Dohun Lim, Hyeonseok Lee, Sungchan Kim
- **Comment**: Accepted to CVPR 2021. The supplementary materials are included
- **Journal**: None
- **Summary**: We present a novel method for reliably explaining the predictions of neural networks. We consider an explanation reliable if it identifies input features relevant to the model output by considering the input and the neighboring data points. Our method is built on top of the assumption of smooth landscape in a loss function of the model prediction: locally consistent loss and gradient profile. A theoretical analysis established in this study suggests that those locally smooth model explanations are learned using a batch of noisy copies of the input with the L1 regularization for a saliency map. Extensive experiments support the analysis results, revealing that the proposed saliency maps retrieve the original classes of adversarial examples crafted against both naturally and adversarially trained models, significantly outperforming previous methods. We further demonstrated that such good performance results from the learning capability of this method to identify input features that are truly relevant to the model output of the input and the neighboring data points, fulfilling the requirements of a reliable explanation.



### Dynamic Domain Adaptation for Efficient Inference
- **Arxiv ID**: http://arxiv.org/abs/2103.16403v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.16403v1)
- **Published**: 2021-03-26 08:53:16+00:00
- **Updated**: 2021-03-26 08:53:16+00:00
- **Authors**: Shuang Li, Jinming Zhang, Wenxuan Ma, Chi Harold Liu, Wei Li
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Domain adaptation (DA) enables knowledge transfer from a labeled source domain to an unlabeled target domain by reducing the cross-domain distribution discrepancy. Most prior DA approaches leverage complicated and powerful deep neural networks to improve the adaptation capacity and have shown remarkable success. However, they may have a lack of applicability to real-world situations such as real-time interaction, where low target inference latency is an essential requirement under limited computational budget. In this paper, we tackle the problem by proposing a dynamic domain adaptation (DDA) framework, which can simultaneously achieve efficient target inference in low-resource scenarios and inherit the favorable cross-domain generalization brought by DA. In contrast to static models, as a simple yet generic method, DDA can integrate various domain confusion constraints into any typical adaptive network, where multiple intermediate classifiers can be equipped to infer "easier" and "harder" target data dynamically. Moreover, we present two novel strategies to further boost the adaptation performance of multiple prediction exits: 1) a confidence score learning strategy to derive accurate target pseudo labels by fully exploring the prediction consistency of different classifiers; 2) a class-balanced self-training strategy to explicitly adapt multi-stage classifiers from source to target without losing prediction diversity. Extensive experiments on multiple benchmarks are conducted to verify that DDA can consistently improve the adaptation performance and accelerate target inference under domain shift and limited resources scenarios



### Geometry-Aware Unsupervised Domain Adaptation for Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2103.14333v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14333v1)
- **Published**: 2021-03-26 08:53:36+00:00
- **Updated**: 2021-03-26 08:53:36+00:00
- **Authors**: Hiroki Sakuma, Yoshinori Konishi
- **Comment**: Accepted to ICRA 2021
- **Journal**: None
- **Summary**: Recently proposed DNN-based stereo matching methods that learn priors directly from data are known to suffer a drastic drop in accuracy in new environments. Although supervised approaches with ground truth disparity maps often work well, collecting them in each deployment environment is cumbersome and costly. For this reason, many unsupervised domain adaptation methods based on image-to-image translation have been proposed, but these methods do not preserve the geometric structure of a stereo image pair because the image-to-image translation is applied to each view separately. To address this problem, in this paper, we propose an attention mechanism that aggregates features in the left and right views, called Stereoscopic Cross Attention (SCA). Incorporating SCA to an image-to-image translation network makes it possible to preserve the geometric structure of a stereo image pair in the process of the image-to-image translation. We empirically demonstrate the effectiveness of the proposed unsupervised domain adaptation based on the image-to-image translation with SCA.



### Hands-on Guidance for Distilling Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2103.14337v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.14337v2)
- **Published**: 2021-03-26 09:00:23+00:00
- **Updated**: 2021-05-12 09:14:27+00:00
- **Authors**: Yangyang Qin, Hefei Ling, Zhenghai He, Yuxuan Shi, Lei Wu
- **Comment**: Accepted at ICME2021
- **Journal**: None
- **Summary**: Knowledge distillation can lead to deploy-friendly networks against the plagued computational complexity problem, but previous methods neglect the feature hierarchy in detectors. Motivated by this, we propose a general framework for detection distillation. Our method, called Hands-on Guidance Distillation, distills the latent knowledge of all stage features for imposing more comprehensive supervision, and focuses on the essence simultaneously for promoting more intense knowledge absorption. Specifically, a series of novel mechanisms are designed elaborately, including correspondence establishment for consistency, hands-on imitation loss measure and re-weighted optimization from both micro and macro perspectives. We conduct extensive evaluations with different distillation configurations over VOC and COCO datasets, which show better performance on accuracy and speed trade-offs. Meanwhile, feasibility experiments on different structural networks further prove the robustness of our HGD.



### Few-Shot Human Motion Transfer by Personalized Geometry and Texture Modeling
- **Arxiv ID**: http://arxiv.org/abs/2103.14338v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2103.14338v1)
- **Published**: 2021-03-26 09:01:33+00:00
- **Updated**: 2021-03-26 09:01:33+00:00
- **Authors**: Zhichao Huang, Xintong Han, Jia Xu, Tong Zhang
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: We present a new method for few-shot human motion transfer that achieves realistic human image generation with only a small number of appearance inputs. Despite recent advances in single person motion transfer, prior methods often require a large number of training images and take long training time. One promising direction is to perform few-shot human motion transfer, which only needs a few of source images for appearance transfer. However, it is particularly challenging to obtain satisfactory transfer results. In this paper, we address this issue by rendering a human texture map to a surface geometry (represented as a UV map), which is personalized to the source person. Our geometry generator combines the shape information from source images, and the pose information from 2D keypoints to synthesize the personalized UV map. A texture generator then generates the texture map conditioned on the texture of source images to fill out invisible parts. Furthermore, we may fine-tune the texture map on the manifold of the texture generator from a few source images at the test time, which improves the quality of the texture map without over-fitting or artifacts. Extensive experiments show the proposed method outperforms state-of-the-art methods both qualitatively and quantitatively. Our code is available at https://github.com/HuangZhiChao95/FewShotMotionTransfer.



### MedSelect: Selective Labeling for Medical Image Classification Combining Meta-Learning with Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.14339v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14339v1)
- **Published**: 2021-03-26 09:09:34+00:00
- **Updated**: 2021-03-26 09:09:34+00:00
- **Authors**: Akshay Smit, Damir Vrabac, Yujie He, Andrew Y. Ng, Andrew L. Beam, Pranav Rajpurkar
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a selective learning method using meta-learning and deep reinforcement learning for medical image interpretation in the setting of limited labeling resources. Our method, MedSelect, consists of a trainable deep learning selector that uses image embeddings obtained from contrastive pretraining for determining which images to label, and a non-parametric selector that uses cosine similarity to classify unseen images. We demonstrate that MedSelect learns an effective selection strategy outperforming baseline selection strategies across seen and unseen medical conditions for chest X-ray interpretation. We also perform an analysis of the selections performed by MedSelect comparing the distribution of latent embeddings and clinical features, and find significant differences compared to the strongest performing baseline. We believe that our method may be broadly applicable across medical imaging settings where labels are expensive to acquire.



### MetaNODE: Prototype Optimization as a Neural ODE for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.14341v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14341v2)
- **Published**: 2021-03-26 09:16:46+00:00
- **Updated**: 2021-12-06 08:43:20+00:00
- **Authors**: Baoquan Zhang, Xutao Li, Shanshan Feng, Yunming Ye, Rui Ye
- **Comment**: Accepted by AAAI 2022
- **Journal**: None
- **Summary**: Few-Shot Learning (FSL) is a challenging task, \emph{i.e.}, how to recognize novel classes with few examples? Pre-training based methods effectively tackle the problem by pre-training a feature extractor and then predicting novel classes via a cosine nearest neighbor classifier with mean-based prototypes. Nevertheless, due to the data scarcity, the mean-based prototypes are usually biased. In this paper, we attempt to diminish the prototype bias by regarding it as a prototype optimization problem. To this end, we propose a novel meta-learning based prototype optimization framework to rectify prototypes, \emph{i.e.}, introducing a meta-optimizer to optimize prototypes. Although the existing meta-optimizers can also be adapted to our framework, they all overlook a crucial gradient bias issue, \emph{i.e.}, the mean-based gradient estimation is also biased on sparse data. To address the issue, we regard the gradient and its flow as meta-knowledge and then propose a novel Neural Ordinary Differential Equation (ODE)-based meta-optimizer to polish prototypes, called MetaNODE. In this meta-optimizer, we first view the mean-based prototypes as initial prototypes, and then model the process of prototype optimization as continuous-time dynamics specified by a Neural ODE. A gradient flow inference network is carefully designed to learn to estimate the continuous gradient flow for prototype dynamics. Finally, the optimal prototypes can be obtained by solving the Neural ODE. Extensive experiments on miniImagenet, tieredImagenet, and CUB-200-2011 show the effectiveness of our method.



### Combating Adversaries with Anti-Adversaries
- **Arxiv ID**: http://arxiv.org/abs/2103.14347v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.14347v2)
- **Published**: 2021-03-26 09:36:59+00:00
- **Updated**: 2021-12-16 17:21:21+00:00
- **Authors**: Motasem Alfarra, Juan C. Pérez, Ali Thabet, Adel Bibi, Philip H. S. Torr, Bernard Ghanem
- **Comment**: Accepted to AAAI Conference on Artificial Intelligence (AAAI'22)
- **Journal**: None
- **Summary**: Deep neural networks are vulnerable to small input perturbations known as adversarial attacks. Inspired by the fact that these adversaries are constructed by iteratively minimizing the confidence of a network for the true class label, we propose the anti-adversary layer, aimed at countering this effect. In particular, our layer generates an input perturbation in the opposite direction of the adversarial one and feeds the classifier a perturbed version of the input. Our approach is training-free and theoretically supported. We verify the effectiveness of our approach by combining our layer with both nominally and robustly trained models and conduct large-scale experiments from black-box to adaptive attacks on CIFAR10, CIFAR100, and ImageNet. Our layer significantly enhances model robustness while coming at no cost on clean accuracy.



### VDM-DA: Virtual Domain Modeling for Source Data-free Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2103.14357v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14357v1)
- **Published**: 2021-03-26 09:56:40+00:00
- **Updated**: 2021-03-26 09:56:40+00:00
- **Authors**: Jiayi Tian, Jing Zhang, Wen Li, Dong Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation aims to leverage a label-rich domain (the source domain) to help model learning in a label-scarce domain (the target domain). Most domain adaptation methods require the co-existence of source and target domain samples to reduce the distribution mismatch, however, access to the source domain samples may not always be feasible in the real world applications due to different problems (e.g., storage, transmission, and privacy issues). In this work, we deal with the source data-free unsupervised domain adaptation problem, and propose a novel approach referred to as Virtual Domain Modeling (VDM-DA). The virtual domain acts as a bridge between the source and target domains. On one hand, we generate virtual domain samples based on an approximated Gaussian Mixture Model (GMM) in the feature space with the pre-trained source model, such that the virtual domain maintains a similar distribution with the source domain without accessing to the original source data. On the other hand, we also design an effective distribution alignment method to reduce the distribution divergence between the virtual domain and the target domain by gradually improving the compactness of the target domain distribution through model learning. In this way, we successfully achieve the goal of distribution alignment between the source and target domains by training deep networks without accessing to the source domain data. We conduct extensive experiments on benchmark datasets for both 2D image-based and 3D point cloud-based cross-domain object recognition tasks, where the proposed method referred to Domain Adaptation with Virtual Domain Modeling (VDM-DA) achieves the state-of-the-art performances on all datasets.



### D2C-SR: A Divergence to Convergence Approach for Real-World Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2103.14373v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14373v4)
- **Published**: 2021-03-26 10:20:28+00:00
- **Updated**: 2022-07-20 10:29:12+00:00
- **Authors**: Youwei Li, Haibin Huang, Lanpeng Jia, Haoqiang Fan, Shuaicheng Liu
- **Comment**: 14 pages, 12 figures
- **Journal**: None
- **Summary**: In this paper, we present D2C-SR, a novel framework for the task of real-world image super-resolution. As an ill-posed problem, the key challenge in super-resolution related tasks is there can be multiple predictions for a given low-resolution input. Most classical deep learning based approaches ignored the fundamental fact and lack explicit modeling of the underlying high-frequency distribution which leads to blurred results. Recently, some methods of GAN-based or learning super-resolution space can generate simulated textures but do not promise the accuracy of the textures which have low quantitative performance. Rethinking both, we learn the distribution of underlying high-frequency details in a discrete form and propose a two-stage pipeline: divergence stage to convergence stage. At divergence stage, we propose a tree-based structure deep network as our divergence backbone. Divergence loss is proposed to encourage the generated results from the tree-based network to diverge into possible high-frequency representations, which is our way of discretely modeling the underlying high-frequency distribution. At convergence stage, we assign spatial weights to fuse these divergent predictions to obtain the final output with more accurate details. Our approach provides a convenient end-to-end manner to inference. We conduct evaluations on several real-world benchmarks, including a new proposed D2CRealSR dataset with x8 scaling factor. Our experiments demonstrate that D2C-SR achieves better accuracy and visual improvements against state-of-the-art methods, with a significantly less parameters number and our D2C structure can also be applied as a generalized structure to some other methods to obtain improvement. Our codes and dataset are available at https://github.com/megvii-research/D2C-SR



### Self-Supervised Learning in Multi-Task Graphs through Iterative Consensus Shift
- **Arxiv ID**: http://arxiv.org/abs/2103.14417v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.14417v3)
- **Published**: 2021-03-26 11:57:42+00:00
- **Updated**: 2021-11-04 17:59:14+00:00
- **Authors**: Emanuela Haller, Elena Burceanu, Marius Leordeanu
- **Comment**: Accepted at The British Machine Vision Conference (BMVC) 2021, 12
  pages, 6 figures, 5 tables
- **Journal**: None
- **Summary**: The human ability to synchronize the feedback from all their senses inspired recent works in multi-task and multi-modal learning. While these works rely on expensive supervision, our multi-task graph requires only pseudo-labels from expert models. Every graph node represents a task, and each edge learns between tasks transformations. Once initialized, the graph learns self-supervised, based on a novel consensus shift algorithm that intelligently exploits the agreement between graph pathways to generate new pseudo-labels for the next learning cycle. We demonstrate significant improvement from one unsupervised learning iteration to the next, outperforming related recent methods in extensive multi-task learning experiments on two challenging datasets. Our code is available at https://github.com/bit-ml/cshift.



### YOLinO: Generic Single Shot Polyline Detection in Real Time
- **Arxiv ID**: http://arxiv.org/abs/2103.14420v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.14420v2)
- **Published**: 2021-03-26 12:00:26+00:00
- **Updated**: 2021-10-05 16:21:35+00:00
- **Authors**: Annika Meyer, Philipp Skudlik, Jan-Hendrik Pauls, Christoph Stiller
- **Comment**: published on ICCV 2021 Workshop
- **Journal**: None
- **Summary**: The detection of polylines is usually either bound to branchless polylines or formulated in a recurrent way, prohibiting their use in real-time systems.   We propose an approach that builds upon the idea of single shot object detection. Reformulating the problem of polyline detection as a bottom-up composition of small line segments allows to detect bounded, dashed and continuous polylines with a single head. This has several major advantages over previous methods. Not only is the method at 187 fps more than suited for real-time applications with virtually any restriction on the shapes of the detected polylines. By predicting multiple line segments for each cell, even branching or crossing polylines can be detected.   We evaluate our approach on three different applications for road marking, lane border and center line detection. Hereby, we demonstrate the ability to generalize to different domains as well as both implicit and explicit polyline detection tasks.



### SegVisRL: Visuomotor Development for a Lunar Rover for Hazard Avoidance using Camera Images
- **Arxiv ID**: http://arxiv.org/abs/2103.14422v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2103.14422v1)
- **Published**: 2021-03-26 12:01:42+00:00
- **Updated**: 2021-03-26 12:01:42+00:00
- **Authors**: Tamir Blum, Gabin Paillet, Watcharawut Masawat, Mickael Laine, Kazuya Yoshida
- **Comment**: 9 pages including references. 8 images, 2 tables. Workshop submission
- **Journal**: None
- **Summary**: The visuomotor system of any animal is critical for its survival, and the development of a complex one within humans is large factor in our success as a species on Earth. This system is an essential part of our ability to adapt to our environment. We use this system continuously throughout the day, when picking something up, or walking around while avoiding bumping into objects. Equipping robots with such capabilities will help produce more intelligent locomotion with the ability to more easily understand their surroundings and to move safely. In particular, such capabilities are desirable for traversing the lunar surface, as it is full of hazardous obstacles, such as rocks. These obstacles need to be identified and avoided in real time. This paper seeks to demonstrate the development of a visuomotor system within a robot for navigation and obstacle avoidance, with complex rock shaped objects representing hazards. Our approach uses deep reinforcement learning with only image data. In this paper, we compare the results from several neural network architectures and a preprocessing methodology which includes producing a segmented image and downsampling.



### Multimodal Knowledge Expansion
- **Arxiv ID**: http://arxiv.org/abs/2103.14431v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2103.14431v3)
- **Published**: 2021-03-26 12:32:07+00:00
- **Updated**: 2021-10-29 00:01:33+00:00
- **Authors**: Zihui Xue, Sucheng Ren, Zhengqi Gao, Hang Zhao
- **Comment**: Accepted by ICCV 2021. Project website:
  https://tsinghua-mars-lab.github.io/MKE/
- **Journal**: None
- **Summary**: The popularity of multimodal sensors and the accessibility of the Internet have brought us a massive amount of unlabeled multimodal data. Since existing datasets and well-trained models are primarily unimodal, the modality gap between a unimodal network and unlabeled multimodal data poses an interesting problem: how to transfer a pre-trained unimodal network to perform the same task on unlabeled multimodal data? In this work, we propose multimodal knowledge expansion (MKE), a knowledge distillation-based framework to effectively utilize multimodal data without requiring labels. Opposite to traditional knowledge distillation, where the student is designed to be lightweight and inferior to the teacher, we observe that a multimodal student model consistently denoises pseudo labels and generalizes better than its teacher. Extensive experiments on four tasks and different modalities verify this finding. Furthermore, we connect the mechanism of MKE to semi-supervised learning and offer both empirical and theoretical explanations to understand the denoising capability of a multimodal student.



### Visual Explanations from Spiking Neural Networks using Interspike Intervals
- **Arxiv ID**: http://arxiv.org/abs/2103.14441v1
- **DOI**: 10.1038/S41598-021-98448
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14441v1)
- **Published**: 2021-03-26 12:49:46+00:00
- **Updated**: 2021-03-26 12:49:46+00:00
- **Authors**: Youngeun Kim, Priyadarshini Panda
- **Comment**: None
- **Journal**: Scientific Reports 11, 2021
- **Summary**: Spiking Neural Networks (SNNs) compute and communicate with asynchronous binary temporal events that can lead to significant energy savings with neuromorphic hardware. Recent algorithmic efforts on training SNNs have shown competitive performance on a variety of classification tasks. However, a visualization tool for analysing and explaining the internal spike behavior of such temporal deep SNNs has not been explored. In this paper, we propose a new concept of bio-plausible visualization for SNNs, called Spike Activation Map (SAM). The proposed SAM circumvents the non-differentiable characteristic of spiking neurons by eliminating the need for calculating gradients to obtain visual explanations. Instead, SAM calculates a temporal visualization map by forward propagating input spikes over different time-steps. SAM yields an attention map corresponding to each time-step of input data by highlighting neurons with short inter-spike interval activity. Interestingly, without both the backpropagation process and the class label, SAM highlights the discriminative region of the image while capturing fine-grained details. With SAM, for the first time, we provide a comprehensive analysis on how internal spikes work in various SNN training configurations depending on optimization types, leak behavior, as well as when faced with adversarial examples.



### Spatial Dual-Modality Graph Reasoning for Key Information Extraction
- **Arxiv ID**: http://arxiv.org/abs/2103.14470v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14470v1)
- **Published**: 2021-03-26 13:46:00+00:00
- **Updated**: 2021-03-26 13:46:00+00:00
- **Authors**: Hongbin Sun, Zhanghui Kuang, Xiaoyu Yue, Chenhao Lin, Wayne Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Key information extraction from document images is of paramount importance in office automation. Conventional template matching based approaches fail to generalize well to document images of unseen templates, and are not robust against text recognition errors. In this paper, we propose an end-to-end Spatial Dual-Modality Graph Reasoning method (SDMG-R) to extract key information from unstructured document images. We model document images as dual-modality graphs, nodes of which encode both the visual and textual features of detected text regions, and edges of which represent the spatial relations between neighboring text regions. The key information extraction is solved by iteratively propagating messages along graph edges and reasoning the categories of graph nodes. In order to roundly evaluate our proposed method as well as boost the future research, we release a new dataset named WildReceipt, which is collected and annotated tailored for the evaluation of key information extraction from document images of unseen templates in the wild. It contains 25 key information categories, a total of about 69000 text boxes, and is about 2 times larger than the existing public datasets. Extensive experiments validate that all information including visual features, textual features and spatial relations can benefit key information extraction. It has been shown that SDMG-R can effectively extract key information from document images of unseen templates, and obtain new state-of-the-art results on the recent popular benchmark SROIE and our WildReceipt. Our code and dataset will be publicly released.



### Multiple GAN Inversion for Exemplar-based Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2103.14471v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14471v2)
- **Published**: 2021-03-26 13:46:14+00:00
- **Updated**: 2021-08-19 17:17:15+00:00
- **Authors**: Taewon Kang
- **Comment**: Accepted to ICCV 2021 Workshop, 3rd International Workshop on
  Real-World Computer Vision From Inputs With Limited Quality (RLQ). Check out
  the project page for more information: http://itsc.kr/2021/06/24/mgi2021/, 9
  pages, 10 figures, v2: corrected typos, extended version of arXiv:2011.09330
- **Journal**: None
- **Summary**: Existing state-of-the-art techniques in exemplar-based image-to-image translation hold several critical concerns. Existing methods related to exemplar-based image-to-image translation are impossible to translate on an image tuple input (source, target) that is not aligned. Additionally, we can confirm that the existing method exhibits limited generalization ability to unseen images. In order to overcome this limitation, we propose Multiple GAN Inversion for Exemplar-based Image-to-Image Translation. Our novel Multiple GAN Inversion avoids human intervention by using a self-deciding algorithm to choose the number of layers using Fr\'echet Inception Distance(FID), which selects more plausible image reconstruction results among multiple hypotheses without any training or supervision. Experimental results have in fact, shown the advantage of the proposed method compared to existing state-of-the-art exemplar-based image-to-image translation methods.



### Distilling a Powerful Student Model via Online Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2103.14473v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14473v3)
- **Published**: 2021-03-26 13:54:24+00:00
- **Updated**: 2022-02-17 02:47:29+00:00
- **Authors**: Shaojie Li, Mingbao Lin, Yan Wang, Yongjian Wu, Yonghong Tian, Ling Shao, Rongrong Ji
- **Comment**: Accepted by IEEE Transactions on Neural Networks and Learning Systems
  (IEEE TNNLS)
- **Journal**: None
- **Summary**: Existing online knowledge distillation approaches either adopt the student with the best performance or construct an ensemble model for better holistic performance. However, the former strategy ignores other students' information, while the latter increases the computational complexity during deployment. In this paper, we propose a novel method for online knowledge distillation, termed FFSD, which comprises two key components: Feature Fusion and Self-Distillation, towards solving the above problems in a unified framework. Different from previous works, where all students are treated equally, the proposed FFSD splits them into a leader student and a common student set. Then, the feature fusion module converts the concatenation of feature maps from all common students into a fused feature map. The fused representation is used to assist the learning of the leader student. To enable the leader student to absorb more diverse information, we design an enhancement strategy to increase the diversity among students. Besides, a self-distillation module is adopted to convert the feature map of deeper layers into a shallower one. Then, the shallower layers are encouraged to mimic the transformed feature maps of the deeper layers, which helps the students to generalize better. After training, we simply adopt the leader student, which achieves superior performance, over the common students, without increasing the storage or inference cost. Extensive experiments on CIFAR-100 and ImageNet demonstrate the superiority of our FFSD over existing works. The code is available at https://github.com/SJLeo/FFSD.



### Contrastive Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2103.15566v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.15566v1)
- **Published**: 2021-03-26 13:55:19+00:00
- **Updated**: 2021-03-26 13:55:19+00:00
- **Authors**: Mamatha Thota, Georgios Leontidis
- **Comment**: 10 pages, 6 figures, 5 tables
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) Workshops, 2021, pp. 2209-2218
- **Summary**: Recently, contrastive self-supervised learning has become a key component for learning visual representations across many computer vision tasks and benchmarks. However, contrastive learning in the context of domain adaptation remains largely underexplored. In this paper, we propose to extend contrastive learning to a new domain adaptation setting, a particular situation occurring where the similarity is learned and deployed on samples following different probability distributions without access to labels. Contrastive learning learns by comparing and contrasting positive and negative pairs of samples in an unsupervised setting without access to source and target labels. We have developed a variation of a recently proposed contrastive learning framework that helps tackle the domain adaptation problem, further identifying and removing possible negatives similar to the anchor to mitigate the effects of false negatives. Extensive experiments demonstrate that the proposed method adapts well, and improves the performance on the downstream domain adaptation task.



### Distilling Object Detectors via Decoupled Features
- **Arxiv ID**: http://arxiv.org/abs/2103.14475v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14475v1)
- **Published**: 2021-03-26 13:58:49+00:00
- **Updated**: 2021-03-26 13:58:49+00:00
- **Authors**: Jianyuan Guo, Kai Han, Yunhe Wang, Han Wu, Xinghao Chen, Chunjing Xu, Chang Xu
- **Comment**: Accepted in CVPR 2021
- **Journal**: None
- **Summary**: Knowledge distillation is a widely used paradigm for inheriting information from a complicated teacher network to a compact student network and maintaining the strong performance. Different from image classification, object detectors are much more sophisticated with multiple loss functions in which features that semantic information rely on are tangled. In this paper, we point out that the information of features derived from regions excluding objects are also essential for distilling the student detector, which is usually ignored in existing approaches. In addition, we elucidate that features from different regions should be assigned with different importance during distillation. To this end, we present a novel distillation algorithm via decoupled features (DeFeat) for learning a better student detector. Specifically, two levels of decoupled features will be processed for embedding useful information into the student, i.e., decoupled features from neck and decoupled proposals from classification head. Extensive experiments on various detectors with different backbones show that the proposed DeFeat is able to surpass the state-of-the-art distillation methods for object detection. For example, DeFeat improves ResNet50 based Faster R-CNN from 37.4% to 40.9% mAP, and improves ResNet50 based RetinaNet from 36.5% to 39.7% mAP on COCO benchmark. Our implementation is available at https://github.com/ggjy/DeFeat.pytorch.



### Weakly-Supervised Domain Adaptation of Deep Regression Trackers via Reinforced Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2103.14496v1
- **DOI**: 10.1109/LRA.2021.3070816
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.14496v1)
- **Published**: 2021-03-26 14:37:33+00:00
- **Updated**: 2021-03-26 14:37:33+00:00
- **Authors**: Matteo Dunnhofer, Niki Martinel, Christian Micheloni
- **Comment**: IEEE Robotics and Automation Letters (RA-L)
- **Journal**: None
- **Summary**: Deep regression trackers are among the fastest tracking algorithms available, and therefore suitable for real-time robotic applications. However, their accuracy is inadequate in many domains due to distribution shift and overfitting. In this paper we overcome such limitations by presenting the first methodology for domain adaption of such a class of trackers. To reduce the labeling effort we propose a weakly-supervised adaptation strategy, in which reinforcement learning is used to express weak supervision as a scalar application-dependent and temporally-delayed feedback. At the same time, knowledge distillation is employed to guarantee learning stability and to compress and transfer knowledge from more powerful but slower trackers. Extensive experiments on five different robotic vision domains demonstrate the relevance of our methodology. Real-time speed is achieved on embedded devices and on machines without GPUs, while accuracy reaches significant results.



### Agent with Warm Start and Adaptive Dynamic Termination for Plane Localization in 3D Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/2103.14502v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.14502v1)
- **Published**: 2021-03-26 14:57:26+00:00
- **Updated**: 2021-03-26 14:57:26+00:00
- **Authors**: Xin Yang, Haoran Dou, Ruobing Huang, Wufeng Xue, Yuhao Huang, Jikuan Qian, Yuanji Zhang, Huanjia Luo, Huizhi Guo, Tianfu Wang, Yi Xiong, Dong Ni
- **Comment**: Accepted by IEEE Transactions on Medical Imaging (12 pages, 8
  figures, 11 tabels)
- **Journal**: None
- **Summary**: Accurate standard plane (SP) localization is the fundamental step for prenatal ultrasound (US) diagnosis. Typically, dozens of US SPs are collected to determine the clinical diagnosis. 2D US has to perform scanning for each SP, which is time-consuming and operator-dependent. While 3D US containing multiple SPs in one shot has the inherent advantages of less user-dependency and more efficiency. Automatically locating SP in 3D US is very challenging due to the huge search space and large fetal posture variations. Our previous study proposed a deep reinforcement learning (RL) framework with an alignment module and active termination to localize SPs in 3D US automatically. However, termination of agent search in RL is important and affects the practical deployment. In this study, we enhance our previous RL framework with a newly designed adaptive dynamic termination to enable an early stop for the agent searching, saving at most 67% inference time, thus boosting the accuracy and efficiency of the RL framework at the same time. Besides, we validate the effectiveness and generalizability of our algorithm extensively on our in-house multi-organ datasets containing 433 fetal brain volumes, 519 fetal abdomen volumes, and 683 uterus volumes. Our approach achieves localization error of 2.52mm/10.26 degrees, 2.48mm/10.39 degrees, 2.02mm/10.48 degrees, 2.00mm/14.57 degrees, 2.61mm/9.71 degrees, 3.09mm/9.58 degrees, 1.49mm/7.54 degrees for the transcerebellar, transventricular, transthalamic planes in fetal brain, abdominal plane in fetal abdomen, and mid-sagittal, transverse and coronal planes in uterus, respectively. Experimental results show that our method is general and has the potential to improve the efficiency and standardization of US scanning.



### On the hidden treasure of dialog in video question answering
- **Arxiv ID**: http://arxiv.org/abs/2103.14517v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14517v2)
- **Published**: 2021-03-26 15:17:01+00:00
- **Updated**: 2021-08-19 12:13:27+00:00
- **Authors**: Deniz Engin, François Schnitzler, Ngoc Q. K. Duong, Yannis Avrithis
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: High-level understanding of stories in video such as movies and TV shows from raw data is extremely challenging. Modern video question answering (VideoQA) systems often use additional human-made sources like plot synopses, scripts, video descriptions or knowledge bases. In this work, we present a new approach to understand the whole story without such external sources. The secret lies in the dialog: unlike any prior work, we treat dialog as a noisy source to be converted into text description via dialog summarization, much like recent methods treat video. The input of each modality is encoded by transformers independently, and a simple fusion method combines all modalities, using soft temporal attention for localization over long inputs. Our model outperforms the state of the art on the KnowIT VQA dataset by a large margin, without using question-specific human annotation or human-made plot summaries. It even outperforms human evaluators who have never watched any whole episode before. Code is available at https://engindeniz.github.io/dialogsummary-videoqa



### Model-based Reconstruction with Learning: From Unsupervised to Supervised and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2103.14528v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.14528v1)
- **Published**: 2021-03-26 15:33:59+00:00
- **Updated**: 2021-03-26 15:33:59+00:00
- **Authors**: Zhishen Huang, Siqi Ye, Michael T. McCann, Saiprasad Ravishankar
- **Comment**: None
- **Journal**: None
- **Summary**: Many techniques have been proposed for image reconstruction in medical imaging that aim to recover high-quality images especially from limited or corrupted measurements. Model-based reconstruction methods have been particularly popular (e.g., in magnetic resonance imaging and tomographic modalities) and exploit models of the imaging system's physics together with statistical models of measurements, noise and often relatively simple object priors or regularizers. For example, sparsity or low-rankness based regularizers have been widely used for image reconstruction from limited data such as in compressed sensing. Learning-based approaches for image reconstruction have garnered much attention in recent years and have shown promise across biomedical imaging applications. These methods include synthesis dictionary learning, sparsifying transform learning, and different forms of deep learning involving complex neural networks. We briefly discuss classical model-based reconstruction methods and then review reconstruction methods at the intersection of model-based and learning-based paradigms in detail. This review includes many recent methods based on unsupervised learning, and supervised learning, as well as a framework to combine multiple types of learned models together.



### 3D Point Cloud Registration with Multi-Scale Architecture and Unsupervised Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.14533v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14533v2)
- **Published**: 2021-03-26 15:38:33+00:00
- **Updated**: 2021-10-14 15:44:33+00:00
- **Authors**: Sofiane Horache, Jean-Emmanuel Deschaud, François Goulette
- **Comment**: Accepted to 3DV 2021
- **Journal**: None
- **Summary**: We propose a method for generalizing deep learning for 3D point cloud registration on new, totally different datasets. It is based on two components, MS-SVConv and UDGE. Using Multi-Scale Sparse Voxel Convolution, MS-SVConv is a fast deep neural network that outputs the descriptors from point clouds for 3D registration between two scenes. UDGE is an algorithm for transferring deep networks on unknown datasets in a unsupervised way. The interest of the proposed method appears while using the two components, MS-SVConv and UDGE, together as a whole, which leads to state-of-the-art results on real world registration datasets such as 3DMatch, ETH and TUM. The code is publicly available at https://github.com/humanpose1/MS-SVConv .



### Detection, growth quantification and malignancy prediction of pulmonary nodules using deep convolutional networks in follow-up CT scans
- **Arxiv ID**: http://arxiv.org/abs/2103.14537v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 65D19, 68T10, I.3; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2103.14537v1)
- **Published**: 2021-03-26 15:41:37+00:00
- **Updated**: 2021-03-26 15:41:37+00:00
- **Authors**: Xavier Rafael-Palou, Anton Aubanell, Mario Ceresa, Vicent Ribas, Gemma Piella, Miguel A. González Ballester
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of supporting radiologists in the longitudinal management of lung cancer. Therefore, we proposed a deep learning pipeline, composed of four stages that completely automatized from the detection of nodules to the classification of cancer, through the detection of growth in the nodules. In addition, the pipeline integrated a novel approach for nodule growth detection, which relied on a recent hierarchical probabilistic U-Net adapted to report uncertainty estimates. Also, a second novel method was introduced for lung cancer nodule classification, integrating into a two stream 3D-CNN network the estimated nodule malignancy probabilities derived from a pretrained nodule malignancy network. The pipeline was evaluated in a longitudinal cohort and reported comparable performances to the state of art.



### DivAug: Plug-in Automated Data Augmentation with Explicit Diversity Maximization
- **Arxiv ID**: http://arxiv.org/abs/2103.14545v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14545v2)
- **Published**: 2021-03-26 16:00:01+00:00
- **Updated**: 2021-08-11 19:32:42+00:00
- **Authors**: Zirui Liu, Haifeng Jin, Ting-Hsiang Wang, Kaixiong Zhou, Xia Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Human-designed data augmentation strategies have been replaced by automatically learned augmentation policy in the past two years. Specifically, recent work has empirically shown that the superior performance of the automated data augmentation methods stems from increasing the diversity of augmented data \cite{autoaug, randaug}. However, two factors regarding the diversity of augmented data are still missing: 1) the explicit definition (and thus measurement) of diversity and 2) the quantifiable relationship between diversity and its regularization effects. To bridge this gap, we propose a diversity measure called Variance Diversity and theoretically show that the regularization effect of data augmentation is promised by Variance Diversity. We validate in experiments that the relative gain from automated data augmentation in test accuracy is highly correlated to Variance Diversity. An unsupervised sampling-based framework, \textbf{DivAug}, is designed to directly maximize Variance Diversity and hence strengthen the regularization effect. Without requiring a separate search process, the performance gain from DivAug is comparable with the state-of-the-art method with better efficiency. Moreover, under the semi-supervised setting, our framework can further improve the performance of semi-supervised learning algorithms compared to RandAugment, making it highly applicable to real-world problems, where labeled data is scarce. The code is available at \texttt{\url{https://github.com/warai-0toko/DivAug}}.



### Sparse Object-level Supervision for Instance Segmentation with Pixel Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2103.14572v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14572v3)
- **Published**: 2021-03-26 16:36:56+00:00
- **Updated**: 2022-04-04 16:36:21+00:00
- **Authors**: Adrian Wolny, Qin Yu, Constantin Pape, Anna Kreshuk
- **Comment**: CVPR 2022 camera-ready
- **Journal**: None
- **Summary**: Most state-of-the-art instance segmentation methods have to be trained on densely annotated images. While difficult in general, this requirement is especially daunting for biomedical images, where domain expertise is often required for annotation and no large public data collections are available for pre-training. We propose to address the dense annotation bottleneck by introducing a proposal-free segmentation approach based on non-spatial embeddings, which exploits the structure of the learned embedding space to extract individual instances in a differentiable way. The segmentation loss can then be applied directly to instances and the overall pipeline can be trained in a fully- or weakly supervised manner. We consider the challenging case of positive-unlabeled supervision, where a novel self-supervised consistency loss is introduced for the unlabeled parts of the training data. We evaluate the proposed method on 2D and 3D segmentation problems in different microscopy modalities as well as on the Cityscapes and CVPPP instance segmentation benchmarks, achieving state-of-the-art results on the latter. The code is available at: https://github.com/kreshuklab/spoco



### Unsupervised Robust Domain Adaptation without Source Data
- **Arxiv ID**: http://arxiv.org/abs/2103.14577v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14577v1)
- **Published**: 2021-03-26 16:42:28+00:00
- **Updated**: 2021-03-26 16:42:28+00:00
- **Authors**: Peshal Agarwal, Danda Pani Paudel, Jan-Nico Zaech, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of robust domain adaptation in the context of unavailable target labels and source data. The considered robustness is against adversarial perturbations. This paper aims at answering the question of finding the right strategy to make the target model robust and accurate in the setting of unsupervised domain adaptation without source data. The major findings of this paper are: (i) robust source models can be transferred robustly to the target; (ii) robust domain adaptation can greatly benefit from non-robust pseudo-labels and the pair-wise contrastive loss. The proposed method of using non-robust pseudo-labels performs surprisingly well on both clean and adversarial samples, for the task of image classification. We show a consistent performance improvement of over $10\%$ in accuracy against the tested baselines on four benchmark datasets.



### GeoSP: A parallel method for a cortical surface parcellation based on geodesic distance
- **Arxiv ID**: http://arxiv.org/abs/2103.14579v1
- **DOI**: 10.1109/EMBC44109.2020.9175779
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14579v1)
- **Published**: 2021-03-26 16:43:04+00:00
- **Updated**: 2021-03-26 16:43:04+00:00
- **Authors**: Narciso López-López, Andrea Vázquez, Cyril Poupon, Jean-François Mangin, Susana Ladra, Pamela Guevara
- **Comment**: This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sklodowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941, ANID PFCHA/DOCTORADO
  NACIONAL/2016-21160342, ANID FONDECYT 1190701, ANID PIA/Anillo de
  Investigaci\'on en Ciencia y Tecnolog\'ia ACT172121, and ANID Basal Project
  FB0008
- **Journal**: None
- **Summary**: We present GeoSP, a parallel method that creates a parcellation of the cortical mesh based on a geodesic distance, in order to consider gyri and sulci topology. The method represents the mesh with a graph and performs a K-means clustering in parallel. It has two modes of use, by default, it performs the geodesic cortical parcellation based on the boundaries of the anatomical parcels provided by the Desikan-Killiany atlas. The other mode performs the complete parcellation of the cortex. Results for both modes and with different values for the total number of sub-parcels show homogeneous sub-parcels. Furthermore, the execution time is 82 s for the whole cortex mode and 18 s for the Desikan-Killiany atlas subdivision, for a parcellation into 350 sub-parcels. The proposed method will be available to the community to perform the evaluation of data-driven cortical parcellations. As an example, we compared GeoSP parcellation with Desikan-Killiany and Destrieux atlases in 50 subjects, obtaining more homogeneous parcels for GeoSP and minor differences in structural connectivity reproducibility across subjects.



### Non-Salient Region Object Mining for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.14581v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14581v1)
- **Published**: 2021-03-26 16:44:03+00:00
- **Updated**: 2021-03-26 16:44:03+00:00
- **Authors**: Yazhou Yao, Tao Chen, Guosen Xie, Chuanyi Zhang, Fumin Shen, Qi Wu, Zhenmin Tang, Jian Zhang
- **Comment**: accepted by IEEE Conference on Computer Vision and Pattern
  Recognition, 2021
- **Journal**: None
- **Summary**: Semantic segmentation aims to classify every pixel of an input image. Considering the difficulty of acquiring dense labels, researchers have recently been resorting to weak labels to alleviate the annotation burden of segmentation. However, existing works mainly concentrate on expanding the seed of pseudo labels within the image's salient region. In this work, we propose a non-salient region object mining approach for weakly supervised semantic segmentation. We introduce a graph-based global reasoning unit to strengthen the classification network's ability to capture global relations among disjoint and distant regions. This helps the network activate the object features outside the salient area. To further mine the non-salient region objects, we propose to exert the segmentation network's self-correction ability. Specifically, a potential object mining module is proposed to reduce the false-negative rate in pseudo labels. Moreover, we propose a non-salient region masking module for complex images to generate masked pseudo labels. Our non-salient region masking module helps further discover the objects in the non-salient region. Extensive experiments on the PASCAL VOC dataset demonstrate state-of-the-art results compared to current methods.



### Understanding Robustness of Transformers for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2103.14586v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14586v2)
- **Published**: 2021-03-26 16:47:55+00:00
- **Updated**: 2021-10-08 15:28:11+00:00
- **Authors**: Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, Andreas Veit
- **Comment**: Accepted for publication at ICCV 2021. Rewrote Section 5 and made
  other minor changes throughout
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNNs) have long been the architecture of choice for computer vision tasks. Recently, Transformer-based architectures like Vision Transformer (ViT) have matched or even surpassed ResNets for image classification. However, details of the Transformer architecture -- such as the use of non-overlapping patches -- lead one to wonder whether these networks are as robust. In this paper, we perform an extensive study of a variety of different measures of robustness of ViT models and compare the findings to ResNet baselines. We investigate robustness to input perturbations as well as robustness to model perturbations. We find that when pre-trained with a sufficient amount of data, ViT models are at least as robust as the ResNet counterparts on a broad range of perturbations. We also find that Transformers are robust to the removal of almost any single layer, and that while activations from later layers are highly correlated with each other, they nevertheless play an important role in classification.



### Data Quality as Predictor of Voice Anti-Spoofing Generalization
- **Arxiv ID**: http://arxiv.org/abs/2103.14602v2
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2103.14602v2)
- **Published**: 2021-03-26 17:09:06+00:00
- **Updated**: 2021-06-21 20:53:23+00:00
- **Authors**: Bhusan Chettri, Rosa González Hautamäki, Md Sahidullah, Tomi Kinnunen
- **Comment**: INTERSPEECH 2021
- **Journal**: None
- **Summary**: Voice anti-spoofing aims at classifying a given utterance either as a bonafide human sample, or a spoofing attack (e.g. synthetic or replayed sample). Many anti-spoofing methods have been proposed but most of them fail to generalize across domains (corpora) -- and we do not know \emph{why}. We outline a novel interpretative framework for gauging the impact of data quality upon anti-spoofing performance. Our within- and between-domain experiments pool data from seven public corpora and three anti-spoofing methods based on Gaussian mixture and convolutive neural network models. We assess the impacts of long-term spectral information, speaker population (through x-vector speaker embeddings), signal-to-noise ratio, and selected voice quality features.



### Training a Task-Specific Image Reconstruction Loss
- **Arxiv ID**: http://arxiv.org/abs/2103.14616v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.14616v2)
- **Published**: 2021-03-26 17:29:57+00:00
- **Updated**: 2021-10-17 08:14:08+00:00
- **Authors**: Aamir Mustafa, Aliaksei Mikhailiuk, Dan Andrei Iliescu, Varun Babbar, Rafal K. Mantiuk
- **Comment**: Accepted at WACV 2022
- **Journal**: None
- **Summary**: The choice of a loss function is an important factor when training neural networks for image restoration problems, such as single image super resolution. The loss function should encourage natural and perceptually pleasing results. A popular choice for a loss is a pre-trained network, such as VGG, which is used as a feature extractor for computing the difference between restored and reference images. However, such an approach has multiple drawbacks: it is computationally expensive, requires regularization and hyper-parameter tuning, and involves a large network trained on an unrelated task. Furthermore, it has been observed that there is no single loss function that works best across all applications and across different datasets. In this work, we instead propose to train a set of loss functions that are application specific in nature. Our loss function comprises a series of discriminators that are trained to detect and penalize the presence of application-specific artifacts. We show that a single natural image and corresponding distortions are sufficient to train our feature extractor that outperforms state-of-the-art loss functions in applications like single image super resolution, denoising, and JPEG artifact removal. Finally, we conclude that an effective loss function does not have to be a good predictor of perceived image quality, but instead needs to be specialized in identifying the distortions for a given restoration method.



### Visionary: Vision architecture discovery for robot learning
- **Arxiv ID**: http://arxiv.org/abs/2103.14633v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2103.14633v1)
- **Published**: 2021-03-26 17:51:43+00:00
- **Updated**: 2021-03-26 17:51:43+00:00
- **Authors**: Iretiayo Akinola, Anelia Angelova, Yao Lu, Yevgen Chebotar, Dmitry Kalashnikov, Jacob Varley, Julian Ibarz, Michael S. Ryoo
- **Comment**: None
- **Journal**: ICRA 2021
- **Summary**: We propose a vision-based architecture search algorithm for robot manipulation learning, which discovers interactions between low dimension action inputs and high dimensional visual inputs. Our approach automatically designs architectures while training on the task - discovering novel ways of combining and attending image feature representations with actions as well as features from previous layers. The obtained new architectures demonstrate better task success rates, in some cases with a large margin, compared to a recent high performing baseline. Our real robot experiments also confirm that it improves grasping performance by 6%. This is the first approach to demonstrate a successful neural architecture search and attention connectivity search for a real-robot task.



### PAConv: Position Adaptive Convolution with Dynamic Kernel Assembling on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2103.14635v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14635v2)
- **Published**: 2021-03-26 17:52:38+00:00
- **Updated**: 2021-04-26 06:50:57+00:00
- **Authors**: Mutian Xu, Runyu Ding, Hengshuang Zhao, Xiaojuan Qi
- **Comment**: To be appear in CVPR2021
- **Journal**: None
- **Summary**: We introduce Position Adaptive Convolution (PAConv), a generic convolution operation for 3D point cloud processing. The key of PAConv is to construct the convolution kernel by dynamically assembling basic weight matrices stored in Weight Bank, where the coefficients of these weight matrices are self-adaptively learned from point positions through ScoreNet. In this way, the kernel is built in a data-driven manner, endowing PAConv with more flexibility than 2D convolutions to better handle the irregular and unordered point cloud data. Besides, the complexity of the learning process is reduced by combining weight matrices instead of brutally predicting kernels from point positions.   Furthermore, different from the existing point convolution operators whose network architectures are often heavily engineered, we integrate our PAConv into classical MLP-based point cloud pipelines without changing network configurations. Even built on simple networks, our method still approaches or even surpasses the state-of-the-art models, and significantly improves baseline performance on both classification and segmentation tasks, yet with decent efficiency. Thorough ablation studies and visualizations are provided to understand PAConv. Code is released on https://github.com/CVMI-Lab/PAConv.



### On Generating Transferable Targeted Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2103.14641v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14641v2)
- **Published**: 2021-03-26 17:55:28+00:00
- **Updated**: 2021-08-13 19:05:41+00:00
- **Authors**: Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Fatih Porikli
- **Comment**: ICCV, 2021. Code is available at
  https://github.com/Muzammal-Naseer/TTP
- **Journal**: None
- **Summary**: While the untargeted black-box transferability of adversarial perturbations has been extensively studied before, changing an unseen model's decisions to a specific `targeted' class remains a challenging feat. In this paper, we propose a new generative approach for highly transferable targeted perturbations (\ours). We note that the existing methods are less suitable for this task due to their reliance on class-boundary information that changes from one model to another, thus reducing transferability. In contrast, our approach matches the perturbed image `distribution' with that of the target class, leading to high targeted transferability rates. To this end, we propose a new objective function that not only aligns the global distributions of source and target images, but also matches the local neighbourhood structure between the two domains. Based on the proposed objective, we train a generator function that can adaptively synthesize perturbations specific to a given input. Our generative approach is independent of the source or target domain labels, while consistently performs well against state-of-the-art methods on a wide range of attack settings. As an example, we achieve $32.63\%$ target transferability from (an adversarially weak) VGG19$_{BN}$ to (a strong) WideResNet on ImageNet val. set, which is 4$\times$ higher than the previous best generative attack and 16$\times$ better than instance-specific iterative attack. Code is available at: {\small\url{https://github.com/Muzammal-Naseer/TTP}}.



### Planar Surface Reconstruction from Sparse Views
- **Arxiv ID**: http://arxiv.org/abs/2103.14644v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14644v2)
- **Published**: 2021-03-26 17:59:20+00:00
- **Updated**: 2021-08-20 17:59:25+00:00
- **Authors**: Linyi Jin, Shengyi Qian, Andrew Owens, David F. Fouhey
- **Comment**: Accepted to ICCV 2021 (Oral Presentation)
- **Journal**: None
- **Summary**: The paper studies planar surface reconstruction of indoor scenes from two views with unknown camera poses. While prior approaches have successfully created object-centric reconstructions of many scenes, they fail to exploit other structures, such as planes, which are typically the dominant components of indoor scenes. In this paper, we reconstruct planar surfaces from multiple views, while jointly estimating camera pose. Our experiments demonstrate that our method is able to advance the state of the art of reconstruction from sparse views, on challenging scenes from Matterport3D. Project site: https://jinlinyi.github.io/SparsePlanes/



### Baking Neural Radiance Fields for Real-Time View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2103.14645v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2103.14645v1)
- **Published**: 2021-03-26 17:59:52+00:00
- **Updated**: 2021-03-26 17:59:52+00:00
- **Authors**: Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, Paul Debevec
- **Comment**: Project page: https://nerf.live
- **Journal**: None
- **Summary**: Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF's computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. "bake") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF's ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.



### Quantum Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.14653v3
- **DOI**: None
- **Categories**: **quant-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14653v3)
- **Published**: 2021-03-26 18:00:00+00:00
- **Updated**: 2022-04-04 09:51:08+00:00
- **Authors**: Ben Jaderberg, Lewis W. Anderson, Weidi Xie, Samuel Albanie, Martin Kiffner, Dieter Jaksch
- **Comment**: 13 pages, 10 figures. Additional results and discussion
- **Journal**: None
- **Summary**: The resurgence of self-supervised learning, whereby a deep learning model generates its own supervisory signal from the data, promises a scalable way to tackle the dramatically increasing size of real-world data sets without human annotation. However, the staggering computational complexity of these methods is such that for state-of-the-art performance, classical hardware requirements represent a significant bottleneck to further progress. Here we take the first steps to understanding whether quantum neural networks could meet the demand for more powerful architectures and test its effectiveness in proof-of-principle hybrid experiments. Interestingly, we observe a numerical advantage for the learning of visual representations using small-scale quantum neural networks over equivalently structured classical networks, even when the quantum circuits are sampled with only 100 shots. Furthermore, we apply our best quantum model to classify unseen images on the ibmq\_paris quantum computer and find that current noisy devices can already achieve equal accuracy to the equivalent classical model on downstream tasks.



### Multi-Modal RGB-D Scene Recognition Across Domains
- **Arxiv ID**: http://arxiv.org/abs/2103.14672v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.14672v2)
- **Published**: 2021-03-26 18:20:29+00:00
- **Updated**: 2021-09-07 14:48:58+00:00
- **Authors**: Andrea Ferreri, Silvia Bucci, Tatiana Tommasi
- **Comment**: Accepted at Deep Multi-Task Learning in Computer Vision (DeepMTL)
  workshop, ICCV 2021
- **Journal**: None
- **Summary**: Scene recognition is one of the basic problems in computer vision research with extensive applications in robotics. When available, depth images provide helpful geometric cues that complement the RGB texture information and help to identify discriminative scene image features. Depth sensing technology developed fast in the last years and a great variety of 3D cameras have been introduced, each with different acquisition properties. However, those properties are often neglected when targeting big data collections, so multi-modal images are gathered disregarding their original nature. In this work, we put under the spotlight the existence of a possibly severe domain shift issue within multi-modality scene recognition datasets. As a consequence, a scene classification model trained on one camera may not generalize on data from a different camera, only providing a low recognition performance. Starting from the well-known SUN RGB-D dataset, we designed an experimental testbed to study this problem and we use it to benchmark the performance of existing methods. Finally, we introduce a novel adaptive scene recognition approach that leverages self-supervised translation between modalities. Indeed, learning to go from RGB to depth and vice-versa is an unsupervised procedure that can be trained jointly on data of multiple cameras and may help to bridge the gap among the extracted feature distributions. Our experimental results confirm the effectiveness of the proposed approach.



### Synthesis of Compositional Animations from Textual Descriptions
- **Arxiv ID**: http://arxiv.org/abs/2103.14675v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14675v6)
- **Published**: 2021-03-26 18:23:29+00:00
- **Updated**: 2023-01-23 11:17:09+00:00
- **Authors**: Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian Theobalt, Philipp Slusallek
- **Comment**: 13 pages, 6 figures, 3 tables. Proceedings of the IEEE/CVF
  International Conference on Computer Vision (ICCV), 2021, pp. 1396-1406
- **Journal**: None
- **Summary**: "How can we animate 3D-characters from a movie script or move robots by simply telling them what we would like them to do?" "How unstructured and complex can we make a sentence and still generate plausible movements from it?" These are questions that need to be answered in the long-run, as the field is still in its infancy. Inspired by these problems, we present a new technique for generating compositional actions, which handles complex input sentences. Our output is a 3D pose sequence depicting the actions in the input sentence. We propose a hierarchical two-stream sequential model to explore a finer joint-level mapping between natural language sentences and 3D pose sequences corresponding to the given motion. We learn two manifold representations of the motion -- one each for the upper body and the lower body movements. Our model can generate plausible pose sequences for short sentences describing single actions as well as long compositional sentences describing multiple sequential and superimposed actions. We evaluate our proposed model on the publicly available KIT Motion-Language Dataset containing 3D pose data with human-annotated sentences. Experimental results show that our model advances the state-of-the-art on text-based motion synthesis in objective evaluations by a margin of 50%. Qualitative evaluations based on a user study indicate that our synthesized motions are perceived to be the closest to the ground-truth motion captures for both short and compositional sentences.



### Focused LRP: Explainable AI for Face Morphing Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.14697v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14697v1)
- **Published**: 2021-03-26 19:05:01+00:00
- **Updated**: 2021-03-26 19:05:01+00:00
- **Authors**: Clemens Seibold, Anna Hilsmann, Peter Eisert
- **Comment**: Published at WACVW 2021
- **Journal**: None
- **Summary**: The task of detecting morphed face images has become highly relevant in recent years to ensure the security of automatic verification systems based on facial images, e.g. automated border control gates. Detection methods based on Deep Neural Networks (DNN) have been shown to be very suitable to this end. However, they do not provide transparency in the decision making and it is not clear how they distinguish between genuine and morphed face images. This is particularly relevant for systems intended to assist a human operator, who should be able to understand the reasoning. In this paper, we tackle this problem and present Focused Layer-wise Relevance Propagation (FLRP). This framework explains to a human inspector on a precise pixel level, which image regions are used by a Deep Neural Network to distinguish between a genuine and a morphed face image. Additionally, we propose another framework to objectively analyze the quality of our method and compare FLRP to other DNN interpretability methods. This evaluation framework is based on removing detected artifacts and analyzing the influence of these changes on the decision of the DNN. Especially, if the DNN is uncertain in its decision or even incorrect, FLRP performs much better in highlighting visible artifacts compared to other methods.



### Tuning IR-cut Filter for Illumination-aware Spectral Reconstruction from RGB
- **Arxiv ID**: http://arxiv.org/abs/2103.14708v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.14708v1)
- **Published**: 2021-03-26 19:42:21+00:00
- **Updated**: 2021-03-26 19:42:21+00:00
- **Authors**: Bo Sun, Junchi Yan, Xiao Zhou, Yinqiang Zheng
- **Comment**: CVPR 2021 - Oral
- **Journal**: None
- **Summary**: To reconstruct spectral signals from multi-channel observations, in particular trichromatic RGBs, has recently emerged as a promising alternative to traditional scanning-based spectral imager. It has been proven that the reconstruction accuracy relies heavily on the spectral response of the RGB camera in use. To improve accuracy, data-driven algorithms have been proposed to retrieve the best response curves of existing RGB cameras, or even to design brand new three-channel response curves. Instead, this paper explores the filter-array based color imaging mechanism of existing RGB cameras, and proposes to design the IR-cut filter properly for improved spectral recovery, which stands out as an in-between solution with better trade-off between reconstruction accuracy and implementation complexity. We further propose a deep learning based spectral reconstruction method, which allows to recover the illumination spectrum as well. Experiment results with both synthetic and real images under daylight illumination have shown the benefits of our IR-cut filter tuning method and our illumination-aware spectral reconstruction method.



### Generating and Evaluating Explanations of Attended and Error-Inducing Input Regions for VQA Models
- **Arxiv ID**: http://arxiv.org/abs/2103.14712v3
- **DOI**: 10.22541/au.162464902.28050142/v1
- **Categories**: **cs.CV**, cs.AI, cs.CY, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2103.14712v3)
- **Published**: 2021-03-26 19:52:32+00:00
- **Updated**: 2021-10-25 18:58:42+00:00
- **Authors**: Arijit Ray, Michael Cogswell, Xiao Lin, Kamran Alipour, Ajay Divakaran, Yi Yao, Giedrius Burachas
- **Comment**: Applied AI Letters, Wiley, 25 October 2021
- **Journal**: None
- **Summary**: Attention maps, a popular heatmap-based explanation method for Visual Question Answering (VQA), are supposed to help users understand the model by highlighting portions of the image/question used by the model to infer answers. However, we see that users are often misled by current attention map visualizations that point to relevant regions despite the model producing an incorrect answer. Hence, we propose Error Maps that clarify the error by highlighting image regions where the model is prone to err. Error maps can indicate when a correctly attended region may be processed incorrectly leading to an incorrect answer, and hence, improve users' understanding of those cases. To evaluate our new explanations, we further introduce a metric that simulates users' interpretation of explanations to evaluate their potential helpfulness to understand model correctness. We finally conduct user studies to see that our new explanations help users understand model correctness better than baselines by an expected 30\% and that our proxy helpfulness metrics correlate strongly ($\rho>0.97$) with how well users can predict model correctness.



### When Few-Shot Learning Meets Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.14724v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.14724v3)
- **Published**: 2021-03-26 20:37:55+00:00
- **Updated**: 2022-08-18 03:17:33+00:00
- **Authors**: Zhongjie Yu, Gaoang Wang, Lin Chen, Sebastian Raschka, Jiebo Luo
- **Comment**: Accepted at ICPR2022
- **Journal**: None
- **Summary**: Different from static images, videos contain additional temporal and spatial information for better object detection. However, it is costly to obtain a large number of videos with bounding box annotations that are required for supervised deep learning. Although humans can easily learn to recognize new objects by watching only a few video clips, deep learning usually suffers from overfitting. This leads to an important question: how to effectively learn a video object detector from only a few labeled video clips? In this paper, we study the new problem of few-shot learning for video object detection. We first define the few-shot setting and create a new benchmark dataset for few-shot video object detection derived from the widely used ImageNet VID dataset. We employ a transfer-learning framework to effectively train the video object detector on a large number of base-class objects and a few video clips of novel-class objects. By analyzing the results of two methods under this framework (Joint and Freeze) on our designed weak and strong base datasets, we reveal insufficiency and overfitting problems. A simple but effective method, called Thaw, is naturally developed to trade off the two problems and validate our analysis. Extensive experiments on our proposed benchmark datasets with different scenarios demonstrate the effectiveness of our novel analysis in this new few-shot video object detection problem.



### Fully Automated 2D and 3D Convolutional Neural Networks Pipeline for Video Segmentation and Myocardial Infarction Detection in Echocardiography
- **Arxiv ID**: http://arxiv.org/abs/2103.14734v2
- **DOI**: 10.1007/s11042-021-11579-4
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.14734v2)
- **Published**: 2021-03-26 21:03:33+00:00
- **Updated**: 2022-08-03 19:37:00+00:00
- **Authors**: Oumaima Hamila, Sheela Ramanna, Christopher J. Henry, Serkan Kiranyaz, Ridha Hamila, Rashid Mazhar, Tahir Hamid
- **Comment**: Multimed Tools Appl (2022)
- **Journal**: None
- **Summary**: Cardiac imaging known as echocardiography is a non-invasive tool utilized to produce data including images and videos, which cardiologists use to diagnose cardiac abnormalities in general and myocardial infarction (MI) in particular. Echocardiography machines can deliver abundant amounts of data that need to be quickly analyzed by cardiologists to help them make a diagnosis and treat cardiac conditions. However, the acquired data quality varies depending on the acquisition conditions and the patient's responsiveness to the setup instructions. These constraints are challenging to doctors especially when patients are facing MI and their lives are at stake. In this paper, we propose an innovative real-time end-to-end fully automated model based on convolutional neural networks (CNN) to detect MI depending on regional wall motion abnormalities (RWMA) of the left ventricle (LV) from videos produced by echocardiography. Our model is implemented as a pipeline consisting of a 2D CNN that performs data preprocessing by segmenting the LV chamber from the apical four-chamber (A4C) view, followed by a 3D CNN that performs a binary classification to detect if the segmented echocardiography shows signs of MI. We trained both CNNs on a dataset composed of 165 echocardiography videos each acquired from a distinct patient. The 2D CNN achieved an accuracy of 97.18% on data segmentation while the 3D CNN achieved 90.9% of accuracy, 100% of precision and 95% of recall on MI detection. Our results demonstrate that creating a fully automated system for MI detection is feasible and propitious.



### Equivariant Imaging: Learning Beyond the Range Space
- **Arxiv ID**: http://arxiv.org/abs/2103.14756v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2103.14756v2)
- **Published**: 2021-03-26 22:38:36+00:00
- **Updated**: 2021-08-23 19:51:53+00:00
- **Authors**: Dongdong Chen, Julián Tachella, Mike E. Davies
- **Comment**: ICCV 2021. Code: https://github.com/edongdongchen/EI
- **Journal**: None
- **Summary**: In various imaging problems, we only have access to compressed measurements of the underlying signals, hindering most learning-based strategies which usually require pairs of signals and associated measurements for training. Learning only from compressed measurements is impossible in general, as the compressed observations do not contain information outside the range of the forward sensing operator. We propose a new end-to-end self-supervised framework that overcomes this limitation by exploiting the equivariances present in natural signals. Our proposed learning strategy performs as well as fully supervised methods. Experiments demonstrate the potential of this framework on inverse problems including sparse-view X-ray computed tomography on real clinical data and image inpainting on natural images. Code has been made available at: https://github.com/edongdongchen/EI.



