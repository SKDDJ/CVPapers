# Arxiv Papers in cs.CV on 2021-03-13
### Revisiting ResNets: Improved Training and Scaling Strategies
- **Arxiv ID**: http://arxiv.org/abs/2103.07579v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.07579v1)
- **Published**: 2021-03-13 00:18:19+00:00
- **Updated**: 2021-03-13 00:18:19+00:00
- **Authors**: Irwan Bello, William Fedus, Xianzhi Du, Ekin D. Cubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens, Barret Zoph
- **Comment**: None
- **Journal**: None
- **Summary**: Novel computer vision architectures monopolize the spotlight, but the impact of the model architecture is often conflated with simultaneous changes to training methodology and scaling strategies. Our work revisits the canonical ResNet (He et al., 2015) and studies these three aspects in an effort to disentangle them. Perhaps surprisingly, we find that training and scaling strategies may matter more than architectural changes, and further, that the resulting ResNets match recent state-of-the-art models. We show that the best performing scaling strategy depends on the training regime and offer two new scaling strategies: (1) scale model depth in regimes where overfitting can occur (width scaling is preferable otherwise); (2) increase image resolution more slowly than previously recommended (Tan & Le, 2019). Using improved training and scaling strategies, we design a family of ResNet architectures, ResNet-RS, which are 1.7x - 2.7x faster than EfficientNets on TPUs, while achieving similar accuracies on ImageNet. In a large-scale semi-supervised learning setup, ResNet-RS achieves 86.2% top-1 ImageNet accuracy, while being 4.7x faster than EfficientNet NoisyStudent. The training techniques improve transfer performance on a suite of downstream tasks (rivaling state-of-the-art self-supervised algorithms) and extend to video classification on Kinetics-400. We recommend practitioners use these simple revised ResNets as baselines for future research.



### Student-Teacher Learning from Clean Inputs to Noisy Inputs
- **Arxiv ID**: http://arxiv.org/abs/2103.07600v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2103.07600v1)
- **Published**: 2021-03-13 02:29:35+00:00
- **Updated**: 2021-03-13 02:29:35+00:00
- **Authors**: Guanzhe Hong, Zhiyuan Mao, Xiaojun Lin, Stanley H. Chan
- **Comment**: Published at the Conference on Computer Vision and Pattern
  Recognition (CVPR 2021)
- **Journal**: None
- **Summary**: Feature-based student-teacher learning, a training method that encourages the student's hidden features to mimic those of the teacher network, is empirically successful in transferring the knowledge from a pre-trained teacher network to the student network. Furthermore, recent empirical results demonstrate that, the teacher's features can boost the student network's generalization even when the student's input sample is corrupted by noise. However, there is a lack of theoretical insights into why and when this method of transferring knowledge can be successful between such heterogeneous tasks. We analyze this method theoretically using deep linear networks, and experimentally using nonlinear networks. We identify three vital factors to the success of the method: (1) whether the student is trained to zero training loss; (2) how knowledgeable the teacher is on the clean-input problem; (3) how the teacher decomposes its knowledge in its hidden features. Lack of proper control in any of the three factors leads to failure of the student-teacher learning method.



### Untrained networks for compressive lensless photography
- **Arxiv ID**: http://arxiv.org/abs/2103.07609v2
- **DOI**: 10.1364/OE.424075
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2103.07609v2)
- **Published**: 2021-03-13 03:47:06+00:00
- **Updated**: 2021-06-22 01:01:25+00:00
- **Authors**: Kristina Monakhova, Vi Tran, Grace Kuo, Laura Waller
- **Comment**: 17 pages, 8 figures
- **Journal**: Optics Express Vol. 29, Issue 13, pp. 20913-20929 (2021)
- **Summary**: Compressive lensless imagers enable novel applications in an extremely compact device, requiring only a phase or amplitude mask placed close to the sensor. They have been demonstrated for 2D and 3D microscopy, single-shot video, and single-shot hyperspectral imaging; in each of these cases, a compressive-sensing-based inverse problem is solved in order to recover a 3D data-cube from a 2D measurement. Typically, this is accomplished using convex optimization and hand-picked priors. Alternatively, deep learning-based reconstruction methods offer the promise of better priors, but require many thousands of ground truth training pairs, which can be difficult or impossible to acquire. In this work, we propose the use of untrained networks for compressive image recovery. Our approach does not require any labeled training data, but instead uses the measurement itself to update the network weights. We demonstrate our untrained approach on lensless compressive 2D imaging as well as single-shot high-speed video recovery using the camera's rolling shutter, and single-shot hyperspectral imaging. We provide simulation and experimental verification, showing that our method results in improved image quality over existing methods.



### An Efficient Multitask Neural Network for Face Alignment, Head Pose Estimation and Face Tracking
- **Arxiv ID**: http://arxiv.org/abs/2103.07615v3
- **DOI**: None
- **Categories**: **cs.CV**, 68T09
- **Links**: [PDF](http://arxiv.org/pdf/2103.07615v3)
- **Published**: 2021-03-13 04:41:15+00:00
- **Updated**: 2022-04-29 02:00:12+00:00
- **Authors**: Jiahao Xia, Haimin Zhang, Shiping Wen, Shuo Yang, Min Xu
- **Comment**: 13 pages, 9 figures, Accepted to ESWA
- **Journal**: None
- **Summary**: While Convolutional Neural Networks (CNNs) have significantly boosted the performance of face related algorithms, maintaining accuracy and efficiency simultaneously in practical use remains challenging. The state-of-the-art methods employ deeper networks for better performance, which makes it less practical for mobile applications because of more parameters and higher computational complexity. Therefore, we propose an efficient multitask neural network, Alignment & Tracking & Pose Network (ATPN) for face alignment, face tracking and head pose estimation. Specifically, to achieve better performance with fewer layers for face alignment, we introduce a shortcut connection between shallow-layer and deep-layer features. We find the shallow-layer features are highly correspond to facial boundaries that can provide the structural information of face and it is crucial for face alignment. Moreover, we generate a cheap heatmap based on the face alignment result and fuse it with features to improve the performance of the other two tasks. Based on the heatmap, the network can utilize both geometric information of landmarks and appearance information for head pose estimation. The heatmap also provides attention clues for face tracking. The face tracking task also saves us the face detection procedure for each frame, which also significantly boost the real-time capability for video-based tasks. We experimentally validate ATPN on four benchmark datasets, WFLW, 300VW, WIDER Face and 300W-LP. The experimental results demonstrate that it achieves better performance with much less parameters and lower computational complexity compared to other light models.



### Potential Escalator-related Injury Identification and Prevention Based on Multi-module Integrated System for Public Health
- **Arxiv ID**: http://arxiv.org/abs/2103.07620v2
- **DOI**: 10.1007/s00138-022-01273-2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.07620v2)
- **Published**: 2021-03-13 05:26:18+00:00
- **Updated**: 2021-03-17 03:39:49+00:00
- **Authors**: Zeyu Jiao, Huan Lei, Hengshan Zong, Yingjie Cai, Zhenyu Zhong
- **Comment**: Please excuse me for taking some of your time. But that we have not
  yet studied our work completely and some new great results are discovered. So
  after carefully thinking, we are going to rearrange this manuscript and try
  to give more precise model. Thus, we decided to withdraw this manuscript with
  great pity
- **Journal**: None
- **Summary**: Escalator-related injuries threaten public health with the widespread use of escalators. The existing studies tend to focus on after-the-fact statistics, reflecting on the original design and use of defects to reduce the impact of escalator-related injuries, but few attention has been paid to ongoing and impending injuries. In this study, a multi-module escalator safety monitoring system based on computer vision is designed and proposed to simultaneously monitor and deal with three major injury triggers, including losing balance, not holding on to handrails and carrying large items. The escalator identification module is utilized to determine the escalator region, namely the region of interest. The passenger monitoring module is leveraged to estimate the passengers' pose to recognize unsafe behaviors on the escalator. The dangerous object detection module detects large items that may enter the escalator and raises alarms. The processing results of the above three modules are summarized in the safety assessment module as the basis for the intelligent decision of the system. The experimental results demonstrate that the proposed system has good performance and great application potential.



### Early Prediction and Diagnosis of Retinoblastoma Using Deep Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/2103.07622v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.07622v1)
- **Published**: 2021-03-13 05:37:19+00:00
- **Updated**: 2021-03-13 05:37:19+00:00
- **Authors**: C. Anand Deva Durai, T Jemima Jebaseeli, Salem Alelyani, Azath Mubharakali
- **Comment**: None
- **Journal**: None
- **Summary**: Retinoblastoma is the most prominent childhood primary intraocular malignancy that impacts the vision of children and adults worldwide. In contrasting and comparing with adults it is uveal melanoma. It is an aggressive tumor that can fill and destroy the eye and the surrounding structures. Therefore early detection of retinoblastoma in childhood is the key. The major impact of the research is to identify the tumor cells in the retina. Also is to find out the stages of the tumor and its corresponding group. The proposed systems assist the ophthalmologists for accurate prediction and diagnosis of retinoblastoma cancer disease at the earliest. The contribution of the proposed approach is to save the life of infants and the grown-up children from vision impairment. The proposed methodology consists of three phases namely, preprocessing, segmentation, and classification. Initially, the fundus images are preprocessed using the Liner Predictive Decision based Median Filter (LPDMF). It removes the noise introduced in the image due to illumination while capturing or scanning the eye of the patients. The preprocessed images are segmented using the Convolutional Neural Network (CNN) to distinguish the foreground tumor cells from the background.



### Generating Unrestricted Adversarial Examples via Three Parameters
- **Arxiv ID**: http://arxiv.org/abs/2103.07640v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.07640v1)
- **Published**: 2021-03-13 07:20:14+00:00
- **Updated**: 2021-03-13 07:20:14+00:00
- **Authors**: Hanieh Naderi, Leili Goli, Shohreh Kasaei
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have been shown to be vulnerable to adversarial examples deliberately constructed to misclassify victim models. As most adversarial examples have restricted their perturbations to $L_{p}$-norm, existing defense methods have focused on these types of perturbations and less attention has been paid to unrestricted adversarial examples; which can create more realistic attacks, able to deceive models without affecting human predictions. To address this problem, the proposed adversarial attack generates an unrestricted adversarial example with a limited number of parameters. The attack selects three points on the input image and based on their locations transforms the image into an adversarial example. By limiting the range of movement and location of these three points and using a discriminatory network, the proposed unrestricted adversarial example preserves the image appearance. Experimental results show that the proposed adversarial examples obtain an average success rate of 93.5% in terms of human evaluation on the MNIST and SVHN datasets. It also reduces the model accuracy by an average of 73% on six datasets MNIST, FMNIST, SVHN, CIFAR10, CIFAR100, and ImageNet. It should be noted that, in the case of attacks, lower accuracy in the victim model denotes a more successful attack. The adversarial train of the attack also improves model robustness against a randomly transformed image.



### Ensemble Learning with Manifold-Based Data Splitting for Noisy Label Correction
- **Arxiv ID**: http://arxiv.org/abs/2103.07641v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.07641v1)
- **Published**: 2021-03-13 07:24:58+00:00
- **Updated**: 2021-03-13 07:24:58+00:00
- **Authors**: Hao-Chiang Shao, Hsin-Chieh Wang, Weng-Tai Su, Chia-Wen Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Label noise in training data can significantly degrade a model's generalization performance for supervised learning tasks. Here we focus on the problem that noisy labels are primarily mislabeled samples, which tend to be concentrated near decision boundaries, rather than uniformly distributed, and whose features should be equivocal. To address the problem, we propose an ensemble learning method to correct noisy labels by exploiting the local structures of feature manifolds. Different from typical ensemble strategies that increase the prediction diversity among sub-models via certain loss terms, our method trains sub-models on disjoint subsets, each being a union of the nearest-neighbors of randomly selected seed samples on the data manifold. As a result, each sub-model can learn a coarse representation of the data manifold along with a corresponding graph. Moreover, only a limited number of sub-models will be affected by locally-concentrated noisy labels. The constructed graphs are used to suggest a series of label correction candidates, and accordingly, our method derives label correction results by voting down inconsistent suggestions. Our experiments on real-world noisy label datasets demonstrate the superiority of the proposed method over existing state-of-the-arts.



### PhotoApp: Photorealistic Appearance Editing of Head Portraits
- **Arxiv ID**: http://arxiv.org/abs/2103.07658v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.07658v2)
- **Published**: 2021-03-13 08:59:49+00:00
- **Updated**: 2021-05-13 17:59:43+00:00
- **Authors**: Mallikarjun B R, Ayush Tewari, Abdallah Dib, Tim Weyrich, Bernd Bickel, Hans-Peter Seidel, Hanspeter Pfister, Wojciech Matusik, Louis Chevallier, Mohamed Elgharib, Christian Theobalt
- **Comment**: http://gvv.mpi-inf.mpg.de/projects/PhotoApp/
- **Journal**: None
- **Summary**: Photorealistic editing of portraits is a challenging task as humans are very sensitive to inconsistencies in faces. We present an approach for high-quality intuitive editing of the camera viewpoint and scene illumination in a portrait image. This requires our method to capture and control the full reflectance field of the person in the image. Most editing approaches rely on supervised learning using training data captured with setups such as light and camera stages. Such datasets are expensive to acquire, not readily available and do not capture all the rich variations of in-the-wild portrait images. In addition, most supervised approaches only focus on relighting, and do not allow camera viewpoint editing. Thus, they only capture and control a subset of the reflectance field. Recently, portrait editing has been demonstrated by operating in the generative model space of StyleGAN. While such approaches do not require direct supervision, there is a significant loss of quality when compared to the supervised approaches. In this paper, we present a method which learns from limited supervised training data. The training images only include people in a fixed neutral expression with eyes closed, without much hair or background variations. Each person is captured under 150 one-light-at-a-time conditions and under 8 camera poses. Instead of training directly in the image space, we design a supervised problem which learns transformations in the latent space of StyleGAN. This combines the best of supervised learning and generative adversarial modeling. We show that the StyleGAN prior allows for generalisation to different expressions, hairstyles and backgrounds. This produces high-quality photorealistic results for in-the-wild images and significantly outperforms existing methods. Our approach can edit the illumination and pose simultaneously, and runs at interactive rates.



### Fine-grained MRI Reconstruction using Attentive Selection Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.07672v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.07672v1)
- **Published**: 2021-03-13 09:58:32+00:00
- **Updated**: 2021-03-13 09:58:32+00:00
- **Authors**: Jingshuai Liu, Mehrdad Yaghoobi
- **Comment**: 5 pages, 2 figures, 1 table, 22 references
- **Journal**: None
- **Summary**: Compressed sensing (CS) leverages the sparsity prior to provide the foundation for fast magnetic resonance imaging (fastMRI). However, iterative solvers for ill-posed problems hinder their adaption to time-critical applications. Moreover, such a prior can be neither rich to capture complicated anatomical structures nor applicable to meet the demand of high-fidelity reconstructions in modern MRI. Inspired by the state-of-the-art methods in image generation, we propose a novel attention-based deep learning framework to provide high-quality MRI reconstruction. We incorporate large-field contextual feature integration and attention selection in a generative adversarial network (GAN) framework. We demonstrate that the proposed model can produce superior results compared to other deep learning-based methods in terms of image quality, and relevance to the MRI reconstruction in an extremely low sampling rate diet.



### uTHCD: A New Benchmarking for Tamil Handwritten OCR
- **Arxiv ID**: http://arxiv.org/abs/2103.07676v1
- **DOI**: 10.1109/ACCESS.2021.3096823
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.07676v1)
- **Published**: 2021-03-13 10:34:08+00:00
- **Updated**: 2021-03-13 10:34:08+00:00
- **Authors**: Noushath Shaffi, Faizal Hajamohideen
- **Comment**: 30 pages, 18 figures, in IEEE Access
- **Journal**: None
- **Summary**: Handwritten character recognition is a challenging research in the field of document image analysis over many decades due to numerous reasons such as large writing styles variation, inherent noise in data, expansive applications it offers, non-availability of benchmark databases etc. There has been considerable work reported in literature about creation of the database for several Indic scripts but the Tamil script is still in its infancy as it has been reported only in one database [5]. In this paper, we present the work done in the creation of an exhaustive and large unconstrained Tamil Handwritten Character Database (uTHCD). Database consists of around 91000 samples with nearly 600 samples in each of 156 classes. The database is a unified collection of both online and offline samples. Offline samples were collected by asking volunteers to write samples on a form inside a specified grid. For online samples, we made the volunteers write in a similar grid using a digital writing pad. The samples collected encompass a vast variety of writing styles, inherent distortions arising from offline scanning process viz stroke discontinuity, variable thickness of stroke, distortion etc. Algorithms which are resilient to such data can be practically deployed for real time applications. The samples were generated from around 650 native Tamil volunteers including school going kids, homemakers, university students and faculty. The isolated character database will be made publicly available as raw images and Hierarchical Data File (HDF) compressed file. With this database, we expect to set a new benchmark in Tamil handwritten character recognition and serve as a launchpad for many avenues in document image analysis domain. Paper also presents an ideal experimental set-up using the database on convolutional neural networks (CNN) with a baseline accuracy of 88% on test data.



### A review of machine learning in processing remote sensing data for mineral exploration
- **Arxiv ID**: http://arxiv.org/abs/2103.07678v2
- **DOI**: 10.1016/j.rse.2021.112750
- **Categories**: **cs.LG**, cs.CV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2103.07678v2)
- **Published**: 2021-03-13 10:36:25+00:00
- **Updated**: 2021-12-04 07:11:24+00:00
- **Authors**: Hojat Shirmard, Ehsan Farahbakhsh, R. Dietmar Muller, Rohitash Chandra
- **Comment**: 26 pages, 4 figures, 2 tables
- **Journal**: Remote Sensing of Environment, 268, 112750 (2022)
- **Summary**: The decline of the number of newly discovered mineral deposits and increase in demand for different minerals in recent years has led exploration geologists to look for more efficient and innovative methods for processing different data types at each stage of mineral exploration. As a primary step, various features, such as lithological units, alteration types, structures, and indicator minerals, are mapped to aid decision-making in targeting ore deposits. Different types of remote sensing datasets, such as satellite and airborne data, make it possible to overcome common problems associated with mapping geological features. The rapid increase in the volume of remote sensing data obtained from different platforms has encouraged scientists to develop advanced, innovative, and robust data processing methodologies. Machine learning methods can help process a wide range of remote sensing datasets and determine the relationship between components such as the reflectance continuum and features of interest. These methods are robust in processing spectral and ground truth measurements against noise and uncertainties. In recent years, many studies have been carried out by supplementing geological surveys with remote sensing datasets, which is now prominent in geoscience research. This paper provides a comprehensive review of the implementation and adaptation of some popular and recently established machine learning methods for processing different types of remote sensing data and investigates their applications for detecting various ore deposit types. We demonstrate the high capability of combining remote sensing data and machine learning methods for mapping different geological features that are critical for providing potential maps. Moreover, we find there is scope for advanced methods to process the new generation of remote sensing data for creating improved mineral prospectivity maps.



### OCID-Ref: A 3D Robotic Dataset with Embodied Language for Clutter Scene Grounding
- **Arxiv ID**: http://arxiv.org/abs/2103.07679v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.07679v2)
- **Published**: 2021-03-13 10:38:15+00:00
- **Updated**: 2021-04-14 09:03:46+00:00
- **Authors**: Ke-Jyun Wang, Yun-Hsuan Liu, Hung-Ting Su, Jen-Wei Wang, Yu-Siang Wang, Winston H. Hsu, Wen-Chin Chen
- **Comment**: NAACL 2021
- **Journal**: None
- **Summary**: To effectively apply robots in working environments and assist humans, it is essential to develop and evaluate how visual grounding (VG) can affect machine performance on occluded objects. However, current VG works are limited in working environments, such as offices and warehouses, where objects are usually occluded due to space utilization issues. In our work, we propose a novel OCID-Ref dataset featuring a referring expression segmentation task with referring expressions of occluded objects. OCID-Ref consists of 305,694 referring expressions from 2,300 scenes with providing RGB image and point cloud inputs. To resolve challenging occlusion issues, we argue that it's crucial to take advantage of both 2D and 3D signals to resolve challenging occlusion issues. Our experimental results demonstrate the effectiveness of aggregating 2D and 3D signals but referring to occluded objects still remains challenging for the modern visual grounding systems. OCID-Ref is publicly available at https://github.com/lluma/OCID-Ref



### NeuralHumanFVV: Real-Time Neural Volumetric Human Performance Rendering using RGB Cameras
- **Arxiv ID**: http://arxiv.org/abs/2103.07700v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.07700v1)
- **Published**: 2021-03-13 12:03:38+00:00
- **Updated**: 2021-03-13 12:03:38+00:00
- **Authors**: Xin Suo, Yuheng Jiang, Pei Lin, Yingliang Zhang, Kaiwen Guo, Minye Wu, Lan Xu
- **Comment**: None
- **Journal**: None
- **Summary**: 4D reconstruction and rendering of human activities is critical for immersive VR/AR experience.Recent advances still fail to recover fine geometry and texture results with the level of detail present in the input images from sparse multi-view RGB cameras. In this paper, we propose NeuralHumanFVV, a real-time neural human performance capture and rendering system to generate both high-quality geometry and photo-realistic texture of human activities in arbitrary novel views. We propose a neural geometry generation scheme with a hierarchical sampling strategy for real-time implicit geometry inference, as well as a novel neural blending scheme to generate high resolution (e.g., 1k) and photo-realistic texture results in the novel views. Furthermore, we adopt neural normal blending to enhance geometry details and formulate our neural geometry and texture rendering into a multi-task learning framework. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality geometry and photo-realistic free view-point reconstruction for challenging human performances.



### ReDet: A Rotation-equivariant Detector for Aerial Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.07733v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.07733v1)
- **Published**: 2021-03-13 15:37:36+00:00
- **Updated**: 2021-03-13 15:37:36+00:00
- **Authors**: Jiaming Han, Jian Ding, Nan Xue, Gui-Song Xia
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Recently, object detection in aerial images has gained much attention in computer vision. Different from objects in natural images, aerial objects are often distributed with arbitrary orientation. Therefore, the detector requires more parameters to encode the orientation information, which are often highly redundant and inefficient. Moreover, as ordinary CNNs do not explicitly model the orientation variation, large amounts of rotation augmented data is needed to train an accurate object detector. In this paper, we propose a Rotation-equivariant Detector (ReDet) to address these issues, which explicitly encodes rotation equivariance and rotation invariance. More precisely, we incorporate rotation-equivariant networks into the detector to extract rotation-equivariant features, which can accurately predict the orientation and lead to a huge reduction of model size. Based on the rotation-equivariant features, we also present Rotation-invariant RoI Align (RiRoI Align), which adaptively extracts rotation-invariant features from equivariant features according to the orientation of RoI. Extensive experiments on several challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and HRSC2016, show that our method can achieve state-of-the-art performance on the task of aerial object detection. Compared with previous best results, our ReDet gains 1.2, 3.5 and 2.6 mAP on DOTA-v1.0, DOTA-v1.5 and HRSC2016 respectively while reducing the number of parameters by 60\% (313 Mb vs. 121 Mb). The code is available at: \url{https://github.com/csuhan/ReDet}.



### Reconsidering Representation Alignment for Multi-view Clustering
- **Arxiv ID**: http://arxiv.org/abs/2103.07738v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.07738v1)
- **Published**: 2021-03-13 15:58:18+00:00
- **Updated**: 2021-03-13 15:58:18+00:00
- **Authors**: Daniel J. Trosten, Sigurd Løkse, Robert Jenssen, Michael Kampffmeyer
- **Comment**: To appear in CVPR 2021. Code available at
  https://github.com/DanielTrosten/mvc
- **Journal**: None
- **Summary**: Aligning distributions of view representations is a core component of today's state of the art models for deep multi-view clustering. However, we identify several drawbacks with na\"ively aligning representation distributions. We demonstrate that these drawbacks both lead to less separable clusters in the representation space, and inhibit the model's ability to prioritize views. Based on these observations, we develop a simple baseline model for deep multi-view clustering. Our baseline model avoids representation alignment altogether, while performing similar to, or better than, the current state of the art. We also expand our baseline model by adding a contrastive learning component. This introduces a selective alignment procedure that preserves the model's ability to prioritize views. Our experiments show that the contrastive learning component enhances the baseline model, improving on the current state of the art by a large margin on several datasets.



### Unsupervised Image Transformation Learning via Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.07751v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.07751v2)
- **Published**: 2021-03-13 17:08:19+00:00
- **Updated**: 2022-10-05 18:01:19+00:00
- **Authors**: Kaiwen Zha, Yujun Shen, Bolei Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we study the image transformation problem, which targets at learning the underlying transformations (e.g., the transition of seasons) from a collection of unlabeled images. However, there could be countless of transformations in the real world, making such a task incredibly challenging, especially under the unsupervised setting. To tackle this obstacle, we propose a novel learning framework built on generative adversarial networks (GANs), where the discriminator and the generator share a transformation space. After the model gets fully optimized, any two points within the shared space are expected to define a valid transformation. In this way, at the inference stage, we manage to adequately extract the variation factor between a customizable image pair by projecting both images onto the transformation space. The resulting transformation vector can further guide the image synthesis, facilitating image editing with continuous semantic change (e.g., altering summer to winter with fall as the intermediate step). Noticeably, the learned transformation space supports not only transferring image styles (e.g., changing day to night), but also manipulating image contents (e.g., adding clouds in the sky). In addition, we make in-depth analysis on the properties of the transformation space to help understand how various transformations are organized. Project page is at https://genforce.github.io/trgan/.



### Image Segmentation Methods for Non-destructive testing Applications
- **Arxiv ID**: http://arxiv.org/abs/2103.07754v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.07754v1)
- **Published**: 2021-03-13 17:13:33+00:00
- **Updated**: 2021-03-13 17:13:33+00:00
- **Authors**: EL-Hachemi Guerrout, Ramdane Mahiou, Randa Boukabene, Assia Ouali
- **Comment**: 10 pages, 3 figures, the article is just accepted in the conference
  JERI 2020 but the conference stopped because of Covid so the article non
  published
- **Journal**: None
- **Summary**: In this paper, we present new image segmentation methods based on hidden Markov random fields (HMRFs) and cuckoo search (CS) variants. HMRFs model the segmentation problem as a minimization of an energy function. CS algorithm is one of the recent powerful optimization techniques. Therefore, five variants of the CS algorithm are used to compute a solution. Through tests, we conduct a study to choose the CS variant with parameters that give good results (execution time and quality of segmentation). CS variants are evaluated and compared with non-destructive testing (NDT) images using a misclassification error (ME) criterion.



### Learning with Feature-Dependent Label Noise: A Progressive Approach
- **Arxiv ID**: http://arxiv.org/abs/2103.07756v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2103.07756v3)
- **Published**: 2021-03-13 17:34:22+00:00
- **Updated**: 2021-03-27 05:34:18+00:00
- **Authors**: Yikai Zhang, Songzhu Zheng, Pengxiang Wu, Mayank Goswami, Chao Chen
- **Comment**: ICLR 2021 (Spotlight)
- **Journal**: None
- **Summary**: Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels.



### Learning Novel Objects Continually Through Curiosity
- **Arxiv ID**: http://arxiv.org/abs/2103.07758v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.07758v2)
- **Published**: 2021-03-13 17:42:09+00:00
- **Updated**: 2021-05-15 21:53:49+00:00
- **Authors**: Ali Ayub, Alan R. Wagner
- **Comment**: Accepted at IEEE ICRA 2021 (Workshop titled, Towards Curious Robots:
  Modern Approaches for Intrinsically-Motivated Intelligent Behavior)
- **Journal**: None
- **Summary**: Children learn continually by asking questions about the concepts they are most curious about. With robots becoming an integral part of our society, they must also learn unknown concepts continually by asking humans questions. The paper analyzes a recent state-of-the-art approach for continual learning. The paper further develops a self-supervised technique to find most of the uncertain objects in an environment by utilizing the cluster representation of the previously learned classes. We test our approach on a benchmark dataset for continual learning on robots. Our results show that our curiosity-driven continual learning approach beats random sampling and softmax-based uncertainty sampling in terms of classification accuracy and the total number of classes learned.



### VMAF And Variants: Towards A Unified VQA
- **Arxiv ID**: http://arxiv.org/abs/2103.07770v7
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.07770v7)
- **Published**: 2021-03-13 18:41:51+00:00
- **Updated**: 2021-10-08 16:16:23+00:00
- **Authors**: Pankaj Topiwala, Wei Dai, Jiangfeng Pian, Katalina Biondi, Arvind Krovvidi
- **Comment**: Some calculational errors have been fixed in this version
- **Journal**: None
- **Summary**: Video quality assessment (VQA) is now a fast-growing subject, maturing in the full reference (FR) case, yet challenging in the exploding no reference (NR) case. We investigate variants of the popular VMAF video quality assessment algorithm for the FR case, using both support vector regression and feedforward neural networks. We extend it to the NR case, using some different features but similar learning, to develop a partially unified framework for VQA. When fully trained, FR algorithms such as VMAF perform very well on test datasets, reaching 90%+ match in PCC and SRCC; but for predicting performance in the wild, we train/test from scratch for each database. With an 80/20 train/test split, we still achieve about 90% performance on average in both PCC and SRCC, with up to 7-9% gains over VMAF, using an improved motion feature and better regression. Moreover, we even get decent performance (about 75%) if we ignore the reference, treating FR as NR, partly justifying our attempts at unification. In the true NR case, we reduce complexity vs. leading recent algorithms VIDEVAL, RAPIQUE, yet achieve performance within 3-5%. Moreover, we develop a method to analyze the saliency of features, and conclude that for both VIDEVAL and RAPIQUE, a small subset of their features are providing the bulk of the performance. In short, we find encouraging improvements in trainability in FR, while constraining training complexity against leading methods in NR, elucidating the saliency of features for feature selection.



### A Novel Visualization System of Using Augmented Reality in Knee Replacement Surgery: Enhanced Bidirectional Maximum Correntropy Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2104.05742v1
- **DOI**: 10.1002/rcs.2154
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2104.05742v1)
- **Published**: 2021-03-13 19:18:16+00:00
- **Updated**: 2021-03-13 19:18:16+00:00
- **Authors**: Nitish Maharjan, Abeer Alsadoon, P. W. C. Prasad, Salma Abdullah, Tarik A. Rashid
- **Comment**: 27 pages
- **Journal**: The International Journal of Medical Robotics and Computer
  Assisted Surgery, 2020
- **Summary**: Background and aim: Image registration and alignment are the main limitations of augmented reality-based knee replacement surgery. This research aims to decrease the registration error, eliminate outcomes that are trapped in local minima to improve the alignment problems, handle the occlusion, and maximize the overlapping parts. Methodology: markerless image registration method was used for Augmented reality-based knee replacement surgery to guide and visualize the surgical operation. While weight least square algorithm was used to enhance stereo camera-based tracking by filling border occlusion in right to left direction and non-border occlusion from left to right direction. Results: This study has improved video precision to 0.57 mm~0.61 mm alignment error. Furthermore, with the use of bidirectional points, for example, forwards and backwards directional cloud point, the iteration on image registration was decreased. This has led to improve the processing time as well. The processing time of video frames was improved to 7.4~11.74 fps. Conclusions: It seems clear that this proposed system has focused on overcoming the misalignment difficulty caused by movement of patient and enhancing the AR visualization during knee replacement surgery. The proposed system was reliable and favorable which helps in eliminating alignment error by ascertaining the optimal rigid transformation between two cloud points and removing the outliers and non-Gaussian noise. The proposed augmented reality system helps in accurate visualization and navigation of anatomy of knee such as femur, tibia, cartilage, blood vessels, etc.



### Multi-Object Tracking using Poisson Multi-Bernoulli Mixture Filtering for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2103.07783v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2103.07783v1)
- **Published**: 2021-03-13 20:24:18+00:00
- **Updated**: 2021-03-13 20:24:18+00:00
- **Authors**: Su Pang, Hayder Radha
- **Comment**: None
- **Journal**: None
- **Summary**: The ability of an autonomous vehicle to perform 3D tracking is essential for safe planing and navigation in cluttered environments. The main challenges for multi-object tracking (MOT) in autonomous driving applications reside in the inherent uncertainties regarding the number of objects, when and where the objects may appear and disappear, and uncertainties regarding objects' states. Random finite set (RFS) based approaches can naturally model these uncertainties accurately and elegantly, and they have been widely used in radar-based tracking applications. In this work, we developed an RFS-based MOT framework for 3D LiDAR data. In partiuclar, we propose a Poisson multi-Bernoulli mixture (PMBM) filter to solve the amodal MOT problem for autonomous driving applications. To the best of our knowledge, this represents a first attempt for employing an RFS-based approach in conjunction with 3D LiDAR data for MOT applications with comprehensive validation using challenging datasets made available by industry leaders. The superior experimental results of our PMBM tracker on public Waymo and Argoverse datasets clearly illustrate that an RFS-based tracker outperforms many state-of-the-art deep learning-based and Kalman filter-based methods, and consequently, these results indicate a great potential for further exploration of RFS-based frameworks for 3D MOT applications.



### A Few-Shot Learning Approach for Accelerated MRI via Fusion of Data-Driven and Subject-Driven Priors
- **Arxiv ID**: http://arxiv.org/abs/2103.07790v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.07790v1)
- **Published**: 2021-03-13 20:51:31+00:00
- **Updated**: 2021-03-13 20:51:31+00:00
- **Authors**: Salman Ul Hassan Dar, Mahmut Yurt, Tolga Çukur
- **Comment**: Accepted for presentation at the 29th Annual Meeting of the
  International Society of Magnetic Resonance in Medicine (ISMRM)
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have recently found emerging use in accelerated MRI reconstruction. DNNs typically learn data-driven priors from large datasets constituting pairs of undersampled and fully-sampled acquisitions. Acquiring such large datasets, however, might be impractical. To mitigate this limitation, we propose a few-shot learning approach for accelerated MRI that merges subject-driven priors obtained via physical signal models with data-driven priors obtained from a few training samples. Demonstrations on brain MR images from the NYU fastMRI dataset indicate that the proposed approach requires just a few samples to outperform traditional parallel imaging and DNN algorithms.



### ORStereo: Occlusion-Aware Recurrent Stereo Matching for 4K-Resolution Images
- **Arxiv ID**: http://arxiv.org/abs/2103.07798v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.07798v1)
- **Published**: 2021-03-13 21:46:06+00:00
- **Updated**: 2021-03-13 21:46:06+00:00
- **Authors**: Yaoyu Hu, Wenshan Wang, Huai Yu, Weikun Zhen, Sebastian Scherer
- **Comment**: Submitted to International Conference on Intelligent Robots and
  Systems (IROS) 2021
- **Journal**: None
- **Summary**: Stereo reconstruction models trained on small images do not generalize well to high-resolution data. Training a model on high-resolution image size faces difficulties of data availability and is often infeasible due to limited computing resources. In this work, we present the Occlusion-aware Recurrent binocular Stereo matching (ORStereo), which deals with these issues by only training on available low disparity range stereo images. ORStereo generalizes to unseen high-resolution images with large disparity ranges by formulating the task as residual updates and refinements of an initial prediction. ORStereo is trained on images with disparity ranges limited to 256 pixels, yet it can operate 4K-resolution input with over 1000 disparities using limited GPU memory. We test the model's capability on both synthetic and real-world high-resolution images. Experimental results demonstrate that ORStereo achieves comparable performance on 4K-resolution images compared to state-of-the-art methods trained on large disparity ranges. Compared to other methods that are only trained on low-resolution images, our method is 70% more accurate on 4K-resolution images.



