# Arxiv Papers in cs.CV on 2021-03-19
### Hopper: Multi-hop Transformer for Spatiotemporal Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2103.10574v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10574v2)
- **Published**: 2021-03-19 00:13:04+00:00
- **Updated**: 2021-03-22 02:00:23+00:00
- **Authors**: Honglu Zhou, Asim Kadav, Farley Lai, Alexandru Niculescu-Mizil, Martin Renqiang Min, Mubbasir Kapadia, Hans Peter Graf
- **Comment**: None
- **Journal**: None
- **Summary**: This paper considers the problem of spatiotemporal object-centric reasoning in videos. Central to our approach is the notion of object permanence, i.e., the ability to reason about the location of objects as they move through the video while being occluded, contained or carried by other objects. Existing deep learning based approaches often suffer from spatiotemporal biases when applied to video reasoning problems. We propose Hopper, which uses a Multi-hop Transformer for reasoning object permanence in videos. Given a video and a localization query, Hopper reasons over image and object tracks to automatically hop over critical frames in an iterative fashion to predict the final position of the object of interest. We demonstrate the effectiveness of using a contrastive loss to reduce spatiotemporal biases. We evaluate over CATER dataset and find that Hopper achieves 73.2% Top-1 accuracy using just 1 FPS by hopping through just a few critical frames. We also demonstrate Hopper can perform long-term reasoning by building a CATER-h dataset that requires multi-step reasoning to localize objects of interest correctly.



### Dynamic Transfer for Multi-Source Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2103.10583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10583v1)
- **Published**: 2021-03-19 01:22:12+00:00
- **Updated**: 2021-03-19 01:22:12+00:00
- **Authors**: Yunsheng Li, Lu Yuan, Yinpeng Chen, Pei Wang, Nuno Vasconcelos
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Recent works of multi-source domain adaptation focus on learning a domain-agnostic model, of which the parameters are static. However, such a static model is difficult to handle conflicts across multiple domains, and suffers from a performance degradation in both source domains and target domain. In this paper, we present dynamic transfer to address domain conflicts, where the model parameters are adapted to samples. The key insight is that adapting model across domains is achieved via adapting model across samples. Thus, it breaks down source domain barriers and turns multi-source domains into a single-source domain. This also simplifies the alignment between source and target domains, as it only requires the target domain to be aligned with any part of the union of source domains. Furthermore, we find dynamic transfer can be simply modeled by aggregating residual matrices and a static convolution matrix. Experimental results show that, without using domain labels, our dynamic transfer outperforms the state-of-the-art method by more than 3% on the large multi-source domain adaptation datasets -- DomainNet. Source code is at https://github.com/liyunsheng13/DRT.



### Fusion-FlowNet: Energy-Efficient Optical Flow Estimation using Sensor Fusion and Deep Fused Spiking-Analog Network Architectures
- **Arxiv ID**: http://arxiv.org/abs/2103.10592v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2103.10592v1)
- **Published**: 2021-03-19 02:03:33+00:00
- **Updated**: 2021-03-19 02:03:33+00:00
- **Authors**: Chankyu Lee, Adarsh Kumar Kosta, Kaushik Roy
- **Comment**: None
- **Journal**: None
- **Summary**: Standard frame-based cameras that sample light intensity frames are heavily impacted by motion blur for high-speed motion and fail to perceive scene accurately when the dynamic range is high. Event-based cameras, on the other hand, overcome these limitations by asynchronously detecting the variation in individual pixel intensities. However, event cameras only provide information about pixels in motion, leading to sparse data. Hence, estimating the overall dense behavior of pixels is difficult. To address such issues associated with the sensors, we present Fusion-FlowNet, a sensor fusion framework for energy-efficient optical flow estimation using both frame- and event-based sensors, leveraging their complementary characteristics. Our proposed network architecture is also a fusion of Spiking Neural Networks (SNNs) and Analog Neural Networks (ANNs) where each network is designed to simultaneously process asynchronous event streams and regular frame-based images, respectively. Our network is end-to-end trained using unsupervised learning to avoid expensive video annotations. The method generalizes well across distinct environments (rapid motion and challenging lighting conditions) and demonstrates state-of-the-art optical flow prediction on the Multi-Vehicle Stereo Event Camera (MVSEC) dataset. Furthermore, our network offers substantial savings in terms of the number of network parameters and computational energy cost.



### PSCC-Net: Progressive Spatio-Channel Correlation Network for Image Manipulation Detection and Localization
- **Arxiv ID**: http://arxiv.org/abs/2103.10596v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10596v2)
- **Published**: 2021-03-19 02:22:53+00:00
- **Updated**: 2022-08-13 12:28:50+00:00
- **Authors**: Xiaohong Liu, Yaojie Liu, Jun Chen, Xiaoming Liu
- **Comment**: Published in IEEE Transactions on Circuits and Systems for Video
  Technology. Codes and models are available at
  https://github.com/proteus1991/PSCC-Net
- **Journal**: None
- **Summary**: To defend against manipulation of image content, such as splicing, copy-move, and removal, we develop a Progressive Spatio-Channel Correlation Network (PSCC-Net) to detect and localize image manipulations. PSCC-Net processes the image in a two-path procedure: a top-down path that extracts local and global features and a bottom-up path that detects whether the input image is manipulated, and estimates its manipulation masks at multiple scales, where each mask is conditioned on the previous one. Different from the conventional encoder-decoder and no-pooling structures, PSCC-Net leverages features at different scales with dense cross-connections to produce manipulation masks in a coarse-to-fine fashion. Moreover, a Spatio-Channel Correlation Module (SCCM) captures both spatial and channel-wise correlations in the bottom-up path, which endows features with holistic cues, enabling the network to cope with a wide range of manipulation attacks. Thanks to the light-weight backbone and progressive mechanism, PSCC-Net can process 1,080P images at 50+ FPS. Extensive experiments demonstrate the superiority of PSCC-Net over the state-of-the-art methods on both detection and localization.



### Noise Modulation: Let Your Model Interpret Itself
- **Arxiv ID**: http://arxiv.org/abs/2103.10603v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.10603v1)
- **Published**: 2021-03-19 02:55:33+00:00
- **Updated**: 2021-03-19 02:55:33+00:00
- **Authors**: Haoyang Li, Xinggang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Given the great success of Deep Neural Networks(DNNs) and the black-box nature of it,the interpretability of these models becomes an important issue.The majority of previous research works on the post-hoc interpretation of a trained model.But recently, adversarial training shows that it is possible for a model to have an interpretable input-gradient through training.However,adversarial training lacks efficiency for interpretability.To resolve this problem, we construct an approximation of the adversarial perturbations and discover a connection between adversarial training and amplitude modulation. Based on a digital analogy,we propose noise modulation as an efficient and model-agnostic alternative to train a model that interprets itself with input-gradients.Experiment results show that noise modulation can effectively increase the interpretability of input-gradients model-agnosticly.



### DCF-ASN: Coarse-to-fine Real-time Visual Tracking via Discriminative Correlation Filter and Attentional Siamese Network
- **Arxiv ID**: http://arxiv.org/abs/2103.10607v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10607v1)
- **Published**: 2021-03-19 03:01:21+00:00
- **Updated**: 2021-03-19 03:01:21+00:00
- **Authors**: Xizhe Xue, Ying Li, Xiaoyue Yin, Qiang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Discriminative correlation filters (DCF) and siamese networks have achieved promising performance on visual tracking tasks thanks to their superior computational efficiency and reliable similarity metric learning, respectively. However, how to effectively take advantages of powerful deep networks, while maintaining the real-time response of DCF, remains a challenging problem. Embedding the cross-correlation operator as a separate layer into siamese networks is a popular choice to enhance the tracking accuracy. Being a key component of such a network, the correlation layer is updated online together with other parts of the network. Yet, when facing serious disturbance, fused trackers may still drift away from the target completely due to accumulated errors. To address these issues, we propose a coarse-to-fine tracking framework, which roughly infers the target state via an online-updating DCF module first and subsequently, finely locates the target through an offline-training asymmetric siamese network (ASN). Benefitting from the guidance of DCF and the learned channel weights obtained through exploiting the given ground-truth template, ASN refines feature representation and implements precise target localization. Systematic experiments on five popular tracking datasets demonstrate that the proposed DCF-ASN achieves the state-of-the-art performance while exhibiting good tracking efficiency.



### Training image classifiers using Semi-Weak Label Data
- **Arxiv ID**: http://arxiv.org/abs/2103.10608v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.10608v1)
- **Published**: 2021-03-19 03:06:07+00:00
- **Updated**: 2021-03-19 03:06:07+00:00
- **Authors**: Anxiang Zhang, Ankit Shah, Bhiksha Raj
- **Comment**: First two authors contributed equally
- **Journal**: None
- **Summary**: In Multiple Instance learning (MIL), weak labels are provided at the bag level with only presence/absence information known. However, there is a considerable gap in performance in comparison to a fully supervised model, limiting the practical applicability of MIL approaches. Thus, this paper introduces a novel semi-weak label learning paradigm as a middle ground to mitigate the problem. We define semi-weak label data as data where we know the presence or absence of a given class and the exact count of each class as opposed to knowing the label proportions. We then propose a two-stage framework to address the problem of learning from semi-weak labels. It leverages the fact that counting information is non-negative and discrete. Experiments are conducted on generated samples from CIFAR-10. We compare our model with a fully-supervised setting baseline, a weakly-supervised setting baseline and learning from pro-portion (LLP) baseline. Our framework not only outperforms both baseline models for MIL-based weakly super-vised setting and learning from proportion setting, but also gives comparable results compared to the fully supervised model. Further, we conduct thorough ablation studies to analyze across datasets and variation with batch size, losses architectural changes, bag size and regularization



### Boosting Adversarial Transferability through Enhanced Momentum
- **Arxiv ID**: http://arxiv.org/abs/2103.10609v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10609v1)
- **Published**: 2021-03-19 03:10:32+00:00
- **Updated**: 2021-03-19 03:10:32+00:00
- **Authors**: Xiaosen Wang, Jiadong Lin, Han Hu, Jingdong Wang, Kun He
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Deep learning models are known to be vulnerable to adversarial examples crafted by adding human-imperceptible perturbations on benign images. Many existing adversarial attack methods have achieved great white-box attack performance, but exhibit low transferability when attacking other models. Various momentum iterative gradient-based methods are shown to be effective to improve the adversarial transferability. In what follows, we propose an enhanced momentum iterative gradient-based method to further enhance the adversarial transferability. Specifically, instead of only accumulating the gradient during the iterative process, we additionally accumulate the average gradient of the data points sampled in the gradient direction of the previous iteration so as to stabilize the update direction and escape from poor local maxima. Extensive experiments on the standard ImageNet dataset demonstrate that our method could improve the adversarial transferability of momentum-based methods by a large margin of 11.1% on average. Moreover, by incorporating with various input transformation methods, the adversarial transferability could be further improved significantly. We also attack several extra advanced defense models under the ensemble-model setting, and the enhancements are remarkable with at least 7.8% on average.



### Knowledge-Guided Object Discovery with Acquired Deep Impressions
- **Arxiv ID**: http://arxiv.org/abs/2103.10611v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.10611v1)
- **Published**: 2021-03-19 03:17:57+00:00
- **Updated**: 2021-03-19 03:17:57+00:00
- **Authors**: Jinyang Yuan, Bin Li, Xiangyang Xue
- **Comment**: AAAI 2021
- **Journal**: None
- **Summary**: We present a framework called Acquired Deep Impressions (ADI) which continuously learns knowledge of objects as "impressions" for compositional scene understanding. In this framework, the model first acquires knowledge from scene images containing a single object in a supervised manner, and then continues to learn from novel multi-object scene images which may contain objects that have not been seen before without any further supervision, under the guidance of the learned knowledge as humans do. By memorizing impressions of objects into parameters of neural networks and applying the generative replay strategy, the learned knowledge can be reused to generate images with pseudo-annotations and in turn assist the learning of novel scenes. The proposed ADI framework focuses on the acquisition and utilization of knowledge, and is complementary to existing deep generative models proposed for compositional scene representation. We adapt a base model to make it fall within the ADI framework and conduct experiments on two types of datasets. Empirical results suggest that the proposed framework is able to effectively utilize the acquired impressions and improve the scene decomposition performance.



### Hyperspectral Image Super-Resolution in Arbitrary Input-Output Band Settings
- **Arxiv ID**: http://arxiv.org/abs/2103.10614v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.10614v3)
- **Published**: 2021-03-19 03:32:28+00:00
- **Updated**: 2021-11-15 05:51:16+00:00
- **Authors**: Zhongyang Zhang, Zhiyang Xu, Zia Ahmed, Asif Salekin, Tauhidur Rahman
- **Comment**: Accepted by WACV 2022 Workshop WACI(Workshop on Applications of
  Computational Imaging)
- **Journal**: None
- **Summary**: Hyperspectral image (HSI) with narrow spectral bands can capture rich spectral information, but it sacrifices its spatial resolution in the process. Many machine-learning-based HSI super-resolution (SR) algorithms have been proposed recently. However, one of the fundamental limitations of these approaches is that they are highly dependent on image and camera settings and can only learn to map an input HSI with one specific setting to an output HSI with another. However, different cameras capture images with different spectral response functions and bands numbers due to the diversity of HSI cameras. Consequently, the existing machine-learning-based approaches fail to learn to super-resolve HSIs for a wide variety of input-output band settings. We propose a single Meta-Learning-Based Super-Resolution (MLSR) model, which can take in HSI images at an arbitrary number of input bands' peak wavelengths and generate SR HSIs with an arbitrary number of output bands' peak wavelengths. We leverage NTIRE2020 and ICVL datasets to train and validate the performance of the MLSR model. The results show that the single proposed model can successfully generate super-resolved HSI bands at arbitrary input-output band settings. The results are better or at least comparable to baselines that are separately trained on a specific input-output band setting.



### Scalable Vision Transformers with Hierarchical Pooling
- **Arxiv ID**: http://arxiv.org/abs/2103.10619v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10619v2)
- **Published**: 2021-03-19 03:55:58+00:00
- **Updated**: 2021-08-18 10:18:22+00:00
- **Authors**: Zizheng Pan, Bohan Zhuang, Jing Liu, Haoyu He, Jianfei Cai
- **Comment**: Accepted to Proc. Int. Conf. Computer Vision (ICCV), 2021
- **Journal**: None
- **Summary**: The recently proposed Visual image Transformers (ViT) with pure attention have achieved promising performance on image recognition tasks, such as image classification. However, the routine of the current ViT model is to maintain a full-length patch sequence during inference, which is redundant and lacks hierarchical representation. To this end, we propose a Hierarchical Visual Transformer (HVT) which progressively pools visual tokens to shrink the sequence length and hence reduces the computational cost, analogous to the feature maps downsampling in Convolutional Neural Networks (CNNs). It brings a great benefit that we can increase the model capacity by scaling dimensions of depth/width/resolution/patch size without introducing extra computational complexity due to the reduced sequence length. Moreover, we empirically find that the average pooled visual tokens contain more discriminative information than the single class token. To demonstrate the improved scalability of our HVT, we conduct extensive experiments on the image classification task. With comparable FLOPs, our HVT outperforms the competitive baselines on ImageNet and CIFAR-100 datasets. Code is available at https://github.com/MonashAI/HVT



### Degrade is Upgrade: Learning Degradation for Low-light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2103.10621v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10621v3)
- **Published**: 2021-03-19 04:00:27+00:00
- **Updated**: 2021-12-13 20:05:39+00:00
- **Authors**: Kui Jiang, Zhongyuan Wang, Zheng Wang, Chen Chen, Peng Yi, Tao Lu, Chia-Wen Lin
- **Comment**: AAAI 2022
- **Journal**: None
- **Summary**: Low-light image enhancement aims to improve an image's visibility while keeping its visual naturalness. Different from existing methods tending to accomplish the relighting task directly by ignoring the fidelity and naturalness recovery, we investigate the intrinsic degradation and relight the low-light image while refining the details and color in two steps. Inspired by the color image formulation (diffuse illumination color plus environment illumination color), we first estimate the degradation from low-light inputs to simulate the distortion of environment illumination color, and then refine the content to recover the loss of diffuse illumination color. To this end, we propose a novel Degradation-to-Refinement Generation Network (DRGN). Its distinctive features can be summarized as 1) A novel two-step generation network for degradation learning and content refinement. It is not only superior to one-step methods, but also capable of synthesizing sufficient paired samples to benefit the model training; 2) A multi-resolution fusion network to represent the target information (degradation or contents) in a multi-scale cooperative manner, which is more effective to address the complex unmixing problems. Extensive experiments on both the enhancement task and joint detection task have verified the effectiveness and efficiency of our proposed method, surpassing the SOTA by \textit{0.70dB on average and 3.18\% in mAP}, respectively. The code is available at \url{https://github.com/kuijiang0802/DRGN}.



### Cluster-to-Conquer: A Framework for End-to-End Multi-Instance Learning for Whole Slide Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2103.10626v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.10626v2)
- **Published**: 2021-03-19 04:24:01+00:00
- **Updated**: 2021-06-13 18:48:18+00:00
- **Authors**: Yash Sharma, Aman Shrivastava, Lubaina Ehsan, Christopher A. Moskaluk, Sana Syed, Donald E. Brown
- **Comment**: Accepted at MIDL, 2021 - https://openreview.net/forum?id=7i1-2oKIELU
- **Journal**: None
- **Summary**: In recent years, the availability of digitized Whole Slide Images (WSIs) has enabled the use of deep learning-based computer vision techniques for automated disease diagnosis. However, WSIs present unique computational and algorithmic challenges. WSIs are gigapixel-sized ($\sim$100K pixels), making them infeasible to be used directly for training deep neural networks. Also, often only slide-level labels are available for training as detailed annotations are tedious and can be time-consuming for experts. Approaches using multiple-instance learning (MIL) frameworks have been shown to overcome these challenges. Current state-of-the-art approaches divide the learning framework into two decoupled parts: a convolutional neural network (CNN) for encoding the patches followed by an independent aggregation approach for slide-level prediction. In this approach, the aggregation step has no bearing on the representations learned by the CNN encoder. We have proposed an end-to-end framework that clusters the patches from a WSI into ${k}$-groups, samples ${k}'$ patches from each group for training, and uses an adaptive attention mechanism for slide level prediction; Cluster-to-Conquer (C2C). We have demonstrated that dividing a WSI into clusters can improve the model training by exposing it to diverse discriminative features extracted from the patches. We regularized the clustering mechanism by introducing a KL-divergence loss between the attention weights of patches in a cluster and the uniform distribution. The framework is optimized end-to-end on slide-level cross-entropy, patch-level cross-entropy, and KL-divergence loss (Implementation: https://github.com/YashSharma/C2C).



### CE-FPN: Enhancing Channel Information for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.10643v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10643v1)
- **Published**: 2021-03-19 05:51:53+00:00
- **Updated**: 2021-03-19 05:51:53+00:00
- **Authors**: Yihao Luo, Xiang Cao, Juntao Zhang, Xiang Cao, Jingjuan Guo, Haibo Shen, Tianjiang Wang, Qi Feng
- **Comment**: 9pages
- **Journal**: None
- **Summary**: Feature pyramid network (FPN) has been an effective framework to extract multi-scale features in object detection. However, current FPN-based methods mostly suffer from the intrinsic flaw of channel reduction, which brings about the loss of semantical information. And the miscellaneous fused feature maps may cause serious aliasing effects. In this paper, we present a novel channel enhancement feature pyramid network (CE-FPN) with three simple yet effective modules to alleviate these problems. Specifically, inspired by sub-pixel convolution, we propose a sub-pixel skip fusion method to perform both channel enhancement and upsampling. Instead of the original 1x1 convolution and linear upsampling, it mitigates the information loss due to channel reduction. Then we propose a sub-pixel context enhancement module for extracting more feature representations, which is superior to other context methods due to the utilization of rich channel information by sub-pixel convolution. Furthermore, a channel attention guided module is introduced to optimize the final integrated features on each level, which alleviates the aliasing effect only with a few computational burdens. Our experiments show that CE-FPN achieves competitive performance compared to state-of-the-art FPN-based detectors on MS COCO benchmark.



### Beyond Linear Subspace Clustering: A Comparative Study of Nonlinear Manifold Clustering Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2103.10656v1
- **DOI**: 10.1016/j.cosrev.2021.100435
- **Categories**: **cs.LG**, cs.AI, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2103.10656v1)
- **Published**: 2021-03-19 06:34:34+00:00
- **Updated**: 2021-03-19 06:34:34+00:00
- **Authors**: Maryam Abdolali, Nicolas Gillis
- **Comment**: 55 pages
- **Journal**: Computer Science Review 42, 100435, 2021
- **Summary**: Subspace clustering is an important unsupervised clustering approach. It is based on the assumption that the high-dimensional data points are approximately distributed around several low-dimensional linear subspaces. The majority of the prominent subspace clustering algorithms rely on the representation of the data points as linear combinations of other data points, which is known as a self-expressive representation. To overcome the restrictive linearity assumption, numerous nonlinear approaches were proposed to extend successful subspace clustering approaches to data on a union of nonlinear manifolds. In this comparative study, we provide a comprehensive overview of nonlinear subspace clustering approaches proposed in the last decade. We introduce a new taxonomy to classify the state-of-the-art approaches into three categories, namely locality preserving, kernel based, and neural network based. The major representative algorithms within each category are extensively compared on carefully designed synthetic and real-world data sets. The detailed analysis of these approaches unfolds potential research directions and unsolved challenges in this field.



### GPNAS: A Neural Network Architecture Search Framework Based on Graphical Predictor
- **Arxiv ID**: http://arxiv.org/abs/2103.11820v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.11820v6)
- **Published**: 2021-03-19 06:51:22+00:00
- **Updated**: 2021-07-08 09:15:05+00:00
- **Authors**: Dige Ai, Hong Zhang
- **Comment**: 12 pages,15 figures
- **Journal**: None
- **Summary**: In practice, the problems encountered in Neural Architecture Search (NAS) training are not simple problems, but often a series of difficult combinations (wrong compensation estimation, curse of dimension, overfitting, high complexity, etc.). In this paper, we propose a framework to decouple network structure from operator search space, and use two BOHBs to search alternatively. Considering that activation function and initialization are also important parts of neural network, the generalization ability of the model will be affected. We introduce an activation function and an initialization method domain, and add them into the operator search space to form a generalized search space, so as to improve the generalization ability of the child model. We then trained a GCN-based predictor using feedback from the child model. This can not only improve the search efficiency, but also solve the problem of dimension curse. Next, unlike other NAS studies, we used predictors to analyze the stability of different network structures. Finally, we applied our framework to neural structure search and achieved significant improvements on multiple datasets.



### XProtoNet: Diagnosis in Chest Radiography with Global and Local Explanations
- **Arxiv ID**: http://arxiv.org/abs/2103.10663v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10663v1)
- **Published**: 2021-03-19 07:18:21+00:00
- **Updated**: 2021-03-19 07:18:21+00:00
- **Authors**: Eunji Kim, Siwon Kim, Minji Seo, Sungroh Yoon
- **Comment**: 10 pages, 7 figures. Accepted to CVPR2021
- **Journal**: None
- **Summary**: Automated diagnosis using deep neural networks in chest radiography can help radiologists detect life-threatening diseases. However, existing methods only provide predictions without accurate explanations, undermining the trustworthiness of the diagnostic methods. Here, we present XProtoNet, a globally and locally interpretable diagnosis framework for chest radiography. XProtoNet learns representative patterns of each disease from X-ray images, which are prototypes, and makes a diagnosis on a given X-ray image based on the patterns. It predicts the area where a sign of the disease is likely to appear and compares the features in the predicted area with the prototypes. It can provide a global explanation, the prototype, and a local explanation, how the prototype contributes to the prediction of a single image. Despite the constraint for interpretability, XProtoNet achieves state-of-the-art classification performance on the public NIH chest X-ray dataset.



### Improving Image co-segmentation via Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.10670v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.10670v1)
- **Published**: 2021-03-19 07:30:42+00:00
- **Updated**: 2021-03-19 07:30:42+00:00
- **Authors**: Zhengwen Li, Xiabi Liu
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Deep Metric Learning (DML) is helpful in computer vision tasks. In this paper, we firstly introduce DML into image co-segmentation. We propose a novel Triplet loss for Image Segmentation, called IS-Triplet loss for short, and combine it with traditional image segmentation loss. Different from the general DML task which learns the metric between pictures, we treat each pixel as a sample, and use their embedded features in high-dimensional space to form triples, then we tend to force the distance between pixels of different categories greater than of the same category by optimizing IS-Triplet loss so that the pixels from different categories are easier to be distinguished in the high-dimensional feature space. We further present an efficient triple sampling strategy to make a feasible computation of IS-Triplet loss. Finally, the IS-Triplet loss is combined with 3 traditional image segmentation losses to perform image segmentation. We apply the proposed approach to image co-segmentation and test it on the SBCoseg dataset and the Internet dataset. The experimental result shows that our approach can effectively improve the discrimination of pixels' categories in high-dimensional space and thus help traditional loss achieve better performance of image segmentation with fewer training epochs.



### Learning Multiscale Correlations for Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2103.10674v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10674v2)
- **Published**: 2021-03-19 07:58:16+00:00
- **Updated**: 2021-07-12 07:09:23+00:00
- **Authors**: Honghong Zhou, Caili Guo, Hao Zhang, Yanjun Wang
- **Comment**: The paper has submitted to IEEE ICDL 2021, The codes will be
  available after the paper was accepted
- **Journal**: None
- **Summary**: In spite of the great progress in human motion prediction, it is still a challenging task to predict those aperiodic and complicated motions. We believe that to capture the correlations among human body components is the key to understand the human motion. In this paper, we propose a novel multiscale graph convolution network (MGCN) to address this problem. Firstly, we design an adaptive multiscale interactional encoding module (MIEM) which is composed of two sub modules: scale transformation module and scale interaction module to learn the human body correlations. Secondly, we apply a coarse-to-fine decoding strategy to decode the motions sequentially. We evaluate our approach on two standard benchmark datasets for human motion prediction: Human3.6M and CMU motion capture dataset. The experiments show that the proposed approach achieves the state-of-the-art performance for both short-term and long-term prediction especially in those complicated action category.



### Learning the Superpixel in a Non-iterative and Lifelong Manner
- **Arxiv ID**: http://arxiv.org/abs/2103.10681v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.10681v2)
- **Published**: 2021-03-19 08:19:37+00:00
- **Updated**: 2021-04-21 10:04:32+00:00
- **Authors**: Lei Zhu, Qi She, Bin Zhang, Yanye Lu, Zhilin Lu, Duo Li, Jie Hu
- **Comment**: Accept by CVPR2021
- **Journal**: None
- **Summary**: Superpixel is generated by automatically clustering pixels in an image into hundreds of compact partitions, which is widely used to perceive the object contours for its excellent contour adherence. Although some works use the Convolution Neural Network (CNN) to generate high-quality superpixel, we challenge the design principles of these networks, specifically for their dependence on manual labels and excess computation resources, which limits their flexibility compared with the traditional unsupervised segmentation methods. We target at redefining the CNN-based superpixel segmentation as a lifelong clustering task and propose an unsupervised CNN-based method called LNS-Net. The LNS-Net can learn superpixel in a non-iterative and lifelong manner without any manual labels. Specifically, a lightweight feature embedder is proposed for LNS-Net to efficiently generate the cluster-friendly features. With those features, seed nodes can be automatically assigned to cluster pixels in a non-iterative way. Additionally, our LNS-Net can adapt the sequentially lifelong learning by rescaling the gradient of weight based on both channel and spatial context to avoid overfitting. Experiments show that the proposed LNS-Net achieves significantly better performance on three benchmarks with nearly ten times lower complexity compared with other state-of-the-art methods.



### ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases
- **Arxiv ID**: http://arxiv.org/abs/2103.10697v2
- **DOI**: 10.1088/1742-5468/ac9830
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2103.10697v2)
- **Published**: 2021-03-19 09:11:20+00:00
- **Updated**: 2021-06-10 08:44:33+00:00
- **Authors**: Stéphane d'Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, Levent Sagun
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional architectures have proven extremely successful for vision tasks. Their hard inductive biases enable sample-efficient learning, but come at the cost of a potentially lower performance ceiling. Vision Transformers (ViTs) rely on more flexible self-attention layers, and have recently outperformed CNNs for image classification. However, they require costly pre-training on large external datasets or distillation from pre-trained convolutional networks. In this paper, we ask the following question: is it possible to combine the strengths of these two architectures while avoiding their respective limitations? To this end, we introduce gated positional self-attention (GPSA), a form of positional self-attention which can be equipped with a ``soft" convolutional inductive bias. We initialise the GPSA layers to mimic the locality of convolutional layers, then give each attention head the freedom to escape locality by adjusting a gating parameter regulating the attention paid to position versus content information. The resulting convolutional-like ViT architecture, ConViT, outperforms the DeiT on ImageNet, while offering a much improved sample efficiency. We further investigate the role of locality in learning by first quantifying how it is encouraged in vanilla self-attention layers, then analysing how it is escaped in GPSA layers. We conclude by presenting various ablations to better understand the success of the ConViT. Our code and models are released publicly at https://github.com/facebookresearch/convit.



### MDMMT: Multidomain Multimodal Transformer for Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2103.10699v1
- **DOI**: 10.1109/CVPRW53098.2021.00374
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10699v1)
- **Published**: 2021-03-19 09:16:39+00:00
- **Updated**: 2021-03-19 09:16:39+00:00
- **Authors**: Maksim Dzabraev, Maksim Kalashnikov, Stepan Komkov, Aleksandr Petiushko
- **Comment**: None
- **Journal**: CVPR Workshops 2021: 3354-3363
- **Summary**: We present a new state-of-the-art on the text to video retrieval task on MSRVTT and LSMDC benchmarks where our model outperforms all previous solutions by a large margin. Moreover, state-of-the-art results are achieved with a single model on two datasets without finetuning. This multidomain generalisation is achieved by a proper combination of different video caption datasets. We show that training on different datasets can improve test results of each other. Additionally we check intersection between many popular datasets and found that MSRVTT has a significant overlap between the test and the train parts, and the same situation is observed for ActivityNet.



### ClawCraneNet: Leveraging Object-level Relation for Text-based Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.10702v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10702v3)
- **Published**: 2021-03-19 09:31:08+00:00
- **Updated**: 2022-03-18 07:47:51+00:00
- **Authors**: Chen Liang, Yu Wu, Yawei Luo, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Text-based video segmentation is a challenging task that segments out the natural language referred objects in videos. It essentially requires semantic comprehension and fine-grained video understanding. Existing methods introduce language representation into segmentation models in a bottom-up manner, which merely conducts vision-language interaction within local receptive fields of ConvNets. We argue that such interaction is not fulfilled since the model can barely construct region-level relationships given partial observations, which is contrary to the description logic of natural language/referring expressions. In fact, people usually describe a target object using relations with other objects, which may not be easily understood without seeing the whole video. To address the issue, we introduce a novel top-down approach by imitating how we human segment an object with the language guidance. We first figure out all candidate objects in videos and then choose the refereed one by parsing relations among those high-level objects. Three kinds of object-level relations are investigated for precise relationship understanding, i.e., positional relation, text-guided semantic relation, and temporal relation. Extensive experiments on A2D Sentences and J-HMDB Sentences show our method outperforms state-of-the-art methods by a large margin. Qualitative results also show our results are more explainable.



### Connecting Images through Time and Sources: Introducing Low-data, Heterogeneous Instance Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2103.10729v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10729v1)
- **Published**: 2021-03-19 10:54:51+00:00
- **Updated**: 2021-03-19 10:54:51+00:00
- **Authors**: Dimitri Gominski, Valérie Gouet-Brunet, Liming Chen
- **Comment**: None
- **Journal**: None
- **Summary**: With impressive results in applications relying on feature learning, deep learning has also blurred the line between algorithm and data. Pick a training dataset, pick a backbone network for feature extraction, and voil\`a ; this usually works for a variety of use cases. But the underlying hypothesis that there exists a training dataset matching the use case is not always met. Moreover, the demand for interconnections regardless of the variations of the content calls for increasing generalization and robustness in features.   An interesting application characterized by these problematics is the connection of historical and cultural databases of images. Through the seemingly simple task of instance retrieval, we propose to show that it is not trivial to pick features responding well to a panel of variations and semantic content. Introducing a new enhanced version of the Alegoria benchmark, we compare descriptors using the detailed annotations. We further give insights about the core problems in instance retrieval, testing four state-of-the-art additional techniques to increase performance.



### Carton dataset synthesis method for domain shift based on foreground texture decoupling and replacement
- **Arxiv ID**: http://arxiv.org/abs/2103.10738v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10738v4)
- **Published**: 2021-03-19 11:21:31+00:00
- **Updated**: 2021-04-26 11:13:11+00:00
- **Authors**: Lijun Gou, Shengkai Wu, Jinrong Yang, Hangcheng Yu, Chenxi Lin, Xiaoping Li, Chao Deng
- **Comment**: None
- **Journal**: None
- **Summary**: One major impediment in rapidly deploying object detection models for industrial applications is the lack of large annotated datasets. We currently have presented the Sacked Carton Dataset(SCD) that contains carton images from three scenarios, such as comprehensive pharmaceutical logistics company(CPLC), e-commerce logistics company(ECLC), fruit market(FM). However, due to domain shift, the model trained with one of the three scenarios in SCD has poor generalization ability when applied to the rest scenarios. To solve this problem, a novel image synthesis method is proposed to replace the foreground texture of the source datasets with the texture of the target datasets. Our method can keep the context relationship of foreground objects and backgrounds unchanged and greatly augment the target datasets. We firstly propose a surface segmentation algorithm to achieve texture decoupling of each instance. Secondly, a contour reconstruction algorithm is proposed to keep the occlusion and truncation relationship of the instance unchanged. Finally, the Gaussian fusion algorithm is used to replace the foreground texture from the source datasets with the texture from the target datasets. The novel image synthesis method can largely boost AP by at least 4.3%~6.5% on RetinaNet and 3.4%~6.8% on Faster R-CNN for the target domain. Code is available at https://github.com/hustgetlijun/RCAN.



### Online Lifelong Generalized Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.10741v2
- **DOI**: 10.1016/j.neunet.2022.08.034
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.10741v2)
- **Published**: 2021-03-19 11:24:05+00:00
- **Updated**: 2021-03-22 03:05:03+00:00
- **Authors**: Chandan Gautam, Sethupathy Parameswaran, Ashish Mishra, Suresh Sundaram
- **Comment**: None
- **Journal**: None
- **Summary**: Methods proposed in the literature for zero-shot learning (ZSL) are typically suitable for offline learning and cannot continually learn from sequential streaming data. The sequential data comes in the form of tasks during training. Recently, a few attempts have been made to handle this issue and develop continual ZSL (CZSL) methods. However, these CZSL methods require clear task-boundary information between the tasks during training, which is not practically possible. This paper proposes a task-free (i.e., task-agnostic) CZSL method, which does not require any task information during continual learning. The proposed task-free CZSL method employs a variational autoencoder (VAE) for performing ZSL. To develop the CZSL method, we combine the concept of experience replay with knowledge distillation and regularization. Here, knowledge distillation is performed using the training sample's dark knowledge, which essentially helps overcome the catastrophic forgetting issue. Further, it is enabled for task-free learning using short-term memory. Finally, a classifier is trained on the synthetic features generated at the latent space of the VAE. Moreover, the experiments are conducted in a challenging and practical ZSL setup, i.e., generalized ZSL (GZSL). These experiments are conducted for two kinds of single-head continual learning settings: (i) mild setting-: task-boundary is known only during training but not during testing; (ii) strict setting-: task-boundary is not known at training, as well as testing. Experimental results on five benchmark datasets exhibit the validity of the approach for CZSL.



### DFS: A Diverse Feature Synthesis Model for Generalized Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.10764v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10764v1)
- **Published**: 2021-03-19 12:24:42+00:00
- **Updated**: 2021-03-19 12:24:42+00:00
- **Authors**: Bonan Li, Xuecheng Nie, Congying Han
- **Comment**: 11 pages,5 figures,conference
- **Journal**: None
- **Summary**: Generative based strategy has shown great potential in the Generalized Zero-Shot Learning task. However, it suffers severe generalization problem due to lacking of feature diversity for unseen classes to train a good classifier. In this paper, we propose to enhance the generalizability of GZSL models via improving feature diversity of unseen classes. For this purpose, we present a novel Diverse Feature Synthesis (DFS) model. Different from prior works that solely utilize semantic knowledge in the generation process, DFS leverages visual knowledge with semantic one in a unified way, thus deriving class-specific diverse feature samples and leading to robust classifier for recognizing both seen and unseen classes in the testing phase. To simplify the learning, DFS represents visual and semantic knowledge in the aligned space, making it able to produce good feature samples with a low-complexity implementation. Accordingly, DFS is composed of two consecutive generators: an aligned feature generator, transferring semantic and visual representations into aligned features; a synthesized feature generator, producing diverse feature samples of unseen classes in the aligned space. We conduct comprehensive experiments to verify the efficacy of DFS. Results demonstrate its effectiveness to generate diverse features for unseen classes, leading to superior performance on multiple benchmarks. Code will be released upon acceptance.



### There and Back Again: Self-supervised Multispectral Correspondence Estimation
- **Arxiv ID**: http://arxiv.org/abs/2103.10768v2
- **DOI**: 10.1109/ICRA48506.2021.9561621
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10768v2)
- **Published**: 2021-03-19 12:33:56+00:00
- **Updated**: 2021-05-26 15:53:22+00:00
- **Authors**: Celyn Walters, Oscar Mendez, Mark Johnson, Richard Bowden
- **Comment**: To be published in IEEE/RSJ International Conference on Robot and
  Automation (ICRA) 2021
- **Journal**: None
- **Summary**: Across a wide range of applications, from autonomous vehicles to medical imaging, multi-spectral images provide an opportunity to extract additional information not present in color images. One of the most important steps in making this information readily available is the accurate estimation of dense correspondences between different spectra.   Due to the nature of cross-spectral images, most correspondence solving techniques for the visual domain are simply not applicable. Furthermore, most cross-spectral techniques utilize spectra-specific characteristics to perform the alignment. In this work, we aim to address the dense correspondence estimation problem in a way that generalizes to more than one spectrum. We do this by introducing a novel cycle-consistency metric that allows us to self-supervise. This, combined with our spectra-agnostic loss functions, allows us to train the same network across multiple spectra.   We demonstrate our approach on the challenging task of dense RGB-FIR correspondence estimation. We also show the performance of our unmodified network on the cases of RGB-NIR and RGB-RGB, where we achieve higher accuracy than similar self-supervised approaches. Our work shows that cross-spectral correspondence estimation can be solved in a common framework that learns to generalize alignment across spectra.



### UniMoCo: Unsupervised, Semi-Supervised and Full-Supervised Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.10773v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10773v1)
- **Published**: 2021-03-19 12:42:09+00:00
- **Updated**: 2021-03-19 12:42:09+00:00
- **Authors**: Zhigang Dai, Bolun Cai, Yugeng Lin, Junying Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Momentum Contrast (MoCo) achieves great success for unsupervised visual representation. However, there are a lot of supervised and semi-supervised datasets, which are already labeled. To fully utilize the label annotations, we propose Unified Momentum Contrast (UniMoCo), which extends MoCo to support arbitrary ratios of labeled data and unlabeled data training. Compared with MoCo, UniMoCo has two modifications as follows: (1) Different from a single positive pair in MoCo, we maintain multiple positive pairs on-the-fly by comparing the query label to a label queue. (2) We propose a Unified Contrastive(UniCon) loss to support an arbitrary number of positives and negatives in a unified pair-wise optimization perspective. Our UniCon is more reasonable and powerful than the supervised contrastive loss in theory and practice. In our experiments, we pre-train multiple UniMoCo models with different ratios of ImageNet labels and evaluate the performance on various downstream tasks. Experiment results show that UniMoCo generalizes well for unsupervised, semi-supervised and supervised visual representation learning.



### LSDAT: Low-Rank and Sparse Decomposition for Decision-based Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/2103.10787v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2103.10787v2)
- **Published**: 2021-03-19 13:10:47+00:00
- **Updated**: 2021-03-22 16:07:28+00:00
- **Authors**: Ashkan Esmaeili, Marzieh Edraki, Nazanin Rahnavard, Mubarak Shah, Ajmal Mian
- **Comment**: None
- **Journal**: None
- **Summary**: We propose LSDAT, an image-agnostic decision-based black-box attack that exploits low-rank and sparse decomposition (LSD) to dramatically reduce the number of queries and achieve superior fooling rates compared to the state-of-the-art decision-based methods under given imperceptibility constraints. LSDAT crafts perturbations in the low-dimensional subspace formed by the sparse component of the input sample and that of an adversarial sample to obtain query-efficiency. The specific perturbation of interest is obtained by traversing the path between the input and adversarial sparse components. It is set forth that the proposed sparse perturbation is the most aligned sparse perturbation with the shortest path from the input sample to the decision boundary for some initial adversarial sample (the best sparse approximation of shortest path, likely to fool the model). Theoretical analyses are provided to justify the functionality of LSDAT. Unlike other dimensionality reduction based techniques aimed at improving query efficiency (e.g, ones based on FFT), LSD works directly in the image pixel domain to guarantee that non-$\ell_2$ constraints, such as sparsity, are satisfied. LSD offers better control over the number of queries and provides computational efficiency as it performs sparse decomposition of the input and adversarial images only once to generate all queries. We demonstrate $\ell_0$, $\ell_2$ and $\ell_\infty$ bounded attacks with LSDAT to evince its efficiency compared to baseline decision-based attacks in diverse low-query budget scenarios as outlined in the experiments.



### CoordiNet: uncertainty-aware pose regressor for reliable vehicle localization
- **Arxiv ID**: http://arxiv.org/abs/2103.10796v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.10796v2)
- **Published**: 2021-03-19 13:32:40+00:00
- **Updated**: 2021-10-07 07:59:55+00:00
- **Authors**: Arthur Moreau, Nathan Piasco, Dzmitry Tsishkou, Bogdan Stanciulescu, Arnaud de La Fortelle
- **Comment**: 8 pages, 8 figures. Accepted at WACV 2022
- **Journal**: None
- **Summary**: In this paper, we investigate visual-based camera re-localization with neural networks for robotics and autonomous vehicles applications. Our solution is a CNN-based algorithm which predicts camera pose (3D translation and 3D rotation) directly from a single image. It also provides an uncertainty estimate of the pose. Pose and uncertainty are learned together with a single loss function and are fused at test time with an EKF. Furthermore, we propose a new fully convolutional architecture, named CoordiNet, designed to embed some of the scene geometry. Our framework outperforms comparable methods on the largest available benchmark, the Oxford RobotCar dataset, with an average error of 8 meters where previous best was 19 meters. We have also investigated the performance of our method on large scenes for real time (18 fps) vehicle localization. In this setup, structure-based methods require a large database, and we show that our proposal is a reliable alternative, achieving 29cm median error in a 1.9km loop in a busy urban area



### Computational Emotion Analysis From Images: Recent Advances and Future Directions
- **Arxiv ID**: http://arxiv.org/abs/2103.10798v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.HC, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2103.10798v1)
- **Published**: 2021-03-19 13:33:34+00:00
- **Updated**: 2021-03-19 13:33:34+00:00
- **Authors**: Sicheng Zhao, Quanwei Huang, Youbao Tang, Xingxu Yao, Jufeng Yang, Guiguang Ding, Björn W. Schuller
- **Comment**: Accepted chapter in the book "Human Perception of Visual Information
  Psychological and Computational Perspective"
- **Journal**: None
- **Summary**: Emotions are usually evoked in humans by images. Recently, extensive research efforts have been dedicated to understanding the emotions of images. In this chapter, we aim to introduce image emotion analysis (IEA) from a computational perspective with the focus on summarizing recent advances and suggesting future directions. We begin with commonly used emotion representation models from psychology. We then define the key computational problems that the researchers have been trying to solve and provide supervised frameworks that are generally used for different IEA tasks. After the introduction of major challenges in IEA, we present some representative methods on emotion feature extraction, supervised classifier learning, and domain adaptation. Furthermore, we introduce available datasets for evaluation and summarize some main results. Finally, we discuss some open questions and future directions that researchers can pursue.



### Skeleton Merger: an Unsupervised Aligned Keypoint Detector
- **Arxiv ID**: http://arxiv.org/abs/2103.10814v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10814v1)
- **Published**: 2021-03-19 14:00:39+00:00
- **Updated**: 2021-03-19 14:00:39+00:00
- **Authors**: Ruoxi Shi, Zhengrong Xue, Yang You, Cewu Lu
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Detecting aligned 3D keypoints is essential under many scenarios such as object tracking, shape retrieval and robotics. However, it is generally hard to prepare a high-quality dataset for all types of objects due to the ambiguity of keypoint itself. Meanwhile, current unsupervised detectors are unable to generate aligned keypoints with good coverage. In this paper, we propose an unsupervised aligned keypoint detector, Skeleton Merger, which utilizes skeletons to reconstruct objects. It is based on an Autoencoder architecture. The encoder proposes keypoints and predicts activation strengths of edges between keypoints. The decoder performs uniform sampling on the skeleton and refines it into small point clouds with pointwise offsets. Then the activation strengths are applied and the sub-clouds are merged. Composite Chamfer Distance (CCD) is proposed as a distance between the input point cloud and the reconstruction composed of sub-clouds masked by activation strengths. We demonstrate that Skeleton Merger is capable of detecting semantically-rich salient keypoints with good alignment, and shows comparable performance to supervised methods on the KeypointNet dataset. It is also shown that the detector is robust to noise and subsampling. Our code is available at https://github.com/eliphatfs/SkeletonMerger.



### Efficient Visual Pretraining with Contrastive Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.10957v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10957v2)
- **Published**: 2021-03-19 14:05:12+00:00
- **Updated**: 2021-08-05 15:51:15+00:00
- **Authors**: Olivier J. Hénaff, Skanda Koppula, Jean-Baptiste Alayrac, Aaron van den Oord, Oriol Vinyals, João Carreira
- **Comment**: Technical report
- **Journal**: None
- **Summary**: Self-supervised pretraining has been shown to yield powerful representations for transfer learning. These performance gains come at a large computational cost however, with state-of-the-art methods requiring an order of magnitude more computation than supervised pretraining. We tackle this computational bottleneck by introducing a new self-supervised objective, contrastive detection, which tasks representations with identifying object-level features across augmentations. This objective extracts a rich learning signal per image, leading to state-of-the-art transfer accuracy on a variety of downstream tasks, while requiring up to 10x less pretraining. In particular, our strongest ImageNet-pretrained model performs on par with SEER, one of the largest self-supervised systems to date, which uses 1000x more pretraining data. Finally, our objective seamlessly handles pretraining on more complex images such as those in COCO, closing the gap with supervised transfer learning from COCO to PASCAL.



### Variational Knowledge Distillation for Disease Classification in Chest X-Rays
- **Arxiv ID**: http://arxiv.org/abs/2103.10825v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.10825v1)
- **Published**: 2021-03-19 14:13:56+00:00
- **Updated**: 2021-03-19 14:13:56+00:00
- **Authors**: Tom van Sonsbeek, Xiantong Zhen, Marcel Worring, Ling Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Disease classification relying solely on imaging data attracts great interest in medical image analysis. Current models could be further improved, however, by also employing Electronic Health Records (EHRs), which contain rich information on patients and findings from clinicians. It is challenging to incorporate this information into disease classification due to the high reliance on clinician input in EHRs, limiting the possibility for automated diagnosis. In this paper, we propose \textit{variational knowledge distillation} (VKD), which is a new probabilistic inference framework for disease classification based on X-rays that leverages knowledge from EHRs. Specifically, we introduce a conditional latent variable model, where we infer the latent representation of the X-ray image with the variational posterior conditioning on the associated EHR text. By doing so, the model acquires the ability to extract the visual features relevant to the disease during learning and can therefore perform more accurate classification for unseen patients at inference based solely on their X-ray scans. We demonstrate the effectiveness of our method on three public benchmark datasets with paired X-ray images and EHRs. The results show that the proposed variational knowledge distillation can consistently improve the performance of medical image classification and significantly surpasses current methods.



### Toward Compact Deep Neural Networks via Energy-Aware Pruning
- **Arxiv ID**: http://arxiv.org/abs/2103.10858v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.10858v2)
- **Published**: 2021-03-19 15:33:16+00:00
- **Updated**: 2022-03-10 14:34:54+00:00
- **Authors**: Seul-Ki Yeom, Kyung-Hwan Shim, Jee-Hyun Hwang
- **Comment**: 10 pages, 5 figures, 3 tables
- **Journal**: None
- **Summary**: Despite the remarkable performance, modern deep neural networks are inevitably accompanied by a significant amount of computational cost for learning and deployment, which may be incompatible with their usage on edge devices. Recent efforts to reduce these overheads involve pruning and decomposing the parameters of various layers without performance deterioration. Inspired by several decomposition studies, in this paper, we propose a novel energy-aware pruning method that quantifies the importance of each filter in the network using nuclear-norm (NN). Proposed energy-aware pruning leads to state-of-the-art performance for Top-1 accuracy, FLOPs, and parameter reduction across a wide range of scenarios with multiple network architectures on CIFAR-10 and ImageNet after fine-grained classification tasks. On toy experiment, without fine-tuning, we can visually observe that NN has a minute change in decision boundaries across classes and outperforms the previous popular criteria. We achieve competitive results with 40.4/49.8% of FLOPs and 45.9/52.9% of parameter reduction with 94.13/94.61% in the Top-1 accuracy with ResNet-56/110 on CIFAR-10, respectively. In addition, our observations are consistent for a variety of different pruning setting in terms of data size as well as data quality which can be emphasized in the stability of the acceleration and compression with negligible accuracy loss.



### GLOWin: A Flow-based Invertible Generative Framework for Learning Disentangled Feature Representations in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2103.10868v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10868v1)
- **Published**: 2021-03-19 15:47:01+00:00
- **Updated**: 2021-03-19 15:47:01+00:00
- **Authors**: Aadhithya Sankar, Matthias Keicher, Rami Eisawy, Abhijeet Parida, Franz Pfister, Seong Tae Kim, Nassir Navab
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Disentangled representations can be useful in many downstream tasks, help to make deep learning models more interpretable, and allow for control over features of synthetically generated images that can be useful in training other models that require a large number of labelled or unlabelled data. Recently, flow-based generative models have been proposed to generate realistic images by directly modeling the data distribution with invertible functions. In this work, we propose a new flow-based generative model framework, named GLOWin, that is end-to-end invertible and able to learn disentangled representations. Feature disentanglement is achieved by factorizing the latent space into components such that each component learns the representation for one generative factor. Comprehensive experiments have been conducted to evaluate the proposed method on a public brain tumor MR dataset. Quantitative and qualitative results suggest that the proposed method is effective in disentangling the features from complex medical images.



### MetaLabelNet: Learning to Generate Soft-Labels from Noisy-Labels
- **Arxiv ID**: http://arxiv.org/abs/2103.10869v1
- **DOI**: 10.1109/TIP.2022.3183841
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.10869v1)
- **Published**: 2021-03-19 15:47:44+00:00
- **Updated**: 2021-03-19 15:47:44+00:00
- **Authors**: Görkem Algan, Ilkay Ulusoy
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world datasets commonly have noisy labels, which negatively affects the performance of deep neural networks (DNNs). In order to address this problem, we propose a label noise robust learning algorithm, in which the base classifier is trained on soft-labels that are produced according to a meta-objective. In each iteration, before conventional training, the meta-objective reshapes the loss function by changing soft-labels, so that resulting gradient updates would lead to model parameters with minimum loss on meta-data. Soft-labels are generated from extracted features of data instances, and the mapping function is learned by a single layer perceptron (SLP) network, which is called MetaLabelNet. Following, base classifier is trained by using these generated soft-labels. These iterations are repeated for each batch of training data. Our algorithm uses a small amount of clean data as meta-data, which can be obtained effortlessly for many cases. We perform extensive experiments on benchmark datasets with both synthetic and real-world noises. Results show that our approach outperforms existing baselines.



### Deep Label Fusion: A 3D End-to-End Hybrid Multi-Atlas Segmentation and Deep Learning Pipeline
- **Arxiv ID**: http://arxiv.org/abs/2103.10892v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.10892v1)
- **Published**: 2021-03-19 16:25:38+00:00
- **Updated**: 2021-03-19 16:25:38+00:00
- **Authors**: Long Xie, Laura E. M. Wisse, Jiancong Wang, Sadhana Ravikumar, Trevor Glenn, Anica Luther, Sydney Lim, David A. Wolk, Paul A. Yushkevich
- **Comment**: 12 pages paper accepted by the international conference of
  Information Processing in Medical Imaging (IPMI) 2021
- **Journal**: None
- **Summary**: Deep learning (DL) is the state-of-the-art methodology in various medical image segmentation tasks. However, it requires relatively large amounts of manually labeled training data, which may be infeasible to generate in some applications. In addition, DL methods have relatively poor generalizability to out-of-sample data. Multi-atlas segmentation (MAS), on the other hand, has promising performance using limited amounts of training data and good generalizability. A hybrid method that integrates the high accuracy of DL and good generalizability of MAS is highly desired and could play an important role in segmentation problems where manually labeled data is hard to generate. Most of the prior work focuses on improving single components of MAS using DL rather than directly optimizing the final segmentation accuracy via an end-to-end pipeline. Only one study explored this idea in binary segmentation of 2D images, but it remains unknown whether it generalizes well to multi-class 3D segmentation problems. In this study, we propose a 3D end-to-end hybrid pipeline, named deep label fusion (DLF), that takes advantage of the strengths of MAS and DL. Experimental results demonstrate that DLF yields significant improvements over conventional label fusion methods and U-Net, a direct DL approach, in the context of segmenting medial temporal lobe subregions using 3T T1-weighted and T2-weighted MRI. Further, when applied to an unseen similar dataset acquired in 7T, DLF maintains its superior performance, which demonstrates its good generalizability.



### Sewer-ML: A Multi-Label Sewer Defect Classification Dataset and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2103.10895v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10895v1)
- **Published**: 2021-03-19 16:32:37+00:00
- **Updated**: 2021-03-19 16:32:37+00:00
- **Authors**: Joakim Bruslund Haurum, Thomas B. Moeslund
- **Comment**: CVPR 2021. Project webpage: https://vap.aau.dk/sewer-ml/
- **Journal**: None
- **Summary**: Perhaps surprisingly sewerage infrastructure is one of the most costly infrastructures in modern society. Sewer pipes are manually inspected to determine whether the pipes are defective. However, this process is limited by the number of qualified inspectors and the time it takes to inspect a pipe. Automatization of this process is therefore of high interest. So far, the success of computer vision approaches for sewer defect classification has been limited when compared to the success in other fields mainly due to the lack of public datasets. To this end, in this work we present a large novel and publicly available multi-label classification dataset for image-based sewer defect classification called Sewer-ML.   The Sewer-ML dataset consists of 1.3 million images annotated by professional sewer inspectors from three different utility companies across nine years. Together with the dataset, we also present a benchmark algorithm and a novel metric for assessing performance. The benchmark algorithm is a result of evaluating 12 state-of-the-art algorithms, six from the sewer defect classification domain and six from the multi-label classification domain, and combining the best performing algorithms. The novel metric is a class-importance weighted F2 score, $\text{F}2_{\text{CIW}}$, reflecting the economic impact of each class, used together with the normal pipe F1 score, $\text{F}1_{\text{Normal}}$. The benchmark algorithm achieves an $\text{F}2_{\text{CIW}}$ score of 55.11% and $\text{F}1_{\text{Normal}}$ score of 90.94%, leaving ample room for improvement on the Sewer-ML dataset. The code, models, and dataset are available at the project page https://vap.aau.dk/sewer-ml/



### Robustness via Cross-Domain Ensembles
- **Arxiv ID**: http://arxiv.org/abs/2103.10919v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.10919v2)
- **Published**: 2021-03-19 17:28:03+00:00
- **Updated**: 2021-09-03 12:49:40+00:00
- **Authors**: Teresa Yeo, Oğuzhan Fatih Kar, Alexander Sax, Amir Zamir
- **Comment**: Project website at https://crossdomain-ensembles.epfl.ch/
- **Journal**: None
- **Summary**: We present a method for making neural network predictions robust to shifts from the training data distribution. The proposed method is based on making predictions via a diverse set of cues (called 'middle domains') and ensembling them into one strong prediction. The premise of the idea is that predictions made via different cues respond differently to a distribution shift, hence one should be able to merge them into one robust final prediction. We perform the merging in a straightforward but principled manner based on the uncertainty associated with each prediction. The evaluations are performed using multiple tasks and datasets (Taskonomy, Replica, ImageNet, CIFAR) under a wide range of adversarial and non-adversarial distribution shifts which demonstrate the proposed method is considerably more robust than its standard learning counterpart, conventional deep ensembles, and several other baselines.



### Paint by Word
- **Arxiv ID**: http://arxiv.org/abs/2103.10951v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, I.2.10; I.4; I.3
- **Links**: [PDF](http://arxiv.org/pdf/2103.10951v3)
- **Published**: 2021-03-19 17:59:08+00:00
- **Updated**: 2023-03-23 21:31:18+00:00
- **Authors**: Alex Andonian, Sabrina Osmany, Audrey Cui, YeonHwan Park, Ali Jahanian, Antonio Torralba, David Bau
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: We investigate the problem of zero-shot semantic image painting. Instead of painting modifications into an image using only concrete colors or a finite set of semantic concepts, we ask how to create semantic paint based on open full-text descriptions: our goal is to be able to point to a location in a synthesized image and apply an arbitrary new concept such as "rustic" or "opulent" or "happy dog." To do this, our method combines a state-of-the art generative model of realistic images with a state-of-the-art text-image semantic similarity network. We find that, to make large changes, it is important to use non-gradient methods to explore latent space, and it is important to relax the computations of the GAN to target changes to a specific region. We conduct user studies to compare our methods to several baselines.



### Probabilistic 3D Human Shape and Pose Estimation from Multiple Unconstrained Images in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2103.10978v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10978v2)
- **Published**: 2021-03-19 18:32:16+00:00
- **Updated**: 2021-03-30 15:17:30+00:00
- **Authors**: Akash Sengupta, Ignas Budvytis, Roberto Cipolla
- **Comment**: Accepted at CVPR 2021, 16 pages, 8 figures
- **Journal**: None
- **Summary**: This paper addresses the problem of 3D human body shape and pose estimation from RGB images. Recent progress in this field has focused on single images, video or multi-view images as inputs. In contrast, we propose a new task: shape and pose estimation from a group of multiple images of a human subject, without constraints on subject pose, camera viewpoint or background conditions between images in the group. Our solution to this task predicts distributions over SMPL body shape and pose parameters conditioned on the input images in the group. We probabilistically combine predicted body shape distributions from each image to obtain a final multi-image shape prediction. We show that the additional body shape information present in multi-image input groups improves 3D human shape estimation metrics compared to single-image inputs on the SSP-3D dataset and a private dataset of tape-measured humans. In addition, predicting distributions over 3D bodies allows us to quantify pose prediction uncertainty, which is useful when faced with challenging input images with significant occlusion. Our method demonstrates meaningful pose uncertainty on the 3DPW dataset and is competitive with the state-of-the-art in terms of pose estimation metrics.



### HDR Video Reconstruction with Tri-Exposure Quad-Bayer Sensors
- **Arxiv ID**: http://arxiv.org/abs/2103.10982v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.10982v1)
- **Published**: 2021-03-19 18:40:09+00:00
- **Updated**: 2021-03-19 18:40:09+00:00
- **Authors**: Yitong Jiang, Inchang Choi, Jun Jiang, Jinwei Gu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel high dynamic range (HDR) video reconstruction method with new tri-exposure quad-bayer sensors. Thanks to the larger number of exposure sets and their spatially uniform deployment over a frame, they are more robust to noise and spatial artifacts than previous spatially varying exposure (SVE) HDR video methods. Nonetheless, the motion blur from longer exposures, the noise from short exposures, and inherent spatial artifacts of the SVE methods remain huge obstacles. Additionally, temporal coherence must be taken into account for the stability of video reconstruction. To tackle these challenges, we introduce a novel network architecture that divides-and-conquers these problems. In order to better adapt the network to the large dynamic range, we also propose LDR-reconstruction loss that takes equal contributions from both the highlighted and the shaded pixels of HDR frames. Through a series of comparisons and ablation studies, we show that the tri-exposure quad-bayer with our solution is more optimal to capture than previous reconstruction methods, particularly for the scenes with larger dynamic range and objects with motion.



### Self-Supervised Classification Network
- **Arxiv ID**: http://arxiv.org/abs/2103.10994v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10994v3)
- **Published**: 2021-03-19 19:29:42+00:00
- **Updated**: 2022-07-12 14:25:52+00:00
- **Authors**: Elad Amrani, Leonid Karlinsky, Alex Bronstein
- **Comment**: ECCV 2022 camera-ready with supplementary
- **Journal**: None
- **Summary**: We present Self-Classifier -- a novel self-supervised end-to-end classification learning approach. Self-Classifier learns labels and representations simultaneously in a single-stage end-to-end manner by optimizing for same-class prediction of two augmented views of the same sample. To guarantee non-degenerate solutions (i.e., solutions where all labels are assigned to the same class) we propose a mathematically motivated variant of the cross-entropy loss that has a uniform prior asserted on the predicted labels. In our theoretical analysis, we prove that degenerate solutions are not in the set of optimal solutions of our approach. Self-Classifier is simple to implement and scalable. Unlike other popular unsupervised classification and contrastive representation learning approaches, it does not require any form of pre-training, expectation-maximization, pseudo-labeling, external clustering, a second network, stop-gradient operation, or negative pairs. Despite its simplicity, our approach sets a new state of the art for unsupervised classification of ImageNet; and even achieves comparable to state-of-the-art results for unsupervised representation learning. Code is available at https://github.com/elad-amrani/self-classifier.



### Attribution of Gradient Based Adversarial Attacks for Reverse Engineering of Deceptions
- **Arxiv ID**: http://arxiv.org/abs/2103.11002v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.11002v1)
- **Published**: 2021-03-19 19:55:00+00:00
- **Updated**: 2021-03-19 19:55:00+00:00
- **Authors**: Michael Goebel, Jason Bunk, Srinjoy Chattopadhyay, Lakshmanan Nataraj, Shivkumar Chandrasekaran, B. S. Manjunath
- **Comment**: None
- **Journal**: None
- **Summary**: Machine Learning (ML) algorithms are susceptible to adversarial attacks and deception both during training and deployment. Automatic reverse engineering of the toolchains behind these adversarial machine learning attacks will aid in recovering the tools and processes used in these attacks. In this paper, we present two techniques that support automated identification and attribution of adversarial ML attack toolchains using Co-occurrence Pixel statistics and Laplacian Residuals. Our experiments show that the proposed techniques can identify parameters used to generate adversarial samples. To the best of our knowledge, this is the first approach to attribute gradient based adversarial attacks and estimate their parameters. Source code and data is available at: https://github.com/michael-goebel/ei_red



### AxonNet: A self-supervised Deep Neural Network for Intravoxel Structure Estimation from DW-MRI
- **Arxiv ID**: http://arxiv.org/abs/2103.11006v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.11006v1)
- **Published**: 2021-03-19 20:11:03+00:00
- **Updated**: 2021-03-19 20:11:03+00:00
- **Authors**: Hanna Ehrlich, Mariano Rivera
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for estimating intravoxel parameters from a DW-MRI based on deep learning techniques. We show that neural networks (DNNs) have the potential to extract information from diffusion-weighted signals to reconstruct cerebral tracts. We present two DNN models: one that estimates the axonal structure in the form of a voxel and the other to calculate the structure of the central voxel using the voxel neighborhood. Our methods are based on a proposed parameter representation suitable for the problem. Since it is practically impossible to have real tagged data for any acquisition protocol, we used a self-supervised strategy. Experiments with synthetic data and real data show that our approach is competitive, and the computational times show that our approach is faster than the SOTA methods, even if training times are considered. This computational advantage increases if we consider the prediction of multiple images with the same acquisition protocol.



### Video Class Agnostic Segmentation Benchmark for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2103.11015v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11015v2)
- **Published**: 2021-03-19 20:41:40+00:00
- **Updated**: 2021-04-20 03:19:47+00:00
- **Authors**: Mennatullah Siam, Alex Kendall, Martin Jagersand
- **Comment**: Accepted in WAD workshop, CVPR 2021
- **Journal**: None
- **Summary**: Semantic segmentation approaches are typically trained on large-scale data with a closed finite set of known classes without considering unknown objects. In certain safety-critical robotics applications, especially autonomous driving, it is important to segment all objects, including those unknown at training time. We formalize the task of video class agnostic segmentation from monocular video sequences in autonomous driving to account for unknown objects. Video class agnostic segmentation can be formulated as an open-set or a motion segmentation problem. We discuss both formulations and provide datasets and benchmark different baseline approaches for both tracks. In the motion-segmentation track we benchmark real-time joint panoptic and motion instance segmentation, and evaluate the effect of ego-flow suppression. In the open-set segmentation track we evaluate baseline methods that combine appearance, and geometry to learn prototypes per semantic class. We then compare it to a model that uses an auxiliary contrastive loss to improve the discrimination between known and unknown objects. Datasets and models are publicly released at https://msiam.github.io/vca/.



### TDIOT: Target-driven Inference for Deep Video Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2103.11017v2
- **DOI**: 10.1109/TIP.2021.3112010
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.11017v2)
- **Published**: 2021-03-19 20:45:06+00:00
- **Updated**: 2021-03-23 08:51:19+00:00
- **Authors**: Filiz Gurkan, Llukman Cerkezi, Ozgun Cirakman, Bilge Gunsel
- **Comment**: None
- **Journal**: None
- **Summary**: Recent tracking-by-detection approaches use deep object detectors as target detection baseline, because of their high performance on still images. For effective video object tracking, object detection is integrated with a data association step performed by either a custom design inference architecture or an end-to-end joint training for tracking purpose. In this work, we adopt the former approach and use the pre-trained Mask R-CNN deep object detector as the baseline. We introduce a novel inference architecture placed on top of FPN-ResNet101 backbone of Mask R-CNN to jointly perform detection and tracking, without requiring additional training for tracking purpose. The proposed single object tracker, TDIOT, applies an appearance similarity-based temporal matching for data association. In order to tackle tracking discontinuities, we incorporate a local search and matching module into the inference head layer that exploits SiamFC for short term tracking. Moreover, in order to improve robustness to scale changes, we introduce a scale adaptive region proposal network that enables to search the target at an adaptively enlarged spatial neighborhood specified by the trace of the target. In order to meet long term tracking requirements, a low cost verification layer is incorporated into the inference architecture to monitor presence of the target based on its LBP histogram model. Performance evaluation on videos from VOT2016, VOT2018 and VOT-LT2018 datasets demonstrate that TDIOT achieves higher accuracy compared to the state-of-the-art short-term trackers while it provides comparable performance in long term tracking.



### Bootstrapped Self-Supervised Training with Monocular Video for Semantic Segmentation and Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2103.11031v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.11031v2)
- **Published**: 2021-03-19 21:28:58+00:00
- **Updated**: 2021-08-01 03:50:19+00:00
- **Authors**: Yihao Zhang, John J. Leonard
- **Comment**: IROS 2021
- **Journal**: None
- **Summary**: For a robot deployed in the world, it is desirable to have the ability of autonomous learning to improve its initial pre-set knowledge. We formalize this as a bootstrapped self-supervised learning problem where a system is initially bootstrapped with supervised training on a labeled dataset and we look for a self-supervised training method that can subsequently improve the system over the supervised training baseline using only unlabeled data. In this work, we leverage temporal consistency between frames in monocular video to perform this bootstrapped self-supervised training. We show that a well-trained state-of-the-art semantic segmentation network can be further improved through our method. In addition, we show that the bootstrapped self-supervised training framework can help a network learn depth estimation better than pure supervised training or self-supervised training.



### A first step towards automated species recognition from camera trap images of mammals using AI in a European temperate forest
- **Arxiv ID**: http://arxiv.org/abs/2103.11052v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, 68T07, I.2; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2103.11052v1)
- **Published**: 2021-03-19 22:48:03+00:00
- **Updated**: 2021-03-19 22:48:03+00:00
- **Authors**: Mateusz Choinski, Mateusz Rogowski, Piotr Tynecki, Dries P. J. Kuijper, Marcin Churski, Jakub W. Bubnicki
- **Comment**: CISIM 2021 conference paper
- **Journal**: None
- **Summary**: Camera traps are used worldwide to monitor wildlife. Despite the increasing availability of Deep Learning (DL) models, the effective usage of this technology to support wildlife monitoring is limited. This is mainly due to the complexity of DL technology and high computing requirements. This paper presents the implementation of the light-weight and state-of-the-art YOLOv5 architecture for automated labeling of camera trap images of mammals in the Bialowieza Forest (BF), Poland. The camera trapping data were organized and harmonized using TRAPPER software, an open source application for managing large-scale wildlife monitoring projects. The proposed image recognition pipeline achieved an average accuracy of 85% F1-score in the identification of the 12 most commonly occurring medium-size and large mammal species in BF using a limited set of training and testing data (a total 2659 images with animals).   Based on the preliminary results, we concluded that the YOLOv5 object detection and classification model is a promising light-weight DL solution after the adoption of transfer learning technique. It can be efficiently plugged in via an API into existing web-based camera trapping data processing platforms such as e.g. TRAPPER system. Since TRAPPER is already used to manage and classify (manually) camera trapping datasets by many research groups in Europe, the implementation of AI-based automated species classification may significantly speed up the data processing workflow and thus better support data-driven wildlife monitoring and conservation. Moreover, YOLOv5 developers perform better performance on edge devices which may open a new chapter in animal population monitoring in real time directly from camera trap devices.



### ConDA: Continual Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2103.11056v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2103.11056v2)
- **Published**: 2021-03-19 23:20:41+00:00
- **Updated**: 2021-04-07 23:28:11+00:00
- **Authors**: Abu Md Niamul Taufique, Chowdhury Sadman Jahan, Andreas Savakis
- **Comment**: 10pages, 4 figures
- **Journal**: None
- **Summary**: Domain Adaptation (DA) techniques are important for overcoming the domain shift between the source domain used for training and the target domain where testing takes place. However, current DA methods assume that the entire target domain is available during adaptation, which may not hold in practice. This paper considers a more realistic scenario, where target data become available in smaller batches and adaptation on the entire target domain is not feasible. In our work, we introduce a new, data-constrained DA paradigm where unlabeled target samples are received in batches and adaptation is performed continually. We propose a novel source-free method for continual unsupervised domain adaptation that utilizes a buffer for selective replay of previously seen samples. In our continual DA framework, we selectively mix samples from incoming batches with data stored in a buffer using buffer management strategies and use the combination to incrementally update our model. We evaluate the classification performance of the continual DA approach with state-of-the-art DA methods based on the entire target domain. Our results on three popular DA datasets demonstrate that our method outperforms many existing state-of-the-art DA methods with access to the entire target domain during adaptation.



