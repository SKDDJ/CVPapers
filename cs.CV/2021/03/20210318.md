# Arxiv Papers in cs.CV on 2021-03-18
### CheXbreak: Misclassification Identification for Deep Learning Models Interpreting Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/2103.09957v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.09957v3)
- **Published**: 2021-03-18 00:30:19+00:00
- **Updated**: 2021-07-20 17:20:35+00:00
- **Authors**: Emma Chen, Andy Kim, Rayan Krishnan, Jin Long, Andrew Y. Ng, Pranav Rajpurkar
- **Comment**: In Proceedings of the 2021 Conference on Machine Learning for Health
  Care, 2021. In ACM Conference on Health, Inference, and Learning (ACM-CHIL)
  Workshop 2021
- **Journal**: None
- **Summary**: A major obstacle to the integration of deep learning models for chest x-ray interpretation into clinical settings is the lack of understanding of their failure modes. In this work, we first investigate whether there are patient subgroups that chest x-ray models are likely to misclassify. We find that patient age and the radiographic finding of lung lesion, pneumothorax or support devices are statistically relevant features for predicting misclassification for some chest x-ray models. Second, we develop misclassification predictors on chest x-ray models using their outputs and clinical features. We find that our best performing misclassification identifier achieves an AUROC close to 0.9 for most diseases. Third, employing our misclassification identifiers, we develop a corrective algorithm to selectively flip model predictions that have high likelihood of misclassification at inference time. We observe F1 improvement on the prediction of Consolidation (0.008 [95% CI 0.005, 0.010]) and Edema (0.003, [95% CI 0.001, 0.006]). By carrying out our investigation on ten distinct and high-performing chest x-ray models, we are able to derive insights across model architectures and offer a generalizable framework applicable to other medical imaging tasks.



### Deep Wiener Deconvolution: Wiener Meets Deep Learning for Image Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2103.09962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09962v1)
- **Published**: 2021-03-18 00:38:11+00:00
- **Updated**: 2021-03-18 00:38:11+00:00
- **Authors**: Jiangxin Dong, Stefan Roth, Bernt Schiele
- **Comment**: Accepted to NeurIPS 2020 as an oral presentation. Project page:
  https://gitlab.mpi-klsb.mpg.de/jdong/dwdn
- **Journal**: None
- **Summary**: We present a simple and effective approach for non-blind image deblurring, combining classical techniques and deep learning. In contrast to existing methods that deblur the image directly in the standard image space, we propose to perform an explicit deconvolution process in a feature space by integrating a classical Wiener deconvolution framework with learned deep features. A multi-scale feature refinement module then predicts the deblurred image from the deconvolved deep features, progressively recovering detail and small-scale structures. The proposed model is trained in an end-to-end manner and evaluated on scenarios with both simulated and real-world image blur. Our extensive experimental results show that the proposed deep Wiener deconvolution network facilitates deblurred results with visibly fewer artifacts. Moreover, our approach quantitatively outperforms state-of-the-art non-blind image deblurring methods by a wide margin.



### Topology-Aware Segmentation Using Discrete Morse Theory
- **Arxiv ID**: http://arxiv.org/abs/2103.09992v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2103.09992v1)
- **Published**: 2021-03-18 02:47:21+00:00
- **Updated**: 2021-03-18 02:47:21+00:00
- **Authors**: Xiaoling Hu, Yusu Wang, Li Fuxin, Dimitris Samaras, Chao Chen
- **Comment**: 19 pages, 11 figures
- **Journal**: None
- **Summary**: In the segmentation of fine-scale structures from natural and biomedical images, per-pixel accuracy is not the only metric of concern. Topological correctness, such as vessel connectivity and membrane closure, is crucial for downstream analysis tasks. In this paper, we propose a new approach to train deep image segmentation networks for better topological accuracy. In particular, leveraging the power of discrete Morse theory (DMT), we identify global structures, including 1D skeletons and 2D patches, which are important for topological accuracy. Trained with a novel loss based on these global structures, the network performance is significantly improved especially near topologically challenging locations (such as weak spots of connections and membranes). On diverse datasets, our method achieves superior performance on both the DICE score and topological metrics.



### Rapid treatment planning for low-dose-rate prostate brachytherapy with TP-GAN
- **Arxiv ID**: http://arxiv.org/abs/2103.09996v1
- **DOI**: 10.1007/978-3-030-87202-1_56
- **Categories**: **cs.CV**, physics.med-ph, I.5.1; I.5.2; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2103.09996v1)
- **Published**: 2021-03-18 03:02:45+00:00
- **Updated**: 2021-03-18 03:02:45+00:00
- **Authors**: Tajwar Abrar Aleef, Ingrid T. Spadinger, Michael D. Peacock, Septimiu E. Salcudean, S. Sara Mahdavi
- **Comment**: 10 pages, 2 figures, 2 tables
- **Journal**: Medical Image Computing and Computer Assisted Intervention MICCAI
  2021, vol 12904. Springer, Cham
- **Summary**: Treatment planning in low-dose-rate prostate brachytherapy (LDR-PB) aims to produce arrangement of implantable radioactive seeds that deliver a minimum prescribed dose to the prostate whilst minimizing toxicity to healthy tissues. There can be multiple seed arrangements that satisfy this dosimetric criterion, not all deemed 'acceptable' for implant from a physician's perspective. This leads to plans that are subjective to the physician's/centre's preference, planning style, and expertise. We propose a method that aims to reduce this variability by training a model to learn from a large pool of successful retrospective LDR-PB data (961 patients) and create consistent plans that mimic the high-quality manual plans. Our model is based on conditional generative adversarial networks that use a novel loss function for penalizing the model on spatial constraints of the seeds. An optional optimizer based on a simulated annealing (SA) algorithm can be used to further fine-tune the plans if necessary (determined by the treating physician). Performance analysis was conducted on 150 test cases demonstrating comparable results to that of the manual prehistorical plans. On average, the clinical target volume covering 100% of the prescribed dose was 98.9% for our method compared to 99.4% for manual plans. Moreover, using our model, the planning time was significantly reduced to an average of 2.5 mins/plan with SA, and less than 3 seconds without SA. Compared to this, manual planning at our centre takes around 20 mins/plan.



### COVIDx-US -- An open-access benchmark dataset of ultrasound imaging data for AI-driven COVID-19 analytics
- **Arxiv ID**: http://arxiv.org/abs/2103.10003v2
- **DOI**: 10.31083/j.fbl2707198
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.10003v2)
- **Published**: 2021-03-18 03:31:33+00:00
- **Updated**: 2021-04-20 13:51:52+00:00
- **Authors**: Ashkan Ebadi, Pengcheng Xi, Alexander MacLean, St√©phane Tremblay, Sonny Kohli, Alexander Wong
- **Comment**: 12 pages, 5 figures, to be submitted to Nature Scientific Data
- **Journal**: Front. Biosci. (Landmark Ed) 2022, 27(7), 198
- **Summary**: The COVID-19 pandemic continues to have a devastating effect on the health and well-being of the global population. Apart from the global health crises, the pandemic has also caused significant economic and financial difficulties and socio-physiological implications. Effective screening, triage, treatment planning, and prognostication of outcome plays a key role in controlling the pandemic. Recent studies have highlighted the role of point-of-care ultrasound imaging for COVID-19 screening and prognosis, particularly given that it is non-invasive, globally available, and easy-to-sanitize. Motivated by these attributes and the promise of artificial intelligence tools to aid clinicians, we introduce COVIDx-US, an open-access benchmark dataset of COVID-19 related ultrasound imaging data. The COVIDx-US dataset was curated from multiple sources and its current version, i.e., v1.2., consists of 150 lung ultrasound videos and 12,943 processed images of patients infected with COVID-19 infection, non-COVID-19 infection, other lung diseases/conditions, as well as normal control cases. The COVIDx-US is the largest open-access fully-curated dataset of its kind that has been systematically curated, processed, and validated specifically for the purpose of building and evaluating artificial intelligence algorithms and models.



### Generating Diverse Structure for Image Inpainting With Hierarchical VQ-VAE
- **Arxiv ID**: http://arxiv.org/abs/2103.10022v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10022v1)
- **Published**: 2021-03-18 05:10:49+00:00
- **Updated**: 2021-03-18 05:10:49+00:00
- **Authors**: Jialun Peng, Dong Liu, Songcen Xu, Houqiang Li
- **Comment**: Accepted in CVPR 2021
- **Journal**: None
- **Summary**: Given an incomplete image without additional constraint, image inpainting natively allows for multiple solutions as long as they appear plausible. Recently, multiplesolution inpainting methods have been proposed and shown the potential of generating diverse results. However, these methods have difficulty in ensuring the quality of each solution, e.g. they produce distorted structure and/or blurry texture. We propose a two-stage model for diverse inpainting, where the first stage generates multiple coarse results each of which has a different structure, and the second stage refines each coarse result separately by augmenting texture. The proposed model is inspired by the hierarchical vector quantized variational auto-encoder (VQ-VAE), whose hierarchical architecture isentangles structural and textural information. In addition, the vector quantization in VQVAE enables autoregressive modeling of the discrete distribution over the structural information. Sampling from the distribution can easily generate diverse and high-quality structures, making up the first stage of our model. In the second stage, we propose a structural attention module inside the texture generation network, where the module utilizes the structural information to capture distant correlations. We further reuse the VQ-VAE to calculate two feature losses, which help improve structure coherence and texture realism, respectively. Experimental results on CelebA-HQ, Places2, and ImageNet datasets show that our method not only enhances the diversity of the inpainting solutions but also improves the visual quality of the generated multiple images. Code and models are available at: https://github.com/USTC-JialunPeng/Diverse-Structure-Inpainting.



### Discriminative and Semantic Feature Selection for Place Recognition towards Dynamic Environments
- **Arxiv ID**: http://arxiv.org/abs/2103.10023v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.10023v2)
- **Published**: 2021-03-18 05:11:46+00:00
- **Updated**: 2021-03-21 03:35:24+00:00
- **Authors**: Yuxin Tian, Jinyu MIao, Xingming Wu, Haosong Yue, Zhong Liu, Weihai Chen
- **Comment**: The paper is under consideration at Pattern Recognition Letters
- **Journal**: None
- **Summary**: Features play an important role in various visual tasks, especially in visual place recognition applied in perceptual changing environments. In this paper, we address the challenges of place recognition due to dynamics and confusable patterns by proposing a discriminative and semantic feature selection network, dubbed as DSFeat. Supervised by both semantic information and attention mechanism, we can estimate pixel-wise stability of features, indicating the probability of a static and stable region from which features are extracted, and then select features that are insensitive to dynamic interference and distinguishable to be correctly matched. The designed feature selection model is evaluated in place recognition and SLAM system in several public datasets with varying appearances and viewpoints. Experimental results conclude that the effectiveness of the proposed method. It should be noticed that our proposal can be readily pluggable into any feature-based SLAM system.



### Efficient Algorithms for Rotation Averaging Problems
- **Arxiv ID**: http://arxiv.org/abs/2103.10024v1
- **DOI**: 10.1145/3451263.
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10024v1)
- **Published**: 2021-03-18 05:22:45+00:00
- **Updated**: 2021-03-18 05:22:45+00:00
- **Authors**: Yihong Dong, Lunchen Xie, Qingjiang Shi
- **Comment**: Rotation Averaging, Reconstruction, BCD, SUM
- **Journal**: None
- **Summary**: The rotation averaging problem is a fundamental task in computer vision applications. It is generally very difficult to solve due to the nonconvex rotation constraints. While a sufficient optimality condition is available in the literature, there is a lack of \yhedit{a} fast convergent algorithm to achieve stationary points. In this paper, by exploring the problem structure, we first propose a block coordinate descent (BCD)-based rotation averaging algorithm with guaranteed convergence to stationary points. Afterwards, we further propose an alternative rotation averaging algorithm by applying successive upper-bound minimization (SUM) method. The SUM-based rotation averaging algorithm can be implemented in parallel and thus is more suitable for addressing large-scale rotation averaging problems. Numerical examples verify that the proposed rotation averaging algorithms have superior convergence performance as compared to the state-of-the-art algorithm. Moreover, by checking the sufficient optimality condition, we find from extensive numerical experiments that the proposed two algorithms can achieve globally optimal solutions.



### Deep Online Correction for Monocular Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/2103.10029v1
- **DOI**: 10.1109/ICRA48506.2021.9561642
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.10029v1)
- **Published**: 2021-03-18 05:55:51+00:00
- **Updated**: 2021-03-18 05:55:51+00:00
- **Authors**: Jiaxin Zhang, Wei Sui, Xinggang Wang, Wenming Meng, Hongmei Zhu, Qian Zhang
- **Comment**: Accepted at 2021 IEEE International Conference on Robotics and
  Automation (ICRA)
- **Journal**: None
- **Summary**: In this work, we propose a novel deep online correction (DOC) framework for monocular visual odometry. The whole pipeline has two stages: First, depth maps and initial poses are obtained from convolutional neural networks (CNNs) trained in self-supervised manners. Second, the poses predicted by CNNs are further improved by minimizing photometric errors via gradient updates of poses during inference phases. The benefits of our proposed method are twofold: 1) Different from online-learning methods, DOC does not need to calculate gradient propagation for parameters of CNNs. Thus, it saves more computation resources during inference phases. 2) Unlike hybrid methods that combine CNNs with traditional methods, DOC fully relies on deep learning (DL) frameworks. Though without complex back-end optimization modules, our method achieves outstanding performance with relative transform error (RTE) = 2.0% on KITTI Odometry benchmark for Seq. 09, which outperforms traditional monocular VO frameworks and is comparable to hybrid methods.



### Robust Vision-Based Cheat Detection in Competitive Gaming
- **Arxiv ID**: http://arxiv.org/abs/2103.10031v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10031v2)
- **Published**: 2021-03-18 06:06:52+00:00
- **Updated**: 2021-03-27 06:15:43+00:00
- **Authors**: Aditya Jonnalagadda, Iuri Frosio, Seth Schneider, Morgan McGuire, Joohwan Kim
- **Comment**: 17 pages, 4 figures
- **Journal**: None
- **Summary**: Game publishers and anti-cheat companies have been unsuccessful in blocking cheating in online gaming. We propose a novel, vision-based approach that captures the final state of the frame buffer and detects illicit overlays. To this aim, we train and evaluate a DNN detector on a new dataset, collected using two first-person shooter games and three cheating software. We study the advantages and disadvantages of different DNN architectures operating on a local or global scale. We use output confidence analysis to avoid unreliable detections and inform when network retraining is required. In an ablation study, we show how to use Interval Bound Propagation to build a detector that is also resistant to potential adversarial attacks and study its interaction with confidence analysis. Our results show that robust and effective anti-cheating through machine learning is practically feasible and can be used to guarantee fair play in online gaming.



### Impressions2Font: Generating Fonts by Specifying Impressions
- **Arxiv ID**: http://arxiv.org/abs/2103.10036v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10036v2)
- **Published**: 2021-03-18 06:10:26+00:00
- **Updated**: 2021-05-21 07:53:42+00:00
- **Authors**: Seiya Matsuda, Akisato Kimura, Seiichi Uchida
- **Comment**: accepted at ICDAR2021
- **Journal**: None
- **Summary**: Various fonts give us various impressions, which are often represented by words. This paper proposes Impressions2Font (Imp2Font) that generates font images with specific impressions. Imp2Font is an extended version of conditional generative adversarial networks (GANs). More precisely, Imp2Font accepts an arbitrary number of impression words as the condition to generate the font images. These impression words are converted into a soft-constraint vector by an impression embedding module built on a word embedding technique. Qualitative and quantitative evaluations prove that Imp2Font generates font images with higher quality than comparative methods by providing multiple impression words or even unlearned words.



### RangeDet:In Defense of Range View for LiDAR-based 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.10039v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.10039v1)
- **Published**: 2021-03-18 06:18:51+00:00
- **Updated**: 2021-03-18 06:18:51+00:00
- **Authors**: Lue Fan, Xuan Xiong, Feng Wang, Naiyan Wang, Zhaoxiang Zhang
- **Comment**: The first two authors contributed equally to this work
- **Journal**: None
- **Summary**: In this paper, we propose an anchor-free single-stage LiDAR-based 3D object detector -- RangeDet. The most notable difference with previous works is that our method is purely based on the range view representation. Compared with the commonly used voxelized or Bird's Eye View (BEV) representations, the range view representation is more compact and without quantization error. Although there are works adopting it for semantic segmentation, its performance in object detection is largely behind voxelized or BEV counterparts. We first analyze the existing range-view-based methods and find two issues overlooked by previous works: 1) the scale variation between nearby and far away objects; 2) the inconsistency between the 2D range image coordinates used in feature extraction and the 3D Cartesian coordinates used in output. Then we deliberately design three components to address these issues in our RangeDet. We test our RangeDet in the large-scale Waymo Open Dataset (WOD). Our best model achieves 72.9/75.9/65.8 3D AP on vehicle/pedestrian/cyclist. These results outperform other range-view-based methods by a large margin (~20 3D AP in vehicle detection), and are overall comparable with the state-of-the-art multi-view-based methods. Codes will be public.



### Suppress-and-Refine Framework for End-to-End 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.10042v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10042v2)
- **Published**: 2021-03-18 06:36:44+00:00
- **Updated**: 2021-09-08 11:53:47+00:00
- **Authors**: Zili Liu, Guodong Xu, Honghui Yang, Minghao Chen, Kuoliang Wu, Zheng Yang, Haifeng Liu, Deng Cai
- **Comment**: None
- **Journal**: None
- **Summary**: 3D object detector based on Hough voting achieves great success and derives many follow-up works. Despite constantly refreshing the detection accuracy, these works suffer from handcrafted components used to eliminate redundant boxes, and thus are non-end-to-end and time-consuming. In this work, we propose a suppress-and-refine framework to remove these handcrafted components. To fully utilize full-resolution information and achieve real-time speed, it directly consumes feature points and redundant 3D proposals. Specifically, it first suppresses noisy 3D feature points and then feeds them to 3D proposals for the following RoI-aware refinement. With the gating mechanism to build fine proposal features and the self-attention mechanism to model relationships, our method can produce high-quality predictions with a small computation budget in an end-to-end manner. To this end, we present the first fully end-to-end 3D detector, SRDet, on the basis of VoteNet. It achieves state-of-the-art performance on the challenging ScanNetV2 and SUN RGB-D datasets with the fastest speed ever. Our code will be available at https://github.com/ZJULearning/SRDet.



### Enhancing Transformer for Video Understanding Using Gated Multi-Level Attention and Temporal Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2103.10043v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2103.10043v1)
- **Published**: 2021-03-18 06:39:09+00:00
- **Updated**: 2021-03-18 06:39:09+00:00
- **Authors**: Saurabh Sahu, Palash Goyal
- **Comment**: None
- **Journal**: None
- **Summary**: The introduction of Transformer model has led to tremendous advancements in sequence modeling, especially in text domain. However, the use of attention-based models for video understanding is still relatively unexplored. In this paper, we introduce Gated Adversarial Transformer (GAT) to enhance the applicability of attention-based models to videos. GAT uses a multi-level attention gate to model the relevance of a frame based on local and global contexts. This enables the model to understand the video at various granularities. Further, GAT uses adversarial training to improve model generalization. We propose temporal attention regularization scheme to improve the robustness of attention modules to adversarial examples. We illustrate the performance of GAT on the large-scale YoutTube-8M data set on the task of video categorization. We further show ablation studies along with quantitative and qualitative analysis to showcase the improvement.



### Similarity Transfer for Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2103.10047v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10047v1)
- **Published**: 2021-03-18 06:54:59+00:00
- **Updated**: 2021-03-18 06:54:59+00:00
- **Authors**: Haoran Zhao, Kun Gong, Xin Sun, Junyu Dong, Hui Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge distillation is a popular paradigm for learning portable neural networks by transferring the knowledge from a large model into a smaller one. Most existing approaches enhance the student model by utilizing the similarity information between the categories of instance level provided by the teacher model. However, these works ignore the similarity correlation between different instances that plays an important role in confidence prediction. To tackle this issue, we propose a novel method in this paper, called similarity transfer for knowledge distillation (STKD), which aims to fully utilize the similarities between categories of multiple samples. Furthermore, we propose to better capture the similarity correlation between different instances by the mixup technique, which creates virtual samples by a weighted linear interpolation. Note that, our distillation loss can fully utilize the incorrect classes similarities by the mixed labels. The proposed approach promotes the performance of student model as the virtual sample created by multiple images produces a similar probability distribution in the teacher and student networks. Experiments and ablation studies on several public classification datasets including CIFAR-10,CIFAR-100,CINIC-10 and Tiny-ImageNet verify that this light-weight method can effectively boost the performance of the compact student model. It shows that STKD substantially has outperformed the vanilla knowledge distillation and has achieved superior accuracy over the state-of-the-art knowledge distillation methods.



### Spatio-temporal Crop Classification On Volumetric Data
- **Arxiv ID**: http://arxiv.org/abs/2103.10050v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10050v1)
- **Published**: 2021-03-18 07:13:53+00:00
- **Updated**: 2021-03-18 07:13:53+00:00
- **Authors**: Muhammad Usman Qadeer, Salar Saeed, Murtaza Taj, Abubakr Muhammad
- **Comment**: Submitted to ICIP 2021
- **Journal**: None
- **Summary**: Large-area crop classification using multi-spectral imagery is a widely studied problem for several decades and is generally addressed using classical Random Forest classifier. Recently, deep convolutional neural networks (DCNN) have been proposed. However, these methods only achieved results comparable with Random Forest. In this work, we present a novel CNN based architecture for large-area crop classification. Our methodology combines both spatio-temporal analysis via 3D CNN as well as temporal analysis via 1D CNN. We evaluated the efficacy of our approach on Yolo and Imperial county benchmark datasets. Our combined strategy outperforms both classical as well as recent DCNN based methods in terms of classification accuracy by 2% while maintaining a minimum number of parameters and the lowest inference time.



### Data-free mixed-precision quantization using novel sensitivity metric
- **Arxiv ID**: http://arxiv.org/abs/2103.10051v1
- **DOI**: 10.1109/ICIP42928.2021.9506527
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.10051v1)
- **Published**: 2021-03-18 07:23:21+00:00
- **Updated**: 2021-03-18 07:23:21+00:00
- **Authors**: Donghyun Lee, Minkyoung Cho, Seungwon Lee, Joonho Song, Changkyu Choi
- **Comment**: Submission to ICIP2021
- **Journal**: 2021 IEEE International Conference on Image Processing (ICIP),
  2021, pp. 1294-1298
- **Summary**: Post-training quantization is a representative technique for compressing neural networks, making them smaller and more efficient for deployment on edge devices. However, an inaccessible user dataset often makes it difficult to ensure the quality of the quantized neural network in practice. In addition, existing approaches may use a single uniform bit-width across the network, resulting in significant accuracy degradation at extremely low bit-widths. To utilize multiple bit-width, sensitivity metric plays a key role in balancing accuracy and compression. In this paper, we propose a novel sensitivity metric that considers the effect of quantization error on task loss and interaction with other layers. Moreover, we develop labeled data generation methods that are not dependent on a specific operation of the neural network. Our experiments show that the proposed metric better represents quantization sensitivity, and generated data are more feasible to be applied to mixed-precision quantization.



### Dementia Severity Classification under Small Sample Size and Weak Supervision in Thick Slice MRI
- **Arxiv ID**: http://arxiv.org/abs/2103.10056v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.10056v1)
- **Published**: 2021-03-18 07:33:57+00:00
- **Updated**: 2021-03-18 07:33:57+00:00
- **Authors**: Reza Shirkavand, Sana Ayromlou, Soroush Farghadani, Maedeh-sadat Tahaei, Fattane Pourakpour, Bahareh Siahlou, Zeynab Khodakarami, Mohammad H. Rohban, Mansoor Fatehi, Hamid R. Rabiee
- **Comment**: 12 pages, 5 figues
- **Journal**: None
- **Summary**: Early detection of dementia through specific biomarkers in MR images plays a critical role in developing support strategies proactively. Fazekas scale facilitates an accurate quantitative assessment of the severity of white matter lesions and hence the disease. Imaging Biomarkers of dementia are multiple and comprehensive documentation of them is time-consuming. Therefore, any effort to automatically extract these biomarkers will be of clinical value while reducing inter-rater discrepancies. To tackle this problem, we propose to classify the disease severity based on the Fazekas scale through the visual biomarkers, namely the Periventricular White Matter (PVWM) and the Deep White Matter (DWM) changes, in the real-world setting of thick-slice MRI. Small training sample size and weak supervision in form of assigning severity labels to the whole MRI stack are among the main challenges. To combat the mentioned issues, we have developed a deep learning pipeline that employs self-supervised representation learning, multiple instance learning, and appropriate pre-processing steps. We use pretext tasks such as non-linear transformation, local shuffling, in- and out-painting for self-supervised learning of useful features in this domain. Furthermore, an attention model is used to determine the relevance of each MRI slice for predicting the Fazekas scale in an unsupervised manner. We show the significant superiority of our method in distinguishing different classes of dementia compared to state-of-the-art methods in our mentioned setting, which improves the macro averaged F1-score of state-of-the-art from 61% to 76% in PVWM, and from 58% to 69.2% in DWM.



### Deep Learning for Vision-Based Fall Detection System: Enhanced Optical Dynamic Flow
- **Arxiv ID**: http://arxiv.org/abs/2104.05744v1
- **DOI**: 10.1111/coin.12428
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.05744v1)
- **Published**: 2021-03-18 08:14:25+00:00
- **Updated**: 2021-03-18 08:14:25+00:00
- **Authors**: Sagar Chhetri, Abeer Alsadoon, Thair Al Dala in, P. W. C. Prasad, Tarik A. Rashid, Angelika Maag
- **Comment**: 16 pages
- **Journal**: Computational Intelligence, 2020
- **Summary**: Accurate fall detection for the assistance of older people is crucial to reduce incidents of deaths or injuries due to falls. Meanwhile, a vision-based fall detection system has shown some significant results to detect falls. Still, numerous challenges need to be resolved. The impact of deep learning has changed the landscape of the vision-based system, such as action recognition. The deep learning technique has not been successfully implemented in vision-based fall detection systems due to the requirement of a large amount of computation power and the requirement of a large amount of sample training data. This research aims to propose a vision-based fall detection system that improves the accuracy of fall detection in some complex environments such as the change of light condition in the room. Also, this research aims to increase the performance of the pre-processing of video images. The proposed system consists of the Enhanced Dynamic Optical Flow technique that encodes the temporal data of optical flow videos by the method of rank pooling, which thereby improves the processing time of fall detection and improves the classification accuracy in dynamic lighting conditions. The experimental results showed that the classification accuracy of the fall detection improved by around 3% and the processing time by 40 to 50ms. The proposed system concentrates on decreasing the processing time of fall detection and improving classification accuracy. Meanwhile, it provides a mechanism for summarizing a video into a single image by using a dynamic optical flow technique, which helps to increase the performance of image pre-processing steps.



### Self-Supervised Adaptation for Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2103.10081v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10081v1)
- **Published**: 2021-03-18 08:30:24+00:00
- **Updated**: 2021-03-18 08:30:24+00:00
- **Authors**: Jinsu Yoo, Tae Hyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Recent single-image super-resolution (SISR) networks, which can adapt their network parameters to specific input images, have shown promising results by exploiting the information available within the input data as well as large external datasets. However, the extension of these self-supervised SISR approaches to video handling has yet to be studied. Thus, we present a new learning algorithm that allows conventional video super-resolution (VSR) networks to adapt their parameters to test video frames without using the ground-truth datasets. By utilizing many self-similar patches across space and time, we improve the performance of fully pre-trained VSR networks and produce temporally consistent video frames. Moreover, we present a test-time knowledge distillation technique that accelerates the adaptation speed with less hardware resources. In our experiments, we demonstrate that our novel learning algorithm can fine-tune state-of-the-art VSR networks and substantially elevate performance on numerous benchmark datasets.



### TPPI-Net: Towards Efficient and Practical Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2103.10084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10084v1)
- **Published**: 2021-03-18 08:35:37+00:00
- **Updated**: 2021-03-18 08:35:37+00:00
- **Authors**: Hao Chen, Xiaohua Li, Jiliu Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral Image(HSI) classification is the most vibrant field of research in the hyperspectral community, which aims to assign each pixel in the image to one certain category based on its spectral-spatial characteristics. Recently, some spectral-spatial-feature based DCNNs have been proposed and demonstrated remarkable classification performance. When facing a real HSI, however, these Networks have to deal with the pixels in the image one by one. The pixel-wise processing strategy is inefficient since there are numerous repeated calculations between adjacent pixels. In this paper, firstly, a brand new Network design mechanism TPPI (training based on pixel and prediction based on image) is proposed for HSI classification, which makes it possible to provide efficient and practical HSI classification with the restrictive conditions attached to the hyperspectral dataset. And then, according to the TPPI mechanism, TPPI-Net is derived based on the state of the art networks for HSI classification. Experimental results show that the proposed TPPI-Net can not only obtain high classification accuracy equivalent to the state of the art networks for HSI classification, but also greatly reduce the computational complexity of hyperspectral image prediction.



### Higher Performance Visual Tracking with Dual-Modal Localization
- **Arxiv ID**: http://arxiv.org/abs/2103.10089v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10089v1)
- **Published**: 2021-03-18 08:47:56+00:00
- **Updated**: 2021-03-18 08:47:56+00:00
- **Authors**: Jinghao Zhou, Bo Li, Lei Qiao, Peng Wang, Weihao Gan, Wei Wu, Junjie Yan, Wanli Ouyang
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Object Tracking (VOT) has synchronous needs for both robustness and accuracy. While most existing works fail to operate simultaneously on both, we investigate in this work the problem of conflicting performance between accuracy and robustness. We first conduct a systematic comparison among existing methods and analyze their restrictions in terms of accuracy and robustness. Specifically, 4 formulations-offline classification (OFC), offline regression (OFR), online classification (ONC), and online regression (ONR)-are considered, categorized by the existence of online update and the types of supervision signal. To account for the problem, we resort to the idea of ensemble and propose a dual-modal framework for target localization, consisting of robust localization suppressing distractors via ONR and the accurate localization attending to the target center precisely via OFC. To yield a final representation (i.e, bounding box), we propose a simple but effective score voting strategy to involve adjacent predictions such that the final representation does not commit to a single location. Operating beyond the real-time demand, our proposed method is further validated on 8 datasets-VOT2018, VOT2019, OTB2015, NFS, UAV123, LaSOT, TrackingNet, and GOT-10k, achieving state-of-the-art performance.



### Which to Match? Selecting Consistent GT-Proposal Assignment for Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.10091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10091v1)
- **Published**: 2021-03-18 08:54:51+00:00
- **Updated**: 2021-03-18 08:54:51+00:00
- **Authors**: Yan Luo, Chongyang Zhang, Muming Zhao, Hao Zhou, Jun Sun
- **Comment**: This manuscript is waiting for further improvement
- **Journal**: None
- **Summary**: Accurate pedestrian classification and localization have received considerable attention due to their wide applications such as security monitoring, autonomous driving, etc. Although pedestrian detectors have made great progress in recent years, the fixed Intersection over Union (IoU) based assignment-regression manner still limits their performance. Two main factors are responsible for this: 1) the IoU threshold faces a dilemma that a lower one will result in more false positives, while a higher one will filter out the matched positives; 2) the IoU-based GT-Proposal assignment suffers from the inconsistent supervision problem that spatially adjacent proposals with similar features are assigned to different ground-truth boxes, which means some very similar proposals may be forced to regress towards different targets, and thus confuses the bounding-box regression when predicting the location results. In this paper, we first put forward the question that \textbf{Regression Direction} would affect the performance for pedestrian detection. Consequently, we address the weakness of IoU by introducing one geometric sensitive search algorithm as a new assignment and regression metric. Different from the previous IoU-based \textbf{one-to-one} assignment manner of one proposal to one ground-truth box, the proposed method attempts to seek a reasonable matching between the sets of proposals and ground-truth boxes. Specifically, we boost the MR-FPPI under R$_{75}$ by 8.8\% on Citypersons dataset. Furthermore, by incorporating this method as a metric into the state-of-the-art pedestrian detectors, we show a consistent improvement.



### KoDF: A Large-scale Korean DeepFake Detection Dataset
- **Arxiv ID**: http://arxiv.org/abs/2103.10094v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.10094v2)
- **Published**: 2021-03-18 09:04:02+00:00
- **Updated**: 2021-08-23 12:00:25+00:00
- **Authors**: Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park, Gyeongsu Chae
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: A variety of effective face-swap and face-reenactment methods have been publicized in recent years, democratizing the face synthesis technology to a great extent. Videos generated as such have come to be called deepfakes with a negative connotation, for various social problems they have caused. Facing the emerging threat of deepfakes, we have built the Korean DeepFake Detection Dataset (KoDF), a large-scale collection of synthesized and real videos focused on Korean subjects. In this paper, we provide a detailed description of methods used to construct the dataset, experimentally show the discrepancy between the distributions of KoDF and existing deepfake detection datasets, and underline the importance of using multiple datasets for real-world generalization. KoDF is publicly available at https://moneybrain-research.github.io/kodf in its entirety (i.e. real clips, synthesized clips, clips with adversarial attack, and metadata).



### On Semantic Similarity in Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2103.10095v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10095v1)
- **Published**: 2021-03-18 09:12:40+00:00
- **Updated**: 2021-03-18 09:12:40+00:00
- **Authors**: Michael Wray, Hazel Doughty, Dima Damen
- **Comment**: Accepted in CVPR 2021. Project Page: https://mwray.github.io/SSVR/
- **Journal**: None
- **Summary**: Current video retrieval efforts all found their evaluation on an instance-based assumption, that only a single caption is relevant to a query video and vice versa. We demonstrate that this assumption results in performance comparisons often not indicative of models' retrieval capabilities. We propose a move to semantic similarity video retrieval, where (i) multiple videos/captions can be deemed equally relevant, and their relative ranking does not affect a method's reported performance and (ii) retrieved videos/captions are ranked by their similarity to a query. We propose several proxies to estimate semantic similarities in large-scale retrieval datasets, without additional annotations. Our analysis is performed on three commonly used video retrieval datasets (MSR-VTT, YouCook2 and EPIC-KITCHENS).



### Danish Fungi 2020 -- Not Just Another Image Recognition Dataset
- **Arxiv ID**: http://arxiv.org/abs/2103.10107v4
- **DOI**: 10.1109/WACV51458.2022.00334
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.10107v4)
- **Published**: 2021-03-18 09:33:11+00:00
- **Updated**: 2021-08-20 14:35:44+00:00
- **Authors**: Luk√°≈° Picek, Milan ≈†ulc, Ji≈ô√≠ Matas, Jacob Heilmann-Clausen, Thomas S. Jeppesen, Thomas L√¶ss√∏e, Tobias Fr√∏slev
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel fine-grained dataset and benchmark, the Danish Fungi 2020 (DF20). The dataset, constructed from observations submitted to the Atlas of Danish Fungi, is unique in its taxonomy-accurate class labels, small number of errors, highly unbalanced long-tailed class distribution, rich observation metadata, and well-defined class hierarchy. DF20 has zero overlap with ImageNet, allowing unbiased comparison of models fine-tuned from publicly available ImageNet checkpoints. The proposed evaluation protocol enables testing the ability to improve classification using metadata -- e.g. precise geographic location, habitat, and substrate, facilitates classifier calibration testing, and finally allows to study the impact of the device settings on the classification performance. Experiments using Convolutional Neural Networks (CNN) and the recent Vision Transformers (ViT) show that DF20 presents a challenging task. Interestingly, ViT achieves results superior to CNN baselines with 80.45% accuracy and 0.743 macro F1 score, reducing the CNN error by 9% and 12% respectively. A simple procedure for including metadata into the decision process improves the classification accuracy by more than 2.95 percentage points, reducing the error rate by 15%. The source code for all methods and experiments is available at https://sites.google.com/view/danish-fungi-dataset.



### Real-Time Visual Object Tracking via Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.10130v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10130v1)
- **Published**: 2021-03-18 10:02:03+00:00
- **Updated**: 2021-03-18 10:02:03+00:00
- **Authors**: Jinghao Zhou, Bo Li, Peng Wang, Peixia Li, Weihao Gan, Wei Wu, Junjie Yan, Wanli Ouyang
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Object Tracking (VOT) can be seen as an extended task of Few-Shot Learning (FSL). While the concept of FSL is not new in tracking and has been previously applied by prior works, most of them are tailored to fit specific types of FSL algorithms and may sacrifice running speed. In this work, we propose a generalized two-stage framework that is capable of employing a large variety of FSL algorithms while presenting faster adaptation speed. The first stage uses a Siamese Regional Proposal Network to efficiently propose the potential candidates and the second stage reformulates the task of classifying these candidates to a few-shot classification problem. Following such a coarse-to-fine pipeline, the first stage proposes informative sparse samples for the second stage, where a large variety of FSL algorithms can be conducted more conveniently and efficiently. As substantiation of the second stage, we systematically investigate several forms of optimization-based few-shot learners from previous works with different objective functions, optimization methods, or solution space. Beyond that, our framework also entails a direct application of the majority of other FSL algorithms to visual tracking, enabling mutual communication between researchers on these two topics. Extensive experiments on the major benchmarks, VOT2018, OTB2015, NFS, UAV123, TrackingNet, and GOT-10k are conducted, demonstrating a desirable performance gain and a real-time speed.



### Learning Multimodal Affinities for Textual Editing in Images
- **Arxiv ID**: http://arxiv.org/abs/2103.10139v1
- **DOI**: 10.1145/3451340
- **Categories**: **cs.CV**, I.7.1; I.5.3; I.5.4; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2103.10139v1)
- **Published**: 2021-03-18 10:09:57+00:00
- **Updated**: 2021-03-18 10:09:57+00:00
- **Authors**: Or Perel, Oron Anschel, Omri Ben-Eliezer, Shai Mazor, Hadar Averbuch-Elor
- **Comment**: ACM Transactions on Graphics 2021, to be presented in SIGGRAPH 2021
- **Journal**: None
- **Summary**: Nowadays, as cameras are rapidly adopted in our daily routine, images of documents are becoming both abundant and prevalent. Unlike natural images that capture physical objects, document-images contain a significant amount of text with critical semantics and complicated layouts. In this work, we devise a generic unsupervised technique to learn multimodal affinities between textual entities in a document-image, considering their visual style, the content of their underlying text and their geometric context within the image. We then use these learned affinities to automatically cluster the textual entities in the image into different semantic groups. The core of our approach is a deep optimization scheme dedicated for an image provided by the user that detects and leverages reliable pairwise connections in the multimodal representation of the textual elements in order to properly learn the affinities. We show that our technique can operate on highly varying images spanning a wide range of documents and demonstrate its applicability for various editing operations manipulating the content, appearance and geometry of the image.



### Sequential End-to-end Network for Efficient Person Search
- **Arxiv ID**: http://arxiv.org/abs/2103.10148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10148v1)
- **Published**: 2021-03-18 10:28:24+00:00
- **Updated**: 2021-03-18 10:28:24+00:00
- **Authors**: Zhengjia Li, Duoqian Miao
- **Comment**: None
- **Journal**: None
- **Summary**: Person search aims at jointly solving Person Detection and Person Re-identification (re-ID). Existing works have designed end-to-end networks based on Faster R-CNN. However, due to the parallel structure of Faster R-CNN, the extracted features come from the low-quality proposals generated by the Region Proposal Network, rather than the detected high-quality bounding boxes. Person search is a fine-grained task and such inferior features will significantly reduce re-ID performance. To address this issue, we propose a Sequential End-to-end Network (SeqNet) to extract superior features. In SeqNet, detection and re-ID are considered as a progressive process and tackled with two sub-networks sequentially. In addition, we design a robust Context Bipartite Graph Matching (CBGM) algorithm to effectively employ context information as an important complementary cue for person matching. Extensive experiments on two widely used person search benchmarks, CUHK-SYSU and PRW, have shown that our method achieves state-of-the-art results. Also, our model runs at 11.5 fps on a single GPU and can be integrated into the existing end-to-end framework easily.



### TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.10158v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.10158v2)
- **Published**: 2021-03-18 10:48:02+00:00
- **Updated**: 2021-08-17 16:18:16+00:00
- **Authors**: Samuel G. M√ºller, Frank Hutter
- **Comment**: Accepted to ICCV 2021 as Oral
- **Journal**: None
- **Summary**: Automatic augmentation methods have recently become a crucial pillar for strong model performance in vision tasks. While existing automatic augmentation methods need to trade off simplicity, cost and performance, we present a most simple baseline, TrivialAugment, that outperforms previous methods for almost free. TrivialAugment is parameter-free and only applies a single augmentation to each image. Thus, TrivialAugment's effectiveness is very unexpected to us and we performed very thorough experiments to study its performance. First, we compare TrivialAugment to previous state-of-the-art methods in a variety of image classification scenarios. Then, we perform multiple ablation studies with different augmentation spaces, augmentation methods and setups to understand the crucial requirements for its performance. Additionally, we provide a simple interface to facilitate the widespread adoption of automatic augmentation methods, as well as our full code base for reproducibility. Since our work reveals a stagnation in many parts of automatic augmentation research, we end with a short proposal of best practices for sustained future progress in automatic augmentation methods.



### A Location-Sensitive Local Prototype Network for Few-Shot Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.10178v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10178v1)
- **Published**: 2021-03-18 11:27:19+00:00
- **Updated**: 2021-03-18 11:27:19+00:00
- **Authors**: Qinji Yu, Kang Dang, Nima Tajbakhsh, Demetri Terzopoulos, Xiaowei Ding
- **Comment**: ISBI2021 accepted
- **Journal**: None
- **Summary**: Despite the tremendous success of deep neural networks in medical image segmentation, they typically require a large amount of costly, expert-level annotated data. Few-shot segmentation approaches address this issue by learning to transfer knowledge from limited quantities of labeled examples. Incorporating appropriate prior knowledge is critical in designing high-performance few-shot segmentation algorithms. Since strong spatial priors exist in many medical imaging modalities, we propose a prototype-based method -- namely, the location-sensitive local prototype network -- that leverages spatial priors to perform few-shot medical image segmentation. Our approach divides the difficult problem of segmenting the entire image with global prototypes into easily solvable subproblems of local region segmentation with local prototypes. For organ segmentation experiments on the VISCERAL CT image dataset, our method outperforms the state-of-the-art approaches by 10% in the mean Dice coefficient. Extensive ablation studies demonstrate the substantial benefits of incorporating spatial information and confirm the effectiveness of our approach.



### Spectral Reconstruction and Disparity from Spatio-Spectrally Coded Light Fields via Multi-Task Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.10179v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10179v2)
- **Published**: 2021-03-18 11:28:05+00:00
- **Updated**: 2021-10-14 11:28:20+00:00
- **Authors**: Maximilian Schambach, Jiayang Shi, Michael Heizmann
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel method to reconstruct a spectral central view and its aligned disparity map from spatio-spectrally coded light fields. Since we do not reconstruct an intermediate full light field from the coded measurement, we refer to this as principal reconstruction. The coded light fields correspond to those captured by a light field camera in the unfocused design with a spectrally coded microlens array. In this application, the spectrally coded light field camera can be interpreted as a single-shot spectral depth camera.   We investigate several multi-task deep learning methods and propose a new auxiliary loss-based training strategy to enhance the reconstruction performance. The results are evaluated using a synthetic as well as a new real-world spectral light field dataset that we captured using a custom-built camera. The results are compared to state-of-the art compressed sensing reconstruction and disparity estimation.   We achieve a high reconstruction quality for both synthetic and real-world coded light fields. The disparity estimation quality is on par with or even outperforms state-of-the-art disparity estimation from uncoded RGB light fields.



### OmniPose: A Multi-Scale Framework for Multi-Person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2103.10180v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.10180v1)
- **Published**: 2021-03-18 11:30:31+00:00
- **Updated**: 2021-03-18 11:30:31+00:00
- **Authors**: Bruno Artacho, Andreas Savakis
- **Comment**: arXiv admin note: text overlap with arXiv:2001.08095
- **Journal**: None
- **Summary**: We propose OmniPose, a single-pass, end-to-end trainable framework, that achieves state-of-the-art results for multi-person pose estimation. Using a novel waterfall module, the OmniPose architecture leverages multi-scale feature representations that increase the effectiveness of backbone feature extractors, without the need for post-processing. OmniPose incorporates contextual information across scales and joint localization with Gaussian heatmap modulation at the multi-scale feature extractor to estimate human pose with state-of-the-art accuracy. The multi-scale representations, obtained by the improved waterfall module in OmniPose, leverage the efficiency of progressive filtering in the cascade architecture, while maintaining multi-scale fields-of-view comparable to spatial pyramid configurations. Our results on multiple datasets demonstrate that OmniPose, with an improved HRNet backbone and waterfall module, is a robust and efficient architecture for multi-person pose estimation that achieves state-of-the-art results.



### Bayesian Imaging With Data-Driven Priors Encoded by Neural Networks: Theory, Methods, and Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2103.10182v1
- **DOI**: None
- **Categories**: **stat.ME**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2103.10182v1)
- **Published**: 2021-03-18 11:34:08+00:00
- **Updated**: 2021-03-18 11:34:08+00:00
- **Authors**: Matthew Holden, Marcelo Pereyra, Konstantinos C. Zygalakis
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a new methodology for performing Bayesian inference in imaging inverse problems where the prior knowledge is available in the form of training data. Following the manifold hypothesis and adopting a generative modelling approach, we construct a data-driven prior that is supported on a sub-manifold of the ambient space, which we can learn from the training data by using a variational autoencoder or a generative adversarial network. We establish the existence and well-posedness of the associated posterior distribution and posterior moments under easily verifiable conditions, providing a rigorous underpinning for Bayesian estimators and uncertainty quantification analyses. Bayesian computation is performed by using a parallel tempered version of the preconditioned Crank-Nicolson algorithm on the manifold, which is shown to be ergodic and robust to the non-convex nature of these data-driven models. In addition to point estimators and uncertainty quantification analyses, we derive a model misspecification test to automatically detect situations where the data-driven prior is unreliable, and explain how to identify the dimension of the latent space directly from the training data. The proposed approach is illustrated with a range of experiments with the MNIST dataset, where it outperforms alternative image reconstruction approaches from the state of the art. A model accuracy analysis suggests that the Bayesian probabilities reported by the data-driven models are also remarkably accurate under a frequentist definition of probability.



### Learning to Amend Facial Expression Representation via De-albino and Affinity
- **Arxiv ID**: http://arxiv.org/abs/2103.10189v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10189v3)
- **Published**: 2021-03-18 11:54:13+00:00
- **Updated**: 2021-10-11 13:55:24+00:00
- **Authors**: Jiawei Shi, Songhao Zhu, Zhiwei Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Facial Expression Recognition (FER) is a classification task that points to face variants. Hence, there are certain affinity features between facial expressions, receiving little attention in the FER literature. Convolution padding, despite helping capture the edge information, causes erosion of the feature map simultaneously. After multi-layer filling convolution, the output feature map named albino feature definitely weakens the representation of the expression. To tackle these challenges, we propose a novel architecture named Amending Representation Module (ARM). ARM is a substitute for the pooling layer. Theoretically, it can be embedded in the back end of any network to deal with the Padding Erosion. ARM efficiently enhances facial expression representation from two different directions: 1) reducing the weight of eroded features to offset the side effect of padding, and 2) decomposing facial features to simplify representation learning. Experiments on public benchmarks prove that our ARM boosts the performance of FER remarkably. The validation accuracies are respectively 90.42% on RAF-DB, 65.2% on Affect-Net, and 58.71% on SFEW, exceeding current state-of-the-art methods. Our implementation and trained models are available at https://github.com/JiaweiShiCV/Amend-Representation-Module.



### Decoupled Spatial Temporal Graphs for Generic Visual Grounding
- **Arxiv ID**: http://arxiv.org/abs/2103.10191v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10191v1)
- **Published**: 2021-03-18 11:56:29+00:00
- **Updated**: 2021-03-18 11:56:29+00:00
- **Authors**: Qianyu Feng, Yunchao Wei, Mingming Cheng, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Visual grounding is a long-lasting problem in vision-language understanding due to its diversity and complexity. Current practices concentrate mostly on performing visual grounding in still images or well-trimmed video clips. This work, on the other hand, investigates into a more general setting, generic visual grounding, aiming to mine all the objects satisfying the given expression, which is more challenging yet practical in real-world scenarios. Importantly, grounding results are expected to accurately localize targets in both space and time. Whereas, it is tricky to make trade-offs between the appearance and motion features. In real scenarios, model tends to fail in distinguishing distractors with similar attributes. Motivated by these considerations, we propose a simple yet effective approach, named DSTG, which commits to 1) decomposing the spatial and temporal representations to collect all-sided cues for precise grounding; 2) enhancing the discriminativeness from distractors and the temporal consistency with a contrastive learning routing strategy. We further elaborate a new video dataset, GVG, that consists of challenging referring cases with far-ranging videos. Empirical experiments well demonstrate the superiority of DSTG over state-of-the-art on Charades-STA, ActivityNet-Caption and GVG datasets. Code and dataset will be made available.



### DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer
- **Arxiv ID**: http://arxiv.org/abs/2103.10206v5
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.10206v5)
- **Published**: 2021-03-18 12:17:38+00:00
- **Updated**: 2023-07-27 08:49:55+00:00
- **Authors**: Buyu Li, Yongchi Zhao, Zhelun Shi, Lu Sheng
- **Comment**: This is the version accepted by AAAI-22
- **Journal**: None
- **Summary**: Generating 3D dances from music is an emerged research task that benefits a lot of applications in vision and graphics. Previous works treat this task as sequence generation, however, it is challenging to render a music-aligned long-term sequence with high kinematic complexity and coherent movements. In this paper, we reformulate it by a two-stage process, ie, a key pose generation and then an in-between parametric motion curve prediction, where the key poses are easier to be synchronized with the music beats and the parametric curves can be efficiently regressed to render fluent rhythm-aligned movements. We named the proposed method as DanceFormer, which includes two cascading kinematics-enhanced transformer-guided networks (called DanTrans) that tackle each stage, respectively. Furthermore, we propose a large-scale music conditioned 3D dance dataset, called PhantomDance, that is accurately labeled by experienced animators rather than reconstruction or motion capture. This dataset also encodes dances as key poses and parametric motion curves apart from pose sequences, thus benefiting the training of our DanceFormer. Extensive experiments demonstrate that the proposed method, even trained by existing datasets, can generate fluent, performative, and music-matched 3D dances that surpass previous works quantitatively and qualitatively. Moreover, the proposed DanceFormer, together with the PhantomDance dataset (https://github.com/libuyu/PhantomDanceDataset), are seamlessly compatible with industrial animation software, thus facilitating the adaptation for various downstream applications.



### Space-Time Crop & Attend: Improving Cross-modal Video Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.10211v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10211v2)
- **Published**: 2021-03-18 12:32:24+00:00
- **Updated**: 2021-10-27 11:56:32+00:00
- **Authors**: Mandela Patrick, Yuki M. Asano, Bernie Huang, Ishan Misra, Florian Metze, Joao Henriques, Andrea Vedaldi
- **Comment**: Accepted to ICCV 2021. Code at
  https://github.com/facebookresearch/GDT
- **Journal**: None
- **Summary**: The quality of the image representations obtained from self-supervised learning depends strongly on the type of data augmentations used in the learning formulation. Recent papers have ported these methods from still images to videos and found that leveraging both audio and video signals yields strong gains; however, they did not find that spatial augmentations such as cropping, which are very important for still images, work as well for videos. In this paper, we improve these formulations in two ways unique to the spatio-temporal aspect of videos. First, for space, we show that spatial augmentations such as cropping do work well for videos too, but that previous implementations, due to the high processing and memory cost, could not do this at a scale sufficient for it to work well. To address this issue, we first introduce Feature Crop, a method to simulate such augmentations much more efficiently directly in feature space. Second, we show that as opposed to naive average pooling, the use of transformer-based attention improves performance significantly, and is well suited for processing feature crops. Combining both of our discoveries into a new method, Space-Time Crop & Attend (STiCA) we achieve state-of-the-art performance across multiple video-representation learning benchmarks. In particular, we achieve new state-of-the-art accuracies of 67.0% on HMDB-51 and 93.1% on UCF-101 when pre-training on Kinetics-400.



### Beyond Trivial Counterfactual Explanations with Diverse Valuable Explanations
- **Arxiv ID**: http://arxiv.org/abs/2103.10226v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.10226v2)
- **Published**: 2021-03-18 12:57:34+00:00
- **Updated**: 2021-11-11 17:55:27+00:00
- **Authors**: Pau Rodriguez, Massimo Caccia, Alexandre Lacoste, Lee Zamparo, Issam Laradji, Laurent Charlin, David Vazquez
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Explainability for machine learning models has gained considerable attention within the research community given the importance of deploying more reliable machine-learning systems. In computer vision applications, generative counterfactual methods indicate how to perturb a model's input to change its prediction, providing details about the model's decision-making. Current methods tend to generate trivial counterfactuals about a model's decisions, as they often suggest to exaggerate or remove the presence of the attribute being classified. For the machine learning practitioner, these types of counterfactuals offer little value, since they provide no new information about undesired model or data biases. In this work, we identify the problem of trivial counterfactual generation and we propose DiVE to alleviate it. DiVE learns a perturbation in a disentangled latent space that is constrained using a diversity-enforcing loss to uncover multiple valuable explanations about the model's prediction. Further, we introduce a mechanism to prevent the model from producing trivial explanations. Experiments on CelebA and Synbols demonstrate that our model improves the success rate of producing high-quality valuable explanations when compared to previous state-of-the-art methods. Code is available at https://github.com/ElementAI/beyond-trivial-explanations.



### Collective Decision of One-vs-Rest Networks for Open Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.10230v2
- **DOI**: 10.1109/TNNLS.2022.3189996
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10230v2)
- **Published**: 2021-03-18 13:06:46+00:00
- **Updated**: 2021-03-19 22:00:04+00:00
- **Authors**: Jaeyeon Jang, Chang Ouk Kim
- **Comment**: 8 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: Unknown examples that are unseen during training often appear in real-world machine learning tasks, and an intelligent self-learning system should be able to distinguish between known and unknown examples. Accordingly, open set recognition (OSR), which addresses the problem of classifying knowns and identifying unknowns, has recently been highlighted. However, conventional deep neural networks using a softmax layer are vulnerable to overgeneralization, producing high confidence scores for unknowns. In this paper, we propose a simple OSR method based on the intuition that OSR performance can be maximized by setting strict and sophisticated decision boundaries that reject unknowns while maintaining satisfactory classification performance on knowns. For this purpose, a novel network structure is proposed, in which multiple one-vs-rest networks (OVRNs) follow a convolutional neural network feature extractor. Here, the OVRN is a simple feed-forward neural network that enhances the ability to reject nonmatches by learning class-specific discriminative features. Furthermore, the collective decision score is modeled by combining the multiple decisions reached by the OVRNs to alleviate overgeneralization. Extensive experiments were conducted on various datasets, and the experimental results showed that the proposed method performed significantly better than the state-of-the-art methods by effectively reducing overgeneralization.



### Pseudo-ISP: Learning Pseudo In-camera Signal Processing Pipeline from A Color Image Denoiser
- **Arxiv ID**: http://arxiv.org/abs/2103.10234v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10234v1)
- **Published**: 2021-03-18 13:11:28+00:00
- **Updated**: 2021-03-18 13:11:28+00:00
- **Authors**: Yue Cao, Xiaohe Wu, Shuran Qi, Xiao Liu, Zhongqin Wu, Wangmeng Zuo
- **Comment**: The source code and pre-trained model are available at
  https://github.com/happycaoyue/Pseudo-ISP
- **Journal**: None
- **Summary**: The success of deep denoisers on real-world color photographs usually relies on the modeling of sensor noise and in-camera signal processing (ISP) pipeline. Performance drop will inevitably happen when the sensor and ISP pipeline of test images are different from those for training the deep denoisers (i.e., noise discrepancy). In this paper, we present an unpaired learning scheme to adapt a color image denoiser for handling test images with noise discrepancy. We consider a practical training setting, i.e., a pre-trained denoiser, a set of test noisy images, and an unpaired set of clean images. To begin with, the pre-trained denoiser is used to generate the pseudo clean images for the test images. Pseudo-ISP is then suggested to jointly learn the pseudo ISP pipeline and signal-dependent rawRGB noise model using the pairs of test and pseudo clean images. We further apply the learned pseudo ISP and rawRGB noise model to clean color images to synthesize realistic noisy images for denoiser adaption. Pseudo-ISP is effective in synthesizing realistic noisy sRGB images, and improved denoising performance can be achieved by alternating between Pseudo-ISP training and denoiser adaption. Experiments show that our Pseudo-ISP not only can boost simple Gaussian blurring-based denoiser to achieve competitive performance against CBDNet, but also is effective in improving state-of-the-art deep denoisers, e.g., CBDNet and RIDNet.



### Equivariant Filters for Efficient Tracking in 3D Imaging
- **Arxiv ID**: http://arxiv.org/abs/2103.10255v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2103.10255v2)
- **Published**: 2021-03-18 13:47:27+00:00
- **Updated**: 2021-09-27 15:53:13+00:00
- **Authors**: Daniel Moyer, Esra Abaci Turk, P Ellen Grant, William M. Wells, Polina Golland
- **Comment**: MICCAI 2021 Early Accept
- **Journal**: None
- **Summary**: We demonstrate an object tracking method for 3D images with fixed computational cost and state-of-the-art performance. Previous methods predicted transformation parameters from convolutional layers. We instead propose an architecture that does not include either flattening of convolutional features or fully connected layers, but instead relies on equivariant filters to preserve transformations between inputs and outputs (e.g. rot./trans. of inputs rotate/translate outputs). The transformation is then derived in closed form from the outputs of the filters. This method is useful for applications requiring low latency, such as real-time tracking. We demonstrate our model on synthetically augmented adult brain MRI, as well as fetal brain MRI, which is the intended use-case.



### SG-Net: Spatial Granularity Network for One-Stage Video Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.10284v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10284v2)
- **Published**: 2021-03-18 14:31:15+00:00
- **Updated**: 2021-04-05 17:54:14+00:00
- **Authors**: Dongfang Liu, Yiming Cui, Wenbo Tan, Yingjie Chen
- **Comment**: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR 2021)
- **Journal**: None
- **Summary**: Video instance segmentation (VIS) is a new and critical task in computer vision. To date, top-performing VIS methods extend the two-stage Mask R-CNN by adding a tracking branch, leaving plenty of room for improvement. In contrast, we approach the VIS task from a new perspective and propose a one-stage spatial granularity network (SG-Net). Compared to the conventional two-stage methods, SG-Net demonstrates four advantages: 1) Our method has a one-stage compact architecture and each task head (detection, segmentation, and tracking) is crafted interdependently so they can effectively share features and enjoy the joint optimization; 2) Our mask prediction is dynamically performed on the sub-regions of each detected instance, leading to high-quality masks of fine granularity; 3) Each of our task predictions avoids using expensive proposal-based RoI features, resulting in much reduced runtime complexity per instance; 4) Our tracking head models objects centerness movements for tracking, which effectively enhances the tracking robustness to different object appearances. In evaluation, we present state-of-the-art comparisons on the YouTube-VIS dataset. Extensive experiments demonstrate that our compact one-stage method can achieve improved performance in both accuracy and inference speed. We hope our SG-Net could serve as a strong and flexible baseline for the VIS task. Our code will be available.



### How I failed machine learning in medical imaging -- shortcomings and recommendations
- **Arxiv ID**: http://arxiv.org/abs/2103.10292v2
- **DOI**: 10.1038/s41746-022-00592-y
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2103.10292v2)
- **Published**: 2021-03-18 14:46:35+00:00
- **Updated**: 2022-05-12 15:03:28+00:00
- **Authors**: Ga√´l Varoquaux, Veronika Cheplygina
- **Comment**: None
- **Journal**: npj Digit. Med. 5, 48 (2022).
  https://doi.org/10.1038/s41746-022-00592-y
- **Summary**: Medical imaging is an important research field with many opportunities for improving patients' health. However, there are a number of challenges that are slowing down the progress of the field as a whole, such optimizing for publication. In this paper we reviewed several problems related to choosing datasets, methods, evaluation metrics, and publication strategies. With a review of literature and our own analysis, we show that at every step, potential biases can creep in. On a positive note, we also see that initiatives to counteract these problems are already being started. Finally we provide a broad range of recommendations on how to further these address problems in the future. For reproducibility, data and code for our analyses are available on \url{https://github.com/GaelVaroquaux/ml_med_imaging_failures}



### Future Frame Prediction for Robot-assisted Surgery
- **Arxiv ID**: http://arxiv.org/abs/2103.10308v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10308v1)
- **Published**: 2021-03-18 15:12:06+00:00
- **Updated**: 2021-03-18 15:12:06+00:00
- **Authors**: Xiaojie Gao, Yueming Jin, Zixu Zhao, Qi Dou, Pheng-Ann Heng
- **Comment**: IPMI2021
- **Journal**: None
- **Summary**: Predicting future frames for robotic surgical video is an interesting, important yet extremely challenging problem, given that the operative tasks may have complex dynamics. Existing approaches on future prediction of natural videos were based on either deterministic models or stochastic models, including deep recurrent neural networks, optical flow, and latent space modeling. However, the potential in predicting meaningful movements of robots with dual arms in surgical scenarios has not been tapped so far, which is typically more challenging than forecasting independent motions of one arm robots in natural scenarios. In this paper, we propose a ternary prior guided variational autoencoder (TPG-VAE) model for future frame prediction in robotic surgical video sequences. Besides content distribution, our model learns motion distribution, which is novel to handle the small movements of surgical tools. Furthermore, we add the invariant prior information from the gesture class into the generation process to constrain the latent space of our model. To our best knowledge, this is the first time that the future frames of dual arm robots are predicted considering their unique characteristics relative to general robotic videos. Experiments demonstrate that our model gains more stable and realistic future frame prediction scenes with the suturing task on the public JIGSAWS dataset.



### Lighting Enhancement Aids Reconstruction of Colonoscopic Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2103.10310v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.10310v1)
- **Published**: 2021-03-18 15:12:59+00:00
- **Updated**: 2021-03-18 15:12:59+00:00
- **Authors**: Yubo Zhang, Shuxian Wang, Ruibin Ma, Sarah K. McGill, Julian G. Rosenman, Stephen M. Pizer
- **Comment**: Accepted at IPMI 2021 (The 27th international conference on
  Information Processing in Medical Imaging)
- **Journal**: None
- **Summary**: High screening coverage during colonoscopy is crucial to effectively prevent colon cancer. Previous work has allowed alerting the doctor to unsurveyed regions by reconstructing the 3D colonoscopic surface from colonoscopy videos in real-time. However, the lighting inconsistency of colonoscopy videos can cause a key component of the colonoscopic reconstruction system, the SLAM optimization, to fail. In this work we focus on the lighting problem in colonoscopy videos. To successfully improve the lighting consistency of colonoscopy videos, we have found necessary a lighting correction that adapts to the intensity distribution of recent video frames. To achieve this in real-time, we have designed and trained an RNN network. This network adapts the gamma value in a gamma-correction process. Applied in the colonoscopic surface reconstruction system, our light-weight model significantly boosts the reconstruction success rate, making a larger proportion of colonoscopy video segments reconstructable and improving the reconstruction quality of the already reconstructed segments.



### Real-Time, Deep Synthetic Aperture Sonar (SAS) Autofocus
- **Arxiv ID**: http://arxiv.org/abs/2103.10312v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10312v3)
- **Published**: 2021-03-18 15:16:29+00:00
- **Updated**: 2021-06-01 15:19:25+00:00
- **Authors**: Isaac D. Gerg, Vishal Monga
- **Comment**: Four pages. Accepted to IGARSS 2021. Fixed Eq 9
- **Journal**: None
- **Summary**: Synthetic aperture sonar (SAS) requires precise time-of-flight measurements of the transmitted/received waveform to produce well-focused imagery. It is not uncommon for errors in these measurements to be present resulting in image defocusing. To overcome this, an \emph{autofocus} algorithm is employed as a post-processing step after image reconstruction to improve image focus. A particular class of these algorithms can be framed as a sharpness/contrast metric-based optimization. To improve convergence, a hand-crafted weighting function to remove "bad" areas of the image is sometimes applied to the image-under-test before the optimization procedure. Additionally, dozens of iterations are necessary for convergence which is a large compute burden for low size, weight, and power (SWaP) systems. We propose a deep learning technique to overcome these limitations and implicitly learn the weighting function in a data-driven manner. Our proposed method, which we call Deep Autofocus, uses features from the single-look-complex (SLC) to estimate the phase correction which is applied in $k$-space. Furthermore, we train our algorithm on batches of training imagery so that during deployment, only a single iteration of our method is sufficient to autofocus. We show results demonstrating the robustness of our technique by comparing our results to four commonly used image sharpness metrics. Our results demonstrate Deep Autofocus can produce imagery perceptually better than common iterative techniques but at a lower computational cost. We conclude that Deep Autofocus can provide a more favorable cost-quality trade-off than alternatives with significant potential of future research.



### Localization of Cochlear Implant Electrodes from Cone Beam Computed Tomography using Particle Belief Propagation
- **Arxiv ID**: http://arxiv.org/abs/2103.10434v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.QM, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2103.10434v1)
- **Published**: 2021-03-18 15:39:23+00:00
- **Updated**: 2021-03-18 15:39:23+00:00
- **Authors**: Hendrik Hachmann, Benjamin Kr√ºger, Bodo Rosenhahn, Waldo Nogueira
- **Comment**: None
- **Journal**: None
- **Summary**: Cochlear implants (CIs) are implantable medical devices that can restore the hearing sense of people suffering from profound hearing loss. The CI uses a set of electrode contacts placed inside the cochlea to stimulate the auditory nerve with current pulses. The exact location of these electrodes may be an important parameter to improve and predict the performance with these devices. Currently the methods used in clinics to characterize the geometry of the cochlea as well as to estimate the electrode positions are manual, error-prone and time consuming. We propose a Markov random field (MRF) model for CI electrode localization for cone beam computed tomography (CBCT) data-sets. Intensity and shape of electrodes are included as prior knowledge as well as distance and angles between contacts. MRF inference is based on slice sampling particle belief propagation and guided by several heuristics. A stochastic search finds the best maximum a posteriori estimation among sampled MRF realizations. We evaluate our algorithm on synthetic and real CBCT data-sets and compare its performance with two state of the art algorithms. An increase of localization precision up to 31.5% (mean), or 48.6% (median) respectively, on real CBCT data-sets is shown.



### Investigate Indistinguishable Points in Semantic Segmentation of 3D Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2103.10339v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.10339v4)
- **Published**: 2021-03-18 15:54:59+00:00
- **Updated**: 2021-08-26 03:11:01+00:00
- **Authors**: Mingye Xu, Zhipeng Zhou, Junhao Zhang, Yu Qiao
- **Comment**: AAAI2021
- **Journal**: None
- **Summary**: This paper investigates the indistinguishable points (difficult to predict label) in semantic segmentation for large-scale 3D point clouds. The indistinguishable points consist of those located in complex boundary, points with similar local textures but different categories, and points in isolate small hard areas, which largely harm the performance of 3D semantic segmentation. To address this challenge, we propose a novel Indistinguishable Area Focalization Network (IAF-Net), which selects indistinguishable points adaptively by utilizing the hierarchical semantic features and enhances fine-grained features for points especially those indistinguishable points. We also introduce multi-stage loss to improve the feature representation in a progressive way. Moreover, in order to analyze the segmentation performances of indistinguishable areas, we propose a new evaluation metric called Indistinguishable Points Based Metric (IPBM). Our IAF-Net achieves the comparable results with state-of-the-art performance on several popular 3D point cloud datasets e.g. S3DIS and ScanNet, and clearly outperforms other methods on IPBM.



### The Case for High-Accuracy Classification: Think Small, Think Many!
- **Arxiv ID**: http://arxiv.org/abs/2103.10350v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.10350v1)
- **Published**: 2021-03-18 16:15:31+00:00
- **Updated**: 2021-03-18 16:15:31+00:00
- **Authors**: Mohammad Hosseini, Mahmudul Hasan
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: To facilitate implementation of high-accuracy deep neural networks especially on resource-constrained devices, maintaining low computation requirements is crucial. Using very deep models for classification purposes not only decreases the neural network training speed and increases the inference time, but also need more data for higher prediction accuracy and to mitigate false positives.   In this paper, we propose an efficient and lightweight deep classification ensemble structure based on a combination of simple color features, which is particularly designed for "high-accuracy" image classifications with low false positives. We designed, implemented, and evaluated our approach for explosion detection use-case applied to images and videos. Our evaluation results based on a large test test show considerable improvements on the prediction accuracy compared to the popular ResNet-50 model, while benefiting from 7.64x faster inference and lower computation cost.   While we applied our approach to explosion detection, our approach is general and can be applied to other similar classification use cases as well. Given the insight gained from our experiments, we hence propose a "think small, think many" philosophy in classification scenarios: that transforming a single, large, monolithic deep model into a verification-based step model ensemble of multiple small, simple, lightweight models with narrowed-down color spaces can possibly lead to predictions with higher accuracy.



### MSMatch: Semi-Supervised Multispectral Scene Classification with Few Labels
- **Arxiv ID**: http://arxiv.org/abs/2103.10368v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, 68T45, I.4.8; I.2.6; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2103.10368v2)
- **Published**: 2021-03-18 16:47:21+00:00
- **Updated**: 2021-08-02 07:05:28+00:00
- **Authors**: Pablo G√≥mez, Gabriele Meoni
- **Comment**: 10 pages, 8 figures, submitted to IEEE Journal of Selected Topics in
  Applied Earth Observations and Remote Sensing
- **Journal**: None
- **Summary**: Supervised learning techniques are at the center of many tasks in remote sensing. Unfortunately, these methods, especially recent deep learning methods, often require large amounts of labeled data for training. Even though satellites acquire large amounts of data, labeling the data is often tedious, expensive and requires expert knowledge. Hence, improved methods that require fewer labeled samples are needed. We present MSMatch, the first semi-supervised learning approach competitive with supervised methods on scene classification on the EuroSAT and UC Merced Land Use benchmark datasets. We test both RGB and multispectral images of EuroSAT and perform various ablation studies to identify the critical parts of the model. The trained neural network achieves state-of-the-art results on EuroSAT with an accuracy that is up to 19.76% better than previous methods depending on the number of labeled training examples. With just five labeled examples per class, we reach 94.53% and 95.86% accuracy on the EuroSAT RGB and multispectral datasets, respectively. On the UC Merced Land Use dataset, we outperform previous works by up to 5.59% and reach 90.71% with five labeled examples. Our results show that MSMatch is capable of greatly reducing the requirements for labeled data. It translates well to multispectral data and should enable various applications that are currently infeasible due to a lack of labeled data. We provide the source code of MSMatch online to enable easy reproduction and quick adoption.



### Consistency-based Active Learning for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.10374v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.10374v3)
- **Published**: 2021-03-18 17:00:34+00:00
- **Updated**: 2022-04-11 15:28:23+00:00
- **Authors**: Weiping Yu, Sijie Zhu, Taojiannan Yang, Chen Chen
- **Comment**: CVPR-2022 Workshop
- **Journal**: None
- **Summary**: Active learning aims to improve the performance of task model by selecting the most informative samples with a limited budget. Unlike most recent works that focused on applying active learning for image classification, we propose an effective Consistency-based Active Learning method for object Detection (CALD), which fully explores the consistency between original and augmented data. CALD has three appealing benefits. (i) CALD is systematically designed by investigating the weaknesses of existing active learning methods, which do not take the unique challenges of object detection into account. (ii) CALD unifies box regression and classification with a single metric, which is not concerned by active learning methods for classification. CALD also focuses on the most informative local region rather than the whole image, which is beneficial for object detection. (iii) CALD not only gauges individual information for sample selection, but also leverages mutual information to encourage a balanced data distribution. Extensive experiments show that CALD significantly outperforms existing state-of-the-art task-agnostic and detection-specific active learning methods on general object detection datasets. Based on the Faster R-CNN detector, CALD consistently surpasses the baseline method (random selection) by 2.9/2.8/0.8 mAP on average on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO. Code is available at \url{https://github.com/we1pingyu/CALD}



### FastNeRF: High-Fidelity Neural Rendering at 200FPS
- **Arxiv ID**: http://arxiv.org/abs/2103.10380v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10380v2)
- **Published**: 2021-03-18 17:09:12+00:00
- **Updated**: 2021-04-15 11:01:16+00:00
- **Authors**: Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, Julien Valentin
- **Comment**: main paper: 10 pages, 6 figures; supplementary: 10 pages, 17 figures
- **Journal**: None
- **Summary**: Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility.



### Challenges of 3D Surface Reconstruction in Capsule Endoscopy
- **Arxiv ID**: http://arxiv.org/abs/2103.10390v4
- **DOI**: 10.3390/jcm12154955
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2103.10390v4)
- **Published**: 2021-03-18 17:18:48+00:00
- **Updated**: 2023-07-27 19:21:56+00:00
- **Authors**: Olivier Rukundo
- **Comment**: 7 pages, 2 figures
- **Journal**: Journal of Clinical Medicine, 2023
- **Summary**: Essential for improving the accuracy and reliability of bowel cancer screening, three-dimensional (3D) surface reconstruction using capsule endoscopy (CE) images remains challenging due to CE hardware and software limitations. This report generally focuses on challenges associated with 3D visualization and specifically investigates the impact of the indeterminate selection of the angle of the line of sight on 3D surfaces. Furthermore, it demonstrates that impact through 3D surfaces viewed at the same azimuth angles and different elevation angles of the line of sight. The report concludes that 3D printing of reconstructed 3D surfaces can potentially overcome line of sight indeterminate selection and 2D screen visual restriction-related errors.



### Learning to Recommend Frame for Interactive Video Object Segmentation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2103.10391v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10391v2)
- **Published**: 2021-03-18 17:19:47+00:00
- **Updated**: 2021-06-17 02:45:20+00:00
- **Authors**: Zhaoyuan Yin, Jia Zheng, Weixin Luo, Shenhan Qian, Hanling Zhang, Shenghua Gao
- **Comment**: To appear in CVPR 2021. Minor revision
- **Journal**: None
- **Summary**: This paper proposes a framework for the interactive video object segmentation (VOS) in the wild where users can choose some frames for annotations iteratively. Then, based on the user annotations, a segmentation algorithm refines the masks. The previous interactive VOS paradigm selects the frame with some worst evaluation metric, and the ground truth is required for calculating the evaluation metric, which is impractical in the testing phase. In contrast, in this paper, we advocate that the frame with the worst evaluation metric may not be exactly the most valuable frame that leads to the most performance improvement across the video. Thus, we formulate the frame selection problem in the interactive VOS as a Markov Decision Process, where an agent is learned to recommend the frame under a deep reinforcement learning framework. The learned agent can automatically determine the most valuable frame, making the interactive setting more practical in the wild. Experimental results on the public datasets show the effectiveness of our learned agent without any changes to the underlying VOS algorithms. Our data, code, and models are available at https://github.com/svip-lab/IVOS-W.



### RP-VIO: Robust Plane-based Visual-Inertial Odometry for Dynamic Environments
- **Arxiv ID**: http://arxiv.org/abs/2103.10400v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.10400v2)
- **Published**: 2021-03-18 17:33:07+00:00
- **Updated**: 2021-12-05 21:30:15+00:00
- **Authors**: Karnik Ram, Chaitanya Kharyal, Sudarshan S. Harithas, K. Madhava Krishna
- **Comment**: Presented at IROS 2021, code and dataset available at
  https://karnikram.info/rp-vio
- **Journal**: None
- **Summary**: Modern visual-inertial navigation systems (VINS) are faced with a critical challenge in real-world deployment: they need to operate reliably and robustly in highly dynamic environments. Current best solutions merely filter dynamic objects as outliers based on the semantics of the object category. Such an approach does not scale as it requires semantic classifiers to encompass all possibly-moving object classes; this is hard to define, let alone deploy. On the other hand, many real-world environments exhibit strong structural regularities in the form of planes such as walls and ground surfaces, which are also crucially static. We present RP-VIO, a monocular visual-inertial odometry system that leverages the simple geometry of these planes for improved robustness and accuracy in challenging dynamic environments. Since existing datasets have a limited number of dynamic elements, we also present a highly-dynamic, photorealistic synthetic dataset for a more effective evaluation of the capabilities of modern VINS systems. We evaluate our approach on this dataset, and three diverse sequences from standard datasets including two real-world dynamic sequences and show a significant improvement in robustness and accuracy over a state-of-the-art monocular visual-inertial odometry system. We also show in simulation an improvement over a simple dynamic-features masking approach. Our code and dataset are publicly available.



### Computer Vision Aided URLL Communications: Proactive Service Identification and Coexistence
- **Arxiv ID**: http://arxiv.org/abs/2103.10419v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2103.10419v1)
- **Published**: 2021-03-18 17:53:29+00:00
- **Updated**: 2021-03-18 17:53:29+00:00
- **Authors**: Muhammad Alrabeiah, Umut Demirhan, Andrew Hredzak, Ahmed Alkhateeb
- **Comment**: The datasets and code files will be available soon on the ViWi
  Dataset website https://viwi-dataset.net/
- **Journal**: None
- **Summary**: The support of coexisting ultra-reliable and low-latency (URLL) and enhanced Mobile BroadBand (eMBB) services is a key challenge for the current and future wireless communication networks. Those two types of services introduce strict, and in some time conflicting, resource allocation requirements that may result in a power-struggle between reliability, latency, and resource utilization in wireless networks. The difficulty in addressing that challenge could be traced back to the predominant reactive approach in allocating the wireless resources. This allocation operation is carried out based on received service requests and global network statistics, which may not incorporate a sense of \textit{proaction}. Therefore, this paper proposes a novel framework termed \textit{service identification} to develop novel proactive resource allocation algorithms. The developed framework is based on visual data (captured for example by RGB cameras) and deep learning (e.g., deep neural networks). The ultimate objective of this framework is to equip future wireless networks with the ability to analyze user behavior, anticipate incoming services, and perform proactive resource allocation. To demonstrate the potential of the proposed framework, a wireless network scenario with two coexisting URLL and eMBB services is considered, and two deep learning algorithms are designed to utilize RGB video frames and predict incoming service type and its request time. An evaluation dataset based on the considered scenario is developed and used to evaluate the performance of the two algorithms. The results confirm the anticipated value of proaction to wireless networks; the proposed models enable efficient network performance ensuring more than $85\%$ utilization of the network resources at $\sim 98\%$ reliability. This highlights a promising direction for the future vision-aided wireless communication networks.



### Using latent space regression to analyze and leverage compositionality in GANs
- **Arxiv ID**: http://arxiv.org/abs/2103.10426v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.10426v2)
- **Published**: 2021-03-18 17:58:01+00:00
- **Updated**: 2021-06-03 19:52:33+00:00
- **Authors**: Lucy Chai, Jonas Wulff, Phillip Isola
- **Comment**: Update to ICLR 2021 camera ready version
- **Journal**: None
- **Summary**: In recent years, Generative Adversarial Networks have become ubiquitous in both research and public perception, but how GANs convert an unstructured latent code to a high quality output is still an open question. In this work, we investigate regression into the latent space as a probe to understand the compositional properties of GANs. We find that combining the regressor and a pretrained generator provides a strong image prior, allowing us to create composite images from a collage of random image parts at inference time while maintaining global consistency. To compare compositional properties across different generators, we measure the trade-offs between reconstruction of the unrealistic input and image quality of the regenerated samples. We find that the regression approach enables more localized editing of individual image parts compared to direct editing in the latent space, and we conduct experiments to quantify this independence effect. Our method is agnostic to the semantics of edits, and does not require labels or predefined concepts during training. Beyond image composition, our method extends to a number of related applications, such as image inpainting or example-based image editing, which we demonstrate on several GANs and datasets, and because it uses only a single forward pass, it can operate in real-time. Code is available on our project page: https://chail.github.io/latent-composition/.



### The Low-Rank Simplicity Bias in Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.10427v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.10427v4)
- **Published**: 2021-03-18 17:58:02+00:00
- **Updated**: 2023-03-23 14:21:02+00:00
- **Authors**: Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, Phillip Isola
- **Comment**: None
- **Journal**: None
- **Summary**: Modern deep neural networks are highly over-parameterized compared to the data on which they are trained, yet they often generalize remarkably well. A flurry of recent work has asked: why do deep networks not overfit to their training data? In this work, we make a series of empirical observations that investigate and extend the hypothesis that deeper networks are inductively biased to find solutions with lower effective rank embeddings. We conjecture that this bias exists because the volume of functions that maps to low effective rank embedding increases with depth. We show empirically that our claim holds true on finite width linear and non-linear models on practical learning paradigms and show that on natural data, these are often the solutions that generalize well. We then show that the simplicity bias exists at both initialization and after training and is resilient to hyper-parameters and learning methods. We further demonstrate how linear over-parameterization of deep non-linear models can be used to induce low-rank bias, improving generalization performance on CIFAR and ImageNet without changing the modeling capacity.



### Large Scale Image Completion via Co-Modulated Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.10428v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.10428v1)
- **Published**: 2021-03-18 17:59:11+00:00
- **Updated**: 2021-03-18 17:59:11+00:00
- **Authors**: Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I Chang, Yan Xu
- **Comment**: ICLR 2021 (Spotlight). Code: https://github.com/zsyzzsoft/co-mod-gan
  Demo: https://comodgan.ml/
- **Journal**: None
- **Summary**: Numerous task-specific variants of conditional generative adversarial networks have been developed for image completion. Yet, a serious limitation remains that all existing algorithms tend to fail when handling large-scale missing regions. To overcome this challenge, we propose a generic new approach that bridges the gap between image-conditional and recent modulated unconditional generative architectures via co-modulation of both conditional and stochastic style representations. Also, due to the lack of good quantitative metrics for image completion, we propose the new Paired/Unpaired Inception Discriminative Score (P-IDS/U-IDS), which robustly measures the perceptual fidelity of inpainted images compared to real images via linear separability in a feature space. Experiments demonstrate superior performance in terms of both quality and diversity over state-of-the-art methods in free-form image completion and easy generalization to image-to-image translation. Code is available at https://github.com/zsyzzsoft/co-mod-gan.



### Neural Parts: Learning Expressive 3D Shape Abstractions with Invertible Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.10429v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10429v1)
- **Published**: 2021-03-18 17:59:31+00:00
- **Updated**: 2021-03-18 17:59:31+00:00
- **Authors**: Despoina Paschalidou, Angelos Katharopoulos, Andreas Geiger, Sanja Fidler
- **Comment**: To appear in CVPR 2021
- **Journal**: None
- **Summary**: Impressive progress in 3D shape extraction led to representations that can capture object geometries with high fidelity. In parallel, primitive-based methods seek to represent objects as semantically consistent part arrangements. However, due to the simplicity of existing primitive representations, these methods fail to accurately reconstruct 3D shapes using a small number of primitives/parts. We address the trade-off between reconstruction quality and number of parts with Neural Parts, a novel 3D primitive representation that defines primitives using an Invertible Neural Network (INN) which implements homeomorphic mappings between a sphere and the target object. The INN allows us to compute the inverse mapping of the homeomorphism, which in turn, enables the efficient computation of both the implicit surface function of a primitive and its mesh, without any additional post-processing. Our model learns to parse 3D objects into semantically consistent part arrangements without any part-level supervision. Evaluations on ShapeNet, D-FAUST and FreiHAND demonstrate that our primitives can capture complex geometries and thus simultaneously achieve geometrically accurate as well as interpretable reconstructions using an order of magnitude fewer primitives than state-of-the-art shape abstraction methods.



### Neural Networks for Semantic Gaze Analysis in XR Settings
- **Arxiv ID**: http://arxiv.org/abs/2103.10451v1
- **DOI**: 10.1145/3448017.3457380
- **Categories**: **cs.CV**, cs.HC, cs.LG, 68T45, I.4.8; I.4.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2103.10451v1)
- **Published**: 2021-03-18 18:05:01+00:00
- **Updated**: 2021-03-18 18:05:01+00:00
- **Authors**: Lena Stubbemann, Dominik D√ºrrschnabel, Robert Refflinghaus
- **Comment**: 16 pages, 6 figures, 1 table, Accepted to: ETRA2021, ACM Symposium on
  Eye Tracking Research and Applications
- **Journal**: None
- **Summary**: Virtual-reality (VR) and augmented-reality (AR) technology is increasingly combined with eye-tracking. This combination broadens both fields and opens up new areas of application, in which visual perception and related cognitive processes can be studied in interactive but still well controlled settings. However, performing a semantic gaze analysis of eye-tracking data from interactive three-dimensional scenes is a resource-intense task, which so far has been an obstacle to economic use. In this paper we present a novel approach which minimizes time and information necessary to annotate volumes of interest (VOIs) by using techniques from object recognition. To do so, we train convolutional neural networks (CNNs) on synthetic data sets derived from virtual models using image augmentation techniques. We evaluate our method in real and virtual environments, showing that the method can compete with state-of-the-art approaches, while not relying on additional markers or preexisting databases but instead offering cross-platform use.



### 3D Human Pose Estimation with Spatial and Temporal Transformers
- **Arxiv ID**: http://arxiv.org/abs/2103.10455v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2103.10455v3)
- **Published**: 2021-03-18 18:14:37+00:00
- **Updated**: 2021-08-22 00:15:03+00:00
- **Authors**: Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen Chen, Zhengming Ding
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Transformer architectures have become the model of choice in natural language processing and are now being introduced into computer vision tasks such as image classification, object detection, and semantic segmentation. However, in the field of human pose estimation, convolutional architectures still remain dominant. In this work, we present PoseFormer, a purely transformer-based approach for 3D human pose estimation in videos without convolutional architectures involved. Inspired by recent developments in vision transformers, we design a spatial-temporal transformer structure to comprehensively model the human joint relations within each frame as well as the temporal correlations across frames, then output an accurate 3D human pose of the center frame. We quantitatively and qualitatively evaluate our method on two popular and standard benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments show that PoseFormer achieves state-of-the-art performance on both datasets. Code is available at \url{https://github.com/zczcwh/PoseFormer}



### Reading Isn't Believing: Adversarial Attacks On Multi-Modal Neurons
- **Arxiv ID**: http://arxiv.org/abs/2103.10480v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.10480v1)
- **Published**: 2021-03-18 18:56:51+00:00
- **Updated**: 2021-03-18 18:56:51+00:00
- **Authors**: David A. Noever, Samantha E. Miller Noever
- **Comment**: None
- **Journal**: None
- **Summary**: With Open AI's publishing of their CLIP model (Contrastive Language-Image Pre-training), multi-modal neural networks now provide accessible models that combine reading with visual recognition. Their network offers novel ways to probe its dual abilities to read text while classifying visual objects. This paper demonstrates several new categories of adversarial attacks, spanning basic typographical, conceptual, and iconographic inputs generated to fool the model into making false or absurd classifications. We demonstrate that contradictory text and image signals can confuse the model into choosing false (visual) options. Like previous authors, we show by example that the CLIP model tends to read first, look later, a phenomenon we describe as reading isn't believing.



### Concentric Spherical GNN for 3D Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.10484v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.10484v1)
- **Published**: 2021-03-18 19:05:04+00:00
- **Updated**: 2021-03-18 19:05:04+00:00
- **Authors**: James Fox, Bo Zhao, Sivasankaran Rajamanickam, Rampi Ramprasad, Le Song
- **Comment**: This paper has been submitted for conference review
- **Journal**: None
- **Summary**: Learning 3D representations that generalize well to arbitrarily oriented inputs is a challenge of practical importance in applications varying from computer vision to physics and chemistry. We propose a novel multi-resolution convolutional architecture for learning over concentric spherical feature maps, of which the single sphere representation is a special case. Our hierarchical architecture is based on alternatively learning to incorporate both intra-sphere and inter-sphere information. We show the applicability of our method for two different types of 3D inputs, mesh objects, which can be regularly sampled, and point clouds, which are irregularly distributed. We also propose an efficient mapping of point clouds to concentric spherical images, thereby bridging spherical convolutions on grids with general point clouds. We demonstrate the effectiveness of our approach in improving state-of-the-art performance on 3D classification tasks with rotated data.



### Recent Advances in Deep Learning Techniques for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.10492v2
- **DOI**: 10.1109/ACCESS.2021.3096136
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.10492v2)
- **Published**: 2021-03-18 19:39:12+00:00
- **Updated**: 2021-07-21 16:31:53+00:00
- **Authors**: Md. Tahmid Hasan Fuad, Awal Ahmed Fime, Delowar Sikder, Md. Akil Raihan Iftee, Jakaria Rabbi, Mabrook S. Al-rakhami, Abdu Gumae, Ovishake Sen, Mohtasim Fuad, Md. Nazrul Islam
- **Comment**: 32 pages and citation: M. T. H. Fuad et al., "Recent Advances in Deep
  Learning Techniques for Face Recognition," in IEEE Access, vol. 9, pp.
  99112-99142, 2021, doi: 10.1109/ACCESS.2021.3096136
- **Journal**: in IEEE Access, vol. 9, pp. 99112-99142, 2021
- **Summary**: In recent years, researchers have proposed many deep learning (DL) methods for various tasks, and particularly face recognition (FR) made an enormous leap using these techniques. Deep FR systems benefit from the hierarchical architecture of the DL methods to learn discriminative face representation. Therefore, DL techniques significantly improve state-of-the-art performance on FR systems and encourage diverse and efficient real-world applications. In this paper, we present a comprehensive analysis of various FR systems that leverage the different types of DL techniques, and for the study, we summarize 168 recent contributions from this area. We discuss the papers related to different algorithms, architectures, loss functions, activation functions, datasets, challenges, improvement ideas, current and future trends of DL-based FR systems. We provide a detailed discussion of various DL methods to understand the current state-of-the-art, and then we discuss various activation and loss functions for the methods. Additionally, we summarize different datasets used widely for FR tasks and discuss challenges related to illumination, expression, pose variations, and occlusion. Finally, we discuss improvement ideas, current and future trends of FR tasks.



### Image Synthesis for Data Augmentation in Medical CT using Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.10493v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10493v2)
- **Published**: 2021-03-18 19:47:11+00:00
- **Updated**: 2021-03-22 01:00:38+00:00
- **Authors**: Arjun Krishna, Kedar Bartake, Chuang Niu, Ge Wang, Youfang Lai, Xun Jia, Klaus Mueller
- **Comment**: Fully3D 2021
- **Journal**: None
- **Summary**: Deep learning has shown great promise for CT image reconstruction, in particular to enable low dose imaging and integrated diagnostics. These merits, however, stand at great odds with the low availability of diverse image data which are needed to train these neural networks. We propose to overcome this bottleneck via a deep reinforcement learning (DRL) approach that is integrated with a style-transfer (ST) methodology, where the DRL generates the anatomical shapes and the ST synthesizes the texture detail. We show that our method bears high promise for generating novel and anatomically accurate high resolution CT images at large and diverse quantities. Our approach is specifically designed to work with even small image datasets which is desirable given the often low amount of image data many researchers have available to them.



### Ano-Graph: Learning Normal Scene Contextual Graphs to Detect Video Anomalies
- **Arxiv ID**: http://arxiv.org/abs/2103.10502v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10502v3)
- **Published**: 2021-03-18 20:08:53+00:00
- **Updated**: 2022-11-16 08:50:27+00:00
- **Authors**: Masoud Pourreza, Mohammadreza Salehi, Mohammad Sabokrou
- **Comment**: Inconsistencies in the results
- **Journal**: None
- **Summary**: Video anomaly detection has proved to be a challenging task owing to its unsupervised training procedure and high spatio-temporal complexity existing in real-world scenarios. In the absence of anomalous training samples, state-of-the-art methods try to extract features that fully grasp normal behaviors in both space and time domains using different approaches such as autoencoders, or generative adversarial networks. However, these approaches completely ignore or, by using the ability of deep networks in the hierarchical modeling, poorly model the spatio-temporal interactions that exist between objects. To address this issue, we propose a novel yet efficient method named Ano-Graph for learning and modeling the interaction of normal objects. Towards this end, a Spatio-Temporal Graph (STG) is made by considering each node as an object's feature extracted from a real-time off-the-shelf object detector, and edges are made based on their interactions. After that, a self-supervised learning method is employed on the STG in such a way that encapsulates interactions in a semantic space. Our method is data-efficient, significantly more robust against common real-world variations such as illumination, and passes SOTA by a large margin on the challenging datasets ADOC and Street Scene while stays competitive on Avenue, ShanghaiTech, and UCSD.



### UNETR: Transformers for 3D Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.10504v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.10504v3)
- **Published**: 2021-03-18 20:17:15+00:00
- **Updated**: 2021-10-09 17:25:43+00:00
- **Authors**: Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger Roth, Daguang Xu
- **Comment**: Accepted to IEEE Winter Conference on Applications of Computer Vision
  (WACV) 2022
- **Journal**: None
- **Summary**: Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful "U-shaped" network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard. Code: https://monai.io/research/unetr



### DeepBF: Malicious URL detection using Learned Bloom Filter and Evolutionary Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.12544v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.NE, 68Txx, 97P80, 92B20, 68Qxx, K.6.5; E.3; E.4; D.4.6; G.3; I.5; I.2.6; G.1.6
- **Links**: [PDF](http://arxiv.org/pdf/2103.12544v2)
- **Published**: 2021-03-18 21:53:22+00:00
- **Updated**: 2022-02-26 21:41:07+00:00
- **Authors**: Ripon Patgiri, Anupam Biswas, Sabuzima Nayak
- **Comment**: This work has been submitted to the Springer for possible publication
- **Journal**: None
- **Summary**: Malicious URL detection is an emerging research area due to continuous modernization of various systems, for instance, Edge Computing. In this article, we present a novel malicious URL detection technique, called deepBF (deep learning and Bloom Filter). deepBF is presented in two-fold. Firstly, we propose a learned Bloom Filter using 2-dimensional Bloom Filter. We experimentally decide the best non-cryptography string hash function. Then, we derive a modified non-cryptography string hash function from the selected hash function for deepBF by introducing biases in the hashing method and compared among the string hash functions. The modified string hash function is compared to other variants of diverse non-cryptography string hash functions. It is also compared with various filters, particularly, counting Bloom Filter, Kirsch \textit{et al.}, and Cuckoo Filter using various use cases. The use cases unearth weakness and strength of the filters. Secondly, we propose a malicious URL detection mechanism using deepBF. We apply the evolutionary convolutional neural network to identify the malicious URLs. The evolutionary convolutional neural network is trained and tested with malicious URL datasets. The output is tested in deepBF for accuracy. We have achieved many conclusions from our experimental evaluation and results and are able to reach various conclusive decisions which are presented in the article.



### CDFI: Compression-Driven Network Design for Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2103.10559v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10559v2)
- **Published**: 2021-03-18 22:59:42+00:00
- **Updated**: 2021-03-28 02:52:52+00:00
- **Authors**: Tianyu Ding, Luming Liang, Zhihui Zhu, Ilya Zharkov
- **Comment**: To appear in the proceedings of 2021 IEEE/CVF Conference on Computer
  Vision and Pattern Recognition (CVPR)
- **Journal**: None
- **Summary**: DNN-based frame interpolation--that generates the intermediate frames given two consecutive frames--typically relies on heavy model architectures with a huge number of features, preventing them from being deployed on systems with limited resources, e.g., mobile devices. We propose a compression-driven network design for frame interpolation (CDFI), that leverages model pruning through sparsity-inducing optimization to significantly reduce the model size while achieving superior performance. Concretely, we first compress the recently proposed AdaCoF model and show that a 10X compressed AdaCoF performs similarly as its original counterpart; then we further improve this compressed model by introducing a multi-resolution warping module, which boosts visual consistencies with multi-level details. As a consequence, we achieve a significant performance gain with only a quarter in size compared with the original AdaCoF. Moreover, our model performs favorably against other state-of-the-arts in a broad range of datasets. Finally, the proposed compression-driven framework is generic and can be easily transferred to other DNN-based frame interpolation algorithm. Our source code is available at https://github.com/tding1/CDFI.



### CLTA: Contents and Length-based Temporal Attention for Few-shot Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.10567v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10567v1)
- **Published**: 2021-03-18 23:40:28+00:00
- **Updated**: 2021-03-18 23:40:28+00:00
- **Authors**: Yang Bo, Yangdi Lu, Wenbo He
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: Few-shot action recognition has attracted increasing attention due to the difficulty in acquiring the properly labelled training samples. Current works have shown that preserving spatial information and comparing video descriptors are crucial for few-shot action recognition. However, the importance of preserving temporal information is not well discussed. In this paper, we propose a Contents and Length-based Temporal Attention (CLTA) model, which learns customized temporal attention for the individual video to tackle the few-shot action recognition problem. CLTA utilizes the Gaussian likelihood function as the template to generate temporal attention and trains the learning matrices to study the mean and standard deviation based on both frame contents and length. We show that even a not fine-tuned backbone with an ordinary softmax classifier can still achieve similar or better results compared to the state-of-the-art few-shot action recognition with precisely captured temporal attention.



### Generic Perceptual Loss for Modeling Structured Output Dependencies
- **Arxiv ID**: http://arxiv.org/abs/2103.10571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.10571v1)
- **Published**: 2021-03-18 23:56:07+00:00
- **Updated**: 2021-03-18 23:56:07+00:00
- **Authors**: Yifan Liu, Hao Chen, Yu Chen, Wei Yin, Chunhua Shen
- **Comment**: Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition
  (CVPR), 2021
- **Journal**: None
- **Summary**: The perceptual loss has been widely used as an effective loss term in image synthesis tasks including image super-resolution, and style transfer. It was believed that the success lies in the high-level perceptual feature representations extracted from CNNs pretrained with a large set of images. Here we reveal that, what matters is the network structure instead of the trained weights. Without any learning, the structure of a deep network is sufficient to capture the dependencies between multiple levels of variable statistics using multiple layers of CNNs. This insight removes the requirements of pre-training and a particular network structure (commonly, VGG) that are previously assumed for the perceptual loss, thus enabling a significantly wider range of applications. To this end, we demonstrate that a randomly-weighted deep CNN can be used to model the structured dependencies of outputs. On a few dense per-pixel prediction tasks such as semantic segmentation, depth estimation and instance segmentation, we show improved results of using the extended randomized perceptual loss, compared to the baselines using pixel-wise loss alone. We hope that this simple, extended perceptual loss may serve as a generic structured-output loss that is applicable to most structured output learning tasks.



