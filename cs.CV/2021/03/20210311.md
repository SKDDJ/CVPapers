# Arxiv Papers in cs.CV on 2021-03-11
### A Vision Based Deep Reinforcement Learning Algorithm for UAV Obstacle Avoidance
- **Arxiv ID**: http://arxiv.org/abs/2103.06403v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.06403v1)
- **Published**: 2021-03-11 01:15:26+00:00
- **Updated**: 2021-03-11 01:15:26+00:00
- **Authors**: Jeremy Roghair, Kyungtae Ko, Amir Ehsan Niaraki Asli, Ali Jannesari
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: Integration of reinforcement learning with unmanned aerial vehicles (UAVs) to achieve autonomous flight has been an active research area in recent years. An important part focuses on obstacle detection and avoidance for UAVs navigating through an environment. Exploration in an unseen environment can be tackled with Deep Q-Network (DQN). However, value exploration with uniform sampling of actions may lead to redundant states, where often the environments inherently bear sparse rewards. To resolve this, we present two techniques for improving exploration for UAV obstacle avoidance. The first is a convergence-based approach that uses convergence error to iterate through unexplored actions and temporal threshold to balance exploration and exploitation. The second is a guidance-based approach using a Domain Network which uses a Gaussian mixture distribution to compare previously seen states to a predicted next state in order to select the next action. Performance and evaluation of these approaches were implemented in multiple 3-D simulation environments, with variation in complexity. The proposed approach demonstrates a two-fold improvement in average rewards compared to state of the art.



### 3D Head-Position Prediction in First-Person View by Considering Head Pose for Human-Robot Eye Contact
- **Arxiv ID**: http://arxiv.org/abs/2103.06417v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.06417v2)
- **Published**: 2021-03-11 02:16:53+00:00
- **Updated**: 2022-01-20 05:24:00+00:00
- **Authors**: Yuki Tamaru, Yasunori Ozaki, Yuki Okafuji, Junya Nakanishi, Yuichiro Yoshikawa, Jun Baba
- **Comment**: Accepted to the 17th ACM/IEEE International Conference on Human-Robot
  Interaction Late-Breaking Reports (HRI 2022 LBR)
- **Journal**: None
- **Summary**: For a humanoid robot to make eye contact and initiate communication with a person, it is necessary to estimate the person's head position. However, eye contact becomes difficult due to the mechanical delay of the robot when the person is moving. Owing to these issues, it is important to conduct a head-position prediction to mitigate the effect of the delay in the robot motion. Based on the fact that humans turn their heads before changing direction while walking, we hypothesized that the accuracy of three-dimensional (3D) head-position prediction from a first-person view can be improved by considering the head pose. We compared our method with a conventional Kalman filter-based approach, and found our method to be more accurate. The experiment results show that considering the head pose helps improve the accuracy of 3D head-position prediction.



### SAR-U-Net: squeeze-and-excitation block and atrous spatial pyramid pooling based residual U-Net for automatic liver segmentation in Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/2103.06419v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.06419v3)
- **Published**: 2021-03-11 02:32:59+00:00
- **Updated**: 2021-07-17 03:40:40+00:00
- **Authors**: Jinke Wang, Peiqing Lv, Haiying Wang, Changfa Shi
- **Comment**: 25 pages, 17 figures, accepted by Computer Methods and Programs in
  Biomedicine, DOI:10.1016/j.cmpb.2021.106268
- **Journal**: None
- **Summary**: Background and objective: In this paper, a modified U-Net based framework is presented, which leverages techniques from Squeeze-and-Excitation (SE) block, Atrous Spatial Pyramid Pooling (ASPP) and residual learning for accurate and robust liver CT segmentation, and the effectiveness of the proposed method was tested on two public datasets LiTS17 and SLiver07.   Methods: A new network architecture called SAR-U-Net was designed. Firstly, the SE block is introduced to adaptively extract image features after each convolution in the U-Net encoder, while suppressing irrelevant regions, and highlighting features of specific segmentation task; Secondly, ASPP was employed to replace the transition layer and the output layer, and acquire multi-scale image information via different receptive fields. Thirdly, to alleviate the degradation problem, the traditional convolution block was replaced with the residual block and thus prompt the network to gain accuracy from considerably increased depth.   Results: In the LiTS17 experiment, the mean values of Dice, VOE, RVD, ASD and MSD were 95.71, 9.52, -0.84, 1.54 and 29.14, respectively. Compared with other closely related 2D-based models, the proposed method achieved the highest accuracy. In the experiment of the SLiver07, the mean values of Dice, VOE, RVD, ASD and MSD were 97.31, 5.37, -1.08, 1.85 and 27.45, respectively. Compared with other closely related models, the proposed method achieved the highest segmentation accuracy except for the RVD.   Conclusion: The proposed model enables a great improvement on the accuracy compared to 2D-based models, and its robustness in circumvent challenging problems, such as small liver regions, discontinuous liver regions, and fuzzy liver boundaries, is also well demonstrated and validated.



### Holistic 3D Scene Understanding from a Single Image with Implicit Representation
- **Arxiv ID**: http://arxiv.org/abs/2103.06422v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06422v3)
- **Published**: 2021-03-11 02:52:46+00:00
- **Updated**: 2021-08-22 09:54:43+00:00
- **Authors**: Cheng Zhang, Zhaopeng Cui, Yinda Zhang, Bing Zeng, Marc Pollefeys, Shuaicheng Liu
- **Comment**: Published in CVPR 2021
- **Journal**: None
- **Summary**: We present a new pipeline for holistic 3D scene understanding from a single image, which could predict object shapes, object poses, and scene layout. As it is a highly ill-posed problem, existing methods usually suffer from inaccurate estimation of both shapes and layout especially for the cluttered scene due to the heavy occlusion between objects. We propose to utilize the latest deep implicit representation to solve this challenge. We not only propose an image-based local structured implicit network to improve the object shape estimation, but also refine the 3D object pose and scene layout via a novel implicit scene graph neural network that exploits the implicit local object features. A novel physical violation loss is also proposed to avoid incorrect context between objects. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods in terms of object shape, scene layout estimation, and 3D object detection.



### Robust 2D/3D Vehicle Parsing in CVIS
- **Arxiv ID**: http://arxiv.org/abs/2103.06432v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.06432v1)
- **Published**: 2021-03-11 03:35:05+00:00
- **Updated**: 2021-03-11 03:35:05+00:00
- **Authors**: Hui Miao, Feixiang Lu, Zongdai Liu, Liangjun Zhang, Dinesh Manocha, Bin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach to robustly detect and perceive vehicles in different camera views as part of a cooperative vehicle-infrastructure system (CVIS). Our formulation is designed for arbitrary camera views and makes no assumptions about intrinsic or extrinsic parameters. First, to deal with multi-view data scarcity, we propose a part-assisted novel view synthesis algorithm for data augmentation. We train a part-based texture inpainting network in a self-supervised manner. Then we render the textured model into the background image with the target 6-DoF pose. Second, to handle various camera parameters, we present a new method that produces dense mappings between image pixels and 3D points to perform robust 2D/3D vehicle parsing. Third, we build the first CVIS dataset for benchmarking, which annotates more than 1540 images (14017 instances) from real-world traffic scenarios. We combine these novel algorithms and datasets to develop a robust approach for 2D/3D vehicle parsing for CVIS. In practice, our approach outperforms SOTA methods on 2D detection, instance segmentation, and 6-DoF pose estimation, by 4.5%, 4.3%, and 2.9%, respectively. More details and results are included in the supplement. To facilitate future research, we will release the source code and the dataset on GitHub.



### Where is your place, Visual Place Recognition?
- **Arxiv ID**: http://arxiv.org/abs/2103.06443v2
- **DOI**: 10.24963/ijcai.2021/603
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.06443v2)
- **Published**: 2021-03-11 04:11:04+00:00
- **Updated**: 2021-11-09 04:10:53+00:00
- **Authors**: Sourav Garg, Tobias Fischer, Michael Milford
- **Comment**: Accepted to the International Joint Conference on Artificial
  Intelligence (IJCAI2021)
- **Journal**: None
- **Summary**: Visual Place Recognition (VPR) is often characterized as being able to recognize the same place despite significant changes in appearance and viewpoint. VPR is a key component of Spatial Artificial Intelligence, enabling robotic platforms and intelligent augmentation platforms such as augmented reality devices to perceive and understand the physical world. In this paper, we observe that there are three "drivers" that impose requirements on spatially intelligent agents and thus VPR systems: 1) the particular agent including its sensors and computational resources, 2) the operating environment of this agent, and 3) the specific task that the artificial agent carries out. In this paper, we characterize and survey key works in the VPR area considering those drivers, including their place representation and place matching choices. We also provide a new definition of VPR based on the visual overlap -- akin to spatial view cells in the brain -- that enables us to find similarities and differences to other research areas in the robotics and computer vision fields. We identify numerous open challenges and suggest areas that require more in-depth attention in future works.



### Full Page Handwriting Recognition via Image to Sequence Extraction
- **Arxiv ID**: http://arxiv.org/abs/2103.06450v3
- **DOI**: 10.1007/978-3-030-86334-0_4
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.06450v3)
- **Published**: 2021-03-11 04:37:29+00:00
- **Updated**: 2022-06-26 21:01:23+00:00
- **Authors**: Sumeet S. Singh, Sergey Karayev
- **Comment**: Appeared in ICDAR 2021
- **Journal**: None
- **Summary**: We present a Neural Network based Handwritten Text Recognition (HTR) model architecture that can be trained to recognize full pages of handwritten or printed text without image segmentation. Being based on Image to Sequence architecture, it can extract text present in an image and then sequence it correctly without imposing any constraints regarding orientation, layout and size of text and non-text. Further, it can also be trained to generate auxiliary markup related to formatting, layout and content. We use character level vocabulary, thereby enabling language and terminology of any subject. The model achieves a new state-of-art in paragraph level recognition on the IAM dataset. When evaluated on scans of real world handwritten free form test answers - beset with curved and slanted lines, drawings, tables, math, chemistry and other symbols - it performs better than all commercially available HTR cloud APIs. It is deployed in production as part of a commercial web application.



### Recent Advances on Neural Network Pruning at Initialization
- **Arxiv ID**: http://arxiv.org/abs/2103.06460v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2103.06460v3)
- **Published**: 2021-03-11 05:01:52+00:00
- **Updated**: 2022-05-23 21:54:31+00:00
- **Authors**: Huan Wang, Can Qin, Yue Bai, Yulun Zhang, Yun Fu
- **Comment**: Accepted in IJCAI'22 Survey Track. Code base:
  https://github.com/mingsun-tse/smile-pruning
- **Journal**: None
- **Summary**: Neural network pruning typically removes connections or neurons from a pretrained converged model; while a new pruning paradigm, pruning at initialization (PaI), attempts to prune a randomly initialized network. This paper offers the first survey concentrated on this emerging pruning fashion. We first introduce a generic formulation of neural network pruning, followed by the major classic pruning topics. Then, as the main body of this paper, a thorough and structured literature review of PaI methods is presented, consisting of two major tracks (sparse training and sparse selection). Finally, we summarize the surge of PaI compared to PaT and discuss the open problems. Apart from the dedicated literature review, this paper also offers a code base for easy sanity-checking and benchmarking of different PaI methods.



### Pavement Distress Detection and Segmentation using YOLOv4 and DeepLabv3 on Pavements in the Philippines
- **Arxiv ID**: http://arxiv.org/abs/2103.06467v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2103.06467v1)
- **Published**: 2021-03-11 05:25:29+00:00
- **Updated**: 2021-03-11 05:25:29+00:00
- **Authors**: James-Andrew Sarmiento
- **Comment**: None
- **Journal**: None
- **Summary**: Road transport infrastructure is critical for safe, fast, economical, and reliable mobility within the whole country that is conducive to a productive society. However, roads tend to deteriorate over time due to natural causes in the environment and repeated traffic loads. Pavement Distress (PD) detection is essential in monitoring the current conditions of the public roads to enable targeted rehabilitation and preventive maintenance. Nonetheless, distress detection surveys are still done via manual inspection for developing countries such as the Philippines. This study proposed the use of deep learning for two ways of recording pavement distresses from 2D RGB images - detection and segmentation. YOLOv4 is used for pavement distress detection while DeepLabv3 is employed for pavement distress segmentation on a small dataset of pavement images in the Philippines. This study aims to provide a basis to potentially spark solutions in building a cheap, scalable, and automated end-to-end solution for PD detection in the country.



### DAFAR: Defending against Adversaries by Feedback-Autoencoder Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2103.06487v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2103.06487v2)
- **Published**: 2021-03-11 06:18:50+00:00
- **Updated**: 2021-03-17 14:49:12+00:00
- **Authors**: Haowen Liu, Ping Yi, Hsiao-Ying Lin, Jie Shi, Weidong Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has shown impressive performance on challenging perceptual tasks and has been widely used in software to provide intelligent services. However, researchers found deep neural networks vulnerable to adversarial examples. Since then, many methods are proposed to defend against adversaries in inputs, but they are either attack-dependent or shown to be ineffective with new attacks. And most of existing techniques have complicated structures or mechanisms that cause prohibitively high overhead or latency, impractical to apply on real software.   We propose DAFAR, a feedback framework that allows deep learning models to detect/purify adversarial examples in high effectiveness and universality, with low area and time overhead. DAFAR has a simple structure, containing a victim model, a plug-in feedback network, and a detector. The key idea is to import the high-level features from the victim model's feature extraction layers into the feedback network to reconstruct the input. This data stream forms a feedback autoencoder. For strong attacks, it transforms the imperceptible attack on the victim model into the obvious reconstruction-error attack on the feedback autoencoder directly, which is much easier to detect; for weak attacks, the reformation process destroys the structure of adversarial examples. Experiments are conducted on MNIST and CIFAR-10 data-sets, showing that DAFAR is effective against popular and arguably most advanced attacks without losing performance on legitimate samples, with high effectiveness and universality across attack methods and parameters.



### Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.06495v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06495v1)
- **Published**: 2021-03-11 06:47:45+00:00
- **Updated**: 2021-03-11 06:47:45+00:00
- **Authors**: Shancheng Fang, Hongtao Xie, Yuxin Wang, Zhendong Mao, Yongdong Zhang
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Linguistic knowledge is of great benefit to scene text recognition. However, how to effectively model linguistic rules in end-to-end deep networks remains a research challenge. In this paper, we argue that the limited capacity of language models comes from: 1) implicitly language modeling; 2) unidirectional feature representation; and 3) language model with noise input. Correspondingly, we propose an autonomous, bidirectional and iterative ABINet for scene text recognition. Firstly, the autonomous suggests to block gradient flow between vision and language models to enforce explicitly language modeling. Secondly, a novel bidirectional cloze network (BCN) as the language model is proposed based on bidirectional feature representation. Thirdly, we propose an execution manner of iterative correction for language model which can effectively alleviate the impact of noise input. Additionally, based on the ensemble of iterative predictions, we propose a self-training method which can learn from unlabeled images effectively. Extensive experiments indicate that ABINet has superiority on low-quality images and achieves state-of-the-art results on several mainstream benchmarks. Besides, the ABINet trained with ensemble self-training shows promising improvement in realizing human-level recognition. Code is available at https://github.com/FangShancheng/ABINet.



### 3D Human Pose, Shape and Texture from Low-Resolution Images and Videos
- **Arxiv ID**: http://arxiv.org/abs/2103.06498v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.06498v1)
- **Published**: 2021-03-11 06:52:12+00:00
- **Updated**: 2021-03-11 06:52:12+00:00
- **Authors**: Xiangyu Xu, Hao Chen, Francesc Moreno-Noguer, Laszlo A. Jeni, Fernando De la Torre
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2007.13666
- **Journal**: None
- **Summary**: 3D human pose and shape estimation from monocular images has been an active research area in computer vision. Existing deep learning methods for this task rely on high-resolution input, which however, is not always available in many scenarios such as video surveillance and sports broadcasting. Two common approaches to deal with low-resolution images are applying super-resolution techniques to the input, which may result in unpleasant artifacts, or simply training one model for each resolution, which is impractical in many realistic applications.   To address the above issues, this paper proposes a novel algorithm called RSC-Net, which consists of a Resolution-aware network, a Self-supervision loss, and a Contrastive learning scheme. The proposed method is able to learn 3D body pose and shape across different resolutions with one single model. The self-supervision loss enforces scale-consistency of the output, and the contrastive learning scheme enforces scale-consistency of the deep features. We show that both these new losses provide robustness when learning in a weakly-supervised manner. Moreover, we extend the RSC-Net to handle low-resolution videos and apply it to reconstruct textured 3D pedestrians from low-resolution input. Extensive experiments demonstrate that the RSC-Net can achieve consistently better results than the state-of-the-art methods for challenging low-resolution images.



### Density-aware Haze Image Synthesis by Self-Supervised Content-Style Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2103.06501v2
- **DOI**: 10.1109/TCSVT.2021.3130158
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.06501v2)
- **Published**: 2021-03-11 06:53:18+00:00
- **Updated**: 2021-11-25 12:04:35+00:00
- **Authors**: Chi Zhang, Zihang Lin, Liheng Xu, Zongliang Li, Wei Tang, Yuehu Liu, Gaofeng Meng, Le Wang, Li Li
- **Comment**: 21 pages, 19 figures, 6 tables
- **Journal**: IEEE Transactions on Circuits and Systems for Video Technology
- **Summary**: The key procedure of haze image translation through adversarial training lies in the disentanglement between the feature only involved in haze synthesis, i.e.style feature, and the feature representing the invariant semantic content, i.e. content feature. Previous methods separate content feature apart by utilizing it to classify haze image during the training process. However, in this paper we recognize the incompleteness of the content-style disentanglement in such technical routine. The flawed style feature entangled with content information inevitably leads the ill-rendering of the haze images. To address, we propose a self-supervised style regression via stochastic linear interpolation to reduce the content information in style feature. The ablative experiments demonstrate the disentangling completeness and its superiority in level-aware haze image synthesis. Moreover, the generated haze data are applied in the testing generalization of vehicle detectors. Further study between haze-level and detection performance shows that haze has obvious impact on the generalization of the vehicle detectors and such performance degrading level is linearly correlated to the haze-level, which, in turn, validates the effectiveness of the proposed method.



### Instance Segmentation GNNs for One-Shot Conformal Tracking at the LHC
- **Arxiv ID**: http://arxiv.org/abs/2103.06509v1
- **DOI**: None
- **Categories**: **cs.CV**, hep-ex
- **Links**: [PDF](http://arxiv.org/pdf/2103.06509v1)
- **Published**: 2021-03-11 07:15:55+00:00
- **Updated**: 2021-03-11 07:15:55+00:00
- **Authors**: Savannah Thais, Gage DeZoort
- **Comment**: Presented at NeurIPS Machine Learning and the Physical Sciences
  Workshop 2020
- **Journal**: None
- **Summary**: 3D instance segmentation remains a challenging problem in computer vision. Particle tracking at colliders like the LHC can be conceptualized as an instance segmentation task: beginning from a point cloud of hits in a particle detector, an algorithm must identify which hits belong to individual particle trajectories and extract track properties. Graph Neural Networks (GNNs) have shown promising performance on standard instance segmentation tasks. In this work we demonstrate the applicability of instance segmentation GNN architectures to particle tracking; moreover, we re-imagine the traditional Cartesian space approach to track-finding and instead work in a conformal geometry that allows the GNN to identify tracks and extract parameters in a single shot.



### A learning-based view extrapolation method for axial super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2103.06510v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.06510v1)
- **Published**: 2021-03-11 07:22:13+00:00
- **Updated**: 2021-03-11 07:22:13+00:00
- **Authors**: Zhaolin Xiao, Jinglei Shi, Xiaoran Jiang, Christine Guillemot
- **Comment**: None
- **Journal**: None
- **Summary**: Axial light field resolution refers to the ability to distinguish features at different depths by refocusing. The axial refocusing precision corresponds to the minimum distance in the axial direction between two distinguishable refocusing planes. High refocusing precision can be essential for some light field applications like microscopy. In this paper, we propose a learning-based method to extrapolate novel views from axial volumes of sheared epipolar plane images (EPIs). As extended numerical aperture (NA) in classical imaging, the extrapolated light field gives re-focused images with a shallower depth of field (DOF), leading to more accurate refocusing results. Most importantly, the proposed approach does not need accurate depth estimation. Experimental results with both synthetic and real light fields show that the method not only works well for light fields with small baselines as those captured by plenoptic cameras (especially for the plenoptic 1.0 cameras), but also applies to light fields with larger baselines.



### DualPoseNet: Category-level 6D Object Pose and Size Estimation Using Dual Pose Network with Refined Learning of Pose Consistency
- **Arxiv ID**: http://arxiv.org/abs/2103.06526v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06526v3)
- **Published**: 2021-03-11 08:33:47+00:00
- **Updated**: 2021-08-16 13:02:58+00:00
- **Authors**: Jiehong Lin, Zewei Wei, Zhihao Li, Songcen Xu, Kui Jia, Yuanqing Li
- **Comment**: Accepted by IEEE International Conference on Computer Vision (ICCV)
  2021
- **Journal**: None
- **Summary**: Category-level 6D object pose and size estimation is to predict full pose configurations of rotation, translation, and size for object instances observed in single, arbitrary views of cluttered scenes. In this paper, we propose a new method of Dual Pose Network with refined learning of pose consistency for this task, shortened as DualPoseNet. DualPoseNet stacks two parallel pose decoders on top of a shared pose encoder, where the implicit decoder predicts object poses with a working mechanism different from that of the explicit one; they thus impose complementary supervision on the training of pose encoder. We construct the encoder based on spherical convolutions, and design a module of Spherical Fusion wherein for a better embedding of pose-sensitive features from the appearance and shape observations. Given no testing CAD models, it is the novel introduction of the implicit decoder that enables the refined pose prediction during testing, by enforcing the predicted pose consistency between the two decoders using a self-adaptive loss term. Thorough experiments on benchmarks of both category- and instance-level object pose datasets confirm efficacy of our designs. DualPoseNet outperforms existing methods with a large margin in the regime of high precision. Our code is released publicly at https://github.com/Gorilla-Lab-SCUT/DualPoseNet.



### Triple-cooperative Video Shadow Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.06533v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06533v1)
- **Published**: 2021-03-11 08:54:19+00:00
- **Updated**: 2021-03-11 08:54:19+00:00
- **Authors**: Zhihao Chen, Liang Wan, Lei Zhu, Jia Shen, Huazhu Fu, Wennan Liu, Jing Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Shadow detection in a single image has received significant research interest in recent years. However, much fewer works have been explored in shadow detection over dynamic scenes. The bottleneck is the lack of a well-established dataset with high-quality annotations for video shadow detection. In this work, we collect a new video shadow detection dataset, which contains 120 videos with 11, 685 frames, covering 60 object categories, varying lengths, and different motion/lighting conditions. All the frames are annotated with a high-quality pixel-level shadow mask. To the best of our knowledge, this is the first learning-oriented dataset for video shadow detection. Furthermore, we develop a new baseline model, named triple-cooperative video shadow detection network (TVSD-Net). It utilizes triple parallel networks in a cooperative manner to learn discriminative representations at intra-video and inter-video levels. Within the network, a dual gated co-attention module is proposed to constrain features from neighboring frames in the same video, while an auxiliary similarity loss is introduced to mine semantic information between different videos. Finally, we conduct a comprehensive study on ViSha, evaluating 12 state-of-the-art models (including single image shadow detectors, video object segmentation, and saliency detection methods). Experiments demonstrate that our model outperforms SOTA competitors.



### Calibrated and Partially Calibrated Semi-Generalized Homographies
- **Arxiv ID**: http://arxiv.org/abs/2103.06535v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06535v3)
- **Published**: 2021-03-11 08:56:24+00:00
- **Updated**: 2021-10-11 11:43:04+00:00
- **Authors**: Snehal Bhayani, Torsten Sattler, Daniel Barath, Patrik Beliansky, Janne Heikkila, Zuzana Kukelova
- **Comment**: Accepted to ICCV 2021 and to appear in the conference proceedings
- **Journal**: None
- **Summary**: In this paper, we propose the first minimal solutions for estimating the semi-generalized homography given a perspective and a generalized camera. The proposed solvers use five 2D-2D image point correspondences induced by a scene plane. One of them assumes the perspective camera to be fully calibrated, while the other solver estimates the unknown focal length together with the absolute pose parameters. This setup is particularly important in structure-from-motion and image-based localization pipelines, where a new camera is localized in each step with respect to a set of known cameras and 2D-3D correspondences might not be available. As a consequence of a clever parametrization and the elimination ideal method, our approach only needs to solve a univariate polynomial of degree five or three. The proposed solvers are stable and efficient as demonstrated by a number of synthetic and real-world experiments.



### Affect2MM: Affective Analysis of Multimedia Content Using Emotion Causality
- **Arxiv ID**: http://arxiv.org/abs/2103.06541v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2103.06541v1)
- **Published**: 2021-03-11 09:07:25+00:00
- **Updated**: 2021-03-11 09:07:25+00:00
- **Authors**: Trisha Mittal, Puneet Mathur, Aniket Bera, Dinesh Manocha
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: We present Affect2MM, a learning method for time-series emotion prediction for multimedia content. Our goal is to automatically capture the varying emotions depicted by characters in real-life human-centric situations and behaviors. We use the ideas from emotion causation theories to computationally model and determine the emotional state evoked in clips of movies. Affect2MM explicitly models the temporal causality using attention-based methods and Granger causality. We use a variety of components like facial features of actors involved, scene understanding, visual aesthetics, action/situation description, and movie script to obtain an affective-rich representation to understand and perceive the scene. We use an LSTM-based learning model for emotion perception. To evaluate our method, we analyze and compare our performance on three datasets, SENDv1, MovieGraphs, and the LIRIS-ACCEDE dataset, and observe an average of 10-15% increase in the performance over SOTA methods for all three datasets.



### Integrated Age Estimation Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2103.06546v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.06546v1)
- **Published**: 2021-03-11 09:14:10+00:00
- **Updated**: 2021-03-11 09:14:10+00:00
- **Authors**: Fan Li, Yongming Li, Pin Wang, Jie Xiao, Fang Yan, Xinke Li
- **Comment**: None
- **Journal**: None
- **Summary**: Machine-learning-based age estimation has received lots of attention. Traditional age estimation mechanism focuses estimation age error, but ignores that there is a deviation between the estimated age and real age due to disease. Pathological age estimation mechanism the author proposed before introduces age deviation to solve the above problem and improves classification capability of the estimated age significantly. However,it does not consider the age estimation error of the normal control (NC) group and results in a larger error between the estimated age and real age of NC group. Therefore, an integrated age estimation mechanism based on Decision-Level fusion of error and deviation orientation model is proposed to solve the problem.Firstly, the traditional age estimation and pathological age estimation mechanisms are weighted together.Secondly, their optimal weights are obtained by minimizing mean absolute error (MAE) between the estimated age and real age of normal people. In the experimental section, several representative age-related datasets are used for verification of the proposed method. The results show that the proposed age estimation mechanism achieves a good tradeoff effect of age estimation. It not only improves the classification ability of the estimated age, but also reduces the age estimation error of the NC group. In general, the proposed age estimation mechanism is effective. Additionally, the mechanism is a framework mechanism that can be used to construct different specific age estimation algorithms, contributing to relevant research.



### PREPRINT: Comparison of deep learning and hand crafted features for mining simulation data
- **Arxiv ID**: http://arxiv.org/abs/2103.06552v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06552v1)
- **Published**: 2021-03-11 09:28:00+00:00
- **Updated**: 2021-03-11 09:28:00+00:00
- **Authors**: Theodoros Georgiou, Sebastian Schmitt, Thomas Bäck, Nan Pu, Wei Chen, Michael Lew
- **Comment**: None
- **Journal**: Proceedings of the International Conference on Pattern Recognition
  (ICPR) 2020
- **Summary**: Computational Fluid Dynamics (CFD) simulations are a very important tool for many industrial applications, such as aerodynamic optimization of engineering designs like cars shapes, airplanes parts etc. The output of such simulations, in particular the calculated flow fields, are usually very complex and hard to interpret for realistic three-dimensional real-world applications, especially if time-dependent simulations are investigated. Automated data analysis methods are warranted but a non-trivial obstacle is given by the very large dimensionality of the data. A flow field typically consists of six measurement values for each point of the computational grid in 3D space and time (velocity vector values, turbulent kinetic energy, pressure and viscosity). In this paper we address the task of extracting meaningful results in an automated manner from such high dimensional data sets. We propose deep learning methods which are capable of processing such data and which can be trained to solve relevant tasks on simulation data, i.e. predicting drag and lift forces applied on an airfoil. We also propose an adaptation of the classical hand crafted features known from computer vision to address the same problem and compare a large variety of descriptors and detectors. Finally, we compile a large dataset of 2D simulations of the flow field around airfoils which contains 16000 flow fields with which we tested and compared approaches. Our results show that the deep learning-based methods, as well as hand crafted feature based approaches, are well-capable to accurately describe the content of the CFD simulation output on the proposed dataset.



### WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/2103.06561v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2103.06561v6)
- **Published**: 2021-03-11 09:39:49+00:00
- **Updated**: 2021-07-08 13:56:05+00:00
- **Authors**: Yuqi Huo, Manli Zhang, Guangzhen Liu, Haoyu Lu, Yizhao Gao, Guoxing Yang, Jingyuan Wen, Heng Zhang, Baogui Xu, Weihao Zheng, Zongzheng Xi, Yueqian Yang, Anwen Hu, Jinming Zhao, Ruichen Li, Yida Zhao, Liang Zhang, Yuqing Song, Xin Hong, Wanqing Cui, Danyang Hou, Yingyan Li, Junyi Li, Peiyu Liu, Zheng Gong, Chuhao Jin, Yuchong Sun, Shizhe Chen, Zhiwu Lu, Zhicheng Dou, Qin Jin, Yanyan Lan, Wayne Xin Zhao, Ruihua Song, Ji-Rong Wen
- **Comment**: This paper is the outcome of the Chinese multi-modal pre-training
  project called 'WenLan'
- **Journal**: None
- **Summary**: Multi-modal pre-training models have been intensively explored to bridge vision and language in recent years. However, most of them explicitly model the cross-modal interaction between image-text pairs, by assuming that there exists strong semantic correlation between the text and image modalities. Since this strong assumption is often invalid in real-world scenarios, we choose to implicitly model the cross-modal correlation for large-scale multi-modal pre-training, which is the focus of the Chinese project `WenLan' led by our team. Specifically, with the weak correlation assumption over image-text pairs, we propose a two-tower pre-training model called BriVL within the cross-modal contrastive learning framework. Unlike OpenAI CLIP that adopts a simple contrastive learning method, we devise a more advanced algorithm by adapting the latest method MoCo into the cross-modal scenario. By building a large queue-based dictionary, our BriVL can incorporate more negative samples in limited GPU resources. We further construct a large Chinese multi-source image-text dataset called RUC-CAS-WenLan for pre-training our BriVL model. Extensive experiments demonstrate that the pre-trained BriVL model outperforms both UNITER and OpenAI CLIP on various downstream tasks.



### PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.06564v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06564v1)
- **Published**: 2021-03-11 09:42:32+00:00
- **Updated**: 2021-03-11 09:42:32+00:00
- **Authors**: Xiangtai Li, Hao He, Xia Li, Duo Li, Guangliang Cheng, Jianping Shi, Lubin Weng, Yunhai Tong, Zhouchen Lin
- **Comment**: accepted by CVPR2021
- **Journal**: None
- **Summary**: Aerial Image Segmentation is a particular semantic segmentation problem and has several challenging characteristics that general semantic segmentation does not have. There are two critical issues: The one is an extremely foreground-background imbalanced distribution, and the other is multiple small objects along with the complex background. Such problems make the recent dense affinity context modeling perform poorly even compared with baselines due to over-introduced background context. To handle these problems, we propose a point-wise affinity propagation module based on the Feature Pyramid Network (FPN) framework, named PointFlow. Rather than dense affinity learning, a sparse affinity map is generated upon selected points between the adjacent features, which reduces the noise introduced by the background while keeping efficiency. In particular, we design a dual point matcher to select points from the salient area and object boundaries, respectively. Experimental results on three different aerial segmentation datasets suggest that the proposed method is more effective and efficient than state-of-the-art general semantic segmentation methods. Especially, our methods achieve the best speed and accuracy trade-off on three aerial benchmarks. Further experiments on three general semantic segmentation datasets prove the generality of our method. Code will be provided in (https: //github.com/lxtGH/PFSegNets).



### An unsupervised deep learning framework for medical image denoising
- **Arxiv ID**: http://arxiv.org/abs/2103.06575v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.06575v1)
- **Published**: 2021-03-11 10:03:02+00:00
- **Updated**: 2021-03-11 10:03:02+00:00
- **Authors**: Swati Rai, Jignesh S. Bhatt, S. K. Patra
- **Comment**: 22 pages, 7 figures, 4 tables
- **Journal**: None
- **Summary**: Medical image acquisition is often intervented by unwanted noise that corrupts the information content. This paper introduces an unsupervised medical image denoising technique that learns noise characteristics from the available images and constructs denoised images. It comprises of two blocks of data processing, viz., patch-based dictionaries that indirectly learn the noise and residual learning (RL) that directly learns the noise. The model is generalized to account for both 2D and 3D images considering different medical imaging instruments. The images are considered one-by-one from the stack of MRI/CT images as well as the entire stack is considered, and decomposed into overlapping image/volume patches. These patches are given to the patch-based dictionary learning to learn noise characteristics via sparse representation while given to the RL part to directly learn the noise properties. K-singular value decomposition (K-SVD) algorithm for sparse representation is used for training patch-based dictionaries. On the other hand, residue in the patches is trained using the proposed deep residue network. Iterating on these two parts, an optimum noise characterization for each image/volume patch is captured and in turn it is subtracted from the available respective image/volume patch. The obtained denoised image/volume patches are finally assembled to a denoised image or 3D stack. We provide an analysis of the proposed approach with other approaches. Experiments on MRI/CT datasets are run on a GPU-based supercomputer and the comparative results show that the proposed algorithm preserves the critical information in the images as well as improves the visual quality of the images.



### Preprint: Norm Loss: An efficient yet effective regularization method for deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/2103.06583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06583v1)
- **Published**: 2021-03-11 10:24:49+00:00
- **Updated**: 2021-03-11 10:24:49+00:00
- **Authors**: Theodoros Georgiou, Sebastian Schmitt, Thomas Bäck, Wei Chen, Michael Lew
- **Comment**: None
- **Journal**: Proceedings of the International Conference on Pattern Recognition
  (ICPR) 2020
- **Summary**: Convolutional neural network training can suffer from diverse issues like exploding or vanishing gradients, scaling-based weight space symmetry and covariant-shift. In order to address these issues, researchers develop weight regularization methods and activation normalization methods. In this work we propose a weight soft-regularization method based on the Oblique manifold. The proposed method uses a loss function which pushes each weight vector to have a norm close to one, i.e. the weight matrix is smoothly steered toward the so-called Oblique manifold. We evaluate our method on the very popular CIFAR-10, CIFAR-100 and ImageNet 2012 datasets using two state-of-the-art architectures, namely the ResNet and wide-ResNet. Our method introduces negligible computational overhead and the results show that it is competitive to the state-of-the-art and in some cases superior to it. Additionally, the results are less sensitive to hyperparameter settings such as batch size and regularization factor.



### Privacy-preserving Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.06587v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06587v1)
- **Published**: 2021-03-11 10:34:54+00:00
- **Updated**: 2021-03-11 10:34:54+00:00
- **Authors**: Peiyang He, Charlie Griffin, Krzysztof Kacprzyk, Artjom Joosen, Michael Collyer, Aleksandar Shtedritski, Yuki M. Asano
- **Comment**: None
- **Journal**: None
- **Summary**: Privacy considerations and bias in datasets are quickly becoming high-priority issues that the computer vision community needs to face. So far, little attention has been given to practical solutions that do not involve collection of new datasets. In this work, we show that for object detection on COCO, both anonymizing the dataset by blurring faces, as well as swapping faces in a balanced manner along the gender and skin tone dimension, can retain object detection performances while preserving privacy and partially balancing bias.



### MagFace: A Universal Representation for Face Recognition and Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2103.06627v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06627v4)
- **Published**: 2021-03-11 11:58:21+00:00
- **Updated**: 2021-07-26 12:54:25+00:00
- **Authors**: Qiang Meng, Shichao Zhao, Zhida Huang, Feng Zhou
- **Comment**: accepted at CVPR 2021, Oral
- **Journal**: IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR), 2021
- **Summary**: The performance of face recognition system degrades when the variability of the acquired faces increases. Prior work alleviates this issue by either monitoring the face quality in pre-processing or predicting the data uncertainty along with the face feature. This paper proposes MagFace, a category of losses that learn a universal feature embedding whose magnitude can measure the quality of the given face. Under the new loss, it can be proven that the magnitude of the feature embedding monotonically increases if the subject is more likely to be recognized. In addition, MagFace introduces an adaptive mechanism to learn a wellstructured within-class feature distributions by pulling easy samples to class centers while pushing hard samples away. This prevents models from overfitting on noisy low-quality samples and improves face recognition in the wild. Extensive experiments conducted on face recognition, quality assessments as well as clustering demonstrate its superiority over state-of-the-arts. The code is available at https://github.com/IrvingMeng/MagFace.



### Generalized Contrastive Optimization of Siamese Networks for Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.06638v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06638v4)
- **Published**: 2021-03-11 12:32:05+00:00
- **Updated**: 2023-04-20 09:24:25+00:00
- **Authors**: María Leyva-Vallina, Nicola Strisciuglio, Nicolai Petkov
- **Comment**: Published at CVPR2023 as arXiv:2303.11739
- **Journal**: None
- **Summary**: Visual place recognition is a challenging task in computer vision and a key component of camera-based localization and navigation systems. Recently, Convolutional Neural Networks (CNNs) achieved high results and good generalization capabilities. They are usually trained using pairs or triplets of images labeled as either similar or dissimilar, in a binary fashion. In practice, the similarity between two images is not binary, but continuous. Furthermore, training these CNNs is computationally complex and involves costly pair and triplet mining strategies. We propose a Generalized Contrastive loss (GCL) function that relies on image similarity as a continuous measure, and use it to train a siamese CNN. Furthermore, we present three techniques for automatic annotation of image pairs with labels indicating their degree of similarity, and deploy them to re-annotate the MSLS, TB-Places, and 7Scenes datasets. We demonstrate that siamese CNNs trained using the GCL function and the improved annotations consistently outperform their binary counterparts. Our models trained on MSLS outperform the state-of-the-art methods, including NetVLAD, NetVLAD-SARE, AP-GeM and Patch-NetVLAD, and generalize well on the Pittsburgh30k, Tokyo 24/7, RobotCar Seasons v2 and Extended CMU Seasons datasets. Furthermore, training a siamese network using the GCL function does not require complex pair mining. We release the source code at https://github.com/marialeyvallina/generalized_contrastive_loss.



### Deep Graph Matching under Quadratic Constraint
- **Arxiv ID**: http://arxiv.org/abs/2103.06643v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06643v2)
- **Published**: 2021-03-11 12:51:12+00:00
- **Updated**: 2021-03-14 06:47:22+00:00
- **Authors**: Quankai Gao, Fudong Wang, Nan Xue, Jin-Gang Yu, Gui-Song Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep learning based methods have demonstrated promising results on the graph matching problem, by relying on the descriptive capability of deep features extracted on graph nodes. However, one main limitation with existing deep graph matching (DGM) methods lies in their ignorance of explicit constraint of graph structures, which may lead the model to be trapped into local minimum in training. In this paper, we propose to explicitly formulate pairwise graph structures as a \textbf{quadratic constraint} incorporated into the DGM framework. The quadratic constraint minimizes the pairwise structural discrepancy between graphs, which can reduce the ambiguities brought by only using the extracted CNN features.   Moreover, we present a differentiable implementation to the quadratic constrained-optimization such that it is compatible with the unconstrained deep learning optimizer. To give more precise and proper supervision, a well-designed false matching loss against class imbalance is proposed, which can better penalize the false negatives and false positives with less overfitting. Exhaustive experiments demonstrate that our method competitive performance on real-world datasets.



### Real-Time Surface Fitting to RGBD Sensor Data
- **Arxiv ID**: http://arxiv.org/abs/2103.06644v1
- **DOI**: 10.1109/SECON.2017.7925286
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06644v1)
- **Published**: 2021-03-11 12:52:31+00:00
- **Updated**: 2021-03-11 12:52:31+00:00
- **Authors**: John Papadakis, Andrew R. Willis
- **Comment**: None
- **Journal**: None
- **Summary**: This article describes novel approaches to quickly estimate planar surfaces from RGBD sensor data. The approach manipulates the standard algebraic fitting equations into a form that allows many of the needed regression variables to be computed directly from the camera calibration information. As such, much of the computational burden required by a standard algebraic surface fit can be pre-computed. This provides a significant time and resource savings, especially when many surface fits are being performed which is often the case when RGBD point-cloud data is being analyzed for normal estimation, curvature estimation, polygonization or 3D segmentation applications. Using an integral image implementation, the proposed approaches show a significant increase in performance compared to the standard algebraic fitting approaches.



### Temporal Action Segmentation from Timestamp Supervision
- **Arxiv ID**: http://arxiv.org/abs/2103.06669v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06669v3)
- **Published**: 2021-03-11 13:52:41+00:00
- **Updated**: 2021-03-26 15:44:26+00:00
- **Authors**: Zhe Li, Yazan Abu Farha, Juergen Gall
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Temporal action segmentation approaches have been very successful recently. However, annotating videos with frame-wise labels to train such models is very expensive and time consuming. While weakly supervised methods trained using only ordered action lists require less annotation effort, the performance is still worse than fully supervised approaches. In this paper, we propose to use timestamp supervision for the temporal action segmentation task. Timestamps require a comparable annotation effort to weakly supervised approaches, and yet provide a more supervisory signal. To demonstrate the effectiveness of timestamp supervision, we propose an approach to train a segmentation model using only timestamps annotations. Our approach uses the model output and the annotated timestamps to generate frame-wise labels by detecting the action changes. We further introduce a confidence loss that forces the predicted probabilities to monotonically decrease as the distance to the timestamps increases. This ensures that all and not only the most distinctive frames of an action are learned during training. The evaluation on four datasets shows that models trained with timestamps annotations achieve comparable performance to the fully supervised approaches.



### Duplex Contextual Relation Network for Polyp Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.06725v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.06725v3)
- **Published**: 2021-03-11 15:19:54+00:00
- **Updated**: 2022-01-19 13:38:41+00:00
- **Authors**: Zijin Yin, Kongming Liang, Zhanyu Ma, Jun Guo
- **Comment**: Accepted to ISBI2022
- **Journal**: None
- **Summary**: Polyp segmentation is of great importance in the early diagnosis and treatment of colorectal cancer. Since polyps vary in their shape, size, color, and texture, accurate polyp segmentation is very challenging. One promising way to mitigate the diversity of polyps is to model the contextual relation for each pixel such as using attention mechanism. However, previous methods only focus on learning the dependencies between the position within an individual image and ignore the contextual relation across different images. In this paper, we propose Duplex Contextual Relation Network (DCRNet) to capture both within-image and cross-image contextual relations. Specifically, we first design Interior Contextual-Relation Module to estimate the similarity between each position and all the positions within the same image. Then Exterior Contextual-Relation Module is incorporated to estimate the similarity between each position and the positions across different images. Based on the above two types of similarity, the feature at one position can be further enhanced by the contextual region embedding within and across images. To store the characteristic region embedding from all the images, a memory bank is designed and operates as a queue. Therefore, the proposed method can relate similar features even though they come from different images. We evaluate the proposed method on the EndoScene, Kvasir-SEG and the recently released large-scale PICCOLO dataset. Experimental results show that the proposed DCRNet outperforms the state-of-the-art methods in terms of the widely-used evaluation metrics.



### Intraclass clustering: an implicit learning ability that regularizes DNNs
- **Arxiv ID**: http://arxiv.org/abs/2103.06733v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.06733v1)
- **Published**: 2021-03-11 15:26:27+00:00
- **Updated**: 2021-03-11 15:26:27+00:00
- **Authors**: Carbonnelle Simon, Christophe De Vleeschouwer
- **Comment**: Published as a conference paper at ICLR 2021
- **Journal**: None
- **Summary**: Several works have shown that the regularization mechanisms underlying deep neural networks' generalization performances are still poorly understood. In this paper, we hypothesize that deep neural networks are regularized through their ability to extract meaningful clusters among the samples of a class. This constitutes an implicit form of regularization, as no explicit training mechanisms or supervision target such behaviour. To support our hypothesis, we design four different measures of intraclass clustering, based on the neuron- and layer-level representations of the training data. We then show that these measures constitute accurate predictors of generalization performance across variations of a large set of hyperparameters (learning rate, batch size, optimizer, weight decay, dropout rate, data augmentation, network depth and width).



### ChallenCap: Monocular 3D Capture of Challenging Human Performances using Multi-Modal References
- **Arxiv ID**: http://arxiv.org/abs/2103.06747v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06747v2)
- **Published**: 2021-03-11 15:49:22+00:00
- **Updated**: 2021-03-29 13:08:54+00:00
- **Authors**: Yannan He, Anqi Pang, Xin Chen, Han Liang, Minye Wu, Yuexin Ma, Lan Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Capturing challenging human motions is critical for numerous applications, but it suffers from complex motion patterns and severe self-occlusion under the monocular setting. In this paper, we propose ChallenCap -- a template-based approach to capture challenging 3D human motions using a single RGB camera in a novel learning-and-optimization framework, with the aid of multi-modal references. We propose a hybrid motion inference stage with a generation network, which utilizes a temporal encoder-decoder to extract the motion details from the pair-wise sparse-view reference, as well as a motion discriminator to utilize the unpaired marker-based references to extract specific challenging motion characteristics in a data-driven manner. We further adopt a robust motion optimization stage to increase the tracking accuracy, by jointly utilizing the learned motion details from the supervised multi-modal references as well as the reliable motion hints from the input image reference. Extensive experiments on our new challenging motion dataset demonstrate the effectiveness and robustness of our approach to capture challenging human motions.



### Automatic Social Distance Estimation From Images: Performance Evaluation, Test Benchmark, and Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2103.06759v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4.7; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2103.06759v3)
- **Published**: 2021-03-11 16:15:20+00:00
- **Updated**: 2021-06-18 05:51:55+00:00
- **Authors**: Mert Seker, Anssi Männistö, Alexandros Iosifidis, Jenni Raitoharju
- **Comment**: 14 pages, 12 figures, 6 tables
- **Journal**: None
- **Summary**: The COVID-19 virus has caused a global pandemic since March 2020. The World Health Organization (WHO) has provided guidelines on how to reduce the spread of the virus and one of the most important measures is social distancing. Maintaining a minimum of one meter distance from other people is strongly suggested to reduce the risk of infection. This has created a strong interest in monitoring the social distances either as a safety measure or to study how the measures have affected human behavior and country-wise differences in this. The need for automatic social distance estimation algorithms is evident, but there is no suitable test benchmark for such algorithms. Collecting images with measured ground-truth pair-wise distances between all the people using different camera settings is cumbersome. Furthermore, performance evaluation for social distance estimation algorithms is not straightforward and there is no widely accepted evaluation protocol. In this paper, we provide a dataset of varying images with measured pair-wise social distances under different camera positionings and focal length values. We suggest a performance evaluation protocol and provide a benchmark to easily evaluate social distance estimation algorithms. We also propose a method for automatic social distance estimation. Our method takes advantage of object detection and human pose estimation. It can be applied on any single image as long as focal length and sensor size information are known. The results on our benchmark are encouraging with 92% human detection rate and only 28.9% average error in distance estimation among the detected people.



### ACTION-Net: Multipath Excitation for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.07372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.07372v1)
- **Published**: 2021-03-11 16:23:40+00:00
- **Updated**: 2021-03-11 16:23:40+00:00
- **Authors**: Zhengwei Wang, Qi She, Aljosa Smolic
- **Comment**: To appear in CVPR 2021
- **Journal**: None
- **Summary**: Spatial-temporal, channel-wise, and motion patterns are three complementary and crucial types of information for video action recognition. Conventional 2D CNNs are computationally cheap but cannot catch temporal relationships; 3D CNNs can achieve good performance but are computationally intensive. In this work, we tackle this dilemma by designing a generic and effective module that can be embedded into 2D CNNs. To this end, we propose a spAtio-temporal, Channel and moTion excitatION (ACTION) module consisting of three paths: Spatio-Temporal Excitation (STE) path, Channel Excitation (CE) path, and Motion Excitation (ME) path. The STE path employs one channel 3D convolution to characterize spatio-temporal representation. The CE path adaptively recalibrates channel-wise feature responses by explicitly modeling interdependencies between channels in terms of the temporal aspect. The ME path calculates feature-level temporal differences, which is then utilized to excite motion-sensitive channels. We equip 2D CNNs with the proposed ACTION module to form a simple yet effective ACTION-Net with very limited extra computational cost. ACTION-Net is demonstrated by consistently outperforming 2D CNN counterparts on three backbones (i.e., ResNet-50, MobileNet V2 and BNInception) employing three datasets (i.e., Something-Something V2, Jester, and EgoGesture). Codes are available at \url{https://github.com/V-Sense/ACTION-Net}.



### Unknown Object Segmentation from Stereo Images
- **Arxiv ID**: http://arxiv.org/abs/2103.06796v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.06796v1)
- **Published**: 2021-03-11 17:03:44+00:00
- **Updated**: 2021-03-11 17:03:44+00:00
- **Authors**: Maximilian Durner, Wout Boerdijk, Martin Sundermeyer, Werner Friedl, Zoltan-Csaba Marton, Rudolph Triebel
- **Comment**: 8 pages, 5 figures, 6 tables, code will be made available
- **Journal**: None
- **Summary**: Although instance-aware perception is a key prerequisite for many autonomous robotic applications, most of the methods only partially solve the problem by focusing solely on known object categories. However, for robots interacting in dynamic and cluttered environments, this is not realistic and severely limits the range of potential applications. Therefore, we propose a novel object instance segmentation approach that does not require any semantic or geometric information of the objects beforehand. In contrast to existing works, we do not explicitly use depth data as input, but rely on the insight that slight viewpoint changes, which for example are provided by stereo image pairs, are often sufficient to determine object boundaries and thus to segment objects. Focusing on the versatility of stereo sensors, we employ a transformer-based architecture that maps directly from the pair of input images to the object instances. This has the major advantage that instead of a noisy, and potentially incomplete depth map as an input, on which the segmentation is computed, we use the original image pair to infer the object instances and a dense depth map. In experiments in several different application domains, we show that our Instance Stereo Transformer (INSTR) algorithm outperforms current state-of-the-art methods that are based on depth maps. Training code and pretrained models will be made available.



### Coming Down to Earth: Satellite-to-Street View Synthesis for Geo-Localization
- **Arxiv ID**: http://arxiv.org/abs/2103.06818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06818v1)
- **Published**: 2021-03-11 17:40:59+00:00
- **Updated**: 2021-03-11 17:40:59+00:00
- **Authors**: Aysim Toker, Qunjie Zhou, Maxim Maximov, Laura Leal-Taixé
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of cross-view image based geo-localization is to determine the location of a given street view image by matching it against a collection of geo-tagged satellite images. This task is notoriously challenging due to the drastic viewpoint and appearance differences between the two domains. We show that we can address this discrepancy explicitly by learning to synthesize realistic street views from satellite inputs. Following this observation, we propose a novel multi-task architecture in which image synthesis and retrieval are considered jointly. The rationale behind this is that we can bias our network to learn latent feature representations that are useful for retrieval if we utilize them to generate images across the two input domains. To the best of our knowledge, ours is the first approach that creates realistic street views from satellite images and localizes the corresponding query street-view simultaneously in an end-to-end manner. In our experiments, we obtain state-of-the-art performance on the CVUSA and CVACT benchmarks. Finally, we show compelling qualitative results for satellite-to-street view synthesis.



### Fast Hyperspectral Image Denoising and Inpainting Based on Low-Rank and Sparse Representations
- **Arxiv ID**: http://arxiv.org/abs/2103.06842v1
- **DOI**: 10.1109/JSTARS.2018.2796570
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.06842v1)
- **Published**: 2021-03-11 18:12:29+00:00
- **Updated**: 2021-03-11 18:12:29+00:00
- **Authors**: Lina Zhuang, Jose M. Bioucas-Dias
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces two very fast and competitive hyperspectral image (HSI) restoration algorithms: fast hyperspectral denoising (FastHyDe), a denoising algorithm able to cope with Gaussian and Poissonian noise, and fast hyperspectral inpainting (FastHyIn), an inpainting algorithm to restore HSIs where some observations from known pixels in some known bands are missing. FastHyDe and FastHyIn fully exploit extremely compact and sparse HSI representations linked with their low-rank and self-similarity characteristics. In a series of experiments with simulated and real data, the newly introduced FastHyDe and FastHyIn compete with the state-of-the-art methods, with much lower computational complexity.



### SMPLicit: Topology-aware Generative Model for Clothed People
- **Arxiv ID**: http://arxiv.org/abs/2103.06871v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06871v2)
- **Published**: 2021-03-11 18:57:03+00:00
- **Updated**: 2021-04-02 21:54:29+00:00
- **Authors**: Enric Corona, Albert Pumarola, Guillem Alenyà, Gerard Pons-Moll, Francesc Moreno-Noguer
- **Comment**: Accepted at CVPR 2021
- **Journal**: None
- **Summary**: In this paper we introduce SMPLicit, a novel generative model to jointly represent body pose, shape and clothing geometry. In contrast to existing learning-based approaches that require training specific models for each type of garment, SMPLicit can represent in a unified manner different garment topologies (e.g. from sleeveless tops to hoodies and to open jackets), while controlling other properties like the garment size or tightness/looseness. We show our model to be applicable to a large variety of garments including T-shirts, hoodies, jackets, shorts, pants, skirts, shoes and even hair. The representation flexibility of SMPLicit builds upon an implicit model conditioned with the SMPL human body parameters and a learnable latent space which is semantically interpretable and aligned with the clothing attributes. The proposed model is fully differentiable, allowing for its use into larger end-to-end trainable systems. In the experimental section, we demonstrate SMPLicit can be readily used for fitting 3D scans and for 3D reconstruction in images of dressed people. In both cases we are able to go beyond state of the art, by retrieving complex garment geometries, handling situations with multiple clothing layers and providing a tool for easy outfit editing. To stimulate further research in this direction, we will make our code and model publicly available at http://www.iri.upc.edu/people/ecorona/smplicit/.



### Fast and Accurate Model Scaling
- **Arxiv ID**: http://arxiv.org/abs/2103.06877v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.06877v1)
- **Published**: 2021-03-11 18:59:14+00:00
- **Updated**: 2021-03-11 18:59:14+00:00
- **Authors**: Piotr Dollár, Mannat Singh, Ross Girshick
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: In this work we analyze strategies for convolutional neural network scaling; that is, the process of scaling a base convolutional network to endow it with greater computational complexity and consequently representational power. Example scaling strategies may include increasing model width, depth, resolution, etc. While various scaling strategies exist, their tradeoffs are not fully understood. Existing analysis typically focuses on the interplay of accuracy and flops (floating point operations). Yet, as we demonstrate, various scaling strategies affect model parameters, activations, and consequently actual runtime quite differently. In our experiments we show the surprising result that numerous scaling strategies yield networks with similar accuracy but with widely varying properties. This leads us to propose a simple fast compound scaling strategy that encourages primarily scaling model width, while scaling depth and resolution to a lesser extent. Unlike currently popular scaling strategies, which result in about $O(s)$ increase in model activation w.r.t. scaling flops by a factor of $s$, the proposed fast compound scaling results in close to $O(\sqrt{s})$ increase in activations, while achieving excellent accuracy. This leads to comparable speedups on modern memory-limited hardware (e.g., GPU, TPU). More generally, we hope this work provides a framework for analyzing and selecting scaling strategies under various computational constraints.



### Diverse Semantic Image Synthesis via Probability Distribution Modeling
- **Arxiv ID**: http://arxiv.org/abs/2103.06878v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2103.06878v1)
- **Published**: 2021-03-11 18:59:25+00:00
- **Updated**: 2021-03-11 18:59:25+00:00
- **Authors**: Zhentao Tan, Menglei Chai, Dongdong Chen, Jing Liao, Qi Chu, Bin Liu, Gang Hua, Nenghai Yu
- **Comment**: Accepted By CVPR 2021
- **Journal**: None
- **Summary**: Semantic image synthesis, translating semantic layouts to photo-realistic images, is a one-to-many mapping problem. Though impressive progress has been recently made, diverse semantic synthesis that can efficiently produce semantic-level multimodal results, still remains a challenge. In this paper, we propose a novel diverse semantic image synthesis framework from the perspective of semantic class distributions, which naturally supports diverse generation at semantic or even instance level. We achieve this by modeling class-level conditional modulation parameters as continuous probability distributions instead of discrete values, and sampling per-instance modulation parameters through instance-adaptive stochastic sampling that is consistent across the network. Moreover, we propose prior noise remapping, through linear perturbation parameters encoded from paired references, to facilitate supervised training and exemplar-based instance style control at test time. Extensive experiments on multiple datasets show that our method can achieve superior diversity and comparable quality compared to state-of-the-art methods. Code will be available at \url{https://github.com/tzt101/INADE.git}



### CoMoGAN: continuous model-guided image-to-image translation
- **Arxiv ID**: http://arxiv.org/abs/2103.06879v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.06879v3)
- **Published**: 2021-03-11 18:59:50+00:00
- **Updated**: 2022-06-29 04:44:37+00:00
- **Authors**: Fabio Pizzati, Pietro Cerri, Raoul de Charette
- **Comment**: CVPR 2021 oral
- **Journal**: None
- **Summary**: CoMoGAN is a continuous GAN relying on the unsupervised reorganization of the target data on a functional manifold. To that matter, we introduce a new Functional Instance Normalization layer and residual mechanism, which together disentangle image content from position on target manifold. We rely on naive physics-inspired models to guide the training while allowing private model/translations features. CoMoGAN can be used with any GAN backbone and allows new types of image translation, such as cyclic image translation like timelapse generation, or detached linear translation. On all datasets, it outperforms the literature. Our code is available at http://github.com/cv-rits/CoMoGAN .



### HumanGAN: A Generative Model of Humans Images
- **Arxiv ID**: http://arxiv.org/abs/2103.06902v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06902v1)
- **Published**: 2021-03-11 19:00:38+00:00
- **Updated**: 2021-03-11 19:00:38+00:00
- **Authors**: Kripasindhu Sarkar, Lingjie Liu, Vladislav Golyanik, Christian Theobalt
- **Comment**: Accepted at CVPR21
- **Journal**: None
- **Summary**: Generative adversarial networks achieve great performance in photorealistic image synthesis in various domains, including human images. However, they usually employ latent vectors that encode the sampled outputs globally. This does not allow convenient control of semantically-relevant individual parts of the image, and is not able to draw samples that only differ in partial aspects, such as clothing style. We address these limitations and present a generative model for images of dressed humans offering control over pose, local body part appearance and garment style. This is the first method to solve various aspects of human image generation such as global appearance sampling, pose transfer, parts and garment transfer, and parts sampling jointly in a unified framework. As our model encodes part-based latent appearance vectors in a normalized pose-independent space and warps them to different poses, it preserves body and clothing appearance under varying posture. Experiments show that our flexible and general generative method outperforms task-specific baselines for pose-conditioned image generation, pose transfer and part sampling in terms of realism and output resolution.



### CORSAIR: Convolutional Object Retrieval and Symmetry-AIded Registration
- **Arxiv ID**: http://arxiv.org/abs/2103.06911v3
- **DOI**: 10.1109/IROS51168.2021.9636347
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.06911v3)
- **Published**: 2021-03-11 19:12:48+00:00
- **Updated**: 2021-09-04 22:55:55+00:00
- **Authors**: Tianyu Zhao, Qiaojun Feng, Sai Jadhav, Nikolay Atanasov
- **Comment**: 8 pages, 8 figures
- **Journal**: 2021 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), Prague, Czech Republic, 2021, pp. 47-54
- **Summary**: This paper considers online object-level mapping using partial point-cloud observations obtained online in an unknown environment. We develop and approach for fully Convolutional Object Retrieval and Symmetry-AIded Registration (CORSAIR). Our model extends the Fully Convolutional Geometric Features model to learn a global object-shape embedding in addition to local point-wise features from the point-cloud observations. The global feature is used to retrieve a similar object from a category database, and the local features are used for robust pose registration between the observed and the retrieved object. Our formulation also leverages symmetries, present in the object shapes, to obtain promising local-feature pairs from different symmetry classes for matching. We present results from synthetic and real-world datasets with different object categories to verify the robustness of our method.



### DefakeHop: A Light-Weight High-Performance Deepfake Detector
- **Arxiv ID**: http://arxiv.org/abs/2103.06929v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06929v1)
- **Published**: 2021-03-11 20:01:30+00:00
- **Updated**: 2021-03-11 20:01:30+00:00
- **Authors**: Hong-Shuo Chen, Mozhdeh Rouhsedaghat, Hamza Ghani, Shuowen Hu, Suya You, C. -C. Jay Kuo
- **Comment**: Accepted at ICME 2021
- **Journal**: None
- **Summary**: A light-weight high-performance Deepfake detection method, called DefakeHop, is proposed in this work. State-of-the-art Deepfake detection methods are built upon deep neural networks. DefakeHop extracts features automatically using the successive subspace learning (SSL) principle from various parts of face images. The features are extracted by c/w Saab transform and further processed by our feature distillation module using spatial dimension reduction and soft classification for each channel to get a more concise description of the face. Extensive experiments are conducted to demonstrate the effectiveness of the proposed DefakeHop method. With a small model size of 42,845 parameters, DefakeHop achieves state-of-the-art performance with the area under the ROC curve (AUC) of 100%, 94.95%, and 90.56% on UADFV, Celeb-DF v1 and Celeb-DF v2 datasets, respectively.



### The Semi-Supervised iNaturalist-Aves Challenge at FGVC7 Workshop
- **Arxiv ID**: http://arxiv.org/abs/2103.06937v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.06937v1)
- **Published**: 2021-03-11 20:21:16+00:00
- **Updated**: 2021-03-11 20:21:16+00:00
- **Authors**: Jong-Chyi Su, Subhransu Maji
- **Comment**: Tech report for Semi-iNat 2020 challenge, please see
  http://github.com/cvl-umass/semi-inat-2020
- **Journal**: None
- **Summary**: This document describes the details and the motivation behind a new dataset we collected for the semi-supervised recognition challenge~\cite{semi-aves} at the FGVC7 workshop at CVPR 2020. The dataset contains 1000 species of birds sampled from the iNat-2018 dataset for a total of nearly 150k images. From this collection, we sample a subset of classes and their labels, while adding the images from the remaining classes to the unlabeled set of images. The presence of out-of-domain data (novel classes), high class-imbalance, and fine-grained similarity between classes poses significant challenges for existing semi-supervised recognition techniques in the literature. The dataset is available here: \url{https://github.com/cvl-umass/semi-inat-2020}



### Efficient Pairwise Neuroimage Analysis using the Soft Jaccard Index and 3D Keypoint Sets
- **Arxiv ID**: http://arxiv.org/abs/2103.06966v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.06966v3)
- **Published**: 2021-03-11 21:41:59+00:00
- **Updated**: 2021-09-17 01:43:43+00:00
- **Authors**: Laurent Chauvin, Kuldeep Kumar, Christian Desrosiers, William Wells III, Matthew Toews
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: We propose a novel pairwise distance measure between image keypoint sets, for the purpose of large-scale medical image indexing. Our measure generalizes the Jaccard index to account for soft set equivalence (SSE) between keypoint elements, via an adaptive kernel framework modeling uncertainty in keypoint appearance and geometry. A new kernel is proposed to quantify the variability of keypoint geometry in location and scale. Our distance measure may be estimated between $O(N^2)$ image pairs in $O(N~\log~N)$ operations via keypoint indexing. Experiments report the first results for the task of predicting family relationships from medical images, using 1010 T1-weighted MRI brain volumes of 434 families including monozygotic and dizygotic twins, siblings and half-siblings sharing 100%-25% of their polymorphic genes. Soft set equivalence and the keypoint geometry kernel improve upon standard hard set equivalence (HSE) and appearance kernels alone in predicting family relationships. Monozygotic twin identification is near 100%, and three subjects with uncertain genotyping are automatically paired with their self-reported families, the first reported practical application of image-based family identification. Our distance measure can also be used to predict group categories, sex is predicted with an AUC=0.97. Software is provided for efficient fine-grained curation of large, generic image datasets.



### Continuous 3D Multi-Channel Sign Language Production via Progressive Transformers and Mixture Density Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.06982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.06982v1)
- **Published**: 2021-03-11 22:11:17+00:00
- **Updated**: 2021-03-11 22:11:17+00:00
- **Authors**: Ben Saunders, Necati Cihan Camgoz, Richard Bowden
- **Comment**: None
- **Journal**: None
- **Summary**: Sign languages are multi-channel visual languages, where signers use a continuous 3D space to communicate.Sign Language Production (SLP), the automatic translation from spoken to sign languages, must embody both the continuous articulation and full morphology of sign to be truly understandable by the Deaf community. Previous deep learning-based SLP works have produced only a concatenation of isolated signs focusing primarily on the manual features, leading to a robotic and non-expressive production.   In this work, we propose a novel Progressive Transformer architecture, the first SLP model to translate from spoken language sentences to continuous 3D multi-channel sign pose sequences in an end-to-end manner. Our transformer network architecture introduces a counter decoding that enables variable length continuous sequence generation by tracking the production progress over time and predicting the end of sequence. We present extensive data augmentation techniques to reduce prediction drift, alongside an adversarial training regime and a Mixture Density Network (MDN) formulation to produce realistic and expressive sign pose sequences.   We propose a back translation evaluation mechanism for SLP, presenting benchmark quantitative results on the challenging PHOENIX14T dataset and setting baselines for future research. We further provide a user evaluation of our SLP model, to understand the Deaf reception of our sign pose productions.



### The Location of Optimal Object Colors with More Than Two Transitions (Preprint)
- **Arxiv ID**: http://arxiv.org/abs/2103.06997v7
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.06997v7)
- **Published**: 2021-03-11 23:14:01+00:00
- **Updated**: 2021-05-14 15:57:48+00:00
- **Authors**: Scott A. Burns
- **Comment**: 5/14/21 version adds notice of acceptance for publication and changes
  made in final version
- **Journal**: None
- **Summary**: The chromaticity diagram associated with the CIE 1931 color matching functions is shown to be slightly non-convex. While having no impact on practical colorimetric computations, the non-convexity does have a significant impact on the shape of some optimal object color reflectance distributions associated with the outer surface of the object color solid. Instead of the usual two-transition Schrodinger form, many optimal colors exhibit higher transition counts. A linear programming formulation is developed and is used to locate where these higher-transition optimal object colors reside on the object color solid surface. The regions of higher transition count appear to have a point-symmetric complementary structure. The final peer-reviewed version (to appear) contains additional material concerning convexification of the color-matching functions and and additional analysis of modern "physiologically-relevant" CMFs transformed from cone fundamentals.



### An Efficient Hypergraph Approach to Robust Point Cloud Resampling
- **Arxiv ID**: http://arxiv.org/abs/2103.06999v1
- **DOI**: 10.1109/TIP.2022.3149225
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2103.06999v1)
- **Published**: 2021-03-11 23:19:54+00:00
- **Updated**: 2021-03-11 23:19:54+00:00
- **Authors**: Qinwen Deng, Songyang Zhang, Zhi Ding
- **Comment**: None
- **Journal**: None
- **Summary**: Efficient processing and feature extraction of largescale point clouds are important in related computer vision and cyber-physical systems. This work investigates point cloud resampling based on hypergraph signal processing (HGSP) to better explore the underlying relationship among different cloud points and to extract contour-enhanced features. Specifically, we design hypergraph spectral filters to capture multi-lateral interactions among the signal nodes of point clouds and to better preserve their surface outlines. Without the need and the computation to first construct the underlying hypergraph, our low complexity approach directly estimates hypergraph spectrum of point clouds by leveraging hypergraph stationary processes from the observed 3D coordinates. Evaluating the proposed resampling methods with several metrics, our test results validate the high efficacy of hypergraph characterization of point clouds and demonstrate the robustness of hypergraph-based resampling under noisy observations.



### Personalizing image enhancement for critical visual tasks: improved legibility of papyri using color processing and visual illusions
- **Arxiv ID**: http://arxiv.org/abs/2104.01106v2
- **DOI**: 10.1007/s10032-021-00386-0
- **Categories**: **cs.CV**, cs.DL, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2104.01106v2)
- **Published**: 2021-03-11 23:48:17+00:00
- **Updated**: 2021-08-30 21:28:00+00:00
- **Authors**: Vlad Atanasiu, Isabelle Marthot-Santaniello
- **Comment**: Article accepted for publication by the International Journal on
  Document Analysis and Recognition (IJDAR) on 2021.08.27. Open Source software
  accessible at https://hierax.ch. Comments to version 2: Extendend Sections
  3.2 Machine learning, 5.3.5 Comparisons and 6 Paradim; added supplemental
  material; other improvements throughout the article
- **Journal**: nternational Journal on Document Analysis and Recognition (IJDAR)
  (2021)
- **Summary**: Purpose: This article develops theoretical, algorithmic, perceptual, and interaction aspects of script legibility enhancement in the visible light spectrum for the purpose of scholarly editing of papyri texts. - Methods: Novel legibility enhancement algorithms based on color processing and visual illusions are compared to classic methods in a user experience experiment. - Results: (1) The proposed methods outperformed the comparison methods. (2) Users exhibited a broad behavioral spectrum, under the influence of factors such as personality and social conditioning, tasks and application domains, expertise level and image quality, and affordances of software, hardware, and interfaces. No single enhancement method satisfied all factor configurations. Therefore, it is suggested to offer users a broad choice of methods to facilitate personalization, contextualization, and complementarity. (3) A distinction is made between casual and critical vision on the basis of signal ambiguity and error consequences. The criteria of a paradigm for enhancing images for critical applications comprise: interpreting images skeptically; approaching enhancement as a system problem; considering all image structures as potential information; and making uncertainty and alternative interpretations explicit, both visually and numerically.



