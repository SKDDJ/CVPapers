# Arxiv Papers in cs.CV on 2021-03-31
### On the Robustness of Vision Transformers to Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/2104.02610v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.02610v2)
- **Published**: 2021-03-31 00:29:12+00:00
- **Updated**: 2021-06-05 00:31:29+00:00
- **Authors**: Kaleel Mahmood, Rigel Mahmood, Marten van Dijk
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in attention-based networks have shown that Vision Transformers can achieve state-of-the-art or near state-of-the-art results on many image classification tasks. This puts transformers in the unique position of being a promising alternative to traditional convolutional neural networks (CNNs). While CNNs have been carefully studied with respect to adversarial attacks, the same cannot be said of Vision Transformers. In this paper, we study the robustness of Vision Transformers to adversarial examples. Our analyses of transformer security is divided into three parts. First, we test the transformer under standard white-box and black-box attacks. Second, we study the transferability of adversarial examples between CNNs and transformers. We show that adversarial examples do not readily transfer between CNNs and transformers. Based on this finding, we analyze the security of a simple ensemble defense of CNNs and transformers. By creating a new attack, the self-attention blended gradient attack, we show that such an ensemble is not secure under a white-box adversary. However, under a black-box adversary, we show that an ensemble can achieve unprecedented robustness without sacrificing clean accuracy. Our analysis for this work is done using six types of white-box attacks and two types of black-box attacks. Our study encompasses multiple Vision Transformers, Big Transfer Models and CNN architectures trained on CIFAR-10, CIFAR-100 and ImageNet.



### Deep Simultaneous Optimisation of Sampling and Reconstruction for Multi-contrast MRI
- **Arxiv ID**: http://arxiv.org/abs/2103.16744v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.16744v1)
- **Published**: 2021-03-31 00:45:58+00:00
- **Updated**: 2021-03-31 00:45:58+00:00
- **Authors**: Xinwen Liu, Jing Wang, Fangfang Tang, Shekhar S. Chandra, Feng Liu, Stuart Crozier
- **Comment**: Presented at ISMRM 28th Annual Meeting & Exhibition (Poster #3619)
- **Journal**: None
- **Summary**: MRI images of the same subject in different contrasts contain shared information, such as the anatomical structure. Utilizing the redundant information amongst the contrasts to sub-sample and faithfully reconstruct multi-contrast images could greatly accelerate the imaging speed, improve image quality and shorten scanning protocols. We propose an algorithm that generates the optimised sampling pattern and reconstruction scheme of one contrast (e.g. T2-weighted image) when images with different contrast (e.g. T1-weighted image) have been acquired. The proposed algorithm achieves increased PSNR and SSIM with the resulting optimal sampling pattern compared to other acquisition patterns and single contrast methods.



### Towards More Flexible and Accurate Object Tracking with Natural Language: Algorithms and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2103.16746v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.16746v1)
- **Published**: 2021-03-31 00:57:32+00:00
- **Updated**: 2021-03-31 00:57:32+00:00
- **Authors**: Xiao Wang, Xiujun Shu, Zhipeng Zhang, Bo Jiang, Yaowei Wang, Yonghong Tian, Feng Wu
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Tracking by natural language specification is a new rising research topic that aims at locating the target object in the video sequence based on its language description. Compared with traditional bounding box (BBox) based tracking, this setting guides object tracking with high-level semantic information, addresses the ambiguity of BBox, and links local and global search organically together. Those benefits may bring more flexible, robust and accurate tracking performance in practical scenarios. However, existing natural language initialized trackers are developed and compared on benchmark datasets proposed for tracking-by-BBox, which can't reflect the true power of tracking-by-language. In this work, we propose a new benchmark specifically dedicated to the tracking-by-language, including a large scale dataset, strong and diverse baseline methods. Specifically, we collect 2k video sequences (contains a total of 1,244,340 frames, 663 words) and split 1300/700 for the train/testing respectively. We densely annotate one sentence in English and corresponding bounding boxes of the target object for each video. We also introduce two new challenges into TNL2K for the object tracking task, i.e., adversarial samples and modality switch. A strong baseline method based on an adaptive local-global-search scheme is proposed for future works to compare. We believe this benchmark will greatly boost related researches on natural language guided tracking.



### Dual Contrastive Loss and Attention for GANs
- **Arxiv ID**: http://arxiv.org/abs/2103.16748v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2103.16748v3)
- **Published**: 2021-03-31 01:10:26+00:00
- **Updated**: 2022-03-17 20:59:31+00:00
- **Authors**: Ning Yu, Guilin Liu, Aysegul Dundar, Andrew Tao, Bryan Catanzaro, Larry Davis, Mario Fritz
- **Comment**: Accepted to ICCV'21
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) produce impressive results on unconditional image generation when powered with large-scale image datasets. Yet generated images are still easy to spot especially on datasets with high variance (e.g. bedroom, church). In this paper, we propose various improvements to further push the boundaries in image generation. Specifically, we propose a novel dual contrastive loss and show that, with this loss, discriminator learns more generalized and distinguishable representations to incentivize generation. In addition, we revisit attention and extensively experiment with different attention blocks in the generator. We find attention to be still an important module for successful image generation even though it was not used in the recent state-of-the-art models. Lastly, we study different attention architectures in the discriminator, and propose a reference attention mechanism. By combining the strengths of these remedies, we improve the compelling state-of-the-art Fr\'{e}chet Inception Distance (FID) by at least 17.5% on several benchmark datasets. We obtain even more significant improvements on compositional synthetic scenes (up to 47.5% in FID). Code and models are available at https://github.com/ningyu1991/AttentionDualContrastGAN .



### Evaluation of Multimodal Semantic Segmentation using RGB-D Data
- **Arxiv ID**: http://arxiv.org/abs/2103.16758v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.16758v1)
- **Published**: 2021-03-31 01:43:43+00:00
- **Updated**: 2021-03-31 01:43:43+00:00
- **Authors**: Jiesi Hu, Ganning Zhao, Suya You, C. C. Jay Kuo
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: Our goal is to develop stable, accurate, and robust semantic scene understanding methods for wide-area scene perception and understanding, especially in challenging outdoor environments. To achieve this, we are exploring and evaluating a range of related technology and solutions, including AI-driven multimodal scene perception, fusion, processing, and understanding. This work reports our efforts on the evaluation of a state-of-the-art approach for semantic segmentation with multiple RGB and depth sensing data. We employ four large datasets composed of diverse urban and terrain scenes and design various experimental methods and metrics. In addition, we also develop new strategies of multi-datasets learning to improve the detection and recognition of unseen objects. Extensive experiments, implementations, and results are reported in the paper.



### Facial Masks and Soft-Biometrics: Leveraging Face Recognition CNNs for Age and Gender Prediction on Mobile Ocular Images
- **Arxiv ID**: http://arxiv.org/abs/2103.16760v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.16760v2)
- **Published**: 2021-03-31 01:48:29+00:00
- **Updated**: 2021-05-04 20:05:40+00:00
- **Authors**: Fernando Alonso-Fernandez, Kevin Hernandez Diaz, Silvia Ramis, Francisco J. Perales, Josef Bigun
- **Comment**: Accepted for publication at IET Biometrics
- **Journal**: None
- **Summary**: We address the use of selfie ocular images captured with smartphones to estimate age and gender. Partial face occlusion has become an issue due to the mandatory use of face masks. Also, the use of mobile devices has exploded, with the pandemic further accelerating the migration to digital services. However, state-of-the-art solutions in related tasks such as identity or expression recognition employ large Convolutional Neural Networks, whose use in mobile devices is infeasible due to hardware limitations and size restrictions of downloadable applications. To counteract this, we adapt two existing lightweight CNNs proposed in the context of the ImageNet Challenge, and two additional architectures proposed for mobile face recognition. Since datasets for soft-biometrics prediction using selfie images are limited, we counteract over-fitting by using networks pre-trained on ImageNet. Furthermore, some networks are further pre-trained for face recognition, for which very large training databases are available. Since both tasks employ similar input data, we hypothesize that such strategy can be beneficial for soft-biometrics estimation. A comprehensive study of the effects of different pre-training over the employed architectures is carried out, showing that, in most cases, a better accuracy is obtained after the networks have been fine-tuned for face recognition.



### Weakly-Supervised Image Semantic Segmentation Using Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.16762v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.16762v2)
- **Published**: 2021-03-31 02:05:01+00:00
- **Updated**: 2021-04-01 11:42:30+00:00
- **Authors**: Shun-Yi Pan, Cheng-You Lu, Shih-Po Lee, Wen-Hsiao Peng
- **Comment**: Accepted by ICME 2021
- **Journal**: None
- **Summary**: This work addresses weakly-supervised image semantic segmentation based on image-level class labels. One common approach to this task is to propagate the activation scores of Class Activation Maps (CAMs) using a random-walk mechanism in order to arrive at complete pseudo labels for training a semantic segmentation network in a fully-supervised manner. However, the feed-forward nature of the random walk imposes no regularization on the quality of the resulting complete pseudo labels. To overcome this issue, we propose a Graph Convolutional Network (GCN)-based feature propagation framework. We formulate the generation of complete pseudo labels as a semi-supervised learning task and learn a 2-layer GCN separately for every training image by back-propagating a Laplacian and an entropy regularization loss. Experimental results on the PASCAL VOC 2012 dataset confirm the superiority of our scheme to several state-of-the-art baselines. Our code is available at https://github.com/Xavier-Pan/WSGCN.



### Prototypical Cross-domain Self-supervised Learning for Few-shot Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2103.16765v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.16765v1)
- **Published**: 2021-03-31 02:07:42+00:00
- **Updated**: 2021-03-31 02:07:42+00:00
- **Authors**: Xiangyu Yue, Zangwei Zheng, Shanghang Zhang, Yang Gao, Trevor Darrell, Kurt Keutzer, Alberto Sangiovanni Vincentelli
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (UDA) transfers predictive models from a fully-labeled source domain to an unlabeled target domain. In some applications, however, it is expensive even to collect labels in the source domain, making most previous works impractical. To cope with this problem, recent work performed instance-wise cross-domain self-supervised learning, followed by an additional fine-tuning stage. However, the instance-wise self-supervised learning only learns and aligns low-level discriminative features. In this paper, we propose an end-to-end Prototypical Cross-domain Self-Supervised Learning (PCS) framework for Few-shot Unsupervised Domain Adaptation (FUDA). PCS not only performs cross-domain low-level feature alignment, but it also encodes and aligns semantic structures in the shared embedding space across domains. Our framework captures category-wise semantic structures of the data by in-domain prototypical contrastive learning; and performs feature alignment through cross-domain prototypical self-supervision. Compared with state-of-the-art methods, PCS improves the mean classification accuracy over different domain pairs on FUDA by 10.5%, 3.5%, 9.0%, and 13.2% on Office, Office-Home, VisDA-2017, and DomainNet, respectively. Our project page is at http://xyue.io/pcs-fuda/index.html



### Topology-Preserving 3D Image Segmentation Based On Hyperelastic Regularization
- **Arxiv ID**: http://arxiv.org/abs/2103.16768v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.16768v1)
- **Published**: 2021-03-31 02:20:46+00:00
- **Updated**: 2021-03-31 02:20:46+00:00
- **Authors**: Daoping Zhang, Lok Ming Lui
- **Comment**: 27 pages
- **Journal**: None
- **Summary**: Image segmentation is to extract meaningful objects from a given image. For degraded images due to occlusions, obscurities or noises, the accuracy of the segmentation result can be severely affected. To alleviate this problem, prior information about the target object is usually introduced. In [10], a topology-preserving registration-based segmentation model was proposed, which is restricted to segment 2D images only. In this paper, we propose a novel 3D topology-preserving registration-based segmentation model with the hyperelastic regularization, which can handle both 2D and 3D images. The existence of the solution of the proposed model is established. We also propose a converging iterative scheme to solve the proposed model. Numerical experiments have been carried out on the synthetic and real images, which demonstrate the effectiveness of our proposed model.



### PAUL: Procrustean Autoencoder for Unsupervised Lifting
- **Arxiv ID**: http://arxiv.org/abs/2103.16773v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.16773v1)
- **Published**: 2021-03-31 02:31:01+00:00
- **Updated**: 2021-03-31 02:31:01+00:00
- **Authors**: Chaoyang Wang, Simon Lucey
- **Comment**: None
- **Journal**: None
- **Summary**: Recent success in casting Non-rigid Structure from Motion (NRSfM) as an unsupervised deep learning problem has raised fundamental questions about what novelty in NRSfM prior could the deep learning offer. In this paper we advocate for a 3D deep auto-encoder framework to be used explicitly as the NRSfM prior. The framework is unique as: (i) it learns the 3D auto-encoder weights solely from 2D projected measurements, and (ii) it is Procrustean in that it jointly resolves the unknown rigid pose for each shape instance. We refer to this architecture as a Procustean Autoencoder for Unsupervised Lifting (PAUL), and demonstrate state-of-the-art performance across a number of benchmarks in comparison to recent innovations such as Deep NRSfM and C3PDO.



### Attention, please! A survey of Neural Attention Models in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.16775v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.16775v1)
- **Published**: 2021-03-31 02:42:28+00:00
- **Updated**: 2021-03-31 02:42:28+00:00
- **Authors**: Alana de Santana Correia, Esther Luna Colombini
- **Comment**: 66 pages, 24 figures
- **Journal**: None
- **Summary**: In humans, Attention is a core property of all perceptual and cognitive operations. Given our limited ability to process competing sources, attention mechanisms select, modulate, and focus on the information most relevant to behavior. For decades, concepts and functions of attention have been studied in philosophy, psychology, neuroscience, and computing. For the last six years, this property has been widely explored in deep neural networks. Currently, the state-of-the-art in Deep Learning is represented by neural attention models in several application domains. This survey provides a comprehensive overview and analysis of developments in neural attention models. We systematically reviewed hundreds of architectures in the area, identifying and discussing those in which attention has shown a significant impact. We also developed and made public an automated methodology to facilitate the development of reviews in the area. By critically analyzing 650 works, we describe the primary uses of attention in convolutional, recurrent networks and generative models, identifying common subgroups of uses and applications. Furthermore, we describe the impact of attention in different application domains and their impact on neural networks' interpretability. Finally, we list possible trends and opportunities for further research, hoping that this review will provide a succinct overview of the main attentional models in the area and guide researchers in developing future approaches that will drive further improvements.



### PointShuffleNet: Learning Non-Euclidean Features with Homotopy Equivalence and Mutual Information
- **Arxiv ID**: http://arxiv.org/abs/2104.02611v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2104.02611v1)
- **Published**: 2021-03-31 03:01:16+00:00
- **Updated**: 2021-03-31 03:01:16+00:00
- **Authors**: Linchao He, Mengting Luo, Dejun Zhang, Xiao Yang, Hu Chen, Yi Zhang
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Point cloud analysis is still a challenging task due to the disorder and sparsity of samplings of their geometric structures from 3D sensors. In this paper, we introduce the homotopy equivalence relation (HER) to make the neural networks learn the data distribution from a high-dimension manifold. A shuffle operation is adopted to construct HER for its randomness and zero-parameter. In addition, inspired by prior works, we propose a local mutual information regularizer (LMIR) to cut off the trivial path that leads to a classification error from HER. LMIR utilizes mutual information to measure the distance between the original feature and HER transformed feature and learns common features in a contrastive learning scheme. Thus, we combine HER and LMIR to give our model the ability to learn non-Euclidean features from a high-dimension manifold. This is named the non-Euclidean feature learner. Furthermore, we propose a new heuristics and efficiency point sampling algorithm named ClusterFPS to obtain approximate uniform sampling but at faster speed. ClusterFPS uses a cluster algorithm to divide a point cloud into several clusters and deploy the farthest point sampling algorithm on each cluster in parallel. By combining the above methods, we propose a novel point cloud analysis neural network called PointShuffleNet (PSN), which shows great promise in point cloud classification and segmentation. Extensive experiments show that our PSN achieves state-of-the-art results on ModelNet40, ShapeNet and S3DIS with high efficiency. Theoretically, we provide mathematical analysis toward understanding of what the data distribution HER has developed and why LMIR can drop the trivial path by maximizing mutual information implicitly.



### DER: Dynamically Expandable Representation for Class Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.16788v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.16788v1)
- **Published**: 2021-03-31 03:16:44+00:00
- **Updated**: 2021-03-31 03:16:44+00:00
- **Authors**: Shipeng Yan, Jiangwei Xie, Xuming He
- **Comment**: Accepted as an oral of CVPR2021
- **Journal**: None
- **Summary**: We address the problem of class incremental learning, which is a core step towards achieving adaptive vision intelligence. In particular, we consider the task setting of incremental learning with limited memory and aim to achieve better stability-plasticity trade-off. To this end, we propose a novel two-stage learning approach that utilizes a dynamically expandable representation for more effective incremental concept modeling. Specifically, at each incremental step, we freeze the previously learned representation and augment it with additional feature dimensions from a new learnable feature extractor. This enables us to integrate new visual concepts with retaining learned knowledge. We dynamically expand the representation according to the complexity of novel concepts by introducing a channel-level mask-based pruning strategy. Moreover, we introduce an auxiliary loss to encourage the model to learn diverse and discriminate features for novel concepts. We conduct extensive experiments on the three class incremental learning benchmarks and our method consistently outperforms other methods with a large margin.



### Learning Camera Localization via Dense Scene Matching
- **Arxiv ID**: http://arxiv.org/abs/2103.16792v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.16792v1)
- **Published**: 2021-03-31 03:47:42+00:00
- **Updated**: 2021-03-31 03:47:42+00:00
- **Authors**: Shitao Tang, Chengzhou Tang, Rui Huang, Siyu Zhu, Ping Tan
- **Comment**: CVPR2021
- **Journal**: None
- **Summary**: Camera localization aims to estimate 6 DoF camera poses from RGB images. Traditional methods detect and match interest points between a query image and a pre-built 3D model. Recent learning-based approaches encode scene structures into a specific convolutional neural network (CNN) and thus are able to predict dense coordinates from RGB images. However, most of them require re-training or re-adaption for a new scene and have difficulties in handling large-scale scenes due to limited network capacity. We present a new method for scene agnostic camera localization using dense scene matching (DSM), where a cost volume is constructed between a query image and a scene. The cost volume and the corresponding coordinates are processed by a CNN to predict dense coordinates. Camera poses can then be solved by PnP algorithms. In addition, our method can be extended to temporal domain, which leads to extra performance boost during testing time. Our scene-agnostic approach achieves comparable accuracy as the existing scene-specific approaches, such as KFNet, on the 7scenes and Cambridge benchmark. This approach also remarkably outperforms state-of-the-art scene-agnostic dense coordinate regression network SANet. The Code is available at https://github.com/Tangshitao/Dense-Scene-Matching.



### Multi-Class Multi-Instance Count Conditioned Adversarial Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2103.16795v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.16795v1)
- **Published**: 2021-03-31 04:06:11+00:00
- **Updated**: 2021-03-31 04:06:11+00:00
- **Authors**: Amrutha Saseendran, Kathrin Skubch, Margret Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: Image generation has rapidly evolved in recent years. Modern architectures for adversarial training allow to generate even high resolution images with remarkable quality. At the same time, more and more effort is dedicated towards controlling the content of generated images. In this paper, we take one further step in this direction and propose a conditional generative adversarial network (GAN) that generates images with a defined number of objects from given classes. This entails two fundamental abilities (1) being able to generate high-quality images given a complex constraint and (2) being able to count object instances per class in a given image. Our proposed model modularly extends the successful StyleGAN2 architecture with a count-based conditioning as well as with a regression sub-network to count the number of generated objects per class during training. In experiments on three different datasets, we show that the proposed model learns to generate images according to the given multiple-class count condition even in the presence of complex backgrounds. In particular, we propose a new dataset, CityCount, which is derived from the Cityscapes street scenes dataset, to evaluate our approach in a challenging and practically relevant scenario.



### OutlierNets: Highly Compact Deep Autoencoder Network Architectures for On-Device Acoustic Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2104.00528v2
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2104.00528v2)
- **Published**: 2021-03-31 04:09:30+00:00
- **Updated**: 2021-04-19 03:25:50+00:00
- **Authors**: Saad Abbasi, Mahmoud Famouri, Mohammad Javad Shafiee, Alexander Wong
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Human operators often diagnose industrial machinery via anomalous sounds. Automated acoustic anomaly detection can lead to reliable maintenance of machinery. However, deep learning-driven anomaly detection methods often require an extensive amount of computational resources which prohibits their deployment in factories. Here we explore a machine-driven design exploration strategy to create OutlierNets, a family of highly compact deep convolutional autoencoder network architectures featuring as few as 686 parameters, model sizes as small as 2.7 KB, and as low as 2.8 million FLOPs, with a detection accuracy matching or exceeding published architectures with as many as 4 million parameters. Furthermore, CPU-accelerated latency experiments show that the OutlierNet architectures can achieve as much as 21x lower latency than published networks.



### Self-Regression Learning for Blind Hyperspectral Image Fusion Without Label
- **Arxiv ID**: http://arxiv.org/abs/2103.16806v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.16806v1)
- **Published**: 2021-03-31 04:48:21+00:00
- **Updated**: 2021-03-31 04:48:21+00:00
- **Authors**: Wu Wang, Yue Huang, Xinhao Ding
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral image fusion (HIF) is critical to a wide range of applications in remote sensing and many computer vision applications. Most traditional HIF methods assume that the observation model is predefined or known. However, in real applications, the observation model involved are often complicated and unknown, which leads to the serious performance drop of many advanced HIF methods. Also, deep learning methods can achieve outstanding performance, but they generally require a large number of image pairs for model training, which are difficult to obtain in realistic scenarios. Towards these issues, we proposed a self-regression learning method that alternatively reconstructs hyperspectral image (HSI) and estimate the observation model. In particular, we adopt an invertible neural network (INN) for restoring the HSI, and two fully-connected network (FCN) for estimating the observation model. Moreover, \emph{SoftMax} nonlinearity is applied to the FCN for satisfying the non-negative, sparsity and equality constraints. Besides, we proposed a local consistency loss function to constrain the observation model by exploring domain specific knowledge. Finally, we proposed an angular loss function to improve spectral reconstruction accuracy. Extensive experiments on both synthetic and real-world dataset show that our model can outperform the state-of-the-art methods



### Learning Generalizable Robotic Reward Functions from "In-The-Wild" Human Videos
- **Arxiv ID**: http://arxiv.org/abs/2103.16817v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.16817v1)
- **Published**: 2021-03-31 05:25:05+00:00
- **Updated**: 2021-03-31 05:25:05+00:00
- **Authors**: Annie S. Chen, Suraj Nair, Chelsea Finn
- **Comment**: https://sites.google.com/view/dvd-human-videos
- **Journal**: None
- **Summary**: We are motivated by the goal of generalist robots that can complete a wide range of tasks across many environments. Critical to this is the robot's ability to acquire some metric of task success or reward, which is necessary for reinforcement learning, planning, or knowing when to ask for help. For a general-purpose robot operating in the real world, this reward function must also be able to generalize broadly across environments, tasks, and objects, while depending only on on-board sensor observations (e.g. RGB images). While deep learning on large and diverse datasets has shown promise as a path towards such generalization in computer vision and natural language, collecting high quality datasets of robotic interaction at scale remains an open challenge. In contrast, "in-the-wild" videos of humans (e.g. YouTube) contain an extensive collection of people doing interesting tasks across a diverse range of settings. In this work, we propose a simple approach, Domain-agnostic Video Discriminator (DVD), that learns multitask reward functions by training a discriminator to classify whether two videos are performing the same task, and can generalize by virtue of learning from a small amount of robot data with a broad dataset of human videos. We find that by leveraging diverse human datasets, this reward function (a) can generalize zero shot to unseen environments, (b) generalize zero shot to unseen tasks, and (c) can be combined with visual model predictive control to solve robotic manipulation tasks on a real WidowX200 robot in an unseen environment from a single human demo.



### Spatial Content Alignment For Pose Transfer
- **Arxiv ID**: http://arxiv.org/abs/2103.16828v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2103.16828v1)
- **Published**: 2021-03-31 06:10:29+00:00
- **Updated**: 2021-03-31 06:10:29+00:00
- **Authors**: Wing-Yin Yu, Lai-Man Po, Yuzhi Zhao, Jingjing Xiong, Kin-Wai Lau
- **Comment**: IEEE International Conference on Multimedia and Expo (ICME) 2021 Oral
- **Journal**: None
- **Summary**: Due to unreliable geometric matching and content misalignment, most conventional pose transfer algorithms fail to generate fine-trained person images. In this paper, we propose a novel framework Spatial Content Alignment GAN (SCAGAN) which aims to enhance the content consistency of garment textures and the details of human characteristics. We first alleviate the spatial misalignment by transferring the edge content to the target pose in advance. Secondly, we introduce a new Content-Style DeBlk which can progressively synthesize photo-realistic person images based on the appearance features of the source image, the target pose heatmap and the prior transferred content in edge domain. We compare the proposed framework with several state-of-the-art methods to show its superiority in quantitative and qualitative analysis. Moreover, detailed ablation study results demonstrate the efficacy of our contributions. Codes are publicly available at github.com/rocketappslab/SCA-GAN.



### Convolutional Hough Matching Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.16831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.16831v1)
- **Published**: 2021-03-31 06:17:03+00:00
- **Updated**: 2021-03-31 06:17:03+00:00
- **Authors**: Juhong Min, Minsu Cho
- **Comment**: Accepted to CVPR 2021 (oral presentation)
- **Journal**: None
- **Summary**: Despite advances in feature representation, leveraging geometric relations is crucial for establishing reliable visual correspondences under large variations of images. In this work we introduce a Hough transform perspective on convolutional matching and propose an effective geometric matching algorithm, dubbed Convolutional Hough Matching (CHM). The method distributes similarities of candidate matches over a geometric transformation space and evaluate them in a convolutional manner. We cast it into a trainable neural layer with a semi-isotropic high-dimensional kernel, which learns non-rigid matching with a small number of interpretable parameters. To validate the effect, we develop the neural network with CHM layers that perform convolutional matching in the space of translation and scaling. Our method sets a new state of the art on standard benchmarks for semantic visual correspondence, proving its strong robustness to challenging intra-class variations.



### Online Learning of a Probabilistic and Adaptive Scene Representation
- **Arxiv ID**: http://arxiv.org/abs/2103.16832v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.16832v1)
- **Published**: 2021-03-31 06:22:05+00:00
- **Updated**: 2021-03-31 06:22:05+00:00
- **Authors**: Zike Yan, Xin Wang, Hongbin Zha
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Constructing and maintaining a consistent scene model on-the-fly is the core task for online spatial perception, interpretation, and action. In this paper, we represent the scene with a Bayesian nonparametric mixture model, seamlessly describing per-point occupancy status with a continuous probability density function. Instead of following the conventional data fusion paradigm, we address the problem of online learning the process how sequential point cloud data are generated from the scene geometry. An incremental and parallel inference is performed to update the parameter space in real-time. We experimentally show that the proposed representation achieves state-of-the-art accuracy with promising efficiency. The consistent probabilistic formulation assures a generative model that is adaptive to different sensor characteristics, and the model complexity can be dynamically adjusted on-the-fly according to different data scales.



### ReMix: Towards Image-to-Image Translation with Limited Data
- **Arxiv ID**: http://arxiv.org/abs/2103.16835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.16835v1)
- **Published**: 2021-03-31 06:24:10+00:00
- **Updated**: 2021-03-31 06:24:10+00:00
- **Authors**: Jie Cao, Luanxuan Hou, Ming-Hsuan Yang, Ran He, Zhenan Sun
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Image-to-image (I2I) translation methods based on generative adversarial networks (GANs) typically suffer from overfitting when limited training data is available. In this work, we propose a data augmentation method (ReMix) to tackle this issue. We interpolate training samples at the feature level and propose a novel content loss based on the perceptual relations among samples. The generator learns to translate the in-between samples rather than memorizing the training set, and thereby forces the discriminator to generalize. The proposed approach effectively reduces the ambiguity of generation and renders content-preserving results. The ReMix method can be easily incorporated into existing GAN models with minor modifications. Experimental results on numerous tasks demonstrate that GAN models equipped with the ReMix method achieve significant improvements.



### Channel-Based Attention for LCC Using Sentinel-2 Time Series
- **Arxiv ID**: http://arxiv.org/abs/2103.16836v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.16836v1)
- **Published**: 2021-03-31 06:24:15+00:00
- **Updated**: 2021-03-31 06:24:15+00:00
- **Authors**: Hermann Courteille, A. Benoît, N Méger, A Atto, D. Ienco
- **Comment**: None
- **Journal**: International Geoscience and Remote Sensing Symposium (IGARSS),
  Jul 2021, Brussels, Belgium
- **Summary**: Deep Neural Networks (DNNs) are getting increasing attention to deal with Land Cover Classification (LCC) relying on Satellite Image Time Series (SITS). Though high performances can be achieved, the rationale of a prediction yielded by a DNN often remains unclear. An architecture expressing predictions with respect to input channels is thus proposed in this paper. It relies on convolutional layers and an attention mechanism weighting the importance of each channel in the final classification decision. The correlation between channels is taken into account to set up shared kernels and lower model complexity. Experiments based on a Sentinel-2 SITS show promising results.



### Fixing the Teacher-Student Knowledge Discrepancy in Distillation
- **Arxiv ID**: http://arxiv.org/abs/2103.16844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.16844v1)
- **Published**: 2021-03-31 06:52:20+00:00
- **Updated**: 2021-03-31 06:52:20+00:00
- **Authors**: Jiangfan Han, Mengya Gao, Yujie Wang, Quanquan Li, Hongsheng Li, Xiaogang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Training a small student network with the guidance of a larger teacher network is an effective way to promote the performance of the student. Despite the different types, the guided knowledge used to distill is always kept unchanged for different teacher and student pairs in previous knowledge distillation methods. However, we find that teacher and student models with different networks or trained from different initialization could have distinct feature representations among different channels. (e.g. the high activated channel for different categories). We name this incongruous representation of channels as teacher-student knowledge discrepancy in the distillation process. Ignoring the knowledge discrepancy problem of teacher and student models will make the learning of student from teacher more difficult. To solve this problem, in this paper, we propose a novel student-dependent distillation method, knowledge consistent distillation, which makes teacher's knowledge more consistent with the student and provides the best suitable knowledge to different student networks for distillation. Extensive experiments on different datasets (CIFAR100, ImageNet, COCO) and tasks (image classification, object detection) reveal the widely existing knowledge discrepancy problem between teachers and students and demonstrate the effectiveness of our proposed method. Our method is very flexible that can be easily combined with other state-of-the-art approaches.



### A Novel Deep ML Architecture by Integrating Visual Simultaneous Localization and Mapping (vSLAM) into Mask R-CNN for Real-time Surgical Video Analysis
- **Arxiv ID**: http://arxiv.org/abs/2103.16847v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2103.16847v3)
- **Published**: 2021-03-31 06:59:13+00:00
- **Updated**: 2022-04-18 01:59:06+00:00
- **Authors**: Ella Selina Lan
- **Comment**: Accepted and Published in ISBI 2022
- **Journal**: None
- **Summary**: Seven million people suffer surgical complications each year, but with sufficient surgical training and review, 50\% of these complications could be prevented. To improve surgical performance, existing research uses various deep learning (DL) technologies including convolutional neural networks (CNN) and recurrent neural networks (RNN) to automate surgical tool and workflow detection. However, there is room to improve accuracy; real-time analysis is also minimal due to the complexity of CNN. In this research, a novel DL architecture is proposed to integrate visual simultaneous localization and mapping (vSLAM) into Mask R-CNN. This architecture, vSLAM-CNN (vCNN), for the first time, integrates the best of both worlds, inclusive of (1) vSLAM for object detection, by focusing on geometric information for region proposals, and (2) CNN for object recognition, by focusing on semantic information for image classification, combining them into one joint end-to-end training process. This method, using spatio-temporal information in addition to visual features, is evaluated on M2CAI 2016 challenge datasets, achieving the state-of-the-art results with 96.8 mAP for tool detection and 97.5 mean Jaccard score for workflow detection, surpassing all previous works, and reaching a 50 FPS performance, 10x faster than the region-based CNN. A region proposal module (RPM) replaces the region proposal network (RPN) in Mask R-CNN, accurately placing bounding boxes and lessening the annotation requirement. Furthermore, a Microsoft HoloLens 2 application is developed to provide an augmented reality (AR)-based solution for surgical training and assistance.



### Embracing Uncertainty: Decoupling and De-bias for Robust Temporal Grounding
- **Arxiv ID**: http://arxiv.org/abs/2103.16848v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.16848v2)
- **Published**: 2021-03-31 07:00:56+00:00
- **Updated**: 2021-06-24 07:29:25+00:00
- **Authors**: Hao Zhou, Chongyang Zhang, Yan Luo, Yanjun Chen, Chuanping Hu
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Temporal grounding aims to localize temporal boundaries within untrimmed videos by language queries, but it faces the challenge of two types of inevitable human uncertainties: query uncertainty and label uncertainty. The two uncertainties stem from human subjectivity, leading to limited generalization ability of temporal grounding. In this work, we propose a novel DeNet (Decoupling and De-bias) to embrace human uncertainty: Decoupling - We explicitly disentangle each query into a relation feature and a modified feature. The relation feature, which is mainly based on skeleton-like words (including nouns and verbs), aims to extract basic and consistent information in the presence of query uncertainty. Meanwhile, modified feature assigned with style-like words (including adjectives, adverbs, etc) represents the subjective information, and thus brings personalized predictions; De-bias - We propose a de-bias mechanism to generate diverse predictions, aim to alleviate the bias caused by single-style annotations in the presence of label uncertainty. Moreover, we put forward new multi-label metrics to diversify the performance evaluation. Extensive experiments show that our approach is more effective and robust than state-of-the-arts on Charades-STA and ActivityNet Captions datasets.



### Attention Map-guided Two-stage Anomaly Detection using Hard Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.16851v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.16851v1)
- **Published**: 2021-03-31 07:04:07+00:00
- **Updated**: 2021-03-31 07:04:07+00:00
- **Authors**: Jou Won Song, Kyeongbo Kong, Ye In Park, Suk-Ju Kang
- **Comment**: None
- **Journal**: None
- **Summary**: Anomaly detection is a task that recognizes whether an input sample is included in the distribution of a target normal class or an anomaly class. Conventional generative adversarial network (GAN)-based methods utilize an entire image including foreground and background as an input. However, in these methods, a useless region unrelated to the normal class (e.g., unrelated background) is learned as normal class distribution, thereby leading to false detection. To alleviate this problem, this paper proposes a novel two-stage network consisting of an attention network and an anomaly detection GAN (ADGAN). The attention network generates an attention map that can indicate the region representing the normal class distribution. To generate an accurate attention map, we propose the attention loss and the adversarial anomaly loss based on synthetic anomaly samples generated from hard augmentation. By applying the attention map to an image feature map, ADGAN learns the normal class distribution from which the useless region is removed, and it is possible to greatly reduce the problem difficulty of the anomaly detection task. Additionally, the estimated attention map can be used for anomaly segmentation because it can distinguish between normal and anomaly regions. As a result, the proposed method outperforms the state-of-the-art anomaly detection and anomaly segmentation methods for widely used datasets.



### Facial Expression Recognition with Visual Transformers and Attentional Selective Fusion
- **Arxiv ID**: http://arxiv.org/abs/2103.16854v3
- **DOI**: 10.1109/TAFFC.2021.3122146
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.16854v3)
- **Published**: 2021-03-31 07:07:56+00:00
- **Updated**: 2022-02-22 01:51:36+00:00
- **Authors**: Fuyan Ma, Bin Sun, Shutao Li
- **Comment**: None
- **Journal**: IEEE Trans. Affective Comput. 1(2021)1-1
- **Summary**: Facial Expression Recognition (FER) in the wild is extremely challenging due to occlusions, variant head poses, face deformation and motion blur under unconstrained conditions. Although substantial progresses have been made in automatic FER in the past few decades, previous studies were mainly designed for lab-controlled FER. Real-world occlusions, variant head poses and other issues definitely increase the difficulty of FER on account of these information-deficient regions and complex backgrounds. Different from previous pure CNNs based methods, we argue that it is feasible and practical to translate facial images into sequences of visual words and perform expression recognition from a global perspective. Therefore, we propose the Visual Transformers with Feature Fusion (VTFF) to tackle FER in the wild by two main steps. First, we propose the attentional selective fusion (ASF) for leveraging two kinds of feature maps generated by two-branch CNNs. The ASF captures discriminative information by fusing multiple features with the global-local attention. The fused feature maps are then flattened and projected into sequences of visual words. Second, inspired by the success of Transformers in natural language processing, we propose to model relationships between these visual words with the global self-attention. The proposed method is evaluated on three public in-the-wild facial expression datasets (RAF-DB, FERPlus and AffectNet). Under the same settings, extensive experiments demonstrate that our method shows superior performance over other methods, setting new state of the art on RAF-DB with 88.14%, FERPlus with 88.81% and AffectNet with 61.85%. The cross-dataset evaluation on CK+ shows the promising generalization capability of the proposed method.



### Robust Registration of Multimodal Remote Sensing Images Based on Structural Similarity
- **Arxiv ID**: http://arxiv.org/abs/2103.16871v1
- **DOI**: 10.1109/TGRS.2017.2656380
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.16871v1)
- **Published**: 2021-03-31 07:51:21+00:00
- **Updated**: 2021-03-31 07:51:21+00:00
- **Authors**: Yuanxin Ye, Jie Shan, Lorenzo Bruzzone, Li Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic registration of multimodal remote sensing data (e.g., optical, LiDAR, SAR) is a challenging task due to the significant non-linear radiometric differences between these data. To address this problem, this paper proposes a novel feature descriptor named the Histogram of Orientated Phase Congruency (HOPC), which is based on the structural properties of images. Furthermore, a similarity metric named HOPCncc is defined, which uses the normalized correlation coefficient (NCC) of the HOPC descriptors for multimodal registration. In the definition of the proposed similarity metric, we first extend the phase congruency model to generate its orientation representation, and use the extended model to build HOPCncc. Then a fast template matching scheme for this metric is designed to detect the control points between images. The proposed HOPCncc aims to capture the structural similarity between images, and has been tested with a variety of optical, LiDAR, SAR and map data. The results show that HOPCncc is robust against complex non-linear radiometric differences and outperforms the state-of-the-art similarities metrics (i.e., NCC and mutual information) in matching performance. Moreover, a robust registration method is also proposed in this paper based on HOPCncc, which is evaluated using six pairs of multimodal remote sensing images. The experimental results demonstrate the effectiveness of the proposed method for multimodal image registration.



### VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization
- **Arxiv ID**: http://arxiv.org/abs/2103.16874v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.16874v2)
- **Published**: 2021-03-31 07:52:41+00:00
- **Updated**: 2021-09-10 12:27:15+00:00
- **Authors**: Seunghwan Choi, Sunghyun Park, Minsoo Lee, Jaegul Choo
- **Comment**: 21 pages; project page: https://psh01087.github.io/VITON-HD; accepted
  to CVPR 2021; code URL added, references formatted
- **Journal**: None
- **Summary**: The task of image-based virtual try-on aims to transfer a target clothing item onto the corresponding region of a person, which is commonly tackled by fitting the item to the desired body part and fusing the warped item with the person. While an increasing number of studies have been conducted, the resolution of synthesized images is still limited to low (e.g., 256x192), which acts as the critical limitation against satisfying online consumers. We argue that the limitation stems from several challenges: as the resolution increases, the artifacts in the misaligned areas between the warped clothes and the desired clothing regions become noticeable in the final results; the architectures used in existing methods have low performance in generating high-quality body parts and maintaining the texture sharpness of the clothes. To address the challenges, we propose a novel virtual try-on method called VITON-HD that successfully synthesizes 1024x768 virtual try-on images. Specifically, we first prepare the segmentation map to guide our virtual try-on synthesis, and then roughly fit the target clothing item to a given person's body. Next, we propose ALIgnment-Aware Segment (ALIAS) normalization and ALIAS generator to handle the misaligned areas and preserve the details of 1024x768 inputs. Through rigorous comparison with existing methods, we demonstrate that VITON-HD highly surpasses the baselines in terms of synthesized image quality both qualitatively and quantitatively. Code is available at https://github.com/shadow2496/VITON-HD.



### ArtFlow: Unbiased Image Style Transfer via Reversible Neural Flows
- **Arxiv ID**: http://arxiv.org/abs/2103.16877v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.16877v2)
- **Published**: 2021-03-31 07:59:02+00:00
- **Updated**: 2021-04-09 16:18:15+00:00
- **Authors**: Jie An, Siyu Huang, Yibing Song, Dejing Dou, Wei Liu, Jiebo Luo
- **Comment**: CVPR 2021 Accepted
- **Journal**: None
- **Summary**: Universal style transfer retains styles from reference images in content images. While existing methods have achieved state-of-the-art style transfer performance, they are not aware of the content leak phenomenon that the image content may corrupt after several rounds of stylization process. In this paper, we propose ArtFlow to prevent content leak during universal style transfer. ArtFlow consists of reversible neural flows and an unbiased feature transfer module. It supports both forward and backward inferences and operates in a projection-transfer-reversion scheme. The forward inference projects input images into deep features, while the backward inference remaps deep features back to input images in a lossless and unbiased way. Extensive experiments demonstrate that ArtFlow achieves comparable performance to state-of-the-art style transfer methods while avoiding content leak.



### Neural Response Interpretation through the Lens of Critical Pathways
- **Arxiv ID**: http://arxiv.org/abs/2103.16886v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.16886v1)
- **Published**: 2021-03-31 08:08:41+00:00
- **Updated**: 2021-03-31 08:08:41+00:00
- **Authors**: Ashkan Khakzar, Soroosh Baselizadeh, Saurabh Khanduja, Christian Rupprecht, Seong Tae Kim, Nassir Navab
- **Comment**: Accepted at CVPR 2021 (IEEE/CVF Conference on Computer Vision and
  Pattern Recognition)
- **Journal**: None
- **Summary**: Is critical input information encoded in specific sparse pathways within the neural network? In this work, we discuss the problem of identifying these critical pathways and subsequently leverage them for interpreting the network's response to an input. The pruning objective -- selecting the smallest group of neurons for which the response remains equivalent to the original network -- has been previously proposed for identifying critical pathways. We demonstrate that sparse pathways derived from pruning do not necessarily encode critical input information. To ensure sparse pathways include critical fragments of the encoded input information, we propose pathway selection via neurons' contribution to the response. We proceed to explain how critical pathways can reveal critical input features. We prove that pathways selected via neuron contribution are locally linear (in an L2-ball), a property that we use for proposing a feature attribution method: "pathway gradient". We validate our interpretation method using mainstream evaluation experiments. The validation of pathway gradient interpretation method further confirms that selected pathways using neuron contributions correspond to critical input features. The code is publicly available.



### Joint Learning of Neural Transfer and Architecture Adaptation for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.16889v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.16889v1)
- **Published**: 2021-03-31 08:15:17+00:00
- **Updated**: 2021-03-31 08:15:17+00:00
- **Authors**: Guangrun Wang, Liang Lin, Rongcong Chen, Guangcong Wang, Jiqi Zhang
- **Comment**: To appear in IEEE Transactions ON Neural Networks and Learning
  Systems. We prove that dynamically adapting network architectures tailored
  for each domain task along with weight finetuning benefits in both efficiency
  and effectiveness, compared to the existing image recognition pipeline that
  only tunes the weights regardless of the architecture
- **Journal**: None
- **Summary**: Current state-of-the-art visual recognition systems usually rely on the following pipeline: (a) pretraining a neural network on a large-scale dataset (e.g., ImageNet) and (b) finetuning the network weights on a smaller, task-specific dataset. Such a pipeline assumes the sole weight adaptation is able to transfer the network capability from one domain to another domain, based on a strong assumption that a fixed architecture is appropriate for all domains. However, each domain with a distinct recognition target may need different levels/paths of feature hierarchy, where some neurons may become redundant, and some others are re-activated to form new network structures. In this work, we prove that dynamically adapting network architectures tailored for each domain task along with weight finetuning benefits in both efficiency and effectiveness, compared to the existing image recognition pipeline that only tunes the weights regardless of the architecture. Our method can be easily generalized to an unsupervised paradigm by replacing supernet training with self-supervised learning in the source domain tasks and performing linear evaluation in the downstream tasks. This further improves the search efficiency of our method. Moreover, we also provide principled and empirical analysis to explain why our approach works by investigating the ineffectiveness of existing neural architecture search. We find that preserving the joint distribution of the network architecture and weights is of importance. This analysis not only benefits image recognition but also provides insights for crafting neural networks. Experiments on five representative image recognition tasks such as person re-identification, age estimation, gender recognition, image classification, and unsupervised domain adaptation demonstrate the effectiveness of our method.



### Generating Multi-scale Maps from Remote Sensing Images via Series Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.16909v1
- **DOI**: 10.1109/LGRS.2021.3129285
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.16909v1)
- **Published**: 2021-03-31 08:58:37+00:00
- **Updated**: 2021-03-31 08:58:37+00:00
- **Authors**: Xu Chen, Bangguo Yin, Songqiang Chen, Haifeng Li, Tian Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Considering the success of generative adversarial networks (GANs) for image-to-image translation, researchers have attempted to translate remote sensing images (RSIs) to maps (rs2map) through GAN for cartography. However, these studies involved limited scales, which hinders multi-scale map creation. By extending their method, multi-scale RSIs can be trivially translated to multi-scale maps (multi-scale rs2map translation) through scale-wise rs2map models trained for certain scales (parallel strategy). However, this strategy has two theoretical limitations. First, inconsistency between various spatial resolutions of multi-scale RSIs and object generalization on multi-scale maps (RS-m inconsistency) increasingly complicate the extraction of geographical information from RSIs for rs2map models with decreasing scale. Second, as rs2map translation is cross-domain, generators incur high computation costs to transform the RSI pixel distribution to that on maps. Thus, we designed a series strategy of generators for multi-scale rs2map translation to address these limitations. In this strategy, high-resolution RSIs are inputted to an rs2map model to output large-scale maps, which are translated to multi-scale maps through series multi-scale map translation models. The series strategy avoids RS-m inconsistency as inputs are high-resolution large-scale RSIs, and reduces the distribution gap in multi-scale map generation through similar pixel distributions among multi-scale maps. Our experimental results showed better quality multi-scale map generation with the series strategy, as shown by average increases of 11.69%, 53.78%, 55.42%, and 72.34% in the structural similarity index, edge structural similarity index, intersection over union (road), and intersection over union (water) for data from Mexico City and Tokyo at zoom level 17-13.



### Using depth information and colour space variations for improving outdoor robustness for instance segmentation of cabbage
- **Arxiv ID**: http://arxiv.org/abs/2103.16923v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.16923v1)
- **Published**: 2021-03-31 09:19:12+00:00
- **Updated**: 2021-03-31 09:19:12+00:00
- **Authors**: Nils Lüling, David Reiser, Alexander Stana, H. W. Griepentrog
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Image-based yield detection in agriculture could raiseharvest efficiency and cultivation performance of farms. Following this goal, this research focuses on improving instance segmentation of field crops under varying environmental conditions. Five data sets of cabbage plants were recorded under varying lighting outdoor conditions. The images were acquired using a commercial mono camera. Additionally, depth information was generated out of the image stream with Structure-from-Motion (SfM). A Mask R-CNN was used to detect and segment the cabbage heads. The influence of depth information and different colour space representations were analysed. The results showed that depth combined with colour information leads to a segmentation accuracy increase of 7.1%. By describing colour information by colour spaces using light and saturation information combined with depth information, additional segmentation improvements of 16.5% could be reached. The CIELAB colour space combined with a depth information layer showed the best results achieving a mean average precision of 75.



### Few-Data Guided Learning Upon End-to-End Point Cloud Network for 3D Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.16927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.16927v1)
- **Published**: 2021-03-31 09:26:14+00:00
- **Updated**: 2021-03-31 09:26:14+00:00
- **Authors**: Yi Yu, Feipeng Da, Ziyu Zhang
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: 3D face recognition has shown its potential in many application scenarios. Among numerous 3D face recognition methods, deep-learning-based methods have developed vigorously in recent years. In this paper, an end-to-end deep learning network entitled Sur3dNet-Face for point-cloud-based 3D face recognition is proposed. The network uses PointNet as the backbone, which is a successful point cloud classification solution but does not work properly in face recognition. Supplemented with modifications in network architecture and a few-data guided learning framework based on Gaussian process morphable model, the backbone is successfully modified for 3D face recognition. Different from existing methods training with a large amount of data in multiple datasets, our method uses Spring2003 subset of FRGC v2.0 for training which contains only 943 facial scans, and the network is well trained with the guidance of such a small amount of real data. Without fine-tuning on the test set, the Rank-1 Recognition Rate (RR1) is achieved as follows: 98.85% on FRGC v2.0 dataset and 99.33% on Bosphorus dataset, which proves the effectiveness and the potentiality of our method.



### Unpaired Single-Image Depth Synthesis with cycle-consistent Wasserstein GANs
- **Arxiv ID**: http://arxiv.org/abs/2103.16938v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.16938v3)
- **Published**: 2021-03-31 09:43:38+00:00
- **Updated**: 2022-07-08 08:38:15+00:00
- **Authors**: Christoph Angermann, Adéla Moravová, Markus Haltmeier, Steinbjörn Jónsson, Christian Laubichler
- **Comment**: This preprint is and will not be considered for publication. The
  paper had a major revision with significant changes of the content. The
  updated version can be found here: arXiv:2201.12170
- **Journal**: None
- **Summary**: Real-time estimation of actual environment depth is an essential module for various autonomous system tasks such as localization, obstacle detection and pose estimation. During the last decade of machine learning, extensive deployment of deep learning methods to computer vision tasks yielded successful approaches for realistic depth synthesis out of a simple RGB modality. While most of these models rest on paired depth data or availability of video sequences and stereo images, there is a lack of methods facing single-image depth synthesis in an unsupervised manner. Therefore, in this study, latest advancements in the field of generative neural networks are leveraged to fully unsupervised single-image depth synthesis. To be more exact, two cycle-consistent generators for RGB-to-depth and depth-to-RGB transfer are implemented and simultaneously optimized using the Wasserstein-1 distance. To ensure plausibility of the proposed method, we apply the models to a self acquised industrial data set as well as to the renown NYU Depth v2 data set, which allows comparison with existing approaches. The observed success in this study suggests high potential for unpaired single-image depth estimation in real world applications.



### Learning with Memory-based Virtual Classes for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.16940v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.16940v2)
- **Published**: 2021-03-31 09:44:29+00:00
- **Updated**: 2021-10-08 06:04:36+00:00
- **Authors**: Byungsoo Ko, Geonmo Gu, Han-Gyu Kim
- **Comment**: Accepted by ICCV2021
- **Journal**: None
- **Summary**: The core of deep metric learning (DML) involves learning visual similarities in high-dimensional embedding space. One of the main challenges is to generalize from seen classes of training data to unseen classes of test data. Recent works have focused on exploiting past embeddings to increase the number of instances for the seen classes. Such methods achieve performance improvement via augmentation, while the strong focus on seen classes still remains. This can be undesirable for DML, where training and test data exhibit entirely different classes. In this work, we present a novel training strategy for DML called MemVir. Unlike previous works, MemVir memorizes both embedding features and class weights to utilize them as additional virtual classes. The exploitation of virtual classes not only utilizes augmented information for training but also alleviates a strong focus on seen classes for better generalization. Moreover, we embed the idea of curriculum learning by slowly adding virtual classes for a gradual increase in learning difficulty, which improves the learning stability as well as the final performance. MemVir can be easily applied to many existing loss functions without any modification. Extensive experimental results on famous benchmarks demonstrate the superiority of MemVir over state-of-the-art competitors. Code of MemVir is publicly available.



### Neural Surface Maps
- **Arxiv ID**: http://arxiv.org/abs/2103.16942v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2103.16942v1)
- **Published**: 2021-03-31 09:48:26+00:00
- **Updated**: 2021-03-31 09:48:26+00:00
- **Authors**: Luca Morreale, Noam Aigerman, Vladimir Kim, Niloy J. Mitra
- **Comment**: project page: http://geometry.cs.ucl.ac.uk/projects/2021/neuralmaps/
- **Journal**: None
- **Summary**: Maps are arguably one of the most fundamental concepts used to define and operate on manifold surfaces in differentiable geometry. Accordingly, in geometry processing, maps are ubiquitous and are used in many core applications, such as paramterization, shape analysis, remeshing, and deformation. Unfortunately, most computational representations of surface maps do not lend themselves to manipulation and optimization, usually entailing hard, discrete problems. While algorithms exist to solve these problems, they are problem-specific, and a general framework for surface maps is still in need. In this paper, we advocate considering neural networks as encoding surface maps. Since neural networks can be composed on one another and are differentiable, we show it is easy to use them to define surfaces via atlases, compose them for surface-to-surface mappings, and optimize differentiable objectives relating to them, such as any notion of distortion, in a trivial manner. In our experiments, we represent surfaces by generating a neural map that approximates a UV parameterization of a 3D model. Then, we compose this map with other neural maps which we optimize with respect to distortion measures. We show that our formulation enables trivial optimization of rather elusive mapping tasks, such as maps between a collection of surfaces.



### Near-field Perception for Low-Speed Vehicle Automation using Surround-view Fisheye Cameras
- **Arxiv ID**: http://arxiv.org/abs/2103.17001v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.17001v4)
- **Published**: 2021-03-31 11:33:36+00:00
- **Updated**: 2023-06-06 15:25:08+00:00
- **Authors**: Ciaran Eising, Jonathan Horgan, Senthil Yogamani
- **Comment**: Accepted for publication at IEEE Transactions on Intelligent
  Transportation Systems
- **Journal**: None
- **Summary**: Cameras are the primary sensor in automated driving systems. They provide high information density and are optimal for detecting road infrastructure cues laid out for human vision. Surround-view camera systems typically comprise of four fisheye cameras with 190{\deg}+ field of view covering the entire 360{\deg} around the vehicle focused on near-field sensing. They are the principal sensors for low-speed, high accuracy, and close-range sensing applications, such as automated parking, traffic jam assistance, and low-speed emergency braking. In this work, we provide a detailed survey of such vision systems, setting up the survey in the context of an architecture that can be decomposed into four modular components namely Recognition, Reconstruction, Relocalization, and Reorganization. We jointly call this the 4R Architecture. We discuss how each component accomplishes a specific aspect and provide a positional argument that they can be synergized to form a complete perception system for low-speed automation. We support this argument by presenting results from previous works and by presenting architecture proposals for such a system. Qualitative results are presented in the video at https://youtu.be/ae8bCOF77uY.



### Knowledge Distillation By Sparse Representation Matching
- **Arxiv ID**: http://arxiv.org/abs/2103.17012v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17012v1)
- **Published**: 2021-03-31 11:47:47+00:00
- **Updated**: 2021-03-31 11:47:47+00:00
- **Authors**: Dat Thanh Tran, Moncef Gabbouj, Alexandros Iosifidis
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Knowledge Distillation refers to a class of methods that transfers the knowledge from a teacher network to a student network. In this paper, we propose Sparse Representation Matching (SRM), a method to transfer intermediate knowledge obtained from one Convolutional Neural Network (CNN) to another by utilizing sparse representation learning. SRM first extracts sparse representations of the hidden features of the teacher CNN, which are then used to generate both pixel-level and image-level labels for training intermediate feature maps of the student network. We formulate SRM as a neural processing block, which can be efficiently optimized using stochastic gradient descent and integrated into any CNN in a plug-and-play manner. Our experiments demonstrate that SRM is robust to architectural differences between the teacher and student networks, and outperforms other KD techniques across several datasets.



### Learning Scalable $\ell_\infty$-constrained Near-lossless Image Compression via Joint Lossy Image and Residual Compression
- **Arxiv ID**: http://arxiv.org/abs/2103.17015v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.17015v1)
- **Published**: 2021-03-31 11:53:36+00:00
- **Updated**: 2021-03-31 11:53:36+00:00
- **Authors**: Yuanchao Bai, Xianming Liu, Wangmeng Zuo, Yaowei Wang, Xiangyang Ji
- **Comment**: Accepted by CVPR 2021; Code:
  https://github.com/BYchao100/Scalable-Near-lossless-Image-Compression
- **Journal**: None
- **Summary**: We propose a novel joint lossy image and residual compression framework for learning $\ell_\infty$-constrained near-lossless image compression. Specifically, we obtain a lossy reconstruction of the raw image through lossy image compression and uniformly quantize the corresponding residual to satisfy a given tight $\ell_\infty$ error bound. Suppose that the error bound is zero, i.e., lossless image compression, we formulate the joint optimization problem of compressing both the lossy image and the original residual in terms of variational auto-encoders and solve it with end-to-end training. To achieve scalable compression with the error bound larger than zero, we derive the probability model of the quantized residual by quantizing the learned probability model of the original residual, instead of training multiple networks. We further correct the bias of the derived probability model caused by the context mismatch between training and inference. Finally, the quantized residual is encoded according to the bias-corrected probability model and is concatenated with the bitstream of the compressed lossy image. Experimental results demonstrate that our near-lossless codec achieves the state-of-the-art performance for lossless and near-lossless image compression, and achieves competitive PSNR while much smaller $\ell_\infty$ error compared with lossy image codecs at high bit rates.



### Semantic-guided Automatic Natural Image Matting with Trimap Generation Network and Light-weight Non-local Attention
- **Arxiv ID**: http://arxiv.org/abs/2103.17020v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17020v3)
- **Published**: 2021-03-31 12:08:28+00:00
- **Updated**: 2021-09-11 00:24:59+00:00
- **Authors**: Yuhongze Zhou, Liguang Zhou, Tin Lun Lam, Yangsheng Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Natural image matting aims to precisely separate foreground objects from background using alpha matte. Fully automatic natural image matting without external annotation is challenging. Well-performed matting methods usually require accurate labor-intensive handcrafted trimap as extra input, while the performance of automatic trimap generation method of dilating foreground segmentation fluctuates with segmentation quality. Therefore, we argue that how to handle trade-off of additional information input is a major issue in automatic matting. This paper presents a semantic-guided automatic natural image matting pipeline with Trimap Generation Network and light-weight non-local attention, which does not need trimap and background as input. Specifically, guided by foreground segmentation, Trimap Generation Network estimates accurate trimap. Then, with estimated trimap as guidance, our light-weight Non-local Matting Network with Refinement produces final alpha matte, whose trimap-guided global aggregation attention block is equipped with stride downsampling convolution, reducing computation complexity and promoting performance. Experimental results show that our matting algorithm has competitive performance with state-of-the-art methods in both trimap-free and trimap-needed aspects.



### Layout-Guided Novel View Synthesis from a Single Indoor Panorama
- **Arxiv ID**: http://arxiv.org/abs/2103.17022v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17022v1)
- **Published**: 2021-03-31 12:12:22+00:00
- **Updated**: 2021-03-31 12:12:22+00:00
- **Authors**: Jiale Xu, Jia Zheng, Yanyu Xu, Rui Tang, Shenghua Gao
- **Comment**: To appear in CVPR 2021
- **Journal**: None
- **Summary**: Existing view synthesis methods mainly focus on the perspective images and have shown promising results. However, due to the limited field-of-view of the pinhole camera, the performance quickly degrades when large camera movements are adopted. In this paper, we make the first attempt to generate novel views from a single indoor panorama and take the large camera translations into consideration. To tackle this challenging problem, we first use Convolutional Neural Networks (CNNs) to extract the deep features and estimate the depth map from the source-view image. Then, we leverage the room layout prior, a strong structural constraint of the indoor scene, to guide the generation of target views. More concretely, we estimate the room layout in the source view and transform it into the target viewpoint as guidance. Meanwhile, we also constrain the room layout of the generated target-view images to enforce geometric consistency. To validate the effectiveness of our method, we further build a large-scale photo-realistic dataset containing both small and large camera translations. The experimental results on our challenging dataset demonstrate that our method achieves state-of-the-art performance. The project page is at https://github.com/bluestyle97/PNVS.



### SRA-LSTM: Social Relationship Attention LSTM for Human Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2103.17045v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.17045v1)
- **Published**: 2021-03-31 12:56:39+00:00
- **Updated**: 2021-03-31 12:56:39+00:00
- **Authors**: Yusheng Peng, Gaofeng Zhang, Jun Shi, Benzhu Xu, Liping Zheng
- **Comment**: Submitted to Neural Computing and Applications
- **Journal**: None
- **Summary**: Pedestrian trajectory prediction for surveillance video is one of the important research topics in the field of computer vision and a key technology of intelligent surveillance systems. Social relationship among pedestrians is a key factor influencing pedestrian walking patterns but was mostly ignored in the literature. Pedestrians with different social relationships play different roles in the motion decision of target pedestrian. Motivated by this idea, we propose a Social Relationship Attention LSTM (SRA-LSTM) model to predict future trajectories. We design a social relationship encoder to obtain the representation of their social relationship through the relative position between each pair of pedestrians. Afterwards, the social relationship feature and latent movements are adopted to acquire the social relationship attention of this pair of pedestrians. Social interaction modeling is achieved by utilizing social relationship attention to aggregate movement information from neighbor pedestrians. Experimental results on two public walking pedestrian video datasets (ETH and UCY), our model achieves superior performance compared with state-of-the-art methods. Contrast experiments with other attention methods also demonstrate the effectiveness of social relationship attention.



### Smart Scribbles for Image Mating
- **Arxiv ID**: http://arxiv.org/abs/2103.17062v1
- **DOI**: 10.1145/3408323
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17062v1)
- **Published**: 2021-03-31 13:30:49+00:00
- **Updated**: 2021-03-31 13:30:49+00:00
- **Authors**: Xin Yang, Yu Qiao, Shaozhe Chen, Shengfeng He, Baocai Yin, Qiang Zhang, Xiaopeng Wei, Rynson W. H. Lau
- **Comment**: ACM Trans. Multimedia Comput. Commun. Appl
- **Journal**: None
- **Summary**: Image matting is an ill-posed problem that usually requires additional user input, such as trimaps or scribbles. Drawing a fne trimap requires a large amount of user effort, while using scribbles can hardly obtain satisfactory alpha mattes for non-professional users. Some recent deep learning-based matting networks rely on large-scale composite datasets for training to improve performance, resulting in the occasional appearance of obvious artifacts when processing natural images. In this article, we explore the intrinsic relationship between user input and alpha mattes and strike a balance between user effort and the quality of alpha mattes. In particular, we propose an interactive framework, referred to as smart scribbles, to guide users to draw few scribbles on the input images to produce high-quality alpha mattes. It frst infers the most informative regions of an image for drawing scribbles to indicate different categories (foreground, background, or unknown) and then spreads these scribbles (i.e., the category labels) to the rest of the image via our well-designed two-phase propagation. Both neighboring low-level afnities and high-level semantic features are considered during the propagation process. Our method can be optimized without large-scale matting datasets and exhibits more universality in real situations. Extensive experiments demonstrate that smart scribbles can produce more accurate alpha mattes with reduced additional input, compared to the state-of-the-art matting methods.



### DA-DETR: Domain Adaptive Detection Transformer with Information Fusion
- **Arxiv ID**: http://arxiv.org/abs/2103.17084v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17084v2)
- **Published**: 2021-03-31 13:55:56+00:00
- **Updated**: 2023-03-22 05:15:36+00:00
- **Authors**: Jingyi Zhang, Jiaxing Huang, Zhipeng Luo, Gongjie Zhang, Xiaoqin Zhang, Shijian Lu
- **Comment**: Accepted to CVPR2023
- **Journal**: None
- **Summary**: The recent detection transformer (DETR) simplifies the object detection pipeline by removing hand-crafted designs and hyperparameters as employed in conventional two-stage object detectors. However, how to leverage the simple yet effective DETR architecture in domain adaptive object detection is largely neglected. Inspired by the unique DETR attention mechanisms, we design DA-DETR, a domain adaptive object detection transformer that introduces information fusion for effective transfer from a labeled source domain to an unlabeled target domain. DA-DETR introduces a novel CNN-Transformer Blender (CTBlender) that fuses the CNN features and Transformer features ingeniously for effective feature alignment and knowledge transfer across domains. Specifically, CTBlender employs the Transformer features to modulate the CNN features across multiple scales where the high-level semantic information and the low-level spatial information are fused for accurate object identification and localization. Extensive experiments show that DA-DETR achieves superior detection performance consistently across multiple widely adopted domain adaptation benchmarks.



### Deep adaptive fuzzy clustering for evolutionary unsupervised representation learning
- **Arxiv ID**: http://arxiv.org/abs/2103.17086v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.17086v1)
- **Published**: 2021-03-31 13:58:10+00:00
- **Updated**: 2021-03-31 13:58:10+00:00
- **Authors**: Dayu Tan, Zheng Huang, Xin Peng, Weimin Zhong, Vladimir Mahalec
- **Comment**: None
- **Journal**: None
- **Summary**: Cluster assignment of large and complex images is a crucial but challenging task in pattern recognition and computer vision. In this study, we explore the possibility of employing fuzzy clustering in a deep neural network framework. Thus, we present a novel evolutionary unsupervised learning representation model with iterative optimization. It implements the deep adaptive fuzzy clustering (DAFC) strategy that learns a convolutional neural network classifier from given only unlabeled data samples. DAFC consists of a deep feature quality-verifying model and a fuzzy clustering model, where deep feature representation learning loss function and embedded fuzzy clustering with the weighted adaptive entropy is implemented. We joint fuzzy clustering to the deep reconstruction model, in which fuzzy membership is utilized to represent a clear structure of deep cluster assignments and jointly optimize for the deep representation learning and clustering. Also, the joint model evaluates current clustering performance by inspecting whether the re-sampled data from estimated bottleneck space have consistent clustering properties to progressively improve the deep clustering model. Comprehensive experiments on a variety of datasets show that the proposed method obtains a substantially better performance for both reconstruction and clustering quality when compared to the other state-of-the-art deep clustering methods, as demonstrated with the in-depth analysis in the extensive experiments.



### Deep Image Harmonization by Bridging the Reality Gap
- **Arxiv ID**: http://arxiv.org/abs/2103.17104v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17104v3)
- **Published**: 2021-03-31 14:19:56+00:00
- **Updated**: 2022-10-12 02:06:23+00:00
- **Authors**: Junyan Cao, Wenyan Cong, Li Niu, Jianfu Zhang, Liqing Zhang
- **Comment**: Accepted by BMVC2022
- **Journal**: None
- **Summary**: Image harmonization has been significantly advanced with large-scale harmonization dataset. However, the current way to build dataset is still labor-intensive, which adversely affects the extendability of dataset. To address this problem, we propose to construct rendered harmonization dataset with fewer human efforts to augment the existing real-world dataset. To leverage both real-world images and rendered images, we propose a cross-domain harmonization network to bridge the domain gap between two domains. Moreover, we also employ well-designed style classifiers and losses to facilitate cross-domain knowledge transfer. Extensive experiments demonstrate the potential of using rendered images for image harmonization and the effectiveness of our proposed network.



### The GIST and RIST of Iterative Self-Training for Semi-Supervised Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.17105v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17105v3)
- **Published**: 2021-03-31 14:20:37+00:00
- **Updated**: 2022-04-29 00:08:16+00:00
- **Authors**: Eu Wern Teh, Terrance DeVries, Brendan Duke, Ruowei Jiang, Parham Aarabi, Graham W. Taylor
- **Comment**: To appear in the Conference on Computer and Robot Vision (CRV), 2022
- **Journal**: None
- **Summary**: We consider the task of semi-supervised semantic segmentation, where we aim to produce pixel-wise semantic object masks given only a small number of human-labeled training examples. We focus on iterative self-training methods in which we explore the behavior of self-training over multiple refinement stages. We show that iterative self-training leads to performance degradation if done na\"ively with a fixed ratio of human-labeled to pseudo-labeled training examples. We propose Greedy Iterative Self-Training (GIST) and Random Iterative Self-Training (RIST) strategies that alternate between training on either human-labeled data or pseudo-labeled data at each refinement stage, resulting in a performance boost rather than degradation. We further show that GIST and RIST can be combined with existing semi-supervised learning methods to boost performance.



### Facial expression and attributes recognition based on multi-task learning of lightweight neural networks
- **Arxiv ID**: http://arxiv.org/abs/2103.17107v3
- **DOI**: 10.1109/SISY52375.2021.9582508
- **Categories**: **cs.CV**, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/2103.17107v3)
- **Published**: 2021-03-31 14:21:04+00:00
- **Updated**: 2021-10-04 10:48:28+00:00
- **Authors**: Andrey V. Savchenko
- **Comment**: 14 pages, 3 figures, accepted at IEEE SISY 2021
- **Journal**: None
- **Summary**: In this paper, the multi-task learning of lightweight convolutional neural networks is studied for face identification and classification of facial attributes (age, gender, ethnicity) trained on cropped faces without margins. The necessity to fine-tune these networks to predict facial expressions is highlighted. Several models are presented based on MobileNet, EfficientNet and RexNet architectures. It was experimentally demonstrated that they lead to near state-of-the-art results in age, gender and race recognition on the UTKFace dataset and emotion classification on the AffectNet dataset. Moreover, it is shown that the usage of the trained models as feature extractors of facial regions in video frames leads to 4.5% higher accuracy than the previously known state-of-the-art single models for the AFEW and the VGAF datasets from the EmotiW challenges. The models and source code are publicly available at https://github.com/HSE-asavchenko/face-emotion-recognition.



### Differentiable Deconvolution for Improved Stroke Perfusion Analysis
- **Arxiv ID**: http://arxiv.org/abs/2103.17111v1
- **DOI**: 10.1007/978-3-030-59728-3_58
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.17111v1)
- **Published**: 2021-03-31 14:29:36+00:00
- **Updated**: 2021-03-31 14:29:36+00:00
- **Authors**: Ezequiel de la Rosa, David Robben, Diana M. Sima, Jan S. Kirschke, Bjoern Menze
- **Comment**: Accepted at MICCAI 2020
- **Journal**: International Conference on Medical Image Computing and
  Computer-Assisted Intervention 2020 Oct 4 (pp. 593-602)
- **Summary**: Perfusion imaging is the current gold standard for acute ischemic stroke analysis. It allows quantification of the salvageable and non-salvageable tissue regions (penumbra and core areas respectively). In clinical settings, the singular value decomposition (SVD) deconvolution is one of the most accepted and used approaches for generating interpretable and physically meaningful maps. Though this method has been widely validated in experimental and clinical settings, it might produce suboptimal results because the chosen inputs to the model cannot guarantee optimal performance. For the most critical input, the arterial input function (AIF), it is still controversial how and where it should be chosen even though the method is very sensitive to this input. In this work we propose an AIF selection approach that is optimized for maximal core lesion segmentation performance. The AIF is regressed by a neural network optimized through a differentiable SVD deconvolution, aiming to maximize core lesion segmentation agreement with ground truth data. To our knowledge, this is the first work exploiting a differentiable deconvolution model with neural networks. We show that our approach is able to generate AIFs without any manual annotation, and hence avoiding manual rater's influences. The method achieves manual expert performance in the ISLES18 dataset. We conclude that the methodology opens new possibilities for improving perfusion imaging quantification with deep neural networks.



### iCurb: Imitation Learning-based Detection of Road Curbs using Aerial Images for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2103.17118v1
- **DOI**: 10.1109/LRA.2021.3056344
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.17118v1)
- **Published**: 2021-03-31 14:40:31+00:00
- **Updated**: 2021-03-31 14:40:31+00:00
- **Authors**: Zhenhua Xu, Yuxiang Sun, Ming Liu
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters,6,(2021),1097-1104
- **Summary**: Detection of road curbs is an essential capability for autonomous driving. It can be used for autonomous vehicles to determine drivable areas on roads. Usually, road curbs are detected on-line using vehicle-mounted sensors, such as video cameras and 3-D Lidars. However, on-line detection using video cameras may suffer from challenging illumination conditions, and Lidar-based approaches may be difficult to detect far-away road curbs due to the sparsity issue of point clouds. In recent years, aerial images are becoming more and more worldwide available. We find that the visual appearances between road areas and off-road areas are usually different in aerial images, so we propose a novel solution to detect road curbs off-line using aerial images. The input to our method is an aerial image, and the output is directly a graph (i.e., vertices and edges) representing road curbs. To this end, we formulate the problem as an imitation learning problem, and design a novel network and an innovative training strategy to train an agent to iteratively find the road-curb graph. The experimental results on a public dataset confirm the effectiveness and superiority of our method. This work is accompanied with a demonstration video and a supplementary document at https://tonyxuqaq.github.io/iCurb/.



### Topo-boundary: A Benchmark Dataset on Topological Road-boundary Detection Using Aerial Images for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2103.17119v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.17119v2)
- **Published**: 2021-03-31 14:42:00+00:00
- **Updated**: 2021-07-02 08:46:54+00:00
- **Authors**: Zhenhua Xu, Yuxiang Sun, Ming Liu
- **Comment**: Accepted by IEEE Robotics and Automation Letters(RA-L) and The
  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
  2021
- **Journal**: None
- **Summary**: Road-boundary detection is important for autonomous driving. It can be used to constrain autonomous vehicles running on road areas to ensure driving safety. Compared with online road-boundary detection using on-vehicle cameras/Lidars, offline detection using aerial images could alleviate the severe occlusion issue. Moreover, the offline detection results can be directly employed to annotate high-definition (HD) maps. In recent years, deep-learning technologies have been used in offline detection. But there still lacks a publicly available dataset for this task, which hinders the research progress in this area. So in this paper, we propose a new benchmark dataset, named \textit{Topo-boundary}, for offline topological road-boundary detection. The dataset contains 25,295 $1000\times1000$-sized 4-channel aerial images. Each image is provided with 8 training labels for different sub-tasks. We also design a new entropy-based metric for connectivity evaluation, which could better handle noises or outliers. We implement and evaluate 3 segmentation-based baselines and 5 graph-based baselines using the dataset. We also propose a new imitation-learning-based baseline which is enhanced from our previous work. The superiority of our enhancement is demonstrated from the comparison. The dataset and our-implemented code for the baselines are available at \texttt{\url{https://tonyxuqaq.github.io/Topo-boundary/}}.



### Camouflaged Instance Segmentation In-The-Wild: Dataset, Method, and Benchmark Suite
- **Arxiv ID**: http://arxiv.org/abs/2103.17123v4
- **DOI**: 10.1109/TIP.2021.3130490
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17123v4)
- **Published**: 2021-03-31 14:46:12+00:00
- **Updated**: 2021-12-12 01:46:26+00:00
- **Authors**: Trung-Nghia Le, Yubo Cao, Tan-Cong Nguyen, Minh-Quan Le, Khanh-Duy Nguyen, Thanh-Toan Do, Minh-Triet Tran, Tam V. Nguyen
- **Comment**: TIP acceptance. Project page:
  https://sites.google.com/view/ltnghia/research/camo_plus_plus
- **Journal**: IEEE Transactions on Image Processing 2021
- **Summary**: This paper pushes the envelope on decomposing camouflaged regions in an image into meaningful components, namely, camouflaged instances. To promote the new task of camouflaged instance segmentation of in-the-wild images, we introduce a dataset, dubbed CAMO++, that extends our preliminary CAMO dataset (camouflaged object segmentation) in terms of quantity and diversity. The new dataset substantially increases the number of images with hierarchical pixel-wise ground truths. We also provide a benchmark suite for the task of camouflaged instance segmentation. In particular, we present an extensive evaluation of state-of-the-art instance segmentation methods on our newly constructed CAMO++ dataset in various scenarios. We also present a camouflage fusion learning (CFL) framework for camouflaged instance segmentation to further improve the performance of state-of-the-art methods. The dataset, model, evaluation suite, and benchmark will be made publicly available on our project page: https://sites.google.com/view/ltnghia/research/camo_plus_plus



### Rank-One Prior: Toward Real-Time Scene Recovery
- **Arxiv ID**: http://arxiv.org/abs/2103.17126v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17126v2)
- **Published**: 2021-03-31 14:47:55+00:00
- **Updated**: 2021-04-07 02:19:36+00:00
- **Authors**: Jun Liu, Ryan Wen Liu, Jianing Sun, Tieyong Zeng
- **Comment**: 9 pages, 6 figures, 1 table, Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Scene recovery is a fundamental imaging task for several practical applications, e.g., video surveillance and autonomous vehicles, etc. To improve visual quality under different weather/imaging conditions, we propose a real-time light correction method to recover the degraded scenes in the cases of sandstorms, underwater, and haze. The heart of our work is that we propose an intensity projection strategy to estimate the transmission. This strategy is motivated by a straightforward rank-one transmission prior. The complexity of transmission estimation is $O(N)$ where $N$ is the size of the single image. Then we can recover the scene in real-time. Comprehensive experiments on different types of weather/imaging conditions illustrate that our method outperforms competitively several state-of-the-art imaging methods in terms of efficiency and robustness.



### SOON: Scenario Oriented Object Navigation with Graph-based Exploration
- **Arxiv ID**: http://arxiv.org/abs/2103.17138v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17138v2)
- **Published**: 2021-03-31 15:01:04+00:00
- **Updated**: 2021-10-15 10:22:14+00:00
- **Authors**: Fengda Zhu, Xiwen Liang, Yi Zhu, Xiaojun Chang, Xiaodan Liang
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to navigate like a human towards a language-guided target from anywhere in a 3D embodied environment is one of the 'holy grail' goals of intelligent robots. Most visual navigation benchmarks, however, focus on navigating toward a target from a fixed starting point, guided by an elaborate set of instructions that depicts step-by-step. This approach deviates from real-world problems in which human-only describes what the object and its surrounding look like and asks the robot to start navigation from anywhere. Accordingly, in this paper, we introduce a Scenario Oriented Object Navigation (SOON) task. In this task, an agent is required to navigate from an arbitrary position in a 3D embodied environment to localize a target following a scene description. To give a promising direction to solve this task, we propose a novel graph-based exploration (GBE) method, which models the navigation state as a graph and introduces a novel graph-based exploration approach to learn knowledge from the graph and stabilize training by learning sub-optimal trajectories. We also propose a new large-scale benchmark named From Anywhere to Object (FAO) dataset. To avoid target ambiguity, the descriptions in FAO provide rich semantic scene information includes: object attribute, object relationship, region description, and nearby region description. Our experiments reveal that the proposed GBE outperforms various state-of-the-arts on both FAO and R2R datasets. And the ablation studies on FAO validates the quality of the dataset.



### Learning Spatio-Temporal Transformer for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2103.17154v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17154v1)
- **Published**: 2021-03-31 15:19:19+00:00
- **Updated**: 2021-03-31 15:19:19+00:00
- **Authors**: Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, Huchuan Lu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a new tracking architecture with an encoder-decoder transformer as the key component. The encoder models the global spatio-temporal feature dependencies between target objects and search regions, while the decoder learns a query embedding to predict the spatial positions of the target objects. Our method casts object tracking as a direct bounding box prediction problem, without using any proposals or predefined anchors. With the encoder-decoder transformer, the prediction of objects just uses a simple fully-convolutional network, which estimates the corners of objects directly. The whole method is end-to-end, does not need any postprocessing steps such as cosine window and bounding box smoothing, thus largely simplifying existing tracking pipelines. The proposed tracker achieves state-of-the-art performance on five challenging short-term and long-term benchmarks, while running at real-time speed, being 6x faster than Siam R-CNN. Code and models are open-sourced at https://github.com/researchmm/Stark.



### Spectral decoupling allows training transferable neural networks in medical imaging
- **Arxiv ID**: http://arxiv.org/abs/2103.17171v4
- **DOI**: 10.1016/j.isci.2022.103767
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2103.17171v4)
- **Published**: 2021-03-31 15:47:01+00:00
- **Updated**: 2021-12-17 17:16:03+00:00
- **Authors**: Joona Pohjonen, Carolin Stürenberg, Antti Rannikko, Tuomas Mirtti, Esa Pitkänen
- **Comment**: 10 pages, 7 figures and 2 tables
- **Journal**: None
- **Summary**: Many current neural networks for medical imaging generalise poorly to data unseen during training. Such behaviour can be caused by networks overfitting easy-to-learn, or statistically dominant, features while disregarding other potentially informative features. For example, indistinguishable differences in the sharpness of the images from two different scanners can degrade the performance of the network significantly. All neural networks intended for clinical practice need to be robust to variation in data caused by differences in imaging equipment, sample preparation and patient populations.   To address these challenges, we evaluate the utility of spectral decoupling as an implicit bias mitigation method. Spectral decoupling encourages the neural network to learn more features by simply regularising the networks' unnormalised prediction scores with an L2 penalty, thus having no added computational costs.   We show that spectral decoupling allows training neural networks on datasets with strong spurious correlations and increases networks' robustness for data distribution shifts. To validate our findings, we train networks with and without spectral decoupling to detect prostate cancer tissue slides and COVID-19 in chest radiographs. Networks trained with spectral decoupling achieve up to 9.5 percent point higher performance on external datasets.   Our results show that spectral decoupling helps with generalisation issues associated with neural networks, and can be used to complement or replace computationally expensive explicit bias mitigation methods, such as stain normalization in histological images. We recommend using spectral decoupling as an implicit bias mitigation method in any neural network intended for clinical use.



### Classification of Hematoma: Joint Learning of Semantic Segmentation and Classification
- **Arxiv ID**: http://arxiv.org/abs/2103.17172v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.17172v1)
- **Published**: 2021-03-31 15:47:33+00:00
- **Updated**: 2021-03-31 15:47:33+00:00
- **Authors**: Hokuto Hirano, Tsuyoshi Okita
- **Comment**: None
- **Journal**: None
- **Summary**: Cerebral hematoma grows rapidly in 6-24 hours and misprediction of the growth can be fatal if it is not operated by a brain surgeon. There are two types of cerebral hematomas: one that grows rapidly and the other that does not grow rapidly. We are developing the technique of artificial intelligence to determine whether the CT image includes the cerebral hematoma which leads to the rapid growth. This problem has various difficulties: the few positive cases in this classification problem of cerebral hematoma and the targeted hematoma has deformable object. Other difficulties include the imbalance classification, the covariate shift, the small data, and the spurious correlation problems. It is difficult with the plain CNN classification such as VGG. This paper proposes the joint learning of semantic segmentation and classification and evaluate the performance of this.



### Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes
- **Arxiv ID**: http://arxiv.org/abs/2103.17185v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2103.17185v1)
- **Published**: 2021-03-31 16:15:03+00:00
- **Updated**: 2021-03-31 16:15:03+00:00
- **Authors**: Dmytro Kotovenko, Matthias Wright, Arthur Heimbrecht, Björn Ommer
- **Comment**: Accepted at CVPR2021
- **Journal**: None
- **Summary**: There have been many successful implementations of neural style transfer in recent years. In most of these works, the stylization process is confined to the pixel domain. However, we argue that this representation is unnatural because paintings usually consist of brushstrokes rather than pixels. We propose a method to stylize images by optimizing parameterized brushstrokes instead of pixels and further introduce a simple differentiable rendering mechanism. Our approach significantly improves visual quality and enables additional control over the stylization process such as controlling the flow of brushstrokes through user input. We provide qualitative and quantitative evaluations that show the efficacy of the proposed parameterized representation.



### A Closer Look at Fourier Spectrum Discrepancies for CNN-generated Images Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.17195v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.17195v1)
- **Published**: 2021-03-31 16:24:54+00:00
- **Updated**: 2021-03-31 16:24:54+00:00
- **Authors**: Keshigeyan Chandrasegaran, Ngoc-Trung Tran, Ngai-Man Cheung
- **Comment**: CVPR 2021 Oral
- **Journal**: None
- **Summary**: CNN-based generative modelling has evolved to produce synthetic images indistinguishable from real images in the RGB pixel space. Recent works have observed that CNN-generated images share a systematic shortcoming in replicating high frequency Fourier spectrum decay attributes. Furthermore, these works have successfully exploited this systematic shortcoming to detect CNN-generated images reporting up to 99% accuracy across multiple state-of-the-art GAN models.   In this work, we investigate the validity of assertions claiming that CNN-generated images are unable to achieve high frequency spectral decay consistency. We meticulously construct a counterexample space of high frequency spectral decay consistent CNN-generated images emerging from our handcrafted experiments using DCGAN, LSGAN, WGAN-GP and StarGAN, where we empirically show that this frequency discrepancy can be avoided by a minor architecture change in the last upsampling operation. We subsequently use images from this counterexample space to successfully bypass the recently proposed forensics detector which leverages on high frequency Fourier spectrum decay attributes for CNN-generated image detection.   Through this study, we show that high frequency Fourier spectrum decay discrepancies are not inherent characteristics for existing CNN-based generative models--contrary to the belief of some existing work--, and such features are not robust to perform synthetic image detection. Our results prompt re-thinking of using high frequency Fourier spectrum decay attributes for CNN-generated image detection. Code and models are available at https://keshik6.github.io/Fourier-Discrepancies-CNN-Detection/



### GrooMeD-NMS: Grouped Mathematically Differentiable NMS for Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.17202v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.17202v1)
- **Published**: 2021-03-31 16:29:50+00:00
- **Updated**: 2021-03-31 16:29:50+00:00
- **Authors**: Abhinav Kumar, Garrick Brazil, Xiaoming Liu
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Modern 3D object detectors have immensely benefited from the end-to-end learning idea. However, most of them use a post-processing algorithm called Non-Maximal Suppression (NMS) only during inference. While there were attempts to include NMS in the training pipeline for tasks such as 2D object detection, they have been less widely adopted due to a non-mathematical expression of the NMS. In this paper, we present and integrate GrooMeD-NMS -- a novel Grouped Mathematically Differentiable NMS for monocular 3D object detection, such that the network is trained end-to-end with a loss on the boxes after NMS. We first formulate NMS as a matrix operation and then group and mask the boxes in an unsupervised manner to obtain a simple closed-form expression of the NMS. GrooMeD-NMS addresses the mismatch between training and inference pipelines and, therefore, forces the network to select the best 3D box in a differentiable manner. As a result, GrooMeD-NMS achieves state-of-the-art monocular 3D object detection results on the KITTI benchmark dataset performing comparably to monocular video-based methods. Code and models at https://github.com/abhi1kumar/groomed_nms



### Long-Term Temporally Consistent Unpaired Video Translation from Simulated Surgical 3D Data
- **Arxiv ID**: http://arxiv.org/abs/2103.17204v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17204v2)
- **Published**: 2021-03-31 16:31:26+00:00
- **Updated**: 2021-08-19 13:19:06+00:00
- **Authors**: Dominik Rivoir, Micha Pfeiffer, Reuben Docea, Fiona Kolbinger, Carina Riediger, Jürgen Weitz, Stefanie Speidel
- **Comment**: Accepted at the International Conference on Computer Vision (ICCV)
  2021
- **Journal**: None
- **Summary**: Research in unpaired video translation has mainly focused on short-term temporal consistency by conditioning on neighboring frames. However for transfer from simulated to photorealistic sequences, available information on the underlying geometry offers potential for achieving global consistency across views. We propose a novel approach which combines unpaired image translation with neural rendering to transfer simulated to photorealistic surgical abdominal scenes. By introducing global learnable textures and a lighting-invariant view-consistency loss, our method produces consistent translations of arbitrary views and thus enables long-term consistent video synthesis. We design and test our model to generate video sequences from minimally-invasive surgical abdominal scenes. Because labeled data is often limited in this domain, photorealistic data where ground truth information from the simulated domain is preserved is especially relevant. By extending existing image-based methods to view-consistent videos, we aim to impact the applicability of simulated training and evaluation environments for surgical applications. Code and data: http://opencas.dkfz.de/video-sim2real.



### An effective and friendly tool for seed image analysis
- **Arxiv ID**: http://arxiv.org/abs/2103.17213v2
- **DOI**: 10.1007/s00371-021-02333-w
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17213v2)
- **Published**: 2021-03-31 16:56:22+00:00
- **Updated**: 2021-07-23 10:34:12+00:00
- **Authors**: Andrea Loddo, Cecilia Di Ruberto, A. M. P. G. Vale, Mariano Ucchesu, J. M. Soares, Gianluigi Bacchetta
- **Comment**: None
- **Journal**: None
- **Summary**: Image analysis is an essential field for several topics in the life sciences, such as biology or botany. In particular, the analysis of seeds (e.g. fossil research) can provide significant information on their evolution, the history of agriculture, plant domestication and knowledge of diets in ancient times. This work aims to present software that performs image analysis for feature extraction and classification from images containing seeds through a novel and unique framework. In detail, we propose two plugins \emph{ImageJ}, one able to extract morphological, textual and colour features from seed images, and another to classify seeds into categories using the extracted features. The experimental results demonstrated the correctness and validity of both the extracted features and the classification predictions. The proposed tool is easily extendable to other fields of image analysis.



### Scale-aware Automatic Augmentation for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.17220v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17220v1)
- **Published**: 2021-03-31 17:11:14+00:00
- **Updated**: 2021-03-31 17:11:14+00:00
- **Authors**: Yukang Chen, Yanwei Li, Tao Kong, Lu Qi, Ruihang Chu, Lei Li, Jiaya Jia
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: We propose Scale-aware AutoAug to learn data augmentation policies for object detection. We define a new scale-aware search space, where both image- and box-level augmentations are designed for maintaining scale invariance. Upon this search space, we propose a new search metric, termed Pareto Scale Balance, to facilitate search with high efficiency. In experiments, Scale-aware AutoAug yields significant and consistent improvement on various object detectors (e.g., RetinaNet, Faster R-CNN, Mask R-CNN, and FCOS), even compared with strong multi-scale training baselines. Our searched augmentation policies are transferable to other datasets and box-level tasks beyond object detection (e.g., instance segmentation and keypoint estimation) to improve performance. The search cost is much less than previous automated augmentation approaches for object detection. It is notable that our searched policies have meaningful patterns, which intuitively provide valuable insight for human data augmentation design. Code and models will be available at https://github.com/Jia-Research-Lab/SA-AutoAug.



### Joint Deep Multi-Graph Matching and 3D Geometry Learning from Inhomogeneous 2D Image Collections
- **Arxiv ID**: http://arxiv.org/abs/2103.17229v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.17229v2)
- **Published**: 2021-03-31 17:25:36+00:00
- **Updated**: 2022-05-07 13:35:05+00:00
- **Authors**: Zhenzhang Ye, Tarun Yenamandra, Florian Bernard, Daniel Cremers
- **Comment**: Presented at the 36th AAAI conference on artificial intelligence,
  2022 (AAAI22)
- **Journal**: None
- **Summary**: Graph matching aims to establish correspondences between vertices of graphs such that both the node and edge attributes agree. Various learning-based methods were recently proposed for finding correspondences between image key points based on deep graph matching formulations. While these approaches mainly focus on learning node and edge attributes, they completely ignore the 3D geometry of the underlying 3D objects depicted in the 2D images. We fill this gap by proposing a trainable framework that takes advantage of graph neural networks for learning a deformable 3D geometry model from inhomogeneous image collections, i.e.,~a set of images that depict different instances of objects from the same category. Experimentally, we demonstrate that our method outperforms recent learning-based approaches for graph matching considering both accuracy and cycle-consistency error, while we in addition obtain the underlying 3D geometry of the objects depicted in the 2D images.



### Rainbow Memory: Continual Learning with a Memory of Diverse Samples
- **Arxiv ID**: http://arxiv.org/abs/2103.17230v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.17230v1)
- **Published**: 2021-03-31 17:28:29+00:00
- **Updated**: 2021-03-31 17:28:29+00:00
- **Authors**: Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, Jonghyun Choi
- **Comment**: Accepted paper at CVPR 2021
- **Journal**: None
- **Summary**: Continual learning is a realistic learning scenario for AI models. Prevalent scenario of continual learning, however, assumes disjoint sets of classes as tasks and is less realistic rather artificial. Instead, we focus on 'blurry' task boundary; where tasks shares classes and is more realistic and practical. To address such task, we argue the importance of diversity of samples in an episodic memory. To enhance the sample diversity in the memory, we propose a novel memory management strategy based on per-sample classification uncertainty and data augmentation, named Rainbow Memory (RM). With extensive empirical validations on MNIST, CIFAR10, CIFAR100, and ImageNet datasets, we show that the proposed method significantly improves the accuracy in blurry continual learning setups, outperforming state of the arts by large margins despite its simplicity. Code and data splits will be available in https://github.com/clovaai/rainbow-memory.



### FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.17235v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.17235v3)
- **Published**: 2021-03-31 17:34:20+00:00
- **Updated**: 2022-03-25 18:17:11+00:00
- **Authors**: Nikhil Kumar Tomar, Debesh Jha, Michael A. Riegler, Håvard D. Johansen, Dag Johansen, Jens Rittscher, Pål Halvorsen, Sharib Ali
- **Comment**: None
- **Journal**: IEEE Transactions on Neural Networks and Learning Systems, 2022
- **Summary**: The increase of available large clinical and experimental datasets has contributed to a substantial amount of important contributions in the area of biomedical image analysis. Image segmentation, which is crucial for any quantitative analysis, has especially attracted attention. Recent hardware advancement has led to the success of deep learning approaches. However, although deep learning models are being trained on large datasets, existing methods do not use the information from different learning epochs effectively. In this work, we leverage the information of each training epoch to prune the prediction maps of the subsequent epochs. We propose a novel architecture called feedback attention network (FANet) that unifies the previous epoch mask with the feature map of the current training epoch. The previous epoch mask is then used to provide a hard attention to the learned feature maps at different convolutional layers. The network also allows to rectify the predictions in an iterative fashion during the test time. We show that our proposed \textit{feedback attention} model provides a substantial improvement on most segmentation metrics tested on seven publicly available biomedical imaging datasets demonstrating the effectiveness of FANet. The source code is available at \url{https://github.com/nikhilroxtomar/FANet}.



### Going deeper with Image Transformers
- **Arxiv ID**: http://arxiv.org/abs/2103.17239v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17239v2)
- **Published**: 2021-03-31 17:37:32+00:00
- **Updated**: 2021-04-07 08:08:39+00:00
- **Authors**: Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, Hervé Jégou
- **Comment**: None
- **Journal**: None
- **Summary**: Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of image transformers has been little studied so far. In this work, we build and optimize deeper transformer networks for image classification. In particular, we investigate the interplay of architecture and optimization of such dedicated transformers. We make two transformers architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models whose performance does not saturate early with more depth, for instance we obtain 86.5% top-1 accuracy on Imagenet when training with no external data, we thus attain the current SOTA with less FLOPs and parameters. Moreover, our best model establishes the new state of the art on Imagenet with Reassessed labels and Imagenet-V2 / match frequency, in the setting with no additional training data. We share our code and models.



### Dogfight: Detecting Drones from Drones Videos
- **Arxiv ID**: http://arxiv.org/abs/2103.17242v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17242v2)
- **Published**: 2021-03-31 17:43:31+00:00
- **Updated**: 2021-04-09 18:19:00+00:00
- **Authors**: Muhammad Waseem Ashraf, Waqas Sultani, Mubarak Shah
- **Comment**: 10 pages, 10 figures, Accepted for CVPR 2021
- **Journal**: None
- **Summary**: As airborne vehicles are becoming more autonomous and ubiquitous, it has become vital to develop the capability to detect the objects in their surroundings. This paper attempts to address the problem of drones detection from other flying drones. The erratic movement of the source and target drones, small size, arbitrary shape, large intensity variations, and occlusion make this problem quite challenging. In this scenario, region-proposal based methods are not able to capture sufficient discriminative foreground-background information. Also, due to the extremely small size and complex motion of the source and target drones, feature aggregation based methods are unable to perform well. To handle this, instead of using region-proposal based methods, we propose to use a two-stage segmentation-based approach employing spatio-temporal attention cues. During the first stage, given the overlapping frame regions, detailed contextual information is captured over convolution feature maps using pyramid pooling. After that pixel and channel-wise attention is enforced on the feature maps to ensure accurate drone localization. In the second stage, first stage detections are verified and new probable drone locations are explored. To discover new drone locations, motion boundaries are used. This is followed by tracking candidate drone detections for a few frames, cuboid formation, extraction of the 3D convolution feature map, and drones detection within each cuboid. The proposed approach is evaluated on two publicly available drone detection datasets and outperforms several competitive baselines.



### StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery
- **Arxiv ID**: http://arxiv.org/abs/2103.17249v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.17249v1)
- **Published**: 2021-03-31 17:51:25+00:00
- **Updated**: 2021-03-31 17:51:25+00:00
- **Authors**: Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski
- **Comment**: 18 pages, 24 figures, code and video may be found here:
  https://github.com/orpatashnik/StyleCLIP
- **Journal**: None
- **Summary**: Inspired by the ability of StyleGAN to generate highly realistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of StyleGAN to manipulate generated and real images. However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation. In this work, we explore leveraging the power of recently introduced Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort. We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt. Next, we describe a latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable text-based manipulation. Finally, we present a method for mapping a text prompts to input-agnostic directions in StyleGAN's style space, enabling interactive text-driven image manipulation. Extensive results and comparisons demonstrate the effectiveness of our approaches.



### Learning by Aligning Videos in Time
- **Arxiv ID**: http://arxiv.org/abs/2103.17260v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17260v2)
- **Published**: 2021-03-31 17:55:52+00:00
- **Updated**: 2023-08-17 07:29:40+00:00
- **Authors**: Sanjay Haresh, Sateesh Kumar, Huseyin Coskun, Shahram Najam Syed, Andrey Konin, Muhammad Zeeshan Zia, Quoc-Huy Tran
- **Comment**: Presented at CVPR 2021
- **Journal**: None
- **Summary**: We present a self-supervised approach for learning video representations using temporal video alignment as a pretext task, while exploiting both frame-level and video-level information. We leverage a novel combination of temporal alignment loss and temporal regularization terms, which can be used as supervision signals for training an encoder network. Specifically, the temporal alignment loss (i.e., Soft-DTW) aims for the minimum cost for temporally aligning videos in the embedding space. However, optimizing solely for this term leads to trivial solutions, particularly, one where all frames get mapped to a small cluster in the embedding space. To overcome this problem, we propose a temporal regularization term (i.e., Contrastive-IDM) which encourages different frames to be mapped to different points in the embedding space. Extensive evaluations on various tasks, including action phase classification, action phase progression, and fine-grained frame retrieval, on three datasets, namely Pouring, Penn Action, and IKEA ASM, show superior performance of our approach over state-of-the-art methods for self-supervised representation learning from videos. In addition, our method provides significant performance gain where labeled data is lacking. Our code and labels are available on our research website: https://retrocausal.ai/research/



### Video-Specific Autoencoders for Exploring, Editing and Transmitting Videos
- **Arxiv ID**: http://arxiv.org/abs/2103.17261v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.17261v2)
- **Published**: 2021-03-31 17:56:13+00:00
- **Updated**: 2022-01-07 20:00:33+00:00
- **Authors**: Kevin Wang, Deva Ramanan, Aayush Bansal
- **Comment**: Project Page: https://www.cs.cmu.edu/~aayushb/Video-ViSA/
- **Journal**: None
- **Summary**: We study video-specific autoencoders that allow a human user to explore, edit, and efficiently transmit videos. Prior work has independently looked at these problems (and sub-problems) and proposed different formulations. In this work, we train a simple autoencoder (from scratch) on multiple frames of a specific video. We observe: (1) latent codes learned by a video-specific autoencoder capture spatial and temporal properties of that video; and (2) autoencoders can project out-of-sample inputs onto the video-specific manifold. These two properties allow us to explore, edit, and efficiently transmit a video using one learned representation. For e.g., linear operations on latent codes allow users to visualize the contents of a video. Associating latent codes of a video and manifold projection enables users to make desired edits. Interpolating latent codes and manifold projection allows the transmission of sparse low-res frames over a network.



### Rethinking Self-supervised Correspondence Learning: A Video Frame-level Similarity Perspective
- **Arxiv ID**: http://arxiv.org/abs/2103.17263v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17263v5)
- **Published**: 2021-03-31 17:56:35+00:00
- **Updated**: 2021-10-14 01:44:48+00:00
- **Authors**: Jiarui Xu, Xiaolong Wang
- **Comment**: ICCV 2021 (oral). Project page and code: https://jerryxu.net/VFS
- **Journal**: None
- **Summary**: Learning a good representation for space-time correspondence is the key for various computer vision tasks, including tracking object bounding boxes and performing video object pixel segmentation. To learn generalizable representation for correspondence in large-scale, a variety of self-supervised pretext tasks are proposed to explicitly perform object-level or patch-level similarity learning. Instead of following the previous literature, we propose to learn correspondence using Video Frame-level Similarity (VFS) learning, i.e, simply learning from comparing video frames. Our work is inspired by the recent success in image-level contrastive learning and similarity learning for visual recognition. Our hypothesis is that if the representation is good for recognition, it requires the convolutional features to find correspondence between similar objects or parts. Our experiments show surprising results that VFS surpasses state-of-the-art self-supervised approaches for both OTB visual object tracking and DAVIS video object segmentation. We perform detailed analysis on what matters in VFS and reveals new properties on image and frame level similarity learning. Project page with code is available at https://jerryxu.net/VFS



### Human POSEitioning System (HPS): 3D Human Pose Estimation and Self-localization in Large Scenes from Body-Mounted Sensors
- **Arxiv ID**: http://arxiv.org/abs/2103.17265v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17265v1)
- **Published**: 2021-03-31 17:58:31+00:00
- **Updated**: 2021-03-31 17:58:31+00:00
- **Authors**: Vladimir Guzov, Aymen Mir, Torsten Sattler, Gerard Pons-Moll
- **Comment**: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR)
- **Journal**: None
- **Summary**: We introduce (HPS) Human POSEitioning System, a method to recover the full 3D pose of a human registered with a 3D scan of the surrounding environment using wearable sensors. Using IMUs attached at the body limbs and a head mounted camera looking outwards, HPS fuses camera based self-localization with IMU-based human body tracking. The former provides drift-free but noisy position and orientation estimates while the latter is accurate in the short-term but subject to drift over longer periods of time. We show that our optimization-based integration exploits the benefits of the two, resulting in pose accuracy free of drift. Furthermore, we integrate 3D scene constraints into our optimization, such as foot contact with the ground, resulting in physically plausible motion. HPS complements more common third-person-based 3D pose estimation methods. It allows capturing larger recording volumes and longer periods of motion, and could be used for VR/AR applications where humans interact with the scene without requiring direct line of sight with an external camera, or to train agents that navigate and interact with the environment based on first-person visual input, like real humans. With HPS, we recorded a dataset of humans interacting with large 3D scenes (300-1000 sq.m) consisting of 7 subjects and more than 3 hours of diverse motion. The dataset, code and video will be available on the project page: http://virtualhumans.mpi-inf.mpg.de/hps/ .



### Semi-supervised Synthesis of High-Resolution Editable Textures for 3D Humans
- **Arxiv ID**: http://arxiv.org/abs/2103.17266v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17266v1)
- **Published**: 2021-03-31 17:58:34+00:00
- **Updated**: 2021-03-31 17:58:34+00:00
- **Authors**: Bindita Chaudhuri, Nikolaos Sarafianos, Linda Shapiro, Tony Tung
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: We introduce a novel approach to generate diverse high fidelity texture maps for 3D human meshes in a semi-supervised setup. Given a segmentation mask defining the layout of the semantic regions in the texture map, our network generates high-resolution textures with a variety of styles, that are then used for rendering purposes. To accomplish this task, we propose a Region-adaptive Adversarial Variational AutoEncoder (ReAVAE) that learns the probability distribution of the style of each region individually so that the style of the generated texture can be controlled by sampling from the region-specific distributions. In addition, we introduce a data generation technique to augment our training set with data lifted from single-view RGB inputs. Our training strategy allows the mixing of reference image styles with arbitrary styles for different regions, a property which can be valuable for virtual try-on AR/VR applications. Experimental results show that our method synthesizes better texture maps compared to prior work while enabling independent layout and style controllability.



### Bit-Mixer: Mixed-precision networks with runtime bit-width selection
- **Arxiv ID**: http://arxiv.org/abs/2103.17267v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.17267v1)
- **Published**: 2021-03-31 17:58:47+00:00
- **Updated**: 2021-03-31 17:58:47+00:00
- **Authors**: Adrian Bulat, Georgios Tzimiropoulos
- **Comment**: None
- **Journal**: None
- **Summary**: Mixed-precision networks allow for a variable bit-width quantization for every layer in the network. A major limitation of existing work is that the bit-width for each layer must be predefined during training time. This allows little flexibility if the characteristics of the device on which the network is deployed change during runtime. In this work, we propose Bit-Mixer, the very first method to train a meta-quantized network where during test time any layer can change its bid-width without affecting at all the overall network's ability for highly accurate inference. To this end, we make 2 key contributions: (a) Transitional Batch-Norms, and (b) a 3-stage optimization process which is shown capable of training such a network. We show that our method can result in mixed precision networks that exhibit the desirable flexibility properties for on-device deployment without compromising accuracy. Code will be made available.



### CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2103.17269v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.17269v1)
- **Published**: 2021-03-31 17:59:24+00:00
- **Updated**: 2021-03-31 17:59:24+00:00
- **Authors**: Michael Niemeyer, Andreas Geiger
- **Comment**: None
- **Journal**: None
- **Summary**: Tremendous progress in deep generative models has led to photorealistic image synthesis. While achieving compelling results, most approaches operate in the two-dimensional image domain, ignoring the three-dimensional nature of our world. Several recent works therefore propose generative models which are 3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably to the image plane. This leads to impressive 3D consistency, but incorporating such a bias comes at a price: the camera needs to be modeled as well. Current approaches assume fixed intrinsics and a predefined prior over camera pose ranges. As a result, parameter tuning is typically required for real-world data, and results degrade if the data distribution is not matched. Our key hypothesis is that learning a camera generator jointly with the image generator leads to a more principled approach to 3D-aware image synthesis. Further, we propose to decompose the scene into a background and foreground model, leading to more efficient and disentangled scene representations. While training from raw, unposed image collections, we learn a 3D- and camera-aware generative model which faithfully recovers not only the image but also the camera data distribution. At test time, our model generates images with explicit control over the camera as well as the shape and appearance of the scene.



### DCVNet: Dilated Cost Volume Networks for Fast Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2103.17271v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.17271v1)
- **Published**: 2021-03-31 17:59:31+00:00
- **Updated**: 2021-03-31 17:59:31+00:00
- **Authors**: Huaizu Jiang, Erik Learned-Miller
- **Comment**: None
- **Journal**: None
- **Summary**: The cost volume, capturing the similarity of possible correspondences across two input images, is a key ingredient in state-of-the-art optical flow approaches. When sampling for correspondences to build the cost volume, a large neighborhood radius is required to deal with large displacements, introducing a significant computational burden. To address this, a sequential strategy is usually adopted, where correspondence sampling in a local neighborhood with a small radius suffices. However, such sequential approaches, instantiated by either a pyramid structure over a deep neural network's feature hierarchy or by a recurrent neural network, are slow due to the inherent need for sequential processing of cost volumes. In this paper, we propose dilated cost volumes to capture small and large displacements simultaneously, allowing optical flow estimation without the need for the sequential estimation strategy. To process the cost volume to get pixel-wise optical flow, existing approaches employ 2D or separable 4D convolutions, which we show either suffer from high GPU memory consumption, inferior accuracy, or large model size. Therefore, we propose using 3D convolutions for cost volume filtering to address these issues. By combining the dilated cost volumes and 3D convolutions, our proposed model DCVNet not only exhibits real-time inference (71 fps on a mid-end 1080ti GPU) but is also compact and obtains comparable accuracy to existing approaches.



### Efficient Large-Scale Face Clustering Using an Online Mixture of Gaussians
- **Arxiv ID**: http://arxiv.org/abs/2103.17272v1
- **DOI**: 10.1016/j.engappai.2022.105079
- **Categories**: **cs.CV**, cs.LG, I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/2103.17272v1)
- **Published**: 2021-03-31 17:59:38+00:00
- **Updated**: 2021-03-31 17:59:38+00:00
- **Authors**: David Montero, Naiara Aginako, Basilio Sierra, Marcos Nieto
- **Comment**: 14 pages, 11 figures
- **Journal**: None
- **Summary**: In this work, we address the problem of large-scale online face clustering: given a continuous stream of unknown faces, create a database grouping the incoming faces by their identity. The database must be updated every time a new face arrives. In addition, the solution must be efficient, accurate and scalable. For this purpose, we present an online gaussian mixture-based clustering method (OGMC). The key idea of this method is the proposal that an identity can be represented by more than just one distribution or cluster. Using feature vectors (f-vectors) extracted from the incoming faces, OGMC generates clusters that may be connected to others depending on their proximity and their robustness. Every time a cluster is updated with a new sample, its connections are also updated. With this approach, we reduce the dependency of the clustering process on the order and the size of the incoming data and we are able to deal with complex data distributions. Experimental results show that the proposed approach outperforms state-of-the-art clustering methods on large-scale face clustering benchmarks not only in accuracy, but also in efficiency and scalability.



### Fast and Accurate Emulation of the SDO/HMI Stokes Inversion with Uncertainty Quantification
- **Arxiv ID**: http://arxiv.org/abs/2103.17273v2
- **DOI**: None
- **Categories**: **astro-ph.SR**, astro-ph.IM, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.17273v2)
- **Published**: 2021-03-31 17:59:40+00:00
- **Updated**: 2021-08-27 17:59:56+00:00
- **Authors**: Richard E. L. Higgins, David F. Fouhey, Dichang Zhang, Spiro K. Antiochos, Graham Barnes, J. Todd Hoeksema, K. D. Leka, Yang Liu, Peter W. Schuck, Tamas I. Gombosi
- **Comment**: None
- **Journal**: None
- **Summary**: The Helioseismic and Magnetic Imager (HMI) onboard NASA's Solar Dynamics Observatory (SDO) produces estimates of the photospheric magnetic field which are a critical input to many space weather modelling and forecasting systems. The magnetogram products produced by HMI and its analysis pipeline are the result of a per-pixel optimization that estimates solar atmospheric parameters and minimizes disagreement between a synthesized and observed Stokes vector. In this paper, we introduce a deep learning-based approach that can emulate the existing HMI pipeline results two orders of magnitude faster than the current pipeline algorithms. Our system is a U-Net trained on input Stokes vectors and their accompanying optimization-based VFISV inversions. We demonstrate that our system, once trained, can produce high-fidelity estimates of the magnetic field and kinematic and thermodynamic parameters while also producing meaningful confidence intervals. We additionally show that despite penalizing only per-pixel loss terms, our system is able to faithfully reproduce known systematic oscillations in full-disk statistics produced by the pipeline. This emulation system could serve as an initialization for the full Stokes inversion or as an ultra-fast proxy inversion. This work is part of the NASA Heliophysics DRIVE Science Center (SOLSTICE) at the University of Michigan, under grant NASA 80NSSC20K0600E, and has been open sourced.



### RetrievalFuse: Neural 3D Scene Reconstruction with a Database
- **Arxiv ID**: http://arxiv.org/abs/2104.00024v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00024v2)
- **Published**: 2021-03-31 18:00:09+00:00
- **Updated**: 2021-08-10 09:16:30+00:00
- **Authors**: Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan, Matthias Nießner, Angela Dai
- **Comment**: Project Page: https://nihalsid.github.io/retrieval-fuse/
- **Journal**: None
- **Summary**: 3D reconstruction of large scenes is a challenging problem due to the high-complexity nature of the solution space, in particular for generative neural networks. In contrast to traditional generative learned models which encode the full generative process into a neural network and can struggle with maintaining local details at the scene level, we introduce a new method that directly leverages scene geometry from the training database. First, we learn to synthesize an initial estimate for a 3D scene, constructed by retrieving a top-k set of volumetric chunks from the scene database. These candidates are then refined to a final scene generation with an attention-based refinement that can effectively select the most consistent set of geometry from the candidates and combine them together to create an output scene, facilitating transfer of coherent structures and local detail from train scene geometry. We demonstrate our neural scene reconstruction with a database for the tasks of 3D super resolution and surface reconstruction from sparse point clouds, showing that our approach enables generation of more coherent, accurate 3D scenes, improving on average by over 8% in IoU over state-of-the-art scene reconstruction.



### NetAdaptV2: Efficient Neural Architecture Search with Fast Super-Network Training and Architecture Optimization
- **Arxiv ID**: http://arxiv.org/abs/2104.00031v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.00031v1)
- **Published**: 2021-03-31 18:03:46+00:00
- **Updated**: 2021-03-31 18:03:46+00:00
- **Authors**: Tien-Ju Yang, Yi-Lun Liao, Vivienne Sze
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Neural architecture search (NAS) typically consists of three main steps: training a super-network, training and evaluating sampled deep neural networks (DNNs), and training the discovered DNN. Most of the existing efforts speed up some steps at the cost of a significant slowdown of other steps or sacrificing the support of non-differentiable search metrics. The unbalanced reduction in the time spent per step limits the total search time reduction, and the inability to support non-differentiable search metrics limits the performance of discovered DNNs.   In this paper, we present NetAdaptV2 with three innovations to better balance the time spent for each step while supporting non-differentiable search metrics. First, we propose channel-level bypass connections that merge network depth and layer width into a single search dimension to reduce the time for training and evaluating sampled DNNs. Second, ordered dropout is proposed to train multiple DNNs in a single forward-backward pass to decrease the time for training a super-network. Third, we propose the multi-layer coordinate descent optimizer that considers the interplay of multiple layers in each iteration of optimization to improve the performance of discovered DNNs while supporting non-differentiable search metrics. With these innovations, NetAdaptV2 reduces the total search time by up to $5.8\times$ on ImageNet and $2.4\times$ on NYU Depth V2, respectively, and discovers DNNs with better accuracy-latency/accuracy-MAC trade-offs than state-of-the-art NAS works. Moreover, the discovered DNN outperforms NAS-discovered MobileNetV3 by 1.8% higher top-1 accuracy with the same latency. The project website is http://netadapt.mit.edu.



### Strengthening the Training of Convolutional Neural Networks By Using Walsh Matrix
- **Arxiv ID**: http://arxiv.org/abs/2104.00035v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.00035v1)
- **Published**: 2021-03-31 18:06:11+00:00
- **Updated**: 2021-03-31 18:06:11+00:00
- **Authors**: Tamer Ölmez, Zümray Dokur
- **Comment**: Keyword: Deep neural networks, Convolutional neural network, Pattern
  recognition, Training deep neural network, Classification. arXiv admin note:
  substantial text overlap with arXiv:2103.10977
- **Journal**: None
- **Summary**: DNN structures are continuously developing and achieving high performances in classification problems. Also, it is observed that success rates obtained with DNNs are higher than those obtained with traditional neural networks. In addition, one of the advantages of DNNs is that there is no need to spend an extra effort to determine the features; the CNN automatically extracts the features from the dataset during the training. Besides their benefits, the DNNs have the following three major drawbacks among the others: (i) Researchers have struggled with over-fitting and under-fitting issues in the training of DNNs, (ii) determination of even a coarse structure for the DNN may take days, and (iii) most of the time, the proposed network structure is too large to be too bulky to be used in real time applications. We have modified the training and structure of DNN to increase the classification performance, to decrease the number of nodes in the structure, and to be used with less number of hyper parameters. A minimum distance network (MDN) following the last layer of the convolutional neural network (CNN) is used as the classifier instead of a fully connected neural network (FCNN). In order to strengthen the training of the CNN, we suggest employing Walsh function. We tested the performances of the proposed DNN (named as DivFE) on the classification of ECG, EEG, heart sound, detection pneumonia in X-ray chest images, detection of BGA solder defects, and patterns of benchmark datasets (MNIST, IRIS, CIFAR10 and CIFAR20). In different areas, it has been observed that a higher classification performance was obtained by using the DivFE with less number of nodes.



### Passive Inter-Photon Imaging
- **Arxiv ID**: http://arxiv.org/abs/2104.00059v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.00059v2)
- **Published**: 2021-03-31 18:44:52+00:00
- **Updated**: 2021-04-11 02:15:51+00:00
- **Authors**: Atul Ingle, Trevor Seets, Mauro Buttafava, Shantanu Gupta, Alberto Tosi, Mohit Gupta, Andreas Velten
- **Comment**: Accepted to CVPR 2021 as an oral presentation
- **Journal**: None
- **Summary**: Digital camera pixels measure image intensities by converting incident light energy into an analog electrical current, and then digitizing it into a fixed-width binary representation. This direct measurement method, while conceptually simple, suffers from limited dynamic range and poor performance under extreme illumination -- electronic noise dominates under low illumination, and pixel full-well capacity results in saturation under bright illumination. We propose a novel intensity cue based on measuring inter-photon timing, defined as the time delay between detection of successive photons. Based on the statistics of inter-photon times measured by a time-resolved single-photon sensor, we develop theory and algorithms for a scene brightness estimator which works over extreme dynamic range; we experimentally demonstrate imaging scenes with a dynamic range of over ten million to one. The proposed techniques, aided by the emergence of single-photon sensors such as single-photon avalanche diodes (SPADs) with picosecond timing resolution, will have implications for a wide range of imaging applications: robotics, consumer photography, astronomy, microscopy and biomedical imaging.



### FAPIS: A Few-shot Anchor-free Part-based Instance Segmenter
- **Arxiv ID**: http://arxiv.org/abs/2104.00073v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00073v1)
- **Published**: 2021-03-31 19:09:43+00:00
- **Updated**: 2021-03-31 19:09:43+00:00
- **Authors**: Khoi Nguyen, Sinisa Todorovic
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: This paper is about few-shot instance segmentation, where training and test image sets do not share the same object classes. We specify and evaluate a new few-shot anchor-free part-based instance segmenter FAPIS. Our key novelty is in explicit modeling of latent object parts shared across training object classes, which is expected to facilitate our few-shot learning on new classes in testing. We specify a new anchor-free object detector aimed at scoring and regressing locations of foreground bounding boxes, as well as estimating relative importance of latent parts within each box. Also, we specify a new network for delineating and weighting latent parts for the final instance segmentation within every detected bounding box. Our evaluation on the benchmark COCO-20i dataset demonstrates that we significantly outperform the state of the art.



### DeepBlur: A Simple and Effective Method for Natural Image Obfuscation
- **Arxiv ID**: http://arxiv.org/abs/2104.02655v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2104.02655v1)
- **Published**: 2021-03-31 19:31:26+00:00
- **Updated**: 2021-03-31 19:31:26+00:00
- **Authors**: Tao Li, Min Soo Choi
- **Comment**: None
- **Journal**: None
- **Summary**: There is a growing privacy concern due to the popularity of social media and surveillance systems, along with advances in face recognition software. However, established image obfuscation techniques are either vulnerable to re-identification attacks by human or deep learning models, insufficient in preserving image fidelity, or too computationally intensive to be practical. To tackle these issues, we present DeepBlur, a simple yet effective method for image obfuscation by blurring in the latent space of an unconditionally pre-trained generative model that is able to synthesize photo-realistic facial images. We compare it with existing methods by efficiency and image quality, and evaluate against both state-of-the-art deep learning models and industrial products (e.g., Face++, Microsoft face service). Experiments show that our method produces high quality outputs and is the strongest defense for most test cases.



### Hierarchical Road Topology Learning for Urban Map-less Driving
- **Arxiv ID**: http://arxiv.org/abs/2104.00084v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.00084v1)
- **Published**: 2021-03-31 19:51:25+00:00
- **Updated**: 2021-03-31 19:51:25+00:00
- **Authors**: Li Zhang, Faezeh Tafazzoli, Gunther Krehl, Runsheng Xu, Timo Rehfeld, Manuel Schier, Arunava Seal
- **Comment**: None
- **Journal**: None
- **Summary**: The majority of current approaches in autonomous driving rely on High-Definition (HD) maps which detail the road geometry and surrounding area. Yet, this reliance is one of the obstacles to mass deployment of autonomous vehicles due to poor scalability of such prior maps. In this paper, we tackle the problem of online road map extraction via leveraging the sensory system aboard the vehicle itself. To this end, we design a structured model where a graph representation of the road network is generated in a hierarchical fashion within a fully convolutional network. The method is able to handle complex road topology and does not require a user in the loop.



### A comparative evaluation of learned feature descriptors on hybrid monocular visual SLAM methods
- **Arxiv ID**: http://arxiv.org/abs/2104.00085v1
- **DOI**: 10.1109/LARS/SBR/WRE51543.2020.9307033
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.00085v1)
- **Published**: 2021-03-31 19:56:32+00:00
- **Updated**: 2021-03-31 19:56:32+00:00
- **Authors**: Hudson M. S. Bruno, Esther L. Colombini
- **Comment**: 6 pages, Published in 2020 Latin American Robotics Symposium (LARS)
- **Journal**: 2020 Latin American Robotics Symposium (LARS), Natal, Brazil,
  2020, pp. 1-6
- **Summary**: Classical Visual Simultaneous Localization and Mapping (VSLAM) algorithms can be easily induced to fail when either the robot's motion or the environment is too challenging. The use of Deep Neural Networks to enhance VSLAM algorithms has recently achieved promising results, which we call hybrid methods. In this paper, we compare the performance of hybrid monocular VSLAM methods with different learned feature descriptors. To this end, we propose a set of experiments to evaluate the robustness of the algorithms under different environments, camera motion, and camera sensor noise. Experiments conducted on KITTI and Euroc MAV datasets confirm that learned feature descriptors can create more robust VSLAM systems.



### LIFT-SLAM: a deep-learning feature-based monocular visual SLAM method
- **Arxiv ID**: http://arxiv.org/abs/2104.00099v2
- **DOI**: 10.1016/j.neucom.2021.05.027
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.00099v2)
- **Published**: 2021-03-31 20:35:10+00:00
- **Updated**: 2021-06-01 13:44:22+00:00
- **Authors**: Hudson M. S. Bruno, Esther L. Colombini
- **Comment**: 30 pages, Published in Neurocomputing
- **Journal**: None
- **Summary**: The Simultaneous Localization and Mapping (SLAM) problem addresses the possibility of a robot to localize itself in an unknown environment and simultaneously build a consistent map of this environment. Recently, cameras have been successfully used to get the environment's features to perform SLAM, which is referred to as visual SLAM (VSLAM). However, classical VSLAM algorithms can be easily induced to fail when either the motion of the robot or the environment is too challenging. Although new approaches based on Deep Neural Networks (DNNs) have achieved promising results in VSLAM, they still are unable to outperform traditional methods. To leverage the robustness of deep learning to enhance traditional VSLAM systems, we propose to combine the potential of deep learning-based feature descriptors with the traditional geometry-based VSLAM, building a new VSLAM system called LIFT-SLAM. Experiments conducted on KITTI and Euroc datasets show that deep learning can be used to improve the performance of traditional VSLAM systems, as the proposed approach was able to achieve results comparable to the state-of-the-art while being robust to sensorial noise. We enhance the proposed VSLAM pipeline by avoiding parameter tuning for specific datasets with an adaptive approach while evaluating how transfer learning can affect the quality of the features extracted.



### MR Slice Profile Estimation by Learning to Match Internal Patch Distributions
- **Arxiv ID**: http://arxiv.org/abs/2104.00100v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2104.00100v1)
- **Published**: 2021-03-31 20:35:44+00:00
- **Updated**: 2021-03-31 20:35:44+00:00
- **Authors**: Shuo Han, Samuel Remedios, Aaron Carass, Michael Schär, Jerry L. Prince
- **Comment**: 12 pages, 6 figures, accepted by Information Processing in Medical
  Imaging (IPMI) 2021
- **Journal**: None
- **Summary**: To super-resolve the through-plane direction of a multi-slice 2D magnetic resonance (MR) image, its slice selection profile can be used as the degeneration model from high resolution (HR) to low resolution (LR) to create paired data when training a supervised algorithm. Existing super-resolution algorithms make assumptions about the slice selection profile since it is not readily known for a given image. In this work, we estimate a slice selection profile given a specific image by learning to match its internal patch distributions. Specifically, we assume that after applying the correct slice selection profile, the image patch distribution along HR in-plane directions should match the distribution along the LR through-plane direction. Therefore, we incorporate the estimation of a slice selection profile as part of learning a generator in a generative adversarial network (GAN). In this way, the slice selection profile can be learned without any external data. Our algorithm was tested using simulations from isotropic MR images, incorporated in a through-plane super-resolution algorithm to demonstrate its benefits, and also used as a tool to measure image resolution. Our code is at https://github.com/shuohan/espreso2.



### Analysis on Image Set Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2104.00107v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.00107v1)
- **Published**: 2021-03-31 20:47:32+00:00
- **Updated**: 2021-03-31 20:47:32+00:00
- **Authors**: Abhinav Khattar, Aviral Joshi, Har Simrat Singh, Pulkit Goel, Rohit Prakash Barnwal
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the challenge of Visual Question Answering in multi-image setting for the ISVQA dataset. Traditional VQA tasks have focused on a single-image setting where the target answer is generated from a single image. Image set VQA, however, comprises of a set of images and requires finding connection between images, relate the objects across images based on these connections and generate a unified answer. In this report, we work with 4 approaches in a bid to improve the performance on the task. We analyse and compare our results with three baseline models - LXMERT, HME-VideoQA and VisualBERT - and show that our approaches can provide a slight improvement over the baselines. In specific, we try to improve on the spatial awareness of the model and help the model identify color using enhanced pre-training, reduce language dependence using adversarial regularization, and improve counting using regression loss and graph based deduplication. We further delve into an in-depth analysis on the language bias in the ISVQA dataset and show how models trained on ISVQA implicitly learn to associate language more strongly with the final answer.



### Drowsiness Detection Based On Driver Temporal Behavior Using a New Developed Dataset
- **Arxiv ID**: http://arxiv.org/abs/2104.00125v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00125v1)
- **Published**: 2021-03-31 21:15:29+00:00
- **Updated**: 2021-03-31 21:15:29+00:00
- **Authors**: Farnoosh Faraji, Faraz Lotfi, Javad Khorramdel, Ali Najafi, Ali Ghaffari
- **Comment**: None
- **Journal**: None
- **Summary**: Driver drowsiness detection has been the subject of many researches in the past few decades and various methods have been developed to detect it. In this study, as an image-based approach with adequate accuracy, along with the expedite process, we applied YOLOv3 (You Look Only Once-version3) CNN (Convolutional Neural Network) for extracting facial features automatically. Then, LSTM (Long-Short Term Memory) neural network is employed to learn driver temporal behaviors including yawning and blinking time period as well as sequence classification. To train YOLOv3, we utilized our collected dataset alongside the transfer learning method. Moreover, the dataset for the LSTM training process is produced by the mentioned CNN and is formatted as a two-dimensional sequence comprised of eye blinking and yawning time durations. The developed dataset considers both disturbances such as illumination and drivers' head posture. To have real-time experiments a multi-thread framework is developed to run both CNN and LSTM in parallel. Finally, results indicate the hybrid of CNN and LSTM ability in drowsiness detection and the effectiveness of the proposed method.



### Rapid quantification of COVID-19 pneumonia burden from computed tomography with convolutional LSTM networks
- **Arxiv ID**: http://arxiv.org/abs/2104.00138v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2104.00138v3)
- **Published**: 2021-03-31 22:09:14+00:00
- **Updated**: 2021-07-17 00:14:16+00:00
- **Authors**: Kajetan Grodecki, Aditya Killekar, Andrew Lin, Sebastien Cadet, Priscilla McElhinney, Aryabod Razipour, Cato Chan, Barry D. Pressman, Peter Julien, Judit Simon, Pal Maurovich-Horvat, Nicola Gaibazzi, Udit Thakur, Elisabetta Mancini, Cecilia Agalbato, Jiro Munechika, Hidenari Matsumoto, Roberto Menè, Gianfranco Parati, Franco Cernigliaro, Nitesh Nerlekar, Camilla Torlasco, Gianluca Pontone, Damini Dey, Piotr J. Slomka
- **Comment**: Fixed some typing mistakes in v2. No other results changed
- **Journal**: None
- **Summary**: Quantitative lung measures derived from computed tomography (CT) have been demonstrated to improve prognostication in coronavirus disease (COVID-19) patients, but are not part of the clinical routine since required manual segmentation of lung lesions is prohibitively time-consuming. We propose a new fully automated deep learning framework for rapid quantification and differentiation between lung lesions in COVID-19 pneumonia from both contrast and non-contrast CT images using convolutional Long Short-Term Memory (ConvLSTM) networks. Utilizing the expert annotations, model training was performed 5 times with separate hold-out sets using 5-fold cross-validation to segment ground-glass opacity and high opacity (including consolidation and pleural effusion). The performance of the method was evaluated on CT data sets from 197 patients with positive reverse transcription polymerase chain reaction test result for SARS-CoV-2. Strong agreement between expert manual and automatic segmentation was obtained for lung lesions with a Dice score coefficient of 0.876 $\pm$ 0.005; excellent correlations of 0.978 and 0.981 for ground-glass opacity and high opacity volumes. In the external validation set of 67 patients, there was dice score coefficient of 0.767 $\pm$ 0.009 as well as excellent correlations of 0.989 and 0.996 for ground-glass opacity and high opacity volumes. Computations for a CT scan comprising 120 slices were performed under 2 seconds on a personal computer equipped with NVIDIA Titan RTX graphics processing unit. Therefore, our deep learning-based method allows rapid fully-automated quantitative measurement of pneumonia burden from CT and may generate results with an accuracy similar to the expert readers.



### Adversarial Heart Attack: Neural Networks Fooled to Segment Heart Symbols in Chest X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2104.00139v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.00139v2)
- **Published**: 2021-03-31 22:20:59+00:00
- **Updated**: 2021-04-07 21:20:07+00:00
- **Authors**: Gerda Bortsova, Florian Dubost, Laurens Hogeweg, Ioannis Katramados, Marleen de Bruijne
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial attacks consist in maliciously changing the input data to mislead the predictions of automated decision systems and are potentially a serious threat for automated medical image analysis. Previous studies have shown that it is possible to adversarially manipulate automated segmentations produced by neural networks in a targeted manner in the white-box attack setting. In this article, we studied the effectiveness of adversarial attacks in targeted modification of segmentations of anatomical structures in chest X-rays. Firstly, we experimented with using anatomically implausible shapes as targets for adversarial manipulation. We showed that, by adding almost imperceptible noise to the image, we can reliably force state-of-the-art neural networks to segment the heart as a heart symbol instead of its real anatomical shape. Moreover, such heart-shaping attack did not appear to require higher adversarial noise level than an untargeted attack based the same attack method. Secondly, we attempted to explore the limits of adversarial manipulation of segmentations. For that, we assessed the effectiveness of shrinking and enlarging segmentation contours for the three anatomical structures. We observed that adversarially extending segmentations of structures into regions with intensity and texture uncharacteristic for them presented a challenge to our attacks, as well as, in some cases, changing segmentations in ways that conflict with class adjacency priors learned by the target network. Additionally, we evaluated performances of the untargeted attacks and targeted heart attacks in the black-box attack scenario, using a surrogate network trained on a different subset of images. In both cases, the attacks were substantially less effective. We believe these findings bring novel insights into the current capabilities and limits of adversarial attacks for semantic segmentation.



### Full Surround Monodepth from Multiple Cameras
- **Arxiv ID**: http://arxiv.org/abs/2104.00152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00152v1)
- **Published**: 2021-03-31 22:52:04+00:00
- **Updated**: 2021-03-31 22:52:04+00:00
- **Authors**: Vitor Guizilini, Igor Vasiljevic, Rares Ambrus, Greg Shakhnarovich, Adrien Gaidon
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised monocular depth and ego-motion estimation is a promising approach to replace or supplement expensive depth sensors such as LiDAR for robotics applications like autonomous driving. However, most research in this area focuses on a single monocular camera or stereo pairs that cover only a fraction of the scene around the vehicle. In this work, we extend monocular self-supervised depth and ego-motion estimation to large-baseline multi-camera rigs. Using generalized spatio-temporal contexts, pose consistency constraints, and carefully designed photometric loss masking, we learn a single network generating dense, consistent, and scale-aware point clouds that cover the same full surround 360 degree field of view as a typical LiDAR scanner. We also propose a new scale-consistent evaluation metric more suitable to multi-camera settings. Experiments on two challenging benchmarks illustrate the benefits of our approach over strong baselines.



### Scalable Visual Attribute Extraction through Hidden Layers of a Residual ConvNet
- **Arxiv ID**: http://arxiv.org/abs/2104.00161v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.00161v1)
- **Published**: 2021-03-31 23:39:20+00:00
- **Updated**: 2021-03-31 23:39:20+00:00
- **Authors**: Andres Baloian, Nils Murrugarra-Llerena, Jose M. Saavedra
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Visual attributes play an essential role in real applications based on image retrieval. For instance, the extraction of attributes from images allows an eCommerce search engine to produce retrieval results with higher precision. The traditional manner to build an attribute extractor is by training a convnet-based classifier with a fixed number of classes. However, this approach does not scale for real applications where the number of attributes changes frequently. Therefore in this work, we propose an approach for extracting visual attributes from images, leveraging the learned capability of the hidden layers of a general convolutional network to discriminate among different visual features. We run experiments with a resnet-50 trained on Imagenet, on which we evaluate the output of its different blocks to discriminate between colors and textures. Our results show that the second block of the resnet is appropriate for discriminating colors, while the fourth block can be used for textures. In both cases, the achieved accuracy of attribute classification is superior to 93%. We also show that the proposed embeddings form local structures in the underlying feature space, which makes it possible to apply reduction techniques like UMAP, maintaining high accuracy and widely reducing the size of the feature space.



