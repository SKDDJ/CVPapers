# Arxiv Papers in cs.CV on 2021-03-17
### Semi-Supervised Learning for Eye Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.09369v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09369v1)
- **Published**: 2021-03-17 00:05:19+00:00
- **Updated**: 2021-03-17 00:05:19+00:00
- **Authors**: Aayush K. Chaudhary, Prashnna K. Gyawali, Linwei Wang, Jeff B. Pelz
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in appearance-based models have shown improved eye tracking performance in difficult scenarios like occlusion due to eyelashes, eyelids or camera placement, and environmental reflections on the cornea and glasses. The key reason for the improvement is the accurate and robust identification of eye parts (pupil, iris, and sclera regions). The improved accuracy often comes at the cost of labeling an enormous dataset, which is complex and time-consuming. This work presents two semi-supervised learning frameworks to identify eye-parts by taking advantage of unlabeled images where labeled datasets are scarce. With these frameworks, leveraging the domain-specific augmentation and novel spatially varying transformations for image segmentation, we show improved performance on various test cases. For instance, for a model trained on just 48 labeled images, these frameworks achieved an improvement of 0.38% and 0.65% in segmentation performance over the baseline model, which is trained only with the labeled dataset.



### Multi-Prize Lottery Ticket Hypothesis: Finding Accurate Binary Neural Networks by Pruning A Randomly Weighted Network
- **Arxiv ID**: http://arxiv.org/abs/2103.09377v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09377v1)
- **Published**: 2021-03-17 00:31:24+00:00
- **Updated**: 2021-03-17 00:31:24+00:00
- **Authors**: James Diffenderfer, Bhavya Kailkhura
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Frankle & Carbin (2019) demonstrated that randomly-initialized dense networks contain subnetworks that once found can be trained to reach test accuracy comparable to the trained dense network. However, finding these high performing trainable subnetworks is expensive, requiring iterative process of training and pruning weights. In this paper, we propose (and prove) a stronger Multi-Prize Lottery Ticket Hypothesis:   A sufficiently over-parameterized neural network with random weights contains several subnetworks (winning tickets) that (a) have comparable accuracy to a dense target network with learned weights (prize 1), (b) do not require any further training to achieve prize 1 (prize 2), and (c) is robust to extreme forms of quantization (i.e., binary weights and/or activation) (prize 3).   This provides a new paradigm for learning compact yet highly accurate binary neural networks simply by pruning and quantizing randomly weighted full precision neural networks. We also propose an algorithm for finding multi-prize tickets (MPTs) and test it by performing a series of experiments on CIFAR-10 and ImageNet datasets. Empirical results indicate that as models grow deeper and wider, multi-prize tickets start to reach similar (and sometimes even higher) test accuracy compared to their significantly larger and full-precision counterparts that have been weight-trained. Without ever updating the weight values, our MPTs-1/32 not only set new binary weight network state-of-the-art (SOTA) Top-1 accuracy -- 94.8% on CIFAR-10 and 74.03% on ImageNet -- but also outperform their full-precision counterparts by 1.78% and 0.76%, respectively. Further, our MPT-1/1 achieves SOTA Top-1 accuracy (91.9%) for binary neural networks on CIFAR-10. Code and pre-trained models are available at: https://github.com/chrundle/biprop.



### SPICE: Semantic Pseudo-labeling for Image Clustering
- **Arxiv ID**: http://arxiv.org/abs/2103.09382v3
- **DOI**: 10.1109/TIP.2022.3221290
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.09382v3)
- **Published**: 2021-03-17 00:52:27+00:00
- **Updated**: 2022-01-14 14:18:19+00:00
- **Authors**: Chuang Niu, Hongming Shan, Ge Wang
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, 2022
- **Summary**: The similarity among samples and the discrepancy between clusters are two crucial aspects of image clustering. However, current deep clustering methods suffer from the inaccurate estimation of either feature similarity or semantic discrepancy. In this paper, we present a Semantic Pseudo-labeling-based Image ClustEring (SPICE) framework, which divides the clustering network into a feature model for measuring the instance-level similarity and a clustering head for identifying the cluster-level discrepancy. We design two semantics-aware pseudo-labeling algorithms, prototype pseudo-labeling, and reliable pseudo-labeling, which enable accurate and reliable self-supervision over clustering. Without using any ground-truth label, we optimize the clustering network in three stages: 1) train the feature model through contrastive learning to measure the instance similarity, 2) train the clustering head with the prototype pseudo-labeling algorithm to identify cluster semantics, and 3) jointly train the feature model and clustering head with the reliable pseudo-labeling algorithm to improve the clustering performance. Extensive experimental results demonstrate that SPICE achieves significant improvements (~10%) over existing methods and establishes the new state-of-the-art clustering results on six image benchmark datasets in terms of three popular metrics. Importantly, SPICE significantly reduces the gap between unsupervised and fully-supervised classification; e.g., there is only a 2% (91.8% vs 93.8%) accuracy difference on CIFAR-10. Our code has been made publically available at https://github.com/niuchuangnn/SPICE.



### Triplet-Watershed for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2103.09384v3
- **DOI**: 10.1109/TGRS.2021.3113721
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09384v3)
- **Published**: 2021-03-17 01:06:49+00:00
- **Updated**: 2021-09-05 09:11:27+00:00
- **Authors**: Aditya Challa, Sravan Danda, B. S. Daya Sagar, Laurent Najman
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp.
  1-14, 2022
- **Summary**: Hyperspectral images (HSI) consist of rich spatial and spectral information, which can potentially be used for several applications. However, noise, band correlations and high dimensionality restrict the applicability of such data. This is recently addressed using creative deep learning network architectures such as ResNet, SSRN, and A2S2K. However, the last layer, i.e the classification layer, remains unchanged and is taken to be the softmax classifier. In this article, we propose to use a watershed classifier. Watershed classifier extends the watershed operator from Mathematical Morphology for classification. In its vanilla form, the watershed classifier does not have any trainable parameters. In this article, we propose a novel approach to train deep learning networks to obtain representations suitable for the watershed classifier. The watershed classifier exploits the connectivity patterns, a characteristic of HSI datasets, for better inference. We show that exploiting such characteristics allows the Triplet-Watershed to achieve state-of-art results in supervised and semi-supervised contexts. These results are validated on Indianpines (IP), University of Pavia (UP), Kennedy Space Center (KSC) and University of Houston (UH) datasets, relying on simple convnet architecture using a quarter of parameters compared to previous state-of-the-art networks. The source code for reproducing the experiments and supplementary material (high resolution images) is available at https://github.com/ac20/TripletWatershed Code.



### Pros and Cons of GAN Evaluation Measures: New Developments
- **Arxiv ID**: http://arxiv.org/abs/2103.09396v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09396v3)
- **Published**: 2021-03-17 01:48:34+00:00
- **Updated**: 2021-10-03 01:05:56+00:00
- **Authors**: Ali Borji
- **Comment**: NA
- **Journal**: None
- **Summary**: This work is an update of a previous paper on the same topic published a few years ago. With the dramatic progress in generative modeling, a suite of new quantitative and qualitative techniques to evaluate models has emerged. Although some measures such as Inception Score, Frechet Inception Distance, Precision-Recall, and Perceptual Path Length are relatively more popular, GAN evaluation is not a settled issue and there is still room for improvement. Here, I describe new dimensions that are becoming important in assessing models (e.g. bias and fairness) and discuss the connection between GAN evaluation and deepfakes. These are important areas of concern in the machine learning community today and progress in GAN evaluation can help mitigate them.



### NAS-TC: Neural Architecture Search on Temporal Convolutions for Complex Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2104.01110v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01110v1)
- **Published**: 2021-03-17 02:02:11+00:00
- **Updated**: 2021-03-17 02:02:11+00:00
- **Authors**: Pengzhen Ren, Gang Xiao, Xiaojun Chang, Yun Xiao, Zhihui Li, Xiaojiang Chen
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In the field of complex action recognition in videos, the quality of the designed model plays a crucial role in the final performance. However, artificially designed network structures often rely heavily on the researchers' knowledge and experience. Accordingly, because of the automated design of its network structure, Neural architecture search (NAS) has achieved great success in the image processing field and attracted substantial research attention in recent years. Although some NAS methods have reduced the number of GPU search days required to single digits in the image field, directly using 3D convolution to extend NAS to the video field is still likely to produce a surge in computing volume. To address this challenge, we propose a new processing framework called Neural Architecture Search- Temporal Convolutional (NAS-TC). Our proposed framework is divided into two phases. In the first phase, the classical CNN network is used as the backbone network to complete the computationally intensive feature extraction task. In the second stage, a simple stitching search to the cell is used to complete the relatively lightweight long-range temporal-dependent information extraction. This ensures our method will have more reasonable parameter assignments and can handle minute-level videos. Finally, we conduct sufficient experiments on multiple benchmark datasets and obtain competitive recognition accuracy.



### Collapsible Linear Blocks for Super-Efficient Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/2103.09404v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.09404v4)
- **Published**: 2021-03-17 02:16:31+00:00
- **Updated**: 2022-03-17 22:04:25+00:00
- **Authors**: Kartikeya Bhardwaj, Milos Milosavljevic, Liam O'Neil, Dibakar Gope, Ramon Matas, Alex Chalfin, Naveen Suda, Lingchuan Meng, Danny Loh
- **Comment**: Accepted at MLSys 2022 conference
- **Journal**: None
- **Summary**: With the advent of smart devices that support 4K and 8K resolution, Single Image Super Resolution (SISR) has become an important computer vision problem. However, most super resolution deep networks are computationally very expensive. In this paper, we propose Super-Efficient Super Resolution (SESR) networks that establish a new state-of-the-art for efficient super resolution. Our approach is based on linear overparameterization of CNNs and creates an efficient model architecture for SISR. With theoretical analysis, we uncover the limitations of existing overparameterization methods and show how the proposed method alleviates them. Detailed experiments across six benchmark datasets demonstrate that SESR achieves similar or better image quality than state-of-the-art models while requiring 2x to 330x fewer Multiply-Accumulate (MAC) operations. As a result, SESR can be used on constrained hardware to perform x2 (1080p to 4K) and x4 (1080p to 8K) SISR. Towards this, we estimate hardware performance numbers for a commercial Arm mobile-Neural Processing Unit (NPU) for 1080p to 4K (x2) and 1080p to 8K (x4) SISR. Our results highlight the challenges faced by super resolution on AI accelerators and demonstrate that SESR is significantly faster (e.g., 6x-8x higher FPS) than existing models on mobile-NPU. Finally, SESR outperforms prior models by 1.5x-2x in latency on Arm CPU and GPU when deployed on a real mobile device. The code for this work is available at https://github.com/ARM-software/sesr.



### WheatNet: A Lightweight Convolutional Neural Network for High-throughput Image-based Wheat Head Detection and Counting
- **Arxiv ID**: http://arxiv.org/abs/2103.09408v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.09408v2)
- **Published**: 2021-03-17 02:38:58+00:00
- **Updated**: 2021-03-20 00:45:25+00:00
- **Authors**: Saeed Khaki, Nima Safaei, Hieu Pham, Lizhi Wang
- **Comment**: 14 pages, 5 figures
- **Journal**: None
- **Summary**: For a globally recognized planting breeding organization, manually-recorded field observation data is crucial for plant breeding decision making. However, certain phenotypic traits such as plant color, height, kernel counts, etc. can only be collected during a specific time-window of a crop's growth cycle. Due to labor-intensive requirements, only a small subset of possible field observations are recorded each season. To help mitigate this data collection bottleneck in wheat breeding, we propose a novel deep learning framework to accurately and efficiently count wheat heads to aid in the gathering of real-time data for decision making. We call our model WheatNet and show that our approach is robust and accurate for a wide range of environmental conditions of the wheat field. WheatNet uses a truncated MobileNetV2 as a lightweight backbone feature extractor which merges feature maps with different scales to counter image scale variations. Then, extracted multi-scale features go to two parallel sub-networks for simultaneous density-based counting and localization tasks. Our proposed method achieves an MAE and RMSE of 3.85 and 5.19 in our wheat head counting task, respectively, while having significantly fewer parameters when compared to other state-of-the-art methods. Our experiments and comparisons with other state-of-the-art methods demonstrate the superiority and effectiveness of our proposed method.



### YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.09422v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09422v1)
- **Published**: 2021-03-17 03:43:54+00:00
- **Updated**: 2021-03-17 03:43:54+00:00
- **Authors**: Yuxuan Liu, Lujia Wang, Ming Liu
- **Comment**: Accepcted by ICRA 2021. The arxiv version contains slightly more
  information than the final ICRA version due to limit in the page number
- **Journal**: None
- **Summary**: Object detection in 3D with stereo cameras is an important problem in computer vision, and is particularly crucial in low-cost autonomous mobile robots without LiDARs.   Nowadays, most of the best-performing frameworks for stereo 3D object detection are based on dense depth reconstruction from disparity estimation, making them extremely computationally expensive.   To enable real-world deployments of vision detection with binocular images, we take a step back to gain insights from 2D image-based detection frameworks and enhance them with stereo features.   We incorporate knowledge and the inference structure from real-time one-stage 2D/3D object detector and introduce a light-weight stereo matching module.   Our proposed framework, YOLOStereo3D, is trained on one single GPU and runs at more than ten fps. It demonstrates performance comparable to state-of-the-art stereo 3D detection frameworks without usage of LiDAR data. The code will be published in https://github.com/Owen-Liuyuxuan/visualDet3D.



### A Comprehensive Survey of Scene Graphs: Generation and Application
- **Arxiv ID**: http://arxiv.org/abs/2104.01111v5
- **DOI**: 10.1109/TPAMI.2021.3137605
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2104.01111v5)
- **Published**: 2021-03-17 04:24:20+00:00
- **Updated**: 2022-01-07 01:35:21+00:00
- **Authors**: Xiaojun Chang, Pengzhen Ren, Pengfei Xu, Zhihui Li, Xiaojiang Chen, Alex Hauptmann
- **Comment**: 25 pages
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2021
- **Summary**: Scene graph is a structured representation of a scene that can clearly express the objects, attributes, and relationships between objects in the scene. As computer vision technology continues to develop, people are no longer satisfied with simply detecting and recognizing objects in images; instead, people look forward to a higher level of understanding and reasoning about visual scenes. For example, given an image, we want to not only detect and recognize objects in the image, but also know the relationship between objects (visual relationship detection), and generate a text description (image captioning) based on the image content. Alternatively, we might want the machine to tell us what the little girl in the image is doing (Visual Question Answering (VQA)), or even remove the dog from the image and find similar images (image editing and retrieval), etc. These tasks require a higher level of understanding and reasoning for image vision tasks. The scene graph is just such a powerful tool for scene understanding. Therefore, scene graphs have attracted the attention of a large number of researchers, and related research is often cross-modal, complex, and rapidly developing. However, no relatively systematic survey of scene graphs exists at present. To this end, this survey conducts a comprehensive investigation of the current scene graph research. More specifically, we first summarized the general definition of the scene graph, then conducted a comprehensive and systematic discussion on the generation method of the scene graph (SGG) and the SGG with the aid of prior knowledge. We then investigated the main applications of scene graphs and summarized the most commonly used datasets. Finally, we provide some insights into the future development of scene graphs. We believe this will be a very helpful foundation for future research on scene graphs.



### Pose-GNN : Camera Pose Estimation System Using Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.09435v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09435v1)
- **Published**: 2021-03-17 04:40:02+00:00
- **Updated**: 2021-03-17 04:40:02+00:00
- **Authors**: Ahmed Elmoogy, Xiaodai Dong, Tao Lu, Robert Westendorp, Kishore Reddy
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel image based localization system using graph neural networks (GNN). The pretrained ResNet50 convolutional neural network (CNN) architecture is used to extract the important features for each image. Following, the extracted features are input to GNN to find the pose of each image by either using the image features as a node in a graph and formulate the pose estimation problem as node pose regression or modelling the image features themselves as a graph and the problem becomes graph pose regression. We do an extensive comparison between the proposed two approaches and the state of the art single image localization methods and show that using GNN leads to enhanced performance for both indoor and outdoor environments.



### Improved Deep Classwise Hashing With Centers Similarity Learning for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2103.09442v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2103.09442v1)
- **Published**: 2021-03-17 05:01:13+00:00
- **Updated**: 2021-03-17 05:01:13+00:00
- **Authors**: Ming Zhang, Hong Yan
- **Comment**: Accepted at ICPR'20
- **Journal**: None
- **Summary**: Deep supervised hashing for image retrieval has attracted researchers' attention due to its high efficiency and superior retrieval performance. Most existing deep supervised hashing works, which are based on pairwise/triplet labels, suffer from the expensive computational cost and insufficient utilization of the semantics information. Recently, deep classwise hashing introduced a classwise loss supervised by class labels information alternatively; however, we find it still has its drawback. In this paper, we propose an improved deep classwise hashing, which enables hashing learning and class centers learning simultaneously. Specifically, we design a two-step strategy on center similarity learning. It interacts with the classwise loss to attract the class center to concentrate on the intra-class samples while pushing other class centers as far as possible. The centers similarity learning contributes to generating more compact and discriminative hashing codes. We conduct experiments on three benchmark datasets. It shows that the proposed method effectively surpasses the original method and outperforms state-of-the-art baselines under various commonly-used evaluation metrics for image retrieval.



### Adversarial Attacks on Camera-LiDAR Models for 3D Car Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.09448v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.09448v2)
- **Published**: 2021-03-17 05:24:48+00:00
- **Updated**: 2021-09-21 20:11:58+00:00
- **Authors**: Mazen Abdelfattah, Kaiwen Yuan, Z. Jane Wang, Rabab Ward
- **Comment**: arXiv admin note: text overlap with arXiv:2101.10747 Updates in v2:
  Expanded conclusion and future work, reduced Figure 5's size, and a small
  correction in Table 3
- **Journal**: None
- **Summary**: Most autonomous vehicles (AVs) rely on LiDAR and RGB camera sensors for perception. Using these point cloud and image data, perception models based on deep neural nets (DNNs) have achieved state-of-the-art performance in 3D detection. The vulnerability of DNNs to adversarial attacks has been heavily investigated in the RGB image domain and more recently in the point cloud domain, but rarely in both domains simultaneously. Multi-modal perception systems used in AVs can be divided into two broad types: cascaded models which use each modality independently, and fusion models which learn from different modalities simultaneously. We propose a universal and physically realizable adversarial attack for each type, and study and contrast their respective vulnerabilities to attacks. We place a single adversarial object with specific shape and texture on top of a car with the objective of making this car evade detection. Evaluating on the popular KITTI benchmark, our adversarial object made the host vehicle escape detection by each model type more than 50% of the time. The dense RGB input contributed more to the success of the adversarial attacks on both cascaded and fusion models.



### Prediction-assistant Frame Super-Resolution for Video Streaming
- **Arxiv ID**: http://arxiv.org/abs/2103.09455v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09455v1)
- **Published**: 2021-03-17 06:05:27+00:00
- **Updated**: 2021-03-17 06:05:27+00:00
- **Authors**: Wang Shen, Wenbo Bao, Guangtao Zhai, Charlie L Wang, Jerry W Hu, Zhiyong Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Video frame transmission delay is critical in real-time applications such as online video gaming, live show, etc. The receiving deadline of a new frame must catch up with the frame rendering time. Otherwise, the system will buffer a while, and the user will encounter a frozen screen, resulting in unsatisfactory user experiences. An effective approach is to transmit frames in lower-quality under poor bandwidth conditions, such as using scalable video coding. In this paper, we propose to enhance video quality using lossy frames in two situations. First, when current frames are too late to receive before rendering deadline (i.e., lost), we propose to use previously received high-resolution images to predict the future frames. Second, when the quality of the currently received frames is low~(i.e., lossy), we propose to use previously received high-resolution frames to enhance the low-quality current ones. For the first case, we propose a small yet effective video frame prediction network. For the second case, we improve the video prediction network to a video enhancement network to associate current frames as well as previous frames to restore high-quality images. Extensive experimental results demonstrate that our method performs favorably against state-of-the-art algorithms in the lossy video streaming environment.



### Learning Discriminative Prototypes with Dynamic Time Warping
- **Arxiv ID**: http://arxiv.org/abs/2103.09458v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09458v1)
- **Published**: 2021-03-17 06:11:11+00:00
- **Updated**: 2021-03-17 06:11:11+00:00
- **Authors**: Xiaobin Chang, Frederick Tung, Greg Mori
- **Comment**: CVPR'21 preview, 10 pages, 8 figures
- **Journal**: None
- **Summary**: Dynamic Time Warping (DTW) is widely used for temporal data processing. However, existing methods can neither learn the discriminative prototypes of different classes nor exploit such prototypes for further analysis. We propose Discriminative Prototype DTW (DP-DTW), a novel method to learn class-specific discriminative prototypes for temporal recognition tasks. DP-DTW shows superior performance compared to conventional DTWs on time series classification benchmarks. Combined with end-to-end deep learning, DP-DTW can handle challenging weakly supervised action segmentation problems and achieves state of the art results on standard benchmarks. Moreover, detailed reasoning on the input video is enabled by the learned action prototypes. Specifically, an action-based video summarization can be obtained by aligning the input sequence with action prototypes.



### You Only Look One-level Feature
- **Arxiv ID**: http://arxiv.org/abs/2103.09460v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09460v1)
- **Published**: 2021-03-17 06:24:10+00:00
- **Updated**: 2021-03-17 06:24:10+00:00
- **Authors**: Qiang Chen, Yingming Wang, Tong Yang, Xiangyu Zhang, Jian Cheng, Jian Sun
- **Comment**: Accepted by CVPR2021. We provide more details and add additional
  comparison with YOLOv4 in this version
- **Journal**: None
- **Summary**: This paper revisits feature pyramids networks (FPN) for one-stage detectors and points out that the success of FPN is due to its divide-and-conquer solution to the optimization problem in object detection rather than multi-scale feature fusion. From the perspective of optimization, we introduce an alternative way to address the problem instead of adopting the complex feature pyramids - {\em utilizing only one-level feature for detection}. Based on the simple and efficient solution, we present You Only Look One-level Feature (YOLOF). In our method, two key components, Dilated Encoder and Uniform Matching, are proposed and bring considerable improvements. Extensive experiments on the COCO benchmark prove the effectiveness of the proposed model. Our YOLOF achieves comparable results with its feature pyramids counterpart RetinaNet while being $2.5\times$ faster. Without transformer layers, YOLOF can match the performance of DETR in a single-level feature manner with $7\times$ less training epochs. With an image size of $608\times608$, YOLOF achieves 44.3 mAP running at 60 fps on 2080Ti, which is $13\%$ faster than YOLOv4. Code is available at \url{https://github.com/megvii-model/YOLOF}.



### Learning with Group Noise
- **Arxiv ID**: http://arxiv.org/abs/2103.09468v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09468v1)
- **Published**: 2021-03-17 06:57:10+00:00
- **Updated**: 2021-03-17 06:57:10+00:00
- **Authors**: Qizhou Wang, Jiangchao Yao, Chen Gong, Tongliang Liu, Mingming Gong, Hongxia Yang, Bo Han
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning in the context of noise is a challenging but practical setting to plenty of real-world applications. Most of the previous approaches in this area focus on the pairwise relation (casual or correlational relationship) with noise, such as learning with noisy labels. However, the group noise, which is parasitic on the coarse-grained accurate relation with the fine-grained uncertainty, is also universal and has not been well investigated. The challenge under this setting is how to discover true pairwise connections concealed by the group relation with its fine-grained noise. To overcome this issue, we propose a novel Max-Matching method for learning with group noise. Specifically, it utilizes a matching mechanism to evaluate the relation confidence of each object w.r.t. the target, meanwhile considering the Non-IID characteristics among objects in the group. Only the most confident object is considered to learn the model, so that the fine-grained noise is mostly dropped. The performance on arange of real-world datasets in the area of several learning paradigms demonstrates the effectiveness of Max-Matching



### Virtual Dress Swap Using Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.09475v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.09475v1)
- **Published**: 2021-03-17 07:11:38+00:00
- **Updated**: 2021-03-17 07:11:38+00:00
- **Authors**: Odar Zeynal, Saber Malekzadeh
- **Comment**: None
- **Journal**: None
- **Summary**: Online shopping has gained popularity recently. This paper addresses one crucial problem of buying dress online, which has not been solved yet. This research tries to implement the idea of clothes swapping with the help of DeepFashion dataset where 6,223 images with eight landmarks each used. Deep Convolutional Neural Network has been built for Landmark detection.



### Disentangled Cycle Consistency for Highly-realistic Virtual Try-On
- **Arxiv ID**: http://arxiv.org/abs/2103.09479v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09479v2)
- **Published**: 2021-03-17 07:18:55+00:00
- **Updated**: 2021-03-19 08:08:17+00:00
- **Authors**: Chongjian Ge, Yibing Song, Yuying Ge, Han Yang, Wei Liu, Ping Luo
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Image virtual try-on replaces the clothes on a person image with a desired in-shop clothes image. It is challenging because the person and the in-shop clothes are unpaired. Existing methods formulate virtual try-on as either in-painting or cycle consistency. Both of these two formulations encourage the generation networks to reconstruct the input image in a self-supervised manner. However, existing methods do not differentiate clothing and non-clothing regions. A straight-forward generation impedes virtual try-on quality because of the heavily coupled image contents. In this paper, we propose a Disentangled Cycle-consistency Try-On Network (DCTON). The DCTON is able to produce highly-realistic try-on images by disentangling important components of virtual try-on including clothes warping, skin synthesis, and image composition. To this end, DCTON can be naturally trained in a self-supervised manner following cycle consistency learning. Extensive experiments on challenging benchmarks show that DCTON outperforms state-of-the-art approaches favorably.



### Revisiting the Loss Weight Adjustment in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.09488v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.09488v4)
- **Published**: 2021-03-17 07:45:06+00:00
- **Updated**: 2022-03-17 14:12:03+00:00
- **Authors**: Wenxin Yu, Xueling Shen, Jiajie Hu, Dong Yin
- **Comment**: Incorrect description of content
- **Journal**: None
- **Summary**: Object detection is a typical multi-task learning application, which optimizes classification and regression simultaneously. However, classification loss always dominates the multi-task loss in anchor-based methods, hampering the consistent and balanced optimization of the tasks. In this paper, we find that shifting the bounding boxes can change the division of positive and negative samples in classification, meaning classification depends on regression. Moreover, we summarize three important conclusions about fine-tuning loss weights, considering different datasets, optimizers and regression loss functions. Based on the above conclusions, we propose Adaptive Loss Weight Adjustment(ALWA) to solve the imbalance in optimizing anchor-based methods according to statistical characteristics of losses. By incorporating ALWA into previous state-of-the-art detectors, we achieve a significant performance gain on PASCAL VOC and MS COCO, even with L1, SmoothL1 and CIoU loss. The code is available at https://github.com/ywx-hub/ALWA.



### PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.09504v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09504v4)
- **Published**: 2021-03-17 08:28:30+00:00
- **Updated**: 2022-04-09 09:51:06+00:00
- **Authors**: Yunbo Wang, Haixu Wu, Jianjin Zhang, Zhifeng Gao, Jianmin Wang, Philip S. Yu, Mingsheng Long
- **Comment**: 17 pages, accepted by TPAMI
- **Journal**: None
- **Summary**: The predictive learning of spatiotemporal sequences aims to generate future images by learning from the historical context, where the visual dynamics are believed to have modular structures that can be learned with compositional subsystems. This paper models these structures by presenting PredRNN, a new recurrent network, in which a pair of memory cells are explicitly decoupled, operate in nearly independent transition manners, and finally form unified representations of the complex environment. Concretely, besides the original memory cell of LSTM, this network is featured by a zigzag memory flow that propagates in both bottom-up and top-down directions across all layers, enabling the learned visual dynamics at different levels of RNNs to communicate. It also leverages a memory decoupling loss to keep the memory cells from learning redundant features. We further propose a new curriculum learning strategy to force PredRNN to learn long-term dynamics from context frames, which can be generalized to most sequence-to-sequence models. We provide detailed ablation studies to verify the effectiveness of each component. Our approach is shown to obtain highly competitive results on five datasets for both action-free and action-conditioned predictive learning scenarios.



### CNN Model & Tuning for Global Road Damage Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.09512v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09512v1)
- **Published**: 2021-03-17 09:01:23+00:00
- **Updated**: 2021-03-17 09:01:23+00:00
- **Authors**: Rahul Vishwakarma, Ravigopal Vennelakanti
- **Comment**: 7 pages, 13 figures, 4 tables, Invited submission to IEEE BigData Cup
  Global Road Damage Detection Challenge 2020
- **Journal**: None
- **Summary**: This paper provides a report on our solution including model selection, tuning strategy and results obtained for Global Road Damage Detection Challenge. This Big Data Cup Challenge was held as a part of IEEE International Conference on Big Data 2020. We assess single and multi-stage network architectures for object detection and provide a benchmark using popular state-of-the-art open-source PyTorch frameworks like Detectron2 and Yolov5. Data preparation for provided Road Damage training dataset, captured using smartphone camera from Czech, India and Japan is discussed. We studied the effect of training on a per country basis with respect to a single generalizable model. We briefly describe the tuning strategy for the experiments conducted on two-stage Faster R-CNN with Deep Residual Network (Resnet) and Feature Pyramid Network (FPN) backbone. Additionally, we compare this to a one-stage Yolov5 model with Cross Stage Partial Network (CSPNet) backbone. We show a mean F1 score of 0.542 on Test2 and 0.536 on Test1 datasets using a multi-stage Faster R-CNN model, with Resnet-50 and Resnet-101 backbones respectively. This shows the generalizability of the Resnet-50 model when compared to its more complex counterparts. Experiments were conducted using Google Colab having K80 and a Linux PC with 1080Ti, NVIDIA consumer grade GPU. A PyTorch based Detectron2 code to preprocess, train, test and submit the Avg F1 score to is made available at https://github.com/vishwakarmarhl/rdd2020



### Meta-learning of Pooling Layers for Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.09528v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09528v2)
- **Published**: 2021-03-17 09:25:47+00:00
- **Updated**: 2021-07-12 05:16:13+00:00
- **Authors**: Takato Otsuzuki, Heon Song, Seiichi Uchida, Hideaki Hayashi
- **Comment**: Accepted at ICDAR2021, 16 pages, 9 figures
- **Journal**: None
- **Summary**: In convolutional neural network-based character recognition, pooling layers play an important role in dimensionality reduction and deformation compensation. However, their kernel shapes and pooling operations are empirically predetermined; typically, a fixed-size square kernel shape and max pooling operation are used. In this paper, we propose a meta-learning framework for pooling layers. As part of our framework, a parameterized pooling layer is proposed in which the kernel shape and pooling operation are trainable using two parameters, thereby allowing flexible pooling of the input data. We also propose a meta-learning algorithm for the parameterized pooling layer, which allows us to acquire a suitable pooling layer across multiple tasks. In the experiment, we applied the proposed meta-learning framework to character recognition tasks. The results demonstrate that a pooling layer that is suitable across character recognition tasks was obtained via meta-learning, and the obtained pooling layer improved the performance of the model in both few-shot character recognition and noisy image recognition tasks.



### A Novel Solution of Using Mixed Reality in Bowel and Oral and Maxillofacial Surgical Telepresence: 3D Mean Value Cloning algorithm
- **Arxiv ID**: http://arxiv.org/abs/2104.06316v1
- **DOI**: 10.1002/rcs.2161
- **Categories**: **physics.med-ph**, cs.CV, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2104.06316v1)
- **Published**: 2021-03-17 10:01:06+00:00
- **Updated**: 2021-03-17 10:01:06+00:00
- **Authors**: Arjina Maharjan, Abeer Alsadoon, P. W. C. Prasad, Nada AlSallami, Tarik A. Rashid, Ahmad Alrubaie, Sami Haddad
- **Comment**: 27 pages
- **Journal**: International Journal of Medical Robotics and Computer Assisted
  Surgery,2020
- **Summary**: Background and aim: Most of the Mixed Reality models used in the surgical telepresence are suffering from discrepancies in the boundary area and spatial-temporal inconsistency due to the illumination variation in the video frames. The aim behind this work is to propose a new solution that helps produce the composite video by merging the augmented video of the surgery site and the virtual hand of the remote expertise surgeon. The purpose of the proposed solution is to decrease the processing time and enhance the accuracy of merged video by decreasing the overlay and visualization error and removing occlusion and artefacts. Methodology: The proposed system enhanced the mean value cloning algorithm that helps to maintain the spatial-temporal consistency of the final composite video. The enhanced algorithm includes the 3D mean value coordinates and improvised mean value interpolant in the image cloning process, which helps to reduce the sawtooth, smudging and discolouration artefacts around the blending region. Results: As compared to the state of the art solution, the accuracy in terms of overlay error of the proposed solution is improved from 1.01mm to 0.80mm whereas the accuracy in terms of visualization error is improved from 98.8% to 99.4%. The processing time is reduced to 0.173 seconds from 0.211 seconds. Conclusion: Our solution helps make the object of interest consistent with the light intensity of the target image by adding the space distance that helps maintain the spatial consistency in the final merged video.



### Glioblastoma Multiforme Prognosis: MRI Missing Modality Generation, Segmentation and Radiogenomic Survival Prediction
- **Arxiv ID**: http://arxiv.org/abs/2104.01149v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01149v2)
- **Published**: 2021-03-17 10:11:51+00:00
- **Updated**: 2021-04-30 14:28:26+00:00
- **Authors**: Mobarakol Islam, Navodini Wijethilake, Hongliang Ren
- **Comment**: Accepted for a journal
- **Journal**: None
- **Summary**: The accurate prognosis of Glioblastoma Multiforme (GBM) plays an essential role in planning correlated surgeries and treatments. The conventional models of survival prediction rely on radiomic features using magnetic resonance imaging (MRI). In this paper, we propose a radiogenomic overall survival (OS) prediction approach by incorporating gene expression data with radiomic features such as shape, geometry, and clinical information. We exploit TCGA (The Cancer Genomic Atlas) dataset and synthesize the missing MRI modalities using a fully convolutional network (FCN) in a conditional Generative Adversarial Network (cGAN). Meanwhile, the same FCN architecture enables the tumor segmentation from the available and the synthesized MRI modalities. The proposed FCN architecture comprises octave convolution (OctConv) and a novel decoder, with skip connections in spatial and channel squeeze & excitation (skip-scSE) block. The OctConv can process low and high-frequency features individually and improve model efficiency by reducing channel-wise redundancy. Skip-scSE applies spatial and channel-wise excitation to signify the essential features and reduces the sparsity in deeper layers learning parameters using skip connections. The proposed approaches are evaluated by comparative experiments with state-of-the-art models in synthesis, segmentation, and overall survival (OS) prediction. We observe that adding missing MRI modality improves the segmentation prediction, and expression levels of gene markers have a high contribution in the GBM prognosis prediction, and fused radiogenomic features boost the OS estimation.



### Multi-channel Deep Supervision for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2103.09553v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09553v1)
- **Published**: 2021-03-17 10:33:58+00:00
- **Updated**: 2021-03-17 10:33:58+00:00
- **Authors**: Bo Wei, Mulin Chen, Qi Wang, Xuelong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Crowd counting is a task worth exploring in modern society because of its wide applications such as public safety and video monitoring. Many CNN-based approaches have been proposed to improve the accuracy of estimation, but there are some inherent issues affect the performance, such as overfitting and details lost caused by pooling layers. To tackle these problems, in this paper, we propose an effective network called MDSNet, which introduces a novel supervision framework called Multi-channel Deep Supervision (MDS). The MDS conducts channel-wise supervision on the decoder of the estimation model to help generate the density maps. To obtain the accurate supervision information of different channels, the MDSNet employs an auxiliary network called SupervisionNet (SN) to generate abundant supervision maps based on existing groundtruth. Besides the traditional density map supervision, we also use the SN to convert the dot annotations into continuous supervision information and conduct dot supervision in the MDSNet. Extensive experiments on several mainstream benchmarks show that the proposed MDSNet achieves competitive results and the MDS significantly improves the performance without changing the network structure.



### Hierarchical Random Walker Segmentation for Large Volumetric Biomedical Images
- **Arxiv ID**: http://arxiv.org/abs/2103.09564v3
- **DOI**: 10.1109/TIP.2022.3185551
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09564v3)
- **Published**: 2021-03-17 11:02:44+00:00
- **Updated**: 2022-08-23 13:38:13+00:00
- **Authors**: Dominik Drees, Florian Eilers, Xiaoyi Jiang
- **Comment**: None
- **Journal**: IEEE Trans. Image Process. 31: pp. 4431-4446 (2022)
- **Summary**: The random walker method for image segmentation is a popular tool for semi-automatic image segmentation, especially in the biomedical field. However, its linear asymptotic run time and memory requirements make application to 3D datasets of increasing sizes impractical. We propose a hierarchical framework that, to the best of our knowledge, is the first attempt to overcome these restrictions for the random walker algorithm and achieves sublinear run time and constant memory complexity. The goal of this framework is -- rather than improving the segmentation quality compared to the baseline method -- to make interactive segmentation on out-of-core datasets possible. The method is evaluated quantitavely on synthetic data and the CT-ORG dataset where the expected improvements in algorithm run time while maintaining high segmentation quality are confirmed. The incremental (i.e., interaction update) run time is demonstrated to be in seconds on a standard PC even for volumes of hundreds of gigabytes in size. In a small case study the applicability to large real world from current biomedical research is demonstrated. An implementation of the presented method is publicly available in version 5.2 of the widely used volume rendering and processing software Voreen (https://www.uni-muenster.de/Voreen/).



### The U-Net based GLOW for Optical-Flow-free Video Interframe Generation
- **Arxiv ID**: http://arxiv.org/abs/2103.09576v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09576v3)
- **Published**: 2021-03-17 11:37:10+00:00
- **Updated**: 2021-06-29 15:06:48+00:00
- **Authors**: Saem Park, Donghoon Han, Nojun Kwak
- **Comment**: None
- **Journal**: None
- **Summary**: Video frame interpolation is the task of creating an interframe between two adjacent frames along the time axis. So, instead of simply averaging two adjacent frames to create an intermediate image, this operation should maintain semantic continuity with the adjacent frames. Most conventional methods use optical flow, and various tools such as occlusion handling and object smoothing are indispensable. Since the use of these various tools leads to complex problems, we tried to tackle the video interframe generation problem without using problematic optical flow . To enable this , we have tried to use a deep neural network with an invertible structure, and developed an U-Net based Generative Flow which is a modified normalizing flow. In addition, we propose a learning method with a new consistency loss in the latent space to maintain semantic temporal consistency between frames. The resolution of the generated image is guaranteed to be identical to that of the original images by using an invertible network. Furthermore, as it is not a random image like the ones by generative models, our network guarantees stable outputs without flicker. Through experiments, we \sam {confirmed the feasibility of the proposed algorithm and would like to suggest the U-Net based Generative Flow as a new possibility for baseline in video frame interpolation. This paper is meaningful in that it is the world's first attempt to use invertible networks instead of optical flows for video interpolation.



### Theoretical bounds on data requirements for the ray-based classification
- **Arxiv ID**: http://arxiv.org/abs/2103.09577v3
- **DOI**: 10.1007/s42979-021-00921-0
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T20, 68Q32, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/2103.09577v3)
- **Published**: 2021-03-17 11:38:45+00:00
- **Updated**: 2022-02-26 15:56:24+00:00
- **Authors**: Brian J. Weber, Sandesh S. Kalantre, Thomas McJunkin, Jacob M. Taylor, Justyna P. Zwolak
- **Comment**: 10 pages, 5 figures
- **Journal**: SN Comput. Sci. 3, 57 (2022)
- **Summary**: The problem of classifying high-dimensional shapes in real-world data grows in complexity as the dimension of the space increases. For the case of identifying convex shapes of different geometries, a new classification framework has recently been proposed in which the intersections of a set of one-dimensional representations, called rays, with the boundaries of the shape are used to identify the specific geometry. This ray-based classification (RBC) has been empirically verified using a synthetic dataset of two- and three-dimensional shapes (Zwolak et al. in Proceedings of Third Workshop on Machine Learning and the Physical Sciences (NeurIPS 2020), Vancouver, Canada [December 11, 2020], arXiv:2010.00500, 2020) and, more recently, has also been validated experimentally (Zwolak et al., PRX Quantum 2:020335, 2021). Here, we establish a bound on the number of rays necessary for shape classification, defined by key angular metrics, for arbitrary convex shapes. For two dimensions, we derive a lower bound on the number of rays in terms of the shape's length, diameter, and exterior angles. For convex polytopes in $\mathbb{R}^N$, we generalize this result to a similar bound given as a function of the dihedral angle and the geometrical parameters of polygonal faces. This result enables a different approach for estimating high-dimensional shapes using substantially fewer data elements than volumetric or surface-based approaches.



### An Efficient Method for the Classification of Croplands in Scarce-Label Regions
- **Arxiv ID**: http://arxiv.org/abs/2103.09588v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.09588v1)
- **Published**: 2021-03-17 12:10:11+00:00
- **Updated**: 2021-03-17 12:10:11+00:00
- **Authors**: Houtan Ghaffari
- **Comment**: None
- **Journal**: None
- **Summary**: Two of the main challenges for cropland classification by satellite time-series images are insufficient ground-truth data and inaccessibility of high-quality hyperspectral images for under-developed areas. Unlabeled medium-resolution satellite images are abundant, but how to benefit from them is an open question. We will show how to leverage their potential for cropland classification using self-supervised tasks. Self-supervision is an approach where we provide simple training signals for the samples, which are apparent from the data's structure. Hence, they are cheap to acquire and explain a simple concept about the data. We introduce three self-supervised tasks for cropland classification. They reduce epistemic uncertainty, and the resulting model shows superior accuracy in a wide range of settings compared to SVM and Random Forest. Subsequently, we use the self-supervised tasks to perform unsupervised domain adaptation and benefit from the labeled samples in other regions. It is crucial to know what information to transfer to avoid degrading the performance. We show how to automate the information selection and transfer process in cropland classification even when the source and target areas have a very different feature distribution. We improved the model by about 24% compared to a baseline architecture without any labeled sample in the target domain. Our method is amenable to gradual improvement, works with medium-resolution satellite images, and does not require complicated models. Code and data are available.



### Automatic Generation of Contrast Sets from Scene Graphs: Probing the Compositional Consistency of GQA
- **Arxiv ID**: http://arxiv.org/abs/2103.09591v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09591v1)
- **Published**: 2021-03-17 12:19:25+00:00
- **Updated**: 2021-03-17 12:19:25+00:00
- **Authors**: Yonatan Bitton, Gabriel Stanovsky, Roy Schwartz, Michael Elhadad
- **Comment**: Accepted to NAACL 2021
- **Journal**: None
- **Summary**: Recent works have shown that supervised models often exploit data artifacts to achieve good test scores while their performance severely degrades on samples outside their training distribution. Contrast sets (Gardneret al., 2020) quantify this phenomenon by perturbing test samples in a minimal way such that the output label is modified. While most contrast sets were created manually, requiring intensive annotation effort, we present a novel method which leverages rich semantic input representation to automatically generate contrast sets for the visual question answering task. Our method computes the answer of perturbed questions, thus vastly reducing annotation cost and enabling thorough evaluation of models' performance on various semantic aspects (e.g., spatial or relational reasoning). We demonstrate the effectiveness of our approach on the GQA dataset and its semantic scene graph image representation. We find that, despite GQA's compositionality and carefully balanced label distribution, two high-performing models drop 13-17% in accuracy compared to the original test set. Finally, we show that our automatic perturbation can be applied to the training set to mitigate the degradation in performance, opening the door to more robust models.



### On the Role of Images for Analyzing Claims in Social Media
- **Arxiv ID**: http://arxiv.org/abs/2103.09602v1
- **DOI**: None
- **Categories**: **cs.SI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09602v1)
- **Published**: 2021-03-17 12:40:27+00:00
- **Updated**: 2021-03-17 12:40:27+00:00
- **Authors**: Gullal S. Cheema, Sherzod Hakimov, Eric Müller-Budack, Ralph Ewerth
- **Comment**: CLEOPATRA-2021 Workshop co-located with The Web Conf 2021
- **Journal**: None
- **Summary**: Fake news is a severe problem in social media. In this paper, we present an empirical study on visual, textual, and multimodal models for the tasks of claim, claim check-worthiness, and conspiracy detection, all of which are related to fake news detection. Recent work suggests that images are more influential than text and often appear alongside fake text. To this end, several multimodal models have been proposed in recent years that use images along with text to detect fake news on social media sites like Twitter. However, the role of images is not well understood for claim detection, specifically using transformer-based textual and multimodal models. We investigate state-of-the-art models for images, text (Transformer-based), and multimodal information for four different datasets across two languages to understand the role of images in the task of claim and conspiracy detection.



### Large-Scale Zero-Shot Image Classification from Rich and Diverse Textual Descriptions
- **Arxiv ID**: http://arxiv.org/abs/2103.09669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09669v1)
- **Published**: 2021-03-17 14:06:56+00:00
- **Updated**: 2021-03-17 14:06:56+00:00
- **Authors**: Sebastian Bujwid, Josephine Sullivan
- **Comment**: Accepted to LANTERN 2021. Project website:
  https://bujwid.eu/p/zsl-imagenet-wiki
- **Journal**: None
- **Summary**: We study the impact of using rich and diverse textual descriptions of classes for zero-shot learning (ZSL) on ImageNet. We create a new dataset ImageNet-Wiki that matches each ImageNet class to its corresponding Wikipedia article. We show that merely employing these Wikipedia articles as class descriptions yields much higher ZSL performance than prior works. Even a simple model using this type of auxiliary data outperforms state-of-the-art models that rely on standard features of word embedding encodings of class names. These results highlight the usefulness and importance of textual descriptions for ZSL, as well as the relative importance of auxiliary data type compared to algorithmic progress. Our experimental results also show that standard zero-shot learning approaches generalize poorly across categories of classes.



### Fourier Transform of Percoll Gradients Boosts CNN Classification of Hereditary Hemolytic Anemias
- **Arxiv ID**: http://arxiv.org/abs/2103.09671v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09671v1)
- **Published**: 2021-03-17 14:09:53+00:00
- **Updated**: 2021-03-17 14:09:53+00:00
- **Authors**: Ario Sadafi, Lucía María Moya Sans, Asya Makhro, Leonid Livshits, Nassir Navab, Anna Bogdanova, Shadi Albarqouni, Carsten Marr
- **Comment**: Accepted for publication at the 2021 IEEE International Symposium on
  Biomedical Imaging (ISBI 2021)
- **Journal**: None
- **Summary**: Hereditary hemolytic anemias are genetic disorders that affect the shape and density of red blood cells. Genetic tests currently used to diagnose such anemias are expensive and unavailable in the majority of clinical labs. Here, we propose a method for identifying hereditary hemolytic anemias based on a standard biochemistry method, called Percoll gradient, obtained by centrifuging a patient's blood. Our hybrid approach consists on using spatial data-driven features, extracted with a convolutional neural network and spectral handcrafted features obtained from fast Fourier transform. We compare late and early feature fusion with AlexNet and VGG16 architectures. AlexNet with late fusion of spectral features performs better compared to other approaches. We achieved an average F1-score of 88% on different classes suggesting the possibility of diagnosing of hereditary hemolytic anemias from Percoll gradients. Finally, we utilize Grad-CAM to explore the spatial features used for classification.



### Generating Annotated Training Data for 6D Object Pose Estimation in Operational Environments with Minimal User Interaction
- **Arxiv ID**: http://arxiv.org/abs/2103.09696v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.09696v3)
- **Published**: 2021-03-17 14:46:21+00:00
- **Updated**: 2022-05-11 10:22:10+00:00
- **Authors**: Paul Koch, Marian Schlüter, Serge Thill
- **Comment**: Paper was not accepted
- **Journal**: None
- **Summary**: Recently developed deep neural networks achieved state-of-the-art results in the subject of 6D object pose estimation for robot manipulation. However, those supervised deep learning methods require expensive annotated training data. Current methods for reducing those costs frequently use synthetic data from simulations, but rely on expert knowledge and suffer from the "domain gap" when shifting to the real world. Here, we present a proof of concept for a novel approach of autonomously generating annotated training data for 6D object pose estimation. This approach is designed for learning new objects in operational environments while requiring little interaction and no expertise on the part of the user. We evaluate our autonomous data generation approach in two grasping experiments, where we archive a similar grasping success rate as related work on a non autonomously generated data set.



### Single Underwater Image Restoration by Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.09697v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09697v2)
- **Published**: 2021-03-17 14:47:03+00:00
- **Updated**: 2021-04-15 17:10:50+00:00
- **Authors**: Junlin Han, Mehrdad Shoeiby, Tim Malthus, Elizabeth Botha, Janet Anstee, Saeed Anwar, Ran Wei, Lars Petersson, Mohammad Ali Armin
- **Comment**: Accepted to IGARSS 2021 as oral presentation. Code is available at
  https://github.com/JunlinHan/CWR
- **Journal**: None
- **Summary**: Underwater image restoration attracts significant attention due to its importance in unveiling the underwater world. This paper elaborates on a novel method that achieves state-of-the-art results for underwater image restoration based on the unsupervised image-to-image translation framework. We design our method by leveraging from contrastive learning and generative adversarial networks to maximize mutual information between raw and restored images. Additionally, we release a large-scale real underwater image dataset to support both paired and unpaired training modules. Extensive experiments with comparisons to recent approaches further demonstrate the superiority of our proposed method.



### ShipSRDet: An End-to-End Remote Sensing Ship Detector Using Super-Resolved Feature Representation
- **Arxiv ID**: http://arxiv.org/abs/2103.09699v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09699v1)
- **Published**: 2021-03-17 14:51:45+00:00
- **Updated**: 2021-03-17 14:51:45+00:00
- **Authors**: Shitian He, Huanxin Zou, Yingqian Wang, Runlin Li, Fei Cheng
- **Comment**: Accepted to IGARSS 2021
- **Journal**: None
- **Summary**: High-resolution remote sensing images can provide abundant appearance information for ship detection. Although several existing methods use image super-resolution (SR) approaches to improve the detection performance, they consider image SR and ship detection as two separate processes and overlook the internal coherence between these two correlated tasks. In this paper, we explore the potential benefits introduced by image SR to ship detection, and propose an end-to-end network named ShipSRDet. In our method, we not only feed the super-resolved images to the detector but also integrate the intermediate features of the SR network with those of the detection network. In this way, the informative feature representation extracted by the SR network can be fully used for ship detection. Experimental results on the HRSC dataset validate the effectiveness of our method. Our ShipSRDet can recover the missing details from the input image and achieves promising ship detection performance.



### What's in My LiDAR Odometry Toolbox?
- **Arxiv ID**: http://arxiv.org/abs/2103.09708v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09708v3)
- **Published**: 2021-03-17 15:04:23+00:00
- **Updated**: 2021-10-07 14:38:02+00:00
- **Authors**: Pierre Dellenbach, Jean-Emmanuel Deschaud, Bastien Jacquet, François Goulette
- **Comment**: This work was realised in the context of the PhD thesis of Pierre
  Dellenbach, financed by Kitware
  (https://www.kitware.fr/equipe-vision-par-odinateur/), under the supervision
  of Bastien Jacquet (Kitware), Jean-Emmanuel Deschaud and Fran\c{c}ois
  Goulette (Mines ParisTech)
- **Journal**: None
- **Summary**: With the democratization of 3D LiDAR sensors, precise LiDAR odometries and SLAM are in high demand. New methods regularly appear, proposing solutions ranging from small variations in classical algorithms to radically new paradigms based on deep learning. Yet it is often difficult to compare these methods, notably due to the few datasets on which the methods can be evaluated and compared. Furthermore, their weaknesses are rarely examined, often letting the user discover the hard way whether a method would be appropriate for a use case. In this paper, we review and organize the main 3D LiDAR odometries into distinct categories. We implemented several approaches (geometric based, deep learning based, and hybrid methods) to conduct an in-depth analysis of their strengths and weaknesses on multiple datasets, guiding the reader through the different LiDAR odometries available. Implementation of the methods has been made publicly available at https://github.com/Kitware/pyLiDAR-SLAM.



### Trans-SVNet: Accurate Phase Recognition from Surgical Videos via Hybrid Embedding Aggregation Transformer
- **Arxiv ID**: http://arxiv.org/abs/2103.09712v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.09712v2)
- **Published**: 2021-03-17 15:12:55+00:00
- **Updated**: 2021-07-12 12:18:39+00:00
- **Authors**: Xiaojie Gao, Yueming Jin, Yonghao Long, Qi Dou, Pheng-Ann Heng
- **Comment**: MICCAI2021
- **Journal**: None
- **Summary**: Real-time surgical phase recognition is a fundamental task in modern operating rooms. Previous works tackle this task relying on architectures arranged in spatio-temporal order, however, the supportive benefits of intermediate spatial features are not considered. In this paper, we introduce, for the first time in surgical workflow analysis, Transformer to reconsider the ignored complementary effects of spatial and temporal features for accurate surgical phase recognition. Our hybrid embedding aggregation Transformer fuses cleverly designed spatial and temporal embeddings by allowing for active queries based on spatial information from temporal embedding sequences. More importantly, our framework processes the hybrid embeddings in parallel to achieve a high inference speed. Our method is thoroughly validated on two large surgical video datasets, i.e., Cholec80 and M2CAI16 Challenge datasets, and outperforms the state-of-the-art approaches at a processing speed of 91 fps.



### Interpretable Distance Metric Learning for Handwritten Chinese Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.09714v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.09714v1)
- **Published**: 2021-03-17 15:17:02+00:00
- **Updated**: 2021-03-17 15:17:02+00:00
- **Authors**: Boxiang Dong, Aparna S. Varde, Danilo Stevanovic, Jiayin Wang, Liang Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Handwriting recognition is of crucial importance to both Human Computer Interaction (HCI) and paperwork digitization. In the general field of Optical Character Recognition (OCR), handwritten Chinese character recognition faces tremendous challenges due to the enormously large character sets and the amazing diversity of writing styles. Learning an appropriate distance metric to measure the difference between data inputs is the foundation of accurate handwritten character recognition. Existing distance metric learning approaches either produce unacceptable error rates, or provide little interpretability in the results. In this paper, we propose an interpretable distance metric learning approach for handwritten Chinese character recognition. The learned metric is a linear combination of intelligible base metrics, and thus provides meaningful insights to ordinary users. Our experimental results on a benchmark dataset demonstrate the superior efficiency, accuracy and interpretability of our proposed approach.



### Quantitative Performance Assessment of CNN Units via Topological Entropy Calculation
- **Arxiv ID**: http://arxiv.org/abs/2103.09716v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2103.09716v5)
- **Published**: 2021-03-17 15:18:18+00:00
- **Updated**: 2022-03-17 08:58:51+00:00
- **Authors**: Yang Zhao, Hao Zhang
- **Comment**: Conference paper at ICLR 2022
- **Journal**: None
- **Summary**: Identifying the status of individual network units is critical for understanding the mechanism of convolutional neural networks (CNNs). However, it is still challenging to reliably give a general indication of unit status, especially for units in different network models. To this end, we propose a novel method for quantitatively clarifying the status of single unit in CNN using algebraic topological tools. Unit status is indicated via the calculation of a defined topological-based entropy, called feature entropy, which measures the degree of chaos of the global spatial pattern hidden in the unit for a category. In this way, feature entropy could provide an accurate indication of status for units in different networks with diverse situations like weight-rescaling operation. Further, we show that feature entropy decreases as the layer goes deeper and shares almost simultaneous trend with loss during training. We show that by investigating the feature entropy of units on only training data, it could give discrimination between networks with different generalization ability from the view of the effectiveness of feature representations.



### Few-Shot Visual Grounding for Natural Human-Robot Interaction
- **Arxiv ID**: http://arxiv.org/abs/2103.09720v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.09720v2)
- **Published**: 2021-03-17 15:24:02+00:00
- **Updated**: 2021-03-31 14:13:29+00:00
- **Authors**: Giorgos Tziafas, Hamidreza Kasaei
- **Comment**: 6 pages, 4 figures, ICARSC2021 accepted
- **Journal**: None
- **Summary**: Natural Human-Robot Interaction (HRI) is one of the key components for service robots to be able to work in human-centric environments. In such dynamic environments, the robot needs to understand the intention of the user to accomplish a task successfully. Towards addressing this point, we propose a software architecture that segments a target object from a crowded scene, indicated verbally by a human user. At the core of our system, we employ a multi-modal deep neural network for visual grounding. Unlike most grounding methods that tackle the challenge using pre-trained object detectors via a two-stepped process, we develop a single stage zero-shot model that is able to provide predictions in unseen data. We evaluate the performance of the proposed model on real RGB-D data collected from public scene datasets. Experimental results showed that the proposed model performs well in terms of accuracy and speed, while showcasing robustness to variation in the natural language input.



### Training GANs with Stronger Augmentations via Contrastive Discriminator
- **Arxiv ID**: http://arxiv.org/abs/2103.09742v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09742v1)
- **Published**: 2021-03-17 16:04:54+00:00
- **Updated**: 2021-03-17 16:04:54+00:00
- **Authors**: Jongheon Jeong, Jinwoo Shin
- **Comment**: 23 pages; ICLR 2021
- **Journal**: None
- **Summary**: Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This "fusion" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.



### On the Whitney near extension problem, BMO, alignment of data, best approximation in algebraic geometry, manifold learning and their beautiful connections: A modern treatment
- **Arxiv ID**: http://arxiv.org/abs/2103.09748v7
- **DOI**: None
- **Categories**: **math.CA**, cs.CV, cs.LG, math.OC, 2B37, 42B35, 30H35, 30E10, 14Q15, 53A45, 58Z05, 68P01, 42B37, 42B35,
  30H35, 30E10, 14Q15, 53A45, 58Z05, 68P01, 49J35, 49J30, 49J10, 49J21
- **Links**: [PDF](http://arxiv.org/pdf/2103.09748v7)
- **Published**: 2021-03-17 16:12:53+00:00
- **Updated**: 2023-02-09 21:32:24+00:00
- **Authors**: Steven B. Damelin
- **Comment**: This version has been removed by arXiv administrators because the
  submitter did not have the authority to grant the license at the time of
  submission
- **Journal**: None
- **Summary**: This paper provides fascinating connections between several mathematical problems which lie on the intersection of several mathematics subjects, namely algebraic geometry, approximation theory, complex-harmonic analysis and high dimensional data science. Modern techniques in algebraic geometry, approximation theory, computational harmonic analysis and extensions develop the first of its kind, a unified framework which allows for a simultaneous study of labeled and unlabeled near alignment data problems in of $\mathbb R^D$ with the near isometry extension problem for discrete and non-discrete subsets of $\mathbb R^D$ with certain geometries. In addition, the paper surveys related work on clustering, dimension reduction, manifold learning, vision as well as minimal energy partitions, discrepancy and min-max optimization. Numerous open problems are given.



### Aggregated Multi-GANs for Controlled 3D Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2103.09755v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09755v1)
- **Published**: 2021-03-17 16:22:36+00:00
- **Updated**: 2021-03-17 16:22:36+00:00
- **Authors**: Zhenguang Liu, Kedi Lyu, Shuang Wu, Haipeng Chen, Yanbin Hao, Shouling Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Human motion prediction from historical pose sequence is at the core of many applications in machine intelligence. However, in current state-of-the-art methods, the predicted future motion is confined within the same activity. One can neither generate predictions that differ from the current activity, nor manipulate the body parts to explore various future possibilities. Undoubtedly, this greatly limits the usefulness and applicability of motion prediction. In this paper, we propose a generalization of the human motion prediction task in which control parameters can be readily incorporated to adjust the forecasted motion. Our method is compelling in that it enables manipulable motion prediction across activity types and allows customization of the human movement in a variety of fine-grained ways. To this aim, a simple yet effective composite GAN structure, consisting of local GANs for different body parts and aggregated via a global GAN is presented. The local GANs game in lower dimensions, while the global GAN adjusts in high dimensional space to avoid mode collapse. Extensive experiments show that our method outperforms state-of-the-art. The codes are available at https://github.com/herolvkd/AM-GAN.



### Gradient Projection Memory for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.09762v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.09762v1)
- **Published**: 2021-03-17 16:31:29+00:00
- **Updated**: 2021-03-17 16:31:29+00:00
- **Authors**: Gobinda Saha, Isha Garg, Kaushik Roy
- **Comment**: Accepted for Oral Presentation at ICLR 2021
  https://openreview.net/forum?id=3AOj0RCNC2
- **Journal**: International Conference on Learning Representations (ICLR), 2021
- **Summary**: The ability to learn continually without forgetting the past tasks is a desired attribute for artificial learning systems. Existing approaches to enable such learning in artificial neural networks usually rely on network growth, importance based weight update or replay of old data from the memory. In contrast, we propose a novel approach where a neural network learns new tasks by taking gradient steps in the orthogonal direction to the gradient subspaces deemed important for the past tasks. We find the bases of these subspaces by analyzing network representations (activations) after learning each task with Singular Value Decomposition (SVD) in a single shot manner and store them in the memory as Gradient Projection Memory (GPM). With qualitative and quantitative analyses, we show that such orthogonal gradient descent induces minimum to no interference with the past tasks, thereby mitigates forgetting. We evaluate our algorithm on diverse image classification datasets with short and long sequences of tasks and report better or on-par performance compared to the state-of-the-art approaches.



### HAMIL: Hierarchical Aggregation-Based Multi-Instance Learning for Microscopy Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2103.09764v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09764v1)
- **Published**: 2021-03-17 16:34:08+00:00
- **Updated**: 2021-03-17 16:34:08+00:00
- **Authors**: Yanlun Tu, Houchao Lei, Wei Long, Yang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-instance learning is common for computer vision tasks, especially in biomedical image processing. Traditional methods for multi-instance learning focus on designing feature aggregation methods and multi-instance classifiers, where the aggregation operation is performed either in feature extraction or learning phase. As deep neural networks (DNNs) achieve great success in image processing via automatic feature learning, certain feature aggregation mechanisms need to be incorporated into common DNN architecture for multi-instance learning. Moreover, flexibility and reliability are crucial considerations to deal with varying quality and number of instances.   In this study, we propose a hierarchical aggregation network for multi-instance learning, called HAMIL. The hierarchical aggregation protocol enables feature fusion in a defined order, and the simple convolutional aggregation units lead to an efficient and flexible architecture. We assess the model performance on two microscopy image classification tasks, namely protein subcellular localization using immunofluorescence images and gene annotation using spatial gene expression images. The experimental results show that HAMIL outperforms the state-of-the-art feature aggregation methods and the existing models for addressing these two tasks. The visualization analyses also demonstrate the ability of HAMIL to focus on high-quality instances.



### ALADIN: All Layer Adaptive Instance Normalization for Fine-grained Style Similarity
- **Arxiv ID**: http://arxiv.org/abs/2103.09776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09776v1)
- **Published**: 2021-03-17 17:03:27+00:00
- **Updated**: 2021-03-17 17:03:27+00:00
- **Authors**: Dan Ruta, Saeid Motiian, Baldo Faieta, Zhe Lin, Hailin Jin, Alex Filipkowski, Andrew Gilbert, John Collomosse
- **Comment**: None
- **Journal**: None
- **Summary**: We present ALADIN (All Layer AdaIN); a novel architecture for searching images based on the similarity of their artistic style. Representation learning is critical to visual search, where distance in the learned search embedding reflects image similarity. Learning an embedding that discriminates fine-grained variations in style is hard, due to the difficulty of defining and labelling style. ALADIN takes a weakly supervised approach to learning a representation for fine-grained style similarity of digital artworks, leveraging BAM-FG, a novel large-scale dataset of user generated content groupings gathered from the web. ALADIN sets a new state of the art accuracy for style-based visual search over both coarse labelled style data (BAM) and BAM-FG; a new 2.62 million image dataset of 310,000 fine-grained style groupings also contributed by this work.



### Temporal Cluster Matching for Change Detection of Structures from Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2103.09787v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09787v2)
- **Published**: 2021-03-17 17:20:16+00:00
- **Updated**: 2021-06-29 15:02:03+00:00
- **Authors**: Caleb Robinson, Anthony Ortiz, Juan M. Lavista Ferres, Brandon Anderson, Daniel E. Ho
- **Comment**: Published in ACM COMPASS 2021
- **Journal**: None
- **Summary**: Longitudinal studies are vital to understanding dynamic changes of the planet, but labels (e.g., buildings, facilities, roads) are often available only for a single point in time. We propose a general model, Temporal Cluster Matching (TCM), for detecting building changes in time series of remotely sensed imagery when footprint labels are observed only once. The intuition behind the model is that the relationship between spectral values inside and outside of building's footprint will change when a building is constructed (or demolished). For instance, in rural settings, the pre-construction area may look similar to the surrounding environment until the building is constructed. Similarly, in urban settings, the pre-construction areas will look different from the surrounding environment until construction. We further propose a heuristic method for selecting the parameters of our model which allows it to be applied in novel settings without requiring data labeling efforts (to fit the parameters). We apply our model over a dataset of poultry barns from 2016/2017 high-resolution aerial imagery in the Delmarva Peninsula and a dataset of solar farms from a 2020 mosaic of Sentinel 2 imagery in India. Our results show that our model performs as well when fit using the proposed heuristic as it does when fit with labeled data, and further, that supervised versions of our model perform the best among all the baselines we test against. Finally, we show that our proposed approach can act as an effective data augmentation strategy -- it enables researchers to augment existing structure footprint labels along the time dimension and thus use imagery from multiple points in time to train deep learning models. We show that this improves the spatial generalization of such models when evaluated on the same change detection task.



### Bias-Free FedGAN: A Federated Approach to Generate Bias-Free Datasets
- **Arxiv ID**: http://arxiv.org/abs/2103.09876v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2103.09876v2)
- **Published**: 2021-03-17 19:27:08+00:00
- **Updated**: 2021-04-16 03:58:13+00:00
- **Authors**: Vaikkunth Mugunthan, Vignesh Gokul, Lalana Kagal, Shlomo Dubnov
- **Comment**: None
- **Journal**: None
- **Summary**: Federated Generative Adversarial Network (FedGAN) is a communication-efficient approach to train a GAN across distributed clients without clients having to share their sensitive training data. In this paper, we experimentally show that FedGAN generates biased data points under non-independent-and-identically-distributed (non-iid) settings. Also, we propose Bias-Free FedGAN, an approach to generate bias-free synthetic datasets using FedGAN. Our approach generates metadata at the aggregator using the models received from clients and retrains the federated model to achieve bias-free results for image synthesis. Bias-Free FedGAN has the same communication cost as that of FedGAN. Experimental results on image datasets (MNIST and FashionMNIST) validate our claims.



### Hierarchical Attention-based Age Estimation and Bias Estimation
- **Arxiv ID**: http://arxiv.org/abs/2103.09882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09882v1)
- **Published**: 2021-03-17 19:41:34+00:00
- **Updated**: 2021-03-17 19:41:34+00:00
- **Authors**: Shakediel Hiba, Yosi Keller
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: In this work we propose a novel deep-learning approach for age estimation based on face images. We first introduce a dual image augmentation-aggregation approach based on attention. This allows the network to jointly utilize multiple face image augmentations whose embeddings are aggregated by a Transformer-Encoder. The resulting aggregated embedding is shown to better encode the face image attributes. We then propose a probabilistic hierarchical regression framework that combines a discrete probabilistic estimate of age labels, with a corresponding ensemble of regressors. Each regressor is particularly adapted and trained to refine the probabilistic estimate over a range of ages. Our scheme is shown to outperform contemporary schemes and provide a new state-of-the-art age estimation accuracy, when applied to the MORPH II dataset for age estimation. Last, we introduce a bias analysis of state-of-the-art age estimation results.



### The Untapped Potential of Off-the-Shelf Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.09891v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.09891v1)
- **Published**: 2021-03-17 20:04:46+00:00
- **Updated**: 2021-03-17 20:04:46+00:00
- **Authors**: Matthew Inkawhich, Nathan Inkawhich, Eric Davis, Hai Li, Yiran Chen
- **Comment**: 12 pages, 8 figures
- **Journal**: None
- **Summary**: Over recent years, a myriad of novel convolutional network architectures have been developed to advance state-of-the-art performance on challenging recognition tasks. As computational resources improve, a great deal of effort has been placed in efficiently scaling up existing designs and generating new architectures with Neural Architecture Search (NAS) algorithms. While network topology has proven to be a critical factor for model performance, we show that significant gains are being left on the table by keeping topology static at inference-time. Due to challenges such as scale variation, we should not expect static models configured to perform well across a training dataset to be optimally configured to handle all test data. In this work, we seek to expose the exciting potential of inference-time-dynamic models. By allowing just four layers to dynamically change configuration at inference-time, we show that existing off-the-shelf models like ResNet-50 are capable of over 95% accuracy on ImageNet. This level of performance currently exceeds that of models with over 20x more parameters and significantly more complex training procedures.



### Impact of Facial Tattoos and Paintings on Face Recognition Systems
- **Arxiv ID**: http://arxiv.org/abs/2103.09939v2
- **DOI**: 10.1049/bme2.12032
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09939v2)
- **Published**: 2021-03-17 22:38:13+00:00
- **Updated**: 2021-03-27 21:26:07+00:00
- **Authors**: Mathias Ibsen, Christian Rathgeb, Thomas Fink, Pawel Drozdowski, Christoph Busch
- **Comment**: Accepted to IET Biometrics
- **Journal**: None
- **Summary**: In the past years, face recognition technologies have shown impressive recognition performance, mainly due to recent developments in deep convolutional neural networks. Notwithstanding those improvements, several challenges which affect the performance of face recognition systems remain. In this work, we investigate the impact that facial tattoos and paintings have on current face recognition systems. To this end, we first collected an appropriate database containing image-pairs of individuals with and without facial tattoos or paintings. The assembled database was used to evaluate how facial tattoos and paintings affect the detection, quality estimation, as well as the feature extraction and comparison modules of a face recognition system. The impact on these modules was evaluated using state-of-the-art open-source and commercial systems. The obtained results show that facial tattoos and paintings affect all the tested modules, especially for images where a large area of the face is covered with tattoos or paintings. Our work is an initial case-study and indicates a need to design algorithms which are robust to the visual changes caused by facial tattoos and paintings.



### Machine Vision based Sample-Tube Localization for Mars Sample Return
- **Arxiv ID**: http://arxiv.org/abs/2103.09942v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.09942v1)
- **Published**: 2021-03-17 23:09:28+00:00
- **Updated**: 2021-03-17 23:09:28+00:00
- **Authors**: Shreyansh Daftry, Barry Ridge, William Seto, Tu-Hoa Pham, Peter Ilhardt, Gerard Maggiolino, Mark Van der Merwe, Alex Brinkman, John Mayo, Eric Kulczyski, Renaud Detry
- **Comment**: IEEE Aerospace Conference, 2021
- **Journal**: None
- **Summary**: A potential Mars Sample Return (MSR) architecture is being jointly studied by NASA and ESA. As currently envisioned, the MSR campaign consists of a series of 3 missions: sample cache, fetch and return to Earth. In this paper, we focus on the fetch part of the MSR, and more specifically the problem of autonomously detecting and localizing sample tubes deposited on the Martian surface. Towards this end, we study two machine-vision based approaches: First, a geometry-driven approach based on template matching that uses hard-coded filters and a 3D shape model of the tube; and second, a data-driven approach based on convolutional neural networks (CNNs) and learned features. Furthermore, we present a large benchmark dataset of sample-tube images, collected in representative outdoor environments and annotated with ground truth segmentation masks and locations. The dataset was acquired systematically across different terrain, illumination conditions and dust-coverage; and benchmarking was performed to study the feasibility of each approach, their relative strengths and weaknesses, and robustness in the presence of adverse environmental conditions.



### Fast and High-Quality Blind Multi-Spectral Image Pansharpening
- **Arxiv ID**: http://arxiv.org/abs/2103.09943v4
- **DOI**: 10.1109/TGRS.2021.3091329
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.09943v4)
- **Published**: 2021-03-17 23:12:14+00:00
- **Updated**: 2021-07-30 21:45:16+00:00
- **Authors**: Lantao Yu, Dehong Liu, Hassan Mansour, Petros T. Boufounos
- **Comment**: 17 pages, 47 figures, journal, accepted by IEEE Transactions on
  Geoscience and Remote Sensing
- **Journal**: None
- **Summary**: Blind pansharpening addresses the problem of generating a high spatial-resolution multi-spectral (HRMS) image given a low spatial-resolution multi-spectral (LRMS) image with the guidance of its associated spatially misaligned high spatial-resolution panchromatic (PAN) image without parametric side information. In this paper, we propose a fast approach to blind pansharpening and achieve state-of-the-art image reconstruction quality. Typical blind pansharpening algorithms are often computationally intensive since the blur kernel and the target HRMS image are often computed using iterative solvers and in an alternating fashion. To achieve fast blind pansharpening, we decouple the solution of the blur kernel and of the HRMS image. First, we estimate the blur kernel by computing the kernel coefficients with minimum total generalized variation that blur a downsampled version of the PAN image to approximate a linear combination of the LRMS image channels. Then, we estimate each channel of the HRMS image using local Laplacian prior to regularize the relationship between each HRMS channel and the PAN image. Solving the HRMS image is accelerated by both parallelizing across the channels and by fast numerical algorithms for each channel. Due to the fast scheme and the powerful priors we used on the blur kernel coefficients (total generalized variation) and on the cross-channel relationship (local Laplacian prior), numerical experiments demonstrate that our algorithm outperforms state-of-the-art model-based counterparts in terms of both computational time and reconstruction quality of the HRMS images.



### Learning to Resize Images for Computer Vision Tasks
- **Arxiv ID**: http://arxiv.org/abs/2103.09950v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.09950v2)
- **Published**: 2021-03-17 23:43:44+00:00
- **Updated**: 2021-08-18 01:11:48+00:00
- **Authors**: Hossein Talebi, Peyman Milanfar
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: For all the ways convolutional neural nets have revolutionized computer vision in recent years, one important aspect has received surprisingly little attention: the effect of image size on the accuracy of tasks being trained for. Typically, to be efficient, the input images are resized to a relatively small spatial resolution (e.g. 224x224), and both training and inference are carried out at this resolution. The actual mechanism for this re-scaling has been an afterthought: Namely, off-the-shelf image resizers such as bilinear and bicubic are commonly used in most machine learning software frameworks. But do these resizers limit the on task performance of the trained networks? The answer is yes. Indeed, we show that the typical linear resizer can be replaced with learned resizers that can substantially improve performance. Importantly, while the classical resizers typically result in better perceptual quality of the downscaled images, our proposed learned resizers do not necessarily give better visual quality, but instead improve task performance. Our learned image resizer is jointly trained with a baseline vision model. This learned CNN-based resizer creates machine friendly visual manipulations that lead to a consistent improvement of the end task metric over the baseline model. Specifically, here we focus on the classification task with the ImageNet dataset, and experiment with four different models to learn resizers adapted to each model. Moreover, we show that the proposed resizer can also be useful for fine-tuning the classification baselines for other vision tasks. To this end, we experiment with three different baselines to develop image quality assessment (IQA) models on the AVA dataset.



