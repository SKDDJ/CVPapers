# Arxiv Papers in cs.CV on 2021-03-23
### F-SIOL-310: A Robotic Dataset and Benchmark for Few-Shot Incremental Object Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.12242v3
- **DOI**: 10.1109/ICRA48506.2021.9561509
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.12242v3)
- **Published**: 2021-03-23 00:25:50+00:00
- **Updated**: 2022-04-20 20:54:22+00:00
- **Authors**: Ali Ayub, Alan R. Wagner
- **Comment**: Fixed the link to dataset
- **Journal**: IEEE International Conference on Robotics and Automation (ICRA)
  2021
- **Summary**: Deep learning has achieved remarkable success in object recognition tasks through the availability of large scale datasets like ImageNet. However, deep learning systems suffer from catastrophic forgetting when learning incrementally without replaying old data. For real-world applications, robots also need to incrementally learn new objects. Further, since robots have limited human assistance available, they must learn from only a few examples. However, very few object recognition datasets and benchmarks exist to test incremental learning capability for robotic vision. Further, there is no dataset or benchmark specifically designed for incremental object learning from a few examples. To fill this gap, we present a new dataset termed F-SIOL-310 (Few-Shot Incremental Object Learning) which is specifically captured for testing few-shot incremental object learning capability for robotic vision. We also provide benchmarks and evaluations of 8 incremental learning algorithms on F-SIOL-310 for future comparisons. Our results demonstrate that the few-shot incremental object learning problem for robotic vision is far from being solved.



### Multiview and Multiclass Image Segmentation using Deep Learning in Fetal Echocardiography
- **Arxiv ID**: http://arxiv.org/abs/2103.12245v1
- **DOI**: 10.1117/12.2582191
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.12245v1)
- **Published**: 2021-03-23 00:33:23+00:00
- **Updated**: 2021-03-23 00:33:23+00:00
- **Authors**: Ken C. L. Wong, Elena S. Sinkovskaya, Alfred Z. Abuhamad, Tanveer Syeda-Mahmood
- **Comment**: This paper was accepted by SPIE Medical Imaging 2021
- **Journal**: None
- **Summary**: Congenital heart disease (CHD) is the most common congenital abnormality associated with birth defects in the United States. Despite training efforts and substantial advancement in ultrasound technology over the past years, CHD remains an abnormality that is frequently missed during prenatal ultrasonography. Therefore, computer-aided detection of CHD can play a critical role in prenatal care by improving screening and diagnosis. Since many CHDs involve structural abnormalities, automatic segmentation of anatomical structures is an important step in the analysis of fetal echocardiograms. While existing methods mainly focus on the four-chamber view with a small number of structures, here we present a more comprehensive deep learning segmentation framework covering 14 anatomical structures in both three-vessel trachea and four-chamber views. Specifically, our framework enhances the V-Net with spatial dropout, group normalization, and deep supervision to train a segmentation model that can be applied on both views regardless of abnormalities. By identifying the pitfall of using the Dice loss when some labels are unavailable in some images, this framework integrates information from multiple views and is robust to missing structures due to anatomical anomalies, achieving an average Dice score of 79%.



### Multi-Modal Answer Validation for Knowledge-Based VQA
- **Arxiv ID**: http://arxiv.org/abs/2103.12248v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2103.12248v3)
- **Published**: 2021-03-23 00:49:36+00:00
- **Updated**: 2021-12-14 01:17:03+00:00
- **Authors**: Jialin Wu, Jiasen Lu, Ashish Sabharwal, Roozbeh Mottaghi
- **Comment**: AAAI 2022
- **Journal**: None
- **Summary**: The problem of knowledge-based visual question answering involves answering questions that require external knowledge in addition to the content of the image. Such knowledge typically comes in various forms, including visual, textual, and commonsense knowledge. Using more knowledge sources increases the chance of retrieving more irrelevant or noisy facts, making it challenging to comprehend the facts and find the answer. To address this challenge, we propose Multi-modal Answer Validation using External knowledge (MAVEx), where the idea is to validate a set of promising answer candidates based on answer-specific knowledge retrieval. Instead of searching for the answer in a vast collection of often irrelevant facts as most existing approaches do, MAVEx aims to learn how to extract relevant knowledge from noisy sources, which knowledge source to trust for each answer candidate, and how to validate the candidate using that source. Our multi-modal setting is the first to leverage external visual knowledge (images searched using Google), in addition to textual knowledge in the form of Wikipedia sentences and ConceptNet concepts. Our experiments with OK-VQA, a challenging knowledge-based VQA dataset, demonstrate that MAVEx achieves new state-of-the-art results. Our code is available at https://github.com/jialinwu17/MAVEX



### Deep Implicit Moving Least-Squares Functions for 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2103.12266v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2103.12266v2)
- **Published**: 2021-03-23 02:26:07+00:00
- **Updated**: 2021-04-06 03:14:20+00:00
- **Authors**: Shi-Lin Liu, Hao-Xiang Guo, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang Liu
- **Comment**: Accepted by CVPR 2021, Code: https://github.com/Andy97/DeepMLS
- **Journal**: None
- **Summary**: Point set is a flexible and lightweight representation widely used for 3D deep learning. However, their discrete nature prevents them from representing continuous and fine geometry, posing a major issue for learning-based shape generation. In this work, we turn the discrete point sets into smooth surfaces by introducing the well-known implicit moving least-squares (IMLS) surface formulation, which naturally defines locally implicit functions on point sets. We incorporate IMLS surface generation into deep neural networks for inheriting both the flexibility of point sets and the high quality of implicit surfaces. Our IMLSNet predicts an octree structure as a scaffold for generating MLS points where needed and characterizes shape geometry with learned local priors. Furthermore, our implicit function evaluation is independent of the neural network once the MLS points are predicted, thus enabling fast runtime evaluation. Our experiments on 3D object reconstruction demonstrate that IMLSNets outperform state-of-the-art learning-based methods in terms of reconstruction quality and computational efficiency. Extensive ablation tests also validate our network design and loss functions.



### Dilated SpineNet for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.12270v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.12270v1)
- **Published**: 2021-03-23 02:39:04+00:00
- **Updated**: 2021-03-23 02:39:04+00:00
- **Authors**: Abdullah Rashwan, Xianzhi Du, Xiaoqi Yin, Jing Li
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Scale-permuted networks have shown promising results on object bounding box detection and instance segmentation. Scale permutation and cross-scale fusion of features enable the network to capture multi-scale semantics while preserving spatial resolution. In this work, we evaluate this meta-architecture design on semantic segmentation - another vision task that benefits from high spatial resolution and multi-scale feature fusion at different network stages. By further leveraging dilated convolution operations, we propose SpineNet-Seg, a network discovered by NAS that is searched from the DeepLabv3 system. SpineNet-Seg is designed with a better scale-permuted network topology with customized dilation ratios per block on a semantic segmentation task. SpineNet-Seg models outperform the DeepLabv3/v3+ baselines at all model scales on multiple popular benchmarks in speed and accuracy. In particular, our SpineNet-S143+ model achieves the new state-of-the-art on the popular Cityscapes benchmark at 83.04% mIoU and attained strong performance on the PASCAL VOC2012 benchmark at 85.56% mIoU. SpineNet-Seg models also show promising results on a challenging Street View segmentation dataset. Code and checkpoints will be open-sourced.



### Conditional Training with Bounding Map for Universal Lesion Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.12277v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.12277v1)
- **Published**: 2021-03-23 03:04:13+00:00
- **Updated**: 2021-03-23 03:04:13+00:00
- **Authors**: Han Li, Long Chen, Hu Han, S. Kevin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Universal Lesion Detection (ULD) in computed tomography plays an essential role in computer-aided diagnosis. Promising ULD results have been reported by coarse-to-fine two-stage detection approaches, but such two-stage ULD methods still suffer from issues like imbalance of positive v.s. negative anchors during object proposal and insufficient supervision problem during localization regression and classification of the region of interest (RoI) proposals. While leveraging pseudo segmentation masks such as bounding map (BM) can reduce the above issues to some degree, it is still an open problem to effectively handle the diverse lesion shapes and sizes in ULD. In this paper, we propose a BM-based conditional training for two-stage ULD, which can (i) reduce positive vs. negative anchor imbalance via BM-based conditioning (BMC) mechanism for anchor sampling instead of traditional IoU-based rule; and (ii) adaptively compute size-adaptive BM (ABM) from lesion bounding box, which is used for improving lesion localization accuracy via ABMsupervised segmentation. Experiments with four state-of-the-art methods show that the proposed approach can bring an almost free detection accuracy improvement without requiring expensive lesion mask annotations.



### Learning Comprehensive Motion Representation for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.12278v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12278v1)
- **Published**: 2021-03-23 03:06:26+00:00
- **Updated**: 2021-03-23 03:06:26+00:00
- **Authors**: Mingyu Wu, Boyuan Jiang, Donghao Luo, Junchi Yan, Yabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, Xiaokang Yang
- **Comment**: Accepted by AAAI21
- **Journal**: None
- **Summary**: For action recognition learning, 2D CNN-based methods are efficient but may yield redundant features due to applying the same 2D convolution kernel to each frame. Recent efforts attempt to capture motion information by establishing inter-frame connections while still suffering the limited temporal receptive field or high latency. Moreover, the feature enhancement is often only performed by channel or space dimension in action recognition. To address these issues, we first devise a Channel-wise Motion Enhancement (CME) module to adaptively emphasize the channels related to dynamic information with a channel-wise gate vector. The channel gates generated by CME incorporate the information from all the other frames in the video. We further propose a Spatial-wise Motion Enhancement (SME) module to focus on the regions with the critical target in motion, according to the point-to-point similarity between adjacent feature maps. The intuition is that the change of background is typically slower than the motion area. Both CME and SME have clear physical meaning in capturing action clues. By integrating the two modules into the off-the-shelf 2D network, we finally obtain a Comprehensive Motion Representation (CMR) learning method for action recognition, which achieves competitive performance on Something-Something V1 & V2 and Kinetics-400. On the temporal reasoning datasets Something-Something V1 and V2, our method outperforms the current state-of-the-art by 2.3% and 1.9% when using 16 frames as input, respectively.



### Optimising the selection of samples for robust lidar camera calibration
- **Arxiv ID**: http://arxiv.org/abs/2103.12287v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, 68U10, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2103.12287v2)
- **Published**: 2021-03-23 03:46:15+00:00
- **Updated**: 2021-09-22 05:46:44+00:00
- **Authors**: Darren Tsai, Stewart Worrall, Mao Shan, Anton Lohr, Eduardo Nebot
- **Comment**: ITSC2021
- **Journal**: None
- **Summary**: We propose a robust calibration pipeline that optimises the selection of calibration samples for the estimation of calibration parameters that fit the entire scene. We minimise user error by automating the data selection process according to a metric, called Variability of Quality (VOQ) that gives a score to each calibration set of samples. We show that this VOQ score is correlated with the estimated calibration parameter's ability to generalise well to the entire scene, thereby overcoming the overfitting problems of existing calibration algorithms. Our approach has the benefits of simplifying the calibration process for practitioners of any calibration expertise level and providing an objective measure of the quality for our calibration pipeline's input and output data. We additionally use a novel method of assessing the accuracy of the calibration parameters. It involves computing reprojection errors for the entire scene to ensure that the parameters are well fitted to all features in the scene. Our proposed calibration pipeline takes 90s, and obtains an average reprojection error of 1-1.2cm, with standard deviation of 0.4-0.5cm over 46 poses evenly distributed in a scene. This process has been validated by experimentation on a high resolution, software definable lidar, Baraja Spectrum-Scan; and a low, fixed resolution lidar, Velodyne VLP-16. We have shown that despite the vast differences in lidar technologies, our proposed approach manages to estimate robust calibration parameters for both. Our code and data set used for this paper are made available as open-source.



### NDT-Transformer: Large-Scale 3D Point Cloud Localisation using the Normal Distribution Transform Representation
- **Arxiv ID**: http://arxiv.org/abs/2103.12292v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.12292v1)
- **Published**: 2021-03-23 04:04:38+00:00
- **Updated**: 2021-03-23 04:04:38+00:00
- **Authors**: Zhicheng Zhou, Cheng Zhao, Daniel Adolfsson, Songzhi Su, Yang Gao, Tom Duckett, Li Sun
- **Comment**: To be appear in ICRA2021
- **Journal**: None
- **Summary**: 3D point cloud-based place recognition is highly demanded by autonomous driving in GPS-challenged environments and serves as an essential component (i.e. loop-closure detection) in lidar-based SLAM systems. This paper proposes a novel approach, named NDT-Transformer, for realtime and large-scale place recognition using 3D point clouds. Specifically, a 3D Normal Distribution Transform (NDT) representation is employed to condense the raw, dense 3D point cloud as probabilistic distributions (NDT cells) to provide the geometrical shape description. Then a novel NDT-Transformer network learns a global descriptor from a set of 3D NDT cell representations. Benefiting from the NDT representation and NDT-Transformer network, the learned global descriptors are enriched with both geometrical and contextual information. Finally, descriptor retrieval is achieved using a query-database for place recognition. Compared to the state-of-the-art methods, the proposed approach achieves an improvement of 7.52% on average top 1 recall and 2.73% on average top 1% recall on the Oxford Robotcar benchmark.



### Gradient Regularized Contrastive Learning for Continual Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2103.12294v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12294v1)
- **Published**: 2021-03-23 04:10:42+00:00
- **Updated**: 2021-03-23 04:10:42+00:00
- **Authors**: Shixiang Tang, Peng Su, Dapeng Chen, Wanli Ouyang
- **Comment**: Accepted by AAAI2021 (poster). arXiv admin note: text overlap with
  arXiv:2007.12942
- **Journal**: None
- **Summary**: Human beings can quickly adapt to environmental changes by leveraging learning experience. However, adapting deep neural networks to dynamic environments by machine learning algorithms remains a challenge. To better understand this issue, we study the problem of continual domain adaptation, where the model is presented with a labelled source domain and a sequence of unlabelled target domains. The obstacles in this problem are both domain shift and catastrophic forgetting. We propose Gradient Regularized Contrastive Learning (GRCL) to solve the obstacles. At the core of our method, gradient regularization plays two key roles: (1) enforcing the gradient not to harm the discriminative ability of source features which can, in turn, benefit the adaptation ability of the model to target domains; (2) constraining the gradient not to increase the classification loss on old target domains, which enables the model to preserve the performance on old target domains when adapting to an in-coming target domain. Experiments on Digits, DomainNet and Office-Caltech benchmarks demonstrate the strong performance of our approach when compared to the other state-of-the-art methods.



### Adaptive Illumination based Depth Sensing using Deep Superpixel and Soft Sampling Approximation
- **Arxiv ID**: http://arxiv.org/abs/2103.12297v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12297v2)
- **Published**: 2021-03-23 04:21:07+00:00
- **Updated**: 2022-02-22 07:55:38+00:00
- **Authors**: Qiqin Dai, Fengqiang Li, Oliver Cossairt, Aggelos K Katsaggelos
- **Comment**: None
- **Journal**: None
- **Summary**: Dense depth map capture is challenging in existing active sparse illumination based depth acquisition techniques, such as LiDAR. Various techniques have been proposed to estimate a dense depth map based on fusion of the sparse depth map measurement with the RGB image. Recent advances in hardware enable adaptive depth measurements resulting in further improvement of the dense depth map estimation. In this paper, we study the topic of estimating dense depth from depth sampling. The adaptive sparse depth sampling network is jointly trained with a fusion network of an RGB image and sparse depth, to generate optimal adaptive sampling masks. We show that such adaptive sampling masks can generalize well to many RGB and sparse depth fusion algorithms under a variety of sampling rates (as low as $0.0625\%$). The proposed adaptive sampling method is fully differentiable and flexible to be trained end-to-end with upstream perception algorithms.



### IAIA-BL: A Case-based Interpretable Deep Learning Model for Classification of Mass Lesions in Digital Mammography
- **Arxiv ID**: http://arxiv.org/abs/2103.12308v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, I.2.6; I.4.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2103.12308v1)
- **Published**: 2021-03-23 05:00:21+00:00
- **Updated**: 2021-03-23 05:00:21+00:00
- **Authors**: Alina Jade Barnett, Fides Regina Schwartz, Chaofan Tao, Chaofan Chen, Yinhao Ren, Joseph Y. Lo, Cynthia Rudin
- **Comment**: 24 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: Interpretability in machine learning models is important in high-stakes decisions, such as whether to order a biopsy based on a mammographic exam. Mammography poses important challenges that are not present in other computer vision tasks: datasets are small, confounding information is present, and it can be difficult even for a radiologist to decide between watchful waiting and biopsy based on a mammogram alone. In this work, we present a framework for interpretable machine learning-based mammography. In addition to predicting whether a lesion is malignant or benign, our work aims to follow the reasoning processes of radiologists in detecting clinically relevant semantic features of each image, such as the characteristics of the mass margins. The framework includes a novel interpretable neural network algorithm that uses case-based reasoning for mammography. Our algorithm can incorporate a combination of data with whole image labelling and data with pixel-wise annotations, leading to better accuracy and interpretability even with a small number of images. Our interpretable models are able to highlight the classification-relevant parts of the image, whereas other methods highlight healthy tissue and confounding information. Our models are decision aids, rather than decision makers, aimed at better overall human-machine collaboration. We do not observe a loss in mass margin classification accuracy over a black box neural network trained on the same data.



### SuctionNet-1Billion: A Large-Scale Benchmark for Suction Grasping
- **Arxiv ID**: http://arxiv.org/abs/2103.12311v2
- **DOI**: 10.1109/LRA.2021.3115406
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.12311v2)
- **Published**: 2021-03-23 05:02:52+00:00
- **Updated**: 2021-10-29 06:19:54+00:00
- **Authors**: Hanwen Cao, Hao-Shu Fang, Wenhai Liu, Cewu Lu
- **Comment**: None
- **Journal**: IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 6, NO. 4, 2021
- **Summary**: Suction is an important solution for the longstanding robotic grasping problem. Compared with other kinds of grasping, suction grasping is easier to represent and often more reliable in practice. Though preferred in many scenarios, it is not fully investigated and lacks sufficient training data and evaluation benchmarks. To address that, firstly, we propose a new physical model to analytically evaluate seal formation and wrench resistance of a suction grasping, which are two key aspects of grasp success. Secondly, a two-step methodology is adopted to generate annotations on a large-scale dataset collected in real-world cluttered scenarios. Thirdly, a standard online evaluation system is proposed to evaluate suction poses in continuous operation space, which can benchmark different algorithms fairly without the need of exhaustive labeling. Real-robot experiments are conducted to show that our annotations align well with real world. Meanwhile, we propose a method to predict numerous suction poses from an RGB-D image of a cluttered scene and demonstrate our superiority against several previous methods. Result analyses are further provided to help readers better understand the challenges in this area. Data and source code are publicly available at www.graspnet.net.



### Enhanced Spatio-Temporal Interaction Learning for Video Deraining: A Faster and Better Framework
- **Arxiv ID**: http://arxiv.org/abs/2103.12318v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12318v2)
- **Published**: 2021-03-23 05:19:35+00:00
- **Updated**: 2022-01-20 02:54:25+00:00
- **Authors**: Kaihao Zhang, Dongxu Li, Wenhan Luo, Wenqi Ren, Wei Liu
- **Comment**: To appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)
- **Journal**: None
- **Summary**: Video deraining is an important task in computer vision as the unwanted rain hampers the visibility of videos and deteriorates the robustness of most outdoor vision systems. Despite the significant success which has been achieved for video deraining recently, two major challenges remain: 1) how to exploit the vast information among continuous frames to extract powerful spatio-temporal features across both the spatial and temporal domains, and 2) how to restore high-quality derained videos with a high-speed approach. In this paper, we present a new end-to-end video deraining framework, named Enhanced Spatio-Temporal Interaction Network (ESTINet), which considerably boosts current state-of-the-art video deraining quality and speed. The ESTINet takes the advantage of deep residual networks and convolutional long short-term memory, which can capture the spatial features and temporal correlations among continuing frames at the cost of very little computational source. Extensive experiments on three public datasets show that the proposed ESTINet can achieve faster speed than the competitors, while maintaining better performance than the state-of-the-art methods.



### Extracting Causal Visual Features for Limited label Classification
- **Arxiv ID**: http://arxiv.org/abs/2103.12322v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.12322v1)
- **Published**: 2021-03-23 05:42:20+00:00
- **Updated**: 2021-03-23 05:42:20+00:00
- **Authors**: Mohit Prabhushankar, Ghassan AlRegib
- **Comment**: Submitted to IEEE International Conference on Image Processing (ICIP)
- **Journal**: None
- **Summary**: Neural networks trained to classify images do so by identifying features that allow them to distinguish between classes. These sets of features are either causal or context dependent. Grad-CAM is a popular method of visualizing both sets of features. In this paper, we formalize this feature divide and provide a methodology to extract causal features from Grad-CAM. We do so by defining context features as those features that allow contrast between predicted class and any contrast class. We then apply a set theoretic approach to separate causal from contrast features for COVID-19 CT scans. We show that on average, the image regions with the proposed causal features require 15% less bits when encoded using Huffman encoding, compared to Grad-CAM, for an average increase of 3% classification accuracy, over Grad-CAM. Moreover, we validate the transfer-ability of causal features between networks and comment on the non-human interpretable causal nature of current networks.



### Decomposing Normal and Abnormal Features of Medical Images into Discrete Latent Codes for Content-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2103.12328v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12328v1)
- **Published**: 2021-03-23 05:53:53+00:00
- **Updated**: 2021-03-23 05:53:53+00:00
- **Authors**: Kazuma Kobayashi, Ryuichiro Hataya, Yusuke Kurose, Mototaka Miyake, Masamichi Takahashi, Akiko Nakagawa, Tatsuya Harada, Ryuji Hamamoto
- **Comment**: None
- **Journal**: None
- **Summary**: In medical imaging, the characteristics purely derived from a disease should reflect the extent to which abnormal findings deviate from the normal features. Indeed, physicians often need corresponding images without abnormal findings of interest or, conversely, images that contain similar abnormal findings regardless of normal anatomical context. This is called comparative diagnostic reading of medical images, which is essential for a correct diagnosis. To support comparative diagnostic reading, content-based image retrieval (CBIR), which can selectively utilize normal and abnormal features in medical images as two separable semantic components, will be useful. Therefore, we propose a neural network architecture to decompose the semantic components of medical images into two latent codes: normal anatomy code and abnormal anatomy code. The normal anatomy code represents normal anatomies that should have existed if the sample is healthy, whereas the abnormal anatomy code attributes to abnormal changes that reflect deviation from the normal baseline. These latent codes are discretized through vector quantization to enable binary hashing, which can reduce the computational burden at the time of similarity search. By calculating the similarity based on either normal or abnormal anatomy codes or the combination of the two codes, our algorithm can retrieve images according to the selected semantic component from a dataset consisting of brain magnetic resonance images of gliomas. Our CBIR system qualitatively and quantitatively achieves remarkable results.



### Contrastive Reasoning in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.12329v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.12329v1)
- **Published**: 2021-03-23 05:54:36+00:00
- **Updated**: 2021-03-23 05:54:36+00:00
- **Authors**: Mohit Prabhushankar, Ghassan AlRegib
- **Comment**: Submitted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: Neural networks represent data as projections on trained weights in a high dimensional manifold. The trained weights act as a knowledge base consisting of causal class dependencies. Inference built on features that identify these dependencies is termed as feed-forward inference. Such inference mechanisms are justified based on classical cause-to-effect inductive reasoning models. Inductive reasoning based feed-forward inference is widely used due to its mathematical simplicity and operational ease. Nevertheless, feed-forward models do not generalize well to untrained situations. To alleviate this generalization challenge, we propose using an effect-to-cause inference model that reasons abductively. Here, the features represent the change from existing weight dependencies given a certain effect. We term this change as contrast and the ensuing reasoning mechanism as contrastive reasoning. In this paper, we formalize the structure of contrastive reasoning and propose a methodology to extract a neural network's notion of contrast. We demonstrate the value of contrastive reasoning in two stages of a neural network's reasoning pipeline : in inferring and visually explaining decisions for the application of object recognition. We illustrate the value of contrastively recognizing images under distortions by reporting an improvement of 3.47%, 2.56%, and 5.48% in average accuracy under the proposed contrastive framework on CIFAR-10C, noisy STL-10, and VisDA datasets respectively.



### Salient Image Matting
- **Arxiv ID**: http://arxiv.org/abs/2103.12337v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12337v1)
- **Published**: 2021-03-23 06:22:33+00:00
- **Updated**: 2021-03-23 06:22:33+00:00
- **Authors**: Rahul Deora, Rishab Sharma, Dinesh Samuel Sathia Raj
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an image matting framework called Salient Image Matting to estimate the per-pixel opacity value of the most salient foreground in an image. To deal with a large amount of semantic diversity in images, a trimap is conventionally required as it provides important guidance about object semantics to the matting process. However, creating a good trimap is often expensive and timeconsuming. The SIM framework simultaneously deals with the challenge of learning a wide range of semantics and salient object types in a fully automatic and an end to end manner. Specifically, our framework is able to produce accurate alpha mattes for a wide range of foreground objects and cases where the foreground class, such as human, appears in a very different context than the train data directly from an RGB input. This is done by employing a salient object detection model to produce a trimap of the most salient object in the image in order to guide the matting model about higher-level object semantics. Our framework leverages large amounts of coarse annotations coupled with a heuristic trimap generation scheme to train the trimap prediction network so it can produce trimaps for arbitrary foregrounds. Moreover, we introduce a multi-scale fusion architecture for the task of matting to better capture finer, low-level opacity semantics. With high-level guidance provided by the trimap network, our framework requires only a fraction of expensive matting data as compared to other automatic methods while being able to produce alpha mattes for a diverse range of inputs. We demonstrate our framework on a range of diverse images and experimental results show our framework compares favourably against state of art matting methods without the need for a trimap



### Generalized Domain Conditioned Adaptation Network
- **Arxiv ID**: http://arxiv.org/abs/2103.12339v1
- **DOI**: 10.1109/TPAMI.2021.3062644
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12339v1)
- **Published**: 2021-03-23 06:24:26+00:00
- **Updated**: 2021-03-23 06:24:26+00:00
- **Authors**: Shuang Li, Binhui Xie, Qiuxia Lin, Chi Harold Liu, Gao Huang, Guoren Wang
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (T-PAMI). Journal version of arXiv:2005.06717 (AAAI 2020). Code
  is available at https://github.com/BIT-DA/GDCAN
- **Journal**: None
- **Summary**: Domain Adaptation (DA) attempts to transfer knowledge learned in the labeled source domain to the unlabeled but related target domain without requiring large amounts of target supervision. Recent advances in DA mainly proceed by aligning the source and target distributions. Despite the significant success, the adaptation performance still degrades accordingly when the source and target domains encounter a large distribution discrepancy. We consider this limitation may attribute to the insufficient exploration of domain-specialized features because most studies merely concentrate on domain-general feature learning in task-specific layers and integrate totally-shared convolutional networks (convnets) to generate common features for both domains. In this paper, we relax the completely-shared convnets assumption adopted by previous DA methods and propose Domain Conditioned Adaptation Network (DCAN), which introduces domain conditioned channel attention module with a multi-path structure to separately excite channel activation for each domain. Such a partially-shared convnets module allows domain-specialized features in low-level to be explored appropriately. Further, given the knowledge transferability varying along with convolutional layers, we develop Generalized Domain Conditioned Adaptation Network (GDCAN) to automatically determine whether domain channel activations should be separately modeled in each attention module. Afterward, the critical domain-specialized knowledge could be adaptively extracted according to the domain statistic gaps. As far as we know, this is the first work to explore the domain-wise convolutional channel activations separately for deep DA networks. Additionally, to effectively match high-level feature distributions across domains, we consider deploying feature adaptation blocks after task-specific layers, which can explicitly mitigate the domain discrepancy.



### Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers
- **Arxiv ID**: http://arxiv.org/abs/2103.12340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12340v1)
- **Published**: 2021-03-23 06:25:42+00:00
- **Updated**: 2021-03-23 06:25:42+00:00
- **Authors**: Lei Ke, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: Accepted by CVPR2021. BCNet Code: https://github.com/lkeab/BCNet
- **Journal**: None
- **Summary**: Segmenting highly-overlapping objects is challenging, because typically no distinction is made between real object contours and occlusion boundaries. Unlike previous two-stage instance segmentation methods, we model image formation as composition of two overlapping layers, and propose Bilayer Convolutional Network (BCNet), where the top GCN layer detects the occluding objects (occluder) and the bottom GCN layer infers partially occluded instance (occludee). The explicit modeling of occlusion relationship with bilayer structure naturally decouples the boundaries of both the occluding and occluded instances, and considers the interaction between them during mask regression. We validate the efficacy of bilayer decoupling on both one-stage and two-stage object detectors with different backbones and network layer choices. Despite its simplicity, extensive experiments on COCO and KINS show that our occlusion-aware BCNet achieves large and consistent performance gain especially for heavy occlusion cases. Code is available at https://github.com/lkeab/BCNet.



### Joint Distribution across Representation Space for Out-of-Distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.12344v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.12344v2)
- **Published**: 2021-03-23 06:39:29+00:00
- **Updated**: 2021-03-30 03:47:32+00:00
- **Authors**: JingWei Xu, Siyuan Zhu, Zenan Li, Chang Xu
- **Comment**: Submitted to a conference of computer vision
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have become a key part of many modern software applications. After training and validating, the DNN is deployed as an irrevocable component and applied in real-world scenarios. Although most DNNs are built meticulously with huge volumes of training data, data in the real world still remain unknown to the DNN model, which leads to the crucial requirement of runtime out-of-distribution (OOD) detection. However, many existing approaches 1) need OOD data for classifier training or parameter tuning, or 2) simply combine the scores of each hidden layer as an ensemble of features for OOD detection. In this paper, we present a novel outlook on in-distribution data in a generative manner, which takes their latent features generated from each hidden layer as a joint distribution across representation spaces. Since only the in-distribution latent features are comprehensively understood in representation space, the internal difference between in-distribution and OOD data can be naturally revealed without the intervention of any OOD data. Specifically, We construct a generative model, called Latent Sequential Gaussian Mixture (LSGM), to depict how the in-distribution latent features are generated in terms of the trace of DNN inference across representation spaces. We first construct the Gaussian Mixture Model (GMM) based on in-distribution latent features for each hidden layer, and then connect GMMs via the transition probabilities of the inference traces. Experimental evaluations on popular benchmark OOD datasets and models validate the superiority of the proposed method over the state-of-the-art methods in OOD detection.



### Co-Grounding Networks with Semantic Attention for Referring Expression Comprehension in Videos
- **Arxiv ID**: http://arxiv.org/abs/2103.12346v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12346v1)
- **Published**: 2021-03-23 06:42:49+00:00
- **Updated**: 2021-03-23 06:42:49+00:00
- **Authors**: Sijie Song, Xudong Lin, Jiaying Liu, Zongming Guo, Shih-Fu Chang
- **Comment**: Accepted to CVPR2021. The project page is at
  https://sijiesong.github.io/co-grounding
- **Journal**: None
- **Summary**: In this paper, we address the problem of referring expression comprehension in videos, which is challenging due to complex expression and scene dynamics. Unlike previous methods which solve the problem in multiple stages (i.e., tracking, proposal-based matching), we tackle the problem from a novel perspective, \textbf{co-grounding}, with an elegant one-stage framework. We enhance the single-frame grounding accuracy by semantic attention learning and improve the cross-frame grounding consistency with co-grounding feature learning. Semantic attention learning explicitly parses referring cues in different attributes to reduce the ambiguity in the complex expression. Co-grounding feature learning boosts visual feature representations by integrating temporal correlation to reduce the ambiguity caused by scene dynamics. Experiment results demonstrate the superiority of our framework on the video grounding datasets VID and LiOTB in generating accurate and stable results across frames. Our model is also applicable to referring expression comprehension in images, illustrated by the improved performance on the RefCOCO dataset. Our project is available at https://sijiesong.github.io/co-grounding.



### Shared Latent Space of Font Shapes and Their Noisy Impressions
- **Arxiv ID**: http://arxiv.org/abs/2103.12347v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12347v3)
- **Published**: 2021-03-23 06:54:45+00:00
- **Updated**: 2021-11-02 11:39:21+00:00
- **Authors**: Jihun Kang, Daichi Haraguchi, Seiya Matsuda, Akisato Kimura, Seiichi Uchida
- **Comment**: accepted at MMM2022
- **Journal**: None
- **Summary**: Styles of typefaces or fonts are often associated with specific impressions, such as heavy, contemporary, or elegant. This indicates that there are certain correlations between font shapes and their impressions. To understand the correlations, this paper realizes a shared latent space where a font and its impressions are embedded nearby. The difficulty is that the impression words attached to a font are often very noisy. This is because impression words are very subjective and diverse. More importantly, some impression words have no direct relevance to the font shapes and will disturb the realization of the shared latent space. We, therefore, use DeepSets for enhancing shape-relevant words and suppressing shape irrelevant words automatically while training the shared latent space. Quantitative and qualitative experimental results with a large-scale font-impression dataset demonstrate that the shared latent space by the proposed method describes the correlation appropriately, especially for the shape-relevant impression words.



### Roughness Index and Roughness Distance for Benchmarking Medical Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.12350v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.12350v1)
- **Published**: 2021-03-23 07:19:32+00:00
- **Updated**: 2021-03-23 07:19:32+00:00
- **Authors**: Vidhiwar Singh Rathour, Kashu Yamakazi, T. Hoang Ngan Le
- **Comment**: Paper has been accepted at BIOIMAGING2021
- **Journal**: None
- **Summary**: Medical image segmentation is one of the most challenging tasks in medical image analysis and has been widely developed for many clinical applications. Most of the existing metrics have been first designed for natural images and then extended to medical images. While object surface plays an important role in medical segmentation and quantitative analysis i.e. analyze brain tumor surface, measure gray matter volume, most of the existing metrics are limited when it comes to analyzing the object surface, especially to tell about surface smoothness or roughness of a given volumetric object or to analyze the topological errors. In this paper, we first analysis both pros and cons of all existing medical image segmentation metrics, specially on volumetric data. We then propose an appropriate roughness index and roughness distance for medical image segmentation analysis and evaluation. Our proposed method addresses two kinds of segmentation errors, i.e. (i)topological errors on boundary/surface and (ii)irregularities on the boundary/surface. The contribution of this work is four-fold: (i) detect irregular spikes/holes on a surface, (ii) propose roughness index to measure surface roughness of a given object, (iii) propose a roughness distance to measure the distance of two boundaries/surfaces by utilizing the proposed roughness index and (iv) suggest an algorithm which helps to remove the irregular spikes/holes to smooth the surface. Our proposed roughness index and roughness distance are built upon the solid surface roughness parameter which has been successfully developed in the civil engineering.



### iMAP: Implicit Mapping and Positioning in Real-Time
- **Arxiv ID**: http://arxiv.org/abs/2103.12352v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12352v2)
- **Published**: 2021-03-23 07:21:22+00:00
- **Updated**: 2021-09-13 17:04:05+00:00
- **Authors**: Edgar Sucar, Shikun Liu, Joseph Ortiz, Andrew J. Davison
- **Comment**: Typos, make pdf smaller
- **Journal**: None
- **Summary**: We show for the first time that a multilayer perceptron (MLP) can serve as the only scene representation in a real-time SLAM system for a handheld RGB-D camera. Our network is trained in live operation without prior data, building a dense, scene-specific implicit 3D model of occupancy and colour which is also immediately used for tracking.   Achieving real-time SLAM via continual training of a neural network against a live image stream requires significant innovation. Our iMAP algorithm uses a keyframe structure and multi-processing computation flow, with dynamic information-guided pixel sampling for speed, with tracking at 10 Hz and global map updating at 2 Hz. The advantages of an implicit MLP over standard dense SLAM techniques include efficient geometry representation with automatic detail control and smooth, plausible filling-in of unobserved regions such as the back surfaces of objects.



### A Sub-Layered Hierarchical Pyramidal Neural Architecture for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.12362v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12362v1)
- **Published**: 2021-03-23 07:50:33+00:00
- **Updated**: 2021-03-23 07:50:33+00:00
- **Authors**: Henrique Siqueira, Pablo Barros, Sven Magg, Cornelius Weber, Stefan Wermter
- **Comment**: None
- **Journal**: None
- **Summary**: In domains where computational resources and labeled data are limited, such as in robotics, deep networks with millions of weights might not be the optimal solution. In this paper, we introduce a connectivity scheme for pyramidal architectures to increase their capacity for learning features. Experiments on facial expression recognition of unseen people demonstrate that our approach is a potential candidate for applications with restricted resources, due to good generalization performance and low computational cost. We show that our approach generalizes as well as convolutional architectures in this task but uses fewer trainable parameters and is more robust for low-resolution faces.



### Group-aware Label Transfer for Domain Adaptive Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2103.12366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12366v1)
- **Published**: 2021-03-23 07:57:39+00:00
- **Updated**: 2021-03-23 07:57:39+00:00
- **Authors**: Kecheng Zheng, Wu Liu, Lingxiao He, Tao Mei, Jiebo Luo, Zheng-Jun Zha
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptive (UDA) person re-identification (ReID) aims at adapting the model trained on a labeled source-domain dataset to a target-domain dataset without any further annotations. Most successful UDA-ReID approaches combine clustering-based pseudo-label prediction with representation learning and perform the two steps in an alternating fashion. However, offline interaction between these two steps may allow noisy pseudo labels to substantially hinder the capability of the model. In this paper, we propose a Group-aware Label Transfer (GLT) algorithm, which enables the online interaction and mutual promotion of pseudo-label prediction and representation learning. Specifically, a label transfer algorithm simultaneously uses pseudo labels to train the data while refining the pseudo labels as an online clustering algorithm. It treats the online label refinery problem as an optimal transport problem, which explores the minimum cost for assigning M samples to N pseudo labels. More importantly, we introduce a group-aware strategy to assign implicit attribute group IDs to samples. The combination of the online label refining algorithm and the group-aware strategy can better correct the noisy pseudo label in an online fashion and narrow down the search space of the target identity. The effectiveness of the proposed GLT is demonstrated by the experimental results (Rank-1 accuracy) for Market1501$\to$DukeMTMC (82.0\%) and DukeMTMC$\to$Market1501 (92.2\%), remarkably closing the gap between unsupervised and supervised performance on person re-identification.



### ReCU: Reviving the Dead Weights in Binary Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.12369v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.12369v2)
- **Published**: 2021-03-23 08:11:20+00:00
- **Updated**: 2021-08-02 08:32:05+00:00
- **Authors**: Zihan Xu, Mingbao Lin, Jianzhuang Liu, Jie Chen, Ling Shao, Yue Gao, Yonghong Tian, Rongrong Ji
- **Comment**: Accepted by International Conference on Computer Vision 2021 (ICCV
  2021)
- **Journal**: None
- **Summary**: Binary neural networks (BNNs) have received increasing attention due to their superior reductions of computation and memory. Most existing works focus on either lessening the quantization error by minimizing the gap between the full-precision weights and their binarization or designing a gradient approximation to mitigate the gradient mismatch, while leaving the "dead weights" untouched. This leads to slow convergence when training BNNs. In this paper, for the first time, we explore the influence of "dead weights" which refer to a group of weights that are barely updated during the training of BNNs, and then introduce rectified clamp unit (ReCU) to revive the "dead weights" for updating. We prove that reviving the "dead weights" by ReCU can result in a smaller quantization error. Besides, we also take into account the information entropy of the weights, and then mathematically analyze why the weight standardization can benefit BNNs. We demonstrate the inherent contradiction between minimizing the quantization error and maximizing the information entropy, and then propose an adaptive exponential scheduler to identify the range of the "dead weights". By considering the "dead weights", our method offers not only faster BNN training, but also state-of-the-art performance on CIFAR-10 and ImageNet, compared with recent methods. Code can be available at https://github.com/z-hXu/ReCU.



### Unsupervised domain adaptation via coarse-to-fine feature alignment method using contrastive learning
- **Arxiv ID**: http://arxiv.org/abs/2103.12371v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12371v1)
- **Published**: 2021-03-23 08:12:28+00:00
- **Updated**: 2021-03-23 08:12:28+00:00
- **Authors**: Shiyu Tang, Peijun Tang, Yanxiang Gong, Zheng Ma, Mei Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Previous feature alignment methods in Unsupervised domain adaptation(UDA) mostly only align global features without considering the mismatch between class-wise features. In this work, we propose a new coarse-to-fine feature alignment method using contrastive learning called CFContra. It draws class-wise features closer than coarse feature alignment or class-wise feature alignment only, therefore improves the model's performance to a great extent. We build it upon one of the most effective methods of UDA called entropy minimization to further improve performance. In particular, to prevent excessive memory occupation when applying contrastive loss in semantic segmentation, we devise a new way to build and update the memory bank. In this way, we make the algorithm more efficient and viable with limited memory. Extensive experiments show the effectiveness of our method and model trained on the GTA5 to Cityscapes dataset has boost mIOU by 3.5 compared to the MinEnt algorithm. Our code will be publicly available.



### Generalizing Face Forgery Detection with High-frequency Features
- **Arxiv ID**: http://arxiv.org/abs/2103.12376v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12376v1)
- **Published**: 2021-03-23 08:19:21+00:00
- **Updated**: 2021-03-23 08:19:21+00:00
- **Authors**: Yuchen Luo, Yong Zhang, Junchi Yan, Wei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Current face forgery detection methods achieve high accuracy under the within-database scenario where training and testing forgeries are synthesized by the same algorithm. However, few of them gain satisfying performance under the cross-database scenario where training and testing forgeries are synthesized by different algorithms. In this paper, we find that current CNN-based detectors tend to overfit to method-specific color textures and thus fail to generalize. Observing that image noises remove color textures and expose discrepancies between authentic and tampered regions, we propose to utilize the high-frequency noises for face forgery detection. We carefully devise three functional modules to take full advantage of the high-frequency features. The first is the multi-scale high-frequency feature extraction module that extracts high-frequency noises at multiple scales and composes a novel modality. The second is the residual-guided spatial attention module that guides the low-level RGB feature extractor to concentrate more on forgery traces from a new perspective. The last is the cross-modality attention module that leverages the correlation between the two complementary modalities to promote feature learning for each other. Comprehensive evaluations on several benchmark databases corroborate the superior generalization performance of our proposed method.



### OFFSEG: A Semantic Segmentation Framework For Off-Road Driving
- **Arxiv ID**: http://arxiv.org/abs/2103.12417v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.12417v1)
- **Published**: 2021-03-23 09:45:41+00:00
- **Updated**: 2021-03-23 09:45:41+00:00
- **Authors**: Kasi Viswanath, Kartikeya Singh, Peng Jiang, Sujit P. B., Srikanth Saripalli
- **Comment**: None
- **Journal**: None
- **Summary**: Off-road image semantic segmentation is challenging due to the presence of uneven terrains, unstructured class boundaries, irregular features and strong textures. These aspects affect the perception of the vehicle from which the information is used for path planning. Current off-road datasets exhibit difficulties like class imbalance and understanding of varying environmental topography. To overcome these issues we propose a framework for off-road semantic segmentation called as OFFSEG that involves (i) a pooled class semantic segmentation with four classes (sky, traversable region, non-traversable region and obstacle) using state-of-the-art deep learning architectures (ii) a colour segmentation methodology to segment out specific sub-classes (grass, puddle, dirt, gravel, etc.) from the traversable region for better scene understanding. The evaluation of the framework is carried out on two off-road driving datasets, namely, RELLIS-3D and RUGD. We have also tested proposed framework in IISERB campus frames. The results show that OFFSEG achieves good performance and also provides detailed information on the traversable region.



### BossNAS: Exploring Hybrid CNN-transformers with Block-wisely Self-supervised Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2103.12424v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.12424v3)
- **Published**: 2021-03-23 10:05:58+00:00
- **Updated**: 2021-08-19 07:19:30+00:00
- **Authors**: Changlin Li, Tao Tang, Guangrun Wang, Jiefeng Peng, Bing Wang, Xiaodan Liang, Xiaojun Chang
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: A myriad of recent breakthroughs in hand-crafted neural architectures for visual recognition have highlighted the urgent need to explore hybrid architectures consisting of diversified building blocks. Meanwhile, neural architecture search methods are surging with an expectation to reduce human efforts. However, whether NAS methods can efficiently and effectively handle diversified search spaces with disparate candidates (e.g. CNNs and transformers) is still an open question. In this work, we present Block-wisely Self-supervised Neural Architecture Search (BossNAS), an unsupervised NAS method that addresses the problem of inaccurate architecture rating caused by large weight-sharing space and biased supervision in previous methods. More specifically, we factorize the search space into blocks and utilize a novel self-supervised training scheme, named ensemble bootstrapping, to train each block separately before searching them as a whole towards the population center. Additionally, we present HyTra search space, a fabric-like hybrid CNN-transformer search space with searchable down-sampling positions. On this challenging search space, our searched model, BossNet-T, achieves up to 82.5% accuracy on ImageNet, surpassing EfficientNet by 2.4% with comparable compute time. Moreover, our method achieves superior architecture rating accuracy with 0.78 and 0.76 Spearman correlation on the canonical MBConv search space with ImageNet and on NATS-Bench size search space with CIFAR-100, respectively, surpassing state-of-the-art NAS methods. Code: https://github.com/changlin31/BossNAS



### Recent Ice Trends in Swiss Mountain Lakes: 20-year Analysis of MODIS Imagery
- **Arxiv ID**: http://arxiv.org/abs/2103.12434v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12434v4)
- **Published**: 2021-03-23 10:25:02+00:00
- **Updated**: 2022-11-29 15:03:01+00:00
- **Authors**: Manu Tom, Tianyu Wu, Emmanuel Baltsavias, Konrad Schindler
- **Comment**: This version of the article has been accepted for publication, after
  peer review but is not the Version of Record and does not reflect
  post-acceptance improvements, or any corrections. The Version of Record is
  available online at: http://dx.doi.org/10.1007/s41064-022-00215-x
- **Journal**: None
- **Summary**: Depleting lake ice is a climate change indicator, just like sea-level rise or glacial retreat. Monitoring Lake Ice Phenology (LIP) is useful because long-term freezing and thawing patterns serve as sentinels to understand regional and global climate change. We report a study for the Oberengadin region of Switzerland, where several small- and medium-sized mountain lakes are located. We observe the LIP events, such as freeze-up, break-up and ice cover duration, across two decades (2000-2020) from optical satellite images. We analyse the time series of MODIS imagery by estimating spatially resolved maps of lake ice for these Alpine lakes with supervised machine learning. To train the classifier we rely on reference data annotated manually based on webcam images. From the ice maps, we derive long-term LIP trends. Since the webcam data are only available for two winters, we cross-check our results against the operational MODIS and VIIRS snow products. We find a change in complete freeze duration of -0.76 and -0.89 days per annum for lakes Sils and Silvaplana, respectively. Furthermore, we observe plausible correlations of the LIP trends with climate data measured at nearby meteorological stations. We notice that mean winter air temperature has a negative correlation with the freeze duration and break-up events and a positive correlation with the freeze-up events. Additionally, we observe a strong negative correlation of sunshine during the winter months with the freeze duration and break-up events.



### Learning without Seeing nor Knowing: Towards Open Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.12437v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12437v2)
- **Published**: 2021-03-23 10:30:50+00:00
- **Updated**: 2021-09-14 14:29:43+00:00
- **Authors**: Federico Marmoreo, Julio Ivan Davila Carrazco, Vittorio Murino, Jacopo Cavazza
- **Comment**: None
- **Journal**: None
- **Summary**: In Generalized Zero-Shot Learning (GZSL), unseen categories (for which no visual data are available at training time) can be predicted by leveraging their class embeddings (e.g., a list of attributes describing them) together with a complementary pool of seen classes (paired with both visual data and class embeddings). Despite GZSL is arguably challenging, we posit that knowing in advance the class embeddings, especially for unseen categories, is an actual limit of the applicability of GZSL towards real-world scenarios. To relax this assumption, we propose Open Zero-Shot Learning (OZSL) to extend GZSL towards the open-world settings. We formalize OZSL as the problem of recognizing seen and unseen classes (as in GZSL) while also rejecting instances from unknown categories, for which neither visual data nor class embeddings are provided. We formalize the OZSL problem introducing evaluation protocols, error metrics and benchmark datasets. We also suggest to tackle the OZSL problem by proposing the idea of performing unknown feature generation (instead of only unseen features generation as done in GZSL). We achieve this by optimizing a generative process to sample unknown class embeddings as complementary to the seen and the unseen. We intend these results to be the ground to foster future research, extending the standard closed-world zero-shot learning (GZSL) with the novel open-world counterpart (OZSL).



### Partial Matching in the Space of Varifolds
- **Arxiv ID**: http://arxiv.org/abs/2103.12441v1
- **DOI**: None
- **Categories**: **cs.CV**, math.DG, 58Axx
- **Links**: [PDF](http://arxiv.org/pdf/2103.12441v1)
- **Published**: 2021-03-23 10:44:22+00:00
- **Updated**: 2021-03-23 10:44:22+00:00
- **Authors**: Pierre-Louis Antonsanti, Joan Glaunès, Thomas Benseghir, Vincent Jugnon, Irène Kaltenmark
- **Comment**: 12 pages, 3 figures, The 27th international conference on Information
  Processing in Medical Imaging (June, 2021)
- **Journal**: None
- **Summary**: In computer vision and medical imaging, the problem of matching structures finds numerous applications from automatic annotation to data reconstruction. The data however, while corresponding to the same anatomy, are often very different in topology or shape and might only partially match each other. We introduce a new asymmetric data dissimilarity term for various geometric shapes like sets of curves or surfaces. This term is based on the Varifold shape representation and assesses the embedding of a shape into another one without relying on correspondences between points. It is designed as data attachment for the Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework, allowing to compute meaningful deformation of one shape onto a subset of the other. Registrations are illustrated on sets of synthetic 3D curves, real vascular trees and livers' surfaces from two different modalities: Computed Tomography (CT) and Cone Beam Computed Tomography (CBCT). All experiments show that this data dissimilarity term leads to coherent partial matching despite the topological differences.



### Dual Mesh Convolutional Networks for Human Shape Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2103.12459v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12459v2)
- **Published**: 2021-03-23 11:22:47+00:00
- **Updated**: 2021-10-16 20:05:01+00:00
- **Authors**: Nitika Verma, Adnane Boukhayma, Jakob Verbeek, Edmond Boyer
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional networks have been extremely successful for regular data structures such as 2D images and 3D voxel grids. The transposition to meshes is, however, not straight-forward due to their irregular structure. We explore how the dual, face-based representation of triangular meshes can be leveraged as a data structure for graph convolutional networks. In the dual mesh, each node (face) has a fixed number of neighbors, which makes the networks less susceptible to overfitting on the mesh topology, and also al-lows the use of input features that are naturally defined over faces, such as surface normals and face areas. We evaluate the dual approach on the shape correspondence task on theFaust human shape dataset and variants of it with differ-ent mesh topologies. Our experiments show that results of graph convolutional networks improve when defined over the dual rather than primal mesh. Moreover, our models that explicitly leverage the neighborhood regularity of dual meshes allow improving results further while being more robust to changes in the mesh topology.



### Lifelong Person Re-Identification via Adaptive Knowledge Accumulation
- **Arxiv ID**: http://arxiv.org/abs/2103.12462v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12462v1)
- **Published**: 2021-03-23 11:30:38+00:00
- **Updated**: 2021-03-23 11:30:38+00:00
- **Authors**: Nan Pu, Wei Chen, Yu Liu, Erwin M. Bakker, Michael S. Lew
- **Comment**: 10 pages, 5 figures, Accepted by CVPR2021
- **Journal**: None
- **Summary**: Person ReID methods always learn through a stationary domain that is fixed by the choice of a given dataset. In many contexts (e.g., lifelong learning), those methods are ineffective because the domain is continually changing in which case incremental learning over multiple domains is required potentially. In this work we explore a new and challenging ReID task, namely lifelong person re-identification (LReID), which enables to learn continuously across multiple domains and even generalise on new and unseen domains. Following the cognitive processes in the human brain, we design an Adaptive Knowledge Accumulation (AKA) framework that is endowed with two crucial abilities: knowledge representation and knowledge operation. Our method alleviates catastrophic forgetting on seen domains and demonstrates the ability to generalize to unseen domains. Correspondingly, we also provide a new and large-scale benchmark for LReID. Extensive experiments demonstrate our method outperforms other competitors by a margin of 5.8% mAP in generalising evaluation.



### RPATTACK: Refined Patch Attack on General Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2103.12469v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.12469v1)
- **Published**: 2021-03-23 11:45:41+00:00
- **Updated**: 2021-03-23 11:45:41+00:00
- **Authors**: Hao Huang, Yongtao Wang, Zhaoyu Chen, Zhi Tang, Wenqiang Zhang, Kai-Kuang Ma
- **Comment**: 6 pages, 4 figures, IEEE International Conference on Multimedia and
  Expo (ICME) 2021
- **Journal**: None
- **Summary**: Nowadays, general object detectors like YOLO and Faster R-CNN as well as their variants are widely exploited in many applications. Many works have revealed that these detectors are extremely vulnerable to adversarial patch attacks. The perturbed regions generated by previous patch-based attack works on object detectors are very large which are not necessary for attacking and perceptible for human eyes. To generate much less but more efficient perturbation, we propose a novel patch-based method for attacking general object detectors. Firstly, we propose a patch selection and refining scheme to find the pixels which have the greatest importance for attack and remove the inconsequential perturbations gradually. Then, for a stable ensemble attack, we balance the gradients of detectors to avoid over-optimizing one of them during the training phase. Our RPAttack can achieve an amazing missed detection rate of 100% for both Yolo v4 and Faster R-CNN while only modifies 0.32% pixels on VOC 2007 test set. Our code is available at https://github.com/VDIGPKU/RPAttack.



### On Exposing the Challenging Long Tail in Future Prediction of Traffic Actors
- **Arxiv ID**: http://arxiv.org/abs/2103.12474v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12474v3)
- **Published**: 2021-03-23 11:56:15+00:00
- **Updated**: 2021-08-08 12:58:37+00:00
- **Authors**: Osama Makansi, Özgün Cicek, Yassine Marrakchi, Thomas Brox
- **Comment**: None
- **Journal**: ICCV 2021
- **Summary**: Predicting the states of dynamic traffic actors into the future is important for autonomous systems to operate safelyand efficiently. Remarkably, the most critical scenarios aremuch less frequent and more complex than the uncriticalones. Therefore, uncritical cases dominate the prediction. In this paper, we address specifically the challenging scenarios at the long tail of the dataset distribution. Our analysis shows that the common losses tend to place challenging cases suboptimally in the embedding space. As a consequence, we propose to supplement the usual loss with aloss that places challenging cases closer to each other. This triggers sharing information among challenging cases andlearning specific predictive features. We show on four public datasets that this leads to improved performance on the challenging scenarios while the overall performance stays stable. The approach is agnostic w.r.t. the used network architecture, input modality or viewpoint, and can be integrated into existing solutions easily. Code is available at https://github.com/lmb-freiburg/Contrastive-Future-Trajectory-Prediction



### Watermark Faker: Towards Forgery of Digital Image Watermarking
- **Arxiv ID**: http://arxiv.org/abs/2103.12489v1
- **DOI**: 10.1109/ICME51207.2021.9428410
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.12489v1)
- **Published**: 2021-03-23 12:28:00+00:00
- **Updated**: 2021-03-23 12:28:00+00:00
- **Authors**: Ruowei Wang, Chenguo Lin, Qijun Zhao, Feiyu Zhu
- **Comment**: 6 pages; accepted by ICME2021
- **Journal**: International Conference on Multimedia and Expo (ICME) 2021
- **Summary**: Digital watermarking has been widely used to protect the copyright and integrity of multimedia data. Previous studies mainly focus on designing watermarking techniques that are robust to attacks of destroying the embedded watermarks. However, the emerging deep learning based image generation technology raises new open issues that whether it is possible to generate fake watermarked images for circumvention. In this paper, we make the first attempt to develop digital image watermark fakers by using generative adversarial learning. Suppose that a set of paired images of original and watermarked images generated by the targeted watermarker are available, we use them to train a watermark faker with U-Net as the backbone, whose input is an original image, and after a domain-specific preprocessing, it outputs a fake watermarked image. Our experiments show that the proposed watermark faker can effectively crack digital image watermarkers in both spatial and frequency domains, suggesting the risk of such forgery attacks.



### Revisiting Self-Supervised Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2103.12496v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.12496v1)
- **Published**: 2021-03-23 12:45:00+00:00
- **Updated**: 2021-03-23 12:45:00+00:00
- **Authors**: Ue-Hwan Kim, Jong-Hwan Kim
- **Comment**: 14 pages, 3 figures, 4 tables
- **Journal**: None
- **Summary**: Self-supervised learning of depth map prediction and motion estimation from monocular video sequences is of vital importance -- since it realizes a broad range of tasks in robotics and autonomous vehicles. A large number of research efforts have enhanced the performance by tackling illumination variation, occlusions, and dynamic objects, to name a few. However, each of those efforts targets individual goals and endures as separate works. Moreover, most of previous works have adopted the same CNN architecture, not reaping architectural benefits. Therefore, the need to investigate the inter-dependency of the previous methods and the effect of architectural factors remains. To achieve these objectives, we revisit numerous previously proposed self-supervised methods for joint learning of depth and motion, perform a comprehensive empirical study, and unveil multiple crucial insights. Furthermore, we remarkably enhance the performance as a result of our study -- outperforming previous state-of-the-art performance.



### Stereo Object Matching Network
- **Arxiv ID**: http://arxiv.org/abs/2103.12498v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12498v1)
- **Published**: 2021-03-23 12:54:43+00:00
- **Updated**: 2021-03-23 12:54:43+00:00
- **Authors**: Jaesung Choe, Kyungdon Joo, Francois Rameau, In So Kweon
- **Comment**: Accepted at ICRA 2021
- **Journal**: None
- **Summary**: This paper presents a stereo object matching method that exploits both 2D contextual information from images as well as 3D object-level information. Unlike existing stereo matching methods that exclusively focus on the pixel-level correspondence between stereo images within a volumetric space (i.e., cost volume), we exploit this volumetric structure in a different manner. The cost volume explicitly encompasses 3D information along its disparity axis, therefore it is a privileged structure that can encapsulate the 3D contextual information from objects. However, it is not straightforward since the disparity values map the 3D metric space in a non-linear fashion. Thus, we present two novel strategies to handle 3D objectness in the cost volume space: selective sampling (RoISelect) and 2D-3D fusion (fusion-by-occupancy), which allow us to seamlessly incorporate 3D object-level information and achieve accurate depth performance near the object boundary regions. Our depth estimation achieves competitive performance in the KITTI dataset and the Virtual-KITTI 2.0 dataset.



### Global Correlation Network: End-to-End Joint Multi-Object Detection and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2103.12511v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12511v2)
- **Published**: 2021-03-23 13:16:42+00:00
- **Updated**: 2021-04-10 09:25:45+00:00
- **Authors**: Xuewu Lin, Yu-ang Guo, Jianqiang Wang
- **Comment**: 9 pages, 5 figures, under review
- **Journal**: None
- **Summary**: Multi-object tracking (MOT) has made great progress in recent years, but there are still some problems. Most MOT algorithms follow tracking-by-detection framework, which separates detection and tracking into two independent parts. Early tracking-by-detection algorithms need to do two feature extractions for detection and tracking. Recently, some algorithms make the feature extraction into one network, but the tracking part still relies on data association and needs complex post-processing for life cycle management. Those methods do not combine detection and tracking well. In this paper, we present a novel network to realize joint multi-object detection and tracking in an end-to-end way, called Global Correlation Network (GCNet). Different from most object detection methods, GCNet introduces the global correlation layer for regression of absolute size and coordinates of bounding boxes instead of offsets prediction. The pipeline of detection and tracking by GCNet is conceptually simple, which does not need non-maximum suppression, data association, and other complicated tracking strategies. GCNet was evaluated on a multi-vehicle tracking dataset, UA-DETRAC, and demonstrates promising performance compared to the state-of-the-art detectors and trackers.



### Region extraction based approach for cigarette usage classification using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2103.12523v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12523v1)
- **Published**: 2021-03-23 13:19:43+00:00
- **Updated**: 2021-03-23 13:19:43+00:00
- **Authors**: Anshul Pundhir, Deepak Verma, Puneet Kumar, Balasubramanian Raman
- **Comment**: 5 pages, 16 figures. To appear in the proceedings of the 28th IEEE
  International Conference on Image Processing (IEEE - ICIP), September 19-22,
  2021, Anchorage, Alaska, USA
- **Journal**: None
- **Summary**: This paper has proposed a novel approach to classify the subjects' smoking behavior by extracting relevant regions from a given image using deep learning. After the classification, we have proposed a conditional detection module based on Yolo-v3, which improves model's performance and reduces its complexity. As per the best of our knowledge, we are the first to work on this dataset. This dataset contains a total of 2,400 images that include smokers and non-smokers equally in various environmental settings. We have evaluated the proposed approach's performance using quantitative and qualitative measures, which confirms its effectiveness in challenging situations. The proposed approach has achieved a classification accuracy of 96.74% on this dataset.



### Enhanced Gradient for Differentiable Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2103.12529v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12529v1)
- **Published**: 2021-03-23 13:27:24+00:00
- **Updated**: 2021-03-23 13:27:24+00:00
- **Authors**: Haichao Zhang, Kuangrong Hao, Lei Gao, Xuesong Tang, Bing Wei
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, neural architecture search (NAS) methods have been proposed for the automatic generation of task-oriented network architecture in image classification. However, the architectures obtained by existing NAS approaches are optimized only for classification performance and do not adapt to devices with limited computational resources. To address this challenge, we propose a neural network architecture search algorithm aiming to simultaneously improve network performance (e.g., classification accuracy) and reduce network complexity. The proposed framework automatically builds the network architecture at two stages: block-level search and network-level search. At the stage of block-level search, a relaxation method based on the gradient is proposed, using an enhanced gradient to design high-performance and low-complexity blocks. At the stage of network-level search, we apply an evolutionary multi-objective algorithm to complete the automatic design from blocks to the target network. The experiment results demonstrate that our method outperforms all evaluated hand-crafted networks in image classification, with an error rate of on CIFAR10 and an error rate of on CIFAR100, both at network parameter size less than one megabit. Moreover, compared with other neural architecture search methods, our method offers a tremendous reduction in designed network architecture parameters.



### Balanced softmax cross-entropy for incremental learning with and without memory
- **Arxiv ID**: http://arxiv.org/abs/2103.12532v5
- **DOI**: 10.1016/j.cviu.2022.103582
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.12532v5)
- **Published**: 2021-03-23 13:30:26+00:00
- **Updated**: 2022-11-14 07:27:58+00:00
- **Authors**: Quentin Jodelet, Xin Liu, Tsuyoshi Murata
- **Comment**: Journal extension of the ICANN 2021 paper (arXiv:2103.12532v3),
  published in Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: When incrementally trained on new classes, deep neural networks are subject to catastrophic forgetting which leads to an extreme deterioration of their performance on the old classes while learning the new ones. Using a small memory containing few samples from past classes has shown to be an effective method to mitigate catastrophic forgetting. However, due to the limited size of the replay memory, there is a large imbalance between the number of samples for the new and the old classes in the training dataset resulting in bias in the final model. To address this issue, we propose to use the Balanced Softmax Cross-Entropy and show that it can be seamlessly combined with state-of-the-art approaches for class-incremental learning in order to improve their accuracy while also potentially decreasing the computational cost of the training procedure. We further extend this approach to the more demanding class-incremental learning without memory setting and achieve competitive results with memory-based approaches. Experiments on the challenging ImageNet, ImageNet-Subset and CIFAR100 benchmarks with various settings demonstrate the benefits of our approach.



### Transferable Semantic Augmentation for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2103.12562v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12562v1)
- **Published**: 2021-03-23 14:04:11+00:00
- **Updated**: 2021-03-23 14:04:11+00:00
- **Authors**: Shuang Li, Mixue Xie, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Wei Li
- **Comment**: Accepted as CVPR 2021. The code is publicly available at
  https://github.com/BIT-DA/TSA
- **Journal**: None
- **Summary**: Domain adaptation has been widely explored by transferring the knowledge from a label-rich source domain to a related but unlabeled target domain. Most existing domain adaptation algorithms attend to adapting feature representations across two domains with the guidance of a shared source-supervised classifier. However, such classifier limits the generalization ability towards unlabeled target recognition. To remedy this, we propose a Transferable Semantic Augmentation (TSA) approach to enhance the classifier adaptation ability through implicitly generating source features towards target semantics. Specifically, TSA is inspired by the fact that deep feature transformation towards a certain direction can be represented as meaningful semantic altering in the original input space. Thus, source features can be augmented to effectively equip with target semantics to train a more transferable classifier. To achieve this, for each class, we first use the inter-domain feature mean difference and target intra-class feature covariance to construct a multivariate normal distribution. Then we augment source features with random directions sampled from the distribution class-wisely. Interestingly, such source augmentation is implicitly implemented through an expected transferable cross-entropy loss over the augmented source distribution, where an upper bound of the expected loss is derived and minimized, introducing negligible computational overhead. As a light-weight and general technique, TSA can be easily plugged into various domain adaptation methods, bringing remarkable improvements. Comprehensive experiments on cross-domain benchmarks validate the efficacy of TSA.



### Multimodal Personal Ear Authentication Using Smartphones
- **Arxiv ID**: http://arxiv.org/abs/2103.12575v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.12575v1)
- **Published**: 2021-03-23 14:19:15+00:00
- **Updated**: 2021-03-23 14:19:15+00:00
- **Authors**: S. Itani, S. Kita, Y. Kajikawa
- **Comment**: 9 pages, 23 figures
- **Journal**: None
- **Summary**: In recent years, biometric authentication technology for smartphones has become widespread, with the mainstream methods being fingerprint authentication and face recognition. However, fingerprint authentication cannot be used when hands are wet, and face recognition cannot be used when a person is wearing a mask. Therefore, we examine a personal authentication system using the pinna as a new approach for biometric authentication on smartphones. Authentication systems based on the acoustic transfer function of the pinna (PRTF: Pinna Related Transfer Function) have been investigated. However, the authentication accuracy decreases due to the positional fluctuation across each measurement. In this paper, we propose multimodal personal authentication on smartphones using PRTF. The pinna image and positional sensor information are used with the PRTF, and the effectiveness of the authentication method is examined. We demonstrate that the proposed authentication system can compensate for the positional changes in each measurement and improve robustness.



### MetaSAug: Meta Semantic Augmentation for Long-Tailed Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.12579v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12579v3)
- **Published**: 2021-03-23 14:31:33+00:00
- **Updated**: 2021-04-07 12:01:14+00:00
- **Authors**: Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, Xinjing Cheng
- **Comment**: Accepted at CVPR 2021
- **Journal**: None
- **Summary**: Real-world training data usually exhibits long-tailed distribution, where several majority classes have a significantly larger number of samples than the remaining minority classes. This imbalance degrades the performance of typical supervised learning algorithms designed for balanced training sets. In this paper, we address this issue by augmenting minority classes with a recently proposed implicit semantic data augmentation (ISDA) algorithm, which produces diversified augmented samples by translating deep features along many semantically meaningful directions. Importantly, given that ISDA estimates the class-conditional statistics to obtain semantic directions, we find it ineffective to do this on minority classes due to the insufficient training data. To this end, we propose a novel approach to learn transformed semantic directions with meta-learning automatically. In specific, the augmentation strategy during training is dynamically optimized, aiming to minimize the loss on a small balanced validation set, which is approximated via a meta update step. Extensive empirical results on CIFAR-LT-10/100, ImageNet-LT, and iNaturalist 2017/2018 validate the effectiveness of our method.



### An augmentation strategy to mimic multi-scanner variability in MRI
- **Arxiv ID**: http://arxiv.org/abs/2103.12595v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.12595v1)
- **Published**: 2021-03-23 14:49:38+00:00
- **Updated**: 2021-03-23 14:49:38+00:00
- **Authors**: Maria Ines Meyer, Ezequiel de la Rosa, Nuno Barros, Roberto Paolella, Koen Van Leemput, Diana M. Sima
- **Comment**: 5 pages, 2 figures. accepted for presentation at the International
  Symposium on Biomedical Imaging (ISBI) 2021. Code available at
  https://github.com/icometrix/gmm-augmentation
- **Journal**: None
- **Summary**: Most publicly available brain MRI datasets are very homogeneous in terms of scanner and protocols, and it is difficult for models that learn from such data to generalize to multi-center and multi-scanner data. We propose a novel data augmentation approach with the aim of approximating the variability in terms of intensities and contrasts present in real world clinical data. We use a Gaussian Mixture Model based approach to change tissue intensities individually, producing new contrasts while preserving anatomical information. We train a deep learning model on a single scanner dataset and evaluate it on a multi-center and multi-scanner dataset. The proposed approach improves the generalization capability of the model to other scanners not present in the training data.



### MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
- **Arxiv ID**: http://arxiv.org/abs/2103.12605v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12605v2)
- **Published**: 2021-03-23 15:03:08+00:00
- **Updated**: 2021-03-24 12:28:15+00:00
- **Authors**: Hansheng Chen, Yuyao Huang, Wei Tian, Zhong Gao, Lu Xiong
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Object localization in 3D space is a challenging aspect in monocular 3D object detection. Recent advances in 6DoF pose estimation have shown that predicting dense 2D-3D correspondence maps between image and object 3D model and then estimating object pose via Perspective-n-Point (PnP) algorithm can achieve remarkable localization accuracy. Yet these methods rely on training with ground truth of object geometry, which is difficult to acquire in real outdoor scenes. To address this issue, we propose MonoRUn, a novel detection framework that learns dense correspondences and geometry in a self-supervised manner, with simple 3D bounding box annotations. To regress the pixel-related 3D object coordinates, we employ a regional reconstruction network with uncertainty awareness. For self-supervised training, the predicted 3D coordinates are projected back to the image plane. A Robust KL loss is proposed to minimize the uncertainty-weighted reprojection error. During testing phase, we exploit the network uncertainty by propagating it through all downstream modules. More specifically, the uncertainty-driven PnP algorithm is leveraged to estimate object pose and its covariance. Extensive experiments demonstrate that our proposed approach outperforms current state-of-the-art methods on KITTI benchmark.



### Incrementally Zero-Shot Detection by an Extreme Value Analyzer
- **Arxiv ID**: http://arxiv.org/abs/2103.12609v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12609v2)
- **Published**: 2021-03-23 15:06:30+00:00
- **Updated**: 2021-03-29 16:12:44+00:00
- **Authors**: Sixiao Zheng, Yanwei Fu, Yanxi Hou
- **Comment**: International Conference on Pattern Recognition (ICPR)
- **Journal**: None
- **Summary**: Human beings not only have the ability to recognize novel unseen classes, but also can incrementally incorporate the new classes to existing knowledge preserved. However, zero-shot learning models assume that all seen classes should be known beforehand, while incremental learning models cannot recognize unseen classes. This paper introduces a novel and challenging task of Incrementally Zero-Shot Detection (IZSD), a practical strategy for both zero-shot learning and class-incremental learning in real-world object detection. An innovative end-to-end model -- IZSD-EVer was proposed to tackle this task that requires incrementally detecting new classes and detecting the classes that have never been seen. Specifically, we propose a novel extreme value analyzer to detect objects from old seen, new seen, and unseen classes, simultaneously. Additionally and technically, we propose two innovative losses, i.e., background-foreground mean squared error loss alleviating the extreme imbalance of the background and foreground of images, and projection distance loss aligning the visual space and semantic spaces of old seen classes. Experiments demonstrate the efficacy of our model in detecting objects from both the seen and unseen classes, outperforming the alternative models on Pascal VOC and MSCOCO datasets.



### Virtual Light Transport Matrices for Non-Line-Of-Sight Imaging
- **Arxiv ID**: http://arxiv.org/abs/2103.12622v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.12622v2)
- **Published**: 2021-03-23 15:17:45+00:00
- **Updated**: 2021-10-05 15:59:03+00:00
- **Authors**: Julio Marco, Adrian Jarabo, Ji Hyun Nam, Xiaochun Liu, Miguel Ángel Cosculluela, Andreas Velten, Diego Gutierrez
- **Comment**: ICCV 2021 (Oral)
- **Journal**: Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV), 2021, pp. 2440-2449
- **Summary**: The light transport matrix (LTM) is an instrumental tool in line-of-sight (LOS) imaging, describing how light interacts with the scene and enabling applications such as relighting or separation of illumination components. We introduce a framework to estimate the LTM of non-line-of-sight (NLOS) scenarios, coupling recent virtual forward light propagation models for NLOS imaging with the LOS light transport equation. We design computational projector-camera setups, and use these virtual imaging systems to estimate the transport matrix of hidden scenes. We introduce the specific illumination functions to compute the different elements of the matrix, overcoming the challenging wide-aperture conditions of NLOS setups. Our NLOS light transport matrix allows us to (re)illuminate specific locations of a hidden scene, and separate direct, first-order indirect, and higher-order indirect illumination of complex cluttered hidden scenes, similar to existing LOS techniques.



### Deep Learning for fully automatic detection, segmentation, and Gleason Grade estimation of prostate cancer in multiparametric Magnetic Resonance Images
- **Arxiv ID**: http://arxiv.org/abs/2103.12650v3
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.12650v3)
- **Published**: 2021-03-23 16:08:43+00:00
- **Updated**: 2022-02-02 15:40:49+00:00
- **Authors**: Oscar J. Pellicer-Valero, José L. Marenco Jiménez, Victor Gonzalez-Perez, Juan Luis Casanova Ramón-Borja, Isabel Martín García, María Barrios Benito, Paula Pelechano Gómez, José Rubio-Briones, María José Rupérez, José D. Martín-Guerrero
- **Comment**: None
- **Journal**: None
- **Summary**: The emergence of multi-parametric magnetic resonance imaging (mpMRI) has had a profound impact on the diagnosis of prostate cancers (PCa), which is the most prevalent malignancy in males in the western world, enabling a better selection of patients for confirmation biopsy. However, analyzing these images is complex even for experts, hence opening an opportunity for computer-aided diagnosis systems to seize. This paper proposes a fully automatic system based on Deep Learning that takes a prostate mpMRI from a PCa-suspect patient and, by leveraging the Retina U-Net detection framework, locates PCa lesions, segments them, and predicts their most likely Gleason grade group (GGG). It uses 490 mpMRIs for training/validation, and 75 patients for testing from two different datasets: ProstateX and IVO (Valencia Oncology Institute Foundation). In the test set, it achieves an excellent lesion-level AUC/sensitivity/specificity for the GGG$\geq$2 significance criterion of 0.96/1.00/0.79 for the ProstateX dataset, and 0.95/1.00/0.80 for the IVO dataset. Evaluated at a patient level, the results are 0.87/1.00/0.375 in ProstateX, and 0.91/1.00/0.762 in IVO. Furthermore, on the online ProstateX grand challenge, the model obtained an AUC of 0.85 (0.87 when trained only on the ProstateX data, tying up with the original winner of the challenge). For expert comparison, IVO radiologist's PI-RADS 4 sensitivity/specificity were 0.88/0.56 at a lesion level, and 0.85/0.58 at a patient level. Additional subsystems for automatic prostate zonal segmentation and mpMRI non-rigid sequence registration were also employed to produce the final fully automated system. The code for the ProstateX-trained system has been made openly available at https://github.com/OscarPellicer/prostate_lesion_detection. We hope that this will represent a landmark for future research to use, compare and improve upon.



### Out-of-Distribution Detection of Melanoma using Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/2103.12672v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12672v1)
- **Published**: 2021-03-23 16:47:19+00:00
- **Updated**: 2021-03-23 16:47:19+00:00
- **Authors**: M. M. A. Valiuddin, C. G. A. Viviers
- **Comment**: None
- **Journal**: None
- **Summary**: Generative modelling has been a topic at the forefront of machine learning research for a substantial amount of time. With the recent success in the field of machine learning, especially in deep learning, there has been an increased interest in explainable and interpretable machine learning. The ability to model distributions and provide insight in the density estimation and exact data likelihood is an example of such a feature. Normalizing Flows (NFs), a relatively new research field of generative modelling, has received substantial attention since it is able to do exactly this at a relatively low cost whilst enabling competitive generative results. While the generative abilities of NFs are typically explored, we focus on exploring the data distribution modelling for Out-of-Distribution (OOD) detection. Using one of the state-of-the-art NF models, GLOW, we attempt to detect OOD examples in the ISIC dataset. We notice that this model under performs in conform related research. To improve the OOD detection, we explore the masking methods to inhibit co-adaptation of the coupling layers however find no substantial improvement. Furthermore, we utilize Wavelet Flow which uses wavelets that can filter particular frequency components, thus simplifying the modeling process to data-driven conditional wavelet coefficients instead of complete images. This enables us to efficiently model larger resolution images in the hopes that it would capture more relevant features for OOD. The paper that introduced Wavelet Flow mainly focuses on its ability of sampling high resolution images and did not treat OOD detection. We present the results and propose several ideas for improvement such as controlling frequency components, using different wavelets and using other state-of-the-art NF architectures.



### PanGEA: The Panoramic Graph Environment Annotation Toolkit
- **Arxiv ID**: http://arxiv.org/abs/2103.12703v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2103.12703v1)
- **Published**: 2021-03-23 17:24:12+00:00
- **Updated**: 2021-03-23 17:24:12+00:00
- **Authors**: Alexander Ku, Peter Anderson, Jordi Pont-Tuset, Jason Baldridge
- **Comment**: None
- **Journal**: None
- **Summary**: PanGEA, the Panoramic Graph Environment Annotation toolkit, is a lightweight toolkit for collecting speech and text annotations in photo-realistic 3D environments. PanGEA immerses annotators in a web-based simulation and allows them to move around easily as they speak and/or listen. It includes database and cloud storage integration, plus utilities for automatically aligning recorded speech with manual transcriptions and the virtual pose of the annotators. Out of the box, PanGEA supports two tasks -- collecting navigation instructions and navigation instruction following -- and it could be easily adapted for annotating walking tours, finding and labeling landmarks or objects, and similar tasks. We share best practices learned from using PanGEA in a 20,000 hour annotation effort to collect the Room-Across-Room dataset. We hope that our open-source annotation toolkit and insights will both expedite future data collection efforts and spur innovation on the kinds of grounded language tasks such environments can support.



### Spatial Intention Maps for Multi-Agent Mobile Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2103.12710v1
- **DOI**: 10.1109/ICRA48506.2021.9561359
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2103.12710v1)
- **Published**: 2021-03-23 17:31:14+00:00
- **Updated**: 2021-03-23 17:31:14+00:00
- **Authors**: Jimmy Wu, Xingyuan Sun, Andy Zeng, Shuran Song, Szymon Rusinkiewicz, Thomas Funkhouser
- **Comment**: To appear at IEEE International Conference on Robotics and Automation
  (ICRA), 2021. Project page: https://spatial-intention-maps.cs.princeton.edu/
- **Journal**: None
- **Summary**: The ability to communicate intention enables decentralized multi-agent robots to collaborate while performing physical tasks. In this work, we present spatial intention maps, a new intention representation for multi-agent vision-based deep reinforcement learning that improves coordination between decentralized mobile manipulators. In this representation, each agent's intention is provided to other agents, and rendered into an overhead 2D map aligned with visual observations. This synergizes with the recently proposed spatial action maps framework, in which state and action representations are spatially aligned, providing inductive biases that encourage emergent cooperative behaviors requiring spatial coordination, such as passing objects to each other or avoiding collisions. Experiments across a variety of multi-agent environments, including heterogeneous robot teams with different abilities (lifting, pushing, or throwing), show that incorporating spatial intention maps improves performance for different mobile manipulation tasks while significantly enhancing cooperative behaviors.



### UltraSR: Spatial Encoding is a Missing Key for Implicit Image Function-based Arbitrary-Scale Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2103.12716v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12716v2)
- **Published**: 2021-03-23 17:36:42+00:00
- **Updated**: 2022-07-05 00:41:41+00:00
- **Authors**: Xingqian Xu, Zhangyang Wang, Humphrey Shi
- **Comment**: None
- **Journal**: None
- **Summary**: The recent success of NeRF and other related implicit neural representation methods has opened a new path for continuous image representation, where pixel values no longer need to be looked up from stored discrete 2D arrays but can be inferred from neural network models on a continuous spatial domain. Although the recent work LIIF has demonstrated that such novel approaches can achieve good performance on the arbitrary-scale super-resolution task, their upscaled images frequently show structural distortion due to the inaccurate prediction of high-frequency textures. In this work, we propose UltraSR, a simple yet effective new network design based on implicit image functions in which we deeply integrated spatial coordinates and periodic encoding with the implicit neural representation. Through extensive experiments and ablation studies, we show that spatial encoding is a missing key toward the next-stage high-performing implicit image function. Our UltraSR sets new state-of-the-art performance on the DIV2K benchmark under all super-resolution scales compared to previous state-of-the-art methods. UltraSR also achieves superior performance on other standard benchmark datasets in which it outperforms prior works in almost all experiments.



### Self-Supervised Pretraining Improves Self-Supervised Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2103.12718v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12718v2)
- **Published**: 2021-03-23 17:37:51+00:00
- **Updated**: 2021-03-25 00:33:47+00:00
- **Authors**: Colorado J. Reed, Xiangyu Yue, Ani Nrusimha, Sayna Ebrahimi, Vivek Vijaykumar, Richard Mao, Bo Li, Shanghang Zhang, Devin Guillory, Sean Metzger, Kurt Keutzer, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: While self-supervised pretraining has proven beneficial for many computer vision tasks, it requires expensive and lengthy computation, large amounts of data, and is sensitive to data augmentation. Prior work demonstrates that models pretrained on datasets dissimilar to their target data, such as chest X-ray models trained on ImageNet, underperform models trained from scratch. Users that lack the resources to pretrain must use existing models with lower performance. This paper explores Hierarchical PreTraining (HPT), which decreases convergence time and improves accuracy by initializing the pretraining process with an existing pretrained model. Through experimentation on 16 diverse vision datasets, we show HPT converges up to 80x faster, improves accuracy across tasks, and improves the robustness of the self-supervised pretraining process to changes in the image augmentation policy or amount of pretraining data. Taken together, HPT provides a simple framework for obtaining better pretrained representations with less computational resources.



### Characterizing and Improving the Robustness of Self-Supervised Learning through Background Augmentations
- **Arxiv ID**: http://arxiv.org/abs/2103.12719v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.12719v2)
- **Published**: 2021-03-23 17:39:16+00:00
- **Updated**: 2021-11-12 08:00:45+00:00
- **Authors**: Chaitanya K. Ryali, David J. Schwab, Ari S. Morcos
- **Comment**: Technical Report; Additional Results
- **Journal**: None
- **Summary**: Recent progress in self-supervised learning has demonstrated promising results in multiple visual tasks. An important ingredient in high-performing self-supervised methods is the use of data augmentation by training models to place different augmented views of the same image nearby in embedding space. However, commonly used augmentation pipelines treat images holistically, ignoring the semantic relevance of parts of an image-e.g. a subject vs. a background-which can lead to the learning of spurious correlations. Our work addresses this problem by investigating a class of simple, yet highly effective "background augmentations", which encourage models to focus on semantically-relevant content by discouraging them from focusing on image backgrounds. Through a systematic investigation, we show that background augmentations lead to substantial improvements in performance across a spectrum of state-of-the-art self-supervised methods (MoCo-v2, BYOL, SwAV) on a variety of tasks, e.g. $\sim$+1-2% gains on ImageNet, enabling performance on par with the supervised baseline. Further, we find the improvement in limited-labels settings is even larger (up to 4.2%). Background augmentations also improve robustness to a number of distribution shifts, including natural adversarial examples, ImageNet-9, adversarial attacks, ImageNet-Renditions. We also make progress in completely unsupervised saliency detection, in the process of generating saliency masks used for background augmentations.



### DeFLOCNet: Deep Image Editing via Flexible Low-level Controls
- **Arxiv ID**: http://arxiv.org/abs/2103.12723v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12723v1)
- **Published**: 2021-03-23 17:47:23+00:00
- **Updated**: 2021-03-23 17:47:23+00:00
- **Authors**: Hongyu Liu, Ziyu Wan, Wei Huang, Yibing Song, Xintong Han, Jing Liao, Bing Jiang, Wei Liu
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: User-intended visual content fills the hole regions of an input image in the image editing scenario. The coarse low-level inputs, which typically consist of sparse sketch lines and color dots, convey user intentions for content creation (\ie, free-form editing). While existing methods combine an input image and these low-level controls for CNN inputs, the corresponding feature representations are not sufficient to convey user intentions, leading to unfaithfully generated content. In this paper, we propose DeFLOCNet which relies on a deep encoder-decoder CNN to retain the guidance of these controls in the deep feature representations. In each skip-connection layer, we design a structure generation block. Instead of attaching low-level controls to an input image, we inject these controls directly into each structure generation block for sketch line refinement and color propagation in the CNN feature space. We then concatenate the modulated features with the original decoder features for structure generation. Meanwhile, DeFLOCNet involves another decoder branch for texture generation and detail enhancement. Both structures and textures are rendered in the decoder, leading to user-intended editing results. Experiments on benchmarks demonstrate that DeFLOCNet effectively transforms different user intentions to create visually pleasing content.



### Scaling Local Self-Attention for Parameter Efficient Visual Backbones
- **Arxiv ID**: http://arxiv.org/abs/2103.12731v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12731v3)
- **Published**: 2021-03-23 17:56:06+00:00
- **Updated**: 2021-06-07 05:42:10+00:00
- **Authors**: Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, Jonathon Shlens
- **Comment**: CVPR 2021 Oral
- **Journal**: None
- **Summary**: Self-attention has the promise of improving computer vision systems due to parameter-independent scaling of receptive fields and content-dependent interactions, in contrast to parameter-dependent scaling and content-independent interactions of convolutions. Self-attention models have recently been shown to have encouraging improvements on accuracy-parameter trade-offs compared to baseline convolutional models such as ResNet-50. In this work, we aim to develop self-attention models that can outperform not just the canonical baseline models, but even the high-performing convolutional models. We propose two extensions to self-attention that, in conjunction with a more efficient implementation of self-attention, improve the speed, memory usage, and accuracy of these models. We leverage these improvements to develop a new self-attention model family, HaloNets, which reach state-of-the-art accuracies on the parameter-limited setting of the ImageNet classification benchmark. In preliminary transfer learning experiments, we find that HaloNet models outperform much larger models and have better inference performance. On harder tasks such as object detection and instance segmentation, our simple local self-attention and convolutional hybrids show improvements over very strong baselines. These results mark another step in demonstrating the efficacy of self-attention models on settings traditionally dominated by convolutional models.



### DA4Event: towards bridging the Sim-to-Real Gap for Event Cameras using Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2103.12768v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12768v2)
- **Published**: 2021-03-23 18:09:20+00:00
- **Updated**: 2021-10-29 13:36:28+00:00
- **Authors**: Mirco Planamente, Chiara Plizzari, Marco Cannici, Marco Ciccone, Francesco Strada, Andrea Bottino, Matteo Matteucci, Barbara Caputo
- **Comment**: Accepted at IROS21
- **Journal**: None
- **Summary**: Event cameras are novel bio-inspired sensors, which asynchronously capture pixel-level intensity changes in the form of "events". The innovative way they acquire data presents several advantages over standard devices, especially in poor lighting and high-speed motion conditions. However, the novelty of these sensors results in the lack of a large amount of training data capable of fully unlocking their potential. The most common approach implemented by researchers to address this issue is to leverage simulated event data. Yet, this approach comes with an open research question: how well simulated data generalize to real data? To answer this, we propose to exploit, in the event-based context, recent Domain Adaptation (DA) advances in traditional computer vision, showing that DA techniques applied to event data help reduce the sim-to-real gap. To this purpose, we propose a novel architecture, which we call Multi-View DA4E (MV-DA4E), that better exploits the peculiarities of frame-based event representations while also promoting domain invariant characteristics in features. Through extensive experiments, we prove the effectiveness of DA methods and MV-DA4E on N-Caltech101. Moreover, we validate their soundness in a real-world scenario through a cross-domain analysis on the popular RGB-D Object Dataset (ROD), which we extended to the event modality (RGB-E).



### Distributed Visual-Inertial Cooperative Localization
- **Arxiv ID**: http://arxiv.org/abs/2103.12770v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.12770v2)
- **Published**: 2021-03-23 18:12:07+00:00
- **Updated**: 2021-08-17 20:58:27+00:00
- **Authors**: Pengxiang Zhu, Patrick Geneva, Wei Ren, Guoquan Huang
- **Comment**: 8 pages, 5 figures, 8 tables; IROS 2021 final version
- **Journal**: None
- **Summary**: In this paper we present a consistent and distributed state estimator for multi-robot cooperative localization (CL) which efficiently fuses environmental features and loop-closure constraints across time and robots. In particular, we leverage covariance intersection (CI) to allow each robot to only estimate its own state and autocovariance and compensate for the unknown correlations between robots. Two novel multi-robot methods for utilizing common environmental SLAM features are introduced and evaluated in terms of accuracy and efficiency. Moreover, we adapt CI to enable drift-free estimation through the use of loop-closure measurement constraints to other robots' historical poses without a significant increase in computational cost. The proposed distributed CL estimator is validated against its non-realtime centralized counterpart extensively in both simulations and real-world experiments.



### Robust and Accurate Object Detection via Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.13886v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.13886v2)
- **Published**: 2021-03-23 19:45:26+00:00
- **Updated**: 2021-03-26 17:34:31+00:00
- **Authors**: Xiangning Chen, Cihang Xie, Mingxing Tan, Li Zhang, Cho-Jui Hsieh, Boqing Gong
- **Comment**: CVPR 2021. Models are available at
  https://github.com/google/automl/tree/master/efficientdet/Det-AdvProp.md
- **Journal**: None
- **Summary**: Data augmentation has become a de facto component for training high-performance deep image classifiers, but its potential is under-explored for object detection. Noting that most state-of-the-art object detectors benefit from fine-tuning a pre-trained classifier, we first study how the classifiers' gains from various data augmentations transfer to object detection. The results are discouraging; the gains diminish after fine-tuning in terms of either accuracy or robustness. This work instead augments the fine-tuning stage for object detectors by exploring adversarial examples, which can be viewed as a model-dependent data augmentation. Our method dynamically selects the stronger adversarial images sourced from a detector's classification and localization branches and evolves with the detector to ensure the augmentation policy stays current and relevant. This model-dependent augmentation generalizes to different object detectors better than AutoAugment, a model-agnostic augmentation policy searched based on one particular detector. Our approach boosts the performance of state-of-the-art EfficientDets by +1.1 mAP on the COCO object detection benchmark. It also improves the detectors' robustness against natural distortions by +3.8 mAP and against domain shift by +1.3 mAP. Models are available at https://github.com/google/automl/tree/master/efficientdet/Det-AdvProp.md



### Co-matching: Combating Noisy Labels by Augmentation Anchoring
- **Arxiv ID**: http://arxiv.org/abs/2103.12814v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.12814v1)
- **Published**: 2021-03-23 20:00:13+00:00
- **Updated**: 2021-03-23 20:00:13+00:00
- **Authors**: Yangdi Lu, Yang Bo, Wenbo He
- **Comment**: 13 pages, 10 figures. arXiv admin note: text overlap with
  arXiv:2003.02752 by other authors
- **Journal**: None
- **Summary**: Deep learning with noisy labels is challenging as deep neural networks have the high capacity to memorize the noisy labels. In this paper, we propose a learning algorithm called Co-matching, which balances the consistency and divergence between two networks by augmentation anchoring. Specifically, we have one network generate anchoring label from its prediction on a weakly-augmented image. Meanwhile, we force its peer network, taking the strongly-augmented version of the same image as input, to generate prediction close to the anchoring label. We then update two networks simultaneously by selecting small-loss instances to minimize both unsupervised matching loss (i.e., measure the consistency of the two networks) and supervised classification loss (i.e. measure the classification performance). Besides, the unsupervised matching loss makes our method not heavily rely on noisy labels, which prevents memorization of noisy labels. Experiments on three benchmark datasets demonstrate that Co-matching achieves results comparable to the state-of-the-art methods.



### Integrating Novelty Detection Capabilities with MSL Mastcam Operations to Enhance Data Analysis
- **Arxiv ID**: http://arxiv.org/abs/2103.12815v1
- **DOI**: 10.1109/AERO50100.2021.9438280
- **Categories**: **cs.LG**, cs.CV, J.2; I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2103.12815v1)
- **Published**: 2021-03-23 20:01:50+00:00
- **Updated**: 2021-03-23 20:01:50+00:00
- **Authors**: Paul Horton, Hannah R. Kerner, Samantha Jacob, Ernest Cisneros, Kiri L. Wagstaff, James Bell
- **Comment**: 8 pages, 5 figure, accepted and presented at IEEE Aerospace
  Conference 2021
- **Journal**: None
- **Summary**: While innovations in scientific instrumentation have pushed the boundaries of Mars rover mission capabilities, the increase in data complexity has pressured Mars Science Laboratory (MSL) and future Mars rover operations staff to quickly analyze complex data sets to meet progressively shorter tactical and strategic planning timelines. MSLWEB is an internal data tracking tool used by operations staff to perform first pass analysis on MSL image sequences, a series of products taken by the Mast camera, Mastcam. Mastcam's multiband multispectral image sequences require more complex analysis compared to standard 3-band RGB images. Typically, these are analyzed using traditional methods to identify unique features within the sequence. Given the short time frame of tactical planning in which downlinked images might need to be analyzed (within 5-10 hours before the next uplink), there exists a need to triage analysis time to focus on the most important sequences and parts of a sequence. We address this need by creating products for MSLWEB that use novelty detection to help operations staff identify unusual data that might be diagnostic of new or atypical compositions or mineralogies detected within an imaging scene. This was achieved in two ways: 1) by creating products for each sequence to identify novel regions in the image, and 2) by assigning multispectral sequences a sortable novelty score. These new products provide colorized heat maps of inferred novelty that operations staff can use to rapidly review downlinked data and focus their efforts on analyzing potentially new kinds of diagnostic multispectral signatures. This approach has the potential to guide scientists to new discoveries by quickly drawing their attention to often subtle variations not detectable with simple color composites.



### Teacher-Explorer-Student Learning: A Novel Learning Method for Open Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.12871v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.12871v1)
- **Published**: 2021-03-23 22:32:32+00:00
- **Updated**: 2021-03-23 22:32:32+00:00
- **Authors**: Jaeyeon Jang, Chang Ouk Kim
- **Comment**: 12 pages, 13 figures, 4 tables
- **Journal**: None
- **Summary**: If an unknown example that is not seen during training appears, most recognition systems usually produce overgeneralized results and determine that the example belongs to one of the known classes. To address this problem, teacher-explorer-student (T/E/S) learning, which adopts the concept of open set recognition (OSR) that aims to reject unknown samples while minimizing the loss of classification performance on known samples, is proposed in this study. In this novel learning method, overgeneralization of deep learning classifiers is significantly reduced by exploring various possibilities of unknowns. Here, the teacher network extracts some hints about unknowns by distilling the pretrained knowledge about knowns and delivers this distilled knowledge to the student. After learning the distilled knowledge, the student network shares the learned information with the explorer network. Then, the explorer network shares its exploration results by generating unknown-like samples and feeding the samples to the student network. By repeating this alternating learning process, the student network experiences a variety of synthetic unknowns, reducing overgeneralization. Extensive experiments were conducted, and the experimental results showed that each component proposed in this paper significantly contributes to the improvement in OSR performance. As a result, the proposed T/E/S learning method outperformed current state-of-the-art methods.



### Weakly Supervised Instance Segmentation for Videos with Temporal Mask Consistency
- **Arxiv ID**: http://arxiv.org/abs/2103.12886v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.12886v1)
- **Published**: 2021-03-23 23:20:46+00:00
- **Updated**: 2021-03-23 23:20:46+00:00
- **Authors**: Qing Liu, Vignesh Ramanathan, Dhruv Mahajan, Alan Yuille, Zhenheng Yang
- **Comment**: 14 pages, 8 figures, accepted by CVPR 2021
- **Journal**: None
- **Summary**: Weakly supervised instance segmentation reduces the cost of annotations required to train models. However, existing approaches which rely only on image-level class labels predominantly suffer from errors due to (a) partial segmentation of objects and (b) missing object predictions. We show that these issues can be better addressed by training with weakly labeled videos instead of images. In videos, motion and temporal consistency of predictions across frames provide complementary signals which can help segmentation. We are the first to explore the use of these video signals to tackle weakly supervised instance segmentation. We propose two ways to leverage this information in our model. First, we adapt inter-pixel relation network (IRN) to effectively incorporate motion information during training. Second, we introduce a new MaskConsist module, which addresses the problem of missing object instances by transferring stable predictions between neighboring frames during training. We demonstrate that both approaches together improve the instance segmentation metric $AP_{50}$ on video frames of two datasets: Youtube-VIS and Cityscapes by $5\%$ and $3\%$ respectively.



### SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms
- **Arxiv ID**: http://arxiv.org/abs/2103.12896v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.12896v1)
- **Published**: 2021-03-23 23:51:22+00:00
- **Updated**: 2021-03-23 23:51:22+00:00
- **Authors**: Nitthilan Kannappan Jayakodi, Janardhan Rao Doppa, Partha Pratim Pande
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the task of photo-realistic unconditional image generation (generate high quality, diverse samples that carry the same visual content as the image) on mobile platforms using Generative Adversarial Networks (GANs). In this paper, we propose a novel approach to trade-off image generation accuracy of a GAN for the energy consumed (compute) at run-time called Scale-Energy Tradeoff GAN (SETGAN). GANs usually take a long time to train and consume a huge memory hence making it difficult to run on edge devices. The key idea behind SETGAN for an image generation task is for a given input image, we train a GAN on a remote server and use the trained model on edge devices. We use SinGAN, a single image unconditional generative model, that contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. During the training process, we determine the optimal number of scales for a given input image and the energy constraint from the target edge device. Results show that with SETGAN's unique client-server-based architecture, we were able to achieve a 56% gain in energy for a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale training, we obtain around 4x gain in training time on the server.



