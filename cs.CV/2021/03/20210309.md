# Arxiv Papers in cs.CV on 2021-03-09
### Knowledge Evolution in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.05152v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05152v1)
- **Published**: 2021-03-09 00:25:34+00:00
- **Updated**: 2021-03-09 00:25:34+00:00
- **Authors**: Ahmed Taha, Abhinav Shrivastava, Larry Davis
- **Comment**: CVPR Oral 2021
- **Journal**: None
- **Summary**: Deep learning relies on the availability of a large corpus of data (labeled or unlabeled). Thus, one challenging unsettled question is: how to train a deep network on a relatively small dataset? To tackle this question, we propose an evolution-inspired training approach to boost performance on relatively small datasets. The knowledge evolution (KE) approach splits a deep network into two hypotheses: the fit-hypothesis and the reset-hypothesis. We iteratively evolve the knowledge inside the fit-hypothesis by perturbing the reset-hypothesis for multiple generations. This approach not only boosts performance, but also learns a slim network with a smaller inference cost. KE integrates seamlessly with both vanilla and residual convolutional networks. KE reduces both overfitting and the burden for data collection.   We evaluate KE on various network architectures and loss functions. We evaluate KE using relatively small datasets (e.g., CUB-200) and randomly initialized deep networks. KE achieves an absolute 21% improvement margin on a state-of-the-art baseline. This performance improvement is accompanied by a relative 73% reduction in inference cost. KE achieves state-of-the-art results on classification and metric learning benchmarks. Code available at http://bit.ly/3uLgwYb



### Deep Learning-based High-precision Depth Map Estimation from Missing Viewpoints for 360 Degree Digital Holography
- **Arxiv ID**: http://arxiv.org/abs/2103.05158v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.05158v1)
- **Published**: 2021-03-09 00:38:23+00:00
- **Updated**: 2021-03-09 00:38:23+00:00
- **Authors**: Hakdong Kim, Heonyeong Lim, Minkyu Jee, Yurim Lee, Jisoo Jeong, Kyudam Choi, MinSung Yoon, Cheongwon Kim
- **Comment**: 12 pages, 10 figures, 5 tables
- **Journal**: None
- **Summary**: In this paper, we propose a novel, convolutional neural network model to extract highly precise depth maps from missing viewpoints, especially well applicable to generate holographic 3D contents. The depth map is an essential element for phase extraction which is required for synthesis of computer-generated hologram (CGH). The proposed model called the HDD Net uses MSE for the better performance of depth map estimation as loss function, and utilizes the bilinear interpolation in up sampling layer with the Relu as activation function. We design and prepare a total of 8,192 multi-view images, each resolution of 640 by 360 for the deep learning study. The proposed model estimates depth maps through extracting features, up sampling. For quantitative assessment, we compare the estimated depth maps with the ground truths by using the PSNR, ACC, and RMSE. We also compare the CGH patterns made from estimated depth maps with ones made from ground truths. Furthermore, we demonstrate the experimental results to test the quality of estimated depth maps through directly reconstructing holographic 3D image scenes from the CGHs.



### Anomalous entities detection using a cascade of deep learning models
- **Arxiv ID**: http://arxiv.org/abs/2103.05164v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05164v1)
- **Published**: 2021-03-09 01:23:19+00:00
- **Updated**: 2021-03-09 01:23:19+00:00
- **Authors**: Hamza Riaz, Muhammad Uzair, Habib Ullah
- **Comment**: None
- **Journal**: None
- **Summary**: Human actions that do not conform to usual behavior are considered as anomalous and such actors are called anomalous entities. Detection of anomalous entities using visual data is a challenging problem in computer vision. This paper presents a new approach to detect anomalous entities in complex situations of examination halls. The proposed method uses a cascade of deep convolutional neural network models. In the first stage, we apply a pretrained model of human pose estimation on frames of videos to extract key feature points of body. Patches extracted from each key point are utilized in the second stage to build a densely connected deep convolutional neural network model for detecting anomalous entities. For experiments we collect a video database of students undertaking examination in a hall. Our results show that the proposed method can detect anomalous entities and warrant unusual behavior with high accuracy.



### Sequential Learning on Liver Tumor Boundary Semantics and Prognostic Biomarker Mining
- **Arxiv ID**: http://arxiv.org/abs/2103.05170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05170v1)
- **Published**: 2021-03-09 01:43:05+00:00
- **Updated**: 2021-03-09 01:43:05+00:00
- **Authors**: Jieneng Chen, Ke Yan, Yu-Dong Zhang, Youbao Tang, Xun Xu, Shuwen Sun, Qiuping Liu, Lingyun Huang, Jing Xiao, Alan L. Yuille, Ya Zhang, Le Lu
- **Comment**: None
- **Journal**: None
- **Summary**: The boundary of tumors (hepatocellular carcinoma, or HCC) contains rich semantics: capsular invasion, visibility, smoothness, folding and protuberance, etc. Capsular invasion on tumor boundary has proven to be clinically correlated with the prognostic indicator, microvascular invasion (MVI). Investigating tumor boundary semantics has tremendous clinical values. In this paper, we propose the first and novel computational framework that disentangles the task into two components: spatial vertex localization and sequential semantic classification. (1) A HCC tumor segmentor is built for tumor mask boundary extraction, followed by polar transform representing the boundary with radius and angle. Vertex generator is used to produce fixed-length boundary vertices where vertex features are sampled on the corresponding spatial locations. (2) The sampled deep vertex features with positional embedding are mapped into a sequential space and decoded by a multilayer perceptron (MLP) for semantic classification. Extensive experiments on tumor capsule semantics demonstrate the effectiveness of our framework. Mining the correlation between the boundary semantics and MVI status proves the feasibility to integrate this boundary semantics as a valid HCC prognostic biomarker.



### Deep Manifold Learning for Dynamic MR Imaging
- **Arxiv ID**: http://arxiv.org/abs/2104.01102v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2104.01102v1)
- **Published**: 2021-03-09 02:18:08+00:00
- **Updated**: 2021-03-09 02:18:08+00:00
- **Authors**: Ziwen Ke, Zhuo-Xu Cui, Wenqi Huang, Jing Cheng, Sen Jia, Haifeng Wang, Xin Liu, Hairong Zheng, Leslie Ying, Yanjie Zhu, Dong Liang
- **Comment**: 17 pages, 7 figures
- **Journal**: None
- **Summary**: Purpose: To develop a deep learning method on a nonlinear manifold to explore the temporal redundancy of dynamic signals to reconstruct cardiac MRI data from highly undersampled measurements.   Methods: Cardiac MR image reconstruction is modeled as general compressed sensing (CS) based optimization on a low-rank tensor manifold. The nonlinear manifold is designed to characterize the temporal correlation of dynamic signals. Iterative procedures can be obtained by solving the optimization model on the manifold, including gradient calculation, projection of the gradient to tangent space, and retraction of the tangent space to the manifold. The iterative procedures on the manifold are unrolled to a neural network, dubbed as Manifold-Net. The Manifold-Net is trained using in vivo data with a retrospective electrocardiogram (ECG)-gated segmented bSSFP sequence.   Results: Experimental results at high accelerations demonstrate that the proposed method can obtain improved reconstruction compared with a compressed sensing (CS) method k-t SLR and two state-of-the-art deep learning-based methods, DC-CNN and CRNN.   Conclusion: This work represents the first study unrolling the optimization on manifolds into neural networks. Specifically, the designed low-rank manifold provides a new technical route for applying low-rank priors in dynamic MR imaging.



### Iterative Shrinking for Referring Expression Grounding Using Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.05187v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05187v1)
- **Published**: 2021-03-09 02:36:45+00:00
- **Updated**: 2021-03-09 02:36:45+00:00
- **Authors**: Mingjie Sun, Jimin Xiao, Eng Gee Lim
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we are tackling the proposal-free referring expression grounding task, aiming at localizing the target object according to a query sentence, without relying on off-the-shelf object proposals. Existing proposal-free methods employ a query-image matching branch to select the highest-score point in the image feature map as the target box center, with its width and height predicted by another branch. Such methods, however, fail to utilize the contextual relation between the target and reference objects, and lack interpretability on its reasoning procedure. To solve these problems, we propose an iterative shrinking mechanism to localize the target, where the shrinking direction is decided by a reinforcement learning agent, with all contents within the current image patch comprehensively considered. Beside, the sequential shrinking process enables to demonstrate the reasoning about how to iteratively find the target. Experiments show that the proposed method boosts the accuracy by 4.32% against the previous state-of-the-art (SOTA) method on the RefCOCOg dataset, where query sentences are long and complex, with many targets referred by other reference objects.



### Generative Transition Mechanism to Image-to-Image Translation via Encoded Transformation
- **Arxiv ID**: http://arxiv.org/abs/2103.05193v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05193v1)
- **Published**: 2021-03-09 02:56:03+00:00
- **Updated**: 2021-03-09 02:56:03+00:00
- **Authors**: Yaxin Shi, Xiaowei Zhou, Ping Liu, Ivor Tsang
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: In this paper, we revisit the Image-to-Image (I2I) translation problem with transition consistency, namely the consistency defined on the conditional data mapping between each data pairs. Explicitly parameterizing each data mappings with a transition variable $t$, i.e., $x \overset{t(x,y)}{\mapsto}y$, we discover that existing I2I translation models mainly focus on maintaining consistency on results, e.g., image reconstruction or attribute prediction, named result consistency in our paper. This restricts their generalization ability to generate satisfactory results with unseen transitions in the test phase. Consequently, we propose to enforce both result consistency and transition consistency for I2I translation, to benefit the problem with a closer consistency between the input and output. To benefit the generalization ability of the translation model, we propose transition encoding to facilitate explicit regularization of these two {kinds} of consistencies on unseen transitions. We further generalize such explicitly regularized consistencies to distribution-level, thus facilitating a generalized overall consistency for I2I translation problems. With the above design, our proposed model, named Transition Encoding GAN (TEGAN), can poss superb generalization ability to generate realistic and semantically consistent translation results with unseen transitions in the test phase. It also provides a unified understanding of the existing GAN-based I2I transition models with our explicitly modeling of the data mapping, i.e., transition. Experiments on four different I2I translation tasks demonstrate the efficacy and generality of TEGAN.



### Enhancing Medical Image Registration via Appearance Adjustment Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.05213v2
- **DOI**: 10.1016/j.neuroimage.2022.119444
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.05213v2)
- **Published**: 2021-03-09 04:24:48+00:00
- **Updated**: 2022-07-04 03:10:24+00:00
- **Authors**: Mingyuan Meng, Lei Bi, Michael Fulham, David Dagan Feng, Jinman Kim
- **Comment**: Published at NeuroImage
- **Journal**: NeuroImage, vol. 259, pp. 119444, 2022
- **Summary**: Deformable image registration is fundamental for many medical image analyses. A key obstacle for accurate image registration lies in image appearance variations such as the variations in texture, intensities, and noise. These variations are readily apparent in medical images, especially in brain images where registration is frequently used. Recently, deep learning-based registration methods (DLRs), using deep neural networks, have shown computational efficiency that is several orders of magnitude faster than traditional optimization-based registration methods (ORs). DLRs rely on a globally optimized network that is trained with a set of training samples to achieve faster registration. DLRs tend, however, to disregard the target-pair-specific optimization inherent in ORs and thus have degraded adaptability to variations in testing samples. This limitation is severe for registering medical images with large appearance variations, especially since few existing DLRs explicitly take into account appearance variations. In this study, we propose an Appearance Adjustment Network (AAN) to enhance the adaptability of DLRs to appearance variations. Our AAN, when integrated into a DLR, provides appearance transformations to reduce the appearance variations during registration. In addition, we propose an anatomy-constrained loss function through which our AAN generates anatomy-preserving transformations. Our AAN has been purposely designed to be readily inserted into a wide range of DLRs and can be trained cooperatively in an unsupervised and end-to-end manner. We evaluated our AAN with three state-of-the-art DLRs on three well-established public datasets of 3D brain magnetic resonance imaging (MRI). The results show that our AAN consistently improved existing DLRs and outperformed state-of-the-art ORs on registration accuracy, while adding a fractional computational load to existing DLRs.



### Universal Undersampled MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2103.05214v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.05214v1)
- **Published**: 2021-03-09 04:25:22+00:00
- **Updated**: 2021-03-09 04:25:22+00:00
- **Authors**: Xinwen Liu, Jing Wang, Feng Liu, S. Kevin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have been extensively studied for undersampled MRI reconstruction. While achieving state-of-the-art performance, they are trained and deployed specifically for one anatomy with limited generalization ability to another anatomy. Rather than building multiple models, a universal model that reconstructs images across different anatomies is highly desirable for efficient deployment and better generalization. Simply mixing images from multiple anatomies for training a single network does not lead to an ideal universal model due to the statistical shift among datasets of various anatomies, the need to retrain from scratch on all datasets with the addition of a new dataset, and the difficulty in dealing with imbalanced sampling when the new dataset is further of a smaller size. In this paper, for the first time, we propose a framework to learn a universal deep neural network for undersampled MRI reconstruction. Specifically, anatomy-specific instance normalization is proposed to compensate for statistical shift and allow easy generalization to new datasets. Moreover, the universal model is trained by distilling knowledge from available independent models to further exploit representations across anatomies. Experimental results show the proposed universal model can reconstruct both brain and knee images with high image quality. Also, it is easy to adapt the trained model to new datasets of smaller size, i.e., abdomen, cardiac and prostate, with little effort and superior performance.



### Prediction of 5-year Progression-Free Survival in Advanced Nasopharyngeal Carcinoma with Pretreatment PET/CT using Multi-Modality Deep Learning-based Radiomics
- **Arxiv ID**: http://arxiv.org/abs/2103.05220v2
- **DOI**: 10.3389/fonc.2022.899351
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2103.05220v2)
- **Published**: 2021-03-09 04:43:33+00:00
- **Updated**: 2022-07-04 04:50:30+00:00
- **Authors**: Bingxin Gu, Mingyuan Meng, Lei Bi, Jinman Kim, David Dagan Feng, Shaoli Song
- **Comment**: Accepted at Frontiers in Oncology
- **Journal**: Frontiers in Oncology, vol. 12, pp. 899352, 2022
- **Summary**: Objective: Deep Learning-based Radiomics (DLR) has achieved great success in medical image analysis and has been considered a replacement for conventional radiomics that relies on handcrafted features. In this study, we aimed to explore the capability of DLR for the prediction of 5-year Progression-Free Survival (PFS) in Nasopharyngeal Carcinoma (NPC) using pretreatment PET/CT. Methods: A total of 257 patients (170/87 in internal/external cohorts) with advanced NPC (TNM stage III or IVa) were enrolled. We developed an end-to-end multi-modality DLR model, in which a 3D convolutional neural network was optimized to extract deep features from pretreatment PET/CT images and predict the probability of 5-year PFS. TNM stage, as a high-level clinical feature, could be integrated into our DLR model to further improve the prognostic performance. To compare conventional radiomics and DLR, 1456 handcrafted features were extracted, and optimal conventional radiomics methods were selected from 54 cross-combinations of 6 feature selection methods and 9 classification methods. In addition, risk group stratification was performed with clinical signature, conventional radiomics signature, and DLR signature. Results: Our multi-modality DLR model using both PET and CT achieved higher prognostic performance than the optimal conventional radiomics method. Furthermore, the multi-modality DLR model outperformed single-modality DLR models using only PET or only CT. For risk group stratification, the conventional radiomics signature and DLR signature enabled significant differences between the high- and low-risk patient groups in both internal and external cohorts, while the clinical signature failed in the external cohort. Conclusion: Our study identified potential prognostic tools for survival prediction in advanced NPC, suggesting that DLR could provide complementary values to the current TNM staging.



### Data augmentation by morphological mixup for solving Raven's Progressive Matrices
- **Arxiv ID**: http://arxiv.org/abs/2103.05222v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05222v2)
- **Published**: 2021-03-09 04:50:32+00:00
- **Updated**: 2021-11-19 07:37:38+00:00
- **Authors**: Wentao He, Jianfeng Ren, Ruibin Bai
- **Comment**: Under review
- **Journal**: None
- **Summary**: Raven's Progressive Matrices (RPMs) are frequently used in testing human's visual reasoning ability. Recent advances of RPM-like datasets and solution models partially address the challenges of visually understanding the RPM questions and logically reasoning the missing answers. In view of the poor generalization performance due to insufficient samples in RPM datasets, we propose an effective scheme, namely Candidate Answer Morphological Mixup (CAM-Mix). CAM-Mix serves as a data augmentation strategy by gray-scale image morphological mixup, which regularizes various solution methods and overcomes the model overfitting problem. By creating new negative candidate answers semantically similar to the correct answers, a more accurate decision boundary could be defined. By applying the proposed data augmentation method, a significant and consistent performance improvement is achieved on various RPM-like datasets compared with the state-of-the-art models.



### DeepSeagrass Dataset
- **Arxiv ID**: http://arxiv.org/abs/2103.05226v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05226v1)
- **Published**: 2021-03-09 05:08:01+00:00
- **Updated**: 2021-03-09 05:08:01+00:00
- **Authors**: Scarlett Raine, Ross Marchant, Peyman Moghadam, Frederic Maire, Brett Kettle, Brano Kusy
- **Comment**: arXiv admin note: text overlap with arXiv:2009.09924
- **Journal**: None
- **Summary**: We introduce a dataset of seagrass images collected by a biologist snorkelling in Moreton Bay, Queensland, Australia, as described in our publication: arXiv:2009.09924. The images are labelled at the image-level by collecting images of the same morphotype in a folder hierarchy. We also release pre-trained models and training codes for detection and classification of seagrass species at the patch level at https://github.com/csiro-robotics/deepseagrass.



### Uncertainty-aware Incremental Learning for Multi-organ Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.05227v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05227v1)
- **Published**: 2021-03-09 05:12:39+00:00
- **Updated**: 2021-03-09 05:12:39+00:00
- **Authors**: Yuhang Zhou, Xiaoman Zhang, Shixiang Feng, Ya Zhang, Yanfeng
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing approaches to train a unified multi-organ segmentation model from several single-organ datasets require simultaneously access multiple datasets during training. In the real scenarios, due to privacy and ethics concerns, the training data of the organs of interest may not be publicly available. To this end, we investigate a data-free incremental organ segmentation scenario and propose a novel incremental training framework to solve it. We use the pretrained model instead of its own training data for privacy protection. Specifically, given a pretrained $K$ organ segmentation model and a new single-organ dataset, we train a unified $K+1$ organ segmentation model without accessing any data belonging to the previous training stages. Our approach consists of two parts: the background label alignment strategy and the uncertainty-aware guidance strategy. The first part is used for knowledge transfer from the pretained model to the training model. The second part is used to extract the uncertainty information from the pretrained model to guide the whole knowledge transfer process. By combing these two strategies, more reliable information is extracted from the pretrained model without original training data. Experiments on multiple publicly available pretrained models and a multi-organ dataset MOBA have demonstrated the effectiveness of our framework.



### Stabilized Medical Image Attacks
- **Arxiv ID**: http://arxiv.org/abs/2103.05232v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.05232v1)
- **Published**: 2021-03-09 05:40:30+00:00
- **Updated**: 2021-03-09 05:40:30+00:00
- **Authors**: Gege Qi, Lijun Gong, Yibing Song, Kai Ma, Yefeng Zheng
- **Comment**: ICLR 2021 (Spotlight)
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have advanced existing medical systems for automatic disease diagnosis. However, a threat to these systems arises that adversarial attacks make CNNs vulnerable. Inaccurate diagnosis results make a negative influence on human healthcare. There is a need to investigate potential adversarial attacks to robustify deep medical diagnosis systems. On the other side, there are several modalities of medical images (e.g., CT, fundus, and endoscopic image) of which each type is significantly different from others. It is more challenging to generate adversarial perturbations for different types of medical images. In this paper, we propose an image-based medical adversarial attack method to consistently produce adversarial perturbations on medical images. The objective function of our method consists of a loss deviation term and a loss stabilization term. The loss deviation term increases the divergence between the CNN prediction of an adversarial example and its ground truth label. Meanwhile, the loss stabilization term ensures similar CNN predictions of this example and its smoothed input. From the perspective of the whole iterations for perturbation generation, the proposed loss stabilization term exhaustively searches the perturbation space to smooth the single spot for local optimum escape. We further analyze the KL-divergence of the proposed loss function and find that the loss stabilization term makes the perturbations updated towards a fixed objective spot while deviating from the ground truth. This stabilization ensures the proposed medical attack effective for different types of medical images while producing perturbations in small variance. Experiments on several medical image analysis benchmarks including the recent COVID-19 dataset show the stability of the proposed method.



### Practical Relative Order Attack in Deep Ranking
- **Arxiv ID**: http://arxiv.org/abs/2103.05248v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2103.05248v4)
- **Published**: 2021-03-09 06:41:18+00:00
- **Updated**: 2021-08-09 15:54:56+00:00
- **Authors**: Mo Zhou, Le Wang, Zhenxing Niu, Qilin Zhang, Yinghui Xu, Nanning Zheng, Gang Hua
- **Comment**: ICCV2021 Poster
- **Journal**: None
- **Summary**: Recent studies unveil the vulnerabilities of deep ranking models, where an imperceptible perturbation can trigger dramatic changes in the ranking result. While previous attempts focus on manipulating absolute ranks of certain candidates, the possibility of adjusting their relative order remains under-explored. In this paper, we formulate a new adversarial attack against deep ranking systems, i.e., the Order Attack, which covertly alters the relative order among a selected set of candidates according to an attacker-specified permutation, with limited interference to other unrelated candidates. Specifically, it is formulated as a triplet-style loss imposing an inequality chain reflecting the specified permutation. However, direct optimization of such white-box objective is infeasible in a real-world attack scenario due to various black-box limitations. To cope with them, we propose a Short-range Ranking Correlation metric as a surrogate objective for black-box Order Attack to approximate the white-box method. The Order Attack is evaluated on the Fashion-MNIST and Stanford-Online-Products datasets under both white-box and black-box threat models. The black-box attack is also successfully implemented on a major e-commerce platform. Comprehensive experimental evaluations demonstrate the effectiveness of the proposed methods, revealing a new type of ranking model vulnerability.



### Enhancing sensor resolution improves CNN accuracy given the same number of parameters or FLOPS
- **Arxiv ID**: http://arxiv.org/abs/2103.05251v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05251v1)
- **Published**: 2021-03-09 06:47:01+00:00
- **Updated**: 2021-03-09 06:47:01+00:00
- **Authors**: Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: High image resolution is critical to obtain a good performance in many computer vision applications. Computational complexity of CNNs, however, grows significantly with the increase in input image size. Here, we show that it is almost always possible to modify a network such that it achieves higher accuracy at a higher input resolution while having the same number of parameters or/and FLOPS. The idea is similar to the EfficientNet paper but instead of optimizing network width, depth and resolution simultaneously, here we focus only on input resolution. This makes the search space much smaller which is more suitable for low computational budget regimes. More importantly, by controlling for the number of model parameters (and hence model capacity), we show that the additional benefit in accuracy is indeed due to the higher input resolution. Preliminary empirical investigation over MNIST, Fashion MNIST, and CIFAR10 datasets demonstrates the efficiency of the proposed approach.



### MetaCorrection: Domain-aware Meta Loss Correction for Unsupervised Domain Adaptation in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.05254v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05254v1)
- **Published**: 2021-03-09 06:57:03+00:00
- **Updated**: 2021-03-09 06:57:03+00:00
- **Authors**: Xiaoqing Guo, Chen Yang, Baopu Li, Yixuan Yuan
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) aims to transfer the knowledge from the labeled source domain to the unlabeled target domain. Existing self-training based UDA approaches assign pseudo labels for target data and treat them as ground truth labels to fully leverage unlabeled target data for model adaptation. However, the generated pseudo labels from the model optimized on the source domain inevitably contain noise due to the domain gap. To tackle this issue, we advance a MetaCorrection framework, where a Domain-aware Meta-learning strategy is devised to benefit Loss Correction (DMLC) for UDA semantic segmentation. In particular, we model the noise distribution of pseudo labels in target domain by introducing a noise transition matrix (NTM) and construct meta data set with domain-invariant source data to guide the estimation of NTM. Through the risk minimization on the meta data set, the optimized NTM thus can correct the noisy issues in pseudo labels and enhance the generalization ability of the model on the target data. Considering the capacity gap between shallow and deep features, we further employ the proposed DMLC strategy to provide matched and compatible supervision signals for different level features, thereby ensuring deep adaptation. Extensive experimental results highlight the effectiveness of our method against existing state-of-the-art methods on three benchmarks.



### Improving Generalizability in Limited-Angle CT Reconstruction with Sinogram Extrapolation
- **Arxiv ID**: http://arxiv.org/abs/2103.05255v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.05255v4)
- **Published**: 2021-03-09 06:58:09+00:00
- **Updated**: 2021-11-17 05:44:54+00:00
- **Authors**: Ce Wang, Haimiao Zhang, Qian Li, Kun Shang, Yuanyuan Lyu, Bin Dong, S. Kevin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Computed tomography (CT) reconstruction from X-ray projections acquired within a limited angle range is challenging, especially when the angle range is extremely small. Both analytical and iterative models need more projections for effective modeling. Deep learning methods have gained prevalence due to their excellent reconstruction performances, but such success is mainly limited within the same dataset and does not generalize across datasets with different distributions. Hereby we propose ExtraPolationNetwork for limited-angle CT reconstruction via the introduction of a sinogram extrapolation module, which is theoretically justified. The module complements extra sinogram information and boots model generalizability. Extensive experimental results show that our reconstruction model achieves state-of-the-art performance on NIH-AAPM dataset, similar to existing approaches. More importantly, we show that using such a sinogram extrapolation module significantly improves the generalization capability of the model on unseen datasets (e.g., COVID-19 and LIDC datasets) when compared to existing approaches.



### 2D histology meets 3D topology: Cytoarchitectonic brain mapping with Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.05259v1
- **DOI**: 10.1007/978-3-030-87237-3_38
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05259v1)
- **Published**: 2021-03-09 07:09:42+00:00
- **Updated**: 2021-03-09 07:09:42+00:00
- **Authors**: Christian Schiffer, Stefan Harmeling, Katrin Amunts, Timo Dickscheid
- **Comment**: None
- **Journal**: None
- **Summary**: Cytoarchitecture describes the spatial organization of neuronal cells in the brain, including their arrangement into layers and columns with respect to cell density, orientation, or presence of certain cell types. It allows to segregate the brain into cortical areas and subcortical nuclei, links structure with connectivity and function, and provides a microstructural reference for human brain atlases. Mapping boundaries between areas requires to scan histological sections at microscopic resolution. While recent high-throughput scanners allow to scan a complete human brain in the order of a year, it is practically impossible to delineate regions at the same pace using the established gold standard method. Researchers have recently addressed cytoarchitectonic mapping of cortical regions with deep neural networks, relying on image patches from individual 2D sections for classification. However, the 3D context, which is needed to disambiguate complex or obliquely cut brain regions, is not taken into account. In this work, we combine 2D histology with 3D topology by reformulating the mapping task as a node classification problem on an approximate 3D midsurface mesh through the isocortex. We extract deep features from cortical patches in 2D histological sections which are descriptive of cytoarchitecture, and assign them to the corresponding nodes on the 3D mesh to construct a large attributed graph. By solving the brain mapping problem on this graph using graph neural networks, we obtain significantly improved classification results. The proposed framework lends itself nicely to integration of additional neuroanatomical priors for mapping.



### BASAR:Black-box Attack on Skeletal Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.05266v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.05266v6)
- **Published**: 2021-03-09 07:29:35+00:00
- **Updated**: 2021-07-26 00:58:45+00:00
- **Authors**: Yunfeng Diao, Tianjia Shao, Yong-Liang Yang, Kun Zhou, He Wang
- **Comment**: Accepted in CVPR 2021
- **Journal**: None
- **Summary**: Skeletal motion plays a vital role in human activity recognition as either an independent data source or a complement. The robustness of skeleton-based activity recognizers has been questioned recently, which shows that they are vulnerable to adversarial attacks when the full-knowledge of the recognizer is accessible to the attacker. However, this white-box requirement is overly restrictive in most scenarios and the attack is not truly threatening. In this paper, we show that such threats do exist under black-box settings too. To this end, we propose the first black-box adversarial attack method BASAR. Through BASAR, we show that adversarial attack is not only truly a threat but also can be extremely deceitful, because on-manifold adversarial samples are rather common in skeletal motions, in contrast to the common belief that adversarial samples only exist off-manifold. Through exhaustive evaluation and comparison, we show that BASAR can deliver successful attacks across models, data, and attack modes. Through harsh perceptual studies, we show that it achieves effective yet imperceptible attacks. By analyzing the attack on different activity recognizers, BASAR helps identify the potential causes of their vulnerability and provides insights on what classifiers are likely to be more robust against attack. Code is available at https://github.com/realcrane/BASAR-Black-box-Attack-on-Skeletal-Action-Recognition.



### PcmNet: Position-Sensitive Context Modeling Network for Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2103.05270v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05270v1)
- **Published**: 2021-03-09 07:34:01+00:00
- **Updated**: 2021-03-09 07:34:01+00:00
- **Authors**: Xin Qin, Hanbin Zhao, Guangchen Lin, Hao Zeng, Songcen Xu, Xi Li
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal action localization is an important and challenging task that aims to locate temporal regions in real-world untrimmed videos where actions occur and recognize their classes. It is widely acknowledged that video context is a critical cue for video understanding, and exploiting the context has become an important strategy to boost localization performance. However, previous state-of-the-art methods focus more on exploring semantic context which captures the feature similarity among frames or proposals, and neglect positional context which is vital for temporal localization. In this paper, we propose a temporal-position-sensitive context modeling approach to incorporate both positional and semantic information for more precise action localization. Specifically, we first augment feature representations with directed temporal positional encoding, and then conduct attention-based information propagation, in both frame-level and proposal-level. Consequently, the generated feature representations are significantly empowered with the discriminative capability of encoding the position-aware context information, and thus benefit boundary detection and proposal evaluation. We achieve state-of-the-art performance on both two challenging datasets, THUMOS-14 and ActivityNet-1.3, demonstrating the effectiveness and generalization ability of our method.



### Probabilistic Modeling of Semantic Ambiguity for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2103.05271v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05271v2)
- **Published**: 2021-03-09 07:36:09+00:00
- **Updated**: 2021-03-10 05:20:48+00:00
- **Authors**: Gengcong Yang, Jingyi Zhang, Yong Zhang, Baoyuan Wu, Yujiu Yang
- **Comment**: CVPR 2021 poster
- **Journal**: None
- **Summary**: To generate "accurate" scene graphs, almost all existing methods predict pairwise relationships in a deterministic manner. However, we argue that visual relationships are often semantically ambiguous. Specifically, inspired by linguistic knowledge, we classify the ambiguity into three types: Synonymy Ambiguity, Hyponymy Ambiguity, and Multi-view Ambiguity. The ambiguity naturally leads to the issue of \emph{implicit multi-label}, motivating the need for diverse predictions. In this work, we propose a novel plug-and-play Probabilistic Uncertainty Modeling (PUM) module. It models each union region as a Gaussian distribution, whose variance measures the uncertainty of the corresponding visual content. Compared to the conventional deterministic methods, such uncertainty modeling brings stochasticity of feature representation, which naturally enables diverse predictions. As a byproduct, PUM also manages to cover more fine-grained relationships and thus alleviates the issue of bias towards frequent relationships. Extensive experiments on the large-scale Visual Genome benchmark show that combining PUM with newly proposed ResCAGCN can achieve state-of-the-art performances, especially under the mean recall metric. Furthermore, we prove the universal effectiveness of PUM by plugging it into some existing models and provide insightful analysis of its ability to generate diverse yet plausible visual relationships.



### Towards New Multiwavelets: Associated Filters and Algorithms. Part I: Theoretical Framework and Investigation of Biomedical Signals, ECG and Coronavirus Cases
- **Arxiv ID**: http://arxiv.org/abs/2103.08657v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, physics.med-ph, 42C40, 65T60, 94A08
- **Links**: [PDF](http://arxiv.org/pdf/2103.08657v1)
- **Published**: 2021-03-09 07:49:21+00:00
- **Updated**: 2021-03-09 07:49:21+00:00
- **Authors**: Malika Jallouli, Makerem Zemni, Anouar Ben Mabrouk, Mohamed Ali Mahjoub
- **Comment**: 28 pages, 9 figures, 6 tables
- **Journal**: None
- **Summary**: Biosignals are nowadays important subjects for scientific researches from both theory and applications especially with the appearance of new pandemics threatening humanity such as the new Coronavirus. One aim in the present work is to prove that Wavelets may be successful machinery to understand such phenomena by applying a step forward extension of wavelets to multiwavelets. We proposed in a first step to improve the multiwavelet notion by constructing more general families using independent components for multi-scaling and multiwavelet mother functions. A special multiwavelet is then introduced, continuous and discrete multiwavelet transforms are associated, as well as new filters and algorithms of decomposition and reconstruction. The constructed multiwavelet framework is applied for some experimentations showing fast algorithms, ECG signal, and a strain of Coronavirus processing.



### Open-book Video Captioning with Retrieve-Copy-Generate Network
- **Arxiv ID**: http://arxiv.org/abs/2103.05284v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2103.05284v1)
- **Published**: 2021-03-09 08:17:17+00:00
- **Updated**: 2021-03-09 08:17:17+00:00
- **Authors**: Ziqi Zhang, Zhongang Qi, Chunfeng Yuan, Ying Shan, Bing Li, Ying Deng, Weiming Hu
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Due to the rapid emergence of short videos and the requirement for content understanding and creation, the video captioning task has received increasing attention in recent years. In this paper, we convert traditional video captioning task into a new paradigm, \ie, Open-book Video Captioning, which generates natural language under the prompts of video-content-relevant sentences, not limited to the video itself. To address the open-book video captioning problem, we propose a novel Retrieve-Copy-Generate network, where a pluggable video-to-text retriever is constructed to retrieve sentences as hints from the training corpus effectively, and a copy-mechanism generator is introduced to extract expressions from multi-retrieved sentences dynamically. The two modules can be trained end-to-end or separately, which is flexible and extensible. Our framework coordinates the conventional retrieval-based methods with orthodox encoder-decoder methods, which can not only draw on the diverse expressions in the retrieved sentences but also generate natural and accurate content of the video. Extensive experiments on several benchmark datasets show that our proposed approach surpasses the state-of-the-art performance, indicating the effectiveness and promising of the proposed paradigm in the task of video captioning.



### 3D-QCNet -- A Pipeline for Automated Artifact Detection in Diffusion MRI images
- **Arxiv ID**: http://arxiv.org/abs/2103.05285v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.05285v1)
- **Published**: 2021-03-09 08:21:53+00:00
- **Updated**: 2021-03-09 08:21:53+00:00
- **Authors**: Adnan Ahmad, Drew Parker, Zahra Riahi Samani, Ragini Verma
- **Comment**: None
- **Journal**: None
- **Summary**: Artifacts are a common occurrence in Diffusion MRI (dMRI) scans. Identifying and removing them is essential to ensure the accuracy and viability of any post processing carried out on these scans. This makes QC (quality control) a crucial first step prior to any analysis of dMRI data. Several QC methods for artifact detection exist, however they suffer from problems like requiring manual intervention and the inability to generalize across different artifacts and datasets. In this paper, we propose an automated deep learning (DL) pipeline that utilizes a 3D-Densenet architecture to train a model on diffusion volumes for automatic artifact detection. Our method is applied on a vast dataset consisting of 9000 volumes sourced from 7 large clinical datasets. These datasets comprise scans from multiple scanners with different gradient directions, high and low b values, single shell and multi shell acquisitions. Additionally, they represent diverse subject demographics like the presence or absence of pathologies. Our QC method is found to accurately generalize across this heterogenous data by correctly detecting 92% artifacts on average across our test set. This consistent performance over diverse datasets underlines the generalizability of our method, which currently is a significant barrier hindering the widespread adoption of automated QC techniques. For these reasons, we believe that 3D-QCNet can be integrated in diffusion pipelines to effectively automate the arduous and time-intensive process of artifact detection.



### Bio-Inspired Representation Learning for Visual Attention Prediction
- **Arxiv ID**: http://arxiv.org/abs/2103.05310v1
- **DOI**: 10.1109/TCYB.2019.2931735
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05310v1)
- **Published**: 2021-03-09 09:15:36+00:00
- **Updated**: 2021-03-09 09:15:36+00:00
- **Authors**: Yuan Yuan, Hailong Ning, Xiaoqiang Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Attention Prediction (VAP) is a significant and imperative issue in the field of computer vision. Most of existing VAP methods are based on deep learning. However, they do not fully take advantage of the low-level contrast features while generating the visual attention map. In this paper, a novel VAP method is proposed to generate visual attention map via bio-inspired representation learning. The bio-inspired representation learning combines both low-level contrast and high-level semantic features simultaneously, which are developed by the fact that human eye is sensitive to the patches with high contrast and objects with high semantics. The proposed method is composed of three main steps: 1) feature extraction, 2) bio-inspired representation learning and 3) visual attention map generation. Firstly, the high-level semantic feature is extracted from the refined VGG16, while the low-level contrast feature is extracted by the proposed contrast feature extraction block in a deep network. Secondly, during bio-inspired representation learning, both the extracted low-level contrast and high-level semantic features are combined by the designed densely connected block, which is proposed to concatenate various features scale by scale. Finally, the weighted-fusion layer is exploited to generate the ultimate visual attention map based on the obtained representations after bio-inspired representation learning. Extensive experiments are performed to demonstrate the effectiveness of the proposed method.



### A Mask R-CNN approach to counting bacterial colony forming units in pharmaceutical development
- **Arxiv ID**: http://arxiv.org/abs/2103.05337v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2103.05337v1)
- **Published**: 2021-03-09 10:31:00+00:00
- **Updated**: 2021-03-09 10:31:00+00:00
- **Authors**: Tanguy Naets, Maarten Huijsmans, Paul Smyth, Laurent Sorber, Gaël de Lannoy
- **Comment**: 9 pages, 3 pdf figures. Extended version of poster presented at ESANN
  2020 (European Symposium on Artificial Neural Networks, Computational
  Intelligence and Machine Learning)
- **Journal**: None
- **Summary**: We present an application of the well-known Mask R-CNN approach to the counting of different types of bacterial colony forming units that were cultured in Petri dishes. Our model was made available to lab technicians in a modern SPA (Single-Page Application). Users can upload images of dishes, after which the Mask R-CNN model that was trained and tuned specifically for this task detects the number of BVG- and BVG+ colonies and displays these in an interactive interface for the user to verify. Users can then check the model's predictions, correct them if deemed necessary, and finally validate them. Our adapted Mask R-CNN model achieves a mean average precision (mAP) of 94\% at an intersection-over-union (IoU) threshold of 50\%. With these encouraging results, we see opportunities to bring the benefits of improved accuracy and time saved to related problems, such as generalising to other bacteria types and viral foci counting.



### Cut-Thumbnail: A Novel Data Augmentation for Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2103.05342v2
- **DOI**: 10.1145/3474085.3475302
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05342v2)
- **Published**: 2021-03-09 10:45:55+00:00
- **Updated**: 2021-10-26 12:51:19+00:00
- **Authors**: Tianshu Xie, Xuan Cheng, Minghui Liu, Jiali Deng, Xiaomin Wang, Ming Liu
- **Comment**: Accepted at ACM MM 2021
- **Journal**: None
- **Summary**: In this paper, we propose a novel data augmentation strategy named Cut-Thumbnail, that aims to improve the shape bias of the network. We reduce an image to a certain size and replace the random region of the original image with the reduced image. The generated image not only retains most of the original image information but also has global information in the reduced image. We call the reduced image as thumbnail. Furthermore, we find that the idea of thumbnail can be perfectly integrated with Mixed Sample Data Augmentation, so we put one image's thumbnail on another image while the ground truth labels are also mixed, making great achievements on various computer vision tasks. Extensive experiments show that Cut-Thumbnail works better than state-of-the-art augmentation strategies across classification, fine-grained image classification, and object detection. On ImageNet classification, ResNet-50 architecture with our method achieves 79.21\% accuracy, which is more than 2.8\% improvement on the baseline.



### ST3D: Self-training for Unsupervised Domain Adaptation on 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.05346v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05346v2)
- **Published**: 2021-03-09 10:51:24+00:00
- **Updated**: 2021-03-27 07:36:13+00:00
- **Authors**: Jihan Yang, Shaoshuai Shi, Zhe Wang, Hongsheng Li, Xiaojuan Qi
- **Comment**: CVPR2021
- **Journal**: None
- **Summary**: We present a new domain adaptive self-training pipeline, named ST3D, for unsupervised domain adaptation on 3D object detection from point clouds. First, we pre-train the 3D detector on the source domain with our proposed random object scaling strategy for mitigating the negative effects of source domain bias. Then, the detector is iteratively improved on the target domain by alternatively conducting two steps, which are the pseudo label updating with the developed quality-aware triplet memory bank and the model training with curriculum data augmentation. These specific designs for 3D object detection enable the detector to be trained with consistent and high-quality pseudo labels and to avoid overfitting to the large number of easy examples in pseudo labeled data. Our ST3D achieves state-of-the-art performance on all evaluated datasets and even surpasses fully supervised results on KITTI 3D object detection benchmark. Code will be available at https://github.com/CVMI-Lab/ST3D.



### Understanding the Robustness of Skeleton-based Action Recognition under Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/2103.05347v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05347v2)
- **Published**: 2021-03-09 10:53:58+00:00
- **Updated**: 2021-03-18 19:49:44+00:00
- **Authors**: He Wang, Feixiang He, Zhexi Peng, Tianjia Shao, Yong-Liang Yang, Kun Zhou, David Hogg
- **Comment**: Accepted in CVPR 2021. arXiv admin note: substantial text overlap
  with arXiv:1911.07107
- **Journal**: None
- **Summary**: Action recognition has been heavily employed in many applications such as autonomous vehicles, surveillance, etc, where its robustness is a primary concern. In this paper, we examine the robustness of state-of-the-art action recognizers against adversarial attack, which has been rarely investigated so far. To this end, we propose a new method to attack action recognizers that rely on 3D skeletal motion. Our method involves an innovative perceptual loss that ensures the imperceptibility of the attack. Empirical studies demonstrate that our method is effective in both white-box and black-box scenarios. Its generalizability is evidenced on a variety of action recognizers and datasets. Its versatility is shown in different attacking strategies. Its deceitfulness is proven in extensive perceptual studies. Our method shows that adversarial attack on 3D skeletal motions, one type of time-series data, is significantly different from traditional adversarial attack problems. Its success raises serious concern on the robustness of action recognizers and provides insights on potential improvements.



### Revisiting Model's Uncertainty and Confidences for Adversarial Example Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.05354v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05354v2)
- **Published**: 2021-03-09 11:06:15+00:00
- **Updated**: 2021-06-21 15:21:49+00:00
- **Authors**: Ahmed Aldahdooh, Wassim Hamidouche, Olivier Déforges
- **Comment**: Under review
- **Journal**: None
- **Summary**: Security-sensitive applications that rely on Deep Neural Networks (DNNs) are vulnerable to small perturbations that are crafted to generate Adversarial Examples(AEs). The AEs are imperceptible to humans and cause DNN to misclassify them. Many defense and detection techniques have been proposed. Model's confidences and Dropout, as a popular way to estimate the model's uncertainty, have been used for AE detection but they showed limited success against black- and gray-box attacks. Moreover, the state-of-the-art detection techniques have been designed for specific attacks or broken by others, need knowledge about the attacks, are not consistent, increase model parameters overhead, are time-consuming, or have latency in inference time. To trade off these factors, we revisit the model's uncertainty and confidences and propose a novel unsupervised ensemble AE detection mechanism that 1) uses the uncertainty method called SelectiveNet, 2) processes model layers outputs, i.e.feature maps, to generate new confidence probabilities. The detection method is called Selective and Feature based Adversarial Detection (SFAD). Experimental results show that the proposed approach achieves better performance against black- and gray-box attacks than the state-of-the-art methods and achieves comparable performance against white-box attacks. Moreover, results show that SFAD is fully robust against High Confidence Attacks (HCAs) for MNIST and partially robust for CIFAR10 datasets.



### MWQ: Multiscale Wavelet Quantized Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.05363v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/2103.05363v1)
- **Published**: 2021-03-09 11:21:59+00:00
- **Updated**: 2021-03-09 11:21:59+00:00
- **Authors**: Qigong Sun, Yan Ren, Licheng Jiao, Xiufang Li, Fanhua Shang, Fang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Model quantization can reduce the model size and computational latency, it has become an essential technique for the deployment of deep neural networks on resourceconstrained hardware (e.g., mobile phones and embedded devices). The existing quantization methods mainly consider the numerical elements of the weights and activation values, ignoring the relationship between elements. The decline of representation ability and information loss usually lead to the performance degradation. Inspired by the characteristics of images in the frequency domain, we propose a novel multiscale wavelet quantization (MWQ) method. This method decomposes original data into multiscale frequency components by wavelet transform, and then quantizes the components of different scales, respectively. It exploits the multiscale frequency and spatial information to alleviate the information loss caused by quantization in the spatial domain. Because of the flexibility of MWQ, we demonstrate three applications (e.g., model compression, quantized network optimization, and information enhancement) on the ImageNet and COCO datasets. Experimental results show that our method has stronger representation ability and can play an effective role in quantized neural networks.



### ChangeSim: Towards End-to-End Online Scene Change Detection in Industrial Indoor Environments
- **Arxiv ID**: http://arxiv.org/abs/2103.05368v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.05368v2)
- **Published**: 2021-03-09 11:36:29+00:00
- **Updated**: 2021-07-22 06:52:15+00:00
- **Authors**: Jin-Man Park, Jae-Hyuk Jang, Sahng-Min Yoo, Sun-Kyung Lee, Ue-Hwan Kim, Jong-Hwan Kim
- **Comment**: Accepted to IROS 2021
- **Journal**: None
- **Summary**: We present a challenging dataset, ChangeSim, aimed at online scene change detection (SCD) and more. The data is collected in photo-realistic simulation environments with the presence of environmental non-targeted variations, such as air turbidity and light condition changes, as well as targeted object changes in industrial indoor environments. By collecting data in simulations, multi-modal sensor data and precise ground truth labels are obtainable such as the RGB image, depth image, semantic segmentation, change segmentation, camera poses, and 3D reconstructions. While the previous online SCD datasets evaluate models given well-aligned image pairs, ChangeSim also provides raw unpaired sequences that present an opportunity to develop an online SCD model in an end-to-end manner, considering both pairing and detection. Experiments show that even the latest pair-based SCD models suffer from the bottleneck of the pairing process, and it gets worse when the environment contains the non-targeted variations. Our dataset is available at http://sammica.github.io/ChangeSim/.



### Pluggable Weakly-Supervised Cross-View Learning for Accurate Vehicle Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2103.05376v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.05376v1)
- **Published**: 2021-03-09 11:51:09+00:00
- **Updated**: 2021-03-09 11:51:09+00:00
- **Authors**: Lu Yang, Hongbang Liu, Jinghao Zhou, Lingqiao Liu, Lei Zhang, Peng Wang, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Learning cross-view consistent feature representation is the key for accurate vehicle Re-identification (ReID), since the visual appearance of vehicles changes significantly under different viewpoints. To this end, most existing approaches resort to the supervised cross-view learning using extensive extra viewpoints annotations, which however, is difficult to deploy in real applications due to the expensive labelling cost and the continous viewpoint variation that makes it hard to define discrete viewpoint labels. In this study, we present a pluggable Weakly-supervised Cross-View Learning (WCVL) module for vehicle ReID. Through hallucinating the cross-view samples as the hardest positive counterparts in feature domain, we can learn the consistent feature representation via minimizing the cross-view feature distance based on vehicle IDs only without using any viewpoint annotation. More importantly, the proposed method can be seamlessly plugged into most existing vehicle ReID baselines for cross-view learning without re-training the baselines. To demonstrate its efficacy, we plug the proposed method into a bunch of off-the-shelf baselines and obtain significant performance improvement on four public benchmark datasets, i.e., VeRi-776, VehicleID, VRIC and VRAI.



### NaroNet: Discovery of tumor microenvironment elements from highly multiplexed images
- **Arxiv ID**: http://arxiv.org/abs/2103.05385v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.QM, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2103.05385v2)
- **Published**: 2021-03-09 12:08:13+00:00
- **Updated**: 2021-03-25 14:43:15+00:00
- **Authors**: Daniel Jiménez-Sánchez, Mikel Ariz, Hang Chang, Xavier Matias-Guiu, Carlos E. de Andrea, Carlos Ortiz-de-Solórzano
- **Comment**: 37 pages, 4 figures
- **Journal**: None
- **Summary**: Many efforts have been made to discover tumor-specific microenvironment elements (TMEs) from immunostained tissue sections. However, the identification of yet unknown but relevant TMEs from multiplex immunostained tissues remains a challenge, due to the number of markers involved (tens) and the complexity of their spatial interactions. We present NaroNet, which uses machine learning to identify and annotate known as well as novel TMEs from self-supervised embeddings of cells, organized at different levels (local cell phenotypes and cellular neighborhoods). Then it uses the abundance of TMEs to classify patients based on biological or clinical features. We validate NaroNet using synthetic patient cohorts with adjustable incidence of different TMEs and two cancer patient datasets. In both synthetic and real datasets, NaroNet unsupervisedly identifies novel TMEs, relevant for the user-defined classification task. As NaroNet requires only patient-level information, it renders state-of-the-art computational methods accessible to a broad audience, accelerating the discovery of biomarker signatures.



### Instance and Pair-Aware Dynamic Networks for Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2103.05395v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05395v2)
- **Published**: 2021-03-09 12:34:41+00:00
- **Updated**: 2021-03-29 09:01:27+00:00
- **Authors**: Bingliang Jiao, Xin Tan, Jinghao Zhou, Lu Yang, Yunlong Wang, Peng Wang
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Re-identification (ReID) is to identify the same instance across different cameras. Existing ReID methods mostly utilize alignment-based or attention-based strategies to generate effective feature representations. However, most of these methods only extract general feature by employing single input image itself, overlooking the exploration of relevance between comparing images. To fill this gap, we propose a novel end-to-end trainable dynamic convolution framework named Instance and Pair-Aware Dynamic Networks in this paper. The proposed model is composed of three main branches where a self-guided dynamic branch is constructed to strengthen instance-specific features, focusing on every single image. Furthermore, we also design a mutual-guided dynamic branch to generate pair-aware features for each pair of images to be compared. Extensive experiments are conducted in order to verify the effectiveness of our proposed algorithm. We evaluate our algorithm in several mainstream person and vehicle ReID datasets including CUHK03, DukeMTMCreID, Market-1501, VeRi776 and VehicleID. In some datasets our algorithm outperforms state-of-the-art methods and in others, our algorithm achieves a comparable performance.



### QPIC: Query-Based Pairwise Human-Object Interaction Detection with Image-Wide Contextual Information
- **Arxiv ID**: http://arxiv.org/abs/2103.05399v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05399v1)
- **Published**: 2021-03-09 12:42:54+00:00
- **Updated**: 2021-03-09 12:42:54+00:00
- **Authors**: Masato Tamura, Hiroki Ohashi, Tomoaki Yoshinaga
- **Comment**: Accepted to CVPR2021
- **Journal**: None
- **Summary**: We propose a simple, intuitive yet powerful method for human-object interaction (HOI) detection. HOIs are so diverse in spatial distribution in an image that existing CNN-based methods face the following three major drawbacks; they cannot leverage image-wide features due to CNN's locality, they rely on a manually defined location-of-interest for the feature aggregation, which sometimes does not cover contextually important regions, and they cannot help but mix up the features for multiple HOI instances if they are located closely. To overcome these drawbacks, we propose a transformer-based feature extractor, in which an attention mechanism and query-based detection play key roles. The attention mechanism is effective in aggregating contextually important information image-wide, while the queries, which we design in such a way that each query captures at most one human-object pair, can avoid mixing up the features from multiple instances. This transformer-based feature extractor produces so effective embeddings that the subsequent detection heads may be fairly simple and intuitive. The extensive analysis reveals that the proposed method successfully extracts contextually important features, and thus outperforms existing methods by large margins (5.37 mAP on HICO-DET, and 5.7 mAP on V-COCO). The source codes are available at $\href{https://github.com/hitachi-rd-cv/qpic}{\text{this https URL}}$.



### Weather GAN: Multi-Domain Weather Translation Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2103.05422v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05422v1)
- **Published**: 2021-03-09 13:51:58+00:00
- **Updated**: 2021-03-09 13:51:58+00:00
- **Authors**: Xuelong Li, Kai Kou, Bin Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a new task is proposed, namely, weather translation, which refers to transferring weather conditions of the image from one category to another. It is important for photographic style transfer. Although lots of approaches have been proposed in traditional image translation tasks, few of them can handle the multi-category weather translation task, since weather conditions have rich categories and highly complex semantic structures. To address this problem, we develop a multi-domain weather translation approach based on generative adversarial networks (GAN), denoted as Weather GAN, which can achieve the transferring of weather conditions among sunny, cloudy, foggy, rainy and snowy. Specifically, the weather conditions in the image are determined by various weather-cues, such as cloud, blue sky, wet ground, etc. Therefore, it is essential for weather translation to focus the main attention on weather-cues. To this end, the generator of Weather GAN is composed of an initial translation module, an attention module and a weather-cue segmentation module. The initial translation module performs global translation during generation procedure. The weather-cue segmentation module identifies the structure and exact distribution of weather-cues. The attention module learns to focus on the interesting areas of the image while keeping other areas unaltered. The final generated result is synthesized by these three parts. This approach suppresses the distortion and deformation caused by weather translation. our approach outperforms the state-of-the-arts has been shown by a large number of experiments and evaluations.



### Deep Learning Based 3D Segmentation: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2103.05423v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05423v3)
- **Published**: 2021-03-09 13:58:35+00:00
- **Updated**: 2023-07-26 08:14:39+00:00
- **Authors**: Yong He, Hongshan Yu, Xiaoyan Liu, Zhengeng Yang, Wei Sun, Ajmal Mian
- **Comment**: 28 pages, 10 tables, 8 figures, update the transformer-based methods
  for 3D segmentation
- **Journal**: None
- **Summary**: 3D segmentation is a fundamental and challenging problem in computer vision with applications in autonomous driving, robotics, augmented reality and medical image analysis. It has received significant attention from the computer vision, graphics and machine learning communities. Conventional methods for 3D segmentation, based on hand-crafted features and machine learning classifiers, lack generalization ability. Driven by their success in 2D computer vision, deep learning techniques have recently become the tool of choice for 3D segmentation tasks. This has led to an influx of a large number of methods in the literature that have been evaluated on different benchmark datasets. Whereas survey papers on RGB-D and point cloud segmentation exist, there is a lack of an in-depth and recent survey that covers all 3D data modalities and application domains. This paper fills the gap and provides a comprehensive survey of the recent progress made in deep learning based 3D segmentation. It covers over 180 works, analyzes their strengths and limitations and discusses their competitive results on benchmark datasets. The survey provides a summary of the most commonly used pipelines and finally highlights promising research directions for the future.



### Pixel-wise Anomaly Detection in Complex Driving Scenes
- **Arxiv ID**: http://arxiv.org/abs/2103.05445v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05445v1)
- **Published**: 2021-03-09 14:26:20+00:00
- **Updated**: 2021-03-09 14:26:20+00:00
- **Authors**: Giancarlo Di Biase, Hermann Blum, Roland Siegwart, Cesar Cadena
- **Comment**: None
- **Journal**: None
- **Summary**: The inability of state-of-the-art semantic segmentation methods to detect anomaly instances hinders them from being deployed in safety-critical and complex applications, such as autonomous driving. Recent approaches have focused on either leveraging segmentation uncertainty to identify anomalous areas or re-synthesizing the image from the semantic label map to find dissimilarities with the input image. In this work, we demonstrate that these two methodologies contain complementary information and can be combined to produce robust predictions for anomaly segmentation. We present a pixel-wise anomaly detection framework that uses uncertainty maps to improve over existing re-synthesis methods in finding dissimilarities between the input and generated images. Our approach works as a general framework around already trained segmentation networks, which ensures anomaly detection without compromising segmentation accuracy, while significantly outperforming all similar methods. Top-2 performance across a range of different anomaly datasets shows the robustness of our approach to handling different anomaly instances.



### Learning Class-Agnostic Pseudo Mask Generation for Box-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.05463v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05463v2)
- **Published**: 2021-03-09 14:54:54+00:00
- **Updated**: 2022-03-02 13:11:59+00:00
- **Authors**: Chaohao Xie, Dongwei Ren, Lei Wang, Wangmeng Zuo
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: Recently, several weakly supervised learning methods have been devoted to utilize bounding box supervision for training deep semantic segmentation models. Most existing methods usually leverage the generic proposal generators (e.g., dense CRF and MCG) to produce enhanced segmentation masks for further training segmentation models. These proposal generators, however, are generic and not specifically designed for box-supervised semantic segmentation, thereby leaving some leeway for improving segmentation performance. In this paper, we aim at seeking for a more accurate learning-based class-agnostic pseudo mask generator tailored to box-supervised semantic segmentation. To this end, we resort to a pixel-level annotated auxiliary dataset where the class labels are non-overlapped with those of the box-annotated dataset. For learning pseudo mask generator from the auxiliary dataset, we present a bi-level optimization formulation. In particular, the lower subproblem is used to learn box-supervised semantic segmentation, while the upper subproblem is used to learn an optimal class-agnostic pseudo mask generator. The learned pseudo segmentation mask generator can then be deployed to the box-annotated dataset for improving weakly supervised semantic segmentation. Experiments on PASCAL VOC 2012 dataset show that the learned pseudo mask generator is effective in boosting segmentation performance, and our method can further close the performance gap between box-supervised and fully-supervised models. Our code will be made publicly available at https://github.com/Vious/LPG_BBox_Segmentation .



### PointDSC: Robust Point Cloud Registration using Deep Spatial Consistency
- **Arxiv ID**: http://arxiv.org/abs/2103.05465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05465v1)
- **Published**: 2021-03-09 14:56:08+00:00
- **Updated**: 2021-03-09 14:56:08+00:00
- **Authors**: Xuyang Bai, Zixin Luo, Lei Zhou, Hongkai Chen, Lei Li, Zeyu Hu, Hongbo Fu, Chiew-Lan Tai
- **Comment**: Accepted to CVPR 2021, supplementary materials included
- **Journal**: None
- **Summary**: Removing outlier correspondences is one of the critical steps for successful feature-based point cloud registration. Despite the increasing popularity of introducing deep learning methods in this field, spatial consistency, which is essentially established by a Euclidean transformation between point clouds, has received almost no individual attention in existing learning frameworks. In this paper, we present PointDSC, a novel deep neural network that explicitly incorporates spatial consistency for pruning outlier correspondences. First, we propose a nonlocal feature aggregation module, weighted by both feature and spatial coherence, for feature embedding of the input correspondences. Second, we formulate a differentiable spectral matching module, supervised by pairwise spatial compatibility, to estimate the inlier confidence of each correspondence from the embedded features. With modest computation cost, our method outperforms the state-of-the-art hand-crafted and learning-based outlier rejection approaches on several real-world datasets by a significant margin. We also show its wide applicability by combining PointDSC with different 3D local descriptors.



### Doubly Contrastive Deep Clustering
- **Arxiv ID**: http://arxiv.org/abs/2103.05484v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05484v1)
- **Published**: 2021-03-09 15:15:32+00:00
- **Updated**: 2021-03-09 15:15:32+00:00
- **Authors**: Zhiyuan Dang, Cheng Deng, Xu Yang, Heng Huang
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Deep clustering successfully provides more effective features than conventional ones and thus becomes an important technique in current unsupervised learning. However, most deep clustering methods ignore the vital positive and negative pairs introduced by data augmentation and further the significance of contrastive learning, which leads to suboptimal performance. In this paper, we present a novel Doubly Contrastive Deep Clustering (DCDC) framework, which constructs contrastive loss over both sample and class views to obtain more discriminative features and competitive results. Specifically, for the sample view, we set the class distribution of the original sample and its augmented version as positive sample pairs and set one of the other augmented samples as negative sample pairs. After that, we can adopt the sample-wise contrastive loss to pull positive sample pairs together and push negative sample pairs apart. Similarly, for the class view, we build the positive and negative pairs from the sample distribution of the class. In this way, two contrastive losses successfully constrain the clustering results of mini-batch samples in both sample and class level. Extensive experimental results on six benchmark datasets demonstrate the superiority of our proposed model against state-of-the-art methods. Particularly in the challenging dataset Tiny-ImageNet, our method leads 5.6\% against the latest comparison method. Our code will be available at \url{https://github.com/ZhiyuanDang/DCDC}.



### TS-Net: OCR Trained to Switch Between Text Transcription Styles
- **Arxiv ID**: http://arxiv.org/abs/2103.05489v2
- **DOI**: 10.1007/978-3-030-86337-1_32
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05489v2)
- **Published**: 2021-03-09 15:21:40+00:00
- **Updated**: 2023-02-13 13:26:41+00:00
- **Authors**: Jan Kohút, Michal Hradiš
- **Comment**: None
- **Journal**: ICDAR 2021: Proceedings, Part IV 16 (pp. 478-493)
- **Summary**: Users of OCR systems, from different institutions and scientific disciplines, prefer and produce different transcription styles. This presents a problem for training of consistent text recognition neural networks on real-world data. We propose to extend existing text recognition networks with a Transcription Style Block (TSB) which can learn from data to switch between multiple transcription styles without any explicit knowledge of transcription rules. TSB is an adaptive instance normalization conditioned by identifiers representing consistently transcribed documents (e.g. single document, documents by a single transcriber, or an institution). We show that TSB is able to learn completely different transcription styles in controlled experiments on artificial data, it improves text recognition accuracy on large-scale real-world data, and it learns semantically meaningful transcription style embedding. We also show how TSB can efficiently adapt to transcription styles of new documents from transcriptions of only a few text lines.



### Deep and Statistical Learning in Biomedical Imaging: State of the Art in 3D MRI Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.05529v2
- **DOI**: 10.1016/j.inffus.2022.12.013
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.05529v2)
- **Published**: 2021-03-09 16:15:47+00:00
- **Updated**: 2022-12-16 19:07:45+00:00
- **Authors**: K. Ruwani M. Fernando, Chris P. Tsokos
- **Comment**: 21 pages, 7 figures
- **Journal**: Information Fusion, Volume 92, 2023, Pages 450-465
- **Summary**: Clinical diagnostic and treatment decisions rely upon the integration of patient-specific data with clinical reasoning. Cancer presents a unique context that influence treatment decisions, given its diverse forms of disease evolution. Biomedical imaging allows noninvasive assessment of disease based on visual evaluations leading to better clinical outcome prediction and therapeutic planning. Early methods of brain cancer characterization predominantly relied upon statistical modeling of neuroimaging data. Driven by the breakthroughs in computer vision, deep learning became the de facto standard in the domain of medical imaging. Integrated statistical and deep learning methods have recently emerged as a new direction in the automation of the medical practice unifying multi-disciplinary knowledge in medicine, statistics, and artificial intelligence. In this study, we critically review major statistical and deep learning models and their applications in brain imaging research with a focus on MRI-based brain tumor segmentation. The results do highlight that model-driven classical statistics and data-driven deep learning is a potent combination for developing automated systems in clinical oncology.



### Quadruple Augmented Pyramid Network for Multi-class COVID-19 Segmentation via CT
- **Arxiv ID**: http://arxiv.org/abs/2103.05546v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.05546v2)
- **Published**: 2021-03-09 16:48:15+00:00
- **Updated**: 2021-09-21 15:14:13+00:00
- **Authors**: Ziyang Wang, Irina Voiculescu
- **Comment**: None
- **Journal**: None
- **Summary**: COVID-19, a new strain of coronavirus disease, has been one of the most serious and infectious disease in the world. Chest CT is essential in prognostication, diagnosing this disease, and assessing the complication. In this paper, a multi-class COVID-19 CT segmentation is proposed aiming at helping radiologists estimate the extent of effected lung volume. We utilized four augmented pyramid networks on an encoder-decoder segmentation framework. Quadruple Augmented Pyramid Network (QAP-Net) not only enable CNN capture features from variation size of CT images, but also act as spatial interconnections and down-sampling to transfer sufficient feature information for semantic segmentation. Experimental results achieve competitive performance in segmentation with the Dice of 0.8163, which outperforms other state-of-the-art methods, demonstrating the proposed framework can segments of consolidation as well as glass, ground area via COVID-19 chest CT efficiently and accurately.



### Exploiting Edge-Oriented Reasoning for 3D Point-based Scene Graph Analysis
- **Arxiv ID**: http://arxiv.org/abs/2103.05558v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05558v2)
- **Published**: 2021-03-09 17:09:46+00:00
- **Updated**: 2021-03-31 09:18:51+00:00
- **Authors**: Chaoyi Zhang, Jianhui Yu, Yang Song, Weidong Cai
- **Comment**: CVPR2021
- **Journal**: None
- **Summary**: Scene understanding is a critical problem in computer vision. In this paper, we propose a 3D point-based scene graph generation ($\mathbf{SGG_{point}}$) framework to effectively bridge perception and reasoning to achieve scene understanding via three sequential stages, namely scene graph construction, reasoning, and inference. Within the reasoning stage, an EDGE-oriented Graph Convolutional Network ($\texttt{EdgeGCN}$) is created to exploit multi-dimensional edge features for explicit relationship modeling, together with the exploration of two associated twinning interaction mechanisms between nodes and edges for the independent evolution of scene graph representations. Overall, our integrated $\mathbf{SGG_{point}}$ framework is established to seek and infer scene structures of interest from both real-world and synthetic 3D point-based scenes. Our experimental results show promising edge-oriented reasoning effects on scene graph generation studies. We also demonstrate our method advantage on several traditional graph representation learning benchmark datasets, including the node-wise classification on citation networks and whole-graph recognition problems for molecular analysis.



### Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2103.05568v3
- **DOI**: 10.1145/3404835.3463259
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05568v3)
- **Published**: 2021-03-09 17:19:50+00:00
- **Updated**: 2021-08-10 14:13:11+00:00
- **Authors**: Aman Jain, Mayank Kothyari, Vishwajeet Kumar, Preethi Jyothi, Ganesh Ramakrishnan, Soumen Chakrabarti
- **Comment**: Accepted at SIGIR 2021
- **Journal**: None
- **Summary**: Multimodal IR, spanning text corpus, knowledge graph and images, called outside knowledge visual question answering (OKVQA), is of much recent interest. However, the popular data set has serious limitations. A surprisingly large fraction of queries do not assess the ability to integrate cross-modal information. Instead, some are independent of the image, some depend on speculation, some require OCR or are otherwise answerable from the image alone. To add to the above limitations, frequency-based guessing is very effective because of (unintended) widespread answer overlaps between the train and test folds. Overall, it is hard to determine when state-of-the-art systems exploit these weaknesses rather than really infer the answers, because they are opaque and their 'reasoning' process is uninterpretable. An equally important limitation is that the dataset is designed for the quantitative assessment only of the end-to-end answer retrieval task, with no provision for assessing the correct(semantic) interpretation of the input query. In response, we identify a key structural idiom in OKVQA ,viz., S3 (select, substitute and search), and build a new data set and challenge around it. Specifically, the questioner identifies an entity in the image and asks a question involving that entity which can be answered only by consulting a knowledge graph or corpus passage mentioning the entity. Our challenge consists of (i)OKVQAS3, a subset of OKVQA annotated based on the structural idiom and (ii)S3VQA, a new dataset built from scratch. We also present a neural but structurally transparent OKVQA system, S3, that explicitly addresses our challenge dataset, and outperforms recent competitive baselines.



### FAIR1M: A Benchmark Dataset for Fine-grained Object Recognition in High-Resolution Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/2103.05569v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05569v2)
- **Published**: 2021-03-09 17:20:15+00:00
- **Updated**: 2021-03-24 18:05:34+00:00
- **Authors**: Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping Wang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng, Tao Xu, Martin Weinmann, Stefan Hinz, Cheng Wang, Kun Fu
- **Comment**: 19 pages, 13 figures
- **Journal**: None
- **Summary**: With the rapid development of deep learning, many deep learning-based approaches have made great achievements in object detection task. It is generally known that deep learning is a data-driven method. Data directly impact the performance of object detectors to some extent. Although existing datasets have included common objects in remote sensing images, they still have some limitations in terms of scale, categories, and images. Therefore, there is a strong requirement for establishing a large-scale benchmark on object detection in high-resolution remote sensing images. In this paper, we propose a novel benchmark dataset with more than 1 million instances and more than 15,000 images for Fine-grAined object recognItion in high-Resolution remote sensing imagery which is named as FAIR1M. All objects in the FAIR1M dataset are annotated with respect to 5 categories and 37 sub-categories by oriented bounding boxes. Compared with existing detection datasets dedicated to object detection, the FAIR1M dataset has 4 particular characteristics: (1) it is much larger than other existing object detection datasets both in terms of the quantity of instances and the quantity of images, (2) it provides more rich fine-grained category information for objects in remote sensing images, (3) it contains geographic information such as latitude, longitude and resolution, (4) it provides better image quality owing to a careful data cleaning procedure. To establish a baseline for fine-grained object recognition, we propose a novel evaluation method and benchmark fine-grained object detection tasks and a visual classification task using several State-Of-The-Art (SOTA) deep learning-based models on our FAIR1M dataset. Experimental results strongly indicate that the FAIR1M dataset is closer to practical application and it is considerably more challenging than existing datasets.



### SimTriplet: Simple Triplet Representation Learning with a Single GPU
- **Arxiv ID**: http://arxiv.org/abs/2103.05585v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05585v1)
- **Published**: 2021-03-09 17:46:09+00:00
- **Updated**: 2021-03-09 17:46:09+00:00
- **Authors**: Quan Liu, Peter C. Louis, Yuzhe Lu, Aadarsh Jha, Mengyang Zhao, Ruining Deng, Tianyuan Yao, Joseph T. Roland, Haichun Yang, Shilin Zhao, Lee E. Wheless, Yuankai Huo
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive learning is a key technique of modern self-supervised learning. The broader accessibility of earlier approaches is hindered by the need of heavy computational resources (e.g., at least 8 GPUs or 32 TPU cores), which accommodate for large-scale negative samples or momentum. The more recent SimSiam approach addresses such key limitations via stop-gradient without momentum encoders. In medical image analysis, multiple instances can be achieved from the same patient or tissue. Inspired by these advances, we propose a simple triplet representation learning (SimTriplet) approach on pathological images. The contribution of the paper is three-fold: (1) The proposed SimTriplet method takes advantage of the multi-view nature of medical images beyond self-augmentation; (2) The method maximizes both intra-sample and inter-sample similarities via triplets from positive pairs, without using negative samples; and (3) The recent mix precision training is employed to advance the training by only using a single GPU with 16GB memory. By learning from 79,000 unlabeled pathological patch images, SimTriplet achieved 10.58% better performance compared with supervised learning. It also achieved 2.13% better performance compared with SimSiam. Our proposed SimTriplet can achieve decent performance using only 1% labeled data. The code and data are available at https://github.com/hrlblab/SimTriple.



### unzipFPGA: Enhancing FPGA-based CNN Engines with On-the-Fly Weights Generation
- **Arxiv ID**: http://arxiv.org/abs/2103.05600v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05600v2)
- **Published**: 2021-03-09 18:19:41+00:00
- **Updated**: 2021-04-03 14:15:01+00:00
- **Authors**: Stylianos I. Venieris, Javier Fernandez-Marques, Nicholas D. Lane
- **Comment**: Accepted at the 29th IEEE International Symposium on
  Field-Programmable Custom Computing Machines (FCCM) 2021
- **Journal**: None
- **Summary**: Single computation engines have become a popular design choice for FPGA-based convolutional neural networks (CNNs) enabling the deployment of diverse models without fabric reconfiguration. This flexibility, however, often comes with significantly reduced performance on memory-bound layers and resource underutilisation due to suboptimal mapping of certain layers on the engine's fixed configuration. In this work, we investigate the implications in terms of CNN engine design for a class of models that introduce a pre-convolution stage to decompress the weights at run time. We refer to these approaches as on-the-fly. To minimise the negative impact of limited bandwidth on memory-bound layers, we present a novel hardware component that enables the on-chip on-the-fly generation of weights. We further introduce an input selective processing element (PE) design that balances the load between PEs on suboptimally mapped layers. Finally, we present unzipFPGA, a framework to train on-the-fly models and traverse the design space to select the highest performing CNN engine configuration. Quantitative evaluation shows that unzipFPGA yields an average speedup of 2.14x and 71% over optimised status-quo and pruned CNN engines under constrained bandwidth and up to 3.69x higher performance density over the state-of-the-art FPGA-based CNN accelerators.



### NeX: Real-time View Synthesis with Neural Basis Expansion
- **Arxiv ID**: http://arxiv.org/abs/2103.05606v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05606v2)
- **Published**: 2021-03-09 18:27:27+00:00
- **Updated**: 2021-04-12 09:40:00+00:00
- **Authors**: Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, Supasorn Suwajanakorn
- **Comment**: CVPR 2021 (Oral)
- **Journal**: None
- **Summary**: We present NeX, a new approach to novel view synthesis based on enhancements of multiplane image (MPI) that can reproduce next-level view-dependent effects -- in real time. Unlike traditional MPI that uses a set of simple RGB$\alpha$ planes, our technique models view-dependent effects by instead parameterizing each pixel as a linear combination of basis functions learned from a neural network. Moreover, we propose a hybrid implicit-explicit modeling strategy that improves upon fine detail and produces state-of-the-art results. Our method is evaluated on benchmark forward-facing datasets as well as our newly-introduced dataset designed to test the limit of view-dependent modeling with significantly more challenging effects such as rainbow reflections on a CD. Our method achieves the best overall scores across all major metrics on these datasets with more than 1000$\times$ faster rendering time than the state of the art. For real-time demos, visit https://nex-mpi.github.io/



### Point-supervised Segmentation of Microscopy Images and Volumes via Objectness Regularization
- **Arxiv ID**: http://arxiv.org/abs/2103.05617v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05617v2)
- **Published**: 2021-03-09 18:40:00+00:00
- **Updated**: 2021-03-19 01:39:18+00:00
- **Authors**: Shijie Li, Neel Dey, Katharina Bermond, Leon von der Emde, Christine A. Curcio, Thomas Ach, Guido Gerig
- **Comment**: Accepted to IEEE ISBI 2021. Code available at
  https://github.com/CJLee94/Point-Supervised-Segmentation
- **Journal**: None
- **Summary**: Annotation is a major hurdle in the semantic segmentation of microscopy images and volumes due to its prerequisite expertise and effort. This work enables the training of semantic segmentation networks on images with only a single point for training per instance, an extreme case of weak supervision which drastically reduces the burden of annotation. Our approach has two key aspects: (1) we construct a graph-theoretic soft-segmentation using individual seeds to be used within a regularizer during training and (2) we use an objective function that enables learning from the constructed soft-labels. We achieve competitive results against the state-of-the-art in point-supervised semantic segmentation on challenging datasets in digital pathology. Finally, we scale our methodology to point-supervised segmentation in 3D fluorescence microscopy volumes, obviating the need for arduous manual volumetric delineation. Our code is freely available.



### ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis
- **Arxiv ID**: http://arxiv.org/abs/2103.05630v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05630v2)
- **Published**: 2021-03-09 18:58:32+00:00
- **Updated**: 2021-07-14 06:26:05+00:00
- **Authors**: Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun Yin, Luchuan Song, Lu Sheng, Jing Shao, Ziwei Liu
- **Comment**: 17 pages, 11 figures, Accepted to CVPR 2021 (Oral), project webpage:
  https://yinanhe.github.io/projects/forgerynet.html
- **Journal**: None
- **Summary**: The rapid progress of photorealistic synthesis techniques has reached at a critical point where the boundary between real and manipulated images starts to blur. Thus, benchmarking and advancing digital forgery analysis have become a pressing issue. However, existing face forgery datasets either have limited diversity or only support coarse-grained analysis. To counter this emerging threat, we construct the ForgeryNet dataset, an extremely large face forgery dataset with unified annotations in image- and video-level data across four tasks: 1) Image Forgery Classification, including two-way (real / fake), three-way (real / fake with identity-replaced forgery approaches / fake with identity-remained forgery approaches), and n-way (real and 15 respective forgery approaches) classification. 2) Spatial Forgery Localization, which segments the manipulated area of fake images compared to their corresponding source real images. 3) Video Forgery Classification, which re-defines the video-level forgery classification with manipulated frames in random positions. This task is important because attackers in real world are free to manipulate any target frame. and 4) Temporal Forgery Localization, to localize the temporal segments which are manipulated. ForgeryNet is by far the largest publicly available deep face forgery dataset in terms of data-scale (2.9 million images, 221,247 videos), manipulations (7 image-level approaches, 8 video-level approaches), perturbations (36 independent and more mixed perturbations) and annotations (6.3 million classification labels, 2.9 million manipulated area annotations and 221,247 temporal forgery segment labels). We perform extensive benchmarking and studies of existing face forensics methods and obtain several valuable observations.



### Self-Supervision by Prediction for Object Discovery in Videos
- **Arxiv ID**: http://arxiv.org/abs/2103.05669v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05669v1)
- **Published**: 2021-03-09 19:14:33+00:00
- **Updated**: 2021-03-09 19:14:33+00:00
- **Authors**: Beril Besbinar, Pascal Frossard
- **Comment**: None
- **Journal**: None
- **Summary**: Despite their irresistible success, deep learning algorithms still heavily rely on annotated data. On the other hand, unsupervised settings pose many challenges, especially about determining the right inductive bias in diverse scenarios. One scalable solution is to make the model generate the supervision for itself by leveraging some part of the input data, which is known as self-supervised learning. In this paper, we use the prediction task as self-supervision and build a novel object-centric model for image sequence representation. In addition to disentangling the notion of objects and the motion dynamics, our compositional structure explicitly handles occlusion and inpaints inferred objects and background for the composition of the predicted frame. With the aid of auxiliary loss functions that promote spatially and temporally consistent object representations, our self-supervised framework can be trained without the help of any manual annotation or pretrained network. Initial experiments confirm that the proposed pipeline is a promising step towards object-centric video prediction.



### SMIL: Multimodal Learning with Severely Missing Modality
- **Arxiv ID**: http://arxiv.org/abs/2103.05677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05677v1)
- **Published**: 2021-03-09 19:27:08+00:00
- **Updated**: 2021-03-09 19:27:08+00:00
- **Authors**: Mengmeng Ma, Jian Ren, Long Zhao, Sergey Tulyakov, Cathy Wu, Xi Peng
- **Comment**: In AAAI 2021 (9 pages)
- **Journal**: None
- **Summary**: A common assumption in multimodal learning is the completeness of training data, i.e., full modalities are available in all training examples. Although there exists research endeavor in developing novel methods to tackle the incompleteness of testing data, e.g., modalities are partially missing in testing examples, few of them can handle incomplete training modalities. The problem becomes even more challenging if considering the case of severely missing, e.g., 90% training examples may have incomplete modalities. For the first time in the literature, this paper formally studies multimodal learning with missing modality in terms of flexibility (missing modalities in training, testing, or both) and efficiency (most training data have incomplete modality). Technically, we propose a new method named SMIL that leverages Bayesian meta-learning in uniformly achieving both objectives. To validate our idea, we conduct a series of experiments on three popular benchmarks: MM-IMDb, CMU-MOSI, and avMNIST. The results prove the state-of-the-art performance of SMIL over existing methods and generative baselines including autoencoders and generative adversarial networks. Our code is available at https://github.com/mengmenm/SMIL.



### Capturing Omni-Range Context for Omnidirectional Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.05687v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.05687v1)
- **Published**: 2021-03-09 19:46:09+00:00
- **Updated**: 2021-03-09 19:46:09+00:00
- **Authors**: Kailun Yang, Jiaming Zhang, Simon Reiß, Xinxin Hu, Rainer Stiefelhagen
- **Comment**: Accepted to CVPR2021
- **Journal**: None
- **Summary**: Convolutional Networks (ConvNets) excel at semantic segmentation and have become a vital component for perception in autonomous driving. Enabling an all-encompassing view of street-scenes, omnidirectional cameras present themselves as a perfect fit in such systems. Most segmentation models for parsing urban environments operate on common, narrow Field of View (FoV) images. Transferring these models from the domain they were designed for to 360-degree perception, their performance drops dramatically, e.g., by an absolute 30.0% (mIoU) on established test-beds. To bridge the gap in terms of FoV and structural distribution between the imaging domains, we introduce Efficient Concurrent Attention Networks (ECANets), directly capturing the inherent long-range dependencies in omnidirectional imagery. In addition to the learned attention-based contextual priors that can stretch across 360-degree images, we upgrade model training by leveraging multi-source and omni-supervised learning, taking advantage of both: Densely labeled and unlabeled data originating from multiple datasets. To foster progress in panoramic image segmentation, we put forward and extensively evaluate models on Wild PAnoramic Semantic Segmentation (WildPASS), a dataset designed to capture diverse scenes from all around the globe. Our novel model, training regimen and multi-source prediction fusion elevate the performance (mIoU) to new state-of-the-art results on the public PASS (60.2%) and the fresh WildPASS (69.0%) benchmarks.



### Multitask 3D CBCT-to-CT Translation and Organs-at-Risk Segmentation Using Physics-Based Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.05690v4
- **DOI**: 10.1002/mp.15083
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.05690v4)
- **Published**: 2021-03-09 19:51:44+00:00
- **Updated**: 2021-08-31 02:37:56+00:00
- **Authors**: Navdeep Dahiya, Sadegh R Alam, Pengpeng Zhang, Si-Yuan Zhang, Anthony Yezzi, Saad Nadeem
- **Comment**: Medical Physics 2021
- **Journal**: None
- **Summary**: In current clinical practice, noisy and artifact-ridden weekly cone-beam computed tomography (CBCT) images are only used for patient setup during radiotherapy. Treatment planning is done once at the beginning of the treatment using high-quality planning CT (pCT) images and manual contours for organs-at-risk (OARs) structures. If the quality of the weekly CBCT images can be improved while simultaneously segmenting OAR structures, this can provide critical information for adapting radiotherapy mid-treatment as well as for deriving biomarkers for treatment response. Using a novel physics-based data augmentation strategy, we synthesize a large dataset of perfectly/inherently registered planning CT and synthetic-CBCT pairs for locally advanced lung cancer patient cohort, which are then used in a multitask 3D deep learning framework to simultaneously segment and translate real weekly CBCT images to high-quality planning CT-like images. We compared the synthetic CT and OAR segmentations generated by the model to real planning CT and manual OAR segmentations and showed promising results. The real week 1 (baseline) CBCT images which had an average MAE of 162.77 HU compared to pCT images are translated to synthetic CT images that exhibit a drastically improved average MAE of 29.31 HU and average structural similarity of 92% with the pCT images. The average DICE scores of the 3D organs-at-risk segmentations are: lungs 0.96, heart 0.88, spinal cord 0.83 and esophagus 0.66. This approach could allow clinicians to adjust treatment plans using only the routine low-quality CBCT images, potentially improving patient outcomes. Our code, data, and pre-trained models will be made available via our physics-based data augmentation library, Physics-ArX, at https://github.com/nadeemlab/Physics-ArX.



### Pixel-wise Distance Regression for Glacier Calving Front Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.05715v1
- **DOI**: 10.1109/TGRS.2022.3158591
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05715v1)
- **Published**: 2021-03-09 20:58:33+00:00
- **Updated**: 2021-03-09 20:58:33+00:00
- **Authors**: Amirabbas Davari, Christoph Baller, Thorsten Seehaus, Matthias Braun, Andreas Maier, Vincent Christlein
- **Comment**: None
- **Journal**: None
- **Summary**: Glacier calving front position (CFP) is an important glaciological variable. Traditionally, delineating the CFPs has been carried out manually, which was subjective, tedious and expensive. Automating this process is crucial for continuously monitoring the evolution and status of glaciers. Recently, deep learning approaches have been investigated for this application. However, the current methods get challenged by a severe class-imbalance problem. In this work, we propose to mitigate the class-imbalance between the calving front class and the non-calving front class by reformulating the segmentation problem into a pixel-wise regression task. A Convolutional Neural Network gets optimized to predict the distance values to the glacier front for each pixel in the image. The resulting distance map localizes the CFP and is further post-processed to extract the calving front line. We propose three post-processing methods, one method based on statistical thresholding, a second method based on conditional random fields (CRF), and finally the use of a second U-Net. The experimental results confirm that our approach significantly outperforms the state-of-the-art methods and produces accurate delineation. The Second U-Net obtains the best performance results, resulting in an average improvement of about 21% dice coefficient enhancement.



### A Multi-resolution Approach to Expression Recognition in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2103.05723v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.05723v1)
- **Published**: 2021-03-09 21:21:02+00:00
- **Updated**: 2021-03-09 21:21:02+00:00
- **Authors**: Fabio Valerio Massoli, Donato Cafarelli, Giuseppe Amato, Fabrizio Falchi
- **Comment**: None
- **Journal**: None
- **Summary**: Facial expressions play a fundamental role in human communication. Indeed, they typically reveal the real emotional status of people beyond the spoken language. Moreover, the comprehension of human affect based on visual patterns is a key ingredient for any human-machine interaction system and, for such reasons, the task of Facial Expression Recognition (FER) draws both scientific and industrial interest. In the recent years, Deep Learning techniques reached very high performance on FER by exploiting different architectures and learning paradigms. In such a context, we propose a multi-resolution approach to solve the FER task. We ground our intuition on the observation that often faces images are acquired at different resolutions. Thus, directly considering such property while training a model can help achieve higher performance on recognizing facial expressions. To our aim, we use a ResNet-like architecture, equipped with Squeeze-and-Excitation blocks, trained on the Affect-in-the-Wild 2 dataset. Not being available a test set, we conduct tests and models selection by employing the validation set only on which we achieve more than 90\% accuracy on classifying the seven expressions that the dataset comprises.



### Structural Connectome Atlas Construction in the Space of Riemannian Metrics
- **Arxiv ID**: http://arxiv.org/abs/2103.05730v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05730v1)
- **Published**: 2021-03-09 21:46:02+00:00
- **Updated**: 2021-03-09 21:46:02+00:00
- **Authors**: Kristen M. Campbell, Haocheng Dai, Zhe Su, Martin Bauer, P. Thomas Fletcher, Sarang C. Joshi
- **Comment**: 12 pages, 3 figures
- **Journal**: None
- **Summary**: The structural connectome is often represented by fiber bundles generated from various types of tractography. We propose a method of analyzing connectomes by representing them as a Riemannian metric, thereby viewing them as points in an infinite-dimensional manifold. After equipping this space with a natural metric structure, the Ebin metric, we apply object-oriented statistical analysis to define an atlas as the Fr\'echet mean of a population of Riemannian metrics. We demonstrate connectome registration and atlas formation using connectomes derived from diffusion tensors estimated from a subset of subjects from the Human Connectome Project.



### Content-Preserving Unpaired Translation from Simulated to Realistic Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/2103.05745v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05745v2)
- **Published**: 2021-03-09 22:35:43+00:00
- **Updated**: 2021-09-30 18:54:17+00:00
- **Authors**: Devavrat Tomar, Lin Zhang, Tiziano Portenier, Orcun Goksel
- **Comment**: None
- **Journal**: None
- **Summary**: Interactive simulation of ultrasound imaging greatly facilitates sonography training. Although ray-tracing based methods have shown promising results, obtaining realistic images requires substantial modeling effort and manual parameter tuning. In addition, current techniques still result in a significant appearance gap between simulated images and real clinical scans. Herein we introduce a novel content-preserving image translation framework (ConPres) to bridge this appearance gap, while maintaining the simulated anatomical layout. We achieve this goal by leveraging both simulated images with semantic segmentations and unpaired in-vivo ultrasound scans. Our framework is based on recent contrastive unpaired translation techniques and we propose a regularization approach by learning an auxiliary segmentation-to-real image translation task, which encourages the disentanglement of content and style. In addition, we extend the generator to be class-conditional, which enables the incorporation of additional losses, in particular a cyclic consistency loss, to further improve the translation quality. Qualitative and quantitative comparisons against state-of-the-art unpaired translation methods demonstrate the superiority of our proposed framework.



