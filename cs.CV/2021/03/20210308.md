# Arxiv Papers in cs.CV on 2021-03-08
### Localization and Mapping using Instance-specific Mesh Models
- **Arxiv ID**: http://arxiv.org/abs/2103.04493v1
- **DOI**: 10.1109/IROS40897.2019.8967662
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.04493v1)
- **Published**: 2021-03-08 00:24:23+00:00
- **Updated**: 2021-03-08 00:24:23+00:00
- **Authors**: Qiaojun Feng, Yue Meng, Mo Shan, Nikolay Atanasov
- **Comment**: 8 pages, 9 figures
- **Journal**: 2019 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), Macau, China, 2019, pp. 4985-4991
- **Summary**: This paper focuses on building semantic maps, containing object poses and shapes, using a monocular camera. This is an important problem because robots need rich understanding of geometry and context if they are to shape the future of transportation, construction, and agriculture. Our contribution is an instance-specific mesh model of object shape that can be optimized online based on semantic information extracted from camera images. Multi-view constraints on the object shape are obtained by detecting objects and extracting category-specific keypoints and segmentation masks. We show that the errors between projections of the mesh model and the observed keypoints and masks can be differentiated in order to obtain accurate instance-specific object shapes. We evaluate the performance of the proposed approach in simulation and on the KITTI dataset by building maps of car poses and shapes.



### Fully Convolutional Geometric Features for Category-level Object Alignment
- **Arxiv ID**: http://arxiv.org/abs/2103.04494v1
- **DOI**: 10.1109/IROS45743.2020.9341550
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.04494v1)
- **Published**: 2021-03-08 00:31:56+00:00
- **Updated**: 2021-03-08 00:31:56+00:00
- **Authors**: Qiaojun Feng, Nikolay Atanasov
- **Comment**: 7 pages, 9 figures
- **Journal**: 2020 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), Las Vegas, NV, USA, 2020, pp. 8492-8498
- **Summary**: This paper focuses on pose registration of different object instances from the same category. This is required in online object mapping because object instances detected at test time usually differ from the training instances. Our approach transforms instances of the same category to a normalized canonical coordinate frame and uses metric learning to train fully convolutional geometric features. The resulting model is able to generate pairs of matching points between the instances, allowing category-level registration. Evaluation on both synthetic and real-world data shows that our method provides robust features, leading to accurate alignment of instances with different shapes.



### End-to-End Human Object Interaction Detection with HOI Transformer
- **Arxiv ID**: http://arxiv.org/abs/2103.04503v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04503v1)
- **Published**: 2021-03-08 01:31:19+00:00
- **Updated**: 2021-03-08 01:31:19+00:00
- **Authors**: Cheng Zou, Bohan Wang, Yue Hu, Junqi Liu, Qian Wu, Yu Zhao, Boxun Li, Chenguang Zhang, Chi Zhang, Yichen Wei, Jian Sun
- **Comment**: Accepted to CVPR2021
- **Journal**: None
- **Summary**: We propose HOI Transformer to tackle human object interaction (HOI) detection in an end-to-end manner. Current approaches either decouple HOI task into separated stages of object detection and interaction classification or introduce surrogate interaction problem. In contrast, our method, named HOI Transformer, streamlines the HOI pipeline by eliminating the need for many hand-designed components. HOI Transformer reasons about the relations of objects and humans from global image context and directly predicts HOI instances in parallel. A quintuple matching loss is introduced to force HOI predictions in a unified way. Our method is conceptually much simpler and demonstrates improved accuracy. Without bells and whistles, HOI Transformer achieves $26.61\% $ $ AP $ on HICO-DET and $52.9\%$ $AP_{role}$ on V-COCO, surpassing previous methods with the advantage of being much simpler. We hope our approach will serve as a simple and effective alternative for HOI tasks. Code is available at https://github.com/bbepoch/HoiTransformer .



### OPANAS: One-Shot Path Aggregation Network Architecture Search for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.04507v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04507v3)
- **Published**: 2021-03-08 01:48:53+00:00
- **Updated**: 2021-03-11 05:17:08+00:00
- **Authors**: Tingting Liang, Yongtao Wang, Zhi Tang, Guosheng Hu, Haibin Ling
- **Comment**: To appear in CVPR 2021
- **Journal**: None
- **Summary**: Recently, neural architecture search (NAS) has been exploited to design feature pyramid networks (FPNs) and achieved promising results for visual object detection. Encouraged by the success, we propose a novel One-Shot Path Aggregation Network Architecture Search (OPANAS) algorithm, which significantly improves both searching efficiency and detection accuracy. Specifically, we first introduce six heterogeneous information paths to build our search space, namely top-down, bottom-up, fusing-splitting, scale-equalizing, skip-connect and none. Second, we propose a novel search space of FPNs, in which each FPN candidate is represented by a densely-connected directed acyclic graph (each node is a feature pyramid and each edge is one of the six heterogeneous information paths). Third, we propose an efficient one-shot search method to find the optimal path aggregation architecture, that is, we first train a super-net and then find the optimal candidate with an evolutionary algorithm. Experimental results demonstrate the efficacy of the proposed OPANAS for object detection: (1) OPANAS is more efficient than state-of-the-art methods (e.g., NAS-FPN and Auto-FPN), at significantly smaller searching cost (e.g., only 4 GPU days on MS-COCO); (2) the optimal architecture found by OPANAS significantly improves main-stream detectors including RetinaNet, Faster R-CNN and Cascade R-CNN, by 2.3-3.2 % mAP comparing to their FPN counterparts; and (3) a new state-of-the-art accuracy-speed trade-off (52.2 % mAP at 7.6 FPS) at smaller training costs than comparable state-of-the-arts. Code will be released at https://github.com/VDIGPKU/OPANAS.



### Predictive Visual Tracking: A New Benchmark and Baseline Approach
- **Arxiv ID**: http://arxiv.org/abs/2103.04508v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.04508v2)
- **Published**: 2021-03-08 01:50:05+00:00
- **Updated**: 2022-11-10 21:53:22+00:00
- **Authors**: Bowen Li, Yiming Li, Junjie Ye, Changhong Fu, Hang Zhao
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: As a crucial robotic perception capability, visual tracking has been intensively studied recently. In the real-world scenarios, the onboard processing time of the image streams inevitably leads to a discrepancy between the tracking results and the real-world states. However, existing visual tracking benchmarks commonly run the trackers offline and ignore such latency in the evaluation. In this work, we aim to deal with a more realistic problem of latency-aware tracking. The state-of-the-art trackers are evaluated in the aerial scenarios with new metrics jointly assessing the tracking accuracy and efficiency. Moreover, a new predictive visual tracking baseline is developed to compensate for the latency stemming from the onboard computation. Our latency-aware benchmark can provide a more realistic evaluation of the trackers for the robotic applications. Besides, exhaustive experiments have proven the effectiveness of the proposed predictive visual tracking baseline approach.



### Improving Global Adversarial Robustness Generalization With Adversarially Trained GAN
- **Arxiv ID**: http://arxiv.org/abs/2103.04513v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2103.04513v1)
- **Published**: 2021-03-08 02:18:24+00:00
- **Updated**: 2021-03-08 02:18:24+00:00
- **Authors**: Desheng Wang, Weidong Jin, Yunpu Wu, Aamir Khan
- **Comment**: 17 pages, 7 figures, uses simpleConference.sty
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have achieved beyond human-level accuracy in the image classification task and are widely deployed in real-world environments. However, CNNs show vulnerability to adversarial perturbations that are well-designed noises aiming to mislead the classification models. In order to defend against the adversarial perturbations, adversarially trained GAN (ATGAN) is proposed to improve the adversarial robustness generalization of the state-of-the-art CNNs trained by adversarial training. ATGAN incorporates adversarial training into standard GAN training procedure to remove obfuscated gradients which can lead to a false sense in defending against the adversarial perturbations and are commonly observed in existing GANs-based adversarial defense methods. Moreover, ATGAN adopts the image-to-image generator as data augmentation to increase the sample complexity needed for adversarial robustness generalization in adversarial training. Experimental results in MNIST SVHN and CIFAR-10 datasets show that the proposed method doesn't rely on obfuscated gradients and achieves better global adversarial robustness generalization performance than the adversarially trained state-of-the-art CNNs.



### Unveiling the Potential of Structure Preserving for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2103.04523v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04523v2)
- **Published**: 2021-03-08 03:04:14+00:00
- **Updated**: 2021-03-30 02:44:50+00:00
- **Authors**: Xingjia Pan, Yingguo Gao, Zhiwen Lin, Fan Tang, Weiming Dong, Haolei Yuan, Feiyue Huang, Changsheng Xu
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Weakly supervised object localization(WSOL) remains an open problem given the deficiency of finding object extent information using a classification network. Although prior works struggled to localize objects through various spatial regularization strategies, we argue that how to extract object structural information from the trained classification network is neglected. In this paper, we propose a two-stage approach, termed structure-preserving activation (SPA), toward fully leveraging the structure information incorporated in convolutional features for WSOL. First, a restricted activation module (RAM) is designed to alleviate the structure-missing issue caused by the classification network on the basis of the observation that the unbounded classification map and global average pooling layer drive the network to focus only on object parts. Second, we designed a post-process approach, termed self-correlation map generating (SCG) module to obtain structure-preserving localization maps on the basis of the activation maps acquired from the first stage. Specifically, we utilize the high-order self-correlation (HSC) to extract the inherent structural information retained in the learned model and then aggregate HSC of multiple points for precise object localization. Extensive experiments on two publicly available benchmarks including CUB-200-2011 and ILSVRC show that the proposed SPA achieves substantial and consistent performance gains compared with baseline approaches.Code and models are available at https://github.com/Panxjia/SPA_CVPR2021



### FastFlowNet: A Lightweight Network for Fast Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/2103.04524v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04524v2)
- **Published**: 2021-03-08 03:09:37+00:00
- **Updated**: 2021-03-21 14:13:41+00:00
- **Authors**: Lingtong Kong, Chunhua Shen, Jie Yang
- **Comment**: Accepted by ICRA 2021
- **Journal**: None
- **Summary**: Dense optical flow estimation plays a key role in many robotic vision tasks. In the past few years, with the advent of deep learning, we have witnessed great progress in optical flow estimation. However, current networks often consist of a large number of parameters and require heavy computation costs, largely hindering its application on low power-consumption devices such as mobile phones. In this paper, we tackle this challenge and design a lightweight model for fast and accurate optical flow prediction. Our proposed FastFlowNet follows the widely-used coarse-to-fine paradigm with following innovations. First, a new head enhanced pooling pyramid (HEPP) feature extractor is employed to intensify high-resolution pyramid features while reducing parameters. Second, we introduce a new center dense dilated correlation (CDDC) layer for constructing compact cost volume that can keep large search radius with reduced computation burden. Third, an efficient shuffle block decoder (SBD) is implanted into each pyramid level to accelerate flow estimation with marginal drops in accuracy. Experiments on both synthetic Sintel data and real-world KITTI datasets demonstrate the effectiveness of the proposed approach, which needs only 1/10 computation of comparable networks to achieve on par accuracy. In particular, FastFlowNet only contains 1.37M parameters; and can execute at 90 FPS (with a single GTX 1080Ti) or 5.7 FPS (embedded Jetson TX2 GPU) on a pair of Sintel images of resolution 1024x436.



### Incremental Learning for Multi-organ Segmentation with Partially Labeled Datasets
- **Arxiv ID**: http://arxiv.org/abs/2103.04526v1
- **DOI**: 10.1007/978-3-031-16440-8_68
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04526v1)
- **Published**: 2021-03-08 03:15:59+00:00
- **Updated**: 2021-03-08 03:15:59+00:00
- **Authors**: Pengbo Liu, Li Xiao, S. Kevin Zhou
- **Comment**: None
- **Journal**: Medical Image Computing and Computer Assisted Intervention--MICCAI
  2022: 25th International Conference, Singapore, September 18--22, 2022,
  Proceedings, Part IV
- **Summary**: There exists a large number of datasets for organ segmentation, which are partially annotated, and sequentially constructed. A typical dataset is constructed at a certain time by curating medical images and annotating the organs of interest. In other words, new datasets with annotations of new organ categories are built over time. To unleash the potential behind these partially labeled, sequentially-constructed datasets, we propose to learn a multi-organ segmentation model through incremental learning (IL). In each IL stage, we lose access to the previous annotations, whose knowledge is assumingly captured by the current model, and gain the access to a new dataset with annotations of new organ categories, from which we learn to update the organ segmentation model to include the new organs. We give the first attempt to conjecture that the different distribution is the key reason for 'catastrophic forgetting' that commonly exists in IL methods, and verify that IL has the natural adaptability to medical image scenarios. Extensive experiments on five open-sourced datasets are conducted to prove the effectiveness of our method and the conjecture mentioned above.



### One-Shot Medical Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.04527v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04527v1)
- **Published**: 2021-03-08 03:16:53+00:00
- **Updated**: 2021-03-08 03:16:53+00:00
- **Authors**: Qingsong Yao, Quan Quan, Li Xiao, S. Kevin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: The success of deep learning methods relies on the availability of a large number of datasets with annotations; however, curating such datasets is burdensome, especially for medical images. To relieve such a burden for a landmark detection task, we explore the feasibility of using only a single annotated image and propose a novel framework named Cascade Comparing to Detect (CC2D) for one-shot landmark detection. CC2D consists of two stages: 1) Self-supervised learning (CC2D-SSL) and 2) Training with pseudo-labels (CC2D-TPL). CC2D-SSL captures the consistent anatomical information in a coarse-to-fine fashion by comparing the cascade feature representations and generates predictions on the training set. CC2D-TPL further improves the performance by training a new landmark detector with those predictions. The effectiveness of CC2D is evaluated on a widely-used public dataset of cephalometric landmark detection, which achieves a competitive detection accuracy of 81.01\% within 4.0mm, comparable to the state-of-the-art fully-supervised methods using a lot more than one training image.



### Multimodal Representation Learning via Maximization of Local Mutual Information
- **Arxiv ID**: http://arxiv.org/abs/2103.04537v5
- **DOI**: 10.1007/978-3-030-87196-3_26
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.04537v5)
- **Published**: 2021-03-08 03:59:59+00:00
- **Updated**: 2021-12-15 03:21:05+00:00
- **Authors**: Ruizhi Liao, Daniel Moyer, Miriam Cha, Keegan Quigley, Seth Berkowitz, Steven Horng, Polina Golland, William M. Wells
- **Comment**: In Proceedings of International Conference on Medical Image Computing
  and Computer Assisted Intervention (MICCAI), 2021
- **Journal**: In International Conference on Medical Image Computing and
  Computer-Assisted Intervention, pp. 273-283. Springer, Cham, 2021
- **Summary**: We propose and demonstrate a representation learning approach by maximizing the mutual information between local features of images and text. The goal of this approach is to learn useful image representations by taking advantage of the rich information contained in the free text that describes the findings in the image. Our method trains image and text encoders by encouraging the resulting representations to exhibit high local mutual information. We make use of recent advances in mutual information estimation with neural network discriminators. We argue that the sum of local mutual information is typically a lower bound on the global mutual information. Our experimental results in the downstream image classification tasks demonstrate the advantages of using local features for image-text representation learning.



### Exploring a Makeup Support System for Transgender Passing based on Automatic Gender Recognition
- **Arxiv ID**: http://arxiv.org/abs/2103.04544v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.04544v1)
- **Published**: 2021-03-08 04:43:10+00:00
- **Updated**: 2021-03-08 04:43:10+00:00
- **Authors**: Toby Chong, Nolwenn Maudet, Katsuki Harima, Takeo Igarashi
- **Comment**: Accepted to CHI2021. Project Page:
  https://sites.google.com/view/flyingcolor
- **Journal**: None
- **Summary**: How to handle gender with machine learning is a controversial topic. A growing critical body of research brought attention to the numerous issues transgender communities face with the adoption of current automatic gender recognition (AGR) systems. In contrast, we explore how such technologies could potentially be appropriated to support transgender practices and needs, especially in non-Western contexts like Japan. We designed a virtual makeup probe to assist transgender individuals with passing, that is to be perceived as the gender they identify as. To understand how such an application might support expressing transgender individuals gender identity or not, we interviewed 15 individuals in Tokyo and found that in the right context and under strict conditions, AGR based systems could assist transgender passing.



### U-DuDoNet: Unpaired dual-domain network for CT metal artifact reduction
- **Arxiv ID**: http://arxiv.org/abs/2103.04552v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.04552v1)
- **Published**: 2021-03-08 05:19:15+00:00
- **Updated**: 2021-03-08 05:19:15+00:00
- **Authors**: Yuanyuan Lyu, Jiajun Fu, Cheng Peng, S. Kevin Zhou
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Recently, both supervised and unsupervised deep learning methods have been widely applied on the CT metal artifact reduction (MAR) task. Supervised methods such as Dual Domain Network (Du-DoNet) work well on simulation data; however, their performance on clinical data is limited due to domain gap. Unsupervised methods are more generalized, but do not eliminate artifacts completely through the sole processing on the image domain. To combine the advantages of both MAR methods, we propose an unpaired dual-domain network (U-DuDoNet) trained using unpaired data. Unlike the artifact disentanglement network (ADN) that utilizes multiple encoders and decoders for disentangling content from artifact, our U-DuDoNet directly models the artifact generation process through additions in both sinogram and image domains, which is theoretically justified by an additive property associated with metal artifact. Our design includes a self-learned sinogram prior net, which provides guidance for restoring the information in the sinogram domain, and cyclic constraints for artifact reduction and addition on unpaired data. Extensive experiments on simulation data and clinical images demonstrate that our novel framework outperforms the state-of-the-art unpaired approaches.



### CRLF: Automatic Calibration and Refinement based on Line Feature for LiDAR and Camera in Road Scenes
- **Arxiv ID**: http://arxiv.org/abs/2103.04558v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.04558v1)
- **Published**: 2021-03-08 06:02:44+00:00
- **Updated**: 2021-03-08 06:02:44+00:00
- **Authors**: Tao Ma, Zhizheng Liu, Guohang Yan, Yikang Li
- **Comment**: 7 pages, 7 figures, submitted to IROS 2021
- **Journal**: None
- **Summary**: For autonomous vehicles, an accurate calibration for LiDAR and camera is a prerequisite for multi-sensor perception systems. However, existing calibration techniques require either a complicated setting with various calibration targets, or an initial calibration provided beforehand, which greatly impedes their applicability in large-scale autonomous vehicle deployment. To tackle these issues, we propose a novel method to calibrate the extrinsic parameter for LiDAR and camera in road scenes. Our method introduces line features from static straight-line-shaped objects such as road lanes and poles in both image and point cloud and formulates the initial calibration of extrinsic parameters as a perspective-3-lines (P3L) problem. Subsequently, a cost function defined under the semantic constraints of the line features is designed to perform refinement on the solved coarse calibration. The whole procedure is fully automatic and user-friendly without the need to adjust environment settings or provide an initial calibration. We conduct extensive experiments on KITTI and our in-house dataset, quantitative and qualitative results demonstrate the robustness and accuracy of our method.



### Parser-Free Virtual Try-on via Distilling Appearance Flows
- **Arxiv ID**: http://arxiv.org/abs/2103.04559v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04559v2)
- **Published**: 2021-03-08 06:05:38+00:00
- **Updated**: 2021-03-09 05:37:48+00:00
- **Authors**: Yuying Ge, Yibing Song, Ruimao Zhang, Chongjian Ge, Wei Liu, Ping Luo
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Image virtual try-on aims to fit a garment image (target clothes) to a person image. Prior methods are heavily based on human parsing. However, slightly-wrong segmentation results would lead to unrealistic try-on images with large artifacts. Inaccurate parsing misleads parser-based methods to produce visually unrealistic results where artifacts usually occur. A recent pioneering work employed knowledge distillation to reduce the dependency of human parsing, where the try-on images produced by a parser-based method are used as supervisions to train a "student" network without relying on segmentation, making the student mimic the try-on ability of the parser-based model. However, the image quality of the student is bounded by the parser-based model. To address this problem, we propose a novel approach, "teacher-tutor-student" knowledge distillation, which is able to produce highly photo-realistic images without human parsing, possessing several appealing advantages compared to prior arts. (1) Unlike existing work, our approach treats the fake images produced by the parser-based method as "tutor knowledge", where the artifacts can be corrected by real "teacher knowledge", which is extracted from the real person images in a self-supervised way. (2) Other than using real images as supervisions, we formulate knowledge distillation in the try-on problem as distilling the appearance flows between the person image and the garment image, enabling us to find accurate dense correspondences between them to produce high-quality results. (3) Extensive evaluations show large superiority of our method (see Fig. 1).



### Improving Transformation-based Defenses against Adversarial Examples with First-order Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2103.04565v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.04565v2)
- **Published**: 2021-03-08 06:27:24+00:00
- **Updated**: 2021-05-10 06:46:56+00:00
- **Authors**: Haimin Zhang, Min Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have been successfully applied in various machine learning tasks. However, studies show that neural networks are susceptible to adversarial attacks. This exposes a potential threat to neural network-based intelligent systems. We observe that the probability of the correct result outputted by the neural network increases by applying small first-order perturbations generated for non-predicted class labels to adversarial examples. Based on this observation, we propose a method for counteracting adversarial perturbations to improve adversarial robustness. In the proposed method, we randomly select a number of class labels and generate small first-order perturbations for these selected labels. The generated perturbations are added together and then clamped onto a specified space. The obtained perturbation is finally added to the adversarial example to counteract the adversarial perturbation contained in the example. The proposed method is applied at inference time and does not require retraining or finetuning the model. We experimentally validate the proposed method on CIFAR-10 and CIFAR-100. The results demonstrate that our method effectively improves the defense performance of several transformation-based defense methods, especially against strong adversarial examples generated using more iterations.



### Differentiable Multi-Granularity Human Representation Learning for Instance-Aware Human Semantic Parsing
- **Arxiv ID**: http://arxiv.org/abs/2103.04570v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04570v1)
- **Published**: 2021-03-08 06:55:00+00:00
- **Updated**: 2021-03-08 06:55:00+00:00
- **Authors**: Tianfei Zhou, Wenguan Wang, Si Liu, Yi Yang, Luc Van Gool
- **Comment**: CVPR 2021 (Oral). Code: https://github.com/tfzhou/MG-HumanParsing
- **Journal**: None
- **Summary**: To address the challenging task of instance-aware human part parsing, a new bottom-up regime is proposed to learn category-level human semantic segmentation as well as multi-person pose estimation in a joint and end-to-end manner. It is a compact, efficient and powerful framework that exploits structural information over different human granularities and eases the difficulty of person partitioning. Specifically, a dense-to-sparse projection field, which allows explicitly associating dense human semantics with sparse keypoints, is learnt and progressively improved over the network feature pyramid for robustness. Then, the difficult pixel grouping problem is cast as an easier, multi-person joint assembling task. By formulating joint association as maximum-weight bipartite matching, a differentiable solution is developed to exploit projected gradient descent and Dykstra's cyclic projection algorithm. This makes our method end-to-end trainable and allows back-propagating the grouping error to directly supervise multi-granularity human representation learning. This is distinguished from current bottom-up human parsers or pose estimators which require sophisticated post-processing or heuristic greedy algorithms. Experiments on three instance-aware human parsing datasets show that our model outperforms other bottom-up alternatives with much more efficient inference.



### Unsupervised Person Re-Identification with Multi-Label Learning Guided Self-Paced Clustering
- **Arxiv ID**: http://arxiv.org/abs/2103.04580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04580v1)
- **Published**: 2021-03-08 07:30:13+00:00
- **Updated**: 2021-03-08 07:30:13+00:00
- **Authors**: Qing Li, Xiaojiang Peng, Yu Qiao, Qi Hao
- **Comment**: None
- **Journal**: None
- **Summary**: Although unsupervised person re-identification (Re-ID) has drawn increasing research attention recently, it remains challenging to learn discriminative features without annotations across disjoint camera views. In this paper, we address the unsupervised person Re-ID with a conceptually novel yet simple framework, termed as Multi-label Learning guided self-paced Clustering (MLC). MLC mainly learns discriminative features with three crucial modules, namely a multi-scale network, a multi-label learning module, and a self-paced clustering module. Specifically, the multi-scale network generates multi-granularity person features in both global and local views. The multi-label learning module leverages a memory feature bank and assigns each image with a multi-label vector based on the similarities between the image and feature bank. After multi-label training for several epochs, the self-paced clustering joins in training and assigns a pseudo label for each image. The benefits of our MLC come from three aspects: i) the multi-scale person features for better similarity measurement, ii) the multi-label assignment based on the whole dataset ensures that every image can be trained, and iii) the self-paced clustering removes some noisy samples for better feature learning. Extensive experiments on three popular large-scale Re-ID benchmarks demonstrate that our MLC outperforms previous state-of-the-art methods and significantly improves the performance of unsupervised person Re-ID.



### Deep Gradient Projection Networks for Pan-sharpening
- **Arxiv ID**: http://arxiv.org/abs/2103.04584v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.04584v1)
- **Published**: 2021-03-08 07:51:58+00:00
- **Updated**: 2021-03-08 07:51:58+00:00
- **Authors**: Shuang Xu, Jiangshe Zhang, Zixiang Zhao, Kai Sun, Junmin Liu, Chunxia Zhang
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Pan-sharpening is an important technique for remote sensing imaging systems to obtain high resolution multispectral images. Recently, deep learning has become the most popular tool for pan-sharpening. This paper develops a model-based deep pan-sharpening approach. Specifically, two optimization problems regularized by the deep prior are formulated, and they are separately responsible for the generative models for panchromatic images and low resolution multispectral images. Then, the two problems are solved by a gradient projection algorithm, and the iterative steps are generalized into two network blocks. By alternatively stacking the two blocks, a novel network, called gradient projection based pan-sharpening neural network, is constructed. The experimental results on different kinds of satellite datasets demonstrate that the new network outperforms state-of-the-art methods both visually and quantitatively. The codes are available at https://github.com/xsxjtu/GPPNN.



### CheXseen: Unseen Disease Detection for Deep Learning Interpretation of Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/2103.04590v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.04590v2)
- **Published**: 2021-03-08 08:13:21+00:00
- **Updated**: 2021-05-17 05:15:55+00:00
- **Authors**: Siyu Shi, Ishaan Malhi, Kevin Tran, Andrew Y. Ng, Pranav Rajpurkar
- **Comment**: Accepted at MIDL Conference 2021. Previous version accepted at ACM
  Conference on Health, Inference, and Learning (ACM-CHIL) Workshop 2021
- **Journal**: None
- **Summary**: We systematically evaluate the performance of deep learning models in the presence of diseases not labeled for or present during training. First, we evaluate whether deep learning models trained on a subset of diseases (seen diseases) can detect the presence of any one of a larger set of diseases. We find that models tend to falsely classify diseases outside of the subset (unseen diseases) as "no disease". Second, we evaluate whether models trained on seen diseases can detect seen diseases when co-occurring with diseases outside the subset (unseen diseases). We find that models are still able to detect seen diseases even when co-occurring with unseen diseases. Third, we evaluate whether feature representations learned by models may be used to detect the presence of unseen diseases given a small labeled set of unseen diseases. We find that the penultimate layer of the deep neural network provides useful features for unseen disease detection. Our results can inform the safe clinical deployment of deep learning models trained on a non-exhaustive set of disease classes.



### Unified Batch All Triplet Loss for Visible-Infrared Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2103.04607v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04607v1)
- **Published**: 2021-03-08 08:58:52+00:00
- **Updated**: 2021-03-08 08:58:52+00:00
- **Authors**: Wenkang Li, Ke Qi, Wenbin Chen, Yicong Zhou
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Visible-Infrared cross-modality person re-identification (VI-ReID), whose aim is to match person images between visible and infrared modality, is a challenging cross-modality image retrieval task. Batch Hard Triplet loss is widely used in person re-identification tasks, but it does not perform well in the Visible-Infrared person re-identification task. Because it only optimizes the hardest triplet for each anchor image within the mini-batch, samples in the hardest triplet may all belong to the same modality, which will lead to the imbalance problem of modality optimization. To address this problem, we adopt the batch all triplet selection strategy, which selects all the possible triplets among samples to optimize instead of the hardest triplet. Furthermore, we introduce Unified Batch All Triplet loss and Cosine Softmax loss to collaboratively optimize the cosine distance between image vectors. Similarly, we rewrite the Hetero Center Triplet loss, which is proposed for VI-ReID task, into a batch all form to improve model performance. Extensive experiments indicate the effectiveness of the proposed methods, which outperform state-of-the-art methods by a wide margin.



### Beyond Max-Margin: Class Margin Equilibrium for Few-shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.04612v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04612v3)
- **Published**: 2021-03-08 09:04:03+00:00
- **Updated**: 2021-05-31 04:55:57+00:00
- **Authors**: Bohao Li, Boyu Yang, Chang Liu, Feng Liu, Rongrong Ji, Qixiang Ye
- **Comment**: This paper has been modified by the author due to errors
- **Journal**: None
- **Summary**: Few-shot object detection has made substantial progressby representing novel class objects using the feature representation learned upon a set of base class objects. However,an implicit contradiction between novel class classification and representation is unfortunately ignored. On the one hand, to achieve accurate novel class classification, the distributions of either two base classes must be far away fromeach other (max-margin). On the other hand, to precisely represent novel classes, the distributions of base classes should be close to each other to reduce the intra-class distance of novel classes (min-margin). In this paper, we propose a class margin equilibrium (CME) approach, with the aim to optimize both feature space partition and novel class reconstruction in a systematic way. CME first converts the few-shot detection problem to the few-shot classification problem by using a fully connected layer to decouple localization features. CME then reserves adequate margin space for novel classes by introducing simple-yet-effective class margin loss during feature learning. Finally, CME pursues margin equilibrium by disturbing the features of novel class instances in an adversarial min-max fashion. Experiments on Pascal VOC and MS-COCO datasets show that CME significantly improves upon two baseline detectors (up to $3\sim 5\%$ in average), achieving state-of-the-art performance. Code is available at https://github.com/Bohao-Lee/CME .



### Synplex: A synthetic simulator of highly multiplexed histological images
- **Arxiv ID**: http://arxiv.org/abs/2103.04617v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM, q-bio.TO, I.6.3
- **Links**: [PDF](http://arxiv.org/pdf/2103.04617v1)
- **Published**: 2021-03-08 09:12:02+00:00
- **Updated**: 2021-03-08 09:12:02+00:00
- **Authors**: Daniel Jiménez-Sánchez, Mikel Ariz, Carlos Ortiz-de-Solórzano
- **Comment**: 17 pages, 6 figures
- **Journal**: None
- **Summary**: Multiplex tissue immunostaining is a technology of growing relevance as it can capture in situ the complex interactions existing between the elements of the tumor microenvironment. The existence and availability of large, annotated image datasets is key for the objective development and benchmarking of bioimage analysis algorithms. Manual annotation of multiplex images, is however, laborious, often impracticable. In this paper, we present Synplex, a simulation system able to generate multiplex immunostained in situ tissue images based on user-defined parameters. This includes the specification of structural attributes, such as the number of cell phenotypes, the number and level of expression of cellular markers, or the cell morphology. Synplex consists of three sequential modules, each being responsible for a separate task: modeling of cellular neighborhoods, modeling of cell phenotypes, and synthesis of realistic cell/tissue textures. Synplex flexibility and accuracy are demonstrated qualitatively and quantitatively by generating synthetic tissues that simulate disease paradigms found in the real scenarios. Synplex is publicly available for scientific purposes, and we believe it will become a valuable tool for the training and/or validation of multiplex image analysis algorithms.



### Joint Noise-Tolerant Learning and Meta Camera Shift Adaptation for Unsupervised Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2103.04618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04618v1)
- **Published**: 2021-03-08 09:13:06+00:00
- **Updated**: 2021-03-08 09:13:06+00:00
- **Authors**: Fengxiang Yang, Zhun Zhong, Zhiming Luo, Yuanzheng Cai, Yaojin Lin, Shaozi Li, Nicu Sebe
- **Comment**: To appear in CVPR 2021
- **Journal**: None
- **Summary**: This paper considers the problem of unsupervised person re-identification (re-ID), which aims to learn discriminative models with unlabeled data. One popular method is to obtain pseudo-label by clustering and use them to optimize the model. Although this kind of approach has shown promising accuracy, it is hampered by 1) noisy labels produced by clustering and 2) feature variations caused by camera shift. The former will lead to incorrect optimization and thus hinders the model accuracy. The latter will result in assigning the intra-class samples of different cameras to different pseudo-label, making the model sensitive to camera variations. In this paper, we propose a unified framework to solve both problems. Concretely, we propose a Dynamic and Symmetric Cross-Entropy loss (DSCE) to deal with noisy samples and a camera-aware meta-learning algorithm (MetaCam) to adapt camera shift. DSCE can alleviate the negative effects of noisy samples and accommodate the change of clusters after each clustering step. MetaCam simulates cross-camera constraint by splitting the training data into meta-train and meta-test based on camera IDs. With the interacted gradient from meta-train and meta-test, the model is enforced to learn camera-invariant features. Extensive experiments on three re-ID benchmarks show the effectiveness and the complementary of the proposed DSCE and MetaCam. Our method outperforms the state-of-the-art methods on both fully unsupervised re-ID and unsupervised domain adaptive re-ID.



### FEDS -- Filtered Edit Distance Surrogate
- **Arxiv ID**: http://arxiv.org/abs/2103.04635v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04635v2)
- **Published**: 2021-03-08 09:47:51+00:00
- **Updated**: 2021-05-26 13:39:53+00:00
- **Authors**: Yash Patel, Jiri Matas
- **Comment**: ICDAR 2021 camera-ready version
- **Journal**: None
- **Summary**: This paper proposes a procedure to train a scene text recognition model using a robust learned surrogate of edit distance. The proposed method borrows from self-paced learning and filters out the training examples that are hard for the surrogate. The filtering is performed by judging the quality of the approximation, using a ramp function, enabling end-to-end training. Following the literature, the experiments are conducted in a post-tuning setup, where a trained scene text recognition model is tuned using the learned surrogate of edit distance. The efficacy is demonstrated by improvements on various challenging scene text datasets such as IIIT-5K, SVT, ICDAR, SVTP, and CUTE. The proposed method provides an average improvement of $11.2 \%$ on total edit distance and an error reduction of $9.5\%$ on accuracy.



### You Only Learn Once: Universal Anatomical Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.04657v3
- **DOI**: 10.1007/978-3-030-87240-3_9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04657v3)
- **Published**: 2021-03-08 10:38:52+00:00
- **Updated**: 2022-03-12 13:19:18+00:00
- **Authors**: Heqin Zhu, Qingsong Yao, Li Xiao, S. Kevin Zhou
- **Comment**: Accepted for MICCAI 2021, 11 pages, 2 figures, 2 tables
- **Journal**: None
- **Summary**: Detecting anatomical landmarks in medical images plays an essential role in understanding the anatomy and planning automated processing. In recent years, a variety of deep neural network methods have been developed to detect landmarks automatically. However, all of those methods are unary in the sense that a highly specialized network is trained for a single task say associated with a particular anatomical region. In this work, for the first time, we investigate the idea of "You Only Learn Once (YOLO)" and develop a universal anatomical landmark detection model to realize multiple landmark detection tasks with end-to-end training based on mixed datasets. The model consists of a local network and a global network: The local network is built upon the idea of universal U-Net to learn multi-domain local features and the global network is a parallelly-duplicated sequential of dilated convolutions that extract global features to further disambiguate the landmark locations. It is worth mentioning that the new model design requires much fewer parameters than models with standard convolutions to train. We evaluate our YOLO model on three X-ray datasets of 1,588 images on the head, hand, and chest, collectively contributing 62 landmarks. The experimental results show that our proposed universal model behaves largely better than any previous models trained on multiple datasets. It even beats the performance of the model that is trained separately for every single dataset. The code is available at https://github.com/MIRACLE-Center/YOLO_Universal_Anatomical_Landmark_Detection



### Contrastive Neural Architecture Search with Neural Architecture Comparators
- **Arxiv ID**: http://arxiv.org/abs/2103.05471v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05471v2)
- **Published**: 2021-03-08 11:24:07+00:00
- **Updated**: 2021-04-06 07:00:10+00:00
- **Authors**: Yaofo Chen, Yong Guo, Qi Chen, Minli Li, Wei Zeng, Yaowei Wang, Mingkui Tan
- **Comment**: Accpeted by CVPR 2021. The code is available at
  https://github.com/chenyaofo/CTNAS
- **Journal**: None
- **Summary**: One of the key steps in Neural Architecture Search (NAS) is to estimate the performance of candidate architectures. Existing methods either directly use the validation performance or learn a predictor to estimate the performance. However, these methods can be either computationally expensive or very inaccurate, which may severely affect the search efficiency and performance. Moreover, as it is very difficult to annotate architectures with accurate performance on specific tasks, learning a promising performance predictor is often non-trivial due to the lack of labeled data. In this paper, we argue that it may not be necessary to estimate the absolute performance for NAS. On the contrary, we may need only to understand whether an architecture is better than a baseline one. However, how to exploit this comparison information as the reward and how to well use the limited labeled data remains two great challenges. In this paper, we propose a novel Contrastive Neural Architecture Search (CTNAS) method which performs architecture search by taking the comparison results between architectures as the reward. Specifically, we design and learn a Neural Architecture Comparator (NAC) to compute the probability of candidate architectures being better than a baseline one. Moreover, we present a baseline updating scheme to improve the baseline iteratively in a curriculum learning manner. More critically, we theoretically show that learning NAC is equivalent to optimizing the ranking over architectures. Extensive experiments in three search spaces demonstrate the superiority of our CTNAS over existing methods.



### Behavior-Driven Synthesis of Human Dynamics
- **Arxiv ID**: http://arxiv.org/abs/2103.04677v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04677v2)
- **Published**: 2021-03-08 11:36:32+00:00
- **Updated**: 2021-04-22 11:53:27+00:00
- **Authors**: Andreas Blattmann, Timo Milbich, Michael Dorkenwald, Björn Ommer
- **Comment**: Accepted to CVPR 2021 as Poster
- **Journal**: None
- **Summary**: Generating and representing human behavior are of major importance for various computer vision applications. Commonly, human video synthesis represents behavior as sequences of postures while directly predicting their likely progressions or merely changing the appearance of the depicted persons, thus not being able to exercise control over their actual behavior during the synthesis process. In contrast, controlled behavior synthesis and transfer across individuals requires a deep understanding of body dynamics and calls for a representation of behavior that is independent of appearance and also of specific postures. In this work, we present a model for human behavior synthesis which learns a dedicated representation of human dynamics independent of postures. Using this representation, we are able to change the behavior of a person depicted in an arbitrary posture, or to even directly transfer behavior observed in a given video sequence. To this end, we propose a conditional variational framework which explicitly disentangles posture from behavior. We demonstrate the effectiveness of our approach on this novel task, evaluating capturing, transferring, and sampling fine-grained, diverse behavior, both quantitatively and qualitatively. Project page is available at https://cutt.ly/5l7rXEp



### Time and Frequency Network for Human Action Detection in Videos
- **Arxiv ID**: http://arxiv.org/abs/2103.04680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04680v1)
- **Published**: 2021-03-08 11:42:05+00:00
- **Updated**: 2021-03-08 11:42:05+00:00
- **Authors**: Changhai Li, Huawei Chen, Jingqing Lu, Yang Huang, Yingying Liu
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Currently, spatiotemporal features are embraced by most deep learning approaches for human action detection in videos, however, they neglect the important features in frequency domain. In this work, we propose an end-to-end network that considers the time and frequency features simultaneously, named TFNet. TFNet holds two branches, one is time branch formed of three-dimensional convolutional neural network(3D-CNN), which takes the image sequence as input to extract time features; and the other is frequency branch, extracting frequency features through two-dimensional convolutional neural network(2D-CNN) from DCT coefficients. Finally, to obtain the action patterns, these two features are deeply fused under the attention mechanism. Experimental results on the JHMDB51-21 and UCF101-24 datasets demonstrate that our approach achieves remarkable performance for frame-mAP.



### Semiotically-grounded distant viewing of diagrams: insights from two multimodal corpora
- **Arxiv ID**: http://arxiv.org/abs/2103.04692v1
- **DOI**: 10.1093/llc/fqab063
- **Categories**: **cs.CL**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2103.04692v1)
- **Published**: 2021-03-08 12:04:06+00:00
- **Updated**: 2021-03-08 12:04:06+00:00
- **Authors**: Tuomo Hiippala, John A. Bateman
- **Comment**: 22 pages, 11 figures. Under review at Digital Scholarship in the
  Humanities
- **Journal**: Digital Scholarship in the Humanities, 2021 (ahead of press)
- **Summary**: In this article, we bring together theories of multimodal communication and computational methods to study how primary school science diagrams combine multiple expressive resources. We position our work within the field of digital humanities, and show how annotations informed by multimodality research, which target expressive resources and discourse structure, allow imposing structure on the output of computational methods. We illustrate our approach by analysing two multimodal diagram corpora: the first corpus is intended to support research on automatic diagram processing, whereas the second is oriented towards studying diagrams as a mode of communication. Our results show that multimodally-informed annotations can bring out structural patterns in the diagrams, which also extend across diagrams that deal with different topics.



### Unsupervised Object-Based Transition Models for 3D Partially Observable Environments
- **Arxiv ID**: http://arxiv.org/abs/2103.04693v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.04693v1)
- **Published**: 2021-03-08 12:10:02+00:00
- **Updated**: 2021-03-08 12:10:02+00:00
- **Authors**: Antonia Creswell, Rishabh Kabra, Chris Burgess, Murray Shanahan
- **Comment**: None
- **Journal**: None
- **Summary**: We present a slot-wise, object-based transition model that decomposes a scene into objects, aligns them (with respect to a slot-wise object memory) to maintain a consistent order across time, and predicts how those objects evolve over successive frames. The model is trained end-to-end without supervision using losses at the level of the object-structured representation rather than pixels. Thanks to its alignment module, the model deals properly with two issues that are not handled satisfactorily by other transition models, namely object persistence and object identity. We show that the combination of an object-level loss and correct object alignment over time enables the model to outperform a state-of-the-art baseline, and allows it to deal well with object occlusion and re-appearance in partially observable environments.



### Interpretable Attention Guided Network for Fine-grained Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/2103.04701v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04701v2)
- **Published**: 2021-03-08 12:27:51+00:00
- **Updated**: 2021-03-09 02:15:22+00:00
- **Authors**: Zhenhuan Huang, Xiaoyue Duan, Bo Zhao, Jinhu Lü, Baochang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-grained visual classification (FGVC) is challenging but more critical than traditional classification tasks. It requires distinguishing different subcategories with the inherently subtle intra-class object variations. Previous works focus on enhancing the feature representation ability using multiple granularities and discriminative regions based on the attention strategy or bounding boxes. However, these methods highly rely on deep neural networks which lack interpretability. We propose an Interpretable Attention Guided Network (IAGN) for fine-grained visual classification. The contributions of our method include: i) an attention guided framework which can guide the network to extract discriminitive regions in an interpretable way; ii) a progressive training mechanism obtained to distill knowledge stage by stage to fuse features of various granularities; iii) the first interpretable FGVC method with a competitive performance on several standard FGVC benchmark datasets.



### On Implicit Attribute Localization for Generalized Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.04704v1
- **DOI**: 10.1109/LSP.2021.3073655
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04704v1)
- **Published**: 2021-03-08 12:31:37+00:00
- **Updated**: 2021-03-08 12:31:37+00:00
- **Authors**: Shiqi Yang, Kai Wang, Luis Herranz, Joost van de Weijer
- **Comment**: To appear in IEEE Signal Processing Letters. Overlapped with
  arXiv:2006.05938
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) aims to discriminate images from unseen classes by exploiting relations to seen classes via their attribute-based descriptions. Since attributes are often related to specific parts of objects, many recent works focus on discovering discriminative regions. However, these methods usually require additional complex part detection modules or attention mechanisms. In this paper, 1) we show that common ZSL backbones (without explicit attention nor part detection) can implicitly localize attributes, yet this property is not exploited. 2) Exploiting it, we then propose SELAR, a simple method that further encourages attribute localization, surprisingly achieving very competitive generalized ZSL (GZSL) performance when compared with more complex state-of-the-art methods. Our findings provide useful insight for designing future GZSL methods, and SELAR provides an easy to implement yet strong baseline.



### Semi-supervised Domain Adaptation based on Dual-level Domain Mixing for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.04705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04705v1)
- **Published**: 2021-03-08 12:33:17+00:00
- **Updated**: 2021-03-08 12:33:17+00:00
- **Authors**: Shuaijun Chen, Xu Jia, Jianzhong He, Yongjie Shi, Jianzhuang Liu
- **Comment**: CVPR2021
- **Journal**: None
- **Summary**: Data-driven based approaches, in spite of great success in many tasks, have poor generalization when applied to unseen image domains, and require expensive cost of annotation especially for dense pixel prediction tasks such as semantic segmentation. Recently, both unsupervised domain adaptation (UDA) from large amounts of synthetic data and semi-supervised learning (SSL) with small set of labeled data have been studied to alleviate this issue. However, there is still a large gap on performance compared to their supervised counterparts. We focus on a more practical setting of semi-supervised domain adaptation (SSDA) where both a small set of labeled target data and large amounts of labeled source data are available. To address the task of SSDA, a novel framework based on dual-level domain mixing is proposed. The proposed framework consists of three stages. First, two kinds of data mixing methods are proposed to reduce domain gap in both region-level and sample-level respectively. We can obtain two complementary domain-mixed teachers based on dual-level mixed data from holistic and partial views respectively. Then, a student model is learned by distilling knowledge from these two teachers. Finally, pseudo labels of unlabeled data are generated in a self-training manner for another few rounds of teachers training. Extensive experimental results have demonstrated the effectiveness of our proposed framework on synthetic-to-real semantic segmentation benchmarks.



### Dual-Task Mutual Learning for Semi-Supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.04708v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.04708v2)
- **Published**: 2021-03-08 12:38:23+00:00
- **Updated**: 2021-07-11 11:43:42+00:00
- **Authors**: Yichi Zhang, Jicong Zhang
- **Comment**: Accepted for PRCV 2021
- **Journal**: None
- **Summary**: The success of deep learning methods in medical image segmentation tasks usually requires a large amount of labeled data. However, obtaining reliable annotations is expensive and time-consuming. Semi-supervised learning has attracted much attention in medical image segmentation by taking the advantage of unlabeled data which is much easier to acquire. In this paper, we propose a novel dual-task mutual learning framework for semi-supervised medical image segmentation. Our framework can be formulated as an integration of two individual segmentation networks based on two tasks: learning region-based shape constraint and learning boundary-based surface mismatch. Different from the one-way transfer between teacher and student networks, an ensemble of dual-task students can learn collaboratively and implicitly explore useful knowledge from each other during the training process. By jointly learning the segmentation probability maps and signed distance maps of targets, our framework can enforce the geometric shape constraint and learn more reliable information. Experimental results demonstrate that our method achieves performance gains by leveraging unlabeled data and outperforms the state-of-the-art semi-supervised segmentation methods.



### Bayesian imaging using Plug & Play priors: when Langevin meets Tweedie
- **Arxiv ID**: http://arxiv.org/abs/2103.04715v6
- **DOI**: 10.1137/21M1406349
- **Categories**: **stat.ME**, cs.CV, eess.IV, math.ST, stat.ML, stat.TH, 65K10, 65K05, 65D18, 62F15, 62C10, 68Q25, 68U10, 90C26
- **Links**: [PDF](http://arxiv.org/pdf/2103.04715v6)
- **Published**: 2021-03-08 12:46:53+00:00
- **Updated**: 2022-01-12 09:33:47+00:00
- **Authors**: Rémi Laumont, Valentin de Bortoli, Andrés Almansa, Julie Delon, Alain Durmus, Marcelo Pereyra
- **Comment**: None
- **Journal**: None
- **Summary**: Since the seminal work of Venkatakrishnan et al. in 2013, Plug & Play (PnP) methods have become ubiquitous in Bayesian imaging. These methods derive Minimum Mean Square Error (MMSE) or Maximum A Posteriori (MAP) estimators for inverse problems in imaging by combining an explicit likelihood function with a prior that is implicitly defined by an image denoising algorithm. The PnP algorithms proposed in the literature mainly differ in the iterative schemes they use for optimisation or for sampling. In the case of optimisation schemes, some recent works guarantee the convergence to a fixed point, albeit not necessarily a MAP estimate. In the case of sampling schemes, to the best of our knowledge, there is no known proof of convergence. There also remain important open questions regarding whether the underlying Bayesian models and estimators are well defined, well-posed, and have the basic regularity properties required to support these numerical schemes. To address these limitations, this paper develops theory, methods, and provably convergent algorithms for performing Bayesian inference with PnP priors. We introduce two algorithms: 1) PnP-ULA (Unadjusted Langevin Algorithm) for Monte Carlo sampling and MMSE inference; and 2) PnP-SGD (Stochastic Gradient Descent) for MAP inference. Using recent results on the quantitative convergence of Markov chains, we establish detailed convergence guarantees for these two algorithms under realistic assumptions on the denoising operators used, with special attention to denoisers based on deep neural networks. We also show that these algorithms approximately target a decision-theoretically optimal Bayesian model that is well-posed. The proposed algorithms are demonstrated on several canonical problems such as image deblurring, inpainting, and denoising, where they are used for point estimation as well as for uncertainty visualisation and quantification.



### Multi-Source Domain Adaptation with Collaborative Learning for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2103.04717v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04717v3)
- **Published**: 2021-03-08 12:51:42+00:00
- **Updated**: 2021-06-18 08:07:33+00:00
- **Authors**: Jianzhong He, Xu Jia, Shuaijun Chen, Jianzhuang Liu
- **Comment**: CVPR2021
- **Journal**: None
- **Summary**: Multi-source unsupervised domain adaptation~(MSDA) aims at adapting models trained on multiple labeled source domains to an unlabeled target domain. In this paper, we propose a novel multi-source domain adaptation framework based on collaborative learning for semantic segmentation. Firstly, a simple image translation method is introduced to align the pixel value distribution to reduce the gap between source domains and target domain to some extent. Then, to fully exploit the essential semantic information across source domains, we propose a collaborative learning method for domain adaptation without seeing any data from target domain. In addition, similar to the setting of unsupervised domain adaptation, unlabeled target domain data is leveraged to further improve the performance of domain adaptation. This is achieved by additionally constraining the outputs of multiple adaptation models with pseudo labels online generated by an ensembled model. Extensive experiments and ablation studies are conducted on the widely-used domain adaptation benchmark datasets in semantic segmentation. Our proposed method achieves 59.0\% mIoU on the validation set of Cityscapes by training on the labeled Synscapes and GTA5 datasets and unlabeled training set of Cityscapes. It significantly outperforms all previous state-of-the-arts single-source and multi-source unsupervised domain adaptation methods.



### Vision-Based Mobile Robotics Obstacle Avoidance With Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2103.04727v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2103.04727v1)
- **Published**: 2021-03-08 13:05:46+00:00
- **Updated**: 2021-03-08 13:05:46+00:00
- **Authors**: Patrick Wenzel, Torsten Schön, Laura Leal-Taixé, Daniel Cremers
- **Comment**: Accepted at 2021 IEEE International Conference on Robotics and
  Automation (ICRA)
- **Journal**: None
- **Summary**: Obstacle avoidance is a fundamental and challenging problem for autonomous navigation of mobile robots. In this paper, we consider the problem of obstacle avoidance in simple 3D environments where the robot has to solely rely on a single monocular camera. In particular, we are interested in solving this problem without relying on localization, mapping, or planning techniques. Most of the existing work consider obstacle avoidance as two separate problems, namely obstacle detection, and control. Inspired by the recent advantages of deep reinforcement learning in Atari games and understanding highly complex situations in Go, we tackle the obstacle avoidance problem as a data-driven end-to-end deep learning approach. Our approach takes raw images as input and generates control commands as output. We show that discrete action spaces are outperforming continuous control commands in terms of expected average reward in maze-like environments. Furthermore, we show how to accelerate the learning and increase the robustness of the policy by incorporating predicted depth maps by a generative adversarial network.



### Self-Augmented Multi-Modal Feature Embedding
- **Arxiv ID**: http://arxiv.org/abs/2103.04731v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04731v1)
- **Published**: 2021-03-08 13:10:52+00:00
- **Updated**: 2021-03-08 13:10:52+00:00
- **Authors**: Shinnosuke Matsuo, Seiichi Uchida, Brian Kenji Iwana
- **Comment**: Accepted at ICASSP2021
- **Journal**: None
- **Summary**: Oftentimes, patterns can be represented through different modalities. For example, leaf data can be in the form of images or contours. Handwritten characters can also be either online or offline. To exploit this fact, we propose the use of self-augmentation and combine it with multi-modal feature embedding. In order to take advantage of the complementary information from the different modalities, the self-augmented multi-modal feature embedding employs a shared feature space. Through experimental results on classification with online handwriting and leaf images, we demonstrate that the proposed method can create effective embeddings.



### Content-Aware Detection of Temporal Metadata Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2103.04736v2
- **DOI**: 10.1109/TIFS.2022.3159154
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.04736v2)
- **Published**: 2021-03-08 13:16:19+00:00
- **Updated**: 2022-03-11 12:19:47+00:00
- **Authors**: Rafael Padilha, Tawfiq Salem, Scott Workman, Fernanda A. Andaló, Anderson Rocha, Nathan Jacobs
- **Comment**: None
- **Journal**: IEEE Transactions on Information Forensics and Security 2022
- **Summary**: Most pictures shared online are accompanied by temporal metadata (i.e., the day and time they were taken), which makes it possible to associate an image content with real-world events. Maliciously manipulating this metadata can convey a distorted version of reality. In this work, we present the emerging problem of detecting timestamp manipulation. We propose an end-to-end approach to verify whether the purported time of capture of an outdoor image is consistent with its content and geographic location. We consider manipulations done in the hour and/or month of capture of a photograph. The central idea is the use of supervised consistency verification, in which we predict the probability that the image content, capture time, and geographical location are consistent. We also include a pair of auxiliary tasks, which can be used to explain the network decision. Our approach improves upon previous work on a large benchmark dataset, increasing the classification accuracy from 59.0% to 81.1%. We perform an ablation study that highlights the importance of various components of the method, showing what types of tampering are detectable using our approach. Finally, we demonstrate how the proposed method can be employed to estimate a possible time-of-capture in scenarios in which the timestamp is missing from the metadata.



### Bridging the Distribution Gap of Visible-Infrared Person Re-identification with Modality Batch Normalization
- **Arxiv ID**: http://arxiv.org/abs/2103.04778v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04778v1)
- **Published**: 2021-03-08 14:16:09+00:00
- **Updated**: 2021-03-08 14:16:09+00:00
- **Authors**: Wenkang Li, Qi Ke, Wenbin Chen, Yicong Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Visible-infrared cross-modality person re-identification (VI-ReID), whose aim is to match person images between visible and infrared modality, is a challenging cross-modality image retrieval task. Most existing works integrate batch normalization layers into their neural network, but we found out that batch normalization layers would lead to two types of distribution gap: 1) inter-mini-batch distribution gap -- the distribution gap of the same modality between each mini-batch; 2) intra-mini-batch modality distribution gap -- the distribution gap of different modality within the same mini-batch. To address these problems, we propose a new batch normalization layer called Modality Batch Normalization (MBN), which normalizes each modality sub-mini-batch respectively instead of the whole mini-batch, and can reduce these distribution gap significantly. Extensive experiments show that our MBN is able to boost the performance of VI-ReID models, even with different datasets, backbones and losses.



### Look, Cast and Mold: Learning 3D Shape Manifold from Single-view Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2103.04789v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04789v3)
- **Published**: 2021-03-08 14:30:18+00:00
- **Updated**: 2022-06-07 05:44:25+00:00
- **Authors**: Qianyu Feng, Yawei Luo, Keyang Luo, Yi Yang
- **Comment**: this work is no longer under development
- **Journal**: None
- **Summary**: Inferring the stereo structure of objects in the real world is a challenging yet practical task. To equip deep models with this ability usually requires abundant 3D supervision which is hard to acquire. It is promising that we can simply benefit from synthetic data, where pairwise ground-truth is easy to access. Nevertheless, the domain gaps are nontrivial considering the variant texture, shape and context. To overcome these difficulties, we propose a Visio-Perceptual Adaptive Network for single-view 3D reconstruction, dubbed VPAN. To generalize the model towards a real scenario, we propose to fulfill several aspects: (1) Look: visually incorporate spatial structure from the single view to enhance the expressiveness of representation; (2) Cast: perceptually align the 2D image features to the 3D shape priors with cross-modal semantic contrastive mapping; (3) Mold: reconstruct stereo-shape of target by transforming embeddings into the desired manifold. Extensive experiments on several benchmarks demonstrate the effectiveness and robustness of the proposed method in learning the 3D shape manifold from synthetic data via a single-view. The proposed method outperforms state-of-the-arts on Pix3D dataset with IoU 0.292 and CD 0.108, and reaches IoU 0.329 and CD 0.104 on Pascal 3D+.



### Boosting Semi-supervised Image Segmentation with Global and Local Mutual Information Regularization
- **Arxiv ID**: http://arxiv.org/abs/2103.04813v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04813v2)
- **Published**: 2021-03-08 15:13:25+00:00
- **Updated**: 2021-06-24 06:25:06+00:00
- **Authors**: Jizong Peng, Marco Pedersoli, Christian Desrosiers
- **Comment**: None
- **Journal**: None
- **Summary**: The scarcity of labeled data often impedes the application of deep learning to the segmentation of medical images. Semi-supervised learning seeks to overcome this limitation by exploiting unlabeled examples in the learning process. In this paper, we present a novel semi-supervised segmentation method that leverages mutual information (MI) on categorical distributions to achieve both global representation invariance and local smoothness. In this method, we maximize the MI for intermediate feature embeddings that are taken from both the encoder and decoder of a segmentation network. We first propose a global MI loss constraining the encoder to learn an image representation that is invariant to geometric transformations. Instead of resorting to computationally-expensive techniques for estimating the MI on continuous feature embeddings, we use projection heads to map them to a discrete cluster assignment where MI can be computed efficiently. Our method also includes a local MI loss to promote spatial consistency in the feature maps of the decoder and provide a smoother segmentation. Since mutual information does not require a strict ordering of clusters in two different assignments, we incorporate a final consistency regularization loss on the output which helps align the cluster labels throughout the network. We evaluate the method on four challenging publicly-available datasets for medical image segmentation. Experimental results show our method to outperform recently-proposed approaches for semi-supervised segmentation and provide an accuracy near to full supervision while training with very few annotated images.



### Deeply Unsupervised Patch Re-Identification for Pre-training Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2103.04814v2
- **DOI**: 10.1109/TPAMI.2022.3164911
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04814v2)
- **Published**: 2021-03-08 15:13:59+00:00
- **Updated**: 2022-04-10 09:02:09+00:00
- **Authors**: Jian Ding, Enze Xie, Hang Xu, Chenhan Jiang, Zhenguo Li, Ping Luo, Gui-Song Xia
- **Comment**: Accepted to IEEE TPAMI
- **Journal**: None
- **Summary**: Unsupervised pre-training aims at learning transferable features that are beneficial for downstream tasks. However, most state-of-the-art unsupervised methods concentrate on learning global representations for image-level classification tasks instead of discriminative local region representations, which limits their transferability to region-level downstream tasks, such as object detection. To improve the transferability of pre-trained features to object detection, we present Deeply Unsupervised Patch Re-ID (DUPR), a simple yet effective method for unsupervised visual representation learning. The patch Re-ID task treats individual patch as a pseudo-identity and contrastively learns its correspondence in two views, enabling us to obtain discriminative local features for object detection. Then the proposed patch Re-ID is performed in a deeply unsupervised manner, appealing to object detection, which usually requires multilevel feature maps. Extensive experiments demonstrate that DUPR outperforms state-of-the-art unsupervised pre-trainings and even the ImageNet supervised pre-training on various downstream tasks related to object detection.



### Multi-phase Deformable Registration for Time-dependent Abdominal Organ Variations
- **Arxiv ID**: http://arxiv.org/abs/2103.05525v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.05525v1)
- **Published**: 2021-03-08 15:43:23+00:00
- **Updated**: 2021-03-08 15:43:23+00:00
- **Authors**: Seyoun Park, Elliot K. Fishman, Alan L. Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Human body is a complex dynamic system composed of various sub-dynamic parts. Especially, thoracic and abdominal organs have complex internal shape variations with different frequencies by various reasons such as respiration with fast motion and peristalsis with slower motion. CT protocols for abdominal lesions are multi-phase scans for various tumor detection to use different vascular contrast, however, they are not aligned well enough to visually check the same area. In this paper, we propose a time-efficient and accurate deformable registration algorithm for multi-phase CT scans considering abdominal organ motions, which can be applied for differentiable or non-differentiable motions of abdominal organs. Experimental results shows the registration accuracy as 0.85 +/- 0.45mm (mean +/- STD) for pancreas within 1 minute for the whole abdominal region.



### Machine-learning based methodologies for 3d x-ray measurement, characterization and optimization for buried structures in advanced ic packages
- **Arxiv ID**: http://arxiv.org/abs/2103.04838v2
- **DOI**: 10.23919/IWLPC52010.2020.9375903
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04838v2)
- **Published**: 2021-03-08 15:44:18+00:00
- **Updated**: 2021-05-20 02:13:02+00:00
- **Authors**: Ramanpreet S Pahwa, Soon Wee Ho, Ren Qin, Richard Chang, Oo Zaw Min, Wang Jie, Vempati Srinivasa Rao, Tin Lay Nwe, Yanjing Yang, Jens Timo Neumann, Ramani Pichumani, Thomas Gregorich
- **Comment**: 7 pages, 9 figures
- **Journal**: International Wafer-Level Packaging Conference (IWLPC) 2020
- **Summary**: For over 40 years lithographic silicon scaling has driven circuit integration and performance improvement in the semiconductor industry. As silicon scaling slows down, the industry is increasingly dependent on IC package technologies to contribute to further circuit integration and performance improvements. This is a paradigm shift and requires the IC package industry to reduce the size and increase the density of internal interconnects on a scale which has never been done before. Traditional package characterization and process optimization relies on destructive techniques such as physical cross-sections and delayering to extract data from internal package features. These destructive techniques are not practical with today's advanced packages. In this paper we will demonstrate how data acquired non-destructively with a 3D X-ray microscope can be enhanced and optimized using machine learning, and can then be used to measure, characterize and optimize the design and production of buried interconnects in advanced IC packages. Test vehicles replicating 2.5D and HBM construction were designed and fabricated, and digital data was extracted from these test vehicles using 3D X-ray and machine learning techniques. The extracted digital data was used to characterize and optimize the design and production of the interconnects and demonstrates a superior alternative to destructive physical analysis. We report an mAP of 0.96 for 3D object detection, a dice score of 0.92 for 3D segmentation, and an average of 2.1um error for 3D metrology on the test dataset. This paper is the first part of a multi-part report.



### Relationship-based Neural Baby Talk
- **Arxiv ID**: http://arxiv.org/abs/2103.04846v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.04846v1)
- **Published**: 2021-03-08 15:51:24+00:00
- **Updated**: 2021-03-08 15:51:24+00:00
- **Authors**: Fan Fu, Tingting Xie, Ioannis Patras, Sepehr Jalali
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding interactions between objects in an image is an important element for generating captions. In this paper, we propose a relationship-based neural baby talk (R-NBT) model to comprehensively investigate several types of pairwise object interactions by encoding each image via three different relationship-based graph attention networks (GATs). We study three main relationships: \textit{spatial relationships} to explore geometric interactions, \textit{semantic relationships} to extract semantic interactions, and \textit{implicit relationships} to capture hidden information that could not be modelled explicitly as above. We construct three relationship graphs with the objects in an image as nodes, and the mutual relationships of pairwise objects as edges. By exploring features of neighbouring regions individually via GATs, we integrate different types of relationships into visual features of each node. Experiments on COCO dataset show that our proposed R-NBT model outperforms state-of-the-art models trained on COCO dataset in three image caption generation tasks.



### From Hand-Perspective Visual Information to Grasp Type Probabilities: Deep Learning via Ranking Labels
- **Arxiv ID**: http://arxiv.org/abs/2103.04863v1
- **DOI**: 10.1145/3316782.3316794
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.04863v1)
- **Published**: 2021-03-08 16:12:38+00:00
- **Updated**: 2021-03-08 16:12:38+00:00
- **Authors**: Mo Han, Sezen Ya{ğ}mur Günay, İlkay Yıldız, Paolo Bonato, Cagdas D. Onal, Taşkın Padır, Gunar Schirner, Deniz Erdo{ğ}muş
- **Comment**: None
- **Journal**: None
- **Summary**: Limb deficiency severely affects the daily lives of amputees and drives efforts to provide functional robotic prosthetic hands to compensate this deprivation. Convolutional neural network-based computer vision control of the prosthetic hand has received increased attention as a method to replace or complement physiological signals due to its reliability by training visual information to predict the hand gesture. Mounting a camera into the palm of a prosthetic hand is proved to be a promising approach to collect visual data. However, the grasp type labelled from the eye and hand perspective may differ as object shapes are not always symmetric. Thus, to represent this difference in a realistic way, we employed a dataset containing synchronous images from eye- and hand- view, where the hand-perspective images are used for training while the eye-view images are only for manual labelling. Electromyogram (EMG) activity and movement kinematics data from the upper arm are also collected for multi-modal information fusion in future work. Moreover, in order to include human-in-the-loop control and combine the computer vision with physiological signal inputs, instead of making absolute positive or negative predictions, we build a novel probabilistic classifier according to the Plackett-Luce model. To predict the probability distribution over grasps, we exploit the statistical model over label rankings to solve the permutation domain problems via a maximum likelihood estimation, utilizing the manually ranked lists of grasps as a new form of label. We indicate that the proposed model is applicable to the most popular and productive convolutional neural network frameworks.



### Domain Adaptive Egocentric Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2103.04870v1
- **DOI**: 10.1007/978-981-16-1103-2_8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04870v1)
- **Published**: 2021-03-08 16:19:32+00:00
- **Updated**: 2021-03-08 16:19:32+00:00
- **Authors**: Ankit Choudhary, Deepak Mishra, Arnab Karmakar
- **Comment**: 12 pages, 4 figures, In Proceedings of the Fifth IAPR International
  Conference on Computer Vision & Image Processing (CVIP), 2020
- **Journal**: None
- **Summary**: Person re-identification (re-ID) in first-person (egocentric) vision is a fairly new and unexplored problem. With the increase of wearable video recording devices, egocentric data becomes readily available, and person re-identification has the potential to benefit greatly from this. However, there is a significant lack of large scale structured egocentric datasets for person re-identification, due to the poor video quality and lack of individuals in most of the recorded content. Although a lot of research has been done in person re-identification based on fixed surveillance cameras, these do not directly benefit egocentric re-ID. Machine learning models trained on the publicly available large scale re-ID datasets cannot be applied to egocentric re-ID due to the dataset bias problem. The proposed algorithm makes use of neural style transfer (NST) that incorporates a variant of Convolutional Neural Network (CNN) to utilize the benefits of both fixed camera vision and first-person vision. NST generates images having features from both egocentric datasets and fixed camera datasets, that are fed through a VGG-16 network trained on a fixed-camera dataset for feature extraction. These extracted features are then used to re-identify individuals. The fixed camera dataset Market-1501 and the first-person dataset EGO Re-ID are applied for this work and the results are on par with the present re-identification models in the egocentric domain.



### The Weakly-Labeled Rand Index
- **Arxiv ID**: http://arxiv.org/abs/2103.04872v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2103.04872v2)
- **Published**: 2021-03-08 16:21:15+00:00
- **Updated**: 2021-03-09 02:58:37+00:00
- **Authors**: Dylan Stewart, Anna Hampton, Alina Zare, Jeff Dale, James Keller
- **Comment**: None
- **Journal**: None
- **Summary**: Synthetic Aperture Sonar (SAS) surveys produce imagery with large regions of transition between seabed types. Due to these regions, it is difficult to label and segment the imagery and, furthermore, challenging to score the image segmentations appropriately. While there are many approaches to quantify performance in standard crisp segmentation schemes, drawing hard boundaries in remote sensing imagery where gradients and regions of uncertainty exist is inappropriate. These cases warrant weak labels and an associated appropriate scoring approach. In this paper, a labeling approach and associated modified version of the Rand index for weakly-labeled data is introduced to address these issues. Results are evaluated with the new index and compared to traditional segmentation evaluation methods. Experimental results on a SAS data set containing must-link and cannot-link labels show that our Weakly-Labeled Rand index scores segmentations appropriately in reference to qualitative performance and is more suitable than traditional quantitative metrics for scoring weakly-labeled data.



### Data-driven Cloud Clustering via a Rotationally Invariant Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2103.04885v2
- **DOI**: 10.1109/TGRS.2021.3098008
- **Categories**: **cs.CV**, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/2103.04885v2)
- **Published**: 2021-03-08 16:45:14+00:00
- **Updated**: 2021-10-28 04:13:07+00:00
- **Authors**: Takuya Kurihana, Elisabeth Moyer, Rebecca Willett, Davis Gilton, Ian Foster
- **Comment**: 25 pages. Accepted by IEEE Transactions on Geoscience and Remote
  Sensing (TGRS)
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, 2021
- **Summary**: Advanced satellite-born remote sensing instruments produce high-resolution multi-spectral data for much of the globe at a daily cadence. These datasets open up the possibility of improved understanding of cloud dynamics and feedback, which remain the biggest source of uncertainty in global climate model projections. As a step towards answering these questions, we describe an automated rotation-invariant cloud clustering (RICC) method that leverages deep learning autoencoder technology to organize cloud imagery within large datasets in an unsupervised fashion, free from assumptions about predefined classes. We describe both the design and implementation of this method and its evaluation, which uses a sequence of testing protocols to determine whether the resulting clusters: (1) are physically reasonable, (i.e., embody scientifically relevant distinctions); (2) capture information on spatial distributions, such as textures; (3) are cohesive and separable in latent space; and (4) are rotationally invariant, (i.e., insensitive to the orientation of an image). Results obtained when these evaluation protocols are applied to RICC outputs suggest that the resultant novel cloud clusters capture meaningful aspects of cloud physics, are appropriately spatially coherent, and are invariant to orientations of input images. Our results support the possibility of using an unsupervised data-driven approach for automated clustering and pattern discovery in cloud imagery.



### Autonomous object harvesting using synchronized optoelectronic microrobots
- **Arxiv ID**: http://arxiv.org/abs/2103.04912v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.04912v1)
- **Published**: 2021-03-08 17:24:15+00:00
- **Updated**: 2021-03-08 17:24:15+00:00
- **Authors**: Christopher Bendkowski, Laurent Mennillo, Tao Xu, Mohamed Elsayed, Filip Stojic, Harrison Edwards, Shuailong Zhang, Cindi Morshead, Vijay Pawar, Aaron R. Wheeler, Danail Stoyanov, Michael Shaw
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: Optoelectronic tweezer-driven microrobots (OETdMs) are a versatile micromanipulation technology based on the use of light induced dielectrophoresis to move small dielectric structures (microrobots) across a photoconductive substrate. The microrobots in turn can be used to exert forces on secondary objects and carry out a wide range of micromanipulation operations, including collecting, transporting and depositing microscopic cargos. In contrast to alternative (direct) micromanipulation techniques, OETdMs are relatively gentle, making them particularly well suited to interacting with sensitive objects such as biological cells. However, at present such systems are used exclusively under manual control by a human operator. This limits the capacity for simultaneous control of multiple microrobots, reducing both experimental throughput and the possibility of cooperative multi-robot operations. In this article, we describe an approach to automated targeting and path planning to enable open-loop control of multiple microrobots. We demonstrate the performance of the method in practice, using microrobots to simultaneously collect, transport and deposit silica microspheres. Using computational simulations based on real microscopic image data, we investigate the capacity of microrobots to collect target cells from within a dissociated tissue culture. Our results indicate the feasibility of using OETdMs to autonomously carry out micromanipulation tasks within complex, unstructured environments.



### Analysis of Convolutional Decoder for Image Caption Generation
- **Arxiv ID**: http://arxiv.org/abs/2103.04914v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.04914v1)
- **Published**: 2021-03-08 17:25:31+00:00
- **Updated**: 2021-03-08 17:25:31+00:00
- **Authors**: Sulabh Katiyar, Samir Kumar Borgohain
- **Comment**: 18 pages, to be published in Book Series: Advances in Intelligent
  Systems and Computing - ISSN 2194-5357
- **Journal**: None
- **Summary**: Recently Convolutional Neural Networks have been proposed for Sequence Modelling tasks such as Image Caption Generation. However, unlike Recurrent Neural Networks, the performance of Convolutional Neural Networks as Decoders for Image Caption Generation has not been extensively studied. In this work, we analyse various aspects of Convolutional Neural Network based Decoders such as Network complexity and depth, use of Data Augmentation, Attention mechanism, length of sentences used during training, etc on performance of the model. We perform experiments using Flickr8k and Flickr30k image captioning datasets and observe that unlike Recurrent Neural Network based Decoder, Convolutional Decoder for Image Captioning does not generally benefit from increase in network depth, in the form of stacked Convolutional Layers, and also the use of Data Augmentation techniques. In addition, use of Attention mechanism also provides limited performance gains with Convolutional Decoder. Furthermore, we observe that Convolutional Decoders show performance comparable with Recurrent Decoders only when trained using sentences of smaller length which contain up to 15 words but they have limitations when trained using higher sentence lengths which suggests that Convolutional Decoders may not be able to model long-term dependencies efficiently. In addition, the Convolutional Decoder usually performs poorly on CIDEr evaluation metric as compared to Recurrent Decoder.



### Differentially Private Imaging via Latent Space Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2103.05472v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2103.05472v2)
- **Published**: 2021-03-08 17:32:08+00:00
- **Updated**: 2021-04-07 06:27:26+00:00
- **Authors**: Tao Li, Chris Clifton
- **Comment**: None
- **Journal**: None
- **Summary**: There is growing concern about image privacy due to the popularity of social media and photo devices, along with increasing use of face recognition systems. However, established image de-identification techniques are either too subject to re-identification, produce photos that are insufficiently realistic, or both. To tackle this, we present a novel approach for image obfuscation by manipulating latent spaces of an unconditionally trained generative model that is able to synthesize photo-realistic facial images of high resolution. This manipulation is done in a way that satisfies the formal privacy standard of local differential privacy. To our knowledge, this is the first approach to image privacy that satisfies $\varepsilon$-differential privacy \emph{for the person.}



### Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models
- **Arxiv ID**: http://arxiv.org/abs/2103.04922v4
- **DOI**: 10.1109/TPAMI.2021.3116668
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T01 (Primary), 68T07 (Secondary), I.5.0; I.4.0; G.3
- **Links**: [PDF](http://arxiv.org/pdf/2103.04922v4)
- **Published**: 2021-03-08 17:34:03+00:00
- **Updated**: 2022-03-28 13:32:41+00:00
- **Authors**: Sam Bond-Taylor, Adam Leach, Yang Long, Chris G. Willcocks
- **Comment**: 20 pages, 9 figures, will appear in IEEE Transactions on Pattern
  Analysis and Machine Intelligence
- **Journal**: None
- **Summary**: Deep generative models are a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which make trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are compared and contrasted, explaining the premises behind each and how they are interrelated, while reviewing current state-of-the-art advances and implementations.



### F-CAD: A Framework to Explore Hardware Accelerators for Codec Avatar Decoding
- **Arxiv ID**: http://arxiv.org/abs/2103.04958v1
- **DOI**: None
- **Categories**: **cs.AR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.04958v1)
- **Published**: 2021-03-08 18:28:53+00:00
- **Updated**: 2021-03-08 18:28:53+00:00
- **Authors**: Xiaofan Zhang, Dawei Wang, Pierce Chuang, Shugao Ma, Deming Chen, Yuecheng Li
- **Comment**: Published as a conference paper at Design Automation Conference 2021
  (DAC'21)
- **Journal**: None
- **Summary**: Creating virtual avatars with realistic rendering is one of the most essential and challenging tasks to provide highly immersive virtual reality (VR) experiences. It requires not only sophisticated deep neural network (DNN) based codec avatar decoders to ensure high visual quality and precise motion expression, but also efficient hardware accelerators to guarantee smooth real-time rendering using lightweight edge devices, like untethered VR headsets. Existing hardware accelerators, however, fail to deliver sufficient performance and efficiency targeting such decoders which consist of multi-branch DNNs and require demanding compute and memory resources. To address these problems, we propose an automation framework, called F-CAD (Facebook Codec avatar Accelerator Design), to explore and deliver optimized hardware accelerators for codec avatar decoding. Novel technologies include 1) a new accelerator architecture to efficiently handle multi-branch DNNs; 2) a multi-branch dynamic design space to enable fine-grained architecture configurations; and 3) an efficient architecture search for picking the optimized hardware design based on both application-specific demands and hardware resource constraints. To the best of our knowledge, F-CAD is the first automation tool that supports the whole design flow of hardware acceleration of codec avatar decoders, allowing joint optimization on decoder designs in popular machine learning frameworks and corresponding customized accelerator design with cycle-accurate evaluation. Results show that the accelerators generated by F-CAD can deliver up to 122.1 frames per second (FPS) and 91.6% hardware efficiency when running the latest codec avatar decoder. Compared to the state-of-the-art designs, F-CAD achieves 4.0X and 2.8X higher throughput, 62.5% and 21.2% higher efficiency than DNNBuilder and HybridDNN by targeting the same hardware device.



### Deep Model Intellectual Property Protection via Deep Watermarking
- **Arxiv ID**: http://arxiv.org/abs/2103.04980v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2103.04980v1)
- **Published**: 2021-03-08 18:58:21+00:00
- **Updated**: 2021-03-08 18:58:21+00:00
- **Authors**: Jie Zhang, Dongdong Chen, Jing Liao, Weiming Zhang, Huamin Feng, Gang Hua, Nenghai Yu
- **Comment**: To appear at TPAMI 2021
- **Journal**: None
- **Summary**: Despite the tremendous success, deep neural networks are exposed to serious IP infringement risks. Given a target deep model, if the attacker knows its full information, it can be easily stolen by fine-tuning. Even if only its output is accessible, a surrogate model can be trained through student-teacher learning by generating many input-output training pairs. Therefore, deep model IP protection is important and necessary. However, it is still seriously under-researched. In this work, we propose a new model watermarking framework for protecting deep networks trained for low-level computer vision or image processing tasks. Specifically, a special task-agnostic barrier is added after the target model, which embeds a unified and invisible watermark into its outputs. When the attacker trains one surrogate model by using the input-output pairs of the barrier target model, the hidden watermark will be learned and extracted afterwards. To enable watermarks from binary bits to high-resolution images, a deep invisible watermarking mechanism is designed. By jointly training the target model and watermark embedding, the extra barrier can even be absorbed into the target model. Through extensive experiments, we demonstrate the robustness of the proposed framework, which can resist attacks with different network structures and objective functions.



### Advances in Inference and Representation for Simultaneous Localization and Mapping
- **Arxiv ID**: http://arxiv.org/abs/2103.05041v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2103.05041v1)
- **Published**: 2021-03-08 19:53:29+00:00
- **Updated**: 2021-03-08 19:53:29+00:00
- **Authors**: David M. Rosen, Kevin J. Doherty, Antonio Teran Espinoza, John J. Leonard
- **Comment**: 30 pages, 4 figures. To appear in Annual Review of Control, Robotics,
  and Autonomous Systems 2021
- **Journal**: None
- **Summary**: Simultaneous localization and mapping (SLAM) is the process of constructing a global model of an environment from local observations of it; this is a foundational capability for mobile robots, supporting such core functions as planning, navigation, and control. This article reviews recent progress in SLAM, focusing on advances in the expressive capacity of the environmental models used in SLAM systems (representation) and the performance of the algorithms used to estimate these models from data (inference). A prominent theme of recent SLAM research is the pursuit of environmental representations (including learned representations) that go beyond the classical attributes of geometry and appearance to model properties such as hierarchical organization, affordance, dynamics, and semantics; these advances equip autonomous agents with a more comprehensive understanding of the world, enabling more versatile and intelligent operation. A second major theme is a revitalized interest in the mathematical properties of the SLAM estimation problem itself (including its computational and information-theoretic performance limits); this work has led to the development of novel classes of certifiable and robust inference methods that dramatically improve the reliability of SLAM systems in real-world operation. We survey these advances with an emphasis on their ramifications for achieving robust, long-duration autonomy, and conclude with a discussion of open challenges and a perspective on future research directions.



### LCDNet: Deep Loop Closure Detection and Point Cloud Registration for LiDAR SLAM
- **Arxiv ID**: http://arxiv.org/abs/2103.05056v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05056v4)
- **Published**: 2021-03-08 20:19:37+00:00
- **Updated**: 2022-02-08 11:16:42+00:00
- **Authors**: Daniele Cattaneo, Matteo Vaghi, Abhinav Valada
- **Comment**: Accepted to IEEE Transactions on Robotics (T-RO), 2022
- **Journal**: None
- **Summary**: Loop closure detection is an essential component of Simultaneous Localization and Mapping (SLAM) systems, which reduces the drift accumulated over time. Over the years, several deep learning approaches have been proposed to address this task, however their performance has been subpar compared to handcrafted techniques, especially while dealing with reverse loops. In this paper, we introduce the novel LCDNet that effectively detects loop closures in LiDAR point clouds by simultaneously identifying previously visited places and estimating the 6-DoF relative transformation between the current scan and the map. LCDNet is composed of a shared encoder, a place recognition head that extracts global descriptors, and a relative pose head that estimates the transformation between two point clouds. We introduce a novel relative pose head based on the unbalanced optimal transport theory that we implement in a differentiable manner to allow for end-to-end training. Extensive evaluations of LCDNet on multiple real-world autonomous driving datasets show that our approach outperforms state-of-the-art loop closure detection and point cloud registration techniques by a large margin, especially while dealing with reverse loops. Moreover, we integrate our proposed loop closure detection approach into a LiDAR SLAM library to provide a complete mapping system and demonstrate the generalization ability using different sensor setup in an unseen city.



### Offboard 3D Object Detection from Point Cloud Sequences
- **Arxiv ID**: http://arxiv.org/abs/2103.05073v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05073v1)
- **Published**: 2021-03-08 21:02:37+00:00
- **Updated**: 2021-03-08 21:02:37+00:00
- **Authors**: Charles R. Qi, Yin Zhou, Mahyar Najibi, Pei Sun, Khoa Vo, Boyang Deng, Dragomir Anguelov
- **Comment**: 18 pages, 7 figures, 19 tables
- **Journal**: None
- **Summary**: While current 3D object recognition research mostly focuses on the real-time, onboard scenario, there are many offboard use cases of perception that are largely under-explored, such as using machines to automatically generate high-quality 3D labels. Existing 3D object detectors fail to satisfy the high-quality requirement for offboard uses due to the limited input and speed constraints. In this paper, we propose a novel offboard 3D object detection pipeline using point cloud sequence data. Observing that different frames capture complementary views of objects, we design the offboard detector to make use of the temporal points through both multi-frame object detection and novel object-centric refinement models. Evaluated on the Waymo Open Dataset, our pipeline named 3D Auto Labeling shows significant gains compared to the state-of-the-art onboard detectors and our offboard baselines. Its performance is even on par with human labels verified through a human label study. Further experiments demonstrate the application of auto labels for semi-supervised learning and provide extensive analysis to validate various design choices.



### How Privacy-Preserving are Line Clouds? Recovering Scene Details from 3D Lines
- **Arxiv ID**: http://arxiv.org/abs/2103.05086v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2103.05086v1)
- **Published**: 2021-03-08 21:32:43+00:00
- **Updated**: 2021-03-08 21:32:43+00:00
- **Authors**: Kunal Chelani, Fredrik Kahl, Torsten Sattler
- **Comment**: Computer Vision and Pattern Recognition (CVPR) 2021
- **Journal**: None
- **Summary**: Visual localization is the problem of estimating the camera pose of a given image with respect to a known scene. Visual localization algorithms are a fundamental building block in advanced computer vision applications, including Mixed and Virtual Reality systems. Many algorithms used in practice represent the scene through a Structure-from-Motion (SfM) point cloud and use 2D-3D matches between a query image and the 3D points for camera pose estimation. As recently shown, image details can be accurately recovered from SfM point clouds by translating renderings of the sparse point clouds to images. To address the resulting potential privacy risks for user-generated content, it was recently proposed to lift point clouds to line clouds by replacing 3D points by randomly oriented 3D lines passing through these points. The resulting representation is unintelligible to humans and effectively prevents point cloud-to-image translation. This paper shows that a significant amount of information about the 3D scene geometry is preserved in these line clouds, allowing us to (approximately) recover the 3D point positions and thus to (approximately) recover image content. Our approach is based on the observation that the closest points between lines can yield a good approximation to the original 3D points. Code is available at https://github.com/kunalchelani/Line2Point.



### CovidGAN: Data Augmentation Using Auxiliary Classifier GAN for Improved Covid-19 Detection
- **Arxiv ID**: http://arxiv.org/abs/2103.05094v1
- **DOI**: 10.1109/ACCESS.2020.2994762
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2103.05094v1)
- **Published**: 2021-03-08 21:53:29+00:00
- **Updated**: 2021-03-08 21:53:29+00:00
- **Authors**: Abdul Waheed, Muskan Goyal, Deepak Gupta, Ashish Khanna, Fadi Al-Turjman, Placido Rogerio Pinheiro
- **Comment**: Accepted at IEEE Access. Received April 30, 2020, accepted May 11,
  2020, date of publication May 14, 2020, date of current version May 28, 2020
- **Journal**: IEEE Access, vol. 8, pp. 91916-91923, 2020
- **Summary**: Coronavirus (COVID-19) is a viral disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The spread of COVID-19 seems to have a detrimental effect on the global economy and health. A positive chest X-ray of infected patients is a crucial step in the battle against COVID-19. Early results suggest that abnormalities exist in chest X-rays of patients suggestive of COVID-19. This has led to the introduction of a variety of deep learning systems and studies have shown that the accuracy of COVID-19 patient detection through the use of chest X-rays is strongly optimistic. Deep learning networks like convolutional neural networks (CNNs) need a substantial amount of training data. Because the outbreak is recent, it is difficult to gather a significant number of radiographic images in such a short time. Therefore, in this research, we present a method to generate synthetic chest X-ray (CXR) images by developing an Auxiliary Classifier Generative Adversarial Network (ACGAN) based model called CovidGAN. In addition, we demonstrate that the synthetic images produced from CovidGAN can be utilized to enhance the performance of CNN for COVID-19 detection. Classification using CNN alone yielded 85% accuracy. By adding synthetic images produced by CovidGAN, the accuracy increased to 95%. We hope this method will speed up COVID-19 detection and lead to more robust systems of radiology.



### ASL to PET Translation by a Semi-supervised Residual-based Attention-guided Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2103.05116v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2103.05116v1)
- **Published**: 2021-03-08 22:06:02+00:00
- **Updated**: 2021-03-08 22:06:02+00:00
- **Authors**: Sahar Yousefi, Hessam Sokooti, Wouter M. Teeuwisse, Dennis F. R. Heijtel, Aart J. Nederveen, Marius Staring, Matthias J. P. van Osch
- **Comment**: 11 pages, 4 tables, 4 figures
- **Journal**: None
- **Summary**: Positron Emission Tomography (PET) is an imaging method that can assess physiological function rather than structural disturbances by measuring cerebral perfusion or glucose consumption. However, this imaging technique relies on injection of radioactive tracers and is expensive. On the contrary, Arterial Spin Labeling (ASL) MRI is a non-invasive, non-radioactive, and relatively cheap imaging technique for brain hemodynamic measurements, which allows quantification to some extent. In this paper we propose a convolutional neural network (CNN) based model for translating ASL to PET images, which could benefit patients as well as the healthcare system in terms of expenses and adverse side effects. However, acquiring a sufficient number of paired ASL-PET scans for training a CNN is prohibitive for many reasons. To tackle this problem, we present a new semi-supervised multitask CNN which is trained on both paired data, i.e. ASL and PET scans, and unpaired data, i.e. only ASL scans, which alleviates the problem of training a network on limited paired data. Moreover, we present a new residual-based-attention guided mechanism to improve the contextual features during the training process. Also, we show that incorporating T1-weighted scans as an input, due to its high resolution and availability of anatomical information, improves the results. We performed a two-stage evaluation based on quantitative image metrics by conducting a 7-fold cross validation followed by a double-blind observer study. The proposed network achieved structural similarity index measure (SSIM), mean squared error (MSE) and peak signal-to-noise ratio (PSNR) values of $0.85\pm0.08$, $0.01\pm0.01$, and $21.8\pm4.5$ respectively, for translating from 2D ASL and T1-weighted images to PET data. The proposed model is publicly available via https://github.com/yousefis/ASL2PET.



### Multiple Instance Captioning: Learning Representations from Histopathology Textbooks and Articles
- **Arxiv ID**: http://arxiv.org/abs/2103.05121v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.05121v1)
- **Published**: 2021-03-08 22:18:36+00:00
- **Updated**: 2021-03-08 22:18:36+00:00
- **Authors**: Jevgenij Gamper, Nasir Rajpoot
- **Comment**: Accepted at CVPR 2021
- **Journal**: None
- **Summary**: We present ARCH, a computational pathology (CP) multiple instance captioning dataset to facilitate dense supervision of CP tasks. Existing CP datasets focus on narrow tasks; ARCH on the other hand contains dense diagnostic and morphological descriptions for a range of stains, tissue types and pathologies. Using intrinsic dimensionality estimation, we show that ARCH is the only CP dataset to (ARCH-)rival its computer vision analog MS-COCO Captions. We conjecture that an encoder pre-trained on dense image captions learns transferable representations for most CP tasks. We support the conjecture with evidence that ARCH representation transfers to a variety of pathology sub-tasks better than ImageNet features or representations obtained via self-supervised or multi-task learning on pathology images alone. We release our best model and invite other researchers to test it on their CP tasks.



### Contemplating real-world object classification
- **Arxiv ID**: http://arxiv.org/abs/2103.05137v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2103.05137v2)
- **Published**: 2021-03-08 23:29:59+00:00
- **Updated**: 2021-03-27 18:50:02+00:00
- **Authors**: Ali Borji
- **Comment**: to appear in iclr 2021
- **Journal**: None
- **Summary**: Deep object recognition models have been very successful over benchmark datasets such as ImageNet. How accurate and robust are they to distribution shifts arising from natural and synthetic variations in datasets? Prior research on this problem has primarily focused on ImageNet variations (e.g., ImageNetV2, ImageNet-A). To avoid potential inherited biases in these studies, we take a different approach. Specifically, we reanalyze the ObjectNet dataset recently proposed by Barbu et al. containing objects in daily life situations. They showed a dramatic performance drop of the state of the art object recognition models on this dataset. Due to the importance and implications of their results regarding the generalization ability of deep models, we take a second look at their analysis. We find that applying deep models to the isolated objects, rather than the entire scene as is done in the original paper, results in around 20-30% performance improvement. Relative to the numbers reported in Barbu et al., around 10-15% of the performance loss is recovered, without any test time data augmentation. Despite this gain, however, we conclude that deep models still suffer drastically on the ObjectNet dataset. We also investigate the robustness of models against synthetic image perturbations such as geometric transformations (e.g., scale, rotation, translation), natural image distortions (e.g., impulse noise, blur) as well as adversarial attacks (e.g., FGSM and PGD-5). Our results indicate that limiting the object area as much as possible (i.e., from the entire image to the bounding box to the segmentation mask) leads to consistent improvement in accuracy and robustness.



