# Arxiv Papers in cs.CV on 2021-05-24
### Deep Visual Anomaly detection with Negative Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.11058v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11058v1)
- **Published**: 2021-05-24 01:48:44+00:00
- **Updated**: 2021-05-24 01:48:44+00:00
- **Authors**: Jin-Ha Lee, Marcella Astrid, Muhammad Zaigham Zaheer, Seung-Ik Lee
- **Comment**: None
- **Journal**: None
- **Summary**: With the increase in the learning capability of deep convolution-based architectures, various applications of such models have been proposed over time. In the field of anomaly detection, improvements in deep learning opened new prospects of exploration for the researchers whom tried to automate the labor-intensive features of data collection. First, in terms of data collection, it is impossible to anticipate all the anomalies that might exist in a given environment. Second, assuming we limit the possibilities of anomalies, it will still be hard to record all these scenarios for the sake of training a model. Third, even if we manage to record a significant amount of abnormal data, it's laborious to annotate this data on pixel or even frame level. Various approaches address the problem by proposing one-class classification using generative models trained on only normal data. In such methods, only the normal data is used, which is abundantly available and doesn't require significant human input. However, these are trained with only normal data and at the test time, given abnormal data as input, may often generate normal-looking output. This happens due to the hallucination characteristic of generative models. Next, these systems are designed to not use abnormal examples during the training. In this paper, we propose anomaly detection with negative learning (ADNL), which employs the negative learning concept for the enhancement of anomaly detection by utilizing a very small number of labeled anomaly data as compared with the normal data during training. The idea is to limit the reconstruction capability of a generative model using the given a small amount of anomaly examples. This way, the network not only learns to reconstruct normal data but also encloses the normal distribution far from the possible distribution of anomalies.



### High-level camera-LiDAR fusion for 3D object detection with machine learning
- **Arxiv ID**: http://arxiv.org/abs/2105.11060v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.11060v1)
- **Published**: 2021-05-24 01:57:34+00:00
- **Updated**: 2021-05-24 01:57:34+00:00
- **Authors**: Gustavo A. Salazar-Gomez, Miguel A. Saavedra-Ruiz, Victor A. Romero-Cano
- **Comment**: LatinX Workshop at CVPR 2021
- **Journal**: None
- **Summary**: This paper tackles the 3D object detection problem, which is of vital importance for applications such as autonomous driving. Our framework uses a Machine Learning (ML) pipeline on a combination of monocular camera and LiDAR data to detect vehicles in the surrounding 3D space of a moving platform. It uses frustum region proposals generated by State-Of-The-Art (SOTA) 2D object detectors to segment LiDAR point clouds into point clusters which represent potentially individual objects. We evaluate the performance of classical ML algorithms as part of an holistic pipeline for estimating the parameters of 3D bounding boxes which surround the vehicles around the moving platform. Our results demonstrate an efficient and accurate inference on a validation set, achieving an overall accuracy of 87.1%.



### Taylor saves for later: disentanglement for video prediction using Taylor representation
- **Arxiv ID**: http://arxiv.org/abs/2105.11062v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.11062v1)
- **Published**: 2021-05-24 01:59:21+00:00
- **Updated**: 2021-05-24 01:59:21+00:00
- **Authors**: Ting Pan, Zhuqing Jiang, Jianan Han, Shiping Wen, Aidong Men, Haiying Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Video prediction is a challenging task with wide application prospects in meteorology and robot systems. Existing works fail to trade off short-term and long-term prediction performances and extract robust latent dynamics laws in video frames. We propose a two-branch seq-to-seq deep model to disentangle the Taylor feature and the residual feature in video frames by a novel recurrent prediction module (TaylorCell) and residual module. TaylorCell can expand the video frames' high-dimensional features into the finite Taylor series to describe the latent laws. In TaylorCell, we propose the Taylor prediction unit (TPU) and the memory correction unit (MCU). TPU employs the first input frame's derivative information to predict the future frames, avoiding error accumulation. MCU distills all past frames' information to correct the predicted Taylor feature from TPU. Correspondingly, the residual module extracts the residual feature complementary to the Taylor feature. On three generalist datasets (Moving MNIST, TaxiBJ, Human 3.6), our model outperforms or reaches state-of-the-art models, and ablation experiments demonstrate the effectiveness of our model in long-term prediction.



### Recent Advances and Trends in Multimodal Deep Learning: A Review
- **Arxiv ID**: http://arxiv.org/abs/2105.11087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11087v1)
- **Published**: 2021-05-24 04:20:45+00:00
- **Updated**: 2021-05-24 04:20:45+00:00
- **Authors**: Jabeen Summaira, Xi Li, Amin Muhammad Shoib, Songyuan Li, Jabbar Abdul
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Learning has implemented a wide range of applications and has become increasingly popular in recent years. The goal of multimodal deep learning is to create models that can process and link information using various modalities. Despite the extensive development made for unimodal learning, it still cannot cover all the aspects of human learning. Multimodal learning helps to understand and analyze better when various senses are engaged in the processing of information. This paper focuses on multiple types of modalities, i.e., image, video, text, audio, body gestures, facial expressions, and physiological signals. Detailed analysis of past and current baseline approaches and an in-depth study of recent advancements in multimodal deep learning applications has been provided. A fine-grained taxonomy of various multimodal deep learning applications is proposed, elaborating on different applications in more depth. Architectures and datasets used in these applications are also discussed, along with their evaluation metrics. Last, main issues are highlighted separately for each domain along with their possible future research directions.



### Towards Book Cover Design via Layout Graphs
- **Arxiv ID**: http://arxiv.org/abs/2105.11088v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11088v2)
- **Published**: 2021-05-24 04:28:35+00:00
- **Updated**: 2021-06-15 09:14:08+00:00
- **Authors**: Wensheng Zhang, Yan Zheng, Taiga Miyazono, Seiichi Uchida, Brian Kenji Iwana
- **Comment**: Accepted at ICDAR2021
- **Journal**: None
- **Summary**: Book covers are intentionally designed and provide an introduction to a book. However, they typically require professional skills to design and produce the cover images. Thus, we propose a generative neural network that can produce book covers based on an easy-to-use layout graph. The layout graph contains objects such as text, natural scene objects, and solid color spaces. This layout graph is embedded using a graph convolutional neural network and then used with a mask proposal generator and a bounding-box generator and filled using an object proposal generator. Next, the objects are compiled into a single image and the entire network is trained using a combination of adversarial training, perceptual training, and reconstruction. Finally, a Style Retention Network (SRNet) is used to transfer the learned font style onto the desired text. Using the proposed method allows for easily controlled and unique book covers.



### FineAction: A Fine-Grained Video Dataset for Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2105.11107v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11107v3)
- **Published**: 2021-05-24 06:06:32+00:00
- **Updated**: 2022-10-20 10:30:59+00:00
- **Authors**: Yi Liu, Limin Wang, Yali Wang, Xiao Ma, Yu Qiao
- **Comment**: Accepted by IEEE T-IP. HomePage:
  https://deeperaction.github.io/datasets/fineaction
- **Journal**: None
- **Summary**: Temporal action localization (TAL) is an important and challenging problem in video understanding. However, most existing TAL benchmarks are built upon the coarse granularity of action classes, which exhibits two major limitations in this task. First, coarse-level actions can make the localization models overfit in high-level context information, and ignore the atomic action details in the video. Second, the coarse action classes often lead to the ambiguous annotations of temporal boundaries, which are inappropriate for temporal action localization. To tackle these problems, we develop a novel large-scale and fine-grained video dataset, coined as FineAction, for temporal action localization. In total, FineAction contains 103K temporal instances of 106 action categories, annotated in 17K untrimmed videos. Compared to the existing TAL datasets, our FineAction takes distinct characteristics of fine action classes with rich diversity, dense annotations of multiple instances, and co-occurring actions of different classes, which introduces new opportunities and challenges for temporal action localization. To benchmark FineAction, we systematically investigate the performance of several popular temporal localization methods on it, and deeply analyze the influence of fine-grained instances in temporal action localization. As a minor contribution, we present a simple baseline approach for handling the fine-grained action detection, which achieves an mAP of 13.17% on our FineAction. We believe that FineAction can advance research of temporal action localization and beyond.



### Oriented RepPoints for Aerial Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.11111v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11111v4)
- **Published**: 2021-05-24 06:18:23+00:00
- **Updated**: 2022-03-24 14:17:16+00:00
- **Authors**: Wentong Li, Yijie Chen, Kaixuan Hu, Jianke Zhu
- **Comment**: 10 pages, 4 figures, Accepted by CVPR2022
- **Journal**: None
- **Summary**: In contrast to the generic object, aerial targets are often non-axis aligned with arbitrary orientations having the cluttered surroundings. Unlike the mainstreamed approaches regressing the bounding box orientations, this paper proposes an effective adaptive points learning approach to aerial object detection by taking advantage of the adaptive points representation, which is able to capture the geometric information of the arbitrary-oriented instances. To this end, three oriented conversion functions are presented to facilitate the classification and localization with accurate orientation. Moreover, we propose an effective quality assessment and sample assignment scheme for adaptive points learning toward choosing the representative oriented reppoints samples during training, which is able to capture the non-axis aligned features from adjacent objects or background noises. A spatial constraint is introduced to penalize the outlier points for roust adaptive learning. Experimental results on four challenging aerial datasets including DOTA, HRSC2016, UCAS-AOD and DIOR-R, demonstrate the efficacy of our proposed approach. The source code is availabel at: https://github.com/LiWentomng/OrientedRepPoints.



### Dynamic Class Queue for Large Scale Face Recognition In the Wild
- **Arxiv ID**: http://arxiv.org/abs/2105.11113v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11113v1)
- **Published**: 2021-05-24 06:31:10+00:00
- **Updated**: 2021-05-24 06:31:10+00:00
- **Authors**: Bi Li, Teng Xi, Gang Zhang, Haocheng Feng, Junyu Han, Jingtuo Liu, Errui Ding, Wenyu Liu
- **Comment**: Accepted in CVPR 2021
- **Journal**: None
- **Summary**: Learning discriminative representation using large-scale face datasets in the wild is crucial for real-world applications, yet it remains challenging. The difficulties lie in many aspects and this work focus on computing resource constraint and long-tailed class distribution. Recently, classification-based representation learning with deep neural networks and well-designed losses have demonstrated good recognition performance. However, the computing and memory cost linearly scales up to the number of identities (classes) in the training set, and the learning process suffers from unbalanced classes. In this work, we propose a dynamic class queue (DCQ) to tackle these two problems. Specifically, for each iteration during training, a subset of classes for recognition are dynamically selected and their class weights are dynamically generated on-the-fly which are stored in a queue. Since only a subset of classes is selected for each iteration, the computing requirement is reduced. By using a single server without model parallel, we empirically verify in large-scale datasets that 10% of classes are sufficient to achieve similar performance as using all classes. Moreover, the class weights are dynamically generated in a few-shot manner and therefore suitable for tail classes with only a few instances. We show clear improvement over a strong baseline in the largest public dataset Megaface Challenge2 (MF2) which has 672K identities and over 88% of them have less than 10 instances. Code is available at https://github.com/bilylee/DCQ



### A Fourier-based Framework for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2105.11120v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11120v1)
- **Published**: 2021-05-24 06:50:30+00:00
- **Updated**: 2021-05-24 06:50:30+00:00
- **Authors**: Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, Qi Tian
- **Comment**: Accepted as CVPR 2021 oral
- **Journal**: None
- **Summary**: Modern deep neural networks suffer from performance degradation when evaluated on testing data under different distributions from training data. Domain generalization aims at tackling this problem by learning transferable knowledge from multiple source domains in order to generalize to unseen target domains. This paper introduces a novel Fourier-based perspective for domain generalization. The main assumption is that the Fourier phase information contains high-level semantics and is not easily affected by domain shifts. To force the model to capture phase information, we develop a novel Fourier-based data augmentation strategy called amplitude mix which linearly interpolates between the amplitude spectrums of two images. A dual-formed consistency loss called co-teacher regularization is further introduced between the predictions induced from original and augmented images. Extensive experiments on three benchmarks have demonstrated that the proposed method is able to achieve state-of-the-arts performance for domain generalization.



### Unsupervised Video Summarization with a Convolutional Attentive Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2105.11131v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2105.11131v1)
- **Published**: 2021-05-24 07:24:39+00:00
- **Updated**: 2021-05-24 07:24:39+00:00
- **Authors**: Guoqiang Liang, Yanbing Lv, Shucheng Li, Shizhou Zhang, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: With the explosive growth of video data, video summarization, which attempts to seek the minimum subset of frames while still conveying the main story, has become one of the hottest topics. Nowadays, substantial achievements have been made by supervised learning techniques, especially after the emergence of deep learning. However, it is extremely expensive and difficult to collect human annotation for large-scale video datasets. To address this problem, we propose a convolutional attentive adversarial network (CAAN), whose key idea is to build a deep summarizer in an unsupervised way. Upon the generative adversarial network, our overall framework consists of a generator and a discriminator. The former predicts importance scores for all frames of a video while the latter tries to distinguish the score-weighted frame features from original frame features. Specifically, the generator employs a fully convolutional sequence network to extract global representation of a video, and an attention-based network to output normalized importance scores. To learn the parameters, our objective function is composed of three loss functions, which can guide the frame-level importance score prediction collaboratively. To validate this proposed method, we have conducted extensive experiments on two public benchmarks SumMe and TVSum. The results show the superiority of our proposed method against other state-of-the-art unsupervised approaches. Our method even outperforms some published supervised approaches.



### CFA-Net: Controllable Face Anonymization Network with Identity Representation Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2105.11137v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11137v2)
- **Published**: 2021-05-24 07:39:54+00:00
- **Updated**: 2021-10-10 08:21:18+00:00
- **Authors**: Tianxiang Ma, Dongze Li, Wei Wang, Jing Dong
- **Comment**: None
- **Journal**: None
- **Summary**: De-identification of face data has drawn increasing attention in recent years. It is important to protect people's identities meanwhile keeping the utility of the data in many computer vision tasks. We propose a Controllable Face Anonymization Network (CFA-Net), a novel approach that can anonymize the identity of given faces in images and videos, based on a generator that can disentangle face identity from other image contents. We reach the goal of controllable face anonymization through manipulating identity vectors in the generator's identity representation space. Various anonymized faces deriving from an original face can be generated through our method and maintain high similarity to the original image contents. Quantitative and qualitative results demonstrate our method's superiority over literature models on visual quality and anonymization validity.



### Out-of-Distribution Detection in Dermatology using Input Perturbation and Subset Scanning
- **Arxiv ID**: http://arxiv.org/abs/2105.11160v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.11160v3)
- **Published**: 2021-05-24 09:04:47+00:00
- **Updated**: 2021-06-02 07:40:54+00:00
- **Authors**: Hannah Kim, Girmaw Abebe Tadesse, Celia Cintas, Skyler Speakman, Kush Varshney
- **Comment**: Under review for 6th Outlier Detection & Description Workshop
- **Journal**: None
- **Summary**: Recent advances in deep learning have led to breakthroughs in the development of automated skin disease classification. As we observe an increasing interest in these models in the dermatology space, it is crucial to address aspects such as the robustness towards input data distribution shifts. Current skin disease models could make incorrect inferences for test samples from different hardware devices and clinical settings or unknown disease samples, which are out-of-distribution (OOD) from the training samples. To this end, we propose a simple yet effective approach that detect these OOD samples prior to making any decision. The detection is performed via scanning in the latent space representation (e.g., activations of the inner layers of any pre-trained skin disease classifier). The input samples could also perturbed to maximise divergence of OOD samples. We validate our ODD detection approach in two use cases: 1) identify samples collected from different protocols, and 2) detect samples from unknown disease classes. Additionally, we evaluate the performance of the proposed approach and compare it with other state-of-the-art methods. Furthermore, data-driven dermatology applications may deepen the disparity in clinical care across racial and ethnic groups since most datasets are reported to suffer from bias in skin tone distribution. Therefore, we also evaluate the fairness of these OOD detection methods across different skin tones. Our experiments resulted in competitive performance across multiple datasets in detecting OOD samples, which could be used (in the future) to design more effective transfer learning techniques prior to inferring on these samples.



### AirNet: Neural Network Transmission over the Air
- **Arxiv ID**: http://arxiv.org/abs/2105.11166v6
- **DOI**: None
- **Categories**: **cs.NI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.11166v6)
- **Published**: 2021-05-24 09:16:04+00:00
- **Updated**: 2023-07-19 19:32:53+00:00
- **Authors**: Mikolaj Jankowski, Deniz Gunduz, Krystian Mikolajczyk
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art performance for many edge applications is achieved by deep neural networks (DNNs). Often, these DNNs are location- and time-sensitive, and must be delivered over a wireless channel rapidly and efficiently. In this paper, we introduce AirNet, a family of novel training and transmission methods that allow DNNs to be efficiently delivered over wireless channels under stringent transmit power and latency constraints. This corresponds to a new class of joint source-channel coding problems, aimed at delivering DNNs with the goal of maximizing their accuracy at the receiver, rather than recovering them with high fidelity. In AirNet, we propose the direct mapping of the DNN parameters to transmitted channel symbols, while the network is trained to meet the channel constraints, and exhibit robustness against channel noise. AirNet achieves higher accuracy compared to separation-based alternatives. We further improve the performance of AirNet by pruning the network below the available bandwidth, and expanding it for improved robustness. We also benefit from unequal error protection by selectively expanding important layers of the network. Finally, we develop an approach, which simultaneously trains a spectrum of DNNs, each targeting a different channel condition, resolving the impractical memory requirements of training distinct networks for different channel conditions.



### Human-centric Relation Segmentation: Dataset and Solution
- **Arxiv ID**: http://arxiv.org/abs/2105.11168v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11168v2)
- **Published**: 2021-05-24 09:20:37+00:00
- **Updated**: 2021-05-25 12:53:03+00:00
- **Authors**: Si Liu, Zitian Wang, Yulu Gao, Lejian Ren, Yue Liao, Guanghui Ren, Bo Li, Shuicheng Yan
- **Comment**: Accepted by TPAMI 2021
- **Journal**: None
- **Summary**: Vision and language understanding techniques have achieved remarkable progress, but currently it is still difficult to well handle problems involving very fine-grained details. For example, when the robot is told to "bring me the book in the girl's left hand", most existing methods would fail if the girl holds one book respectively in her left and right hand. In this work, we introduce a new task named human-centric relation segmentation (HRS), as a fine-grained case of HOI-det. HRS aims to predict the relations between the human and surrounding entities and identify the relation-correlated human parts, which are represented as pixel-level masks. For the above exemplar case, our HRS task produces results in the form of relation triplets <girl [left hand], hold, book> and exacts segmentation masks of the book, with which the robot can easily accomplish the grabbing task. Correspondingly, we collect a new Person In Context (PIC) dataset for this new task, which contains 17,122 high-resolution images and densely annotated entity segmentation and relations, including 141 object categories, 23 relation categories and 25 semantic human parts. We also propose a Simultaneous Matching and Segmentation (SMS) framework as a solution to the HRS task. I Outputs of the three branches are fused to produce the final HRS results. Extensive experiments on PIC and V-COCO datasets show that the proposed SMS method outperforms baselines with the 36 FPS inference speed.



### Smart mobile microscopy: towards fully-automated digitization
- **Arxiv ID**: http://arxiv.org/abs/2105.11179v1
- **DOI**: 10.1007/978-3-030-89880-9_46
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.11179v1)
- **Published**: 2021-05-24 09:55:29+00:00
- **Updated**: 2021-05-24 09:55:29+00:00
- **Authors**: A. Kornilova, I. Kirilenko, D. Iarosh, V. Kutuev, M. Strutovsky
- **Comment**: None
- **Journal**: None
- **Summary**: Mobile microscopy is a newly formed field that emerged from a combination of optical microscopy capabilities and spread, functionality, and ever-increasing computing resources of mobile devices. Despite the idea of creating a system that would successfully merge a microscope, numerous computer vision methods, and a mobile device is regularly examined, the resulting implementations still require the presence of a qualified operator to control specimen digitization. In this paper, we address the task of surpassing this constraint and present a ``smart'' mobile microscope concept aimed at automatic digitization of the most valuable visual information about the specimen. We perform this through combining automated microscope setup control and classic techniques such as auto-focusing, in-focus filtering, and focus-stacking -- adapted and optimized as parts of a mobile cross-platform library.



### Pulmonary embolism identification in computerized tomography pulmonary angiography scans with deep learning technologies in COVID-19 patients
- **Arxiv ID**: http://arxiv.org/abs/2105.11187v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.11187v3)
- **Published**: 2021-05-24 10:23:21+00:00
- **Updated**: 2021-05-28 06:05:15+00:00
- **Authors**: Chairi Kiourt, Georgios Feretzakis, Konstantinos Dalamarinis, Dimitris Kalles, Georgios Pantos, Ioannis Papadopoulos, Spyros Kouris, George Ioannakis, Evangelos Loupelis, Petros Antonopoulos, Aikaterini Sakagianni
- **Comment**: 16 pages, 6 figures, 1 table, Submitted to the European Radiology
  journal of Springer
- **Journal**: None
- **Summary**: The main objective of this work is to utilize state-of-the-art deep learning approaches for the identification of pulmonary embolism in CTPA-Scans for COVID-19 patients, provide an initial assessment of their performance and, ultimately, provide a fast-track prototype solution (system). We adopted and assessed some of the most popular convolutional neural network architectures through transfer learning approaches, to strive to combine good model accuracy with fast training. Additionally, we exploited one of the most popular one-stage object detection models for the localization (through object detection) of the pulmonary embolism regions-of-interests. The models of both approaches are trained on an original CTPA-Scan dataset, where we annotated of 673 CTPA-Scan images with 1,465 bounding boxes in total, highlighting pulmonary embolism regions-of-interests. We provide a brief assessment of some state-of-the-art image classification models by achieving validation accuracies of 91% in pulmonary embolism classification. Additionally, we achieved a precision of about 68% on average in the object detection model for the pulmonary embolism localization under 50% IoU threshold. For both approaches, we provide the entire training pipelines for future studies (step by step processes through source code). In this study, we present some of the most accurate and fast deep learning models for pulmonary embolism identification in CTPA-Scans images, through classification and localization (object detection) approaches for patients infected by COVID-19. We provide a fast-track solution (system) for the research community of the area, which combines both classification and object detection models for improving the precision of identifying pulmonary embolisms.



### Mapping oil palm density at country scale: An active learning approach
- **Arxiv ID**: http://arxiv.org/abs/2105.11207v1
- **DOI**: 10.1016/j.rse.2021.112479
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.11207v1)
- **Published**: 2021-05-24 11:23:55+00:00
- **Updated**: 2021-05-24 11:23:55+00:00
- **Authors**: Andrés C. Rodríguez, Stefano D'Aronco, Konrad Schindler, Jan D. Wegner
- **Comment**: None
- **Journal**: Remote Sensing of Environment Volume 261, August 2021, 112479
- **Summary**: Accurate mapping of oil palm is important for understanding its past and future impact on the environment. We propose to map and count oil palms by estimating tree densities per pixel for large-scale analysis. This allows for fine-grained analysis, for example regarding different planting patterns. To that end, we propose a new, active deep learning method to estimate oil palm density at large scale from Sentinel-2 satellite images, and apply it to generate complete maps for Malaysia and Indonesia. What makes the regression of oil palm density challenging is the need for representative reference data that covers all relevant geographical conditions across a large territory. Specifically for density estimation, generating reference data involves counting individual trees. To keep the associated labelling effort low we propose an active learning (AL) approach that automatically chooses the most relevant samples to be labelled. Our method relies on estimates of the epistemic model uncertainty and of the diversity among samples, making it possible to retrieve an entire batch of relevant samples in a single iteration. Moreover, our algorithm has linear computational complexity and is easily parallelisable to cover large areas. We use our method to compute the first oil palm density map with $10\,$m Ground Sampling Distance (GSD) , for all of Indonesia and Malaysia and for two different years, 2017 and 2019. The maps have a mean absolute error of $\pm$7.3 trees/$ha$, estimated from an independent validation set. We also analyse density variations between different states within a country and compare them to official estimates. According to our estimates there are, in total, $>1.2$ billion oil palms in Indonesia covering $>$15 million $ha$, and $>0.5$ billion oil palms in Malaysia covering $>6$ million $ha$.



### Continual Learning at the Edge: Real-Time Training on Smartphone Devices
- **Arxiv ID**: http://arxiv.org/abs/2105.13127v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.13127v1)
- **Published**: 2021-05-24 12:00:31+00:00
- **Updated**: 2021-05-24 12:00:31+00:00
- **Authors**: Lorenzo Pellegrini, Vincenzo Lomonaco, Gabriele Graffieti, Davide Maltoni
- **Comment**: 6 pages, 2 figures, 1 table
- **Journal**: None
- **Summary**: On-device training for personalized learning is a challenging research problem. Being able to quickly adapt deep prediction models at the edge is necessary to better suit personal user needs. However, adaptation on the edge poses some questions on both the efficiency and sustainability of the learning process and on the ability to work under shifting data distributions. Indeed, naively fine-tuning a prediction model only on the newly available data results in catastrophic forgetting, a sudden erasure of previously acquired knowledge. In this paper, we detail the implementation and deployment of a hybrid continual learning strategy (AR1*) on a native Android application for real-time on-device personalization without forgetting. Our benchmark, based on an extension of the CORe50 dataset, shows the efficiency and effectiveness of our solution.



### Towards Compact CNNs via Collaborative Compression
- **Arxiv ID**: http://arxiv.org/abs/2105.11228v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.11228v1)
- **Published**: 2021-05-24 12:07:38+00:00
- **Updated**: 2021-05-24 12:07:38+00:00
- **Authors**: Yuchao Li, Shaohui Lin, Jianzhuang Liu, Qixiang Ye, Mengdi Wang, Fei Chao, Fan Yang, Jincheng Ma, Qi Tian, Rongrong Ji
- **Comment**: This paper is published in CVPR 2021
- **Journal**: None
- **Summary**: Channel pruning and tensor decomposition have received extensive attention in convolutional neural network compression. However, these two techniques are traditionally deployed in an isolated manner, leading to significant accuracy drop when pursuing high compression rates. In this paper, we propose a Collaborative Compression (CC) scheme, which joints channel pruning and tensor decomposition to compress CNN models by simultaneously learning the model sparsity and low-rankness. Specifically, we first investigate the compression sensitivity of each layer in the network, and then propose a Global Compression Rate Optimization that transforms the decision problem of compression rate into an optimization problem. After that, we propose multi-step heuristic compression to remove redundant compression units step-by-step, which fully considers the effect of the remaining compression space (i.e., unremoved compression units). Our method demonstrates superior performance gains over previous ones on various datasets and backbone architectures. For example, we achieve 52.9% FLOPs reduction by removing 48.4% parameters on ResNet-50 with only a Top-1 accuracy drop of 0.56% on ImageNet 2012.



### SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2105.11237v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11237v4)
- **Published**: 2021-05-24 12:21:25+00:00
- **Updated**: 2021-07-19 11:31:23+00:00
- **Authors**: Jinlong Peng, Zhengkai Jiang, Yueyang Gu, Yang Wu, Yabiao Wang, Ying Tai, Chengjie Wang, Weiyao Lin
- **Comment**: The 30th International Joint Conference on Artificial Intelligence
  (IJCAI 2021)
- **Journal**: None
- **Summary**: Recently, most siamese network based trackers locate targets via object classification and bounding-box regression. Generally, they select the bounding-box with maximum classification confidence as the final prediction. This strategy may miss the right result due to the accuracy misalignment between classification and regression. In this paper, we propose a novel siamese tracking algorithm called SiamRCR, addressing this problem with a simple, light and effective solution. It builds reciprocal links between classification and regression branches, which can dynamically re-weight their losses for each positive sample. In addition, we add a localization branch to predict the localization accuracy, so that it can work as the replacement of the regression assistance link during inference. This branch makes the training and inference more consistent. Extensive experimental results demonstrate the effectiveness of SiamRCR and its superiority over the state-of-the-art competitors on GOT-10k, LaSOT, TrackingNet, OTB-2015, VOT-2018 and VOT-2019. Moreover, our SiamRCR runs at 65 FPS, far above the real-time requirement.



### A self-supervised learning strategy for postoperative brain cavity segmentation simulating resections
- **Arxiv ID**: http://arxiv.org/abs/2105.11239v1
- **DOI**: 10.1007/s11548-021-02420-2
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.11239v1)
- **Published**: 2021-05-24 12:27:06+00:00
- **Updated**: 2021-05-24 12:27:06+00:00
- **Authors**: Fernando Pérez-García, Reuben Dorent, Michele Rizzi, Francesco Cardinale, Valerio Frazzini, Vincent Navarro, Caroline Essert, Irène Ollivier, Tom Vercauteren, Rachel Sparks, John S. Duncan, Sébastien Ourselin
- **Comment**: To be published in the International Journal of Computer Assisted
  Radiology and Surgery (IJCARS) - Special issue MICCAI 2020
- **Journal**: None
- **Summary**: Accurate segmentation of brain resection cavities (RCs) aids in postoperative analysis and determining follow-up treatment. Convolutional neural networks (CNNs) are the state-of-the-art image segmentation technique, but require large annotated datasets for training. Annotation of 3D medical images is time-consuming, requires highly-trained raters, and may suffer from high inter-rater variability. Self-supervised learning strategies can leverage unlabeled data for training.   We developed an algorithm to simulate resections from preoperative magnetic resonance images (MRIs). We performed self-supervised training of a 3D CNN for RC segmentation using our simulation method. We curated EPISURG, a dataset comprising 430 postoperative and 268 preoperative MRIs from 430 refractory epilepsy patients who underwent resective neurosurgery. We fine-tuned our model on three small annotated datasets from different institutions and on the annotated images in EPISURG, comprising 20, 33, 19 and 133 subjects.   The model trained on data with simulated resections obtained median (interquartile range) Dice score coefficients (DSCs) of 81.7 (16.4), 82.4 (36.4), 74.9 (24.2) and 80.5 (18.7) for each of the four datasets. After fine-tuning, DSCs were 89.2 (13.3), 84.1 (19.8), 80.2 (20.1) and 85.2 (10.8). For comparison, inter-rater agreement between human annotators from our previous study was 84.0 (9.9).   We present a self-supervised learning strategy for 3D CNNs using simulated RCs to accurately segment real RCs on postoperative MRI. Our method generalizes well to data from different institutions, pathologies and modalities. Source code, segmentation models and the EPISURG dataset are available at https://github.com/fepegar/ressegijcars .



### What is the State of the Art of Computer Vision-Assisted Cytology? A Systematic Literature Review
- **Arxiv ID**: http://arxiv.org/abs/2105.11277v1
- **DOI**: 10.1016/j.compmedimag.2021.101934
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.11277v1)
- **Published**: 2021-05-24 13:50:45+00:00
- **Updated**: 2021-05-24 13:50:45+00:00
- **Authors**: André Victória Matias, João Gustavo Atkinson Amorim, Luiz Antonio Buschetto Macarini, Allan Cerentini, Alexandre Sherlley Casimiro Onofre, Fabiana Botelho de Miranda Onofre, Felipe Perozzo Daltoé, Marcelo Ricardo Stemmer, Aldo von Wangenheim
- **Comment**: None
- **Journal**: None
- **Summary**: Cytology is a low-cost and non-invasive diagnostic procedure employed to support the diagnosis of a broad range of pathologies. Computer Vision technologies, by automatically generating quantitative and objective descriptions of examinations' contents, can help minimize the chances of misdiagnoses and shorten the time required for analysis. To identify the state-of-art of computer vision techniques currently applied to cytology, we conducted a Systematic Literature Review. We analyzed papers published in the last 5 years. The initial search was executed in September 2020 and resulted in 431 articles. After applying the inclusion/exclusion criteria, 157 papers remained, which we analyzed to build a picture of the tendencies and problems present in this research area, highlighting the computer vision methods, staining techniques, evaluation metrics, and the availability of the used datasets and computer code. As a result, we identified that the most used methods in the analyzed works are deep learning-based (70 papers), while fewer works employ classic computer vision only (101 papers). The most recurrent metric used for classification and object detection was the accuracy (33 papers and 5 papers), while for segmentation it was the Dice Similarity Coefficient (38 papers). Regarding staining techniques, Papanicolaou was the most employed one (130 papers), followed by H&E (20 papers) and Feulgen (5 papers). Twelve of the datasets used in the papers are publicly available, with the DTU/Herlev dataset being the most used one. We conclude that there still is a lack of high-quality datasets for many types of stains and most of the works are not mature enough to be applied in a daily clinical diagnostic routine. We also identified a growing tendency towards adopting deep learning-based approaches as the methods of choice.



### Coarse-to-Fine for Sim-to-Real: Sub-Millimetre Precision Across Wide Task Spaces
- **Arxiv ID**: http://arxiv.org/abs/2105.11283v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.11283v2)
- **Published**: 2021-05-24 14:12:38+00:00
- **Updated**: 2021-07-29 13:42:53+00:00
- **Authors**: Eugene Valassakis, Norman Di Palo, Edward Johns
- **Comment**: To be published at IROS 2021. 8 pages, 6 figures
- **Journal**: None
- **Summary**: In this paper, we study the problem of zero-shot sim-to-real when the task requires both highly precise control with sub-millimetre error tolerance, and wide task space generalisation. Our framework involves a coarse-to-fine controller, where trajectories begin with classical motion planning using ICP-based pose estimation, and transition to a learned end-to-end controller which maps images to actions and is trained in simulation with domain randomisation. In this way, we achieve precise control whilst also generalising the controller across wide task spaces, and keeping the robustness of vision-based, end-to-end control. Real-world experiments on a range of different tasks show that, by exploiting the best of both worlds, our framework significantly outperforms purely motion planning methods, and purely learning-based methods. Furthermore, we answer a range of questions on best practices for precise sim-to-real transfer, such as how different image sensor modalities and image feature representations perform.



### LineCounter: Learning Handwritten Text Line Segmentation by Counting
- **Arxiv ID**: http://arxiv.org/abs/2105.11307v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11307v1)
- **Published**: 2021-05-24 14:42:54+00:00
- **Updated**: 2021-05-24 14:42:54+00:00
- **Authors**: Deng Li, Yue Wu, Yicong Zhou
- **Comment**: Submitted to 28th IEEE International Conference on Image Processing
- **Journal**: None
- **Summary**: Handwritten Text Line Segmentation (HTLS) is a low-level but important task for many higher-level document processing tasks like handwritten text recognition. It is often formulated in terms of semantic segmentation or object detection in deep learning. However, both formulations have serious shortcomings. The former requires heavy post-processing of splitting/merging adjacent segments, while the latter may fail on dense or curved texts. In this paper, we propose a novel Line Counting formulation for HTLS -- that involves counting the number of text lines from the top at every pixel location. This formulation helps learn an end-to-end HTLS solution that directly predicts per-pixel line number for a given document image. Furthermore, we propose a deep neural network (DNN) model LineCounter to perform HTLS through the Line Counting formulation. Our extensive experiments on the three public datasets (ICDAR2013-HSC, HIT-MW, and VML-AHTE) demonstrate that LineCounter outperforms state-of-the-art HTLS approaches. Source code is available at https://github.com/Leedeng/Line-Counter.



### Real-time Human Action Recognition Using Locally Aggregated Kinematic-Guided Skeletonlet and Supervised Hashing-by-Analysis Model
- **Arxiv ID**: http://arxiv.org/abs/2105.11312v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11312v2)
- **Published**: 2021-05-24 14:46:40+00:00
- **Updated**: 2021-09-07 10:50:15+00:00
- **Authors**: Bin Sun, Shaofan Wang, Dehui Kong, Lichun Wang, Baocai Yin
- **Comment**: Accepted by IEEE Transactions on Cybernetics; 13 pages
- **Journal**: None
- **Summary**: 3D action recognition is referred to as the classification of action sequences which consist of 3D skeleton joints. While many research work are devoted to 3D action recognition, it mainly suffers from three problems: highly complicated articulation, a great amount of noise, and a low implementation efficiency. To tackle all these problems, we propose a real-time 3D action recognition framework by integrating the locally aggregated kinematic-guided skeletonlet (LAKS) with a supervised hashing-by-analysis (SHA) model. We first define the skeletonlet as a few combinations of joint offsets grouped in terms of kinematic principle, and then represent an action sequence using LAKS, which consists of a denoising phase and a locally aggregating phase. The denoising phase detects the noisy action data and adjust it by replacing all the features within it with the features of the corresponding previous frame, while the locally aggregating phase sums the difference between an offset feature of the skeletonlet and its cluster center together over all the offset features of the sequence. Finally, the SHA model which combines sparse representation with a hashing model, aiming at promoting the recognition accuracy while maintaining a high efficiency. Experimental results on MSRAction3D, UTKinectAction3D and Florence3DAction datasets demonstrate that the proposed method outperforms state-of-the-art methods in both recognition accuracy and implementation efficiency.



### Multi-modal Understanding and Generation for Medical Images and Text via Vision-Language Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/2105.11333v3
- **DOI**: 10.1109/JBHI.2022.3207502
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11333v3)
- **Published**: 2021-05-24 15:14:09+00:00
- **Updated**: 2022-09-21 06:20:20+00:00
- **Authors**: Jong Hak Moon, Hyungyung Lee, Woncheol Shin, Young-Hak Kim, Edward Choi
- **Comment**: Accepted by IEEE Journal of Biomedical and Health Informatics
- **Journal**: IEEE Journal of Biomedical and Health Informatics 2022
- **Summary**: Recently a number of studies demonstrated impressive performance on diverse vision-language multi-modal tasks such as image captioning and visual question answering by extending the BERT architecture with multi-modal pre-training objectives. In this work we explore a broad set of multi-modal representation learning tasks in the medical domain, specifically using radiology images and the unstructured report. We propose Medical Vision Language Learner (MedViLL), which adopts a BERT-based architecture combined with a novel multi-modal attention masking scheme to maximize generalization performance for both vision-language understanding tasks (diagnosis classification, medical image-report retrieval, medical visual question answering) and vision-language generation task (radiology report generation). By statistically and rigorously evaluating the proposed model on four downstream tasks with three radiographic image-report datasets (MIMIC-CXR, Open-I, and VQA-RAD), we empirically demonstrate the superior downstream task performance of MedViLL against various baselines, including task-specific architectures. The source code is publicly available at: https://github.com/SuperSupermoon/MedViLL



### Reconstructing Small 3D Objects in front of a Textured Background
- **Arxiv ID**: http://arxiv.org/abs/2105.11352v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.5; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2105.11352v1)
- **Published**: 2021-05-24 15:36:33+00:00
- **Updated**: 2021-05-24 15:36:33+00:00
- **Authors**: Petr Hruby, Tomas Pajdla
- **Comment**: 36 pages total: 11 pages paper and 25 pages supplementary
- **Journal**: None
- **Summary**: We present a technique for a complete 3D reconstruction of small objects moving in front of a textured background. It is a particular variation of multibody structure from motion, which specializes to two objects only. The scene is captured in several static configurations between which the relative pose of the two objects may change. We reconstruct every static configuration individually and segment the points locally by finding multiple poses of cameras that capture the scene's other configurations. Then, the local segmentation results are combined, and the reconstructions are merged into the resulting model of the scene. In experiments with real artifacts, we show that our approach has practical advantages when reconstructing 3D objects from all sides. In this setting, our method outperforms the state-of-the-art. We integrate our method into the state of the art 3D reconstruction pipeline COLMAP.



### Brain tumour segmentation using a triplanar ensemble of U-Nets
- **Arxiv ID**: http://arxiv.org/abs/2105.11356v1
- **DOI**: 10.1007/978-3-030-72084-1_31
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.11356v1)
- **Published**: 2021-05-24 15:39:06+00:00
- **Updated**: 2021-05-24 15:39:06+00:00
- **Authors**: Vaanathi Sundaresan, Ludovica Griffanti, Mark Jenkinson
- **Comment**: None
- **Journal**: In: Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic
  Brain Injuries. BrainLes 2020, MICCAI 2020. LNCS, vol 12658. Springer, Cham
  2021
- **Summary**: Gliomas appear with wide variation in their characteristics both in terms of their appearance and location on brain MR images, which makes robust tumour segmentation highly challenging, and leads to high inter-rater variability even in manual segmentations. In this work, we propose a triplanar ensemble network, with an independent tumour core prediction module, for accurate segmentation of these tumours and their sub-regions. On evaluating our method on the MICCAI Brain Tumor Segmentation (BraTS) challenge validation dataset, for tumour sub-regions, we achieved a Dice similarity coefficient of 0.77 for both enhancing tumour (ET) and tumour core (TC). In the case of the whole tumour (WT) region, we achieved a Dice value of 0.89, which is on par with the top-ranking methods from BraTS'17-19. Our method achieved an evaluation score that was the equal 5th highest value (with our method ranking in 10th place) in the BraTS'20 challenge, with mean Dice values of 0.81, 0.89 and 0.84 on ET, WT and TC regions respectively on the BraTS'20 unseen test dataset.



### DDR-Net: Dividing and Downsampling Mixed Network for Diffeomorphic Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2105.11361v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.11361v1)
- **Published**: 2021-05-24 15:45:10+00:00
- **Updated**: 2021-05-24 15:45:10+00:00
- **Authors**: Ankita Joshi, Yi Hong
- **Comment**: None
- **Journal**: None
- **Summary**: Deep diffeomorphic registration faces significant challenges for high-dimensional images, especially in terms of memory limits. Existing approaches either downsample original images, or approximate underlying transformations, or reduce model size. The information loss during the approximation or insufficient model capacity is a hindrance to the registration accuracy for high-dimensional images, e.g., 3D medical volumes. In this paper, we propose a Dividing and Downsampling mixed Registration network (DDR-Net), a general architecture that preserves most of the image information at multiple scales. DDR-Net leverages the global context via downsampling the input and utilizes the local details from divided chunks of the input images. This design reduces the network input size and its memory cost; meanwhile, by fusing global and local information, DDR-Net obtains both coarse-level and fine-level alignments in the final deformation fields. We evaluate DDR-Net on three public datasets, i.e., OASIS, IBSR18, and 3DIRCADB-01, and the experimental results demonstrate our approach outperforms existing approaches.



### Large-Scale Attribute-Object Compositions
- **Arxiv ID**: http://arxiv.org/abs/2105.11373v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11373v1)
- **Published**: 2021-05-24 16:05:41+00:00
- **Updated**: 2021-05-24 16:05:41+00:00
- **Authors**: Filip Radenovic, Animesh Sinha, Albert Gordo, Tamara Berg, Dhruv Mahajan
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of learning how to predict attribute-object compositions from images, and its generalization to unseen compositions missing from the training data. To the best of our knowledge, this is a first large-scale study of this problem, involving hundreds of thousands of compositions. We train our framework with images from Instagram using hashtags as noisy weak supervision. We make careful design choices for data collection and modeling, in order to handle noisy annotations and unseen compositions. Finally, extensive evaluations show that learning to compose classifiers outperforms late fusion of individual attribute and object predictions, especially in the case of unseen attribute-object pairs.



### Multi-Level Attentive Convoluntional Neural Network for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2105.11422v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.11422v1)
- **Published**: 2021-05-24 17:29:00+00:00
- **Updated**: 2021-05-24 17:29:00+00:00
- **Authors**: Mengxiao Tian, Hao Guo, Chengjiang Long
- **Comment**: None
- **Journal**: None
- **Summary**: Recently the crowd counting has received more and more attention. Especially the technology of high-density environment has become an important research content, and the relevant methods for the existence of extremely dense crowd are not optimal. In this paper, we propose a multi-level attentive Convolutional Neural Network (MLAttnCNN) for crowd counting. We extract high-level contextual information with multiple different scales applied in pooling, and use multi-level attention modules to enrich the characteristics at different layers to achieve more efficient multi-scale feature fusion, which is able to be used to generate a more accurate density map with dilated convolutions and a $1\times 1$ convolution. The extensive experiments on three available public datasets show that our proposed network achieves outperformance to the state-of-the-art approaches.



### Attention-guided Temporally Coherent Video Object Matting
- **Arxiv ID**: http://arxiv.org/abs/2105.11427v3
- **DOI**: 10.1145/3474085.3475623
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11427v3)
- **Published**: 2021-05-24 17:34:57+00:00
- **Updated**: 2021-07-29 16:12:46+00:00
- **Authors**: Yunke Zhang, Chi Wang, Miaomiao Cui, Peiran Ren, Xuansong Xie, Xian-sheng Hua, Hujun Bao, Qixing Huang, Weiwei Xu
- **Comment**: 10 pages, 6 figures, MM '21 camera-ready
- **Journal**: None
- **Summary**: This paper proposes a novel deep learning-based video object matting method that can achieve temporally coherent matting results. Its key component is an attention-based temporal aggregation module that maximizes image matting networks' strength for video matting networks. This module computes temporal correlations for pixels adjacent to each other along the time axis in feature space, which is robust against motion noises. We also design a novel loss term to train the attention weights, which drastically boosts the video matting performance. Besides, we show how to effectively solve the trimap generation problem by fine-tuning a state-of-the-art video object segmentation network with a sparse set of user-annotated keyframes. To facilitate video matting and trimap generation networks' training, we construct a large-scale video matting dataset with 80 training and 28 validation foreground video clips with ground-truth alpha mattes. Experimental results show that our method can generate high-quality alpha mattes for various videos featuring appearance change, occlusion, and fast motion. Our code and dataset can be found at: https://github.com/yunkezhang/TCVOM



### Design to automate the detection and counting of Tuberculosis(TB) bacilli
- **Arxiv ID**: http://arxiv.org/abs/2105.11432v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.11432v1)
- **Published**: 2021-05-24 17:41:39+00:00
- **Updated**: 2021-05-24 17:41:39+00:00
- **Authors**: Dinesh Jackson Samuel, Rajesh Kanna Baskaran
- **Comment**: None
- **Journal**: None
- **Summary**: Tuberculosis is a contagious disease which is one of the leading causes of death, globally. The general diagnosis methods for tuberculosis include microscopic examination, tuberculin skin test, culture method, enzyme linked immunosorbent assay (ELISA) and electronic nose system. World Health Organization (WHO) recommends standard microscopic examination for early diagnosis of tuberculosis. In microscopy, the technician examines field of views (FOVs) in sputum smear for presence of any TB bacilli and counts the number of TB bacilli per FOV to report the level of severity. This process is time consuming with an increased concentration for an experienced staff to examine a single sputum smear. The examination demands for skilled technicians in high-prevalence countries which may lead to overload, fatigue and diminishes the quality of microscopy. Thus, a computer assisted system is proposed and designed for the detection of tuberculosis bacilli to assist pathologists with increased sensitivity and specificity. The manual efforts in detecting and counting the number of TB bacilli is greatly minimized. The system obtains Ziehl-Neelsen stained microscopic images from conventional microscope at 100x magnification and passes the data to the detection system. Initially the segmentation of TB bacilli was done using RGB thresholding and Sauvola's adaptive thresholding algorithm. To eliminate the non-TB bacilli from coarse level segmentation, shape descriptors like area, perimeter, convex hull, major axis length and eccentricity are used to extract only the TB bacilli features. Finally, the TB bacilli are counted using the generated bounding boxes to report the level of severity.



### LuvHarris: A Practical Corner Detector for Event-cameras
- **Arxiv ID**: http://arxiv.org/abs/2105.11443v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11443v2)
- **Published**: 2021-05-24 17:54:06+00:00
- **Updated**: 2021-10-15 11:55:40+00:00
- **Authors**: Arren Glover, Aiko Dinale, Leandro De Souza Rosa, Simeon Bamford, Chiara Bartolozzi
- **Comment**: None
- **Journal**: None
- **Summary**: There have been a number of corner detection methods proposed for event cameras in the last years, since event-driven computer vision has become more accessible. Current state-of-the-art have either unsatisfactory accuracy or real-time performance when considered for practical use, for example when a camera is randomly moved in an unconstrained environment. In this paper, we present yet another method to perform corner detection, dubbed look-up event-Harris (luvHarris), that employs the Harris algorithm for high accuracy but manages an improved event throughput. Our method has two major contributions, 1. a novel "threshold ordinal event-surface" that removes certain tuning parameters and is well suited for Harris operations, and 2. an implementation of the Harris algorithm such that the computational load per event is minimised and computational heavy convolutions are performed only "as-fast-as-possible", i.e. only as computational resources are available. The result is a practical, real-time, and robust corner detector that runs more than 2.6x the speed of current state-of-the-art; a necessity when using high-resolution event-camera in real-time. We explain the considerations taken for the approach, compare the algorithm to current state-of-the-art in terms of computational performance and detection accuracy, and discuss the validity of the proposed approach for event cameras.



### SAT: 2D Semantics Assisted Training for 3D Visual Grounding
- **Arxiv ID**: http://arxiv.org/abs/2105.11450v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11450v2)
- **Published**: 2021-05-24 17:58:36+00:00
- **Updated**: 2021-09-22 04:35:38+00:00
- **Authors**: Zhengyuan Yang, Songyang Zhang, Liwei Wang, Jiebo Luo
- **Comment**: ICCV 2021 Oral
- **Journal**: None
- **Summary**: 3D visual grounding aims at grounding a natural language description about a 3D scene, usually represented in the form of 3D point clouds, to the targeted object region. Point clouds are sparse, noisy, and contain limited semantic information compared with 2D images. These inherent limitations make the 3D visual grounding problem more challenging. In this study, we propose 2D Semantics Assisted Training (SAT) that utilizes 2D image semantics in the training stage to ease point-cloud-language joint representation learning and assist 3D visual grounding. The main idea is to learn auxiliary alignments between rich, clean 2D object representations and the corresponding objects or mentioned entities in 3D scenes. SAT takes 2D object semantics, i.e., object label, image feature, and 2D geometric feature, as the extra input in training but does not require such inputs during inference. By effectively utilizing 2D semantics in training, our approach boosts the accuracy on the Nr3D dataset from 37.7% to 49.2%, which significantly surpasses the non-SAT baseline with the identical network architecture and inference input. Our approach outperforms the state of the art by large margins on multiple 3D visual grounding datasets, i.e., +10.4% absolute accuracy on Nr3D, +9.9% on Sr3D, and +5.6% on ScanRef.



### Experimenting with Knowledge Distillation techniques for performing Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.11486v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.11486v1)
- **Published**: 2021-05-24 18:17:01+00:00
- **Updated**: 2021-05-24 18:17:01+00:00
- **Authors**: Ashwin Nalwade, Jackie Kisa
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-modal magnetic resonance imaging (MRI) is a crucial method for analyzing the human brain. It is usually used for diagnosing diseases and for making valuable decisions regarding the treatments - for instance, checking for gliomas in the human brain. With varying degrees of severity and detection, properly diagnosing gliomas is one of the most daunting and significant analysis tasks in modern-day medicine. Our primary focus is on working with different approaches to perform the segmentation of brain tumors in multimodal MRI scans. Now, the quantity, variability of the data used for training has always been considered to be crucial for developing excellent models. Hence, we also want to experiment with Knowledge Distillation techniques.



### 3D-Aware Ellipse Prediction for Object-Based Camera Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2105.11494v1
- **DOI**: 10.1109/3DV50981.2020.00038
- **Categories**: **cs.CV**, 65D19, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2105.11494v1)
- **Published**: 2021-05-24 18:40:18+00:00
- **Updated**: 2021-05-24 18:40:18+00:00
- **Authors**: Matthieu Zins, Gilles Simon, Marie-Odile Berger
- **Comment**: Presented at 3DV 2020. Code and models released at
  https://github.com/zinsmatt/3D-Aware-Ellipses-for-Visual-Localization
- **Journal**: 3DV 2020
- **Summary**: In this paper, we propose a method for coarse camera pose computation which is robust to viewing conditions and does not require a detailed model of the scene. This method meets the growing need of easy deployment of robotics or augmented reality applications in any environments, especially those for which no accurate 3D model nor huge amount of ground truth data are available. It exploits the ability of deep learning techniques to reliably detect objects regardless of viewing conditions. Previous works have also shown that abstracting the geometry of a scene of objects by an ellipsoid cloud allows to compute the camera pose accurately enough for various application needs. Though promising, these approaches use the ellipses fitted to the detection bounding boxes as an approximation of the imaged objects. In this paper, we go one step further and propose a learning-based method which detects improved elliptic approximations of objects which are coherent with the 3D ellipsoid in terms of perspective projection. Experiments prove that the accuracy of the computed pose significantly increases thanks to our method and is more robust to the variability of the boundaries of the detection boxes. This is achieved with very little effort in terms of training data acquisition -- a few hundred calibrated images of which only three need manual object annotation. Code and models are released at https://github.com/zinsmatt/3D-Aware-Ellipses-for-Visual-Localization.



### Unsupervised Visual Representation Learning by Online Constrained K-Means
- **Arxiv ID**: http://arxiv.org/abs/2105.11527v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11527v3)
- **Published**: 2021-05-24 20:38:32+00:00
- **Updated**: 2022-03-28 20:15:05+00:00
- **Authors**: Qi Qian, Yuanhong Xu, Juhua Hu, Hao Li, Rong Jin
- **Comment**: accepted by CVPR'22
- **Journal**: None
- **Summary**: Cluster discrimination is an effective pretext task for unsupervised representation learning, which often consists of two phases: clustering and discrimination. Clustering is to assign each instance a pseudo label that will be used to learn representations in discrimination. The main challenge resides in clustering since prevalent clustering methods (e.g., k-means) have to run in a batch mode. Besides, there can be a trivial solution consisting of a dominating cluster. To address these challenges, we first investigate the objective of clustering-based representation learning. Based on this, we propose a novel clustering-based pretext task with online \textbf{Co}nstrained \textbf{K}-m\textbf{e}ans (\textbf{CoKe}). Compared with the balanced clustering that each cluster has exactly the same size, we only constrain the minimal size of each cluster to flexibly capture the inherent data structure. More importantly, our online assignment method has a theoretical guarantee to approach the global optimum. By decoupling clustering and discrimination, CoKe can achieve competitive performance when optimizing with only a single view from each instance. Extensive experiments on ImageNet and other benchmark data sets verify both the efficacy and efficiency of our proposal. Code is available at \url{https://github.com/idstcv/CoKe}.



### Learning Better Visual Dialog Agents with Pretrained Visual-Linguistic Representation
- **Arxiv ID**: http://arxiv.org/abs/2105.11541v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11541v1)
- **Published**: 2021-05-24 21:09:20+00:00
- **Updated**: 2021-05-24 21:09:20+00:00
- **Authors**: Tao Tu, Qing Ping, Govind Thattai, Gokhan Tur, Prem Natarajan
- **Comment**: None
- **Journal**: None
- **Summary**: GuessWhat?! is a two-player visual dialog guessing game where player A asks a sequence of yes/no questions (Questioner) and makes a final guess (Guesser) about a target object in an image, based on answers from player B (Oracle). Based on this dialog history between the Questioner and the Oracle, a Guesser makes a final guess of the target object. Previous baseline Oracle model encodes no visual information in the model, and it cannot fully understand complex questions about color, shape, relationships and so on. Most existing work for Guesser encode the dialog history as a whole and train the Guesser models from scratch on the GuessWhat?! dataset. This is problematic since language encoder tend to forget long-term history and the GuessWhat?! data is sparse in terms of learning visual grounding of objects. Previous work for Questioner introduces state tracking mechanism into the model, but it is learned as a soft intermediates without any prior vision-linguistic insights. To bridge these gaps, in this paper we propose Vilbert-based Oracle, Guesser and Questioner, which are all built on top of pretrained vision-linguistic model, Vilbert. We introduce two-way background/target fusion mechanism into Vilbert-Oracle to account for both intra and inter-object questions. We propose a unified framework for Vilbert-Guesser and Vilbert-Questioner, where state-estimator is introduced to best utilize Vilbert's power on single-turn referring expression comprehension. Experimental results show that our proposed models outperform state-of-the-art models significantly by 7%, 10%, 12% for Oracle, Guesser and End-to-End Questioner respectively.



### Elastic Shape Analysis of Brain Structures for Predictive Modeling of PTSD
- **Arxiv ID**: http://arxiv.org/abs/2105.11547v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2105.11547v1)
- **Published**: 2021-05-24 21:33:58+00:00
- **Updated**: 2021-05-24 21:33:58+00:00
- **Authors**: Yuexuan Wu, Suprateek Kundu, Jennifer S. Stevens, Negar Fani, Anuj Srivastava
- **Comment**: 33 pages; Supplementary Materials and interactive visualizations are
  available in https://www.dropbox.com/home/Paper%20Interactive%20Figures
- **Journal**: None
- **Summary**: There is increasing evidence on the importance of brain morphology in predicting and classifying mental disorders. However, the vast majority of current shape approaches rely heavily on vertex-wise analysis that may not successfully capture complexities of subcortical structures. Additionally, the past works do not include interactions between these structures and exposure factors. Predictive modeling with such interactions is of paramount interest in heterogeneous mental disorders such as PTSD, where trauma exposure interacts with brain shape changes to influence behavior. We propose a comprehensive framework that overcomes these limitations by representing brain substructures as continuous parameterized surfaces and quantifying their shape differences using elastic shape metrics. Using the elastic shape metric, we compute shape summaries of subcortical data and represent individual shapes by their principal scores. These representations allow visualization tools that help localize changes when these PCs are varied. Subsequently, these PCs, the auxiliary exposure variables, and their interactions are used for regression modeling. We apply our method to data from the Grady Trauma Project, where the goal is to predict clinical measures of PTSD using shapes of brain substructures. Our analysis revealed considerably greater predictive power under the elastic shape analysis than widely used approaches such as vertex-wise shape analysis and even volumetric analysis. It helped identify local deformations in brain shapes related to change in PTSD severity. To our knowledge, this is one of the first brain shape analysis approaches that can seamlessly integrate the pre-processing steps under one umbrella for improved accuracy and are naturally able to account for interactions between brain shape and additional covariates to yield superior predictive performance when modeling clinical outcomes.



### TRACE: A Differentiable Approach to Line-level Stroke Recovery for Offline Handwritten Text
- **Arxiv ID**: http://arxiv.org/abs/2105.11559v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.11559v1)
- **Published**: 2021-05-24 22:15:50+00:00
- **Updated**: 2021-05-24 22:15:50+00:00
- **Authors**: Taylor Archibald, Mason Poggemann, Aaron Chan, Tony Martinez
- **Comment**: Accepted as a conference paper at the 16th International Conference
  on Document Analysis and Recognition (ICDAR), Lausanne, Switzerland, 2021
- **Journal**: None
- **Summary**: Stroke order and velocity are helpful features in the fields of signature verification, handwriting recognition, and handwriting synthesis. Recovering these features from offline handwritten text is a challenging and well-studied problem. We propose a new model called TRACE (Trajectory Recovery by an Adaptively-trained Convolutional Encoder). TRACE is a differentiable approach that uses a convolutional recurrent neural network (CRNN) to infer temporal stroke information from long lines of offline handwritten text with many characters and dynamic time warping (DTW) to align predictions and ground truth points. TRACE is perhaps the first system to be trained end-to-end on entire lines of text of arbitrary width and does not require the use of dynamic exemplars. Moreover, the system does not require images to undergo any pre-processing, nor do the predictions require any post-processing. Consequently, the recovered trajectory is differentiable and can be used as a loss function for other tasks, including synthesizing offline handwritten text.   We demonstrate that temporal stroke information recovered by TRACE from offline data can be used for handwriting synthesis and establish the first benchmarks for a stroke trajectory recovery system trained on the IAM online handwriting dataset.



### Pan-sharpening via High-pass Modification Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2105.11576v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.11576v1)
- **Published**: 2021-05-24 23:39:04+00:00
- **Updated**: 2021-05-24 23:39:04+00:00
- **Authors**: Jiaming Wang, Zhenfeng Shao, Xiao Huang, Tao Lu, Ruiqian Zhang, Jiayi Ma
- **Comment**: 5 pages, 5 figures, accepted by the 28th IEEE International
  Conference on Image Processing (ICIP 2021)
- **Journal**: None
- **Summary**: Most existing deep learning-based pan-sharpening methods have several widely recognized issues, such as spectral distortion and insufficient spatial texture enhancement, we propose a novel pan-sharpening convolutional neural network based on a high-pass modification block. Different from existing methods, the proposed block is designed to learn the high-pass information, leading to enhance spatial information in each band of the multi-spectral-resolution images. To facilitate the generation of visually appealing pan-sharpened images, we propose a perceptual loss function and further optimize the model based on high-level features in the near-infrared space. Experiments demonstrate the superior performance of the proposed method compared to the state-of-the-art pan-sharpening methods, both quantitatively and qualitatively. The proposed model is open-sourced at https://github.com/jiaming-wang/HMB.



### SHD360: A Benchmark Dataset for Salient Human Detection in 360° Videos
- **Arxiv ID**: http://arxiv.org/abs/2105.11578v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11578v7)
- **Published**: 2021-05-24 23:51:29+00:00
- **Updated**: 2021-12-22 11:07:40+00:00
- **Authors**: Yi Zhang, Lu Zhang, Kang Wang, Wassim Hamidouche, Olivier Deforges
- **Comment**: 21 pages, 13 figures, 5 tables; Project page:
  https://github.com/PanoAsh/SHD360; Technical report
- **Journal**: None
- **Summary**: Salient human detection (SHD) in dynamic 360{\deg} immersive videos is of great importance for various applications such as robotics, inter-human and human-object interaction in augmented reality. However, 360{\deg} video SHD has been seldom discussed in the computer vision community due to a lack of datasets with large-scale omnidirectional videos and rich annotations. To this end, we propose SHD360, the first 360{\deg} video SHD dataset which contains various real-life daily scenes. Since so far there is no method proposed for 360{\deg} image/video SHD, we systematically benchmark 11 representative state-of-the-art salient object detection (SOD) approaches on our SHD360, and explore key issues derived from extensive experimenting results. We hope our proposed dataset and benchmark could serve as a good starting point for advancing human-centric researches towards 360{\deg} panoramic data. The dataset is available at https://github.com/PanoAsh/SHD360.



