# Arxiv Papers in cs.CV on 2021-05-20
### VTNet: Visual Transformer Network for Object Goal Navigation
- **Arxiv ID**: http://arxiv.org/abs/2105.09447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09447v1)
- **Published**: 2021-05-20 01:23:15+00:00
- **Updated**: 2021-05-20 01:23:15+00:00
- **Authors**: Heming Du, Xin Yu, Liang Zheng
- **Comment**: accepted paper at ICLR 2021
- **Journal**: None
- **Summary**: Object goal navigation aims to steer an agent towards a target object based on observations of the agent. It is of pivotal importance to design effective visual representations of the observed scene in determining navigation actions. In this paper, we introduce a Visual Transformer Network (VTNet) for learning informative visual representation in navigation. VTNet is a highly effective structure that embodies two key properties for visual representations: First, the relationships among all the object instances in a scene are exploited; Second, the spatial locations of objects and image regions are emphasized so that directional navigation signals can be learned. Furthermore, we also develop a pre-training scheme to associate the visual representations with navigation signals, and thus facilitate navigation policy learning. In a nutshell, VTNet embeds object and region features with their location cues as spatial-aware descriptors and then incorporates all the encoded descriptors through attention operations to achieve informative representation for navigation. Given such visual representations, agents are able to explore the correlations between visual observations and navigation actions. For example, an agent would prioritize "turning right" over "turning left" when the visual representation emphasizes on the right side of activation map. Experiments in the artificial environment AI2-Thor demonstrate that VTNet significantly outperforms state-of-the-art methods in unseen testing environments.



### Superpixel-based Knowledge Infusion in Deep Neural Networks for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2105.09448v2
- **DOI**: 10.1145/3476883.3520216
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.09448v2)
- **Published**: 2021-05-20 01:25:42+00:00
- **Updated**: 2022-02-23 09:22:22+00:00
- **Authors**: Gunjan Chhablani, Abheesht Sharma, Harshit Pandey, Tirtharaj Dash
- **Comment**: ACM Proc. format: 5 pages; Accepted at ACM SE'22, April 18-20, 2022,
  Virtual Event, USA
- **Journal**: Proceedings of the 2022 ACM Southeast Conference, April 2022,
  Pages 243-247
- **Summary**: Superpixels are higher-order perceptual groups of pixels in an image, often carrying much more information than the raw pixels. There is an inherent relational structure to the relationship among different superpixels of an image such as adjacent superpixels are neighbours of each other. Our interest here is to treat these relative positions of various superpixels as relational information of an image. This relational information can convey higher-order spatial information about the image, such as the relationship between superpixels representing two eyes in an image of a cat. That is, two eyes are placed adjacent to each other in a straight line or the mouth is below the nose. Our motive in this paper is to assist computer vision models, specifically those based on Deep Neural Networks (DNNs), by incorporating this higher-order information from superpixels. We construct a hybrid model that leverages (a) Convolutional Neural Network (CNN) to deal with spatial information in an image and (b) Graph Neural Network (GNN) to deal with relational superpixel information in the image. The proposed model is learned using a generic hybrid loss function. Our experiments are extensive, and we evaluate the predictive performance of our proposed hybrid vision model on seven different image classification datasets from a variety of domains such as digit and object recognition, biometrics, medical imaging. The results demonstrate that the relational superpixel information processed by a GNN can improve the performance of a standard CNN-based vision system.



### RaspberryPI for mosquito neutralization by power laser
- **Arxiv ID**: http://arxiv.org/abs/2105.14190v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.14190v1)
- **Published**: 2021-05-20 01:38:45+00:00
- **Updated**: 2021-05-20 01:38:45+00:00
- **Authors**: R. Ildar
- **Comment**: None
- **Journal**: None
- **Summary**: In this article for the first time, comprehensive studies of mosquito neutralization using machine vision and a 1 W power laser are considered. Developed laser installation with Raspberry Pi that changing the direction of the laser with a galvanometer. We developed a program for mosquito tracking in real. The possibility of using deep neural networks, Haar cascades, machine learning for mosquito recognition was considered. We considered in detail the classification problems of mosquitoes in images. A recommendation is given for the implementation of this device based on a microcontroller for subsequent use as part of an unmanned aerial vehicle. Any harmful insects in the fields can be used as objects for control.



### Anabranch Network for Camouflaged Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.09451v1
- **DOI**: 10.1016/j.cviu.2019.04.006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09451v1)
- **Published**: 2021-05-20 01:52:44+00:00
- **Updated**: 2021-05-20 01:52:44+00:00
- **Authors**: Trung-Nghia Le, Tam V. Nguyen, Zhongliang Nie, Minh-Triet Tran, Akihiro Sugimoto
- **Comment**: Published in CVIU 2019. Project page:
  https://sites.google.com/view/ltnghia/research/camo
- **Journal**: Computer Vision and Image Understanding 184 (2019) 45-56
- **Summary**: Camouflaged objects attempt to conceal their texture into the background and discriminating them from the background is hard even for human beings. The main objective of this paper is to explore the camouflaged object segmentation problem, namely, segmenting the camouflaged object(s) for a given image. This problem has not been well studied in spite of a wide range of potential applications including the preservation of wild animals and the discovery of new species, surveillance systems, search-and-rescue missions in the event of natural disasters such as earthquakes, floods or hurricanes. This paper addresses a new challenging problem of camouflaged object segmentation. To address this problem, we provide a new image dataset of camouflaged objects for benchmarking purposes. In addition, we propose a general end-to-end network, called the Anabranch Network, that leverages both classification and segmentation tasks. Different from existing networks for segmentation, our proposed network possesses the second branch for classification to predict the probability of containing camouflaged object(s) in an image, which is then fused into the main branch for segmentation to boost up the segmentation accuracy. Extensive experiments conducted on the newly built dataset demonstrate the effectiveness of our network using various fully convolutional networks. \url{https://sites.google.com/view/ltnghia/research/camo}



### Content-Augmented Feature Pyramid Network with Light Linear Spatial Transformers for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.09464v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T45, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2105.09464v3)
- **Published**: 2021-05-20 02:31:31+00:00
- **Updated**: 2022-04-13 13:10:46+00:00
- **Authors**: Yongxiang Gu, Xiaolin Qin, Yuncong Peng, Lu Li
- **Comment**: 16 pages,7 figures,8 tables
- **Journal**: None
- **Summary**: As one of the prevalent components, Feature Pyramid Network (FPN) is widely used in current object detection models for improving multi-scale object detection performance. However, its feature fusion mode is still in a misaligned and local manner, thus limiting the representation power. To address the inherit defects of FPN, a novel architecture termed Content-Augmented Feature Pyramid Network (CA-FPN) is proposed in this paper. Firstly, a Global Content Extraction Module (GCEM) is proposed to extract multi-scale context information. Secondly, lightweight linear spatial Transformer connections are added in the top-down pathway to augment each feature map with multi-scale features, where a linearized approximate self-attention function is designed for reducing model complexity. By means of the self-attention mechanism in Transformer, there is no longer need to align feature maps during feature fusion, thus solving the misaligned defect. By setting the query scope to the entire feature map, the local defect can also be solved. Extensive experiments on COCO and PASCAL VOC datasets demonstrated that our CA-FPN outperforms other FPN-based detectors without bells and whistles and is robust in different settings.



### Generalized Few-Shot Object Detection without Forgetting
- **Arxiv ID**: http://arxiv.org/abs/2105.09491v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09491v1)
- **Published**: 2021-05-20 03:25:29+00:00
- **Updated**: 2021-05-20 03:25:29+00:00
- **Authors**: Zhibo Fan, Yuchen Ma, Zeming Li, Jian Sun
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Recently few-shot object detection is widely adopted to deal with data-limited situations. While most previous works merely focus on the performance on few-shot categories, we claim that detecting all classes is crucial as test samples may contain any instances in realistic applications, which requires the few-shot detector to learn new concepts without forgetting. Through analysis on transfer learning based methods, some neglected but beneficial properties are utilized to design a simple yet effective few-shot detector, Retentive R-CNN. It consists of Bias-Balanced RPN to debias the pretrained RPN and Re-detector to find few-shot class objects without forgetting previous knowledge. Extensive experiments on few-shot detection benchmarks show that Retentive R-CNN significantly outperforms state-of-the-art methods on overall performance among all settings as it can achieve competitive results on few-shot classes and does not degrade the base class performance at all. Our approach has demonstrated that the long desired never-forgetting learner is available in object detection.



### DeepCAD: A Deep Generative Network for Computer-Aided Design Models
- **Arxiv ID**: http://arxiv.org/abs/2105.09492v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.09492v2)
- **Published**: 2021-05-20 03:29:18+00:00
- **Updated**: 2021-08-16 03:47:17+00:00
- **Authors**: Rundi Wu, Chang Xiao, Changxi Zheng
- **Comment**: Accepted to ICCV 2021. Code and data are available at
  http://www.cs.columbia.edu/cg/deepcad/
- **Journal**: None
- **Summary**: Deep generative models of 3D shapes have received a great deal of research interest. Yet, almost all of them generate discrete shape representations, such as voxels, point clouds, and polygon meshes. We present the first 3D generative model for a drastically different shape representation --- describing a shape as a sequence of computer-aided design (CAD) operations. Unlike meshes and point clouds, CAD models encode the user creation process of 3D shapes, widely used in numerous industrial and engineering design tasks. However, the sequential and irregular structure of CAD operations poses significant challenges for existing 3D generative models. Drawing an analogy between CAD operations and natural language, we propose a CAD generative network based on the Transformer. We demonstrate the performance of our model for both shape autoencoding and random shape generation. To train our network, we create a new CAD dataset consisting of 178,238 models and their CAD construction sequences. We have made this dataset publicly available to promote future research on this topic.



### Drone-based AI and 3D Reconstruction for Digital Twin Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.03797v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2106.03797v1)
- **Published**: 2021-05-20 03:31:15+00:00
- **Updated**: 2021-05-20 03:31:15+00:00
- **Authors**: Alex To, Maican Liu, Muhammad Hazeeq Bin Muhammad Hairul, Joseph G. Davis, Jeannie S. A. Lee, Henrik Hesse, Hoang D. Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: Digital Twin is an emerging technology at the forefront of Industry 4.0, with the ultimate goal of combining the physical space and the virtual space. To date, the Digital Twin concept has been applied in many engineering fields, providing useful insights in the areas of engineering design, manufacturing, automation, and construction industry. While the nexus of various technologies opens up new opportunities with Digital Twin, the technology requires a framework to integrate the different technologies, such as the Building Information Model used in the Building and Construction industry. In this work, an Information Fusion framework is proposed to seamlessly fuse heterogeneous components in a Digital Twin framework from the variety of technologies involved. This study aims to augment Digital Twin in buildings with the use of AI and 3D reconstruction empowered by unmanned aviation vehicles. We proposed a drone-based Digital Twin augmentation framework with reusable and customisable components. A proof of concept is also developed, and extensive evaluation is conducted for 3D reconstruction and applications of AI for defect detection.



### Medical Image Segmentation Using Squeeze-and-Expansion Transformers
- **Arxiv ID**: http://arxiv.org/abs/2105.09511v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.09511v3)
- **Published**: 2021-05-20 04:45:47+00:00
- **Updated**: 2021-06-02 02:42:19+00:00
- **Authors**: Shaohua Li, Xiuchao Sui, Xiangde Luo, Xinxing Xu, Yong Liu, Rick Goh
- **Comment**: Camera ready for IJCAI'2021
- **Journal**: None
- **Summary**: Medical image segmentation is important for computer-aided diagnosis. Good segmentation demands the model to see the big picture and fine details simultaneously, i.e., to learn image features that incorporate large context while keep high spatial resolutions. To approach this goal, the most widely used methods -- U-Net and variants, extract and fuse multi-scale features. However, the fused features still have small "effective receptive fields" with a focus on local image cues, limiting their performance. In this work, we propose Segtran, an alternative segmentation framework based on transformers, which have unlimited "effective receptive fields" even at high feature resolutions. The core of Segtran is a novel Squeeze-and-Expansion transformer: a squeezed attention block regularizes the self attention of transformers, and an expansion block learns diversified representations. Additionally, we propose a new positional encoding scheme for transformers, imposing a continuity inductive bias for images. Experiments were performed on 2D and 3D medical image segmentation tasks: optic disc/cup segmentation in fundus images (REFUGE'20 challenge), polyp segmentation in colonoscopy images, and brain tumor segmentation in MRI scans (BraTS'19 challenge). Compared with representative existing methods, Segtran consistently achieved the highest segmentation accuracy, and exhibited good cross-domain generalization capabilities. The source code of Segtran is released at https://github.com/askerlee/segtran.



### Egocentric Activity Recognition and Localization on a 3D Map
- **Arxiv ID**: http://arxiv.org/abs/2105.09544v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09544v3)
- **Published**: 2021-05-20 06:58:15+00:00
- **Updated**: 2022-08-12 21:50:31+00:00
- **Authors**: Miao Liu, Lingni Ma, Kiran Somasundaram, Yin Li, Kristen Grauman, James M. Rehg, Chao Li
- **Comment**: European Conference on Computer Vision (ECCV) 2022
- **Journal**: None
- **Summary**: Given a video captured from a first person perspective and the environment context of where the video is recorded, can we recognize what the person is doing and identify where the action occurs in the 3D space? We address this challenging problem of jointly recognizing and localizing actions of a mobile user on a known 3D map from egocentric videos. To this end, we propose a novel deep probabilistic model. Our model takes the inputs of a Hierarchical Volumetric Representation (HVR) of the 3D environment and an egocentric video, infers the 3D action location as a latent variable, and recognizes the action based on the video and contextual cues surrounding its potential locations. To evaluate our model, we conduct extensive experiments on the subset of Ego4D dataset, in which both human naturalistic actions and photo-realistic 3D environment reconstructions are captured. Our method demonstrates strong results on both action recognition and 3D action localization across seen and unseen environments. We believe our work points to an exciting research direction in the intersection of egocentric vision, and 3D scene understanding.



### A low-rank representation for unsupervised registration of medical images
- **Arxiv ID**: http://arxiv.org/abs/2105.09548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09548v1)
- **Published**: 2021-05-20 07:04:10+00:00
- **Updated**: 2021-05-20 07:04:10+00:00
- **Authors**: Dengqiang Jia, Shangqi Gao, Qunlong Chen, Xinzhe Luo, Xiahai Zhuang
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: Registration networks have shown great application potentials in medical image analysis. However, supervised training methods have a great demand for large and high-quality labeled datasets, which is time-consuming and sometimes impractical due to data sharing issues. Unsupervised image registration algorithms commonly employ intensity-based similarity measures as loss functions without any manual annotations. These methods estimate the parameterized transformations between pairs of moving and fixed images through the optimization of the network parameters during training. However, these methods become less effective when the image quality varies, e.g., some images are corrupted by substantial noise or artifacts. In this work, we propose a novel approach based on a low-rank representation, i.e., Regnet-LRR, to tackle the problem. We project noisy images into a noise-free low-rank space, and then compute the similarity between the images. Based on the low-rank similarity measure, we train the registration network to predict the dense deformation fields of noisy image pairs. We highlight that the low-rank projection is reformulated in a way that the registration network can successfully update gradients. With two tasks, i.e., cardiac and abdominal intra-modality registration, we demonstrate that the low-rank representation can boost the generalization ability and robustness of models as well as bring significant improvements in noisy data registration scenarios.



### Intra-Model Collaborative Learning of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.09590v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09590v1)
- **Published**: 2021-05-20 08:30:33+00:00
- **Updated**: 2021-05-20 08:30:33+00:00
- **Authors**: Shijie Fang, Tong Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, collaborative learning proposed by Song and Chai has achieved remarkable improvements in image classification tasks by simultaneously training multiple classifier heads. However, huge memory footprints required by such multi-head structures may hinder the training of large-capacity baseline models. The natural question is how to achieve collaborative learning within a single network without duplicating any modules. In this paper, we propose four ways of collaborative learning among different parts of a single network with negligible engineering efforts. To improve the robustness of the network, we leverage the consistency of the output layer and intermediate layers for training under the collaborative learning framework. Besides, the similarity of intermediate representation and convolution kernel is also introduced to reduce the reduce redundant in a neural network. Compared to the method of Song and Chai, our framework further considers the collaboration inside a single model and takes smaller overhead. Extensive experiments on Cifar-10, Cifar-100, ImageNet32 and STL-10 corroborate the effectiveness of these four ways separately while combining them leads to further improvements. In particular, test errors on the STL-10 dataset are decreased by $9.28\%$ and $5.45\%$ for ResNet-18 and VGG-16 respectively. Moreover, our method is proven to be robust to label noise with experiments on Cifar-10 dataset. For example, our method has $3.53\%$ higher performance under $50\%$ noise ratio setting.



### AGSFCOS: Based on attention mechanism and Scale-Equalizing pyramid network of object detection
- **Arxiv ID**: http://arxiv.org/abs/2105.09596v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.09596v1)
- **Published**: 2021-05-20 08:41:02+00:00
- **Updated**: 2021-05-20 08:41:02+00:00
- **Authors**: Li Wang, Wei Xiang, Ruhui Xue, Kaida Zou, Laili Zhu
- **Comment**: 9 pages,9 figures
- **Journal**: None
- **Summary**: Recently, the anchor-free object detection model has shown great potential for accuracy and speed to exceed anchor-based object detection. Therefore, two issues are mainly studied in this article: (1) How to let the backbone network in the anchor-free object detection model learn feature extraction? (2) How to make better use of the feature pyramid network? In order to solve the above problems, Experiments show that our model has a certain improvement in accuracy compared with the current popular detection models on the COCO dataset, the designed attention mechanism module can capture contextual information well, improve detection accuracy, and use sepc network to help balance abstract and detailed information, and reduce the problem of semantic gap in the feature pyramid network. Whether it is anchor-based network model YOLOv3, Faster RCNN, or anchor-free network model Foveabox, FSAF, FCOS. Our optimal model can get 39.5% COCO AP under the background of ResNet50.



### More Than Just Attention: Improving Cross-Modal Attentions with Contrastive Constraints for Image-Text Matching
- **Arxiv ID**: http://arxiv.org/abs/2105.09597v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09597v3)
- **Published**: 2021-05-20 08:48:10+00:00
- **Updated**: 2022-10-03 21:48:05+00:00
- **Authors**: Yuxiao Chen, Jianbo Yuan, Long Zhao, Tianlang Chen, Rui Luo, Larry Davis, Dimitris N. Metaxas
- **Comment**: Accepted to WACV 2023
- **Journal**: None
- **Summary**: Cross-modal attention mechanisms have been widely applied to the image-text matching task and have achieved remarkable improvements thanks to its capability of learning fine-grained relevance across different modalities. However, the cross-modal attention models of existing methods could be sub-optimal and inaccurate because there is no direct supervision provided during the training process. In this work, we propose two novel training strategies, namely Contrastive Content Re-sourcing (CCR) and Contrastive Content Swapping (CCS) constraints, to address such limitations. These constraints supervise the training of cross-modal attention models in a contrastive learning manner without requiring explicit attention annotations. They are plug-in training strategies and can be easily integrated into existing cross-modal attention models. Additionally, we introduce three metrics including Attention Precision, Recall, and F1-Score to quantitatively measure the quality of learned attention models. We evaluate the proposed constraints by incorporating them into four state-of-the-art cross-modal attention-based image-text matching models. Experimental results on both Flickr30k and MS-COCO datasets demonstrate that integrating these constraints improves the model performance in terms of both retrieval performance and attention metrics.



### FVC: A New Framework towards Deep Video Compression in Feature Space
- **Arxiv ID**: http://arxiv.org/abs/2105.09600v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.09600v2)
- **Published**: 2021-05-20 08:55:32+00:00
- **Updated**: 2021-08-23 10:56:50+00:00
- **Authors**: Zhihao Hu, Guo Lu, Dong Xu
- **Comment**: CVPR2021(oral)
- **Journal**: None
- **Summary**: Learning based video compression attracts increasing attention in the past few years. The previous hybrid coding approaches rely on pixel space operations to reduce spatial and temporal redundancy, which may suffer from inaccurate motion estimation or less effective motion compensation. In this work, we propose a feature-space video coding network (FVC) by performing all major operations (i.e., motion estimation, motion compression, motion compensation and residual compression) in the feature space. Specifically, in the proposed deformable compensation module, we first apply motion estimation in the feature space to produce motion information (i.e., the offset maps), which will be compressed by using the auto-encoder style network. Then we perform motion compensation by using deformable convolution and generate the predicted feature. After that, we compress the residual feature between the feature from the current frame and the predicted feature from our deformable compensation module. For better frame reconstruction, the reference features from multiple previous reconstructed frames are also fused by using the non-local attention mechanism in the multi-frame feature fusion module. Comprehensive experimental results demonstrate that the proposed framework achieves the state-of-the-art performance on four benchmark datasets including HEVC, UVG, VTL and MCL-JCV.



### Semantic segmentation of multispectral photoacoustic images using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2105.09624v3
- **DOI**: 10.1016/j.pacs.2022.100341
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2105.09624v3)
- **Published**: 2021-05-20 09:33:55+00:00
- **Updated**: 2022-04-16 14:15:19+00:00
- **Authors**: Melanie Schellenberg, Kris Dreher, Niklas Holzwarth, Fabian Isensee, Annika Reinke, Nicholas Schreck, Alexander Seitel, Minu D. Tizabi, Lena Maier-Hein, Janek Gröhl
- **Comment**: 8 pages, 7 figures, 3 tables
- **Journal**: Photoacoustics 26 (2022): 100341
- **Summary**: Photoacoustic (PA) imaging has the potential to revolutionize functional medical imaging in healthcare due to the valuable information on tissue physiology contained in multispectral photoacoustic measurements. Clinical translation of the technology requires conversion of the high-dimensional acquired data into clinically relevant and interpretable information. In this work, we present a deep learning-based approach to semantic segmentation of multispectral photoacoustic images to facilitate image interpretability. Manually annotated photoacoustic {and ultrasound} imaging data are used as reference and enable the training of a deep learning-based segmentation algorithm in a supervised manner. Based on a validation study with experimentally acquired data from 16 healthy human volunteers, we show that automatic tissue segmentation can be used to create powerful analyses and visualizations of multispectral photoacoustic images. Due to the intuitive representation of high-dimensional information, such a preprocessing algorithm could be a valuable means to facilitate the clinical translation of photoacoustic imaging.



### Content-adaptive Representation Learning for Fast Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2105.09645v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09645v1)
- **Published**: 2021-05-20 10:24:29+00:00
- **Updated**: 2021-05-20 10:24:29+00:00
- **Authors**: Yukai Shi, Jinghui Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional networks have attracted great attention in image restoration and enhancement. Generally, restoration quality has been improved by building more and more convolutional block. However, these methods mostly learn a specific model to handle all images and ignore difficulty diversity. In other words, an area in the image with high frequency tend to lose more information during compressing while an area with low frequency tends to lose less. In this article, we adrress the efficiency issue in image SR by incorporating a patch-wise rolling network(PRN) to content-adaptively recover images according to difficulty levels. In contrast to existing studies that ignore difficulty diversity, we adopt different stage of a neural network to perform image restoration. In addition, we propose a rolling strategy that utilizes the parameters of each stage more flexible. Extensive experiments demonstrate that our model not only shows a significant acceleration but also maintain state-of-the-art performance.



### A Connected Component Labelling algorithm for multi-pixel per clock cycle video stream
- **Arxiv ID**: http://arxiv.org/abs/2105.09658v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.09658v1)
- **Published**: 2021-05-20 10:43:58+00:00
- **Updated**: 2021-05-20 10:43:58+00:00
- **Authors**: Marcin Kowalczyk, Tomasz Kryjak
- **Comment**: Submitted to DSD 2021
- **Journal**: None
- **Summary**: This work describes the hardware implementation of a connected component labelling (CCL) module in reprogammable logic. The main novelty of the design is the "full", i.e. without any simplifications, support of a 4 pixel per clock format (4 ppc) and real-time processing of a 4K/UltraHD video stream (3840 x 2160 pixels) at 60 frames per second. To achieve this, a special labelling method was designed and a functionality that stops the input data stream in order to process pixel groups which require writing more than one merger into the equivalence table. The proposed module was verified in simulation and in hardware on the Xilinx Zynq Ultrascale+ MPSoC chip on the ZCU104 evaluation board.



### DPN-SENet:A self-attention mechanism neural network for detection and diagnosis of COVID-19 from chest x-ray images
- **Arxiv ID**: http://arxiv.org/abs/2105.09683v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.09683v1)
- **Published**: 2021-05-20 11:50:52+00:00
- **Updated**: 2021-05-20 11:50:52+00:00
- **Authors**: Bo Cheng, Ruhui Xue, Hang Yang, Laili Zhu, Wei Xiang
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Background and Objective: The new type of coronavirus is also called COVID-19. It began to spread at the end of 2019 and has now spread across the world. Until October 2020, It has infected around 37 million people and claimed about 1 million lives. We propose a deep learning model that can help radiologists and clinicians use chest X-rays to diagnose COVID-19 cases and show the diagnostic features of pneumonia. Methods: The approach in this study is: 1) we propose a data enhancement method to increase the diversity of the data set, thereby improving the generalization performance of the model. 2) Our deep convolution neural network model DPN-SE adds a self-attention mechanism to the DPN network. The addition of a self-attention mechanism has greatly improved the performance of the network. 3) Use the Lime interpretable library to mark the feature regions on the X-ray medical image that helps doctors more quickly diagnose COVID-19 in people. Results: Under the same network model, the data with and without data enhancement is put into the model for training respectively. At last, comparing two experimental results: among the 10 network models with different structures, 7 network models have improved their effects after using data enhancement, with an average improvement of 1% in recognition accuracy. We propose that the accuracy and recall rates of the DPN-SE network are 93% and 98% of cases (COVID vs. pneumonia bacteria vs. viral pneumonia vs. normal). Compared with the original DPN, the respective accuracy is improved by 2%. Conclusion: The data augmentation method we used has achieved effective results on a small amount of data set, showing that a reasonable data augmentation method can improve the recognition accuracy without changing the sample size and model structure. Overall, the proposed method and model can effectively become a very useful tool for clinical radiologists.



### Crowd Counting by Self-supervised Transfer Colorization Learning and Global Prior Classification
- **Arxiv ID**: http://arxiv.org/abs/2105.09684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09684v1)
- **Published**: 2021-05-20 11:54:05+00:00
- **Updated**: 2021-05-20 11:54:05+00:00
- **Authors**: Haoyue Bai, Song Wen, S. -H. Gary Chan
- **Comment**: None
- **Journal**: None
- **Summary**: Labeled crowd scene images are expensive and scarce. To significantly reduce the requirement of the labeled images, we propose ColorCount, a novel CNN-based approach by combining self-supervised transfer colorization learning and global prior classification to leverage the abundantly available unlabeled data. The self-supervised colorization branch learns the semantics and surface texture of the image by using its color components as pseudo labels. The classification branch extracts global group priors by learning correlations among image clusters. Their fused resultant discriminative features (global priors, semantics and textures) provide ample priors for counting, hence significantly reducing the requirement of labeled images. We conduct extensive experiments on four challenging benchmarks. ColorCount achieves much better performance as compared with other unsupervised approaches. Its performance is close to the supervised baseline with substantially less labeled data (10\% of the original one).



### Simple Transparent Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/2105.09685v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.09685v1)
- **Published**: 2021-05-20 11:54:26+00:00
- **Updated**: 2021-05-20 11:54:26+00:00
- **Authors**: Jaydeep Borkar, Pin-Yu Chen
- **Comment**: 14 pages, 9 figures, Published at ICLR 2021 Workshop on Security and
  Safety in Machine Learning Systems
- **Journal**: None
- **Summary**: There has been a rise in the use of Machine Learning as a Service (MLaaS) Vision APIs as they offer multiple services including pre-built models and algorithms, which otherwise take a huge amount of resources if built from scratch. As these APIs get deployed for high-stakes applications, it's very important that they are robust to different manipulations. Recent works have only focused on typical adversarial attacks when evaluating the robustness of vision APIs. We propose two new aspects of adversarial image generation methods and evaluate them on the robustness of Google Cloud Vision API's optical character recognition service and object detection APIs deployed in real-world settings such as sightengine.com, picpurify.com, Google Cloud Vision API, and Microsoft Azure's Computer Vision API. Specifically, we go beyond the conventional small-noise adversarial attacks and introduce secret embedding and transparent adversarial examples as a simpler way to evaluate robustness. These methods are so straightforward that even non-specialists can craft such attacks. As a result, they pose a serious threat where APIs are used for high-stakes applications. Our transparent adversarial examples successfully evade state-of-the art object detections APIs such as Azure Cloud Vision (attack success rate 52%) and Google Cloud Vision (attack success rate 36%). 90% of the images have a secret embedded text that successfully fools the vision of time-limited humans but is detected by Google Cloud Vision API's optical character recognition. Complementing to current research, our results provide simple but unconventional methods on robustness evaluation.



### An Empirical Study of Vehicle Re-Identification on the AI City Challenge
- **Arxiv ID**: http://arxiv.org/abs/2105.09701v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09701v1)
- **Published**: 2021-05-20 12:20:52+00:00
- **Updated**: 2021-05-20 12:20:52+00:00
- **Authors**: Hao Luo, Weihua Chen, Xianzhe Xu, Jianyang Gu, Yuqi Zhang, Chong Liu, Yiqi Jiang, Shuting He, Fan Wang, Hao Li
- **Comment**: CVPR 2021 AI CITY CHALLENGE City-Scale Multi-Camera Vehicle
  Re-Identification Top 1. arXiv admin note: text overlap with arXiv:2004.10547
- **Journal**: None
- **Summary**: This paper introduces our solution for the Track2 in AI City Challenge 2021 (AICITY21). The Track2 is a vehicle re-identification (ReID) task with both the real-world data and synthetic data. We mainly focus on four points, i.e. training data, unsupervised domain-adaptive (UDA) training, post-processing, model ensembling in this challenge. (1) Both cropping training data and using synthetic data can help the model learn more discriminative features. (2) Since there is a new scenario in the test set that dose not appear in the training set, UDA methods perform well in the challenge. (3) Post-processing techniques including re-ranking, image-to-track retrieval, inter-camera fusion, etc, significantly improve final performance. (4) We ensemble CNN-based models and transformer-based models which provide different representation diversity. With aforementioned techniques, our method finally achieves 0.7445 mAP score, yielding the first place in the competition. Codes are available at https://github.com/michuanhaohao/AICITY2021_Track2_DMT.



### An Attractor-Guided Neural Networks for Skeleton-Based Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2105.09711v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09711v2)
- **Published**: 2021-05-20 12:51:39+00:00
- **Updated**: 2022-05-06 07:14:55+00:00
- **Authors**: Pengxiang Ding, Junying Wang, Jianqin Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Joint relation modeling is a curial component in human motion prediction. Most existing methods tend to design skeletal-based graphs to build the relations among joints, where local interactions between joint pairs are well learned. However, the global coordination of all joints, which reflects human motion's balance property, is usually weakened because it is learned from part to whole progressively and asynchronously. Thus, the final predicted motions are sometimes unnatural. To tackle this issue, we learn a medium, called balance attractor (BA), from the spatiotemporal features of motion to characterize the global motion features, which is subsequently used to build new joint relations. Through the BA, all joints are related synchronously, and thus the global coordination of all joints can be better learned. Based on the BA, we propose our framework, referred to Attractor-Guided Neural Network, mainly including Attractor-Based Joint Relation Extractor (AJRE) and Multi-timescale Dynamics Extractor (MTDE). The AJRE mainly includes Global Coordination Extractor (GCE) and Local Interaction Extractor (LIE). The former presents the global coordination of all joints, and the latter encodes local interactions between joint pairs. The MTDE is designed to extract dynamic information from raw position information for effective prediction. Extensive experiments show that the proposed framework outperforms state-of-the-art methods in both short and long-term predictions in H3.6M, CMU-Mocap, and 3DPW.



### Generation of COVID-19 Chest CT Scan Images using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.11241v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.11241v1)
- **Published**: 2021-05-20 13:04:21+00:00
- **Updated**: 2021-05-20 13:04:21+00:00
- **Authors**: Prerak Mann, Sahaj Jain, Saurabh Mittal, Aruna Bhat
- **Comment**: None
- **Journal**: None
- **Summary**: SARS-CoV-2, also known as COVID-19 or Coronavirus, is a viral contagious disease that is infected by a novel coronavirus, and has been rapidly spreading across the globe. It is very important to test and isolate people to reduce spread, and from here comes the need to do this quickly and efficiently. According to some studies, Chest-CT outperforms RT-PCR lab testing, which is the current standard, when diagnosing COVID-19 patients. Due to this, computer vision researchers have developed various deep learning systems that can predict COVID-19 using a Chest-CT scan correctly to a certain degree. The accuracy of these systems is limited since deep learning neural networks such as CNNs (Convolutional Neural Networks) need a significantly large quantity of data for training in order to produce good quality results. Since the disease is relatively recent and more focus has been on CXR (Chest XRay) images, the available chest CT Scan image dataset is much less. We propose a method, by utilizing GANs, to generate synthetic chest CT images of both positive and negative COVID-19 patients. Using a pre-built predictive model, we concluded that around 40% of the generated images are correctly predicted as COVID-19 positive. The dataset thus generated can be used to train a CNN-based classifier which can help determine COVID-19 in a patient with greater accuracy.



### Covid-19 Detection from Chest X-ray and Patient Metadata using Graph Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.09720v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.09720v2)
- **Published**: 2021-05-20 13:13:29+00:00
- **Updated**: 2021-05-21 12:38:45+00:00
- **Authors**: Thosini Bamunu Mudiyanselage, Nipuna Senanayake, Chunyan Ji, Yi Pan, Yanqing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The novel corona virus (Covid-19) has introduced significant challenges due to its rapid spreading nature through respiratory transmission. As a result, there is a huge demand for Artificial Intelligence (AI) based quick disease diagnosis methods as an alternative to high demand tests such as Polymerase Chain Reaction (PCR). Chest X-ray (CXR) Image analysis is such cost-effective radiography technique due to resource availability and quick screening. But, a sufficient and systematic data collection that is required by complex deep leaning (DL) models is more difficult and hence there are recent efforts that utilize transfer learning to address this issue. Still these transfer learnt models suffer from lack of generalization and increased bias to the training dataset resulting poor performance for unseen data. Limited correlation of the transferred features from the pre-trained model to a specific medical imaging domain like X-ray and overfitting on fewer data can be reasons for this circumstance. In this work, we propose a novel Graph Convolution Neural Network (GCN) that is capable of identifying bio-markers of Covid-19 pneumonia from CXR images and meta information about patients. The proposed method exploits important relational knowledge between data instances and their features using graph representation and applies convolution to learn the graph data which is not possible with conventional convolution on Euclidean domain. The results of extensive experiments of proposed model on binary (Covid vs normal) and three class (Covid, normal, other pneumonia) classification problems outperform different benchmark transfer learnt models, hence overcoming the aforementioned drawbacks.



### Quantifying Topology In Pancreatic Tubular Networks From Live Imaging 3D Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2105.09737v2
- **DOI**: 10.59275/j.melba.2022-4bf2
- **Categories**: **cs.CV**, cs.LG, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2105.09737v2)
- **Published**: 2021-05-20 13:35:44+00:00
- **Updated**: 2022-07-04 15:12:45+00:00
- **Authors**: Kasra Arnavaz, Oswin Krause, Kilian Zepf, Jelena M. Krivokapic, Silja Heilmann, Jakob Andreas Bærentzen, Pia Nyeng, Aasa Feragen
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA)
  https://www.melba-journal.org/papers/2022:015.html"
- **Journal**: None
- **Summary**: Motivated by the challenging segmentation task of pancreatic tubular networks, this paper tackles two commonly encountered problems in biomedical imaging: Topological consistency of the segmentation, and expensive or difficult annotation. Our contributions are the following: a) We propose a topological score which measures both topological and geometric consistency between the predicted and ground truth segmentations, applied to model selection and validation. b) We provide a full deep-learning methodology for this difficult noisy task on time-series image data. In our method, we first use a semisupervised U-net architecture, applicable to generic segmentation tasks, which jointly trains an autoencoder and a segmentation network. We then use tracking of loops over time to further improve the predicted topology. This semi-supervised approach allows us to utilize unannotated data to learn feature representations that generalize to test data with high variability, in spite of our annotated training data having very limited variation. Our contributions are validated on a challenging segmentation task, locating tubular structures in the fetal pancreas from noisy live imaging confocal microscopy. We show that our semi-supervised model outperforms not only fully supervised and pre-trained models but also an approach which takes topological consistency into account during training. Further, our approach achieves a mean loop score of 0.808 for detecting loops in the fetal pancreas, compared to a U-net trained with clDice with mean loop score 0.762.



### Anchor-based Plain Net for Mobile Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2105.09750v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.09750v2)
- **Published**: 2021-05-20 13:52:53+00:00
- **Updated**: 2021-09-25 02:19:22+00:00
- **Authors**: Zongcai Du, Jie Liu, Jie Tang, Gangshan Wu
- **Comment**: accepted by CVPR2021 MAI Workshop
- **Journal**: None
- **Summary**: Along with the rapid development of real-world applications, higher requirements on the accuracy and efficiency of image super-resolution (SR) are brought forward. Though existing methods have achieved remarkable success, the majority of them demand plenty of computational resources and large amount of RAM, and thus they can not be well applied to mobile device. In this paper, we aim at designing efficient architecture for 8-bit quantization and deploy it on mobile device. First, we conduct an experiment about meta-node latency by decomposing lightweight SR architectures, which determines the portable operations we can utilize. Then, we dig deeper into what kind of architecture is beneficial to 8-bit quantization and propose anchor-based plain net (ABPN). Finally, we adopt quantization-aware training strategy to further boost the performance. Our model can outperform 8-bit quantized FSRCNN by nearly 2dB in terms of PSNR, while satisfying realistic needs at the same time. Code is avaliable at https://github.com/NJU- Jet/SR_Mobile_Quantization.



### A Spatio-temporal Attention-based Model for Infant Movement Assessment from Videos
- **Arxiv ID**: http://arxiv.org/abs/2105.09783v1
- **DOI**: 10.1109/JBHI.2021.3077957
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.09783v1)
- **Published**: 2021-05-20 14:31:54+00:00
- **Updated**: 2021-05-20 14:31:54+00:00
- **Authors**: Binh Nguyen-Thai, Vuong Le, Catherine Morgan, Nadia Badawi, Truyen Tran, Svetha Venkatesh
- **Comment**: Accepted by IEEE Journal of Biomedical and Health Informatics (JBHI)
- **Journal**: None
- **Summary**: The absence or abnormality of fidgety movements of joints or limbs is strongly indicative of cerebral palsy in infants. Developing computer-based methods for assessing infant movements in videos is pivotal for improved cerebral palsy screening. Most existing methods use appearance-based features and are thus sensitive to strong but irrelevant signals caused by background clutter or a moving camera. Moreover, these features are computed over the whole frame, thus they measure gross whole body movements rather than specific joint/limb motion.   Addressing these challenges, we develop and validate a new method for fidgety movement assessment from consumer-grade videos using human poses extracted from short clips. Human poses capture only relevant motion profiles of joints and limbs and are thus free from irrelevant appearance artifacts. The dynamics and coordination between joints are modeled using spatio-temporal graph convolutional networks. Frames and body parts that contain discriminative information about fidgety movements are selected through a spatio-temporal attention mechanism. We validate the proposed model on the cerebral palsy screening task using a real-life consumer-grade video dataset collected at an Australian hospital through the Cerebral Palsy Alliance, Australia. Our experiments show that the proposed method achieves the ROC-AUC score of 81.87%, significantly outperforming existing competing methods with better interpretability.



### Weakly-Supervised Physically Unconstrained Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/2105.09803v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09803v1)
- **Published**: 2021-05-20 14:58:52+00:00
- **Updated**: 2021-05-20 14:58:52+00:00
- **Authors**: Rakshit Kothari, Shalini De Mello, Umar Iqbal, Wonmin Byeon, Seonwook Park, Jan Kautz
- **Comment**: CVPR 2021 (Oral)
- **Journal**: None
- **Summary**: A major challenge for physically unconstrained gaze estimation is acquiring training data with 3D gaze annotations for in-the-wild and outdoor scenarios. In contrast, videos of human interactions in unconstrained environments are abundantly available and can be much more easily annotated with frame-level activity labels. In this work, we tackle the previously unexplored problem of weakly-supervised gaze estimation from videos of human interactions. We leverage the insight that strong gaze-related geometric constraints exist when people perform the activity of "looking at each other" (LAEO). To acquire viable 3D gaze supervision from LAEO labels, we propose a training algorithm along with several novel loss functions especially designed for the task. With weak supervision from two large scale CMU-Panoptic and AVA-LAEO activity datasets, we show significant improvements in (a) the accuracy of semi-supervised gaze estimation and (b) cross-domain generalization on the state-of-the-art physically unconstrained in-the-wild Gaze360 gaze estimation benchmark. We open source our code at https://github.com/NVlabs/weakly-supervised-gaze.



### Biologically Inspired Semantic Lateral Connectivity for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.09830v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09830v1)
- **Published**: 2021-05-20 15:24:42+00:00
- **Updated**: 2021-05-20 15:24:42+00:00
- **Authors**: Tonio Weidler, Julian Lehnen, Quinton Denman, Dávid Sebők, Gerhard Weiss, Kurt Driessens, Mario Senden
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Lateral connections play an important role for sensory processing in visual cortex by supporting discriminable neuronal responses even to highly similar features. In the present work, we show that establishing a biologically inspired Mexican hat lateral connectivity profile along the filter domain can significantly improve the classification accuracy of a variety of lightweight convolutional neural networks without the addition of trainable network parameters. Moreover, we demonstrate that it is possible to analytically determine the stationary distribution of modulated filter activations and thereby avoid using recurrence for modeling temporal dynamics. We furthermore reveal that the Mexican hat connectivity profile has the effect of ordering filters in a sequence resembling the topographic organization of feature selectivity in early visual cortex. In an ordered filter sequence, this profile then sharpens the filters' tuning curves.



### M4Depth: Monocular depth estimation for autonomous vehicles in unseen environments
- **Arxiv ID**: http://arxiv.org/abs/2105.09847v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09847v3)
- **Published**: 2021-05-20 15:46:02+00:00
- **Updated**: 2022-07-01 10:08:30+00:00
- **Authors**: Michaël Fonder, Damien Ernst, Marc Van Droogenbroeck
- **Comment**: Main paper: 9 pages, Appendix: 4 pages, References: 2 pages. Code
  available on GitHub: https://github.com/michael-fonder/M4Depth
- **Journal**: None
- **Summary**: Estimating the distance to objects is crucial for autonomous vehicles when using depth sensors is not possible. In this case, the distance has to be estimated from on-board mounted RGB cameras, which is a complex task especially in environments such as natural outdoor landscapes. In this paper, we present a new method named M4Depth for depth estimation. First, we establish a bijective relationship between depth and the visual disparity of two consecutive frames and show how to exploit it to perform motion-invariant pixel-wise depth estimation. Then, we detail M4Depth which is based on a pyramidal convolutional neural network architecture where each level refines an input disparity map estimate by using two customized cost volumes. We use these cost volumes to leverage the visual spatio-temporal constraints imposed by motion and to make the network robust for varied scenes. We benchmarked our approach both in test and generalization modes on public datasets featuring synthetic camera trajectories recorded in a wide variety of outdoor scenes. Results show that our network outperforms the state of the art on these datasets, while also performing well on a standard depth estimation benchmark. The code of our method is publicly available at https://github.com/michael-fonder/M4Depth.



### Flexible Compositional Learning of Structured Visual Concepts
- **Arxiv ID**: http://arxiv.org/abs/2105.09848v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.09848v1)
- **Published**: 2021-05-20 15:48:05+00:00
- **Updated**: 2021-05-20 15:48:05+00:00
- **Authors**: Yanli Zhou, Brenden M. Lake
- **Comment**: Please cite as: Zhou, Y. and Lake, B. M. (2021). Flexible
  compositional learning of structured visual concepts. In Proceedings of the
  43rd Annual Conference of the Cognitive Science Society
- **Journal**: None
- **Summary**: Humans are highly efficient learners, with the ability to grasp the meaning of a new concept from just a few examples. Unlike popular computer vision systems, humans can flexibly leverage the compositional structure of the visual world, understanding new concepts as combinations of existing concepts. In the current paper, we study how people learn different types of visual compositions, using abstract visual forms with rich relational structure. We find that people can make meaningful compositional generalizations from just a few examples in a variety of scenarios, and we develop a Bayesian program induction model that provides a close fit to the behavioral data. Unlike past work examining special cases of compositionality, our work shows how a single computational approach can account for many distinct types of compositional generalization.



### DeepDarts: Modeling Keypoints as Objects for Automatic Scorekeeping in Darts using a Single Camera
- **Arxiv ID**: http://arxiv.org/abs/2105.09880v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.09880v1)
- **Published**: 2021-05-20 16:25:57+00:00
- **Updated**: 2021-05-20 16:25:57+00:00
- **Authors**: William McNally, Pascale Walters, Kanav Vats, Alexander Wong, John McPhee
- **Comment**: None
- **Journal**: None
- **Summary**: Existing multi-camera solutions for automatic scorekeeping in steel-tip darts are very expensive and thus inaccessible to most players. Motivated to develop a more accessible low-cost solution, we present a new approach to keypoint detection and apply it to predict dart scores from a single image taken from any camera angle. This problem involves detecting multiple keypoints that may be of the same class and positioned in close proximity to one another. The widely adopted framework for regressing keypoints using heatmaps is not well-suited for this task. To address this issue, we instead propose to model keypoints as objects. We develop a deep convolutional neural network around this idea and use it to predict dart locations and dartboard calibration points within an overall pipeline for automatic dart scoring, which we call DeepDarts. Additionally, we propose several task-specific data augmentation strategies to improve the generalization of our method. As a proof of concept, two datasets comprising 16k images originating from two different dartboard setups were manually collected and annotated to evaluate the system. In the primary dataset containing 15k images captured from a face-on view of the dartboard using a smartphone, DeepDarts predicted the total score correctly in 94.7% of the test images. In a second more challenging dataset containing limited training data (830 images) and various camera angles, we utilize transfer learning and extensive data augmentation to achieve a test accuracy of 84.0%. Because DeepDarts relies only on single images, it has the potential to be deployed on edge devices, giving anyone with a smartphone access to an automatic dart scoring system for steel-tip darts. The code and datasets are available.



### Error Resilient Collaborative Intelligence via Low-Rank Tensor Completion
- **Arxiv ID**: http://arxiv.org/abs/2105.10341v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.10341v1)
- **Published**: 2021-05-20 16:47:25+00:00
- **Updated**: 2021-05-20 16:47:25+00:00
- **Authors**: Lior Bragilevsky, Ivan V. Bajić
- **Comment**: 2 pages, 1 figure, extended abstract for a poster at IEEE
  Communication Theory Workshop (CTW) 2020 (moved to 2021)
- **Journal**: None
- **Summary**: In the race to bring Artificial Intelligence (AI) to the edge, collaborative intelligence has emerged as a promising way to lighten the computation load on edge devices that run applications based on Deep Neural Networks (DNNs). Typically, a deep model is split at a certain layer into edge and cloud sub-models. The deep feature tensor produced by the edge sub-model is transmitted to the cloud, where the remaining computationally intensive workload is performed by the cloud sub-model. The communication channel between the edge and cloud is imperfect, which will result in missing data in the deep feature tensor received at the cloud side. In this study, we examine the effectiveness of four low-rank tensor completion methods in recovering missing data in the deep feature tensor. We consider both sparse tensors, such as those produced by the VGG16 model, as well as non-sparse tensors, such as those produced by ResNet34 model. We study tensor completion effectiveness in both conplexity-constrained and unconstrained scenario.



### DeepAVO: Efficient Pose Refining with Feature Distilling for Deep Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/2105.09899v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.09899v1)
- **Published**: 2021-05-20 17:05:31+00:00
- **Updated**: 2021-05-20 17:05:31+00:00
- **Authors**: Ran Zhu, Mingkun Yang, Wang Liu, Rujun Song, Bo Yan, Zhuoling Xiao
- **Comment**: 17 pages,14 figures, Neurocomputing Journal
- **Journal**: None
- **Summary**: The technology for Visual Odometry (VO) that estimates the position and orientation of the moving object through analyzing the image sequences captured by on-board cameras, has been well investigated with the rising interest in autonomous driving. This paper studies monocular VO from the perspective of Deep Learning (DL). Unlike most current learning-based methods, our approach, called DeepAVO, is established on the intuition that features contribute discriminately to different motion patterns. Specifically, we present a novel four-branch network to learn the rotation and translation by leveraging Convolutional Neural Networks (CNNs) to focus on different quadrants of optical flow input. To enhance the ability of feature selection, we further introduce an effective channel-spatial attention mechanism to force each branch to explicitly distill related information for specific Frame to Frame (F2F) motion estimation. Experiments on various datasets involving outdoor driving and indoor walking scenarios show that the proposed DeepAVO outperforms the state-of-the-art monocular methods by a large margin, demonstrating competitive performance to the stereo VO algorithm and verifying promising potential for generalization.



### Multi-Perspective Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.09903v2
- **DOI**: 10.3390/s21165311
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.09903v2)
- **Published**: 2021-05-20 17:07:36+00:00
- **Updated**: 2021-08-09 16:56:21+00:00
- **Authors**: Peter Jakob, Manav Madan, Tobias Schmid-Schirling, Abhinav Valada
- **Comment**: 20 pages, 5 figures, 8 tables
- **Journal**: Sensors. 2021; 21(16):5311
- **Summary**: Anomaly detection is a critical problem in the manufacturing industry. In many applications, images of objects to be analyzed are captured from multiple perspectives which can be exploited to improve the robustness of anomaly detection. In this work, we build upon the deep support vector data description algorithm and address multi-perspective anomaly detection using three different fusion techniques, i.e., early fusion, late fusion, and late fusion with multiple decoders. We employ different augmentation techniques with a denoising process to deal with scarce one-class data, which further improves the performance (ROC AUC $= 80\%$). Furthermore, we introduce the dices dataset, which consists of over 2000 grayscale images of falling dices from multiple perspectives, with 5\% of the images containing rare anomalies (e.g., drill holes, sawing, or scratches). We evaluate our approach on the new dices dataset using images from two different perspectives and also benchmark on the standard MNIST dataset. Extensive experiments demonstrate that our proposed {multi-perspective} approach exceeds the state-of-the-art {single-perspective anomaly detection on both the MNIST and dices datasets}. To the best of our knowledge, this is the first work that focuses on addressing multi-perspective anomaly detection in images by jointly using different perspectives together with one single objective function for anomaly detection.



### POCFormer: A Lightweight Transformer Architecture for Detection of COVID-19 Using Point of Care Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/2105.09913v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.09913v1)
- **Published**: 2021-05-20 17:14:01+00:00
- **Updated**: 2021-05-20 17:14:01+00:00
- **Authors**: Shehan Perera, Srikar Adhikari, Alper Yilmaz
- **Comment**: None
- **Journal**: None
- **Summary**: The rapid and seemingly endless expansion of COVID-19 can be traced back to the inefficiency and shortage of testing kits that offer accurate results in a timely manner. An emerging popular technique, which adopts improvements made in mobile ultrasound technology, allows for healthcare professionals to conduct rapid screenings on a large scale. We present an image-based solution that aims at automating the testing process which allows for rapid mass testing to be conducted with or without a trained medical professional that can be applied to rural environments and third world countries. Our contributions towards rapid large-scale testing include a novel deep learning architecture capable of analyzing ultrasound data that can run in real-time and significantly improve the current state-of-the-art detection accuracies using image-based COVID-19 detection.



### Efficient and Robust LiDAR-Based End-to-End Navigation
- **Arxiv ID**: http://arxiv.org/abs/2105.09932v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.09932v1)
- **Published**: 2021-05-20 17:52:37+00:00
- **Updated**: 2021-05-20 17:52:37+00:00
- **Authors**: Zhijian Liu, Alexander Amini, Sibo Zhu, Sertac Karaman, Song Han, Daniela Rus
- **Comment**: ICRA 2021. The first two authors contributed equally to this work.
  Project page: https://le2ed.mit.edu/
- **Journal**: None
- **Summary**: Deep learning has been used to demonstrate end-to-end neural network learning for autonomous vehicle control from raw sensory input. While LiDAR sensors provide reliably accurate information, existing end-to-end driving solutions are mainly based on cameras since processing 3D data requires a large memory footprint and computation cost. On the other hand, increasing the robustness of these systems is also critical; however, even estimating the model's uncertainty is very challenging due to the cost of sampling-based methods. In this paper, we present an efficient and robust LiDAR-based end-to-end navigation framework. We first introduce Fast-LiDARNet that is based on sparse convolution kernel optimization and hardware-aware model design. We then propose Hybrid Evidential Fusion that directly estimates the uncertainty of the prediction from only a single forward pass and then fuses the control predictions intelligently. We evaluate our system on a full-scale vehicle and demonstrate lane-stable as well as navigation capabilities. In the presence of out-of-distribution events (e.g., sensor failures), our system significantly improves robustness and reduces the number of takeovers in the real world.



### Probing the Effect of Selection Bias on Generalization: A Thought Experiment
- **Arxiv ID**: http://arxiv.org/abs/2105.09934v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.10; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2105.09934v2)
- **Published**: 2021-05-20 17:54:48+00:00
- **Updated**: 2022-04-30 14:20:14+00:00
- **Authors**: John K. Tsotsos, Jun Luo
- **Comment**: 26 pages, 6 figures, 2 Tables
- **Journal**: None
- **Summary**: Learned systems in the domain of visual recognition and cognition impress in part because even though they are trained with datasets many orders of magnitude smaller than the full population of possible images, they exhibit sufficient generalization to be applicable to new and previously unseen data. Since training data sets typically represent small sampling of a domain, the possibility of bias in their composition is very real. But what are the limits of generalization given such bias, and up to what point might it be sufficient for a real problem task? Although many have examined issues regarding generalization, this question may require examining the data itself. Here, we focus on the characteristics of the training data that may play a role. Other disciplines have grappled with these problems, most interestingly epidemiology, where experimental bias is a critical concern. The range and nature of data biases seen clinically are really quite relatable to learned vision systems. One obvious way to deal with bias is to ensure a large enough training set, but this might be infeasible for many domains. Another approach might be to perform a statistical analysis of the actual training set, to determine if all aspects of the domain are fairly captured. This too is difficult, in part because the full set of variables might not be known, or perhaps not even knowable. Here, we try a different approach in the tradition of the Thought Experiment, whose most famous instance may be Schr\"odinger's Cat. There are many types of bias as will be seen, but we focus only on one, selection bias. The point of the thought experiment is not to demonstrate problems with all learned systems. Rather, this might be a simple theoretical tool to probe into bias during data collection to highlight deficiencies that might then deserve extra attention either in data collection or system development.



### BodyPressure -- Inferring Body Pose and Contact Pressure from a Depth Image
- **Arxiv ID**: http://arxiv.org/abs/2105.09936v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09936v1)
- **Published**: 2021-05-20 17:55:31+00:00
- **Updated**: 2021-05-20 17:55:31+00:00
- **Authors**: Henry M. Clever, Patrick Grady, Greg Turk, Charles C. Kemp
- **Comment**: 19 pages, 11 figures, 4 tables
- **Journal**: None
- **Summary**: Contact pressure between the human body and its surroundings has important implications. For example, it plays a role in comfort, safety, posture, and health. We present a method that infers contact pressure between a human body and a mattress from a depth image. Specifically, we focus on using a depth image from a downward facing camera to infer pressure on a body at rest in bed occluded by bedding, which is directly applicable to the prevention of pressure injuries in healthcare. Our approach involves augmenting a real dataset with synthetic data generated via a soft-body physics simulation of a human body, a mattress, a pressure sensing mat, and a blanket. We introduce a novel deep network that we trained on an augmented dataset and evaluated with real data. The network contains an embedded human body mesh model and uses a white-box model of depth and pressure image generation. Our network successfully infers body pose, outperforming prior work. It also infers contact pressure across a 3D mesh model of the human body, which is a novel capability, and does so in the presence of occlusion from blankets.



### AnaXNet: Anatomy Aware Multi-label Finding Classification in Chest X-ray
- **Arxiv ID**: http://arxiv.org/abs/2105.09937v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.09937v1)
- **Published**: 2021-05-20 17:58:02+00:00
- **Updated**: 2021-05-20 17:58:02+00:00
- **Authors**: Nkechinyere N. Agu, Joy T. Wu, Hanqing Chao, Ismini Lourentzou, Arjun Sharma, Mehdi Moradi, Pingkun Yan, James Hendler
- **Comment**: Accepted to MICCAI 2021
- **Journal**: None
- **Summary**: Radiologists usually observe anatomical regions of chest X-ray images as well as the overall image before making a decision. However, most existing deep learning models only look at the entire X-ray image for classification, failing to utilize important anatomical information. In this paper, we propose a novel multi-label chest X-ray classification model that accurately classifies the image finding and also localizes the findings to their correct anatomical regions. Specifically, our model consists of two modules, the detection module and the anatomical dependency module. The latter utilizes graph convolutional networks, which enable our model to learn not only the label dependency but also the relationship between the anatomical regions in the chest X-ray. We further utilize a method to efficiently create an adjacency matrix for the anatomical regions using the correlation of the label across the different regions. Detailed experiments and analysis of our results show the effectiveness of our method when compared to the current state-of-the-art multi-label chest X-ray image classification methods while also providing accurate location information.



### Face, Body, Voice: Video Person-Clustering with Multiple Modalities
- **Arxiv ID**: http://arxiv.org/abs/2105.09939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09939v1)
- **Published**: 2021-05-20 17:59:40+00:00
- **Updated**: 2021-05-20 17:59:40+00:00
- **Authors**: Andrew Brown, Vicky Kalogeiton, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: The objective of this work is person-clustering in videos -- grouping characters according to their identity. Previous methods focus on the narrower task of face-clustering, and for the most part ignore other cues such as the person's voice, their overall appearance (hair, clothes, posture), and the editing structure of the videos. Similarly, most current datasets evaluate only the task of face-clustering, rather than person-clustering. This limits their applicability to downstream applications such as story understanding which require person-level, rather than only face-level, reasoning. In this paper we make contributions to address both these deficiencies: first, we introduce a Multi-Modal High-Precision Clustering algorithm for person-clustering in videos using cues from several modalities (face, body, and voice). Second, we introduce a Video Person-Clustering dataset, for evaluating multi-modal person-clustering. It contains body-tracks for each annotated character, face-tracks when visible, and voice-tracks when speaking, with their associated features. The dataset is by far the largest of its kind, and covers films and TV-shows representing a wide range of demographics. Finally, we show the effectiveness of using multiple modalities for person-clustering, explore the use of this new broad task for story understanding through character co-occurrences, and achieve a new state of the art on all available datasets for face and person-clustering.



### Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on Twitter
- **Arxiv ID**: http://arxiv.org/abs/2105.09967v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.09967v1)
- **Published**: 2021-05-20 18:01:05+00:00
- **Updated**: 2021-05-20 18:01:05+00:00
- **Authors**: Boaz Shmueli, Soumya Ray, Lun-Wei Ku
- **Comment**: To be published in ACL 2021. 7 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: Datasets with induced emotion labels are scarce but of utmost importance for many NLP tasks. We present a new, automated method for collecting texts along with their induced reaction labels. The method exploits the online use of reaction GIFs, which capture complex affective states. We show how to augment the data with induced emotion and induced sentiment labels. We use our method to create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K tweets. We provide baselines for three new tasks, including induced sentiment prediction and multilabel classification of induced emotions. Our method and dataset open new research opportunities in emotion detection and affective computing.



### Pseudo Pixel-level Labeling for Images with Evolving Content
- **Arxiv ID**: http://arxiv.org/abs/2105.09975v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.09975v1)
- **Published**: 2021-05-20 18:14:19+00:00
- **Updated**: 2021-05-20 18:14:19+00:00
- **Authors**: Sara Mousavi, Zhenning Yang, Kelley Cross, Dawnie Steadman, Audris Mockus
- **Comment**: None
- **Journal**: None
- **Summary**: Annotating images for semantic segmentation requires intense manual labor and is a time-consuming and expensive task especially for domains with a scarcity of experts, such as Forensic Anthropology. We leverage the evolving nature of images depicting the decay process in human decomposition data to design a simple yet effective pseudo-pixel-level label generation technique to reduce the amount of effort for manual annotation of such images. We first identify sequences of images with a minimum variation that are most suitable to share the same or similar annotation using an unsupervised approach. Given one user-annotated image in each sequence, we propagate the annotation to the remaining images in the sequence by merging it with annotations produced by a state-of-the-art CAM-based pseudo label generation technique. To evaluate the quality of our pseudo-pixel-level labels, we train two semantic segmentation models with VGG and ResNet backbones on images labeled using our pseudo labeling method and those of a state-of-the-art method. The results indicate that using our pseudo-labels instead of those generated using the state-of-the-art method in the training process improves the mean-IoU and the frequency-weighted-IoU of the VGG and ResNet-based semantic segmentation models by 3.36%, 2.58%, 10.39%, and 12.91% respectively.



### Dense Reconstruction of Transparent Objects by Altering Incident Light Paths Through Refraction
- **Arxiv ID**: http://arxiv.org/abs/2105.09993v1
- **DOI**: 10.1007/s11263-017-1045-3
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.09993v1)
- **Published**: 2021-05-20 19:01:12+00:00
- **Updated**: 2021-05-20 19:01:12+00:00
- **Authors**: Kai Han, Kwan-Yee K. Wong, Miaomiao Liu
- **Comment**: International Journal of Computer Vision (IJCV)
- **Journal**: None
- **Summary**: This paper addresses the problem of reconstructing the surface shape of transparent objects. The difficulty of this problem originates from the viewpoint dependent appearance of a transparent object, which quickly makes reconstruction methods tailored for diffuse surfaces fail disgracefully. In this paper, we introduce a fixed viewpoint approach to dense surface reconstruction of transparent objects based on refraction of light. We present a simple setup that allows us to alter the incident light paths before light rays enter the object by immersing the object partially in a liquid, and develop a method for recovering the object surface through reconstructing and triangulating such incident light paths. Our proposed approach does not need to model the complex interactions of light as it travels through the object, neither does it assume any parametric form for the object shape nor the exact number of refractions and reflections taken place along the light paths. It can therefore handle transparent objects with a relatively complex shape and structure, with unknown and inhomogeneous refractive index. We also show that for thin transparent objects, our proposed acquisition setup can be further simplified by adopting a single refraction approximation. Experimental results on both synthetic and real data demonstrate the feasibility and accuracy of our proposed approach.



### VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/2105.09996v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2105.09996v3)
- **Published**: 2021-05-20 19:13:27+00:00
- **Updated**: 2021-09-30 22:43:19+00:00
- **Authors**: Hu Xu, Gargi Ghosh, Po-Yao Huang, Prahal Arora, Masoumeh Aminzadeh, Christoph Feichtenhofer, Florian Metze, Luke Zettlemoyer
- **Comment**: 9 pages, ACL Findings 2021
- **Journal**: None
- **Summary**: We present a simplified, task-agnostic multi-modal pre-training approach that can accept either video or text input, or both for a variety of end tasks. Existing pre-training are task-specific by adopting either a single cross-modal encoder that requires both modalities, limiting their use for retrieval-style end tasks or more complex multitask learning with two unimodal encoders, limiting early cross-modal fusion. We instead introduce new pretraining masking schemes that better mix across modalities (e.g. by forcing masks for text to predict the closest video embeddings) while also maintaining separability (e.g. unimodal predictions are sometimes required, without using all the input). Experimental results show strong performance across a wider range of tasks than any previous methods, often outperforming task-specific pre-training. Code is made available at https://github.com/pytorch/fairseq/tree/main/examples/MMPT.



### Document Domain Randomization for Deep Learning Document Layout Extraction
- **Arxiv ID**: http://arxiv.org/abs/2105.14931v1
- **DOI**: 10.1007/978-3-030-86549-8_32
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.14931v1)
- **Published**: 2021-05-20 19:16:04+00:00
- **Updated**: 2021-05-20 19:16:04+00:00
- **Authors**: Meng Ling, Jian Chen, Torsten Möller, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Robert S. Laramee, Han-Wei Shen, Jian Wu, C. Lee Giles
- **Comment**: Main paper to appear in ICDAR 2021 (16th International Conference on
  Document Analysis and Recognition). This version contains additional
  materials. The associated test data is hosted on IEEE Data Port:
  http://doi.org/10.21227/326q-bf39
- **Journal**: International Conference on Document Analysis and Recognition
  (ICDAR), 2021
- **Summary**: We present document domain randomization (DDR), the first successful transfer of convolutional neural networks (CNNs) trained only on graphically rendered pseudo-paper pages to real-world document segmentation. DDR renders pseudo-document pages by modeling randomized textual and non-textual contents of interest, with user-defined layout and font styles to support joint learning of fine-grained classes. We demonstrate competitive results using our DDR approach to extract nine document classes from the benchmark CS-150 and papers published in two domains, namely annual meetings of Association for Computational Linguistics (ACL) and IEEE Visualization (VIS). We compare DDR to conditions of style mismatch, fewer or more noisy samples that are more easily obtained in the real world. We show that high-fidelity semantic information is not necessary to label semantic classes but style mismatch between train and test can lower model accuracy. Using smaller training samples had a slightly detrimental effect. Finally, network models still achieved high test accuracy when correct labels are diluted towards confusing labels; this behavior hold across several classes.



### Robust Unsupervised Multi-Object Tracking in Noisy Environments
- **Arxiv ID**: http://arxiv.org/abs/2105.10005v4
- **DOI**: 10.1109/ICIP42928.2021.9506029
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2105.10005v4)
- **Published**: 2021-05-20 19:38:03+00:00
- **Updated**: 2021-06-15 06:52:21+00:00
- **Authors**: C. -H. Huck Yang, Mohit Chhabra, Y. -C. Liu, Quan Kong, Tomoaki Yoshinaga, Tomokazu Murakami
- **Comment**: Accepted to IEEE ICIP 2021
- **Journal**: 2021 IEEE International Conference on Image Processing (ICIP)
- **Summary**: Physical processes, camera movement, and unpredictable environmental conditions like the presence of dust can induce noise and artifacts in video feeds. We observe that popular unsupervised MOT methods are dependent on noise-free inputs. We show that the addition of a small amount of artificial random noise causes a sharp degradation in model performance on benchmark metrics. We resolve this problem by introducing a robust unsupervised multi-object tracking (MOT) model: AttU-Net. The proposed single-head attention model helps limit the negative impact of noise by learning visual representations at different segment scales. AttU-Net shows better unsupervised MOT tracking performance over variational inference-based state-of-the-art baselines. We evaluate our method in the MNIST-MOT and the Atari game video benchmark. We also provide two extended video datasets: ``Kuzushiji-MNIST MOT'' which consists of moving Japanese characters and ``Fashion-MNIST MOT'' to validate the effectiveness of the MOT models.



### Opening Deep Neural Networks with Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2105.10013v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.10013v3)
- **Published**: 2021-05-20 20:02:29+00:00
- **Updated**: 2021-06-30 03:00:04+00:00
- **Authors**: Marcos Vendramini, Hugo Oliveira, Alexei Machado, Jefersson A. dos Santos
- **Comment**: None
- **Journal**: None
- **Summary**: Image classification methods are usually trained to perform predictions taking into account a predefined group of known classes. Real-world problems, however, may not allow for a full knowledge of the input and label spaces, making failures in recognition a hazard to deep visual learning. Open set recognition methods are characterized by the ability to correctly identify inputs of known and unknown classes. In this context, we propose GeMOS: simple and plug-and-play open set recognition modules that can be attached to pretrained Deep Neural Networks for visual recognition. The GeMOS framework pairs pre-trained Convolutional Neural Networks with generative models for open set recognition to extract open set scores for each sample, allowing for failure recognition in object recognition tasks. We conduct a thorough evaluation of the proposed method in comparison with state-of-the-art open set algorithms, finding that GeMOS either outperforms or is statistically indistinguishable from more complex and costly models.



### Evaluating Robustness over High Level Driving Instruction for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2105.10014v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.10014v1)
- **Published**: 2021-05-20 20:10:14+00:00
- **Updated**: 2021-05-20 20:10:14+00:00
- **Authors**: Florence Carton, David Filliat, Jaonary Rabarisoa, Quoc Cuong Pham
- **Comment**: Accepted to IV21, 32nd IEEE Intelligent Vehicles Symposium
- **Journal**: None
- **Summary**: In recent years, we have witnessed increasingly high performance in the field of autonomous end-to-end driving. In particular, more and more research is being done on driving in urban environments, where the car has to follow high level commands to navigate. However, few evaluations are made on the ability of these agents to react in an unexpected situation. Specifically, no evaluations are conducted on the robustness of driving agents in the event of a bad high-level command. We propose here an evaluation method, namely a benchmark that allows to assess the robustness of an agent, and to appreciate its understanding of the environment through its ability to keep a safe behavior, regardless of the instruction.



### Improving Generation and Evaluation of Visual Stories via Semantic Consistency
- **Arxiv ID**: http://arxiv.org/abs/2105.10026v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10026v1)
- **Published**: 2021-05-20 20:42:42+00:00
- **Updated**: 2021-05-20 20:42:42+00:00
- **Authors**: Adyasha Maharana, Darryl Hannan, Mohit Bansal
- **Comment**: NAACL 2021 (16 pages)
- **Journal**: None
- **Summary**: Story visualization is an under-explored task that falls at the intersection of many important research directions in both computer vision and natural language processing. In this task, given a series of natural language captions which compose a story, an agent must generate a sequence of images that correspond to the captions. Prior work has introduced recurrent generative models which outperform text-to-image synthesis models on this task. However, there is room for improvement of generated images in terms of visual quality, coherence and relevance. We present a number of improvements to prior modeling approaches, including (1) the addition of a dual learning framework that utilizes video captioning to reinforce the semantic alignment between the story and generated images, (2) a copy-transform mechanism for sequentially-consistent story visualization, and (3) MART-based transformers to model complex interactions between frames. We present ablation studies to demonstrate the effect of each of these techniques on the generative power of the model for both individual images as well as the entire narrative. Furthermore, due to the complexity and generative nature of the task, standard evaluation metrics do not accurately reflect performance. Therefore, we also provide an exploration of evaluation metrics for the model, focused on aspects of the generated frames such as the presence/quality of generated characters, the relevance to captions, and the diversity of the generated images. We also present correlation experiments of our proposed automated metrics with human evaluations. Code and data available at: https://github.com/adymaharana/StoryViz



### Uma implementação do jogo Pedra, Papel e Tesoura utilizando Visao Computacional
- **Arxiv ID**: http://arxiv.org/abs/2105.10063v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10063v1)
- **Published**: 2021-05-20 23:11:36+00:00
- **Updated**: 2021-05-20 23:11:36+00:00
- **Authors**: Ezequiel França dos Santos, Gabriel Fontenelle
- **Comment**: 14 pages, in Portuguese
- **Journal**: None
- **Summary**: This paper presents a game, controlled by computer vision, in identification of hand gestures (hand-tracking). The proposed work is based on image segmentation and construction of a convex hull with Jarvis Algorithm , and determination of the pattern based on the extraction of area characteristics in the convex hull.



