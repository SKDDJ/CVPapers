# Arxiv Papers in cs.CV on 2021-05-28
### Empirical Study of Multi-Task Hourglass Model for Semantic Segmentation Task
- **Arxiv ID**: http://arxiv.org/abs/2105.13531v1
- **DOI**: 10.1109/ACCESS.2021.3085218
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13531v1)
- **Published**: 2021-05-28 01:08:10+00:00
- **Updated**: 2021-05-28 01:08:10+00:00
- **Authors**: Darwin Saire, Adín Ramírez Rivera
- **Comment**: To appear in IEEE Access. Code available at
  https://gitlab.com/mipl/mtl-ss
- **Journal**: None
- **Summary**: The semantic segmentation (SS) task aims to create a dense classification by labeling at the pixel level each object present on images. Convolutional neural network (CNN) approaches have been widely used, and exhibited the best results in this task. However, the loss of spatial precision on the results is a main drawback that has not been solved. In this work, we propose to use a multi-task approach by complementing the semantic segmentation task with edge detection, semantic contour, and distance transform tasks. We propose that by sharing a common latent space, the complementary tasks can produce more robust representations that can enhance the semantic labels. We explore the influence of contour-based tasks on latent space, as well as their impact on the final results of SS. We demonstrate the effectiveness of learning in a multi-task setting for hourglass models in the Cityscapes, CamVid, and Freiburg Forest datasets by improving the state-of-the-art without any refinement post-processing.



### Inertial Sensor Data To Image Encoding For Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2105.13533v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2105.13533v1)
- **Published**: 2021-05-28 01:22:52+00:00
- **Updated**: 2021-05-28 01:22:52+00:00
- **Authors**: Zeeshan Ahmad, Naimul Khan
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are successful deep learning models in the field of computer vision. To get the maximum advantage of CNN model for Human Action Recognition (HAR) using inertial sensor data, in this paper, we use 4 types of spatial domain methods for transforming inertial sensor data to activity images, which are then utilized in a novel fusion framework. These four types of activity images are Signal Images (SI), Gramian Angular Field (GAF) Images, Markov Transition Field (MTF) Images and Recurrence Plot (RP) Images. Furthermore, for creating a multimodal fusion framework and to exploit activity image, we made each type of activity images multimodal by convolving with two spatial domain filters : Prewitt filter and High-boost filter. Resnet-18, a CNN model, is used to learn deep features from multi-modalities. Learned features are extracted from the last pooling layer of each ReNet and then fused by canonical correlation based fusion (CCF) for improving the accuracy of human action recognition. These highly informative features are served as input to a multiclass Support Vector Machine (SVM). Experimental results on three publicly available inertial datasets show the superiority of the proposed method over the current state-of-the-art.



### ECG Heart-beat Classification Using Multimodal Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/2105.13536v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.13536v1)
- **Published**: 2021-05-28 01:31:35+00:00
- **Updated**: 2021-05-28 01:31:35+00:00
- **Authors**: Zeeshan Ahmad, Anika Tabassum, Naimul Khan, Ling Guan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel Image Fusion Model (IFM) for ECG heart-beat classification to overcome the weaknesses of existing machine learning techniques that rely either on manual feature extraction or direct utilization of 1D raw ECG signal. At the input of IFM, we first convert the heart beats of ECG into three different images using Gramian Angular Field (GAF), Recurrence Plot (RP) and Markov Transition Field (MTF) and then fuse these images to create a single imaging modality. We use AlexNet for feature extraction and classification and thus employ end to end deep learning. We perform experiments on PhysioNet MIT-BIH dataset for five different arrhythmias in accordance with the AAMI EC57 standard and on PTB diagnostics dataset for myocardial infarction (MI) classification. We achieved an state of an art results in terms of prediction accuracy, precision and recall.



### Self-supervised Detransformation Autoencoder for Representation Learning in Open Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/2105.13557v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.13557v2)
- **Published**: 2021-05-28 02:45:57+00:00
- **Updated**: 2022-07-06 17:05:44+00:00
- **Authors**: Jingyun Jia, Philip K. Chan
- **Comment**: arXiv admin note: text overlap with arXiv:2006.15117
- **Journal**: None
- **Summary**: The objective of Open set recognition (OSR) is to learn a classifier that can reject the unknown samples while classifying the known classes accurately. In this paper, we propose a self-supervision method, Detransformation Autoencoder (DTAE), for the OSR problem. This proposed method engages in learning representations that are invariant to the transformations of the input data. Experiments on several standard image datasets indicate that the pre-training process significantly improves the model performance in the OSR tasks. Meanwhile, our proposed self-supervision method achieves significant gains in detecting the unknown class and classifying the known classes. Moreover, our analysis indicates that DTAE can yield representations that contain more target class information and less transformation information than RotNet.



### One-shot Learning with Absolute Generalization
- **Arxiv ID**: http://arxiv.org/abs/2105.13559v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.13559v1)
- **Published**: 2021-05-28 02:52:52+00:00
- **Updated**: 2021-05-28 02:52:52+00:00
- **Authors**: Hao Su
- **Comment**: 8 pages, 41 figures
- **Journal**: None
- **Summary**: One-shot learning is proposed to make a pretrained classifier workable on a new dataset based on one labeled samples from each pattern. However, few of researchers consider whether the dataset itself supports one-shot learning. In this paper, we propose a set of definitions to explain what kind of datasets can support one-shot learning and propose the concept "absolute generalization". Based on these definitions, we proposed a method to build an absolutely generalizable classifier. The proposed method concatenates two samples as a new single sample, and converts a classification problem to an identity identification problem or a similarity metric problem. Experiments demonstrate that the proposed method is superior to baseline on one-shot learning datasets and artificial datasets.



### 2nd Place Solution for IJCAI-PRICAI 2020 3D AI Challenge: 3D Object Reconstruction from A Single Image
- **Arxiv ID**: http://arxiv.org/abs/2105.13575v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.13575v1)
- **Published**: 2021-05-28 03:54:39+00:00
- **Updated**: 2021-05-28 03:54:39+00:00
- **Authors**: Yichen Cao, Yufei Wei, Shichao Liu, Lin Xu
- **Comment**: 5 pages, 2 figures, 5 tables
- **Journal**: IJCAI 2020 workshop
- **Summary**: In this paper, we present our solution for the {\it IJCAI--PRICAI--20 3D AI Challenge: 3D Object Reconstruction from A Single Image}. We develop a variant of AtlasNet that consumes single 2D images and generates 3D point clouds through 2D to 3D mapping. To push the performance to the limit and present guidance on crucial implementation choices, we conduct extensive experiments to analyze the influence of decoder design and different settings on the normalization, projection, and sampling methods. Our method achieves 2nd place in the final track with a score of $70.88$, a chamfer distance of $36.87$, and a mean f-score of $59.18$. The source code of our method will be available at https://github.com/em-data/Enhanced_AtlasNet_3DReconstruction.



### MODISSA: a multipurpose platform for the prototypical realization of vehicle-related applications using optical sensors
- **Arxiv ID**: http://arxiv.org/abs/2105.13580v1
- **DOI**: 10.1364/AO.423599
- **Categories**: **cs.CV**, cs.SY, eess.IV, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2105.13580v1)
- **Published**: 2021-05-28 04:21:39+00:00
- **Updated**: 2021-05-28 04:21:39+00:00
- **Authors**: Björn Borgmann, Volker Schatz, Marcus Hammer, Marcus Hebel, Michael Arens, Uwe Stilla
- **Comment**: Authors' version of an article accepted for publication in Applied
  Optics, 9 May 2021
- **Journal**: Applied Optic 60(22), pp. F50-F65, 2021
- **Summary**: We present the current state of development of the sensor-equipped car MODISSA, with which Fraunhofer IOSB realizes a configurable experimental platform for hardware evaluation and software development in the context of mobile mapping and vehicle-related safety and protection. MODISSA is based on a van that has successively been equipped with a variety of optical sensors over the past few years, and contains hardware for complete raw data acquisition, georeferencing, real-time data analysis, and immediate visualization on in-car displays. We demonstrate the capabilities of MODISSA by giving a deeper insight into experiments with its specific configuration in the scope of three different applications. Other research groups can benefit from these experiences when setting up their own mobile sensor system, especially regarding the selection of hardware and software, the knowledge of possible sources of error, and the handling of the acquired sensor data.



### Semi-supervised Anatomical Landmark Detection via Shape-regulated Self-training
- **Arxiv ID**: http://arxiv.org/abs/2105.13593v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13593v2)
- **Published**: 2021-05-28 05:23:07+00:00
- **Updated**: 2021-11-27 09:29:49+00:00
- **Authors**: Runnan Chen, Yuexin Ma, Lingjie Liu, Nenglun Chen, Zhiming Cui, Guodong Wei, Wenping Wang
- **Comment**: Accepted to Neurocomputing
- **Journal**: None
- **Summary**: Well-annotated medical images are costly and sometimes even impossible to acquire, hindering landmark detection accuracy to some extent. Semi-supervised learning alleviates the reliance on large-scale annotated data by exploiting the unlabeled data to understand the population structure of anatomical landmarks. The global shape constraint is the inherent property of anatomical landmarks that provides valuable guidance for more consistent pseudo labelling of the unlabeled data, which is ignored in the previously semi-supervised methods. In this paper, we propose a model-agnostic shape-regulated self-training framework for semi-supervised landmark detection by fully considering the global shape constraint. Specifically, to ensure pseudo labels are reliable and consistent, a PCA-based shape model adjusts pseudo labels and eliminate abnormal ones. A novel Region Attention loss to make the network automatically focus on the structure consistent regions around pseudo labels. Extensive experiments show that our approach outperforms other semi-supervised methods and achieves notable improvement on three medical image datasets. Moreover, our framework is flexible and can be used as a plug-and-play module integrated into most supervised methods to improve performance further.



### KVT: k-NN Attention for Boosting Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2106.00515v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00515v3)
- **Published**: 2021-05-28 06:49:10+00:00
- **Updated**: 2022-07-22 23:18:16+00:00
- **Authors**: Pichao Wang, Xue Wang, Fan Wang, Ming Lin, Shuning Chang, Hao Li, Rong Jin
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have dominated computer vision for years, due to its ability in capturing locality and translation invariance. Recently, many vision transformer architectures have been proposed and they show promising performance. A key component in vision transformers is the fully-connected self-attention which is more powerful than CNNs in modelling long range dependencies. However, since the current dense self-attention uses all image patches (tokens) to compute attention matrix, it may neglect locality of images patches and involve noisy tokens (e.g., clutter background and occlusion), leading to a slow training process and potential degradation of performance. To address these problems, we propose the $k$-NN attention for boosting vision transformers. Specifically, instead of involving all the tokens for attention matrix calculation, we only select the top-$k$ similar tokens from the keys for each query to compute the attention map. The proposed $k$-NN attention naturally inherits the local bias of CNNs without introducing convolutional operations, as nearby tokens tend to be more similar than others. In addition, the $k$-NN attention allows for the exploration of long range correlation and at the same time filters out irrelevant tokens by choosing the most similar tokens from the entire image. Despite its simplicity, we verify, both theoretically and empirically, that $k$-NN attention is powerful in speeding up training and distilling noise from input tokens. Extensive experiments are conducted by using 11 different vision transformer architectures to verify that the proposed $k$-NN attention can work with any existing transformer architectures to improve its prediction performance. The codes are available at \url{https://github.com/damo-cv/KVT}.



### FReTAL: Generalizing Deepfake Detection using Knowledge Distillation and Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.13617v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2105.13617v1)
- **Published**: 2021-05-28 06:54:10+00:00
- **Updated**: 2021-05-28 06:54:10+00:00
- **Authors**: Minha Kim, Shahroz Tariq, Simon S. Woo
- **Comment**: 12 pages, 2 figures, 5 tables, accepted for publication at the
  Workshop on Media Forensics 2021
- **Journal**: None
- **Summary**: As GAN-based video and image manipulation technologies become more sophisticated and easily accessible, there is an urgent need for effective deepfake detection technologies. Moreover, various deepfake generation techniques have emerged over the past few years. While many deepfake detection methods have been proposed, their performance suffers from new types of deepfake methods on which they are not sufficiently trained. To detect new types of deepfakes, the model should learn from additional data without losing its prior knowledge about deepfakes (catastrophic forgetting), especially when new deepfakes are significantly different. In this work, we employ the Representation Learning (ReL) and Knowledge Distillation (KD) paradigms to introduce a transfer learning-based Feature Representation Transfer Adaptation Learning (FReTAL) method. We use FReTAL to perform domain adaptation tasks on new deepfake datasets while minimizing catastrophic forgetting. Our student model can quickly adapt to new types of deepfake by distilling knowledge from a pre-trained teacher model and applying transfer learning without using source domain data during domain adaptation. Through experiments on FaceForensics++ datasets, we demonstrate that FReTAL outperforms all baselines on the domain adaptation task with up to 86.97% accuracy on low-quality deepfakes.



### Deception Detection in Videos using the Facial Action Coding System
- **Arxiv ID**: http://arxiv.org/abs/2105.13659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13659v1)
- **Published**: 2021-05-28 08:10:21+00:00
- **Updated**: 2021-05-28 08:10:21+00:00
- **Authors**: Hammad Ud Din Ahmed, Usama Ijaz Bajwa, Fan Zhang, Muhammad Waqas Anwar
- **Comment**: None
- **Journal**: None
- **Summary**: Facts are important in decision making in every situation, which is why it is important to catch deceptive information before they are accepted as facts. Deception detection in videos has gained traction in recent times for its various real-life application. In our approach, we extract facial action units using the facial action coding system which we use as parameters for training a deep learning model. We specifically use long short-term memory (LSTM) which we trained using the real-life trial dataset and it provided one of the best facial only approaches to deception detection. We also tested cross-dataset validation using the Real-life trial dataset, the Silesian Deception Dataset, and the Bag-of-lies Deception Dataset which has not yet been attempted by anyone else for a deception detection system. We tested and compared all datasets amongst each other individually and collectively using the same deep learning training model. The results show that adding different datasets for training worsen the accuracy of the model. One of the primary reasons is that the nature of these datasets vastly differs from one another.



### Highlight Timestamp Detection Model for Comedy Videos via Multimodal Sentiment Analysis
- **Arxiv ID**: http://arxiv.org/abs/2106.00451v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2106.00451v1)
- **Published**: 2021-05-28 08:39:19+00:00
- **Updated**: 2021-05-28 08:39:19+00:00
- **Authors**: Fan Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, the videos on the Internet are prevailing. The precise and in-depth understanding of the videos is a difficult but valuable problem for both platforms and researchers. The existing video understand models do well in object recognition tasks but currently still cannot understand the abstract and contextual features like highlight humor frames in comedy videos. The current industrial works are also mainly focused on the basic category classification task based on the appearances of objects. The feature detection methods for the abstract category remains blank. A data structure that includes the information of video frames, audio spectrum and texts provide a new direction to explore. The multimodal models are proposed to make this in-depth video understanding mission possible. In this paper, we analyze the difficulties in abstract understanding of videos and propose a multimodal structure to obtain state-of-the-art performance in this field. Then we select several benchmarks for multimodal video understanding and apply the most suitable model to find the best performance. At last, we evaluate the overall spotlights and drawbacks of the models and methods in this paper and point out the possible directions for further improvements.



### ResT: An Efficient Transformer for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2105.13677v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13677v5)
- **Published**: 2021-05-28 08:53:54+00:00
- **Updated**: 2021-10-14 08:43:50+00:00
- **Authors**: Qinglong Zhang, Yubin Yang
- **Comment**: ResT is an efficient multi-scale vision Transformer that can tackle
  input images with arbitrary size. arXiv admin note: text overlap with
  arXiv:2103.14030 by other authors
- **Journal**: None
- **Summary**: This paper presents an efficient multi-scale vision Transformer, called ResT, that capably served as a general-purpose backbone for image recognition. Unlike existing Transformer methods, which employ standard Transformer blocks to tackle raw images with a fixed resolution, our ResT have several advantages: (1) A memory-efficient multi-head self-attention is built, which compresses the memory by a simple depth-wise convolution, and projects the interaction across the attention-heads dimension while keeping the diversity ability of multi-heads; (2) Position encoding is constructed as spatial attention, which is more flexible and can tackle with input images of arbitrary size without interpolation or fine-tune; (3) Instead of the straightforward tokenization at the beginning of each stage, we design the patch embedding as a stack of overlapping convolution operation with stride on the 2D-reshaped token map. We comprehensively validate ResT on image classification and downstream tasks. Experimental results show that the proposed ResT can outperform the recently state-of-the-art backbones by a large margin, demonstrating the potential of ResT as strong backbones. The code and models will be made publicly available at https://github.com/wofmanaf/ResT.



### Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
- **Arxiv ID**: http://arxiv.org/abs/2105.13680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13680v1)
- **Published**: 2021-05-28 08:59:14+00:00
- **Updated**: 2021-05-28 08:59:14+00:00
- **Authors**: Zhan Qu, Huan Jin, Yang Zhou, Zhen Yang, Wei Zhang
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Mainstream lane marker detection methods are implemented by predicting the overall structure and deriving parametric curves through post-processing. Complex lane line shapes require high-dimensional output of CNNs to model global structures, which further increases the demand for model capacity and training data. In contrast, the locality of a lane marker has finite geometric variations and spatial coverage. We propose a novel lane marker detection solution, FOLOLane, that focuses on modeling local patterns and achieving prediction of global structures in a bottom-up manner. Specifically, the CNN models lowcomplexity local patterns with two separate heads, the first one predicts the existence of key points, and the second refines the location of key points in the local range and correlates key points of the same lane line. The locality of the task is consistent with the limited FOV of the feature in CNN, which in turn leads to more stable training and better generalization. In addition, an efficiency-oriented decoding algorithm was proposed as well as a greedy one, which achieving 36% runtime gains at the cost of negligible performance degradation. Both of the two decoders integrated local information into the global geometry of lane markers. In the absence of a complex network architecture design, the proposed method greatly outperforms all existing methods on public datasets while achieving the best state-of-the-art results and real-time processing simultaneously.



### Learning Uncertainty For Safety-Oriented Semantic Segmentation In Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2105.13688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13688v1)
- **Published**: 2021-05-28 09:23:05+00:00
- **Updated**: 2021-05-28 09:23:05+00:00
- **Authors**: Victor Besnier, David Picard, Alexandre Briot
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we show how uncertainty estimation can be leveraged to enable safety critical image segmentation in autonomous driving, by triggering a fallback behavior if a target accuracy cannot be guaranteed. We introduce a new uncertainty measure based on disagreeing predictions as measured by a dissimilarity function. We propose to estimate this dissimilarity by training a deep neural architecture in parallel to the task-specific network. It allows this observer to be dedicated to the uncertainty estimation, and let the task-specific network make predictions. We propose to use self-supervision to train the observer, which implies that our method does not require additional training data. We show experimentally that our proposed approach is much less computationally intensive at inference time than competing methods (e.g. MCDropout), while delivering better results on safety-oriented evaluation metrics on the CamVid dataset, especially in the case of glare artifacts.



### AutoSampling: Search for Effective Data Sampling Schedules
- **Arxiv ID**: http://arxiv.org/abs/2105.13695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13695v1)
- **Published**: 2021-05-28 09:39:41+00:00
- **Updated**: 2021-05-28 09:39:41+00:00
- **Authors**: Ming Sun, Haoxuan Dou, Baopu Li, Lei Cui, Junjie Yan, Wanli Ouyang
- **Comment**: Automl for sampling firstly without any assumpation
- **Journal**: ICML 2021
- **Summary**: Data sampling acts as a pivotal role in training deep learning models. However, an effective sampling schedule is difficult to learn due to the inherently high dimension of parameters in learning the sampling schedule. In this paper, we propose an AutoSampling method to automatically learn sampling schedules for model training, which consists of the multi-exploitation step aiming for optimal local sampling schedules and the exploration step for the ideal sampling distribution. More specifically, we achieve sampling schedule search with shortened exploitation cycle to provide enough supervision. In addition, we periodically estimate the sampling distribution from the learned sampling schedules and perturb it to search in the distribution space. The combination of two searches allows us to learn a robust sampling schedule. We apply our AutoSampling method to a variety of image classification tasks illustrating the effectiveness of the proposed method.



### DeepTag: A General Framework for Fiducial Marker Design and Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.13731v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.13731v2)
- **Published**: 2021-05-28 10:54:59+00:00
- **Updated**: 2022-05-10 02:24:39+00:00
- **Authors**: Zhuming Zhang, Yongtao Hu, Guoxing Yu, Jingwen Dai
- **Comment**: accepted to IEEE TPAMI
- **Journal**: None
- **Summary**: A fiducial marker system usually consists of markers, a detection algorithm, and a coding system. The appearance of markers and the detection robustness are generally limited by the existing detection algorithms, which are hand-crafted with traditional low-level image processing techniques. Furthermore, a sophisticatedly designed coding system is required to overcome the shortcomings of both markers and detection algorithms. To improve the flexibility and robustness in various applications, we propose a general deep learning based framework, DeepTag, for fiducial marker design and detection. DeepTag not only supports detection of a wide variety of existing marker families, but also makes it possible to design new marker families with customized local patterns. Moreover, we propose an effective procedure to synthesize training data on the fly without manual annotations. Thus, DeepTag can easily adapt to existing and newly-designed marker families. To validate DeepTag and existing methods, beside existing datasets, we further collect a new large and challenging dataset where markers are placed in different view distances and angles. Experiments show that DeepTag well supports different marker families and greatly outperforms the existing methods in terms of both detection robustness and pose accuracy. Both code and dataset are available at https://herohuyongtao.github.io/research/publications/deep-tag/.



### New Encoder Learning for Captioning Heavy Rain Images via Semantic Visual Feature Matching
- **Arxiv ID**: http://arxiv.org/abs/2105.13753v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13753v3)
- **Published**: 2021-05-28 11:40:40+00:00
- **Updated**: 2021-09-15 15:21:14+00:00
- **Authors**: Chang-Hwan Son, Pung-Hwi Ye
- **Comment**: None
- **Journal**: Journal of Imaging Science and Technology, Sept. 2021
- **Summary**: Image captioning generates text that describes scenes from input images. It has been developed for high quality images taken in clear weather. However, in bad weather conditions, such as heavy rain, snow, and dense fog, the poor visibility owing to rain streaks, rain accumulation, and snowflakes causes a serious degradation of image quality. This hinders the extraction of useful visual features and results in deteriorated image captioning performance. To address practical issues, this study introduces a new encoder for captioning heavy rain images. The central idea is to transform output features extracted from heavy rain input images into semantic visual features associated with words and sentence context. To achieve this, a target encoder is initially trained in an encoder-decoder framework to associate visual features with semantic words. Subsequently, the objects in a heavy rain image are rendered visible by using an initial reconstruction subnetwork (IRS) based on a heavy rain model. The IRS is then combined with another semantic visual feature matching subnetwork (SVFMS) to match the output features of the IRS with the semantic visual features of the pretrained target encoder. The proposed encoder is based on the joint learning of the IRS and SVFMS. It is is trained in an end-to-end manner, and then connected to the pretrained decoder for image captioning. It is experimentally demonstrated that the proposed encoder can generate semantic visual features associated with words even from heavy rain images, thereby increasing the accuracy of the generated captions.



### Chromatic and spatial analysis of one-pixel attacks against an image classifier
- **Arxiv ID**: http://arxiv.org/abs/2105.13771v4
- **DOI**: 10.1007/978-3-031-17436-0_20
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.13771v4)
- **Published**: 2021-05-28 12:21:58+00:00
- **Updated**: 2022-09-28 07:04:34+00:00
- **Authors**: Janne Alatalo, Joni Korpihalkola, Tuomo Sipola, Tero Kokkonen
- **Comment**: The 10th International Conference on Networked Systems (NETYS 2022)
- **Journal**: None
- **Summary**: One-pixel attack is a curious way of deceiving neural network classifier by changing only one pixel in the input image. The full potential and boundaries of this attack method are not yet fully understood. In this research, the successful and unsuccessful attacks are studied in more detail to illustrate the working mechanisms of a one-pixel attack created using differential evolution. The data comes from our earlier studies where we applied the attack against medical imaging. We used a real breast cancer tissue dataset and a real classifier as the attack target. This research presents ways to analyze chromatic and spatial distributions of one-pixel attacks. In addition, we present one-pixel attack confidence maps to illustrate the behavior of the target classifier. We show that the more effective attacks change the color of the pixel more, and that the successful attacks are situated at the center of the images. This kind of analysis is not only useful for understanding the behavior of the attack but also the qualities of the classifying neural network.



### Using Convolutional Neural Networks for Relative Pose Estimation of a Non-Cooperative Spacecraft with Thermal Infrared Imagery
- **Arxiv ID**: http://arxiv.org/abs/2105.13789v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2105.13789v1)
- **Published**: 2021-05-28 12:51:38+00:00
- **Updated**: 2021-05-28 12:51:38+00:00
- **Authors**: Maxwell Hogan, Duarte Rondao, Nabil Aouf, Olivier Dubois-Matra
- **Comment**: 14 pages; 11 figures; European Space Agency Guidance, Navigation and
  Control Conference 2021
- **Journal**: None
- **Summary**: Recent interest in on-orbit servicing and Active Debris Removal (ADR) missions have driven the need for technologies to enable non-cooperative rendezvous manoeuvres. Such manoeuvres put heavy burden on the perception capabilities of a chaser spacecraft. This paper demonstrates Convolutional Neural Networks (CNNs) capable of providing an initial coarse pose estimation of a target from a passive thermal infrared camera feed. Thermal cameras offer a promising alternative to visible cameras, which struggle in low light conditions and are susceptible to overexposure. Often, thermal information on the target is not available a priori; this paper therefore proposes using visible images to train networks. The robustness of the models is demonstrated on two different targets, first on synthetic data, and then in a laboratory environment for a realistic scenario that might be faced during an ADR mission. Given that there is much concern over the use of CNN in critical applications due to their black box nature, we use innovative techniques to explain what is important to our network and fault conditions.



### A systematic review of transfer learning based approaches for diabetic retinopathy detection
- **Arxiv ID**: http://arxiv.org/abs/2105.13793v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.13793v1)
- **Published**: 2021-05-28 12:58:31+00:00
- **Updated**: 2021-05-28 12:58:31+00:00
- **Authors**: Burcu Oltu, Büşra Kübra Karaca, Hamit Erdem, Atilla Özgür
- **Comment**: 25 pages 9 figures 10 tables
- **Journal**: None
- **Summary**: Cases of diabetes and related diabetic retinopathy (DR) have been increasing at an alarming rate in modern times. Early detection of DR is an important problem since it may cause permanent blindness in the late stages. In the last two decades, many different approaches have been applied in DR detection. Reviewing academic literature shows that deep neural networks (DNNs) have become the most preferred approach for DR detection. Among these DNN approaches, Convolutional Neural Network (CNN) models are the most used ones in the field of medical image classification. Designing a new CNN architecture is a tedious and time-consuming approach. Additionally, training an enormous number of parameters is also a difficult task. Due to this reason, instead of training CNNs from scratch, using pre-trained models has been suggested in recent years as transfer learning approach. Accordingly, the present study as a review focuses on DNN and Transfer Learning based applications of DR detection considering 38 publications between 2015 and 2020. The published papers are summarized using 9 figures and 10 tables, giving information about 22 pre-trained CNN models, 12 DR data sets and standard performance metrics.



### The Wits Intelligent Teaching System: Detecting Student Engagement During Lectures Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.13794v1
- **DOI**: 10.1109/ICIP.2017.8296804
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13794v1)
- **Published**: 2021-05-28 12:59:37+00:00
- **Updated**: 2021-05-28 12:59:37+00:00
- **Authors**: Richard Klein, Turgay Celik
- **Comment**: None
- **Journal**: 2017 IEEE International Conference on Image Processing (ICIP),
  2017, pp. 2856-2860
- **Summary**: To perform contingent teaching and be responsive to students' needs during class, lecturers must be able to quickly assess the state of their audience. While effective teachers are able to gauge easily the affective state of the students, as class sizes grow this becomes increasingly difficult and less precise. The Wits Intelligent Teaching System (WITS) aims to assist lecturers with real-time feedback regarding student affect. The focus is primarily on recognising engagement or lack thereof. Student engagement is labelled based on behaviour and postures that are common to classroom settings. These proxies are then used in an observational checklist to construct a dataset of engagement upon which a CNN based on AlexNet is successfully trained and which significantly outperforms a Support Vector Machine approach. The deep learning approach provides satisfactory results on a challenging, real-world dataset with significant occlusion, lighting and resolution constraints.



### The Herbarium 2021 Half-Earth Challenge Dataset
- **Arxiv ID**: http://arxiv.org/abs/2105.13808v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13808v1)
- **Published**: 2021-05-28 13:24:12+00:00
- **Updated**: 2021-05-28 13:24:12+00:00
- **Authors**: Riccardo de Lutio, Damon Little, Barbara Ambrose, Serge Belongie
- **Comment**: FGVC8 Workshop at CVPR 2021
- **Journal**: None
- **Summary**: Herbarium sheets present a unique view of the world's botanical history, evolution, and diversity. This makes them an all-important data source for botanical research. With the increased digitisation of herbaria worldwide and the advances in the fine-grained classification domain that can facilitate automatic identification of herbarium specimens, there are a lot of opportunities for supporting research in this field. However, existing datasets are either too small, or not diverse enough, in terms of represented taxa, geographic distribution or host institutions. Furthermore, aggregating multiple datasets is difficult as taxa exist under a multitude of different names and the taxonomy requires alignment to a common reference. We present the Herbarium Half-Earth dataset, the largest and most diverse dataset of herbarium specimens to date for automatic taxon recognition.



### Improving Facial Attribute Recognition by Group and Graph Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.13825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13825v1)
- **Published**: 2021-05-28 13:36:28+00:00
- **Updated**: 2021-05-28 13:36:28+00:00
- **Authors**: Zhenghao Chen, Shuhang Gu, Feng Zhu, Jing Xu, Rui Zhao
- **Comment**: ICME2021(Oral)
- **Journal**: None
- **Summary**: Exploiting the relationships between attributes is a key challenge for improving multiple facial attribute recognition. In this work, we are concerned with two types of correlations that are spatial and non-spatial relationships. For the spatial correlation, we aggregate attributes with spatial similarity into a part-based group and then introduce a Group Attention Learning to generate the group attention and the part-based group feature. On the other hand, to discover the non-spatial relationship, we model a group-based Graph Correlation Learning to explore affinities of predefined part-based groups. We utilize such affinity information to control the communication between all groups and then refine the learned group features. Overall, we propose a unified network called Multi-scale Group and Graph Network. It incorporates these two newly proposed learning strategies and produces coarse-to-fine graph-based group features for improving facial attribute recognition. Comprehensive experiments demonstrate that our approach outperforms the state-of-the-art methods.



### Recursive Contour Saliency Blending Network for Accurate Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.13865v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13865v3)
- **Published**: 2021-05-28 14:19:54+00:00
- **Updated**: 2021-08-22 07:17:18+00:00
- **Authors**: Yi Ke Yun, Takahiro Tsubono
- **Comment**: None
- **Journal**: None
- **Summary**: Contour information plays a vital role in salient object detection. However, excessive false positives remain in predictions from existing contour-based models due to insufficient contour-saliency fusion. In this work, we designed a network for better edge quality in salient object detection. We proposed a contour-saliency blending module to exchange information between contour and saliency. We adopted recursive CNN to increase contour-saliency fusion while keeping the total trainable parameters the same. Furthermore, we designed a stage-wise feature extraction module to help the model pick up the most helpful features from previous intermediate saliency predictions. Besides, we proposed two new loss functions, namely Dual Confinement Loss and Confidence Loss, for our model to generate better boundary predictions. Evaluation results on five common benchmark datasets reveal that our model achieves competitive state-of-the-art performance.



### Learning Relation Alignment for Calibrated Cross-modal Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2105.13868v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2105.13868v2)
- **Published**: 2021-05-28 14:25:49+00:00
- **Updated**: 2021-06-01 05:16:22+00:00
- **Authors**: Shuhuai Ren, Junyang Lin, Guangxiang Zhao, Rui Men, An Yang, Jingren Zhou, Xu Sun, Hongxia Yang
- **Comment**: Accepted by ACL-IJCNLP 2021 main conference (Long Paper)
- **Journal**: None
- **Summary**: Despite the achievements of large-scale multimodal pre-training approaches, cross-modal retrieval, e.g., image-text retrieval, remains a challenging task. To bridge the semantic gap between the two modalities, previous studies mainly focus on word-region alignment at the object level, lacking the matching between the linguistic relation among the words and the visual relation among the regions. The neglect of such relation consistency impairs the contextualized representation of image-text pairs and hinders the model performance and the interpretability. In this paper, we first propose a novel metric, Intra-modal Self-attention Distance (ISD), to quantify the relation consistency by measuring the semantic distance between linguistic and visual relations. In response, we present Inter-modal Alignment on Intra-modal Self-attentions (IAIS), a regularized training method to optimize the ISD and calibrate intra-modal self-attentions from the two modalities mutually via inter-modal alignment. The IAIS regularizer boosts the performance of prevailing models on Flickr30k and MS COCO datasets by a considerable margin, which demonstrates the superiority of our approach.



### Demotivate adversarial defense in remote sensing
- **Arxiv ID**: http://arxiv.org/abs/2105.13902v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.13902v1)
- **Published**: 2021-05-28 15:04:37+00:00
- **Updated**: 2021-05-28 15:04:37+00:00
- **Authors**: Adrien Chan-Hon-Tong, Gaston Lenczner, Aurelien Plyer
- **Comment**: 4 pages, 1 figure, 2 tables. Conference IGARSS 2021
- **Journal**: None
- **Summary**: Convolutional neural networks are currently the state-of-the-art algorithms for many remote sensing applications such as semantic segmentation or object detection. However, these algorithms are extremely sensitive to over-fitting, domain change and adversarial examples specifically designed to fool them. While adversarial attacks are not a threat in most remote sensing applications, one could wonder if strengthening networks to adversarial attacks could also increase their resilience to over-fitting and their ability to deal with the inherent variety of worldwide data. In this work, we study both adversarial retraining and adversarial regularization as adversarial defenses to this purpose. However, we show through several experiments on public remote sensing datasets that adversarial robustness seems uncorrelated to geographic and over-fitting robustness.



### Training of SSD(Single Shot Detector) for Facial Detection using Nvidia Jetson Nano
- **Arxiv ID**: http://arxiv.org/abs/2105.13906v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.DC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.13906v1)
- **Published**: 2021-05-28 15:16:24+00:00
- **Updated**: 2021-05-28 15:16:24+00:00
- **Authors**: Saif Ur Rehman, Muhammad Rashid Razzaq, Muhammad Hadi Hussian
- **Comment**: 7 Pages, 7 figures
- **Journal**: None
- **Summary**: In this project, we have used the computer vision algorithm SSD (Single Shot detector) computer vision algorithm and trained this algorithm from the dataset which consists of 139 Pictures. Images were labeled using Intel CVAT (Computer Vision Annotation Tool)   We trained this model for facial detection. We have deployed our trained model and software in the Nvidia Jetson Nano Developer kit. Model code is written in Pytorch's deep learning framework. The programming language used is Python.



### Geometric Deep Learning and Equivariant Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.13926v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, hep-th
- **Links**: [PDF](http://arxiv.org/pdf/2105.13926v1)
- **Published**: 2021-05-28 15:41:52+00:00
- **Updated**: 2021-05-28 15:41:52+00:00
- **Authors**: Jan E. Gerken, Jimmy Aronsson, Oscar Carlsson, Hampus Linander, Fredrik Ohlsson, Christoffer Petersson, Daniel Persson
- **Comment**: 57 pages
- **Journal**: None
- **Summary**: We survey the mathematical foundations of geometric deep learning, focusing on group equivariant and gauge equivariant neural networks. We develop gauge equivariant convolutional neural networks on arbitrary manifolds $\mathcal{M}$ using principal bundles with structure group $K$ and equivariant maps between sections of associated vector bundles. We also discuss group equivariant neural networks for homogeneous spaces $\mathcal{M}=G/K$, which are instead equivariant with respect to the global symmetry $G$ on $\mathcal{M}$. Group equivariant layers can be interpreted as intertwiners between induced representations of $G$, and we show their relation to gauge equivariant convolutional layers. We analyze several applications of this formalism, including semantic segmentation and object detection networks. We also discuss the case of spherical networks in great detail, corresponding to the case $\mathcal{M}=S^2=\mathrm{SO}(3)/\mathrm{SO}(2)$. Here we emphasize the use of Fourier analysis involving Wigner matrices, spherical harmonics and Clebsch-Gordan coefficients for $G=\mathrm{SO}(3)$, illustrating the power of representation theory for deep learning.



### Predicting the Solar Potential of Rooftops using Image Segmentation and Structured Data
- **Arxiv ID**: http://arxiv.org/abs/2106.15268v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15268v1)
- **Published**: 2021-05-28 15:49:13+00:00
- **Updated**: 2021-05-28 15:49:13+00:00
- **Authors**: Daniel de Barros Soares, François Andrieux, Bastien Hell, Julien Lenhardt, Jordi Badosa, Sylvain Gavoille, Stéphane Gaiffas, Emmanuel Bacry
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating the amount of electricity that can be produced by rooftop photovoltaic systems is a time-consuming process that requires on-site measurements, a difficult task to achieve on a large scale. In this paper, we present an approach to estimate the solar potential of rooftops based on their location and architectural characteristics, as well as the amount of solar radiation they receive annually. Our technique uses computer vision to achieve semantic segmentation of roof sections and roof objects on the one hand, and a machine learning model based on structured building features to predict roof pitch on the other hand. We then compute the azimuth and maximum number of solar panels that can be installed on a rooftop with geometric approaches. Finally, we compute precise shading masks and combine them with solar irradiation data that enables us to estimate the yearly solar potential of a rooftop.



### Unsupervised detection of mouse behavioural anomalies using two-stream convolutional autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2106.00598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.00598v1)
- **Published**: 2021-05-28 16:30:09+00:00
- **Updated**: 2021-05-28 16:30:09+00:00
- **Authors**: Ezechukwu I Nwokedi, Rasneer S Bains, Luc Bidaut, Sara Wells, Xujiong Ye, James M Brown
- **Comment**: None
- **Journal**: None
- **Summary**: This paper explores the application of unsupervised learning to detecting anomalies in mouse video data. The two models presented in this paper are a dual-stream, 3D convolutional autoencoder (with residual connections) and a dual-stream, 2D convolutional autoencoder. The publicly available dataset used here contains twelve videos of single home-caged mice alongside frame-level annotations. Under the pretext that the autoencoder only sees normal events, the video data was handcrafted to treat each behaviour as a pseudo-anomaly thereby eliminating them from the others during training. The results are presented for one conspicuous behaviour (hang) and one inconspicuous behaviour (groom). The performance of these models is compared to a single stream autoencoder and a supervised learning model, which are both based on the custom CAE. Both models are also tested on the CUHK Avenue dataset were found to perform as well as some state-of-the-art architectures.



### NViSII: A Scriptable Tool for Photorealistic Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2105.13962v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.13962v1)
- **Published**: 2021-05-28 16:35:32+00:00
- **Updated**: 2021-05-28 16:35:32+00:00
- **Authors**: Nathan Morrical, Jonathan Tremblay, Yunzhi Lin, Stephen Tyree, Stan Birchfield, Valerio Pascucci, Ingo Wald
- **Comment**: SDG Workshop at ICLR 2021. Project page is at
  https://github.com/owl-project/NVISII
- **Journal**: None
- **Summary**: We present a Python-based renderer built on NVIDIA's OptiX ray tracing engine and the OptiX AI denoiser, designed to generate high-quality synthetic images for research in computer vision and deep learning. Our tool enables the description and manipulation of complex dynamic 3D scenes containing object meshes, materials, textures, lighting, volumetric data (e.g., smoke), and backgrounds. Metadata, such as 2D/3D bounding boxes, segmentation masks, depth maps, normal maps, material properties, and optical flow vectors, can also be generated. In this work, we discuss design goals, architecture, and performance. We demonstrate the use of data generated by path tracing for training an object detector and pose estimator, showing improved performance in sim-to-real transfer in situations that are difficult for traditional raster-based renderers. We offer this tool as an easy-to-use, performant, high-quality renderer for advancing research in synthetic data generation and deep learning.



### Revitalizing Optimization for 3D Human Pose and Shape Estimation: A Sparse Constrained Formulation
- **Arxiv ID**: http://arxiv.org/abs/2105.13965v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.13965v2)
- **Published**: 2021-05-28 16:44:56+00:00
- **Updated**: 2021-10-04 17:07:59+00:00
- **Authors**: Taosha Fan, Kalyan Vasudev Alwala, Donglai Xiang, Weipeng Xu, Todd Murphey, Mustafa Mukadam
- **Comment**: 21 pages, including appendix
- **Journal**: None
- **Summary**: We propose a novel sparse constrained formulation and from it derive a real-time optimization method for 3D human pose and shape estimation. Our optimization method, SCOPE (Sparse Constrained Optimization for 3D human Pose and shapE estimation), is orders of magnitude faster (avg. 4 ms convergence) than existing optimization methods, while being mathematically equivalent to their dense unconstrained formulation under mild assumptions. We achieve this by exploiting the underlying sparsity and constraints of our formulation to efficiently compute the Gauss-Newton direction. We show that this computation scales linearly with the number of joints and measurements of a complex 3D human model, in contrast to prior work where it scales cubically due to their dense unconstrained formulation. Based on our optimization method, we present a real-time motion capture framework that estimates 3D human poses and shapes from a single image at over 30 FPS. In benchmarks against state-of-the-art methods on multiple public datasets, our framework outperforms other optimization methods and achieves competitive accuracy against regression methods. Project page with code and videos: https://sites.google.com/view/scope-human/.



### What Is Considered Complete for Visual Recognition?
- **Arxiv ID**: http://arxiv.org/abs/2105.13978v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13978v1)
- **Published**: 2021-05-28 16:59:14+00:00
- **Updated**: 2021-05-28 16:59:14+00:00
- **Authors**: Lingxi Xie, Xiaopeng Zhang, Longhui Wei, Jianlong Chang, Qi Tian
- **Comment**: 13 pages, 5 figures, 1 table
- **Journal**: None
- **Summary**: This is an opinion paper. We hope to deliver a key message that current visual recognition systems are far from complete, i.e., recognizing everything that human can recognize, yet it is very unlikely that the gap can be bridged by continuously increasing human annotations. Based on the observation, we advocate for a new type of pre-training task named learning-by-compression. The computational models (e.g., a deep network) are optimized to represent the visual data using compact features, and the features preserve the ability to recover the original data. Semantic annotations, when available, play the role of weak supervision. An important yet challenging issue is the evaluation of image recovery, where we suggest some design principles and future research directions. We hope our proposal can inspire the community to pursue the compression-recovery tradeoff rather than the accuracy-complexity tradeoff.



### EDEN: Deep Feature Distribution Pooling for Saimaa Ringed Seals Pattern Matching
- **Arxiv ID**: http://arxiv.org/abs/2105.13979v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13979v2)
- **Published**: 2021-05-28 16:59:39+00:00
- **Updated**: 2021-08-24 11:43:25+00:00
- **Authors**: Ilia Chelak, Ekaterina Nepovinnykh, Tuomas Eerola, Heikki Kälviäinen, Igor Belykh
- **Comment**: 10 pages, 4 figures,submitted to 2nd International Conference on
  Cyber-Physical Systems & Control (CPS&C'2021)
- **Journal**: None
- **Summary**: In this paper, pelage pattern matching is considered to solve the individual re-identification of the Saimaa ringed seals. Animal re-identification together with the access to large amount of image material through camera traps and crowd-sourcing provide novel possibilities for animal monitoring and conservation. We propose a novel feature pooling approach that allow aggregating the local pattern features to get a fixed size embedding vector that incorporate global features by taking into account the spatial distribution of features. This is obtained by eigen decomposition of covariances computed for probability mass functions representing feature maps. Embedding vectors can then be used to find the best match in the database of known individuals allowing animal re-identification. The results show that the proposed pooling method outperforms the existing methods on the challenging Saimaa ringed seal image data.



### PTNet: A High-Resolution Infant MRI Synthesizer Based on Transformer
- **Arxiv ID**: http://arxiv.org/abs/2105.13993v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.13993v1)
- **Published**: 2021-05-28 17:20:19+00:00
- **Updated**: 2021-05-28 17:20:19+00:00
- **Authors**: Xuzhe Zhang, Xinzi He, Jia Guo, Nabil Ettehadi, Natalie Aw, David Semanek, Jonathan Posner, Andrew Laine, Yun Wang
- **Comment**: arXiv Preprint
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) noninvasively provides critical information about how human brain structures develop across stages of life. Developmental scientists are particularly interested in the first few years of neurodevelopment. Despite the success of MRI collection and analysis for adults, it is a challenge for researchers to collect high-quality multimodal MRIs from developing infants mainly because of their irregular sleep pattern, limited attention, inability to follow instructions to stay still, and a lack of analysis approaches. These challenges often lead to a significant reduction of usable data. To address this issue, researchers have explored various solutions to replace corrupted scans through synthesizing realistic MRIs. Among them, the convolution neural network (CNN) based generative adversarial network has demonstrated promising results and achieves state-of-the-art performance. However, adversarial training is unstable and may need careful tuning of regularization terms to stabilize the training. In this study, we introduced a novel MRI synthesis framework - Pyramid Transformer Net (PTNet). PTNet consists of transformer layers, skip-connections, and multi-scale pyramid representation. Compared with the most widely used CNN-based conditional GAN models (namely pix2pix and pix2pixHD), our model PTNet shows superior performance in terms of synthesis accuracy and model size. Notably, PTNet does not require any type of adversarial training and can be easily trained using the simple mean squared error loss.



### Linguistic Structures as Weak Supervision for Visual Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2105.13994v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.13994v1)
- **Published**: 2021-05-28 17:20:27+00:00
- **Updated**: 2021-05-28 17:20:27+00:00
- **Authors**: Keren Ye, Adriana Kovashka
- **Comment**: To appear in CVPR 2021
- **Journal**: None
- **Summary**: Prior work in scene graph generation requires categorical supervision at the level of triplets - subjects and objects, and predicates that relate them, either with or without bounding box information. However, scene graph generation is a holistic task: thus holistic, contextual supervision should intuitively improve performance. In this work, we explore how linguistic structures in captions can benefit scene graph generation. Our method captures the information provided in captions about relations between individual triplets, and context for subjects and objects (e.g. visual properties are mentioned). Captions are a weaker type of supervision than triplets since the alignment between the exhaustive list of human-annotated subjects and objects in triplets, and the nouns in captions, is weak. However, given the large and diverse sources of multimodal data on the web (e.g. blog posts with images and captions), linguistic supervision is more scalable than crowdsourced triplets. We show extensive experimental comparisons against prior methods which leverage instance- and image-level supervision, and ablate our method to show the impact of leveraging phrasal and sequential context, and techniques to improve localization of subjects and objects.



### On Hamilton-Jacobi PDEs and image denoising models with certain non-additive noise
- **Arxiv ID**: http://arxiv.org/abs/2105.13997v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.13997v2)
- **Published**: 2021-05-28 17:21:25+00:00
- **Updated**: 2022-02-25 19:06:14+00:00
- **Authors**: Jérôme Darbon, Tingwei Meng, Elena Resmerita
- **Comment**: None
- **Journal**: None
- **Summary**: We consider image denoising problems formulated as variational problems. It is known that Hamilton-Jacobi PDEs govern the solution of such optimization problems when the noise model is additive. In this work, we address certain non-additive noise models and show that they are also related to Hamilton-Jacobi PDEs. These findings allow us to establish new connections between additive and non-additive noise imaging models. Specifically, we study how the solutions to these optimization problems depend on the parameters and the observed images. We show that the optimal values are ruled by some Hamilton-Jacobi PDEs, while the optimizers are characterized by the spatial gradient of the solution to the Hamilton-Jacobi PDEs. Moreover, we use these relations to investigate the asymptotic behavior of the variational model as the parameter goes to infinity, that is, when the influence of the noise vanishes. With these connections, some non-convex models for non-additive noise can be solved by applying convex optimization algorithms to the equivalent convex models for additive noise. Several numerical results are provided for denoising problems with Poisson noise or multiplicative noise.



### Iris Liveness Detection using a Cascade of Dedicated Deep Learning Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.14009v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.14009v1)
- **Published**: 2021-05-28 17:37:11+00:00
- **Updated**: 2021-05-28 17:37:11+00:00
- **Authors**: Juan Tapia, Sebastian Gonzalez, Christoph Busch
- **Comment**: None
- **Journal**: None
- **Summary**: Iris pattern recognition has significantly improved the biometric authentication field due to its high stability and uniqueness. Such physical characteristics have played an essential role in security and other related areas. However, presentation attacks, also known as spoofing techniques, can bypass biometric authentication systems using artefacts such as printed images, artificial eyes, textured contact lenses, etc. Many liveness detection methods that improve the security of these systems have been proposed. The first International Iris Liveness Detection competition, where the effectiveness of liveness detection methods is evaluated, was first launched in 2013, and its latest iteration was held in 2020. This paper proposes a serial architecture based on a MobileNetV2 modification, trained from scratch to classify bona fide iris images versus presentation attack images. The bona fide class consists of live iris images, whereas the attack presentation instrument classes are comprised of cadaver, printed, and contact lenses images, for a total of four scenarios. All the images were pre-processed and weighted per class to present a fair evaluation. This proposal won the LivDet-Iris 2020 competition using two-class scenarios. Additionally, we present new three-class and four-class scenarios that further improve the competition results. This approach is primarily focused in detecting the bona fide class over improving the detection of presentation attack instruments. For the two, three, and four classes scenarios, an Equal Error Rate (EER) of 4.04\%, 0.33\%, and 4,53\% was obtained respectively. Overall, the best serial model proposed, using three scenarios, reached an ERR of 0.33\% with an Attack Presentation Classification Error Rate (APCER) of 0.0100 and a Bona Fide Classification Error Rate (BPCER) of 0.000. This work outperforms the LivDet-Iris 2020 competition results.



### Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging
- **Arxiv ID**: http://arxiv.org/abs/2105.14021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.14021v1)
- **Published**: 2021-05-28 17:55:15+00:00
- **Updated**: 2021-05-28 17:55:15+00:00
- **Authors**: S. Mahdi H. Miangoleh, Sebastian Dille, Long Mai, Sylvain Paris, Yağız Aksoy
- **Comment**: For more details visit http://yaksoy.github.io/highresdepth/
- **Journal**: Proc. CVPR (2021)
- **Summary**: Neural networks have shown great abilities in estimating depth from a single image. However, the inferred depth maps are well below one-megapixel resolution and often lack fine-grained details, which limits their practicality. Our method builds on our analysis on how the input resolution and the scene structure affects depth estimation performance. We demonstrate that there is a trade-off between a consistent scene structure and the high-frequency details, and merge low- and high-resolution estimations to take advantage of this duality using a simple depth merging network. We present a double estimation method that improves the whole-image depth estimation and a patch selection method that adds local details to the final result. We demonstrate that by merging estimations at different resolutions with changing context, we can generate multi-megapixel depth maps with a high level of detail using a pre-trained model.



### An Inexact Projected Gradient Method with Rounding and Lifting by Nonlinear Programming for Solving Rank-One Semidefinite Relaxation of Polynomial Optimization
- **Arxiv ID**: http://arxiv.org/abs/2105.14033v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.LG, 90C06, 90C22, 90C23, 90C55
- **Links**: [PDF](http://arxiv.org/pdf/2105.14033v2)
- **Published**: 2021-05-28 18:07:16+00:00
- **Updated**: 2021-10-26 15:48:50+00:00
- **Authors**: Heng Yang, Ling Liang, Luca Carlone, Kim-Chuan Toh
- **Comment**: Code available at https://github.com/MIT-SPARK/STRIDE
- **Journal**: None
- **Summary**: We consider solving high-order semidefinite programming (SDP) relaxations of nonconvex polynomial optimization problems (POPs) that often admit degenerate rank-one optimal solutions. Instead of solving the SDP alone, we propose a new algorithmic framework that blends local search using the nonconvex POP into global descent using the convex SDP. In particular, we first design a globally convergent inexact projected gradient method (iPGM) for solving the SDP that serves as the backbone of our framework. We then accelerate iPGM by taking long, but safeguarded, rank-one steps generated by fast nonlinear programming algorithms. We prove that the new framework is still globally convergent for solving the SDP. To solve the iPGM subproblem of projecting a given point onto the feasible set of the SDP, we design a two-phase algorithm with phase one using a symmetric Gauss-Seidel based accelerated proximal gradient method (sGS-APG) to generate a good initial point, and phase two using a modified limited-memory BFGS (L-BFGS) method to obtain an accurate solution. We analyze the convergence for both phases and establish a novel global convergence result for the modified L-BFGS that does not require the objective function to be twice continuously differentiable. We conduct numerical experiments for solving second-order SDP relaxations arising from a diverse set of POPs. Our framework demonstrates state-of-the-art efficiency, scalability, and robustness in solving degenerate rank-one SDPs to high accuracy, even in the presence of millions of equality constraints.



### TransCamP: Graph Transformer for 6-DoF Camera Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2105.14065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.14065v1)
- **Published**: 2021-05-28 19:08:43+00:00
- **Updated**: 2021-05-28 19:08:43+00:00
- **Authors**: Xinyi Li, Haibin Ling
- **Comment**: None
- **Journal**: None
- **Summary**: Camera pose estimation or camera relocalization is the centerpiece in numerous computer vision tasks such as visual odometry, structure from motion (SfM) and SLAM. In this paper we propose a neural network approach with a graph transformer backbone, namely TransCamP, to address the camera relocalization problem. In contrast with prior work where the pose regression is mainly guided by photometric consistency, TransCamP effectively fuses the image features, camera pose information and inter-frame relative camera motions into encoded graph attributes and is trained towards the graph consistency and accuracy instead, yielding significantly higher computational efficiency. By leveraging graph transformer layers with edge features and enabling tensorized adjacency matrix, TransCamP dynamically captures the global attention and thus endows the pose graph with evolving structures to achieve improved robustness and accuracy. In addition, optional temporal transformer layers actively enhance the spatiotemporal inter-frame relation for sequential inputs. Evaluation of the proposed network on various public benchmarks demonstrates that TransCamP outperforms state-of-the-art approaches.



### Classification of Brain Tumours in MR Images using Deep Spatiospatial Models
- **Arxiv ID**: http://arxiv.org/abs/2105.14071v2
- **DOI**: 10.1038/s41598-022-05572-6
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.14071v2)
- **Published**: 2021-05-28 19:27:51+00:00
- **Updated**: 2022-01-14 10:24:56+00:00
- **Authors**: Soumick Chatterjee, Faraz Ahmed Nizamani, Andreas Nürnberger, Oliver Speck
- **Comment**: None
- **Journal**: Sci Rep 12, 1505 (2022)
- **Summary**: A brain tumour is a mass or cluster of abnormal cells in the brain, which has the possibility of becoming life-threatening because of its ability to invade neighbouring tissues and also form metastases. An accurate diagnosis is essential for successful treatment planning and magnetic resonance imaging is the principal imaging modality for diagnostic of brain tumours and their extent. Deep Learning methods in computer vision applications have shown significant improvement in recent years, most of which can be credited to the fact that a sizeable amount of data is available to train models on, and the improvements in the model architectures yielding better approximations in a supervised setting. Classifying tumours using such deep learning methods has made significant progress with the availability of open datasets with reliable annotations. Typically those methods are either 3D models, which use 3D volumetric MRIs or even 2D models considering each slice separately. However, by treating the slice spatial dimension separately, spatiotemporal models can be employed as spatiospatial models for this task. These models have the capabilities of learning specific spatial and temporal relationship, while reducing computational costs. This paper uses two spatiotemporal models, ResNet (2+1)D and ResNet Mixed Convolution, to classify different types of brain tumours. It was observed that both these models performed superior to the pure 3D convolutional model, ResNet18. Furthermore, it was also observed that pre-training the models on a different, even unrelated dataset before training them for the task of tumour classification improves the performance. Finally, Pre-trained ResNet Mixed Convolution was observed to be the best model in these experiments, achieving a macro F1-score of 0.93 and a test accuracy of 96.98\%, while at the same time being the model with the least computational cost.



### On the Bias Against Inductive Biases
- **Arxiv ID**: http://arxiv.org/abs/2105.14077v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.14077v1)
- **Published**: 2021-05-28 19:41:48+00:00
- **Updated**: 2021-05-28 19:41:48+00:00
- **Authors**: George Cazenavette, Simon Lucey
- **Comment**: Under Review at NeurIPS 2021
- **Journal**: None
- **Summary**: Borrowing from the transformer models that revolutionized the field of natural language processing, self-supervised feature learning for visual tasks has also seen state-of-the-art success using these extremely deep, isotropic networks. However, the typical AI researcher does not have the resources to evaluate, let alone train, a model with several billion parameters and quadratic self-attention activations. To facilitate further research, it is necessary to understand the features of these huge transformer models that can be adequately studied by the typical researcher. One interesting characteristic of these transformer models is that they remove most of the inductive biases present in classical convolutional networks. In this work, we analyze the effect of these and more inductive biases on small to moderately-sized isotropic networks used for unsupervised visual feature learning and show that their removal is not always ideal.



### Gotta Go Fast When Generating Data with Score-Based Models
- **Arxiv ID**: http://arxiv.org/abs/2105.14080v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2105.14080v1)
- **Published**: 2021-05-28 19:48:51+00:00
- **Updated**: 2021-05-28 19:48:51+00:00
- **Authors**: Alexia Jolicoeur-Martineau, Ke Li, Rémi Piché-Taillefer, Tal Kachman, Ioannis Mitliagkas
- **Comment**: Code is available on
  https://github.com/AlexiaJM/score_sde_fast_sampling
- **Journal**: None
- **Summary**: Score-based (denoising diffusion) generative models have recently gained a lot of success in generating realistic and diverse data. These approaches define a forward diffusion process for transforming data to noise and generate data by reversing it (thereby going from noise to data). Unfortunately, current score-based models generate data very slowly due to the sheer number of score network evaluations required by numerical SDE solvers.   In this work, we aim to accelerate this process by devising a more efficient SDE solver. Existing approaches rely on the Euler-Maruyama (EM) solver, which uses a fixed step size. We found that naively replacing it with other SDE solvers fares poorly - they either result in low-quality samples or become slower than EM. To get around this issue, we carefully devise an SDE solver with adaptive step sizes tailored to score-based generative models piece by piece. Our solver requires only two score function evaluations, rarely rejects samples, and leads to high-quality samples. Our approach generates data 2 to 10 times faster than EM while achieving better or equal sample quality. For high-resolution images, our method leads to significantly higher quality samples than all other methods tested. Our SDE solver has the benefit of requiring no step size tuning.



### Augmenting Anchors by the Detector Itself
- **Arxiv ID**: http://arxiv.org/abs/2105.14086v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.14086v2)
- **Published**: 2021-05-28 20:11:08+00:00
- **Updated**: 2022-04-27 09:01:29+00:00
- **Authors**: Xiaopei Wan, Guoqiu Li, Yujiu Yang, Zhenhua Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Usually, it is difficult to determine the scale and aspect ratio of anchors for anchor-based object detection methods. Current state-of-the-art object detectors either determine anchor parameters according to objects' shape and scale in a dataset, or avoid this problem by utilizing anchor-free methods, however, the former scheme is dataset-specific and the latter methods could not get better performance than the former ones. In this paper, we propose a novel anchor augmentation method named AADI, which means Augmenting Anchors by the Detector Itself. AADI is not an anchor-free method, instead, it can convert the scale and aspect ratio of anchors from a continuous space to a discrete space, which greatly alleviates the problem of anchors' designation. Furthermore, AADI is a learning-based anchor augmentation method, but it does not add any parameters or hyper-parameters, which is beneficial for research and downstream tasks. Extensive experiments on COCO dataset demonstrate the effectiveness of AADI, specifically, AADI achieves significant performance boosts on many state-of-the-art object detectors (eg. at least +2.4 box AP on Faster R-CNN, +2.2 box AP on Mask R-CNN, and +0.9 box AP on Cascade Mask R-CNN). We hope that this simple and cost-efficient method can be widely used in object detection. Code and models are available at https://github.com/WanXiaopei/aadi.



### An Attention Free Transformer
- **Arxiv ID**: http://arxiv.org/abs/2105.14103v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.14103v2)
- **Published**: 2021-05-28 20:45:30+00:00
- **Updated**: 2021-09-21 18:04:55+00:00
- **Authors**: Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, Josh Susskind
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Attention Free Transformer (AFT), an efficient variant of Transformers that eliminates the need for dot product self attention. In an AFT layer, the key and value are first combined with a set of learned position biases, the result of which is multiplied with the query in an element-wise fashion. This new operation has a memory complexity linear w.r.t. both the context size and the dimension of features, making it compatible to both large input and model sizes. We also introduce AFT-local and AFT-conv, two model variants that take advantage of the idea of locality and spatial weight sharing while maintaining global connectivity. We conduct extensive experiments on two autoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image recognition task (ImageNet-1K classification). We show that AFT demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.



### Smaller Is Better: An Analysis of Instance Quantity/Quality Trade-off in Rehearsal-based Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.14106v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.14106v4)
- **Published**: 2021-05-28 21:05:51+00:00
- **Updated**: 2022-04-29 17:01:14+00:00
- **Authors**: Francesco Pelosin, Andrea Torsello
- **Comment**: Accepted at IJCNN 2022
- **Journal**: None
- **Summary**: The design of machines and algorithms capable of learning in a dynamically changing environment has become an increasingly topical problem with the increase of the size and heterogeneity of data available to learning systems. As a consequence, the key issue of Continual Learning has become that of addressing the stability-plasticity dilemma of connectionist systems, as they need to adapt their model without forgetting previously acquired knowledge. Within this context, rehearsal-based methods i.e., solutions in where the learner exploits memory to revisit past data, has proven to be very effective, leading to performance at the state-of-the-art. In our study, we propose an analysis of the memory quantity/quality trade-off adopting various data reduction approaches to increase the number of instances storable in memory. In particular, we investigate complex instance compression techniques such as deep encoders, but also trivial approaches such as image resizing and linear dimensionality reduction. Our findings suggest that the optimal trade-off is severely skewed toward instance quantity, where rehearsal approaches with several heavily compressed instances easily outperform state-of-the-art approaches with the same amount of memory at their disposal. Further, in high memory configurations, deep approaches extracting spatial structure combined with extreme resizing (of the order of $8\times8$ images) yield the best results, while in memory-constrained configurations where deep approaches cannot be used due to their memory requirement in training, Extreme Learning Machines (ELM) offer a clear advantage.



### MixerGAN: An MLP-Based Architecture for Unpaired Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2105.14110v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.14110v2)
- **Published**: 2021-05-28 21:12:52+00:00
- **Updated**: 2021-08-19 07:38:02+00:00
- **Authors**: George Cazenavette, Manuel Ladron De Guevara
- **Comment**: Under Review for WACV 2022
- **Journal**: None
- **Summary**: While attention-based transformer networks achieve unparalleled success in nearly all language tasks, the large number of tokens (pixels) found in images coupled with the quadratic activation memory usage makes them prohibitive for problems in computer vision. As such, while language-to-language translation has been revolutionized by the transformer model, convolutional networks remain the de facto solution for image-to-image translation. The recently proposed MLP-Mixer architecture alleviates some of the computational issues associated with attention-based networks while still retaining the long-range connections that make transformer models desirable. Leveraging this memory-efficient alternative to self-attention, we propose a new exploratory model in unpaired image-to-image translation called MixerGAN: a simpler MLP-based architecture that considers long-distance relationships between pixels without the need for expensive attention mechanisms. Quantitative and qualitative analysis shows that MixerGAN achieves competitive results when compared to prior convolutional-based methods.



### About Explicit Variance Minimization: Training Neural Networks for Medical Imaging With Limited Data Annotations
- **Arxiv ID**: http://arxiv.org/abs/2105.14117v4
- **DOI**: None
- **Categories**: **cs.CV**, 68T07 (Primary) 68T45 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/2105.14117v4)
- **Published**: 2021-05-28 21:34:04+00:00
- **Updated**: 2021-08-24 17:34:37+00:00
- **Authors**: Dmitrii Shubin, Danny Eytan, Sebastian D. Goodfellow
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning methods for computer vision have demonstrated the effectiveness of pre-training feature representations, resulting in well-generalizing Deep Neural Networks, even if the annotated data are limited. However, representation learning techniques require a significant amount of time for model training, with most of the time spent on precise hyper-parameter optimization and selection of augmentation techniques. We hypothesized that if the annotated dataset has enough morphological diversity to capture the diversity of the general population, as is common in medical imaging due to conserved similarities of tissue morphology, the variance error of the trained model is the dominant component of the Bias-Variance Trade-off. Therefore, we proposed the Variance Aware Training (VAT) method that exploits this data property by introducing the variance error into the model loss function, thereby, explicitly regularizing the model. Additionally, we provided a theoretical formulation and proof of the proposed method to aid interpreting the approach. Our method requires selecting only one hyper-parameter and matching or improving the performance of state-of-the-art self-supervised methods while achieving an order of magnitude reduction in the GPU training time. We validated VAT on three medical imaging datasets from diverse domains and for various learning objectives. These included a Magnetic Resonance Imaging (MRI) dataset for the heart semantic segmentation (MICCAI 2017 ACDC challenge), fundus photography dataset for ordinary regression of diabetic retinopathy progression (Kaggle 2019 APTOS Blindness Detection challenge), and classification of histopathologic scans of lymph node sections (PatchCamelyon dataset). Our code is available at https://github.com/DmitriiShubin/Variance-Aware-Training.



### 3D U-NetR: Low Dose Computed Tomography Reconstruction via Deep Learning and 3 Dimensional Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2105.14130v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.14130v2)
- **Published**: 2021-05-28 22:37:50+00:00
- **Updated**: 2022-04-15 20:13:48+00:00
- **Authors**: Doga Gunduzalp, Batuhan Cengiz, Mehmet Ozan Unal, Isa Yildirim
- **Comment**: 18 pages, 5 figures
- **Journal**: None
- **Summary**: In this paper, we introduced a novel deep learning-based reconstruction technique for low-dose CT imaging using 3 dimensional convolutions to include the sagittal information unlike the existing 2 dimensional networks which exploits correlation only in transverse plane. In the proposed reconstruction technique, sparse and noisy sinograms are back-projected to the image domain with FBP operation, then the denoising process is applied with a U-Net like 3-dimensional network called 3D U-NetR. The proposed network is trained with synthetic and real chest CT images, and 2D U-Net is also trained with the same dataset to show the importance of the third dimension in terms of recovering the fine details. The proposed network shows better quantitative performance on SSIM and PSNR, especially in the real chest CT data. More importantly, 3D U-NetR captures medically critical visual details that cannot be visualized by a 2D network on the reconstruction of real CT images with 1/10 of the normal dose.



### Transformer-Based Source-Free Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2105.14138v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.14138v1)
- **Published**: 2021-05-28 23:06:26+00:00
- **Updated**: 2021-05-28 23:06:26+00:00
- **Authors**: Guanglei Yang, Hao Tang, Zhun Zhong, Mingli Ding, Ling Shao, Nicu Sebe, Elisa Ricci
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the task of source-free domain adaptation (SFDA), where the source data are not available during target adaptation. Previous works on SFDA mainly focus on aligning the cross-domain distributions. However, they ignore the generalization ability of the pretrained source model, which largely influences the initial target outputs that are vital to the target adaptation stage. To address this, we make the interesting observation that the model accuracy is highly correlated with whether or not attention is focused on the objects in an image. To this end, we propose a generic and effective framework based on Transformer, named TransDA, for learning a generalized model for SFDA. Specifically, we apply the Transformer as the attention module and inject it into a convolutional network. By doing so, the model is encouraged to turn attention towards the object regions, which can effectively improve the model's generalization ability on the target domains. Moreover, a novel self-supervised knowledge distillation approach is proposed to adapt the Transformer with target pseudo-labels, thus further encouraging the network to focus on the object regions. Experiments on three domain adaptation tasks, including closed-set, partial-set, and open-set adaption, demonstrate that TransDA can greatly improve the adaptation accuracy and produce state-of-the-art results. The source code and trained models are available at https://github.com/ygjwd12345/TransDA.



### OpenMatch: Open-set Consistency Regularization for Semi-supervised Learning with Outliers
- **Arxiv ID**: http://arxiv.org/abs/2105.14148v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.14148v2)
- **Published**: 2021-05-28 23:57:15+00:00
- **Updated**: 2021-08-24 18:31:40+00:00
- **Authors**: Kuniaki Saito, Donghyun Kim, Kate Saenko
- **Comment**: Code https://github.com/VisionLearningGroup/OP_Match
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL) is an effective means to leverage unlabeled data to improve a model's performance. Typical SSL methods like FixMatch assume that labeled and unlabeled data share the same label space. However, in practice, unlabeled data can contain categories unseen in the labeled set, i.e., outliers, which can significantly harm the performance of SSL algorithms. To address this problem, we propose a novel Open-set Semi-Supervised Learning (OSSL) approach called OpenMatch. Learning representations of inliers while rejecting outliers is essential for the success of OSSL. To this end, OpenMatch unifies FixMatch with novelty detection based on one-vs-all (OVA) classifiers. The OVA-classifier outputs the confidence score of a sample being an inlier, providing a threshold to detect outliers. Another key contribution is an open-set soft-consistency regularization loss, which enhances the smoothness of the OVA-classifier with respect to input transformations and greatly improves outlier detection. OpenMatch achieves state-of-the-art performance on three datasets, and even outperforms a fully supervised model in detecting outliers unseen in unlabeled data on CIFAR10.



