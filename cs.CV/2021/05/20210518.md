# Arxiv Papers in cs.CV on 2021-05-18
### Reinforcement Learning for Adaptive Video Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/2105.08205v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2105.08205v1)
- **Published**: 2021-05-18 00:01:27+00:00
- **Updated**: 2021-05-18 00:01:27+00:00
- **Authors**: Sidi Lu, Xin Yuan, Aggelos K Katsaggelos, Weisong Shi
- **Comment**: 12 pages, 11 figures, 2 tables
- **Journal**: None
- **Summary**: We apply reinforcement learning to video compressive sensing to adapt the compression ratio. Specifically, video snapshot compressive imaging (SCI), which captures high-speed video using a low-speed camera is considered in this work, in which multiple (B) video frames can be reconstructed from a snapshot measurement. One research gap in previous studies is how to adapt B in the video SCI system for different scenes. In this paper, we fill this gap utilizing reinforcement learning (RL). An RL model, as well as various convolutional neural networks for reconstruction, are learned to achieve adaptive sensing of video SCI systems. Furthermore, the performance of an object detection network using directly the video SCI measurements without reconstruction is also used to perform RL-based adaptive video compressive sensing. Our proposed adaptive SCI method can thus be implemented in low cost and real time. Our work takes the technology one step further towards real applications of video SCI.



### Decorating Your Own Bedroom: Locally Controlling Image Generation with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.08222v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2105.08222v1)
- **Published**: 2021-05-18 01:31:49+00:00
- **Updated**: 2021-05-18 01:31:49+00:00
- **Authors**: Chen Zhang, Yinghao Xu, Yujun Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have made great success in synthesizing high-quality images. However, how to steer the generation process of a well-trained GAN model and customize the output image is much less explored. It has been recently found that modulating the input latent code used in GANs can reasonably alter some variation factors in the output image, but such manipulation usually presents to change the entire image as a whole. In this work, we propose an effective approach, termed as LoGAN, to support local editing of the output image. Concretely, we introduce two operators, i.e., content modulation and style modulation, together with a priority mask to facilitate the precise control of the intermediate generative features. Taking bedroom synthesis as an instance, we are able to seamlessly remove, insert, shift, and rotate the individual objects inside a room. Furthermore, our method can completely clear out a room and then refurnish it with customized furniture and styles. Experimental results show the great potentials of steering the image generation of pre-trained GANs for versatile image editing.



### Single View Geocentric Pose in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2105.08229v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08229v1)
- **Published**: 2021-05-18 01:55:15+00:00
- **Updated**: 2021-05-18 01:55:15+00:00
- **Authors**: Gordon Christie, Kevin Foster, Shea Hagstrom, Gregory D. Hager, Myron Z. Brown
- **Comment**: To be published in the proceedings of the CVPR 2021 EarthVision
  Workshop
- **Journal**: None
- **Summary**: Current methods for Earth observation tasks such as semantic mapping, map alignment, and change detection rely on near-nadir images; however, often the first available images in response to dynamic world events such as natural disasters are oblique. These tasks are much more difficult for oblique images due to observed object parallax. There has been recent success in learning to regress geocentric pose, defined as height above ground and orientation with respect to gravity, by training with airborne lidar registered to satellite images. We present a model for this novel task that exploits affine invariance properties to outperform state of the art performance by a wide margin. We also address practical issues required to deploy this method in the wild for real-world applications. Our data and code are publicly available.



### Towards Unsupervised Sketch-based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2105.08237v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08237v4)
- **Published**: 2021-05-18 02:38:22+00:00
- **Updated**: 2022-11-18 08:48:33+00:00
- **Authors**: Conghui Hu, Yongxin Yang, Yunpeng Li, Timothy M. Hospedales, Yi-Zhe Song
- **Comment**: None
- **Journal**: None
- **Summary**: The practical value of existing supervised sketch-based image retrieval (SBIR) algorithms is largely limited by the requirement for intensive data collection and labeling. In this paper, we present the first attempt at unsupervised SBIR to remove the labeling cost (both category annotations and sketch-photo pairings) that is conventionally needed for training. Existing single-domain unsupervised representation learning methods perform poorly in this application, due to the unique cross-domain (sketch and photo) nature of the problem. We therefore introduce a novel framework that simultaneously performs sketch-photo domain alignment and semantic-aware representation learning. Technically this is underpinned by introducing joint distribution optimal transport (JDOT) to align data from different domains, which we extend with trainable cluster prototypes and feature memory banks to further improve scalability and efficacy. Extensive experiments show that our framework achieves excellent performance in the new unsupervised setting, and performs comparably or better than state-of-the-art in the zero-shot setting.



### Self-Point-Flow: Self-Supervised Scene Flow Estimation from Point Clouds with Optimal Transport and Random Walk
- **Arxiv ID**: http://arxiv.org/abs/2105.08248v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08248v1)
- **Published**: 2021-05-18 03:12:42+00:00
- **Updated**: 2021-05-18 03:12:42+00:00
- **Authors**: Ruibo Li, Guosheng Lin, Lihua Xie
- **Comment**: Accepted to CVPR2021 (oral)
- **Journal**: None
- **Summary**: Due to the scarcity of annotated scene flow data, self-supervised scene flow learning in point clouds has attracted increasing attention. In the self-supervised manner, establishing correspondences between two point clouds to approximate scene flow is an effective approach. Previous methods often obtain correspondences by applying point-wise matching that only takes the distance on 3D point coordinates into account, introducing two critical issues: (1) it overlooks other discriminative measures, such as color and surface normal, which often bring fruitful clues for accurate matching; and (2) it often generates sub-par performance, as the matching is operated in an unconstrained situation, where multiple points can be ended up with the same corresponding point. To address the issues, we formulate this matching task as an optimal transport problem. The output optimal assignment matrix can be utilized to guide the generation of pseudo ground truth. In this optimal transport, we design the transport cost by considering multiple descriptors and encourage one-to-one matching by mass equality constraints. Also, constructing a graph on the points, a random walk module is introduced to encourage the local consistency of the pseudo labels. Comprehensive experiments on FlyingThings3D and KITTI show that our method achieves state-of-the-art performance among self-supervised learning methods. Our self-supervised method even performs on par with some supervised learning approaches, although we do not need any ground truth flow for training.



### Weakly Supervised Dense Video Captioning via Jointly Usage of Knowledge Distillation and Cross-modal Matching
- **Arxiv ID**: http://arxiv.org/abs/2105.08252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08252v1)
- **Published**: 2021-05-18 03:21:37+00:00
- **Updated**: 2021-05-18 03:21:37+00:00
- **Authors**: Bofeng Wu, Guocheng Niu, Jun Yu, Xinyan Xiao, Jian Zhang, Hua Wu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an approach to Dense Video Captioning (DVC) without pairwise event-sentence annotation. First, we adopt the knowledge distilled from relevant and well solved tasks to generate high-quality event proposals. Then we incorporate contrastive loss and cycle-consistency loss typically applied to cross-modal retrieval tasks to build semantic matching between the proposals and sentences, which are eventually used to train the caption generation module. In addition, the parameters of matching module are initialized via pre-training based on annotated images to improve the matching performance. Extensive experiments on ActivityNet-Caption dataset reveal the significance of distillation-based event proposal generation and cross-modal retrieval-based semantic matching to weakly supervised DVC, and demonstrate the superiority of our method to existing state-of-the-art methods.



### Finding a Needle in a Haystack: Tiny Flying Object Detection in 4K Videos using a Joint Detection-and-Tracking Approach
- **Arxiv ID**: http://arxiv.org/abs/2105.08253v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08253v1)
- **Published**: 2021-05-18 03:22:03+00:00
- **Updated**: 2021-05-18 03:22:03+00:00
- **Authors**: Ryota Yoshihashi, Rei Kawakami, Shaodi You, Tu Tuan Trinh, Makoto Iida, Takeshi Naemura
- **Comment**: arXiv admin note: text overlap with arXiv:1709.04666
- **Journal**: None
- **Summary**: Detecting tiny objects in a high-resolution video is challenging because the visual information is little and unreliable. Specifically, the challenge includes very low resolution of the objects, MPEG artifacts due to compression and a large searching area with many hard negatives. Tracking is equally difficult because of the unreliable appearance, and the unreliable motion estimation. Luckily, we found that by combining this two challenging tasks together, there will be mutual benefits. Following the idea, in this paper, we present a neural network model called the Recurrent Correlational Network, where detection and tracking are jointly performed over a multi-frame representation learned through a single, trainable, and end-to-end network. The framework exploits a convolutional long short-term memory network for learning informative appearance changes for detection, while the learned representation is shared in tracking for enhancing its performance. In experiments with datasets containing images of scenes with small flying objects, such as birds and unmanned aerial vehicles, the proposed method yielded consistent improvements in detection performance over deep single-frame detectors and existing motion-based detectors. Furthermore, our network performs as well as state-of-the-art generic object trackers when it was evaluated as a tracker on a bird image dataset.



### EchoCP: An Echocardiography Dataset in Contrast Transthoracic Echocardiography for Patent Foramen Ovale Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2105.08267v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.08267v2)
- **Published**: 2021-05-18 04:24:53+00:00
- **Updated**: 2021-09-16 01:27:12+00:00
- **Authors**: Tianchen Wang, Zhihe Li, Meiping Huang, Jian Zhuang, Shanshan Bi, Jiawei Zhang, Yiyu Shi, Hongwen Fei, Xiaowei Xu
- **Comment**: MICCAI2021
- **Journal**: None
- **Summary**: Patent foramen ovale (PFO) is a potential separation between the septum, primum and septum secundum located in the anterosuperior portion of the atrial septum. PFO is one of the main factors causing cryptogenic stroke which is the fifth leading cause of death in the United States. For PFO diagnosis, contrast transthoracic echocardiography (cTTE) is preferred as being a more robust method compared with others. However, the current PFO diagnosis through cTTE is extremely slow as it is proceeded manually by sonographers on echocardiography videos. Currently there is no publicly available dataset for this important topic in the community. In this paper, we present EchoCP, as the first echocardiography dataset in cTTE targeting PFO diagnosis.   EchoCP consists of 30 patients with both rest and Valsalva maneuver videos which covers various PFO grades. We further establish an automated baseline method for PFO diagnosis based on the state-of-the-art cardiac chamber segmentation technique, which achieves 0.89 average mean Dice score, but only 0.60/0.67 mean accuracies for PFO diagnosis, leaving large room for improvement. We hope that the challenging EchoCP dataset can stimulate further research and lead to innovative and generic solutions that would have an impact in multiple domains. Our dataset is released.



### SemSegLoss: A python package of loss functions for semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.05844v1
- **DOI**: 10.1016/j.simpa.2021.100078
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2106.05844v1)
- **Published**: 2021-05-18 04:27:06+00:00
- **Updated**: 2021-05-18 04:27:06+00:00
- **Authors**: Shruti Jadon
- **Comment**: 8 pages, 2 tables
- **Journal**: None
- **Summary**: Image Segmentation has been an active field of research as it has a wide range of applications, ranging from automated disease detection to self-driving cars. In recent years, various research papers proposed different loss functions used in case of biased data, sparse segmentation, and unbalanced dataset. In this paper, we introduce SemSegLoss, a python package consisting of some of the well-known loss functions widely used for image segmentation. It is developed with the intent to help researchers in the development of novel loss functions and perform an extensive set of experiments on model architectures for various applications. The ease-of-use and flexibility of the presented package have allowed reducing the development time and increased evaluation strategies of machine learning models for semantic segmentation. Furthermore, different applications that use image segmentation can use SemSegLoss because of the generality of its functions. This wide range of applications will lead to the development and growth of AI across all industries.



### Sparta: Spatially Attentive and Adversarially Robust Activation
- **Arxiv ID**: http://arxiv.org/abs/2105.08269v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.08269v2)
- **Published**: 2021-05-18 04:36:46+00:00
- **Updated**: 2022-12-03 11:56:46+00:00
- **Authors**: Qing Guo, Felix Juefei-Xu, Changqing Zhou, Wei Feng, Yang Liu, Song Wang
- **Comment**: 25 pages, 5 figures
- **Journal**: None
- **Summary**: Adversarial training (AT) is one of the most effective ways for improving the robustness of deep convolution neural networks (CNNs). Just like common network training, the effectiveness of AT relies on the design of basic network components. In this paper, we conduct an in-depth study on the role of the basic ReLU activation component in AT for robust CNNs. We find that the spatially-shared and input-independent properties of ReLU activation make CNNs less robust to white-box adversarial attacks with either standard or adversarial training. To address this problem, we extend ReLU to a novel Sparta activation function (Spatially attentive and Adversarially Robust Activation), which enables CNNs to achieve both higher robustness, i.e., lower error rate on adversarial examples, and higher accuracy, i.e., lower error rate on clean examples, than the existing state-of-the-art (SOTA) activation functions. We further study the relationship between Sparta and the SOTA activation functions, providing more insights about the advantages of our method. With comprehensive experiments, we also find that the proposed method exhibits superior cross-CNN and cross-dataset transferability. For the former, the adversarially trained Sparta function for one CNN (e.g., ResNet-18) can be fixed and directly used to train another adversarially robust CNN (e.g., ResNet-34). For the latter, the Sparta function trained on one dataset (e.g., CIFAR-10) can be employed to train adversarially robust CNNs on another dataset (e.g., SVHN). In both cases, Sparta leads to CNNs with higher robustness than the vanilla ReLU, verifying the flexibility and versatility of the proposed method.



### NExT-QA:Next Phase of Question-Answering to Explaining Temporal Actions
- **Arxiv ID**: http://arxiv.org/abs/2105.08276v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.08276v2)
- **Published**: 2021-05-18 04:56:46+00:00
- **Updated**: 2021-05-23 09:37:24+00:00
- **Authors**: Junbin Xiao, Xindi Shang, Angela Yao, Tat-Seng Chua
- **Comment**: To appear at CVPR2021.(Receive one 'Strong Accept'. Review comments:
  The benchmark will be beneficial for an important video understanding
  problem. The analysis is comprehensive and provides meaningful insights.)
- **Journal**: None
- **Summary**: We introduce NExT-QA, a rigorously designed video question answering (VideoQA) benchmark to advance video understanding from describing to explaining the temporal actions. Based on the dataset, we set up multi-choice and open-ended QA tasks targeting causal action reasoning, temporal action reasoning, and common scene comprehension. Through extensive analysis of baselines and established VideoQA techniques, we find that top-performing methods excel at shallow scene descriptions but are weak in causal and temporal action reasoning. Furthermore, the models that are effective on multi-choice QA, when adapted to open-ended QA, still struggle in generalizing the answers. This raises doubt on the ability of these models to reason and highlights possibilities for improvement. With detailed results for different question types and heuristic observations for future works, we hope NExT-QA will guide the next generation of VQA research to go beyond superficial scene description towards a deeper understanding of videos. (The dataset and related resources are available at https://github.com/doc-doc/NExT-QA.git)



### Exploring Driving-aware Salient Object Detection via Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2105.08286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08286v1)
- **Published**: 2021-05-18 05:24:21+00:00
- **Updated**: 2021-05-18 05:24:21+00:00
- **Authors**: Jinming Su, Changqun Xia, Jia Li
- **Comment**: Accepted by ICME 2021 (oral)
- **Journal**: None
- **Summary**: Recently, general salient object detection (SOD) has made great progress with the rapid development of deep neural networks. However, task-aware SOD has hardly been studied due to the lack of task-specific datasets. In this paper, we construct a driving task-oriented dataset where pixel-level masks of salient objects have been annotated. Comparing with general SOD datasets, we find that the cross-domain knowledge difference and task-specific scene gap are two main challenges to focus the salient objects when driving. Inspired by these findings, we proposed a baseline model for the driving task-aware SOD via a knowledge transfer convolutional neural network. In this network, we construct an attentionbased knowledge transfer module to make up the knowledge difference. In addition, an efficient boundary-aware feature decoding module is introduced to perform fine feature decoding for objects in the complex task-specific scenes. The whole network integrates the knowledge transfer and feature decoding modules in a progressive manner. Experiments show that the proposed dataset is very challenging, and the proposed method outperforms 12 state-of-the-art methods on the dataset, which facilitates the development of task-aware SOD.



### Exemplar-Based Open-Set Panoptic Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/2105.08336v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08336v2)
- **Published**: 2021-05-18 07:59:21+00:00
- **Updated**: 2021-05-19 00:38:26+00:00
- **Authors**: Jaedong Hwang, Seoung Wug Oh, Joon-Young Lee, Bohyung Han
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: We extend panoptic segmentation to the open-world and introduce an open-set panoptic segmentation (OPS) task. This task requires performing panoptic segmentation for not only known classes but also unknown ones that have not been acknowledged during training. We investigate the practical challenges of the task and construct a benchmark on top of an existing dataset, COCO. In addition, we propose a novel exemplar-based open-set panoptic segmentation network (EOPSN) inspired by exemplar theory. Our approach identifies a new class based on exemplars, which are identified by clustering and employed as pseudo-ground-truths. The size of each class increases by mining new exemplars based on the similarities to the existing ones associated with the class. We evaluate EOPSN on the proposed benchmark and demonstrate the effectiveness of our proposals. The primary goal of our work is to draw the attention of the community to the recognition in the open-world scenarios. The implementation of our algorithm is available on the project webpage: https://cv.snu.ac.kr/research/EOPSN.



### Transfer learning approach to Classify the X-ray image that corresponds to corona disease Using ResNet50 pretrained by ChexNet
- **Arxiv ID**: http://arxiv.org/abs/2105.08382v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.08382v1)
- **Published**: 2021-05-18 09:12:23+00:00
- **Updated**: 2021-05-18 09:12:23+00:00
- **Authors**: Mahyar Bolhassani
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Coronavirus adversely has affected people worldwide. There are common symptoms between the Covid19 virus disease and other respiratory diseases like pneumonia or Influenza. Therefore, diagnosing it fast is crucial not only to save patients but also to prevent it from spreading. One of the most reliant methods of diagnosis is through X-ray images of a lung. With the help of deep learning approaches, we can teach the deep model to learn the condition of an affected lung. Therefore, it can classify the new sample as if it is a Covid19 infected patient or not. In this project, we train a deep model based on ResNet50 pretrained by ImageNet dataset and CheXNet dataset. Based on the imbalanced CoronaHack Chest X-Ray dataset introducing by Kaggle we applied both binary and multi-class classification. Also, we compare the results when using Focal loss and Cross entropy loss.



### I2C2W: Image-to-Character-to-Word Transformers for Accurate Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2105.08383v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08383v3)
- **Published**: 2021-05-18 09:20:58+00:00
- **Updated**: 2022-12-19 02:13:43+00:00
- **Authors**: Chuhui Xue, Jiaxing Huang, Wenqing Zhang, Shijian Lu, Changhu Wang, Song Bai
- **Comment**: Accepted by special issue Transformer Models in Vision of the
  Transactions on Pattern Analysis and Machine Intelligence
- **Journal**: None
- **Summary**: Leveraging the advances of natural language processing, most recent scene text recognizers adopt an encoder-decoder architecture where text images are first converted to representative features and then a sequence of characters via `sequential decoding'. However, scene text images suffer from rich noises of different sources such as complex background and geometric distortions which often confuse the decoder and lead to incorrect alignment of visual features at noisy decoding time steps. This paper presents I2C2W, a novel scene text recognition technique that is tolerant to geometric and photometric degradation by decomposing scene text recognition into two inter-connected tasks. The first task focuses on image-to-character (I2C) mapping which detects a set of character candidates from images based on different alignments of visual features in an non-sequential way. The second task tackles character-to-word (C2W) mapping which recognizes scene text by decoding words from the detected character candidates. The direct learning from character semantics (instead of noisy image features) corrects falsely detected character candidates effectively which improves the final text recognition accuracy greatly. Extensive experiments over nine public datasets show that the proposed I2C2W outperforms the state-of-the-art by large margins for challenging scene text datasets with various curvature and perspective distortions. It also achieves very competitive recognition performance over multiple normal scene text datasets.



### Improved detection of small objects in road network sequences
- **Arxiv ID**: http://arxiv.org/abs/2105.08416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08416v1)
- **Published**: 2021-05-18 10:13:23+00:00
- **Updated**: 2021-05-18 10:13:23+00:00
- **Authors**: Iván García, Rafael Marcos Luque, Ezequiel López
- **Comment**: None
- **Journal**: None
- **Summary**: The vast number of existing IP cameras in current road networks is an opportunity to take advantage of the captured data and analyze the video and detect any significant events. For this purpose, it is necessary to detect moving vehicles, a task that was carried out using classical artificial vision techniques until a few years ago. Nowadays, significant improvements have been obtained by deep learning networks. Still, object detection is considered one of the leading open issues within computer vision.   The current scenario is constantly evolving, and new models and techniques are appearing trying to improve this field. In particular, new problems and drawbacks appear regarding detecting small objects, which correspond mainly to the vehicles that appear in the road scenes. All this means that new solutions that try to improve the low detection rate of small elements are essential. Among the different emerging research lines, this work focuses on the detection of small objects. In particular, our proposal aims to vehicle detection from images captured by video surveillance cameras.   In this work, we propose a new procedure for detecting small-scale objects by applying super-resolution processes based on detections performed by convolutional neural networks \emph{(CNN)}. The neural network is integrated with processes that are in charge of increasing the resolution of the images to improve the object detection performance. This solution has been tested for a set of traffic images containing elements of different scales to test the efficiency according to the detections obtained by the model, thus demonstrating that our proposal achieves good results in a wide range of situations.



### A parameter refinement method for Ptychography based on Deep Learning concepts
- **Arxiv ID**: http://arxiv.org/abs/2105.08058v2
- **DOI**: 10.3390/condmat6040036
- **Categories**: **eess.IV**, cs.CV, cs.NA, math.NA, 68U10, 94A08, 92C55
- **Links**: [PDF](http://arxiv.org/pdf/2105.08058v2)
- **Published**: 2021-05-18 10:15:17+00:00
- **Updated**: 2021-10-19 14:27:02+00:00
- **Authors**: Francesco Guzzi, George Kourousias, Fulvio Billè, Roberto Pugliese, Alessandra Gianoncelli, Sergio Carrato
- **Comment**: None
- **Journal**: Condens. Matter 2021, 6, 36
- **Summary**: X-ray Ptychography is an advanced computational microscopy technique which is delivering exceptionally detailed quantitative imaging of biological and nanotechnology specimens. However coarse parametrisation in propagation distance, position errors and partial coherence frequently menaces the experiment viability. In this work we formally introduced these actors, solving the whole reconstruction as an optimisation problem. A modern Deep Learning framework is used to correct autonomously the setup incoherences, thus improving the quality of a ptychography reconstruction. Automatic procedures are indeed crucial to reduce the time for a reliable analysis, which has a significant impact on all the fields that use this kind of microscopy. We implemented our algorithm in our software framework, SciComPty, releasing it as open-source. We tested our system on both synthetic datasets and also on real data acquired at the TwinMic beamline of the Elettra synchrotron facility.



### Deep Active Contours Using Locally Controlled Distance Vector Flow
- **Arxiv ID**: http://arxiv.org/abs/2105.08447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08447v1)
- **Published**: 2021-05-18 11:38:01+00:00
- **Updated**: 2021-05-18 11:38:01+00:00
- **Authors**: Parastoo Akbari, Atefeh Ziaei, Hamed Azarnoush
- **Comment**: 22 pages with 12 figures
- **Journal**: None
- **Summary**: Active contours Model (ACM) has been extensively used in computer vision and image processing. In recent studies, Convolutional Neural Networks (CNNs) have been combined with active contours replacing the user in the process of contour evolution and image segmentation to eliminate limitations associated with ACM's dependence on parameters of the energy functional and initialization. However, prior works did not aim for automatic initialization which is addressed here. In addition to manual initialization, current methods are highly sensitive to initial location and fail to delineate borders accurately. We propose a fully automatic image segmentation method to address problems of manual initialization, insufficient capture range, and poor convergence to boundaries, in addition to the problem of assignment of energy functional parameters. We train two CNNs, which predict active contour weighting parameters and generate a ground truth mask to extract Distance Transform (DT) and an initialization circle. Distance transform is used to form a vector field pointing from each pixel of the image towards the closest point on the boundary, the size of which is equal to the Euclidean distance map. We evaluate our method on four publicly available datasets including two building instance segmentation datasets, Vaihingen and Bing huts, and two mammography image datasets, INBreast and DDSM-BCRP. Our approach outperforms latest research by 0.59 ans 2.39 percent in mean Intersection-over-Union (mIoU), 7.38 and 8.62 percent in Boundary F-score (BoundF) for Vaihingen and Bing huts datasets, respectively. Dice similarity coefficient for the INBreast and DDSM-BCRP datasets is 94.23% and 90.89%, respectively indicating our method is comparable to state-of-the-art frameworks.



### Unsupervised Compound Domain Adaptation for Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2105.08463v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.08463v1)
- **Published**: 2021-05-18 12:08:07+00:00
- **Updated**: 2021-05-18 12:08:07+00:00
- **Authors**: Ankush Panwar, Pratyush Singh, Suman Saha, Danda Pani Paudel, Luc Van Gool
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: We address the problem of face anti-spoofing which aims to make the face verification systems robust in the real world settings. The context of detecting live vs. spoofed face images may differ significantly in the target domain, when compared to that of labeled source domain where the model is trained. Such difference may be caused due to new and unknown spoof types, illumination conditions, scene backgrounds, among many others. These varieties of differences make the target a compound domain, thus calling for the problem of the unsupervised compound domain adaptation. We demonstrate the effectiveness of the compound domain assumption for the task of face anti-spoofing, for the first time in this work. To this end, we propose a memory augmentation method for adapting the source model to the target domain in a domain aware manner. The adaptation process is further improved by using the curriculum learning and the domain agnostic source network training approaches. The proposed method successfully adapts to the compound target domain consisting multiple new spoof types. Our experiments on multiple benchmark datasets demonstrate the superiority of the proposed method over the state-of-the-art.



### Progressively Normalized Self-Attention Network for Video Polyp Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.08468v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08468v2)
- **Published**: 2021-05-18 12:20:00+00:00
- **Updated**: 2021-05-24 06:31:00+00:00
- **Authors**: Ge-Peng Ji, Yu-Cheng Chou, Deng-Ping Fan, Geng Chen, Huazhu Fu, Debesh Jha, Ling Shao
- **Comment**: MICCAI 2021 (Provisional accept); Code:
  https://github.com/GewelsJI/PNS-Net
- **Journal**: None
- **Summary**: Existing video polyp segmentation (VPS) models typically employ convolutional neural networks (CNNs) to extract features. However, due to their limited receptive fields, CNNs can not fully exploit the global temporal and spatial information in successive video frames, resulting in false-positive segmentation results. In this paper, we propose the novel PNS-Net (Progressively Normalized Self-attention Network), which can efficiently learn representations from polyp videos with real-time speed (~140fps) on a single RTX 2080 GPU and no post-processing. Our PNS-Net is based solely on a basic normalized self-attention block, equipping with recurrence and CNNs entirely. Experiments on challenging VPS datasets demonstrate that the proposed PNS-Net achieves state-of-the-art performance. We also conduct extensive experiments to study the effectiveness of the channel split, soft-attention, and progressive learning strategy. We find that our PNS-Net works well under different settings, making it a promising solution to the VPS task.



### Overparametrization of HyperNetworks at Fixed FLOP-Count Enables Fast Neural Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2105.08470v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2105.08470v1)
- **Published**: 2021-05-18 12:27:05+00:00
- **Updated**: 2021-05-18 12:27:05+00:00
- **Authors**: Lorenz K. Muller
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks can enhance images taken with small mobile camera sensors and excel at tasks like demoisaicing, denoising and super-resolution. However, for practical use on mobile devices these networks often require too many FLOPs and reducing the FLOPs of a convolution layer, also reduces its parameter count. This is problematic in view of the recent finding that heavily over-parameterized neural networks are often the ones that generalize best. In this paper we propose to use HyperNetworks to break the fixed ratio of FLOPs to parameters of standard convolutions. This allows us to exceed previous state-of-the-art architectures in SSIM and MS-SSIM on the Zurich RAW- to-DSLR (ZRR) data-set at > 10x reduced FLOP-count. On ZRR we further observe generalization curves consistent with 'double-descent' behavior at fixed FLOP-count, in the large image limit. Finally we demonstrate the same technique can be applied to an existing network (VDN) to reduce its computational cost while maintaining fidelity on the Smartphone Image Denoising Dataset (SIDD). Code for key functions is given in the appendix.



### Parallel Attention Network with Sequence Matching for Video Grounding
- **Arxiv ID**: http://arxiv.org/abs/2105.08481v1
- **DOI**: 10.18653/v1/2021.findings-acl.69
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.08481v1)
- **Published**: 2021-05-18 12:43:20+00:00
- **Updated**: 2021-05-18 12:43:20+00:00
- **Authors**: Hao Zhang, Aixin Sun, Wei Jing, Liangli Zhen, Joey Tianyi Zhou, Rick Siow Mong Goh
- **Comment**: 15 pages, 10 figures, 7 tables, Findings at ACL 2021
- **Journal**: None
- **Summary**: Given a video, video grounding aims to retrieve a temporal moment that semantically corresponds to a language query. In this work, we propose a Parallel Attention Network with Sequence matching (SeqPAN) to address the challenges in this task: multi-modal representation learning, and target moment boundary prediction. We design a self-guided parallel attention module to effectively capture self-modal contexts and cross-modal attentive information between video and text. Inspired by sequence labeling tasks in natural language processing, we split the ground truth moment into begin, inside, and end regions. We then propose a sequence matching strategy to guide start/end boundary predictions using region labels. Experimental results on three datasets show that SeqPAN is superior to state-of-the-art methods. Furthermore, the effectiveness of the self-guided parallel attention module and the sequence matching module is verified.



### Unsupervised identification of surgical robotic actions from small non homogeneous datasets
- **Arxiv ID**: http://arxiv.org/abs/2105.08488v2
- **DOI**: 10.1109/LRA.2021.3104880
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08488v2)
- **Published**: 2021-05-18 13:06:32+00:00
- **Updated**: 2021-10-11 12:12:01+00:00
- **Authors**: Daniele Meli, Paolo Fiorini
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters, vol.6, issue 4, October 2021
- **Summary**: Robot-assisted surgery is an established clinical practice. The automatic identification of surgical actions is needed for a range of applications, including performance assessment of trainees and surgical process modeling for autonomous execution and monitoring. However, supervised action identification is not feasible, due to the burden of manually annotating recordings of potentially complex and long surgical executions. Moreover, often few example executions of a surgical procedure can be recorded. This paper proposes a novel fast algorithm for unsupervised identification of surgical actions in a standard surgical training task, the ring transfer, executed with da Vinci Research Kit. Exploiting kinematic and semantic visual features automatically extracted from a very limited dataset of executions, we are able to significantly outperform state-of-the-art results on a dataset of non-expert executions (58\% vs. 24\% F1-score), and improve performance in the presence of noise, short actions and non-homogeneous workflows, i.e. non repetitive action sequences.



### Self-supervised Remote Sensing Images Change Detection at Pixel-level
- **Arxiv ID**: http://arxiv.org/abs/2105.08501v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.08501v2)
- **Published**: 2021-05-18 13:28:46+00:00
- **Updated**: 2021-10-08 15:27:14+00:00
- **Authors**: Yuxing Chen, Lorenzo Bruzzone
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning techniques have achieved great success in remote sensing image change detection. Most of them are supervised techniques, which usually require large amounts of training data and are limited to a particular application. Self-supervised methods as an unsupervised approach are popularly used to solve this problem and are widely used in unsupervised binary change detection tasks. However, the existing self-supervised methods in change detection are based on pre-tasks or at patch-level, which may be sub-optimal for pixel-wise change detection tasks. Therefore, in this work, a pixel-wise contrastive approach is proposed to overcome this limitation. This is achieved by using contrastive loss in pixel-level features on an unlabeled multi-view setting. In this approach, a Siamese ResUnet is trained to obtain pixel-wise representations and to align features from shifted positive pairs. Meanwhile, vector quantization is used to augment the learned features in two branches. The final binary change map is obtained by subtracting features of one branch from features of the other branch and using the Rosin thresholding method. To overcome the effects of regular seasonal changes in binary change maps, we also used an uncertainty method to enhance the temporal robustness of the proposed approach. Two homogeneous (OSCD and MUDS) datasets and one heterogeneous (California Flood) dataset are used to evaluate the performance of the proposed approach. Results demonstrate improvements in both efficiency and accuracy over the patch-wise multi-view contrastive method.



### Fixed $β$-VAE Encoding for Curious Exploration in Complex 3D Environments
- **Arxiv ID**: http://arxiv.org/abs/2105.08568v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.08568v1)
- **Published**: 2021-05-18 14:52:36+00:00
- **Updated**: 2021-05-18 14:52:36+00:00
- **Authors**: Auguste Lehuger, Matthew Crosby
- **Comment**: None
- **Journal**: None
- **Summary**: Curiosity is a general method for augmenting an environment reward with an intrinsic reward, which encourages exploration and is especially useful in sparse reward settings. As curiosity is calculated using next state prediction error, the type of state encoding used has a large impact on performance. Random features and inverse-dynamics features are generally preferred over VAEs based on previous results from Atari and other mostly 2D environments. However, unlike VAEs, they may not encode sufficient information for optimal behaviour, which becomes increasingly important as environments become more complex. In this paper, we use the sparse reward 3D physics environment Animal-AI, to demonstrate how a fixed $\beta$-VAE encoding can be used effectively with curiosity. We combine this with curriculum learning to solve the previously unsolved exploration intensive detour tasks while achieving 22\% gain in sample efficiency on the training curriculum against the next best encoding. We also corroborate the results on Atari Breakout, with our custom encoding outperforming random features and inverse-dynamics features.



### Dependent Multi-Task Learning with Causal Intervention for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2105.08573v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.08573v1)
- **Published**: 2021-05-18 14:57:33+00:00
- **Updated**: 2021-05-18 14:57:33+00:00
- **Authors**: Wenqing Chen, Jidong Tian, Caoyun Fan, Hao He, Yaohui Jin
- **Comment**: To be published in IJCAI 2021
- **Journal**: None
- **Summary**: Recent work for image captioning mainly followed an extract-then-generate paradigm, pre-extracting a sequence of object-based features and then formulating image captioning as a single sequence-to-sequence task. Although promising, we observed two problems in generated captions: 1) content inconsistency where models would generate contradicting facts; 2) not informative enough where models would miss parts of important information. From a causal perspective, the reason is that models have captured spurious statistical correlations between visual features and certain expressions (e.g., visual features of "long hair" and "woman"). In this paper, we propose a dependent multi-task learning framework with the causal intervention (DMTCI). Firstly, we involve an intermediate task, bag-of-categories generation, before the final task, image captioning. The intermediate task would help the model better understand the visual features and thus alleviate the content inconsistency problem. Secondly, we apply Pearl's do-calculus on the model, cutting off the link between the visual features and possible confounders and thus letting models focus on the causal visual features. Specifically, the high-frequency concept set is considered as the proxy confounders where the real confounders are inferred in the continuous space. Finally, we use a multi-agent reinforcement learning (MARL) strategy to enable end-to-end training and reduce the inter-task error accumulations. The extensive experiments show that our model outperforms the baseline models and achieves competitive performance with state-of-the-art models.



### Vision Transformer for Fast and Efficient Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2105.08582v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08582v1)
- **Published**: 2021-05-18 15:08:54+00:00
- **Updated**: 2021-05-18 15:08:54+00:00
- **Authors**: Rowel Atienza
- **Comment**: To appear at ICDAR2021 Springer Lecture Notes in Computer Science
  series
- **Journal**: ICDAR2021
- **Summary**: Scene text recognition (STR) enables computers to read text in natural scenes such as object labels, road signs and instructions. STR helps machines perform informed decisions such as what object to pick, which direction to go, and what is the next step of action. In the body of work on STR, the focus has always been on recognition accuracy. There is little emphasis placed on speed and computational efficiency which are equally important especially for energy-constrained mobile machines. In this paper we propose ViTSTR, an STR with a simple single stage model architecture built on a compute and parameter efficient vision transformer (ViT). On a comparable strong baseline method such as TRBA with accuracy of 84.3%, our small ViTSTR achieves a competitive accuracy of 82.6% (84.2% with data augmentation) at 2.4x speed up, using only 43.4% of the number of parameters and 42.2% FLOPS. The tiny version of ViTSTR achieves 80.3% accuracy (82.1% with data augmentation), at 2.5x the speed, requiring only 10.9% of the number of parameters and 11.9% FLOPS. With data augmentation, our base ViTSTR outperforms TRBA at 85.2% accuracy (83.7% without augmentation) at 2.3x the speed but requires 73.2% more parameters and 61.5% more FLOPS. In terms of trade-offs, nearly all ViTSTR configurations are at or near the frontiers to maximize accuracy, speed and computational efficiency all at the same time.



### Contrastive Model Inversion for Data-Free Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2105.08584v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.08584v1)
- **Published**: 2021-05-18 15:13:00+00:00
- **Updated**: 2021-05-18 15:13:00+00:00
- **Authors**: Gongfan Fang, Jie Song, Xinchao Wang, Chengchao Shen, Xingen Wang, Mingli Song
- **Comment**: None
- **Journal**: None
- **Summary**: Model inversion, whose goal is to recover training data from a pre-trained model, has been recently proved feasible. However, existing inversion methods usually suffer from the mode collapse problem, where the synthesized instances are highly similar to each other and thus show limited effectiveness for downstream tasks, such as knowledge distillation. In this paper, we propose Contrastive Model Inversion~(CMI), where the data diversity is explicitly modeled as an optimizable objective, to alleviate the mode collapse issue. Our main observation is that, under the constraint of the same amount of data, higher data diversity usually indicates stronger instance discrimination. To this end, we introduce in CMI a contrastive learning objective that encourages the synthesizing instances to be distinguishable from the already synthesized ones in previous batches. Experiments of pre-trained models on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that CMI not only generates more visually plausible instances than the state of the arts, but also achieves significantly superior performance when the generated data are used for knowledge distillation. Code is available at \url{https://github.com/zju-vipa/DataFree}.



### UncertaintyFuseNet: Robust Uncertainty-aware Hierarchical Feature Fusion Model with Ensemble Monte Carlo Dropout for COVID-19 Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.08590v3
- **DOI**: 10.1016/j.inffus.2022.09.023
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.08590v3)
- **Published**: 2021-05-18 15:20:34+00:00
- **Updated**: 2022-01-30 08:45:27+00:00
- **Authors**: Moloud Abdar, Soorena Salari, Sina Qahremani, Hak-Keung Lam, Fakhri Karray, Sadiq Hussain, Abbas Khosravi, U. Rajendra Acharya, Vladimir Makarenkov, Saeid Nahavandi
- **Comment**: 16 pages, 18 figures
- **Journal**: Information Fusion 2023
- **Summary**: The COVID-19 (Coronavirus disease 2019) pandemic has become a major global threat to human health and well-being. Thus, the development of computer-aided detection (CAD) systems that are capable to accurately distinguish COVID-19 from other diseases using chest computed tomography (CT) and X-ray data is of immediate priority. Such automatic systems are usually based on traditional machine learning or deep learning methods. Differently from most of existing studies, which used either CT scan or X-ray images in COVID-19-case classification, we present a simple but efficient deep learning feature fusion model, called UncertaintyFuseNet, which is able to classify accurately large datasets of both of these types of images. We argue that the uncertainty of the model's predictions should be taken into account in the learning process, even though most of existing studies have overlooked it. We quantify the prediction uncertainty in our feature fusion model using effective Ensemble MC Dropout (EMCD) technique. A comprehensive simulation study has been conducted to compare the results of our new model to the existing approaches, evaluating the performance of competing models in terms of Precision, Recall, F-Measure, Accuracy and ROC curves. The obtained results prove the efficiency of our model which provided the prediction accuracy of 99.08\% and 96.35\% for the considered CT scan and X-ray datasets, respectively. Moreover, our UncertaintyFuseNet model was generally robust to noise and performed well with previously unseen data. The source code of our implementation is freely available at: https://github.com/moloud1987/UncertaintyFuseNet-for-COVID-19-Classification.



### ACAE-REMIND for Online Continual Learning with Compressed Feature Replay
- **Arxiv ID**: http://arxiv.org/abs/2105.08595v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08595v2)
- **Published**: 2021-05-18 15:27:51+00:00
- **Updated**: 2021-07-08 15:19:08+00:00
- **Authors**: Kai Wang, Luis Herranz, Joost van de Weijer
- **Comment**: Pattern Recognition Letters
- **Journal**: None
- **Summary**: Online continual learning aims to learn from a non-IID stream of data from a number of different tasks, where the learner is only allowed to consider data once. Methods are typically allowed to use a limited buffer to store some of the images in the stream. Recently, it was found that feature replay, where an intermediate layer representation of the image is stored (or generated) leads to superior results than image replay, while requiring less memory. Quantized exemplars can further reduce the memory usage. However, a drawback of these methods is that they use a fixed (or very intransigent) backbone network. This significantly limits the learning of representations that can discriminate between all tasks. To address this problem, we propose an auxiliary classifier auto-encoder (ACAE) module for feature replay at intermediate layers with high compression rates. The reduced memory footprint per image allows us to save more exemplars for replay. In our experiments, we conduct task-agnostic evaluation under online continual learning setting and get state-of-the-art performance on ImageNet-Subset, CIFAR100 and CIFAR10 dataset.



### Shape Analysis of Functional Data with Elastic Partial Matching
- **Arxiv ID**: http://arxiv.org/abs/2105.08604v1
- **DOI**: None
- **Categories**: **stat.ME**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.08604v1)
- **Published**: 2021-05-18 15:36:51+00:00
- **Updated**: 2021-05-18 15:36:51+00:00
- **Authors**: Darshan Bryner, Anuj Srivastava
- **Comment**: Submitted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: Elastic Riemannian metrics have been used successfully in the past for statistical treatments of functional and curve shape data. However, this usage has suffered from an important restriction: the function boundaries are assumed fixed and matched. Functional data exhibiting unmatched boundaries typically arise from dynamical systems with variable evolution rates such as COVID-19 infection rate curves associated with different geographical regions. In this case, it is more natural to model such data with sliding boundaries and use partial matching, i.e., only a part of a function is matched to another function. Here, we develop a comprehensive Riemannian framework that allows for partial matching, comparing, and clustering of functions under both phase variability and uncertain boundaries. We extend past work by: (1) Forming a joint action of the time-warping and time-scaling groups; (2) Introducing a metric that is invariant to this joint action, allowing for a gradient-based approach to elastic partial matching; and (3) Presenting a modification that, while losing the metric property, allows one to control relative influence of the two groups. This framework is illustrated for registering and clustering shapes of COVID-19 rate curves, identifying essential patterns, minimizing mismatch errors, and reducing variability within clusters compared to previous methods.



### SAIL-VOS 3D: A Synthetic Dataset and Baselines for Object Detection and 3D Mesh Reconstruction from Video Data
- **Arxiv ID**: http://arxiv.org/abs/2105.08612v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.08612v1)
- **Published**: 2021-05-18 15:42:37+00:00
- **Updated**: 2021-05-18 15:42:37+00:00
- **Authors**: Yuan-Ting Hu, Jiahong Wang, Raymond A. Yeh, Alexander G. Schwing
- **Comment**: CVPR 2021 Oral
- **Journal**: None
- **Summary**: Extracting detailed 3D information of objects from video data is an important goal for holistic scene understanding. While recent methods have shown impressive results when reconstructing meshes of objects from a single image, results often remain ambiguous as part of the object is unobserved. Moreover, existing image-based datasets for mesh reconstruction don't permit to study models which integrate temporal information. To alleviate both concerns we present SAIL-VOS 3D: a synthetic video dataset with frame-by-frame mesh annotations which extends SAIL-VOS. We also develop first baselines for reconstruction of 3D meshes from video data via temporal models. We demonstrate efficacy of the proposed baseline on SAIL-VOS 3D and Pix3D, showing that temporal information improves reconstruction quality. Resources and additional information are available at http://sailvos.web.illinois.edu.



### Detecting Adversarial Examples with Bayesian Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2105.08620v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.08620v2)
- **Published**: 2021-05-18 15:51:24+00:00
- **Updated**: 2021-05-28 20:04:48+00:00
- **Authors**: Yao Li, Tongyi Tang, Cho-Jui Hsieh, Thomas C. M. Lee
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new framework to detect adversarial examples motivated by the observations that random components can improve the smoothness of predictors and make it easier to simulate output distribution of deep neural network. With these observations, we propose a novel Bayesian adversarial example detector, short for BATer, to improve the performance of adversarial example detection. In specific, we study the distributional difference of hidden layer output between natural and adversarial examples, and propose to use the randomness of Bayesian neural network (BNN) to simulate hidden layer output distribution and leverage the distribution dispersion to detect adversarial examples. The advantage of BNN is that the output is stochastic while neural networks without random components do not have such characteristics. Empirical results on several benchmark datasets against popular attacks show that the proposed BATer outperforms the state-of-the-art detectors in adversarial example detection.



### Assessing aesthetics of generated abstract images using correlation structure
- **Arxiv ID**: http://arxiv.org/abs/2105.08635v1
- **DOI**: 10.1109/SSCI44817.2019.9002779
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08635v1)
- **Published**: 2021-05-18 16:05:59+00:00
- **Updated**: 2021-05-18 16:05:59+00:00
- **Authors**: Sina Khajehabdollahi, Georg Martius, Anna Levina
- **Comment**: None
- **Journal**: 2019 IEEE Symposium Series on Computational Intelligence (SSCI),
  306-313
- **Summary**: Can we generate abstract aesthetic images without bias from natural or human selected image corpi? Are aesthetic images singled out in their correlation functions? In this paper we give answers to these and more questions. We generate images using compositional pattern-producing networks with random weights and varying architecture. We demonstrate that even with the randomly selected weights the correlation functions remain largely determined by the network architecture. In a controlled experiment, human subjects picked aesthetic images out of a large dataset of all generated images. Statistical analysis reveals that the correlation function is indeed different for aesthetic images.



### IntFormer: Predicting pedestrian intention with the aid of the Transformer architecture
- **Arxiv ID**: http://arxiv.org/abs/2105.08647v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.08647v1)
- **Published**: 2021-05-18 16:23:15+00:00
- **Updated**: 2021-05-18 16:23:15+00:00
- **Authors**: J. Lorenzo, I. Parra, M. A. Sotelo
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: Understanding pedestrian crossing behavior is an essential goal in intelligent vehicle development, leading to an improvement in their security and traffic flow. In this paper, we developed a method called IntFormer. It is based on transformer architecture and a novel convolutional video classification model called RubiksNet. Following the evaluation procedure in a recent benchmark, we show that our model reaches state-of-the-art results with good performance ($\approx 40$ seq. per second) and size ($8\times $smaller than the best performing model), making it suitable for real-time usage. We also explore each of the input features, finding that ego-vehicle speed is the most important variable, possibly due to the similarity in crossing cases in PIE dataset.



### A multimodal deep learning framework for scalable content based visual media retrieval
- **Arxiv ID**: http://arxiv.org/abs/2105.08665v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.08665v1)
- **Published**: 2021-05-18 16:49:08+00:00
- **Updated**: 2021-05-18 16:49:08+00:00
- **Authors**: Ambareesh Ravi, Amith Nandakumar
- **Comment**: Paper pertaining to a course project
- **Journal**: None
- **Summary**: We propose a novel, efficient, modular and scalable framework for content based visual media retrieval systems by leveraging the power of Deep Learning which is flexible to work both for images and videos conjointly and we also introduce an efficient comparison and filtering metric for retrieval. We put forward our findings from critical performance tests comparing our method to the predominant conventional approach to demonstrate the feasibility and efficiency of the proposed solution with best practices, possible improvements that may further augment the ability of retrieval architectures.



### Image Cropping on Twitter: Fairness Metrics, their Limitations, and the Importance of Representation, Design, and Agency
- **Arxiv ID**: http://arxiv.org/abs/2105.08667v2
- **DOI**: 10.1145/3479594
- **Categories**: **cs.CY**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.08667v2)
- **Published**: 2021-05-18 16:50:50+00:00
- **Updated**: 2021-09-09 17:24:31+00:00
- **Authors**: Kyra Yee, Uthaipon Tantipongpipat, Shubhanshu Mishra
- **Comment**: Accepted to CSCW 2021
- **Journal**: None
- **Summary**: Twitter uses machine learning to crop images, where crops are centered around the part predicted to be the most salient. In fall 2020, Twitter users raised concerns that the automated image cropping system on Twitter favored light-skinned over dark-skinned individuals, as well as concerns that the system favored cropping woman's bodies instead of their heads. In order to address these concerns, we conduct an extensive analysis using formalized group fairness metrics. We find systematic disparities in cropping and identify contributing factors, including the fact that the cropping based on the single most salient point can amplify the disparities because of an effect we term argmax bias. However, we demonstrate that formalized fairness metrics and quantitative analysis on their own are insufficient for capturing the risk of representational harm in automatic cropping. We suggest the removal of saliency-based cropping in favor of a solution that better preserves user agency. For developing a new solution that sufficiently address concerns related to representational harm, our critique motivates a combination of quantitative and qualitative methods that include human-centered design.



### Content Disentanglement for Semantically Consistent Synthetic-to-Real Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2105.08704v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08704v4)
- **Published**: 2021-05-18 17:42:26+00:00
- **Updated**: 2021-08-03 15:55:28+00:00
- **Authors**: Mert Keser, Artem Savkin, Federico Tombari
- **Comment**: None
- **Journal**: None
- **Summary**: Synthetic data generation is an appealing approach to generate novel traffic scenarios in autonomous driving. However, deep learning perception algorithms trained solely on synthetic data encounter serious performance drops when they are tested on real data. Such performance drops are commonly attributed to the domain gap between real and synthetic data. Domain adaptation methods that have been applied to mitigate the aforementioned domain gap achieve visually appealing results, but usually introduce semantic inconsistencies into the translated samples. In this work, we propose a novel, unsupervised, end-to-end domain adaptation network architecture that enables semantically consistent \textit{sim2real} image transfer. Our method performs content disentanglement by employing shared content encoder and fixed style code.



### Fighting Gradients with Gradients: Dynamic Defenses against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2105.08714v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.08714v1)
- **Published**: 2021-05-18 17:55:07+00:00
- **Updated**: 2021-05-18 17:55:07+00:00
- **Authors**: Dequan Wang, An Ju, Evan Shelhamer, David Wagner, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial attacks optimize against models to defeat defenses. Existing defenses are static, and stay the same once trained, even while attacks change. We argue that models should fight back, and optimize their defenses against attacks at test time. We propose dynamic defenses, to adapt the model and input during testing, by defensive entropy minimization (dent). Dent alters testing, but not training, for compatibility with existing models and train-time defenses. Dent improves the robustness of adversarially-trained defenses and nominally-trained models against white-box, black-box, and adaptive attacks on CIFAR-10/100 and ImageNet. In particular, dent boosts state-of-the-art defenses by 20+ points absolute against AutoAttack on CIFAR-10 at $\epsilon_\infty$ = 8/255.



### Human Motion Prediction Using Manifold-Aware Wasserstein GAN
- **Arxiv ID**: http://arxiv.org/abs/2105.08715v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.08715v2)
- **Published**: 2021-05-18 17:56:10+00:00
- **Updated**: 2021-07-16 21:16:53+00:00
- **Authors**: Baptiste Chopin, Naima Otberdout, Mohamed Daoudi, Angela Bartolo
- **Comment**: IEEE International Conference on Automatic Face and Gesture
  Recognition 2021 Jodhpur, India December 15 - 18, 2021
- **Journal**: None
- **Summary**: Human motion prediction aims to forecast future human poses given a prior pose sequence. The discontinuity of the predicted motion and the performance deterioration in long-term horizons are still the main challenges encountered in current literature. In this work, we tackle these issues by using a compact manifold-valued representation of human motion. Specifically, we model the temporal evolution of the 3D human poses as trajectory, what allows us to map human motions to single points on a sphere manifold. To learn these non-Euclidean representations, we build a manifold-aware Wasserstein generative adversarial model that captures the temporal and spatial dependencies of human motion through different losses. Extensive experiments show that our approach outperforms the state-of-the-art on CMU MoCap and Human 3.6M datasets. Our qualitative results show the smoothness of the predicted motions.



### Pathdreamer: A World Model for Indoor Navigation
- **Arxiv ID**: http://arxiv.org/abs/2105.08756v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.08756v2)
- **Published**: 2021-05-18 18:13:53+00:00
- **Updated**: 2021-08-16 19:00:16+00:00
- **Authors**: Jing Yu Koh, Honglak Lee, Yinfei Yang, Jason Baldridge, Peter Anderson
- **Comment**: In ICCV 2021
- **Journal**: None
- **Summary**: People navigating in unfamiliar buildings take advantage of myriad visual, spatial and semantic cues to efficiently achieve their navigation goals. Towards equipping computational agents with similar capabilities, we introduce Pathdreamer, a visual world model for agents navigating in novel indoor environments. Given one or more previous visual observations, Pathdreamer generates plausible high-resolution 360 visual observations (RGB, semantic segmentation and depth) for viewpoints that have not been visited, in buildings not seen during training. In regions of high uncertainty (e.g. predicting around corners, imagining the contents of an unseen room), Pathdreamer can predict diverse scenes, allowing an agent to sample multiple realistic outcomes for a given trajectory. We demonstrate that Pathdreamer encodes useful and accessible visual, spatial and semantic knowledge about human environments by using it in the downstream task of Vision-and-Language Navigation (VLN). Specifically, we show that planning ahead with Pathdreamer brings about half the benefit of looking ahead at actual observations from unobserved parts of the environment. We hope that Pathdreamer will help unlock model-based approaches to challenging embodied navigation tasks such as navigating to specified objects and VLN.



### Self-Supervised Learning for Fine-Grained Visual Categorization
- **Arxiv ID**: http://arxiv.org/abs/2105.08788v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08788v1)
- **Published**: 2021-05-18 19:16:05+00:00
- **Updated**: 2021-05-18 19:16:05+00:00
- **Authors**: Muhammad Maaz, Hanoona Abdul Rasheed, Dhanalaxmi Gaddam
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Recent research in self-supervised learning (SSL) has shown its capability in learning useful semantic representations from images for classification tasks. Through our work, we study the usefulness of SSL for Fine-Grained Visual Categorization (FGVC). FGVC aims to distinguish objects of visually similar sub categories within a general category. The small inter-class, but large intra-class variations within the dataset makes it a challenging task. The limited availability of annotated labels for such a fine-grained data encourages the need for SSL, where additional supervision can boost learning without the cost of extra annotations. Our baseline achieves $86.36\%$ top-1 classification accuracy on CUB-200-2011 dataset by utilizing random crop augmentation during training and center crop augmentation during testing. In this work, we explore the usefulness of various pretext tasks, specifically, rotation, pretext invariant representation learning (PIRL), and deconstruction and construction learning (DCL) for FGVC. Rotation as an auxiliary task promotes the model to learn global features, and diverts it from focusing on the subtle details. PIRL that uses jigsaw patches attempts to focus on discriminative local regions, but struggles to accurately localize them. DCL helps in learning local discriminating features and outperforms the baseline by achieving $87.41\%$ top-1 accuracy. The deconstruction learning forces the model to focus on local object parts, while reconstruction learning helps in learning the correlation between the parts. We perform extensive experiments to reason our findings. Our code is available at https://github.com/mmaaz60/ssl_for_fgvc.



### Masked Contrastive Learning for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.08793v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.08793v2)
- **Published**: 2021-05-18 19:27:02+00:00
- **Updated**: 2023-01-30 07:02:33+00:00
- **Authors**: Hyunsoo Cho, Jinseok Seol, Sang-goo Lee
- **Comment**: Accepted to IJCAI 2021
- **Journal**: None
- **Summary**: Detecting anomalies is one fundamental aspect of a safety-critical software system, however, it remains a long-standing problem. Numerous branches of works have been proposed to alleviate the complication and have demonstrated their efficiencies. In particular, self-supervised learning based methods are spurring interest due to their capability of learning diverse representations without additional labels. Among self-supervised learning tactics, contrastive learning is one specific framework validating their superiority in various fields, including anomaly detection. However, the primary objective of contrastive learning is to learn task-agnostic features without any labels, which is not entirely suited to discern anomalies. In this paper, we propose a task-specific variant of contrastive learning named masked contrastive learning, which is more befitted for anomaly detection. Moreover, we propose a new inference method dubbed self-ensemble inference that further boosts performance by leveraging the ability learned through auxiliary self-supervision tasks. By combining our models, we can outperform previous state-of-the-art methods by a significant margin on various benchmark datasets.



### Analyzing the effectiveness of image augmentations for face recognition from limited data
- **Arxiv ID**: http://arxiv.org/abs/2105.08796v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2105.08796v1)
- **Published**: 2021-05-18 19:33:17+00:00
- **Updated**: 2021-05-18 19:33:17+00:00
- **Authors**: Aleksei Zhuchkov
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents an analysis of the efficiency of image augmentations for the face recognition problem from limited data. We considered basic manipulations, generative methods, and their combinations for augmentations. Our results show that augmentations, in general, can considerably improve the quality of face recognition systems and the combination of generative and basic approaches performs better than the other tested techniques.



### Correlated Adversarial Joint Discrepancy Adaptation Network
- **Arxiv ID**: http://arxiv.org/abs/2105.08808v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08808v1)
- **Published**: 2021-05-18 19:52:08+00:00
- **Updated**: 2021-05-18 19:52:08+00:00
- **Authors**: Youshan Zhang, Brian D. Davison
- **Comment**: None
- **Journal**: None
- **Summary**: Domain adaptation aims to mitigate the domain shift problem when transferring knowledge from one domain into another similar but different domain. However, most existing works rely on extracting marginal features without considering class labels. Moreover, some methods name their model as so-called unsupervised domain adaptation while tuning the parameters using the target domain label. To address these issues, we propose a novel approach called correlated adversarial joint discrepancy adaptation network (CAJNet), which minimizes the joint discrepancy of two domains and achieves competitive performance with tuning parameters using the correlated label. By training the joint features, we can align the marginal and conditional distributions between the two domains. In addition, we introduce a probability-based top-$\mathcal{K}$ correlated label ($\mathcal{K}$-label), which is a powerful indicator of the target domain and effective metric to tune parameters to aid predictions. Extensive experiments on benchmark datasets demonstrate significant improvements in classification accuracy over the state of the art.



### Multimodal Deep Learning Framework for Image Popularity Prediction on Social Media
- **Arxiv ID**: http://arxiv.org/abs/2105.08809v1
- **DOI**: 10.1109/TCDS.2020.3036690
- **Categories**: **cs.CV**, cs.MM, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2105.08809v1)
- **Published**: 2021-05-18 19:58:58+00:00
- **Updated**: 2021-05-18 19:58:58+00:00
- **Authors**: Fatma S. Abousaleh, Wen-Huang Cheng, Neng-Hao Yu, Yu Tsao
- **Comment**: 14 pages, 11 figures, 7 tables
- **Journal**: IEEE Transactions on Cognitive and Developmental Systems. 2020 Nov
  9
- **Summary**: Billions of photos are uploaded to the web daily through various types of social networks. Some of these images receive millions of views and become popular, whereas others remain completely unnoticed. This raises the problem of predicting image popularity on social media. The popularity of an image can be affected by several factors, such as visual content, aesthetic quality, user, post metadata, and time. Thus, considering all these factors is essential for accurately predicting image popularity. In addition, the efficiency of the predictive model also plays a crucial role. In this study, motivated by multimodal learning, which uses information from various modalities, and the current success of convolutional neural networks (CNNs) in various fields, we propose a deep learning model, called visual-social convolutional neural network (VSCNN), which predicts the popularity of a posted image by incorporating various types of visual and social features into a unified network model. VSCNN first learns to extract high-level representations from the input visual and social features by utilizing two individual CNNs. The outputs of these two networks are then fused into a joint network to estimate the popularity score in the output layer. We assess the performance of the proposed method by conducting extensive experiments on a dataset of approximately 432K images posted on Flickr. The simulation results demonstrate that the proposed VSCNN model significantly outperforms state-of-the-art models, with a relative improvement of greater than 2.33%, 7.59%, and 14.16% in terms of Spearman's Rho, mean absolute error, and mean squared error, respectively.



### Non-contact Pain Recognition from Video Sequences with Remote Physiological Measurements Prediction
- **Arxiv ID**: http://arxiv.org/abs/2105.08822v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08822v2)
- **Published**: 2021-05-18 20:47:45+00:00
- **Updated**: 2021-12-25 19:40:01+00:00
- **Authors**: Ruijing Yang, Ziyu Guan, Zitong Yu, Xiaoyi Feng, Jinye Peng, Guoying Zhao
- **Comment**: IJCAI 2021
- **Journal**: https://www.ijcai.org/proceedings/2021/170
- **Summary**: Automatic pain recognition is paramount for medical diagnosis and treatment. The existing works fall into three categories: assessing facial appearance changes, exploiting physiological cues, or fusing them in a multi-modal manner. However, (1) appearance changes are easily affected by subjective factors which impedes objective pain recognition. Besides, the appearance-based approaches ignore long-range spatial-temporal dependencies that are important for modeling expressions over time; (2) the physiological cues are obtained by attaching sensors on human body, which is inconvenient and uncomfortable. In this paper, we present a novel multi-task learning framework which encodes both appearance changes and physiological cues in a non-contact manner for pain recognition. The framework is able to capture both local and long-range dependencies via the proposed attention mechanism for the learned appearance representations, which are further enriched by temporally attended physiological cues (remote photoplethysmography, rPPG) that are recovered from videos in the auxiliary task. This framework is dubbed rPPG-enriched Spatio-Temporal Attention Network (rSTAN) and allows us to establish the state-of-the-art performance of non-contact pain recognition on publicly available pain databases. It demonstrates that rPPG predictions can be used as an auxiliary task to facilitate non-contact automatic pain recognition.



### Multi-Person Extreme Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2105.08825v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08825v7)
- **Published**: 2021-05-18 20:52:05+00:00
- **Updated**: 2022-06-19 16:58:07+00:00
- **Authors**: Wen Guo, Xiaoyu Bie, Xavier Alameda-Pineda, Francesc Moreno-Noguer
- **Comment**: CVPR 2022, update results of MSR in Table 3
- **Journal**: None
- **Summary**: Human motion prediction aims to forecast future poses given a sequence of past 3D skeletons. While this problem has recently received increasing attention, it has mostly been tackled for single humans in isolation. In this paper, we explore this problem when dealing with humans performing collaborative tasks, we seek to predict the future motion of two interacted persons given two sequences of their past skeletons. We propose a novel cross interaction attention mechanism that exploits historical information of both persons, and learns to predict cross dependencies between the two pose sequences. Since no dataset to train such interactive situations is available, we collected ExPI (Extreme Pose Interaction), a new lab-based person interaction dataset of professional dancers performing Lindy-hop dancing actions, which contains 115 sequences with 30K frames annotated with 3D body poses and shapes. We thoroughly evaluate our cross interaction network on ExPI and show that both in short- and long-term predictions, it consistently outperforms state-of-the-art methods for single-person motion prediction.



### Fusion-DHL: WiFi, IMU, and Floorplan Fusion for Dense History of Locations in Indoor Environments
- **Arxiv ID**: http://arxiv.org/abs/2105.08837v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.08837v1)
- **Published**: 2021-05-18 21:26:45+00:00
- **Updated**: 2021-05-18 21:26:45+00:00
- **Authors**: Sachini Herath, Saghar Irandoust, Bowen Chen, Yiming Qian, Pyojin Kim, Yasutaka Furukawa
- **Comment**: To be published in ICRA 2021. Code and data:
  https://github.com/Sachini/Fusion-DHL
- **Journal**: ICRA 2021
- **Summary**: The paper proposes a multi-modal sensor fusion algorithm that fuses WiFi, IMU, and floorplan information to infer an accurate and dense location history in indoor environments. The algorithm uses 1) an inertial navigation algorithm to estimate a relative motion trajectory from IMU sensor data; 2) a WiFi-based localization API in industry to obtain positional constraints and geo-localize the trajectory; and 3) a convolutional neural network to refine the location history to be consistent with the floorplan.   We have developed a data acquisition app to build a new dataset with WiFi, IMU, and floorplan data with ground-truth positions at 4 university buildings and 3 shopping malls. Our qualitative and quantitative evaluations demonstrate that the proposed system is able to produce twice as accurate and a few orders of magnitude denser location history than the current standard, while requiring minimal additional energy consumption. We will publicly share our code, data and models.



