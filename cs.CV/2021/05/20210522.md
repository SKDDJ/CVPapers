# Arxiv Papers in cs.CV on 2021-05-22
### HPNet: Deep Primitive Segmentation Using Hybrid Representations
- **Arxiv ID**: http://arxiv.org/abs/2105.10620v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10620v4)
- **Published**: 2021-05-22 02:12:46+00:00
- **Updated**: 2021-10-21 00:29:08+00:00
- **Authors**: Siming Yan, Zhenpei Yang, Chongyang Ma, Haibin Huang, Etienne Vouga, Qixing Huang
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: This paper introduces HPNet, a novel deep-learning approach for segmenting a 3D shape represented as a point cloud into primitive patches. The key to deep primitive segmentation is learning a feature representation that can separate points of different primitives. Unlike utilizing a single feature representation, HPNet leverages hybrid representations that combine one learned semantic descriptor, two spectral descriptors derived from predicted geometric parameters, as well as an adjacency matrix that encodes sharp edges. Moreover, instead of merely concatenating the descriptors, HPNet optimally combines hybrid representations by learning combination weights. This weighting module builds on the entropy of input features. The output primitive segmentation is obtained from a mean-shift clustering module. Experimental results on benchmark datasets ANSI and ABCParts show that HPNet leads to significant performance gains from baseline approaches.



### Searching Collaborative Agents for Multi-plane Localization in 3D Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/2105.10626v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MA, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10626v1)
- **Published**: 2021-05-22 02:48:23+00:00
- **Updated**: 2021-05-22 02:48:23+00:00
- **Authors**: Xin Yang, Yuhao Huang, Ruobing Huang, Haoran Dou, Rui Li, Jikuan Qian, Xiaoqiong Huang, Wenlong Shi, Chaoyu Chen, Yuanji Zhang, Haixia Wang, Yi Xiong, Dong Ni
- **Comment**: Accepted by Medical Image Analysis (10 figures, 8 tabels)
- **Journal**: None
- **Summary**: 3D ultrasound (US) has become prevalent due to its rich spatial and diagnostic information not contained in 2D US. Moreover, 3D US can contain multiple standard planes (SPs) in one shot. Thus, automatically localizing SPs in 3D US has the potential to improve user-independence and scanning-efficiency. However, manual SP localization in 3D US is challenging because of the low image quality, huge search space and large anatomical variability. In this work, we propose a novel multi-agent reinforcement learning (MARL) framework to simultaneously localize multiple SPs in 3D US. Our contribution is four-fold. First, our proposed method is general and it can accurately localize multiple SPs in different challenging US datasets. Second, we equip the MARL system with a recurrent neural network (RNN) based collaborative module, which can strengthen the communication among agents and learn the spatial relationship among planes effectively. Third, we explore to adopt the neural architecture search (NAS) to automatically design the network architecture of both the agents and the collaborative module. Last, we believe we are the first to realize automatic SP localization in pelvic US volumes, and note that our approach can handle both normal and abnormal uterus cases. Extensively validated on two challenging datasets of the uterus and fetal brain, our proposed method achieves the average localization accuracy of 7.03 degrees/1.59mm and 9.75 degrees/1.19mm. Experimental results show that our light-weight MARL model has higher accuracy than state-of-the-art methods.



### Revisiting Knowledge Distillation for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.10633v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10633v1)
- **Published**: 2021-05-22 03:46:58+00:00
- **Updated**: 2021-05-22 03:46:58+00:00
- **Authors**: Amin Banitalebi-Dehkordi
- **Comment**: None
- **Journal**: None
- **Summary**: The existing solutions for object detection distillation rely on the availability of both a teacher model and ground-truth labels. We propose a new perspective to relax this constraint. In our framework, a student is first trained with pseudo labels generated by the teacher, and then fine-tuned using labeled data, if any available. Extensive experiments demonstrate improvements over existing object detection distillation algorithms. In addition, decoupling the teacher and ground-truth distillation in this framework provides interesting properties such: as 1) using unlabeled data to further improve the student's performance, 2) combining multiple teacher models of different architectures, even with different object categories, and 3) reducing the need for labeled data (with only 20% of COCO labels, this method achieves the same performance as the model trained on the entire set of labels). Furthermore, a by-product of this approach is the potential usage for domain adaptation. We verify these properties through extensive experiments.



### Semi-Supervised Few-Shot Classification with Deep Invertible Hybrid Models
- **Arxiv ID**: http://arxiv.org/abs/2105.10644v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.10644v1)
- **Published**: 2021-05-22 05:55:16+00:00
- **Updated**: 2021-05-22 05:55:16+00:00
- **Authors**: Yusuke Ohtsubo, Tetsu Matsukawa, Einoshin Suzuki
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: In this paper, we propose a deep invertible hybrid model which integrates discriminative and generative learning at a latent space level for semi-supervised few-shot classification. Various tasks for classifying new species from image data can be modeled as a semi-supervised few-shot classification, which assumes a labeled and unlabeled training examples and a small support set of the target classes. Predicting target classes with a few support examples per class makes the learning task difficult for existing semi-supervised classification methods, including selftraining, which iteratively estimates class labels of unlabeled training examples to learn a classifier for the training classes. To exploit unlabeled training examples effectively, we adopt as the objective function the composite likelihood, which integrates discriminative and generative learning and suits better with deep neural networks than the parameter coupling prior, the other popular integrated learning approach. In our proposed model, the discriminative and generative models are respectively Prototypical Networks, which have shown excellent performance in various kinds of few-shot learning, and Normalizing Flow a deep invertible model which returns the exact marginal likelihood unlike the other three major methods, i.e., VAE, GAN, and autoregressive model. Our main originality lies in our integration of these components at a latent space level, which is effective in preventing overfitting. Experiments using mini-ImageNet and VGG-Face datasets show that our method outperforms selftraining based Prototypical Networks.



### Post-Radiotherapy PET Image Outcome Prediction by Deep Learning under Biological Model Guidance: A Feasibility Study of Oropharyngeal Cancer Application
- **Arxiv ID**: http://arxiv.org/abs/2105.10650v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10650v1)
- **Published**: 2021-05-22 06:32:58+00:00
- **Updated**: 2021-05-22 06:32:58+00:00
- **Authors**: Hangjie Ji, Kyle Lafata, Yvonne Mowery, David Brizel, Andrea L. Bertozzi, Fang-Fang Yin, Chunhao Wang
- **Comment**: 26 pages, 5 figures
- **Journal**: None
- **Summary**: This paper develops a method of biologically guided deep learning for post-radiation FDG-PET image outcome prediction based on pre-radiation images and radiotherapy dose information. Based on the classic reaction-diffusion mechanism, a novel biological model was proposed using a partial differential equation that incorporates spatial radiation dose distribution as a patient-specific treatment information variable. A 7-layer encoder-decoder-based convolutional neural network (CNN) was designed and trained to learn the proposed biological model. As such, the model could generate post-radiation FDG-PET image outcome predictions with possible time-series transition from pre-radiotherapy image states to post-radiotherapy states. The proposed method was developed using 64 oropharyngeal patients with paired FDG-PET studies before and after 20Gy delivery (2Gy/daily fraction) by IMRT. In a two-branch deep learning execution, the proposed CNN learns specific terms in the biological model from paired FDG-PET images and spatial dose distribution as in one branch, and the biological model generates post-20Gy FDG-PET image prediction in the other branch. The proposed method successfully generated post-20Gy FDG-PET image outcome prediction with breakdown illustrations of biological model components. Time-series FDG-PET image predictions were generated to demonstrate the feasibility of disease response rendering. The developed biologically guided deep learning method achieved post-20Gy FDG-PET image outcome predictions in good agreement with ground-truth results. With break-down biological modeling components, the outcome image predictions could be used in adaptive radiotherapy decision-making to optimize personalized plans for the best outcome in the future.



### Video-based Person Re-identification without Bells and Whistles
- **Arxiv ID**: http://arxiv.org/abs/2105.10678v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10678v2)
- **Published**: 2021-05-22 10:17:38+00:00
- **Updated**: 2021-09-22 11:07:44+00:00
- **Authors**: Chih-Ting Liu, Jun-Cheng Chen, Chu-Song Chen, Shao-Yi Chien
- **Comment**: This paper was accepted by CVPR 2021 Biometrics Workshop
- **Journal**: None
- **Summary**: Video-based person re-identification (Re-ID) aims at matching the video tracklets with cropped video frames for identifying the pedestrians under different cameras. However, there exists severe spatial and temporal misalignment for those cropped tracklets due to the imperfect detection and tracking results generated with obsolete methods. To address this issue, we present a simple re-Detect and Link (DL) module which can effectively reduce those unexpected noise through applying the deep learning-based detection and tracking on the cropped tracklets. Furthermore, we introduce an improved model called Coarse-to-Fine Axial-Attention Network (CF-AAN). Based on the typical Non-local Network, we replace the non-local module with three 1-D position-sensitive axial attentions, in addition to our proposed coarse-to-fine structure. With the developed CF-AAN, compared to the original non-local operation, we can not only significantly reduce the computation cost but also obtain the state-of-the-art performance (91.3% in rank-1 and 86.5% in mAP) on the large-scale MARS dataset. Meanwhile, by simply adopting our DL module for data alignment, to our surprise, several baseline models can achieve better or comparable results with the current state-of-the-arts. Besides, we discover the errors not only for the identity labels of tracklets but also for the evaluation protocol for the test data of MARS. We hope that our work can help the community for the further development of invariant representation without the hassle of the spatial and temporal alignment and dataset noise. The code, corrected labels, evaluation protocol, and the aligned data will be available at https://github.com/jackie840129/CF-AAN.



### Towards Automatic Recognition of Pure & Mixed Stones using Intraoperative Endoscopic Digital Images
- **Arxiv ID**: http://arxiv.org/abs/2105.10686v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.10686v1)
- **Published**: 2021-05-22 10:52:19+00:00
- **Updated**: 2021-05-22 10:52:19+00:00
- **Authors**: Vincent Estrade, Michel Daudon, Emmanuel Richard, Jean-Christophe Bernhard, Franck Bladou, Gregoire Robert, Baudouin Denis de Senneville
- **Comment**: 19 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: Objective: To assess automatic computer-aided in-situ recognition of morphological features of pure and mixed urinary stones using intraoperative digital endoscopic images acquired in a clinical setting. Materials and methods: In this single-centre study, an experienced urologist intraoperatively and prospectively examined the surface and section of all kidney stones encountered. Calcium oxalate monohydrate (COM/Ia), dihydrate (COD/IIb) and uric acid (UA/IIIb) morphological criteria were collected and classified to generate annotated datasets. A deep convolutional neural network (CNN) was trained to predict the composition of both pure and mixed stones. To explain the predictions of the deep neural network model, coarse localisation heat-maps were plotted to pinpoint key areas identified by the network. Results: This study included 347 and 236 observations of stone surface and stone section, respectively. A highest sensitivity of 98 % was obtained for the type "pure IIIb/UA" using surface images. The most frequently encountered morphology was that of the type "pure Ia/COM"; it was correctly predicted in 91 % and 94 % of cases using surface and section images, respectively. Of the mixed type "Ia/COM+IIb/COD", Ia/COM was predicted in 84 % of cases using surface images, IIb/COD in 70 % of cases, and both in 65 % of cases. Concerning mixed Ia/COM+IIIb/UA stones, Ia/COM was predicted in 91 % of cases using section images, IIIb/UA in 69 % of cases, and both in 74 % of cases. Conclusions: This preliminary study demonstrates that deep convolutional neural networks are promising to identify kidney stone composition from endoscopic images acquired intraoperatively. Both pure and mixed stone composition could be discriminated. Collected in a clinical setting, surface and section images analysed by deep CNN provide valuable information about stone morphology for computer-aided diagnosis.



### ADNet: Attention-guided Deformable Convolutional Network for High Dynamic Range Imaging
- **Arxiv ID**: http://arxiv.org/abs/2105.10697v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10697v1)
- **Published**: 2021-05-22 11:37:09+00:00
- **Updated**: 2021-05-22 11:37:09+00:00
- **Authors**: Zhen Liu, Wenjie Lin, Xinpeng Li, Qing Rao, Ting Jiang, Mingyan Han, Haoqiang Fan, Jian Sun, Shuaicheng Liu
- **Comment**: Accepted by CVPRW 2021
- **Journal**: None
- **Summary**: In this paper, we present an attention-guided deformable convolutional network for hand-held multi-frame high dynamic range (HDR) imaging, namely ADNet. This problem comprises two intractable challenges of how to handle saturation and noise properly and how to tackle misalignments caused by object motion or camera jittering. To address the former, we adopt a spatial attention module to adaptively select the most appropriate regions of various exposure low dynamic range (LDR) images for fusion. For the latter one, we propose to align the gamma-corrected images in the feature-level with a Pyramid, Cascading and Deformable (PCD) alignment module. The proposed ADNet shows state-of-the-art performance compared with previous methods, achieving a PSNR-$l$ of 39.4471 and a PSNR-$\mu$ of 37.6359 in NTIRE 2021 Multi-Frame HDR Challenge.



### Denoising Noisy Neural Networks: A Bayesian Approach with Compensation
- **Arxiv ID**: http://arxiv.org/abs/2105.10699v3
- **DOI**: 10.1109/TSP.2023.3290327
- **Categories**: **cs.LG**, cs.CV, cs.DC, cs.IT, eess.SP, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2105.10699v3)
- **Published**: 2021-05-22 11:51:20+00:00
- **Updated**: 2022-05-19 15:28:09+00:00
- **Authors**: Yulin Shao, Soung Chang Liew, Deniz Gunduz
- **Comment**: Keywords: Noisy neural network, denoiser, wireless transmission of
  neural networks, federated edge learning, analog device. 18 pages, 9 figures
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) with noisy weights, which we refer to as noisy neural networks (NoisyNNs), arise from the training and inference of DNNs in the presence of noise. NoisyNNs emerge in many new applications, including the wireless transmission of DNNs, the efficient deployment or storage of DNNs in analog devices, and the truncation or quantization of DNN weights. This paper studies a fundamental problem of NoisyNNs: how to reconstruct the DNN weights from their noisy manifestations. While all prior works relied on the maximum likelihood (ML) estimation, this paper puts forth a denoising approach to reconstruct DNNs with the aim of maximizing the inference accuracy of the reconstructed models. The superiority of our denoiser is rigorously proven in two small-scale problems, wherein we consider a quadratic neural network function and a shallow feedforward neural network, respectively. When applied to advanced learning tasks with modern DNN architectures, our denoiser exhibits significantly better performance than the ML estimator. Consider the average test accuracy of the denoised DNN model versus the weight variance to noise power ratio (WNR) performance. When denoising a noisy ResNet34 model arising from noisy inference, our denoiser outperforms ML estimation by up to 4.1 dB to achieve a test accuracy of 60%.When denoising a noisy ResNet18 model arising from noisy training, our denoiser outperforms ML estimation by 13.4 dB and 8.3 dB to achieve test accuracies of 60% and 80%, respectively.



### Soccer Player Tracking in Low Quality Video
- **Arxiv ID**: http://arxiv.org/abs/2105.10700v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10700v1)
- **Published**: 2021-05-22 11:55:08+00:00
- **Updated**: 2021-05-22 11:55:08+00:00
- **Authors**: Eloi Martins, José Henrique Brito
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a system capable of tracking multiple soccer players in different types of video quality. The main goal, in contrast to most state-of-art soccer player tracking systems, is the ability of execute effectively tracking in videos of low-quality. We adapted a state-of-art Multiple Object Tracking to the task. In order to do that adaptation, we created a Detection and a Tracking Dataset for 3 different qualities of video. The results of our system are conclusive of its high performance.



### Automated Knee X-ray Report Generation
- **Arxiv ID**: http://arxiv.org/abs/2105.10702v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.10702v1)
- **Published**: 2021-05-22 11:59:42+00:00
- **Updated**: 2021-05-22 11:59:42+00:00
- **Authors**: Aydan Gasimova, Giovanni Montana, Daniel Rueckert
- **Comment**: None
- **Journal**: NeurIPS Machine Learning for Health Workshop 2017
- **Summary**: Gathering manually annotated images for the purpose of training a predictive model is far more challenging in the medical domain than for natural images as it requires the expertise of qualified radiologists. We therefore propose to take advantage of past radiological exams (specifically, knee X-ray examinations) and formulate a framework capable of learning the correspondence between the images and reports, and hence be capable of generating diagnostic reports for a given X-ray examination consisting of an arbitrary number of image views. We demonstrate how aggregating the image features of individual exams and using them as conditional inputs when training a language generation model results in auto-generated exam reports that correlate well with radiologist-generated reports.



### PAL: Intelligence Augmentation using Egocentric Visual Context Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.10735v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2105.10735v1)
- **Published**: 2021-05-22 14:01:22+00:00
- **Updated**: 2021-05-22 14:01:22+00:00
- **Authors**: Mina Khan, Pattie Maes
- **Comment**: None
- **Journal**: CVPR EPIC Workshop 2021
- **Summary**: Egocentric visual context detection can support intelligence augmentation applications. We created a wearable system, called PAL, for wearable, personalized, and privacy-preserving egocentric visual context detection. PAL has a wearable device with a camera, heart-rate sensor, on-device deep learning, and audio input/output. PAL also has a mobile/web application for personalized context labeling. We used on-device deep learning models for generic object and face detection, low-shot custom face and context recognition (e.g., activities like brushing teeth), and custom context clustering (e.g., indoor locations). The models had over 80\% accuracy in in-the-wild contexts (~1000 images) and we tested PAL for intelligence augmentation applications like behavior change. We have made PAL is open-source to further support intelligence augmentation using personalized and privacy-preserving egocentric visual contexts.



### MIASSR: An Approach for Medical Image Arbitrary Scale Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2105.10738v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10738v1)
- **Published**: 2021-05-22 14:24:25+00:00
- **Updated**: 2021-05-22 14:24:25+00:00
- **Authors**: Jin Zhu, Chuan Tan, Junwei Yang, Guang Yang, Pietro Lio'
- **Comment**: None
- **Journal**: None
- **Summary**: Single image super-resolution (SISR) aims to obtain a high-resolution output from one low-resolution image. Currently, deep learning-based SISR approaches have been widely discussed in medical image processing, because of their potential to achieve high-quality, high spatial resolution images without the cost of additional scans. However, most existing methods are designed for scale-specific SR tasks and are unable to generalise over magnification scales. In this paper, we propose an approach for medical image arbitrary-scale super-resolution (MIASSR), in which we couple meta-learning with generative adversarial networks (GANs) to super-resolve medical images at any scale of magnification in (1, 4]. Compared to state-of-the-art SISR algorithms on single-modal magnetic resonance (MR) brain images (OASIS-brains) and multi-modal MR brain images (BraTS), MIASSR achieves comparable fidelity performance and the best perceptual quality with the smallest model size. We also employ transfer learning to enable MIASSR to tackle SR tasks of new medical modalities, such as cardiac MR images (ACDC) and chest computed tomography images (COVID-CT). The source code of our work is also public. Thus, MIASSR has the potential to become a new foundational pre-/post-processing step in clinical image analysis tasks such as reconstruction, image quality enhancement, and segmentation.



### PLM: Partial Label Masking for Imbalanced Multi-label Classification
- **Arxiv ID**: http://arxiv.org/abs/2105.10782v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10782v1)
- **Published**: 2021-05-22 18:07:56+00:00
- **Updated**: 2021-05-22 18:07:56+00:00
- **Authors**: Kevin Duarte, Yogesh S. Rawat, Mubarak Shah
- **Comment**: Accepted to the CVPR 2021 Learning from Limited or Imperfect Data
  (L2ID) Workshop
- **Journal**: None
- **Summary**: Neural networks trained on real-world datasets with long-tailed label distributions are biased towards frequent classes and perform poorly on infrequent classes. The imbalance in the ratio of positive and negative samples for each class skews network output probabilities further from ground-truth distributions. We propose a method, Partial Label Masking (PLM), which utilizes this ratio during training. By stochastically masking labels during loss computation, the method balances this ratio for each class, leading to improved recall on minority classes and improved precision on frequent classes. The ratio is estimated adaptively based on the network's performance by minimizing the KL divergence between predicted and ground-truth distributions. Whereas most existing approaches addressing data imbalance are mainly focused on single-label classification and do not generalize well to the multi-label case, this work proposes a general approach to solve the long-tail data imbalance issue for multi-label classification. PLM is versatile: it can be applied to most objective functions and it can be used alongside other strategies for class imbalance. Our method achieves strong performance when compared to existing methods on both multi-label (MultiMNIST and MSCOCO) and single-label (imbalanced CIFAR-10 and CIFAR-100) image classification datasets.



### GOO: A Dataset for Gaze Object Prediction in Retail Environments
- **Arxiv ID**: http://arxiv.org/abs/2105.10793v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10793v2)
- **Published**: 2021-05-22 18:55:35+00:00
- **Updated**: 2021-06-22 03:00:55+00:00
- **Authors**: Henri Tomas, Marcus Reyes, Raimarc Dionido, Mark Ty, Jonric Mirando, Joel Casimiro, Rowel Atienza, Richard Guinto
- **Comment**: CVPR 20201 Workshop on Gaze Estimation and Prediction in the Wild
  (GAZE 2021)
- **Journal**: None
- **Summary**: One of the most fundamental and information-laden actions humans do is to look at objects. However, a survey of current works reveals that existing gaze-related datasets annotate only the pixel being looked at, and not the boundaries of a specific object of interest. This lack of object annotation presents an opportunity for further advancing gaze estimation research. To this end, we present a challenging new task called gaze object prediction, where the goal is to predict a bounding box for a person's gazed-at object. To train and evaluate gaze networks on this task, we present the Gaze On Objects (GOO) dataset. GOO is composed of a large set of synthetic images (GOO Synth) supplemented by a smaller subset of real images (GOO-Real) of people looking at objects in a retail environment. Our work establishes extensive baselines on GOO by re-implementing and evaluating selected state-of-the art models on the task of gaze following and domain adaptation. Code is available on github.



### Texture synthesis via projection onto multiscale, multilayer statistics
- **Arxiv ID**: http://arxiv.org/abs/2105.10825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10825v1)
- **Published**: 2021-05-22 23:32:34+00:00
- **Updated**: 2021-05-22 23:32:34+00:00
- **Authors**: Jieqian He, Matthew Hirn
- **Comment**: 14 pages, 16 figures
- **Journal**: None
- **Summary**: We provide a new model for texture synthesis based on a multiscale, multilayer feature extractor. Within the model, textures are represented by a set of statistics computed from ReLU wavelet coefficients at different layers, scales and orientations. A new image is synthesized by matching the target statistics via an iterative projection algorithm. We explain the necessity of the different types of pre-defined wavelet filters used in our model and the advantages of multilayer structures for image synthesis. We demonstrate the power of our model by generating samples of high quality textures and providing insights into deep representations for texture images.



### Orthogonal Ensemble Networks for Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.10827v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10827v1)
- **Published**: 2021-05-22 23:44:55+00:00
- **Updated**: 2021-05-22 23:44:55+00:00
- **Authors**: Agostina J. Larrazabal, César Martínez, Jose Dolz, Enzo Ferrante
- **Comment**: Accepted for publication at MICCAI 2021
- **Journal**: None
- **Summary**: Despite the astonishing performance of deep-learning based approaches for visual tasks such as semantic segmentation, they are known to produce miscalibrated predictions, which could be harmful for critical decision-making processes. Ensemble learning has shown to not only boost the performance of individual models but also reduce their miscalibration by averaging independent predictions. In this scenario, model diversity has become a key factor, which facilitates individual models converging to different functional solutions. In this work, we introduce Orthogonal Ensemble Networks (OEN), a novel framework to explicitly enforce model diversity by means of orthogonal constraints. The proposed method is based on the hypothesis that inducing orthogonality among the constituents of the ensemble will increase the overall model diversity. We resort to a new pairwise orthogonality constraint which can be used to regularize a sequential ensemble training process, resulting on improved predictive performance and better calibrated model outputs. We benchmark the proposed framework in two challenging brain lesion segmentation tasks --brain tumor and white matter hyper-intensity segmentation in MR images. The experimental results show that our approach produces more robust and well-calibrated ensemble models and can deal with challenging tasks in the context of biomedical image segmentation.



