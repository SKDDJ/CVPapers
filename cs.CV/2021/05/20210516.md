# Arxiv Papers in cs.CV on 2021-05-16
### Unsupervised Super-Resolution of Satellite Imagery for High Fidelity Material Label Transfer
- **Arxiv ID**: http://arxiv.org/abs/2105.07322v1
- **DOI**: 10.1109/IGARSS.2019.8900639
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.07322v1)
- **Published**: 2021-05-16 00:57:43+00:00
- **Updated**: 2021-05-16 00:57:43+00:00
- **Authors**: Arthita Ghosh, Max Ehrlich, Larry Davis, Rama Chellappa
- **Comment**: Published in the proceedings of the 2019 IEEE International
  Geoscience and Remote Sensing Symposium
- **Journal**: IGARSS (2019), 5144-5147
- **Summary**: Urban material recognition in remote sensing imagery is a highly relevant, yet extremely challenging problem due to the difficulty of obtaining human annotations, especially on low resolution satellite images. To this end, we propose an unsupervised domain adaptation based approach using adversarial learning. We aim to harvest information from smaller quantities of high resolution data (source domain) and utilize the same to super-resolve low resolution imagery (target domain). This can potentially aid in semantic as well as material label transfer from a richly annotated source to a target domain.



### Is In-Domain Data Really Needed? A Pilot Study on Cross-Domain Calibration for Network Quantization
- **Arxiv ID**: http://arxiv.org/abs/2105.07331v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.07331v1)
- **Published**: 2021-05-16 02:07:44+00:00
- **Updated**: 2021-05-16 02:07:44+00:00
- **Authors**: Haichao Yu, Linjie Yang, Humphrey Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Post-training quantization methods use a set of calibration data to compute quantization ranges for network parameters and activations. The calibration data usually comes from the training dataset which could be inaccessible due to sensitivity of the data. In this work, we want to study such a problem: can we use out-of-domain data to calibrate the trained networks without knowledge of the original dataset? Specifically, we go beyond the domain of natural images to include drastically different domains such as X-ray images, satellite images and ultrasound images. We find cross-domain calibration leads to surprisingly stable performance of quantized models on 10 tasks in different image domains with 13 different calibration datasets. We also find that the performance of quantized models is correlated with the similarity of the Gram matrices between the source and calibration domains, which can be used as a criterion to choose calibration set for better performance. We believe our research opens the door to borrow cross-domain knowledge for network quantization and compression.



### Real-time Detection of Practical Universal Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2105.07334v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.07334v2)
- **Published**: 2021-05-16 03:01:29+00:00
- **Updated**: 2021-05-22 23:33:20+00:00
- **Authors**: Kenneth T. Co, Luis Muñoz-González, Leslie Kanthan, Emil C. Lupu
- **Comment**: None
- **Journal**: None
- **Summary**: Universal Adversarial Perturbations (UAPs) are a prominent class of adversarial examples that exploit the systemic vulnerabilities and enable physically realizable and robust attacks against Deep Neural Networks (DNNs). UAPs generalize across many different inputs; this leads to realistic and effective attacks that can be applied at scale. In this paper we propose HyperNeuron, an efficient and scalable algorithm that allows for the real-time detection of UAPs by identifying suspicious neuron hyper-activations. Our results show the effectiveness of HyperNeuron on multiple tasks (image classification, object detection), against a wide variety of universal attacks, and in realistic scenarios, like perceptual ad-blocking and adversarial patches. HyperNeuron is able to simultaneously detect both adversarial mask and patch UAPs with comparable or better performance than existing UAP defenses whilst introducing a significantly reduced latency of only 0.86 milliseconds per image. This suggests that many realistic and practical universal attacks can be reliably mitigated in real-time, which shows promise for the robust deployment of machine learning systems.



### Neighbourhood-guided Feature Reconstruction for Occluded Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2105.07345v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.07345v1)
- **Published**: 2021-05-16 03:53:55+00:00
- **Updated**: 2021-05-16 03:53:55+00:00
- **Authors**: Shijie Yu, Dapeng Chen, Rui Zhao, Haobin Chen, Yu Qiao
- **Comment**: None
- **Journal**: None
- **Summary**: Person images captured by surveillance cameras are often occluded by various obstacles, which lead to defective feature representation and harm person re-identification (Re-ID) performance. To tackle this challenge, we propose to reconstruct the feature representation of occluded parts by fully exploiting the information of its neighborhood in a gallery image set. Specifically, we first introduce a visible part-based feature by body mask for each person image. Then we identify its neighboring samples using the visible features and reconstruct the representation of the full body by an outlier-removable graph neural network with all the neighboring samples as input. Extensive experiments show that the proposed approach obtains significant improvements. In the large-scale Occluded-DukeMTMC benchmark, our approach achieves 64.2% mAP and 67.6% rank-1 accuracy which outperforms the state-of-the-art approaches by large margins, i.e.,20.4% and 12.5%, respectively, indicating the effectiveness of our method on occluded Re-ID problem.



### ExSinGAN: Learning an Explainable Generative Model from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2105.07350v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.07350v2)
- **Published**: 2021-05-16 04:38:46+00:00
- **Updated**: 2022-01-06 04:11:45+00:00
- **Authors**: ZiCheng Zhang, CongYing Han, TianDe Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Generating images from a single sample, as a newly developing branch of image synthesis, has attracted extensive attention. In this paper, we formulate this problem as sampling from the conditional distribution of a single image, and propose a hierarchical framework that simplifies the learning of the intricate conditional distributions through the successive learning of the distributions about structure, semantics and texture, making the process of learning and generation comprehensible. On this basis, we design ExSinGAN composed of three cascaded GANs for learning an explainable generative model from a given image, where the cascaded GANs model the distributions about structure, semantics and texture successively. ExSinGAN is learned not only from the internal patches of the given image as the previous works did, but also from the external prior obtained by the GAN inversion technique. Benefiting from the appropriate combination of internal and external information, ExSinGAN has a more powerful capability of generation and competitive generalization ability for the image manipulation tasks compared with prior works.



### BDANet: Multiscale Convolutional Neural Network with Cross-directional Attention for Building Damage Assessment from Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2105.07364v1
- **DOI**: 10.1109/TGRS.2021.3080580
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.07364v1)
- **Published**: 2021-05-16 06:13:28+00:00
- **Updated**: 2021-05-16 06:13:28+00:00
- **Authors**: Yu Shen, Sijie Zhu, Taojiannan Yang, Chen Chen, Delu Pan, Jianyu Chen, Liang Xiao, Qian Du
- **Comment**: arXiv admin note: text overlap with arXiv:2010.14014
- **Journal**: None
- **Summary**: Fast and effective responses are required when a natural disaster (e.g., earthquake, hurricane, etc.) strikes. Building damage assessment from satellite imagery is critical before relief effort is deployed. With a pair of pre- and post-disaster satellite images, building damage assessment aims at predicting the extent of damage to buildings. With the powerful ability of feature representation, deep neural networks have been successfully applied to building damage assessment. Most existing works simply concatenate pre- and post-disaster images as input of a deep neural network without considering their correlations. In this paper, we propose a novel two-stage convolutional neural network for Building Damage Assessment, called BDANet. In the first stage, a U-Net is used to extract the locations of buildings. Then the network weights from the first stage are shared in the second stage for building damage assessment. In the second stage, a two-branch multi-scale U-Net is employed as backbone, where pre- and post-disaster images are fed into the network separately. A cross-directional attention module is proposed to explore the correlations between pre- and post-disaster images. Moreover, CutMix data augmentation is exploited to tackle the challenge of difficult classes. The proposed method achieves state-of-the-art performance on a large-scale dataset -- xBD. The code is available at https://github.com/ShaneShen/BDANet-Building-Damage-Assessment.



### Expressive Explanations of DNNs by Combining Concept Analysis with ILP
- **Arxiv ID**: http://arxiv.org/abs/2105.07371v1
- **DOI**: 10.1007/978-3-030-58285-2_11
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.07371v1)
- **Published**: 2021-05-16 07:00:27+00:00
- **Updated**: 2021-05-16 07:00:27+00:00
- **Authors**: Johannes Rabold, Gesina Schwalbe, Ute Schmid
- **Comment**: 14 pages, 4 figures; Camera-ready submission to KI2020; The final
  authenticated publication is available online at
  https://doi.org/10.1007/978-3-030-58285-2_11; code available at
  https://github.com/mc-lovin-mlem/concept-embeddings-and-ilp/tree/ki2020
- **Journal**: None
- **Summary**: Explainable AI has emerged to be a key component for black-box machine learning approaches in domains with a high demand for reliability or transparency. Examples are medical assistant systems, and applications concerned with the General Data Protection Regulation of the European Union, which features transparency as a cornerstone. Such demands require the ability to audit the rationale behind a classifier's decision. While visualizations are the de facto standard of explanations, they come short in terms of expressiveness in many ways: They cannot distinguish between different attribute manifestations of visual features (e.g. eye open vs. closed), and they cannot accurately describe the influence of absence of, and relations between features. An alternative would be more expressive symbolic surrogate models. However, these require symbolic inputs, which are not readily available in most computer vision tasks. In this paper we investigate how to overcome this: We use inherent features learned by the network to build a global, expressive, verbal explanation of the rationale of a feed-forward convolutional deep neural network (DNN). The semantics of the features are mined by a concept analysis approach trained on a set of human understandable visual concepts. The explanation is found by an Inductive Logic Programming (ILP) method and presented as first-order rules. We show that our explanation is faithful to the original black-box model.   The code for our experiments is available at https://github.com/mc-lovin-mlem/concept-embeddings-and-ilp/tree/ki2020.



### Semi-supervised Contrastive Learning with Similarity Co-calibration
- **Arxiv ID**: http://arxiv.org/abs/2105.07387v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.07387v1)
- **Published**: 2021-05-16 09:13:56+00:00
- **Updated**: 2021-05-16 09:13:56+00:00
- **Authors**: Yuhang Zhang, Xiaopeng Zhang, Robert. C. Qiu, Jie Li, Haohang Xu, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised learning acts as an effective way to leverage massive unlabeled data. In this paper, we propose a novel training strategy, termed as Semi-supervised Contrastive Learning (SsCL), which combines the well-known contrastive loss in self-supervised learning with the cross entropy loss in semi-supervised learning, and jointly optimizes the two objectives in an end-to-end way. The highlight is that different from self-training based semi-supervised learning that conducts prediction and retraining over the same model weights, SsCL interchanges the predictions over the unlabeled data between the two branches, and thus formulates a co-calibration procedure, which we find is beneficial for better prediction and avoid being trapped in local minimum. Towards this goal, the contrastive loss branch models pairwise similarities among samples, using the nearest neighborhood generated from the cross entropy branch, and in turn calibrates the prediction distribution of the cross entropy branch with the contrastive similarity. We show that SsCL produces more discriminative representation and is beneficial to few shot learning. Notably, on ImageNet with ResNet50 as the backbone, SsCL achieves 60.2% and 72.1% top-1 accuracy with 1% and 10% labeled samples, respectively, which significantly outperforms the baseline, and is better than previous semi-supervised and self-supervised methods.



### Semi-Supervised Classification and Segmentation on High Resolution Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2105.08655v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.08655v2)
- **Published**: 2021-05-16 09:30:03+00:00
- **Updated**: 2021-06-24 17:45:20+00:00
- **Authors**: Sahil Khose, Abhiraj Tiwari, Ankita Ghosh
- **Comment**: 5 pages, 2 tables, 1 figure
- **Journal**: None
- **Summary**: FloodNet is a high-resolution image dataset acquired by a small UAV platform, DJI Mavic Pro quadcopters, after Hurricane Harvey. The dataset presents a unique challenge of advancing the damage assessment process for post-disaster scenarios using unlabeled and limited labeled dataset. We propose a solution to address their classification and semantic segmentation challenge. We approach this problem by generating pseudo labels for both classification and segmentation during training and slowly incrementing the amount by which the pseudo label loss affects the final loss. Using this semi-supervised method of training helped us improve our baseline supervised loss by a huge margin for classification, allowing the model to generalize and perform better on the validation and test splits of the dataset. In this paper, we compare and contrast the various methods and models for image classification and semantic segmentation on the FloodNet dataset.



### Survey of Visual-Semantic Embedding Methods for Zero-Shot Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2105.07391v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.07391v2)
- **Published**: 2021-05-16 09:43:25+00:00
- **Updated**: 2021-09-28 08:51:47+00:00
- **Authors**: Kazuya Ueki
- **Comment**: Accepted by 20th IEEE International Conference on Machine Learning
  and Applications (ICMLA2021)
- **Journal**: None
- **Summary**: Visual-semantic embedding is an interesting research topic because it is useful for various tasks, such as visual question answering (VQA), image-text retrieval, image captioning, and scene graph generation. In this paper, we focus on zero-shot image retrieval using sentences as queries and present a survey of the technological trends in this area. First, we provide a comprehensive overview of the history of the technology, starting with a discussion of the early studies of image-to-text matching and how the technology has evolved over time. In addition, a description of the datasets commonly used in experiments and a comparison of the evaluation results of each method are presented. We also introduce the implementation available on github for use in confirming the accuracy of experiments and for further improvement. We hope that this survey paper will encourage researchers to further develop their research on bridging images and languages.



### Unsupervised Multi-Modality Registration Network based on Spatially Encoded Gradient Information
- **Arxiv ID**: http://arxiv.org/abs/2105.07392v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.07392v3)
- **Published**: 2021-05-16 09:47:42+00:00
- **Updated**: 2021-08-29 09:58:38+00:00
- **Authors**: Wangbin Ding, Lei Li, Xiahai Zhuang, Liqin Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-modality medical images can provide relevant or complementary information for a target (organ, tumor or tissue). Registering multi-modality images to a common space can fuse these comprehensive information, and bring convenience for clinical application. Recently, neural networks have been widely investigated to boost registration methods. However, it is still challenging to develop a multi-modality registration network due to the lack of robust criteria for network training. In this work, we propose a multi-modality registration network (MMRegNet), which can perform registration between multi-modality images. Meanwhile, we present spatially encoded gradient information to train MMRegNet in an unsupervised manner. The proposed network was evaluated on MM-WHS 2017. Results show that MMRegNet can achieve promising performance for left ventricle cardiac registration tasks. Meanwhile, to demonstrate the versatility of MMRegNet, we further evaluate the method with a liver dataset from CHAOS 2019. Source code will be released publicly\footnote{https://github.com/NanYoMy/mmregnet} once the manuscript is accepted.



### Is the aspect ratio of cells important in deep learning? A robust comparison of deep learning methods for multi-scale cytopathology cell image classification: from convolutional neural networks to visual transformers
- **Arxiv ID**: http://arxiv.org/abs/2105.07402v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.07402v4)
- **Published**: 2021-05-16 10:37:36+00:00
- **Updated**: 2021-11-08 14:36:24+00:00
- **Authors**: Wanli Liu, Chen Li, Md Mamunur Rahamana, Tao Jiang, Hongzan Sun, Xiangchen Wu, Weiming Hu, Haoyuan Chen, Changhao Sun, Yudong Yao, Marcin Grzegorzek
- **Comment**: None
- **Journal**: None
- **Summary**: Cervical cancer is a very common and fatal type of cancer in women. Cytopathology images are often used to screen for this cancer. Given that there is a possibility that many errors can occur during manual screening, a computer-aided diagnosis system based on deep learning has been developed. Deep learning methods require a fixed dimension of input images, but the dimensions of clinical medical images are inconsistent. The aspect ratios of the images suffer while resizing them directly. Clinically, the aspect ratios of cells inside cytopathological images provide important information for doctors to diagnose cancer. Therefore, it is difficult to resize directly. However, many existing studies have resized the images directly and have obtained highly robust classification results. To determine a reasonable interpretation, we have conducted a series of comparative experiments. First, the raw data of the SIPaKMeD dataset are pre-processed to obtain standard and scaled datasets. Then, the datasets are resized to 224 x 224 pixels. Finally, 22 deep learning models are used to classify the standard and scaled datasets. The results of the study indicate that deep learning models are robust to changes in the aspect ratio of cells in cervical cytopathological images. This conclusion is also validated via the Herlev dataset.



### MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions
- **Arxiv ID**: http://arxiv.org/abs/2105.07404v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.07404v2)
- **Published**: 2021-05-16 10:40:30+00:00
- **Updated**: 2021-08-18 05:27:50+00:00
- **Authors**: Yixuan Li, Lei Chen, Runyu He, Zhenzhi Wang, Gangshan Wu, Limin Wang
- **Comment**: ICCV 2021 camera ready version. One track of DeeperAction
  Workshop@ICCV2021. HomePage: https://deeperaction.github.io/multisports/
- **Journal**: None
- **Summary**: Spatio-temporal action detection is an important and challenging problem in video understanding. The existing action detection benchmarks are limited in aspects of small numbers of instances in a trimmed video or low-level atomic actions. This paper aims to present a new multi-person dataset of spatio-temporal localized sports actions, coined as MultiSports. We first analyze the important ingredients of constructing a realistic and challenging dataset for spatio-temporal action detection by proposing three criteria: (1) multi-person scenes and motion dependent identification, (2) with well-defined boundaries, (3) relatively fine-grained classes of high complexity. Based on these guide-lines, we build the dataset of MultiSports v1.0 by selecting 4 sports classes, collecting 3200 video clips, and annotating 37701 action instances with 902k bounding boxes. Our datasets are characterized with important properties of high diversity, dense annotation, and high quality. Our Multi-Sports, with its realistic setting and detailed annotations, exposes the intrinsic challenges of spatio-temporal action detection. To benchmark this, we adapt several baseline methods to our dataset and give an in-depth analysis on the action detection results in our dataset. We hope our MultiSports can serve as a standard benchmark for spatio-temporal action detection in the future. Our dataset website is at https://deeperaction.github.io/multisports/.



### MSRF-Net: A Multi-Scale Residual Fusion Network for Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.07451v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.07451v2)
- **Published**: 2021-05-16 15:19:56+00:00
- **Updated**: 2022-01-31 03:56:04+00:00
- **Authors**: Abhishek Srivastava, Debesh Jha, Sukalpa Chanda, Umapada Pal, Håvard D. Johansen, Dag Johansen, Michael A. Riegler, Sharib Ali, Pål Halvorsen
- **Comment**: None
- **Journal**: IEEE Journal of Biomedical and Health Informatics, 2022
- **Summary**: Methods based on convolutional neural networks have improved the performance of biomedical image segmentation. However, most of these methods cannot efficiently segment objects of variable sizes and train on small and biased datasets, which are common for biomedical use cases. While methods exist that incorporate multi-scale fusion approaches to address the challenges arising with variable sizes, they usually use complex models that are more suitable for general semantic segmentation problems. In this paper, we propose a novel architecture called Multi-Scale Residual Fusion Network (MSRF-Net), which is specially designed for medical image segmentation. The proposed MSRF-Net is able to exchange multi-scale features of varying receptive fields using a Dual-Scale Dense Fusion (DSDF) block. Our DSDF block can exchange information rigorously across two different resolution scales, and our MSRF sub-network uses multiple DSDF blocks in sequence to perform multi-scale fusion. This allows the preservation of resolution, improved information flow and propagation of both high- and low-level features to obtain accurate segmentation maps. The proposed MSRF-Net allows to capture object variabilities and provides improved results on different biomedical datasets. Extensive experiments on MSRF-Net demonstrate that the proposed method outperforms the cutting-edge medical image segmentation methods on four publicly available datasets. We achieve the dice coefficient of 0.9217, 0.9420, and 0.9224, 0.8824 on Kvasir-SEG, CVC-ClinicDB, 2018 Data Science Bowl dataset, and ISIC-2018 skin lesion segmentation challenge dataset respectively. We further conducted generalizability tests and achieved a dice coefficient of 0.7921 and 0.7575 on CVC-ClinicDB and Kvasir-SEG, respectively.



### Sparse to Dense Dynamic 3D Facial Expression Generation
- **Arxiv ID**: http://arxiv.org/abs/2105.07463v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.07463v2)
- **Published**: 2021-05-16 15:52:29+00:00
- **Updated**: 2022-03-03 08:34:05+00:00
- **Authors**: Naima Otberdout, Claudio Ferrari, Mohamed Daoudi, Stefano Berretti, Alberto Del Bimbo
- **Comment**: paper accepted at CVPR 2022
- **Journal**: None
- **Summary**: In this paper, we propose a solution to the task of generating dynamic 3D facial expressions from a neutral 3D face and an expression label. This involves solving two sub-problems: (i)modeling the temporal dynamics of expressions, and (ii) deforming the neutral mesh to obtain the expressive counterpart. We represent the temporal evolution of expressions using the motion of a sparse set of 3D landmarks that we learn to generate by training a manifold-valued GAN (Motion3DGAN). To better encode the expression-induced deformation and disentangle it from the identity information, the generated motion is represented as per-frame displacement from a neutral configuration. To generate the expressive meshes, we train a Sparse2Dense mesh Decoder (S2D-Dec) that maps the landmark displacements to a dense, per-vertex displacement. This allows us to learn how the motion of a sparse set of landmarks influences the deformation of the overall face surface, independently from the identity. Experimental results on the CoMA and D3DFACS datasets show that our solution brings significant improvements with respect to previous solutions in terms of both dynamic expression generation and mesh reconstruction, while retaining good generalization to unseen data. The code and the pretrained model will be made publicly available.



### Focus U-Net: A novel dual attention-gated CNN for polyp segmentation during colonoscopy
- **Arxiv ID**: http://arxiv.org/abs/2105.07467v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2105.07467v2)
- **Published**: 2021-05-16 16:10:32+00:00
- **Updated**: 2021-06-22 11:03:09+00:00
- **Authors**: Michael Yeung, Evis Sala, Carola-Bibiane Schönlieb, Leonardo Rundo
- **Comment**: None
- **Journal**: None
- **Summary**: Background: Colonoscopy remains the gold-standard screening for colorectal cancer. However, significant miss rates for polyps have been reported, particularly when there are multiple small adenomas. This presents an opportunity to leverage computer-aided systems to support clinicians and reduce the number of polyps missed.   Method: In this work we introduce the Focus U-Net, a novel dual attention-gated deep neural network, which combines efficient spatial and channel-based attention into a single Focus Gate module to encourage selective learning of polyp features. The Focus U-Net further incorporates short-range skip connections and deep supervision. Furthermore, we introduce the Hybrid Focal loss, a new compound loss function based on the Focal loss and Focal Tversky loss, to handle class-imbalanced image segmentation. For our experiments, we selected five public datasets containing images of polyps obtained during optical colonoscopy: CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, ETIS-Larib PolypDB and EndoScene test set. To evaluate model performance, we use the Dice similarity coefficient (DSC) and Intersection over Union (IoU) metrics.   Results: Our model achieves state-of-the-art results for both CVC-ClinicDB and Kvasir-SEG, with a mean DSC of 0.941 and 0.910, respectively. When evaluated on a combination of five public polyp datasets, our model similarly achieves state-of-the-art results with a mean DSC of 0.878 and mean IoU of 0.809, a 14% and 15% improvement over the previous state-of-the-art results of 0.768 and 0.702, respectively.   Conclusions: This study shows the potential for deep learning to provide fast and accurate polyp segmentation results for use during colonoscopy. The Focus U-Net may be adapted for future use in newer non-invasive screening and more broadly to other biomedical image segmentation tasks involving class imbalance and requiring efficiency.



### TSDF++: A Multi-Object Formulation for Dynamic Object Tracking and Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2105.07468v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.07468v1)
- **Published**: 2021-05-16 16:15:05+00:00
- **Updated**: 2021-05-16 16:15:05+00:00
- **Authors**: Margarita Grinvald, Federico Tombari, Roland Siegwart, Juan Nieto
- **Comment**: 7 pages, 3 figures. To be published in the 2021 IEEE International
  Conference on Robotics and Automation (ICRA). Code is available at
  https://github.com/ethz-asl/tsdf-plusplus and the accompanying video material
  can be found at https://youtu.be/dSJmoeVasI0
- **Journal**: None
- **Summary**: The ability to simultaneously track and reconstruct multiple objects moving in the scene is of the utmost importance for robotic tasks such as autonomous navigation and interaction. Virtually all of the previous attempts to map multiple dynamic objects have evolved to store individual objects in separate reconstruction volumes and track the relative pose between them. While simple and intuitive, such formulation does not scale well with respect to the number of objects in the scene and introduces the need for an explicit occlusion handling strategy. In contrast, we propose a map representation that allows maintaining a single volume for the entire scene and all the objects therein. To this end, we introduce a novel multi-object TSDF formulation that can encode multiple object surfaces at any given location in the map. In a multiple dynamic object tracking and reconstruction scenario, our representation allows maintaining accurate reconstruction of surfaces even while they become temporarily occluded by other objects moving in their proximity. We evaluate the proposed TSDF++ formulation on a public synthetic dataset and demonstrate its ability to preserve reconstructions of occluded surfaces when compared to the standard TSDF map representation.



### Uncertainty in Minimum Cost Multicuts for Image and Motion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.07469v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.07469v1)
- **Published**: 2021-05-16 16:22:38+00:00
- **Updated**: 2021-05-16 16:22:38+00:00
- **Authors**: Amirhossein Kardoost, Margret Keuper
- **Comment**: Accepted in the 37th Conference on Uncertainty in Artificial
  Intelligence (UAI 2021)
- **Journal**: None
- **Summary**: The minimum cost lifted multicut approach has proven practically good performance in a wide range of applications such as image decomposition, mesh segmentation, multiple object tracking, and motion segmentation. It addresses such problems in a graph-based model, where real-valued costs are assigned to the edges between entities such that the minimum cut decomposes the graph into an optimal number of segments. Driven by a probabilistic formulation of minimum cost multicuts, we provide a measure for the uncertainties of the decisions made during the optimization. We argue that access to such uncertainties is crucial for many practical applications and conduct an evaluation by means of sparsifications on three different, widely used datasets in the context of image decomposition (BSDS-500) and motion segmentation (DAVIS2016 and FBMS59) in terms of variation of information (VI) and Rand index (RI).



### Fast-GANFIT: Generative Adversarial Network for High Fidelity 3D Face Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2105.07474v1
- **DOI**: 10.1109/TPAMI.2021.3084524
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.07474v1)
- **Published**: 2021-05-16 16:35:44+00:00
- **Updated**: 2021-05-16 16:35:44+00:00
- **Authors**: Baris Gecer, Stylianos Ploumpis, Irene Kotsia, Stefanos Zafeiriou
- **Comment**: TPAMI camera ready (submitted: 05-May-2020); Check project page:
  https://github.com/barisgecer/GANFit. arXiv admin note: substantial text
  overlap with arXiv:1902.05978
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (2021)
- **Summary**: A lot of work has been done towards reconstructing the 3D facial structure from single images by capitalizing on the power of Deep Convolutional Neural Networks (DCNNs). In the recent works, the texture features either correspond to components of a linear texture space or are learned by auto-encoders directly from in-the-wild images. In all cases, the quality of the facial texture reconstruction is still not capable of modeling facial texture with high-frequency details. In this paper, we take a radically different approach and harness the power of Generative Adversarial Networks (GANs) and DCNNs in order to reconstruct the facial texture and shape from single images. That is, we utilize GANs to train a very powerful facial texture prior \edit{from a large-scale 3D texture dataset}. Then, we revisit the original 3D Morphable Models (3DMMs) fitting making use of non-linear optimization to find the optimal latent parameters that best reconstruct the test image but under a new perspective. In order to be robust towards initialisation and expedite the fitting process, we propose a novel self-supervised regression based approach. We demonstrate excellent results in photorealistic and identity preserving 3D face reconstructions and achieve for the first time, to the best of our knowledge, facial texture reconstruction with high-frequency details.



### Leveraging Semantic Scene Characteristics and Multi-Stream Convolutional Architectures in a Contextual Approach for Video-Based Visual Emotion Recognition in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2105.07484v1
- **DOI**: 10.1109/FG52635.2021.9666957
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.07484v1)
- **Published**: 2021-05-16 17:31:59+00:00
- **Updated**: 2021-05-16 17:31:59+00:00
- **Authors**: Ioannis Pikoulis, Panagiotis P. Filntisis, Petros Maragos
- **Comment**: 9 pages, 4 figures, 5 tables, submitted to the 16th IEEE
  International Conference on Automatic Face and Gesture Recognition
- **Journal**: 2021 16th IEEE International Conference on Automatic Face and
  Gesture Recognition (FG 2021)
- **Summary**: In this work we tackle the task of video-based visual emotion recognition in the wild. Standard methodologies that rely solely on the extraction of bodily and facial features often fall short of accurate emotion prediction in cases where the aforementioned sources of affective information are inaccessible due to head/body orientation, low resolution and poor illumination. We aspire to alleviate this problem by leveraging visual context in the form of scene characteristics and attributes, as part of a broader emotion recognition framework. Temporal Segment Networks (TSN) constitute the backbone of our proposed model. Apart from the RGB input modality, we make use of dense Optical Flow, following an intuitive multi-stream approach for a more effective encoding of motion. Furthermore, we shift our attention towards skeleton-based learning and leverage action-centric data as means of pre-training a Spatial-Temporal Graph Convolutional Network (ST-GCN) for the task of emotion recognition. Our extensive experiments on the challenging Body Language Dataset (BoLD) verify the superiority of our methods over existing approaches, while by properly incorporating all of the aforementioned modules in a network ensemble, we manage to surpass the previous best published recognition scores, by a large margin.



### COVID-19 Detection in Computed Tomography Images with 2D and 3D Approaches
- **Arxiv ID**: http://arxiv.org/abs/2105.08506v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.08506v2)
- **Published**: 2021-05-16 20:12:02+00:00
- **Updated**: 2021-05-20 08:47:45+00:00
- **Authors**: Sara Atito Ali Ahmed, Mehmet Can Yavuz, Mehmet Umut Sen, Fatih Gulsen, Onur Tutar, Bora Korkmazer, Cesur Samanci, Sabri Sirolu, Rauf Hamid, Ali Ergun Eryurekli, Toghrul Mammadov, Berrin Yanikoglu
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting COVID-19 in computed tomography (CT) or radiography images has been proposed as a supplement to the definitive RT-PCR test. We present a deep learning ensemble for detecting COVID-19 infection, combining slice-based (2D) and volume-based (3D) approaches. The 2D system detects the infection on each CT slice independently, combining them to obtain the patient-level decision via different methods (averaging and long-short term memory networks). The 3D system takes the whole CT volume to arrive to the patient-level decision in one step. A new high resolution chest CT scan dataset, called the IST-C dataset, is also collected in this work. The proposed ensemble, called IST-CovNet, obtains 90.80% accuracy and 0.95 AUC score overall on the IST-C dataset in detecting COVID-19 among normal controls and other types of lung pathologies; and 93.69% accuracy and 0.99 AUC score on the publicly available MosMed dataset that consists of COVID-19 scans and normal controls only. The system is deployed at Istanbul University Cerrahpasa School of Medicine.



### Substitutional Neural Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2105.07512v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.07512v1)
- **Published**: 2021-05-16 20:53:31+00:00
- **Updated**: 2021-05-16 20:53:31+00:00
- **Authors**: Xiao Wang, Wei Jiang, Wei Wang, Shan Liu, Brian Kulis, Peter Chin
- **Comment**: None
- **Journal**: None
- **Summary**: We describe Substitutional Neural Image Compression (SNIC), a general approach for enhancing any neural image compression model, that requires no data or additional tuning of the trained model. It boosts compression performance toward a flexible distortion metric and enables bit-rate control using a single model instance. The key idea is to replace the image to be compressed with a substitutional one that outperforms the original one in a desired way. Finding such a substitute is inherently difficult for conventional codecs, yet surprisingly favorable for neural compression models thanks to their fully differentiable structures. With gradients of a particular loss backpropogated to the input, a desired substitute can be efficiently crafted iteratively. We demonstrate the effectiveness of SNIC, when combined with various neural compression models and target metrics, in improving compression quality and performing bit-rate control measured by rate-distortion curves. Empirical results of control precision and generation speed are also discussed.



### Private Facial Diagnosis as an Edge Service for Parkinson's DBS Treatment Valuation
- **Arxiv ID**: http://arxiv.org/abs/2105.07533v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.07533v1)
- **Published**: 2021-05-16 22:24:37+00:00
- **Updated**: 2021-05-16 22:24:37+00:00
- **Authors**: Richard Jiang, Paul Chazot, Danny Crookes, Ahmed Bouridane, M Emre Celebi
- **Comment**: Under review
- **Journal**: None
- **Summary**: Facial phenotyping has recently been successfully exploited for medical diagnosis as a novel way to diagnose a range of diseases, where facial biometrics has been revealed to have rich links to underlying genetic or medical causes. In this paper, taking Parkinson's Diseases (PD) as a case study, we proposed an Artificial-Intelligence-of-Things (AIoT) edge-oriented privacy-preserving facial diagnosis framework to analyze the treatment of Deep Brain Stimulation (DBS) on PD patients. In the proposed framework, a new edge-based information theoretically secure framework is proposed to implement private deep facial diagnosis as a service over a privacy-preserving AIoT-oriented multi-party communication scheme, where partial homomorphic encryption (PHE) is leveraged to enable privacy-preserving deep facial diagnosis directly on encrypted facial patterns. In our experiments with a collected facial dataset from PD patients, for the first time, we demonstrated that facial patterns could be used to valuate the improvement of PD patients undergoing DBS treatment. We further implemented a privacy-preserving deep facial diagnosis framework that can achieve the same accuracy as the non-encrypted one, showing the potential of our privacy-preserving facial diagnosis as an trustworthy edge service for grading the severity of PD in patients.



### Algorithmic Principles of Camera-based Respiratory Motion Extraction
- **Arxiv ID**: http://arxiv.org/abs/2105.07537v1
- **DOI**: 10.1088/1361-6579/ac5b49
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.07537v1)
- **Published**: 2021-05-16 22:45:41+00:00
- **Updated**: 2021-05-16 22:45:41+00:00
- **Authors**: Wenjin Wang, Albertus C. den Brinker
- **Comment**: Camera based contactless health monitoring
- **Journal**: None
- **Summary**: Measuring the respiratory signal from a video based on body motion has been proposed and recently matured in products for video health monitoring. The core algorithm for this measurement is the estimation of tiny chest/abdominal motions induced by respiration, and the fundamental challenge is motion sensitivity. Though prior arts reported on the validation with real human subjects, there is no thorough/rigorous benchmark to quantify the sensitivities and boundary conditions of motion-based core respiratory algorithms that measure sub-pixel displacement between video frames. In this paper, we designed a setup with a fully-controllable physical phantom to investigate the essence of core algorithms, together with a mathematical model incorporating two motion estimation strategies and three spatial representations, leading to six algorithmic combinations for respiratory signal extraction. Their promises and limitations are discussed and clarified via the phantom benchmark. The insights gained in this paper are intended to improve the understanding and applications of camera-based respiration measurement in health monitoring.



### Deep learning for detecting pulmonary tuberculosis via chest radiography: an international study across 10 countries
- **Arxiv ID**: http://arxiv.org/abs/2105.07540v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.07540v2)
- **Published**: 2021-05-16 22:56:06+00:00
- **Updated**: 2021-10-29 22:41:00+00:00
- **Authors**: Sahar Kazemzadeh, Jin Yu, Shahar Jamshy, Rory Pilgrim, Zaid Nabulsi, Christina Chen, Neeral Beladia, Charles Lau, Scott Mayer McKinney, Thad Hughes, Atilla Kiraly, Sreenivasa Raju Kalidindi, Monde Muyoyeta, Jameson Malemela, Ting Shih, Greg S. Corrado, Lily Peng, Katherine Chou, Po-Hsuan Cameron Chen, Yun Liu, Krish Eswaran, Daniel Tse, Shravya Shetty, Shruthi Prabhakara
- **Comment**: None
- **Journal**: None
- **Summary**: Tuberculosis (TB) is a top-10 cause of death worldwide. Though the WHO recommends chest radiographs (CXRs) for TB screening, the limited availability of CXR interpretation is a barrier. We trained a deep learning system (DLS) to detect active pulmonary TB using CXRs from 9 countries across Africa, Asia, and Europe, and utilized large-scale CXR pretraining, attention pooling, and noisy student semi-supervised learning. Evaluation was on (1) a combined test set spanning China, India, US, and Zambia, and (2) an independent mining population in South Africa. Given WHO targets of 90% sensitivity and 70% specificity, the DLS's operating point was prespecified to favor sensitivity over specificity. On the combined test set, the DLS's ROC curve was above all 9 India-based radiologists, with an AUC of 0.90 (95%CI 0.87-0.92). The DLS's sensitivity (88%) was higher than the India-based radiologists (75% mean sensitivity), p<0.001 for superiority; and its specificity (79%) was non-inferior to the radiologists (84% mean specificity), p=0.004. Similar trends were observed within HIV positive and sputum smear positive sub-groups, and in the South Africa test set. We found that 5 US-based radiologists (where TB isn't endemic) were more sensitive and less specific than the India-based radiologists (where TB is endemic). The DLS also remained non-inferior to the US-based radiologists. In simulations, using the DLS as a prioritization tool for confirmatory testing reduced the cost per positive case detected by 40-80% compared to using confirmatory testing alone. To conclude, our DLS generalized to 5 countries, and merits prospective evaluation to assist cost-effective screening efforts in radiologist-limited settings. Operating point flexibility may permit customization of the DLS to account for site-specific factors such as TB prevalence, demographics, clinical resources, and customary practice patterns.



