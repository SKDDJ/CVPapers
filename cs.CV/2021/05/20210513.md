# Arxiv Papers in cs.CV on 2021-05-13
### Extreme Face Inpainting with Sketch-Guided Conditional GAN
- **Arxiv ID**: http://arxiv.org/abs/2105.06033v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06033v1)
- **Published**: 2021-05-13 01:45:45+00:00
- **Updated**: 2021-05-13 01:45:45+00:00
- **Authors**: Nilesh Pandey, Andreas Savakis
- **Comment**: None
- **Journal**: None
- **Summary**: Recovering badly damaged face images is a useful yet challenging task, especially in extreme cases where the masked or damaged region is very large. One of the major challenges is the ability of the system to generalize on faces outside the training dataset. We propose to tackle this extreme inpainting task with a conditional Generative Adversarial Network (GAN) that utilizes structural information, such as edges, as a prior condition. Edge information can be obtained from the partially masked image and a structurally similar image or a hand drawing. In our proposed conditional GAN, we pass the conditional input in every layer of the encoder while maintaining consistency in the distributions between the learned weights and the incoming conditional input. We demonstrate the effectiveness of our method with badly damaged face examples.



### Compatibility-aware Heterogeneous Visual Search
- **Arxiv ID**: http://arxiv.org/abs/2105.06047v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.06047v1)
- **Published**: 2021-05-13 02:30:50+00:00
- **Updated**: 2021-05-13 02:30:50+00:00
- **Authors**: Rahul Duggal, Hao Zhou, Shuo Yang, Yuanjun Xiong, Wei Xia, Zhuowen Tu, Stefano Soatto
- **Comment**: Accepted at CVPR 2021
- **Journal**: None
- **Summary**: We tackle the problem of visual search under resource constraints. Existing systems use the same embedding model to compute representations (embeddings) for the query and gallery images. Such systems inherently face a hard accuracy-efficiency trade-off: the embedding model needs to be large enough to ensure high accuracy, yet small enough to enable query-embedding computation on resource-constrained platforms. This trade-off could be mitigated if gallery embeddings are generated from a large model and query embeddings are extracted using a compact model. The key to building such a system is to ensure representation compatibility between the query and gallery models. In this paper, we address two forms of compatibility: One enforced by modifying the parameters of each model that computes the embeddings. The other by modifying the architectures that compute the embeddings, leading to compatibility-aware neural architecture search (CMP-NAS). We test CMP-NAS on challenging retrieval tasks for fashion images (DeepFashion2), and face images (IJB-C). Compared to ordinary (homogeneous) visual search using the largest embedding model (paragon), CMP-NAS achieves 80-fold and 23-fold cost reduction while maintaining accuracy within 0.3% and 1.6% of the paragon on DeepFashion2 and IJB-C respectively.



### TopoTxR: A Topological Biomarker for Predicting Treatment Response in Breast Cancer
- **Arxiv ID**: http://arxiv.org/abs/2105.06049v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.06049v1)
- **Published**: 2021-05-13 02:38:48+00:00
- **Updated**: 2021-05-13 02:38:48+00:00
- **Authors**: Fan Wang, Saarthak Kapse, Steven Liu, Prateek Prasanna, Chao Chen
- **Comment**: 12 pages, 5 figures, 2 tables, accepted to International Conference
  on Information Processing in Medical Imaging (IPMI) 2021
- **Journal**: None
- **Summary**: Characterization of breast parenchyma on dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is a challenging task owing to the complexity of underlying tissue structures. Current quantitative approaches, including radiomics and deep learning models, do not explicitly capture the complex and subtle parenchymal structures, such as fibroglandular tissue. In this paper, we propose a novel method to direct a neural network's attention to a dedicated set of voxels surrounding biologically relevant tissue structures. By extracting multi-dimensional topological structures with high saliency, we build a topology-derived biomarker, TopoTxR. We demonstrate the efficacy of TopoTxR in predicting response to neoadjuvant chemotherapy in breast cancer. Our qualitative and quantitative results suggest differential topological behavior of breast tissue on treatment-na\"ive imaging, in patients who respond favorably to therapy versus those who do not.



### Model Pruning Based on Quantified Similarity of Feature Maps
- **Arxiv ID**: http://arxiv.org/abs/2105.06052v2
- **DOI**: 10.1109/JIOT.2022.3190873
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.06052v2)
- **Published**: 2021-05-13 02:57:30+00:00
- **Updated**: 2022-08-10 04:25:21+00:00
- **Authors**: Zidu Wang, Xuexin Liu, Long Huang, Yunqing Chen, Yufei Zhang, Zhikang Lin, Rui Wang
- **Comment**: To be published in IOTJ, also available on
  https://ieeexplore.ieee.org/document/9829719
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) has been applied in numerous Internet of Things (IoT) devices for multifarious downstream tasks. However, with the increasing amount of data on edge devices, CNNs can hardly complete some tasks in time with limited computing and storage resources. Recently, filter pruning has been regarded as an effective technique to compress and accelerate CNNs, but existing methods rarely prune CNNs from the perspective of compressing high-dimensional tensors. In this paper, we propose a novel theory to find redundant information in three-dimensional tensors, namely Quantified Similarity between Feature Maps (QSFM), and utilize this theory to guide the filter pruning procedure. We perform QSFM on datasets (CIFAR-10, CIFAR-100 and ILSVRC-12) and edge devices, demonstrate that the proposed method can find the redundant information in the neural networks effectively with comparable compression and tolerable drop of accuracy. Without any fine-tuning operation, QSFM can compress ResNet-56 on CIFAR-10 significantly (48.7% FLOPs and 57.9% parameters are reduced) with only a loss of 0.54% in the top-1 accuracy. For the practical application of edge devices, QSFM can accelerate MobileNet-V2 inference speed by 1.53 times with only a loss of 1.23% in the ILSVRC-12 top-1 accuracy.



### GAN Prior Embedded Network for Blind Face Restoration in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2105.06070v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06070v1)
- **Published**: 2021-05-13 04:14:00+00:00
- **Updated**: 2021-05-13 04:14:00+00:00
- **Authors**: Tao Yang, Peiran Ren, Xuansong Xie, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Blind face restoration (BFR) from severely degraded face images in the wild is a very challenging problem. Due to the high illness of the problem and the complex unknown degradation, directly training a deep neural network (DNN) usually cannot lead to acceptable results. Existing generative adversarial network (GAN) based methods can produce better results but tend to generate over-smoothed restorations. In this work, we propose a new method by first learning a GAN for high-quality face image generation and embedding it into a U-shaped DNN as a prior decoder, then fine-tuning the GAN prior embedded DNN with a set of synthesized low-quality face images. The GAN blocks are designed to ensure that the latent code and noise input to the GAN can be respectively generated from the deep and shallow features of the DNN, controlling the global face structure, local face details and background of the reconstructed image. The proposed GAN prior embedded network (GPEN) is easy-to-implement, and it can generate visually photo-realistic results. Our experiments demonstrated that the proposed GPEN achieves significantly superior results to state-of-the-art BFR methods both quantitatively and qualitatively, especially for the restoration of severely degraded face images in the wild. The source code and models can be found at https://github.com/yangxy/GPEN.



### Learning symbol relation tree for online mathematical expression recognition
- **Arxiv ID**: http://arxiv.org/abs/2105.06084v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2105.06084v1)
- **Published**: 2021-05-13 05:18:17+00:00
- **Updated**: 2021-05-13 05:18:17+00:00
- **Authors**: Thanh-Nghia Truong, Hung Tuan Nguyen, Cuong Tuan Nguyen, Masaki Nakagawa
- **Comment**: 13 pages, conference
- **Journal**: None
- **Summary**: This paper proposes a method for recognizing online handwritten mathematical expressions (OnHME) by building a symbol relation tree (SRT) directly from a sequence of strokes. A bidirectional recurrent neural network learns from multiple derived paths of SRT to predict both symbols and spatial relations between symbols using global context. The recognition system has two parts: a temporal classifier and a tree connector. The temporal classifier produces an SRT by recognizing an OnHME pattern. The tree connector splits the SRT into several sub-SRTs. The final SRT is formed by looking up the best combination among those sub-SRTs. Besides, we adopt a tree sorting method to deal with various stroke orders. Recognition experiments indicate that the proposed OnHME recognition system is competitive to other methods. The recognition system achieves 44.12% and 41.76% expression recognition rates on the Competition on Recognition of Online Handwritten Mathematical Expressions (CROHME) 2014 and 2016 testing sets.



### HINet: Half Instance Normalization Network for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2105.06086v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.06086v2)
- **Published**: 2021-05-13 05:25:01+00:00
- **Updated**: 2022-05-01 14:43:45+00:00
- **Authors**: Liangyu Chen, Xin Lu, Jie Zhang, Xiaojie Chu, Chengpeng Chen
- **Comment**: None
- **Journal**: CVPRW2021
- **Summary**: In this paper, we explore the role of Instance Normalization in low-level vision tasks. Specifically, we present a novel block: Half Instance Normalization Block (HIN Block), to boost the performance of image restoration networks. Based on HIN Block, we design a simple and powerful multi-stage network named HINet, which consists of two subnetworks. With the help of HIN Block, HINet surpasses the state-of-the-art (SOTA) on various image restoration tasks. For image denoising, we exceed it 0.11dB and 0.28 dB in PSNR on SIDD dataset, with only 7.5% and 30% of its multiplier-accumulator operations (MACs), 6.8 times and 2.9 times speedup respectively. For image deblurring, we get comparable performance with 22.5% of its MACs and 3.3 times speedup on REDS and GoPro datasets. For image deraining, we exceed it by 0.3 dB in PSNR on the average result of multiple datasets with 1.4 times speedup. With HINet, we won 1st place on the NTIRE 2021 Image Deblurring Challenge - Track2. JPEG Artifacts, with a PSNR of 29.70. The code is available at https://github.com/megvii-model/HINet.



### Superevents: Towards Native Semantic Segmentation for Event-based Cameras
- **Arxiv ID**: http://arxiv.org/abs/2105.06091v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.06091v1)
- **Published**: 2021-05-13 05:49:41+00:00
- **Updated**: 2021-05-13 05:49:41+00:00
- **Authors**: Weng Fei Low, Ankit Sonthalia, Zhi Gao, André van Schaik, Bharath Ramesh
- **Comment**: None
- **Journal**: None
- **Summary**: Most successful computer vision models transform low-level features, such as Gabor filter responses, into richer representations of intermediate or mid-level complexity for downstream visual tasks. These mid-level representations have not been explored for event cameras, although it is especially relevant to the visually sparse and often disjoint spatial information in the event stream. By making use of locally consistent intermediate representations, termed as superevents, numerous visual tasks ranging from semantic segmentation, visual tracking, depth estimation shall benefit. In essence, superevents are perceptually consistent local units that delineate parts of an object in a scene. Inspired by recent deep learning architectures, we present a novel method that employs lifetime augmentation for obtaining an event stream representation that is fed to a fully convolutional network to extract superevents. Our qualitative and quantitative experimental results on several sequences of a benchmark dataset highlights the significant potential for event-based downstream applications.



### TAR: Generalized Forensic Framework to Detect Deepfakes using Weakly Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.06117v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06117v1)
- **Published**: 2021-05-13 07:31:08+00:00
- **Updated**: 2021-05-13 07:31:08+00:00
- **Authors**: Sangyup Lee, Shahroz Tariq, Junyaup Kim, Simon S. Woo
- **Comment**: 16 pages, 3 figures, to be published in IFIP-SEC 2021
- **Journal**: None
- **Summary**: Deepfakes have become a critical social problem, and detecting them is of utmost importance. Also, deepfake generation methods are advancing, and it is becoming harder to detect. While many deepfake detection models can detect different types of deepfakes separately, they perform poorly on generalizing the detection performance over multiple types of deepfake. This motivates us to develop a generalized model to detect different types of deepfakes. Therefore, in this work, we introduce a practical digital forensic tool to detect different types of deepfakes simultaneously and propose Transfer learning-based Autoencoder with Residuals (TAR). The ultimate goal of our work is to develop a unified model to detect various types of deepfake videos with high accuracy, with only a small number of training samples that can work well in real-world settings. We develop an autoencoder-based detection model with Residual blocks and sequentially perform transfer learning to detect different types of deepfakes simultaneously. Our approach achieves a much higher generalized detection performance than the state-of-the-art methods on the FaceForensics++ dataset. In addition, we evaluate our model on 200 real-world Deepfake-in-the-Wild (DW) videos of 50 celebrities available on the Internet and achieve 89.49% zero-shot accuracy, which is significantly higher than the best baseline model (gaining 10.77%), demonstrating and validating the practicability of our approach.



### Deep Unsupervised Hashing by Distilled Smooth Guidance
- **Arxiv ID**: http://arxiv.org/abs/2105.06125v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06125v1)
- **Published**: 2021-05-13 07:59:57+00:00
- **Updated**: 2021-05-13 07:59:57+00:00
- **Authors**: Xiao Luo, Zeyu Ma, Daqing Wu, Huasong Zhong, Chong Chen, Jinwen Ma, Minghua Deng
- **Comment**: 7 pages, 3 figures
- **Journal**: ICME 2021
- **Summary**: Hashing has been widely used in approximate nearest neighbor search for its storage and computational efficiency. Deep supervised hashing methods are not widely used because of the lack of labeled data, especially when the domain is transferred. Meanwhile, unsupervised deep hashing models can hardly achieve satisfactory performance due to the lack of reliable similarity signals. To tackle this problem, we propose a novel deep unsupervised hashing method, namely Distilled Smooth Guidance (DSG), which can learn a distilled dataset consisting of similarity signals as well as smooth confidence signals. To be specific, we obtain the similarity confidence weights based on the initial noisy similarity signals learned from local structures and construct a priority loss function for smooth similarity-preserving learning. Besides, global information based on clustering is utilized to distill the image pairs by removing contradictory similarity signals. Extensive experiments on three widely used benchmark datasets show that the proposed DSG consistently outperforms the state-of-the-art search methods.



### SAFIN: Arbitrary Style Transfer With Self-Attentive Factorized Instance Normalization
- **Arxiv ID**: http://arxiv.org/abs/2105.06129v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.06129v2)
- **Published**: 2021-05-13 08:01:01+00:00
- **Updated**: 2021-05-20 05:44:54+00:00
- **Authors**: Aaditya Singh, Shreeshail Hingane, Xinyu Gong, Zhangyang Wang
- **Comment**: Accepted at ICME 2021, 5 Pages + 1 Page (references)
- **Journal**: None
- **Summary**: Artistic style transfer aims to transfer the style characteristics of one image onto another image while retaining its content. Existing approaches commonly leverage various normalization techniques, although these face limitations in adequately transferring diverse textures to different spatial locations. Self-Attention-based approaches have tackled this issue with partial success but suffer from unwanted artifacts. Motivated by these observations, this paper aims to combine the best of both worlds: self-attention and normalization. That yields a new plug-and-play module that we name Self-Attentive Factorized Instance Normalization (SAFIN). SAFIN is essentially a spatially adaptive normalization module whose parameters are inferred through attention on the content and style image. We demonstrate that plugging SAFIN into the base network of another state-of-the-art method results in enhanced stylization. We also develop a novel base network composed of Wavelet Transform for multi-scale style transfer, which when combined with SAFIN, produces visually appealing results with lesser unwanted textures.



### Unsupervised Hashing with Contrastive Information Bottleneck
- **Arxiv ID**: http://arxiv.org/abs/2105.06138v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06138v2)
- **Published**: 2021-05-13 08:30:16+00:00
- **Updated**: 2021-05-19 02:57:51+00:00
- **Authors**: Zexuan Qiu, Qinliang Su, Zijing Ou, Jianxing Yu, Changyou Chen
- **Comment**: IJCAI 2021
- **Journal**: None
- **Summary**: Many unsupervised hashing methods are implicitly established on the idea of reconstructing the input data, which basically encourages the hashing codes to retain as much information of original data as possible. However, this requirement may force the models spending lots of their effort on reconstructing the unuseful background information, while ignoring to preserve the discriminative semantic information that is more important for the hashing task. To tackle this problem, inspired by the recent success of contrastive learning in learning continuous representations, we propose to adapt this framework to learn binary hashing codes. Specifically, we first propose to modify the objective function to meet the specific requirement of hashing and then introduce a probabilistic binary representation layer into the model to facilitate end-to-end training of the entire model. We further prove the strong connection between the proposed contrastive-learning-based hashing method and the mutual information, and show that the proposed model can be considered under the broader framework of the information bottleneck (IB). Under this perspective, a more general hashing model is naturally obtained. Extensive experimental results on three benchmark image datasets demonstrate that the proposed hashing method significantly outperforms existing baselines.



### A hybrid machine learning/deep learning COVID-19 severity predictive model from CT images and clinical data
- **Arxiv ID**: http://arxiv.org/abs/2105.06141v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2105.06141v1)
- **Published**: 2021-05-13 08:39:56+00:00
- **Updated**: 2021-05-13 08:39:56+00:00
- **Authors**: Matteo Chieregato, Fabio Frangiamore, Mauro Morassi, Claudia Baresi, Stefania Nici, Chiara Bassetti, Claudio Bnà, Marco Galelli
- **Comment**: 16 pages, 10 figures, 2 supplementary tables
- **Journal**: None
- **Summary**: COVID-19 clinical presentation and prognosis are highly variable, ranging from asymptomatic and paucisymptomatic cases to acute respiratory distress syndrome and multi-organ involvement. We developed a hybrid machine learning/deep learning model to classify patients in two outcome categories, non-ICU and ICU (intensive care admission or death), using 558 patients admitted in a northern Italy hospital in February/May of 2020. A fully 3D patient-level CNN classifier on baseline CT images is used as feature extractor. Features extracted, alongside with laboratory and clinical data, are fed for selection in a Boruta algorithm with SHAP game theoretical values. A classifier is built on the reduced feature space using CatBoost gradient boosting algorithm and reaching a probabilistic AUC of 0.949 on holdout test set. The model aims to provide clinical decision support to medical doctors, with the probability score of belonging to an outcome class and with case-based SHAP interpretation of features importance.



### Boosting Light-Weight Depth Estimation Via Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2105.06143v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06143v3)
- **Published**: 2021-05-13 08:42:42+00:00
- **Updated**: 2023-04-16 06:41:31+00:00
- **Authors**: Junjie Hu, Chenyou Fan, Hualie Jiang, Xiyue Guo, Yuan Gao, Xiangyong Lu, Tin Lun Lam
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular depth estimation (MDE) methods are often either too computationally expensive or not accurate enough due to the trade-off between model complexity and inference performance. In this paper, we propose a lightweight network that can accurately estimate depth maps using minimal computing resources. We achieve this by designing a compact model architecture that maximally reduces model complexity. To improve the performance of our lightweight network, we adopt knowledge distillation (KD) techniques. We consider a large network as an expert teacher that accurately estimates depth maps on the target domain. The student, which is the lightweight network, is then trained to mimic the teacher's predictions. However, this KD process can be challenging and insufficient due to the large model capacity gap between the teacher and the student. To address this, we propose to use auxiliary unlabeled data to guide KD, enabling the student to better learn from the teacher's predictions. This approach helps fill the gap between the teacher and the student, resulting in improved data-driven learning. Our extensive experiments show that our method achieves comparable performance to state-of-the-art methods while using only 1% of their parameters. Furthermore, our method outperforms previous lightweight methods regarding inference accuracy, computational efficiency, and generalizability.



### When Human Pose Estimation Meets Robustness: Adversarial Algorithms and Benchmarks
- **Arxiv ID**: http://arxiv.org/abs/2105.06152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06152v1)
- **Published**: 2021-05-13 09:09:07+00:00
- **Updated**: 2021-05-13 09:09:07+00:00
- **Authors**: Jiahang Wang, Sheng Jin, Wentao Liu, Weizhong Liu, Chen Qian, Ping Luo
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Human pose estimation is a fundamental yet challenging task in computer vision, which aims at localizing human anatomical keypoints. However, unlike human vision that is robust to various data corruptions such as blur and pixelation, current pose estimators are easily confused by these corruptions. This work comprehensively studies and addresses this problem by building rigorous robust benchmarks, termed COCO-C, MPII-C, and OCHuman-C, to evaluate the weaknesses of current advanced pose estimators, and a new algorithm termed AdvMix is proposed to improve their robustness in different corruptions. Our work has several unique benefits. (1) AdvMix is model-agnostic and capable in a wide-spectrum of pose estimation models. (2) AdvMix consists of adversarial augmentation and knowledge distillation. Adversarial augmentation contains two neural network modules that are trained jointly and competitively in an adversarial manner, where a generator network mixes different corrupted images to confuse a pose estimator, improving the robustness of the pose estimator by learning from harder samples. To compensate for the noise patterns by adversarial augmentation, knowledge distillation is applied to transfer clean pose structure knowledge to the target pose estimator. (3) Extensive experiments show that AdvMix significantly increases the robustness of pose estimations across a wide range of corruptions, while maintaining accuracy on clean data in various challenging benchmark datasets.



### Relation-aware Hierarchical Attention Framework for Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2105.06160v2
- **DOI**: 10.1145/3460426.3463635
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.06160v2)
- **Published**: 2021-05-13 09:35:42+00:00
- **Updated**: 2021-05-14 02:34:56+00:00
- **Authors**: Fangtao Li, Ting Bai, Chenyu Cao, Zihe Liu, Chenghao Yan, Bin Wu
- **Comment**: 9 pages, This paper is accepted by ICMR 2021
- **Journal**: None
- **Summary**: Video Question Answering (VideoQA) is a challenging video understanding task since it requires a deep understanding of both question and video. Previous studies mainly focus on extracting sophisticated visual and language embeddings, fusing them by delicate hand-crafted networks. However, the relevance of different frames, objects, and modalities to the question are varied along with the time, which is ignored in most of existing methods. Lacking understanding of the the dynamic relationships and interactions among objects brings a great challenge to VideoQA task. To address this problem, we propose a novel Relation-aware Hierarchical Attention (RHA) framework to learn both the static and dynamic relations of the objects in videos. In particular, videos and questions are embedded by pre-trained models firstly to obtain the visual and textual features. Then a graph-based relation encoder is utilized to extract the static relationship between visual objects. To capture the dynamic changes of multimodal objects in different video frames, we consider the temporal, spatial, and semantic relations, and fuse the multimodal features by hierarchical attention mechanism to predict the answer. We conduct extensive experiments on a large scale VideoQA dataset, and the experimental results demonstrate that our RHA outperforms the state-of-the-art methods.



### Global Wheat Challenge 2020: Analysis of the competition design and winning models
- **Arxiv ID**: http://arxiv.org/abs/2105.06182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06182v1)
- **Published**: 2021-05-13 10:41:09+00:00
- **Updated**: 2021-05-13 10:41:09+00:00
- **Authors**: Etienne David, Franklin Ogidi, Wei Guo, Frederic Baret, Ian Stavness
- **Comment**: None
- **Journal**: None
- **Summary**: Data competitions have become a popular approach to crowdsource new data analysis methods for general and specialized data science problems. In plant phenotyping, data competitions have a rich history, and new outdoor field datasets have potential for new data competitions. We developed the Global Wheat Challenge as a generalization competition to see if solutions for wheat head detection from field images would work in different regions around the world. In this paper, we analyze the winning challenge solutions in terms of their robustness and the relative importance of model and data augmentation design decisions. We found that the design of the competition influence the selection of winning solutions and provide recommendations for future competitions in an attempt to garner more robust winning solutions.



### Adaptive Test-Time Augmentation for Low-Power CPU
- **Arxiv ID**: http://arxiv.org/abs/2105.06183v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.06183v1)
- **Published**: 2021-05-13 10:50:13+00:00
- **Updated**: 2021-05-13 10:50:13+00:00
- **Authors**: Luca Mocerino, Roberto G. Rizzo, Valentino Peluso, Andrea Calimera, Enrico Macii
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (ConvNets) are trained offline using the few available data and may therefore suffer from substantial accuracy loss when ported on the field, where unseen input patterns received under unpredictable external conditions can mislead the model. Test-Time Augmentation (TTA) techniques aim to alleviate such common side effect at inference-time, first running multiple feed-forward passes on a set of altered versions of the same input sample, and then computing the main outcome through a consensus of the aggregated predictions. Unfortunately, the implementation of TTA on embedded CPUs introduces latency penalties that limit its adoption on edge applications. To tackle this issue, we propose AdapTTA, an adaptive implementation of TTA that controls the number of feed-forward passes dynamically, depending on the complexity of the input. Experimental results on state-of-the-art ConvNets for image classification deployed on a commercial ARM Cortex-A CPU demonstrate AdapTTA reaches remarkable latency savings, from 1.49X to 2.21X, and hence a higher frame rate compared to static TTA, still preserving the same accuracy gain.



### SizeNet: Object Recognition via Object Real Size-based Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.06188v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.06188v2)
- **Published**: 2021-05-13 11:03:24+00:00
- **Updated**: 2021-05-26 02:32:13+00:00
- **Authors**: Xiaofei Li, Zhong Dong
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by the conclusion that humans choose the visual cortex regions corresponding to the real size of an object to analyze its features when identifying objects in the real world, this paper presents a framework, SizeNet, which is based on both the real sizes and features of objects to solve object recognition problems. SizeNet was used for object recognition experiments on the homemade Rsize dataset, and was compared with the state-of-the-art methods AlexNet, VGG-16, Inception V3, Resnet-18, and DenseNet-121. The results showed that SizeNet provides much higher accuracy rates for object recognition than the other algorithms. SizeNet can solve the two problems of correctly recognizing objects with highly similar features but real sizes that are obviously different from each other, and correctly distinguishing a target object from interference objects whose real sizes are obviously different from the target object. This is because SizeNet recognizes objects based not only on their features, but also on their real size. The real size of an object can help exclude the interference object's categories whose real size ranges do not match the real size of the object, which greatly reduces the object's categories' number in the label set used for the downstream object recognition based on object features. SizeNet is of great significance for studying the interpretable computer vision. Our code and dataset will thus be made public.



### Geometric Model Checking of Continuous Space
- **Arxiv ID**: http://arxiv.org/abs/2105.06194v5
- **DOI**: 10.46298/lmcs-18(4:7)2022
- **Categories**: **cs.LO**, cs.AI, cs.CV, cs.GR, 68Q60, F.4.1; D.2.4; I.2.4; I.3.6; I.4.6; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2105.06194v5)
- **Published**: 2021-05-13 11:25:25+00:00
- **Updated**: 2022-11-21 10:14:19+00:00
- **Authors**: Nick Bezhanishvili, Vincenzo Ciancia, David Gabelaia, Gianluca Grilletti, Diego Latella, Mieke Massink
- **Comment**: None
- **Journal**: Logical Methods in Computer Science, Volume 18, Issue 4 (November
  22, 2022) lmcs:9060
- **Summary**: Topological Spatial Model Checking is a recent paradigm where model checking techniques are developed for the topological interpretation of Modal Logic. The Spatial Logic of Closure Spaces, SLCS, extends Modal Logic with reachability connectives that, in turn, can be used for expressing interesting spatial properties, such as "being near to" or "being surrounded by". SLCS constitutes the kernel of a solid logical framework for reasoning about discrete space, such as graphs and digital images, interpreted as quasi discrete closure spaces. Following a recently developed geometric semantics of Modal Logic, we propose an interpretation of SLCS in continuous space, admitting a geometric spatial model checking procedure, by resorting to models based on polyhedra. Such representations of space are increasingly relevant in many domains of application, due to recent developments of 3D scanning and visualisation techniques that exploit mesh processing. We introduce PolyLogicA, a geometric spatial model checker for SLCS formulas on polyhedra and demonstrate feasibility of our approach on two 3D polyhedral models of realistic size. Finally, we introduce a geometric definition of bisimilarity, proving that it characterises logical equivalence.



### Quantized Proximal Averaging Network for Analysis Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/2105.06211v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, 68T07, I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2105.06211v1)
- **Published**: 2021-05-13 12:05:35+00:00
- **Updated**: 2021-05-13 12:05:35+00:00
- **Authors**: Kartheek Kumar Reddy Nareddy, Mani Madhoolika Bulusu, Praveen Kumar Pokala, Chandra Sekhar Seelamantula
- **Comment**: 8 pages + references, 7 figures and 4 tables
- **Journal**: None
- **Summary**: We solve the analysis sparse coding problem considering a combination of convex and non-convex sparsity promoting penalties. The multi-penalty formulation results in an iterative algorithm involving proximal-averaging. We then unfold the iterative algorithm into a trainable network that facilitates learning the sparsity prior. We also consider quantization of the network weights. Quantization makes neural networks efficient both in terms of memory and computation during inference, and also renders them compatible for low-precision hardware deployment. Our learning algorithm is based on a variant of the ADAM optimizer in which the quantizer is part of the forward pass and the gradients of the loss function are evaluated corresponding to the quantized weights while doing a book-keeping of the high-precision weights. We demonstrate applications to compressed image recovery and magnetic resonance image reconstruction. The proposed approach offers superior reconstruction accuracy and quality than state-of-the-art unfolding techniques and the performance degradation is minimal even when the weights are subjected to extreme quantization.



### TransferI2I: Transfer Learning for Image-to-Image Translation from Small Datasets
- **Arxiv ID**: http://arxiv.org/abs/2105.06219v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06219v2)
- **Published**: 2021-05-13 12:19:36+00:00
- **Updated**: 2021-05-14 07:14:12+00:00
- **Authors**: Yaxing Wang, Hector Laria Mantecon, Joost van de Weijer, Laura Lopez-Fuentes, Bogdan Raducanu
- **Comment**: Technical report
- **Journal**: None
- **Summary**: Image-to-image (I2I) translation has matured in recent years and is able to generate high-quality realistic images. However, despite current success, it still faces important challenges when applied to small domains. Existing methods use transfer learning for I2I translation, but they still require the learning of millions of parameters from scratch. This drawback severely limits its application on small domains. In this paper, we propose a new transfer learning for I2I translation (TransferI2I). We decouple our learning process into the image generation step and the I2I translation step. In the first step we propose two novel techniques: source-target initialization and self-initialization of the adaptor layer. The former finetunes the pretrained generative model (e.g., StyleGAN) on source and target data. The latter allows to initialize all non-pretrained network parameters without the need of any data. These techniques provide a better initialization for the I2I translation step. In addition, we introduce an auxiliary GAN that further facilitates the training of deep I2I systems even from small datasets. In extensive experiments on three datasets, (Animal faces, Birds, and Foods), we show that we outperform existing methods and that mFID improves on several datasets with over 25 points.



### VSR: A Unified Framework for Document Layout Analysis combining Vision, Semantics and Relations
- **Arxiv ID**: http://arxiv.org/abs/2105.06220v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06220v1)
- **Published**: 2021-05-13 12:20:30+00:00
- **Updated**: 2021-05-13 12:20:30+00:00
- **Authors**: Peng Zhang, Can Li, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu, Fei Wu
- **Comment**: Accepted by ICDAR2021
- **Journal**: None
- **Summary**: Document layout analysis is crucial for understanding document structures. On this task, vision and semantics of documents, and relations between layout components contribute to the understanding process. Though many works have been proposed to exploit the above information, they show unsatisfactory results. NLP-based methods model layout analysis as a sequence labeling task and show insufficient capabilities in layout modeling. CV-based methods model layout analysis as a detection or segmentation task, but bear limitations of inefficient modality fusion and lack of relation modeling between layout components. To address the above limitations, we propose a unified framework VSR for document layout analysis, combining vision, semantics and relations. VSR supports both NLP-based and CV-based methods. Specifically, we first introduce vision through document image and semantics through text embedding maps. Then, modality-specific visual and semantic features are extracted using a two-stream network, which are adaptively fused to make full use of complementary information. Finally, given component candidates, a relation module based on graph neural network is incorported to model relations between components and output final results. On three popular benchmarks, VSR outperforms previous models by large margins. Code will be released soon.



### LGPMA: Complicated Table Structure Recognition with Local and Global Pyramid Mask Alignment
- **Arxiv ID**: http://arxiv.org/abs/2105.06224v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06224v3)
- **Published**: 2021-05-13 12:24:12+00:00
- **Updated**: 2022-04-20 03:41:11+00:00
- **Authors**: Liang Qiao, Zaisheng Li, Zhanzhan Cheng, Peng Zhang, Shiliang Pu, Yi Niu, Wenqi Ren, Wenming Tan, Fei Wu
- **Comment**: Award of ICDAR2021 Best Industry Paper. Code is available at
  https://davar-lab.github.io/publication.html or
  https://github.com/hikopensource/DAVAR-Lab-OCR -------------- Fixed formula
  typos in Eq. 1
- **Journal**: None
- **Summary**: Table structure recognition is a challenging task due to the various structures and complicated cell spanning relations. Previous methods handled the problem starting from elements in different granularities (rows/columns, text regions), which somehow fell into the issues like lossy heuristic rules or neglect of empty cell division. Based on table structure characteristics, we find that obtaining the aligned bounding boxes of text region can effectively maintain the entire relevant range of different cells. However, the aligned bounding boxes are hard to be accurately predicted due to the visual ambiguities. In this paper, we aim to obtain more reliable aligned bounding boxes by fully utilizing the visual information from both text regions in proposed local features and cell relations in global features. Specifically, we propose the framework of Local and Global Pyramid Mask Alignment, which adopts the soft pyramid mask learning mechanism in both the local and global feature maps. It allows the predicted boundaries of bounding boxes to break through the limitation of original proposals. A pyramid mask re-scoring module is then integrated to compromise the local and global information and refine the predicted boundaries. Finally, we propose a robust table structure recovery pipeline to obtain the final structure, in which we also effectively solve the problems of empty cells locating and division. Experimental results show that the proposed method achieves competitive and even new state-of-the-art performance on several public benchmarks.



### Reciprocal Feature Learning via Explicit and Implicit Tasks in Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2105.06229v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06229v2)
- **Published**: 2021-05-13 12:27:35+00:00
- **Updated**: 2021-10-25 09:29:48+00:00
- **Authors**: Hui Jiang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Yi Niu, Wenqi Ren, Fei Wu, Wenming Tan
- **Comment**: Accepted by ICDAR 2021. Code is available at
  https://davar-lab.github.io/publication.html or
  https://github.com/hikopensource/DAVAR-Lab-OCR
- **Journal**: None
- **Summary**: Text recognition is a popular topic for its broad applications. In this work, we excavate the implicit task, character counting within the traditional text recognition, without additional labor annotation cost. The implicit task plays as an auxiliary branch for complementing the sequential recognition. We design a two-branch reciprocal feature learning framework in order to adequately utilize the features from both the tasks. Through exploiting the complementary effect between explicit and implicit tasks, the feature is reliably enhanced. Extensive experiments on 7 benchmarks show the advantages of the proposed methods in both text recognition and the new-built character counting tasks. In addition, it is convenient yet effective to equip with variable networks and tasks. We offer abundant ablation studies, generalizing experiments with deeper understanding on the tasks. Code is available.



### Multi-scale Regional Attention Deeplab3+: Multiple Myeloma Plasma Cells Segmentation in Microscopic Images
- **Arxiv ID**: http://arxiv.org/abs/2105.06238v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.06238v1)
- **Published**: 2021-05-13 12:42:32+00:00
- **Updated**: 2021-05-13 12:42:32+00:00
- **Authors**: Afshin Bozorgpour, Reza Azad, Eman Showkatian, Alaa Sulaiman
- **Comment**: 10 pages, 5 figures, presented at ISBI2021
- **Journal**: None
- **Summary**: Multiple myeloma cancer is a type of blood cancer that happens when the growth of abnormal plasma cells becomes out of control in the bone marrow. There are various ways to diagnose multiple myeloma in bone marrow such as complete blood count test (CBC) or counting myeloma plasma cell in aspirate slide images using manual visualization or through image processing technique. In this work, an automatic deep learning method for the detection and segmentation of multiple myeloma plasma cell have been explored. To this end, a two-stage deep learning method is designed. In the first stage, the nucleus detection network is utilized to extract each instance of a cell of interest. The extracted instance is then fed to the multi-scale function to generate a multi-scale representation. The objective of the multi-scale function is to capture the shape variation and reduce the effect of object scale on the cytoplasm segmentation network. The generated scales are then fed into a pyramid of cytoplasm networks to learn the segmentation map in various scales. On top of the cytoplasm segmentation network, we included a scale aggregation function to refine and generate a final prediction. The proposed approach has been evaluated on the SegPC2021 grand-challenge and ranked second on the final test phase among all teams.



### Video Corpus Moment Retrieval with Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.06247v1
- **DOI**: 10.1145/3404835.3462874
- **Categories**: **cs.CL**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2105.06247v1)
- **Published**: 2021-05-13 12:54:39+00:00
- **Updated**: 2021-05-13 12:54:39+00:00
- **Authors**: Hao Zhang, Aixin Sun, Wei Jing, Guoshun Nan, Liangli Zhen, Joey Tianyi Zhou, Rick Siow Mong Goh
- **Comment**: 11 pages, 7 figures and 6 tables. Accepted by SIGIR 2021
- **Journal**: None
- **Summary**: Given a collection of untrimmed and unsegmented videos, video corpus moment retrieval (VCMR) is to retrieve a temporal moment (i.e., a fraction of a video) that semantically corresponds to a given text query. As video and text are from two distinct feature spaces, there are two general approaches to address VCMR: (i) to separately encode each modality representations, then align the two modality representations for query processing, and (ii) to adopt fine-grained cross-modal interaction to learn multi-modal representations for query processing. While the second approach often leads to better retrieval accuracy, the first approach is far more efficient. In this paper, we propose a Retrieval and Localization Network with Contrastive Learning (ReLoCLNet) for VCMR. We adopt the first approach and introduce two contrastive learning objectives to refine video encoder and text encoder to learn video and text representations separately but with better alignment for VCMR. The video contrastive learning (VideoCL) is to maximize mutual information between query and candidate video at video-level. The frame contrastive learning (FrameCL) aims to highlight the moment region corresponds to the query at frame-level, within a video. Experimental results show that, although ReLoCLNet encodes text and video separately for efficiency, its retrieval accuracy is comparable with baselines adopting cross-modal interaction learning.



### Assessing bikeability with street view imagery and computer vision
- **Arxiv ID**: http://arxiv.org/abs/2105.08499v3
- **DOI**: 10.1016/j.trc.2021.103371
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.08499v3)
- **Published**: 2021-05-13 14:08:58+00:00
- **Updated**: 2021-09-20 11:11:39+00:00
- **Authors**: Koichi Ito, Filip Biljecki
- **Comment**: None
- **Journal**: Transportation Research Part C 132: 103371, 2021
- **Summary**: Studies evaluating bikeability usually compute spatial indicators shaping cycling conditions and conflate them in a quantitative index. Much research involves site visits or conventional geospatial approaches, and few studies have leveraged street view imagery (SVI) for conducting virtual audits. These have assessed a limited range of aspects, and not all have been automated using computer vision (CV). Furthermore, studies have not yet zeroed in on gauging the usability of these technologies thoroughly. We investigate, with experiments at a fine spatial scale and across multiple geographies (Singapore and Tokyo), whether we can use SVI and CV to assess bikeability comprehensively. Extending related work, we develop an exhaustive index of bikeability composed of 34 indicators. The results suggest that SVI and CV are adequate to evaluate bikeability in cities comprehensively. As they outperformed non-SVI counterparts by a wide margin, SVI indicators are also found to be superior in assessing urban bikeability, and potentially can be used independently, replacing traditional techniques. However, the paper exposes some limitations, suggesting that the best way forward is combining both SVI and non-SVI approaches. The new bikeability index presents a contribution in transportation and urban analytics, and it is scalable to assess cycling appeal widely.



### Vision-Guided Active Tactile Perception for Crack Detection and Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2105.06325v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.06325v1)
- **Published**: 2021-05-13 14:25:08+00:00
- **Updated**: 2021-05-13 14:25:08+00:00
- **Authors**: Jiaqi Jiang, Guanqun Cao, Daniel Fernandes Gomes, Shan Luo
- **Comment**: 7 pages, accepted by Mediterranean Conference on Control and
  Automation 2021
- **Journal**: None
- **Summary**: Crack detection is of great significance for monitoring the integrity and well-being of the infrastructure such as bridges and underground pipelines, which are harsh environments for people to access. In recent years, computer vision techniques have been applied in detecting cracks in concrete structures. However, they suffer from variances in light conditions and shadows, lacking robustness and resulting in many false positives. To address the uncertainty in vision, human inspectors actively touch the surface of the structures, guided by vision, which has not been explored in autonomous crack detection. In this paper, we propose a novel approach to detect and reconstruct cracks in concrete structures using vision-guided active tactile perception. Given an RGB-D image of a structure, the rough profile of the crack in the structure surface will first be segmented with a fine-tuned Deep Convolutional Neural Networks, and a set of contact points are generated to guide the collection of tactile images by a camera-based optical tactile sensor. When contacts are made, a pixel-wise mask of the crack can be obtained from the tactile images and therefore the profile of the crack can be refined by aligning the RGB-D image and the tactile images. Extensive experiment results have shown that the proposed method improves the effectiveness and robustness of crack detection and reconstruction significantly, compared to crack detection with vision only, and has the potential to enable robots to help humans with the inspection and repair of the concrete infrastructure.



### 3D-CNN for Facial Micro- and Macro-expression Spotting on Long Video Sequences using Temporal Oriented Reference Frame
- **Arxiv ID**: http://arxiv.org/abs/2105.06340v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06340v4)
- **Published**: 2021-05-13 14:55:06+00:00
- **Updated**: 2022-05-26 21:47:22+00:00
- **Authors**: Chuin Hong Yap, Moi Hoon Yap, Adrian K. Davison, Connah Kendrick, Jingting Li, Sujing Wang, Ryan Cunningham
- **Comment**: None
- **Journal**: None
- **Summary**: Facial expression spotting is the preliminary step for micro- and macro-expression analysis. The task of reliably spotting such expressions in video sequences is currently unsolved. The current best systems depend upon optical flow methods to extract regional motion features, before categorisation of that motion into a specific class of facial movement. Optical flow is susceptible to drift error, which introduces a serious problem for motions with long-term dependencies, such as high frame-rate macro-expression. We propose a purely deep learning solution which, rather than tracking frame differential motion, compares via a convolutional model, each frame with two temporally local reference frames. Reference frames are sampled according to calculated micro- and macro-expression duration. As baseline for MEGC2021 using leave-one-subject-out evaluation method, we show that our solution achieves F1-score of 0.105 in a high frame-rate (200 fps) SAMM long videos dataset (SAMM-LV) and is competitive in a low frame-rate (30 fps) (CAS(ME)2) dataset. On unseen MEGC2022 challenge dataset, the baseline results are 0.1176 on SAMM Challenge dataset, 0.1739 on CAS(ME)3 and overall performance of 0.1531 on both dataset.



### Forensic Analysis of Video Files Using Metadata
- **Arxiv ID**: http://arxiv.org/abs/2105.06361v2
- **DOI**: 10.1109/CVPRW53098.2021.00115
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.06361v2)
- **Published**: 2021-05-13 15:40:39+00:00
- **Updated**: 2022-01-22 05:33:37+00:00
- **Authors**: Ziyue Xiang, János Horváth, Sriram Baireddy, Paolo Bestagini, Stefano Tubaro, Edward J. Delp
- **Comment**: v2: fixed a typo in Section 3.4; added page number; added IEEE
  copyright notice
- **Journal**: None
- **Summary**: The unprecedented ease and ability to manipulate video content has led to a rapid spread of manipulated media. The availability of video editing tools greatly increased in recent years, allowing one to easily generate photo-realistic alterations. Such manipulations can leave traces in the metadata embedded in video files. This metadata information can be used to determine video manipulations, brand of video recording device, the type of video editing tool, and other important evidence. In this paper, we focus on the metadata contained in the popular MP4 video wrapper/container. We describe our method for metadata extractor that uses the MP4's tree structure. Our approach for analyzing the video metadata produces a more compact representation. We will describe how we construct features from the metadata and then use dimensionality reduction and nearest neighbor classification for forensic analysis of a video file. Our approach allows one to visually inspect the distribution of metadata features and make decisions. The experimental results confirm that the performance of our approach surpasses other methods.



### Neighborhood-Aware Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2105.06369v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.06369v2)
- **Published**: 2021-05-13 15:56:52+00:00
- **Updated**: 2021-10-29 08:16:36+00:00
- **Authors**: Xiaofang Wang, Shengcao Cao, Mengtian Li, Kris M. Kitani
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: Existing neural architecture search (NAS) methods often return an architecture with good search performance but generalizes poorly to the test setting. To achieve better generalization, we propose a novel neighborhood-aware NAS formulation to identify flat-minima architectures in the search space, with the assumption that flat minima generalize better than sharp minima. The phrase ``flat-minima architecture'' refers to architectures whose performance is stable under small perturbations in the architecture (e.g., replacing a convolution with a skip connection). Our formulation takes the ``flatness'' of an architecture into account by aggregating the performance over the neighborhood of this architecture. We demonstrate a principled way to apply our formulation to existing search algorithms, including sampling-based algorithms and gradient-based algorithms. To facilitate the application to gradient-based algorithms, we also propose a differentiable representation for the neighborhood of architectures. Based on our formulation, we propose neighborhood-aware random search (NA-RS) and neighborhood-aware differentiable architecture search (NA-DARTS). Notably, by simply augmenting DARTS with our formulation, NA-DARTS outperforms DARTS and achieves state-of-the-art performance on established benchmarks, including CIFAR-10, CIFAR-100 and ImageNet.



### SyntheticFur dataset for neural rendering
- **Arxiv ID**: http://arxiv.org/abs/2105.06409v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.2.10; I.3.3
- **Links**: [PDF](http://arxiv.org/pdf/2105.06409v1)
- **Published**: 2021-05-13 16:31:15+00:00
- **Updated**: 2021-05-13 16:31:15+00:00
- **Authors**: Trung Le, Ryan Poplin, Fred Bertsch, Andeep Singh Toor, Margaret L. Oh
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new dataset called SyntheticFur built specifically for machine learning training. The dataset consists of ray traced synthetic fur renders with corresponding rasterized input buffers and simulation data files. We procedurally generated approximately 140,000 images and 15 simulations with Houdini. The images consist of fur groomed with different skin primitives and move with various motions in a predefined set of lighting environments. We also demonstrated how the dataset could be used with neural rendering to significantly improve fur graphics using inexpensive input buffers by training a conditional generative adversarial network with perceptual loss. We hope the availability of such high fidelity fur renders will encourage new advances with neural rendering for a variety of applications.



### Using Self-Supervised Auxiliary Tasks to Improve Fine-Grained Facial Representation
- **Arxiv ID**: http://arxiv.org/abs/2105.06421v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.06421v3)
- **Published**: 2021-05-13 16:56:36+00:00
- **Updated**: 2022-08-08 17:48:09+00:00
- **Authors**: Mahdi Pourmirzaei, Gholam Ali Montazer, Farzaneh Esmaili
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, at first, the impact of ImageNet pre-training on fine-grained Facial Emotion Recognition (FER) is investigated which shows that when enough augmentations on images are applied, training from scratch provides better result than fine-tuning on ImageNet pre-training. Next, we propose a method to improve fine-grained and in-the-wild FER, called Hybrid Multi-Task Learning (HMTL). HMTL uses Self-Supervised Learning (SSL) as an auxiliary task during classical Supervised Learning (SL) in the form of Multi-Task Learning (MTL). Leveraging SSL during training can gain additional information from images for the primary fine-grained SL task. We investigate how proposed HMTL can be used in the FER domain by designing two customized version of common pre-text task techniques, puzzling and in-painting. We achieve state-of-the-art results on the AffectNet benchmark via two types of HMTL, without utilizing pre-training on additional data. Experimental results on the common SSL pre-training and proposed HMTL demonstrate the difference and superiority of our work. However, HMTL is not only limited to FER domain. Experiments on two types of fine-grained facial tasks, i.e., head pose estimation and gender recognition, reveals the potential of using HMTL to improve fine-grained facial representation.



### DeepQAMVS: Query-Aware Hierarchical Pointer Networks for Multi-Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/2105.06441v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2105.06441v1)
- **Published**: 2021-05-13 17:33:26+00:00
- **Updated**: 2021-05-13 17:33:26+00:00
- **Authors**: Safa Messaoud, Ismini Lourentzou, Assma Boughoula, Mona Zehni, Zhizhen Zhao, Chengxiang Zhai, Alexander G. Schwing
- **Comment**: None
- **Journal**: None
- **Summary**: The recent growth of web video sharing platforms has increased the demand for systems that can efficiently browse, retrieve and summarize video content. Query-aware multi-video summarization is a promising technique that caters to this demand. In this work, we introduce a novel Query-Aware Hierarchical Pointer Network for Multi-Video Summarization, termed DeepQAMVS, that jointly optimizes multiple criteria: (1) conciseness, (2) representativeness of important query-relevant events and (3) chronological soundness. We design a hierarchical attention model that factorizes over three distributions, each collecting evidence from a different modality, followed by a pointer network that selects frames to include in the summary. DeepQAMVS is trained with reinforcement learning, incorporating rewards that capture representativeness, diversity, query-adaptability and temporal coherence. We achieve state-of-the-art results on the MVS1K dataset, with inference time scaling linearly with the number of input video frames.



### Episodic Transformer for Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2105.06453v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.06453v2)
- **Published**: 2021-05-13 17:51:46+00:00
- **Updated**: 2021-08-25 14:33:51+00:00
- **Authors**: Alexander Pashevich, Cordelia Schmid, Chen Sun
- **Comment**: ICCV 2021; 19 pages; Code available at
  https://github.com/alexpashevich/E.T
- **Journal**: None
- **Summary**: Interaction and navigation defined by natural language instructions in dynamic environments pose significant challenges for neural agents. This paper focuses on addressing two challenges: handling long sequence of subtasks, and understanding complex human instructions. We propose Episodic Transformer (E.T.), a multimodal transformer that encodes language inputs and the full episode history of visual observations and actions. To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions. We demonstrate that encoding the history with a transformer is critical to solve compositional tasks, and that pretraining and joint training with synthetic instructions further improve the performance. Our approach sets a new state of the art on the challenging ALFRED benchmark, achieving 38.4% and 8.5% task success rates on seen and unseen test splits.



### High-Resolution Complex Scene Synthesis with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2105.06458v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06458v1)
- **Published**: 2021-05-13 17:56:07+00:00
- **Updated**: 2021-05-13 17:56:07+00:00
- **Authors**: Manuel Jahn, Robin Rombach, Björn Ommer
- **Comment**: AI for Content Creation Workshop, CVPR 2021
- **Journal**: None
- **Summary**: The use of coarse-grained layouts for controllable synthesis of complex scene images via deep generative models has recently gained popularity. However, results of current approaches still fall short of their promise of high-resolution synthesis. We hypothesize that this is mostly due to the highly engineered nature of these approaches which often rely on auxiliary losses and intermediate steps such as mask generators. In this note, we present an orthogonal approach to this task, where the generative model is based on pure likelihood training without additional objectives. To do so, we first optimize a powerful compression model with adversarial training which learns to reconstruct its inputs via a discrete latent bottleneck and thereby effectively strips the latent representation of high-frequency details such as texture. Subsequently, we train an autoregressive transformer model to learn the distribution of the discrete image representations conditioned on a tokenized version of the layouts. Our experiments show that the resulting system is able to synthesize high-quality images consistent with the given layouts. In particular, we improve the state-of-the-art FID score on COCO-Stuff and on Visual Genome by up to 19% and 53% and demonstrate the synthesis of images up to 512 x 512 px on COCO and Open Images.



### End-to-End Sequential Sampling and Reconstruction for MRI
- **Arxiv ID**: http://arxiv.org/abs/2105.06460v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.06460v2)
- **Published**: 2021-05-13 17:56:18+00:00
- **Updated**: 2022-07-17 02:04:58+00:00
- **Authors**: Tianwei Yin, Zihui Wu, He Sun, Adrian V. Dalca, Yisong Yue, Katherine L. Bouman
- **Comment**: Code and supplementary materials are available at
  http://imaging.cms.caltech.edu/seq-mri
- **Journal**: Proceedings of Machine Learning for Health, PMLR 158:261-281, 2021
- **Summary**: Accelerated MRI shortens acquisition time by subsampling in the measurement $\kappa$-space. Recovering a high-fidelity anatomical image from subsampled measurements requires close cooperation between two components: (1) a sampler that chooses the subsampling pattern and (2) a reconstructor that recovers images from incomplete measurements. In this paper, we leverage the sequential nature of MRI measurements, and propose a fully differentiable framework that jointly learns a sequential sampling policy simultaneously with a reconstruction strategy. This co-designed framework is able to adapt during acquisition in order to capture the most informative measurements for a particular target. Experimental results on the fastMRI knee dataset demonstrate that the proposed approach successfully utilizes intermediate information during the sampling process to boost reconstruction performance. In particular, our proposed method can outperform the current state-of-the-art learned $\kappa$-space sampling baseline on over 96% of test samples. We also investigate the individual and collective benefits of the sequential sampling and co-design strategies.



### 3D Spatial Recognition without Spatially Labeled 3D
- **Arxiv ID**: http://arxiv.org/abs/2105.06461v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2105.06461v1)
- **Published**: 2021-05-13 17:58:07+00:00
- **Updated**: 2021-05-13 17:58:07+00:00
- **Authors**: Zhongzheng Ren, Ishan Misra, Alexander G. Schwing, Rohit Girdhar
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: We introduce WyPR, a Weakly-supervised framework for Point cloud Recognition, requiring only scene-level class tags as supervision. WyPR jointly addresses three core 3D recognition tasks: point-level semantic segmentation, 3D proposal generation, and 3D object detection, coupling their predictions through self and cross-task consistency losses. We show that in conjunction with standard multiple-instance learning objectives, WyPR can detect and segment objects in point cloud data without access to any spatial labels at training time. We demonstrate its efficacy using the ScanNet and S3DIS datasets, outperforming prior state of the art on weakly-supervised segmentation by more than 6% mIoU. In addition, we set up the first benchmark for weakly-supervised 3D object detection on both datasets, where WyPR outperforms standard approaches and establishes strong baselines for future work.



### Self-Supervised Collision Handling via Generative 3D Garment Models for Virtual Try-On
- **Arxiv ID**: http://arxiv.org/abs/2105.06462v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06462v1)
- **Published**: 2021-05-13 17:58:20+00:00
- **Updated**: 2021-05-13 17:58:20+00:00
- **Authors**: Igor Santesteban, Nils Thuerey, Miguel A. Otaduy, Dan Casas
- **Comment**: Accepted to CVPR 2021. Project website
  http://mslab.es/projects/SelfSupervisedGarmentCollisions
- **Journal**: None
- **Summary**: We propose a new generative model for 3D garment deformations that enables us to learn, for the first time, a data-driven method for virtual try-on that effectively addresses garment-body collisions. In contrast to existing methods that require an undesirable postprocessing step to fix garment-body interpenetrations at test time, our approach directly outputs 3D garment configurations that do not collide with the underlying body. Key to our success is a new canonical space for garments that removes pose-and-shape deformations already captured by a new diffused human body model, which extrapolates body surface properties such as skinning weights and blendshapes to any 3D point. We leverage this representation to train a generative model with a novel self-supervised collision term that learns to reliably solve garment-body interpenetrations. We extensively evaluate and compare our results with recently proposed data-driven methods, and show that our method is the first to successfully address garment-body contact in unseen body shapes and motions, without compromising realism and detail.



### Contrastive Learning of Image Representations with Cross-Video Cycle-Consistency
- **Arxiv ID**: http://arxiv.org/abs/2105.06463v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06463v1)
- **Published**: 2021-05-13 17:59:11+00:00
- **Updated**: 2021-05-13 17:59:11+00:00
- **Authors**: Haiping Wu, Xiaolong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works have advanced the performance of self-supervised representation learning by a large margin. The core among these methods is intra-image invariance learning. Two different transformations of one image instance are considered as a positive sample pair, where various tasks are designed to learn invariant representations by comparing the pair. Analogically, for video data, representations of frames from the same video are trained to be closer than frames from other videos, i.e. intra-video invariance. However, cross-video relation has barely been explored for visual representation learning. Unlike intra-video invariance, ground-truth labels of cross-video relation is usually unavailable without human labors. In this paper, we propose a novel contrastive learning method which explores the cross-video relation by using cycle-consistency for general image representation learning. This allows to collect positive sample pairs across different video instances, which we hypothesize will lead to higher-level semantics. We validate our method by transferring our image representation to multiple downstream tasks including visual object tracking, image classification, and action recognition. We show significant improvement over state-of-the-art contrastive learning methods. Project page is available at https://happywu.github.io/cycle_contrast_video.



### DiscoBox: Weakly Supervised Instance Segmentation and Semantic Correspondence from Box Supervision
- **Arxiv ID**: http://arxiv.org/abs/2105.06464v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.06464v2)
- **Published**: 2021-05-13 17:59:41+00:00
- **Updated**: 2021-06-05 23:19:53+00:00
- **Authors**: Shiyi Lan, Zhiding Yu, Christopher Choy, Subhashree Radhakrishnan, Guilin Liu, Yuke Zhu, Larry S. Davis, Anima Anandkumar
- **Comment**: Tech Report
- **Journal**: None
- **Summary**: We introduce DiscoBox, a novel framework that jointly learns instance segmentation and semantic correspondence using bounding box supervision. Specifically, we propose a self-ensembling framework where instance segmentation and semantic correspondence are jointly guided by a structured teacher in addition to the bounding box supervision. The teacher is a structured energy model incorporating a pairwise potential and a cross-image potential to model the pairwise pixel relationships both within and across the boxes. Minimizing the teacher energy simultaneously yields refined object masks and dense correspondences between intra-class objects, which are taken as pseudo-labels to supervise the task network and provide positive/negative correspondence pairs for dense constrastive learning. We show a symbiotic relationship where the two tasks mutually benefit from each other. Our best model achieves 37.9% AP on COCO instance segmentation, surpassing prior weakly supervised methods and is competitive to supervised methods. We also obtain state of the art weakly supervised results on PASCAL VOC12 and PF-PASCAL with real-time inference.



### Editing Conditional Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2105.06466v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.06466v2)
- **Published**: 2021-05-13 17:59:48+00:00
- **Updated**: 2021-06-04 16:30:47+00:00
- **Authors**: Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, Bryan Russell
- **Comment**: Code: https://github.com/stevliu/editnerf Website:
  http://editnerf.csail.mit.edu/, v2 updated figure 8 and included additional
  details
- **Journal**: None
- **Summary**: A neural radiance field (NeRF) is a scene model supporting high-quality view synthesis, optimized per scene. In this paper, we explore enabling user editing of a category-level NeRF - also known as a conditional radiance field - trained on a shape category. Specifically, we introduce a method for propagating coarse 2D user scribbles to the 3D space, to modify the color or shape of a local region. First, we propose a conditional radiance field that incorporates new modular network components, including a shape branch that is shared across object instances. Observing multiple instances of the same category, our model learns underlying part semantics without any supervision, thereby allowing the propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair seat). Next, we propose a hybrid network update strategy that targets specific network components, which balances efficiency and accuracy. During user interaction, we formulate an optimization problem that both satisfies the user's constraints and preserves the original object structure. We demonstrate our approach on various editing tasks over three shape datasets and show that it outperforms prior neural editing approaches. Finally, we edit the appearance and shape of a real photograph and show that the edit propagates to extrapolated novel views.



### Dynamic View Synthesis from Dynamic Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/2105.06468v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06468v1)
- **Published**: 2021-05-13 17:59:50+00:00
- **Updated**: 2021-05-13 17:59:50+00:00
- **Authors**: Chen Gao, Ayush Saraf, Johannes Kopf, Jia-Bin Huang
- **Comment**: Project webpage: https://free-view-video.github.io/
- **Journal**: None
- **Summary**: We present an algorithm for generating novel views at arbitrary viewpoints and any input time step given a monocular video of a dynamic scene. Our work builds upon recent advances in neural implicit representation and uses continuous and differentiable functions for modeling the time-varying structure and the appearance of the scene. We jointly train a time-invariant static NeRF and a time-varying dynamic NeRF, and learn how to blend the results in an unsupervised manner. However, learning this implicit function from a single video is highly ill-posed (with infinitely many solutions that match the input video). To resolve the ambiguity, we introduce regularization losses to encourage a more physically plausible solution. We show extensive quantitative and qualitative results of dynamic view synthesis from casually captured videos.



### Deepfake Detection by Human Crowds, Machines, and Machine-informed Crowds
- **Arxiv ID**: http://arxiv.org/abs/2105.06496v2
- **DOI**: 10.1073/pnas.2110013119
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.06496v2)
- **Published**: 2021-05-13 18:22:16+00:00
- **Updated**: 2021-10-27 19:32:41+00:00
- **Authors**: Matthew Groh, Ziv Epstein, Chaz Firestone, Rosalind Picard
- **Comment**: None
- **Journal**: Proceedings of the National Academy of Sciences Jan 2022, 119 (1)
  e2110013119
- **Summary**: The recent emergence of machine-manipulated media raises an important societal question: how can we know if a video that we watch is real or fake? In two online studies with 15,016 participants, we present authentic videos and deepfakes and ask participants to identify which is which. We compare the performance of ordinary human observers against the leading computer vision deepfake detection model and find them similarly accurate while making different kinds of mistakes. Together, participants with access to the model's prediction are more accurate than either alone, but inaccurate model predictions often decrease participants' accuracy. To probe the relative strengths and weaknesses of humans and machines as detectors of deepfakes, we examine human and machine performance across video-level features, and we evaluate the impact of pre-registered randomized interventions on deepfake detection. We find that manipulations designed to disrupt visual processing of faces hinder human participants' performance while mostly not affecting the model's performance, suggesting a role for specialized cognitive capacities in explaining human deepfake detection performance.



### Internet of Things (IoT) Based Video Analytics: a use case of Smart Doorbell
- **Arxiv ID**: http://arxiv.org/abs/2105.06508v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.06508v2)
- **Published**: 2021-05-13 18:48:48+00:00
- **Updated**: 2021-09-13 11:49:16+00:00
- **Authors**: Shailesh Arya
- **Comment**: Need to derive more results on different IoT devices!
- **Journal**: None
- **Summary**: The vision of the internet of things (IoT) is a reality now. IoT devices are getting cheaper, smaller. They are becoming more and more computationally and energy-efficient. The global market of IoT-based video analytics has seen significant growth in recent years and it is expected to be a growing market segment. For any IoT-based video analytics application, few key points required, such as cost-effectiveness, widespread use, flexible design, accurate scene detection, reusability of the framework. Video-based smart doorbell system is one such application domain for video analytics where many commercial offerings are available in the consumer market. However, such existing offerings are costly, monolithic, and proprietary. Also, there will be a trade-off between accuracy and portability. To address the foreseen problems, I'm proposing a distributed framework for video analytics with a use case of a smart doorbell system. The proposed framework uses AWS cloud services as a base platform and to meet the price affordability constraint, the system was implemented on affordable Raspberry Pi. The smart doorbell will be able to recognize the known/unknown person with at most accuracy. The smart doorbell system is also having additional detection functionalities such as harmful weapon detection, noteworthy vehicle detection, animal/pet detection. An iOS application is specifically developed for this implementation which can receive the notification from the smart doorbell in real-time. Finally, the paper also mentions the classical approaches for video analytics, their feasibility in implementing with this use-case, and comparative analysis in terms of accuracy and time required to detect an object in the frame is carried out. Results conclude that AWS cloud-based approach is worthy for this smart doorbell use case.



### CrossRoI: Cross-camera Region of Interest Optimization for Efficient Real Time Video Analytics at Scale
- **Arxiv ID**: http://arxiv.org/abs/2105.06524v1
- **DOI**: 10.1145/3458305.3463381
- **Categories**: **cs.DC**, cs.CV, cs.MM, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/2105.06524v1)
- **Published**: 2021-05-13 19:29:14+00:00
- **Updated**: 2021-05-13 19:29:14+00:00
- **Authors**: Hongpeng Guo, Shuochao Yao, Zhe Yang, Qian Zhou, Klara Nahrstedt
- **Comment**: accepted in 12th ACM Multimedia Systems Conference (MMsys 21')
- **Journal**: None
- **Summary**: Video cameras are pervasively deployed in city scale for public good or community safety (i.e. traffic monitoring or suspected person tracking). However, analyzing large scale video feeds in real time is data intensive and poses severe challenges to network and computation systems today. We present CrossRoI, a resource-efficient system that enables real time video analytics at scale via harnessing the videos content associations and redundancy across a fleet of cameras. CrossRoI exploits the intrinsic physical correlations of cross-camera viewing fields to drastically reduce the communication and computation costs. CrossRoI removes the repentant appearances of same objects in multiple cameras without harming comprehensive coverage of the scene. CrossRoI operates in two phases - an offline phase to establish cross-camera correlations, and an efficient online phase for real time video inference. Experiments on real-world video feeds show that CrossRoI achieves 42% - 65% reduction for network overhead and 25% - 34% reduction for response delay in real time video analytics applications with more than 99% query accuracy, when compared to baseline methods. If integrated with SotA frame filtering systems, the performance gains of CrossRoI reach 50% - 80% (network overhead) and 33% - 61% (end-to-end delay).



### Network Architecture Search for Face Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2105.06528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06528v1)
- **Published**: 2021-05-13 19:46:05+00:00
- **Updated**: 2021-05-13 19:46:05+00:00
- **Authors**: Rajeev Yasarla, Hamid Reza Vaezi Joze, Vishal M Patel
- **Comment**: None
- **Journal**: None
- **Summary**: Various factors such as ambient lighting conditions, noise, motion blur, etc. affect the quality of captured face images. Poor quality face images often reduce the performance of face analysis and recognition systems. Hence, it is important to enhance the quality of face images collected in such conditions. We present a multi-task face restoration network, called Network Architecture Search for Face Enhancement (NASFE), which can enhance poor quality face images containing a single degradation (i.e. noise or blur) or multiple degradations (noise+blur+low-light). During training, NASFE uses clean face images of a person present in the degraded image to extract the identity information in terms of features for restoring the image. Furthermore, the network is guided by an identity-loss so that the identity in-formation is maintained in the restored image. Additionally, we propose a network architecture search-based fusion network in NASFE which fuses the task-specific features that are extracted using the task-specific encoders. We introduce FFT-op and deveiling operators in the fusion network to efficiently fuse the task-specific features. Comprehensive experiments on synthetic and real images demonstrate that the proposed method outperforms many recent state-of-the-art face restoration and enhancement methods in terms of quantitative and visual performance.



### Stroke Lesion Segmentation with Visual Cortex Anatomy Alike Neural Nets
- **Arxiv ID**: http://arxiv.org/abs/2105.06544v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.06544v2)
- **Published**: 2021-05-13 20:39:29+00:00
- **Updated**: 2021-05-23 07:01:06+00:00
- **Authors**: Chuanlong Li
- **Comment**: Update the segmentation examples figure (Fig. 7); Add the
  implementation link; Language related issues
- **Journal**: None
- **Summary**: Cerebrovascular accident, or commonly known as stroke, is an acute disease with extreme impact on patients and healthcare systems and is the second largest cause of death worldwide. Fast and precise stroke lesion detection and location is an extreme important process with regards to stroke diagnosis, treatment, and prognosis. Except from the manual segmentation approach, machine learning based segmentation methods are the most promising ones when considering efficiency and accuracy, and convolutional neural network based models are the first of its kind. However, most of these neural network models do not really align with the brain anatomical structures. Intuitively, this work presents a more brain alike model which mimics the anatomical structure of the human visual cortex. Through the preliminary experiments on the stroke lesion segmentation task, the proposed model is found to be able to perform equally well or better to the de-facto standard U-Net. Part of the implementation will be made available at https://github.com/DarkoBomer/VCA-Net.



### SpikeMS: Deep Spiking Neural Network for Motion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.06562v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.06562v1)
- **Published**: 2021-05-13 21:34:55+00:00
- **Updated**: 2021-05-13 21:34:55+00:00
- **Authors**: Chethan M. Parameshwara, Simin Li, Cornelia Fermüller, Nitin J. Sanket, Matthew S. Evanusa, Yiannis Aloimonos
- **Comment**: 7 pages, 6 figures, 3 tables, Under review IROS 2021
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNN) are the so-called third generation of neural networks which attempt to more closely match the functioning of the biological brain. They inherently encode temporal data, allowing for training with less energy usage and can be extremely energy efficient when coded on neuromorphic hardware. In addition, they are well suited for tasks involving event-based sensors, which match the event-based nature of the SNN. However, SNNs have not been as effectively applied to real-world, large-scale tasks as standard Artificial Neural Networks (ANNs) due to the algorithmic and training complexity. To exacerbate the situation further, the input representation is unconventional and requires careful analysis and deep understanding. In this paper, we propose \textit{SpikeMS}, the first deep encoder-decoder SNN architecture for the real-world large-scale problem of motion segmentation using the event-based DVS camera as input. To accomplish this, we introduce a novel spatio-temporal loss formulation that includes both spike counts and classification labels in conjunction with the use of new techniques for SNN backpropagation. In addition, we show that \textit{SpikeMS} is capable of \textit{incremental predictions}, or predictions from smaller amounts of test data than it is trained on. This is invaluable for providing outputs even with partial input data for low-latency applications and those requiring fast predictions. We evaluated \textit{SpikeMS} on challenging synthetic and real-world sequences from EV-IMO, EED and MOD datasets and achieving results on a par with a comparable ANN method, but using potentially 50 times less power.



### Handwriting Recognition with Novelty
- **Arxiv ID**: http://arxiv.org/abs/2105.06582v2
- **DOI**: 10.1007/978-3-030-86337-1_33
- **Categories**: **cs.CV**, I.7.5; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2105.06582v2)
- **Published**: 2021-05-13 23:01:07+00:00
- **Updated**: 2021-05-17 20:14:41+00:00
- **Authors**: Derek S. Prijatelj, Samuel Grieggs, Futoshi Yumoto, Eric Robertson, Walter J. Scheirer
- **Comment**: 16 pages, 3 Figures, 2 Tables, To be published in ICDAR 2021.
  Camera-ready version 1. Supplementary Material 22 pages, 4 Figures, 18
  Tables. Moved novelty type examples from supp mat to main. Added brief
  explanation of usefulness of formalization. Added comment on joint
  information between transcription and style tasks in CRNN's encoding
- **Journal**: None
- **Summary**: This paper introduces an agent-centric approach to handle novelty in the visual recognition domain of handwriting recognition (HWR). An ideal transcription agent would rival or surpass human perception, being able to recognize known and new characters in an image, and detect any stylistic changes that may occur within or across documents. A key confound is the presence of novelty, which has continued to stymie even the best machine learning-based algorithms for these tasks. In handwritten documents, novelty can be a change in writer, character attributes, writing attributes, or overall document appearance, among other things. Instead of looking at each aspect independently, we suggest that an integrated agent that can process known characters and novelties simultaneously is a better strategy. This paper formalizes the domain of handwriting recognition with novelty, describes a baseline agent, introduces an evaluation protocol with benchmark data, and provides experimentation to set the state-of-the-art. Results show feasibility for the agent-centric approach, but more work is needed to approach human-levels of reading ability, giving the HWR community a formal basis to build upon as they solve this challenging problem.



