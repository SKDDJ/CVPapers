# Arxiv Papers in cs.CV on 2021-05-10
### Reconstructive Sequence-Graph Network for Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/2105.04066v1
- **DOI**: 10.1109/TPAMI.2021.3072117
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.04066v1)
- **Published**: 2021-05-10 01:47:55+00:00
- **Updated**: 2021-05-10 01:47:55+00:00
- **Authors**: Bin Zhao, Haopeng Li, Xiaoqiang Lu, Xuelong Li
- **Comment**: Accepted by IEEE TPAMI 2021
- **Journal**: None
- **Summary**: Exploiting the inner-shot and inter-shot dependencies is essential for key-shot based video summarization. Current approaches mainly devote to modeling the video as a frame sequence by recurrent neural networks. However, one potential limitation of the sequence models is that they focus on capturing local neighborhood dependencies while the high-order dependencies in long distance are not fully exploited. In general, the frames in each shot record a certain activity and vary smoothly over time, but the multi-hop relationships occur frequently among shots. In this case, both the local and global dependencies are important for understanding the video content. Motivated by this point, we propose a Reconstructive Sequence-Graph Network (RSGN) to encode the frames and shots as sequence and graph hierarchically, where the frame-level dependencies are encoded by Long Short-Term Memory (LSTM), and the shot-level dependencies are captured by the Graph Convolutional Network (GCN). Then, the videos are summarized by exploiting both the local and global dependencies among shots. Besides, a reconstructor is developed to reward the summary generator, so that the generator can be optimized in an unsupervised manner, which can avert the lack of annotated data in video summarization. Furthermore, under the guidance of reconstruction loss, the predicted summary can better preserve the main video content and shot-level dependencies. Practically, the experimental results on three popular datasets i.e., SumMe, TVsum and VTW) have demonstrated the superiority of our proposed approach to the summarization task.



### Robust Training Using Natural Transformation
- **Arxiv ID**: http://arxiv.org/abs/2105.04070v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.04070v1)
- **Published**: 2021-05-10 01:56:03+00:00
- **Updated**: 2021-05-10 01:56:03+00:00
- **Authors**: Shuo Wang, Lingjuan Lyu, Surya Nepal, Carsten Rudolph, Marthie Grobler, Kristen Moore
- **Comment**: arXiv admin note: text overlap with arXiv:1912.03192,
  arXiv:2004.02546 by other authors
- **Journal**: None
- **Summary**: Previous robustness approaches for deep learning models such as data augmentation techniques via data transformation or adversarial training cannot capture real-world variations that preserve the semantics of the input, such as a change in lighting conditions. To bridge this gap, we present NaTra, an adversarial training scheme that is designed to improve the robustness of image classification algorithms. We target attributes of the input images that are independent of the class identification, and manipulate those attributes to mimic real-world natural transformations (NaTra) of the inputs, which are then used to augment the training dataset of the image classifier. Specifically, we apply \textit{Batch Inverse Encoding and Shifting} to map a batch of given images to corresponding disentangled latent codes of well-trained generative models. \textit{Latent Codes Expansion} is used to boost image reconstruction quality through the incorporation of extended feature maps. \textit{Unsupervised Attribute Directing and Manipulation} enables identification of the latent directions that correspond to specific attribute changes, and then produce interpretable manipulations of those attributes, thereby generating natural transformations to the input data. We demonstrate the efficacy of our scheme by utilizing the disentangled latent representations derived from well-trained GANs to mimic transformations of an image that are similar to real-world natural variations (such as lighting conditions or hairstyle), and train models to be invariant to these natural transformations. Extensive experiments show that our method improves generalization of classification models and increases its robustness to various real-world distortions



### CFPNet-M: A Light-Weight Encoder-Decoder Based Network for Multimodal Biomedical Image Real-Time Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.04075v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.04075v2)
- **Published**: 2021-05-10 02:29:11+00:00
- **Updated**: 2021-05-30 15:07:16+00:00
- **Authors**: Ange Lou, Shuyue Guan, Murray Loew
- **Comment**: None
- **Journal**: None
- **Summary**: Currently, developments of deep learning techniques are providing instrumental to identify, classify, and quantify patterns in medical images. Segmentation is one of the important applications in medical image analysis. In this regard, U-Net is the predominant approach to medical image segmentation tasks. However, we found that those U-Net based models have limitations in several aspects, for example, millions of parameters in the U-Net consuming considerable computation resource and memory, lack of global information, and missing some tough objects. Therefore, we applied two modifications to improve the U-Net model: 1) designed and added the dilated channel-wise CNN module, 2) simplified the U shape network. Based on these two modifications, we proposed a novel light-weight architecture -- Channel-wise Feature Pyramid Network for Medicine (CFPNet-M). To evaluate our method, we selected five datasets with different modalities: thermography, electron microscopy, endoscopy, dermoscopy, and digital retinal images. And we compared its performance with several models having different parameter scales. This paper also involves our previous studies of DC-UNet and some commonly used light-weight neural networks. We applied the Tanimoto similarity instead of the Jaccard index for gray-level image measurements. By comparison, CFPNet-M achieves comparable segmentation results on all five medical datasets with only 0.65 million parameters, which is about 2% of U-Net, and 8.8 MB memory. Meanwhile, the inference speed can reach 80 FPS on a single RTX 2070Ti GPU with the 256 by 192 pixels input size.



### Self-supervised spectral matching network for hyperspectral target detection
- **Arxiv ID**: http://arxiv.org/abs/2105.04078v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04078v1)
- **Published**: 2021-05-10 02:32:58+00:00
- **Updated**: 2021-05-10 02:32:58+00:00
- **Authors**: Can Yao, Yuan Yuan, Zhiyu Jiang
- **Comment**: IGARSS 2021
- **Journal**: None
- **Summary**: Hyperspectral target detection is a pixel-level recognition problem. Given a few target samples, it aims to identify the specific target pixels such as airplane, vehicle, ship, from the entire hyperspectral image. In general, the background pixels take the majority of the image and complexly distributed. As a result, the datasets are weak annotated and extremely imbalanced. To address these problems, a spectral mixing based self-supervised paradigm is designed for hyperspectral data to obtain an effective feature representation. The model adopts a spectral similarity based matching network framework. In order to learn more discriminative features, a pair-based loss is adopted to minimize the distance between target pixels while maximizing the distances between target and background. Furthermore, through a background separated step, the complex unlabeled spectra are downsampled into different sub-categories. The experimental results on three real hyperspectral datasets demonstrate that the proposed framework achieves better results compared with the existing detectors.



### Elastic Weight Consolidation (EWC): Nuts and Bolts
- **Arxiv ID**: http://arxiv.org/abs/2105.04093v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2105.04093v1)
- **Published**: 2021-05-10 03:48:55+00:00
- **Updated**: 2021-05-10 03:48:55+00:00
- **Authors**: Abhishek Aich
- **Comment**: None
- **Journal**: None
- **Summary**: In this report, we present a theoretical support of the continual learning method \textbf{Elastic Weight Consolidation}, introduced in paper titled `Overcoming catastrophic forgetting in neural networks'. Being one of the most cited paper in regularized methods for continual learning, this report disentangles the underlying concept of the proposed objective function. We assume that the reader is aware of the basic terminologies of continual learning.



### Deep feature selection-and-fusion for RGB-D semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.04102v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04102v1)
- **Published**: 2021-05-10 04:02:32+00:00
- **Updated**: 2021-05-10 04:02:32+00:00
- **Authors**: Yuejiao Su, Yuan Yuan, Zhiyu Jiang
- **Comment**: ICME 2021
- **Journal**: None
- **Summary**: Scene depth information can help visual information for more accurate semantic segmentation. However, how to effectively integrate multi-modality information into representative features is still an open problem. Most of the existing work uses DCNNs to implicitly fuse multi-modality information. But as the network deepens, some critical distinguishing features may be lost, which reduces the segmentation performance. This work proposes a unified and efficient feature selectionand-fusion network (FSFNet), which contains a symmetric cross-modality residual fusion module used for explicit fusion of multi-modality information. Besides, the network includes a detailed feature propagation module, which is used to maintain low-level detailed information during the forward process of the network. Compared with the state-of-the-art methods, experimental evaluations demonstrate that the proposed model achieves competitive performance on two public datasets.



### Multi-Agent Semi-Siamese Training for Long-tail and Shallow Face Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.04113v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2105.04113v2)
- **Published**: 2021-05-10 04:57:32+00:00
- **Updated**: 2022-01-24 10:15:39+00:00
- **Authors**: Hailin Shi, Dan Zeng, Yichun Tai, Hang Du, Yibo Hu, Zicheng Zhang, Tao Mei
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: With the recent development of deep convolutional neural networks and large-scale datasets, deep face recognition has made remarkable progress and been widely used in various applications. However, unlike the existing public face datasets, in many real-world scenarios of face recognition, the depth of training dataset is shallow, which means only two face images are available for each ID. With the non-uniform increase of samples, such issue is converted to a more general case, a.k.a long-tail face learning, which suffers from data imbalance and intra-class diversity dearth simultaneously. These adverse conditions damage the training and result in the decline of model performance. Based on the Semi-Siamese Training (SST), we introduce an advanced solution, named Multi-Agent Semi-Siamese Training (MASST), to address these problems. MASST includes a probe network and multiple gallery agents, the former aims to encode the probe features, and the latter constitutes a stack of networks that encode the prototypes (gallery features). For each training iteration, the gallery network, which is sequentially rotated from the stack, and the probe network form a pair of semi-siamese networks. We give the theoretical and empirical analysis that, given the long-tail (or shallow) data and training loss, MASST smooths the loss landscape and satisfies the Lipschitz continuity with the help of multiple agents and the updating gallery queue. The proposed method is out of extra-dependency, thus can be easily integrated with the existing loss functions and network architectures. It is worth noting that, although multiple gallery agents are employed for training, only the probe network is needed for inference, without increasing the inference cost. Extensive experiments and comparisons demonstrate the advantages of MASST for long-tail and shallow face learning.



### Examining and Mitigating Kernel Saturation in Convolutional Neural Networks using Negative Images
- **Arxiv ID**: http://arxiv.org/abs/2105.04128v1
- **DOI**: 10.1109/IECON43393.2020.9255147
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2105.04128v1)
- **Published**: 2021-05-10 06:06:49+00:00
- **Updated**: 2021-05-10 06:06:49+00:00
- **Authors**: Nidhi Gowdra, Roopak Sinha, Stephen MacDonell
- **Comment**: Conference paper, 6 pages, 3 figures, 1 table
- **Journal**: Proceedings of the 46th Annual Conference of the IEEE Industrial
  Electronics Society (IECON2020). IEEE Computer Society Press, pp.465-470
- **Summary**: Neural saturation in Deep Neural Networks (DNNs) has been studied extensively, but remains relatively unexplored in Convolutional Neural Networks (CNNs). Understanding and alleviating the effects of convolutional kernel saturation is critical for enhancing CNN models classification accuracies. In this paper, we analyze the effect of convolutional kernel saturation in CNNs and propose a simple data augmentation technique to mitigate saturation and increase classification accuracy, by supplementing negative images to the training dataset. We hypothesize that greater semantic feature information can be extracted using negative images since they have the same structural information as standard images but differ in their data representations. Varied data representations decrease the probability of kernel saturation and thus increase the effectiveness of kernel weight updates. The two datasets selected to evaluate our hypothesis were CIFAR- 10 and STL-10 as they have similar image classes but differ in image resolutions thus making for a better understanding of the saturation phenomenon. MNIST dataset was used to highlight the ineffectiveness of the technique for linearly separable data. The ResNet CNN architecture was chosen since the skip connections in the network ensure the most important features contributing the most to classification accuracy are retained. Our results show that CNNs are indeed susceptible to convolutional kernel saturation and that supplementing negative images to the training dataset can offer a statistically significant increase in classification accuracies when compared against models trained on the original datasets. Our results present accuracy increases of 6.98% and 3.16% on the STL-10 and CIFAR-10 datasets respectively.



### An Attention-Fused Network for Semantic Segmentation of Very-High-Resolution Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/2105.04132v2
- **DOI**: 10.1016/j.isprsjprs.2021.05.004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04132v2)
- **Published**: 2021-05-10 06:23:27+00:00
- **Updated**: 2021-05-28 11:21:24+00:00
- **Authors**: Xuan Yang, Shanshan Li, Zhengchao Chen, Jocelyn Chanussot, Xiuping Jia, Bing Zhang, Baipeng Li, Pan Chen
- **Comment**: 35 pages. Published by ISPRS Journal of Photogrammetry and Remote
  Sensing
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, 177: 238-262,
  2021
- **Summary**: Semantic segmentation is an essential part of deep learning. In recent years, with the development of remote sensing big data, semantic segmentation has been increasingly used in remote sensing. Deep convolutional neural networks (DCNNs) face the challenge of feature fusion: very-high-resolution remote sensing image multisource data fusion can increase the network's learnable information, which is conducive to correctly classifying target objects by DCNNs; simultaneously, the fusion of high-level abstract features and low-level spatial features can improve the classification accuracy at the border between target objects. In this paper, we propose a multipath encoder structure to extract features of multipath inputs, a multipath attention-fused block module to fuse multipath features, and a refinement attention-fused block module to fuse high-level abstract features and low-level spatial features. Furthermore, we propose a novel convolutional neural network architecture, named attention-fused network (AFNet). Based on our AFNet, we achieve state-of-the-art performance with an overall accuracy of 91.7% and a mean F1 score of 90.96% on the ISPRS Vaihingen 2D dataset and an overall accuracy of 92.1% and a mean F1 score of 93.44% on the ISPRS Potsdam 2D dataset.



### Coupling Intent and Action for Pedestrian Crossing Behavior Prediction
- **Arxiv ID**: http://arxiv.org/abs/2105.04133v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04133v1)
- **Published**: 2021-05-10 06:26:25+00:00
- **Updated**: 2021-05-10 06:26:25+00:00
- **Authors**: Yu Yao, Ella Atkins, Matthew Johnson Roberson, Ram Vasudevan, Xiaoxiao Du
- **Comment**: 7pages, 4 figures, 3 tables. Accepted to IJCAI2021
- **Journal**: None
- **Summary**: Accurate prediction of pedestrian crossing behaviors by autonomous vehicles can significantly improve traffic safety. Existing approaches often model pedestrian behaviors using trajectories or poses but do not offer a deeper semantic interpretation of a person's actions or how actions influence a pedestrian's intention to cross in the future. In this work, we follow the neuroscience and psychological literature to define pedestrian crossing behavior as a combination of an unobserved inner will (a probabilistic representation of binary intent of crossing vs. not crossing) and a set of multi-class actions (e.g., walking, standing, etc.). Intent generates actions, and the future actions in turn reflect the intent. We present a novel multi-task network that predicts future pedestrian actions and uses predicted future action as a prior to detect the present intent and action of the pedestrian. We also designed an attention relation network to incorporate external environmental contexts thus further improve intent and action detection performance. We evaluated our approach on two naturalistic driving datasets, PIE and JAAD, and extensive experiments show significantly improved and more explainable results for both intent detection and action prediction over state-of-the-art approaches. Our code is available at: https://github.com/umautobots/pedestrian_intent_action_detection.



### Matching Visual Features to Hierarchical Semantic Topics for Image Paragraph Captioning
- **Arxiv ID**: http://arxiv.org/abs/2105.04143v2
- **DOI**: 10.1007/s11263-022-01624-6
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2105.04143v2)
- **Published**: 2021-05-10 06:55:39+00:00
- **Updated**: 2022-07-26 02:26:47+00:00
- **Authors**: Dandan Guo, Ruiying Lu, Bo Chen, Zequn Zeng, Mingyuan Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Observing a set of images and their corresponding paragraph-captions, a challenging task is to learn how to produce a semantically coherent paragraph to describe the visual content of an image. Inspired by recent successes in integrating semantic topics into this task, this paper develops a plug-and-play hierarchical-topic-guided image paragraph generation framework, which couples a visual extractor with a deep topic model to guide the learning of a language model. To capture the correlations between the image and text at multiple levels of abstraction and learn the semantic topics from images, we design a variational inference network to build the mapping from image features to textual captions. To guide the paragraph generation, the learned hierarchical topics and visual features are integrated into the language model, including Long Short-Term Memory (LSTM) and Transformer, and jointly optimized. Experiments on public datasets demonstrate that the proposed models, which are competitive with many state-of-the-art approaches in terms of standard evaluation metrics, can be used to both distill interpretable multi-layer semantic topics and generate diverse and coherent captions. We release our code at https://github.com/DandanGuo1993/VTCM-based-image-paragraph-caption.git



### Unsupervised Human Pose Estimation through Transforming Shape Templates
- **Arxiv ID**: http://arxiv.org/abs/2105.04154v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.04154v1)
- **Published**: 2021-05-10 07:15:56+00:00
- **Updated**: 2021-05-10 07:15:56+00:00
- **Authors**: Luca Schmidtke, Athanasios Vlontzos, Simon Ellershaw, Anna Lukens, Tomoki Arichi, Bernhard Kainz
- **Comment**: CVPR 2021 (poster). Project page: https://infantmotion.github.io/
- **Journal**: None
- **Summary**: Human pose estimation is a major computer vision problem with applications ranging from augmented reality and video capture to surveillance and movement tracking. In the medical context, the latter may be an important biomarker for neurological impairments in infants. Whilst many methods exist, their application has been limited by the need for well annotated large datasets and the inability to generalize to humans of different shapes and body compositions, e.g. children and infants. In this paper we present a novel method for learning pose estimators for human adults and infants in an unsupervised fashion. We approach this as a learnable template matching problem facilitated by deep feature extractors. Human-interpretable landmarks are estimated by transforming a template consisting of predefined body parts that are characterized by 2D Gaussian distributions. Enforcing a connectivity prior guides our model to meaningful human shape representations. We demonstrate the effectiveness of our approach on two different datasets including adults and infants.



### Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2105.04165v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.FL
- **Links**: [PDF](http://arxiv.org/pdf/2105.04165v3)
- **Published**: 2021-05-10 07:46:55+00:00
- **Updated**: 2021-07-20 23:22:27+00:00
- **Authors**: Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, Song-Chun Zhu
- **Comment**: Accepted to ACL 2021, 13 pages, 6 figures
- **Journal**: None
- **Summary**: Geometry problem solving has attracted much attention in the NLP community recently. The task is challenging as it requires abstract problem understanding and symbolic reasoning with axiomatic knowledge. However, current datasets are either small in scale or not publicly available. Thus, we construct a new large-scale benchmark, Geometry3K, consisting of 3,002 geometry problems with dense annotation in formal language. We further propose a novel geometry solving approach with formal language and symbolic reasoning, called Interpretable Geometry Problem Solver (Inter-GPS). Inter-GPS first parses the problem text and diagram into formal language automatically via rule-based text parsing and neural object detecting, respectively. Unlike implicit learning in existing methods, Inter-GPS incorporates theorem knowledge as conditional rules and performs symbolic reasoning step by step. Also, a theorem predictor is designed to infer the theorem application sequence fed to the symbolic solver for the more efficient and reasonable searching path. Extensive experiments on the Geometry3K and GEOS datasets demonstrate that Inter-GPS achieves significant improvements over existing methods. The project with code and data is available at https://lupantech.github.io/inter-gps.



### PillarSegNet: Pillar-based Semantic Grid Map Estimation using Sparse LiDAR Data
- **Arxiv ID**: http://arxiv.org/abs/2105.04169v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.04169v2)
- **Published**: 2021-05-10 08:03:11+00:00
- **Updated**: 2021-07-05 21:24:22+00:00
- **Authors**: Juncong Fei, Kunyu Peng, Philipp Heidenreich, Frank Bieder, Christoph Stiller
- **Comment**: Accepted to present in the 2021 IEEE Intelligent Vehicles Symposium
  (IV21)
- **Journal**: None
- **Summary**: Semantic understanding of the surrounding environment is essential for automated vehicles. The recent publication of the SemanticKITTI dataset stimulates the research on semantic segmentation of LiDAR point clouds in urban scenarios. While most existing approaches predict sparse pointwise semantic classes for the sparse input LiDAR scan, we propose PillarSegNet to be able to output a dense semantic grid map. In contrast to a previously proposed grid map method, PillarSegNet uses PointNet to learn features directly from the 3D point cloud and then conducts 2D semantic segmentation in the top view. To train and evaluate our approach, we use both sparse and dense ground truth, where the dense ground truth is obtained from multiple superimposed scans. Experimental results on the SemanticKITTI dataset show that PillarSegNet achieves a performance gain of about 10% mIoU over the state-of-the-art grid map method.



### KDExplainer: A Task-oriented Attention Model for Explaining Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2105.04181v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04181v2)
- **Published**: 2021-05-10 08:15:26+00:00
- **Updated**: 2021-05-12 11:54:17+00:00
- **Authors**: Mengqi Xue, Jie Song, Xinchao Wang, Ying Chen, Xingen Wang, Mingli Song
- **Comment**: 7 pages, 4 figures, accepted to IJCAI 2021
- **Journal**: None
- **Summary**: Knowledge distillation (KD) has recently emerged as an efficacious scheme for learning compact deep neural networks (DNNs). Despite the promising results achieved, the rationale that interprets the behavior of KD has yet remained largely understudied. In this paper, we introduce a novel task-oriented attention model, termed as KDExplainer, to shed light on the working mechanism underlying the vanilla KD. At the heart of KDExplainer is a Hierarchical Mixture of Experts (HME), in which a multi-class classification is reformulated as a multi-task binary one. Through distilling knowledge from a free-form pre-trained DNN to KDExplainer, we observe that KD implicitly modulates the knowledge conflicts between different subtasks, and in reality has much more to offer than label smoothing. Based on such findings, we further introduce a portable tool, dubbed as virtual attention module (VAM), that can be seamlessly integrated with various DNNs to enhance their performance under KD. Experimental results demonstrate that with a negligible additional cost, student models equipped with VAM consistently outperform their non-VAM counterparts across different benchmarks. Furthermore, when combined with other KD methods, VAM remains competent in promoting results, even though it is only motivated by vanilla KD. The code is available at https://github.com/zju-vipa/KDExplainer.



### The Modulo Radon Transform: Theory, Algorithms and Applications
- **Arxiv ID**: http://arxiv.org/abs/2105.04194v1
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, eess.SP, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2105.04194v1)
- **Published**: 2021-05-10 08:38:48+00:00
- **Updated**: 2021-05-10 08:38:48+00:00
- **Authors**: Matthias Beckmann, Ayush Bhandari, Felix Krahmer
- **Comment**: 32 pages, submitted for possible publication
- **Journal**: None
- **Summary**: Recently, experiments have been reported where researchers were able to perform high dynamic range (HDR) tomography in a heuristic fashion, by fusing multiple tomographic projections. This approach to HDR tomography has been inspired by HDR photography and inherits the same disadvantages. Taking a computational imaging approach to the HDR tomography problem, we here suggest a new model based on the Modulo Radon Transform (MRT), which we rigorously introduce and analyze. By harnessing a joint design between hardware and algorithms, we present a single-shot HDR tomography approach, which to our knowledge, is the only approach that is backed by mathematical guarantees.   On the hardware front, instead of recording the Radon Transform projections that my potentially saturate, we propose to measure modulo values of the same. This ensures that the HDR measurements are folded into a lower dynamic range. On the algorithmic front, our recovery algorithms reconstruct the HDR images from folded measurements. Beyond mathematical aspects such as injectivity and inversion of the MRT for different scenarios including band-limited and approximately compactly supported images, we also provide a first proof-of-concept demonstration. To do so, we implement MRT by experimentally folding tomographic measurements available as an open source data set using our custom designed modulo hardware. Our reconstruction clearly shows the advantages of our approach for experimental data. In this way, our MRT based solution paves a path for HDR acquisition in a number of related imaging problems.



### You Only Learn One Representation: Unified Network for Multiple Tasks
- **Arxiv ID**: http://arxiv.org/abs/2105.04206v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04206v1)
- **Published**: 2021-05-10 09:03:11+00:00
- **Updated**: 2021-05-10 09:03:11+00:00
- **Authors**: Chien-Yao Wang, I-Hau Yeh, Hong-Yuan Mark Liao
- **Comment**: None
- **Journal**: None
- **Summary**: People ``understand'' the world via vision, hearing, tactile, and also the past experience. Human experience can be learned through normal learning (we call it explicit knowledge), or subconsciously (we call it implicit knowledge). These experiences learned through normal learning or subconsciously will be encoded and stored in the brain. Using these abundant experience as a huge database, human beings can effectively process data, even they were unseen beforehand. In this paper, we propose a unified network to encode implicit knowledge and explicit knowledge together, just like the human brain can learn knowledge from normal learning as well as subconsciousness learning. The unified network can generate a unified representation to simultaneously serve various tasks. We can perform kernel space alignment, prediction refinement, and multi-task learning in a convolutional neural network. The results demonstrate that when implicit knowledge is introduced into the neural network, it benefits the performance of all tasks. We further analyze the implicit representation learnt from the proposed unified network, and it shows great capability on catching the physical meaning of different tasks. The source code of this work is at : https://github.com/WongKinYiu/yolor.



### Action Shuffling for Weakly Supervised Temporal Localization
- **Arxiv ID**: http://arxiv.org/abs/2105.04208v1
- **DOI**: 10.1109/TIP.2022.3185485
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04208v1)
- **Published**: 2021-05-10 09:05:58+00:00
- **Updated**: 2021-05-10 09:05:58+00:00
- **Authors**: Xiao-Yu Zhang, Haichao Shi, Changsheng Li, Xinchu Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly supervised action localization is a challenging task with extensive applications, which aims to identify actions and the corresponding temporal intervals with only video-level annotations available. This paper analyzes the order-sensitive and location-insensitive properties of actions, and embodies them into a self-augmented learning framework to improve the weakly supervised action localization performance. To be specific, we propose a novel two-branch network architecture with intra/inter-action shuffling, referred to as ActShufNet. The intra-action shuffling branch lays out a self-supervised order prediction task to augment the video representation with inner-video relevance, whereas the inter-action shuffling branch imposes a reorganizing strategy on the existing action contents to augment the training set without resorting to any external resources. Furthermore, the global-local adversarial training is presented to enhance the model's robustness to irrelevant noises. Extensive experiments are conducted on three benchmark datasets, and the results clearly demonstrate the efficacy of the proposed method.



### Temporal-Spatial Feature Pyramid for Video Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.04213v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04213v2)
- **Published**: 2021-05-10 09:14:14+00:00
- **Updated**: 2021-09-14 00:40:20+00:00
- **Authors**: Qinyao Chang, Shiping Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-level features are important for saliency detection. Better combination and use of multi-level features with time information can greatly improve the accuracy of the video saliency model. In order to fully combine multi-level features and make it serve the video saliency model, we propose a 3D fully convolutional encoder-decoder architecture for video saliency detection, which combines scale, space and time information for video saliency modeling. The encoder extracts multi-scale temporal-spatial features from the input continuous video frames, and then constructs temporal-spatial feature pyramid through temporal-spatial convolution and top-down feature integration. The decoder performs hierarchical decoding of temporal-spatial features from different scales, and finally produces a saliency map from the integration of multiple video frames. Our model is simple yet effective, and can run in real time. We perform abundant experiments, and the results indicate that the well-designed structure can improve the precision of video saliency detection significantly. Experimental results on three purely visual video saliency benchmarks and six audio-video saliency benchmarks demonstrate that our method outperforms the existing state-of-the-art methods.



### Event-LSTM: An Unsupervised and Asynchronous Learning-based Representation for Event-based Data
- **Arxiv ID**: http://arxiv.org/abs/2105.04216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04216v1)
- **Published**: 2021-05-10 09:18:52+00:00
- **Updated**: 2021-05-10 09:18:52+00:00
- **Authors**: Lakshmi Annamalai, Vignesh Ramanathan, Chetan Singh Thakur
- **Comment**: 7 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: Event cameras are activity-driven bio-inspired vision sensors, thereby resulting in advantages such as sparsity,high temporal resolution, low latency, and power consumption. Given the different sensing modality of event camera and high quality of conventional vision paradigm, event processing is predominantly solved by transforming the sparse and asynchronous events into 2D grid and subsequently applying standard vision pipelines. Despite the promising results displayed by supervised learning approaches in 2D grid generation, these approaches treat the task in supervised manner. Labeled task specific ground truth event data is challenging to acquire. To overcome this limitation, we propose Event-LSTM, an unsupervised Auto-Encoder architecture made up of LSTM layers as a promising alternative to learn 2D grid representation from event sequence. Compared to competing supervised approaches, ours is a task-agnostic approach ideally suited for the event domain, where task specific labeled data is scarce. We also tailor the proposed solution to exploit asynchronous nature of event stream, which gives it desirable charateristics such as speed invariant and energy-efficient 2D grid generation. Besides, we also push state-of-the-art event de-noising forward by introducing memory into the de-noising process. Evaluations on activity recognition and gesture recognition demonstrate that our approach yields improvement over state-of-the-art approaches, while providing the flexibilty to learn from unlabelled data.



### De-homogenization using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.04232v1
- **DOI**: 10.1016/j.cma.2021.114197
- **Categories**: **cs.LG**, cs.CV, J.6; I.4.9; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2105.04232v1)
- **Published**: 2021-05-10 09:50:06+00:00
- **Updated**: 2021-05-10 09:50:06+00:00
- **Authors**: Martin O. Elingaard, Niels Aage, J. Andreas Bærentzen, Ole Sigmund
- **Comment**: 28 pages, 16 figures
- **Journal**: None
- **Summary**: This paper presents a deep learning-based de-homogenization method for structural compliance minimization. By using a convolutional neural network to parameterize the mapping from a set of lamination parameters on a coarse mesh to a one-scale design on a fine mesh, we avoid solving the least square problems associated with traditional de-homogenization approaches and save time correspondingly. To train the neural network, a two-step custom loss function has been developed which ensures a periodic output field that follows the local lamination orientations. A key feature of the proposed method is that the training is carried out without any use of or reference to the underlying structural optimization problem, which renders the proposed method robust and insensitive wrt. domain size, boundary conditions, and loading. A post-processing procedure utilizing a distance transform on the output field skeleton is used to project the desired lamination widths onto the output field while ensuring a predefined minimum length-scale and volume fraction. To demonstrate that the deep learning approach has excellent generalization properties, numerical examples are shown for several different load and boundary conditions. For an appropriate choice of parameters, the de-homogenized designs perform within $7-25\%$ of the homogenization-based solution at a fraction of the computational cost. With several options for further improvements, the scheme may provide the basis for future interactive high-resolution topology optimization.



### T-EMDE: Sketching-based global similarity for cross-modal retrieval
- **Arxiv ID**: http://arxiv.org/abs/2105.04242v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, I.5.1; I.5.4; I.2.7; I.2.10; H.3.3
- **Links**: [PDF](http://arxiv.org/pdf/2105.04242v1)
- **Published**: 2021-05-10 10:14:32+00:00
- **Updated**: 2021-05-10 10:14:32+00:00
- **Authors**: Barbara Rychalska, Mikolaj Wieczorek, Jacek Dabrowski
- **Comment**: 10 pages,5 figures, 4 tables, 1 code snippet
- **Journal**: None
- **Summary**: The key challenge in cross-modal retrieval is to find similarities between objects represented with different modalities, such as image and text. However, each modality embeddings stem from non-related feature spaces, which causes the notorious 'heterogeneity gap'. Currently, many cross-modal systems try to bridge the gap with self-attention. However, self-attention has been widely criticized for its quadratic complexity, which prevents many real-life applications. In response to this, we propose T-EMDE - a neural density estimator inspired by the recently introduced Efficient Manifold Density Estimator (EMDE) from the area of recommender systems. EMDE operates on sketches - representations especially suitable for multimodal operations. However, EMDE is non-differentiable and ingests precomputed, static embeddings. With T-EMDE we introduce a trainable version of EMDE which allows full end-to-end training. In contrast to self-attention, the complexity of our solution is linear to the number of tokens/segments. As such, T-EMDE is a drop-in replacement for the self-attention module, with beneficial influence on both speed and metric performance in cross-modal settings. It facilitates communication between modalities, as each global text/image representation is expressed with a standardized sketch histogram which represents the same manifold structures irrespective of the underlying modality. We evaluate T-EMDE by introducing it into two recent cross-modal SOTA models and achieving new state-of-the-art results on multiple datasets and decreasing model latency by up to 20%.



### Overcoming the Distance Estimation Bottleneck in Estimating Animal Abundance with Camera Traps
- **Arxiv ID**: http://arxiv.org/abs/2105.04244v2
- **DOI**: 10.1016/j.ecoinf.2021.101536
- **Categories**: **cs.CV**, I.4.9; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2105.04244v2)
- **Published**: 2021-05-10 10:17:34+00:00
- **Updated**: 2021-12-22 10:27:39+00:00
- **Authors**: Timm Haucke, Hjalmar S. Kühl, Jacqueline Hoyer, Volker Steinhage
- **Comment**: None
- **Journal**: Ecological Informatics, Volume 68, May 2022, 101536
- **Summary**: The biodiversity crisis is still accelerating, despite increasing efforts by the international community. Estimating animal abundance is of critical importance to assess, for example, the consequences of land-use change and invasive species on community composition, or the effectiveness of conservation interventions. Various approaches have been developed to estimate abundance of unmarked animal populations. Whereas these approaches differ in methodological details, they all require the estimation of the effective area surveyed in front of a camera trap. Until now camera-to-animal distance measurements are derived by laborious, manual and subjective estimation methods. To overcome this distance estimation bottleneck, this study proposes an automatized pipeline utilizing monocular depth estimation and depth image calibration methods. We are able to reduce the manual effort required by a factor greater than 21 and provide our system at https://timm.haucke.xyz/publications/distance-estimation-animal-abundance



### In-Hindsight Quantization Range Estimation for Quantized Training
- **Arxiv ID**: http://arxiv.org/abs/2105.04246v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.04246v1)
- **Published**: 2021-05-10 10:25:28+00:00
- **Updated**: 2021-05-10 10:25:28+00:00
- **Authors**: Marios Fournarakis, Markus Nagel
- **Comment**: None
- **Journal**: None
- **Summary**: Quantization techniques applied to the inference of deep neural networks have enabled fast and efficient execution on resource-constraint devices. The success of quantization during inference has motivated the academic community to explore fully quantized training, i.e. quantizing back-propagation as well. However, effective gradient quantization is still an open problem. Gradients are unbounded and their distribution changes significantly during training, which leads to the need for dynamic quantization. As we show, dynamic quantization can lead to significant memory overhead and additional data traffic slowing down training. We propose a simple alternative to dynamic quantization, in-hindsight range estimation, that uses the quantization ranges estimated on previous iterations to quantize the present. Our approach enables fast static quantization of gradients and activations while requiring only minimal hardware support from the neural network accelerator to keep track of output statistics in an online fashion. It is intended as a drop-in replacement for estimating quantization ranges and can be used in conjunction with other advances in quantized training. We compare our method to existing methods for range estimation from the quantized training literature and demonstrate its effectiveness with a range of architectures, including MobileNetV2, on image classification benchmarks (Tiny ImageNet & ImageNet).



### Weakly supervised pan-cancer segmentation tool
- **Arxiv ID**: http://arxiv.org/abs/2105.04269v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.04269v1)
- **Published**: 2021-05-10 11:11:09+00:00
- **Updated**: 2021-05-10 11:11:09+00:00
- **Authors**: Marvin Lerousseau, Marion Classe, Enzo Battistella, Théo Estienne, Théophraste Henry, Amaury Leroy, Roger Sun, Maria Vakalopoulou, Jean-Yves Scoazec, Eric Deutsch, Nikos Paragios
- **Comment**: None
- **Journal**: None
- **Summary**: The vast majority of semantic segmentation approaches rely on pixel-level annotations that are tedious and time consuming to obtain and suffer from significant inter and intra-expert variability. To address these issues, recent approaches have leveraged categorical annotations at the slide-level, that in general suffer from robustness and generalization. In this paper, we propose a novel weakly supervised multi-instance learning approach that deciphers quantitative slide-level annotations which are fast to obtain and regularly present in clinical routine. The extreme potentials of the proposed approach are demonstrated for tumor segmentation of solid cancer subtypes. The proposed approach achieves superior performance in out-of-distribution, out-of-location, and out-of-domain testing sets.



### Visual Grounding with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2105.04281v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04281v3)
- **Published**: 2021-05-10 11:46:12+00:00
- **Updated**: 2022-03-14 02:21:54+00:00
- **Authors**: Ye Du, Zehua Fu, Qingjie Liu, Yunhong Wang
- **Comment**: 7 pagrs, 3 figures. Accepted by ICME'22
- **Journal**: None
- **Summary**: In this paper, we propose a transformer based approach for visual grounding. Unlike previous proposal-and-rank frameworks that rely heavily on pretrained object detectors or proposal-free frameworks that upgrade an off-the-shelf one-stage detector by fusing textual embeddings, our approach is built on top of a transformer encoder-decoder and is independent of any pretrained detectors or word embedding models. Termed VGTR -- Visual Grounding with TRansformers, our approach is designed to learn semantic-discriminative visual features under the guidance of the textual description without harming their location ability. This information flow enables our VGTR to have a strong capability in capturing context-level semantics of both vision and language modalities, rendering us to aggregate accurate visual clues implied by the description to locate the interested object instance. Experiments show that our method outperforms state-of-the-art proposal-free approaches by a considerable margin on five benchmarks while maintaining fast inference speed.



### Primitive Representation Learning for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2105.04286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04286v1)
- **Published**: 2021-05-10 11:54:49+00:00
- **Updated**: 2021-05-10 11:54:49+00:00
- **Authors**: Ruijie Yan, Liangrui Peng, Shanyu Xiao, Gang Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Scene text recognition is a challenging task due to diverse variations of text instances in natural scene images. Conventional methods based on CNN-RNN-CTC or encoder-decoder with attention mechanism may not fully investigate stable and efficient feature representations for multi-oriented scene texts. In this paper, we propose a primitive representation learning method that aims to exploit intrinsic representations of scene text images. We model elements in feature maps as the nodes of an undirected graph. A pooling aggregator and a weighted aggregator are proposed to learn primitive representations, which are transformed into high-level visual text representations by graph convolutional networks. A Primitive REpresentation learning Network (PREN) is constructed to use the visual text representations for parallel decoding. Furthermore, by integrating visual text representations into an encoder-decoder model with the 2D attention mechanism, we propose a framework called PREN2D to alleviate the misalignment problem in attention-based methods. Experimental results on both English and Chinese scene text recognition tasks demonstrate that PREN keeps a balance between accuracy and efficiency, while PREN2D achieves state-of-the-art performance.



### Video Anomaly Detection By The Duality Of Normality-Granted Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2105.04302v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04302v1)
- **Published**: 2021-05-10 12:25:00+00:00
- **Updated**: 2021-05-10 12:25:00+00:00
- **Authors**: Hongyong Wang, Xinjian Zhang, Su Yang, Weishan Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Video anomaly detection is a challenging task because of diverse abnormal events. To this task, methods based on reconstruction and prediction are wildly used in recent works, which are built on the assumption that learning on normal data, anomalies cannot be reconstructed or predicated as good as normal patterns, namely the anomaly result with more errors. In this paper, we propose to discriminate anomalies from normal ones by the duality of normality-granted optical flow, which is conducive to predict normal frames but adverse to abnormal frames. The normality-granted optical flow is predicted from a single frame, to keep the motion knowledge focused on normal patterns. Meanwhile, We extend the appearance-motion correspondence scheme from frame reconstruction to prediction, which not only helps to learn the knowledge about object appearances and correlated motion, but also meets the fact that motion is the transformation between appearances. We also introduce a margin loss to enhance the learning of frame prediction. Experiments on standard benchmark datasets demonstrate the impressive performance of our approach.



### Semi-Supervised Metric Learning: A Deep Resurrection
- **Arxiv ID**: http://arxiv.org/abs/2105.05061v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.05061v1)
- **Published**: 2021-05-10 12:28:45+00:00
- **Updated**: 2021-05-10 12:28:45+00:00
- **Authors**: Ujjal Kr Dutta, Mehrtash Harandi, Chellu Chandra Sekhar
- **Comment**: In AAAI-2021
- **Journal**: None
- **Summary**: Distance Metric Learning (DML) seeks to learn a discriminative embedding where similar examples are closer, and dissimilar examples are apart. In this paper, we address the problem of Semi-Supervised DML (SSDML) that tries to learn a metric using a few labeled examples, and abundantly available unlabeled examples. SSDML is important because it is infeasible to manually annotate all the examples present in a large dataset. Surprisingly, with the exception of a few classical approaches that learn a linear Mahalanobis metric, SSDML has not been studied in the recent years, and lacks approaches in the deep SSDML scenario. In this paper, we address this challenging problem, and revamp SSDML with respect to deep learning. In particular, we propose a stochastic, graph-based approach that first propagates the affinities between the pairs of examples from labeled data, to that of the unlabeled pairs. The propagated affinities are used to mine triplet based constraints for metric learning. We impose orthogonality constraint on the metric parameters, as it leads to a better performance by avoiding a model collapse.



### DocReader: Bounding-Box Free Training of a Document Information Extraction Model
- **Arxiv ID**: http://arxiv.org/abs/2105.04313v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.04313v1)
- **Published**: 2021-05-10 12:48:18+00:00
- **Updated**: 2021-05-10 12:48:18+00:00
- **Authors**: Shachar Klaiman, Marius Lehne
- **Comment**: None
- **Journal**: None
- **Summary**: Information extraction from documents is a ubiquitous first step in many business applications. During this step, the entries of various fields must first be read from the images of scanned documents before being further processed and inserted into the corresponding databases. While many different methods have been developed over the past years in order to automate the above extraction step, they all share the requirement of bounding-box or text segment annotations of their training documents. In this work we present DocReader, an end-to-end neural-network-based information extraction solution which can be trained using solely the images and the target values that need to be read. The DocReader can thus leverage existing historical extraction data, completely eliminating the need for any additional annotations beyond what is naturally available in existing human-operated service centres. We demonstrate that the DocReader can reach and surpass other methods which require bounding-boxes for training, as well as provide a clear path for continual learning during its deployment in production.



### RelationTrack: Relation-aware Multiple Object Tracking with Decoupled Representation
- **Arxiv ID**: http://arxiv.org/abs/2105.04322v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04322v1)
- **Published**: 2021-05-10 13:00:40+00:00
- **Updated**: 2021-05-10 13:00:40+00:00
- **Authors**: En Yu, Zhuoling Li, Shoudong Han, Hongwei Wang
- **Comment**: 11 pages, 5 figures, conference
- **Journal**: None
- **Summary**: Existing online multiple object tracking (MOT) algorithms often consist of two subtasks, detection and re-identification (ReID). In order to enhance the inference speed and reduce the complexity, current methods commonly integrate these double subtasks into a unified framework. Nevertheless, detection and ReID demand diverse features. This issue would result in an optimization contradiction during the training procedure. With the target of alleviating this contradiction, we devise a module named Global Context Disentangling (GCD) that decouples the learned representation into detection-specific and ReID-specific embeddings. As such, this module provides an implicit manner to balance the different requirements of these two subtasks. Moreover, we observe that preceding MOT methods typically leverage local information to associate the detected targets and neglect to consider the global semantic relation. To resolve this restriction, we develop a module, referred to as Guided Transformer Encoder (GTE), by combining the powerful reasoning ability of Transformer encoder and deformable attention. Unlike previous works, GTE avoids analyzing all the pixels and only attends to capture the relation between query nodes and a few self-adaptively selected key samples. Therefore, it is computationally efficient. Extensive experiments have been conducted on the MOT16, MOT17 and MOT20 benchmarks to demonstrate the superiority of the proposed MOT framework, namely RelationTrack. The experimental results indicate that RelationTrack has surpassed preceding methods significantly and established a new state-of-the-art performance, e.g., IDF1 of 70.5% and MOTA of 67.2% on MOT20.



### An Autonomous Drone for Search and Rescue in Forests using Airborne Optical Sectioning
- **Arxiv ID**: http://arxiv.org/abs/2105.04328v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04328v1)
- **Published**: 2021-05-10 13:05:22+00:00
- **Updated**: 2021-05-10 13:05:22+00:00
- **Authors**: D. C. Schedl, I. Kurmi, O. Bimber
- **Comment**: 21 pages, 9 figures
- **Journal**: None
- **Summary**: Drones will play an essential role in human-machine teaming in future search and rescue (SAR) missions. We present a first prototype that finds people fully autonomously in densely occluded forests. In the course of 17 field experiments conducted over various forest types and under different flying conditions, our drone found 38 out of 42 hidden persons; average precision was 86% for predefined flight paths, while adaptive path planning (where potential findings are double-checked) increased confidence by 15%. Image processing, classification, and dynamic flight-path adaptation are computed onboard in real-time and while flying. Our finding that deep-learning-based person classification is unaffected by sparse and error-prone sampling within one-dimensional synthetic apertures allows flights to be shortened and reduces recording requirements to one-tenth of the number of images needed for sampling using two-dimensional synthetic apertures. The goal of our adaptive path planning is to find people as reliably and quickly as possible, which is essential in time-critical applications, such as SAR. Our drone enables SAR operations in remote areas without stable network coverage, as it transmits to the rescue team only classification results that indicate detections and can thus operate with intermittent minimal-bandwidth connections (e.g., by satellite). Once received, these results can be visually enhanced for interpretation on remote mobile devices.



### AFINet: Attentive Feature Integration Networks for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2105.04354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04354v1)
- **Published**: 2021-05-10 13:40:11+00:00
- **Updated**: 2021-05-10 13:40:11+00:00
- **Authors**: Xinglin Pan, Jing Xu, Yu Pan, liangjian Wen, WenXiang Lin, Kun Bai, Zenglin Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have achieved tremendous success in a number of learning tasks including image classification. Recent advanced models in CNNs, such as ResNets, mainly focus on the skip connection to avoid gradient vanishing. DenseNet designs suggest creating additional bypasses to transfer features as an alternative strategy in network design. In this paper, we design Attentive Feature Integration (AFI) modules, which are widely applicable to most recent network architectures, leading to new architectures named AFI-Nets. AFI-Nets explicitly model the correlations among different levels of features and selectively transfer features with a little overhead.AFI-ResNet-152 obtains a 1.24% relative improvement on the ImageNet dataset while decreases the FLOPs by about 10% and the number of parameters by about 9.2% compared to ResNet-152.



### Coconut trees detection and segmentation in aerial imagery using mask region-based convolution neural network
- **Arxiv ID**: http://arxiv.org/abs/2105.04356v1
- **DOI**: 10.1049/cvi2.12028
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.04356v1)
- **Published**: 2021-05-10 13:42:19+00:00
- **Updated**: 2021-05-10 13:42:19+00:00
- **Authors**: Muhammad Shakaib Iqbal, Hazrat Ali, Son N. Tran, Talha Iqbal
- **Comment**: Published in IET Computer Vision, 09 April 2021
- **Journal**: None
- **Summary**: Food resources face severe damages under extraordinary situations of catastrophes such as earthquakes, cyclones, and tsunamis. Under such scenarios, speedy assessment of food resources from agricultural land is critical as it supports aid activity in the disaster hit areas. In this article, a deep learning approach is presented for the detection and segmentation of coconut tress in aerial imagery provided through the AI competition organized by the World Bank in collaboration with OpenAerialMap and WeRobotics. Maked Region-based Convolutional Neural Network approach was used identification and segmentation of coconut trees. For the segmentation task, Mask R-CNN model with ResNet50 and ResNet1010 based architectures was used. Several experiments with different configuration parameters were performed and the best configuration for the detection of coconut trees with more than 90% confidence factor was reported. For the purpose of evaluation, Microsoft COCO dataset evaluation metric namely mean average precision (mAP) was used. An overall 91% mean average precision for coconut trees detection was achieved.



### A framework for the automation of testing computer vision systems
- **Arxiv ID**: http://arxiv.org/abs/2105.04383v1
- **DOI**: None
- **Categories**: **cs.SE**, cs.CV, 68N30
- **Links**: [PDF](http://arxiv.org/pdf/2105.04383v1)
- **Published**: 2021-05-10 14:02:42+00:00
- **Updated**: 2021-05-10 14:02:42+00:00
- **Authors**: Franz Wotawa, Lorenz Klampfl, Ledio Jahaj
- **Comment**: 4 pages, Submission version, Accepted at the 2nd ACM/IEEE
  International Conference on Automation of Software Test AST 2021
- **Journal**: None
- **Summary**: Vision systems, i.e., systems that allow to detect and track objects in images, have gained substantial importance over the past decades. They are used in quality assurance applications, e.g., for finding surface defects in products during manufacturing, surveillance, but also automated driving, requiring reliable behavior. Interestingly, there is only little work on quality assurance and especially testing of vision systems in general. In this paper, we contribute to the area of testing vision software, and present a framework for the automated generation of tests for systems based on vision and image recognition. The framework makes use of existing libraries allowing to modify original images and to obtain similarities between the original and modified images. We show how such a framework can be used for testing a particular industrial application on identifying defects on riblet surfaces and present preliminary results from the image classification domain.



### An Enhanced Randomly Initialized Convolutional Neural Network for Columnar Cactus Recognition in Unmanned Aerial Vehicle Imagery
- **Arxiv ID**: http://arxiv.org/abs/2105.04430v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.04430v1)
- **Published**: 2021-05-10 14:41:03+00:00
- **Updated**: 2021-05-10 14:41:03+00:00
- **Authors**: Safa Ben Atitallah, Maha Driss, Wadii Boulila, Anis Koubaa, Nesrine Atitallah, Henda Ben Ghézala
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Convolutional Neural Networks (CNNs) have made a great performance for remote sensing image classification. Plant recognition using CNNs is one of the active deep learning research topics due to its added-value in different related fields, especially environmental conservation and natural areas preservation. Automatic recognition of plants in protected areas helps in the surveillance process of these zones and ensures the sustainability of their ecosystems. In this work, we propose an Enhanced Randomly Initialized Convolutional Neural Network (ERI-CNN) for the recognition of columnar cactus, which is an endemic plant that exists in the Tehuac\'an-Cuicatl\'an Valley in southeastern Mexico. We used a public dataset created by a group of researchers that consists of more than 20000 remote sensing images. The experimental results confirm the effectiveness of the proposed model compared to other models reported in the literature like InceptionV3 and the modified LeNet-5 CNN. Our ERI-CNN provides 98% of accuracy, 97% of precision, 97% of recall, 97.5% as f1-score, and 0.056 loss.



### Boosting Semi-Supervised Face Recognition with Noise Robustness
- **Arxiv ID**: http://arxiv.org/abs/2105.04431v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04431v1)
- **Published**: 2021-05-10 14:43:11+00:00
- **Updated**: 2021-05-10 14:43:11+00:00
- **Authors**: Yuchi Liu, Hailin Shi, Hang Du, Rui Zhu, Jun Wang, Liang Zheng, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Although deep face recognition benefits significantly from large-scale training data, a current bottleneck is the labelling cost. A feasible solution to this problem is semi-supervised learning, exploiting a small portion of labelled data and large amounts of unlabelled data. The major challenge, however, is the accumulated label errors through auto-labelling, compromising the training. This paper presents an effective solution to semi-supervised face recognition that is robust to the label noise aroused by the auto-labelling. Specifically, we introduce a multi-agent method, named GroupNet (GN), to endow our solution with the ability to identify the wrongly labelled samples and preserve the clean samples. We show that GN alone achieves the leading accuracy in traditional supervised face recognition even when the noisy labels take over 50\% of the training data. Further, we develop a semi-supervised face recognition solution, named Noise Robust Learning-Labelling (NRoLL), which is based on the robust training ability empowered by GN. It starts with a small amount of labelled data and consequently conducts high-confidence labelling on a large amount of unlabelled data to boost further training. The more data is labelled by NRoLL, the higher confidence is with the label in the dataset. To evaluate the competitiveness of our method, we run NRoLL with a rough condition that only one-fifth of the labelled MSCeleb is available and the rest is used as unlabelled data. On a wide range of benchmarks, our method compares favorably against the state-of-the-art methods.



### SCTN: Sparse Convolution-Transformer Network for Scene Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/2105.04447v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.04447v4)
- **Published**: 2021-05-10 15:16:14+00:00
- **Updated**: 2022-03-09 16:51:21+00:00
- **Authors**: Bing Li, Cheng Zheng, Silvio Giancola, Bernard Ghanem
- **Comment**: Accepted to the 36th AAAI Conference on Artificial Intelligence (AAAI
  2022)
- **Journal**: None
- **Summary**: We propose a novel scene flow estimation approach to capture and infer 3D motions from point clouds. Estimating 3D motions for point clouds is challenging, since a point cloud is unordered and its density is significantly non-uniform. Such unstructured data poses difficulties in matching corresponding points between point clouds, leading to inaccurate flow estimation. We propose a novel architecture named Sparse Convolution-Transformer Network (SCTN) that equips the sparse convolution with the transformer. Specifically, by leveraging the sparse convolution, SCTN transfers irregular point cloud into locally consistent flow features for estimating continuous and consistent motions within an object/local object part. We further propose to explicitly learn point relations using a point transformer module, different from exiting methods. We show that the learned relation-based contextual information is rich and helpful for matching corresponding points, benefiting scene flow estimation. In addition, a novel loss function is proposed to adaptively encourage flow consistency according to feature similarity. Extensive experiments demonstrate that our proposed approach achieves a new state of the art in scene flow estimation. Our approach achieves an error of 0.038 and 0.037 (EPE3D) on FlyingThings3D and KITTI Scene Flow respectively, which significantly outperforms previous methods by large margins.



### ICON: Learning Regular Maps Through Inverse Consistency
- **Arxiv ID**: http://arxiv.org/abs/2105.04459v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04459v3)
- **Published**: 2021-05-10 15:52:12+00:00
- **Updated**: 2021-06-17 16:39:52+00:00
- **Authors**: Hastings Greer, Roland Kwitt, Francois-Xavier Vialard, Marc Niethammer
- **Comment**: None
- **Journal**: None
- **Summary**: Learning maps between data samples is fundamental. Applications range from representation learning, image translation and generative modeling, to the estimation of spatial deformations. Such maps relate feature vectors, or map between feature spaces. Well-behaved maps should be regular, which can be imposed explicitly or may emanate from the data itself. We explore what induces regularity for spatial transformations, e.g., when computing image registrations. Classical optimization-based models compute maps between pairs of samples and rely on an appropriate regularizer for well-posedness. Recent deep learning approaches have attempted to avoid using such regularizers altogether by relying on the sample population instead. We explore if it is possible to obtain spatial regularity using an inverse consistency loss only and elucidate what explains map regularity in such a context. We find that deep networks combined with an inverse consistency loss and randomized off-grid interpolation yield well behaved, approximately diffeomorphic, spatial transformations. Despite the simplicity of this approach, our experiments present compelling evidence, on both synthetic and real data, that regular maps can be obtained without carefully tuned explicit regularizers, while achieving competitive registration performance.



### Galois/monodromy groups for decomposing minimal problems in 3D reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2105.04460v1
- **DOI**: None
- **Categories**: **math.AG**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2105.04460v1)
- **Published**: 2021-05-10 15:55:09+00:00
- **Updated**: 2021-05-10 15:55:09+00:00
- **Authors**: Timothy Duff, Viktor Korotynskiy, Tomas Pajdla, Margaret H. Regan
- **Comment**: None
- **Journal**: None
- **Summary**: We consider Galois/monodromy groups arising in computer vision applications, with a view towards building more efficient polynomial solvers. The Galois/monodromy group allows us to decide when a given problem decomposes into algebraic subproblems, and whether or not it has any symmetries. Tools from numerical algebraic geometry and computational group theory allow us to apply this framework to classical and novel reconstruction problems. We consider three classical cases--3-point absolute pose, 5-point relative pose, and 4-point homography estimation for calibrated cameras--where the decomposition and symmetries may be naturally understood in terms of the Galois/monodromy group. We then show how our framework can be applied to novel problems from absolute and relative pose estimation. For instance, we discover new symmetries for absolute pose problems involving mixtures of point and line features. We also describe a problem of estimating a pair of calibrated homographies between three images. For this problem of degree 64, we can reduce the degree to 16; the latter better reflecting the intrinsic difficulty of algebraically solving the problem. As a byproduct, we obtain new constraints on compatible homographies, which may be of independent interest.



### Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions
- **Arxiv ID**: http://arxiv.org/abs/2105.04489v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2105.04489v1)
- **Published**: 2021-05-10 16:30:46+00:00
- **Updated**: 2021-05-10 16:30:46+00:00
- **Authors**: Mathew Monfort, SouYoung Jin, Alexander Liu, David Harwath, Rogerio Feris, James Glass, Aude Oliva
- **Comment**: To appear at CVPR 2021
- **Journal**: None
- **Summary**: When people observe events, they are able to abstract key information and build concise summaries of what is happening. These summaries include contextual and semantic information describing the important high-level details (what, where, who and how) of the observed event and exclude background information that is deemed unimportant to the observer. With this in mind, the descriptions people generate for videos of different dynamic events can greatly improve our understanding of the key information of interest in each video. These descriptions can be captured in captions that provide expanded attributes for video labeling (e.g. actions/objects/scenes/sentiment/etc.) while allowing us to gain new insight into what people find important or necessary to summarize specific events. Existing caption datasets for video understanding are either small in scale or restricted to a specific domain. To address this, we present the Spoken Moments (S-MiT) dataset of 500k spoken captions each attributed to a unique short video depicting a broad range of different events. We collect our descriptions using audio recordings to ensure that they remain as natural and concise as possible while allowing us to scale the size of a large classification dataset. In order to utilize our proposed dataset, we present a novel Adaptive Mean Margin (AMM) approach to contrastive learning and evaluate our models on video/caption retrieval on multiple datasets. We show that our AMM approach consistently improves our results and that models trained on our Spoken Moments dataset generalize better than those trained on other video-caption datasets.



### MDA-Net: Multi-Dimensional Attention-Based Neural Network for 3D Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.04508v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.04508v1)
- **Published**: 2021-05-10 16:58:34+00:00
- **Updated**: 2021-05-10 16:58:34+00:00
- **Authors**: Rutu Gandhi, Yi Hong
- **Comment**: None
- **Journal**: None
- **Summary**: Segmenting an entire 3D image often has high computational complexity and requires large memory consumption; by contrast, performing volumetric segmentation in a slice-by-slice manner is efficient but does not fully leverage the 3D data. To address this challenge, we propose a multi-dimensional attention network (MDA-Net) to efficiently integrate slice-wise, spatial, and channel-wise attention into a U-Net based network, which results in high segmentation accuracy with a low computational cost. We evaluate our model on the MICCAI iSeg and IBSR datasets, and the experimental results demonstrate consistent improvements over existing methods.



### A Survey of Performance Optimization in Neural Network-Based Video Analytics Systems
- **Arxiv ID**: http://arxiv.org/abs/2105.14195v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.14195v1)
- **Published**: 2021-05-10 17:06:44+00:00
- **Updated**: 2021-05-10 17:06:44+00:00
- **Authors**: Nada Ibrahim, Preeti Maurya, Omid Jafari, Parth Nagarkar
- **Comment**: None
- **Journal**: None
- **Summary**: Video analytics systems perform automatic events, movements, and actions recognition in a video and make it possible to execute queries on the video. As a result of a large number of video data that need to be processed, optimizing the performance of video analytics systems has become an important research topic. Neural networks are the state-of-the-art for performing video analytics tasks such as video annotation and object detection. Prior survey papers consider application-specific video analytics techniques that improve accuracy of the results; however, in this survey paper, we provide a review of the techniques that focus on optimizing the performance of Neural Network-Based Video Analytics Systems.



### An end-to-end Optical Character Recognition approach for ultra-low-resolution printed text images
- **Arxiv ID**: http://arxiv.org/abs/2105.04515v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T10, I.7.5
- **Links**: [PDF](http://arxiv.org/pdf/2105.04515v1)
- **Published**: 2021-05-10 17:08:06+00:00
- **Updated**: 2021-05-10 17:08:06+00:00
- **Authors**: Julian D. Gilbey, Carola-Bibiane Schönlieb
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Some historical and more recent printed documents have been scanned or stored at very low resolutions, such as 60 dpi. Though such scans are relatively easy for humans to read, they still present significant challenges for optical character recognition (OCR) systems. The current state-of-the art is to use super-resolution to reconstruct an approximation of the original high-resolution image and to feed this into a standard OCR system. Our novel end-to-end method bypasses the super-resolution step and produces better OCR results. This approach is inspired from our understanding of the human visual system, and builds on established neural networks for performing OCR.   Our experiments have shown that it is possible to perform OCR on 60 dpi scanned images of English text, which is a significantly lower resolution than the state-of-the-art, and we achieved a mean character level accuracy (CLA) of 99.7% and word level accuracy (WLA) of 98.9% across a set of about 1000 pages of 60 dpi text in a wide range of fonts. For 75 dpi images, the mean CLA was 99.9% and the mean WLA was 99.4% on the same sample of texts. We make our code and data (including a set of low-resolution images with their ground truths) publicly available as a benchmark for future work in this field.



### Generalized Jensen-Shannon Divergence Loss for Learning with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2105.04522v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2105.04522v4)
- **Published**: 2021-05-10 17:19:38+00:00
- **Updated**: 2021-10-29 06:26:48+00:00
- **Authors**: Erik Englesson, Hossein Azizpour
- **Comment**: Neural Information Processing Systems (NeurIPS 2021)
- **Journal**: None
- **Summary**: Prior works have found it beneficial to combine provably noise-robust loss functions e.g., mean absolute error (MAE) with standard categorical loss function e.g. cross entropy (CE) to improve their learnability. Here, we propose to use Jensen-Shannon divergence as a noise-robust loss function and show that it interestingly interpolate between CE and MAE with a controllable mixing parameter. Furthermore, we make a crucial observation that CE exhibit lower consistency around noisy data points. Based on this observation, we adopt a generalized version of the Jensen-Shannon divergence for multiple distributions to encourage consistency around data points. Using this loss function, we show state-of-the-art results on both synthetic (CIFAR), and real-world (e.g., WebVision) noise with varying noise rates.



### Improved Simultaneous Multi-Slice Functional MRI Using Self-supervised Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.04532v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2105.04532v1)
- **Published**: 2021-05-10 17:36:27+00:00
- **Updated**: 2021-05-10 17:36:27+00:00
- **Authors**: Omer Burak Demirel, Burhaneddin Yaman, Logan Dowdle, Steen Moeller, Luca Vizioli, Essa Yacoub, John Strupp, Cheryl A. Olman, Kâmil Uğurbil, Mehmet Akçakaya
- **Comment**: None
- **Journal**: None
- **Summary**: Functional MRI (fMRI) is commonly used for interpreting neural activities across the brain. Numerous accelerated fMRI techniques aim to provide improved spatiotemporal resolutions. Among these, simultaneous multi-slice (SMS) imaging has emerged as a powerful strategy, becoming a part of large-scale studies, such as the Human Connectome Project. However, when SMS imaging is combined with in-plane acceleration for higher acceleration rates, conventional SMS reconstruction methods may suffer from noise amplification and other artifacts. Recently, deep learning (DL) techniques have gained interest for improving MRI reconstruction. However, these methods are typically trained in a supervised manner that necessitates fully-sampled reference data, which is not feasible in highly-accelerated fMRI acquisitions. Self-supervised learning that does not require fully-sampled data has recently been proposed and has shown similar performance to supervised learning. However, it has only been applied for in-plane acceleration. Furthermore the effect of DL reconstruction on subsequent fMRI analysis remains unclear. In this work, we extend self-supervised DL reconstruction to SMS imaging. Our results on prospectively 10-fold accelerated 7T fMRI data show that self-supervised DL reduces reconstruction noise and suppresses residual artifacts. Subsequent fMRI analysis remains unaltered by DL processing, while the improved temporal signal-to-noise ratio produces higher coherence estimates between task runs.



### Learning High-Dimensional Distributions with Latent Neural Fokker-Planck Kernels
- **Arxiv ID**: http://arxiv.org/abs/2105.04538v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.04538v1)
- **Published**: 2021-05-10 17:42:01+00:00
- **Updated**: 2021-05-10 17:42:01+00:00
- **Authors**: Yufan Zhou, Changyou Chen, Jinhui Xu
- **Comment**: code will be updated at https://github.com/drboog/FPK
- **Journal**: None
- **Summary**: Learning high-dimensional distributions is an important yet challenging problem in machine learning with applications in various domains. In this paper, we introduce new techniques to formulate the problem as solving Fokker-Planck equation in a lower-dimensional latent space, aiming to mitigate challenges in high-dimensional data space. Our proposed model consists of latent-distribution morphing, a generator and a parameterized Fokker-Planck kernel function. One fascinating property of our model is that it can be trained with arbitrary steps of latent distribution morphing or even without morphing, which makes it flexible and as efficient as Generative Adversarial Networks (GANs). Furthermore, this property also makes our latent-distribution morphing an efficient plug-and-play scheme, thus can be used to improve arbitrary GANs, and more interestingly, can effectively correct failure cases of the GAN models. Extensive experiments illustrate the advantages of our proposed method over existing models.



### Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth
- **Arxiv ID**: http://arxiv.org/abs/2105.04550v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2105.04550v2)
- **Published**: 2021-05-10 17:59:01+00:00
- **Updated**: 2021-05-26 05:55:42+00:00
- **Authors**: Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, Kenji Kawaguchi
- **Comment**: None
- **Journal**: None
- **Summary**: Graph Neural Networks (GNNs) have been studied through the lens of expressive power and generalization. However, their optimization properties are less well understood. We take the first step towards analyzing GNN training by studying the gradient dynamics of GNNs. First, we analyze linearized GNNs and prove that despite the non-convexity of training, convergence to a global minimum at a linear rate is guaranteed under mild assumptions that we validate on real-world graphs. Second, we study what may affect the GNNs' training speed. Our results show that the training of GNNs is implicitly accelerated by skip connections, more depth, and/or a good label distribution. Empirical results confirm that our theoretical results for linearized GNNs align with the training behavior of nonlinear GNNs. Our results provide the first theoretical support for the success of GNNs with skip connections in terms of optimization, and suggest that deep GNNs with skip connections would be promising in practice.



### Stochastic Image-to-Video Synthesis using cINNs
- **Arxiv ID**: http://arxiv.org/abs/2105.04551v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04551v2)
- **Published**: 2021-05-10 17:59:09+00:00
- **Updated**: 2021-06-17 13:09:20+00:00
- **Authors**: Michael Dorkenwald, Timo Milbich, Andreas Blattmann, Robin Rombach, Konstantinos G. Derpanis, Björn Ommer
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Video understanding calls for a model to learn the characteristic interplay between static scene content and its dynamics: Given an image, the model must be able to predict a future progression of the portrayed scene and, conversely, a video should be explained in terms of its static image content and all the remaining characteristics not present in the initial frame. This naturally suggests a bijective mapping between the video domain and the static content as well as residual information. In contrast to common stochastic image-to-video synthesis, such a model does not merely generate arbitrary videos progressing the initial image. Given this image, it rather provides a one-to-one mapping between the residual vectors and the video with stochastic outcomes when sampling. The approach is naturally implemented using a conditional invertible neural network (cINN) that can explain videos by independently modelling static and other video characteristics, thus laying the basis for controlled video synthesis. Experiments on four diverse video datasets demonstrate the effectiveness of our approach in terms of both the quality and diversity of the synthesized results. Our project page is available at https://bit.ly/3t66bnU.



### Self-Supervised Learning with Swin Transformers
- **Arxiv ID**: http://arxiv.org/abs/2105.04553v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04553v2)
- **Published**: 2021-05-10 17:59:45+00:00
- **Updated**: 2021-05-11 17:28:00+00:00
- **Authors**: Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, Han Hu
- **Comment**: None
- **Journal**: None
- **Summary**: We are witnessing a modeling shift from CNN to Transformers in computer vision. In this work, we present a self-supervised learning approach called MoBY, with Vision Transformers as its backbone architecture. The approach basically has no new inventions, which is combined from MoCo v2 and BYOL and tuned to achieve reasonably high accuracy on ImageNet-1K linear evaluation: 72.8% and 75.0% top-1 accuracy using DeiT-S and Swin-T, respectively, by 300-epoch training. The performance is slightly better than recent works of MoCo v3 and DINO which adopt DeiT as the backbone, but with much lighter tricks.   More importantly, the general-purpose Swin Transformer backbone enables us to also evaluate the learnt representations on downstream tasks such as object detection and semantic segmentation, in contrast to a few recent approaches built on ViT/DeiT which only report linear evaluation results on ImageNet-1K due to ViT/DeiT not tamed for these dense prediction tasks. We hope our results can facilitate more comprehensive evaluation of self-supervised learning methods designed for Transformer architectures. Our code and models are available at https://github.com/SwinTransformer/Transformer-SSL, which will be continually enriched.



### Towards Discovery and Attribution of Open-world GAN Generated Images
- **Arxiv ID**: http://arxiv.org/abs/2105.04580v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.04580v2)
- **Published**: 2021-05-10 18:00:13+00:00
- **Updated**: 2021-09-20 23:59:06+00:00
- **Authors**: Sharath Girish, Saksham Suri, Saketh Rambhatla, Abhinav Shrivastava
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: With the recent progress in Generative Adversarial Networks (GANs), it is imperative for media and visual forensics to develop detectors which can identify and attribute images to the model generating them. Existing works have shown to attribute images to their corresponding GAN sources with high accuracy. However, these works are limited to a closed set scenario, failing to generalize to GANs unseen during train time and are therefore, not scalable with a steady influx of new GANs. We present an iterative algorithm for discovering images generated from previously unseen GANs by exploiting the fact that all GANs leave distinct fingerprints on their generated images. Our algorithm consists of multiple components including network training, out-of-distribution detection, clustering, merge and refine steps. Through extensive experiments, we show that our algorithm discovers unseen GANs with high accuracy and also generalizes to GANs trained on unseen real datasets. We additionally apply our algorithm to attribution and discovery of GANs in an online fashion as well as to the more standard task of real/fake detection. Our experiments demonstrate the effectiveness of our approach to discover new GANs and can be used in an open-world setup.



### TransPose: Real-time 3D Human Translation and Pose Estimation with Six Inertial Sensors
- **Arxiv ID**: http://arxiv.org/abs/2105.04605v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.04605v1)
- **Published**: 2021-05-10 18:41:42+00:00
- **Updated**: 2021-05-10 18:41:42+00:00
- **Authors**: Xinyu Yi, Yuxiao Zhou, Feng Xu
- **Comment**: Accepted by SIGGRAPH 2021. Project page:
  https://xinyu-yi.github.io/TransPose/
- **Journal**: None
- **Summary**: Motion capture is facing some new possibilities brought by the inertial sensing technologies which do not suffer from occlusion or wide-range recordings as vision-based solutions do. However, as the recorded signals are sparse and quite noisy, online performance and global translation estimation turn out to be two key difficulties. In this paper, we present TransPose, a DNN-based approach to perform full motion capture (with both global translations and body poses) from only 6 Inertial Measurement Units (IMUs) at over 90 fps. For body pose estimation, we propose a multi-stage network that estimates leaf-to-full joint positions as intermediate results. This design makes the pose estimation much easier, and thus achieves both better accuracy and lower computation cost. For global translation estimation, we propose a supporting-foot-based method and an RNN-based method to robustly solve for the global translations with a confidence-based fusion technique. Quantitative and qualitative comparisons show that our method outperforms the state-of-the-art learning- and optimization-based methods with a large margin in both accuracy and efficiency. As a purely inertial sensor-based approach, our method is not limited by environmental settings (e.g., fixed cameras), making the capture free from common difficulties such as wide-range motion space and strong occlusion.



### Enhancing Photorealism Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2105.04619v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2105.04619v1)
- **Published**: 2021-05-10 19:00:49+00:00
- **Updated**: 2021-05-10 19:00:49+00:00
- **Authors**: Stephan R. Richter, Hassan Abu AlHaija, Vladlen Koltun
- **Comment**: Code and data available at
  https://github.com/intel-isl/PhotorealismEnhancement Video available at
  https://youtu.be/P1IcaBn3ej0
- **Journal**: None
- **Summary**: We present an approach to enhancing the realism of synthetic images. The images are enhanced by a convolutional network that leverages intermediate representations produced by conventional rendering pipelines. The network is trained via a novel adversarial objective, which provides strong supervision at multiple perceptual levels. We analyze scene layout distributions in commonly used datasets and find that they differ in important ways. We hypothesize that this is one of the causes of strong artifacts that can be observed in the results of many prior methods. To address this we propose a new strategy for sampling image patches during training. We also introduce multiple architectural improvements in the deep network modules used for photorealism enhancement. We confirm the benefits of our contributions in controlled experiments and report substantial gains in stability and realism in comparison to recent image-to-image translation methods and a variety of other baselines.



### Local Frequency Domain Transformer Networks for Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/2105.04637v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.04637v1)
- **Published**: 2021-05-10 19:48:42+00:00
- **Updated**: 2021-05-10 19:48:42+00:00
- **Authors**: Hafez Farazi, Jan Nogga, Sven Behnke
- **Comment**: None
- **Journal**: None
- **Summary**: Video prediction is commonly referred to as forecasting future frames of a video sequence provided several past frames thereof. It remains a challenging domain as visual scenes evolve according to complex underlying dynamics, such as the camera's egocentric motion or the distinct motility per individual object viewed. These are mostly hidden from the observer and manifest as often highly non-linear transformations between consecutive video frames. Therefore, video prediction is of interest not only in anticipating visual changes in the real world but has, above all, emerged as an unsupervised learning rule targeting the formation and dynamics of the observed environment. Many of the deep learning-based state-of-the-art models for video prediction utilize some form of recurrent layers like Long Short-Term Memory (LSTMs) or Gated Recurrent Units (GRUs) at the core of their models. Although these models can predict the future frames, they rely entirely on these recurrent structures to simultaneously perform three distinct tasks: extracting transformations, projecting them into the future, and transforming the current frame. In order to completely interpret the formed internal representations, it is crucial to disentangle these tasks. This paper proposes a fully differentiable building block that can perform all of those tasks separately while maintaining interpretability. We derive the relevant theoretical foundations and showcase results on synthetic as well as real data. We demonstrate that our method is readily extended to perform motion segmentation and account for the scene's composition, and learns to produce reliable predictions in an entirely interpretable manner by only observing unlabeled video data.



### SUPR-GAN: SUrgical PRediction GAN for Event Anticipation in Laparoscopic and Robotic Surgery
- **Arxiv ID**: http://arxiv.org/abs/2105.04642v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04642v2)
- **Published**: 2021-05-10 19:56:45+00:00
- **Updated**: 2022-03-09 18:39:02+00:00
- **Authors**: Yutong Ban, Guy Rosman, Jennifer A. Eckhoff, Thomas M. Ward, Daniel A. Hashimoto, Taisei Kondo, Hidekazu Iwaki, Ozanan R. Meireles, Daniela Rus
- **Comment**: RA-L ICRA 2022
- **Journal**: None
- **Summary**: Comprehension of surgical workflow is the foundation upon which artificial intelligence (AI) and machine learning (ML) holds the potential to assist intraoperative decision-making and risk mitigation. In this work, we move beyond mere identification of past surgical phases, into the prediction of future surgical steps and specification of the transitions between them. We use a novel Generative Adversarial Network (GAN) formulation to sample future surgical phases trajectories conditioned on past video frames from laparoscopic cholecystectomy (LC) videos and compare it to state-of-the-art approaches for surgical video analysis and alternative prediction methods. We demonstrate the GAN formulation's effectiveness through inferring and predicting the progress of LC videos. We quantify the horizon-accuracy trade-off and explored average performance, as well as the performance on the more challenging, and clinically relevant transitions between phases. Furthermore, we conduct a survey, asking 16 surgeons of different specialties and educational levels to qualitatively evaluate predicted surgery phases.



### DEEMD: Drug Efficacy Estimation against SARS-CoV-2 based on cell Morphology with Deep multiple instance learning
- **Arxiv ID**: http://arxiv.org/abs/2105.05758v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2105.05758v2)
- **Published**: 2021-05-10 20:38:34+00:00
- **Updated**: 2022-06-16 13:23:37+00:00
- **Authors**: M. Sadegh Saberian, Kathleen P. Moriarty, Andrea D. Olmstead, Christian Hallgrimson, François Jean, Ivan R. Nabi, Maxwell W. Libbrecht, Ghassan Hamarneh
- **Comment**: None
- **Journal**: None
- **Summary**: Drug repurposing can accelerate the identification of effective compounds for clinical use against SARS-CoV-2, with the advantage of pre-existing clinical safety data and an established supply chain. RNA viruses such as SARS-CoV-2 manipulate cellular pathways and induce reorganization of subcellular structures to support their life cycle. These morphological changes can be quantified using bioimaging techniques. In this work, we developed DEEMD: a computational pipeline using deep neural network models within a multiple instance learning framework, to identify putative treatments effective against SARS-CoV-2 based on morphological analysis of the publicly available RxRx19a dataset. This dataset consists of fluorescence microscopy images of SARS-CoV-2 non-infected cells and infected cells, with and without drug treatment. DEEMD first extracts discriminative morphological features to generate cell morphological profiles from the non-infected and infected cells. These morphological profiles are then used in a statistical model to estimate the applied treatment efficacy on infected cells based on similarities to non-infected cells. DEEMD is capable of localizing infected cells via weak supervision without any expensive pixel-level annotations. DEEMD identifies known SARS-CoV-2 inhibitors, such as Remdesivir and Aloxistatin, supporting the validity of our approach. DEEMD can be explored for use on other emerging viruses and datasets to rapidly identify candidate antiviral treatments in the future}. Our implementation is available online at https://www.github.com/Sadegh-Saberian/DEEMD



### HuMoR: 3D Human Motion Model for Robust Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2105.04668v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.04668v2)
- **Published**: 2021-05-10 21:04:55+00:00
- **Updated**: 2021-08-18 05:52:31+00:00
- **Authors**: Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, Leonidas J. Guibas
- **Comment**: ICCV 2021 camera ready
- **Journal**: None
- **Summary**: We introduce HuMoR: a 3D Human Motion Model for Robust Estimation of temporal pose and shape. Though substantial progress has been made in estimating 3D human motion and shape from dynamic observations, recovering plausible pose sequences in the presence of noise and occlusions remains a challenge. For this purpose, we propose an expressive generative model in the form of a conditional variational autoencoder, which learns a distribution of the change in pose at each step of a motion sequence. Furthermore, we introduce a flexible optimization-based approach that leverages HuMoR as a motion prior to robustly estimate plausible pose and shape from ambiguous observations. Through extensive evaluations, we demonstrate that our model generalizes to diverse motions and body shapes after training on a large motion capture dataset, and enables motion reconstruction from multiple input modalities including 3D keypoints and RGB(-D) videos.



### Sample selection for efficient image annotation
- **Arxiv ID**: http://arxiv.org/abs/2105.04678v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.04678v1)
- **Published**: 2021-05-10 21:25:10+00:00
- **Updated**: 2021-05-10 21:25:10+00:00
- **Authors**: Bishwo Adhikari, Esa Rahtu, Heikki Huttunen
- **Comment**: This work has been accepted in EUVIP 2021
- **Journal**: None
- **Summary**: Supervised object detection has been proven to be successful in many benchmark datasets achieving human-level performances. However, acquiring a large amount of labeled image samples for supervised detection training is tedious, time-consuming, and costly. In this paper, we propose an efficient image selection approach that samples the most informative images from the unlabeled dataset and utilizes human-machine collaboration in an iterative train-annotate loop. Image features are extracted by the CNN network followed by the similarity score calculation, Euclidean distance. Unlabeled images are then sampled into different approaches based on the similarity score. The proposed approach is straightforward, simple and sampling takes place prior to the network training. Experiments on datasets show that our method can reduce up to 80% of manual annotation workload, compared to full manual labeling setting, and performs better than random sampling.



### Sample and Computation Redistribution for Efficient Face Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.04714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04714v1)
- **Published**: 2021-05-10 23:51:14+00:00
- **Updated**: 2021-05-10 23:51:14+00:00
- **Authors**: Jia Guo, Jiankang Deng, Alexandros Lattas, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: Although tremendous strides have been made in uncontrolled face detection, efficient face detection with a low computation cost as well as high precision remains an open challenge. In this paper, we point out that training data sampling and computation distribution strategies are the keys to efficient and accurate face detection. Motivated by these observations, we introduce two simple but effective methods (1) Sample Redistribution (SR), which augments training samples for the most needed stages, based on the statistics of benchmark datasets; and (2) Computation Redistribution (CR), which reallocates the computation between the backbone, neck and head of the model, based on a meticulously defined search methodology. Extensive experiments conducted on WIDER FACE demonstrate the state-of-the-art efficiency-accuracy trade-off for the proposed \scrfd family across a wide range of compute regimes. In particular, \scrfdf{34} outperforms the best competitor, TinaFace, by $3.86\%$ (AP at hard set) while being more than \emph{3$\times$ faster} on GPUs with VGA-resolution images. We also release our code to facilitate future research.



