# Arxiv Papers in cs.CV on 2021-05-11
### Unsupervised domain adaptation via double classifiers based on high confidence pseudo label
- **Arxiv ID**: http://arxiv.org/abs/2105.04729v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04729v1)
- **Published**: 2021-05-11 00:51:31+00:00
- **Updated**: 2021-05-11 00:51:31+00:00
- **Authors**: Huihuang Chen, Li Li, Jie Chen, Kuo-Yi Lin
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) aims to solve the problem of knowledge transfer from labeled source domain to unlabeled target domain. Recently, many domain adaptation (DA) methods use centroid to align the local distribution of different domains, that is, to align different classes. This improves the effect of domain adaptation, but domain differences exist not only between classes, but also between samples. This work rethinks what is the alignment between different domains, and studies how to achieve the real alignment between different domains. Previous DA methods only considered one distribution feature of aligned samples, such as full distribution or local distribution. In addition to aligning the global distribution, the real domain adaptation should also align the meso distribution and the micro distribution. Therefore, this study propose a double classifier method based on high confidence label (DCP). By aligning the centroid and the distribution between centroid and sample of different classifiers, the meso and micro distribution alignment of different domains is realized. In addition, in order to reduce the chain error caused by error marking, This study propose a high confidence marking method to reduce the marking error. To verify its versatility, this study evaluates DCP on digital recognition and target recognition data sets. The results show that our method achieves state-of-the-art results on most of the current domain adaptation benchmark datasets.



### Disentangling Noise from Images: A Flow-Based Image Denoising Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2105.04746v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04746v1)
- **Published**: 2021-05-11 01:52:26+00:00
- **Updated**: 2021-05-11 01:52:26+00:00
- **Authors**: Yang Liu, Saeed Anwar, Zhenyue Qin, Pan Ji, Sabrina Caldwell, Tom Gedeon
- **Comment**: None
- **Journal**: None
- **Summary**: The prevalent convolutional neural network (CNN) based image denoising methods extract features of images to restore the clean ground truth, achieving high denoising accuracy. However, these methods may ignore the underlying distribution of clean images, inducing distortions or artifacts in denoising results. This paper proposes a new perspective to treat image denoising as a distribution learning and disentangling task. Since the noisy image distribution can be viewed as a joint distribution of clean images and noise, the denoised images can be obtained via manipulating the latent representations to the clean counterpart. This paper also provides a distribution learning based denoising framework. Following this framework, we present an invertible denoising network, FDN, without any assumptions on either clean or noise distributions, as well as a distribution disentanglement method. FDN learns the distribution of noisy images, which is different from the previous CNN based discriminative mapping. Experimental results demonstrate FDN's capacity to remove synthetic additive white Gaussian noise (AWGN) on both category-specific and remote sensing images. Furthermore, the performance of FDN surpasses that of previously published methods in real image denoising with fewer parameters and faster speed. Our code is available at: https://github.com/Yang-Liu1082/FDN.git.



### Graph Consistency Based Mean-Teaching for Unsupervised Domain Adaptive Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2105.04776v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.04776v5)
- **Published**: 2021-05-11 04:09:49+00:00
- **Updated**: 2021-05-31 01:02:48+00:00
- **Authors**: Xiaobin Liu, Shiliang Zhang
- **Comment**: IJCAI 2021
- **Journal**: None
- **Summary**: Recent works show that mean-teaching is an effective framework for unsupervised domain adaptive person re-identification. However, existing methods perform contrastive learning on selected samples between teacher and student networks, which is sensitive to noises in pseudo labels and neglects the relationship among most samples. Moreover, these methods are not effective in cooperation of different teacher networks. To handle these issues, this paper proposes a Graph Consistency based Mean-Teaching (GCMT) method with constructing the Graph Consistency Constraint (GCC) between teacher and student networks. Specifically, given unlabeled training images, we apply teacher networks to extract corresponding features and further construct a teacher graph for each teacher network to describe the similarity relationships among training images. To boost the representation learning, different teacher graphs are fused to provide the supervise signal for optimizing student networks. GCMT fuses similarity relationships predicted by different teacher networks as supervision and effectively optimizes student networks with more sample relationships involved. Experiments on three datasets, i.e., Market-1501, DukeMTMCreID, and MSMT17, show that proposed GCMT outperforms state-of-the-art methods by clear margin. Specially, GCMT even outperforms the previous method that uses a deeper backbone. Experimental results also show that GCMT can effectively boost the performance with multiple teacher and student networks. Our code is available at https://github.com/liu-xb/GCMT .



### Cross-Modal Generative Augmentation for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2105.04780v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2105.04780v2)
- **Published**: 2021-05-11 04:51:26+00:00
- **Updated**: 2021-10-22 22:04:03+00:00
- **Authors**: Zixu Wang, Yishu Miao, Lucia Specia
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: Data augmentation has been shown to effectively improve the performance of multimodal machine learning models. This paper introduces a generative model for data augmentation by leveraging the correlations among multiple modalities. Different from conventional data augmentation approaches that apply low-level operations with deterministic heuristics, our method learns a generator that generates samples of the target modality conditioned on observed modalities in the variational auto-encoder framework. Additionally, the proposed model is able to quantify the confidence of augmented data by its generative probability, and can be jointly optimised with a downstream task. Experiments on Visual Question Answering as downstream task demonstrate the effectiveness of the proposed generative model, which is able to improve strong UpDn-based models to achieve state-of-the-art performance.



### A Feature Fusion-Net Using Deep Spatial Context Encoder and Nonstationary Joint Statistical Model for High Resolution SAR Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2105.04799v1
- **DOI**: 10.1109/TGRS.2021.3137029
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04799v1)
- **Published**: 2021-05-11 06:20:14+00:00
- **Updated**: 2021-05-11 06:20:14+00:00
- **Authors**: Wenkai Liang, Yan Wu, Ming Li, Peng Zhang, Yice Cao, Xin Hu
- **Comment**: 17 pages,11 figures
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have been applied to learn spatial features for high-resolution (HR) synthetic aperture radar (SAR) image classification. However, there has been little work on integrating the unique statistical distributions of SAR images which can reveal physical properties of terrain objects, into CNNs in a supervised feature learning framework. To address this problem, a novel end-to-end supervised classification method is proposed for HR SAR images by considering both spatial context and statistical features. First, to extract more effective spatial features from SAR images, a new deep spatial context encoder network (DSCEN) is proposed, which is a lightweight structure and can be effectively trained with a small number of samples. Meanwhile, to enhance the diversity of statistics, the nonstationary joint statistical model (NS-JSM) is adopted to form the global statistical features. Specifically, SAR images are transformed into the Gabor wavelet domain and the produced multi-subbands magnitudes and phases are modeled by the log-normal and uniform distribution. The covariance matrix is further utilized to capture the inter-scale and intra-scale nonstationary correlation between the statistical subbands and make the joint statistical features more compact and distinguishable. Considering complementary advantages, a feature fusion network (Fusion-Net) base on group compression and smooth normalization is constructed to embed the statistical features into the spatial features and optimize the fusion feature representation. As a result, our model can learn the discriminative features and improve the final classification performance. Experiments on four HR SAR images validate the superiority of the proposed method over other related algorithms.



### ORCEA: Object Recognition by Continuous Evidence Assimilation
- **Arxiv ID**: http://arxiv.org/abs/2105.04807v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04807v1)
- **Published**: 2021-05-11 06:38:55+00:00
- **Updated**: 2021-05-11 06:38:55+00:00
- **Authors**: Oded Cohen
- **Comment**: None
- **Journal**: None
- **Summary**: ORCEA is a novel object recognition method applicable for objects describable by a generative model. The primary goal of ORCEA is to maintain a probability density distribution of possible matches over the object parameter space, while continuously updating it with incoming evidence; detection and regression are by-products of this process. ORCEA can project primitive evidence of various types (edge element, area patches etc.) directly on the object parameter space; this made possible by the study phase where ORCEA builds a probabilistic model, for each evidence type, that links evidence and the object-parameters under which they were created. The detection phase consists of building the joint distribution of possible matches resulting from the set of given evidence, including possible grouping to signal/noise; no additional algorithmic steps are needed, as the resulting PDF encapsulates all knowledge about possible solutions. ORCEA represents the match distribution over the parameter space as a set of Gaussian distributions, each representing a concrete probabilistic hypothesis about the object, which can be used outside its scope as well. ORCEA was tested on synthetic images with varying levels of complexity and noise, and shows satisfactory results.



### Learning Implicit Temporal Alignment for Few-shot Video Classification
- **Arxiv ID**: http://arxiv.org/abs/2105.04823v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.04823v1)
- **Published**: 2021-05-11 07:18:57+00:00
- **Updated**: 2021-05-11 07:18:57+00:00
- **Authors**: Songyang Zhang, Jiale Zhou, Xuming He
- **Comment**: Accepted by IJCAI 2021
- **Journal**: None
- **Summary**: Few-shot video classification aims to learn new video categories with only a few labeled examples, alleviating the burden of costly annotation in real-world applications. However, it is particularly challenging to learn a class-invariant spatial-temporal representation in such a setting. To address this, we propose a novel matching-based few-shot learning strategy for video sequences in this work. Our main idea is to introduce an implicit temporal alignment for a video pair, capable of estimating the similarity between them in an accurate and robust manner. Moreover, we design an effective context encoding module to incorporate spatial and feature channel context, resulting in better modeling of intra-class variations. To train our model, we develop a multi-task loss for learning video matching, leading to video features with better generalization. Extensive experimental results on two challenging benchmarks, show that our method outperforms the prior arts with a sizable margin on SomethingSomething-V2 and competitive results on Kinetics.



### Uncover Common Facial Expressions in Terracotta Warriors: A Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2105.04826v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04826v1)
- **Published**: 2021-05-11 07:28:25+00:00
- **Updated**: 2021-05-11 07:28:25+00:00
- **Authors**: Wenhong Tian, Yuanlun Xie, Tingsong Ma, Hengxin Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Can advanced deep learning technologies be applied to analyze some ancient humanistic arts? Can deep learning technologies be directly applied to special scenes such as facial expression analysis of Terracotta Warriors? The big challenging is that the facial features of the Terracotta Warriors are very different from today's people. We found that it is very poor to directly use the models that have been trained on other classic facial expression datasets to analyze the facial expressions of the Terracotta Warriors. At the same time, the lack of public high-quality facial expression data of the Terracotta Warriors also limits the use of deep learning technologies. Therefore, we firstly use Generative Adversarial Networks (GANs) to generate enough high-quality facial expression data for subsequent training and recognition. We also verify the effectiveness of this approach. For the first time, this paper uses deep learning technologies to find common facial expressions of general and postured Terracotta Warriors. These results will provide an updated technical means for the research of art of the Terracotta Warriors and shine lights on the research of other ancient arts.



### Improving Adversarial Transferability with Gradient Refining
- **Arxiv ID**: http://arxiv.org/abs/2105.04834v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04834v3)
- **Published**: 2021-05-11 07:44:29+00:00
- **Updated**: 2022-04-14 09:13:01+00:00
- **Authors**: Guoqiu Wang, Huanqian Yan, Ying Guo, Xingxing Wei
- **Comment**: Accepted at CVPR 2021 Workshop on Adversarial Machine Learning in
  Real-World Computer Vision Systems and Online Challenges. The extension
  vision of this paper, please refer to arxiv:2203.13479
- **Journal**: None
- **Summary**: Deep neural networks are vulnerable to adversarial examples, which are crafted by adding human-imperceptible perturbations to original images. Most existing adversarial attack methods achieve nearly 100% attack success rates under the white-box setting, but only achieve relatively low attack success rates under the black-box setting. To improve the transferability of adversarial examples for the black-box setting, several methods have been proposed, e.g., input diversity, translation-invariant attack, and momentum-based attack. In this paper, we propose a method named Gradient Refining, which can further improve the adversarial transferability by correcting useless gradients introduced by input diversity through multiple transformations. Our method is generally applicable to many gradient-based attack methods combined with input diversity. Extensive experiments are conducted on the ImageNet dataset and our method can achieve an average transfer success rate of 82.07% for three different models under single-model setting, which outperforms the other state-of-the-art methods by a large margin of 6.0% averagely. And we have applied the proposed method to the competition CVPR 2021 Unrestricted Adversarial Attacks on ImageNet organized by Alibaba and won the second place in attack success rates among 1558 teams.



### Found a Reason for me? Weakly-supervised Grounded Visual Question Answering using Capsules
- **Arxiv ID**: http://arxiv.org/abs/2105.04836v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04836v1)
- **Published**: 2021-05-11 07:45:32+00:00
- **Updated**: 2021-05-11 07:45:32+00:00
- **Authors**: Aisha Urooj Khan, Hilde Kuehne, Kevin Duarte, Chuang Gan, Niels Lobo, Mubarak Shah
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2021
- **Journal**: None
- **Summary**: The problem of grounding VQA tasks has seen an increased attention in the research community recently, with most attempts usually focusing on solving this task by using pretrained object detectors. However, pre-trained object detectors require bounding box annotations for detecting relevant objects in the vocabulary, which may not always be feasible for real-life large-scale applications. In this paper, we focus on a more relaxed setting: the grounding of relevant visual entities in a weakly supervised manner by training on the VQA task alone. To address this problem, we propose a visual capsule module with a query-based selection mechanism of capsule features, that allows the model to focus on relevant regions based on the textual cues about visual information in the question. We show that integrating the proposed capsule module in existing VQA systems significantly improves their performance on the weakly supervised grounding task. Overall, we demonstrate the effectiveness of our approach on two state-of-the-art VQA systems, stacked NMN and MAC, on the CLEVR-Answers benchmark, our new evaluation set based on CLEVR scenes with ground truth bounding boxes for objects that are relevant for the correct answer, as well as on GQA, a real world VQA dataset with compositional questions. We show that the systems with the proposed capsule module consistently outperform the respective baseline systems in terms of answer grounding, while achieving comparable performance on VQA task.



### Poisoning MorphNet for Clean-Label Backdoor Attack to Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2105.04839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04839v1)
- **Published**: 2021-05-11 07:48:39+00:00
- **Updated**: 2021-05-11 07:48:39+00:00
- **Authors**: Guiyu Tian, Wenhao Jiang, Wei Liu, Yadong Mu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents Poisoning MorphNet, the first backdoor attack method on point clouds. Conventional adversarial attack takes place in the inference stage, often fooling a model by perturbing samples. In contrast, backdoor attack aims to implant triggers into a model during the training stage, such that the victim model acts normally on the clean data unless a trigger is present in a sample. This work follows a typical setting of clean-label backdoor attack, where a few poisoned samples (with their content tampered yet labels unchanged) are injected into the training set. The unique contributions of MorphNet are two-fold. First, it is key to ensure the implanted triggers both visually imperceptible to humans and lead to high attack success rate on the point clouds. To this end, MorphNet jointly optimizes two objectives for sample-adaptive poisoning: a reconstruction loss that preserves the visual similarity between benign / poisoned point clouds, and a classification loss that enforces a modern recognition model of point clouds tends to mis-classify the poisoned sample to a pre-specified target category. This implicitly conducts spectral separation over point clouds, hiding sample-adaptive triggers in fine-grained high-frequency details. Secondly, existing backdoor attack methods are mainly designed for image data, easily defended by some point cloud specific operations (such as denoising). We propose a third loss in MorphNet for suppressing isolated points, leading to improved resistance to denoising-based defense. Comprehensive evaluations are conducted on ModelNet40 and ShapeNetcorev2. Our proposed Poisoning MorphNet outstrips all previous methods with clear margins.



### Vision-based Neural Scene Representations for Spacecraft
- **Arxiv ID**: http://arxiv.org/abs/2105.06405v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06405v1)
- **Published**: 2021-05-11 08:35:05+00:00
- **Updated**: 2021-05-11 08:35:05+00:00
- **Authors**: Anne Mergy, Gurvan Lecuyer, Dawa Derksen, Dario Izzo
- **Comment**: None
- **Journal**: None
- **Summary**: In advanced mission concepts with high levels of autonomy, spacecraft need to internally model the pose and shape of nearby orbiting objects. Recent works in neural scene representations show promising results for inferring generic three-dimensional scenes from optical images. Neural Radiance Fields (NeRF) have shown success in rendering highly specular surfaces using a large number of images and their pose. More recently, Generative Radiance Fields (GRAF) achieved full volumetric reconstruction of a scene from unposed images only, thanks to the use of an adversarial framework to train a NeRF. In this paper, we compare and evaluate the potential of NeRF and GRAF to render novel views and extract the 3D shape of two different spacecraft, the Soil Moisture and Ocean Salinity satellite of ESA's Living Planet Programme and a generic cube sat. Considering the best performances of both models, we observe that NeRF has the ability to render more accurate images regarding the material specularity of the spacecraft and its pose. For its part, GRAF generates precise novel views with accurate details even when parts of the satellites are shadowed while having the significant advantage of not needing any information about the relative pose.



### EDPN: Enhanced Deep Pyramid Network for Blurry Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2105.04872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04872v1)
- **Published**: 2021-05-11 08:50:20+00:00
- **Updated**: 2021-05-11 08:50:20+00:00
- **Authors**: Ruikang Xu, Zeyu Xiao, Jie Huang, Yueyi Zhang, Zhiwei Xiong
- **Comment**: Accepted at NTIRE Workshop, CVPR 2021. Ruikang and Zeyu contribute
  equally to this work
- **Journal**: None
- **Summary**: Image deblurring has seen a great improvement with the development of deep neural networks. In practice, however, blurry images often suffer from additional degradations such as downscaling and compression. To address these challenges, we propose an Enhanced Deep Pyramid Network (EDPN) for blurry image restoration from multiple degradations, by fully exploiting the self- and cross-scale similarities in the degraded image.Specifically, we design two pyramid-based modules, i.e., the pyramid progressive transfer (PPT) module and the pyramid self-attention (PSA) module, as the main components of the proposed network. By taking several replicated blurry images as inputs, the PPT module transfers both self- and cross-scale similarity information from the same degraded image in a progressive manner. Then, the PSA module fuses the above transferred features for subsequent restoration using self- and spatial-attention mechanisms. Experimental results demonstrate that our method significantly outperforms existing solutions for blurry image super-resolution and blurry image deblocking. In the NTIRE 2021 Image Deblurring Challenge, EDPN achieves the best PSNR/SSIM/LPIPS scores in Track 1 (Low Resolution) and the best SSIM/LPIPS scores in Track 2 (JPEG Artifacts).



### Video-based Analysis of Soccer Matches
- **Arxiv ID**: http://arxiv.org/abs/2105.04875v1
- **DOI**: 10.1145/3347318.3355515
- **Categories**: **cs.CV**, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2105.04875v1)
- **Published**: 2021-05-11 09:01:02+00:00
- **Updated**: 2021-05-11 09:01:02+00:00
- **Authors**: Maximilian T. Fischer, Daniel A. Keim, Manuel Stein
- **Comment**: 9 pages, 4 figures, 1 table, 2nd International Workshop on Multimedia
  Content Analysis in Sports (MMSports '19)
- **Journal**: International Workshop on Multimedia Content Analysis in Sports
  (MMSports), 2019
- **Summary**: With the increasingly detailed investigation of game play and tactics in invasive team sports such as soccer, it becomes ever more important to present causes, actions and findings in a meaningful manner. Visualizations, especially when augmenting relevant information directly inside a video recording of a match, can significantly improve and simplify soccer match preparation and tactic planning. However, while many visualization techniques for soccer have been developed in recent years, few have been directly applied to the video-based analysis of soccer matches. This paper provides a comprehensive overview and categorization of the methods developed for the video-based visual analysis of soccer matches. While identifying the advantages and disadvantages of the individual approaches, we identify and discuss open research questions, soon enabling analysts to develop winning strategies more efficiently, do rapid failure analysis or identify weaknesses in opposing teams.



### Consistent Multiple Graph Embedding for Multi-View Clustering
- **Arxiv ID**: http://arxiv.org/abs/2105.04880v2
- **DOI**: 10.1109/TMM.2021.3136098
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2105.04880v2)
- **Published**: 2021-05-11 09:08:22+00:00
- **Updated**: 2021-12-20 12:51:00+00:00
- **Authors**: Yiming Wang, Dongxia Chang, Zhiqiang Fu, Yao Zhao
- **Comment**: None
- **Journal**: IEEE Transactions on Multimedia, 2021
- **Summary**: Graph-based multi-view clustering aiming to obtain a partition of data across multiple views, has received considerable attention in recent years. Although great efforts have been made for graph-based multi-view clustering, it remains a challenge to fuse characteristics from various views to learn a common representation for clustering. In this paper, we propose a novel Consistent Multiple Graph Embedding Clustering framework(CMGEC). Specifically, a multiple graph auto-encoder(M-GAE) is designed to flexibly encode the complementary information of multi-view data using a multi-graph attention fusion encoder. To guide the learned common representation maintaining the similarity of the neighboring characteristics in each view, a Multi-view Mutual Information Maximization module(MMIM) is introduced. Furthermore, a graph fusion network(GFN) is devised to explore the relationship among graphs from different views and provide a common consensus graph needed in M-GAE. By jointly training these models, the common latent representation can be obtained which encodes more complementary information from multiple views and depicts data more comprehensively. Experiments on three types of multi-view datasets demonstrate CMGEC outperforms the state-of-the-art clustering methods.



### Applications of Deep Learning Techniques for Automated Multiple Sclerosis Detection Using Magnetic Resonance Imaging: A Review
- **Arxiv ID**: http://arxiv.org/abs/2105.04881v2
- **DOI**: 10.1016/j.compbiomed.2021.104697
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.04881v2)
- **Published**: 2021-05-11 09:08:48+00:00
- **Updated**: 2021-08-09 04:30:23+00:00
- **Authors**: Afshin Shoeibi, Marjane Khodatars, Mahboobeh Jafari, Parisa Moridian, Mitra Rezaei, Roohallah Alizadehsani, Fahime Khozeimeh, Juan Manuel Gorriz, Jónathan Heras, Maryam Panahiazar, Saeid Nahavandi, U. Rajendra Acharya
- **Comment**: None
- **Journal**: Computers in Biology and Medicine,Volume 136,2021,104697
- **Summary**: Multiple Sclerosis (MS) is a type of brain disease which causes visual, sensory, and motor problems for people with a detrimental effect on the functioning of the nervous system. In order to diagnose MS, multiple screening methods have been proposed so far; among them, magnetic resonance imaging (MRI) has received considerable attention among physicians. MRI modalities provide physicians with fundamental information about the structure and function of the brain, which is crucial for the rapid diagnosis of MS lesions. Diagnosing MS using MRI is time-consuming, tedious, and prone to manual errors. Hence, computer aided diagnosis systems (CADS) based on artificial intelligence (AI) methods have been proposed in recent years for accurate diagnosis of MS using MRI neuroimaging modalities. In the AI field, automated MS diagnosis is being conducted using (i) conventional machine learning and (ii) deep learning (DL) techniques. The conventional machine learning approach is based on feature extraction and selection by trial and error. In DL, these steps are performed by the DL model itself. In this paper, a complete review of automated MS diagnosis methods performed using DL techniques with MRI neuroimaging modalities are discussed. Also, each work is thoroughly reviewed and discussed. Finally, the most important challenges and future directions in the automated MS diagnosis using DL techniques coupled with MRI modalities are presented in detail.



### Graph-based Neural Architecture Search with Operation Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2105.04885v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.04885v2)
- **Published**: 2021-05-11 09:17:10+00:00
- **Updated**: 2021-08-17 16:29:07+00:00
- **Authors**: Michail Chatzianastasis, George Dasoulas, Georgios Siolas, Michalis Vazirgiannis
- **Comment**: 12 pages, 10 figures
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) has recently gained increased attention, as a class of approaches that automatically searches in an input space of network architectures. A crucial part of the NAS pipeline is the encoding of the architecture that consists of the applied computational blocks, namely the operations and the links between them. Most of the existing approaches either fail to capture the structural properties of the architectures or use hand-engineered vector to encode the operator information. In this paper, we propose the replacement of fixed operator encoding with learnable representations in the optimization process. This approach, which effectively captures the relations of different operations, leads to smoother and more accurate representations of the architectures and consequently to improved performance of the end task. Our extensive evaluation in ENAS benchmark demonstrates the effectiveness of the proposed operation embeddings to the generation of highly accurate models, achieving state-of-the-art performance. Finally, our method produces top-performing architectures that share similar operation and graph patterns, highlighting a strong correlation between the structural properties of the architecture and its performance.



### Museum Painting Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2105.04891v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04891v1)
- **Published**: 2021-05-11 09:28:14+00:00
- **Updated**: 2021-05-11 09:28:14+00:00
- **Authors**: Òscar Lorente, Ian Riera, Shauryadeep Chaudhuri, Oriol Catalan, Víctor Casales
- **Comment**: None
- **Journal**: None
- **Summary**: To retrieve images based on their content is one of the most studied topics in the field of computer vision. Nowadays, this problem can be addressed using modern techniques such as feature extraction using machine learning, but over the years different classical methods have been developed. In this paper, we implement a query by example retrieval system for finding paintings in a museum image collection using classic computer vision techniques. Specifically, we study the performance of the color, texture, text and feature descriptors in datasets with different perturbations in the images: noise, overlapping text boxes, color corruption and rotation. We evaluate each of the cases using the Mean Average Precision (MAP) metric, and we obtain results that vary between 0.5 and 1.0 depending on the problem conditions.



### Image Classification with Classic and Deep Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/2105.04895v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04895v1)
- **Published**: 2021-05-11 09:32:38+00:00
- **Updated**: 2021-05-11 09:32:38+00:00
- **Authors**: Òscar Lorente, Ian Riera, Aditya Rana
- **Comment**: None
- **Journal**: None
- **Summary**: To classify images based on their content is one of the most studied topics in the field of computer vision. Nowadays, this problem can be addressed using modern techniques such as Convolutional Neural Networks (CNN), but over the years different classical methods have been developed. In this report, we implement an image classifier using both classic computer vision and deep learning techniques. Specifically, we study the performance of a Bag of Visual Words classifier using Support Vector Machines, a Multilayer Perceptron, an existing architecture named InceptionV3 and our own CNN, TinyNet, designed from scratch. We evaluate each of the cases in terms of accuracy and loss, and we obtain results that vary between 0.6 and 0.96 depending on the model and configuration used.



### Scene Understanding for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2105.04905v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04905v1)
- **Published**: 2021-05-11 09:50:05+00:00
- **Updated**: 2021-05-11 09:50:05+00:00
- **Authors**: Òscar Lorente, Ian Riera, Aditya Rana
- **Comment**: None
- **Journal**: None
- **Summary**: To detect and segment objects in images based on their content is one of the most active topics in the field of computer vision. Nowadays, this problem can be addressed using Deep Learning architectures such as Faster R-CNN or YOLO, among others. In this paper, we study the behaviour of different configurations of RetinaNet, Faster R-CNN and Mask R-CNN presented in Detectron2. First, we evaluate qualitatively and quantitatively (AP) the performance of the pre-trained models on KITTI-MOTS and MOTSChallenge datasets. We observe a significant improvement in performance after fine-tuning these models on the datasets of interest and optimizing hyperparameters. Finally, we run inference in unusual situations using out of context datasets, and present interesting results that help us understanding better the networks.



### VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.04906v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.04906v3)
- **Published**: 2021-05-11 09:53:21+00:00
- **Updated**: 2022-01-28 12:23:37+00:00
- **Authors**: Adrien Bardes, Jean Ponce, Yann LeCun
- **Comment**: Accepted at ICLR 2022
- **Journal**: None
- **Summary**: Recent self-supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image. A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. VICReg combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements.



### Video Surveillance for Road Traffic Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2105.04908v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04908v1)
- **Published**: 2021-05-11 09:54:20+00:00
- **Updated**: 2021-05-11 09:54:20+00:00
- **Authors**: Pol Albacar, Òscar Lorente, Eduard Mainou, Ian Riera
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents the learned techniques during the Video Analysis Module of the Master in Computer Vision from the Universitat Aut\`onoma de Barcelona, used to solve the third track of the AI-City Challenge. This challenge aims to track vehicles across multiple cameras placed in multiple intersections spread out over a city. The methodology followed focuses first in solving multi-tracking in a single camera and then extending it to multiple cameras. The qualitative results of the implemented techniques are presented using standard metrics for video analysis such as mAP for object detection and IDF1 for tracking. The source code is publicly available at: https://github.com/mcv-m6-video/mcv-m6-2021-team4.



### Pruning of Deep Spiking Neural Networks through Gradient Rewiring
- **Arxiv ID**: http://arxiv.org/abs/2105.04916v3
- **DOI**: 10.24963/ijcai.2021/236
- **Categories**: **cs.NE**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.04916v3)
- **Published**: 2021-05-11 10:05:53+00:00
- **Updated**: 2021-06-22 08:38:17+00:00
- **Authors**: Yanqi Chen, Zhaofei Yu, Wei Fang, Tiejun Huang, Yonghong Tian
- **Comment**: 9 pages, 7 figures, 4 tables. To appear in the 30th International
  Joint Conference on Artificial Intelligence (IJCAI 2021)
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNNs) have been attached great importance due to their biological plausibility and high energy-efficiency on neuromorphic chips. As these chips are usually resource-constrained, the compression of SNNs is thus crucial along the road of practical use of SNNs. Most existing methods directly apply pruning approaches in artificial neural networks (ANNs) to SNNs, which ignore the difference between ANNs and SNNs, thus limiting the performance of the pruned SNNs. Besides, these methods are only suitable for shallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination in the neural system, we propose gradient rewiring (Grad R), a joint learning algorithm of connectivity and weight for SNNs, that enables us to seamlessly optimize network structure without retraining. Our key innovation is to redefine the gradient to a new synaptic parameter, allowing better exploration of network structures by taking full advantage of the competition between pruning and regrowth of connections. The experimental results show that the proposed method achieves minimal loss of SNNs' performance on MNIST and CIFAR-10 dataset so far. Moreover, it reaches a $\sim$3.5% accuracy loss under unprecedented 0.73% connectivity, which reveals remarkable structure refining capability in SNNs. Our work suggests that there exists extremely high redundancy in deep SNNs. Our codes are available at https://github.com/Yanqi-Chen/Gradient-Rewiring.



### One Shot Face Swapping on Megapixels
- **Arxiv ID**: http://arxiv.org/abs/2105.04932v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04932v2)
- **Published**: 2021-05-11 10:41:47+00:00
- **Updated**: 2022-03-19 09:22:41+00:00
- **Authors**: Yuhao Zhu, Qi Li, Jian Wang, Chengzhong Xu, Zhenan Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Face swapping has both positive applications such as entertainment, human-computer interaction, etc., and negative applications such as DeepFake threats to politics, economics, etc. Nevertheless, it is necessary to understand the scheme of advanced methods for high-quality face swapping and generate enough and representative face swapping images to train DeepFake detection algorithms. This paper proposes the first Megapixel level method for one shot Face Swapping (or MegaFS for short). Firstly, MegaFS organizes face representation hierarchically by the proposed Hierarchical Representation Face Encoder (HieRFE) in an extended latent space to maintain more facial details, rather than compressed representation in previous face swapping methods. Secondly, a carefully designed Face Transfer Module (FTM) is proposed to transfer the identity from a source image to the target by a non-linear trajectory without explicit feature disentanglement. Finally, the swapped faces can be synthesized by StyleGAN2 with the benefits of its training stability and powerful generative capability. Each part of MegaFS can be trained separately so the requirement of our model for GPU memory can be satisfied for megapixel face swapping. In summary, complete face representation, stable training, and limited memory usage are the three novel contributions to the success of our method. Extensive experiments demonstrate the superiority of MegaFS and the first megapixel level face swapping database is released for research on DeepFake detection and face image editing in the public domain. The dataset is at this link.



### Task-Related Self-Supervised Learning for Remote Sensing Image Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.04951v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.04951v2)
- **Published**: 2021-05-11 11:44:04+00:00
- **Updated**: 2021-05-23 06:41:34+00:00
- **Authors**: Zhinan Cai, Zhiyu Jiang, Yuan Yuan
- **Comment**: IEEE ICASSP 2021
- **Journal**: None
- **Summary**: Change detection for remote sensing images is widely applied for urban change detection, disaster assessment and other fields. However, most of the existing CNN-based change detection methods still suffer from the problem of inadequate pseudo-changes suppression and insufficient feature representation. In this work, an unsupervised change detection method based on Task-related Self-supervised Learning Change Detection network with smooth mechanism(TSLCD) is proposed to eliminate it. The main contributions include: (1) the task-related self-supervised learning module is introduced to extract spatial features more effectively. (2) a hard-sample-mining loss function is applied to pay more attention to the hard-to-classify samples. (3) a smooth mechanism is utilized to remove some of pseudo-changes and noise. Experiments on four remote sensing change detection datasets reveal that the proposed TSLCD method achieves the state-of-the-art for change detection task.



### Open Set Domain Recognition via Attention-Based GCN and Semantic Matching Optimization
- **Arxiv ID**: http://arxiv.org/abs/2105.04967v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04967v2)
- **Published**: 2021-05-11 12:05:36+00:00
- **Updated**: 2021-05-12 02:16:33+00:00
- **Authors**: Xinxing He, Yuan Yuan, Zhiyu Jiang
- **Comment**: ICPR 2020
- **Journal**: None
- **Summary**: Open set domain recognition has got the attention in recent years. The task aims to specifically classify each sample in the practical unlabeled target domain, which consists of all known classes in the manually labeled source domain and target-specific unknown categories. The absence of annotated training data or auxiliary attribute information for unknown categories makes this task especially difficult. Moreover, exiting domain discrepancy in label space and data distribution further distracts the knowledge transferred from known classes to unknown classes. To address these issues, this work presents an end-to-end model based on attention-based GCN and semantic matching optimization, which first employs the attention mechanism to enable the central node to learn more discriminating representations from its neighbors in the knowledge graph. Moreover, a coarse-to-fine semantic matching optimization approach is proposed to progressively bridge the domain gap. Experimental results validate that the proposed model not only has superiority on recognizing the images of known and unknown classes, but also can adapt to various openness of the target domain.



### A Comparison of Multi-View Learning Strategies for Satellite Image-Based Real Estate Appraisal
- **Arxiv ID**: http://arxiv.org/abs/2105.04984v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04984v1)
- **Published**: 2021-05-11 12:41:22+00:00
- **Updated**: 2021-05-11 12:41:22+00:00
- **Authors**: Jan-Peter Kucklick, Oliver Müller
- **Comment**: Presented at: The AAAI-21 Workshop on Knowledge Discovery from
  Unstructured Data in Financial Services
- **Journal**: None
- **Summary**: In the house credit process, banks and lenders rely on a fast and accurate estimation of a real estate price to determine the maximum loan value. Real estate appraisal is often based on relational data, capturing the hard facts of the property. Yet, models benefit strongly from including image data, capturing additional soft factors. The combination of the different data types requires a multi-view learning method. Therefore, the question arises which strengths and weaknesses different multi-view learning strategies have. In our study, we test multi-kernel learning, multi-view concatenation and multi-view neural networks on real estate data and satellite images from Asheville, NC. Our results suggest that multi-view learning increases the predictive performance up to 13% in MAE. Multi-view neural networks perform best, however result in intransparent black-box models. For users seeking interpretability, hybrid multi-view neural networks or a boosting strategy are a suitable alternative.



### Weighted Hierarchical Sparse Representation for Hyperspectral Target Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.04990v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.04990v1)
- **Published**: 2021-05-11 12:50:16+00:00
- **Updated**: 2021-05-11 12:50:16+00:00
- **Authors**: Chenlu Wei, Zhiyu Jiang, Yuan Yuan
- **Comment**: IGARSS 2020
- **Journal**: None
- **Summary**: Hyperspectral target detection has been widely studied in the field of remote sensing. However, background dictionary building issue and the correlation analysis of target and background dictionary issue have not been well studied. To tackle these issues, a \emph{Weighted Hierarchical Sparse Representation} for hyperspectral target detection is proposed. The main contributions of this work are listed as follows. 1) Considering the insufficient representation of the traditional background dictionary building by dual concentric window structure, a hierarchical background dictionary is built considering the local and global spectral information simultaneously. 2) To reduce the impureness impact of background dictionary, target scores from target dictionary and background dictionary are weighted considered according to the dictionary quality. Three hyperspectral target detection data sets are utilized to verify the effectiveness of the proposed method. And the experimental results show a better performance when compared with the state-of-the-arts.



### Instance-aware Remote Sensing Image Captioning with Cross-hierarchy Attention
- **Arxiv ID**: http://arxiv.org/abs/2105.04996v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.04996v1)
- **Published**: 2021-05-11 12:59:07+00:00
- **Updated**: 2021-05-11 12:59:07+00:00
- **Authors**: Chengze Wang, Zhiyu Jiang, Yuan Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: The spatial attention is a straightforward approach to enhance the performance for remote sensing image captioning. However, conventional spatial attention approaches consider only the attention distribution on one fixed coarse grid, resulting in the semantics of tiny objects can be easily ignored or disturbed during the visual feature extraction. Worse still, the fixed semantic level of conventional spatial attention limits the image understanding in different levels and perspectives, which is critical for tackling the huge diversity in remote sensing images. To address these issues, we propose a remote sensing image caption generator with instance-awareness and cross-hierarchy attention. 1) The instances awareness is achieved by introducing a multi-level feature architecture that contains the visual information of multi-level instance-possible regions and their surroundings. 2) Moreover, based on this multi-level feature extraction, a cross-hierarchy attention mechanism is proposed to prompt the decoder to dynamically focus on different semantic hierarchies and instances at each time step. The experimental results on public datasets demonstrate the superiority of proposed approach over existing methods.



### CondLaneNet: a Top-to-down Lane Detection Framework Based on Conditional Convolution
- **Arxiv ID**: http://arxiv.org/abs/2105.05003v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.05003v3)
- **Published**: 2021-05-11 13:10:34+00:00
- **Updated**: 2023-02-10 08:38:34+00:00
- **Authors**: Lizhe Liu, Xiaohao Chen, Siyu Zhu, Ping Tan
- **Comment**: None
- **Journal**: Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV), 2021, pp. 3773-3782
- **Summary**: Modern deep-learning-based lane detection methods are successful in most scenarios but struggling for lane lines with complex topologies. In this work, we propose CondLaneNet, a novel top-to-down lane detection framework that detects the lane instances first and then dynamically predicts the line shape for each instance. Aiming to resolve lane instance-level discrimination problem, we introduce a conditional lane detection strategy based on conditional convolution and row-wise formulation. Further, we design the Recurrent Instance Module(RIM) to overcome the problem of detecting lane lines with complex topologies such as dense lines and fork lines. Benefit from the end-to-end pipeline which requires little post-process, our method has real-time efficiency. We extensively evaluate our method on three benchmarks of lane detection. Results show that our method achieves state-of-the-art performance on all three benchmark datasets. Moreover, our method has the coexistence of accuracy and efficiency, e.g. a 78.14 F1 score and 220 FPS on CULane. Our code is available at https://github.com/aliyun/conditional-lane-detection.



### Let There be Light: Improved Traffic Surveillance via Detail Preserving Night-to-Day Transfer
- **Arxiv ID**: http://arxiv.org/abs/2105.05011v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.05011v1)
- **Published**: 2021-05-11 13:18:50+00:00
- **Updated**: 2021-05-11 13:18:50+00:00
- **Authors**: Lan Fu, Hongkai Yu, Felix Juefei-Xu, Jinlong Li, Qing Guo, Song Wang
- **Comment**: to appear in TCSVT 2021
- **Journal**: None
- **Summary**: In recent years, image and video surveillance have made considerable progresses to the Intelligent Transportation Systems (ITS) with the help of deep Convolutional Neural Networks (CNNs). As one of the state-of-the-art perception approaches, detecting the interested objects in each frame of video surveillance is widely desired by ITS. Currently, object detection shows remarkable efficiency and reliability in standard scenarios such as daytime scenes with favorable illumination conditions. However, in face of adverse conditions such as the nighttime, object detection loses its accuracy significantly. One of the main causes of the problem is the lack of sufficient annotated detection datasets of nighttime scenes. In this paper, we propose a framework to alleviate the accuracy decline when object detection is taken to adverse conditions by using image translation method. We propose to utilize style translation based StyleMix method to acquire pairs of day time image and nighttime image as training data for following nighttime to daytime image translation. To alleviate the detail corruptions caused by Generative Adversarial Networks (GANs), we propose to utilize Kernel Prediction Network (KPN) based method to refine the nighttime to daytime image translation. The KPN network is trained with object detection task together to adapt the trained daytime model to nighttime vehicle detection directly. Experiments on vehicle detection verified the accuracy and effectiveness of the proposed approach.



### Semantic Distribution-aware Contrastive Adaptation for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.05013v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.05013v1)
- **Published**: 2021-05-11 13:21:25+00:00
- **Updated**: 2021-05-11 13:21:25+00:00
- **Authors**: Shuang Li, Binhui Xie, Bin Zang, Chi Harold Liu, Xinjing Cheng, Ruigang Yang, Guoren Wang
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Domain adaptive semantic segmentation refers to making predictions on a certain target domain with only annotations of a specific source domain. Current state-of-the-art works suggest that performing category alignment can alleviate domain shift reasonably. However, they are mainly based on image-to-image adversarial training and little consideration is given to semantic variations of an object among images, failing to capture a comprehensive picture of different categories. This motivates us to explore a holistic representative, the semantic distribution from each category in source domain, to mitigate the problem above. In this paper, we present semantic distribution-aware contrastive adaptation algorithm that enables pixel-wise representation alignment under the guidance of semantic distributions. Specifically, we first design a pixel-wise contrastive loss by considering the correspondences between semantic distributions and pixel-wise representations from both domains. Essentially, clusters of pixel representations from the same category should cluster together and those from different categories should spread out. Next, an upper bound on this formulation is derived by involving the learning of an infinite number of (dis)similar pairs, making it efficient. Finally, we verify that SDCA can further improve segmentation accuracy when integrated with the self-supervised learning. We evaluate SDCA on multiple benchmarks, achieving considerable improvements over existing algorithms.The code is publicly available at https://github.com/BIT-DA/SDCA



### BikNN: Anomaly Estimation in Bilateral Domains with k-Nearest Neighbors
- **Arxiv ID**: http://arxiv.org/abs/2105.05037v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2105.05037v1)
- **Published**: 2021-05-11 13:45:29+00:00
- **Updated**: 2021-05-11 13:45:29+00:00
- **Authors**: Zhongping Ji
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: In this paper, a novel framework for anomaly estimation is proposed. The basic idea behind our method is to reduce the data into a two-dimensional space and then rank each data point in the reduced space. We attempt to estimate the degree of anomaly in both spatial and density domains. Specifically, we transform the data points into a density space and measure the distances in density domain between each point and its k-Nearest Neighbors in spatial domain. Then, an anomaly coordinate system is built by collecting two unilateral anomalies from k-nearest neighbors of each point. Further more, we introduce two schemes to model their correlation and combine them to get the final anomaly score. Experiments performed on the synthetic and real world datasets demonstrate that the proposed method performs well and achieve highest average performance. We also show that the proposed method can provide visualization and classification of the anomalies in a simple manner. Due to the complexity of the anomaly, none of the existing methods can perform best on all benchmark datasets. Our method takes into account both the spatial domain and the density domain and can be adapted to different datasets by adjusting a few parameters manually.



### ChaLearn LAP Large Scale Signer Independent Isolated Sign Language Recognition Challenge: Design, Results and Future Research
- **Arxiv ID**: http://arxiv.org/abs/2105.05066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.05066v1)
- **Published**: 2021-05-11 14:17:39+00:00
- **Updated**: 2021-05-11 14:17:39+00:00
- **Authors**: Ozge Mercanoglu Sincan, Julio C. S. Jacques Junior, Sergio Escalera, Hacer Yalim Keles
- **Comment**: Preprint of the accepted paper at ChaLearn Looking at People Sign
  Language Recognition in the Wild Workshop at CVPR 2021
- **Journal**: None
- **Summary**: The performances of Sign Language Recognition (SLR) systems have improved considerably in recent years. However, several open challenges still need to be solved to allow SLR to be useful in practice. The research in the field is in its infancy in regards to the robustness of the models to a large diversity of signs and signers, and to fairness of the models to performers from different demographics. This work summarises the ChaLearn LAP Large Scale Signer Independent Isolated SLR Challenge, organised at CVPR 2021 with the goal of overcoming some of the aforementioned challenges. We analyse and discuss the challenge design, top winning solutions and suggestions for future research. The challenge attracted 132 participants in the RGB track and 59 in the RGB+Depth track, receiving more than 1.5K submissions in total. Participants were evaluated using a new large-scale multi-modal Turkish Sign Language (AUTSL) dataset, consisting of 226 sign labels and 36,302 isolated sign video samples performed by 43 different signers. Winning teams achieved more than 96% recognition rate, and their approaches benefited from pose/hand/face estimation, transfer learning, external data, fusion/ensemble of modalities and different strategies to model spatio-temporal information. However, methods still fail to distinguish among very similar signs, in particular those sharing similar hand trajectories.



### Research on Mosaic Image Data Enhancement for Overlapping Ship Targets
- **Arxiv ID**: http://arxiv.org/abs/2105.05090v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.05090v1)
- **Published**: 2021-05-11 14:44:03+00:00
- **Updated**: 2021-05-11 14:44:03+00:00
- **Authors**: Guangmiao Zeng, Wanneng Yu, Rongjie Wang, Anhui Lin
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of overlapping occlusion in target recognition has been a difficult research problem, and the situation of mutual occlusion of ship targets in narrow waters still exists. In this paper, an improved mosaic data enhancement method is proposed, which optimizes the reading method of the data set, strengthens the learning ability of the detection algorithm for local features, improves the recognition accuracy of overlapping targets while keeping the test speed unchanged, reduces the decay rate of recognition ability under different resolutions, and strengthens the robustness of the algorithm. The real test experiments prove that, relative to the original algorithm, the improved algorithm improves the recognition accuracy of overlapping targets by 2.5%, reduces the target loss time by 17%, and improves the recognition stability under different video resolutions by 27.01%.



### DeepLight: Robust & Unobtrusive Real-time Screen-Camera Communication for Real-World Displays
- **Arxiv ID**: http://arxiv.org/abs/2105.05092v1
- **DOI**: 10.1145/3412382.3458269
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.05092v1)
- **Published**: 2021-05-11 14:44:12+00:00
- **Updated**: 2021-05-11 14:44:12+00:00
- **Authors**: Vu Tran, Gihan Jayatilaka, Ashwin Ashok, Archan Misra
- **Comment**: Accepted for IPSN 2021 (ACM/IEEE International Conference on
  Information Processing in Sensor Networks 2021)
- **Journal**: None
- **Summary**: The paper introduces a novel, holistic approach for robust Screen-Camera Communication (SCC), where video content on a screen is visually encoded in a human-imperceptible fashion and decoded by a camera capturing images of such screen content. We first show that state-of-the-art SCC techniques have two key limitations for in-the-wild deployment: (a) the decoding accuracy drops rapidly under even modest screen extraction errors from the captured images, and (b) they generate perceptible flickers on common refresh rate screens even with minimal modulation of pixel intensity. To overcome these challenges, we introduce DeepLight, a system that incorporates machine learning (ML) models in the decoding pipeline to achieve humanly-imperceptible, moderately high SCC rates under diverse real-world conditions. Deep-Light's key innovation is the design of a Deep Neural Network (DNN) based decoder that collectively decodes all the bits spatially encoded in a display frame, without attempting to precisely isolate the pixels associated with each encoded bit. In addition, DeepLight supports imperceptible encoding by selectively modulating the intensity of only the Blue channel, and provides reasonably accurate screen extraction (IoU values >= 83%) by using state-of-the-art object detection DNN pipelines. We show that a fully functional DeepLight system is able to robustly achieve high decoding accuracy (frame error rate < 0.2) and moderately-high data goodput (>=0.95Kbps) using a human-held smartphone camera, even over larger screen-camera distances (approx =2m).



### Segmentation of Anatomical Layers and Artifacts in Intravascular Polarization Sensitive Optical Coherence Tomography Using Attending Physician and Boundary Cardinality Losses
- **Arxiv ID**: http://arxiv.org/abs/2105.05137v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.05137v3)
- **Published**: 2021-05-11 15:52:31+00:00
- **Updated**: 2022-09-14 02:46:22+00:00
- **Authors**: Mohammad Haft-Javaherian, Martin Villiger, Kenichiro Otsuka, Joost Daemen, Peter Libby, Polina Golland, Brett E. Bouma
- **Comment**: None
- **Journal**: None
- **Summary**: Intravascular ultrasound and optical coherence tomography are widely available for characterizing coronary stenoses and provide critical vessel parameters to optimize percutaneous intervention. Intravascular polarization-sensitive optical coherence tomography (PS-OCT) simultaneously provides high-resolution cross-sectional images of vascular structures while also revealing preponderant tissue components such as collagen and smooth muscle and thereby enhances plaque characterization. Automated interpretation of these features promises to facilitate the objective clinical investigation of the natural history and significance of coronary atheromas. Here, we propose a convolutional neural network model, optimized using a new multi-term loss function, to classify the lumen, intima, and media layers in addition to the guidewire and plaque shadows. We demonstrate that our multi-class classification model outperforms state-of-the-art methods in detecting the coronary anatomical layers. Furthermore, the proposed model segments two classes of common imaging artifacts and detects the anatomical layers within the thickened vessel wall regions that were excluded from analysis by other studies. The source code and the trained model are publicly available at https://github.com/mhaft/OCTseg



### Visual Perspective Taking for Opponent Behavior Modeling
- **Arxiv ID**: http://arxiv.org/abs/2105.05145v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2105.05145v1)
- **Published**: 2021-05-11 16:02:32+00:00
- **Updated**: 2021-05-11 16:02:32+00:00
- **Authors**: Boyuan Chen, Yuhang Hu, Robert Kwiatkowski, Shuran Song, Hod Lipson
- **Comment**: ICRA 2021. Website: http://www.cs.columbia.edu/~bchen/vpttob/
- **Journal**: None
- **Summary**: In order to engage in complex social interaction, humans learn at a young age to infer what others see and cannot see from a different point-of-view, and learn to predict others' plans and behaviors. These abilities have been mostly lacking in robots, sometimes making them appear awkward and socially inept. Here we propose an end-to-end long-term visual prediction framework for robots to begin to acquire both these critical cognitive skills, known as Visual Perspective Taking (VPT) and Theory of Behavior (TOB). We demonstrate our approach in the context of visual hide-and-seek - a game that represents a cognitive milestone in human development. Unlike traditional visual predictive model that generates new frames from immediate past frames, our agent can directly predict to multiple future timestamps (25s), extrapolating by 175% beyond the training horizon. We suggest that visual behavior modeling and perspective taking skills will play a critical role in the ability of physical robots to fully integrate into real-world multi-agent activities. Our website is at http://www.cs.columbia.edu/~bchen/vpttob/.



### AdaMML: Adaptive Multi-Modal Learning for Efficient Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2105.05165v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.05165v2)
- **Published**: 2021-05-11 16:19:07+00:00
- **Updated**: 2021-05-12 17:49:10+00:00
- **Authors**: Rameswar Panda, Chun-Fu Chen, Quanfu Fan, Ximeng Sun, Kate Saenko, Aude Oliva, Rogerio Feris
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-modal learning, which focuses on utilizing various modalities to improve the performance of a model, is widely used in video recognition. While traditional multi-modal learning offers excellent recognition results, its computational expense limits its impact for many real-world applications. In this paper, we propose an adaptive multi-modal learning framework, called AdaMML, that selects on-the-fly the optimal modalities for each segment conditioned on the input for efficient video recognition. Specifically, given a video segment, a multi-modal policy network is used to decide what modalities should be used for processing by the recognition model, with the goal of improving both accuracy and efficiency. We efficiently train the policy network jointly with the recognition model using standard back-propagation. Extensive experiments on four challenging diverse datasets demonstrate that our proposed adaptive approach yields 35%-55% reduction in computation when compared to the traditional baseline that simply uses all the modalities irrespective of the input, while also achieving consistent improvements in accuracy over the state-of-the-art methods.



### Development of a Multi-Task Learning V-Net for Pulmonary Lobar Segmentation on Computed Tomography and Application to Diseased Lungs
- **Arxiv ID**: http://arxiv.org/abs/2105.05204v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.05204v1)
- **Published**: 2021-05-11 17:10:25+00:00
- **Updated**: 2021-05-11 17:10:25+00:00
- **Authors**: Marc Boubnovski Martell, Mitchell Chen, Kristofer Linton-Reid, Joram M. Posma, Susan J Copley, Eric O. Aboagye
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: Automated lobar segmentation allows regional evaluation of lung disease and is important for diagnosis and therapy planning. Advanced statistical workflows permitting such evaluation is a needed area within respiratory medicine; their adoption remains slow, with poor workflow accuracy. Diseased lung regions often produce high-density zones on CT images, limiting an algorithm's execution to specify damaged lobes due to oblique or lacking fissures. This impact motivated developing an improved machine learning method to segment lung lobes that utilises tracheobronchial tree information to enhance segmentation accuracy through the algorithm's spatial familiarity to define lobar extent more accurately. The method undertakes parallel segmentation of lobes and auxiliary tissues simultaneously by employing multi-task learning (MTL) in conjunction with V-Net-attention, a popular convolutional neural network in the imaging realm. In keeping with the model's adeptness for better generalisation, high performance was retained in an external dataset of patients with four distinct diseases: severe lung cancer, COVID-19 pneumonitis, collapsed lungs and Chronic Obstructive Pulmonary Disease (COPD), even though the training data included none of these cases. The benefit of our external validation test is specifically relevant since our choice includes those patients who have diagnosed lung disease with associated radiological abnormalities. To ensure equal rank is given to all segmentations in the main task we report the following performance (Dice score) on a per-segment basis: normal lungs 0.97, COPD 0.94, lung cancer 0.94, COVID-19 pneumonitis 0.94 and collapsed lung 0.92, all at p<0.05. Even segmenting lobes with large deformations on CT images, the model maintained high accuracy. The approach can be readily adopted in the clinical setting as a robust tool for radiologists.



### Rethinking of Radar's Role: A Camera-Radar Dataset and Systematic Annotator via Coordinate Alignment
- **Arxiv ID**: http://arxiv.org/abs/2105.05207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.05207v1)
- **Published**: 2021-05-11 17:13:45+00:00
- **Updated**: 2021-05-11 17:13:45+00:00
- **Authors**: Yizhou Wang, Gaoang Wang, Hung-Min Hsu, Hui Liu, Jenq-Neng Hwang
- **Comment**: 10 pages, 7 figures, 6 tables, CVPR 2021 Workshop on Autonomous
  Driving
- **Journal**: None
- **Summary**: Radar has long been a common sensor on autonomous vehicles for obstacle ranging and speed estimation. However, as a robust sensor to all-weather conditions, radar's capability has not been well-exploited, compared with camera or LiDAR. Instead of just serving as a supplementary sensor, radar's rich information hidden in the radio frequencies can potentially provide useful clues to achieve more complicated tasks, like object classification and detection. In this paper, we propose a new dataset, named CRUW, with a systematic annotator and performance evaluation system to address the radar object detection (ROD) task, which aims to classify and localize the objects in 3D purely from radar's radio frequency (RF) images. To the best of our knowledge, CRUW is the first public large-scale dataset with a systematic annotation and evaluation system, which involves camera RGB images and radar RF images, collected in various driving scenarios.



### ReflectNet -- A Generative Adversarial Method for Single Image Reflection Suppression
- **Arxiv ID**: http://arxiv.org/abs/2105.05216v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.05216v1)
- **Published**: 2021-05-11 17:33:40+00:00
- **Updated**: 2021-05-11 17:33:40+00:00
- **Authors**: Andreea Birhala, Ionut Mironica
- **Comment**: None
- **Journal**: None
- **Summary**: Taking pictures through glass windows almost always produces undesired reflections that degrade the quality of the photo. The ill-posed nature of the reflection removal problem reached the attention of many researchers for more than decades. The main challenge of this problem is the lack of real training data and the necessity of generating realistic synthetic data. In this paper, we proposed a single image reflection removal method based on context understanding modules and adversarial training to efficiently restore the transmission layer without reflection. We also propose a complex data generation model in order to create a large training set with various type of reflections. Our proposed reflection removal method outperforms state-of-the-art methods in terms of PSNR and SSIM on the SIR benchmark dataset.



### Representation Learning via Global Temporal Alignment and Cycle-Consistency
- **Arxiv ID**: http://arxiv.org/abs/2105.05217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.05217v1)
- **Published**: 2021-05-11 17:34:04+00:00
- **Updated**: 2021-05-11 17:34:04+00:00
- **Authors**: Isma Hadji, Konstantinos G. Derpanis, Allan D. Jepson
- **Comment**: accepted to CVPR 2021
- **Journal**: None
- **Summary**: We introduce a weakly supervised method for representation learning based on aligning temporal sequences (e.g., videos) of the same process (e.g., human action). The main idea is to use the global temporal ordering of latent correspondences across sequence pairs as a supervisory signal. In particular, we propose a loss based on scoring the optimal sequence alignment to train an embedding network. Our loss is based on a novel probabilistic path finding view of dynamic time warping (DTW) that contains the following three key features: (i) the local path routing decisions are contrastive and differentiable, (ii) pairwise distances are cast as probabilities that are contrastive as well, and (iii) our formulation naturally admits a global cycle consistency loss that verifies correspondences. For evaluation, we consider the tasks of fine-grained action classification, few shot learning, and video synchronization. We report significant performance increases over previous methods. In addition, we report two applications of our temporal alignment framework, namely 3D pose reconstruction and fine-grained audio/visual retrieval.



### Home Action Genome: Cooperative Compositional Action Understanding
- **Arxiv ID**: http://arxiv.org/abs/2105.05226v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.05226v1)
- **Published**: 2021-05-11 17:42:47+00:00
- **Updated**: 2021-05-11 17:42:47+00:00
- **Authors**: Nishant Rai, Haofeng Chen, Jingwei Ji, Rishi Desai, Kazuki Kozuka, Shun Ishizaka, Ehsan Adeli, Juan Carlos Niebles
- **Comment**: CVPR '21
- **Journal**: None
- **Summary**: Existing research on action recognition treats activities as monolithic events occurring in videos. Recently, the benefits of formulating actions as a combination of atomic-actions have shown promise in improving action understanding with the emergence of datasets containing such annotations, allowing us to learn representations capturing this information. However, there remains a lack of studies that extend action composition and leverage multiple viewpoints and multiple modalities of data for representation learning. To promote research in this direction, we introduce Home Action Genome (HOMAGE): a multi-view action dataset with multiple modalities and view-points supplemented with hierarchical activity and atomic action labels together with dense scene composition labels. Leveraging rich multi-modal and multi-view settings, we propose Cooperative Compositional Action Understanding (CCAU), a cooperative learning framework for hierarchical action recognition that is aware of compositional action elements. CCAU shows consistent performance improvements across all modalities. Furthermore, we demonstrate the utility of co-learning compositions in few-shot action recognition by achieving 28.6% mAP with just a single sample.



### Diffusion Models Beat GANs on Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2105.05233v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2105.05233v4)
- **Published**: 2021-05-11 17:50:24+00:00
- **Updated**: 2021-06-01 17:49:49+00:00
- **Authors**: Prafulla Dhariwal, Alex Nichol
- **Comment**: Added compute requirements, ImageNet 256$\times$256 upsampling FID
  and samples, DDIM guided sampler, fixed typos
- **Journal**: None
- **Summary**: We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\times$128, 4.59 on ImageNet 256$\times$256, and 7.72 on ImageNet 512$\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\times$256 and 3.85 on ImageNet 512$\times$512. We release our code at https://github.com/openai/guided-diffusion



### One-shot Compositional Data Generation for Low Resource Handwritten Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2105.05300v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.05300v2)
- **Published**: 2021-05-11 18:53:01+00:00
- **Updated**: 2021-10-05 14:04:43+00:00
- **Authors**: Mohamed Ali Souibgui, Ali Furkan Biten, Sounak Dey, Alicia Fornés, Yousri Kessentini, Lluis Gomez, Dimosthenis Karatzas, Josep Lladós
- **Comment**: Accepted in WACV 2022
- **Journal**: None
- **Summary**: Low resource Handwritten Text Recognition (HTR) is a hard problem due to the scarce annotated data and the very limited linguistic information (dictionaries and language models). For example, in the case of historical ciphered manuscripts, which are usually written with invented alphabets to hide the message contents. Thus, in this paper we address this problem through a data generation technique based on Bayesian Program Learning (BPL). Contrary to traditional generation approaches, which require a huge amount of annotated images, our method is able to generate human-like handwriting using only one sample of each symbol in the alphabet. After generating symbols, we create synthetic lines to train state-of-the-art HTR architectures in a segmentation free fashion. Quantitative and qualitative analyses were carried out and confirm the effectiveness of the proposed method.



### Collaborative Regression of Expressive Bodies using Moderation
- **Arxiv ID**: http://arxiv.org/abs/2105.05301v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.05301v2)
- **Published**: 2021-05-11 18:55:59+00:00
- **Updated**: 2021-10-15 21:05:05+00:00
- **Authors**: Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios Tzionas, Michael J. Black
- **Comment**: 21 pages. The first two authors contributed equally to this work
- **Journal**: None
- **Summary**: Recovering expressive humans from images is essential for understanding human behavior. Methods that estimate 3D bodies, faces, or hands have progressed significantly, yet separately. Face methods recover accurate 3D shape and geometric details, but need a tight crop and struggle with extreme views and low resolution. Whole-body methods are robust to a wide range of poses and resolutions, but provide only a rough 3D face shape without details like wrinkles. To get the best of both worlds, we introduce PIXIE, which produces animatable, whole-body 3D avatars with realistic facial detail, from a single image. For this, PIXIE uses two key observations. First, existing work combines independent estimates from body, face, and hand experts, by trusting them equally. PIXIE introduces a novel moderator that merges the features of the experts, weighted by their confidence. All part experts can contribute to the whole, using SMPL-X's shared shape space across all body parts. Second, human shape is highly correlated with gender, but existing work ignores this. We label training images as male, female, or non-binary, and train PIXIE to infer "gendered" 3D body shapes with a novel shape loss. In addition to 3D body pose and shape parameters, PIXIE estimates expression, illumination, albedo and 3D facial surface displacements. Quantitative and qualitative evaluation shows that PIXIE estimates more accurate whole-body shape and detailed face shape than the state of the art. Models and code are available at https://pixie.is.tue.mpg.de.



### Incremental Few-Shot Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.05312v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.05312v1)
- **Published**: 2021-05-11 19:15:02+00:00
- **Updated**: 2021-05-11 19:15:02+00:00
- **Authors**: Dan Andrei Ganea, Bas Boom, Ronald Poppe
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Few-shot instance segmentation methods are promising when labeled training data for novel classes is scarce. However, current approaches do not facilitate flexible addition of novel classes. They also require that examples of each class are provided at train and test time, which is memory intensive. In this paper, we address these limitations by presenting the first incremental approach to few-shot instance segmentation: iMTFA. We learn discriminative embeddings for object instances that are merged into class representatives. Storing embedding vectors rather than images effectively solves the memory overhead problem. We match these class embeddings at the RoI-level using cosine similarity. This allows us to add new classes without the need for further training or access to previous training data. In a series of experiments, we consistently outperform the current state-of-the-art. Moreover, the reduced memory requirements allow us to evaluate, for the first time, few-shot instance segmentation performance on all classes in COCO jointly.



### GANs for Medical Image Synthesis: An Empirical Study
- **Arxiv ID**: http://arxiv.org/abs/2105.05318v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.05318v2)
- **Published**: 2021-05-11 19:21:39+00:00
- **Updated**: 2021-07-19 11:51:58+00:00
- **Authors**: Youssef Skandarani, Pierre-Marc Jodoin, Alain Lalande
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have become increasingly powerful, generating mind-blowing photorealistic images that mimic the content of datasets they were trained to replicate. One recurrent theme in medical imaging is whether GANs can also be effective at generating workable medical data as they are for generating realistic RGB images. In this paper, we perform a multi-GAN and multi-application study to gauge the benefits of GANs in medical imaging. We tested various GAN architectures from basic DCGAN to more sophisticated style-based GANs on three medical imaging modalities and organs namely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on well-known and widely utilized datasets from which their FID score were computed to measure the visual acuity of their generated images. We further tested their usefulness by measuring the segmentation accuracy of a U-Net trained on these generated images.   Results reveal that GANs are far from being equal as some are ill-suited for medical imaging applications while others are much better off. The top-performing GANs are capable of generating realistic-looking medical images by FID standards that can fool trained experts in a visual Turing test and comply to some metrics. However, segmentation results suggests that no GAN is capable of reproducing the full richness of a medical datasets.



### The DEVIL is in the Details: A Diagnostic Evaluation Benchmark for Video Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2105.05332v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.05332v2)
- **Published**: 2021-05-11 20:13:53+00:00
- **Updated**: 2022-04-25 16:18:39+00:00
- **Authors**: Ryan Szeto, Jason J. Corso
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Quantitative evaluation has increased dramatically among recent video inpainting work, but the video and mask content used to gauge performance has received relatively little attention. Although attributes such as camera and background scene motion inherently change the difficulty of the task and affect methods differently, existing evaluation schemes fail to control for them, thereby providing minimal insight into inpainting failure modes. To address this gap, we propose the Diagnostic Evaluation of Video Inpainting on Landscapes (DEVIL) benchmark, which consists of two contributions: (i) a novel dataset of videos and masks labeled according to several key inpainting failure modes, and (ii) an evaluation scheme that samples slices of the dataset characterized by a fixed content attribute, and scores performance on each slice according to reconstruction, realism, and temporal consistency quality. By revealing systematic changes in performance induced by particular characteristics of the input content, our challenging benchmark enables more insightful analysis into video inpainting methods and serves as an invaluable diagnostic tool for the field. Our code and data are available at https://github.com/MichiganCOG/devil .



### Unsupervised Representation Learning from Pathology Images with Multi-directional Contrastive Predictive Coding
- **Arxiv ID**: http://arxiv.org/abs/2105.05345v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T45 (Primary) 68T07 (Secondary), I.2.10; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2105.05345v1)
- **Published**: 2021-05-11 21:17:13+00:00
- **Updated**: 2021-05-11 21:17:13+00:00
- **Authors**: Jacob Carse, Frank Carey, Stephen McKenna
- **Comment**: 5 pages, 4 figures, presented at IEEE International Symposium on
  Biomedical Imaging (ISBI) 2021
- **Journal**: None
- **Summary**: Digital pathology tasks have benefited greatly from modern deep learning algorithms. However, their need for large quantities of annotated data has been identified as a key challenge. This need for data can be countered by using unsupervised learning in situations where data are abundant but access to annotations is limited. Feature representations learned from unannotated data using contrastive predictive coding (CPC) have been shown to enable classifiers to obtain state of the art performance from relatively small amounts of annotated computer vision data. We present a modification to the CPC framework for use with digital pathology patches. This is achieved by introducing an alternative mask for building the latent context and using a multi-directional PixelCNN autoregressor. To demonstrate our proposed method we learn feature representations from the Patch Camelyon histology dataset. We show that our proposed modification can yield improved deep classification of histology patches.



### Few-Shot Learning by Integrating Spatial and Frequency Representation
- **Arxiv ID**: http://arxiv.org/abs/2105.05348v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.05348v2)
- **Published**: 2021-05-11 21:44:31+00:00
- **Updated**: 2021-07-07 00:17:17+00:00
- **Authors**: Xiangyu Chen, Guanghui Wang
- **Comment**: Accepted to CRV 2021
- **Journal**: None
- **Summary**: Human beings can recognize new objects with only a few labeled examples, however, few-shot learning remains a challenging problem for machine learning systems. Most previous algorithms in few-shot learning only utilize spatial information of the images. In this paper, we propose to integrate the frequency information into the learning model to boost the discrimination ability of the system. We employ Discrete Cosine Transformation (DCT) to generate the frequency representation, then, integrate the features from both the spatial domain and frequency domain for classification. The proposed strategy and its effectiveness are validated with different backbones, datasets, and algorithms. Extensive experiments demonstrate that the frequency information is complementary to the spatial representations in few-shot classification. The classification accuracy is boosted significantly by integrating features from both the spatial and frequency domains in different few-shot learning tasks.



### Video Frame Interpolation via Structure-Motion based Iterative Fusion
- **Arxiv ID**: http://arxiv.org/abs/2105.05353v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.05353v1)
- **Published**: 2021-05-11 22:11:17+00:00
- **Updated**: 2021-05-11 22:11:17+00:00
- **Authors**: Xi Li, Meng Cao, Yingying Tang, Scott Johnston, Zhendong Hong, Huimin Ma, Jiulong Shan
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: Video Frame Interpolation synthesizes non-existent images between adjacent frames, with the aim of providing a smooth and consistent visual experience. Two approaches for solving this challenging task are optical flow based and kernel-based methods. In existing works, optical flow based methods can provide accurate point-to-point motion description, however, they lack constraints on object structure. On the contrary, kernel-based methods focus on structural alignment, which relies on semantic and apparent features, but tends to blur results. Based on these observations, we propose a structure-motion based iterative fusion method. The framework is an end-to-end learnable structure with two stages. First, interpolated frames are synthesized by structure-based and motion-based learning branches respectively, then, an iterative refinement module is established via spatial and temporal feature integration. Inspired by the observation that audiences have different visual preferences on foreground and background objects, we for the first time propose to use saliency masks in the evaluation processes of the task of video frame interpolation. Experimental results on three typical benchmarks show that the proposed method achieves superior performance on all evaluation metrics over the state-of-the-art methods, even when our models are trained with only one-tenth of the data other methods use.



