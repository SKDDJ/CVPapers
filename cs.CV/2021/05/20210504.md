# Arxiv Papers in cs.CV on 2021-05-04
### COMISR: Compression-Informed Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2105.01237v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01237v2)
- **Published**: 2021-05-04 01:24:44+00:00
- **Updated**: 2021-10-12 05:16:39+00:00
- **Authors**: Yinxiao Li, Pengchong Jin, Feng Yang, Ce Liu, Ming-Hsuan Yang, Peyman Milanfar
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: Most video super-resolution methods focus on restoring high-resolution video frames from low-resolution videos without taking into account compression. However, most videos on the web or mobile devices are compressed, and the compression can be severe when the bandwidth is limited. In this paper, we propose a new compression-informed video super-resolution model to restore high-resolution content without introducing artifacts caused by compression. The proposed model consists of three modules for video super-resolution: bi-directional recurrent warping, detail-preserving flow estimation, and Laplacian enhancement. All these three modules are used to deal with compression properties such as the location of the intra-frames in the input and smoothness in the output frames. For thorough performance evaluation, we conducted extensive experiments on standard datasets with a wide range of compression rates, covering many real video use cases. We showed that our method not only recovers high-resolution content on uncompressed frames from the widely-used benchmark datasets, but also achieves state-of-the-art performance in super-resolving compressed videos based on numerous quantitative metrics. We also evaluated the proposed method by simulating streaming from YouTube to demonstrate its effectiveness and robustness. The source codes and trained models are available at https://github.com/google-research/google-research/tree/master/comisr.



### End-to-end One-shot Human Parsing
- **Arxiv ID**: http://arxiv.org/abs/2105.01241v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01241v2)
- **Published**: 2021-05-04 01:35:50+00:00
- **Updated**: 2022-06-23 10:13:09+00:00
- **Authors**: Haoyu He, Bohan Zhuang, Jing Zhang, Jianfei Cai, Dacheng Tao
- **Comment**: Tech report
- **Journal**: None
- **Summary**: Previous human parsing models are limited to parsing humans into pre-defined classes, which is inflexible for practical fashion applications that often have new fashion item classes. In this paper, we define a novel one-shot human parsing (OSHP) task that requires parsing humans into an open set of classes defined by any test example. During training, only base classes are exposed, which only overlap with part of the test-time classes. To address three main challenges in OSHP, i.e., small sizes, testing bias, and similar parts, we devise an End-to-end One-shot human Parsing Network (EOP-Net). Firstly, an end-to-end human parsing framework is proposed to parse the query image into both coarse-grained and fine-grained human classes, which builds a strong embedding network with rich semantic information shared across different granularities, facilitating identifying small-sized human classes. Then, we propose learning momentum-updated prototypes by gradually smoothing the training time static prototypes, which helps stabilize the training and learn robust features. Moreover, we devise a dual metric learning scheme which encourages the network to enhance features' both representational capability and transferability. Therefore, our EOP-Net can learn representative features that can quickly adapt to the novel classes and mitigate the testing bias issue. In addition, we employ a contrastive loss at the prototype level, thereby enforcing the distances among the classes in the fine-grained metric space to discriminate similar parts. We tailor three existing popular human parsing benchmarks to the OSHP task. Experiments on the new benchmarks demonstrate that EOP-Net outperforms representative one-shot segmentation models by large margins, which serves as a strong baseline for further research on this new task. The source code is available at https://github.com/Charleshhy/One-shot-Human-Parsing.



### Self-Supervised Approach for Facial Movement Based Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2105.01256v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, I.2.10; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2105.01256v1)
- **Published**: 2021-05-04 02:38:11+00:00
- **Updated**: 2021-05-04 02:38:11+00:00
- **Authors**: Muhannad Alkaddour, Usman Tariq, Abhinav Dhall
- **Comment**: 14 pages, 4 figures, 5 tables The supplemental material (error
  histograms) can be found on
  https://www.dropbox.com/s/o7158gi46tppvb1/SupplementalMaterial_OpticalFlow.docx?dl=0
  Manuscript submitted to: IEEE Transactions on Affective Computing
- **Journal**: None
- **Summary**: Computing optical flow is a fundamental problem in computer vision. However, deep learning-based optical flow techniques do not perform well for non-rigid movements such as those found in faces, primarily due to lack of the training data representing the fine facial motion. We hypothesize that learning optical flow on face motion data will improve the quality of predicted flow on faces. The aim of this work is threefold: (1) exploring self-supervised techniques to generate optical flow ground truth for face images; (2) computing baseline results on the effects of using face data to train Convolutional Neural Networks (CNN) for predicting optical flow; and (3) using the learned optical flow in micro-expression recognition to demonstrate its effectiveness. We generate optical flow ground truth using facial key-points in the BP4D-Spontaneous dataset. The generated optical flow is used to train the FlowNetS architecture to test its performance on the generated dataset. The performance of FlowNetS trained on face images surpassed that of other optical flow CNN architectures, demonstrating its usefulness. Our optical flow features are further compared with other methods using the STSTNet micro-expression classifier, and the results indicate that the optical flow obtained using this work has promising applications in facial expression analysis.



### COVID-Net CT-S: 3D Convolutional Neural Network Architectures for COVID-19 Severity Assessment using Chest CT Images
- **Arxiv ID**: http://arxiv.org/abs/2105.01284v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.01284v1)
- **Published**: 2021-05-04 04:44:41+00:00
- **Updated**: 2021-05-04 04:44:41+00:00
- **Authors**: Hossein Aboutalebi, Saad Abbasi, Mohammad Javad Shafiee, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: The health and socioeconomic difficulties caused by the COVID-19 pandemic continues to cause enormous tensions around the world. In particular, this extraordinary surge in the number of cases has put considerable strain on health care systems around the world. A critical step in the treatment and management of COVID-19 positive patients is severity assessment, which is challenging even for expert radiologists given the subtleties at different stages of lung disease severity. Motivated by this challenge, we introduce COVID-Net CT-S, a suite of deep convolutional neural networks for predicting lung disease severity due to COVID-19 infection. More specifically, a 3D residual architecture design is leveraged to learn volumetric visual indicators characterizing the degree of COVID-19 lung disease severity. Experimental results using the patient cohort collected by the China National Center for Bioinformation (CNCB) showed that the proposed COVID-Net CT-S networks, by leveraging volumetric features, can achieve significantly improved severity assessment performance when compared to traditional severity assessment networks that learn and leverage 2D visual features to characterize COVID-19 severity.



### Walk in the Cloud: Learning Curves for Point Clouds Shape Analysis
- **Arxiv ID**: http://arxiv.org/abs/2105.01288v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01288v2)
- **Published**: 2021-05-04 05:03:47+00:00
- **Updated**: 2021-07-29 13:53:55+00:00
- **Authors**: Tiange Xiang, Chaoyi Zhang, Yang Song, Jianhui Yu, Weidong Cai
- **Comment**: ICCV 2021
- **Journal**: None
- **Summary**: Discrete point cloud objects lack sufficient shape descriptors of 3D geometries. In this paper, we present a novel method for aggregating hypothetical curves in point clouds. Sequences of connected points (curves) are initially grouped by taking guided walks in the point clouds, and then subsequently aggregated back to augment their point-wise features. We provide an effective implementation of the proposed aggregation strategy including a novel curve grouping operator followed by a curve aggregation operator. Our method was benchmarked on several point cloud analysis tasks where we achieved the state-of-the-art classification accuracy of 94.2% on the ModelNet40 classification task, instance IoU of 86.8 on the ShapeNetPart segmentation task, and cosine error of 0.11 on the ModelNet40 normal estimation task.



### Representation Learning for Clustering via Building Consensus
- **Arxiv ID**: http://arxiv.org/abs/2105.01289v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.01289v2)
- **Published**: 2021-05-04 05:04:03+00:00
- **Updated**: 2022-04-25 05:04:50+00:00
- **Authors**: Aniket Anand Deshmukh, Jayanth Reddy Regatti, Eren Manavoglu, Urun Dogan
- **Comment**: Paper is accepted at Springer Machine Learning Journal 2022. The code
  and the trained models are available at
  https://github.com/JayanthRR/ConCURL_NCE
- **Journal**: None
- **Summary**: In this paper, we focus on unsupervised representation learning for clustering of images. Recent advances in deep clustering and unsupervised representation learning are based on the idea that different views of an input image (generated through data augmentation techniques) must be close in the representation space (exemplar consistency), and/or similar images must have similar cluster assignments (population consistency). We define an additional notion of consistency, consensus consistency, which ensures that representations are learned to induce similar partitions for variations in the representation space, different clustering algorithms or different initializations of a single clustering algorithm. We define a clustering loss by executing variations in the representation space and seamlessly integrate all three consistencies (consensus, exemplar and population) into an end-to-end learning framework. The proposed algorithm, consensus clustering using unsupervised representation learning (ConCURL), improves upon the clustering performance of state-of-the-art methods on four out of five image datasets. Furthermore, we extend the evaluation procedure for clustering to reflect the challenges encountered in real-world clustering tasks, such as maintaining clustering performance in cases with distribution shifts. We also perform a detailed ablation study for a deeper understanding of the proposed algorithm. The code and the trained models are available at https://github.com/JayanthRR/ConCURL_NCE.



### Dual-Cross Central Difference Network for Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2105.01290v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01290v1)
- **Published**: 2021-05-04 05:11:47+00:00
- **Updated**: 2021-05-04 05:11:47+00:00
- **Authors**: Zitong Yu, Yunxiao Qin, Hengshuang Zhao, Xiaobai Li, Guoying Zhao
- **Comment**: Accepted by IJCAI2021
- **Journal**: None
- **Summary**: Face anti-spoofing (FAS) plays a vital role in securing face recognition systems. Recently, central difference convolution (CDC) has shown its excellent representation capacity for the FAS task via leveraging local gradient features. However, aggregating central difference clues from all neighbors/directions simultaneously makes the CDC redundant and sub-optimized in the training phase. In this paper, we propose two Cross Central Difference Convolutions (C-CDC), which exploit the difference of the center and surround sparse local features from the horizontal/vertical and diagonal directions, respectively. It is interesting to find that, with only five ninth parameters and less computational cost, C-CDC even outperforms the full directional CDC. Based on these two decoupled C-CDC, a powerful Dual-Cross Central Difference Network (DC-CDN) is established with Cross Feature Interaction Modules (CFIM) for mutual relation mining and local detailed representation enhancement. Furthermore, a novel Patch Exchange (PE) augmentation strategy for FAS is proposed via simply exchanging the face patches as well as their dense labels from random samples. Thus, the augmented samples contain richer live/spoof patterns and diverse domain distributions, which benefits the intrinsic and robust feature learning. Comprehensive experiments are performed on four benchmark datasets with three testing protocols to demonstrate our state-of-the-art performance.



### Hallucination Improves Few-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.01294v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01294v1)
- **Published**: 2021-05-04 05:19:53+00:00
- **Updated**: 2021-05-04 05:19:53+00:00
- **Authors**: Weilin Zhang, Yu-Xiong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Learning to detect novel objects from few annotated examples is of great practical importance. A particularly challenging yet common regime occurs when there are extremely limited examples (less than three). One critical factor in improving few-shot detection is to address the lack of variation in training data. We propose to build a better model of variation for novel classes by transferring the shared within-class variation from base classes. To this end, we introduce a hallucinator network that learns to generate additional, useful training examples in the region of interest (RoI) feature space, and incorporate it into a modern object detection model. Our approach yields significant performance improvements on two state-of-the-art few-shot detectors with different proposal generation procedures. In particular, we achieve new state of the art in the extremely-few-shot regime on the challenging COCO benchmark.



### LAFFNet: A Lightweight Adaptive Feature Fusion Network for Underwater Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2105.01299v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01299v2)
- **Published**: 2021-05-04 05:31:10+00:00
- **Updated**: 2021-05-05 02:16:23+00:00
- **Authors**: Hao-Hsiang Yang, Kuan-Chih Huang, Wei-Ting Chen
- **Comment**: Accepted by ICRA 2021 (Oral)
- **Journal**: None
- **Summary**: Underwater image enhancement is an important low-level computer vision task for autonomous underwater vehicles and remotely operated vehicles to explore and understand the underwater environments. Recently, deep convolutional neural networks (CNNs) have been successfully used in many computer vision problems, and so does underwater image enhancement. There are many deep-learning-based methods with impressive performance for underwater image enhancement, but their memory and model parameter costs are hindrances in practical application. To address this issue, we propose a lightweight adaptive feature fusion network (LAFFNet). The model is the encoder-decoder model with multiple adaptive feature fusion (AAF) modules. AAF subsumes multiple branches with different kernel sizes to generate multi-scale feature maps. Furthermore, channel attention is used to merge these feature maps adaptively. Our method reduces the number of parameters from 2.5M to 0.15M (around 94% reduction) but outperforms state-of-the-art algorithms by extensive experiments. Furthermore, we demonstrate our LAFFNet effectively improves high-level vision tasks like salience object detection and single image depth estimation.



### An Empirical Review of Deep Learning Frameworks for Change Detection: Model Design, Experimental Frameworks, Challenges and Research Needs
- **Arxiv ID**: http://arxiv.org/abs/2105.01342v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01342v1)
- **Published**: 2021-05-04 07:42:40+00:00
- **Updated**: 2021-05-04 07:42:40+00:00
- **Authors**: Murari Mandal, Santosh Kumar Vipparthi
- **Comment**: IEEE Transactions on Intelligent Transportation Systems, Accepted
- **Journal**: None
- **Summary**: Visual change detection, aiming at segmentation of video frames into foreground and background regions, is one of the elementary tasks in computer vision and video analytics. The applications of change detection include anomaly detection, object tracking, traffic monitoring, human machine interaction, behavior analysis, action recognition, and visual surveillance. Some of the challenges in change detection include background fluctuations, illumination variation, weather changes, intermittent object motion, shadow, fast/slow object motion, camera motion, heterogeneous object shapes and real-time processing. Traditionally, this problem has been solved using hand-crafted features and background modelling techniques. In recent years, deep learning frameworks have been successfully adopted for robust change detection. This article aims to provide an empirical review of the state-of-the-art deep learning methods for change detection. More specifically, we present a detailed analysis of the technical characteristics of different model designs and experimental frameworks. We provide model design based categorization of the existing approaches, including the 2D-CNN, 3D-CNN, ConvLSTM, multi-scale features, residual connections, autoencoders and GAN based methods. Moreover, an empirical analysis of the evaluation settings adopted by the existing deep learning methods is presented. To the best of our knowledge, this is a first attempt to comparatively analyze the different evaluation frameworks used in the existing deep change detection methods. Finally, we point out the research needs, future directions and draw our own conclusions.



### One Model for All Quantization: A Quantized Network Supporting Hot-Swap Bit-Width Adjustment
- **Arxiv ID**: http://arxiv.org/abs/2105.01353v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01353v1)
- **Published**: 2021-05-04 08:10:50+00:00
- **Updated**: 2021-05-04 08:10:50+00:00
- **Authors**: Qigong Sun, Xiufang Li, Yan Ren, Zhongjian Huang, Xu Liu, Licheng Jiao, Fang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: As an effective technique to achieve the implementation of deep neural networks in edge devices, model quantization has been successfully applied in many practical applications. No matter the methods of quantization aware training (QAT) or post-training quantization (PTQ), they all depend on the target bit-widths. When the precision of quantization is adjusted, it is necessary to fine-tune the quantized model or minimize the quantization noise, which brings inconvenience in practical applications. In this work, we propose a method to train a model for all quantization that supports diverse bit-widths (e.g., form 8-bit to 1-bit) to satisfy the online quantization bit-width adjustment. It is hot-swappable that can provide specific quantization strategies for different candidates through multiscale quantization. We use wavelet decomposition and reconstruction to increase the diversity of weights, thus significantly improving the performance of each quantization candidate, especially at ultra-low bit-widths (e.g., 3-bit, 2-bit, and 1-bit). Experimental results on ImageNet and COCO show that our method can achieve accuracy comparable performance to dedicated models trained at the same precision.



### Canonical Saliency Maps: Decoding Deep Face Models
- **Arxiv ID**: http://arxiv.org/abs/2105.01386v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2105.01386v2)
- **Published**: 2021-05-04 09:42:56+00:00
- **Updated**: 2021-08-16 17:56:33+00:00
- **Authors**: Thrupthi Ann John, Vineeth N Balasubramanian, C V Jawahar
- **Comment**: Under review. Added three new experiments, cleaned up some figures
  and equations
- **Journal**: None
- **Summary**: As Deep Neural Network models for face processing tasks approach human-like performance, their deployment in critical applications such as law enforcement and access control has seen an upswing, where any failure may have far-reaching consequences. We need methods to build trust in deployed systems by making their working as transparent as possible. Existing visualization algorithms are designed for object recognition and do not give insightful results when applied to the face domain. In this work, we present 'Canonical Saliency Maps', a new method that highlights relevant facial areas by projecting saliency maps onto a canonical face model. We present two kinds of Canonical Saliency Maps: image-level maps and model-level maps. Image-level maps highlight facial features responsible for the decision made by a deep face model on a given image, thus helping to understand how a DNN made a prediction on the image. Model-level maps provide an understanding of what the entire DNN model focuses on in each task and thus can be used to detect biases in the model. Our qualitative and quantitative results show the usefulness of the proposed canonical saliency maps, which can be used on any deep face model regardless of the architecture.



### Weak Multi-View Supervision for Surface Mapping Estimation
- **Arxiv ID**: http://arxiv.org/abs/2105.01388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01388v1)
- **Published**: 2021-05-04 09:46:26+00:00
- **Updated**: 2021-05-04 09:46:26+00:00
- **Authors**: Nishant Rai, Aidas Liaudanskas, Srinivas Rao, Rodrigo Ortiz Cayon, Matteo Munaro, Stefan Holzer
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a weakly-supervised multi-view learning approach to learn category-specific surface mapping without dense annotations. We learn the underlying surface geometry of common categories, such as human faces, cars, and airplanes, given instances from those categories. While traditional approaches solve this problem using extensive supervision in the form of pixel-level annotations, we take advantage of the fact that pixel-level UV and mesh predictions can be combined with 3D reprojections to form consistency cycles. As a result of exploiting these cycles, we can establish a dense correspondence mapping between image pixels and the mesh acting as a self-supervisory signal, which in turn helps improve our overall estimates. Our approach leverages information from multiple views of the object to establish additional consistency cycles, thus improving surface mapping understanding without the need for explicit annotations. We also propose the use of deformation fields for predictions of an instance specific mesh. Given the lack of datasets providing multiple images of similar object instances from different viewpoints, we generate and release a multi-view ShapeNet Cars and Airplanes dataset created by rendering ShapeNet meshes using a 360 degree camera trajectory around the mesh. For the human faces category, we process and adapt an existing dataset to a multi-view setup. Through experimental evaluations, we show that, at test time, our method can generate accurate variations away from the mean shape, is multi-view consistent, and performs comparably to fully supervised approaches.



### Moving Towards Centers: Re-ranking with Attention and Memory for Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2105.01447v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01447v2)
- **Published**: 2021-05-04 12:14:30+00:00
- **Updated**: 2022-03-21 14:56:12+00:00
- **Authors**: Yunhao Zhou, Yi Wang, Lap-Pui Chau
- **Comment**: 13 pages. Accepted for Publication at IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Re-ranking utilizes contextual information to optimize the initial ranking list of person or vehicle re-identification (re-ID), which boosts the retrieval performance at post-processing steps. This paper proposes a re-ranking network to predict the correlations between the probe and top-ranked neighbor samples. Specifically, all the feature embeddings of query and gallery images are expanded and enhanced by a linear combination of their neighbors, with the correlation prediction serving as discriminative combination weights. The combination process is equivalent to moving independent embeddings toward the identity centers, improving cluster compactness. For correlation prediction, we first aggregate the contextual information for probe's k-nearest neighbors via the Transformer encoder. Then, we distill and refine the probe-related features into the Contextual Memory cell via attention mechanism. Like humans that retrieve images by not only considering probe images but also memorizing the retrieved ones, the Contextual Memory produces multi-view descriptions for each instance. Finally, the neighbors are reconstructed with features fetched from the Contextual Memory, and a binary classifier predicts their correlations with the probe. Experiments on six widely-used person and vehicle re-ID benchmarks demonstrate the effectiveness of the proposed method. Especially, our method surpasses the state-of-the-art re-ranking approaches on large-scale datasets by a significant margin, i.e., with an average 4.83% CMC@1 and 14.83% mAP improvements on VERI-Wild, MSMT17, and VehicleID datasets.



### Computer vision for liquid samples in hospitals and medical labs using hierarchical image segmentation and relations prediction
- **Arxiv ID**: http://arxiv.org/abs/2105.01456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01456v1)
- **Published**: 2021-05-04 12:39:39+00:00
- **Updated**: 2021-05-04 12:39:39+00:00
- **Authors**: Sagi Eppel, Haoping Xu, Alan Aspuru-Guzik
- **Comment**: None
- **Journal**: None
- **Summary**: This work explores the use of computer vision for image segmentation and classification of medical fluid samples in transparent containers (for example, tubes, syringes, infusion bags). Handling fluids such as infusion fluids, blood, and urine samples is a significant part of the work carried out in medical labs and hospitals. The ability to accurately identify and segment the liquids and the vessels that contain them from images can help in automating such processes. Modern computer vision typically involves training deep neural nets on large datasets of annotated images. This work presents a new dataset containing 1,300 annotated images of medical samples involving vessels containing liquids and solid material. The images are annotated with the type of liquid (e.g., blood, urine), the phase of the material (e.g., liquid, solid, foam, suspension), the type of vessel (e.g., syringe, tube, cup, infusion bottle/bag), and the properties of the vessel (transparent, opaque). In addition, vessel parts such as corks, labels, spikes, and valves are annotated. Relations and hierarchies between vessels and materials are also annotated, such as which vessel contains which material or which vessels are linked or contain each other. Three neural networks are trained on the dataset: One network learns to detect vessels, a second net detects the materials and parts inside each vessel, and a third net identifies relationships and connectivity between vessels.



### Technical Report for Valence-Arousal Estimation on Affwild2 Dataset
- **Arxiv ID**: http://arxiv.org/abs/2105.01502v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01502v2)
- **Published**: 2021-05-04 14:00:07+00:00
- **Updated**: 2021-05-13 06:24:54+00:00
- **Authors**: I-Hsuan Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we describe our method for tackling the valence-arousal estimation challenge from ABAW FG-2020 Competition. The competition organizers provide an in-the-wild Aff-Wild2 dataset for participants to analyze affective behavior in real-life settings. We use MIMAMO Net \cite{deng2020mimamo} model to achieve information about micro-motion and macro-motion for improving video emotion recognition and achieve Concordance Correlation Coefficient (CCC) of 0.415 and 0.511 for valence and arousal on the reselected validation set.



### Multipath Graph Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.01510v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.01510v1)
- **Published**: 2021-05-04 14:11:20+00:00
- **Updated**: 2021-05-04 14:11:20+00:00
- **Authors**: Rangan Das, Bikram Boote, Saumik Bhattacharya, Ujjwal Maulik
- **Comment**: 2 pages, 1 figure
- **Journal**: None
- **Summary**: Graph convolution networks have recently garnered a lot of attention for representation learning on non-Euclidean feature spaces. Recent research has focused on stacking multiple layers like in convolutional neural networks for the increased expressive power of graph convolution networks. However, simply stacking multiple graph convolution layers lead to issues like vanishing gradient, over-fitting and over-smoothing. Such problems are much less when using shallower networks, even though the shallow networks have lower expressive power. In this work, we propose a novel Multipath Graph convolutional neural network that aggregates the output of multiple different shallow networks. We train and test our model on various benchmarks datasets for the task of node property prediction. Results show that the proposed method not only attains increased test accuracy but also requires fewer training epochs to converge. The full implementation is available at https://github.com/rangan2510/MultiPathGCN



### Where and When: Space-Time Attention for Audio-Visual Explanations
- **Arxiv ID**: http://arxiv.org/abs/2105.01517v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.01517v1)
- **Published**: 2021-05-04 14:16:55+00:00
- **Updated**: 2021-05-04 14:16:55+00:00
- **Authors**: Yanbei Chen, Thomas Hummel, A. Sophia Koepke, Zeynep Akata
- **Comment**: None
- **Journal**: None
- **Summary**: Explaining the decision of a multi-modal decision-maker requires to determine the evidence from both modalities. Recent advances in XAI provide explanations for models trained on still images. However, when it comes to modeling multiple sensory modalities in a dynamic world, it remains underexplored how to demystify the mysterious dynamics of a complex multi-modal model. In this work, we take a crucial step forward and explore learnable explanations for audio-visual recognition. Specifically, we propose a novel space-time attention network that uncovers the synergistic dynamics of audio and visual data over both space and time. Our model is capable of predicting the audio-visual video events, while justifying its decision by localizing where the relevant visual cues appear, and when the predicted sounds occur in videos. We benchmark our model on three audio-visual video event datasets, comparing extensively to multiple recent multi-modal representation learners and intrinsic explanation models. Experimental results demonstrate the clear superior performance of our model over the existing methods on audio-visual video event recognition. Moreover, we conduct an in-depth study to analyze the explainability of our model based on robustness analysis via perturbation tests and pointing games using human annotations.



### Combining Supervised and Un-supervised Learning for Automatic Citrus Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.01553v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01553v1)
- **Published**: 2021-05-04 15:09:01+00:00
- **Updated**: 2021-05-04 15:09:01+00:00
- **Authors**: Heqing Huang, Tongbin Huang, Zhen Li, Zhiwei Wei, Shilei Lv
- **Comment**: 7 pages,4 figures,Prepare for submission
- **Journal**: None
- **Summary**: Citrus segmentation is a key step of automatic citrus picking. While most current image segmentation approaches achieve good segmentation results by pixel-wise segmentation, these supervised learning-based methods require a large amount of annotated data, and do not consider the continuous temporal changes of citrus position in real-world applications. In this paper, we first train a simple CNN with a small number of labelled citrus images in a supervised manner, which can roughly predict the citrus location from each frame. Then, we extend a state-of-the-art unsupervised learning approach to pre-learn the citrus's potential movements between frames from unlabelled citrus's videos. To take advantages of both networks, we employ the multimodal transformer to combine supervised learned static information and unsupervised learned movement information. The experimental results show that combing both network allows the prediction accuracy reached at 88.3$\%$ IOU and 93.6$\%$ precision, outperforming the original supervised baseline 1.2$\%$ and 2.4$\%$. Compared with most of the existing citrus segmentation methods, our method uses a small amount of supervised data and a large number of unsupervised data, while learning the pixel level location information and the temporal information of citrus changes to enhance the segmentation effect.



### Fusing Higher-order Features in Graph Neural Networks for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2105.01563v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01563v5)
- **Published**: 2021-05-04 15:23:29+00:00
- **Updated**: 2022-08-23 04:34:39+00:00
- **Authors**: Zhenyue Qin, Yang Liu, Pan Ji, Dongwoo Kim, Lei Wang, Bob McKay, Saeed Anwar, Tom Gedeon
- **Comment**: Accepted by IEEE Transactions on Neural Networks and Learning Systems
- **Journal**: None
- **Summary**: Skeleton sequences are lightweight and compact, and thus are ideal candidates for action recognition on edge devices. Recent skeleton-based action recognition methods extract features from 3D joint coordinates as spatial-temporal cues, using these representations in a graph neural network for feature fusion to boost recognition performance. The use of first- and second-order features, i.e., joint and bone representations, has led to high accuracy. Nonetheless, many models are still confused by actions that have similar motion trajectories. To address these issues, we propose fusing higher-order features in the form of angular encoding into modern architectures to robustly capture the relationships between joints and body parts. This simple fusion with popular spatial-temporal graph neural networks achieves new state-of-the-art accuracy in two large benchmarks, including NTU60 and NTU120, while employing fewer parameters and reduced run time. Our source code is publicly available at: https://github.com/ZhenyueQin/Angular-Skeleton-Encoding.



### Robustness Enhancement of Object Detection in Advanced Driver Assistance Systems (ADAS)
- **Arxiv ID**: http://arxiv.org/abs/2105.01580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01580v1)
- **Published**: 2021-05-04 15:42:43+00:00
- **Updated**: 2021-05-04 15:42:43+00:00
- **Authors**: Le-Anh Tran, Truong-Dong Do, Dong-Chul Park, My-Ha Le
- **Comment**: 6 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: A unified system integrating a compact object detector and a surrounding environmental condition classifier for enhancing the robustness of object detection scheme in advanced driver assistance systems (ADAS) is proposed in this paper. ADAS are invented to improve traffic safety and effectiveness in autonomous driving systems where object detection plays an extremely important role. However, modern object detectors integrated in ADAS are still unstable due to high latency and the variation of the environmental contexts in the deployment phase. Our system is proposed to address the aforementioned problems. The proposed system includes two main components: (1) a compact one-stage object detector which is expected to be able to perform at a comparable accuracy compared to state-of-the-art object detectors, and (2) an environmental condition detector that helps to send a warning signal to the cloud in case the self-driving car needs human actions due to the significance of the situation. The empirical results prove the reliability and the scalability of the proposed system to realistic scenarios.



### Self-Improving Semantic Perception for Indoor Localisation
- **Arxiv ID**: http://arxiv.org/abs/2105.01595v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.01595v2)
- **Published**: 2021-05-04 16:06:12+00:00
- **Updated**: 2021-09-15 14:02:54+00:00
- **Authors**: Hermann Blum, Francesco Milano, René Zurbrügg, Roland Siegward, Cesar Cadena, Abel Gawel
- **Comment**: A summary video can be accessed at https://youtu.be/awsynhkkFpk
- **Journal**: CoRL 2021 https://openreview.net/forum?id=X2KJq-S11BC
- **Summary**: We propose a novel robotic system that can improve its perception during deployment. Contrary to the established approach of learning semantics from large datasets and deploying fixed models, we propose a framework in which semantic models are continuously updated on the robot to adapt to the deployment environments. By combining continual learning with self-supervision, our robotic system learns online during deployment without external supervision. We conduct real-world experiments with robots localising in 3D floorplans. Our experiments show how the robot's semantic perception improves during deployment and how this translates into improved localisation, even across drastically different environments. We further study the risk of catastrophic forgetting that such a continuous learning setting poses. We find memory replay an effective measure to reduce forgetting and show how the robotic system can improve even when switching between different environments. On average, our system improves by 60% in segmentation and 10% in localisation accuracy compared to deployment of a fixed model, and it maintains this improvement while adapting to further environments.



### MLP-Mixer: An all-MLP Architecture for Vision
- **Arxiv ID**: http://arxiv.org/abs/2105.01601v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.01601v4)
- **Published**: 2021-05-04 16:17:21+00:00
- **Updated**: 2021-06-11 09:36:50+00:00
- **Authors**: Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, Alexey Dosovitskiy
- **Comment**: v2: Fixed parameter counts in Table 1. v3: Added results on JFT-3B in
  Figure 2(right); Added Section 3.4 on the input permutations. v4: Updated the
  x label in Figure 2(right)
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. "mixing" the per-location features), and one with MLPs applied across patches (i.e. "mixing" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.



### Orienting Point Clouds with Dipole Propagation
- **Arxiv ID**: http://arxiv.org/abs/2105.01604v1
- **DOI**: 10.1145/3450626.3459835
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.01604v1)
- **Published**: 2021-05-04 16:25:36+00:00
- **Updated**: 2021-05-04 16:25:36+00:00
- **Authors**: Gal Metzer, Rana Hanocka, Denis Zorin, Raja Giryes, Daniele Panozzo, Daniel Cohen-Or
- **Comment**: SIGGRAPH 2021
- **Journal**: None
- **Summary**: Establishing a consistent normal orientation for point clouds is a notoriously difficult problem in geometry processing, requiring attention to both local and global shape characteristics. The normal direction of a point is a function of the local surface neighborhood; yet, point clouds do not disclose the full underlying surface structure. Even assuming known geodesic proximity, calculating a consistent normal orientation requires the global context. In this work, we introduce a novel approach for establishing a globally consistent normal orientation for point clouds. Our solution separates the local and global components into two different sub-problems. In the local phase, we train a neural network to learn a coherent normal direction per patch (i.e., consistently oriented normals within a single patch). In the global phase, we propagate the orientation across all coherent patches using a dipole propagation. Our dipole propagation decides to orient each patch using the electric field defined by all previously orientated patches. This gives rise to a global propagation that is stable, as well as being robust to nearby surfaces, holes, sharp features and noise.



### Poisoning the Unlabeled Dataset of Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.01622v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.01622v2)
- **Published**: 2021-05-04 16:55:20+00:00
- **Updated**: 2021-08-10 07:38:44+00:00
- **Authors**: Nicholas Carlini
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised machine learning models learn from a (small) set of labeled training examples, and a (large) set of unlabeled training examples. State-of-the-art models can reach within a few percentage points of fully-supervised training, while requiring 100x less labeled data.   We study a new class of vulnerabilities: poisoning attacks that modify the unlabeled dataset. In order to be useful, unlabeled datasets are given strictly less review than labeled datasets, and adversaries can therefore poison them easily. By inserting maliciously-crafted unlabeled examples totaling just 0.1% of the dataset size, we can manipulate a model trained on this poisoned dataset to misclassify arbitrary examples at test time (as any desired label). Our attacks are highly effective across datasets and semi-supervised learning methods.   We find that more accurate methods (thus more likely to be used) are significantly more vulnerable to poisoning attacks, and as such better training methods are unlikely to prevent this attack. To counter this we explore the space of defenses, and propose two methods that mitigate our attack.



### Remote Pathological Gait Classification System
- **Arxiv ID**: http://arxiv.org/abs/2105.01634v1
- **DOI**: 10.3390/diagnostics11101824
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01634v1)
- **Published**: 2021-05-04 17:21:29+00:00
- **Updated**: 2021-05-04 17:21:29+00:00
- **Authors**: Pedro Albuquerque, Joao Machado, Tanmay Tulsidas Verlekar, Luis Ducla Soares, Paulo Lobato Correia
- **Comment**: None
- **Journal**: https://www.mdpi.com/2075-4418/11/10/1824
- **Summary**: Several pathologies can alter the way people walk, i.e. their gait. Gait analysis can therefore be used to detect impairments and help diagnose illnesses and assess patient recovery. Using vision-based systems, diagnoses could be done at home or in a clinic, with the needed computation being done remotely. State-of-the-art vision-based gait analysis systems use deep learning, requiring large datasets for training. However, to our best knowledge, the biggest publicly available pathological gait dataset contains only 10 subjects, simulating 4 gait pathologies. This paper presents a new dataset called GAIT-IT, captured from 21 subjects simulating 4 gait pathologies, with 2 severity levels, besides normal gait, being considerably larger than publicly available gait pathology datasets, allowing to train a deep learning model for gait pathology classification. Moreover, it was recorded in a professional studio, making it possible to obtain nearly perfect silhouettes, free of segmentation errors. Recognizing the importance of remote healthcare, this paper proposes a prototype of a web application allowing to upload a walking person's video, possibly acquired using a smartphone camera, and execute a web service that classifies the person's gait as normal or across different pathologies. The web application has a user friendly interface and could be used by healthcare professionals or other end users. An automatic gait analysis system is also developed and integrated with the web application for pathology classification. Compared to state-of-the-art solutions, it achieves a drastic reduction in the number of model parameters, which means significantly lower memory requirements, as well as lower training and execution times. Classification accuracy is on par with the state-of-the-art.



### Motion-Augmented Self-Training for Video Recognition at Smaller Scale
- **Arxiv ID**: http://arxiv.org/abs/2105.01646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01646v1)
- **Published**: 2021-05-04 17:43:19+00:00
- **Updated**: 2021-05-04 17:43:19+00:00
- **Authors**: Kirill Gavrilyuk, Mihir Jain, Ilia Karmanov, Cees G. M. Snoek
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of this paper is to self-train a 3D convolutional neural network on an unlabeled video collection for deployment on small-scale video collections. As smaller video datasets benefit more from motion than appearance, we strive to train our network using optical flow, but avoid its computation during inference. We propose the first motion-augmented self-training regime, we call MotionFit. We start with supervised training of a motion model on a small, and labeled, video collection. With the motion model we generate pseudo-labels for a large unlabeled video collection, which enables us to transfer knowledge by learning to predict these pseudo-labels with an appearance model. Moreover, we introduce a multi-clip loss as a simple yet efficient way to improve the quality of the pseudo-labeling, even without additional auxiliary tasks. We also take into consideration the temporal granularity of videos during self-training of the appearance model, which was missed in previous works. As a result we obtain a strong motion-augmented representation model suited for video downstream tasks like action recognition and clip retrieval. On small-scale video datasets, MotionFit outperforms alternatives for knowledge transfer by 5%-8%, video-only self-supervision by 1%-7% and semi-supervised learning by 9%-18% using the same amount of class labels.



### The Pursuit of Knowledge: Discovering and Localizing Novel Categories using Dual Memory
- **Arxiv ID**: http://arxiv.org/abs/2105.01652v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.01652v3)
- **Published**: 2021-05-04 17:55:59+00:00
- **Updated**: 2021-09-15 21:44:26+00:00
- **Authors**: Sai Saketh Rambhatla, Rama Chellappa, Abhinav Shrivastava
- **Comment**: Accepted to ICCV2021
- **Journal**: None
- **Summary**: We tackle object category discovery, which is the problem of discovering and localizing novel objects in a large unlabeled dataset. While existing methods show results on datasets with less cluttered scenes and fewer object instances per image, we present our results on the challenging COCO dataset. Moreover, we argue that, rather than discovering new categories from scratch, discovery algorithms can benefit from identifying what is already known and focusing their attention on the unknown. We propose a method that exploits prior knowledge about certain object types to discover new categories by leveraging two memory modules, namely Working and Semantic memory. We show the performance of our detector on the COCO minival dataset to demonstrate its in-the-wild capabilities.



### Height Estimation of Children under Five Years using Depth Images
- **Arxiv ID**: http://arxiv.org/abs/2105.01688v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.01688v2)
- **Published**: 2021-05-04 18:15:57+00:00
- **Updated**: 2021-07-30 19:16:59+00:00
- **Authors**: Anusua Trivedi, Mohit Jain, Nikhil Kumar Gupta, Markus Hinsche, Prashant Singh, Markus Matiaschek, Tristan Behrens, Mirco Militeri, Cameron Birge, Shivangi Kaushik, Archisman Mohapatra, Rita Chatterjee, Rahul Dodhia, Juan Lavista Ferres
- **Comment**: None
- **Journal**: None
- **Summary**: Malnutrition is a global health crisis and is the leading cause of death among children under five. Detecting malnutrition requires anthropometric measurements of weight, height, and middle-upper arm circumference. However, measuring them accurately is a challenge, especially in the global south, due to limited resources. In this work, we propose a CNN-based approach to estimate the height of standing children under five years from depth images collected using a smart-phone. According to the SMART Methodology Manual [5], the acceptable accuracy for height is less than 1.4 cm. On training our deep learning model on 87131 depth images, our model achieved an average mean absolute error of 1.64% on 57064 test images. For 70.3% test images, we estimated height accurately within the acceptable 1.4 cm range. Thus, our proposed solution can accurately detect stunting (low height-for-age) in standing children below five years of age.



### Effectively Leveraging Attributes for Visual Similarity
- **Arxiv ID**: http://arxiv.org/abs/2105.01695v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01695v2)
- **Published**: 2021-05-04 18:28:35+00:00
- **Updated**: 2021-08-20 13:48:47+00:00
- **Authors**: Samarth Mishra, Zhongping Zhang, Yuan Shen, Ranjitha Kumar, Venkatesh Saligrama, Bryan Plummer
- **Comment**: Accepted to ICCV 2021
- **Journal**: None
- **Summary**: Measuring similarity between two images often requires performing complex reasoning along different axes (e.g., color, texture, or shape). Insights into what might be important for measuring similarity can can be provided by annotated attributes, but prior work tends to view these annotations as complete, resulting in them using a simplistic approach of predicting attributes on single images, which are, in turn, used to measure similarity. However, it is impractical for a dataset to fully annotate every attribute that may be important. Thus, only representing images based on these incomplete annotations may miss out on key information. To address this issue, we propose the Pairwise Attribute-informed similarity Network (PAN), which breaks similarity learning into capturing similarity conditions and relevance scores from a joint representation of two images. This enables our model to identify that two images contain the same attribute, but can have it deemed irrelevant (e.g., due to fine-grained differences between them) and ignored for measuring similarity between the two images. Notably, while prior methods of using attribute annotations are often unable to outperform prior art, PAN obtains a 4-9% improvement on compatibility prediction between clothing items on Polyvore Outfits, a 5% gain on few shot classification of images using Caltech-UCSD Birds (CUB), and over 1% boost to Recall@1 on In-Shop Clothes Retrieval. Implementation available at https://github.com/samarth4149/PAN



### Attention-based Stylisation for Exemplar Image Colourisation
- **Arxiv ID**: http://arxiv.org/abs/2105.01705v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2105.01705v1)
- **Published**: 2021-05-04 18:56:26+00:00
- **Updated**: 2021-05-04 18:56:26+00:00
- **Authors**: Marc Gorriz Blanch, Issa Khalifeh, Alan Smeaton, Noel O'Connor, Marta Mrak
- **Comment**: None
- **Journal**: None
- **Summary**: Exemplar-based colourisation aims to add plausible colours to a grayscale image using the guidance of a colour reference image. Most of the existing methods tackle the task as a style transfer problem, using a convolutional neural network (CNN) to obtain deep representations of the content of both inputs. Stylised outputs are then obtained by computing similarities between both feature representations in order to transfer the style of the reference to the content of the target input. However, in order to gain robustness towards dissimilar references, the stylised outputs need to be refined with a second colourisation network, which significantly increases the overall system complexity. This work reformulates the existing methodology introducing a novel end-to-end colourisation network that unifies the feature matching with the colourisation process. The proposed architecture integrates attention modules at different resolutions that learn how to perform the style transfer task in an unsupervised way towards decoding realistic colour predictions. Moreover, axial attention is proposed to simplify the attention operations and to obtain a fast but robust cost-effective architecture. Experimental validations demonstrate efficiency of the proposed methodology which generates high quality and visual appealing colourisation. Furthermore, the complexity of the proposed methodology is reduced compared to the state-of-the-art methods.



### COVID-19 Detection from Chest X-ray Images using Imprinted Weights Approach
- **Arxiv ID**: http://arxiv.org/abs/2105.01710v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.01710v1)
- **Published**: 2021-05-04 19:01:40+00:00
- **Updated**: 2021-05-04 19:01:40+00:00
- **Authors**: Jianxing Zhang, Pengcheng Xi, Ashkan Ebadi, Hilda Azimi, Stephane Tremblay, Alexander Wong
- **Comment**: Accepted to ICLR 2021 Workshop: Machine Learning for Preventing and
  Combating Pandemics
- **Journal**: None
- **Summary**: The COVID-19 pandemic has had devastating effects on the well-being of the global population. The pandemic has been so prominent partly due to the high infection rate of the virus and its variants. In response, one of the most effective ways to stop infection is rapid diagnosis. The main-stream screening method, reverse transcription-polymerase chain reaction (RT-PCR), is time-consuming, laborious and in short supply. Chest radiography is an alternative screening method for the COVID-19 and computer-aided diagnosis (CAD) has proven to be a viable solution at low cost and with fast speed; however, one of the challenges in training the CAD models is the limited number of training data, especially at the onset of the pandemic. This becomes outstanding precisely when the quick and cheap type of diagnosis is critically needed for flattening the infection curve. To address this challenge, we propose the use of a low-shot learning approach named imprinted weights, taking advantage of the abundance of samples from known illnesses such as pneumonia to improve the detection performance on COVID-19.



### A Fast Partial Video Copy Detection Using KNN and Global Feature Database
- **Arxiv ID**: http://arxiv.org/abs/2105.01713v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01713v2)
- **Published**: 2021-05-04 19:03:21+00:00
- **Updated**: 2021-10-05 19:25:23+00:00
- **Authors**: Weijun Tan, Hongwei Guo, Rushuai Liu
- **Comment**: None
- **Journal**: IEEE WACV 2022
- **Summary**: We propose a fast partial video copy detection framework in this paper. In this framework all frame features of the reference videos are organized in a KNN searchable database. Instead of scanning all reference videos, the query video segment does a fast KNN search in the global feature database. The returned results are used to generate a short list of candidate videos. A modified temporal network is then used to localize the copy segment in the candidate videos. We evaluate different choice of CNN features on the VCDB dataset. Our benchmark F1 score exceeds the state of the art by a big margin.



### GANs for Urban Design
- **Arxiv ID**: http://arxiv.org/abs/2105.01727v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T07, J.6; I.4.9; I.2.10; J.5
- **Links**: [PDF](http://arxiv.org/pdf/2105.01727v1)
- **Published**: 2021-05-04 19:50:24+00:00
- **Updated**: 2021-05-04 19:50:24+00:00
- **Authors**: Stanislava Fedorova
- **Comment**: Presented in SimAUD 2021 9 pages, 8 figures Related github
  repositories: https://github.com/STASYA00/urban_datasets
  https://github.com/STASYA00/UrbanGen
- **Journal**: None
- **Summary**: Development and diffusion of machine learning and big data tools provide a new tool for architects and urban planners that could be used as analytical or design instruments. The topic investigated in this paper is the application of Generative Adversarial Networks to the design of an urban block. The research presents a flexible model able to adapt to the morphological characteristics of a city. This method does not define explicitly any of the parameters of an urban block typical for a city, the algorithm learns them from the existing urban context. This approach has been applied to the cities with different morphology: Milan, Amsterdam, Tallinn, Turin, and Bengaluru in order to see the performance of the model and the possibility of style translation between different cities. The data are gathered from Openstreetmap and Open Data portals of the cities. This research presents the results of the experiments and their quantitative and qualitative evaluation.



### Surveilling Surveillance: Estimating the Prevalence of Surveillance Cameras with Street View Data
- **Arxiv ID**: http://arxiv.org/abs/2105.01764v3
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.01764v3)
- **Published**: 2021-05-04 21:06:01+00:00
- **Updated**: 2021-08-31 02:24:45+00:00
- **Authors**: Hao Sheng, Keniel Yao, Sharad Goel
- **Comment**: We now credit Turtiainen et al. (2020) both for creating a
  state-of-the-art camera detection model and for suggesting that computer
  vision could, in theory, be applied to street view data to map surveillance
  cameras. Also, we discovered a coding error in our image sampling strategy
  that corrupted our analysis of camera density over time. We have now removed
  the results of that analysis
- **Journal**: None
- **Summary**: The use of video surveillance in public spaces -- both by government agencies and by private citizens -- has attracted considerable attention in recent years, particularly in light of rapid advances in face-recognition technology. But it has been difficult to systematically measure the prevalence and placement of cameras, hampering efforts to assess the implications of surveillance on privacy and public safety. Here, we combine computer vision, human verification, and statistical analysis to estimate the spatial distribution of surveillance cameras. Specifically, we build a camera detection model and apply it to 1.6 million street view images sampled from 10 large U.S. cities and 6 other major cities around the world, with positive model detections verified by human experts. After adjusting for the estimated recall of our model, and accounting for the spatial coverage of our sampled images, we are able to estimate the density of surveillance cameras visible from the road. Across the 16 cities we consider, the estimated number of surveillance cameras per linear kilometer ranges from 0.2 (in Los Angeles) to 0.9 (in Seoul). In a detailed analysis of the 10 U.S. cities, we find that cameras are concentrated in commercial, industrial, and mixed zones, and in neighborhoods with higher shares of non-white residents -- a pattern that persists even after adjusting for land use. These results help inform ongoing discussions on the use of surveillance technology, including its potential disparate impacts on communities of color.



### Enabling 3D Object Detection with a Low-Resolution LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2105.01765v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.01765v2)
- **Published**: 2021-05-04 21:08:20+00:00
- **Updated**: 2022-05-04 00:54:39+00:00
- **Authors**: Lin Bai, Yiming Zhao, Xinming Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Light Detection And Ranging (LiDAR) has been widely used in autonomous vehicles for perception and localization. However, the cost of a high-resolution LiDAR is still prohibitively expensive, while its low-resolution counterpart is much more affordable. Therefore, using low-resolution LiDAR for autonomous driving is an economically viable solution, but the point cloud sparsity makes it extremely challenging. In this paper, we propose a two-stage neural network framework that enables 3D object detection using a low-resolution LiDAR. Taking input from a low-resolution LiDAR point cloud and a monocular camera image, a depth completion network is employed to produce dense point cloud that is subsequently processed by a voxel-based network for 3D object detection. Evaluated with KITTI dataset for 3D object detection in Bird-Eye View (BEV), the experimental result shows that the proposed approach performs significantly better than directly applying the 16-line LiDAR point cloud for object detection. For both easy and moderate cases, our 3D vehicle detection results are close to those using 64-line high-resolution LiDARs.



### Texture for Colors: Natural Representations of Colors Using Variable Bit-Depth Textures
- **Arxiv ID**: http://arxiv.org/abs/2105.01768v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.01768v1)
- **Published**: 2021-05-04 21:22:02+00:00
- **Updated**: 2021-05-04 21:22:02+00:00
- **Authors**: Shumeet Baluja
- **Comment**: None
- **Journal**: None
- **Summary**: Numerous methods have been proposed to transform color and grayscale images to their single bit-per-pixel binary counterparts. Commonly, the goal is to enhance specific attributes of the original image to make it more amenable for analysis. However, when the resulting binarized image is intended for human viewing, aesthetics must also be considered. Binarization techniques, such as half-toning, stippling, and hatching, have been widely used for modeling the original image's intensity profile. We present an automated method to transform an image to a set of binary textures that represent not only the intensities, but also the colors of the original. The foundation of our method is information preservation: creating a set of textures that allows for the reconstruction of the original image's colors solely from the binarized representation. We present techniques to ensure that the textures created are not visually distracting, preserve the intensity profile of the images, and are natural in that they map sets of colors that are perceptually similar to patterns that are similar. The approach uses deep-neural networks and is entirely self-supervised; no examples of good vs. bad binarizations are required. The system yields aesthetically pleasing binary images when tested on a variety of image sources.



### Uncertainty-aware INVASE: Enhanced Breast Cancer Diagnosis Feature Selection
- **Arxiv ID**: http://arxiv.org/abs/2105.02693v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.02693v1)
- **Published**: 2021-05-04 21:30:33+00:00
- **Updated**: 2021-05-04 21:30:33+00:00
- **Authors**: Jia-Xing Zhong, Hongbo Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present an uncertainty-aware INVASE to quantify predictive confidence of healthcare problem. By introducing learnable Gaussian distributions, we lever-age their variances to measure the degree of uncertainty. Based on the vanilla INVASE, two additional modules are proposed, i.e., an uncertainty quantification module in the predictor, and a reward shaping module in the selector. We conduct extensive experiments on UCI-WDBC dataset. Notably, our method eliminates almost all predictive bias with only about 20% queries, while the uncertainty-agnostic counterpart requires nearly 100% queries. The open-source implementation with a detailed tutorial is available at https://github.com/jx-zhong-for-academic-purpose/Uncertainty-aware-INVASE/blob/main/tutorialinvase%2B.ipynb.



### Intensity Harmonization for Airborne LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2105.01793v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.01793v1)
- **Published**: 2021-05-04 23:26:48+00:00
- **Updated**: 2021-05-04 23:26:48+00:00
- **Authors**: David Jones, Nathan Jacobs
- **Comment**: IGARSS 2021. Project website with video and code at
  https://davidthomasjones.me/publications/airborne-lidar-intensity-harmonization
- **Journal**: None
- **Summary**: Constructing a point cloud for a large geographic region, such as a state or country, can require multiple years of effort. Often several vendors will be used to acquire LiDAR data, and a single region may be captured by multiple LiDAR scans. A key challenge is maintaining consistency between these scans, which includes point density, number of returns, and intensity. Intensity in particular can be very different between scans, even in areas that are overlapping. Harmonizing the intensity between scans to remove these discrepancies is expensive and time consuming. In this paper, we propose a novel method for point cloud harmonization based on deep neural networks. We evaluate our method quantitatively and qualitatively using a high quality real world LiDAR dataset. We compare our method to several baselines, including standard interpolation methods as well as histogram matching. We show that our method performs as well as the best baseline in areas with similar intensity distributions, and outperforms all baselines in areas with different intensity distributions. Source code is available at https://github.com/mvrl/lidar-harmonization .



### Real-time Deep Dynamic Characters
- **Arxiv ID**: http://arxiv.org/abs/2105.01794v2
- **DOI**: 10.1145/3450626.3459749
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01794v2)
- **Published**: 2021-05-04 23:28:55+00:00
- **Updated**: 2021-08-31 07:37:44+00:00
- **Authors**: Marc Habermann, Lingjie Liu, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, Christian Theobalt
- **Comment**: None
- **Journal**: ACM Transactions on Graphics (SIGGRAPH 2021)
- **Summary**: We propose a deep videorealistic 3D human character model displaying highly realistic shape, motion, and dynamic appearance learned in a new weakly supervised way from multi-view imagery. In contrast to previous work, our controllable 3D character displays dynamics, e.g., the swing of the skirt, dependent on skeletal body motion in an efficient data-driven way, without requiring complex physics simulation. Our character model also features a learned dynamic texture model that accounts for photo-realistic motion-dependent appearance details, as well as view-dependent lighting effects. During training, we do not need to resort to difficult dynamic 3D capture of the human; instead we can train our model entirely from multi-view video in a weakly supervised manner. To this end, we propose a parametric and differentiable character representation which allows us to model coarse and fine dynamic deformations, e.g., garment wrinkles, as explicit space-time coherent mesh geometry that is augmented with high-quality dynamic textures dependent on motion and view point. As input to the model, only an arbitrary 3D skeleton motion is required, making it directly compatible with the established 3D animation pipeline. We use a novel graph convolutional network architecture to enable motion-dependent deformation learning of body and clothing, including dynamics, and a neural generative dynamic texture model creates corresponding dynamic texture maps. We show that by merely providing new skeletal motions, our model creates motion-dependent surface deformations, physically plausible dynamic clothing deformations, as well as video-realistic surface textures at a much higher level of detail than previous state of the art approaches, and even in real-time.



### Generative Adversarial Networks (GAN) Powered Fast Magnetic Resonance Imaging -- Mini Review, Comparison and Perspectives
- **Arxiv ID**: http://arxiv.org/abs/2105.01800v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68T01
- **Links**: [PDF](http://arxiv.org/pdf/2105.01800v1)
- **Published**: 2021-05-04 23:59:00+00:00
- **Updated**: 2021-05-04 23:59:00+00:00
- **Authors**: Guang Yang, Jun Lv, Yutong Chen, Jiahao Huang, Jin Zhu
- **Comment**: 18 figures, Generative Adversarial Learning: Architectures and
  Applications
- **Journal**: None
- **Summary**: Magnetic Resonance Imaging (MRI) is a vital component of medical imaging. When compared to other image modalities, it has advantages such as the absence of radiation, superior soft tissue contrast, and complementary multiple sequence information. However, one drawback of MRI is its comparatively slow scanning and reconstruction compared to other image modalities, limiting its usage in some clinical applications when imaging time is critical. Traditional compressive sensing based MRI (CS-MRI) reconstruction can speed up MRI acquisition, but suffers from a long iterative process and noise-induced artefacts. Recently, Deep Neural Networks (DNNs) have been used in sparse MRI reconstruction models to recreate relatively high-quality images from heavily undersampled k-space data, allowing for much faster MRI scanning. However, there are still some hurdles to tackle. For example, directly training DNNs based on L1/L2 distance to the target fully sampled images could result in blurry reconstruction because L1/L2 loss can only enforce overall image or patch similarity and does not take into account local information such as anatomical sharpness. It is also hard to preserve fine image details while maintaining a natural appearance. More recently, Generative Adversarial Networks (GAN) based methods are proposed to solve fast MRI with enhanced image perceptual quality. The encoder obtains a latent space for the undersampling image, and the image is reconstructed by the decoder using the GAN loss. In this chapter, we review the GAN powered fast MRI methods with a comparative study on various anatomical datasets to demonstrate the generalisability and robustness of this kind of fast MRI while providing future perspectives.



