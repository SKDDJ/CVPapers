# Arxiv Papers in cs.CV on 2021-05-23
### Stereo Matching Based on Visual Sensitive Information
- **Arxiv ID**: http://arxiv.org/abs/2105.10831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10831v1)
- **Published**: 2021-05-23 00:20:32+00:00
- **Updated**: 2021-05-23 00:20:32+00:00
- **Authors**: Hewei Wang, Muhammad Salman Pathan, Soumyabrata Dev
- **Comment**: Published in 6th IEEE International Conference on Image, Vision and
  Computing (ICIVC), 2021
- **Journal**: None
- **Summary**: The area of computer vision is one of the most discussed topics amongst many scholars, and stereo matching is its most important sub fields. After the parallax map is transformed into a depth map, it can be applied to many intelligent fields. In this paper, a stereo matching algorithm based on visual sensitive information is proposed by using standard images from Middlebury dataset. Aiming at the limitation of traditional stereo matching algorithms regarding the cost window, a cost aggregation algorithm based on the dynamic window is proposed, and the disparity image is optimized by using left and right consistency detection to further reduce the error matching rate. The experimental results show that the proposed algorithm can effectively enhance the stereo matching effect of the image providing significant improvement in accuracy as compared with the classical census algorithm. The proposed model code, dataset, and experimental results are available at https://github.com/WangHewei16/Stereo-Matching.



### Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data
- **Arxiv ID**: http://arxiv.org/abs/2105.10837v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10837v2)
- **Published**: 2021-05-23 01:20:40+00:00
- **Updated**: 2022-01-22 21:14:20+00:00
- **Authors**: Shuangjun Liu, Naveen Sehgal, Sarah Ostadabbas
- **Comment**: None
- **Journal**: None
- **Summary**: The ultimate goal for an inference model is to be robust and functional in real life applications. However, training vs. test data domain gaps often negatively affect model performance. This issue is especially critical for the monocular 3D human pose estimation problem, in which 3D human data is often collected in a controlled lab setting. In this paper, we focus on alleviating the negative effect of domain shift in both appearance and pose space for 3D human pose estimation by presenting our adapted human pose (AHuP) approach. AHuP is built upon two key components: (1) semantically aware adaptation (SAA) for the cross-domain feature space adaptation, and (2) skeletal pose adaptation (SPA) for the pose space adaptation which takes only limited information from the target domain. By using zero real 3D human pose data, one of our adapted synthetic models shows comparable performance with the SOTA pose estimation models trained with large scale real 3D human datasets. The proposed SPA can be also employed independently as a light-weighted head to improve existing SOTA models in a novel context. A new 3D scan-based synthetic human dataset called ScanAva+ is also going to be publicly released with this work.



### Exploring Robustness of Unsupervised Domain Adaptation in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.10843v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10843v2)
- **Published**: 2021-05-23 01:50:44+00:00
- **Updated**: 2021-07-25 17:13:43+00:00
- **Authors**: Jinyu Yang, Chunyuan Li, Weizhi An, Hehuan Ma, Yuzhi Guo, Yu Rong, Peilin Zhao, Junzhou Huang
- **Comment**: ICCV 2021 (Oral)
- **Journal**: None
- **Summary**: Recent studies imply that deep neural networks are vulnerable to adversarial examples -- inputs with a slight but intentional perturbation are incorrectly classified by the network. Such vulnerability makes it risky for some security-related applications (e.g., semantic segmentation in autonomous cars) and triggers tremendous concerns on the model reliability. For the first time, we comprehensively evaluate the robustness of existing UDA methods and propose a robust UDA approach. It is rooted in two observations: (i) the robustness of UDA methods in semantic segmentation remains unexplored, which pose a security concern in this field; and (ii) although commonly used self-supervision (e.g., rotation and jigsaw) benefits image tasks such as classification and recognition, they fail to provide the critical supervision signals that could learn discriminative representation for segmentation tasks. These observations motivate us to propose adversarial self-supervision UDA (or ASSUDA) that maximizes the agreement between clean images and their adversarial examples by a contrastive loss in the output space. Extensive empirical studies on commonly used benchmarks demonstrate that ASSUDA is resistant to adversarial attacks.



### Transparent Model of Unabridged Data (TMUD)
- **Arxiv ID**: http://arxiv.org/abs/2106.07558v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.07558v1)
- **Published**: 2021-05-23 04:04:22+00:00
- **Updated**: 2021-05-23 04:04:22+00:00
- **Authors**: Jie Xu, Min Ding
- **Comment**: 6 figures and 6 tables
- **Journal**: None
- **Summary**: Recent advancements in computational power and algorithms have enabled unabridged data (e.g., raw images or audio) to be used as input in some models (e.g., deep learning). However, the black box nature of such models reduces their likelihood of adoption by marketing scholars. Our paradigm of analysis, the Transparent Model of Unabridged Data (TMUD), enables researchers to investigate the inner workings of such black box models by incorporating an ex ante filtration module and an ex post experimentation module. We empirically demonstrate the TMUD by investigating the role of facial components and sexual dimorphism in face perceptions, which have implications for four marketing contexts: advertisement (perceptions of approachability, trustworthiness, and competence), brand (perceptions of whether a face represents a brand's typical customer), category (perceptions of whether a face represents a category's typical customer), and customer persona (perceptions of whether a face represents the persona of a brand's customer segment). Our results reveal new and useful findings that enrich the existing literature on face perception, most of which is based on abridged attributes (e.g., width of mouth). The TMUD has great potential to be a useful paradigm for generating theoretical insights and may encourage more marketing researchers and practitioners to use unabridged data.



### Coarse to Fine Multi-Resolution Temporal Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2105.10859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10859v1)
- **Published**: 2021-05-23 06:07:40+00:00
- **Updated**: 2021-05-23 06:07:40+00:00
- **Authors**: Dipika Singhania, Rahul Rahaman, Angela Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal convolutional networks (TCNs) are a commonly used architecture for temporal video segmentation. TCNs however, tend to suffer from over-segmentation errors and require additional refinement modules to ensure smoothness and temporal coherency. In this work, we propose a novel temporal encoder-decoder to tackle the problem of sequence fragmentation. In particular, the decoder follows a coarse-to-fine structure with an implicit ensemble of multiple temporal resolutions. The ensembling produces smoother segmentations that are more accurate and better-calibrated, bypassing the need for additional refinement modules. In addition, we enhance our training with a multi-resolution feature-augmentation strategy to promote robustness to varying temporal resolutions. Finally, to support our architecture and encourage further sequence coherency, we propose an action loss that penalizes misclassifications at the video level. Experiments show that our stand-alone architecture, together with our novel feature-augmentation strategy and new loss, outperforms the state-of-the-art on three temporal video segmentation benchmarks.



### FCCDN: Feature Constraint Network for VHR Image Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.10860v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10860v2)
- **Published**: 2021-05-23 06:13:47+00:00
- **Updated**: 2021-09-02 10:19:34+00:00
- **Authors**: Pan Chen, Danfeng Hong, Zhengchao Chen, Xuan Yang, Baipeng Li, Bing Zhang
- **Comment**: 46 pages, 16 figures. Submitted to ISPRS Journal of Photogrammetry
  and Remote Sensing. Under review
- **Journal**: None
- **Summary**: Change detection is the process of identifying pixelwise differences in bitemporal co-registered images. It is of great significance to Earth observations. Recently, with the emergence of deep learning (DL), the power and feasibility of deep convolutional neural network (CNN)-based methods have been shown in the field of change detection. However, there is still a lack of effective supervision for change feature learning. In this work, a feature constraint change detection network (FCCDN) is proposed. We constrain features both in bitemporal feature extraction and feature fusion. More specifically, we propose a dual encoder-decoder network backbone for the change detection task. At the center of the backbone, we design a nonlocal feature pyramid network to extract and fuse multiscale features. To fuse bitemporal features in a robust way, we build a dense connection-based feature fusion module. Moreover, a self-supervised learning-based strategy is proposed to constrain feature learning. Based on FCCDN, we achieve state-of-the-art performance on two building change detection datasets (LEVIR-CD and WHU). On the LEVIR-CD dataset, we achieve an IoU of 0.8569 and an F1 score of 0.9229. On the WHU dataset, we achieve an IoU of 0.8820 and an F1 score of 0.9373. Moreover, for the first time, the acquisition of accurate bitemporal semantic segmentation results is achieved without using semantic segmentation labels. This is vital for the application of change detection because it saves the cost of labeling.



### CMUA-Watermark: A Cross-Model Universal Adversarial Watermark for Combating Deepfakes
- **Arxiv ID**: http://arxiv.org/abs/2105.10872v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.10872v2)
- **Published**: 2021-05-23 07:28:36+00:00
- **Updated**: 2021-12-14 02:56:14+00:00
- **Authors**: Hao Huang, Yongtao Wang, Zhaoyu Chen, Yuze Zhang, Yuheng Li, Zhi Tang, Wei Chu, Jingdong Chen, Weisi Lin, Kai-Kuang Ma
- **Comment**: 9 pages, 7 figures, Thirty-Sixth AAAI Conference on Artificial
  Intelligence, AAAI22
- **Journal**: None
- **Summary**: Malicious applications of deepfakes (i.e., technologies generating target facial attributes or entire faces from facial images) have posed a huge threat to individuals' reputation and security. To mitigate these threats, recent studies have proposed adversarial watermarks to combat deepfake models, leading them to generate distorted outputs. Despite achieving impressive results, these adversarial watermarks have low image-level and model-level transferability, meaning that they can protect only one facial image from one specific deepfake model. To address these issues, we propose a novel solution that can generate a Cross-Model Universal Adversarial Watermark (CMUA-Watermark), protecting a large number of facial images from multiple deepfake models. Specifically, we begin by proposing a cross-model universal attack pipeline that attacks multiple deepfake models iteratively. Then, we design a two-level perturbation fusion strategy to alleviate the conflict between the adversarial watermarks generated by different facial images and models. Moreover, we address the key problem in cross-model optimization with a heuristic approach to automatically find the suitable attack step sizes for different models, further weakening the model-level conflict. Finally, we introduce a more reasonable and comprehensive evaluation method to fully test the proposed method and compare it with existing ones. Extensive experimental results demonstrate that the proposed CMUA-Watermark can effectively distort the fake facial images generated by multiple deepfake models while achieving a better performance than existing methods.



### Weakly-supervised 3D Human Pose Estimation with Cross-view U-shaped Graph Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2105.10882v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10882v2)
- **Published**: 2021-05-23 08:16:25+00:00
- **Updated**: 2022-05-17 10:04:03+00:00
- **Authors**: Guoliang Hua, Hong Liu, Wenhao Li, Qian Zhang, Runwei Ding, Xin Xu
- **Comment**: Accepted by IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Although monocular 3D human pose estimation methods have made significant progress, it is far from being solved due to the inherent depth ambiguity. Instead, exploiting multi-view information is a practical way to achieve absolute 3D human pose estimation. In this paper, we propose a simple yet effective pipeline for weakly-supervised cross-view 3D human pose estimation. By only using two camera views, our method can achieve state-of-the-art performance in a weakly-supervised manner, requiring no 3D ground truth but only 2D annotations. Specifically, our method contains two steps: triangulation and refinement. First, given the 2D keypoints that can be obtained through any classic 2D detection methods, triangulation is performed across two views to lift the 2D keypoints into coarse 3D poses. Then, a novel cross-view U-shaped graph convolutional network (CV-UGCN), which can explore the spatial configurations and cross-view correlations, is designed to refine the coarse 3D poses. In particular, the refinement progress is achieved through weakly-supervised learning, in which geometric and structure-aware consistency checks are performed. We evaluate our method on the standard benchmark dataset, Human3.6M. The Mean Per Joint Position Error on the benchmark dataset is 27.4 mm, which outperforms existing state-of-the-art methods remarkably (27.4 mm vs 30.2 mm).



### VS-Net: Voting with Segmentation for Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/2105.10886v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10886v1)
- **Published**: 2021-05-23 08:44:11+00:00
- **Updated**: 2021-05-23 08:44:11+00:00
- **Authors**: Zhaoyang Huang, Han Zhou, Yijin Li, Bangbang Yang, Yan Xu, Xiaowei Zhou, Hujun Bao, Guofeng Zhang, Hongsheng Li
- **Comment**: Project Page: https://drinkingcoder.github.io/publication/vs-net/
- **Journal**: None
- **Summary**: Visual localization is of great importance in robotics and computer vision. Recently, scene coordinate regression based methods have shown good performance in visual localization in small static scenes. However, it still estimates camera poses from many inferior scene coordinates. To address this problem, we propose a novel visual localization framework that establishes 2D-to-3D correspondences between the query image and the 3D map with a series of learnable scene-specific landmarks. In the landmark generation stage, the 3D surfaces of the target scene are over-segmented into mosaic patches whose centers are regarded as the scene-specific landmarks. To robustly and accurately recover the scene-specific landmarks, we propose the Voting with Segmentation Network (VS-Net) to segment the pixels into different landmark patches with a segmentation branch and estimate the landmark locations within each patch with a landmark location voting branch. Since the number of landmarks in a scene may reach up to 5000, training a segmentation network with such a large number of classes is both computation and memory costly for the commonly used cross-entropy loss. We propose a novel prototype-based triplet loss with hard negative mining, which is able to train semantic segmentation networks with a large number of labels efficiently. Our proposed VS-Net is extensively tested on multiple public benchmarks and can outperform state-of-the-art visual localization methods. Code and models are available at \href{https://github.com/zju3dv/VS-Net}{https://github.com/zju3dv/VS-Net}.



### A hybrid classification-regression approach for 3D hand pose estimation using graph convolutional networks
- **Arxiv ID**: http://arxiv.org/abs/2105.10902v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.10902v1)
- **Published**: 2021-05-23 10:09:10+00:00
- **Updated**: 2021-05-23 10:09:10+00:00
- **Authors**: Ikram Kourbane, Yakup Genc
- **Comment**: 18 pages, 8 figures, 6 tables, 4 Algorithms
- **Journal**: None
- **Summary**: Hand pose estimation is a crucial part of a wide range of augmented reality and human-computer interaction applications. Predicting the 3D hand pose from a single RGB image is challenging due to occlusion and depth ambiguities. GCN-based (Graph Convolutional Networks) methods exploit the structural relationship similarity between graphs and hand joints to model kinematic dependencies between joints. These techniques use predefined or globally learned joint relationships, which may fail to capture pose-dependent constraints. To address this problem, we propose a two-stage GCN-based framework that learns per-pose relationship constraints. Specifically, the first phase quantizes the 2D/3D space to classify the joints into 2D/3D blocks based on their locality. This spatial dependency information guides this phase to estimate reliable 2D and 3D poses. The second stage further improves the 3D estimation through a GCN-based module that uses an adaptative nearest neighbor algorithm to determine joint relationships. Extensive experiments show that our multi-stage GCN approach yields an efficient model that produces accurate 2D/3D hand poses and outperforms the state-of-the-art on two public datasets.



### Skeleton-aware multi-scale heatmap regression for 2D hand pose estimation
- **Arxiv ID**: http://arxiv.org/abs/2105.10904v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.10904v1)
- **Published**: 2021-05-23 10:23:51+00:00
- **Updated**: 2021-05-23 10:23:51+00:00
- **Authors**: Ikram Kourbane, Yakup Genc
- **Comment**: 5 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: Existing RGB-based 2D hand pose estimation methods learn the joint locations from a single resolution, which is not suitable for different hand sizes. To tackle this problem, we propose a new deep learning-based framework that consists of two main modules. The former presents a segmentation-based approach to detect the hand skeleton and localize the hand bounding box. The second module regresses the 2D joint locations through a multi-scale heatmap regression approach that exploits the predicted hand skeleton as a constraint to guide the model. Furthermore, we construct a new dataset that is suitable for both hand detection and pose estimation. We qualitatively and quantitatively validate our method on two datasets. Results demonstrate that the proposed method outperforms state-of-the-art and can recover the pose even in cluttered images and complex poses.



### Autonomous Driving Implementation in an Experimental Environment
- **Arxiv ID**: http://arxiv.org/abs/2106.15274v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2106.15274v1)
- **Published**: 2021-05-23 11:14:09+00:00
- **Updated**: 2021-05-23 11:14:09+00:00
- **Authors**: Namig Aliyev, Oguzhan Sezer, Mehmet Turan Guzel
- **Comment**: 8 pages, 21 figures.This is a bachelor's thesis research report and
  was supported by the Scientific and Technological Research Council of Turkey
- **Journal**: None
- **Summary**: Autonomous systems require identifying the environment and it has a long way to go before putting it safely into practice. In autonomous driving systems, the detection of obstacles and traffic lights are of importance as well as lane tracking. In this study, an autonomous driving system is developed and tested in the experimental environment designed for this purpose. In this system, a model vehicle having a camera is used to trace the lanes and avoid obstacles to experimentally study autonomous driving behavior. Convolutional Neural Network models were trained for Lane tracking. For the vehicle to avoid obstacles, corner detection, optical flow, focus of expansion, time to collision, balance calculation, and decision mechanism were created, respectively.



### End-to-End Video Object Detection with Spatial-Temporal Transformers
- **Arxiv ID**: http://arxiv.org/abs/2105.10920v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10920v1)
- **Published**: 2021-05-23 11:44:22+00:00
- **Updated**: 2021-05-23 11:44:22+00:00
- **Authors**: Lu He, Qianyu Zhou, Xiangtai Li, Li Niu, Guangliang Cheng, Xiao Li, Wenxuan Liu, Yunhai Tong, Lizhuang Ma, Liqing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, DETR and Deformable DETR have been proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance as previous complex hand-crafted detectors. However, their performance on Video Object Detection (VOD) has not been well explored. In this paper, we present TransVOD, an end-to-end video object detection model based on a spatial-temporal Transformer architecture. The goal of this paper is to streamline the pipeline of VOD, effectively removing the need for many hand-crafted components for feature aggregation, e.g., optical flow, recurrent neural networks, relation networks. Besides, benefited from the object query design in DETR, our method does not need complicated post-processing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean. In particular, we present temporal Transformer to aggregate both the spatial object queries and the feature memories of each frame. Our temporal Transformer consists of three components: Temporal Deformable Transformer Encoder (TDTE) to encode the multiple frame spatial details, Temporal Query Encoder (TQE) to fuse object queries, and Temporal Deformable Transformer Decoder to obtain current frame detection results. These designs boost the strong baseline deformable DETR by a significant margin (3%-4% mAP) on the ImageNet VID dataset. TransVOD yields comparable results performance on the benchmark of ImageNet VID. We hope our TransVOD can provide a new perspective for video object detection. Code will be made publicly available at https://github.com/SJTU-LuHe/TransVOD.



### COTR: Convolution in Transformer Network for End to End Polyp Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.10925v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10925v1)
- **Published**: 2021-05-23 12:36:48+00:00
- **Updated**: 2021-05-23 12:36:48+00:00
- **Authors**: Zhiqiang Shen, Chaonan Lin, Shaohua Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Colorectal cancer (CRC) is the second most common cause of cancer mortality worldwide. Colonoscopy is a widely used technique for colon screening and polyp lesions diagnosis. Nevertheless, manual screening using colonoscopy suffers from a substantial miss rate of polyps and is an overwhelming burden for endoscopists. Computer-aided diagnosis (CAD) for polyp detection has the potential to reduce human error and human burden. However, current polyp detection methods based on object detection framework need many handcrafted pre-processing and post-processing operations or user guidance that require domain-specific knowledge.   Methods: In this paper, we propose a convolution in transformer (COTR) network for end-to-end polyp detection. Motivated by the detection transformer (DETR), COTR is constituted by a CNN for feature extraction, transformer encoder layers interleaved with convolutional layers for feature encoding and recalibration, transformer decoder layers for object querying, and a feed-forward network for detection prediction. Considering the slow convergence of DETR, COTR embeds convolution layers into transformer encoder for feature reconstruction and convergence acceleration.   Results: Experimental results on two public polyp datasets show that COTR achieved 91.49\% precision, 82.69% sensitivity, and 86.87% F1-score on the ETIS-LARIB, and 91.67% precision, 93.54% sensitivity, and 92.60% F1-score on the CVC-ColonDB.   Conclusion: This study proposed an end to end detection method based on detection transformer for colorectal polyp detection. Experimental results on ETIS-LARIB and CVC-ColonDB dataset demonstrated that the proposed model achieved comparable performance against state-of-the-art methods.



### Boosting Crowd Counting with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2105.10926v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10926v1)
- **Published**: 2021-05-23 12:44:27+00:00
- **Updated**: 2021-05-23 12:44:27+00:00
- **Authors**: Guolei Sun, Yun Liu, Thomas Probst, Danda Pani Paudel, Nikola Popovic, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Significant progress on the crowd counting problem has been achieved by integrating larger context into convolutional neural networks (CNNs). This indicates that global scene context is essential, despite the seemingly bottom-up nature of the problem. This may be explained by the fact that context knowledge can adapt and improve local feature extraction to a given scene. In this paper, we therefore investigate the role of global context for crowd counting. Specifically, a pure transformer is used to extract features with global information from overlapping image patches. Inspired by classification, we add a context token to the input sequence, to facilitate information exchange with tokens corresponding to image patches throughout transformer layers. Due to the fact that transformers do not explicitly model the tried-and-true channel-wise interactions, we propose a token-attention module (TAM) to recalibrate encoded features through channel-wise attention informed by the context token. Beyond that, it is adopted to predict the total person count of the image through regression-token module (RTM). Extensive experiments demonstrate that our method achieves state-of-the-art performance on various datasets, including ShanghaiTech, UCF-QNRF, JHU-CROWD++ and NWPU. On the large-scale JHU-CROWD++ dataset, our method improves over the previous best results by 26.9% and 29.9% in terms of MAE and MSE, respectively.



### Deep Learning Traversability Estimator for Mobile Robots in Unstructured Environments
- **Arxiv ID**: http://arxiv.org/abs/2105.10937v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.10937v2)
- **Published**: 2021-05-23 13:49:05+00:00
- **Updated**: 2021-07-24 15:03:56+00:00
- **Authors**: Marco Visca, Sampo Kuutti, Roger Powell, Yang Gao, Saber Fallah
- **Comment**: Accepted for inclusion in Towards Autonomous Robotic Systems
  Conference (TAROS) 2021
- **Journal**: None
- **Summary**: Terrain traversability analysis plays a major role in ensuring safe robotic navigation in unstructured environments. However, real-time constraints frequently limit the accuracy of online tests especially in scenarios where realistic robot-terrain interactions are complex to model. In this context, we propose a deep learning framework trained in an end-to-end fashion from elevation maps and trajectories to estimate the occurrence of failure events. The network is first trained and tested in simulation over synthetic maps generated by the OpenSimplex algorithm. The prediction performance of the Deep Learning framework is illustrated by being able to retain over 94% recall of the original simulator at 30% of the computational time. Finally, the network is transferred and tested on real elevation maps collected by the SEEKER consortium during the Martian rover test trial in the Atacama desert in Chile. We show that transferring and fine-tuning of an application-independent pre-trained model retains better performance than training uniquely on scarcely available real data.



### SSCAN: A Spatial-spectral Cross Attention Network for Hyperspectral Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2105.10949v1
- **DOI**: 10.1109/LGRS.2021.3112038
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10949v1)
- **Published**: 2021-05-23 14:36:17+00:00
- **Updated**: 2021-05-23 14:36:17+00:00
- **Authors**: Zhiqiang Wang, Zhenfeng Shao, Xiao Huang, Jiaming Wang, Tao Lu, Sihang Zhang
- **Comment**: 5 pages, 5 figures, submitted to IEEE Signal Processing Letters
- **Journal**: None
- **Summary**: Hyperspectral images (HSIs) have been widely used in a variety of applications thanks to the rich spectral information they are able to provide. Among all HSI processing tasks, HSI denoising is a crucial step. Recently, deep learning-based image denoising methods have made great progress and achieved great performance. However, existing methods tend to ignore the correlations between adjacent spectral bands, leading to problems such as spectral distortion and blurred edges in denoised results. In this study, we propose a novel HSI denoising network, termed SSCAN, that combines group convolutions and attention modules. Specifically, we use a group convolution with a spatial attention module to facilitate feature extraction by directing models' attention to band-wise important features. We propose a spectral-spatial attention block (SSAB) to exploit the spatial and spectral information in hyperspectral images in an effective manner. In addition, we adopt residual learning operations with skip connections to ensure training stability. The experimental results indicate that the proposed SSCAN outperforms several state-of-the-art HSI denoising algorithms.



### FBI-Denoiser: Fast Blind Image Denoiser for Poisson-Gaussian Noise
- **Arxiv ID**: http://arxiv.org/abs/2105.10967v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10967v1)
- **Published**: 2021-05-23 16:20:36+00:00
- **Updated**: 2021-05-23 16:20:36+00:00
- **Authors**: Jaeseok Byun, Sungmin Cha, Taesup Moon
- **Comment**: CVPR 2021 camera ready version
- **Journal**: None
- **Summary**: We consider the challenging blind denoising problem for Poisson-Gaussian noise, in which no additional information about clean images or noise level parameters is available. Particularly, when only "single" noisy images are available for training a denoiser, the denoising performance of existing methods was not satisfactory. Recently, the blind pixelwise affine image denoiser (BP-AIDE) was proposed and significantly improved the performance in the above setting, to the extent that it is competitive with denoisers which utilized additional information. However, BP-AIDE seriously suffered from slow inference time due to the inefficiency of noise level estimation procedure and that of the blind-spot network (BSN) architecture it used. To that end, we propose Fast Blind Image Denoiser (FBI-Denoiser) for Poisson-Gaussian noise, which consists of two neural network models; 1) PGE-Net that estimates Poisson-Gaussian noise parameters 2000 times faster than the conventional methods and 2) FBI-Net that realizes a much more efficient BSN for pixelwise affine denoiser in terms of the number of parameters and inference speed. Consequently, we show that our FBI-Denoiser blindly trained solely based on single noisy images can achieve the state-of-the-art performance on several real-world noisy image benchmark datasets with much faster inference time (x 10), compared to BP-AIDE. The official code of our method is available at https://github.com/csm9493/FBI-Denoiser.



### HOME: Heatmap Output for future Motion Estimation
- **Arxiv ID**: http://arxiv.org/abs/2105.10968v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.10968v2)
- **Published**: 2021-05-23 16:27:04+00:00
- **Updated**: 2021-06-02 11:26:47+00:00
- **Authors**: Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bogdan Stanciulescu, Fabien Moutarde
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose HOME, a framework tackling the motion forecasting problem with an image output representing the probability distribution of the agent's future location. This method allows for a simple architecture with classic convolution networks coupled with attention mechanism for agent interactions, and outputs an unconstrained 2D top-view representation of the agent's possible future. Based on this output, we design two methods to sample a finite set of agent's future locations. These methods allow us to control the optimization trade-off between miss rate and final displacement error for multiple modalities without having to retrain any part of the model. We apply our method to the Argoverse Motion Forecasting Benchmark and achieve 1st place on the online leaderboard.



### Weakly Supervised Instance Attention for Multisource Fine-Grained Object Recognition with an Application to Tree Species Classification
- **Arxiv ID**: http://arxiv.org/abs/2105.10983v2
- **DOI**: 10.1016/j.isprsjprs.2021.03.021
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10983v2)
- **Published**: 2021-05-23 17:51:14+00:00
- **Updated**: 2021-05-25 20:35:38+00:00
- **Authors**: Bulut Aygunes, Ramazan Gokberk Cinbis, Selim Aksoy
- **Comment**: Accepted for publication in ISPRS Journal of Photogrammetry and
  Remote Sensing
- **Journal**: None
- **Summary**: Multisource image analysis that leverages complementary spectral, spatial, and structural information benefits fine-grained object recognition that aims to classify an object into one of many similar subcategories. However, for multisource tasks that involve relatively small objects, even the smallest registration errors can introduce high uncertainty in the classification process. We approach this problem from a weakly supervised learning perspective in which the input images correspond to larger neighborhoods around the expected object locations where an object with a given class label is present in the neighborhood without any knowledge of its exact location. The proposed method uses a single-source deep instance attention model with parallel branches for joint localization and classification of objects, and extends this model into a multisource setting where a reference source that is assumed to have no location uncertainty is used to aid the fusion of multiple sources in four different levels: probability level, logit level, feature level, and pixel level. We show that all levels of fusion provide higher accuracies compared to the state-of-the-art, with the best performing method of feature-level fusion resulting in 53% accuracy for the recognition of 40 different types of trees, corresponding to an improvement of 5.7% over the best performing baseline when RGB, multispectral, and LiDAR data are used. We also provide an in-depth comparison by evaluating each model at various parameter complexity settings, where the increased model capacity results in a further improvement of 6.3% over the default capacity setting.



### Wisdom for the Crowd: Discoursive Power in Annotation Instructions for Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2105.10990v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2105.10990v1)
- **Published**: 2021-05-23 18:20:39+00:00
- **Updated**: 2021-05-23 18:20:39+00:00
- **Authors**: Milagros Miceli, Julian Posada
- **Comment**: None
- **Journal**: CVPR 2021 Workshop: Beyond Fairness: Towards a Just, Equitable,
  and Accountable Computer Vision
- **Summary**: Developers of computer vision algorithms outsource some of the labor involved in annotating training data through business process outsourcing companies and crowdsourcing platforms. Many data annotators are situated in the Global South and are considered independent contractors. This paper focuses on the experiences of Argentinian and Venezuelan annotation workers. Through qualitative methods, we explore the discourses encoded in the task instructions that these workers follow to annotate computer vision datasets. Our preliminary findings indicate that annotation instructions reflect worldviews imposed on workers and, through their labor, on datasets. Moreover, we observe that for-profit goals drive task instructions and that managers and algorithms make sure annotations are done according to requesters' commands. This configuration presents a form of commodified labor that perpetuates power asymmetries while reinforcing social inequalities and is compelled to reproduce them into datasets and, subsequently, in computer vision systems.



### Heuristic Weakly Supervised 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2105.10996v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10996v3)
- **Published**: 2021-05-23 18:40:29+00:00
- **Updated**: 2023-05-12 15:31:17+00:00
- **Authors**: Shuangjun Liu, Michael Wan, Sarah Ostadabbas
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular 3D human pose estimation from RGB images has attracted significant attention in recent years. However, recent models depend on supervised training with 3D pose ground truth data or known pose priors for their target domains. 3D pose data is typically collected with motion capture devices, severely limiting their applicability. In this paper, we present a heuristic weakly supervised 3D human pose (HW-HuP) solution to estimate 3D poses in when no ground truth 3D pose data is available. HW-HuP learns partial pose priors from 3D human pose datasets and uses easy-to-access observations from the target domain to estimate 3D human pose and shape in an optimization and regression cycle. We employ depth data for weak supervision during training, but not inference. We show that HW-HuP meaningfully improves upon state-of-the-art models in two practical settings where 3D pose data can hardly be obtained: human poses in bed, and infant poses in the wild. Furthermore, we show that HW-HuP retains comparable performance to cutting-edge models on public benchmarks, even when such models train on 3D pose data.



### Post-Training Sparsity-Aware Quantization
- **Arxiv ID**: http://arxiv.org/abs/2105.11010v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.11010v2)
- **Published**: 2021-05-23 20:12:35+00:00
- **Updated**: 2021-10-28 06:31:41+00:00
- **Authors**: Gil Shomron, Freddy Gabbay, Samer Kurzum, Uri Weiser
- **Comment**: None
- **Journal**: None
- **Summary**: Quantization is a technique used in deep neural networks (DNNs) to increase execution performance and hardware efficiency. Uniform post-training quantization (PTQ) methods are common, since they can be implemented efficiently in hardware and do not require extensive hardware resources or a training set. Mapping FP32 models to INT8 using uniform PTQ yields models with negligible accuracy degradation; however, reducing precision below 8 bits with PTQ is challenging, as accuracy degradation becomes noticeable, due to the increase in quantization noise. In this paper, we propose a sparsity-aware quantization (SPARQ) method, in which the unstructured and dynamic activation sparsity is leveraged in different representation granularities. 4-bit quantization, for example, is employed by dynamically examining the bits of 8-bit values and choosing a window of 4 bits, while first skipping zero-value bits. Moreover, instead of quantizing activation-by-activation to 4 bits, we focus on pairs of 8-bit activations and examine whether one of the two is equal to zero. If one is equal to zero, the second can opportunistically use the other's 4-bit budget; if both do not equal zero, then each is dynamically quantized to 4 bits, as described. SPARQ achieves minor accuracy degradation and a practical hardware implementation. The code is available at https://github.com/gilshm/sparq.



### Revisiting 2D Convolutional Neural Networks for Graph-based Applications
- **Arxiv ID**: http://arxiv.org/abs/2105.11016v1
- **DOI**: 10.1109/TPAMI.2021.3083614
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11016v1)
- **Published**: 2021-05-23 20:34:43+00:00
- **Updated**: 2021-05-23 20:34:43+00:00
- **Authors**: Yecheng Lyu, Xinming Huang, Ziming Zhang
- **Comment**: Accepted by T-PAMI. arXiv admin note: substantial text overlap with
  arXiv:1909.12383
- **Journal**: None
- **Summary**: Graph convolutional networks (GCNs) are widely used in graph-based applications such as graph classification and segmentation. However, current GCNs have limitations on implementation such as network architectures due to their irregular inputs. In contrast, convolutional neural networks (CNNs) are capable of extracting rich features from large-scale input data, but they do not support general graph inputs. To bridge the gap between GCNs and CNNs, in this paper we study the problem of how to effectively and efficiently map general graphs to 2D grids that CNNs can be directly applied to, while preserving graph topology as much as possible. We therefore propose two novel graph-to-grid mapping schemes, namely, {\em graph-preserving grid layout (GPGL)} and its extension {\em Hierarchical GPGL (H-GPGL)} for computational efficiency. We formulate the GPGL problem as integer programming and further propose an approximate yet efficient solver based on a penalized Kamada-Kawai method, a well-known optimization algorithm in 2D graph drawing. We propose a novel vertex separation penalty that encourages graph vertices to lay on the grid without any overlap. Along with this image representation, even extra 2D maxpooling layers contribute to the PointNet, a widely applied point-based neural network. We demonstrate the empirical success of GPGL on general graph classification with small graphs and H-GPGL on 3D point cloud segmentation with large graphs, based on 2D CNNs including VGG16, ResNet50 and multi-scale maxout (MSM) CNN.



### Multi-Type-TD-TSR -- Extracting Tables from Document Images using a Multi-stage Pipeline for Table Detection and Table Structure Recognition: from OCR to Structured Table Representations
- **Arxiv ID**: http://arxiv.org/abs/2105.11021v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, 65D19 (Primary), 05C50 (Secondary), I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2105.11021v1)
- **Published**: 2021-05-23 21:17:18+00:00
- **Updated**: 2021-05-23 21:17:18+00:00
- **Authors**: Pascal Fischer, Alen Smajic, Alexander Mehler, Giuseppe Abrami
- **Comment**: 8 pages, 8 figure
- **Journal**: None
- **Summary**: As global trends are shifting towards data-driven industries, the demand for automated algorithms that can convert digital images of scanned documents into machine readable information is rapidly growing. Besides the opportunity of data digitization for the application of data analytic tools, there is also a massive improvement towards automation of processes, which previously would require manual inspection of the documents. Although the introduction of optical character recognition technologies mostly solved the task of converting human-readable characters from images into machine-readable characters, the task of extracting table semantics has been less focused on over the years. The recognition of tables consists of two main tasks, namely table detection and table structure recognition. Most prior work on this problem focuses on either task without offering an end-to-end solution or paying attention to real application conditions like rotated images or noise artefacts inside the document image. Recent work shows a clear trend towards deep learning approaches coupled with the use of transfer learning for the task of table structure recognition due to the lack of sufficiently large datasets. In this paper we present a multistage pipeline named Multi-Type-TD-TSR, which offers an end-to-end solution for the problem of table recognition. It utilizes state-of-the-art deep learning models for table detection and differentiates between 3 different types of tables based on the tables' borders. For the table structure recognition we use a deterministic non-data driven algorithm, which works on all table types. We additionally present two algorithms. One for unbordered tables and one for bordered tables, which are the base of the used table structure recognition algorithm. We evaluate Multi-Type-TD-TSR on the ICDAR 2019 table structure recognition dataset and achieve a new state-of-the-art.



