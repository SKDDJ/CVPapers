# Arxiv Papers in cs.CV on 2021-05-26
### FINNger -- Applying artificial intelligence to ease math learning for children
- **Arxiv ID**: http://arxiv.org/abs/2105.12281v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12281v1)
- **Published**: 2021-05-26 01:02:31+00:00
- **Updated**: 2021-05-26 01:02:31+00:00
- **Authors**: Rafael Baldasso Audibert, Vinicius Marinho Maschio
- **Comment**: None
- **Journal**: None
- **Summary**: Kids have an amazing capacity to use modern electronic devices such as tablets, smartphones, etc. This has been incredibly boosted by the ease of access of these devices given the expansion of such devices through the world, reaching even third world countries. Also, it is well known that children tend to have difficulty learning some subjects at pre-school. We as a society focus extensively on alphabetization, but in the end, children end up having differences in another essential area: Mathematics. With this work, we create the basis for an intuitive application that could join the fact that children have a lot of ease when using such technological applications, trying to shrink the gap between a fun and enjoyable activity with something that will improve the children knowledge and ability to understand concepts when in a low age, by using a novel convolutional neural network to achieve so, named FINNger.



### Performance Analysis of a Foreground Segmentation Neural Network Model
- **Arxiv ID**: http://arxiv.org/abs/2105.12311v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2105.12311v1)
- **Published**: 2021-05-26 03:07:07+00:00
- **Updated**: 2021-05-26 03:07:07+00:00
- **Authors**: Joel Tomás Morais, António Ramires Fernandes, André Leite Ferreira, Bruno Faria
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: In recent years the interest in segmentation has been growing, being used in a wide range of applications such as fraud detection, anomaly detection in public health and intrusion detection. We present an ablation study of FgSegNet_v2, analysing its three stages: (i) Encoder, (ii) Feature Pooling Module and (iii) Decoder. The result of this study is a proposal of a variation of the aforementioned method that surpasses state of the art results. Three datasets are used for testing: CDNet2014, SBI2015 and CityScapes. In CDNet2014 we got an overall improvement compared to the state of the art, mainly in the LowFrameRate subset. The presented approach is promising as it produces comparable results with the state of the art (SBI2015 and Cityscapes datasets) in very different conditions, such as different lighting conditions.



### PSGAN++: Robust Detail-Preserving Makeup Transfer and Removal
- **Arxiv ID**: http://arxiv.org/abs/2105.12324v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12324v1)
- **Published**: 2021-05-26 04:37:57+00:00
- **Updated**: 2021-05-26 04:37:57+00:00
- **Authors**: Si Liu, Wentao Jiang, Chen Gao, Ran He, Jiashi Feng, Bo Li, Shuicheng Yan
- **Comment**: Accepted by TPAMI 2021
- **Journal**: None
- **Summary**: In this paper, we address the makeup transfer and removal tasks simultaneously, which aim to transfer the makeup from a reference image to a source image and remove the makeup from the with-makeup image respectively. Existing methods have achieved much advancement in constrained scenarios, but it is still very challenging for them to transfer makeup between images with large pose and expression differences, or handle makeup details like blush on cheeks or highlight on the nose. In addition, they are hardly able to control the degree of makeup during transferring or to transfer a specified part in the input face. In this work, we propose the PSGAN++, which is capable of performing both detail-preserving makeup transfer and effective makeup removal. For makeup transfer, PSGAN++ uses a Makeup Distill Network to extract makeup information, which is embedded into spatial-aware makeup matrices. We also devise an Attentive Makeup Morphing module that specifies how the makeup in the source image is morphed from the reference image, and a makeup detail loss to supervise the model within the selected makeup detail area. On the other hand, for makeup removal, PSGAN++ applies an Identity Distill Network to embed the identity information from with-makeup images into identity matrices. Finally, the obtained makeup/identity matrices are fed to a Style Transfer Network that is able to edit the feature maps to achieve makeup transfer or removal. To evaluate the effectiveness of our PSGAN++, we collect a Makeup Transfer In the Wild dataset that contains images with diverse poses and expressions and a Makeup Transfer High-Resolution dataset that contains high-resolution images. Experiments demonstrate that PSGAN++ not only achieves state-of-the-art results with fine makeup details even in cases of large pose/expression differences but also can perform partial or degree-controllable makeup transfer.



### SimNet: Learning Reactive Self-driving Simulations from Real-world Observations
- **Arxiv ID**: http://arxiv.org/abs/2105.12332v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.12332v1)
- **Published**: 2021-05-26 05:14:23+00:00
- **Updated**: 2021-05-26 05:14:23+00:00
- **Authors**: Luca Bergamini, Yawei Ye, Oliver Scheel, Long Chen, Chih Hu, Luca Del Pero, Blazej Osinski, Hugo Grimmett, Peter Ondruska
- **Comment**: Published at 2021 International Conference on Robotics and Automation
  (ICRA2021)
- **Journal**: None
- **Summary**: In this work, we present a simple end-to-end trainable machine learning system capable of realistically simulating driving experiences. This can be used for the verification of self-driving system performance without relying on expensive and time-consuming road testing. In particular, we frame the simulation problem as a Markov Process, leveraging deep neural networks to model both state distribution and transition function. These are trainable directly from the existing raw observations without the need for any handcrafting in the form of plant or kinematic models. All that is needed is a dataset of historical traffic episodes. Our formulation allows the system to construct never seen scenes that unfold realistically reacting to the self-driving car's behaviour. We train our system directly from 1,000 hours of driving logs and measure both realism, reactivity of the simulation as the two key properties of the simulation. At the same time, we apply the method to evaluate the performance of a recently proposed state-of-the-art ML planning system trained from human driving logs. We discover this planning system is prone to previously unreported causal confusion issues that are difficult to test by non-reactive simulation. To the best of our knowledge, this is the first work that directly merges highly realistic data-driven simulations with a closed-loop evaluation for self-driving vehicles. We make the data, code, and pre-trained models publicly available to further stimulate simulation development.



### What data do we need for training an AV motion planner?
- **Arxiv ID**: http://arxiv.org/abs/2105.12337v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.12337v1)
- **Published**: 2021-05-26 05:37:12+00:00
- **Updated**: 2021-05-26 05:37:12+00:00
- **Authors**: Long Chen, Lukas Platinsky, Stefanie Speichert, Blazej Osinski, Oliver Scheel, Yawei Ye, Hugo Grimmett, Luca del Pero, Peter Ondruska
- **Comment**: Published at 2021 International Conference on Robotics and Automation
  (ICRA2021)
- **Journal**: None
- **Summary**: We investigate what grade of sensor data is required for training an imitation-learning-based AV planner on human expert demonstration. Machine-learned planners are very hungry for training data, which is usually collected using vehicles equipped with the same sensors used for autonomous operation. This is costly and non-scalable. If cheaper sensors could be used for collection instead, data availability would go up, which is crucial in a field where data volume requirements are large and availability is small. We present experiments using up to 1000 hours worth of expert demonstration and find that training with 10x lower-quality data outperforms 1x AV-grade data in terms of planner performance. The important implication of this is that cheaper sensors can indeed be used. This serves to improve data access and democratize the field of imitation-based motion planning. Alongside this, we perform a sensitivity analysis of planner performance as a function of perception range, field-of-view, accuracy, and data volume, and the reason why lower-quality data still provide good planning results.



### Multiple Domain Experts Collaborative Learning: Multi-Source Domain Generalization For Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2105.12355v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12355v2)
- **Published**: 2021-05-26 06:38:23+00:00
- **Updated**: 2021-09-11 16:31:43+00:00
- **Authors**: Shijie Yu, Feng Zhu, Dapeng Chen, Rui Zhao, Haobin Chen, Shixiang Tang, Jinguo Zhu, Yu Qiao
- **Comment**: update the mAP results, and add the experiments on DG-classification
  task
- **Journal**: None
- **Summary**: Recent years have witnessed significant progress in person re-identification (ReID). However, current ReID approaches still suffer from considerable performance degradation when unseen testing domains exhibit different characteristics from the source training ones, known as the domain generalization problem. Given multiple source training domains, previous Domain Generalizable ReID (DG-ReID) methods usually learn all domains together using a shared network, which can't learn sufficient knowledge from each domain. In this paper, we propose a novel Multiple Domain Experts Collaborative Learning (MECL) framework for better exploiting all training domains, which benefits from the proposed Domain-Domain Collaborative Learning (DDCL) and Universal-Domain Collaborative Learning (UDCL). DDCL utilizes domain-specific experts for fully exploiting each domain, and prevents experts from over-fitting the corresponding domain using a meta-learning strategy. In UDCL, a universal expert supervises the learning of domain experts and continuously gathers knowledge from all domain experts. Note, only the universal expert will be used for inference. Extensive experiments on DG-ReID benchmarks demonstrate the effectiveness of DDCL and UDCL, and show that the whole MECL framework significantly outperforms state-of-the-arts. Experimental results on DG-classification benchmarks also reveal the great potential of applying MECL to other DG tasks.



### Using the Overlapping Score to Improve Corruption Benchmarks
- **Arxiv ID**: http://arxiv.org/abs/2105.12357v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.12357v1)
- **Published**: 2021-05-26 06:42:54+00:00
- **Updated**: 2021-05-26 06:42:54+00:00
- **Authors**: Alfred Laugros, Alice Caplier, Matthieu Ospici
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Networks are sensitive to various corruptions that usually occur in real-world applications such as blurs, noises, low-lighting conditions, etc. To estimate the robustness of neural networks to these common corruptions, we generally use a group of modeled corruptions gathered into a benchmark. Unfortunately, no objective criterion exists to determine whether a benchmark is representative of a large diversity of independent corruptions. In this paper, we propose a metric called corruption overlapping score, which can be used to reveal flaws in corruption benchmarks. Two corruptions overlap when the robustnesses of neural networks to these corruptions are correlated. We argue that taking into account overlappings between corruptions can help to improve existing benchmarks or build better ones.



### How to Calibrate Your Event Camera
- **Arxiv ID**: http://arxiv.org/abs/2105.12362v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12362v1)
- **Published**: 2021-05-26 07:06:58+00:00
- **Updated**: 2021-05-26 07:06:58+00:00
- **Authors**: Manasi Muglikar, Mathias Gehrig, Daniel Gehrig, Davide Scaramuzza
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition Workshops
- **Journal**: None
- **Summary**: We propose a generic event camera calibration framework using image reconstruction. Instead of relying on blinking LED patterns or external screens, we show that neural-network-based image reconstruction is well suited for the task of intrinsic and extrinsic calibration of event cameras. The advantage of our proposed approach is that we can use standard calibration patterns that do not rely on active illumination. Furthermore, our approach enables the possibility to perform extrinsic calibration between frame-based and event-based sensors without additional complexity. Both simulation and real-world experiments indicate that calibration through image reconstruction is accurate under common distortion models and a wide variety of distortion parameters



### Learning to Detect Fortified Areas
- **Arxiv ID**: http://arxiv.org/abs/2105.12385v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.12385v1)
- **Published**: 2021-05-26 08:03:42+00:00
- **Updated**: 2021-05-26 08:03:42+00:00
- **Authors**: Allan Grønlund, Jonas Tranberg
- **Comment**: None
- **Journal**: None
- **Summary**: High resolution data models like grid terrain models made from LiDAR data are a prerequisite for modern day Geographic Information Systems applications. Besides providing the foundation for the very accurate digital terrain models, LiDAR data is also extensively used to classify which parts of the considered surface comprise relevant elements like water, buildings and vegetation. In this paper we consider the problem of classifying which areas of a given surface are fortified by for instance, roads, sidewalks, parking spaces, paved driveways and terraces. We consider using LiDAR data and orthophotos, combined and alone, to show how well the modern machine learning algorithms Gradient Boosted Trees and Convolutional Neural Networks are able to detect fortified areas on large real world data. The LiDAR data features, in particular the intensity feature that measures the signal strength of the return, that we consider in this project are heavily dependent on the actual LiDAR sensor that made the measurement. This is highly problematic, in particular for the generalisation capability of pattern matching algorithms, as this means that data features for test data may be very different from the data the model is trained on. We propose an algorithmic solution to this problem by designing a neural net embedding architecture that transforms data from all the different sensor systems into a new common representation that works as well as if the training data and test data originated from the same sensor. The final algorithm result has an accuracy above 96 percent, and an AUC score above 0.99.



### CBANet: Towards Complexity and Bitrate Adaptive Deep Image Compression using a Single Network
- **Arxiv ID**: http://arxiv.org/abs/2105.12386v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.12386v1)
- **Published**: 2021-05-26 08:13:56+00:00
- **Updated**: 2021-05-26 08:13:56+00:00
- **Authors**: Jinyang Guo, Dong Xu, Guo Lu
- **Comment**: Submitted to T-IP
- **Journal**: None
- **Summary**: In this paper, we propose a new deep image compression framework called Complexity and Bitrate Adaptive Network (CBANet), which aims to learn one single network to support variable bitrate coding under different computational complexity constraints. In contrast to the existing state-of-the-art learning based image compression frameworks that only consider the rate-distortion trade-off without introducing any constraint related to the computational complexity, our CBANet considers the trade-off between the rate and distortion under dynamic computational complexity constraints. Specifically, to decode the images with one single decoder under various computational complexity constraints, we propose a new multi-branch complexity adaptive module, in which each branch only takes a small portion of the computational budget of the decoder. The reconstructed images with different visual qualities can be readily generated by using different numbers of branches. Furthermore, to achieve variable bitrate decoding with one single decoder, we propose a bitrate adaptive module to project the representation from a base bitrate to the expected representation at a target bitrate for transmission. Then it will project the transmitted representation at the target bitrate back to that at the base bitrate for the decoding process. The proposed bit adaptive module can significantly reduce the storage requirement for deployment platforms. As a result, our CBANet enables one single codec to support multiple bitrate decoding under various computational complexity constraints. Comprehensive experiments on two benchmark datasets demonstrate the effectiveness of our CBANet for deep image compression.



### Improving Sign Language Translation with Monolingual Data by Sign Back-Translation
- **Arxiv ID**: http://arxiv.org/abs/2105.12397v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2105.12397v1)
- **Published**: 2021-05-26 08:49:30+00:00
- **Updated**: 2021-05-26 08:49:30+00:00
- **Authors**: Hao Zhou, Wengang Zhou, Weizhen Qi, Junfu Pu, Houqiang Li
- **Comment**: To appear in 2021 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR 2021)
- **Journal**: None
- **Summary**: Despite existing pioneering works on sign language translation (SLT), there is a non-trivial obstacle, i.e., the limited quantity of parallel sign-text data. To tackle this parallel data bottleneck, we propose a sign back-translation (SignBT) approach, which incorporates massive spoken language texts into SLT training. With a text-to-gloss translation model, we first back-translate the monolingual text to its gloss sequence. Then, the paired sign sequence is generated by splicing pieces from an estimated gloss-to-sign bank at the feature level. Finally, the synthetic parallel data serves as a strong supplement for the end-to-end training of the encoder-decoder SLT framework.   To promote the SLT research, we further contribute CSL-Daily, a large-scale continuous SLT dataset. It provides both spoken language translations and gloss-level annotations. The topic revolves around people's daily lives (e.g., travel, shopping, medical care), the most likely SLT application scenario. Extensive experimental results and analysis of SLT methods are reported on CSL-Daily. With the proposed sign back-translation method, we obtain a substantial improvement over previous state-of-the-art SLT methods.



### Unsupervised Part Segmentation through Disentangling Appearance and Shape
- **Arxiv ID**: http://arxiv.org/abs/2105.12405v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12405v1)
- **Published**: 2021-05-26 08:59:31+00:00
- **Updated**: 2021-05-26 08:59:31+00:00
- **Authors**: Shilong Liu, Lei Zhang, Xiao Yang, Hang Su, Jun Zhu
- **Comment**: Accepted in CVPR 2021
- **Journal**: None
- **Summary**: We study the problem of unsupervised discovery and segmentation of object parts, which, as an intermediate local representation, are capable of finding intrinsic object structure and providing more explainable recognition results. Recent unsupervised methods have greatly relaxed the dependency on annotated data which are costly to obtain, but still rely on additional information such as object segmentation mask or saliency map. To remove such a dependency and further improve the part segmentation performance, we develop a novel approach by disentangling the appearance and shape representations of object parts followed with reconstruction losses without using additional object mask information. To avoid degenerated solutions, a bottleneck block is designed to squeeze and expand the appearance representation, leading to a more effective disentanglement between geometry and appearance. Combined with a self-supervised part classification loss and an improved geometry concentration constraint, we can segment more consistent parts with semantic meanings. Comprehensive experiments on a wide variety of objects such as face, bird, and PASCAL VOC objects demonstrate the effectiveness of the proposed method.



### Permutation invariance and uncertainty in multitemporal image super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2105.12409v1
- **DOI**: 10.1109/TGRS.2021.3130673
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.12409v1)
- **Published**: 2021-05-26 09:03:12+00:00
- **Updated**: 2021-05-26 09:03:12+00:00
- **Authors**: Diego Valsesia, Enrico Magli
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances have shown how deep neural networks can be extremely effective at super-resolving remote sensing imagery, starting from a multitemporal collection of low-resolution images. However, existing models have neglected the issue of temporal permutation, whereby the temporal ordering of the input images does not carry any relevant information for the super-resolution task and causes such models to be inefficient with the, often scarce, ground truth data that available for training. Thus, models ought not to learn feature extractors that rely on temporal ordering. In this paper, we show how building a model that is fully invariant to temporal permutation significantly improves performance and data efficiency. Moreover, we study how to quantify the uncertainty of the super-resolved image so that the final user is informed on the local quality of the product. We show how uncertainty correlates with temporal variation in the series, and how quantifying it further improves model performance. Experiments on the Proba-V challenge dataset show significant improvements over the state of the art without the need for self-ensembling, as well as improved data efficiency, reaching the performance of the challenge winner with just 25% of the training data.



### Anticipating human actions by correlating past with the future with Jaccard similarity measures
- **Arxiv ID**: http://arxiv.org/abs/2105.12414v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12414v1)
- **Published**: 2021-05-26 09:11:02+00:00
- **Updated**: 2021-05-26 09:11:02+00:00
- **Authors**: Basura Fernando, Samitha Herath
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: We propose a framework for early action recognition and anticipation by correlating past features with the future using three novel similarity measures called Jaccard vector similarity, Jaccard cross-correlation and Jaccard Frobenius inner product over covariances. Using these combinations of novel losses and using our framework, we obtain state-of-the-art results for early action recognition in UCF101 and JHMDB datasets by obtaining 91.7 % and 83.5 % accuracy respectively for an observation percentage of 20. Similarly, we obtain state-of-the-art results for Epic-Kitchen55 and Breakfast datasets for action anticipation by obtaining 20.35 and 41.8 top-1 accuracy respectively.



### Weighing Features of Lung and Heart Regions for Thoracic Disease Classification
- **Arxiv ID**: http://arxiv.org/abs/2105.12430v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.12430v1)
- **Published**: 2021-05-26 09:37:39+00:00
- **Updated**: 2021-05-26 09:37:39+00:00
- **Authors**: Jiansheng Fang, Yanwu Xu, Yitian Zhao, Yuguang Yan, Junling Liu, Jiang Liu
- **Comment**: 17 pages, 4 figures, BMC Medical Imaging
- **Journal**: None
- **Summary**: Chest X-rays are the most commonly available and affordable radiological examination for screening thoracic diseases. According to the domain knowledge of screening chest X-rays, the pathological information usually lay on the lung and heart regions. However, it is costly to acquire region-level annotation in practice, and model training mainly relies on image-level class labels in a weakly supervised manner, which is highly challenging for computer-aided chest X-ray screening. To address this issue, some methods have been proposed recently to identify local regions containing pathological information, which is vital for thoracic disease classification. Inspired by this, we propose a novel deep learning framework to explore discriminative information from lung and heart regions. We design a feature extractor equipped with a multi-scale attention module to learn global attention maps from global images. To exploit disease-specific cues effectively, we locate lung and heart regions containing pathological information by a well-trained pixel-wise segmentation model to generate binarization masks. By introducing element-wise logical AND operator on the learned global attention maps and the binarization masks, we obtain local attention maps in which pixels are $1$ for lung and heart region and $0$ for other regions. By zeroing features of non-lung and heart regions in attention maps, we can effectively exploit their disease-specific cues in lung and heart regions. Compared to existing methods fusing global and local features, we adopt feature weighting to avoid weakening visual cues unique to lung and heart regions. Evaluated by the benchmark split on the publicly available chest X-ray14 dataset, the comprehensive experiments show that our method achieves superior performance compared to the state-of-the-art methods.



### Towards an IMU-based Pen Online Handwriting Recognizer
- **Arxiv ID**: http://arxiv.org/abs/2105.12434v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.12434v1)
- **Published**: 2021-05-26 09:47:19+00:00
- **Updated**: 2021-05-26 09:47:19+00:00
- **Authors**: Mohamad Wehbi, Tim Hamann, Jens Barth, Peter Kaempf, Dario Zanca, Bjoern Eskofier
- **Comment**: Accepted at ICDAR 2021
- **Journal**: None
- **Summary**: Most online handwriting recognition systems require the use of specific writing surfaces to extract positional data. In this paper we present a online handwriting recognition system for word recognition which is based on inertial measurement units (IMUs) for digitizing text written on paper. This is obtained by means of a sensor-equipped pen that provides acceleration, angular velocity, and magnetic forces streamed via Bluetooth. Our model combines convolutional and bidirectional LSTM networks, and is trained with the Connectionist Temporal Classification loss that allows the interpretation of raw sensor data into words without the need of sequence segmentation. We use a dataset of words collected using multiple sensor-enhanced pens and evaluate our model on distinct test sets of seen and unseen words achieving a character error rate of 17.97% and 17.08%, respectively, without the use of a dictionary or language model



### Social-IWSTCNN: A Social Interaction-Weighted Spatio-Temporal Convolutional Neural Network for Pedestrian Trajectory Prediction in Urban Traffic Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2105.12436v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.12436v1)
- **Published**: 2021-05-26 09:53:19+00:00
- **Updated**: 2021-05-26 09:53:19+00:00
- **Authors**: Chi Zhang, Christian Berger, Marco Dozza
- **Comment**: 8 pages, 4 figures. Accepted in IEEE Intelligent Vehicles Symposium
  (IV), 2021
- **Journal**: None
- **Summary**: Pedestrian trajectory prediction in urban scenarios is essential for automated driving. This task is challenging because the behavior of pedestrians is influenced by both their own history paths and the interactions with others. Previous research modeled these interactions with pooling mechanisms or aggregating with hand-crafted attention weights. In this paper, we present the Social Interaction-Weighted Spatio-Temporal Convolutional Neural Network (Social-IWSTCNN), which includes both the spatial and the temporal features. We propose a novel design, namely the Social Interaction Extractor, to learn the spatial and social interaction features of pedestrians. Most previous works used ETH and UCY datasets which include five scenes but do not cover urban traffic scenarios extensively for training and evaluation. In this paper, we use the recently released large-scale Waymo Open Dataset in urban traffic scenarios, which includes 374 urban training scenes and 76 urban testing scenes to analyze the performance of our proposed algorithm in comparison to the state-of-the-art (SOTA) models. The results show that our algorithm outperforms SOTA algorithms such as Social-LSTM, Social-GAN, and Social-STGCNN on both Average Displacement Error (ADE) and Final Displacement Error (FDE). Furthermore, our Social-IWSTCNN is 54.8 times faster in data pre-processing speed, and 4.7 times faster in total test speed than the current best SOTA algorithm Social-STGCNN.



### DeepGaze IIE: Calibrated prediction in and out-of-domain for state-of-the-art saliency modeling
- **Arxiv ID**: http://arxiv.org/abs/2105.12441v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.12441v3)
- **Published**: 2021-05-26 09:59:56+00:00
- **Updated**: 2021-09-20 10:37:23+00:00
- **Authors**: Akis Linardos, Matthias Kümmerer, Ori Press, Matthias Bethge
- **Comment**: Joint first authors, published in ICCV
- **Journal**: None
- **Summary**: Since 2014 transfer learning has become the key driver for the improvement of spatial saliency prediction; however, with stagnant progress in the last 3-5 years. We conduct a large-scale transfer learning study which tests different ImageNet backbones, always using the same read out architecture and learning protocol adopted from DeepGaze II. By replacing the VGG19 backbone of DeepGaze II with ResNet50 features we improve the performance on saliency prediction from 78% to 85%. However, as we continue to test better ImageNet models as backbones (such as EfficientNetB5) we observe no additional improvement on saliency prediction. By analyzing the backbones further, we find that generalization to other datasets differs substantially, with models being consistently overconfident in their fixation predictions. We show that by combining multiple backbones in a principled manner a good confidence calibration on unseen datasets can be achieved. This new model, "DeepGaze IIE", yields a significant leap in benchmark performance in and out-of-domain with a 15 percent point improvement over DeepGaze II to 93% on MIT1003, marking a new state of the art on the MIT/Tuebingen Saliency Benchmark in all available metrics (AUC: 88.3%, sAUC: 79.4%, CC: 82.4%).



### Direct Servo Control from In-Sensor CNN Inference with A Pixel Processor Array
- **Arxiv ID**: http://arxiv.org/abs/2106.07561v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07561v1)
- **Published**: 2021-05-26 10:10:32+00:00
- **Updated**: 2021-05-26 10:10:32+00:00
- **Authors**: Yanan Liu, Jianing Chen, Laurie Bose, Piotr Dudek, Walterio Mayol-Cuevas
- **Comment**: None
- **Journal**: None
- **Summary**: This work demonstrates direct visual sensory-motor control using high-speed CNN inference via a SCAMP-5 Pixel Processor Array (PPA). We demonstrate how PPAs are able to efficiently bridge the gap between perception and action. A binary Convolutional Neural Network (CNN) is used for a classic rock, paper, scissors classification problem at over 8000 FPS. Control instructions are directly sent to a servo motor from the PPA according to the CNN's classification result without any other intermediate hardware.



### Pattern Detection in the Activation Space for Identifying Synthesized Content
- **Arxiv ID**: http://arxiv.org/abs/2105.12479v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.12479v2)
- **Published**: 2021-05-26 11:28:36+00:00
- **Updated**: 2021-05-27 08:40:27+00:00
- **Authors**: Celia Cintas, Skyler Speakman, Girmaw Abebe Tadesse, Victor Akinwande, Edward McFowland III, Komminist Weldemariam
- **Comment**: The paper is under consideration at Pattern Recognition Letters
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have recently achieved unprecedented success in photo-realistic image synthesis from low-dimensional random noise. The ability to synthesize high-quality content at a large scale brings potential risks as the generated samples may lead to misinformation that can create severe social, political, health, and business hazards. We propose SubsetGAN to identify generated content by detecting a subset of anomalous node-activations in the inner layers of pre-trained neural networks. These nodes, as a group, maximize a non-parametric measure of divergence away from the expected distribution of activations created from real data. This enable us to identify synthesised images without prior knowledge of their distribution. SubsetGAN efficiently scores subsets of nodes and returns the group of nodes within the pre-trained classifier that contributed to the maximum score. The classifier can be a general fake classifier trained over samples from multiple sources or the discriminator network from different GANs. Our approach shows consistently higher detection power than existing detection methods across several state-of-the-art GANs (PGGAN, StarGAN, and CycleGAN) and over different proportions of generated content.



### Adversarial Robustness against Multiple and Single $l_p$-Threat Models via Quick Fine-Tuning of Robust Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2105.12508v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.12508v2)
- **Published**: 2021-05-26 12:20:47+00:00
- **Updated**: 2022-08-07 21:58:14+00:00
- **Authors**: Francesco Croce, Matthias Hein
- **Comment**: ICML 2022
- **Journal**: None
- **Summary**: A major drawback of adversarially robust models, in particular for large scale datasets like ImageNet, is the extremely long training time compared to standard ones. Moreover, models should be robust not only to one $l_p$-threat model but ideally to all of them. In this paper we propose Extreme norm Adversarial Training (E-AT) for multiple-norm robustness which is based on geometric properties of $l_p$-balls. E-AT costs up to three times less than other adversarial training methods for multiple-norm robustness. Using E-AT we show that for ImageNet a single epoch and for CIFAR-10 three epochs are sufficient to turn any $l_p$-robust model into a multiple-norm robust model. In this way we get the first multiple-norm robust model for ImageNet and boost the state-of-the-art for multiple-norm robustness to more than $51\%$ on CIFAR-10. Finally, we study the general transfer via fine-tuning of adversarial robustness between different individual $l_p$-threat models and improve the previous SOTA $l_1$-robustness on both CIFAR-10 and ImageNet. Extensive experiments show that our scheme works across datasets and architectures including vision transformers.



### Unsupervised Video Summarization via Multi-source Features
- **Arxiv ID**: http://arxiv.org/abs/2105.12532v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.12532v1)
- **Published**: 2021-05-26 13:12:46+00:00
- **Updated**: 2021-05-26 13:12:46+00:00
- **Authors**: Hussain Kanafani, Junaid Ahmed Ghauri, Sherzod Hakimov, Ralph Ewerth
- **Comment**: Accepted for publication at the ACM International Conference on
  Multimedia Retrieval (ICMR) 2021
- **Journal**: None
- **Summary**: Video summarization aims at generating a compact yet representative visual summary that conveys the essence of the original video. The advantage of unsupervised approaches is that they do not require human annotations to learn the summarization capability and generalize to a wider range of domains. Previous work relies on the same type of deep features, typically based on a model pre-trained on ImageNet data. Therefore, we propose the incorporation of multiple feature sources with chunk and stride fusion to provide more information about the visual content. For a comprehensive evaluation on the two benchmarks TVSum and SumMe, we compare our method with four state-of-the-art approaches. Two of these approaches were implemented by ourselves to reproduce the reported results. Our evaluation shows that we obtain state-of-the-art results on both datasets, while also highlighting the shortcomings of previous work with regard to the evaluation methodology. Finally, we perform error analysis on videos for the two benchmark datasets to summarize and spot the factors that lead to misclassifications.



### KLIEP-based Density Ratio Estimation for Semantically Consistent Synthetic to Real Images Adaptation in Urban Traffic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2105.12549v1
- **DOI**: 10.1109/IROS45743.2020.9341547
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12549v1)
- **Published**: 2021-05-26 13:59:19+00:00
- **Updated**: 2021-05-26 13:59:19+00:00
- **Authors**: Artem Savkin, Federico Tombari
- **Comment**: None
- **Journal**: 2020 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS) (2020) 5901-5908
- **Summary**: Synthetic data has been applied in many deep learning based computer vision tasks. Limited performance of algorithms trained solely on synthetic data has been approached with domain adaptation techniques such as the ones based on generative adversarial framework. We demonstrate how adversarial training alone can introduce semantic inconsistencies in translated images. To tackle this issue we propose density prematching strategy using KLIEP-based density ratio estimation procedure. Finally, we show that aforementioned strategy improves quality of translated images of underlying method and their usability for the semantic segmentation task in the context of autonomous driving.



### Context-aware Cross-level Fusion Network for Camouflaged Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.12555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12555v1)
- **Published**: 2021-05-26 14:03:36+00:00
- **Updated**: 2021-05-26 14:03:36+00:00
- **Authors**: Yujia Sun, Geng Chen, Tao Zhou, Yi Zhang, Nian Liu
- **Comment**: 7 pages, 4 figures. Accepted by IJCAI-2021
- **Journal**: None
- **Summary**: Camouflaged object detection (COD) is a challenging task due to the low boundary contrast between the object and its surroundings. In addition, the appearance of camouflaged objects varies significantly, e.g., object size and shape, aggravating the difficulties of accurate COD. In this paper, we propose a novel Context-aware Cross-level Fusion Network (C2F-Net) to address the challenging COD task. Specifically, we propose an Attention-induced Cross-level Fusion Module (ACFM) to integrate the multi-level features with informative attention coefficients. The fused features are then fed to the proposed Dual-branch Global Context Module (DGCM), which yields multi-scale feature representations for exploiting rich global context information. In C2F-Net, the two modules are conducted on high-level features using a cascaded manner. Extensive experiments on three widely used benchmark datasets demonstrate that our C2F-Net is an effective COD model and outperforms state-of-the-art models remarkably. Our code is publicly available at: https://github.com/thograce/C2FNet.



### Predicting invasive ductal carcinoma using a Reinforcement Sample Learning Strategy using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.12564v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.12564v2)
- **Published**: 2021-05-26 14:14:45+00:00
- **Updated**: 2021-11-07 22:57:06+00:00
- **Authors**: Rushabh Patel
- **Comment**: Updated References
- **Journal**: None
- **Summary**: Invasive ductal carcinoma is a prevalent, potentially deadly disease associated with a high rate of morbidity and mortality. Its malignancy is the second leading cause of death from cancer in women. The mammogram is an extremely useful resource for mass detection and invasive ductal carcinoma diagnosis. We are proposing a method for Invasive ductal carcinoma that will use convolutional neural networks (CNN) on mammograms to assist radiologists in diagnosing the disease. Due to the varying image clarity and structure of certain mammograms, it is difficult to observe major cancer characteristics such as microcalcification and mass, and it is often difficult to interpret and diagnose these attributes. The aim of this study is to establish a novel method for fully automated feature extraction and classification in invasive ductal carcinoma computer-aided diagnosis (CAD) systems. This article presents a tumor classification algorithm that makes novel use of convolutional neural networks on breast mammogram images to increase feature extraction and training speed. The algorithm makes two contributions.



### Predict then Interpolate: A Simple Algorithm to Learn Stable Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2105.12628v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.12628v1)
- **Published**: 2021-05-26 15:37:48+00:00
- **Updated**: 2021-05-26 15:37:48+00:00
- **Authors**: Yujia Bao, Shiyu Chang, Regina Barzilay
- **Comment**: ICML 2021
- **Journal**: None
- **Summary**: We propose Predict then Interpolate (PI), a simple algorithm for learning correlations that are stable across environments. The algorithm follows from the intuition that when using a classifier trained on one environment to make predictions on examples from another environment, its mistakes are informative as to which correlations are unstable. In this work, we prove that by interpolating the distributions of the correct predictions and the wrong predictions, we can uncover an oracle distribution where the unstable correlation vanishes. Since the oracle interpolation coefficients are not accessible, we use group distributionally robust optimization to minimize the worst-case risk across all such interpolations. We evaluate our method on both text classification and image classification. Empirical results demonstrate that our algorithm is able to learn robust classifiers (outperforms IRM by 23.85% on synthetic environments and 12.41% on natural environments). Our code and data are available at https://github.com/YujiaBao/Predict-then-Interpolate.



### Edge Detection for Satellite Images without Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.12633v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12633v1)
- **Published**: 2021-05-26 15:47:42+00:00
- **Updated**: 2021-05-26 15:47:42+00:00
- **Authors**: Joshua Abraham, Calden Wloka
- **Comment**: None
- **Journal**: None
- **Summary**: Satellite imagery is widely used in many application sectors, including agriculture, navigation, and urban planning. Frequently, satellite imagery involves both large numbers of images as well as high pixel counts, making satellite datasets computationally expensive to analyze. Recent approaches to satellite image analysis have largely emphasized deep learning methods. Though extremely powerful, deep learning has some drawbacks, including the requirement of specialized computing hardware and a high reliance on training data. When dealing with large satellite datasets, the cost of both computational resources and training data annotation may be prohibitive.



### Blurs Behave Like Ensembles: Spatial Smoothings to Improve Accuracy, Uncertainty, and Robustness
- **Arxiv ID**: http://arxiv.org/abs/2105.12639v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2105.12639v4)
- **Published**: 2021-05-26 15:58:11+00:00
- **Updated**: 2022-07-14 15:27:38+00:00
- **Authors**: Namuk Park, Songkuk Kim
- **Comment**: ICML 2022
- **Journal**: None
- **Summary**: Neural network ensembles, such as Bayesian neural networks (BNNs), have shown success in the areas of uncertainty estimation and robustness. However, a crucial challenge prohibits their use in practice. BNNs require a large number of predictions to produce reliable results, leading to a significant increase in computational cost. To alleviate this issue, we propose spatial smoothing, a method that spatially ensembles neighboring feature map points of convolutional neural networks. By simply adding a few blur layers to the models, we empirically show that spatial smoothing improves accuracy, uncertainty estimation, and robustness of BNNs across a whole range of ensemble sizes. In particular, BNNs incorporating spatial smoothing achieve high predictive performance merely with a handful of ensembles. Moreover, this method also can be applied to canonical deterministic neural networks to improve the performances. A number of evidences suggest that the improvements can be attributed to the stabilized feature maps and the smoothing of the loss landscape. In addition, we provide a fundamental explanation for prior works - namely, global average pooling, pre-activation, and ReLU6 - by addressing them as special cases of spatial smoothing. These not only enhance accuracy, but also improve uncertainty estimation and robustness by making the loss landscape smoother in the same manner as spatial smoothing. The code is available at https://github.com/xxxnell/spatial-smoothing.



### Recent Standard Development Activities on Video Coding for Machines
- **Arxiv ID**: http://arxiv.org/abs/2105.12653v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12653v1)
- **Published**: 2021-05-26 16:11:11+00:00
- **Updated**: 2021-05-26 16:11:11+00:00
- **Authors**: Wen Gao, Shan Liu, Xiaozhong Xu, Manouchehr Rafie, Yuan Zhang, Igor Curcio
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: In recent years, video data has dominated internet traffic and becomes one of the major data formats. With the emerging 5G and internet of things (IoT) technologies, more and more videos are generated by edge devices, sent across networks, and consumed by machines. The volume of video consumed by machine is exceeding the volume of video consumed by humans. Machine vision tasks include object detection, segmentation, tracking, and other machine-based applications, which are quite different from those for human consumption. On the other hand, due to large volumes of video data, it is essential to compress video before transmission. Thus, efficient video coding for machines (VCM) has become an important topic in academia and industry. In July 2019, the international standardization organization, i.e., MPEG, created an Ad-Hoc group named VCM to study the requirements for potential standardization work. In this paper, we will address the recent development activities in the MPEG VCM group. Specifically, we will first provide an overview of the MPEG VCM group including use cases, requirements, processing pipelines, plan for potential VCM standards, followed by the evaluation framework including machine-vision tasks, dataset, evaluation metrics, and anchor generation. We then introduce technology solutions proposed so far and discuss the recent responses to the Call for Evidence issued by MPEG VCM group.



### Disentangled Face Attribute Editing via Instance-Aware Latent Space Search
- **Arxiv ID**: http://arxiv.org/abs/2105.12660v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12660v2)
- **Published**: 2021-05-26 16:19:08+00:00
- **Updated**: 2021-05-27 14:24:35+00:00
- **Authors**: Yuxuan Han, Jiaolong Yang, Ying Fu
- **Comment**: Accepted by IJCAI-2021
- **Journal**: None
- **Summary**: Recent works have shown that a rich set of semantic directions exist in the latent space of Generative Adversarial Networks (GANs), which enables various facial attribute editing applications. However, existing methods may suffer poor attribute variation disentanglement, leading to unwanted change of other attributes when altering the desired one. The semantic directions used by existing methods are at attribute level, which are difficult to model complex attribute correlations, especially in the presence of attribute distribution bias in GAN's training set. In this paper, we propose a novel framework (IALS) that performs Instance-Aware Latent-Space Search to find semantic directions for disentangled attribute editing. The instance information is injected by leveraging the supervision from a set of attribute classifiers evaluated on the input images. We further propose a Disentanglement-Transformation (DT) metric to quantify the attribute transformation and disentanglement efficacy and find the optimal control factor between attribute-level and instance-specific directions based on it. Experimental results on both GAN-generated and real-world images collectively show that our method outperforms state-of-the-art methods proposed recently by a wide margin. Code is available at https://github.com/yxuhan/IALS.



### Detecting Biological Locomotion in Video: A Computational Approach
- **Arxiv ID**: http://arxiv.org/abs/2105.12661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12661v1)
- **Published**: 2021-05-26 16:19:23+00:00
- **Updated**: 2021-05-26 16:19:23+00:00
- **Authors**: Soo Min Kang, Richard P. Wildes
- **Comment**: None
- **Journal**: None
- **Summary**: Animals locomote for various reasons: to search for food, find suitable habitat, pursue prey, escape from predators, or seek a mate. The grand scale of biodiversity contributes to the great locomotory design and mode diversity. Various creatures make use of legs, wings, fins and other means to move through the world. In this report, we refer to the locomotion of general biological species as biolocomotion. We present a computational approach to detect biolocomotion in unprocessed video.   Significantly, the motion exhibited by the body parts of a biological entity to navigate through an environment can be modeled by a combination of an overall positional advance with an overlaid asymmetric oscillatory pattern, a distinctive signature that tends to be absent in non-biological objects in locomotion. We exploit this key trait of positional advance with asymmetric oscillation along with differences in an object's common motion (extrinsic motion) and localized motion of its parts (intrinsic motion) to detect biolocomotion. An algorithm is developed to measure the presence of these traits in tracked objects to determine if they correspond to a biological entity in locomotion. An alternative algorithm, based on generic features combined with learning is assembled out of components from allied areas of investigation, also is presented as a basis of comparison.   A novel biolocomotion dataset encompassing a wide range of moving biological and non-biological objects in natural settings is provided. Also, biolocomotion annotations to an extant camouflage animals dataset are provided. Quantitative results indicate that the proposed algorithm considerably outperforms the alternative approach, supporting the hypothesis that biolocomotion can be detected reliably based on its distinct signature of positional advance with asymmetric oscillation and extrinsic/intrinsic motion dissimilarity.



### CogView: Mastering Text-to-Image Generation via Transformers
- **Arxiv ID**: http://arxiv.org/abs/2105.13290v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.13290v3)
- **Published**: 2021-05-26 16:52:53+00:00
- **Updated**: 2021-11-05 06:10:15+00:00
- **Authors**: Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, Jie Tang
- **Comment**: to appear in NeurIPS 2021
- **Journal**: None
- **Summary**: Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E.



### Low Resolution Information Also Matters: Learning Multi-Resolution Representations for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2105.12684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12684v1)
- **Published**: 2021-05-26 16:54:56+00:00
- **Updated**: 2021-05-26 16:54:56+00:00
- **Authors**: Guoqing Zhang, Yuhao Chen, Weisi Lin, Arun Chandran, Xuan Jing
- **Comment**: accepted by IJCAI 2021
- **Journal**: None
- **Summary**: As a prevailing task in video surveillance and forensics field, person re-identification (re-ID) aims to match person images captured from non-overlapped cameras. In unconstrained scenarios, person images often suffer from the resolution mismatch problem, i.e., \emph{Cross-Resolution Person Re-ID}. To overcome this problem, most existing methods restore low resolution (LR) images to high resolution (HR) by super-resolution (SR). However, they only focus on the HR feature extraction and ignore the valid information from original LR images. In this work, we explore the influence of resolutions on feature extraction and develop a novel method for cross-resolution person re-ID called \emph{\textbf{M}ulti-Resolution \textbf{R}epresentations \textbf{J}oint \textbf{L}earning} (\textbf{MRJL}). Our method consists of a Resolution Reconstruction Network (RRN) and a Dual Feature Fusion Network (DFFN). The RRN uses an input image to construct a HR version and a LR version with an encoder and two decoders, while the DFFN adopts a dual-branch structure to generate person representations from multi-resolution images. Comprehensive experiments on five benchmarks verify the superiority of the proposed MRJL over the relevent state-of-the-art methods.



### Dynamic Probabilistic Pruning: A general framework for hardware-constrained pruning at different granularities
- **Arxiv ID**: http://arxiv.org/abs/2105.12686v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.12686v1)
- **Published**: 2021-05-26 17:01:52+00:00
- **Updated**: 2021-05-26 17:01:52+00:00
- **Authors**: Lizeth Gonzalez-Carabarin, Iris A. M. Huijben, Bastiaan S. Veeling, Alexandre Schmid, Ruud J. G. van Sloun
- **Comment**: None
- **Journal**: None
- **Summary**: Unstructured neural network pruning algorithms have achieved impressive compression rates. However, the resulting - typically irregular - sparse matrices hamper efficient hardware implementations, leading to additional memory usage and complex control logic that diminishes the benefits of unstructured pruning. This has spurred structured coarse-grained pruning solutions that prune entire filters or even layers, enabling efficient implementation at the expense of reduced flexibility. Here we propose a flexible new pruning mechanism that facilitates pruning at different granularities (weights, kernels, filters/feature maps), while retaining efficient memory organization (e.g. pruning exactly k-out-of-n weights for every output neuron, or pruning exactly k-out-of-n kernels for every feature map). We refer to this algorithm as Dynamic Probabilistic Pruning (DPP). DPP leverages the Gumbel-softmax relaxation for differentiable k-out-of-n sampling, facilitating end-to-end optimization. We show that DPP achieves competitive compression rates and classification accuracy when pruning common deep learning models trained on different benchmark datasets for image classification. Relevantly, the non-magnitude-based nature of DPP allows for joint optimization of pruning and weight quantization in order to even further compress the network, which we show as well. Finally, we propose novel information theoretic metrics that show the confidence and pruning diversity of pruning masks within a layer.



### On the Advantages of Multiple Stereo Vision Camera Designs for Autonomous Drone Navigation
- **Arxiv ID**: http://arxiv.org/abs/2105.12691v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.12691v1)
- **Published**: 2021-05-26 17:10:20+00:00
- **Updated**: 2021-05-26 17:10:20+00:00
- **Authors**: Rui Pimentel de Figueiredo, Jakob Grimm Hansen, Jonas Le Fevre, Martim Brandão, Erdal Kayacan
- **Comment**: None
- **Journal**: in ICRA workshop on Resilient and Long-Term Autonomy for Aerial
  Robotic Systems, 2021
- **Summary**: In this work we showcase the design and assessment of the performance of a multi-camera UAV, when coupled with state-of-the-art planning and mapping algorithms for autonomous navigation. The system leverages state-of-the-art receding horizon exploration techniques for Next-Best-View (NBV) planning with 3D and semantic information, provided by a reconfigurable multi stereo camera system. We employ our approaches in an autonomous drone-based inspection task and evaluate them in an autonomous exploration and mapping scenario. We discuss the advantages and limitations of using multi stereo camera flying systems, and the trade-off between number of cameras and mapping performance.



### Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2105.12694v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12694v1)
- **Published**: 2021-05-26 17:15:53+00:00
- **Updated**: 2021-05-26 17:15:53+00:00
- **Authors**: Feifei Shao, Long Chen, Jian Shao, Wei Ji, Shaoning Xiao, Lu Ye, Yueting Zhuang, Jun Xiao
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: Weakly-Supervised Object Detection (WSOD) and Localization (WSOL), i.e., detecting multiple and single instances with bounding boxes in an image using image-level labels, are long-standing and challenging tasks in the CV community. With the success of deep neural networks in object detection, both WSOD and WSOL have received unprecedented attention. Hundreds of WSOD and WSOL methods and numerous techniques have been proposed in the deep learning era. To this end, in this paper, we consider WSOL is a sub-task of WSOD and provide a comprehensive survey of the recent achievements of WSOD. Specifically, we firstly describe the formulation and setting of the WSOD, including the background, challenges, basic framework. Meanwhile, we summarize and analyze all advanced techniques and training tricks for improving detection performance. Then, we introduce the widely-used datasets and evaluation metrics of WSOD. Lastly, we discuss the future directions of WSOD. We believe that these summaries can help pave a way for future research on WSOD and WSOL.



### Towards Transparent Application of Machine Learning in Video Processing
- **Arxiv ID**: http://arxiv.org/abs/2105.12700v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2105.12700v2)
- **Published**: 2021-05-26 17:24:23+00:00
- **Updated**: 2021-05-27 09:35:54+00:00
- **Authors**: Luka Murn, Marc Gorriz Blanch, Maria Santamaria, Fiona Rivera, Marta Mrak
- **Comment**: International Broadcasting Convention, 11-14 Sep 2020, Amsterdam,
  Netherlands (Technical Paper section, Virtual)
- **Journal**: None
- **Summary**: Machine learning techniques for more efficient video compression and video enhancement have been developed thanks to breakthroughs in deep learning. The new techniques, considered as an advanced form of Artificial Intelligence (AI), bring previously unforeseen capabilities. However, they typically come in the form of resource-hungry black-boxes (overly complex with little transparency regarding the inner workings). Their application can therefore be unpredictable and generally unreliable for large-scale use (e.g. in live broadcast). The aim of this work is to understand and optimise learned models in video processing applications so systems that incorporate them can be used in a more trustworthy manner. In this context, the presented work introduces principles for simplification of learned models targeting improved transparency in implementing machine learning for video production and distribution applications. These principles are demonstrated on video compression examples, showing how bitrate savings and reduced complexity can be achieved by simplifying relevant deep learning models.



### Enhance to Read Better: A Multi-Task Adversarial Network for Handwritten Document Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2105.12710v2
- **DOI**: 10.1016/j.patcog.2021.108370
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12710v2)
- **Published**: 2021-05-26 17:44:45+00:00
- **Updated**: 2021-10-22 15:30:46+00:00
- **Authors**: Sana Khamekhem Jemni, Mohamed Ali Souibgui, Yousri Kessentini, Alicia Fornés
- **Comment**: Accepted in Pattern Recognition
- **Journal**: None
- **Summary**: Handwritten document images can be highly affected by degradation for different reasons: Paper ageing, daily-life scenarios (wrinkles, dust, etc.), bad scanning process and so on. These artifacts raise many readability issues for current Handwritten Text Recognition (HTR) algorithms and severely devalue their efficiency. In this paper, we propose an end to end architecture based on Generative Adversarial Networks (GANs) to recover the degraded documents into a clean and readable form. Unlike the most well-known document binarization methods, which try to improve the visual quality of the degraded document, the proposed architecture integrates a handwritten text recognizer that promotes the generated document image to be more readable. To the best of our knowledge, this is the first work to use the text information while binarizing handwritten documents. Extensive experiments conducted on degraded Arabic and Latin handwritten documents demonstrate the usefulness of integrating the recognizer within the GAN architecture, which improves both the visual quality and the readability of the degraded document images. Moreover, we outperform the state of the art in H-DIBCO challenges, after fine tuning our pre-trained model with synthetically degraded Latin handwritten images, on this task.



### Spatio-Contextual Deep Network Based Multimodal Pedestrian Detection For Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2105.12713v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12713v3)
- **Published**: 2021-05-26 17:50:36+00:00
- **Updated**: 2022-01-24 12:16:04+00:00
- **Authors**: Kinjal Dasgupta, Arindam Das, Sudip Das, Ujjwal Bhattacharya, Senthil Yogamani
- **Comment**: To be published at IEEE Transactions on Intelligent Transportation
  Systems
- **Journal**: None
- **Summary**: Pedestrian Detection is the most critical module of an Autonomous Driving system. Although a camera is commonly used for this purpose, its quality degrades severely in low-light night time driving scenarios. On the other hand, the quality of a thermal camera image remains unaffected in similar conditions. This paper proposes an end-to-end multimodal fusion model for pedestrian detection using RGB and thermal images. Its novel spatio-contextual deep network architecture is capable of exploiting the multimodal input efficiently. It consists of two distinct deformable ResNeXt-50 encoders for feature extraction from the two modalities. Fusion of these two encoded features takes place inside a multimodal feature embedding module (MuFEm) consisting of several groups of a pair of Graph Attention Network and a feature fusion unit. The output of the last feature fusion unit of MuFEm is subsequently passed to two CRFs for their spatial refinement. Further enhancement of the features is achieved by applying channel-wise attention and extraction of contextual information with the help of four RNNs traversing in four different directions. Finally, these feature maps are used by a single-stage decoder to generate the bounding box of each pedestrian and the score map. We have performed extensive experiments of the proposed framework on three publicly available multimodal pedestrian detection benchmark datasets, namely KAIST, CVC-14, and UTokyo. The results on each of them improved the respective state-of-the-art performance. A short video giving an overview of this work along with its qualitative results can be seen at https://youtu.be/FDJdSifuuCs. Our source code will be released upon publication of the paper.



### Sli2Vol: Annotate a 3D Volume from a Single Slice with Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.12722v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.12722v2)
- **Published**: 2021-05-26 17:56:39+00:00
- **Updated**: 2021-07-08 18:28:19+00:00
- **Authors**: Pak-Hei Yeung, Ana I. L. Namburete, Weidi Xie
- **Comment**: International Conference on Medical Image Computing and Computer
  Assisted Intervention (MICCAI) 2021
- **Journal**: None
- **Summary**: The objective of this work is to segment any arbitrary structures of interest (SOI) in 3D volumes by only annotating a single slice, (i.e. semi-automatic 3D segmentation). We show that high accuracy can be achieved by simply propagating the 2D slice segmentation with an affinity matrix between consecutive slices, which can be learnt in a self-supervised manner, namely slice reconstruction. Specifically, we compare the proposed framework, termed as Sli2Vol, with supervised approaches and two other unsupervised/ self-supervised slice registration approaches, on 8 public datasets (both CT and MRI scans), spanning 9 different SOIs. Without any parameter-tuning, the same model achieves superior performance with Dice scores (0-100 scale) of over 80 for most of the benchmarks, including the ones that are unseen during training. Our results show generalizability of the proposed approach across data from different machines and with different SOIs: a major use case of semi-automatic segmentation methods where fully supervised approaches would normally struggle. The source code will be made publicly available at https://github.com/pakheiyeung/Sli2Vol.



### Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding
- **Arxiv ID**: http://arxiv.org/abs/2105.12723v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12723v4)
- **Published**: 2021-05-26 17:56:48+00:00
- **Updated**: 2021-12-30 17:37:57+00:00
- **Authors**: Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Sercan O. Arik, Tomas Pfister
- **Comment**: AAAI2022
- **Journal**: None
- **Summary**: Hierarchical structures are popular in recent vision transformers, however, they require sophisticated designs and massive datasets to work well. In this paper, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical way. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture that requires minor code changes upon the original vision transformer. The benefits of the proposed judiciously-selected design are threefold: (1) NesT converges faster and requires much less training data to achieve good generalization on both ImageNet and small datasets like CIFAR; (2) when extending our key ideas to image generation, NesT leads to a strong decoder that is 8$\times$ faster than previous transformer-based generators; and (3) we show that decoupling the feature learning and abstraction processes via this nested hierarchy in our design enables constructing a novel method (named GradCAT) for visually interpreting the learned model. Source code is available https://github.com/google-research/nested-transformer.



### Smile Like You Mean It: Driving Animatronic Robotic Face with Learned Models
- **Arxiv ID**: http://arxiv.org/abs/2105.12724v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.12724v1)
- **Published**: 2021-05-26 17:57:19+00:00
- **Updated**: 2021-05-26 17:57:19+00:00
- **Authors**: Boyuan Chen, Yuhang Hu, Lianfeng Li, Sara Cummings, Hod Lipson
- **Comment**: ICRA 2021. Website:http://www.cs.columbia.edu/~bchen/aiface/
- **Journal**: None
- **Summary**: Ability to generate intelligent and generalizable facial expressions is essential for building human-like social robots. At present, progress in this field is hindered by the fact that each facial expression needs to be programmed by humans. In order to adapt robot behavior in real time to different situations that arise when interacting with human subjects, robots need to be able to train themselves without requiring human labels, as well as make fast action decisions and generalize the acquired knowledge to diverse and new contexts. We addressed this challenge by designing a physical animatronic robotic face with soft skin and by developing a vision-based self-supervised learning framework for facial mimicry. Our algorithm does not require any knowledge of the robot's kinematic model, camera calibration or predefined expression set. By decomposing the learning process into a generative model and an inverse model, our framework can be trained using a single motor babbling dataset. Comprehensive evaluations show that our method enables accurate and diverse face mimicry across diverse human subjects. The project website is at http://www.cs.columbia.edu/~bchen/aiface/



### Computer Vision and Conflicting Values: Describing People with Automated Alt Text
- **Arxiv ID**: http://arxiv.org/abs/2105.12754v1
- **DOI**: 10.1145/3461702.3462620
- **Categories**: **cs.CY**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.12754v1)
- **Published**: 2021-05-26 18:01:16+00:00
- **Updated**: 2021-05-26 18:01:16+00:00
- **Authors**: Margot Hanley, Solon Barocas, Karen Levy, Shiri Azenkot, Helen Nissenbaum
- **Comment**: None
- **Journal**: Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and
  Society (AIES '21)
- **Summary**: Scholars have recently drawn attention to a range of controversial issues posed by the use of computer vision for automatically generating descriptions of people in images. Despite these concerns, automated image description has become an important tool to ensure equitable access to information for blind and low vision people. In this paper, we investigate the ethical dilemmas faced by companies that have adopted the use of computer vision for producing alt text: textual descriptions of images for blind and low vision people, We use Facebook's automatic alt text tool as our primary case study. First, we analyze the policies that Facebook has adopted with respect to identity categories, such as race, gender, age, etc., and the company's decisions about whether to present these terms in alt text. We then describe an alternative -- and manual -- approach practiced in the museum community, focusing on how museums determine what to include in alt text descriptions of cultural artifacts. We compare these policies, using notable points of contrast to develop an analytic framework that characterizes the particular apprehensions behind these policy choices. We conclude by considering two strategies that seem to sidestep some of these concerns, finding that there are no easy ways to avoid the normative dilemmas posed by the use of computer vision to automate alt text.



### An Online Learning System for Wireless Charging Alignment using Surround-view Fisheye Cameras
- **Arxiv ID**: http://arxiv.org/abs/2105.12763v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12763v5)
- **Published**: 2021-05-26 18:02:59+00:00
- **Updated**: 2022-12-21 22:50:02+00:00
- **Authors**: Ashok Dahal, Varun Ravi Kumar, Senthil Yogamani, Ciaran Eising
- **Comment**: Accepted for publication at IEEE Transactions on Intelligent
  Transportation Systems. Chargepad Dataset is shared at
  https://drive.google.com/drive/folders/1KeLFIqOnhU2CGsD0vbiN9UqKmBSyHERd
- **Journal**: None
- **Summary**: Electric Vehicles are increasingly common, with inductive chargepads being considered a convenient and efficient means of charging electric vehicles. However, drivers are typically poor at aligning the vehicle to the necessary accuracy for efficient inductive charging, making the automated alignment of the two charging plates desirable. In parallel to the electrification of the vehicular fleet, automated parking systems that make use of surround-view camera systems are becoming increasingly popular. In this work, we propose a system based on the surround-view camera architecture to detect, localize, and automatically align the vehicle with the inductive chargepad. The visual design of the chargepads is not standardized and not necessarily known beforehand. Therefore, a system that relies on offline training will fail in some situations. Thus, we propose a self-supervised online learning method that leverages the driver's actions when manually aligning the vehicle with the chargepad and combine it with weak supervision from semantic segmentation and depth to learn a classifier to auto-annotate the chargepad in the video for further training. In this way, when faced with a previously unseen chargepad, the driver needs only manually align the vehicle a single time. As the chargepad is flat on the ground, it is not easy to detect it from a distance. Thus, we propose using a Visual SLAM pipeline to learn landmarks relative to the chargepad to enable alignment from a greater range. We demonstrate the working system on an automated vehicle as illustrated in the video at https://youtu.be/_cLCmkW4UYo. To encourage further research, we will share a chargepad dataset used in this work.



### DSLR: Dynamic to Static LiDAR Scan Reconstruction Using Adversarially Trained Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2105.12774v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.12774v1)
- **Published**: 2021-05-26 18:19:21+00:00
- **Updated**: 2021-05-26 18:19:21+00:00
- **Authors**: Prashant Kumar, Sabyasachi Sahoo, Vanshil Shah, Vineetha Kondameedi, Abhinav Jain, Akshaj Verma, Chiranjib Bhattacharyya, Vinay Viswanathan
- **Comment**: 17 pages, 15 figures, Accepted at AAAI 2021
- **Journal**: None
- **Summary**: Accurate reconstruction of static environments from LiDAR scans of scenes containing dynamic objects, which we refer to as Dynamic to Static Translation (DST), is an important area of research in Autonomous Navigation. This problem has been recently explored for visual SLAM, but to the best of our knowledge no work has been attempted to address DST for LiDAR scans. The problem is of critical importance due to wide-spread adoption of LiDAR in Autonomous Vehicles. We show that state-of the art methods developed for the visual domain when adapted for LiDAR scans perform poorly.   We develop DSLR, a deep generative model which learns a mapping between dynamic scan to its static counterpart through an adversarially trained autoencoder. Our model yields the first solution for DST on LiDAR that generates static scans without using explicit segmentation labels. DSLR cannot always be applied to real world data due to lack of paired dynamic-static scans. Using Unsupervised Domain Adaptation, we propose DSLR-UDA for transfer to real world data and experimentally show that this performs well in real world settings. Additionally, if segmentation information is available, we extend DSLR to DSLR-Seg to further improve the reconstruction quality.   DSLR gives the state of the art performance on simulated and real-world datasets and also shows at least 4x improvement. We show that DSLR, unlike the existing baselines, is a practically viable model with its reconstruction quality within the tolerable limits for tasks pertaining to autonomous navigation like SLAM in dynamic environments.



### cofga: A Dataset for Fine Grained Classification of Objects from Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/2105.12786v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12786v1)
- **Published**: 2021-05-26 18:39:47+00:00
- **Updated**: 2021-05-26 18:39:47+00:00
- **Authors**: Eran Dahan, Tzvi Diskin, Amit Amram, Amit Moryossef, Omer Koren
- **Comment**: None
- **Journal**: None
- **Summary**: Detection and classification of objects in overhead images are two important and challenging problems in computer vision. Among various research areas in this domain, the task of fine-grained classification of objects in overhead images has become ubiquitous in diverse real-world applications, due to recent advances in high-resolution satellite and airborne imaging systems. The small inter-class variations and the large intra class variations caused by the fine grained nature make it a challenging task, especially in low-resource cases. In this paper, we introduce COFGA a new open dataset for the advancement of fine-grained classification research. The 2,104 images in the dataset are collected from an airborne imaging system at 5 15 cm ground sampling distance, providing higher spatial resolution than most public overhead imagery datasets. The 14,256 annotated objects in the dataset were classified into 2 classes, 15 subclasses, 14 unique features, and 8 perceived colors a total of 37 distinct labels making it suitable to the task of fine-grained classification more than any other publicly available overhead imagery dataset. We compare COFGA to other overhead imagery datasets and then describe some distinguished fine-grain classification approaches that were explored during an open data-science competition we have conducted for this task.



### RSCA: Real-time Segmentation-based Context-Aware Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.12789v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12789v1)
- **Published**: 2021-05-26 18:43:17+00:00
- **Updated**: 2021-05-26 18:43:17+00:00
- **Authors**: Jiachen Li, Yuan Lin, Rongrong Liu, Chiu Man Ho, Humphrey Shi
- **Comment**: CVPR 2021 Workshop
- **Journal**: None
- **Summary**: Segmentation-based scene text detection methods have been widely adopted for arbitrary-shaped text detection recently, since they make accurate pixel-level predictions on curved text instances and can facilitate real-time inference without time-consuming processing on anchors. However, current segmentation-based models are unable to learn the shapes of curved texts and often require complex label assignments or repeated feature aggregations for more accurate detection. In this paper, we propose RSCA: a Real-time Segmentation-based Context-Aware model for arbitrary-shaped scene text detection, which sets a strong baseline for scene text detection with two simple yet effective strategies: Local Context-Aware Upsampling and Dynamic Text-Spine Labeling, which model local spatial transformation and simplify label assignments separately. Based on these strategies, RSCA achieves state-of-the-art performance in both speed and accuracy, without complex label assignments or repeated feature aggregations. We conduct extensive experiments on multiple benchmarks to validate the effectiveness of our method. RSCA-640 reaches 83.9% F-measure at 48.3 FPS on CTW1500 dataset.



### DFPN: Deformable Frame Prediction Network
- **Arxiv ID**: http://arxiv.org/abs/2105.12794v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.12794v1)
- **Published**: 2021-05-26 19:00:19+00:00
- **Updated**: 2021-05-26 19:00:19+00:00
- **Authors**: M. Akın Yılmaz, A. Murat Tekalp
- **Comment**: Accepted for publication in IEEE International Conference on Image
  Processing (ICIP) 2021
- **Journal**: None
- **Summary**: Learned frame prediction is a current problem of interest in computer vision and video compression. Although several deep network architectures have been proposed for learned frame prediction, to the best of our knowledge, there is no work based on using deformable convolutions for frame prediction. To this effect, we propose a deformable frame prediction network (DFPN) for task oriented implicit motion modeling and next frame prediction. Experimental results demonstrate that the proposed DFPN model achieves state of the art results in next frame prediction. Our models and results are available at https://github.com/makinyilmaz/DFPN.



### ViPTT-Net: Video pretraining of spatio-temporal model for tuberculosis type classification from chest CT scans
- **Arxiv ID**: http://arxiv.org/abs/2105.12810v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.12810v1)
- **Published**: 2021-05-26 20:00:31+00:00
- **Updated**: 2021-05-26 20:00:31+00:00
- **Authors**: Hasib Zunair, Aimon Rahman, Nabeel Mohammed
- **Comment**: Under review at CLEF 2021. 10 pages
- **Journal**: None
- **Summary**: Pretraining has sparked groundswell of interest in deep learning workflows to learn from limited data and improve generalization. While this is common for 2D image classification tasks, its application to 3D medical imaging tasks like chest CT interpretation is limited. We explore the idea of whether pretraining a model on realistic videos could improve performance rather than training the model from scratch, intended for tuberculosis type classification from chest CT scans. To incorporate both spatial and temporal features, we develop a hybrid convolutional neural network (CNN) and recurrent neural network (RNN) model, where the features are extracted from each axial slice of the CT scan by a CNN, these sequence of image features are input to a RNN for classification of the CT scan. Our model termed as ViPTT-Net, was trained on over 1300 video clips with labels of human activities, and then fine-tuned on chest CT scans with labels of tuberculosis type. We find that pretraining the model on videos lead to better representations and significantly improved model validation performance from a kappa score of 0.17 to 0.35, especially for under-represented class samples. Our best method achieved 2nd place in the ImageCLEF 2021 Tuberculosis - TBT classification task with a kappa score of 0.20 on the final test set with only image information (without using clinical meta-data). All codes and models are made available.



### Issues in Object Detection in Videos using Common Single-Image CNNs
- **Arxiv ID**: http://arxiv.org/abs/2105.12822v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.12822v1)
- **Published**: 2021-05-26 20:33:51+00:00
- **Updated**: 2021-05-26 20:33:51+00:00
- **Authors**: Spencer Ploeger, Lucas Dasovic
- **Comment**: 5 pages, 6 figures, supplementary material at:
  https://drive.google.com/drive/folders/1qXZd8nObw84jSYklAFjNnH6djVymr65N?usp=sharing
- **Journal**: None
- **Summary**: A growing branch of computer vision is object detection. Object detection is used in many applications such as industrial process, medical imaging analysis, and autonomous vehicles. The ability to detect objects in videos is crucial. Object detection systems are trained on large image datasets. For applications such as autonomous vehicles, it is crucial that the object detection system can identify objects through multiple frames in video. There are many problems with applying these systems to video. Shadows or changes in brightness that can cause the system to incorrectly identify objects frame to frame and cause an unintended system response. There are many neural networks that have been used for object detection and if there was a way of connecting objects between frames then these problems could be eliminated. For these neural networks to get better at identifying objects in video, they need to be re-trained. A dataset must be created with images that represent consecutive video frames and have matching ground-truth layers. A method is proposed that can generate these datasets. The ground-truth layer contains only moving objects. To generate this layer, FlowNet2-Pytorch was used to create the flow mask using the novel Magnitude Method. As well, a segmentation mask will be generated using networks such as Mask R-CNN or Refinenet. These segmentation masks will contain all objects detected in a frame. By comparing this segmentation mask to the flow mask ground-truth layer, a loss function is generated. This loss function can be used to train a neural network to be better at making consistent predictions on video. The system was tested on multiple video samples and a loss was generated for each frame, proving the Magnitude Method's ability to be used to train object detection neural networks in future work.



### Multi-Modal Semantic Inconsistency Detection in Social Media News Posts
- **Arxiv ID**: http://arxiv.org/abs/2105.12855v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12855v1)
- **Published**: 2021-05-26 21:25:27+00:00
- **Updated**: 2021-05-26 21:25:27+00:00
- **Authors**: Scott McCrae, Kehan Wang, Avideh Zakhor
- **Comment**: None
- **Journal**: None
- **Summary**: As computer-generated content and deepfakes make steady improvements, semantic approaches to multimedia forensics will become more important. In this paper, we introduce a novel classification architecture for identifying semantic inconsistencies between video appearance and text caption in social media news posts. We develop a multi-modal fusion framework to identify mismatches between videos and captions in social media posts by leveraging an ensemble method based on textual analysis of the caption, automatic audio transcription, semantic video analysis, object detection, named entity consistency, and facial verification. To train and test our approach, we curate a new video-based dataset of 4,000 real-world Facebook news posts for analysis. Our multi-modal approach achieves 60.5% classification accuracy on random mismatches between caption and appearance, compared to accuracy below 50% for uni-modal models. Further ablation studies confirm the necessity of fusion across modalities for correctly identifying semantic inconsistencies.



### Benchmarking Scientific Image Forgery Detectors
- **Arxiv ID**: http://arxiv.org/abs/2105.12872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.12872v1)
- **Published**: 2021-05-26 22:58:20+00:00
- **Updated**: 2021-05-26 22:58:20+00:00
- **Authors**: João P. Cardenuto, Anderson Rocha
- **Comment**: None
- **Journal**: None
- **Summary**: The scientific image integrity area presents a challenging research bottleneck, the lack of available datasets to design and evaluate forensic techniques. Its data sensitivity creates a legal hurdle that prevents one to rely on real tampered cases to build any sort of accessible forensic benchmark. To mitigate this bottleneck, we present an extendable open-source library that reproduces the most common image forgery operations reported by the research integrity community: duplication, retouching, and cleaning. Using this library and realistic scientific images, we create a large scientific forgery image benchmark (39,423 images) with an enriched ground-truth. In addition, concerned about the high number of retracted papers due to image duplication, this work evaluates the state-of-the-art copy-move detection methods in the proposed dataset, using a new metric that asserts consistent match detection between the source and the copied region. The dataset and source-code will be freely available upon acceptance of the paper.



