# Arxiv Papers in cs.CV on 2021-05-14
### TriPose: A Weakly-Supervised 3D Human Pose Estimation via Triangulation from Video
- **Arxiv ID**: http://arxiv.org/abs/2105.06599v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06599v1)
- **Published**: 2021-05-14 00:46:48+00:00
- **Updated**: 2021-05-14 00:46:48+00:00
- **Authors**: Mohsen Gholami, Ahmad Rezaei, Helge Rhodin, Rabab Ward, Z. Jane Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating 3D human poses from video is a challenging problem. The lack of 3D human pose annotations is a major obstacle for supervised training and for generalization to unseen datasets. In this work, we address this problem by proposing a weakly-supervised training scheme that does not require 3D annotations or calibrated cameras. The proposed method relies on temporal information and triangulation. Using 2D poses from multiple views as the input, we first estimate the relative camera orientations and then generate 3D poses via triangulation. The triangulation is only applied to the views with high 2D human joint confidence. The generated 3D poses are then used to train a recurrent lifting network (RLN) that estimates 3D poses from 2D poses. We further apply a multi-view re-projection loss to the estimated 3D poses and enforce the 3D poses estimated from multi-views to be consistent. Therefore, our method relaxes the constraints in practice, only multi-view videos are required for training, and is thus convenient for in-the-wild settings. At inference, RLN merely requires single-view videos. The proposed method outperforms previous works on two challenging datasets, Human3.6M and MPI-INF-3DHP. Codes and pretrained models will be publicly available.



### Meta Auxiliary Learning for Facial Action Unit Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.06620v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06620v1)
- **Published**: 2021-05-14 02:28:40+00:00
- **Updated**: 2021-05-14 02:28:40+00:00
- **Authors**: Yong Li, Shiguang Shan
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the success of deep neural networks on facial action unit (AU) detection, better performance depends on a large number of training images with accurate AU annotations. However, labeling AU is time-consuming, expensive, and error-prone. Considering AU detection and facial expression recognition (FER) are two highly correlated tasks, and facial expression (FE) is relatively easy to annotate, we consider learning AU detection and FER in a multi-task manner. However, the performance of the AU detection task cannot be always enhanced due to the negative transfer in the multi-task scenario. To alleviate this issue, we propose a Meta Auxiliary Learning method (MAL) that automatically selects highly related FE samples by learning adaptative weights for the training FE samples in a meta learning manner. The learned sample weights alleviate the negative transfer from two aspects: 1) balance the loss of each task automatically, and 2) suppress the weights of FE samples that have large uncertainties. Experimental results on several popular AU datasets demonstrate MAL consistently improves the AU detection performance compared with the state-of-the-art multi-task and auxiliary learning methods. MAL automatically estimates adaptive weights for the auxiliary FE samples according to their semantic relevance with the primary AU detection task.



### City-Scale Multi-Camera Vehicle Tracking Guided by Crossroad Zones
- **Arxiv ID**: http://arxiv.org/abs/2105.06623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06623v1)
- **Published**: 2021-05-14 03:01:17+00:00
- **Updated**: 2021-05-14 03:01:17+00:00
- **Authors**: Chong Liu, Yuqi Zhang, Hao Luo, Jiasheng Tang, Weihua Chen, Xianzhe Xu, Fan Wang, Hao Li, Yi-Dong Shen
- **Comment**: CVPR 2021 AI CITY CHALLENGE City-Scale Multi-Camera Vehicle Tracking
  Top 1
- **Journal**: None
- **Summary**: Multi-Target Multi-Camera Tracking has a wide range of applications and is the basis for many advanced inferences and predictions. This paper describes our solution to the Track 3 multi-camera vehicle tracking task in 2021 AI City Challenge (AICITY21). This paper proposes a multi-target multi-camera vehicle tracking framework guided by the crossroad zones. The framework includes: (1) Use mature detection and vehicle re-identification models to extract targets and appearance features. (2) Use modified JDETracker (without detection module) to track single-camera vehicles and generate single-camera tracklets. (3) According to the characteristics of the crossroad, the Tracklet Filter Strategy and the Direction Based Temporal Mask are proposed. (4) Propose Sub-clustering in Adjacent Cameras for multi-camera tracklets matching. Through the above techniques, our method obtained an IDF1 score of 0.8095, ranking first on the leaderboard. The code have released: https://github.com/LCFractal/AIC21-MTMC.



### Biometrics: Trust, but Verify
- **Arxiv ID**: http://arxiv.org/abs/2105.06625v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06625v2)
- **Published**: 2021-05-14 03:07:25+00:00
- **Updated**: 2021-05-31 16:02:59+00:00
- **Authors**: Anil K. Jain, Debayan Deb, Joshua J. Engelsma
- **Comment**: 20 pages, 15 figures
- **Journal**: None
- **Summary**: Over the past two decades, biometric recognition has exploded into a plethora of different applications around the globe. This proliferation can be attributed to the high levels of authentication accuracy and user convenience that biometric recognition systems afford end-users. However, in-spite of the success of biometric recognition systems, there are a number of outstanding problems and concerns pertaining to the various sub-modules of biometric recognition systems that create an element of mistrust in their use - both by the scientific community and also the public at large. Some of these problems include: i) questions related to system recognition performance, ii) security (spoof attacks, adversarial attacks, template reconstruction attacks and demographic information leakage), iii) uncertainty over the bias and fairness of the systems to all users, iv) explainability of the seemingly black-box decisions made by most recognition systems, and v) concerns over data centralization and user privacy. In this paper, we provide an overview of each of the aforementioned open-ended challenges. We survey work that has been conducted to address each of these concerns and highlight the issues requiring further attention. Finally, we provide insights into how the biometric community can address core biometric recognition systems design issues to better instill trust, fairness, and security for all.



### COVID-Net CXR-2: An Enhanced Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2105.06640v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.06640v1)
- **Published**: 2021-05-14 04:29:21+00:00
- **Updated**: 2021-05-14 04:29:21+00:00
- **Authors**: Maya Pavlova, Naomi Terhljan, Audrey G. Chung, Andy Zhao, Siddharth Surana, Hossein Aboutalebi, Hayden Gunraj, Ali Sabri, Amer Alaref, Alexander Wong
- **Comment**: 12 pages. arXiv admin note: text overlap with arXiv:2105.00256
- **Journal**: None
- **Summary**: As the COVID-19 pandemic continues to devastate globally, the use of chest X-ray (CXR) imaging as a complimentary screening strategy to RT-PCR testing continues to grow given its routine clinical use for respiratory complaint. As part of the COVID-Net open source initiative, we introduce COVID-Net CXR-2, an enhanced deep convolutional neural network design for COVID-19 detection from CXR images built using a greater quantity and diversity of patients than the original COVID-Net. To facilitate this, we also introduce a new benchmark dataset composed of 19,203 CXR images from a multinational cohort of 16,656 patients from at least 51 countries, making it the largest, most diverse COVID-19 CXR dataset in open access form. The COVID-Net CXR-2 network achieves sensitivity and positive predictive value of 95.5%/97.0%, respectively, and was audited in a transparent and responsible manner. Explainability-driven performance validation was used during auditing to gain deeper insights in its decision-making behaviour and to ensure clinically relevant factors are leveraged for improving trust in its usage. Radiologist validation was also conducted, where select cases were reviewed and reported on by two board-certified radiologists with over 10 and 19 years of experience, respectively, and showed that the critical factors leveraged by COVID-Net CXR-2 are consistent with radiologist interpretations. While not a production-ready solution, we hope the open-source, open-access release of COVID-Net CXR-2 and the respective CXR benchmark dataset will encourage researchers, clinical scientists, and citizen scientists to accelerate advancements and innovations in the fight against the pandemic.



### Empirical Analysis of Image Caption Generation using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.09906v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.09906v2)
- **Published**: 2021-05-14 05:38:13+00:00
- **Updated**: 2021-05-22 15:17:21+00:00
- **Authors**: Aditya Bhattacharya, Eshwar Shamanna Girishekar, Padmakar Anil Deshpande
- **Comment**: Withdrawing for further updates to the work
- **Journal**: None
- **Summary**: Automated image captioning is one of the applications of Deep Learning which involves fusion of work done in computer vision and natural language processing, and it is typically performed using Encoder-Decoder architectures. In this project, we have implemented and experimented with various flavors of multi-modal image captioning networks where ResNet101, DenseNet121 and VGG19 based CNN Encoders and Attention based LSTM Decoders were explored. We have studied the effect of beam size and the use of pretrained word embeddings and compared them to baseline CNN encoder and RNN decoder architecture. The goal is to analyze the performance of each approach using various evaluation metrics including BLEU, CIDEr, ROUGE and METEOR. We have also explored model explainability using Visual Attention Maps (VAM) to highlight parts of the images which has maximum contribution for predicting each word of the generated caption.



### One Network to Solve Them All: A Sequential Multi-Task Joint Learning Network Framework for MR Imaging Pipeline
- **Arxiv ID**: http://arxiv.org/abs/2105.06653v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2105.06653v1)
- **Published**: 2021-05-14 05:55:27+00:00
- **Updated**: 2021-05-14 05:55:27+00:00
- **Authors**: Zhiwen Wang, Wenjun Xia, Zexin Lu, Yongqiang Huang, Yan Liu, Hu Chen, Jiliu Zhou, Yi Zhang
- **Comment**: 9 pages, 3 figures
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) acquisition, reconstruction, and segmentation are usually processed independently in the conventional practice of MRI workflow. It is easy to notice that there are significant relevances among these tasks and this procedure artificially cuts off these potential connections, which may lead to losing clinically important information for the final diagnosis. To involve these potential relations for further performance improvement, a sequential multi-task joint learning network model is proposed to train a combined end-to-end pipeline in a differentiable way, aiming at exploring the mutual influence among those tasks simultaneously. Our design consists of three cascaded modules: 1) deep sampling pattern learning module optimizes the $k$-space sampling pattern with predetermined sampling rate; 2) deep reconstruction module is dedicated to reconstructing MR images from the undersampled data using the learned sampling pattern; 3) deep segmentation module encodes MR images reconstructed from the previous module to segment the interested tissues. The proposed model retrieves the latently interactive and cyclic relations among those tasks, from which each task will be mutually beneficial. The proposed framework is verified on MRB dataset, which achieves superior performance on other SOTA methods in terms of both reconstruction and segmentation.



### Sketch2Model: View-Aware 3D Modeling from Single Free-Hand Sketches
- **Arxiv ID**: http://arxiv.org/abs/2105.06663v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06663v1)
- **Published**: 2021-05-14 06:27:48+00:00
- **Updated**: 2021-05-14 06:27:48+00:00
- **Authors**: Song-Hai Zhang, Yuan-Chen Guo, Qing-Wen Gu
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: We investigate the problem of generating 3D meshes from single free-hand sketches, aiming at fast 3D modeling for novice users. It can be regarded as a single-view reconstruction problem, but with unique challenges, brought by the variation and conciseness of sketches. Ambiguities in poorly-drawn sketches could make it hard to determine how the sketched object is posed. In this paper, we address the importance of viewpoint specification for overcoming such ambiguities, and propose a novel view-aware generation approach. By explicitly conditioning the generation process on a given viewpoint, our method can generate plausible shapes automatically with predicted viewpoints, or with specified viewpoints to help users better express their intentions. Extensive evaluations on various datasets demonstrate the effectiveness of our view-aware design in solving sketch ambiguities and improving reconstruction quality.



### Attentional Prototype Inference for Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.06668v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06668v3)
- **Published**: 2021-05-14 06:58:44+00:00
- **Updated**: 2023-05-30 01:28:07+00:00
- **Authors**: Haoliang Sun, Xiankai Lu, Haochen Wang, Yilong Yin, Xiantong Zhen, Cees G. M. Snoek, Ling Shao
- **Comment**: Pattern Recognition Journal
- **Journal**: None
- **Summary**: This paper aims to address few-shot segmentation. While existing prototype-based methods have achieved considerable success, they suffer from uncertainty and ambiguity caused by limited labeled examples. In this work, we propose attentional prototype inference (API), a probabilistic latent variable framework for few-shot segmentation. We define a global latent variable to represent the prototype of each object category, which we model as a probabilistic distribution. The probabilistic modeling of the prototype enhances the model's generalization ability by handling the inherent uncertainty caused by limited data and intra-class variations of objects. To further enhance the model, we introduce a local latent variable to represent the attention map of each query image, which enables the model to attend to foreground objects while suppressing the background. The optimization of the proposed model is formulated as a variational Bayesian inference problem, which is established by amortized inference networks. We conduct extensive experiments on four benchmarks, where our proposal obtains at least competitive and often better performance than state-of-the-art prototype-based methods. We also provide comprehensive analyses and ablation studies to gain insight into the effectiveness of our method for few-shot segmentation.



### XAI Handbook: Towards a Unified Framework for Explainable AI
- **Arxiv ID**: http://arxiv.org/abs/2105.06677v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.06677v1)
- **Published**: 2021-05-14 07:28:21+00:00
- **Updated**: 2021-05-14 07:28:21+00:00
- **Authors**: Sebastian Palacio, Adriano Lucieri, Mohsin Munir, Jörn Hees, Sheraz Ahmed, Andreas Dengel
- **Comment**: None
- **Journal**: Proceedings of the 2021 IEEE/CVF International Conference on
  Computer Vision (ICCV) Workshops
- **Summary**: The field of explainable AI (XAI) has quickly become a thriving and prolific community. However, a silent, recurrent and acknowledged issue in this area is the lack of consensus regarding its terminology. In particular, each new contribution seems to rely on its own (and often intuitive) version of terms like "explanation" and "interpretation". Such disarray encumbers the consolidation of advances in the field towards the fulfillment of scientific and regulatory demands e.g., when comparing methods or establishing their compliance with respect to biases and fairness constraints. We propose a theoretical framework that not only provides concrete definitions for these terms, but it also outlines all steps necessary to produce explanations and interpretations. The framework also allows for existing contributions to be re-contextualized such that their scope can be measured, thus making them comparable to other methods. We show that this framework is compliant with desiderata on explanations, on interpretability and on evaluation metrics. We present a use-case showing how the framework can be used to compare LIME, SHAP and MDNet, establishing their advantages and shortcomings. Finally, we discuss relevant trends in XAI as well as recommendations for future work, all from the standpoint of our framework.



### REGINA - Reasoning Graph Convolutional Networks in Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2105.06711v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06711v1)
- **Published**: 2021-05-14 08:46:42+00:00
- **Updated**: 2021-05-14 08:46:42+00:00
- **Authors**: Bruno Degardin, Vasco Lopes, Hugo Proença
- **Comment**: None
- **Journal**: None
- **Summary**: It is known that the kinematics of the human body skeleton reveals valuable information in action recognition. Recently, modeling skeletons as spatio-temporal graphs with Graph Convolutional Networks (GCNs) has been reported to solidly advance the state-of-the-art performance. However, GCN-based approaches exclusively learn from raw skeleton data, and are expected to extract the inherent structural information on their own. This paper describes REGINA, introducing a novel way to REasoning Graph convolutional networks IN Human Action recognition. The rationale is to provide to the GCNs additional knowledge about the skeleton data, obtained by handcrafted features, in order to facilitate the learning process, while guaranteeing that it remains fully trainable in an end-to-end manner. The challenge is to capture complementary information over the dynamics between consecutive frames, which is the key information extracted by state-of-the-art GCN techniques. Moreover, the proposed strategy can be easily integrated in the existing GCN-based methods, which we also regard positively. Our experiments were carried out in well known action recognition datasets and enabled to conclude that REGINA contributes for solid improvements in performance when incorporated to other GCN-based approaches, without any other adjustment regarding the original method. For reproducibility, the REGINA code and all the experiments carried out will be publicly available at https://github.com/DegardinBruno.



### Confidence-guided Adaptive Gate and Dual Differential Enhancement for Video Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.06714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06714v1)
- **Published**: 2021-05-14 08:49:37+00:00
- **Updated**: 2021-05-14 08:49:37+00:00
- **Authors**: Peijia Chen, Jianhuang Lai, Guangcong Wang, Huajun Zhou
- **Comment**: Accepted by ICME2021 as oral
- **Journal**: None
- **Summary**: Video salient object detection (VSOD) aims to locate and segment the most attractive object by exploiting both spatial cues and temporal cues hidden in video sequences. However, spatial and temporal cues are often unreliable in real-world scenarios, such as low-contrast foreground, fast motion, and multiple moving objects. To address these problems, we propose a new framework to adaptively capture available information from spatial and temporal cues, which contains Confidence-guided Adaptive Gate (CAG) modules and Dual Differential Enhancement (DDE) modules. For both RGB features and optical flow features, CAG estimates confidence scores supervised by the IoU between predictions and the ground truths to re-calibrate the information with a gate mechanism. DDE captures the differential feature representation to enrich the spatial and temporal information and generate the fused features. Experimental results on four widely used datasets demonstrate the effectiveness of the proposed method against thirteen state-of-the-art methods.



### Verification of Size Invariance in DNN Activations using Concept Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2105.06727v1
- **DOI**: 10.1007/978-3-030-79150-6_30
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2105.06727v1)
- **Published**: 2021-05-14 09:25:33+00:00
- **Updated**: 2021-05-14 09:25:33+00:00
- **Authors**: Gesina Schwalbe
- **Comment**: 12 pages, 7 figures; Camera-ready version for AIAI2021
- **Journal**: None
- **Summary**: The benefits of deep neural networks (DNNs) have become of interest for safety critical applications like medical ones or automated driving. Here, however, quantitative insights into the DNN inner representations are mandatory. One approach to this is concept analysis, which aims to establish a mapping between the internal representation of a DNN and intuitive semantic concepts. Such can be sub-objects like human body parts that are valuable for validation of pedestrian detection. To our knowledge, concept analysis has not yet been applied to large object detectors, specifically not for sub-parts. Therefore, this work first suggests a substantially improved version of the Net2Vec approach (arXiv:1801.03454) for post-hoc segmentation of sub-objects. Its practical applicability is then demonstrated on a new concept dataset by two exemplary assessments of three standard networks, including the larger Mask R-CNN model (arXiv:1703.06870): (1) the consistency of body part similarity, and (2) the invariance of internal representations of body parts with respect to the size in pixels of the depicted person. The findings show that the representation of body parts is mostly size invariant, which may suggest an early intelligent fusion of information in different size categories.



### Automated segmentation of microtomography imaging of Egyptian mummies
- **Arxiv ID**: http://arxiv.org/abs/2105.06738v2
- **DOI**: 10.1371/journal.pone.0260707
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06738v2)
- **Published**: 2021-05-14 09:56:13+00:00
- **Updated**: 2021-12-16 09:23:43+00:00
- **Authors**: Marc Tanti, Camille Berruyer, Paul Tafforeau, Adrian Muscat, Reuben Farrugia, Kenneth Scerri, Gianluca Valentino, V. Armando Solé, Johann A. Briffa
- **Comment**: None
- **Journal**: PLOS ONE, vol. 16, no. 12, p. e0260707, 2021
- **Summary**: Propagation Phase Contrast Synchrotron Microtomography (PPC-SR${\mu}$CT) is the gold standard for non-invasive and non-destructive access to internal structures of archaeological remains. In this analysis, the virtual specimen needs to be segmented to separate different parts or materials, a process that normally requires considerable human effort. In the Automated SEgmentation of Microtomography Imaging (ASEMI) project, we developed a tool to automatically segment these volumetric images, using manually segmented samples to tune and train a machine learning model. For a set of four specimens of ancient Egyptian animal mummies we achieve an overall accuracy of 94-98% when compared with manually segmented slices, approaching the results of off-the-shelf commercial software using deep learning (97-99%) at much lower complexity. A qualitative analysis of the segmented output shows that our results are close in terms of usability to those from deep learning, justifying the use of these techniques.



### Facial Age Estimation using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.06746v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.06746v1)
- **Published**: 2021-05-14 10:09:47+00:00
- **Updated**: 2021-05-14 10:09:47+00:00
- **Authors**: Adrian Kjærran, Christian Bakke Vennerød, Erling Stray Bugge
- **Comment**: None
- **Journal**: None
- **Summary**: This paper is a part of a student project in Machine Learning at the Norwegian University of Science and Technology. In this paper, a deep convolutional neural network with five convolutional layers and three fully-connected layers is presented to estimate the ages of individuals based on images. The model is in its entirety trained from scratch, where a combination of three different datasets is used as training data. These datasets are the APPA dataset, UTK dataset, and the IMDB dataset. The images were preprocessed using a proprietary face-recognition software. Our model is evaluated on both a held-out test set, and on the Adience benchmark. On the test set, our model achieves a categorical accuracy of 52%. On the Adience benchmark, our model proves inferior compared with other leading models, with an exact accuray of 30%, and an one-off accuracy of 46%. Furthermore, a script was created, allowing users to estimate their age directly using their web camera. The script, alongside all other code, is located in our GitHub repository: AgeNet.



### Troubleshooting Blind Image Quality Models in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2105.06747v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06747v1)
- **Published**: 2021-05-14 10:10:48+00:00
- **Updated**: 2021-05-14 10:10:48+00:00
- **Authors**: Zhihua Wang, Haotao Wang, Tianlong Chen, Zhangyang Wang, Kede Ma
- **Comment**: 7 pages, 3 tables
- **Journal**: None
- **Summary**: Recently, the group maximum differentiation competition (gMAD) has been used to improve blind image quality assessment (BIQA) models, with the help of full-reference metrics. When applying this type of approach to troubleshoot "best-performing" BIQA models in the wild, we are faced with a practical challenge: it is highly nontrivial to obtain stronger competing models for efficient failure-spotting. Inspired by recent findings that difficult samples of deep models may be exposed through network pruning, we construct a set of "self-competitors," as random ensembles of pruned versions of the target model to be improved. Diverse failures can then be efficiently identified via self-gMAD competition. Next, we fine-tune both the target and its pruned variants on the human-rated gMAD set. This allows all models to learn from their respective failures, preparing themselves for the next round of self-gMAD competition. Experimental results demonstrate that our method efficiently troubleshoots BIQA models in the wild with improved generalizability.



### Learning Group Activities from Skeletons without Individual Action Labels
- **Arxiv ID**: http://arxiv.org/abs/2105.06754v1
- **DOI**: 10.1109/ICPR48806.2021.9413195
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06754v1)
- **Published**: 2021-05-14 10:31:32+00:00
- **Updated**: 2021-05-14 10:31:32+00:00
- **Authors**: Fabio Zappardino, Tiberio Uricchio, Lorenzo Seidenari, Alberto Del Bimbo
- **Comment**: ICPR 2020
- **Journal**: None
- **Summary**: To understand human behavior we must not just recognize individual actions but model possibly complex group activity and interactions. Hierarchical models obtain the best results in group activity recognition but require fine grained individual action annotations at the actor level. In this paper we show that using only skeletal data we can train a state-of-the art end-to-end system using only group activity labels at the sequence level. Our experiments show that models trained without individual action supervision perform poorly. On the other hand we show that pseudo-labels can be computed from any pre-trained feature extractor with comparable final performance. Finally our carefully designed lean pose only architecture shows highly competitive results versus more complex multimodal approaches even in the self-supervised variant.



### Exploring the Intrinsic Probability Distribution for Hyperspectral Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.06775v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.06775v1)
- **Published**: 2021-05-14 11:42:09+00:00
- **Updated**: 2021-05-14 11:42:09+00:00
- **Authors**: Shaoqi Yu, Xiaorun Li, Shuhan Chen, Liaoying Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, neural network-based anomaly detection methods have attracted considerable attention in the hyperspectral remote sensing domain due to the powerful reconstruction ability compared with traditional methods. However, actual probability distribution statistics hidden in the latent space are not discovered by exploiting the reconstruction error because the probability distribution of anomalies is not explicitly modeled. To address the issue, we propose a novel probability distribution representation detector (PDRD) that explores the intrinsic distribution of both the background and the anomalies in original data for hyperspectral anomaly detection in this paper. First, we represent the hyperspectral data with multivariate Gaussian distributions from a probabilistic perspective. Then, we combine the local statistics with the obtained distributions to leverage the spatial information. Finally, the difference between the corresponding distributions of the test pixel and the average expectation of the pixels in the Chebyshev neighborhood is measured by computing the modified Wasserstein distance to acquire the detection map. We conduct the experiments on four real data sets to evaluate the performance of our proposed method. Experimental results demonstrate the accuracy and efficiency of our proposed method compared to the state-of-the-art detection methods.



### DARNet: Dual-Attention Residual Network for Automatic Diagnosis of COVID-19 via CT Images
- **Arxiv ID**: http://arxiv.org/abs/2105.06779v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.06779v2)
- **Published**: 2021-05-14 11:59:47+00:00
- **Updated**: 2021-08-30 13:29:39+00:00
- **Authors**: Jun Shi, Huite Yi, Shulan Ruan, Zhaohui Wang, Xiaoyu Hao, Hong An, Wei Wei
- **Comment**: 7 pages, 4 figures,
- **Journal**: None
- **Summary**: The ongoing global pandemic of Coronavirus Disease 2019 (COVID-19) poses a serious threat to public health and the economy. Rapid and accurate diagnosis of COVID-19 is crucial to prevent the further spread of the disease and reduce its mortality. Chest Computed tomography (CT) is an effective tool for the early diagnosis of lung diseases including pneumonia. However, detecting COVID-19 from CT is demanding and prone to human errors as some early-stage patients may have negative findings on images. Recently, many deep learning methods have achieved impressive performance in this regard. Despite their effectiveness, most of these methods underestimate the rich spatial information preserved in the 3D structure or suffer from the propagation of errors. To address this problem, we propose a Dual-Attention Residual Network (DARNet) to automatically identify COVID-19 from other common pneumonia (CP) and healthy people using 3D chest CT images. Specifically, we design a dual-attention module consisting of channel-wise attention and depth-wise attention mechanisms. The former is utilized to enhance channel independence, while the latter is developed to recalibrate the depth-level features. Then, we integrate them in a unified manner to extract and refine the features at different levels to further improve the diagnostic performance. We evaluate DARNet on a large public CT dataset and obtain superior performance. Besides, the ablation study and visualization analysis prove the effectiveness and interpretability of the proposed method.



### Salient Feature Extractor for Adversarial Defense on Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.06807v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2105.06807v1)
- **Published**: 2021-05-14 12:56:06+00:00
- **Updated**: 2021-05-14 12:56:06+00:00
- **Authors**: Jinyin Chen, Ruoxi Chen, Haibin Zheng, Zhaoyan Ming, Wenrong Jiang, Chen Cui
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have witnessed unprecedented success achieved by deep learning models in the field of computer vision. However, their vulnerability towards carefully crafted adversarial examples has also attracted the increasing attention of researchers. Motivated by the observation that adversarial examples are due to the non-robust feature learned from the original dataset by models, we propose the concepts of salient feature(SF) and trivial feature(TF). The former represents the class-related feature, while the latter is usually adopted to mislead the model. We extract these two features with coupled generative adversarial network model and put forward a novel detection and defense method named salient feature extractor (SFE) to defend against adversarial attacks. Concretely, detection is realized by separating and comparing the difference between SF and TF of the input. At the same time, correct labels are obtained by re-identifying SF to reach the purpose of defense. Extensive experiments are carried out on MNIST, CIFAR-10, and ImageNet datasets where SFE shows state-of-the-art results in effectiveness and efficiency compared with baselines. Furthermore, we provide an interpretable understanding of the defense and detection process.



### Collaborative Spatial-Temporal Modeling for Language-Queried Video Actor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.06818v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2105.06818v1)
- **Published**: 2021-05-14 13:27:53+00:00
- **Updated**: 2021-05-14 13:27:53+00:00
- **Authors**: Tianrui Hui, Shaofei Huang, Si Liu, Zihan Ding, Guanbin Li, Wenguan Wang, Jizhong Han, Fei Wang
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Language-queried video actor segmentation aims to predict the pixel-level mask of the actor which performs the actions described by a natural language query in the target frames. Existing methods adopt 3D CNNs over the video clip as a general encoder to extract a mixed spatio-temporal feature for the target frame. Though 3D convolutions are amenable to recognizing which actor is performing the queried actions, it also inevitably introduces misaligned spatial information from adjacent frames, which confuses features of the target frame and yields inaccurate segmentation. Therefore, we propose a collaborative spatial-temporal encoder-decoder framework which contains a 3D temporal encoder over the video clip to recognize the queried actions, and a 2D spatial encoder over the target frame to accurately segment the queried actors. In the decoder, a Language-Guided Feature Selection (LGFS) module is proposed to flexibly integrate spatial and temporal features from the two encoders. We also propose a Cross-Modal Adaptive Modulation (CMAM) module to dynamically recombine spatial- and temporal-relevant linguistic features for multimodal feature interaction in each stage of the two encoders. Our method achieves new state-of-the-art performance on two popular benchmarks with less computational overhead than previous approaches.



### Predicting Surface Reflectance Properties of Outdoor Scenes Under Unknown Natural Illumination
- **Arxiv ID**: http://arxiv.org/abs/2105.06820v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2105.06820v1)
- **Published**: 2021-05-14 13:31:47+00:00
- **Updated**: 2021-05-14 13:31:47+00:00
- **Authors**: Farhan Rahman Wasee, Alen Joy, Charalambos Poullis
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating and modelling the appearance of an object under outdoor illumination conditions is a complex process. Although there have been several studies on illumination estimation and relighting, very few of them focus on estimating the reflectance properties of outdoor objects and scenes. This paper addresses this problem and proposes a complete framework to predict surface reflectance properties of outdoor scenes under unknown natural illumination. Uniquely, we recast the problem into its two constituent components involving the BRDF incoming light and outgoing view directions: (i) surface points' radiance captured in the images, and outgoing view directions are aggregated and encoded into reflectance maps, and (ii) a neural network trained on reflectance maps of renders of a unit sphere under arbitrary light directions infers a low-parameter reflection model representing the reflectance properties at each surface in the scene. Our model is based on a combination of phenomenological and physics-based scattering models and can relight the scenes from novel viewpoints. We present experiments that show that rendering with the predicted reflectance properties results in a visually similar appearance to using textures that cannot otherwise be disentangled from the reflectance properties.



### Multi-task Graph Convolutional Neural Network for Calcification Morphology and Distribution Analysis in Mammograms
- **Arxiv ID**: http://arxiv.org/abs/2105.06822v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06822v1)
- **Published**: 2021-05-14 13:32:47+00:00
- **Updated**: 2021-05-14 13:32:47+00:00
- **Authors**: Hao Du, Melissa Min-Szu Yao, Liangyu Chen, Wing P. Chan, Mengling Feng
- **Comment**: None
- **Journal**: None
- **Summary**: The morphology and distribution of microcalcifications in a cluster are the most important characteristics for radiologists to diagnose breast cancer. However, it is time-consuming and difficult for radiologists to identify these characteristics, and there also lacks of effective solutions for automatic characterization. In this study, we proposed a multi-task deep graph convolutional network (GCN) method for the automatic characterization of morphology and distribution of microcalcifications in mammograms. Our proposed method transforms morphology and distribution characterization into node and graph classification problem and learns the representations concurrently. Through extensive experiments, we demonstrate significant improvements with the proposed multi-task GCN comparing to the baselines. Moreover, the achieved improvements can be related to and enhance clinical understandings. We explore, for the first time, the application of GCNs in microcalcification characterization that suggests the potential of graph learning for more robust understanding of medical images.



### Domestic waste detection and grasping points for robotic picking up
- **Arxiv ID**: http://arxiv.org/abs/2105.06825v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.06825v1)
- **Published**: 2021-05-14 13:37:33+00:00
- **Updated**: 2021-05-14 13:37:33+00:00
- **Authors**: Victor De Gea, Santiago T. Puente, Pablo Gil
- **Comment**: 2 pages, 3 figures, accepted as poster for presentation in ICRA 2021
  Workshop: Emerging paradigms for robotic manipulation: from the lab to the
  productive world
- **Journal**: None
- **Summary**: This paper presents an AI system applied to location and robotic grasping. Experimental setup is based on a parameter study to train a deep-learning network based on Mask-RCNN to perform waste location in indoor and outdoor environment, using five different classes and generating a new waste dataset. Initially the AI system obtain the RGBD data of the environment, followed by the detection of objects using the neural network. Later, the 3D object shape is computed using the network result and the depth channel. Finally, the shape is used to compute grasping for a robot arm with a two-finger gripper. The objective is to classify the waste in groups to improve a recycling strategy.



### Exploiting Aliasing for Manga Restoration
- **Arxiv ID**: http://arxiv.org/abs/2105.06830v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.06830v1)
- **Published**: 2021-05-14 13:47:04+00:00
- **Updated**: 2021-05-14 13:47:04+00:00
- **Authors**: Minshan Xie, Menghan Xia, Tien-Tsin Wong
- **Comment**: None
- **Journal**: None
- **Summary**: As a popular entertainment art form, manga enriches the line drawings details with bitonal screentones. However, manga resources over the Internet usually show screentone artifacts because of inappropriate scanning/rescaling resolution. In this paper, we propose an innovative two-stage method to restore quality bitonal manga from degraded ones. Our key observation is that the aliasing induced by downsampling bitonal screentones can be utilized as informative clues to infer the original resolution and screentones. First, we predict the target resolution from the degraded manga via the Scale Estimation Network (SE-Net) with spatial voting scheme. Then, at the target resolution, we restore the region-wise bitonal screentones via the Manga Restoration Network (MR-Net) discriminatively, depending on the degradation degree. Specifically, the original screentones are directly restored in pattern-identifiable regions, and visually plausible screentones are synthesized in pattern-agnostic regions. Quantitative evaluation on synthetic data and visual assessment on real-world cases illustrate the effectiveness of our method.



### Fit4CAD: A point cloud benchmark for fitting simple geometric primitives in CAD objects
- **Arxiv ID**: http://arxiv.org/abs/2105.06858v3
- **DOI**: 10.1016/j.cag.2021.09.013
- **Categories**: **cs.GR**, cs.CV, J.6; I.3.5
- **Links**: [PDF](http://arxiv.org/pdf/2105.06858v3)
- **Published**: 2021-05-14 14:32:08+00:00
- **Updated**: 2021-10-05 09:00:56+00:00
- **Authors**: Chiara Romanengo, Andrea Raffo, Yifan Qie, Nabil Anwer, Bianca Falcidieno
- **Comment**: None
- **Journal**: Computers & Graphics 102 (2022) 133-143
- **Summary**: We propose Fit4CAD, a benchmark for the evaluation and comparison of methods for fitting simple geometric primitives in point clouds representing CAD objects. This benchmark is meant to help both method developers and those who want to identify the best performing tools. The Fit4CAD dataset is composed by 225 high quality point clouds, each of which has been obtained by sampling a CAD object. The way these elements were created by using existing platforms and datasets makes the benchmark easily expandable. The dataset is already split into a training set and a test set. To assess performance and accuracy of the different primitive fitting methods, various measures are defined. To demonstrate the effective use of Fit4CAD, we have tested it on two methods belonging to two different categories of approaches to the primitive fitting problem: a clustering method based on a primitive growing framework and a parametric method based on the Hough transform.



### VICE: Visual Identification and Correction of Neural Circuit Errors
- **Arxiv ID**: http://arxiv.org/abs/2105.06861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06861v1)
- **Published**: 2021-05-14 14:34:58+00:00
- **Updated**: 2021-05-14 14:34:58+00:00
- **Authors**: Felix Gonda, Xueying Wang, Johanna Beyer, Markus Hadwiger, Jeff W. Lichtman, Hanspeter Pfister
- **Comment**: This paper has been accepted for publication and presentation at the
  23rd EG Conference on Visualization (EuroVis 2021)
- **Journal**: None
- **Summary**: A connectivity graph of neurons at the resolution of single synapses provides scientists with a tool for understanding the nervous system in health and disease. Recent advances in automatic image segmentation and synapse prediction in electron microscopy (EM) datasets of the brain have made reconstructions of neurons possible at the nanometer scale. However, automatic segmentation sometimes struggles to segment large neurons correctly, requiring human effort to proofread its output. General proofreading involves inspecting large volumes to correct segmentation errors at the pixel level, a visually intensive and time-consuming process. This paper presents the design and implementation of an analytics framework that streamlines proofreading, focusing on connectivity-related errors. We accomplish this with automated likely-error detection and synapse clustering that drives the proofreading effort with highly interactive 3D visualizations. In particular, our strategy centers on proofreading the local circuit of a single cell to ensure a basic level of completeness. We demonstrate our framework's utility with a user study and report quantitative and subjective feedback from our users. Overall, users find the framework more efficient for proofreading, understanding evolving graphs, and sharing error correction strategies.



### End-to-end Alternating Optimization for Blind Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/2105.06878v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06878v1)
- **Published**: 2021-05-14 15:05:30+00:00
- **Updated**: 2021-05-14 15:05:30+00:00
- **Authors**: Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang, Tieniu Tan
- **Comment**: Submited to PAMI. arXiv admin note: substantial text overlap with
  arXiv:2010.02631
- **Journal**: None
- **Summary**: Previous methods decompose the blind super-resolution (SR) problem into two sequential steps: \textit{i}) estimating the blur kernel from given low-resolution (LR) image and \textit{ii}) restoring the SR image based on the estimated kernel. This two-step solution involves two independently trained models, which may not be well compatible with each other. A small estimation error of the first step could cause a severe performance drop of the second one. While on the other hand, the first step can only utilize limited information from the LR image, which makes it difficult to predict a highly accurate blur kernel. Towards these issues, instead of considering these two steps separately, we adopt an alternating optimization algorithm, which can estimate the blur kernel and restore the SR image in a single model. Specifically, we design two convolutional neural modules, namely \textit{Restorer} and \textit{Estimator}. \textit{Restorer} restores the SR image based on the predicted kernel, and \textit{Estimator} estimates the blur kernel with the help of the restored SR image. We alternate these two modules repeatedly and unfold this process to form an end-to-end trainable network. In this way, \textit{Estimator} utilizes information from both LR and SR images, which makes the estimation of the blur kernel easier. More importantly, \textit{Restorer} is trained with the kernel estimated by \textit{Estimator}, instead of the ground-truth kernel, thus \textit{Restorer} could be more tolerant to the estimation error of \textit{Estimator}. Extensive experiments on synthetic datasets and real-world images show that our model can largely outperform state-of-the-art methods and produce more visually favorable results at a much higher speed. The source code is available at \url{https://github.com/greatlog/DAN.git}.



### A Frequency Domain Constraint for Synthetic and Real X-ray Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/2105.06887v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.06887v2)
- **Published**: 2021-05-14 15:17:27+00:00
- **Updated**: 2021-08-10 19:19:50+00:00
- **Authors**: Qing Ma, Jae Chul Koh, WonSook Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Synthetic X-ray images are simulated X-ray images projected from CT data. High-quality synthetic X-ray images can facilitate various applications such as surgical image guidance systems and VR training simulations. However, it is difficult to produce high-quality arbitrary view synthetic X-ray images in real-time due to different CT slice thickness, high computational cost, and the complexity of algorithms. Our goal is to generate high-resolution synthetic X-ray images in real-time by upsampling low-resolution images with deep learning-based super-resolution methods. Reference-based Super Resolution (RefSR) has been well studied in recent years and has shown higher performance than traditional Single Image Super-Resolution (SISR). It can produce fine details by utilizing the reference image but still inevitably generates some artifacts and noise. In this paper, we introduce frequency domain loss as a constraint to further improve the quality of the RefSR results with fine details and without obvious artifacts. To the best of our knowledge, this is the first paper utilizing the frequency domain for the loss functions in the field of super-resolution. We achieved good results in evaluating our method on both synthetic and real X-ray image datasets.



### Open-set Face Recognition for Small Galleries Using Siamese Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.06967v1
- **DOI**: 10.1109/IWSSIP48289.2020.9145245
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06967v1)
- **Published**: 2021-05-14 17:16:37+00:00
- **Updated**: 2021-05-14 17:16:37+00:00
- **Authors**: Gabriel Salomon, Alceu Britto, Rafael H. Vareto, William R. Schwartz, David Menotti
- **Comment**: None
- **Journal**: 2020 International Conference on Systems, Signals and Image
  Processing (IWSSIP), 2020, pp. 161-166
- **Summary**: Face recognition has been one of the most relevant and explored fields of Biometrics. In real-world applications, face recognition methods usually must deal with scenarios where not all probe individuals were seen during the training phase (open-set scenarios). Therefore, open-set face recognition is a subject of increasing interest as it deals with identifying individuals in a space where not all faces are known in advance. This is useful in several applications, such as access authentication, on which only a few individuals that have been previously enrolled in a gallery are allowed. The present work introduces a novel approach towards open-set face recognition focusing on small galleries and in enrollment detection, not identity retrieval. A Siamese Network architecture is proposed to learn a model to detect if a face probe is enrolled in the gallery based on a verification-like approach. Promising results were achieved for small galleries on experiments carried out on Pubfig83, FRGCv1 and LFW datasets. State-of-the-art methods like HFCN and HPLS were outperformed on FRGCv1. Besides, a new evaluation protocol is introduced for experiments in small galleries on LFW.



### Evaluating the Robustness of Self-Supervised Learning in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2105.06986v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06986v1)
- **Published**: 2021-05-14 17:49:52+00:00
- **Updated**: 2021-05-14 17:49:52+00:00
- **Authors**: Fernando Navarro, Christopher Watanabe, Suprosanna Shit, Anjany Sekuboyina, Jan C. Peeken, Stephanie E. Combs, Bjoern H. Menze
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervision has demonstrated to be an effective learning strategy when training target tasks on small annotated data-sets. While current research focuses on creating novel pretext tasks to learn meaningful and reusable representations for the target task, these efforts obtain marginal performance gains compared to fully-supervised learning. Meanwhile, little attention has been given to study the robustness of networks trained in a self-supervised manner. In this work, we demonstrate that networks trained via self-supervised learning have superior robustness and generalizability compared to fully-supervised learning in the context of medical imaging. Our experiments on pneumonia detection in X-rays and multi-organ segmentation in CT yield consistent results exposing the hidden benefits of self-supervision for learning robust feature representations.



### Automatic Non-Linear Video Editing Transfer
- **Arxiv ID**: http://arxiv.org/abs/2105.06988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06988v1)
- **Published**: 2021-05-14 17:52:25+00:00
- **Updated**: 2021-05-14 17:52:25+00:00
- **Authors**: Nathan Frey, Peggy Chi, Weilong Yang, Irfan Essa
- **Comment**: Published to AI for Content Creation Workshop at CVPR 2021
- **Journal**: AI for Content Creation Workshop at CVPR 2021
- **Summary**: We propose an automatic approach that extracts editing styles in a source video and applies the edits to matched footage for video creation. Our Computer Vision based techniques considers framing, content type, playback speed, and lighting of each input video segment. By applying a combination of these features, we demonstrate an effective method that automatically transfers the visual and temporal styles from professionally edited videos to unseen raw footage. We evaluated our approach with real-world videos that contained a total of 3872 video shots of a variety of editing styles, including different subjects, camera motions, and lighting. We reported feedback from survey participants who reviewed a set of our results.



### Omnimatte: Associating Objects and Their Effects in Video
- **Arxiv ID**: http://arxiv.org/abs/2105.06993v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.06993v2)
- **Published**: 2021-05-14 17:57:08+00:00
- **Updated**: 2021-10-01 01:26:22+00:00
- **Authors**: Erika Lu, Forrester Cole, Tali Dekel, Andrew Zisserman, William T. Freeman, Michael Rubinstein
- **Comment**: CVPR 2021 Oral. Project webpage: https://omnimatte.github.io/. Added
  references
- **Journal**: None
- **Summary**: Computer vision is increasingly effective at segmenting objects in images and videos; however, scene effects related to the objects -- shadows, reflections, generated smoke, etc -- are typically overlooked. Identifying such scene effects and associating them with the objects producing them is important for improving our fundamental understanding of visual scenes, and can also assist a variety of applications such as removing, duplicating, or enhancing objects in video. In this work, we take a step towards solving this novel problem of automatically associating objects with their effects in video. Given an ordinary video and a rough segmentation mask over time of one or more subjects of interest, we estimate an omnimatte for each subject -- an alpha matte and color image that includes the subject along with all its related time-varying scene elements. Our model is trained only on the input video in a self-supervised manner, without any manual labels, and is generic -- it produces omnimattes automatically for arbitrary objects and a variety of effects. We show results on real-world videos containing interactions between different types of subjects (cars, animals, people) and complex effects, ranging from semi-transparent elements such as smoke and reflections, to fully opaque effects such as objects attached to the subject.



### SMURF: Self-Teaching Multi-Frame Unsupervised RAFT with Full-Image Warping
- **Arxiv ID**: http://arxiv.org/abs/2105.07014v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.07014v1)
- **Published**: 2021-05-14 18:02:50+00:00
- **Updated**: 2021-05-14 18:02:50+00:00
- **Authors**: Austin Stone, Daniel Maurer, Alper Ayvaci, Anelia Angelova, Rico Jonschkowski
- **Comment**: Accepted at CVPR 2021, all code available at
  https://github.com/google-research/google-research/tree/master/smurf
- **Journal**: None
- **Summary**: We present SMURF, a method for unsupervised learning of optical flow that improves state of the art on all benchmarks by $36\%$ to $40\%$ (over the prior best method UFlow) and even outperforms several supervised approaches such as PWC-Net and FlowNet2. Our method integrates architecture improvements from supervised optical flow, i.e. the RAFT model, with new ideas for unsupervised learning that include a sequence-aware self-supervision loss, a technique for handling out-of-frame motion, and an approach for learning effectively from multi-frame video data while still only requiring two frames for inference.



### Learning a Universal Template for Few-shot Dataset Generalization
- **Arxiv ID**: http://arxiv.org/abs/2105.07029v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.07029v2)
- **Published**: 2021-05-14 18:46:06+00:00
- **Updated**: 2021-06-21 15:31:54+00:00
- **Authors**: Eleni Triantafillou, Hugo Larochelle, Richard Zemel, Vincent Dumoulin
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot dataset generalization is a challenging variant of the well-studied few-shot classification problem where a diverse training set of several datasets is given, for the purpose of training an adaptable model that can then learn classes from new datasets using only a few examples. To this end, we propose to utilize the diverse training set to construct a universal template: a partial model that can define a wide array of dataset-specialized models, by plugging in appropriate components. For each new few-shot classification problem, our approach therefore only requires inferring a small number of parameters to insert into the universal template. We design a separate network that produces an initialization of those parameters for each given task, and we then fine-tune its proposed initialization via a few steps of gradient descent. Our approach is more parameter-efficient, scalable and adaptable compared to previous methods, and achieves the state-of-the-art on the challenging Meta-Dataset benchmark.



### SA-GAN: Structure-Aware GAN for Organ-Preserving Synthetic CT Generation
- **Arxiv ID**: http://arxiv.org/abs/2105.07044v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.07044v3)
- **Published**: 2021-05-14 19:34:23+00:00
- **Updated**: 2021-09-14 20:01:56+00:00
- **Authors**: Hajar Emami, Ming Dong, Siamak Nejad-Davarani, Carri Glide-Hurst
- **Comment**: Accepted to MICCAI 2021
- **Journal**: None
- **Summary**: In medical image synthesis, model training could be challenging due to the inconsistencies between images of different modalities even with the same patient, typically caused by internal status/tissue changes as different modalities are usually obtained at a different time. This paper proposes a novel deep learning method, Structure-aware Generative Adversarial Network (SA-GAN), that preserves the shapes and locations of in-consistent structures when generating medical images. SA-GAN is employed to generate synthetic computed tomography (synCT) images from magnetic resonance imaging (MRI) with two parallel streams: the global stream translates the input from the MRI to the CT domain while the local stream automatically segments the inconsistent organs, maintains their locations and shapes in MRI, and translates the organ intensities to CT. Through extensive experiments on a pelvic dataset, we demonstrate that SA-GAN provides clinically acceptable accuracy on both synCTs and organ segmentation and supports MR-only treatment planning in disease sites with internal organ status changes.



### Evolving Deep Convolutional Neural Network by Hybrid Sine-Cosine and Extreme Learning Machine for Real-time COVID19 Diagnosis from X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2105.14192v1
- **DOI**: 10.1007/s00500-021-05839-6
- **Categories**: **eess.IV**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2105.14192v1)
- **Published**: 2021-05-14 19:40:16+00:00
- **Updated**: 2021-05-14 19:40:16+00:00
- **Authors**: Wu Chao, Mohammad Khishe, Mokhtar Mohammadi, Sarkhel H. Taher Karim, Tarik A. Rashid
- **Comment**: 28 pages, Soft Computing, 2021
- **Journal**: None
- **Summary**: The COVID19 pandemic globally and significantly has affected the life and health of many communities. The early detection of infected patients is effective in fighting COVID19. Using radiology (X-Ray) images is perhaps the fastest way to diagnose the patients. Thereby, deep Convolutional Neural Networks (CNNs) can be considered as applicable tools to diagnose COVID19 positive cases. Due to the complicated architecture of a deep CNN, its real-time training and testing become a challenging problem. This paper proposes using the Extreme Learning Machine (ELM) instead of the last fully connected layer to address this deficiency. However, the parameters' stochastic tuning of ELM's supervised section causes the final model unreliability. Therefore, to cope with this problem and maintain network reliability, the sine-cosine algorithm was utilized to tune the ELM's parameters. The designed network is then benchmarked on the COVID-Xray-5k dataset, and the results are verified by a comparative study with canonical deep CNN, ELM optimized by cuckoo search, ELM optimized by genetic algorithm, and ELM optimized by whale optimization algorithm. The proposed approach outperforms comparative benchmarks with a final accuracy of 98.83% on the COVID-Xray-5k dataset, leading to a relative error reduction of 2.33% compared to a canonical deep CNN. Even more critical, the designed network's training time is only 0.9421 milliseconds and the overall detection test time for 3100 images is 2.721 seconds.



### Face Attributes as Cues for Deep Face Recognition Understanding
- **Arxiv ID**: http://arxiv.org/abs/2105.07054v1
- **DOI**: 10.1109/FG47880.2020.00088
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.07054v1)
- **Published**: 2021-05-14 19:54:24+00:00
- **Updated**: 2021-05-14 19:54:24+00:00
- **Authors**: Matheus Alves Diniz, William Robson Schwartz
- **Comment**: 7 pages, 5 figures, published at automatic face and gesture
  recognition 2020
- **Journal**: None
- **Summary**: Deeply learned representations are the state-of-the-art descriptors for face recognition methods. These representations encode latent features that are difficult to explain, compromising the confidence and interpretability of their predictions. Most attempts to explain deep features are visualization techniques that are often open to interpretation. Instead of relying only on visualizations, we use the outputs of hidden layers to predict face attributes. The obtained performance is an indicator of how well the attribute is implicitly learned in that layer of the network. Using a variable selection technique, we also analyze how these semantic concepts are distributed inside each layer, establishing the precise location of relevant neurons for each attribute. According to our experiments, gender, eyeglasses and hat usage can be predicted with over 96% accuracy even when only a single neural output is used to predict each attribute. These performances are less than 3 percentage points lower than the ones achieved by deep supervised face attribute networks. In summary, our experiments show that, inside DCNNs optimized for face identification, there exists latent neurons encoding face attributes almost as accurately as DCNNs optimized for these attributes.



### Momentum Contrastive Voxel-wise Representation Learning for Semi-supervised Volumetric Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.07059v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.07059v4)
- **Published**: 2021-05-14 20:27:23+00:00
- **Updated**: 2022-03-07 07:56:42+00:00
- **Authors**: Chenyu You, Ruihan Zhao, Lawrence Staib, James S. Duncan
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive learning (CL) aims to learn useful representation without relying on expert annotations in the context of medical image segmentation. Existing approaches mainly contrast a single positive vector (i.e., an augmentation of the same image) against a set of negatives within the entire remainder of the batch by simply mapping all input features into the same constant vector. Despite the impressive empirical performance, those methods have the following shortcomings: (1) it remains a formidable challenge to prevent the collapsing problems to trivial solutions; and (2) we argue that not all voxels within the same image are equally positive since there exist the dissimilar anatomical structures with the same image. In this work, we present a novel Contrastive Voxel-wise Representation Learning (CVRL) method to effectively learn low-level and high-level features by capturing 3D spatial context and rich anatomical information along both the feature and the batch dimensions. Specifically, we first introduce a novel CL strategy to ensure feature diversity promotion among the 3D representation dimensions. We train the framework through bi-level contrastive optimization (i.e., low-level and high-level) on 3D images. Experiments on two benchmark datasets and different labeled settings demonstrate the superiority of our proposed framework. More importantly, we also prove that our method inherits the benefit of hardness-aware property from the standard CL approaches.



### High-Robustness, Low-Transferability Fingerprinting of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.07078v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.07078v1)
- **Published**: 2021-05-14 21:48:23+00:00
- **Updated**: 2021-05-14 21:48:23+00:00
- **Authors**: Siyue Wang, Xiao Wang, Pin-Yu Chen, Pu Zhao, Xue Lin
- **Comment**: ICLR 2021 Workshop on Security and Safety in Machine Learning Systems
- **Journal**: None
- **Summary**: This paper proposes Characteristic Examples for effectively fingerprinting deep neural networks, featuring high-robustness to the base model against model pruning as well as low-transferability to unassociated models. This is the first work taking both robustness and transferability into consideration for generating realistic fingerprints, whereas current methods lack practical assumptions and may incur large false positive rates. To achieve better trade-off between robustness and transferability, we propose three kinds of characteristic examples: vanilla C-examples, RC-examples, and LTRC-example, to derive fingerprints from the original base model. To fairly characterize the trade-off between robustness and transferability, we propose Uniqueness Score, a comprehensive metric that measures the difference between robustness and transferability, which also serves as an indicator to the false alarm problem.



### MutualNet: Adaptive ConvNet via Mutual Learning from Different Model Configurations
- **Arxiv ID**: http://arxiv.org/abs/2105.07085v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.07085v2)
- **Published**: 2021-05-14 22:30:13+00:00
- **Updated**: 2021-12-30 15:57:46+00:00
- **Authors**: Taojiannan Yang, Sijie Zhu, Matias Mendieta, Pu Wang, Ravikumar Balakrishnan, Minwoo Lee, Tao Han, Mubarak Shah, Chen Chen
- **Comment**: Extended version of arXiv:1909.12978. Updated analyses and results.
  Accepted to TPAMI
- **Journal**: None
- **Summary**: Most existing deep neural networks are static, which means they can only do inference at a fixed complexity. But the resource budget can vary substantially across different devices. Even on a single device, the affordable budget can change with different scenarios, and repeatedly training networks for each required budget would be incredibly expensive. Therefore, in this work, we propose a general method called MutualNet to train a single network that can run at a diverse set of resource constraints. Our method trains a cohort of model configurations with various network widths and input resolutions. This mutual learning scheme not only allows the model to run at different width-resolution configurations but also transfers the unique knowledge among these configurations, helping the model to learn stronger representations overall. MutualNet is a general training methodology that can be applied to various network structures (e.g., 2D networks: MobileNets, ResNet, 3D networks: SlowFast, X3D) and various tasks (e.g., image classification, object detection, segmentation, and action recognition), and is demonstrated to achieve consistent improvements on a variety of datasets. Since we only train the model once, it also greatly reduces the training cost compared to independently training several models. Surprisingly, MutualNet can also be used to significantly boost the performance of a single network, if dynamic resource constraint is not a concern. In summary, MutualNet is a unified method for both static and adaptive, 2D and 3D networks. Codes and pre-trained models are available at \url{https://github.com/taoyang1122/MutualNet}.



