# Arxiv Papers in cs.CV on 2021-05-03
### Noisy Student learning for cross-institution brain hemorrhage detection
- **Arxiv ID**: http://arxiv.org/abs/2105.00582v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.00582v1)
- **Published**: 2021-05-03 00:14:43+00:00
- **Updated**: 2021-05-03 00:14:43+00:00
- **Authors**: Emily Lin, Weicheng Kuo, Esther Yuh
- **Comment**: None
- **Journal**: None
- **Summary**: Computed tomography (CT) is the imaging modality used in the diagnosis of neurological emergencies, including acute stroke and traumatic brain injury. Advances in deep learning have led to models that can detect and segment hemorrhage on head CT. PatchFCN, one such supervised fully convolutional network (FCN), recently demonstrated expert-level detection of intracranial hemorrhage on in-sample data. However, its potential for similar accuracy outside the training domain is hindered by its need for pixel-labeled data from outside institutions. Also recently, a semi-supervised technique, Noisy Student (NS) learning, demonstrated state-of-the-art performance on ImageNet by moving from a fully-supervised to a semi-supervised learning paradigm. We combine the PatchFCN and Noisy Student approaches, extending semi-supervised learning to an intracranial hemorrhage segmentation task. Surprisingly, the NS model performance surpasses that of a fully-supervised oracle model trained with image-level labels on the same data. It also performs comparably to another recently reported supervised model trained on a labeled dataset 600x larger than that used to train the NS model. To our knowledge, we are the first to demonstrate the effectiveness of semi-supervised learning on a head CT detection and segmentation task.



### Single-Training Collaborative Object Detectors Adaptive to Bandwidth and Computation
- **Arxiv ID**: http://arxiv.org/abs/2105.00591v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.00591v2)
- **Published**: 2021-05-03 01:08:34+00:00
- **Updated**: 2021-08-09 16:56:25+00:00
- **Authors**: Juliano S. Assine, J. C. S. Santos Filho, Eduardo Valle
- **Comment**: None
- **Journal**: None
- **Summary**: In the past few years, mobile deep-learning deployment progressed by leaps and bounds, but solutions still struggle to accommodate its severe and fluctuating operational restrictions, which include bandwidth, latency, computation, and energy. In this work, we help to bridge that gap, introducing the first configurable solution for object detection that manages the triple communication-computation-accuracy trade-off with a single set of weights. Our solution shows state-of-the-art results on COCO-2017, adding only a minor penalty on the base EfficientDet-D2 architecture. Our design is robust to the choice of base architecture and compressor and should adapt well for future architectures.



### Physical world assistive signals for deep neural network classifiers -- neither defense nor attack
- **Arxiv ID**: http://arxiv.org/abs/2105.00622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00622v1)
- **Published**: 2021-05-03 04:02:48+00:00
- **Updated**: 2021-05-03 04:02:48+00:00
- **Authors**: Camilo Pestana, Wei Liu, David Glance, Robyn Owens, Ajmal Mian
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks lead the state of the art of computer vision tasks. Despite this, Neural Networks are brittle in that small changes in the input can drastically affect their prediction outcome and confidence. Consequently and naturally, research in this area mainly focus on adversarial attacks and defenses. In this paper, we take an alternative stance and introduce the concept of Assistive Signals, which are optimized to improve a model's confidence score regardless if it's under attack or not. We analyse some interesting properties of these assistive perturbations and extend the idea to optimize assistive signals in the 3D space for real-life scenarios simulating different lighting conditions and viewing angles. Experimental evaluations show that the assistive signals generated by our optimization method increase the accuracy and confidence of deep models more than those generated by conventional methods that work in the 2D space. In addition, our Assistive Signals illustrate the intrinsic bias of ML models towards certain patterns in real-life objects. We discuss how we can exploit these insights to re-think, or avoid, some patterns that might contribute to, or degrade, the detectability of objects in the real-world.



### Black-Box Dissector: Towards Erasing-based Hard-Label Model Stealing Attack
- **Arxiv ID**: http://arxiv.org/abs/2105.00623v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00623v3)
- **Published**: 2021-05-03 04:12:31+00:00
- **Updated**: 2022-09-26 15:31:11+00:00
- **Authors**: Yixu Wang, Jie Li, Hong Liu, Yan Wang, Yongjian Wu, Feiyue Huang, Rongrong Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Previous studies have verified that the functionality of black-box models can be stolen with full probability outputs. However, under the more practical hard-label setting, we observe that existing methods suffer from catastrophic performance degradation. We argue this is due to the lack of rich information in the probability prediction and the overfitting caused by hard labels. To this end, we propose a novel hard-label model stealing method termed \emph{black-box dissector}, which consists of two erasing-based modules. One is a CAM-driven erasing strategy that is designed to increase the information capacity hidden in hard labels from the victim model. The other is a random-erasing-based self-knowledge distillation module that utilizes soft labels from the substitute model to mitigate overfitting. Extensive experiments on four widely-used datasets consistently demonstrate that our method outperforms state-of-the-art methods, with an improvement of at most $8.27\%$. We also validate the effectiveness and practical potential of our method on real-world APIs and defense methods. Furthermore, our method promotes other downstream tasks, \emph{i.e.}, transfer adversarial attacks.



### EQFace: A Simple Explicit Quality Network for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2105.00634v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00634v1)
- **Published**: 2021-05-03 05:45:57+00:00
- **Updated**: 2021-05-03 05:45:57+00:00
- **Authors**: Rushuai Liu, Weijun Tan
- **Comment**: Accepted to CVPR 2021 AMFG Workshop
- **Journal**: None
- **Summary**: As the deep learning makes big progresses in still-image face recognition, unconstrained video face recognition is still a challenging task due to low quality face images caused by pose, blur, occlusion, illumination etc. In this paper we propose a network for face recognition which gives an explicit and quantitative quality score at the same time when a feature vector is extracted. To our knowledge this is the first network that implements these two functions in one network online. This network is very simple by adding a quality network branch to the baseline network of face recognition. It does not require training datasets with annotated face quality labels. We evaluate this network on both still-image face datasets and video face datasets and achieve the state-of-the-art performance in many cases. This network enables a lot of applications where an explicit face quality scpre is used. We demonstrate three applications of the explicit face quality, one of which is a progressive feature aggregation scheme in online video face recognition. We design an experiment to prove the benefits of using the face quality in this application. Code will be available at \url{https://github.com/deepcam-cn/facequality}.



### Learning to drive from a world on rails
- **Arxiv ID**: http://arxiv.org/abs/2105.00636v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.00636v3)
- **Published**: 2021-05-03 05:55:30+00:00
- **Updated**: 2021-10-02 19:13:59+00:00
- **Authors**: Dian Chen, Vladlen Koltun, Philipp Krähenbühl
- **Comment**: Paper published in ICCV 2021(Oral); Code and data available at:
  https://dotchen.github.io/world_on_rails/
- **Journal**: None
- **Summary**: We learn an interactive vision-based driving policy from pre-recorded driving logs via a model-based approach. A forward model of the world supervises a driving policy that predicts the outcome of any potential driving trajectory. To support learning from pre-recorded logs, we assume that the world is on rails, meaning neither the agent nor its actions influence the environment. This assumption greatly simplifies the learning problem, factorizing the dynamics into a nonreactive world model and a low-dimensional and compact forward model of the ego-vehicle. Our approach computes action-values for each training trajectory using a tabular dynamic-programming evaluation of the Bellman equations; these action-values in turn supervise the final vision-based driving policy. Despite the world-on-rails assumption, the final driving policy acts well in a dynamic and reactive world. At the time of writing, our method ranks first on the CARLA leaderboard, attaining a 25% higher driving score while using 40 times less data. Our method is also an order of magnitude more sample-efficient than state-of-the-art model-free reinforcement learning techniques on navigational tasks in the ProcGen benchmark.



### ISTR: End-to-End Instance Segmentation with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2105.00637v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00637v2)
- **Published**: 2021-05-03 06:00:09+00:00
- **Updated**: 2021-05-06 03:10:33+00:00
- **Authors**: Jie Hu, Liujuan Cao, Yao Lu, ShengChuan Zhang, Yan Wang, Ke Li, Feiyue Huang, Ling Shao, Rongrong Ji
- **Comment**: None
- **Journal**: None
- **Summary**: End-to-end paradigms significantly improve the accuracy of various deep-learning-based computer vision models. To this end, tasks like object detection have been upgraded by replacing non-end-to-end components, such as removing non-maximum suppression by training with a set loss based on bipartite matching. However, such an upgrade is not applicable to instance segmentation, due to its significantly higher output dimensions compared to object detection. In this paper, we propose an instance segmentation Transformer, termed ISTR, which is the first end-to-end framework of its kind. ISTR predicts low-dimensional mask embeddings, and matches them with ground truth mask embeddings for the set loss. Besides, ISTR concurrently conducts detection and segmentation with a recurrent refinement strategy, which provides a new way to achieve instance segmentation compared to the existing top-down and bottom-up frameworks. Benefiting from the proposed end-to-end mechanism, ISTR demonstrates state-of-the-art performance even with approximation-based suboptimal embeddings. Specifically, ISTR obtains a 46.8/38.6 box/mask AP using ResNet50-FPN, and a 48.1/39.9 box/mask AP using ResNet101-FPN, on the MS COCO dataset. Quantitative and qualitative results reveal the promising potential of ISTR as a solid baseline for instance-level recognition. Code has been made available at: https://github.com/hujiecpp/ISTR.



### S3Net: A Single Stream Structure for Depth Guided Image Relighting
- **Arxiv ID**: http://arxiv.org/abs/2105.00681v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00681v2)
- **Published**: 2021-05-03 08:33:53+00:00
- **Updated**: 2021-05-05 02:09:53+00:00
- **Authors**: Hao-Hsiang Yang, Wei-Ting Chen, and Sy-Yen Kuo
- **Comment**: Accepted by CVPRW 2021. This solution obtains the 3 rd position in
  the NTIRE 2021 Depth Guided Any-to-any Relighting Challenge
- **Journal**: None
- **Summary**: Depth guided any-to-any image relighting aims to generate a relit image from the original image and corresponding depth maps to match the illumination setting of the given guided image and its depth map. To the best of our knowledge, this task is a new challenge that has not been addressed in the previous literature. To address this issue, we propose a deep learning-based neural Single Stream Structure network called S3Net for depth guided image relighting. This network is an encoder-decoder model. We concatenate all images and corresponding depth maps as the input and feed them into the model. The decoder part contains the attention module and the enhanced module to focus on the relighting-related regions in the guided images. Experiments performed on challenging benchmark show that the proposed model achieves the 3 rd highest SSIM in the NTIRE 2021 Depth Guided Any-to-any Relighting Challenge.



### Multi-modal Bifurcated Network for Depth Guided Image Relighting
- **Arxiv ID**: http://arxiv.org/abs/2105.00690v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00690v2)
- **Published**: 2021-05-03 08:52:25+00:00
- **Updated**: 2021-05-05 02:13:15+00:00
- **Authors**: Hao-Hsiang Yang, Wei-Ting Chen, Hao-Lun Luo, Sy-Yen Kuo
- **Comment**: Accepted by CVPRW 2021. This solution is the winner in the NTIRE 2021
  Depth Guide One-to-one Relighting Challenge
- **Journal**: None
- **Summary**: Image relighting aims to recalibrate the illumination setting in an image. In this paper, we propose a deep learning-based method called multi-modal bifurcated network (MBNet) for depth guided image relighting. That is, given an image and the corresponding depth maps, a new image with the given illuminant angle and color temperature is generated by our network. This model extracts the image and the depth features by the bifurcated network in the encoder. To use the two features effectively, we adopt the dynamic dilated pyramid modules in the decoder. Moreover, to increase the variety of training data, we propose a novel data process pipeline to increase the number of the training data. Experiments conducted on the VIDIT dataset show that the proposed solution obtains the \textbf{1}$^{st}$ place in terms of SSIM and PMS in the NTIRE 2021 Depth Guide One-to-one Relighting Challenge.



### Exploiting Audio-Visual Consistency with Partial Supervision for Spatial Audio Generation
- **Arxiv ID**: http://arxiv.org/abs/2105.00708v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.MM, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2105.00708v1)
- **Published**: 2021-05-03 09:34:11+00:00
- **Updated**: 2021-05-03 09:34:11+00:00
- **Authors**: Yan-Bo Lin, Yu-Chiang Frank Wang
- **Comment**: AAAI'21
- **Journal**: None
- **Summary**: Human perceives rich auditory experience with distinct sound heard by ears. Videos recorded with binaural audio particular simulate how human receives ambient sound. However, a large number of videos are with monaural audio only, which would degrade the user experience due to the lack of ambient information. To address this issue, we propose an audio spatialization framework to convert a monaural video into a binaural one exploiting the relationship across audio and visual components. By preserving the left-right consistency in both audio and visual modalities, our learning strategy can be viewed as a self-supervised learning technique, and alleviates the dependency on a large amount of video data with ground truth binaural audio data during training. Experiments on benchmark datasets confirm the effectiveness of our proposed framework in both semi-supervised and fully supervised scenarios, with ablation studies and visualization further support the use of our model for audio spatialization.



### Synthetic Data for Model Selection
- **Arxiv ID**: http://arxiv.org/abs/2105.00717v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00717v2)
- **Published**: 2021-05-03 09:52:03+00:00
- **Updated**: 2023-07-05 15:59:52+00:00
- **Authors**: Alon Shoshan, Nadav Bhonker, Igor Kviatkovsky, Matan Fintz, Gerard Medioni
- **Comment**: None
- **Journal**: None
- **Summary**: Recent breakthroughs in synthetic data generation approaches made it possible to produce highly photorealistic images which are hardly distinguishable from real ones. Furthermore, synthetic generation pipelines have the potential to generate an unlimited number of images. The combination of high photorealism and scale turn synthetic data into a promising candidate for improving various machine learning (ML) pipelines. Thus far, a large body of research in this field has focused on using synthetic images for training, by augmenting and enlarging training data. In contrast to using synthetic data for training, in this work we explore whether synthetic data can be beneficial for model selection. Considering the task of image classification, we demonstrate that when data is scarce, synthetic data can be used to replace the held out validation set, thus allowing to train on a larger dataset. We also introduce a novel method to calibrate the synthetic error estimation to fit that of the real domain. We show that such calibration significantly improves the usefulness of synthetic data for model selection.



### Spectral Machine Learning for Pancreatic Mass Imaging Classification
- **Arxiv ID**: http://arxiv.org/abs/2105.00728v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML, 62P10
- **Links**: [PDF](http://arxiv.org/pdf/2105.00728v1)
- **Published**: 2021-05-03 10:17:32+00:00
- **Updated**: 2021-05-03 10:17:32+00:00
- **Authors**: Yiming Liu, Ying Chen, Guangming Pan, Weichung Wang, Wei-Chih Liao, Yee Liang Thian, Cheng E. Chee, Constantinos P. Anastassiades
- **Comment**: 17 pages, 3 figures
- **Journal**: None
- **Summary**: We present a novel spectral machine learning (SML) method in screening for pancreatic mass using CT imaging. Our algorithm is trained with approximately 30,000 images from 250 patients (50 patients with normal pancreas and 200 patients with abnormal pancreas findings) based on public data sources. A test accuracy of 94.6 percents was achieved in the out-of-sample diagnosis classification based on a total of approximately 15,000 images from 113 patients, whereby 26 out of 32 patients with normal pancreas and all 81 patients with abnormal pancreas findings were correctly diagnosed. SML is able to automatically choose fundamental images (on average 5 or 9 images for each patient) in the diagnosis classification and achieve the above mentioned accuracy. The computational time is 75 seconds for diagnosing 113 patients in a laptop with standard CPU running environment. Factors that influenced high performance of a well-designed integration of spectral learning and machine learning included: 1) use of eigenvectors corresponding to several of the largest eigenvalues of sample covariance matrix (spike eigenvectors) to choose input attributes in classification training, taking into account only the fundamental information of the raw images with less noise; 2) removal of irrelevant pixels based on mean-level spectral test to lower the challenges of memory capacity and enhance computational efficiency while maintaining superior classification accuracy; 3) adoption of state-of-the-art machine learning classification, gradient boosting and random forest. Our methodology showcases practical utility and improved accuracy of image diagnosis in pancreatic mass screening in the era of AI.



### Recognition of Oracle Bone Inscriptions by using Two Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2105.00777v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.00777v2)
- **Published**: 2021-05-03 12:31:57+00:00
- **Updated**: 2021-05-04 05:23:14+00:00
- **Authors**: Yoshiyuki Fujikawa, Hengyi Li, Xuebin Yue, Aravinda C V, Amar Prabhu G, Lin Meng
- **Comment**: None
- **Journal**: None
- **Summary**: Oracle bone inscriptions (OBIs) contain some of the oldest characters in the world and were used in China about 3000 years ago. As an ancient form of literature, OBIs store a lot of information that can help us understand the world history, character evaluations, and more. However, as OBIs were found only discovered about 120 years ago, few studies have described them, and the aging process has made the inscriptions less legible. Hence, automatic character detection and recognition has become an important issue. This paper aims to design a online OBI recognition system for helping preservation and organization the cultural heritage. We evaluated two deep learning models for OBI recognition, and have designed an API that can be accessed online for OBI recognition. In the first stage, you only look once (YOLO) is applied for detecting and recognizing OBIs. However, not all of the OBIs can be detected correctly by YOLO, so we next utilize MobileNet to recognize the undetected OBIs by manually cropping the undetected OBI in the image. MobileNet is used for this second stage of recognition as our evaluation of ten state-of-the-art models showed that it is the best network for OBI recognition due to its superior performance in terms of accuracy, loss and time consumption. We installed our system on an application programming interface (API) and opened it for OBI detection and recognition.



### Weakly supervised deep learning-based intracranial hemorrhage localization
- **Arxiv ID**: http://arxiv.org/abs/2105.00781v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2105.00781v1)
- **Published**: 2021-05-03 12:37:23+00:00
- **Updated**: 2021-05-03 12:37:23+00:00
- **Authors**: Jakub Nemcek, Tomas Vicar, Roman Jakubicek
- **Comment**: 4 pages, 2 figures, Submitted to EMBC 2021 - paper has not been
  reviewed yet
- **Journal**: None
- **Summary**: Intracranial hemorrhage is a life-threatening disease, which requires fast medical intervention. Owing to the duration of data annotation, head CT images are usually available only with slice-level labeling. This paper presents a weakly supervised method of precise hemorrhage localization in axial slices using only position-free labels, which is based on multiple instance learning. An algorithm is introduced that generates hemorrhage likelihood maps and finds the coordinates of bleeding. The Dice coefficient of 58.08 % is achieved on data from a publicly available dataset.



### Improving Landslide Detection on SAR Data through Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.00782v1
- **DOI**: 10.1109/LGRS.2021.3127073
- **Categories**: **cs.CV**, cs.LG, 65D19, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2105.00782v1)
- **Published**: 2021-05-03 12:37:57+00:00
- **Updated**: 2021-05-03 12:37:57+00:00
- **Authors**: Lorenzo Nava, Oriol Monserrat, Filippo Catani
- **Comment**: 8 pages, 2 figures, 3 tables
- **Journal**: None
- **Summary**: In this letter, we use deep-learning convolution neural networks (CNNs) to assess the landslide mapping and classification performances on optical images (from Sentinel-2) and SAR images (from Sentinel-1). The training and test zones used to independently evaluate the performance of the CNNs on different datasets are located in the eastern Iburi subprefecture in Hokkaido, where, at 03.08 local time (JST) on September 6, 2018, an Mw 6.6 earthquake triggered about 8000 coseismic landslides. We analyzed the conditions before and after the earthquake exploiting multi-polarization SAR as well as optical data by means of a CNN implemented in TensorFlow that points out the locations where the Landslide class is predicted as more likely. As expected, the CNN run on optical images proved itself excellent for the landslide detection task, achieving an overall accuracy of 99.20% while CNNs based on the combination of ground range detected (GRD) SAR data reached overall accuracies beyond 94%. Our findings show that the integrated use of SAR data may also allow for rapid mapping even during storms and under dense cloud cover and seems to provide comparable accuracy to classical optical change detection in landslide recognition and mapping.



### Robust 3D Cell Segmentation: Extending the View of Cellpose
- **Arxiv ID**: http://arxiv.org/abs/2105.00794v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.00794v3)
- **Published**: 2021-05-03 12:47:41+00:00
- **Updated**: 2022-02-01 11:09:22+00:00
- **Authors**: Dennis Eschweiler, Richard S. Smith, Johannes Stegmaier
- **Comment**: None
- **Journal**: None
- **Summary**: Increasing data set sizes of 3D microscopy imaging experiments demand for an automation of segmentation processes to be able to extract meaningful biomedical information. Due to the shortage of annotated 3D image data that can be used for machine learning-based approaches, 3D segmentation approaches are required to be robust and to generalize well to unseen data. The Cellpose approach proposed by Stringer et al. proved to be such a generalist approach for cell instance segmentation tasks. In this paper, we extend the Cellpose approach to improve segmentation accuracy on 3D image data and we further show how the formulation of the gradient maps can be simplified while still being robust and reaching similar segmentation accuracy. The code is publicly available and was integrated into two established open-source applications that allow using the 3D extension of Cellpose without any programming knowledge.



### Beyond pixel-wise supervision for segmentation: A few global shape descriptors might be surprisingly good!
- **Arxiv ID**: http://arxiv.org/abs/2105.00859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00859v1)
- **Published**: 2021-05-03 13:44:36+00:00
- **Updated**: 2021-05-03 13:44:36+00:00
- **Authors**: Hoel Kervadec, Houda Bahig, Laurent Letourneau-Guillon, Jose Dolz, Ismail Ben Ayed
- **Comment**: Accepted at Medical Imaging with Deep Learning (MIDL) 2021
- **Journal**: None
- **Summary**: Standard losses for training deep segmentation networks could be seen as individual classifications of pixels, instead of supervising the global shape of the predicted segmentations. While effective, they require exact knowledge of the label of each pixel in an image.   This study investigates how effective global geometric shape descriptors could be, when used on their own as segmentation losses for training deep networks. Not only interesting theoretically, there exist deeper motivations to posing segmentation problems as a reconstruction of shape descriptors: Annotations to obtain approximations of low-order shape moments could be much less cumbersome than their full-mask counterparts, and anatomical priors could be readily encoded into invariant shape descriptions, which might alleviate the annotation burden. Also, and most importantly, we hypothesize that, given a task, certain shape descriptions might be invariant across image acquisition protocols/modalities and subject populations, which might open interesting research avenues for generalization in medical image segmentation.   We introduce and formulate a few shape descriptors in the context of deep segmentation, and evaluate their potential as standalone losses on two different challenging tasks. Inspired by recent works in constrained optimization for deep networks, we propose a way to use those descriptors to supervise segmentation, without any pixel-level label. Very surprisingly, as little as 4 descriptors values per class can approach the performance of a segmentation mask with 65k individual discrete labels. We also found that shape descriptors can be a valid way to encode anatomical priors about the task, enabling to leverage expert knowledge without additional annotations. Our implementation is publicly available and can be easily extended to other tasks and descriptors: https://github.com/hkervadec/shape_descriptors



### MemX: An Attention-Aware Smart Eyewear System for Personalized Moment Auto-capture
- **Arxiv ID**: http://arxiv.org/abs/2105.00916v4
- **DOI**: 10.1145/3463509
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2105.00916v4)
- **Published**: 2021-05-03 14:54:16+00:00
- **Updated**: 2021-10-09 13:09:28+00:00
- **Authors**: Yuhu Chang, Yingying Zhao, Mingzhi Dong, Yujiang Wang, Yutian Lu, Qin Lv, Robert P. Dick, Tun Lu, Ning Gu, Li Shang
- **Comment**: Proceedings of the ACM on Interactive, Mobile, Wearable and
  Ubiquitous Technologies (IMWUT)
- **Journal**: Proc. ACM Interact. Mob. Wearable Ubiquitous Technol., Volume 5
  Issue 2, Article 56. June 2021
- **Summary**: This work presents MemX: a biologically-inspired attention-aware eyewear system developed with the goal of pursuing the long-awaited vision of a personalized visual Memex. MemX captures human visual attention on the fly, analyzes the salient visual content, and records moments of personal interest in the form of compact video snippets. Accurate attentive scene detection and analysis on resource-constrained platforms is challenging because these tasks are computation and energy intensive. We propose a new temporal visual attention network that unifies human visual attention tracking and salient visual content analysis. Attention tracking focuses computation-intensive video analysis on salient regions, while video analysis makes human attention detection and tracking more accurate. Using the YouTube-VIS dataset and 30 participants, we experimentally show that MemX significantly improves the attention tracking accuracy over the eye-tracking-alone method, while maintaining high system energy efficiency. We have also conducted 11 in-field pilot studies across a range of daily usage scenarios, which demonstrate the feasibility and potential benefits of MemX.



### Bird-Area Water-Bodies Dataset (BAWD) and Predictive AI Model for Avian Botulism Outbreak (AVI-BoT)
- **Arxiv ID**: http://arxiv.org/abs/2105.00924v2
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.00924v2)
- **Published**: 2021-05-03 15:00:12+00:00
- **Updated**: 2022-11-17 08:00:45+00:00
- **Authors**: Narayani Bhatia, Devang Mahesh, Jashandeep Singh, Manan Suri
- **Comment**: None
- **Journal**: None
- **Summary**: Avian botulism is a paralytic bacterial disease in birds often leading to high fatality. In-vitro diagnostic techniques such as Mouse Bioassay, ELISA, PCR are usually non-preventive, post-mortem in nature, and require invasive sample collection from affected sites or dead birds. In this study, we build a first-ever multi-spectral, remote-sensing imagery based global Bird-Area Water-bodies Dataset (BAWD) (i.e. fused satellite images of warm-water lakes/marshy-lands or similar water-body sites that are important for avian fauna) backed by on-ground reporting evidence of outbreaks. BAWD consists of 16 topographically diverse global sites monitored over a time-span of 4 years (2016-2021). We propose a first-ever Artificial Intelligence based (AI) model to predict potential outbreak of Avian botulism called AVI-BoT (Aerosol Visible, Infra-red (NIR/SWIR) and Bands of Thermal). We also train and investigate a simpler (5-band) Causative-Factor model (based on prominent physiological factors reported in literature) to predict Avian botulism. AVI-BoT demonstrates a training accuracy of 0.96 and validation accuracy of 0.989 on BAWD, far superior in comparison to our model based on causative factors. We also perform an ablation study and perform a detailed feature-space analysis. We further analyze three test case study locations - Lower Klamath National Wildlife Refuge and Langvlei and Rondevlei lakes where an outbreak had occurred, and Pong Dam where an outbreak had not occurred and confirm predictions with on-ground reportings. The proposed technique presents a scale-able, low-cost, non-invasive methodology for continuous monitoring of bird-habitats against botulism outbreaks with the potential of saving valuable fauna lives.



### CMA-Net: A Cascaded Mutual Attention Network for Light Field Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.00949v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00949v5)
- **Published**: 2021-05-03 15:32:12+00:00
- **Updated**: 2021-12-07 10:14:19+00:00
- **Authors**: Yi Zhang, Lu Zhang, Wassim Hamidouche, Olivier Deforges
- **Comment**: 6 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: In the past few years, numerous deep learning methods have been proposed to address the task of segmenting salient objects from RGB images. However, these approaches depending on single modality fail to achieve the state-of-the-art performance on widely used light field salient object detection (SOD) datasets, which collect large-scale natural images and provide multiple modalities such as multi-view, micro-lens images and depth maps. Most recently proposed light field SOD methods have acquired improving detecting accuracy, yet still predict rough objects' structures and perform slow inference speed. To this end, we propose CMA-Net, which consists of two novel cascaded mutual attention modules aiming at fusing the high level features from the modalities of all-in-focus and depth. Our proposed CMA-Net outperforms 30 SOD methods on two widely applied light field benchmark datasets. Besides, the proposed CMA-Net is able to inference at the speed of 53 fps. Extensive quantitative and qualitative experiments illustrate both the effectiveness and efficiency of our CMA-Net.



### Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.00957v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.00957v2)
- **Published**: 2021-05-03 15:49:01+00:00
- **Updated**: 2021-05-11 02:17:33+00:00
- **Authors**: Tsung-Wei Ke, Jyh-Jing Hwang, Stella X. Yu
- **Comment**: In ICLR 2021. Webpage & Code:
  https://twke18.github.io/projects/spml.html
- **Journal**: None
- **Summary**: Weakly supervised segmentation requires assigning a label to every pixel based on training instances with partial annotations such as image-level tags, object bounding boxes, labeled points and scribbles. This task is challenging, as coarse annotations (tags, boxes) lack precise pixel localization whereas sparse annotations (points, scribbles) lack broad region coverage. Existing methods tackle these two types of weak supervision differently: Class activation maps are used to localize coarse labels and iteratively refine the segmentation model, whereas conditional random fields are used to propagate sparse labels to the entire image.   We formulate weakly supervised segmentation as a semi-supervised metric learning problem, where pixels of the same (different) semantics need to be mapped to the same (distinctive) features. We propose 4 types of contrastive relationships between pixels and segments in the feature space, capturing low-level image similarity, semantic annotation, co-occurrence, and feature affinity They act as priors; the pixel-wise feature can be learned from training images with any partial annotations in a data-driven fashion. In particular, unlabeled pixels in training images participate not only in data-driven grouping within each image, but also in discriminative feature learning within and across images. We deliver a universal weakly supervised segmenter with significant gains on Pascal VOC and DensePose. Our code is publicly available at https://github.com/twke18/SPML.



### Enhanced U-Net: A Feature Enhancement Network for Polyp Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.00999v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.00999v1)
- **Published**: 2021-05-03 16:46:26+00:00
- **Updated**: 2021-05-03 16:46:26+00:00
- **Authors**: Krushi Patel, Andres M. Bur, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Colonoscopy is a procedure to detect colorectal polyps which are the primary cause for developing colorectal cancer. However, polyp segmentation is a challenging task due to the diverse shape, size, color, and texture of polyps, shuttle difference between polyp and its background, as well as low contrast of the colonoscopic images. To address these challenges, we propose a feature enhancement network for accurate polyp segmentation in colonoscopy images. Specifically, the proposed network enhances the semantic information using the novel Semantic Feature Enhance Module (SFEM). Furthermore, instead of directly adding encoder features to the respective decoder layer, we introduce an Adaptive Global Context Module (AGCM), which focuses only on the encoder's significant and hard fine-grained features. The integration of these two modules improves the quality of features layer by layer, which in turn enhances the final feature representation. The proposed approach is evaluated on five colonoscopy datasets and demonstrates superior performance compared to other state-of-the-art models.



### Learning Graph Embeddings for Open World Compositional Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.01017v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01017v3)
- **Published**: 2021-05-03 17:08:21+00:00
- **Updated**: 2022-04-08 09:01:38+00:00
- **Authors**: Massimiliano Mancini, Muhammad Ferjad Naeem, Yongqin Xian, Zeynep Akata
- **Comment**: Accepted by T-PAMI in March, 2022. arXiv admin note: text overlap
  with arXiv:2101.12609
- **Journal**: None
- **Summary**: Compositional Zero-Shot learning (CZSL) aims to recognize unseen compositions of state and object visual primitives seen during training. A problem with standard CZSL is the assumption of knowing which unseen compositions will be available at test time. In this work, we overcome this assumption operating on the open world setting, where no limit is imposed on the compositional space at test time, and the search space contains a large number of unseen compositions. To address this problem, we propose a new approach, Compositional Cosine Graph Embeddings (Co-CGE), based on two principles. First, Co-CGE models the dependency between states, objects and their compositions through a graph convolutional neural network. The graph propagates information from seen to unseen concepts, improving their representations. Second, since not all unseen compositions are equally feasible, and less feasible ones may damage the learned representations, Co-CGE estimates a feasibility score for each unseen composition, using the scores as margins in a cosine similarity-based loss and as weights in the adjacency matrix of the graphs. Experiments show that our approach achieves state-of-the-art performances in standard CZSL while outperforming previous methods in the open world scenario.



### Initialization and Regularization of Factorized Neural Layers
- **Arxiv ID**: http://arxiv.org/abs/2105.01029v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.01029v2)
- **Published**: 2021-05-03 17:28:07+00:00
- **Updated**: 2022-10-04 19:00:39+00:00
- **Authors**: Mikhail Khodak, Neil Tenenholtz, Lester Mackey, Nicolò Fusi
- **Comment**: ICLR 2021 camera-ready, amended due to error pointed out in
  arXiv:2209.13569v1 (amendment shown in blue)
- **Journal**: None
- **Summary**: Factorized layers--operations parameterized by products of two or more matrices--occur in a variety of deep learning contexts, including compressed model training, certain types of knowledge distillation, and multi-head self-attention architectures. We study how to initialize and regularize deep nets containing such layers, examining two simple, understudied schemes, spectral initialization and Frobenius decay, for improving their performance. The guiding insight is to design optimization routines for these networks that are as close as possible to that of their well-tuned, non-decomposed counterparts; we back this intuition with an analysis of how the initialization and regularization schemes impact training with gradient descent, drawing on modern attempts to understand the interplay of weight-decay and batch-normalization. Empirically, we highlight the benefits of spectral initialization and Frobenius decay across a variety of settings. In model compression, we show that they enable low-rank methods to significantly outperform both unstructured sparsity and tensor methods on the task of training low-memory residual networks; analogs of the schemes also improve the performance of tensor decomposition techniques. For knowledge distillation, Frobenius decay enables a simple, overcomplete baseline that yields a compact model from over-parameterized training without requiring retraining with or pruning a teacher network. Finally, we show how both schemes applied to multi-head attention lead to improved performance on both translation and unsupervised pre-training.



### Act the Part: Learning Interaction Strategies for Articulated Object Part Discovery
- **Arxiv ID**: http://arxiv.org/abs/2105.01047v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01047v1)
- **Published**: 2021-05-03 17:48:29+00:00
- **Updated**: 2021-05-03 17:48:29+00:00
- **Authors**: Samir Yitzhak Gadre, Kiana Ehsani, Shuran Song
- **Comment**: 16 pages, 16 figures
- **Journal**: None
- **Summary**: People often use physical intuition when manipulating articulated objects, irrespective of object semantics. Motivated by this observation, we identify an important embodied task where an agent must play with objects to recover their parts. To this end, we introduce Act the Part (AtP) to learn how to interact with articulated objects to discover and segment their pieces. By coupling action selection and motion segmentation, AtP is able to isolate structures to make perceptual part recovery possible without semantic labels. Our experiments show AtP learns efficient strategies for part discovery, can generalize to unseen categories, and is capable of conditional reasoning for the task. Although trained in simulation, we show convincing transfer to real world data with no fine-tuning.



### Neural Monocular 3D Human Motion Capture with Physical Awareness
- **Arxiv ID**: http://arxiv.org/abs/2105.01057v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2105.01057v1)
- **Published**: 2021-05-03 17:57:07+00:00
- **Updated**: 2021-05-03 17:57:07+00:00
- **Authors**: Soshi Shimada, Vladislav Golyanik, Weipeng Xu, Patrick Pérez, Christian Theobalt
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new trainable system for physically plausible markerless 3D human motion capture, which achieves state-of-the-art results in a broad range of challenging scenarios. Unlike most neural methods for human motion capture, our approach, which we dub physionical, is aware of physical and environmental constraints. It combines in a fully differentiable way several key innovations, i.e., 1. a proportional-derivative controller, with gains predicted by a neural network, that reduces delays even in the presence of fast motions, 2. an explicit rigid body dynamics model and 3. a novel optimisation layer that prevents physically implausible foot-floor penetration as a hard constraint. The inputs to our system are 2D joint keypoints, which are canonicalised in a novel way so as to reduce the dependency on intrinsic camera parameters -- both at train and test time. This enables more accurate global translation estimation without generalisability loss. Our model can be finetuned only with 2D annotations when the 3D annotations are not available. It produces smooth and physically principled 3D motions in an interactive frame rate in a wide variety of challenging scenes, including newly recorded ones. Its advantages are especially noticeable on in-the-wild sequences that significantly differ from common 3D pose estimation benchmarks such as Human 3.6M and MPI-INF-3DHP. Qualitative results are available at http://gvv.mpi-inf.mpg.de/projects/PhysAware/



### A Dataset and System for Real-Time Gun Detection in Surveillance Video Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.01058v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01058v2)
- **Published**: 2021-05-03 17:58:45+00:00
- **Updated**: 2021-08-16 15:16:34+00:00
- **Authors**: Delong Qi, Weijun Tan, Zhifu Liu, Qi Yao, Jingfeng Liu
- **Comment**: IEEE SMC 2021 Oral
- **Journal**: None
- **Summary**: Gun violence is a severe problem in the world, particularly in the United States. Deep learning methods have been studied to detect guns in surveillance video cameras or smart IP cameras and to send a real-time alert to security personals. One problem for the development of gun detection algorithms is the lack of large public datasets. In this work, we first publish a dataset with 51K annotated gun images for gun detection and other 51K cropped gun chip images for gun classification we collect from a few different sources. To our knowledge, this is the largest dataset for the study of gun detection. This dataset can be downloaded at www.linksprite.com/gun-detection-datasets. We present a gun detection system using a smart IP camera as an embedded edge device, and a cloud server as a manager for device, data, alert, and to further reduce the false positive rate. We study to find solutions for gun detection in an embedded device, and for gun classification on the edge device and the cloud server. This edge/cloud framework makes the deployment of gun detection in the real world possible.



### Curious Representation Learning for Embodied Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2105.01060v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.01060v2)
- **Published**: 2021-05-03 17:59:20+00:00
- **Updated**: 2021-08-31 01:43:15+00:00
- **Authors**: Yilun Du, Chuang Gan, Phillip Isola
- **Comment**: To apear at ICCV 2021. Code is available at
  https://yilundu.github.io/crl
- **Journal**: None
- **Summary**: Self-supervised representation learning has achieved remarkable success in recent years. By subverting the need for supervised labels, such approaches are able to utilize the numerous unlabeled images that exist on the Internet and in photographic datasets. Yet to build truly intelligent agents, we must construct representation learning algorithms that can learn not only from datasets but also learn from environments. An agent in a natural environment will not typically be fed curated data. Instead, it must explore its environment to acquire the data it will learn from. We propose a framework, curious representation learning (CRL), which jointly learns a reinforcement learning policy and a visual representation model. The policy is trained to maximize the error of the representation learner, and in doing so is incentivized to explore its environment. At the same time, the learned representation becomes stronger and stronger as the policy feeds it ever harder data to learn from. Our learned representations enable promising transfer to downstream navigation tasks, performing better than or comparably to ImageNet pretraining without using any supervision at all. In addition, despite being trained in simulation, our learned representations can obtain interpretable results on real images. Code is available at https://yilundu.github.io/crl/.



### Collision Replay: What Does Bumping Into Things Tell You About Scene Geometry?
- **Arxiv ID**: http://arxiv.org/abs/2105.01061v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01061v1)
- **Published**: 2021-05-03 17:59:46+00:00
- **Updated**: 2021-05-03 17:59:46+00:00
- **Authors**: Alexander Raistrick, Nilesh Kulkarni, David F. Fouhey
- **Comment**: None
- **Journal**: None
- **Summary**: What does bumping into things in a scene tell you about scene geometry? In this paper, we investigate the idea of learning from collisions. At the heart of our approach is the idea of collision replay, where we use examples of a collision to provide supervision for observations at a past frame. We use collision replay to train convolutional neural networks to predict a distribution over collision time from new images. This distribution conveys information about the navigational affordances (e.g., corridors vs open spaces) and, as we show, can be converted into the distance function for the scene geometry. We analyze this approach with an agent that has noisy actuation in a photorealistic simulator.



### Towards A Multi-agent System for Online Hate Speech Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.01129v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2105.01129v1)
- **Published**: 2021-05-03 19:06:42+00:00
- **Updated**: 2021-05-03 19:06:42+00:00
- **Authors**: Gaurav Sahu, Robin Cohen, Olga Vechtomova
- **Comment**: Accepted to the 2nd International Workshop on Autonomous Agents for
  Social Good (AASG), AAMAS, 2021
- **Journal**: None
- **Summary**: This paper envisions a multi-agent system for detecting the presence of hate speech in online social media platforms such as Twitter and Facebook. We introduce a novel framework employing deep learning techniques to coordinate the channels of textual and im-age processing. Our experimental results aim to demonstrate the effectiveness of our methods for classifying online content, training the proposed neural network model to effectively detect hateful instances in the input. We conclude with a discussion of how our system may be of use to provide recommendations to users who are managing online social networks, showcasing the immense potential of intelligent multi-agent systems towards delivering social good.



### Prediction of clinical tremor severity using Rank Consistent Ordinal Regression
- **Arxiv ID**: http://arxiv.org/abs/2105.01133v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01133v1)
- **Published**: 2021-05-03 19:22:05+00:00
- **Updated**: 2021-05-03 19:22:05+00:00
- **Authors**: Li Zhang, Vijay Yadav, Vidya Koesmahargyo, Anzar Abbas, Isaac Galatzer-Levy
- **Comment**: None
- **Journal**: None
- **Summary**: Tremor is a key diagnostic feature of Parkinson's Disease (PD), Essential Tremor (ET), and other central nervous system (CNS) disorders. Clinicians or trained raters assess tremor severity with TETRAS scores by observing patients. Lacking quantitative measures, inter- or intra- observer variabilities are almost inevitable as the distinction between adjacent tremor scores is subtle. Moreover, clinician assessments also require patient visits, which limits the frequency of disease progress evaluation. Therefore it is beneficial to develop an automated assessment that can be performed remotely and repeatably at patients' convenience for continuous monitoring. In this work, we proposed to train a deep neural network (DNN) with rank-consistent ordinal regression using 276 clinical videos from 36 essential tremor patients. The videos are coupled with clinician assessed TETRAS scores, which are used as ground truth labels to train the DNN. To tackle the challenge of limited training data, optical flows are used to eliminate irrelevant background and statistic objects from RGB frames. In addition to optical flows, transfer learning is also applied to leverage pre-trained network weights from a related task of tremor frequency estimate. The approach was evaluated by splitting the clinical videos into training (67%) and testing sets (0.33%). The mean absolute error on TETRAS score of the testing results is 0.45, indicating that most of the errors were from the mismatch of adjacent labels, which is expected and acceptable. The model predications also agree well with clinical ratings. This model is further applied to smart phone videos collected from a PD patient who has an implanted device to turn "On" or "Off" tremor. The model outputs were consistent with the patient tremor states. The results demonstrate that our trained model can be used as a means to assess and track tremor severity.



### Sketches image analysis: Web image search engine usingLSH index and DNN InceptionV3
- **Arxiv ID**: http://arxiv.org/abs/2105.01147v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01147v1)
- **Published**: 2021-05-03 20:01:54+00:00
- **Updated**: 2021-05-03 20:01:54+00:00
- **Authors**: Alessio Schiavo, Filippo Minutella, Mattia Daole, Marsha Gomez Gomez
- **Comment**: None
- **Journal**: None
- **Summary**: The adoption of an appropriate approximate similarity search method is an essential prereq-uisite for developing a fast and efficient CBIR system, especially when dealing with large amount ofdata. In this study we implement a web image search engine on top of a Locality Sensitive Hashing(LSH) Index to allow fast similarity search on deep features. Specifically, we exploit transfer learningfor deep features extraction from images. Firstly, we adopt InceptionV3 pretrained on ImageNet asfeatures extractor, secondly, we try out several CNNs built on top of InceptionV3 as convolutionalbase fine-tuned on our dataset. In both of the previous cases we index the features extracted within ourLSH index implementation so as to compare the retrieval performances with and without fine-tuning.In our approach we try out two different LSH implementations: the first one working with real numberfeature vectors and the second one with the binary transposed version of those vectors. Interestingly,we obtain the best performances when using the binary LSH, reaching almost the same result, in termsof mean average precision, obtained by performing sequential scan of the features, thus avoiding thebias introduced by the LSH index. Lastly, we carry out a performance analysis class by class in terms ofrecall againstmAPhighlighting, as expected, a strong positive correlation between the two.



### Pedestrian Detection in 3D Point Clouds using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2105.01151v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2105.01151v1)
- **Published**: 2021-05-03 20:12:11+00:00
- **Updated**: 2021-05-03 20:12:11+00:00
- **Authors**: Òscar Lorente, Josep R. Casas, Santiago Royo, Ivan Caminal
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting pedestrians is a crucial task in autonomous driving systems to ensure the safety of drivers and pedestrians. The technologies involved in these algorithms must be precise and reliable, regardless of environment conditions. Relying solely on RGB cameras may not be enough to recognize road environments in situations where cameras cannot capture scenes properly. Some approaches aim to compensate for these limitations by combining RGB cameras with TOF sensors, such as LIDARs. However, there are few works that address this problem using exclusively the 3D geometric information provided by LIDARs. In this paper, we propose a PointNet++ based architecture to detect pedestrians in dense 3D point clouds. The aim is to explore the potential contribution of geometric information alone in pedestrian detection systems. We also present a semi-automatic labeling system that transfers pedestrian and non-pedestrian labels from RGB images onto the 3D domain. The fact that our datasets have RGB registered with point clouds enables label transferring by back projection from 2D bounding boxes to point clouds, with only a light manual supervision to validate results. We train PointNet++ with the geometry of the resulting 3D labelled clusters. The evaluation confirms the effectiveness of the proposed method, yielding precision and recall values around 98%.



### Automated Estimation of Total Lung Volume using Chest Radiographs and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.01181v1
- **DOI**: 10.1002/mp.15655
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.01181v1)
- **Published**: 2021-05-03 21:35:16+00:00
- **Updated**: 2021-05-03 21:35:16+00:00
- **Authors**: Ecem Sogancioglu, Keelin Murphy, Ernst Th. Scholten, Luuk H. Boulogne, Mathias Prokop, Bram van Ginneken
- **Comment**: Under review
- **Journal**: None
- **Summary**: Total lung volume is an important quantitative biomarker and is used for the assessment of restrictive lung diseases. In this study, we investigate the performance of several deep-learning approaches for automated measurement of total lung volume from chest radiographs. 7621 posteroanterior and lateral view chest radiographs (CXR) were collected from patients with chest CT available. Similarly, 928 CXR studies were chosen from patients with pulmonary function test (PFT) results. The reference total lung volume was calculated from lung segmentation on CT or PFT data, respectively. This dataset was used to train deep-learning architectures to predict total lung volume from chest radiographs. The experiments were constructed in a step-wise fashion with increasing complexity to demonstrate the effect of training with CT-derived labels only and the sources of error. The optimal models were tested on 291 CXR studies with reference lung volume obtained from PFT. The optimal deep-learning regression model showed an MAE of 408 ml and a MAPE of 8.1\% and Pearson's r = 0.92 using both frontal and lateral chest radiographs as input. CT-derived labels were useful for pre-training but the optimal performance was obtained by fine-tuning the network with PFT-derived labels. We demonstrate, for the first time, that state-of-the-art deep learning solutions can accurately measure total lung volume from plain chest radiographs. The proposed model can be used to obtain total lung volume from routinely acquired chest radiographs at no additional cost and could be a useful tool to identify trends over time in patients referred regularly for chest x-rays.



### Weighted Least Squares Twin Support Vector Machine with Fuzzy Rough Set Theory for Imbalanced Data Classification
- **Arxiv ID**: http://arxiv.org/abs/2105.01198v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.01198v2)
- **Published**: 2021-05-03 22:33:39+00:00
- **Updated**: 2021-05-21 20:29:05+00:00
- **Authors**: Maysam Behmanesh, Peyman Adibi, Hossein Karshenas
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Support vector machines (SVMs) are powerful supervised learning tools developed to solve classification problems. However, SVMs are likely to perform poorly in the classification of imbalanced data. The rough set theory presents a mathematical tool for inference in nondeterministic cases that provides methods for removing irrelevant information from data. In this work, we propose an approach that efficiently used fuzzy rough set theory in weighted least squares twin support vector machine called FRLSTSVM for classification of imbalanced data. The first innovation is introducing a new fuzzy rough set-based under-sampling strategy to make the classifier robust in terms of the imbalanced data. For constructing the two proximal hyperplanes in FRLSTSVM, data points from the minority class remain unchanged while a subset of data points in the majority class are selected using a new method. In this model, we embed the weight biases in the LSTSVM formulations to overcome the bias phenomenon in the original twin SVM for the classification of imbalanced data. In order to determine these weights in this formulation, we introduce a new strategy that uses fuzzy rough set theory as the second innovation. Experimental results on the famous imbalanced datasets, compared to the related traditional SVM-based methods, demonstrate the superiority of the proposed FRLSTSVM model in the imbalanced data classification.



### Event Camera Simulator Design for Modeling Attention-based Inference Architectures
- **Arxiv ID**: http://arxiv.org/abs/2105.01203v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.01203v1)
- **Published**: 2021-05-03 22:41:45+00:00
- **Updated**: 2021-05-03 22:41:45+00:00
- **Authors**: Md Jubaer Hossain Pantho, Joel Mandebi Mbongue, Pankaj Bhowmik, Christophe Bobda
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, there has been a growing interest in realizing methodologies to integrate more and more computation at the level of the image sensor. The rising trend has seen an increased research interest in developing novel event cameras that can facilitate CNN computation directly in the sensor. However, event-based cameras are not generally available in the market, limiting performance exploration on high-level models and algorithms. This paper presents an event camera simulator that can be a potent tool for hardware design prototyping, parameter optimization, attention-based innovative algorithm development, and benchmarking. The proposed simulator implements a distributed computation model to identify relevant regions in an image frame. Our simulator's relevance computation model is realized as a collection of modules and performs computations in parallel. The distributed computation model is configurable, making it highly useful for design space exploration. The Rendering engine of the simulator samples frame-regions only when there is a new event. The simulator closely emulates an image processing pipeline similar to that of physical cameras. Our experimental results show that the simulator can effectively emulate event vision with low overheads.



### Multi-Target Multi-Camera Tracking of Vehicles using Metadata-Aided Re-ID and Trajectory-Based Camera Link Model
- **Arxiv ID**: http://arxiv.org/abs/2105.01213v1
- **DOI**: 10.1109/TIP.2021.3078124
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.01213v1)
- **Published**: 2021-05-03 23:20:37+00:00
- **Updated**: 2021-05-03 23:20:37+00:00
- **Authors**: Hung-Min Hsu, Jiarui Cai, Yizhou Wang, Jenq-Neng Hwang, Kwang-Ju Kim
- **Comment**: IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: In this paper, we propose a novel framework for multi-target multi-camera tracking (MTMCT) of vehicles based on metadata-aided re-identification (MA-ReID) and the trajectory-based camera link model (TCLM). Given a video sequence and the corresponding frame-by-frame vehicle detections, we first address the isolated tracklets issue from single camera tracking (SCT) by the proposed traffic-aware single-camera tracking (TSCT). Then, after automatically constructing the TCLM, we solve MTMCT by the MA-ReID. The TCLM is generated from camera topological configuration to obtain the spatial and temporal information to improve the performance of MTMCT by reducing the candidate search of ReID. We also use the temporal attention model to create more discriminative embeddings of trajectories from each camera to achieve robust distance measures for vehicle ReID. Moreover, we train a metadata classifier for MTMCT to obtain the metadata feature, which is concatenated with the temporal attention based embeddings. Finally, the TCLM and hierarchical clustering are jointly applied for global ID assignment. The proposed method is evaluated on the CityFlow dataset, achieving IDF1 76.77%, which outperforms the state-of-the-art MTMCT methods.



### Weakly-Supervised Universal Lesion Segmentation with Regional Level Set Loss
- **Arxiv ID**: http://arxiv.org/abs/2105.01218v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.01218v1)
- **Published**: 2021-05-03 23:33:37+00:00
- **Updated**: 2021-05-03 23:33:37+00:00
- **Authors**: Youbao Tang, Jinzheng Cai, Ke Yan, Lingyun Huang, Guotong Xie, Jing Xiao, Jingjing Lu, Gigin Lin, Le Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Accurately segmenting a variety of clinically significant lesions from whole body computed tomography (CT) scans is a critical task on precision oncology imaging, denoted as universal lesion segmentation (ULS). Manual annotation is the current clinical practice, being highly time-consuming and inconsistent on tumor's longitudinal assessment. Effectively training an automatic segmentation model is desirable but relies heavily on a large number of pixel-wise labelled data. Existing weakly-supervised segmentation approaches often struggle with regions nearby the lesion boundaries. In this paper, we present a novel weakly-supervised universal lesion segmentation method by building an attention enhanced model based on the High-Resolution Network (HRNet), named AHRNet, and propose a regional level set (RLS) loss for optimizing lesion boundary delineation. AHRNet provides advanced high-resolution deep image features by involving a decoder, dual-attention and scale attention mechanisms, which are crucial to performing accurate lesion segmentation. RLS can optimize the model reliably and effectively in a weakly-supervised fashion, forcing the segmentation close to lesion boundary. Extensive experimental results demonstrate that our method achieves the best performance on the publicly large-scale DeepLesion dataset and a hold-out test set.



