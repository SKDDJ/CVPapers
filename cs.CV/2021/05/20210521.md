# Arxiv Papers in cs.CV on 2021-05-21
### An Optical physics inspired CNN approach for intrinsic image decomposition
- **Arxiv ID**: http://arxiv.org/abs/2105.10076v2
- **DOI**: 10.1109/ICIP42928.2021.9506375
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10076v2)
- **Published**: 2021-05-21 00:54:01+00:00
- **Updated**: 2021-12-20 15:52:17+00:00
- **Authors**: Harshana Weligampola, Gihan Jayatilaka, Suren Sritharan, Parakrama Ekanayake, Roshan Ragel, Vijitha Herath, Roshan Godaliyadda
- **Comment**: 5 pages, 3 figures, 1 table, ICIP 2021
- **Journal**: IEEE International Conference on Image Processing (ICIP), 2021,
  pp. 1864-1868
- **Summary**: Intrinsic Image Decomposition is an open problem of generating the constituents of an image. Generating reflectance and shading from a single image is a challenging task specifically when there is no ground truth. There is a lack of unsupervised learning approaches for decomposing an image into reflectance and shading using a single image. We propose a neural network architecture capable of this decomposition using physics-based parameters derived from the image. Through experimental results, we show that (a) the proposed methodology outperforms the existing deep learning-based IID techniques and (b) the derived parameters improve the efficacy significantly. We conclude with a closer analysis of the results (numerical and example images) showing several avenues for improvement.



### An interpretable object detection based model for the diagnosis of neonatal lung diseases using Ultrasound images
- **Arxiv ID**: http://arxiv.org/abs/2105.10081v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10081v1)
- **Published**: 2021-05-21 01:12:35+00:00
- **Updated**: 2021-05-21 01:12:35+00:00
- **Authors**: Rodina Bassiouny, Adel Mohamed, Karthi Umapathy, Naimul Khan
- **Comment**: 7 pages, 8 figures, 4 tables, full paper has been submitted to the
  2021 43rd Annual International Conference of the IEEE Engineering in Medicine
  & Biology Society (EMBC) (EMBC 2021) at the Expo Guadalajara, Mexico, on Oct
  31 - Nov 4, 2021
- **Journal**: None
- **Summary**: Over the last few decades, Lung Ultrasound (LUS) has been increasingly used to diagnose and monitor different lung diseases in neonates. It is a non invasive tool that allows a fast bedside examination while minimally handling the neonate. Acquiring a LUS scan is easy, but understanding the artifacts concerned with each respiratory disease is challenging. Mixed artifact patterns found in different respiratory diseases may limit LUS readability by the operator. While machine learning (ML), especially deep learning can assist in automated analysis, simply feeding the ultrasound images to an ML model for diagnosis is not enough to earn the trust of medical professionals. The algorithm should output LUS features that are familiar to the operator instead. Therefore, in this paper we present a unique approach for extracting seven meaningful LUS features that can be easily associated with a specific pathological lung condition: Normal pleura, irregular pleura, thick pleura, Alines, Coalescent B-lines, Separate B-lines and Consolidations. These artifacts can lead to early prediction of infants developing later respiratory distress symptoms. A single multi-class region proposal-based object detection model faster-RCNN (fRCNN) was trained on lower posterior lung ultrasound videos to detect these LUS features which are further linked to four common neonatal diseases. Our results show that fRCNN surpasses single stage models such as RetinaNet and can successfully detect the aforementioned LUS features with a mean average precision of 86.4%. Instead of a fully automatic diagnosis from images without any interpretability, detection of such LUS features leave the ultimate control of diagnosis to the clinician, which can result in a more trustworthy intelligent system.



### DSR: Direct Simultaneous Registration for Multiple 3D Images
- **Arxiv ID**: http://arxiv.org/abs/2105.10087v2
- **DOI**: 10.1007/978-3-031-16446-0_10
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10087v2)
- **Published**: 2021-05-21 01:42:11+00:00
- **Updated**: 2022-08-15 07:01:56+00:00
- **Authors**: Zhehua Mao, Liang Zhao, Shoudong Huang, Yiting Fan, Alex Pui-Wai Lee
- **Comment**: 10 pages, 3 figures, The 25th International Conference on Medical
  Image Computing and Computer Assisted Intervention, MICCAI 2022
- **Journal**: Medical Image Computing and Computer Assisted Intervention (2022)
- **Summary**: This paper presents a novel algorithm named Direct Simultaneous Registration (DSR) that registers a collection of 3D images in a simultaneous fashion without specifying any reference image, feature extraction and matching, or information loss or reuse. The algorithm optimizes the global poses of local image frames by maximizing the similarity between a predefined panoramic image and local images. Although we formulate the problem as a Direct Bundle Adjustment (DBA) that jointly optimizes the poses of local frames and the intensities of the panoramic image, by investigating the independence of pose estimation from the panoramic image in the solving process, DSR is proposed to solve the poses only and proved to be able to obtain the same optimal poses as DBA. The proposed method is particularly suitable for the scenarios where distinct features are not available, such as Transesophageal Echocardiography (TEE) images. DSR is evaluated by comparing it with four widely used methods via simulated and in-vivo 3D TEE images. It is shown that the proposed method outperforms these four methods in terms of accuracy and requires much fewer computational resources than the state-of-the-art accumulated pairwise estimates (APE).



### EMface: Detecting Hard Faces by Exploring Receptive Field Pyraminds
- **Arxiv ID**: http://arxiv.org/abs/2105.10104v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.10104v1)
- **Published**: 2021-05-21 03:01:37+00:00
- **Updated**: 2021-05-21 03:01:37+00:00
- **Authors**: Leilei Cao, Yao Xiao, Lin Xu
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Scale variation is one of the most challenging problems in face detection. Modern face detectors employ feature pyramids to deal with scale variation. However, it might break the feature consistency across different scales of faces. In this paper, we propose a simple yet effective method named the receptive field pyramids (RFP) method to enhance the representation ability of feature pyramids. It can learn different receptive fields in each feature map adaptively based on the varying scales of detected faces. Empirical results on two face detection benchmark datasets, i.e., WIDER FACE and UFDD, demonstrate that our proposed method can accelerate the inference rate significantly while achieving state-of-the-art performance. The source code of our method is available at \url{https://github.com/emdata-ailab/EMface}.



### Guidance and Teaching Network for Video Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.10110v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10110v3)
- **Published**: 2021-05-21 03:25:38+00:00
- **Updated**: 2021-06-06 13:55:47+00:00
- **Authors**: Yingxia Jiao, Xiao Wang, Yu-Cheng Chou, Shouyuan Yang, Ge-Peng Ji, Rong Zhu, Ge Gao
- **Comment**: Accepted at IEEE ICIP 2021
- **Journal**: None
- **Summary**: Owing to the difficulties of mining spatial-temporal cues, the existing approaches for video salient object detection (VSOD) are limited in understanding complex and noisy scenarios, and often fail in inferring prominent objects. To alleviate such shortcomings, we propose a simple yet efficient architecture, termed Guidance and Teaching Network (GTNet), to independently distil effective spatial and temporal cues with implicit guidance and explicit teaching at feature- and decision-level, respectively. To be specific, we (a) introduce a temporal modulator to implicitly bridge features from motion into the appearance branch, which is capable of fusing cross-modal features collaboratively, and (b) utilise motion-guided mask to propagate the explicit cues during the feature aggregation. This novel learning strategy achieves satisfactory results via decoupling the complex spatial-temporal cues and mapping informative cues across different modalities. Extensive experiments on three challenging benchmarks show that the proposed method can run at ~28 fps on a single TITAN Xp GPU and perform competitively against 14 cutting-edge baselines.



### IDEAL: Independent Domain Embedding Augmentation Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.10112v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.10112v1)
- **Published**: 2021-05-21 03:40:24+00:00
- **Updated**: 2021-05-21 03:40:24+00:00
- **Authors**: Zhiyuan Chen, Guang Yao, Wennan Ma, Lin Xu
- **Comment**: 11 pages, 2 figures, 4 tables
- **Journal**: None
- **Summary**: Many efforts have been devoted to designing sampling, mining, and weighting strategies in high-level deep metric learning (DML) loss objectives. However, little attention has been paid to low-level but essential data transformation. In this paper, we develop a novel mechanism, the independent domain embedding augmentation learning ({IDEAL}) method. It can simultaneously learn multiple independent embedding spaces for multiple domains generated by predefined data transformations. Our IDEAL is orthogonal to existing DML techniques and can be seamlessly combined with prior DML approaches for enhanced performance. Empirical results on visual retrieval tasks demonstrate the superiority of the proposed method. For example, the IDEAL improves the performance of MS loss by a large margin, 84.5\% $\rightarrow$ 87.1\% on Cars-196, and 65.8\% $\rightarrow$ 69.5\% on CUB-200 at Recall$@1$. Our IDEAL with MS loss also achieves the new state-of-the-art performance on three image retrieval benchmarks, \ie, \emph{Cars-196}, \emph{CUB-200}, and \emph{SOP}. It outperforms the most recent DML approaches, such as Circle loss and XBM, significantly. The source code and pre-trained models of our method will be available at\emph{\url{https://github.com/emdata-ailab/IDEAL}}.



### Backdoor Attacks on Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.10123v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10123v3)
- **Published**: 2021-05-21 04:22:05+00:00
- **Updated**: 2022-06-09 00:18:53+00:00
- **Authors**: Aniruddha Saha, Ajinkya Tejankar, Soroush Abbasi Koohpayegani, Hamed Pirsiavash
- **Comment**: CVPR 2022 (Oral)
- **Journal**: None
- **Summary**: Large-scale unlabeled data has spurred recent progress in self-supervised learning methods that learn rich visual representations. State-of-the-art self-supervised methods for learning representations from images (e.g., MoCo, BYOL, MSF) use an inductive bias that random augmentations (e.g., random crops) of an image should produce similar embeddings. We show that such methods are vulnerable to backdoor attacks - where an attacker poisons a small part of the unlabeled data by adding a trigger (image patch chosen by the attacker) to the images. The model performance is good on clean test images, but the attacker can manipulate the decision of the model by showing the trigger at test time. Backdoor attacks have been studied extensively in supervised learning and to the best of our knowledge, we are the first to study them for self-supervised learning. Backdoor attacks are more practical in self-supervised learning, since the use of large unlabeled data makes data inspection to remove poisons prohibitive. We show that in our targeted attack, the attacker can produce many false positives for the target category by using the trigger at test time. We also propose a defense method based on knowledge distillation that succeeds in neutralizing the attack. Our code is available here: https://github.com/UMBCvision/SSL-Backdoor .



### A Novel 3D-UNet Deep Learning Framework Based on High-Dimensional Bilateral Grid for Edge Consistent Single Image Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2105.10129v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10129v1)
- **Published**: 2021-05-21 04:53:14+00:00
- **Updated**: 2021-05-21 04:53:14+00:00
- **Authors**: Mansi Sharma, Abheesht Sharma, Kadvekar Rohit Tushar, Avinash Panneer
- **Comment**: 8 pages, 5 figures, accepted at IC3D 2020
- **Journal**: In 2020 International Conference on 3D Immersion (IC3D), IEEE,
  2020
- **Summary**: The task of predicting smooth and edge-consistent depth maps is notoriously difficult for single image depth estimation. This paper proposes a novel Bilateral Grid based 3D convolutional neural network, dubbed as 3DBG-UNet, that parameterizes high dimensional feature space by encoding compact 3D bilateral grids with UNets and infers sharp geometric layout of the scene. Further, another novel 3DBGES-UNet model is introduced that integrate 3DBG-UNet for inferring an accurate depth map given a single color view. The 3DBGES-UNet concatenates 3DBG-UNet geometry map with the inception network edge accentuation map and a spatial object's boundary map obtained by leveraging semantic segmentation and train the UNet model with ResNet backbone. Both models are designed with a particular attention to explicitly account for edges or minute details. Preserving sharp discontinuities at depth edges is critical for many applications such as realistic integration of virtual objects in AR video or occlusion-aware view synthesis for 3D display applications.The proposed depth prediction network achieves state-of-the-art performance in both qualitative and quantitative evaluations on the challenging NYUv2-Depth data. The code and corresponding pre-trained weights will be made publicly available.



### Visual representation of negation: Real world data analysis on comic image design
- **Arxiv ID**: http://arxiv.org/abs/2105.10131v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2105.10131v1)
- **Published**: 2021-05-21 04:57:43+00:00
- **Updated**: 2021-05-21 04:57:43+00:00
- **Authors**: Yuri Sato, Koji Mineshima, Kazuhiro Ueda
- **Comment**: To appear in Proceedings of the 43rd Annual Conference of the
  Cognitive Science Society (CogSci 2021)
- **Journal**: None
- **Summary**: There has been a widely held view that visual representations (e.g., photographs and illustrations) do not depict negation, for example, one that can be expressed by a sentence "the train is not coming". This view is empirically challenged by analyzing the real-world visual representations of comic (manga) illustrations. In the experiment using image captioning tasks, we gave people comic illustrations and asked them to explain what they could read from them. The collected data showed that some comic illustrations could depict negation without any aid of sequences (multiple panels) or conventional devices (special symbols). This type of comic illustrations was subjected to further experiments, classifying images into those containing negation and those not containing negation. While this image classification was easy for humans, it was difficult for data-driven machines, i.e., deep learning models (CNN), to achieve the same high performance. Given the findings, we argue that some comic illustrations evoke background knowledge and thus can depict negation with purely visual elements.



### Safety Metrics for Semantic Segmentation in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2105.10142v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.10142v2)
- **Published**: 2021-05-21 05:59:49+00:00
- **Updated**: 2021-09-25 11:54:52+00:00
- **Authors**: Chih-Hong Cheng, Alois Knoll, Hsuan-Cheng Liao
- **Comment**: Paper accepted at IEEE AI Test'21
- **Journal**: None
- **Summary**: Within the context of autonomous driving, safety-related metrics for deep neural networks have been widely studied for image classification and object detection. In this paper, we further consider safety-aware correctness and robustness metrics specialized for semantic segmentation. The novelty of our proposal is to move beyond pixel-level metrics: Given two images with each having N pixels being class-flipped, the designed metrics should, depending on the clustering of pixels being class-flipped or the location of occurrence, reflect a different level of safety criticality. The result evaluated on an autonomous driving dataset demonstrates the validity and practicality of our proposed methodology.



### ViPNAS: Efficient Video Pose Estimation via Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2105.10154v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10154v1)
- **Published**: 2021-05-21 06:36:40+00:00
- **Updated**: 2021-05-21 06:36:40+00:00
- **Authors**: Lumin Xu, Yingda Guan, Sheng Jin, Wentao Liu, Chen Qian, Ping Luo, Wanli Ouyang, Xiaogang Wang
- **Comment**: Accepted to CVPR 2021
- **Journal**: None
- **Summary**: Human pose estimation has achieved significant progress in recent years. However, most of the recent methods focus on improving accuracy using complicated models and ignoring real-time efficiency. To achieve a better trade-off between accuracy and efficiency, we propose a novel neural architecture search (NAS) method, termed ViPNAS, to search networks in both spatial and temporal levels for fast online video pose estimation. In the spatial level, we carefully design the search space with five different dimensions including network depth, width, kernel size, group number, and attentions. In the temporal level, we search from a series of temporal feature fusions to optimize the total accuracy and speed across multiple video frames. To the best of our knowledge, we are the first to search for the temporal feature fusion and automatic computation allocation in videos. Extensive experiments demonstrate the effectiveness of our approach on the challenging COCO2017 and PoseTrack2018 datasets. Our discovered model family, S-ViPNAS and T-ViPNAS, achieve significantly higher inference speed (CPU real-time) without sacrificing the accuracy compared to the previous state-of-the-art methods.



### Global Context for improving recognition of Online Handwritten Mathematical Expressions
- **Arxiv ID**: http://arxiv.org/abs/2105.10156v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10156v1)
- **Published**: 2021-05-21 06:39:47+00:00
- **Updated**: 2021-05-21 06:39:47+00:00
- **Authors**: Cuong Tuan Nguyen, Thanh-Nghia Truong, Hung Tuan Nguyen, Masaki Nakagawa
- **Comment**: 16 pages, ICDAR2021
- **Journal**: None
- **Summary**: This paper presents a temporal classification method for all three subtasks of symbol segmentation, symbol recognition and relation classification in online handwritten mathematical expressions (HMEs). The classification model is trained by multiple paths of symbols and spatial relations derived from the Symbol Relation Tree (SRT) representation of HMEs. The method benefits from global context of a deep bidirectional Long Short-term Memory network, which learns the temporal classification directly from online handwriting by the Connectionist Temporal Classification loss. To recognize an online HME, a symbol-level parse tree with Context-Free Grammar is constructed, where symbols and spatial relations are obtained from the temporal classification results. We show the effectiveness of the proposed method on the two latest CROHME datasets.



### GSSF: A Generative Sequence Similarity Function based on a Seq2Seq model for clustering online handwritten mathematical answers
- **Arxiv ID**: http://arxiv.org/abs/2105.10159v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10159v1)
- **Published**: 2021-05-21 06:48:02+00:00
- **Updated**: 2021-05-21 06:48:02+00:00
- **Authors**: Huy Quang Ung, Cuong Tuan Nguyen, Hung Tuan Nguyen, Masaki Nakagawa
- **Comment**: 16 pages, ICDAR2021
- **Journal**: None
- **Summary**: Toward a computer-assisted marking for descriptive math questions,this paper presents clustering of online handwritten mathematical expressions (OnHMEs) to help human markers to mark them efficiently and reliably. We propose a generative sequence similarity function for computing a similarity score of two OnHMEs based on a sequence-to-sequence OnHME recognizer. Each OnHME is represented by a similarity-based representation (SbR) vector. The SbR matrix is inputted to the k-means algorithm for clustering OnHMEs. Experiments are conducted on an answer dataset (Dset_Mix) of 200 OnHMEs mixed of real patterns and synthesized patterns for each of 10 questions and a real online handwritten mathematical answer dataset of 122 student answers at most for each of 15 questions (NIER_CBT). The best clustering results achieved around 0.916 and 0.915 for purity, and around 0.556 and 0.702 for the marking cost on Dset_Mix and NIER_CBT, respectively. Our method currently outperforms the previous methods for clustering HMEs.



### Act Like a Radiologist: Towards Reliable Multi-view Correspondence Reasoning for Mammogram Mass Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.10160v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10160v1)
- **Published**: 2021-05-21 06:48:34+00:00
- **Updated**: 2021-05-21 06:48:34+00:00
- **Authors**: Yuhang Liu, Fandong Zhang, Chaoqi Chen, Siwen Wang, Yizhou Wang, Yizhou Yu
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)
- **Journal**: None
- **Summary**: Mammogram mass detection is crucial for diagnosing and preventing the breast cancers in clinical practice. The complementary effect of multi-view mammogram images provides valuable information about the breast anatomical prior structure and is of great significance in digital mammography interpretation. However, unlike radiologists who can utilize the natural reasoning ability to identify masses based on multiple mammographic views, how to endow the existing object detection models with the capability of multi-view reasoning is vital for decision-making in clinical diagnosis but remains the boundary to explore. In this paper, we propose an Anatomy-aware Graph convolutional Network (AGN), which is tailored for mammogram mass detection and endows existing detection methods with multi-view reasoning ability. The proposed AGN consists of three steps. Firstly, we introduce a Bipartite Graph convolutional Network (BGN) to model the intrinsic geometric and semantic relations of ipsilateral views. Secondly, considering that the visual asymmetry of bilateral views is widely adopted in clinical practice to assist the diagnosis of breast lesions, we propose an Inception Graph convolutional Network (IGN) to model the structural similarities of bilateral views. Finally, based on the constructed graphs, the multi-view information is propagated through nodes methodically, which equips the features learned from the examined view with multi-view reasoning ability. Experiments on two standard benchmarks reveal that AGN significantly exceeds the state-of-the-art performance. Visualization results show that AGN provides interpretable visual cues for clinical diagnosis.



### A Multi-Branch Hybrid Transformer Networkfor Corneal Endothelial Cell Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2106.07557v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2106.07557v1)
- **Published**: 2021-05-21 07:31:09+00:00
- **Updated**: 2021-05-21 07:31:09+00:00
- **Authors**: Yinglin Zhang, Risa Higashita, Huazhu Fu, Yanwu Xu, Yang Zhang, Haofeng Liu, Jian Zhang, Jiang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Corneal endothelial cell segmentation plays a vital role inquantifying clinical indicators such as cell density, coefficient of variation,and hexagonality. However, the corneal endothelium's uneven reflectionand the subject's tremor and movement cause blurred cell edges in theimage, which is difficult to segment, and need more details and contextinformation to release this problem. Due to the limited receptive field oflocal convolution and continuous downsampling, the existing deep learn-ing segmentation methods cannot make full use of global context andmiss many details. This paper proposes a Multi-Branch hybrid Trans-former Network (MBT-Net) based on the transformer and body-edgebranch. Firstly, We use the convolutional block to focus on local tex-ture feature extraction and establish long-range dependencies over space,channel, and layer by the transformer and residual connection. Besides,We use the body-edge branch to promote local consistency and to provideedge position information. On the self-collected dataset TM-EM3000 andpublic Alisarine dataset, compared with other State-Of-The-Art (SOTA)methods, the proposed method achieves an improvement.



### Rotation invariant CNN using scattering transform for image classification
- **Arxiv ID**: http://arxiv.org/abs/2105.10175v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10175v1)
- **Published**: 2021-05-21 07:36:34+00:00
- **Updated**: 2021-05-21 07:36:34+00:00
- **Authors**: Rosemberg Rodriguez Salas, Eva Dokladalova, Petr Dokládal
- **Comment**: None
- **Journal**: IEEE International Conference on Image Processing (ICIP), Sep
  2019, Taipei, Taiwan
- **Summary**: Deep convolutional neural networks accuracy is heavily impacted by rotations of the input data. In this paper, we propose a convolutional predictor that is invariant to rotations in the input. This architecture is capable of predicting the angular orientation without angle-annotated data. Furthermore, the predictor maps continuously the random rotation of the input to a circular space of the prediction. For this purpose, we use the roto-translation properties existing in the Scattering Transform Networks with a series of 3D Convolutions. We validate the results by training with upright and randomly rotated samples. This allows further applications of this work on fields like automatic re-orientation of randomly oriented datasets.



### Combining Transformer Generators with Convolutional Discriminators
- **Arxiv ID**: http://arxiv.org/abs/2105.10189v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10189v3)
- **Published**: 2021-05-21 07:56:59+00:00
- **Updated**: 2021-07-10 10:16:47+00:00
- **Authors**: Ricard Durall, Stanislav Frolov, Jörn Hees, Federico Raue, Franz-Josef Pfreundt, Andreas Dengel, Janis Keupe
- **Comment**: None
- **Journal**: None
- **Summary**: Transformer models have recently attracted much interest from computer vision researchers and have since been successfully employed for several problems traditionally addressed with convolutional neural networks. At the same time, image synthesis using generative adversarial networks (GANs) has drastically improved over the last few years. The recently proposed TransGAN is the first GAN using only transformer-based architectures and achieves competitive results when compared to convolutional GANs. However, since transformers are data-hungry architectures, TransGAN requires data augmentation, an auxiliary super-resolution task during training, and a masking prior to guide the self-attention mechanism. In this paper, we study the combination of a transformer-based generator and convolutional discriminator and successfully remove the need of the aforementioned required design choices. We evaluate our approach by conducting a benchmark of well-known CNN discriminators, ablate the size of the transformer-based generator, and show that combining both architectural elements into a hybrid model leads to better results. Furthermore, we investigate the frequency spectrum properties of generated images and observe that our model retains the benefits of an attention based generator.



### Pyramid Fusion Dark Channel Prior for Single Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2105.10192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10192v1)
- **Published**: 2021-05-21 08:01:57+00:00
- **Updated**: 2021-05-21 08:01:57+00:00
- **Authors**: Qiyuan Liang, Bin Zhu, Chong-Wah Ngo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose the pyramid fusion dark channel prior (PF-DCP) for single image dehazing. Based on the well-known Dark Channel Prior (DCP), we introduce an easy yet effective approach PF-DCP by employing the DCP algorithm at a pyramid of multi-scale images to alleviate the problem of patch size selection. In this case, we obtain the final transmission map by fusing transmission maps at each level to recover a high-quality haze-free image. Experiments on RESIDE SOTS show that PF-DCP not only outperforms the traditional prior-based methods with a large margin but also achieves comparable or even better results of state-of-art deep learning approaches. Furthermore, the visual quality is also greatly improved with much fewer color distortions and halo artifacts.



### Endmember-Guided Unmixing Network (EGU-Net): A General Deep Learning Framework for Self-Supervised Hyperspectral Unmixing
- **Arxiv ID**: http://arxiv.org/abs/2105.10194v1
- **DOI**: 10.1109/TNNLS.2021
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10194v1)
- **Published**: 2021-05-21 08:07:12+00:00
- **Updated**: 2021-05-21 08:07:12+00:00
- **Authors**: Danfeng Hong, Lianru Gao, Jing Yao, Naoto Yokoya, Jocelyn Chanussot, Uta Heiden, Bing Zhang
- **Comment**: None
- **Journal**: IEEE Transactions on Neural Networks and Learning Systems, 2021
- **Summary**: Over the past decades, enormous efforts have been made to improve the performance of linear or nonlinear mixing models for hyperspectral unmixing, yet their ability to simultaneously generalize various spectral variabilities and extract physically meaningful endmembers still remains limited due to the poor ability in data fitting and reconstruction and the sensitivity to various spectral variabilities. Inspired by the powerful learning ability of deep learning, we attempt to develop a general deep learning approach for hyperspectral unmixing, by fully considering the properties of endmembers extracted from the hyperspectral imagery, called endmember-guided unmixing network (EGU-Net). Beyond the alone autoencoder-like architecture, EGU-Net is a two-stream Siamese deep network, which learns an additional network from the pure or nearly-pure endmembers to correct the weights of another unmixing network by sharing network parameters and adding spectrally meaningful constraints (e.g., non-negativity and sum-to-one) towards a more accurate and interpretable unmixing solution. Furthermore, the resulting general framework is not only limited to pixel-wise spectral unmixing but also applicable to spatial information modeling with convolutional operators for spatial-spectral unmixing. Experimental results conducted on three different datasets with the ground-truth of abundance maps corresponding to each material demonstrate the effectiveness and superiority of the EGU-Net over state-of-the-art unmixing algorithms. The codes will be available from the website: https://github.com/danfenghong/IEEE_TNNLS_EGU-Net.



### Aligning Visual Prototypes with BERT Embeddings for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.10195v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10195v1)
- **Published**: 2021-05-21 08:08:28+00:00
- **Updated**: 2021-05-21 08:08:28+00:00
- **Authors**: Kun Yan, Zied Bouraoui, Ping Wang, Shoaib Jameel, Steven Schockaert
- **Comment**: Accepted by ICMR2021
- **Journal**: None
- **Summary**: Few-shot learning (FSL) is the task of learning to recognize previously unseen categories of images from a small number of training examples. This is a challenging task, as the available examples may not be enough to unambiguously determine which visual features are most characteristic of the considered categories. To alleviate this issue, we propose a method that additionally takes into account the names of the image classes. While the use of class names has already been explored in previous work, our approach differs in two key aspects. First, while previous work has aimed to directly predict visual prototypes from word embeddings, we found that better results can be obtained by treating visual and text-based prototypes separately. Second, we propose a simple strategy for learning class name embeddings using the BERT language model, which we found to substantially outperform the GloVe vectors that were used in previous work. We furthermore propose a strategy for dealing with the high dimensionality of these vectors, inspired by models for aligning cross-lingual word embeddings. We provide experiments on miniImageNet, CUB and tieredImageNet, showing that our approach consistently improves the state-of-the-art in metric-based FSL.



### Multimodal Remote Sensing Benchmark Datasets for Land Cover Classification with A Shared and Specific Feature Learning Model
- **Arxiv ID**: http://arxiv.org/abs/2105.10196v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10196v1)
- **Published**: 2021-05-21 08:14:21+00:00
- **Updated**: 2021-05-21 08:14:21+00:00
- **Authors**: Danfeng Hong, Jingliang Hu, Jing Yao, Jocelyn Chanussot, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, 2021
- **Summary**: As remote sensing (RS) data obtained from different sensors become available largely and openly, multimodal data processing and analysis techniques have been garnering increasing interest in the RS and geoscience community. However, due to the gap between different modalities in terms of imaging sensors, resolutions, and contents, embedding their complementary information into a consistent, compact, accurate, and discriminative representation, to a great extent, remains challenging. To this end, we propose a shared and specific feature learning (S2FL) model. S2FL is capable of decomposing multimodal RS data into modality-shared and modality-specific components, enabling the information blending of multi-modalities more effectively, particularly for heterogeneous data sources. Moreover, to better assess multimodal baselines and the newly-proposed S2FL model, three multimodal RS benchmark datasets, i.e., Houston2013 -- hyperspectral and multispectral data, Berlin -- hyperspectral and synthetic aperture radar (SAR) data, Augsburg -- hyperspectral, SAR, and digital surface model (DSM) data, are released and used for land cover classification. Extensive experiments conducted on the three datasets demonstrate the superiority and advancement of our S2FL model in the task of land cover classification in comparison with previously-proposed state-of-the-art baselines. Furthermore, the baseline codes and datasets used in this paper will be made available freely at https://github.com/danfenghong/ISPRS_S2FL.



### DAVOS: Semi-Supervised Video Object Segmentation via Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2105.10201v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10201v2)
- **Published**: 2021-05-21 08:23:51+00:00
- **Updated**: 2021-05-24 16:00:12+00:00
- **Authors**: Jinshuo Zhang, Zhicheng Wang, Songyan Zhang, Gang Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Domain shift has always been one of the primary issues in video object segmentation (VOS), for which models suffer from degeneration when tested on unfamiliar datasets. Recently, many online methods have emerged to narrow the performance gap between training data (source domain) and test data (target domain) by fine-tuning on annotations of test data which are usually in shortage. In this paper, we propose a novel method to tackle domain shift by first introducing adversarial domain adaptation to the VOS task, with supervised training on the source domain and unsupervised training on the target domain. By fusing appearance and motion features with a convolution layer, and by adding supervision onto the motion branch, our model achieves state-of-the-art performance on DAVIS2016 with 82.6% mean IoU score after supervised training. Meanwhile, our adversarial domain adaptation strategy significantly raises the performance of the trained model when applied on FBMS59 and Youtube-Object, without exploiting extra annotations.



### Omni-supervised Point Cloud Segmentation via Gradual Receptive Field Component Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2105.10203v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10203v1)
- **Published**: 2021-05-21 08:32:02+00:00
- **Updated**: 2021-05-21 08:32:02+00:00
- **Authors**: Jingyu Gong, Jiachen Xu, Xin Tan, Haichuan Song, Yanyun Qu, Yuan Xie, Lizhuang Ma
- **Comment**: Accepted by CVPR2021
- **Journal**: None
- **Summary**: Hidden features in neural network usually fail to learn informative representation for 3D segmentation as supervisions are only given on output prediction, while this can be solved by omni-scale supervision on intermediate layers. In this paper, we bring the first omni-scale supervision method to point cloud segmentation via the proposed gradual Receptive Field Component Reasoning (RFCR), where target Receptive Field Component Codes (RFCCs) are designed to record categories within receptive fields for hidden units in the encoder. Then, target RFCCs will supervise the decoder to gradually infer the RFCCs in a coarse-to-fine categories reasoning manner, and finally obtain the semantic labels. Because many hidden features are inactive with tiny magnitude and make minor contributions to RFCC prediction, we propose a Feature Densification with a centrifugal potential to obtain more unambiguous features, and it is in effect equivalent to entropy regularization over features. More active features can further unleash the potential of our omni-supervision method. We embed our method into four prevailing backbones and test on three challenging benchmarks. Our method can significantly improve the backbones in all three datasets. Specifically, our method brings new state-of-the-art performances for S3DIS as well as Semantic3D and ranks the 1st in the ScanNet benchmark among all the point-based methods. Code will be publicly available at https://github.com/azuki-miho/RFCR.



### Anomaly Detection By Autoencoder Based On Weighted Frequency Domain Loss
- **Arxiv ID**: http://arxiv.org/abs/2105.10214v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10214v1)
- **Published**: 2021-05-21 09:10:36+00:00
- **Updated**: 2021-05-21 09:10:36+00:00
- **Authors**: Masaki Nakanishi, Kazuki Sato, Hideo Terada
- **Comment**: None
- **Journal**: None
- **Summary**: In image anomaly detection, Autoencoders are the popular methods that reconstruct the input image that might contain anomalies and output a clean image with no abnormalities. These Autoencoder-based methods usually calculate the anomaly score from the reconstruction error, the difference between the input image and the reconstructed image. On the other hand, the accuracy of the reconstruction is insufficient in many of these methods, so it leads to degraded accuracy of anomaly detection. To improve the accuracy of the reconstruction, we consider defining loss function in the frequency domain. In general, we know that natural images contain many low-frequency components and few high-frequency components. Hence, to improve the accuracy of the reconstruction of high-frequency components, we introduce a new loss function named weighted frequency domain loss(WFDL). WFDL provides a sharper reconstructed image, which contributes to improving the accuracy of anomaly detection. In this paper, we show our method's superiority over the conventional Autoencoder methods by comparing it with AUROC on the MVTec AD dataset.



### Random Hash Code Generation for Cancelable Fingerprint Templates using Vector Permutation and Shift-order Process
- **Arxiv ID**: http://arxiv.org/abs/2105.10227v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10227v1)
- **Published**: 2021-05-21 09:37:54+00:00
- **Updated**: 2021-05-21 09:37:54+00:00
- **Authors**: Sani M. Abdullahi, Sun Shuifa
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Cancelable biometric techniques have been used to prevent the compromise of biometric data by generating and using their corresponding cancelable templates for user authentication. However, the non-invertible distance preserving transformation methods employed in various schemes are often vulnerable to information leakage since matching is performed in the transformed domain. In this paper, we propose a non-invertible distance preserving scheme based on vector permutation and shift-order process. First, the dimension of feature vectors is reduced using kernelized principle component analysis (KPCA) prior to randomly permuting the extracted vector features. A shift-order process is then applied to the generated features in order to achieve non-invertibility and combat similarity-based attacks. The generated hash codes are resilient to different security and privacy attacks whilst fulfilling the major revocability and unlinkability requirements. Experimental evaluation conducted on 6 datasets of FVC2002 and FVC2004 reveals a high-performance accuracy of the proposed scheme better than other existing state-of-the-art schemes.



### Multi-color balance for color constancy
- **Arxiv ID**: http://arxiv.org/abs/2105.10228v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10228v1)
- **Published**: 2021-05-21 09:38:56+00:00
- **Updated**: 2021-05-21 09:38:56+00:00
- **Authors**: Teruaki Akazawa, Yuma Kinoshita, Hitoshi Kiya
- **Comment**: \c{opyright} 2021 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: In this paper, we propose a novel multi-color balance adjustment for color constancy. The proposed method, called "n-color balancing," allows us not only to perfectly correct n target colors on the basis of corresponding ground truth colors but also to correct colors other than the n colors. In contrast, although white-balancing can perfectly adjust white, colors other than white are not considered in the framework of white-balancing in general. In an experiment, the proposed multi-color balancing is demonstrated to outperform both conventional white and multi-color balance adjustments including Bradford's model.



### Helsinki Deblur Challenge 2021: description of photographic data
- **Arxiv ID**: http://arxiv.org/abs/2105.10233v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10233v1)
- **Published**: 2021-05-21 09:47:12+00:00
- **Updated**: 2021-05-21 09:47:12+00:00
- **Authors**: Markus Juvonen, Samuli Siltanen, Fernando Silva de Moura
- **Comment**: None
- **Journal**: None
- **Summary**: The photographic dataset collected for the Helsinki Deblur Challenge 2021 (HDC2021) contains pairs of images taken by two identical cameras of the same target but with different conditions. One camera is always in focus and produces sharp and low-noise images the other camera produces blurred and noisy images as it is gradually more and more out of focus and has a higher ISO setting. Even though the dataset was designed and captured with the HDC2021 in mind it can be used for any testing and benchmarking of image deblurring algorithms. The data is available here: https://doi.org/10.5281/zenodo.477228



### An Interpretable Approach to Automated Severity Scoring in Pelvic Trauma
- **Arxiv ID**: http://arxiv.org/abs/2105.10238v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.10238v1)
- **Published**: 2021-05-21 09:52:33+00:00
- **Updated**: 2021-05-21 09:52:33+00:00
- **Authors**: Anna Zapaishchykova, David Dreizin, Zhaoshuo Li, Jie Ying Wu, Shahrooz Faghih Roohi, Mathias Unberath
- **Comment**: 10 pages, 3 figures
- **Journal**: None
- **Summary**: Pelvic ring disruptions result from blunt injury mechanisms and are often found in patients with multi-system trauma. To grade pelvic fracture severity in trauma victims based on whole-body CT, the Tile AO/OTA classification is frequently used. Due to the high volume of whole-body trauma CTs generated in busy trauma centers, an automated approach to Tile classification would provide substantial value, e.,g., to prioritize the reading queue of the attending trauma radiologist. In such scenario, an automated method should perform grading based on a transparent process and based on interpretable features to enable interaction with human readers and lower their workload by offering insights from a first automated read of the scan. This paper introduces an automated yet interpretable pelvic trauma decision support system to assist radiologists in fracture detection and Tile grade classification. The method operates similarly to human interpretation of CT scans and first detects distinct pelvic fractures on CT with high specificity using a Faster-RCNN model that are then interpreted using a structural causal model based on clinical best practices to infer an initial Tile grade. The Bayesian causal model and finally, the object detector are then queried for likely co-occurring fractures that may have been rejected initially due to the highly specific operating point of the detector, resulting in an updated list of detected fractures and corresponding final Tile grade. Our method is transparent in that it provides finding location and type using the object detector, as well as information on important counterfactuals that would invalidate the system's recommendation and achieves an AUC of 83.3%/85.1% for translational/rotational instability. Despite being designed for human-machine teaming, our approach does not compromise on performance compared to previous black-box approaches.



### AC-CovidNet: Attention Guided Contrastive CNN for Recognition of Covid-19 in Chest X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2105.10239v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.10239v2)
- **Published**: 2021-05-21 09:53:07+00:00
- **Updated**: 2022-01-23 03:12:19+00:00
- **Authors**: Anirudh Ambati, Shiv Ram Dubey
- **Comment**: Accepted in Sixth IAPR International Conference on Computer Vision &
  Image Processing (CVIP2021)
- **Journal**: None
- **Summary**: Covid-19 global pandemic continues to devastate health care systems across the world. At present, the Covid-19 testing is costly and time-consuming. Chest X-Ray (CXR) testing can be a fast, scalable, and non-invasive method. The existing methods suffer due to the limited CXR samples available from Covid-19. Thus, inspired by the limitations of the open-source work in this field, we propose attention guided contrastive CNN architecture (AC-CovidNet) for Covid-19 detection in CXR images. The proposed method learns the robust and discriminative features with the help of contrastive loss. Moreover, the proposed method gives more importance to the infected regions as guided by the attention mechanism. We compute the sensitivity of the proposed method over the publicly available Covid-19 dataset. It is observed that the proposed AC-CovidNet exhibits very promising performance as compared to the existing methods even with limited training data. It can tackle the bottleneck of CXR Covid-19 datasets being faced by the researchers. The code used in this paper is released publicly at \url{https://github.com/shivram1987/AC-CovidNet/}.



### Joint Triplet Autoencoder for Histopathological Colon Cancer Nuclei Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2105.10262v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10262v2)
- **Published**: 2021-05-21 10:31:14+00:00
- **Updated**: 2021-05-24 04:56:58+00:00
- **Authors**: Satya Rajendra Singh, Shiv Ram Dubey, Shruthi MS, Sairathan Ventrapragada, Saivamshi Salla Dasharatha
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has shown a great improvement in the performance of visual tasks. Image retrieval is the task of extracting the visually similar images from a database for a query image. The feature matching is performed to rank the images. Various hand-designed features have been derived in past to represent the images. Nowadays, the power of deep learning is being utilized for automatic feature learning from data in the field of biomedical image analysis. Autoencoder and Siamese networks are two deep learning models to learn the latent space (i.e., features or embedding). Autoencoder works based on the reconstruction of the image from latent space. Siamese network utilizes the triplets to learn the intra-class similarity and inter-class dissimilarity. Moreover, Autoencoder is unsupervised, whereas Siamese network is supervised. We propose a Joint Triplet Autoencoder Network (JTANet) by facilitating the triplet learning in autoencoder framework. A joint supervised learning for Siamese network and unsupervised learning for Autoencoder is performed. Moreover, the Encoder network of Autoencoder is shared with Siamese network and referred as the Siamcoder network. The features are extracted by using the trained Siamcoder network for retrieval purpose. The experiments are performed over Histopathological Routine Colon Cancer dataset. We have observed the promising performance using the proposed JTANet model against the Autoencoder and Siamese models for colon cancer nuclei retrieval in histopathological images.



### Extremely Lightweight Quantization Robust Real-Time Single-Image Super Resolution for Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/2105.10288v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10288v1)
- **Published**: 2021-05-21 11:29:48+00:00
- **Updated**: 2021-05-21 11:29:48+00:00
- **Authors**: Mustafa Ayazoglu
- **Comment**: None
- **Journal**: IEEE Computer Vision Pattern Recognition Workshops (Mobile AI 2021
  Workshop)
- **Summary**: Single-Image Super Resolution (SISR) is a classical computer vision problem and it has been studied for over decades. With the recent success of deep learning methods, recent work on SISR focuses solutions with deep learning methodologies and achieves state-of-the-art results. However most of the state-of-the-art SISR methods contain millions of parameters and layers, which limits their practical applications. In this paper, we propose a hardware (Synaptics Dolphin NPU) limitation aware, extremely lightweight quantization robust real-time super resolution network (XLSR). The proposed model's building block is inspired from root modules for Image classification. We successfully applied root modules to SISR problem, further more to make the model uint8 quantization robust we used Clipped ReLU at the last layer of the network and achieved great balance between reconstruction quality and runtime. Furthermore, although the proposed network contains 30x fewer parameters than VDSR its performance surpasses it on Div2K validation set. The network proved itself by winning Mobile AI 2021 Real-Time Single Image Super Resolution Challenge.



### WSSOD: A New Pipeline for Weakly- and Semi-Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.11293v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.11293v1)
- **Published**: 2021-05-21 11:58:50+00:00
- **Updated**: 2021-05-21 11:58:50+00:00
- **Authors**: Shijie Fang, Yuhang Cao, Xinjiang Wang, Kai Chen, Dahua Lin, Wayne Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of object detection, to a great extent, depends on the availability of large annotated datasets. To alleviate the annotation cost, the research community has explored a number of ways to exploit unlabeled or weakly labeled data. However, such efforts have met with limited success so far. In this work, we revisit the problem with a pragmatic standpoint, trying to explore a new balance between detection performance and annotation cost by jointly exploiting fully and weakly annotated data. Specifically, we propose a weakly- and semi-supervised object detection framework (WSSOD), which involves a two-stage learning procedure. An agent detector is first trained on a joint dataset and then used to predict pseudo bounding boxes on weakly-annotated images. The underlying assumptions in the current as well as common semi-supervised pipelines are also carefully examined under a unified EM formulation. On top of this framework, weakly-supervised loss (WSL), label attention and random pseudo-label sampling (RPS) strategies are introduced to relax these assumptions, bringing additional improvement on the efficacy of the detection pipeline. The proposed framework demonstrates remarkable performance on PASCAL-VOC and MSCOCO benchmark, achieving a high performance comparable to those obtained in fully-supervised settings, with only one third of the annotations.



### Multi-Task, Multi-Domain Deep Segmentation with Shared Representations and Contrastive Regularization for Sparse Pediatric Datasets
- **Arxiv ID**: http://arxiv.org/abs/2105.10310v2
- **DOI**: 10.1007/978-3-030-87193-2_23
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10310v2)
- **Published**: 2021-05-21 12:26:05+00:00
- **Updated**: 2022-02-02 09:11:30+00:00
- **Authors**: Arnaud Boutillon, Pierre-Henri Conze, Christelle Pons, Valérie Burdin, Bhushan Borotikar
- **Comment**: 11 pages, 4 figures, 2 tables, accepted at the 24th International
  Conference on Medical Image Computing and Computer-Assisted Intervention
  (MICCAI 2021)
- **Journal**: None
- **Summary**: Automatic segmentation of magnetic resonance (MR) images is crucial for morphological evaluation of the pediatric musculoskeletal system in clinical practice. However, the accuracy and generalization performance of individual segmentation models are limited due to the restricted amount of annotated pediatric data. Hence, we propose to train a segmentation model on multiple datasets, arising from different parts of the anatomy, in a multi-task and multi-domain learning framework. This approach allows to overcome the inherent scarcity of pediatric data while benefiting from a more robust shared representation. The proposed segmentation network comprises shared convolutional filters, domain-specific batch normalization parameters that compute the respective dataset statistics and a domain-specific segmentation layer. Furthermore, a supervised contrastive regularization is integrated to further improve generalization capabilities, by promoting intra-domain similarity and impose inter-domain margins in embedded space. We evaluate our contributions on two pediatric imaging datasets of the ankle and shoulder joints for bone segmentation. Results demonstrate that the proposed model outperforms state-of-the-art approaches.



### Sharing Pain: Using Pain Domain Transfer for Video Recognition of Low Grade Orthopedic Pain in Horses
- **Arxiv ID**: http://arxiv.org/abs/2105.10313v3
- **DOI**: 10.1371/journal.pone.0263854
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10313v3)
- **Published**: 2021-05-21 12:35:00+00:00
- **Updated**: 2022-01-07 14:35:54+00:00
- **Authors**: Sofia Broomé, Katrina Ask, Maheen Rashid, Pia Haubro Andersen, Hedvig Kjellström
- **Comment**: None
- **Journal**: None
- **Summary**: Orthopedic disorders are common among horses, often leading to euthanasia, which often could have been avoided with earlier detection. These conditions often create varying degrees of subtle long-term pain. It is challenging to train a visual pain recognition method with video data depicting such pain, since the resulting pain behavior also is subtle, sparsely appearing, and varying, making it challenging for even an expert human labeller to provide accurate ground-truth for the data. We show that a model trained solely on a dataset of horses with acute experimental pain (where labeling is less ambiguous) can aid recognition of the more subtle displays of orthopedic pain. Moreover, we present a human expert baseline for the problem, as well as an extensive empirical study of various domain transfer methods and of what is detected by the pain recognition method trained on clean experimental pain in the orthopedic dataset. Finally, this is accompanied with a discussion around the challenges posed by real-world animal behavior datasets and how best practices can be established for similar fine-grained action recognition tasks. Our code is available at https://github.com/sofiabroome/painface-recognition.



### Analysis of voxel-based 3D object detection methods efficiency for real-time embedded systems
- **Arxiv ID**: http://arxiv.org/abs/2105.10316v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10316v1)
- **Published**: 2021-05-21 12:40:59+00:00
- **Updated**: 2021-05-21 12:40:59+00:00
- **Authors**: Illia Oleksiienko, Alexandros Iosifidis
- **Comment**: 6 pages, 4 figures. Accepted in IEEE ICETCI 2021
- **Journal**: None
- **Summary**: Real-time detection of objects in the 3D scene is one of the tasks an autonomous agent needs to perform for understanding its surroundings. While recent Deep Learning-based solutions achieve satisfactory performance, their high computational cost renders their application in real-life settings in which computations need to be performed on embedded platforms intractable. In this paper, we analyze the efficiency of two popular voxel-based 3D object detection methods providing a good compromise between high performance and speed based on two aspects, their ability to detect objects located at large distances from the agent and their ability to operate in real time on embedded platforms equipped with high-performance GPUs. Our experiments show that these methods mostly fail to detect distant small objects due to the sparsity of the input point clouds at large distances. Moreover, models trained on near objects achieve similar or better performance compared to those trained on all objects in the scene. This means that the models learn object appearance representations mostly from near objects. Our findings suggest that a considerable part of the computations of existing methods is focused on locations of the scene that do not contribute with successful detection. This means that the methods can achieve a speed-up of $40$-$60\%$ by restricting operation to near objects while not sacrificing much in performance.



### Behind the leaves -- Estimation of occluded grapevine berries with conditional generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/2105.10325v1
- **DOI**: 10.3389/frai.2022.830026
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2105.10325v1)
- **Published**: 2021-05-21 12:57:48+00:00
- **Updated**: 2021-05-21 12:57:48+00:00
- **Authors**: Jana Kierdorf, Immanuel Weber, Anna Kicherer, Laura Zabawa, Lukas Drees, Ribana Roscher
- **Comment**: 45 pages, 18 figures, 1 table
- **Journal**: None
- **Summary**: The need for accurate yield estimates for viticulture is becoming more important due to increasing competition in the wine market worldwide. One of the most promising methods to estimate the harvest is berry counting, as it can be approached non-destructively, and its process can be automated. In this article, we present a method that addresses the challenge of occluded berries with leaves to obtain a more accurate estimate of the number of berries that will enable a better estimate of the harvest. We use generative adversarial networks, a deep learning-based approach that generates a likely scenario behind the leaves exploiting learned patterns from images with non-occluded berries. Our experiments show that the estimate of the number of berries after applying our method is closer to the manually counted reference. In contrast to applying a factor to the berry count, our approach better adapts to local conditions by directly involving the appearance of the visible berries. Furthermore, we show that our approach can identify which areas in the image should be changed by adding new berries without explicitly requiring information about hidden areas.



### Hierarchical Consistency Regularized Mean Teacher for Semi-supervised 3D Left Atrium Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.10369v2
- **DOI**: 10.1109/EMBC46164.2021.9629941
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10369v2)
- **Published**: 2021-05-21 14:15:45+00:00
- **Updated**: 2021-08-15 15:44:00+00:00
- **Authors**: Shumeng Li, Ziyuan Zhao, Kaixin Xu, Zeng Zeng, Cuntai Guan
- **Comment**: Accepted in 43rd Annual International Conference of the IEEE
  Engineering in Medicine and Biology Society, IEEE EMBC 2021
- **Journal**: 2021 43rd Annual International Conference of the IEEE Engineering
  in Medicine & Biology Society (EMBC)
- **Summary**: Deep learning has achieved promising segmentation performance on 3D left atrium MR images. However, annotations for segmentation tasks are expensive, costly and difficult to obtain. In this paper, we introduce a novel hierarchical consistency regularized mean teacher framework for 3D left atrium segmentation. In each iteration, the student model is optimized by multi-scale deep supervision and hierarchical consistency regularization, concurrently. Extensive experiments have shown that our method achieves competitive performance as compared with full annotation, outperforming other state-of-the-art semi-supervised segmentation methods.



### An Efficient Training Approach for Very Large Scale Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2105.10375v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10375v5)
- **Published**: 2021-05-21 14:34:00+00:00
- **Updated**: 2022-03-03 07:52:18+00:00
- **Authors**: Kai Wang, Shuo Wang, Panpan Zhang, Zhipeng Zhou, Zheng Zhu, Xiaobo Wang, Xiaojiang Peng, Baigui Sun, Hao Li, Yang You
- **Comment**: This paper has been accepted by CVPR2022!
- **Journal**: None
- **Summary**: Face recognition has achieved significant progress in deep learning era due to the ultra-large-scale and welllabeled datasets. However, training on the outsize datasets is time-consuming and takes up a lot of hardware resource. Therefore, designing an efficient training approach is indispensable. The heavy computational and memory costs mainly result from the million-level dimensionality of thefully connected (FC) layer. To this end, we propose a novel training approach, termed Faster Face Classification (F2C), to alleviate time and cost without sacrificing the performance. This method adopts Dynamic Class Pool (DCP) for storing and updating the identities features dynamically, which could be regarded as a substitute for the FC layer. DCP is efficiently time-saving and cost-saving, as its smaller size with the independence from the whole face identities together. We further validate the proposed F2C method across several face benchmarks and private datasets, and display comparable results, meanwhile the speed is faster than state-of-the-art FC-based methods in terms of recognition accuracy and hardware costs. Moreover, our method is further improved by a well-designed dual data loader including indentity-based and instancebased loaders, which makes it more efficient for the updating DCP parameters.



### 3D Human Pose Regression using Graph Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2105.10379v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10379v2)
- **Published**: 2021-05-21 14:41:31+00:00
- **Updated**: 2022-12-13 14:50:53+00:00
- **Authors**: Soubarna Banik, Alejandro Mendoza Gracia, Alois Knoll
- **Comment**: Paper accepted in IEEE ICIP 2021, DOI will be updated once published
- **Journal**: None
- **Summary**: 3D human pose estimation is a difficult task, due to challenges such as occluded body parts and ambiguous poses. Graph convolutional networks encode the structural information of the human skeleton in the form of an adjacency matrix, which is beneficial for better pose prediction. We propose one such graph convolutional network named PoseGraphNet for 3D human pose regression from 2D poses. Our network uses an adaptive adjacency matrix and kernels specific to neighbor groups. We evaluate our model on the Human3.6M dataset which is a standard dataset for 3D pose estimation. Our model's performance is close to the state-of-the-art, but with much fewer parameters. The model learns interesting adjacency relations between joints that have no physical connections, but are behaviorally similar.



### Learning general and distinctive 3D local deep descriptors for point cloud registration
- **Arxiv ID**: http://arxiv.org/abs/2105.10382v3
- **DOI**: 10.1109/TPAMI.2022.3175371
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2105.10382v3)
- **Published**: 2021-05-21 14:47:55+00:00
- **Updated**: 2022-05-12 19:47:29+00:00
- **Authors**: Fabio Poiesi, Davide Boscaini
- **Comment**: Accepted in IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: An effective 3D descriptor should be invariant to different geometric transformations, such as scale and rotation, robust to occlusions and clutter, and capable of generalising to different application domains. We present a simple yet effective method to learn general and distinctive 3D local descriptors that can be used to register point clouds that are captured in different domains. Point cloud patches are extracted, canonicalised with respect to their local reference frame, and encoded into scale and rotation-invariant compact descriptors by a deep neural network that is invariant to permutations of the input points. This design is what enables our descriptors to generalise across domains. We evaluate and compare our descriptors with alternative handcrafted and deep learning-based descriptors on several indoor and outdoor datasets that are reconstructed by using both RGBD sensors and laser scanners. Our descriptors outperform most recent descriptors by a large margin in terms of generalisation, and also become the state of the art in benchmarks where training and testing are performed in the same domain.



### High Fidelity Fingerprint Generation: Quality, Uniqueness, and Privacy
- **Arxiv ID**: http://arxiv.org/abs/2105.10403v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10403v1)
- **Published**: 2021-05-21 15:18:28+00:00
- **Updated**: 2021-05-21 15:18:28+00:00
- **Authors**: Keivan Bahmani, Richard Plesh, Peter Johnson, Stephanie Schuckers, Timothy Swyka
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we utilize progressive growth-based Generative Adversarial Networks (GANs) to develop the Clarkson Fingerprint Generator (CFG). We demonstrate that the CFG is capable of generating realistic, high fidelity, $512\times512$ pixels, full, plain impression fingerprints. Our results suggest that the fingerprints generated by the CFG are unique, diverse, and resemble the training dataset in terms of minutiae configuration and quality, while not revealing the underlying identities of the training data. We make the pre-trained CFG model and the synthetically generated dataset publicly available at https://github.com/keivanB/Clarkson_Finger_Gen



### Sheaves as a Framework for Understanding and Interpreting Model Fit
- **Arxiv ID**: http://arxiv.org/abs/2105.10414v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.AT, math.CT
- **Links**: [PDF](http://arxiv.org/pdf/2105.10414v1)
- **Published**: 2021-05-21 15:34:09+00:00
- **Updated**: 2021-05-21 15:34:09+00:00
- **Authors**: Henry Kvinge, Brett Jefferson, Cliff Joslyn, Emilie Purvine
- **Comment**: 12 page
- **Journal**: None
- **Summary**: As data grows in size and complexity, finding frameworks which aid in interpretation and analysis has become critical. This is particularly true when data comes from complex systems where extensive structure is available, but must be drawn from peripheral sources. In this paper we argue that in such situations, sheaves can provide a natural framework to analyze how well a statistical model fits at the local level (that is, on subsets of related datapoints) vs the global level (on all the data). The sheaf-based approach that we propose is suitably general enough to be useful in a range of applications, from analyzing sensor networks to understanding the feature space of a deep learning model.



### Self-learning for weakly supervised Gleason grading of local patterns
- **Arxiv ID**: http://arxiv.org/abs/2105.10420v1
- **DOI**: 10.1109/JBHI.2021.3061457
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10420v1)
- **Published**: 2021-05-21 15:39:50+00:00
- **Updated**: 2021-05-21 15:39:50+00:00
- **Authors**: Julio Silva-Rodríguez, Adrián Colomer, Jose Dolz, Valery Naranjo
- **Comment**: None
- **Journal**: None
- **Summary**: Prostate cancer is one of the main diseases affecting men worldwide. The gold standard for diagnosis and prognosis is the Gleason grading system. In this process, pathologists manually analyze prostate histology slides under microscope, in a high time-consuming and subjective task. In the last years, computer-aided-diagnosis (CAD) systems have emerged as a promising tool that could support pathologists in the daily clinical practice. Nevertheless, these systems are usually trained using tedious and prone-to-error pixel-level annotations of Gleason grades in the tissue. To alleviate the need of manual pixel-wise labeling, just a handful of works have been presented in the literature. Motivated by this, we propose a novel weakly-supervised deep-learning model, based on self-learning CNNs, that leverages only the global Gleason score of gigapixel whole slide images during training to accurately perform both, grading of patch-level patterns and biopsy-level scoring. To evaluate the performance of the proposed method, we perform extensive experiments on three different external datasets for the patch-level Gleason grading, and on two different test sets for global Grade Group prediction. We empirically demonstrate that our approach outperforms its supervised counterpart on patch-level Gleason grading by a large margin, as well as state-of-the-art methods on global biopsy-level scoring. Particularly, the proposed model brings an average improvement on the Cohen's quadratic kappa (k) score of nearly 18% compared to full-supervision for the patch-level Gleason grading task.



### LAPAR: Linearly-Assembled Pixel-Adaptive Regression Network for Single Image Super-Resolution and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2105.10422v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10422v1)
- **Published**: 2021-05-21 15:47:18+00:00
- **Updated**: 2021-05-21 15:47:18+00:00
- **Authors**: Wenbo Li, Kun Zhou, Lu Qi, Nianjuan Jiang, Jiangbo Lu, Jiaya Jia
- **Comment**: NeurIPS2020
- **Journal**: None
- **Summary**: Single image super-resolution (SISR) deals with a fundamental problem of upsampling a low-resolution (LR) image to its high-resolution (HR) version. Last few years have witnessed impressive progress propelled by deep learning methods. However, one critical challenge faced by existing methods is to strike a sweet spot of deep model complexity and resulting SISR quality. This paper addresses this pain point by proposing a linearly-assembled pixel-adaptive regression network (LAPAR), which casts the direct LR to HR mapping learning into a linear coefficient regression task over a dictionary of multiple predefined filter bases. Such a parametric representation renders our model highly lightweight and easy to optimize while achieving state-of-the-art results on SISR benchmarks. Moreover, based on the same idea, LAPAR is extended to tackle other restoration tasks, e.g., image denoising and JPEG image deblocking, and again, yields strong performance. The code is available at https://github.com/dvlab-research/Simple-SR.



### Compressing Deep CNNs using Basis Representation and Spectral Fine-tuning
- **Arxiv ID**: http://arxiv.org/abs/2105.10436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10436v1)
- **Published**: 2021-05-21 16:14:26+00:00
- **Updated**: 2021-05-21 16:14:26+00:00
- **Authors**: Muhammad Tayyab, Fahad Ahmad Khan, Abhijit Mahalanobis
- **Comment**: arXiv admin note: text overlap with arXiv:1906.04509
- **Journal**: None
- **Summary**: We propose an efficient and straightforward method for compressing deep convolutional neural networks (CNNs) that uses basis filters to represent the convolutional layers, and optimizes the performance of the compressed network directly in the basis space. Specifically, any spatial convolution layer of the CNN can be replaced by two successive convolution layers: the first is a set of three-dimensional orthonormal basis filters, followed by a layer of one-dimensional filters that represents the original spatial filters in the basis space. We jointly fine-tune both the basis and the filter representation to directly mitigate any performance loss due to the truncation. Generality of the proposed approach is demonstrated by applying it to several well known deep CNN architectures and data sets for image classification and object detection. We also present the execution time and power usage at different compression levels on the Xavier Jetson AGX processor.



### Compositional Fine-Grained Low-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2105.10438v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10438v1)
- **Published**: 2021-05-21 16:18:24+00:00
- **Updated**: 2021-05-21 16:18:24+00:00
- **Authors**: Dat Huynh, Ehsan Elhamifar
- **Comment**: None
- **Journal**: None
- **Summary**: We develop a novel compositional generative model for zero- and few-shot learning to recognize fine-grained classes with a few or no training samples. Our key observation is that generating holistic features for fine-grained classes fails to capture small attribute differences between classes. Therefore, we propose a feature composition framework that learns to extract attribute features from training samples and combines them to construct fine-grained features for rare and unseen classes. Feature composition allows us to not only selectively compose features of every class from only relevant training samples, but also obtain diversity among composed features via changing samples used for the composition. In addition, instead of building holistic features for classes, we use our attribute features to form dense representations capable of capturing fine-grained attribute details of classes. We propose a training scheme that uses a discriminative model to construct features that are subsequently used to train the model itself. Therefore, we directly train the discriminative model on the composed features without learning a separate generative model. We conduct experiments on four popular datasets of DeepFashion, AWA2, CUB, and SUN, showing the effectiveness of our method.



### Driving-Signal Aware Full-Body Avatars
- **Arxiv ID**: http://arxiv.org/abs/2105.10441v2
- **DOI**: 10.1145/3450626.3459850
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2105.10441v2)
- **Published**: 2021-05-21 16:22:38+00:00
- **Updated**: 2021-06-25 18:30:39+00:00
- **Authors**: Timur Bagautdinov, Chenglei Wu, Tomas Simon, Fabian Prada, Takaaki Shiratori, Shih-En Wei, Weipeng Xu, Yaser Sheikh, Jason Saragih
- **Comment**: None
- **Journal**: None
- **Summary**: We present a learning-based method for building driving-signal aware full-body avatars. Our model is a conditional variational autoencoder that can be animated with incomplete driving signals, such as human pose and facial keypoints, and produces a high-quality representation of human geometry and view-dependent appearance. The core intuition behind our method is that better drivability and generalization can be achieved by disentangling the driving signals and remaining generative factors, which are not available during animation. To this end, we explicitly account for information deficiency in the driving signal by introducing a latent space that exclusively captures the remaining information, thus enabling the imputation of the missing factors required during full-body animation, while remaining faithful to the driving signal. We also propose a learnable localized compression for the driving signal which promotes better generalization, and helps minimize the influence of global chance-correlations often found in real datasets. For a given driving signal, the resulting variational model produces a compact space of uncertainty for missing factors that allows for an imputation strategy best suited to a particular application. We demonstrate the efficacy of our approach on the challenging problem of full-body animation for virtual telepresence with driving signals acquired from minimal sensors placed in the environment and mounted on a VR-headset.



### WeGleNet: A Weakly-Supervised Convolutional Neural Network for the Semantic Segmentation of Gleason Grades in Prostate Histology Images
- **Arxiv ID**: http://arxiv.org/abs/2105.10445v1
- **DOI**: 10.1016/j.compmedimag.2020.101846
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10445v1)
- **Published**: 2021-05-21 16:27:16+00:00
- **Updated**: 2021-05-21 16:27:16+00:00
- **Authors**: Julio Silva-Rodríguez, Adrián Colomer, Valery Naranjo
- **Comment**: None
- **Journal**: None
- **Summary**: Prostate cancer is one of the main diseases affecting men worldwide. The Gleason scoring system is the primary diagnostic tool for prostate cancer. This is obtained via the visual analysis of cancerous patterns in prostate biopsies performed by expert pathologists, and the aggregation of the main Gleason grades in a combined score. Computer-aided diagnosis systems allow to reduce the workload of pathologists and increase the objectivity. Recently, efforts have been made in the literature to develop algorithms aiming the direct estimation of the global Gleason score at biopsy/core level with global labels. However, these algorithms do not cover the accurate localization of the Gleason patterns into the tissue. In this work, we propose a deep-learning-based system able to detect local cancerous patterns in the prostate tissue using only the global-level Gleason score during training. The methodological core of this work is the proposed weakly-supervised-trained convolutional neural network, WeGleNet, based on a multi-class segmentation layer after the feature extraction module, a global-aggregation, and the slicing of the background class for the model loss estimation during training. We obtained a Cohen's quadratic kappa (k) of 0.67 for the pixel-level prediction of cancerous patterns in the validation cohort. We compared the model performance for semantic segmentation of Gleason grades with supervised state-of-the-art architectures in the test cohort. We obtained a pixel-level k of 0.61 and a macro-averaged f1-score of 0.58, at the same level as fully-supervised methods. Regarding the estimation of the core-level Gleason score, we obtained a k of 0.76 and 0.67 between the model and two different pathologists. WeGleNet is capable of performing the semantic segmentation of Gleason grades similarly to fully-supervised methods without requiring pixel-level annotations.



### ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction
- **Arxiv ID**: http://arxiv.org/abs/2105.10446v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2105.10446v3)
- **Published**: 2021-05-21 16:29:57+00:00
- **Updated**: 2021-11-29 01:48:29+00:00
- **Authors**: Kwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, Yi Ma
- **Comment**: This paper integrates previous two manuscripts: arXiv:2006.08558 and
  arXiv:2010.14765, with significantly improved organization, presentation, and
  new results; V2 polishes writing and adds citation; V3 polishes writing, adds
  citation and experiments
- **Journal**: None
- **Summary**: This work attempts to provide a plausible theoretical framework that aims to interpret modern deep (convolutional) networks from the principles of data compression and discriminative representation. We argue that for high-dimensional multi-class data, the optimal linear discriminative representation maximizes the coding rate difference between the whole dataset and the average of all the subsets. We show that the basic iterative gradient ascent scheme for optimizing the rate reduction objective naturally leads to a multi-layer deep network, named ReduNet, which shares common characteristics of modern deep networks. The deep layered architectures, linear and nonlinear operators, and even parameters of the network are all explicitly constructed layer-by-layer via forward propagation, although they are amenable to fine-tuning via back propagation. All components of so-obtained "white-box" network have precise optimization, statistical, and geometric interpretation. Moreover, all linear operators of the so-derived network naturally become multi-channel convolutions when we enforce classification to be rigorously shift-invariant. The derivation in the invariant setting suggests a trade-off between sparsity and invariance, and also indicates that such a deep convolution network is significantly more efficient to construct and learn in the spectral domain. Our preliminary simulations and experiments clearly verify the effectiveness of both the rate reduction objective and the associated ReduNet. All code and data are available at \url{https://github.com/Ma-Lab-Berkeley}.



### Distinguishing artefacts: evaluating the saturation point of convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2105.10448v1
- **DOI**: 10.1016/j.procir.2021.05.089
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10448v1)
- **Published**: 2021-05-21 16:33:20+00:00
- **Updated**: 2021-05-21 16:33:20+00:00
- **Authors**: Ric Real, James Gopsill, David Jones, Chris Snider, Ben Hicks
- **Comment**: 6 Pages, 5 Figures, 2 Tables, Conference, Design Engineering, CNN,
  Digital Twin
- **Journal**: January 2021 Procedia CIRP 100:385-390
- **Summary**: Prior work has shown Convolutional Neural Networks (CNNs) trained on surrogate Computer Aided Design (CAD) models are able to detect and classify real-world artefacts from photographs. The applications of which support twinning of digital and physical assets in design, including rapid extraction of part geometry from model repositories, information search \& retrieval and identifying components in the field for maintenance, repair, and recording. The performance of CNNs in classification tasks have been shown dependent on training data set size and number of classes. Where prior works have used relatively small surrogate model data sets ($<100$ models), the question remains as to the ability of a CNN to differentiate between models in increasingly large model repositories. This paper presents a method for generating synthetic image data sets from online CAD model repositories, and further investigates the capacity of an off-the-shelf CNN architecture trained on synthetic data to classify models as class size increases. 1,000 CAD models were curated and processed to generate large scale surrogate data sets, featuring model coverage at steps of 10$^{\circ}$, 30$^{\circ}$, 60$^{\circ}$, and 120$^{\circ}$ degrees. The findings demonstrate the capability of computer vision algorithms to classify artefacts in model repositories of up to 200, beyond this point the CNN's performance is observed to deteriorate significantly, limiting its present ability for automated twinning of physical to digital artefacts. Although, a match is more often found in the top-5 results showing potential for information search and retrieval on large repositories of surrogate models.



### Elliptical Ordinal Embedding
- **Arxiv ID**: http://arxiv.org/abs/2105.10457v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10457v2)
- **Published**: 2021-05-21 16:54:53+00:00
- **Updated**: 2021-05-25 16:45:02+00:00
- **Authors**: Aïssatou Diallo, Johannes Fürnkranz
- **Comment**: 14 pages, 24 figures
- **Journal**: None
- **Summary**: Ordinal embedding aims at finding a low dimensional representation of objects from a set of constraints of the form "item $j$ is closer to item $i$ than item $k$". Typically, each object is mapped onto a point vector in a low dimensional metric space. We argue that mapping to a density instead of a point vector provides some interesting advantages, including an inherent reflection of the uncertainty about the representation itself and its relative location in the space. Indeed, in this paper, we propose to embed each object as a Gaussian distribution. We investigate the ability of these embeddings to capture the underlying structure of the data while satisfying the constraints, and explore properties of the representation. Experiments on synthetic and real-world datasets showcase the advantages of our approach. In addition, we illustrate the merit of modelling uncertainty, which enriches the visual perception of the mapped objects in the space.



### Graph Convolutional Networks in Feature Space for Image Deblurring and Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2105.10465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10465v1)
- **Published**: 2021-05-21 17:02:15+00:00
- **Updated**: 2021-05-21 17:02:15+00:00
- **Authors**: Boyan Xu, Hujun Yin
- **Comment**: Accepted by IJCNN 2021 (Oral)
- **Journal**: International Joint Conference on Neural Networks (IJCNN) 2021
- **Summary**: Graph convolutional networks (GCNs) have achieved great success in dealing with data of non-Euclidean structures. Their success directly attributes to fitting graph structures effectively to data such as in social media and knowledge databases. For image processing applications, the use of graph structures and GCNs have not been fully explored. In this paper, we propose a novel encoder-decoder network with added graph convolutions by converting feature maps to vertexes of a pre-generated graph to synthetically construct graph-structured data. By doing this, we inexplicitly apply graph Laplacian regularization to the feature maps, making them more structured. The experiments show that it significantly boosts performance for image restoration tasks, including deblurring and super-resolution. We believe it opens up opportunities for GCN-based approaches in more applications.



### Towards Realization of Augmented Intelligence in Dermatology: Advances and Future Directions
- **Arxiv ID**: http://arxiv.org/abs/2105.10477v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2105.10477v1)
- **Published**: 2021-05-21 17:39:16+00:00
- **Updated**: 2021-05-21 17:39:16+00:00
- **Authors**: Roxana Daneshjou, Carrie Kovarik, Justin M Ko
- **Comment**: 5 pages, no figures
- **Journal**: None
- **Summary**: Artificial intelligence (AI) algorithms using deep learning have advanced the classification of skin disease images; however these algorithms have been mostly applied "in silico" and not validated clinically. Most dermatology AI algorithms perform binary classification tasks (e.g. malignancy versus benign lesions), but this task is not representative of dermatologists' diagnostic range. The American Academy of Dermatology Task Force on Augmented Intelligence published a position statement emphasizing the importance of clinical validation to create human-computer synergy, termed augmented intelligence (AuI). Liu et al's recent paper, "A deep learning system for differential diagnosis of skin diseases" represents a significant advancement of AI in dermatology, bringing it closer to clinical impact. However, significant issues must be addressed before this algorithm can be integrated into clinical workflow. These issues include accurate and equitable model development, defining and assessing appropriate clinical outcomes, and real-world integration.



### Going Deeper through the Gleason Scoring Scale: An Automatic end-to-end System for Histology Prostate Grading and Cribriform Pattern Detection
- **Arxiv ID**: http://arxiv.org/abs/2105.10490v1
- **DOI**: 10.1016/j.cmpb.2020.105637
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10490v1)
- **Published**: 2021-05-21 17:51:53+00:00
- **Updated**: 2021-05-21 17:51:53+00:00
- **Authors**: Julio Silva-Rodríguez, Adrián Colomer, María A. Sales, Rafael Molina, Valery Naranjo
- **Comment**: None
- **Journal**: None
- **Summary**: The Gleason scoring system is the primary diagnostic and prognostic tool for prostate cancer. In recent years, with the development of digitisation devices, the use of computer vision techniques for the analysis of biopsies has increased. However, to the best of the authors' knowledge, the development of algorithms to automatically detect individual cribriform patterns belonging to Gleason grade 4 has not yet been studied in the literature. The objective of the work presented in this paper is to develop a deep-learning-based system able to support pathologists in the daily analysis of prostate biopsies. The methodological core of this work is a patch-wise predictive model based on convolutional neural networks able to determine the presence of cancerous patterns. In particular, we train from scratch a simple self-design architecture. The cribriform pattern is detected by retraining the set of filters of the last convolutional layer in the network. From the reconstructed prediction map, we compute the percentage of each Gleason grade in the tissue to feed a multi-layer perceptron which provides a biopsy-level score.mIn our SICAPv2 database, composed of 182 annotated whole slide images, we obtained a Cohen's quadratic kappa of 0.77 in the test set for the patch-level Gleason grading with the proposed architecture trained from scratch. Our results outperform previous ones reported in the literature. Furthermore, this model reaches the level of fine-tuned state-of-the-art architectures in a patient-based four groups cross validation. In the cribriform pattern detection task, we obtained an area under ROC curve of 0.82. Regarding the biopsy Gleason scoring, we achieved a quadratic Cohen's Kappa of 0.81 in the test subset. Shallow CNN architectures trained from scratch outperform current state-of-the-art methods for Gleason grades classification.



### Intriguing Properties of Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2105.10497v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2105.10497v3)
- **Published**: 2021-05-21 17:59:18+00:00
- **Updated**: 2021-11-25 18:36:22+00:00
- **Authors**: Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang
- **Comment**: NeurIPS'21 (Spotlight), Code: https://git.io/Js15X
- **Journal**: None
- **Summary**: Vision transformers (ViT) have demonstrated impressive performance across various machine vision problems. These models are based on multi-head self-attention mechanisms that can flexibly attend to a sequence of image patches to encode contextual cues. An important question is how such flexibility in attending image-wide context conditioned on a given patch can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing three ViT families and comparisons with a high-performing convolutional neural network (CNN). We show and analyze the following intriguing properties of ViT: (a) Transformers are highly robust to severe occlusions, perturbations and domain shifts, e.g., retain as high as 60% top-1 accuracy on ImageNet even after randomly occluding 80% of the image content. (b) The robust performance to occlusions is not due to a bias towards local textures, and ViTs are significantly less biased towards textures compared to CNNs. When properly trained to encode shape-based features, ViTs demonstrate shape recognition capability comparable to that of human visual system, previously unmatched in the literature. (c) Using ViTs to encode shape representation leads to an interesting consequence of accurate semantic segmentation without pixel-level supervision. (d) Off-the-shelf features from a single ViT model can be combined to create a feature ensemble, leading to high accuracy rates across a range of classification datasets in both traditional and few-shot learning paradigms. We show effective features of ViTs are due to flexible and dynamic receptive fields possible via the self-attention mechanism.



### SmartPatch: Improving Handwritten Word Imitation with Patch Discriminators
- **Arxiv ID**: http://arxiv.org/abs/2105.10528v1
- **DOI**: 10.1007/978-3-030-86549-8_18
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10528v1)
- **Published**: 2021-05-21 18:34:21+00:00
- **Updated**: 2021-05-21 18:34:21+00:00
- **Authors**: Alexander Mattick, Martin Mayr, Mathias Seuret, Andreas Maier, Vincent Christlein
- **Comment**: to be published the in 16th International Conference on Document
  Analysis and Recognition 2021 ICDAR
- **Journal**: None
- **Summary**: As of recent generative adversarial networks have allowed for big leaps in the realism of generated images in diverse domains, not the least of which being handwritten text generation. The generation of realistic-looking hand-written text is important because it can be used for data augmentation in handwritten text recognition (HTR) systems or human-computer interaction. We propose SmartPatch, a new technique increasing the performance of current state-of-the-art methods by augmenting the training feedback with a tailored solution to mitigate pen-level artifacts. We combine the well-known patch loss with information gathered from the parallel trained handwritten text recognition system and the separate characters of the word. This leads to a more enhanced local discriminator and results in more realistic and higher-quality generated handwritten words.



### BCNet: Searching for Network Width with Bilaterally Coupled Network
- **Arxiv ID**: http://arxiv.org/abs/2105.10533v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10533v1)
- **Published**: 2021-05-21 18:54:03+00:00
- **Updated**: 2021-05-21 18:54:03+00:00
- **Authors**: Xiu Su, Shan You, Fei Wang, Chen Qian, Changshui Zhang, Chang Xu
- **Comment**: Accepted by CVPR 2021
- **Journal**: None
- **Summary**: Searching for a more compact network width recently serves as an effective way of channel pruning for the deployment of convolutional neural networks (CNNs) under hardware constraints. To fulfill the searching, a one-shot supernet is usually leveraged to efficiently evaluate the performance \wrt~different network widths. However, current methods mainly follow a \textit{unilaterally augmented} (UA) principle for the evaluation of each width, which induces the training unfairness of channels in supernet. In this paper, we introduce a new supernet called Bilaterally Coupled Network (BCNet) to address this issue. In BCNet, each channel is fairly trained and responsible for the same amount of network widths, thus each network width can be evaluated more accurately. Besides, we leverage a stochastic complementary strategy for training the BCNet, and propose a prior initial population sampling method to boost the performance of the evolutionary search. Extensive experiments on benchmark CIFAR-10 and ImageNet datasets indicate that our method can achieve state-of-the-art or competing performance over other baseline methods. Moreover, our method turns out to further boost the performance of NAS models by refining their network widths. For example, with the same FLOPs budget, our obtained EfficientNet-B0 achieves 77.36\% Top-1 accuracy on ImageNet dataset, surpassing the performance of original setting by 0.48\%.



### Prostate Gland Segmentation in Histology Images via Residual and Multi-Resolution U-Net
- **Arxiv ID**: http://arxiv.org/abs/2105.10556v1
- **DOI**: 10.1007/978-3-030-62362-3_1
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10556v1)
- **Published**: 2021-05-21 20:11:36+00:00
- **Updated**: 2021-05-21 20:11:36+00:00
- **Authors**: Julio Silva-Rodríguez, Elena Payá-Bosch, Gabriel García, Adrián Colomer, Valery Naranjo
- **Comment**: None
- **Journal**: None
- **Summary**: Prostate cancer is one of the most prevalent cancers worldwide. One of the key factors in reducing its mortality is based on early detection. The computer-aided diagnosis systems for this task are based on the glandular structural analysis in histology images. Hence, accurate gland detection and segmentation is crucial for a successful prediction. The methodological basis of this work is a prostate gland segmentation based on U-Net convolutional neural network architectures modified with residual and multi-resolution blocks, trained using data augmentation techniques. The residual configuration outperforms in the test subset the previous state-of-the-art approaches in an image-level comparison, reaching an average Dice Index of 0.77.



### Hyper-Convolution Networks for Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2105.10559v2
- **DOI**: 10.1109/WACV51458.2022.00205
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10559v2)
- **Published**: 2021-05-21 20:31:08+00:00
- **Updated**: 2022-10-06 18:48:42+00:00
- **Authors**: Tianyu Ma, Adrian V. Dalca, Mert R. Sabuncu
- **Comment**: WACV 2022
- **Journal**: None
- **Summary**: The convolution operation is a central building block of neural network architectures widely used in computer vision. The size of the convolution kernels determines both the expressiveness of convolutional neural networks (CNN), as well as the number of learnable parameters. Increasing the network capacity to capture rich pixel relationships requires increasing the number of learnable parameters, often leading to overfitting and/or lack of robustness. In this paper, we propose a powerful novel building block, the hyper-convolution, which implicitly represents the convolution kernel as a function of kernel coordinates. Hyper-convolutions enable decoupling the kernel size, and hence its receptive field, from the number of learnable parameters. In our experiments, focused on challenging biomedical image segmentation tasks, we demonstrate that replacing regular convolutions with hyper-convolutions leads to more efficient architectures that achieve improved accuracy. Our analysis also shows that learned hyper-convolutions are naturally regularized, which can offer better generalization performance. We believe that hyper-convolutions can be a powerful building block in future neural network architectures for computer vision tasks. We provide all of our code here: https://github.com/tym002/Hyper-Convolution



### Puck localization and multi-task event recognition in broadcast hockey videos
- **Arxiv ID**: http://arxiv.org/abs/2105.10563v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2105.10563v1)
- **Published**: 2021-05-21 20:43:54+00:00
- **Updated**: 2021-05-21 20:43:54+00:00
- **Authors**: Kanav Vats, Mehrnaz Fani, David A. Clausi, John Zelek
- **Comment**: Accepted at CVSports 2021
- **Journal**: None
- **Summary**: Puck localization is an important problem in ice hockey video analytics useful for analyzing the game, determining play location, and assessing puck possession. The problem is challenging due to the small size of the puck, excessive motion blur due to high puck velocity and occlusions due to players and boards. In this paper, we introduce and implement a network for puck localization in broadcast hockey video. The network leverages expert NHL play-by-play annotations and uses temporal context to locate the puck. Player locations are incorporated into the network through an attention mechanism by encoding player positions with a Gaussian-based spatial heatmap drawn at player positions. Since event occurrence on the rink and puck location are related, we also perform event recognition by augmenting the puck localization network with an event recognition head and training the network through multi-task learning. Experimental results demonstrate that the network is able to localize the puck with an AUC of $73.1 \%$ on the test set. The puck location can be inferred in 720p broadcast videos at $5$ frames per second. It is also demonstrated that multi-task learning with puck location improves event recognition accuracy.



### High Throughput Soybean Pod-Counting with In-Field Robotic Data Collection and Machine-Vision Based Data Analysis
- **Arxiv ID**: http://arxiv.org/abs/2105.10568v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10568v2)
- **Published**: 2021-05-21 20:52:18+00:00
- **Updated**: 2021-05-27 21:20:51+00:00
- **Authors**: Michael McGuire, Chinmay Soman, Brian Diers, Girish Chowdhary
- **Comment**: None
- **Journal**: None
- **Summary**: We report promising results for high-throughput on-field soybean pod count with small mobile robots and machine-vision algorithms. Our results show that the machine-vision based soybean pod counts are strongly correlated with soybean yield. While pod counts has a strong correlation with soybean yield, pod counting is extremely labor intensive, and has been difficult to automate. Our results establish that an autonomous robot equipped with vision sensors can autonomously collect soybean data at maturity. Machine-vision algorithms can be used to estimate pod-counts across a large diversity panel planted across experimental units (EUs, or plots) in a high-throughput, automated manner. We report a correlation of 0.67 between our automated pod counts and soybean yield. The data was collected in an experiment consisting of 1463 single-row plots maintained by the University of Illinois soybean breeding program during the 2020 growing season. We also report a correlation of 0.88 between automated pod counts and manual pod counts over a smaller data set of 16 plots.



### Embracing New Techniques in Deep Learning for Estimating Image Memorability
- **Arxiv ID**: http://arxiv.org/abs/2105.10598v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, J.4; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2105.10598v3)
- **Published**: 2021-05-21 23:05:23+00:00
- **Updated**: 2022-01-08 22:57:41+00:00
- **Authors**: Coen D. Needell, Wilma A. Bainbridge
- **Comment**: 27 pages, 15 figures, Presented at the Proceedings of the Vision
  Sciences Society 2021
- **Journal**: None
- **Summary**: Various work has suggested that the memorability of an image is consistent across people, and thus can be treated as an intrinsic property of an image. Using computer vision models, we can make specific predictions about what people will remember or forget. While older work has used now-outdated deep learning architectures to predict image memorability, innovations in the field have given us new techniques to apply to this problem. Here, we propose and evaluate five alternative deep learning models which exploit developments in the field from the last five years, largely the introduction of residual neural networks, which are intended to allow the model to use semantic information in the memorability estimation process. These new models were tested against the prior state of the art with a combined dataset built to optimize both within-category and across-category predictions. Our findings suggest that the key prior memorability network had overstated its generalizability and was overfit on its training set. Our new models outperform this prior model, leading us to conclude that Residual Networks outperform simpler convolutional neural networks in memorability regression. We make our new state-of-the-art model readily available to the research community, allowing memory researchers to make predictions about memorability on a wider range of images.



### Automatic calibration of time of flight based non-line-of-sight reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2105.10603v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2105.10603v1)
- **Published**: 2021-05-21 23:16:32+00:00
- **Updated**: 2021-05-21 23:16:32+00:00
- **Authors**: Subhash Chandra Sadhu, Abhishek Singh, Tomohiro Maeda, Tristan Swedish, Ryan Kim, Lagnojita Sinha, Ramesh Raskar
- **Comment**: None
- **Journal**: None
- **Summary**: Time of flight based Non-line-of-sight (NLOS) imaging approaches require precise calibration of illumination and detector positions on the visible scene to produce reasonable results. If this calibration error is sufficiently high, reconstruction can fail entirely without any indication to the user. In this work, we highlight the necessity of building autocalibration into NLOS reconstruction in order to handle mis-calibration. We propose a forward model of NLOS measurements that is differentiable with respect to both, the hidden scene albedo, and virtual illumination and detector positions. With only a mean squared error loss and no regularization, our model enables joint reconstruction and recovery of calibration parameters by minimizing the measurement residual using gradient descent. We demonstrate our method is able to produce robust reconstructions using simulated and real data where the calibration error applied causes other state of the art algorithms to fail.



