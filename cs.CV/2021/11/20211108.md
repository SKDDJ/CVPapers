# Arxiv Papers in cs.CV on 2021-11-08
### Dense Representative Tooth Landmark/axis Detection Network on 3D Model
- **Arxiv ID**: http://arxiv.org/abs/2111.04212v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2111.04212v2)
- **Published**: 2021-11-08 00:42:22+00:00
- **Updated**: 2021-11-09 03:48:51+00:00
- **Authors**: Guangshun Wei, Zhiming Cui, Jie Zhu, Lei Yang, Yuanfeng Zhou, Pradeep Singh, Min Gu, Wenping Wang
- **Comment**: 11pages,27figures
- **Journal**: None
- **Summary**: Artificial intelligence (AI) technology is increasingly used for digital orthodontics, but one of the challenges is to automatically and accurately detect tooth landmarks and axes. This is partly because of sophisticated geometric definitions of them, and partly due to large variations among individual tooth and across different types of tooth. As such, we propose a deep learning approach with a labeled dataset by professional dentists to the tooth landmark/axis detection on tooth model that are crucial for orthodontic treatments. Our method can extract not only tooth landmarks in the form of point (e.g. cusps), but also axes that measure the tooth angulation and inclination. The proposed network takes as input a 3D tooth model and predicts various types of the tooth landmarks and axes. Specifically, we encode the landmarks and axes as dense fields defined on the surface of the tooth model. This design choice and a set of added components make the proposed network more suitable for extracting sparse landmarks from a given 3D tooth model. Extensive evaluation of the proposed method was conducted on a set of dental models prepared by experienced dentists. Results show that our method can produce tooth landmarks with high accuracy. Our method was examined and justified via comparison with the state-of-the-art methods as well as the ablation studies.



### Rethinking Deconvolution for 2D Human Pose Estimation Light yet Accurate Model for Real-time Edge Computing
- **Arxiv ID**: http://arxiv.org/abs/2111.04226v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2111.04226v1)
- **Published**: 2021-11-08 01:44:46+00:00
- **Updated**: 2021-11-08 01:44:46+00:00
- **Authors**: Masayuki Yamazaki, Eigo Mori
- **Comment**: IEEE International Conference on Automatic Face and Gesture
  Recognition 2021
- **Journal**: None
- **Summary**: In this study, we present a pragmatic lightweight pose estimation model. Our model can achieve real-time predictions using low-power embedded devices. This system was found to be very accurate and achieved a 94.5% accuracy of SOTA HRNet 256x192 using a computational cost of only 3.8% on COCO test dataset. Our model adopts an encoder-decoder architecture and is carefully downsized to improve its efficiency. We especially focused on optimizing the deconvolution layers and observed that the channel reduction of the deconvolution layers contributes significantly to reducing computational resource consumption without degrading the accuracy of this system. We also incorporated recent model agnostic techniques such as DarkPose and distillation training to maximize the efficiency of our model. Furthermore, we applied model quantization to exploit multi/mixed precision features. Our FP16'ed model (COCO AP 70.0) operates at ~60-fps on NVIDIA Jetson AGX Xavier and ~200 fps on NVIDIA Quadro RTX6000.



### Practical, Fast and Robust Point Cloud Registration for 3D Scene Stitching and Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2111.04228v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04228v1)
- **Published**: 2021-11-08 01:49:04+00:00
- **Updated**: 2021-11-08 01:49:04+00:00
- **Authors**: Lei Sun
- **Comment**: None
- **Journal**: None
- **Summary**: 3D point cloud registration ranks among the most fundamental problems in remote sensing, photogrammetry, robotics and geometric computer vision. Due to the limited accuracy of 3D feature matching techniques, outliers may exist, sometimes even in very large numbers, among the correspondences. Since existing robust solvers may encounter high computational cost or restricted robustness, we propose a novel, fast and highly robust solution, named VOCRA (VOting with Cost function and Rotating Averaging), for the point cloud registration problem with extreme outlier rates. Our first contribution is to employ the Tukey's Biweight robust cost to introduce a new voting and correspondence sorting technique, which proves to be rather effective in distinguishing true inliers from outliers even with extreme (99%) outlier rates. Our second contribution consists in designing a time-efficient consensus maximization paradigm based on robust rotation averaging, serving to seek inlier candidates among the correspondences. Finally, we apply Graduated Non-Convexity with Tukey's Biweight (GNC-TB) to estimate the correct transformation with the inlier candidates obtained, which is then used to find the complete inlier set. Both standard benchmarking and realistic experiments with application to two real-data problems are conducted, and we show that our solver VOCRA is robust against over 99% outliers and more time-efficient than the state-of-the-art competitors.



### A Study of the Human Perception of Synthetic Faces
- **Arxiv ID**: http://arxiv.org/abs/2111.04230v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04230v1)
- **Published**: 2021-11-08 02:03:18+00:00
- **Updated**: 2021-11-08 02:03:18+00:00
- **Authors**: Bingyu Shen, Brandon RichardWebster, Alice O'Toole, Kevin Bowyer, Walter J. Scheirer
- **Comment**: None
- **Journal**: None
- **Summary**: Advances in face synthesis have raised alarms about the deceptive use of synthetic faces. Can synthetic identities be effectively used to fool human observers? In this paper, we introduce a study of the human perception of synthetic faces generated using different strategies including a state-of-the-art deep learning-based GAN model. This is the first rigorous study of the effectiveness of synthetic face generation techniques grounded in experimental techniques from psychology. We answer important questions such as how often do GAN-based and more traditional image processing-based techniques confuse human observers, and are there subtle cues within a synthetic face image that cause humans to perceive it as a fake without having to search for obvious clues? To answer these questions, we conducted a series of large-scale crowdsourced behavioral experiments with different sources of face imagery. Results show that humans are unable to distinguish synthetic faces from real faces under several different circumstances. This finding has serious implications for many different applications where face images are presented to human users.



### Deep Learning Adapted Acceleration for Limited-view Photoacoustic Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/2111.05194v1
- **DOI**: 10.1364/OL.450860
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05194v1)
- **Published**: 2021-11-08 02:05:58+00:00
- **Updated**: 2021-11-08 02:05:58+00:00
- **Authors**: Hengrong Lan, Jiali Gong, Fei Gao
- **Comment**: submitted the journal version
- **Journal**: None
- **Summary**: Photoacoustic imaging (PAI) is a non-invasive imaging modality that detects the ultrasound signal generated from tissue with light excitation. Photoacoustic computed tomography (PACT) uses unfocused large-area light to illuminate the target with ultrasound transducer array for PA signal detection. Limited-view issue could cause a low-quality image in PACT due to the limitation of geometric condition. The model-based method is used to resolve this problem, which contains different regularization. To adapt fast and high-quality reconstruction of limited-view PA data, in this paper, a model-based method that combines the mathematical variational model with deep learning is proposed to speed up and regularize the unrolled procedure of reconstruction. A deep neural network is designed to adapt the step of the gradient updated term of data consistency in the gradient descent procedure, which can obtain a high-quality PA image only with a few iterations. Note that all parameters and priors are automatically learned during the offline training stage. In experiments, we show that this method outperforms the other methods with half-view (180 degrees) simulation and real data. The comparison of different model-based methods show that our proposed scheme has superior performances (over 0.05 for SSIM) with same iteration (3 times) steps. Furthermore, an unseen data is used to validate the generalization of different methods. Finally, we find that our method obtains superior results (0.94 value of SSIM for in vivo) with a high robustness and accelerated reconstruction.



### Template NeRF: Towards Modeling Dense Shape Correspondences from Category-Specific Object Images
- **Arxiv ID**: http://arxiv.org/abs/2111.04237v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04237v1)
- **Published**: 2021-11-08 02:16:48+00:00
- **Updated**: 2021-11-08 02:16:48+00:00
- **Authors**: Jianfei Guo, Zhiyuan Yang, Xi Lin, Qingfu Zhang
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: We present neural radiance fields (NeRF) with templates, dubbed Template-NeRF, for modeling appearance and geometry and generating dense shape correspondences simultaneously among objects of the same category from only multi-view posed images, without the need of either 3D supervision or ground-truth correspondence knowledge. The learned dense correspondences can be readily used for various image-based tasks such as keypoint detection, part segmentation, and texture transfer that previously require specific model designs. Our method can also accommodate annotation transfer in a one or few-shot manner, given only one or a few instances of the category. Using periodic activation and feature-wise linear modulation (FiLM) conditioning, we introduce deep implicit templates on 3D data into the 3D-aware image synthesis pipeline NeRF. By representing object instances within the same category as shape and appearance variation of a shared NeRF template, our proposed method can achieve dense shape correspondences reasoning on images for a wide range of object classes. We demonstrate the results and applications on both synthetic and real-world data with competitive results compared with other methods based on 3D information.



### Cross-Modal Object Tracking: Modality-Aware Representations and A Unified Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2111.04264v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04264v2)
- **Published**: 2021-11-08 03:58:55+00:00
- **Updated**: 2021-11-11 08:30:58+00:00
- **Authors**: Chenglong Li, Tianhao Zhu, Lei Liu, Xiaonan Si, Zilin Fan, Sulan Zhai
- **Comment**: In Submission
- **Journal**: None
- **Summary**: In many visual systems, visual tracking often bases on RGB image sequences, in which some targets are invalid in low-light conditions, and tracking performance is thus affected significantly. Introducing other modalities such as depth and infrared data is an effective way to handle imaging limitations of individual sources, but multi-modal imaging platforms usually require elaborate designs and cannot be applied in many real-world applications at present. Near-infrared (NIR) imaging becomes an essential part of many surveillance cameras, whose imaging is switchable between RGB and NIR based on the light intensity. These two modalities are heterogeneous with very different visual properties and thus bring big challenges for visual tracking. However, existing works have not studied this challenging problem. In this work, we address the cross-modal object tracking problem and contribute a new video dataset, including 654 cross-modal image sequences with over 481K frames in total, and the average video length is more than 735 frames. To promote the research and development of cross-modal object tracking, we propose a new algorithm, which learns the modality-aware target representation to mitigate the appearance gap between RGB and NIR modalities in the tracking process. It is plug-and-play and could thus be flexibly embedded into different tracking frameworks. Extensive experiments on the dataset are conducted, and we demonstrate the effectiveness of the proposed algorithm in two representative tracking frameworks against 17 state-of-the-art tracking methods. We will release the dataset for free academic usage, dataset download link and code will be released soon.



### Adaptive area-preserving parameterization of open and closed anatomical surfaces
- **Arxiv ID**: http://arxiv.org/abs/2111.04265v1
- **DOI**: 10.1016/j.compbiomed.2022.105715
- **Categories**: **cs.CG**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2111.04265v1)
- **Published**: 2021-11-08 04:15:22+00:00
- **Updated**: 2021-11-08 04:15:22+00:00
- **Authors**: Gary P. T. Choi, Amita Giri, Lalan Kumar
- **Comment**: None
- **Journal**: Computers in Biology and Medicine, 105715 (2022)
- **Summary**: The parameterization of open and closed anatomical surfaces is of fundamental importance in many biomedical applications. Spherical harmonics, a set of basis functions defined on the unit sphere, are widely used for anatomical shape description. However, establishing a one-to-one correspondence between the object surface and the entire unit sphere may induce a large geometric distortion in case the shape of the surface is too different from a perfect sphere. In this work, we propose adaptive area-preserving parameterization methods for simply-connected open and closed surfaces with the target of the parameterization being a spherical cap. Our methods optimize the shape of the parameter domain along with the mapping from the object surface to the parameter domain. The object surface will be globally mapped to an optimal spherical cap region of the unit sphere in an area-preserving manner while also exhibiting low conformal distortion. We further develop a set of spherical harmonics-like basis functions defined over the adaptive spherical cap domain, which we call the adaptive harmonics. Experimental results show that the proposed parameterization methods outperform the existing methods for both open and closed anatomical surfaces in terms of area and angle distortion. Surface description of the object surfaces can be effectively achieved using a novel combination of the adaptive parameterization and the adaptive harmonics. Our work provides a novel way of mapping anatomical surfaces with improved accuracy and greater flexibility. More broadly, the idea of using an adaptive parameter domain allows easy handling of a wide range of biomedical shapes.



### Generative Dynamic Patch Attack
- **Arxiv ID**: http://arxiv.org/abs/2111.04266v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04266v2)
- **Published**: 2021-11-08 04:15:34+00:00
- **Updated**: 2021-11-15 16:39:22+00:00
- **Authors**: Xiang Li, Shihao Ji
- **Comment**: Published as a conference paper at BMVC 2021
- **Journal**: None
- **Summary**: Adversarial patch attack is a family of attack algorithms that perturb a part of image to fool a deep neural network model. Existing patch attacks mostly consider injecting adversarial patches at input-agnostic locations: either a predefined location or a random location. This attack setup may be sufficient for attack but has considerable limitations when using it for adversarial training. Thus, robust models trained with existing patch attacks cannot effectively defend other adversarial attacks. In this paper, we first propose an end-to-end patch attack algorithm, Generative Dynamic Patch Attack (GDPA), which generates both patch pattern and patch location adversarially for each input image. We show that GDPA is a generic attack framework that can produce dynamic/static and visible/invisible patches with a few configuration changes. Secondly, GDPA can be readily integrated for adversarial training to improve model robustness to various adversarial attacks. Extensive experiments on VGGFace, Traffic Sign and ImageNet show that GDPA achieves higher attack success rates than state-of-the-art patch attacks, while adversarially trained model with GDPA demonstrates superior robustness to adversarial patch attacks than competing methods. Our source code can be found at https://github.com/lxuniverse/gdpa.



### Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2111.04276v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.04276v1)
- **Published**: 2021-11-08 05:29:35+00:00
- **Updated**: 2021-11-08 05:29:35+00:00
- **Authors**: Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, Sanja Fidler
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce DMTet, a deep 3D conditional generative model that can synthesize high-resolution 3D shapes using simple user guides such as coarse voxels. It marries the merits of implicit and explicit 3D representations by leveraging a novel hybrid 3D representation. Compared to the current implicit approaches, which are trained to regress the signed distance values, DMTet directly optimizes for the reconstructed surface, which enables us to synthesize finer geometric details with fewer artifacts. Unlike deep 3D generative models that directly generate explicit representations such as meshes, our model can synthesize shapes with arbitrary topology. The core of DMTet includes a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of the surface geometry and topology as well as generation of the hierarchy of subdivisions using reconstruction and adversarial losses defined explicitly on the surface mesh. Our approach significantly outperforms existing work on conditional shape synthesis from coarse voxel inputs, trained on a dataset of complex 3D animal shapes. Project page: https://nv-tlabs.github.io/DMTet/.



### Residual-Guided Learning Representation for Self-Supervised Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2111.04310v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04310v1)
- **Published**: 2021-11-08 07:44:31+00:00
- **Updated**: 2021-11-08 07:44:31+00:00
- **Authors**: Byeongjun Park, Taekyung Kim, Hyojun Go, Changick Kim
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: Photometric consistency loss is one of the representative objective functions commonly used for self-supervised monocular depth estimation. However, this loss often causes unstable depth predictions in textureless or occluded regions due to incorrect guidance. Recent self-supervised learning approaches tackle this issue by utilizing feature representations explicitly learned from auto-encoders, expecting better discriminability than the input image. Despite the use of auto-encoded features, we observe that the method does not embed features as discriminative as auto-encoded features. In this paper, we propose residual guidance loss that enables the depth estimation network to embed the discriminative feature by transferring the discriminability of auto-encoded features. We conducted experiments on the KITTI benchmark and verified our method's superiority and orthogonality on other state-of-the-art methods.



### A Relational Model for One-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2111.04313v1
- **DOI**: 10.14428/esann/2021.ES2021-75
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04313v1)
- **Published**: 2021-11-08 07:53:12+00:00
- **Updated**: 2021-11-08 07:53:12+00:00
- **Authors**: Arturs Polis, Alexander Ilin
- **Comment**: Published at ESANN 2021
- **Journal**: None
- **Summary**: We show that a deep learning model with built-in relational inductive bias can bring benefits to sample-efficient learning, without relying on extensive data augmentation. The proposed one-shot classification model performs relational matching of a pair of inputs in the form of local and pairwise attention. Our approach solves perfectly the one-shot image classification Omniglot challenge. Our model exceeds human level accuracy, as well as the previous state of the art, with no data augmentation.



### Real-time landmark detection for precise endoscopic submucosal dissection via shape-aware relation network
- **Arxiv ID**: http://arxiv.org/abs/2111.04733v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04733v1)
- **Published**: 2021-11-08 07:57:30+00:00
- **Updated**: 2021-11-08 07:57:30+00:00
- **Authors**: Jiacheng Wang, Yueming Jin, Shuntian Cai, Hongzhi Xu, Pheng-Ann Heng, Jing Qin, Liansheng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel shape-aware relation network for accurate and real-time landmark detection in endoscopic submucosal dissection (ESD) surgery. This task is of great clinical significance but extremely challenging due to bleeding, lighting reflection, and motion blur in the complicated surgical environment. Compared with existing solutions, which either neglect geometric relationships among targeting objects or capture the relationships by using complicated aggregation schemes, the proposed network is capable of achieving satisfactory accuracy while maintaining real-time performance by taking full advantage of the spatial relations among landmarks. We first devise an algorithm to automatically generate relation keypoint heatmaps, which are able to intuitively represent the prior knowledge of spatial relations among landmarks without using any extra manual annotation efforts. We then develop two complementary regularization schemes to progressively incorporate the prior knowledge into the training process. While one scheme introduces pixel-level regularization by multi-task learning, the other integrates global-level regularization by harnessing a newly designed grouped consistency evaluator, which adds relation constraints to the proposed network in an adversarial manner. Both schemes are beneficial to the model in training, and can be readily unloaded in inference to achieve real-time detection. We establish a large in-house dataset of ESD surgery for esophageal cancer to validate the effectiveness of our proposed method. Extensive experimental results demonstrate that our approach outperforms state-of-the-art methods in terms of accuracy and efficiency, achieving better detection results faster. Promising results on two downstream applications further corroborate the great potential of our method in ESD clinical practice.



### SEGA: Semantic Guided Attention on Visual Prototype for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.04316v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04316v1)
- **Published**: 2021-11-08 08:03:44+00:00
- **Updated**: 2021-11-08 08:03:44+00:00
- **Authors**: Fengyuan Yang, Ruiping Wang, Xilin Chen
- **Comment**: 11 pages, 7 figures, 4 tables. Accepted by WACV2022
- **Journal**: None
- **Summary**: Teaching machines to recognize a new category based on few training samples especially only one remains challenging owing to the incomprehensive understanding of the novel category caused by the lack of data. However, human can learn new classes quickly even given few samples since human can tell what discriminative features should be focused on about each category based on both the visual and semantic prior knowledge. To better utilize those prior knowledge, we propose the SEmantic Guided Attention (SEGA) mechanism where the semantic knowledge is used to guide the visual perception in a top-down manner about what visual features should be paid attention to when distinguishing a category from the others. As a result, the embedding of the novel class even with few samples can be more discriminative. Concretely, a feature extractor is trained to embed few images of each novel class into a visual prototype with the help of transferring visual prior knowledge from base classes. Then we learn a network that maps semantic knowledge to category-specific attention vectors which will be used to perform feature selection to enhance the visual prototypes. Extensive experiments on miniImageNet, tieredImageNet, CIFAR-FS, and CUB indicate that our semantic guided attention realizes anticipated function and outperforms state-of-the-art results.



### Auto-Encoding Knowledge Graph for Unsupervised Medical Report Generation
- **Arxiv ID**: http://arxiv.org/abs/2111.04318v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04318v2)
- **Published**: 2021-11-08 08:10:47+00:00
- **Updated**: 2021-11-15 09:20:17+00:00
- **Authors**: Fenglin Liu, Chenyu You, Xian Wu, Shen Ge, Sheng Wang, Xu Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Medical report generation, which aims to automatically generate a long and coherent report of a given medical image, has been receiving growing research interests. Existing approaches mainly adopt a supervised manner and heavily rely on coupled image-report pairs. However, in the medical domain, building a large-scale image-report paired dataset is both time-consuming and expensive. To relax the dependency on paired data, we propose an unsupervised model Knowledge Graph Auto-Encoder (KGAE) which accepts independent sets of images and reports in training. KGAE consists of a pre-constructed knowledge graph, a knowledge-driven encoder and a knowledge-driven decoder. The knowledge graph works as the shared latent space to bridge the visual and textual domains; The knowledge-driven encoder projects medical images and reports to the corresponding coordinates in this latent space and the knowledge-driven decoder generates a medical report given a coordinate in this space. Since the knowledge-driven encoder and decoder can be trained with independent sets of images and reports, KGAE is unsupervised. The experiments show that the unsupervised KGAE generates desirable medical reports without using any image-report training pairs. Moreover, KGAE can also work in both semi-supervised and supervised settings, and accept paired images and reports in training. By further fine-tuning with image-report pairs, KGAE consistently outperforms the current state-of-the-art models on two datasets.



### Towards Debiasing Temporal Sentence Grounding in Video
- **Arxiv ID**: http://arxiv.org/abs/2111.04321v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2111.04321v1)
- **Published**: 2021-11-08 08:18:25+00:00
- **Updated**: 2021-11-08 08:18:25+00:00
- **Authors**: Hao Zhang, Aixin Sun, Wei Jing, Joey Tianyi Zhou
- **Comment**: 13 pages, 6 figures, 11 tables
- **Journal**: None
- **Summary**: The temporal sentence grounding in video (TSGV) task is to locate a temporal moment from an untrimmed video, to match a language query, i.e., a sentence. Without considering bias in moment annotations (e.g., start and end positions in a video), many models tend to capture statistical regularities of the moment annotations, and do not well learn cross-modal reasoning between video and language query. In this paper, we propose two debiasing strategies, data debiasing and model debiasing, to "force" a TSGV model to capture cross-modal interactions. Data debiasing performs data oversampling through video truncation to balance moment temporal distribution in train set. Model debiasing leverages video-only and query-only models to capture the distribution bias, and forces the model to learn cross-modal interactions. Using VSLNet as the base model, we evaluate impact of the two strategies on two datasets that contain out-of-distribution test instances. Results show that both strategies are effective in improving model generalization capability. Equipped with both debiasing strategies, VSLNet achieves best results on both datasets.



### Enhancing Prototypical Few-Shot Learning by Leveraging the Local-Level Strategy
- **Arxiv ID**: http://arxiv.org/abs/2111.04331v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04331v1)
- **Published**: 2021-11-08 08:45:15+00:00
- **Updated**: 2021-11-08 08:45:15+00:00
- **Authors**: Junying Huang, Fan Chen, Keze Wang, Liang Lin, Dongyu Zhang
- **Comment**: 5 pages, 4 figures, submitted to ICASSP 2022
- **Journal**: None
- **Summary**: Aiming at recognizing the samples from novel categories with few reference samples, few-shot learning (FSL) is a challenging problem. We found that the existing works often build their few-shot model based on the image-level feature by mixing all local-level features, which leads to the discriminative location bias and information loss in local details. To tackle the problem, this paper returns the perspective to the local-level feature and proposes a series of local-level strategies. Specifically, we present (a) a local-agnostic training strategy to avoid the discriminative location bias between the base and novel categories, (b) a novel local-level similarity measure to capture the accurate comparison between local-level features, and (c) a local-level knowledge transfer that can synthesize different knowledge transfers from the base category according to different location features. Extensive experiments justify that our proposed local-level strategies can significantly boost the performance and achieve 2.8%-7.2% improvements over the baseline across different benchmark datasets, which also achieves state-of-the-art accuracy.



### Partial Attack Supervision and Regional Weighted Inference for Masked Face Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2111.04336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04336v1)
- **Published**: 2021-11-08 08:53:46+00:00
- **Updated**: 2021-11-08 08:53:46+00:00
- **Authors**: Meiling Fang, Fadi Boutros, Arjan Kuijper, Naser Damer
- **Comment**: Accepted at the 16th IEEE International Conference on Automatic Face
  and Gesture Recognition, FG 2021
- **Journal**: None
- **Summary**: Wearing a mask has proven to be one of the most effective ways to prevent the transmission of SARS-CoV-2 coronavirus. However, wearing a mask poses challenges for different face recognition tasks and raises concerns about the performance of masked face presentation detection (PAD). The main issues facing the mask face PAD are the wrongly classified bona fide masked faces and the wrongly classified partial attacks (covered by real masks). This work addresses these issues by proposing a method that considers partial attack labels to supervise the PAD model training, as well as regional weighted inference to further improve the PAD performance by varying the focus on different facial areas. Our proposed method is not directly linked to specific network architecture and thus can be directly incorporated into any common or custom-designed network. In our work, two neural networks (DeepPixBis and MixFaceNet) are selected as backbones. The experiments are demonstrated on the collaborative real mask attack (CRMA) database. Our proposed method outperforms established PAD methods in the CRMA database by reducing the mentioned shortcomings when facing masked faces. Moreover, we present a detailed step-wise ablation study pointing out the individual and joint benefits of the proposed concepts on the overall PAD performance.



### Mixed Transformer U-Net For Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.04734v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04734v2)
- **Published**: 2021-11-08 09:03:46+00:00
- **Updated**: 2021-11-11 05:51:20+00:00
- **Authors**: Hongyi Wang, Shiao Xie, Lanfen Lin, Yutaro Iwamoto, Xian-Hua Han, Yen-Wei Chen, Ruofeng Tong
- **Comment**: None
- **Journal**: None
- **Summary**: Though U-Net has achieved tremendous success in medical image segmentation tasks, it lacks the ability to explicitly model long-range dependencies. Therefore, Vision Transformers have emerged as alternative segmentation structures recently, for their innate ability of capturing long-range correlations through Self-Attention (SA). However, Transformers usually rely on large-scale pre-training and have high computational complexity. Furthermore, SA can only model self-affinities within a single sample, ignoring the potential correlations of the overall dataset. To address these problems, we propose a novel Transformer module named Mixed Transformer Module (MTM) for simultaneous inter- and intra- affinities learning. MTM first calculates self-affinities efficiently through our well-designed Local-Global Gaussian-Weighted Self-Attention (LGG-SA). Then, it mines inter-connections between data samples through External Attention (EA). By using MTM, we construct a U-shaped model named Mixed Transformer U-Net (MT-UNet) for accurate medical image segmentation. We test our method on two different public datasets, and the experimental results show that the proposed method achieves better performance over other state-of-the-art methods. The code is available at: https://github.com/Dootmaan/MT-UNet.



### Off-policy Imitation Learning from Visual Inputs
- **Arxiv ID**: http://arxiv.org/abs/2111.04345v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04345v1)
- **Published**: 2021-11-08 09:06:12+00:00
- **Updated**: 2021-11-08 09:06:12+00:00
- **Authors**: Zhihao Cheng, Li Shen, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, various successful applications utilizing expert states in imitation learning (IL) have been witnessed. However, another IL setting -- IL from visual inputs (ILfVI), which has a greater promise to be applied in reality by utilizing online visual resources, suffers from low data-efficiency and poor performance resulted from an on-policy learning manner and high-dimensional visual inputs. We propose OPIfVI (Off-Policy Imitation from Visual Inputs), which is composed of an off-policy learning manner, data augmentation, and encoder techniques, to tackle the mentioned challenges, respectively. More specifically, to improve data-efficiency, OPIfVI conducts IL in an off-policy manner, with which sampled data can be used multiple times. In addition, we enhance the stability of OPIfVI with spectral normalization to mitigate the side-effect of off-policy training. The core factor, contributing to the poor performance of ILfVI, that we think is the agent could not extract meaningful features from visual inputs. Hence, OPIfVI employs data augmentation from computer vision to help train encoders that can better extract features from visual inputs. In addition, a specific structure of gradient backpropagation for the encoder is designed to stabilize the encoder training. At last, we demonstrate that OPIfVI is able to achieve expert-level performance and outperform existing baselines no matter visual demonstrations or visual observations are provided via extensive experiments using DeepMind Control Suite.



### Segmentation of Multiple Myeloma Plasma Cells in Microscopy Images with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2111.05125v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2111.05125v1)
- **Published**: 2021-11-08 09:16:23+00:00
- **Updated**: 2021-11-08 09:16:23+00:00
- **Authors**: Álvaro García Faura, Dejan Štepec, Tomaž Martinčič, Danijel Skočaj
- **Comment**: Accepted to SPIE Medical Imaging conference
- **Journal**: None
- **Summary**: A key component towards an improved and fast cancer diagnosis is the development of computer-assisted tools. In this article, we present the solution that won the SegPC-2021 competition for the segmentation of multiple myeloma plasma cells in microscopy images. The labels used in the competition dataset were generated semi-automatically and presented noise. To deal with it, a heavy image augmentation procedure was carried out and predictions from several models were combined using a custom ensemble strategy. State-of-the-art feature extractors and instance segmentation architectures were used, resulting in a mean Intersection-over-Union of 0.9389 on the SegPC-2021 final test set.



### Grassmannian learning mutual subspace method for image set recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.04352v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04352v1)
- **Published**: 2021-11-08 09:16:36+00:00
- **Updated**: 2021-11-08 09:16:36+00:00
- **Authors**: Lincon S. Souza, Naoya Sogi, Bernardo B. Gatto, Takumi Kobayashi, Kazuhiro Fukui
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of object recognition given a set of images as input (e.g., multiple camera sources and video frames). Convolutional neural network (CNN)-based frameworks do not exploit these sets effectively, processing a pattern as observed, not capturing the underlying feature distribution as it does not consider the variance of images in the set. To address this issue, we propose the Grassmannian learning mutual subspace method (G-LMSM), a NN layer embedded on top of CNNs as a classifier, that can process image sets more effectively and can be trained in an end-to-end manner. The image set is represented by a low-dimensional input subspace; and this input subspace is matched with reference subspaces by a similarity of their canonical angles, an interpretable and easy to compute metric. The key idea of G-LMSM is that the reference subspaces are learned as points on the Grassmann manifold, optimized with Riemannian stochastic gradient descent. This learning is stable, efficient and theoretically well-grounded. We demonstrate the effectiveness of our proposed method on hand shape recognition, face identification, and facial emotion recognition.



### Geometrically Adaptive Dictionary Attack on Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.04371v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.04371v1)
- **Published**: 2021-11-08 10:26:28+00:00
- **Updated**: 2021-11-08 10:26:28+00:00
- **Authors**: Junyoung Byun, Hyojun Go, Changick Kim
- **Comment**: Accepted at WACV 2022
- **Journal**: None
- **Summary**: CNN-based face recognition models have brought remarkable performance improvement, but they are vulnerable to adversarial perturbations. Recent studies have shown that adversaries can fool the models even if they can only access the models' hard-label output. However, since many queries are needed to find imperceptible adversarial noise, reducing the number of queries is crucial for these attacks. In this paper, we point out two limitations of existing decision-based black-box attacks. We observe that they waste queries for background noise optimization, and they do not take advantage of adversarial perturbations generated for other images. We exploit 3D face alignment to overcome these limitations and propose a general strategy for query-efficient black-box attacks on face recognition named Geometrically Adaptive Dictionary Attack (GADA). Our core idea is to create an adversarial perturbation in the UV texture map and project it onto the face in the image. It greatly improves query efficiency by limiting the perturbation search space to the facial area and effectively recycling previous perturbations. We apply the GADA strategy to two existing attack methods and show overwhelming performance improvement in the experiments on the LFW and CPLFW datasets. Furthermore, we also present a novel attack strategy that can circumvent query similarity-based stateful detection that identifies the process of query-based black-box attacks.



### Feature-enhanced Generation and Multi-modality Fusion based Deep Neural Network for Brain Tumor Segmentation with Missing MR Modalities
- **Arxiv ID**: http://arxiv.org/abs/2111.04735v1
- **DOI**: 10.1016/j.neucom.2021.09.032
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.04735v1)
- **Published**: 2021-11-08 10:59:40+00:00
- **Updated**: 2021-11-08 10:59:40+00:00
- **Authors**: Tongxue Zhou, Stéphane Canu, Pierre Vera, Su Ruan
- **Comment**: 30 pages, 7 figures
- **Journal**: Neurocomputing 2021
- **Summary**: Using multimodal Magnetic Resonance Imaging (MRI) is necessary for accurate brain tumor segmentation. The main problem is that not all types of MRIs are always available in clinical exams. Based on the fact that there is a strong correlation between MR modalities of the same patient, in this work, we propose a novel brain tumor segmentation network in the case of missing one or more modalities. The proposed network consists of three sub-networks: a feature-enhanced generator, a correlation constraint block and a segmentation network. The feature-enhanced generator utilizes the available modalities to generate 3D feature-enhanced image representing the missing modality. The correlation constraint block can exploit the multi-source correlation between the modalities and also constrain the generator to synthesize a feature-enhanced modality which must have a coherent correlation with the available modalities. The segmentation network is a multi-encoder based U-Net to achieve the final brain tumor segmentation. The proposed method is evaluated on BraTS 2018 dataset. Experimental results demonstrate the effectiveness of the proposed method which achieves the average Dice Score of 82.9, 74.9 and 59.1 on whole tumor, tumor core and enhancing tumor, respectively across all the situations, and outperforms the best method by 3.5%, 17% and 18.2%.



### Get a Model! Model Hijacking Attack Against Machine Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2111.04394v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.04394v1)
- **Published**: 2021-11-08 11:30:50+00:00
- **Updated**: 2021-11-08 11:30:50+00:00
- **Authors**: Ahmed Salem, Michael Backes, Yang Zhang
- **Comment**: To Appear in NDSS 2022
- **Journal**: None
- **Summary**: Machine learning (ML) has established itself as a cornerstone for various critical applications ranging from autonomous driving to authentication systems. However, with this increasing adoption rate of machine learning models, multiple attacks have emerged. One class of such attacks is training time attack, whereby an adversary executes their attack before or during the machine learning model training. In this work, we propose a new training time attack against computer vision based machine learning models, namely model hijacking attack. The adversary aims to hijack a target model to execute a different task than its original one without the model owner noticing. Model hijacking can cause accountability and security risks since a hijacked model owner can be framed for having their model offering illegal or unethical services. Model hijacking attacks are launched in the same way as existing data poisoning attacks. However, one requirement of the model hijacking attack is to be stealthy, i.e., the data samples used to hijack the target model should look similar to the model's original training dataset. To this end, we propose two different model hijacking attacks, namely Chameleon and Adverse Chameleon, based on a novel encoder-decoder style ML model, namely the Camouflager. Our evaluation shows that both of our model hijacking attacks achieve a high attack success rate, with a negligible drop in model utility.



### GROWL: Group Detection With Link Prediction
- **Arxiv ID**: http://arxiv.org/abs/2111.04397v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2111.04397v1)
- **Published**: 2021-11-08 11:52:48+00:00
- **Updated**: 2021-11-08 11:52:48+00:00
- **Authors**: Viktor Schmuck, Oya Celiktutan
- **Comment**: None
- **Journal**: None
- **Summary**: Interaction group detection has been previously addressed with bottom-up approaches which relied on the position and orientation information of individuals. These approaches were primarily based on pairwise affinity matrices and were limited to static, third-person views. This problem can greatly benefit from a holistic approach based on Graph Neural Networks (GNNs) beyond pairwise relationships, due to the inherent spatial configuration that exists between individuals who form interaction groups. Our proposed method, GROup detection With Link prediction (GROWL), demonstrates the effectiveness of a GNN based approach. GROWL predicts the link between two individuals by generating a feature embedding based on their neighbourhood in the graph and determines whether they are connected with a shallow binary classification method such as Multi-layer Perceptrons (MLPs). We test our method against other state-of-the-art group detection approaches on both a third-person view dataset and a robocentric (i.e., egocentric) dataset. In addition, we propose a multimodal approach based on RGB and depth data to calculate a representation GROWL can utilise as input. Our results show that a GNN based approach can significantly improve accuracy across different camera views, i.e., third-person and egocentric views.



### 3D Siamese Voxel-to-BEV Tracker for Sparse Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2111.04426v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04426v2)
- **Published**: 2021-11-08 12:47:11+00:00
- **Updated**: 2021-11-17 13:16:18+00:00
- **Authors**: Le Hui, Lingpeng Wang, Mingmei Cheng, Jin Xie, Jian Yang
- **Comment**: Accepted by NeurIPS 2021
- **Journal**: None
- **Summary**: 3D object tracking in point clouds is still a challenging problem due to the sparsity of LiDAR points in dynamic environments. In this work, we propose a Siamese voxel-to-BEV tracker, which can significantly improve the tracking performance in sparse 3D point clouds. Specifically, it consists of a Siamese shape-aware feature learning network and a voxel-to-BEV target localization network. The Siamese shape-aware feature learning network can capture 3D shape information of the object to learn the discriminative features of the object so that the potential target from the background in sparse point clouds can be identified. To this end, we first perform template feature embedding to embed the template's feature into the potential target and then generate a dense 3D shape to characterize the shape information of the potential target. For localizing the tracked target, the voxel-to-BEV target localization network regresses the target's 2D center and the $z$-axis center from the dense bird's eye view (BEV) feature map in an anchor-free manner. Concretely, we compress the voxelized point cloud along $z$-axis through max pooling to obtain a dense BEV feature map, where the regression of the 2D center and the $z$-axis center can be performed more effectively. Extensive evaluation on the KITTI and nuScenes datasets shows that our method significantly outperforms the current state-of-the-art methods by a large margin.



### Multi-Modality Cardiac Image Analysis with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.04736v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04736v1)
- **Published**: 2021-11-08 12:54:11+00:00
- **Updated**: 2021-11-08 12:54:11+00:00
- **Authors**: Lei Li, Fuping Wu, Sihang Wang, Xiahai Zhuang
- **Comment**: Under review as a chapter of book 'Deep Learning for Medical Image
  Analysis, 2E'
- **Journal**: None
- **Summary**: Accurate cardiac computing, analysis and modeling from multi-modality images are important for the diagnosis and treatment of cardiac disease. Late gadolinium enhancement magnetic resonance imaging (LGE MRI) is a promising technique to visualize and quantify myocardial infarction (MI) and atrial scars. Automating quantification of MI and atrial scars can be challenging due to the low image quality and complex enhancement patterns of LGE MRI. Moreover, compared with the other sequences LGE MRIs with gold standard labels are particularly limited, which represents another obstacle for developing novel algorithms for automatic segmentation and quantification of LGE MRIs. This chapter aims to summarize the state-of-the-art and our recent advanced contributions on deep learning based multi-modality cardiac image analysis. Firstly, we introduce two benchmark works for multi-sequence cardiac MRI based myocardial and pathology segmentation. Secondly, two novel frameworks for left atrial scar segmentation and quantification from LGE MRI were presented. Thirdly, we present three unsupervised domain adaptation techniques for cross-modality cardiac image segmentation.



### Triple-level Model Inferred Collaborative Network Architecture for Video Deraining
- **Arxiv ID**: http://arxiv.org/abs/2111.04459v1
- **DOI**: 10.1109/TIP.2021.3128327
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04459v1)
- **Published**: 2021-11-08 13:09:00+00:00
- **Updated**: 2021-11-08 13:09:00+00:00
- **Authors**: Pan Mu, Zhu Liu, Yaohua Liu, Risheng Liu, Xin Fan
- **Comment**: Accepted at IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Video deraining is an important issue for outdoor vision systems and has been investigated extensively. However, designing optimal architectures by the aggregating model formation and data distribution is a challenging task for video deraining. In this paper, we develop a model-guided triple-level optimization framework to deduce network architecture with cooperating optimization and auto-searching mechanism, named Triple-level Model Inferred Cooperating Searching (TMICS), for dealing with various video rain circumstances. In particular, to mitigate the problem that existing methods cannot cover various rain streaks distribution, we first design a hyper-parameter optimization model about task variable and hyper-parameter. Based on the proposed optimization model, we design a collaborative structure for video deraining. This structure includes Dominant Network Architecture (DNA) and Companionate Network Architecture (CNA) that is cooperated by introducing an Attention-based Averaging Scheme (AAS). To better explore inter-frame information from videos, we introduce a macroscopic structure searching scheme that searches from Optical Flow Module (OFM) and Temporal Grouping Module (TGM) to help restore latent frame. In addition, we apply the differentiable neural architecture searching from a compact candidate set of task-specific operations to discover desirable rain streaks removal architectures automatically. Extensive experiments on various datasets demonstrate that our model shows significant improvements in fidelity and temporal consistency over the state-of-the-art works. Source code is available at https://github.com/vis-opt-group/TMICS.



### Synthetic magnetic resonance images for domain adaptation: Application to fetal brain tissue segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.04737v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04737v1)
- **Published**: 2021-11-08 13:22:14+00:00
- **Updated**: 2021-11-08 13:22:14+00:00
- **Authors**: Priscille de Dumast, Hamza Kebiri, Kelly Payette, Andras Jakab, Hélène Lajous, Meritxell Bach Cuadra
- **Comment**: 4 pages, 4 figures. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
- **Journal**: None
- **Summary**: The quantitative assessment of the developing human brain in utero is crucial to fully understand neurodevelopment. Thus, automated multi-tissue fetal brain segmentation algorithms are being developed, which in turn require annotated data to be trained. However, the available annotated fetal brain datasets are limited in number and heterogeneity, hampering domain adaptation strategies for robust segmentation. In this context, we use FaBiAN, a Fetal Brain magnetic resonance Acquisition Numerical phantom, to simulate various realistic magnetic resonance images of the fetal brain along with its class labels. We demonstrate that these multiple synthetic annotated data, generated at no cost and further reconstructed using the target super-resolution technique, can be successfully used for domain adaptation of a deep learning method that segments seven brain tissues. Overall, the accuracy of the segmentation is significantly enhanced, especially in the cortical gray matter, the white matter, the cerebellum, the deep gray matter and the brain stem.



### HEROHE Challenge: assessing HER2 status in breast cancer without immunohistochemistry or in situ hybridization
- **Arxiv ID**: http://arxiv.org/abs/2111.04738v1
- **DOI**: 10.3390/jimaging8080213
- **Categories**: **q-bio.QM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04738v1)
- **Published**: 2021-11-08 13:39:41+00:00
- **Updated**: 2021-11-08 13:39:41+00:00
- **Authors**: Eduardo Conde-Sousa, João Vale, Ming Feng, Kele Xu, Yin Wang, Vincenzo Della Mea, David La Barbera, Ehsan Montahaei, Mahdieh Soleymani Baghshah, Andreas Turzynski, Jacob Gildenblat, Eldad Klaiman, Yiyu Hong, Guilherme Aresta, Teresa Araújo, Paulo Aguiar, Catarina Eloy, António Polónia
- **Comment**: None
- **Journal**: None
- **Summary**: Breast cancer is the most common malignancy in women, being responsible for more than half a million deaths every year. As such, early and accurate diagnosis is of paramount importance. Human expertise is required to diagnose and correctly classify breast cancer and define appropriate therapy, which depends on the evaluation of the expression of different biomarkers such as the transmembrane protein receptor HER2. This evaluation requires several steps, including special techniques such as immunohistochemistry or in situ hybridization to assess HER2 status. With the goal of reducing the number of steps and human bias in diagnosis, the HEROHE Challenge was organized, as a parallel event of the 16th European Congress on Digital Pathology, aiming to automate the assessment of the HER2 status based only on hematoxylin and eosin stained tissue sample of invasive breast cancer. Methods to assess HER2 status were presented by 21 teams worldwide and the results achieved by some of the proposed methods open potential perspectives to advance the state-of-the-art.



### DR-VNet: Retinal Vessel Segmentation via Dense Residual UNet
- **Arxiv ID**: http://arxiv.org/abs/2111.04739v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04739v2)
- **Published**: 2021-11-08 14:52:13+00:00
- **Updated**: 2022-03-22 08:42:40+00:00
- **Authors**: Ali Karaali, Rozenn Dahyot, Donal J. Sexton
- **Comment**: Accepted to ICPRAI 2022 - 3rd International Conference on Pattern
  Recognition and Artificial Intelligence
- **Journal**: None
- **Summary**: Accurate retinal vessel segmentation is an important task for many computer-aided diagnosis systems. Yet, it is still a challenging problem due to the complex vessel structures of an eye. Numerous vessel segmentation methods have been proposed recently, however more research is needed to deal with poor segmentation of thin and tiny vessels. To address this, we propose a new deep learning pipeline combining the efficiency of residual dense net blocks and, residual squeeze and excitation blocks. We validate experimentally our approach on three datasets and show that our pipeline outperforms current state of the art techniques on the sensitivity metric relevant to assess capture of small vessels.



### BRACS: A Dataset for BReAst Carcinoma Subtyping in H&E Histology Images
- **Arxiv ID**: http://arxiv.org/abs/2111.04740v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.AI, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04740v1)
- **Published**: 2021-11-08 15:04:16+00:00
- **Updated**: 2021-11-08 15:04:16+00:00
- **Authors**: Nadia Brancati, Anna Maria Anniciello, Pushpak Pati, Daniel Riccio, Giosuè Scognamiglio, Guillaume Jaume, Giuseppe De Pietro, Maurizio Di Bonito, Antonio Foncubierta, Gerardo Botti, Maria Gabrani, Florinda Feroce, Maria Frucci
- **Comment**: 10 pages, 3 figures, 8 tables, 30 references
- **Journal**: None
- **Summary**: Breast cancer is the most commonly diagnosed cancer and registers the highest number of deaths for women with cancer. Recent advancements in diagnostic activities combined with large-scale screening policies have significantly lowered the mortality rates for breast cancer patients. However, the manual inspection of tissue slides by the pathologists is cumbersome, time-consuming, and is subject to significant inter- and intra-observer variability. Recently, the advent of whole-slide scanning systems have empowered the rapid digitization of pathology slides, and enabled to develop digital workflows. These advances further enable to leverage Artificial Intelligence (AI) to assist, automate, and augment pathological diagnosis. But the AI techniques, especially Deep Learning (DL), require a large amount of high-quality annotated data to learn from. Constructing such task-specific datasets poses several challenges, such as, data-acquisition level constrains, time-consuming and expensive annotations, and anonymization of private information. In this paper, we introduce the BReAst Carcinoma Subtyping (BRACS) dataset, a large cohort of annotated Hematoxylin & Eosin (H&E)-stained images to facilitate the characterization of breast lesions. BRACS contains 547 Whole-Slide Images (WSIs), and 4539 Regions of Interest (ROIs) extracted from the WSIs. Each WSI, and respective ROIs, are annotated by the consensus of three board-certified pathologists into different lesion categories. Specifically, BRACS includes three lesion types, i.e., benign, malignant and atypical, which are further subtyped into seven categories. It is, to the best of our knowledge, the largest annotated dataset for breast cancer subtyping both at WSI- and ROI-level. Further, by including the understudied atypical lesions, BRACS offers an unique opportunity for leveraging AI to better understand their characteristics.



### Tensor-based Subspace Factorization for StyleGAN
- **Arxiv ID**: http://arxiv.org/abs/2111.04554v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04554v1)
- **Published**: 2021-11-08 15:11:39+00:00
- **Updated**: 2021-11-08 15:11:39+00:00
- **Authors**: René Haas, Stella Graßhof, Sami Sebastian Brandt
- **Comment**: Accepted for FG2021
- **Journal**: None
- **Summary**: In this paper, we propose $\tau$GAN a tensor-based method for modeling the latent space of generative models. The objective is to identify semantic directions in latent space. To this end, we propose to fit a multilinear tensor model on a structured facial expression database, which is initially embedded into latent space. We validate our approach on StyleGAN trained on FFHQ using BU-3DFE as a structured facial expression database. We show how the parameters of the multilinear tensor model can be approximated by Alternating Least Squares. Further, we introduce a tacked style-separated tensor model, defined as an ensemble of style-specific models to integrate our approach with the extended latent space of StyleGAN. We show that taking the individual styles of the extended latent space into account leads to higher model flexibility and lower reconstruction error. Finally, we do several experiments comparing our approach to former work on both GANs and multilinear models. Concretely, we analyze the expression subspace and find that the expression trajectories meet at an apathetic face that is consistent with earlier work. We also show that by changing the pose of a person, the generated image from our approach is closer to the ground truth than results from two competing approaches.



### Improved Regularization and Robustness for Fine-tuning in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2111.04578v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2111.04578v1)
- **Published**: 2021-11-08 15:39:44+00:00
- **Updated**: 2021-11-08 15:39:44+00:00
- **Authors**: Dongyue Li, Hongyang R. Zhang
- **Comment**: 22 pages, 6 figures, 11 tables
- **Journal**: None
- **Summary**: A widely used algorithm for transfer learning is fine-tuning, where a pre-trained model is fine-tuned on a target task with a small amount of labeled data. When the capacity of the pre-trained model is much larger than the size of the target data set, fine-tuning is prone to overfitting and "memorizing" the training labels. Hence, an important question is to regularize fine-tuning and ensure its robustness to noise. To address this question, we begin by analyzing the generalization properties of fine-tuning. We present a PAC-Bayes generalization bound that depends on the distance traveled in each layer during fine-tuning and the noise stability of the fine-tuned model. We empirically measure these quantities. Based on the analysis, we propose regularized self-labeling -- the interpolation between regularization and self-labeling methods, including (i) layer-wise regularization to constrain the distance traveled in each layer; (ii) self label-correction and label-reweighting to correct mislabeled data points (that the model is confident) and reweight less confident data points. We validate our approach on an extensive collection of image and text data sets using multiple pre-trained model architectures. Our approach improves baseline methods by 1.76% (on average) for seven image classification tasks and 0.75% for a few-shot classification task. When the target data set includes noisy labels, our approach outperforms baseline methods by 3.56% on average in two noisy settings.



### E(2) Equivariant Self-Attention for Radio Astronomy
- **Arxiv ID**: http://arxiv.org/abs/2111.04742v2
- **DOI**: None
- **Categories**: **astro-ph.IM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04742v2)
- **Published**: 2021-11-08 15:58:30+00:00
- **Updated**: 2021-11-30 16:50:04+00:00
- **Authors**: Micah Bowles, Matthew Bromley, Max Allen, Anna Scaife
- **Comment**: Accepted in: Fourth Workshop on Machine Learning and the Physical
  Sciences (35th Conference on Neural Information Processing Systems;
  NeurIPS2021); final version; 7 pages, 3 figures
- **Journal**: None
- **Summary**: In this work we introduce group-equivariant self-attention models to address the problem of explainable radio galaxy classification in astronomy. We evaluate various orders of both cyclic and dihedral equivariance, and show that including equivariance as a prior both reduces the number of epochs required to fit the data and results in improved performance. We highlight the benefits of equivariance when using self-attention as an explainable model and illustrate how equivariant models statistically attend the same features in their classifications as human astronomers.



### Machine Learning Guided 3D Image Recognition for Carbonate Pore and Mineral Volumes Determination
- **Arxiv ID**: http://arxiv.org/abs/2111.04612v2
- **DOI**: None
- **Categories**: **physics.geo-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04612v2)
- **Published**: 2021-11-08 16:34:08+00:00
- **Updated**: 2022-01-05 12:19:36+00:00
- **Authors**: Omar Alfarisi, Aikifa Raza, Hongtao Zhang, Djamel Ozzane, Mohamed Sassi, Tiejun Zhang
- **Comment**: 1- Added Affiliation section. 2- Updated the Acknowledgement section
- **Journal**: None
- **Summary**: Automated image processing algorithms can improve the quality, efficiency, and consistency of classifying the morphology of heterogeneous carbonate rock and can deal with a massive amount of data and images seamlessly. Geoscientists face difficulties in setting the direction of the optimum method for determining petrophysical properties from rock images, Micro-Computed Tomography (uCT), or Magnetic Resonance Imaging (MRI). Most of the successful work is from the homogeneous rocks focusing on 2D images with less focus on 3D and requiring numerical simulation. Currently, image analysis methods converge to three approaches: image processing, artificial intelligence, and combined image processing with artificial intelligence. In this work, we propose two methods to determine the porosity from 3D uCT and MRI images: an image processing method with Image Resolution Optimized Gaussian Algorithm (IROGA); advanced image recognition method enabled by Machine Learning Difference of Gaussian Random Forest (MLDGRF). We have built reference 3D micro models and collected images for calibration of IROGA and MLDGRF methods. To evaluate the predictive capability of these calibrated approaches, we ran them on 3D uCT and MRI images of natural heterogeneous carbonate rock. We measured the porosity and lithology of the carbonate rock using three and two industry-standard ways, respectively, as reference values. Notably, IROGA and MLDGRF have produced porosity results with an accuracy of 96.2% and 97.1% on the training set and 91.7% and 94.4% on blind test validation, respectively, in comparison with the three experimental measurements. We measured limestone and pyrite reference values using two methods, X-ray powder diffraction, and grain density measurements. MLDGRF has produced lithology (limestone and Pyrite) volumes with 97.7% accuracy.



### DeepSteal: Advanced Model Extractions Leveraging Efficient Weight Stealing in Memories
- **Arxiv ID**: http://arxiv.org/abs/2111.04625v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.04625v1)
- **Published**: 2021-11-08 16:55:45+00:00
- **Updated**: 2021-11-08 16:55:45+00:00
- **Authors**: Adnan Siraj Rakin, Md Hafizul Islam Chowdhuryy, Fan Yao, Deliang Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advancements of Deep Neural Networks (DNNs) have seen widespread deployment in multiple security-sensitive domains. The need of resource-intensive training and use of valuable domain-specific training data have made these models a top intellectual property (IP) for model owners. One of the major threats to the DNN privacy is model extraction attacks where adversaries attempt to steal sensitive information in DNN models. Recent studies show hardware-based side channel attacks can reveal internal knowledge about DNN models (e.g., model architectures) However, to date, existing attacks cannot extract detailed model parameters (e.g., weights/biases). In this work, for the first time, we propose an advanced model extraction attack framework DeepSteal that effectively steals DNN weights with the aid of memory side-channel attack. Our proposed DeepSteal comprises two key stages. Firstly, we develop a new weight bit information extraction method, called HammerLeak, through adopting the rowhammer based hardware fault technique as the information leakage vector. HammerLeak leverages several novel system-level techniques tailed for DNN applications to enable fast and efficient weight stealing. Secondly, we propose a novel substitute model training algorithm with Mean Clustering weight penalty, which leverages the partial leaked bit information effectively and generates a substitute prototype of the target victim model. We evaluate this substitute model extraction method on three popular image datasets (e.g., CIFAR-10/100/GTSRB) and four DNN architectures (e.g., ResNet-18/34/Wide-ResNet/VGG-11). The extracted substitute model has successfully achieved more than 90 % test accuracy on deep residual networks for the CIFAR-10 dataset. Moreover, our extracted substitute model could also generate effective adversarial input samples to fool the victim model.



### S3RP: Self-Supervised Super-Resolution and Prediction for Advection-Diffusion Process
- **Arxiv ID**: http://arxiv.org/abs/2111.04639v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, physics.comp-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.04639v1)
- **Published**: 2021-11-08 17:07:44+00:00
- **Updated**: 2021-11-08 17:07:44+00:00
- **Authors**: Chulin Wang, Kyongmin Yeo, Xiao Jin, Andres Codas, Levente J. Klein, Bruce Elmegreen
- **Comment**: 9 pages, 8 figures
- **Journal**: Neural Information Processing Systems (NeurIPS 2021) Workshop
- **Summary**: We present a super-resolution model for an advection-diffusion process with limited information. While most of the super-resolution models assume high-resolution (HR) ground-truth data in the training, in many cases such HR dataset is not readily accessible. Here, we show that a Recurrent Convolutional Network trained with physics-based regularizations is able to reconstruct the HR information without having the HR ground-truth data. Moreover, considering the ill-posed nature of a super-resolution problem, we employ the Recurrent Wasserstein Autoencoder to model the uncertainty.



### Composition and Style Attributes Guided Image Aesthetic Assessment
- **Arxiv ID**: http://arxiv.org/abs/2111.04647v3
- **DOI**: 10.1109/TIP.2022.3191853
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04647v3)
- **Published**: 2021-11-08 17:16:38+00:00
- **Updated**: 2022-07-07 10:14:03+00:00
- **Authors**: Luigi Celona, Marco Leonardi, Paolo Napoletano, Alessandro Rozza
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, 31 (2022) 5009-5024
- **Summary**: The aesthetic quality of an image is defined as the measure or appreciation of the beauty of an image. Aesthetics is inherently a subjective property but there are certain factors that influence it such as, the semantic content of the image, the attributes describing the artistic aspect, the photographic setup used for the shot, etc. In this paper we propose a method for the automatic prediction of the aesthetics of an image that is based on the analysis of the semantic content, the artistic style and the composition of the image. The proposed network includes: a pre-trained network for semantic features extraction (the Backbone); a Multi Layer Perceptron (MLP) network that relies on the Backbone features for the prediction of image attributes (the AttributeNet); a self-adaptive Hypernetwork that exploits the attributes prior encoded into the embedding generated by the AttributeNet to predict the parameters of the target network dedicated to aesthetic estimation (the AestheticNet). Given an image, the proposed multi-network is able to predict: style and composition attributes, and aesthetic score distribution. Results on three benchmark datasets demonstrate the effectiveness of the proposed method, while the ablation study gives a better understanding of the proposed network.



### Approximate Neural Architecture Search via Operation Distribution Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.04670v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2111.04670v1)
- **Published**: 2021-11-08 17:38:29+00:00
- **Updated**: 2021-11-08 17:38:29+00:00
- **Authors**: Xingchen Wan, Binxin Ru, Pedro M. Esperança, Fabio M. Carlucci
- **Comment**: WACV 2022. 10 pages, 3 figures and 5 tables (15 pages, 7 figures and
  6 tables including appendices)
- **Journal**: None
- **Summary**: The standard paradigm in Neural Architecture Search (NAS) is to search for a fully deterministic architecture with specific operations and connections. In this work, we instead propose to search for the optimal operation distribution, thus providing a stochastic and approximate solution, which can be used to sample architectures of arbitrary length. We propose and show, that given an architectural cell, its performance largely depends on the ratio of used operations, rather than any specific connection pattern in typical search spaces; that is, small changes in the ordering of the operations are often irrelevant. This intuition is orthogonal to any specific search strategy and can be applied to a diverse set of NAS algorithms. Through extensive validation on 4 data-sets and 4 NAS techniques (Bayesian optimisation, differentiable search, local search and random search), we show that the operation distribution (1) holds enough discriminating power to reliably identify a solution and (2) is significantly easier to optimise than traditional encodings, leading to large speed-ups at little to no cost in performance. Indeed, this simple intuition significantly reduces the cost of current approaches and potentially enable NAS to be used in a broader range of applications.



### Information-Theoretic Bias Assessment Of Learned Representations Of Pretrained Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.04673v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04673v2)
- **Published**: 2021-11-08 17:41:17+00:00
- **Updated**: 2021-11-09 03:00:08+00:00
- **Authors**: Jiazhi Li, Wael Abd-Almageed
- **Comment**: IEEE International Conference on Automatic Face and Gesture
  Recognition 2021
- **Journal**: None
- **Summary**: As equality issues in the use of face recognition have garnered a lot of attention lately, greater efforts have been made to debiased deep learning models to improve fairness to minorities. However, there is still no clear definition nor sufficient analysis for bias assessment metrics. We propose an information-theoretic, independent bias assessment metric to identify degree of bias against protected demographic attributes from learned representations of pretrained facial recognition systems. Our metric differs from other methods that rely on classification accuracy or examine the differences between ground truth and predicted labels of protected attributes predicted using a shallow network. Also, we argue, theoretically and experimentally, that logits-level loss is not adequate to explain bias since predictors based on neural networks will always find correlations. Further, we present a synthetic dataset that mitigates the issue of insufficient samples in certain cohorts. Lastly, we establish a benchmark metric by presenting advantages in clear discrimination and small variation comparing with other metrics, and evaluate the performance of different debiased models with the proposed metric.



### SMU: smooth activation function for deep networks using smoothing maximum technique
- **Arxiv ID**: http://arxiv.org/abs/2111.04682v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2111.04682v2)
- **Published**: 2021-11-08 17:54:08+00:00
- **Updated**: 2022-04-11 14:28:28+00:00
- **Authors**: Koushik Biswas, Sandeep Kumar, Shilpak Banerjee, Ashish Kumar Pandey
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Deep learning researchers have a keen interest in proposing two new novel activation functions which can boost network performance. A good choice of activation function can have significant consequences in improving network performance. A handcrafted activation is the most common choice in neural network models. ReLU is the most common choice in the deep learning community due to its simplicity though ReLU has some serious drawbacks. In this paper, we have proposed a new novel activation function based on approximation of known activation functions like Leaky ReLU, and we call this function Smooth Maximum Unit (SMU). Replacing ReLU by SMU, we have got 6.22% improvement in the CIFAR100 dataset with the ShuffleNet V2 model.



### Stain-free Detection of Embryo Polarization using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.05315v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, eess.IV, physics.bio-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.05315v1)
- **Published**: 2021-11-08 17:54:25+00:00
- **Updated**: 2021-11-08 17:54:25+00:00
- **Authors**: Cheng Shen, Adiyant Lamba, Meng Zhu, Ray Zhang, Changhuei Yang, Magdalena Zernicka Goetz
- **Comment**: None
- **Journal**: None
- **Summary**: Polarization of the mammalian embryo at the right developmental time is critical for its development to term and would be valuable in assessing the potential of human embryos. However, tracking polarization requires invasive fluorescence staining, impermissible in the in vitro fertilization clinic. Here, we report the use of artificial intelligence to detect polarization from unstained time-lapse movies of mouse embryos. We assembled a dataset of bright-field movie frames from 8-cell-stage embryos, side-by-side with corresponding images of fluorescent markers of cell polarization. We then used an ensemble learning model to detect whether any bright-field frame showed an embryo before or after onset of polarization. Our resulting model has an accuracy of 85% for detecting polarization, significantly outperforming human volunteers trained on the same data (61% accuracy). We discovered that our self-learning model focuses upon the angle between cells as one known cue for compaction, which precedes polarization, but it outperforms the use of this cue alone. By compressing three-dimensional time-lapsed image data into two-dimensions, we are able to reduce data to an easily manageable size for deep learning processing. In conclusion, we describe a method for detecting a key developmental feature of embryo development that avoids clinically impermissible fluorescence staining.



### Automated pharyngeal phase detection and bolus localization in videofluoroscopic swallowing study: Killing two birds with one stone?
- **Arxiv ID**: http://arxiv.org/abs/2111.04699v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04699v2)
- **Published**: 2021-11-08 18:25:01+00:00
- **Updated**: 2022-05-26 09:01:53+00:00
- **Authors**: Andrea Bandini, Sana Smaoui, Catriona M. Steele
- **Comment**: None
- **Journal**: None
- **Summary**: The videofluoroscopic swallowing study (VFSS) is a gold-standard imaging technique for assessing swallowing, but analysis and rating of VFSS recordings is time consuming and requires specialized training and expertise. Researchers have recently demonstrated that it is possible to automatically detect the pharyngeal phase of swallowing and to localize the bolus in VFSS recordings via computer vision, fostering the development of novel techniques for automatic VFSS analysis. However, training of algorithms to perform these tasks requires large amounts of annotated data that are seldom available. We demonstrate that the challenges of pharyngeal phase detection and bolus localization can be solved together using a single approach. We propose a deep-learning framework that jointly tackles pharyngeal phase detection and bolus localization in a weakly-supervised manner, requiring only the initial and final frames of the pharyngeal phase as ground truth annotations for the training. Our approach stems from the observation that bolus presence in the pharynx is the most prominent visual feature upon which to infer whether individual VFSS frames belong to the pharyngeal phase. We conducted extensive experiments with multiple convolutional neural networks (CNNs) on a dataset of 1245 bolus-level clips from 59 healthy subjects. We demonstrated that the pharyngeal phase can be detected with an F1-score higher than 0.9. Moreover, by processing the class activation maps of the CNNs, we were able to localize the bolus with promising results, obtaining correlations with ground truth trajectories higher than 0.9, without any manual annotations of bolus location used for training purposes. Once validated on a larger sample of participants with swallowing disorders, our framework will pave the way for the development of intelligent tools for VFSS analysis to support clinicians in swallowing assessment.



### OMD: Orthogonal Malware Detection Using Audio, Image, and Static Features
- **Arxiv ID**: http://arxiv.org/abs/2111.04710v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2111.04710v1)
- **Published**: 2021-11-08 18:42:30+00:00
- **Updated**: 2021-11-08 18:42:30+00:00
- **Authors**: Lakshmanan Nataraj, Tajuddin Manhar Mohammed, Tejaswi Nanjundaswamy, Satish Chikkagoudar, Shivkumar Chandrasekaran, B. S. Manjunath
- **Comment**: Submitted version - MILCOM 2021 IEEE Military Communications
  Conference
- **Journal**: None
- **Summary**: With the growing number of malware and cyber attacks, there is a need for "orthogonal" cyber defense approaches, which are complementary to existing methods by detecting unique malware samples that are not predicted by other methods. In this paper, we propose a novel and orthogonal malware detection (OMD) approach to identify malware using a combination of audio descriptors, image similarity descriptors and other static/statistical features. First, we show how audio descriptors are effective in classifying malware families when the malware binaries are represented as audio signals. Then, we show that the predictions made on the audio descriptors are orthogonal to the predictions made on image similarity descriptors and other static features. Further, we develop a framework for error analysis and a metric to quantify how orthogonal a new feature set (or type) is with respect to other feature sets. This allows us to add new features and detection methods to our overall framework. Experimental results on malware datasets show that our approach provides a robust framework for orthogonal malware detection.



### SustainBench: Benchmarks for Monitoring the Sustainable Development Goals with Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.04724v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04724v1)
- **Published**: 2021-11-08 18:59:04+00:00
- **Updated**: 2021-11-08 18:59:04+00:00
- **Authors**: Christopher Yeh, Chenlin Meng, Sherrie Wang, Anne Driscoll, Erik Rozi, Patrick Liu, Jihyeon Lee, Marshall Burke, David B. Lobell, Stefano Ermon
- **Comment**: NeurIPS 2021 (Track on Datasets and Benchmarks)
- **Journal**: None
- **Summary**: Progress toward the United Nations Sustainable Development Goals (SDGs) has been hindered by a lack of data on key environmental and socioeconomic indicators, which historically have come from ground surveys with sparse temporal and spatial coverage. Recent advances in machine learning have made it possible to utilize abundant, frequently-updated, and globally available data, such as from satellites or social media, to provide insights into progress toward SDGs. Despite promising early results, approaches to using such data for SDG measurement thus far have largely evaluated on different datasets or used inconsistent evaluation metrics, making it hard to understand whether performance is improving and where additional research would be most fruitful. Furthermore, processing satellite and ground survey data requires domain knowledge that many in the machine learning community lack. In this paper, we introduce SustainBench, a collection of 15 benchmark tasks across 7 SDGs, including tasks related to economic development, agriculture, health, education, water and sanitation, climate action, and life on land. Datasets for 11 of the 15 tasks are released publicly for the first time. Our goals for SustainBench are to (1) lower the barriers to entry for the machine learning community to contribute to measuring and achieving the SDGs; (2) provide standard benchmarks for evaluating machine learning models on tasks across a variety of SDGs; and (3) encourage the development of novel machine learning methods where improved model performance facilitates progress towards the SDGs.



### Frustum Fusion: Pseudo-LiDAR and LiDAR Fusion for 3D Detection
- **Arxiv ID**: http://arxiv.org/abs/2111.04780v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2111.04780v1)
- **Published**: 2021-11-08 19:29:59+00:00
- **Updated**: 2021-11-08 19:29:59+00:00
- **Authors**: Farzin Negahbani, Onur Berk Töre, Fatma Güney, Baris Akgun
- **Comment**: None
- **Journal**: None
- **Summary**: Most autonomous vehicles are equipped with LiDAR sensors and stereo cameras. The former is very accurate but generates sparse data, whereas the latter is dense, has rich texture and color information but difficult to extract robust 3D representations from. In this paper, we propose a novel data fusion algorithm to combine accurate point clouds with dense but less accurate point clouds obtained from stereo pairs. We develop a framework to integrate this algorithm into various 3D object detection methods. Our framework starts with 2D detections from both of the RGB images, calculates frustums and their intersection, creates Pseudo-LiDAR data from the stereo images, and fills in the parts of the intersection region where the LiDAR data is lacking with the dense Pseudo-LiDAR points. We train multiple 3D object detection methods and show that our fusion strategy consistently improves the performance of detectors.



### Visual Question Answering based on Formal Logic
- **Arxiv ID**: http://arxiv.org/abs/2111.04785v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2111.04785v1)
- **Published**: 2021-11-08 19:43:53+00:00
- **Updated**: 2021-11-08 19:43:53+00:00
- **Authors**: Muralikrishnna G. Sethuraman, Ali Payani, Faramarz Fekri, J. Clayton Kerce
- **Comment**: None
- **Journal**: None
- **Summary**: Visual question answering (VQA) has been gaining a lot of traction in the machine learning community in the recent years due to the challenges posed in understanding information coming from multiple modalities (i.e., images, language). In VQA, a series of questions are posed based on a set of images and the task at hand is to arrive at the answer. To achieve this, we take a symbolic reasoning based approach using the framework of formal logic. The image and the questions are converted into symbolic representations on which explicit reasoning is performed. We propose a formal logic framework where (i) images are converted to logical background facts with the help of scene graphs, (ii) the questions are translated to first-order predicate logic clauses using a transformer based deep learning model, and (iii) perform satisfiability checks, by using the background knowledge and the grounding of predicate clauses, to obtain the answer. Our proposed method is highly interpretable and each step in the pipeline can be easily analyzed by a human. We validate our approach on the CLEVR and the GQA dataset. We achieve near perfect accuracy of 99.6% on the CLEVR dataset comparable to the state of art models, showcasing that formal logic is a viable tool to tackle visual question answering. Our model is also data efficient, achieving 99.1% accuracy on CLEVR dataset when trained on just 10% of the training data.



### TAGLETS: A System for Automatic Semi-Supervised Learning with Auxiliary Data
- **Arxiv ID**: http://arxiv.org/abs/2111.04798v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04798v3)
- **Published**: 2021-11-08 20:08:45+00:00
- **Updated**: 2022-05-05 23:49:23+00:00
- **Authors**: Wasu Piriyakulkij, Cristina Menghini, Ross Briden, Nihal V. Nayak, Jeffrey Zhu, Elaheh Raisi, Stephen H. Bach
- **Comment**: Paper published at MLSys 2022. It passed the artifact evaluation
  earning two ACM badges: (1) Artifacts Evaluated Functional v1.1 and (2)
  Artifacts Available v1.1
- **Journal**: None
- **Summary**: Machine learning practitioners often have access to a spectrum of data: labeled data for the target task (which is often limited), unlabeled data, and auxiliary data, the many available labeled datasets for other tasks. We describe TAGLETS, a system built to study techniques for automatically exploiting all three types of data and creating high-quality, servable classifiers. The key components of TAGLETS are: (1) auxiliary data organized according to a knowledge graph, (2) modules encapsulating different methods for exploiting auxiliary and unlabeled data, and (3) a distillation stage in which the ensembled modules are combined into a servable model. We compare TAGLETS with state-of-the-art transfer learning and semi-supervised learning methods on four image classification tasks. Our study covers a range of settings, varying the amount of labeled data and the semantic relatedness of the auxiliary data to the target task. We find that the intelligent incorporation of auxiliary and unlabeled data into multiple learning techniques enables TAGLETS to match-and most often significantly surpass-these alternatives. TAGLETS is available as an open-source system at github.com/BatsResearch/taglets.



### Unsupervised Approaches for Out-Of-Distribution Dermoscopic Lesion Detection
- **Arxiv ID**: http://arxiv.org/abs/2111.04807v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.04807v1)
- **Published**: 2021-11-08 20:22:34+00:00
- **Updated**: 2021-11-08 20:22:34+00:00
- **Authors**: Max Torop, Sandesh Ghimire, Wenqian Liu, Dana H. Brooks, Octavia Camps, Milind Rajadhyaksha, Jennifer Dy, Kivanc Kose
- **Comment**: NeurIPS: Medical Imaging Meets NeurIPS Workshop
- **Journal**: None
- **Summary**: There are limited works showing the efficacy of unsupervised Out-of-Distribution (OOD) methods on complex medical data. Here, we present preliminary findings of our unsupervised OOD detection algorithm, SimCLR-LOF, as well as a recent state of the art approach (SSD), applied on medical images. SimCLR-LOF learns semantically meaningful features using SimCLR and uses LOF for scoring if a test sample is OOD. We evaluated on the multi-source International Skin Imaging Collaboration (ISIC) 2019 dataset, and show results that are competitive with SSD as well as with recent supervised approaches applied on the same data.



### Cascaded Multilingual Audio-Visual Learning from Videos
- **Arxiv ID**: http://arxiv.org/abs/2111.04823v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.MM, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.04823v1)
- **Published**: 2021-11-08 20:53:50+00:00
- **Updated**: 2021-11-08 20:53:50+00:00
- **Authors**: Andrew Rouditchenko, Angie Boggust, David Harwath, Samuel Thomas, Hilde Kuehne, Brian Chen, Rameswar Panda, Rogerio Feris, Brian Kingsbury, Michael Picheny, James Glass
- **Comment**: Presented at Interspeech 2021. This version contains updated results
  using the YouCook-Japanese dataset
- **Journal**: None
- **Summary**: In this paper, we explore self-supervised audio-visual models that learn from instructional videos. Prior work has shown that these models can relate spoken words and sounds to visual content after training on a large-scale dataset of videos, but they were only trained and evaluated on videos in English. To learn multilingual audio-visual representations, we propose a cascaded approach that leverages a model trained on English videos and applies it to audio-visual data in other languages, such as Japanese videos. With our cascaded approach, we show an improvement in retrieval performance of nearly 10x compared to training on the Japanese videos solely. We also apply the model trained on English videos to Japanese and Hindi spoken captions of images, achieving state-of-the-art performance.



### Evolving Evocative 2D Views of Generated 3D Objects
- **Arxiv ID**: http://arxiv.org/abs/2111.04839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.04839v1)
- **Published**: 2021-11-08 21:23:37+00:00
- **Updated**: 2021-11-08 21:23:37+00:00
- **Authors**: Eric Chu
- **Comment**: None
- **Journal**: NeurIPS 2021 Workshop on Machine Learning for Creativity and
  Design
- **Summary**: We present a method for jointly generating 3D models of objects and 2D renders at different viewing angles, with the process guided by ImageNet and CLIP -based models. Our results indicate that it can generate anamorphic objects, with renders that both evoke the target caption and look visually appealing.



### Hybrid BYOL-ViT: Efficient approach to deal with small datasets
- **Arxiv ID**: http://arxiv.org/abs/2111.04845v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.04845v2)
- **Published**: 2021-11-08 21:44:31+00:00
- **Updated**: 2021-11-15 16:54:48+00:00
- **Authors**: Safwen Naimi, Rien van Leeuwen, Wided Souidene, Slim Ben Saoud
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised learning can learn large representational spaces, which are crucial for handling difficult learning tasks. However, due to the design of the model, classical image classification approaches struggle to generalize to new problems and new situations when dealing with small datasets. In fact, supervised learning can lose the location of image features which leads to supervision collapse in very deep architectures. In this paper, we investigate how self-supervision with strong and sufficient augmentation of unlabeled data can train effectively the first layers of a neural network even better than supervised learning, with no need for millions of labeled data. The main goal is to disconnect pixel data from annotation by getting generic task-agnostic low-level features. Furthermore, we look into Vision Transformers (ViT) and show that the low-level features derived from a self-supervised architecture can improve the robustness and the overall performance of this emergent architecture. We evaluated our method on one of the smallest open-source datasets STL-10 and we obtained a significant boost of performance from 41.66% to 83.25% when inputting low-level features from a self-supervised learning architecture to the ViT instead of the raw images.



### Explaining Face Presentation Attack Detection Using Natural Language
- **Arxiv ID**: http://arxiv.org/abs/2111.04862v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2111.04862v1)
- **Published**: 2021-11-08 22:55:55+00:00
- **Updated**: 2021-11-08 22:55:55+00:00
- **Authors**: Hengameh Mirzaalian, Mohamed E. Hussein, Leonidas Spinoulas, Jonathan May, Wael Abd-Almageed
- **Comment**: To Appear in the Proceedings of the IEEE International Conference on
  Automatic Face and Gesture Recognition 2021
- **Journal**: None
- **Summary**: A large number of deep neural network based techniques have been developed to address the challenging problem of face presentation attack detection (PAD). Whereas such techniques' focus has been on improving PAD performance in terms of classification accuracy and robustness against unseen attacks and environmental conditions, there exists little attention on the explainability of PAD predictions. In this paper, we tackle the problem of explaining PAD predictions through natural language. Our approach passes feature representations of a deep layer of the PAD model to a language model to generate text describing the reasoning behind the PAD prediction. Due to the limited amount of annotated data in our study, we apply a light-weight LSTM network as our natural language generation model. We investigate how the quality of the generated explanations is affected by different loss functions, including the commonly used word-wise cross entropy loss, a sentence discriminative loss, and a sentence semantic loss. We perform our experiments using face images from a dataset consisting of 1,105 bona-fide and 924 presentation attack samples. Our quantitative and qualitative results show the effectiveness of our model for generating proper PAD explanations through text as well as the power of the sentence-wise losses. To the best of our knowledge, this is the first introduction of a joint biometrics-NLP task. Our dataset can be obtained through our GitHub page.



### LiMoSeg: Real-time Bird's Eye View based LiDAR Motion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.04875v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2111.04875v3)
- **Published**: 2021-11-08 23:40:55+00:00
- **Updated**: 2022-01-22 21:46:40+00:00
- **Authors**: Sambit Mohapatra, Mona Hodaei, Senthil Yogamani, Stefan Milz, Heinrich Gotzig, Martin Simon, Hazem Rashed, Patrick Maeder
- **Comment**: Accepted for Presentation at International Conference on Computer
  Vision Theory and Applications (VISAPP 2022)
- **Journal**: None
- **Summary**: Moving object detection and segmentation is an essential task in the Autonomous Driving pipeline. Detecting and isolating static and moving components of a vehicle's surroundings are particularly crucial in path planning and localization tasks. This paper proposes a novel real-time architecture for motion segmentation of Light Detection and Ranging (LiDAR) data. We use three successive scans of LiDAR data in 2D Bird's Eye View (BEV) representation to perform pixel-wise classification as static or moving. Furthermore, we propose a novel data augmentation technique to reduce the significant class imbalance between static and moving objects. We achieve this by artificially synthesizing moving objects by cutting and pasting static vehicles. We demonstrate a low latency of 8 ms on a commonly used automotive embedded platform, namely Nvidia Jetson Xavier. To the best of our knowledge, this is the first work directly performing motion segmentation in LiDAR BEV space. We provide quantitative results on the challenging SemanticKITTI dataset, and qualitative results are provided in https://youtu.be/2aJ-cL8b0LI.



### Combining machine learning with physics: A framework for tracking and sorting multiple dark solitons
- **Arxiv ID**: http://arxiv.org/abs/2111.04881v2
- **DOI**: 10.1103/PhysRevResearch.4.023163
- **Categories**: **cond-mat.quant-gas**, cs.CV, cs.LG, quant-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.04881v2)
- **Published**: 2021-11-08 23:49:04+00:00
- **Updated**: 2022-06-01 20:38:44+00:00
- **Authors**: Shangjie Guo, Sophia M. Koh, Amilson R. Fritsch, I. B. Spielman, Justyna P. Zwolak
- **Comment**: 13 pages, 9 figures
- **Journal**: Phys. Rev. Research 4, 023163 (2022)
- **Summary**: In ultracold-atom experiments, data often comes in the form of images which suffer information loss inherent in the techniques used to prepare and measure the system. This is particularly problematic when the processes of interest are complicated, such as interactions among excitations in Bose-Einstein condensates (BECs). In this paper, we describe a framework combining machine learning (ML) models with physics-based traditional analyses to identify and track multiple solitonic excitations in images of BECs. We use an ML-based object detector to locate the solitonic excitations and develop a physics-informed classifier to sort solitonic excitations into physically motivated subcategories. Lastly, we introduce a quality metric quantifying the likelihood that a specific feature is a longitudinal soliton. Our trained implementation of this framework, SolDet, is publicly available as an open-source python package. SolDet is broadly applicable to feature identification in cold-atom images when trained on a suitable user-provided dataset.



