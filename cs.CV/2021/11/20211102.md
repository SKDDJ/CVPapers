# Arxiv Papers in cs.CV on 2021-11-02
### Exploring the Semi-supervised Video Object Segmentation Problem from a Cyclic Perspective
- **Arxiv ID**: http://arxiv.org/abs/2111.01323v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.01323v2)
- **Published**: 2021-11-02 01:50:23+00:00
- **Updated**: 2022-07-25 12:04:37+00:00
- **Authors**: Yuxi Li, Ning Xu, Wenjie Yang, John See, Weiyao Lin
- **Comment**: modified version to appear in IJCV. arXiv admin note: substantial
  text overlap with arXiv:2010.12176
- **Journal**: None
- **Summary**: Modern video object segmentation (VOS) algorithms have achieved remarkably high performance in a sequential processing order, while most of currently prevailing pipelines still show some obvious inadequacy like accumulative error, unknown robustness or lack of proper interpretation tools. In this paper, we place the semi-supervised video object segmentation problem into a cyclic workflow and find the defects above can be collectively addressed via the inherent cyclic property of semi-supervised VOS systems. Firstly, a cyclic mechanism incorporated to the standard sequential flow can produce more consistent representations for pixel-wise correspondance. Relying on the accurate reference mask in the starting frame, we show that the error propagation problem can be mitigated. Next, a simple gradient correction module, which naturally extends the offline cyclic pipeline to an online manner, can highlight the high-frequent and detailed part of results to further improve the segmentation quality while keeping feasible computation cost. Meanwhile such correction can protect the network from severe performance degration resulted from interference signals. Finally we develop cycle effective receptive field (cycle-ERF) based on gradient correction process to provide a new perspective into analyzing object-specific regions of interests. We conduct comprehensive comparison and detailed analysis on challenging benchmarks of DAVIS16, DAVIS17 and Youtube-VOS, demonstrating that the cyclic mechanism is helpful to enhance segmentation quality, improve the robustness of VOS systems, and further provide qualitative comparison and interpretation on how different VOS algorithms work. The code of this project can be found at https://github.com/lyxok1/STM-Training



### Attribute-Based Deep Periocular Recognition: Leveraging Soft Biometrics to Improve Periocular Recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.01325v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.01325v1)
- **Published**: 2021-11-02 01:51:37+00:00
- **Updated**: 2021-11-02 01:51:37+00:00
- **Authors**: Veeru Talreja, Nasser M. Nasrabadi, Matthew C. Valenti
- **Comment**: Accepted to be published in WACV 2022
- **Journal**: None
- **Summary**: In recent years, periocular recognition has been developed as a valuable biometric identification approach, especially in wild environments (for example, masked faces due to COVID-19 pandemic) where facial recognition may not be applicable. This paper presents a new deep periocular recognition framework called attribute-based deep periocular recognition (ADPR), which predicts soft biometrics and incorporates the prediction into a periocular recognition algorithm to determine identity from periocular images with high accuracy. We propose an end-to-end framework, which uses several shared convolutional neural network (CNN)layers (a common network) whose output feeds two separate dedicated branches (modality dedicated layers); the first branch classifies periocular images while the second branch predicts softn biometrics. Next, the features from these two branches are fused together for a final periocular recognition. The proposed method is different from existing methods as it not only uses a shared CNN feature space to train these two tasks jointly, but it also fuses predicted soft biometric features with the periocular features in the training step to improve the overall periocular recognition performance. Our proposed model is extensively evaluated using four different publicly available datasets. Experimental results indicate that our soft biometric based periocular recognition approach outperforms other state-of-the-art methods for periocular recognition in wild environments.



### Federated Split Vision Transformer for COVID-19 CXR Diagnosis using Task-Agnostic Training
- **Arxiv ID**: http://arxiv.org/abs/2111.01338v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.01338v2)
- **Published**: 2021-11-02 02:54:30+00:00
- **Updated**: 2021-11-03 14:49:34+00:00
- **Authors**: Sangjoon Park, Gwanghyun Kim, Jeongsol Kim, Boah Kim, Jong Chul Ye
- **Comment**: Accepted for NeurIPS 2021
- **Journal**: None
- **Summary**: Federated learning, which shares the weights of the neural network across clients, is gaining attention in the healthcare sector as it enables training on a large corpus of decentralized data while maintaining data privacy. For example, this enables neural network training for COVID-19 diagnosis on chest X-ray (CXR) images without collecting patient CXR data across multiple hospitals. Unfortunately, the exchange of the weights quickly consumes the network bandwidth if highly expressive network architecture is employed. So-called split learning partially solves this problem by dividing a neural network into a client and a server part, so that the client part of the network takes up less extensive computation resources and bandwidth. However, it is not clear how to find the optimal split without sacrificing the overall network performance. To amalgamate these methods and thereby maximize their distinct strengths, here we show that the Vision Transformer, a recently developed deep learning architecture with straightforward decomposable configuration, is ideally suitable for split learning without sacrificing performance. Even under the non-independent and identically distributed data distribution which emulates a real collaboration between hospitals using CXR datasets from multiple sources, the proposed framework was able to attain performance comparable to data-centralized training. In addition, the proposed framework along with heterogeneous multi-task clients also improves individual task performances including the diagnosis of COVID-19, eliminating the need for sharing large weights with innumerable parameters. Our results affirm the suitability of Transformer for collaborative learning in medical imaging and pave the way forward for future real-world implementations.



### Constructing High-Order Signed Distance Maps from Computed Tomography Data with Application to Bone Morphometry
- **Arxiv ID**: http://arxiv.org/abs/2111.01350v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.01350v1)
- **Published**: 2021-11-02 03:18:47+00:00
- **Updated**: 2021-11-02 03:18:47+00:00
- **Authors**: Bryce A. Besler, Tannis D. Kemp, Nils D. Forkert, Steven K. Boyd
- **Comment**: 14 pages, 14 figures
- **Journal**: None
- **Summary**: An algorithm is presented for constructing high-order signed distance fields for two phase materials imaged with computed tomography. The signed distance field is high-order in that it is free of the quantization artifact associated with the distance transform of sampled signals. The narrowband is solved using a closest point algorithm extended for implicit embeddings that are not a signed distance field. The high-order fast sweeping algorithm is used to extend the narrowband to the remainder of the domain. The order of accuracy of the narrowband and extension methods are verified on ideal implicit surfaces. The method is applied to ten excised cubes of bovine trabecular bone. Localization of the surface, estimation of phase densities, and local morphometry is validated with these subjects. Since the embedding is high-order, gradients and thus curvatures can be accurately estimated locally in the image data.



### Can Vision Transformers Perform Convolution?
- **Arxiv ID**: http://arxiv.org/abs/2111.01353v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.01353v2)
- **Published**: 2021-11-02 03:30:17+00:00
- **Updated**: 2021-11-03 00:53:16+00:00
- **Authors**: Shanda Li, Xiangning Chen, Di He, Cho-Jui Hsieh
- **Comment**: None
- **Journal**: None
- **Summary**: Several recent studies have demonstrated that attention-based networks, such as Vision Transformer (ViT), can outperform Convolutional Neural Networks (CNNs) on several computer vision tasks without using convolutional layers. This naturally leads to the following questions: Can a self-attention layer of ViT express any convolution operation? In this work, we prove that a single ViT layer with image patches as the input can perform any convolution operation constructively, where the multi-head attention mechanism and the relative positional encoding play essential roles. We further provide a lower bound on the number of heads for Vision Transformers to express CNNs. Corresponding with our analysis, experimental results show that the construction in our proof can help inject convolutional bias into Transformers and significantly improve the performance of ViT in low data regimes.



### Boundary Distribution Estimation for Precise Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2111.01396v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.01396v2)
- **Published**: 2021-11-02 06:58:22+00:00
- **Updated**: 2023-07-19 08:55:05+00:00
- **Authors**: Peng Zhi, Haoran Zhou, Hang Huang, Rui Zhao, Rui Zhou, Qingguo Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: In the field of state-of-the-art object detection, the task of object localization is typically accomplished through a dedicated subnet that emphasizes bounding box regression. This subnet traditionally predicts the object's position by regressing the box's center position and scaling factors. Despite the widespread adoption of this approach, we have observed that the localization results often suffer from defects, leading to unsatisfactory detector performance. In this paper, we address the shortcomings of previous methods through theoretical analysis and experimental verification and present an innovative solution for precise object detection. Instead of solely focusing on the object's center and size, our approach enhances the accuracy of bounding box localization by refining the box edges based on the estimated distribution at the object's boundary. Experimental results demonstrate the potential and generalizability of our proposed method.



### A Pixel-Level Meta-Learner for Weakly Supervised Few-Shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.01418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.01418v1)
- **Published**: 2021-11-02 08:28:11+00:00
- **Updated**: 2021-11-02 08:28:11+00:00
- **Authors**: Yuan-Hao Lee, Fu-En Yang, Yu-Chiang Frank Wang
- **Comment**: Accepted to WACV 2022
- **Journal**: None
- **Summary**: Few-shot semantic segmentation addresses the learning task in which only few images with ground truth pixel-level labels are available for the novel classes of interest. One is typically required to collect a large mount of data (i.e., base classes) with such ground truth information, followed by meta-learning strategies to address the above learning task. When only image-level semantic labels can be observed during both training and testing, it is considered as an even more challenging task of weakly supervised few-shot semantic segmentation. To address this problem, we propose a novel meta-learning framework, which predicts pseudo pixel-level segmentation masks from a limited amount of data and their semantic labels. More importantly, our learning scheme further exploits the produced pixel-level information for query image inputs with segmentation guarantees. Thus, our proposed learning model can be viewed as a pixel-level meta-learner. Through extensive experiments on benchmark datasets, we show that our model achieves satisfactory performances under fully supervised settings, yet performs favorably against state-of-the-art methods under weakly supervised settings.



### HHP-Net: A light Heteroscedastic neural network for Head Pose estimation with uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2111.01440v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.01440v2)
- **Published**: 2021-11-02 08:55:45+00:00
- **Updated**: 2021-11-03 11:41:42+00:00
- **Authors**: Giorgio Cantarini, Federico Figari Tomenotti, Nicoletta Noceti, Francesca Odone
- **Comment**: Accepted at WACV 2022
- **Journal**: None
- **Summary**: In this paper we introduce a novel method to estimate the head pose of people in single images starting from a small set of head keypoints. To this purpose, we propose a regression model that exploits keypoints computed automatically by 2D pose estimation algorithms and outputs the head pose represented by yaw, pitch, and roll. Our model is simple to implement and more efficient with respect to the state of the art -- faster in inference and smaller in terms of memory occupancy -- with comparable accuracy. Our method also provides a measure of the heteroscedastic uncertainties associated with the three angles, through an appropriately designed loss function; we show there is a correlation between error and uncertainty values, thus this extra source of information may be used in subsequent computational steps. As an example application, we address social interaction analysis in images: we propose an algorithm for a quantitative estimation of the level of interaction between people, starting from their head poses and reasoning on their mutual positions. The code is available at https://github.com/cantarinigiorgio/HHP-Net.



### Out of distribution detection for skin and malaria images
- **Arxiv ID**: http://arxiv.org/abs/2111.01505v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.01505v1)
- **Published**: 2021-11-02 11:16:07+00:00
- **Updated**: 2021-11-02 11:16:07+00:00
- **Authors**: Muhammad Zaida, Shafaqat Ali, Mohsen Ali, Sarfaraz Hussein, Asma Saadia, Waqas Sultani
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have shown promising results in disease detection and classification using medical image data. However, they still suffer from the challenges of handling real-world scenarios especially reliably detecting out-of-distribution (OoD) samples. We propose an approach to robustly classify OoD samples in skin and malaria images without the need to access labeled OoD samples during training. Specifically, we use metric learning along with logistic regression to force the deep networks to learn much rich class representative features. To guide the learning process against the OoD examples, we generate ID similar-looking examples by either removing class-specific salient regions in the image or permuting image parts and distancing them away from in-distribution samples. During inference time, the K-reciprocal nearest neighbor is employed to detect out-of-distribution samples. For skin cancer OoD detection, we employ two standard benchmark skin cancer ISIC datasets as ID, and six different datasets with varying difficulty levels were taken as out of distribution. For malaria OoD detection, we use the BBBC041 malaria dataset as ID and five different challenging datasets as out of distribution. We achieved state-of-the-art results, improving 5% and 4% in TNR@TPR95% over the previous state-of-the-art for skin cancer and malaria OoD detection respectively.



### ISP-Agnostic Image Reconstruction for Under-Display Cameras
- **Arxiv ID**: http://arxiv.org/abs/2111.01511v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.01511v1)
- **Published**: 2021-11-02 11:30:13+00:00
- **Updated**: 2021-11-02 11:30:13+00:00
- **Authors**: Miao Qi, Yuqi Li, Wolfgang Heidrich
- **Comment**: None
- **Journal**: None
- **Summary**: Under-display cameras have been proposed in recent years as a way to reduce the form factor of mobile devices while maximizing the screen area. Unfortunately, placing the camera behind the screen results in significant image distortions, including loss of contrast, blur, noise, color shift, scattering artifacts, and reduced light sensitivity. In this paper, we propose an image-restoration pipeline that is ISP-agnostic, i.e. it can be combined with any legacy ISP to produce a final image that matches the appearance of regular cameras using the same ISP. This is achieved with a deep learning approach that performs a RAW-to-RAW image restoration. To obtain large quantities of real under-display camera training data with sufficient contrast and scene diversity, we furthermore develop a data capture method utilizing an HDR monitor, as well as a data augmentation method to generate suitable HDR content. The monitor data is supplemented with real-world data that has less scene diversity but allows us to achieve fine detail recovery without being limited by the monitor resolution. Together, this approach successfully restores color and contrast as well as image detail.



### Fitness Landscape Footprint: A Framework to Compare Neural Architecture Search Problems
- **Arxiv ID**: http://arxiv.org/abs/2111.01584v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2111.01584v1)
- **Published**: 2021-11-02 13:20:01+00:00
- **Updated**: 2021-11-02 13:20:01+00:00
- **Authors**: Kalifou René Traoré, Andrés Camero, Xiao Xiang Zhu
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Neural architecture search is a promising area of research dedicated to automating the design of neural network models. This field is rapidly growing, with a surge of methodologies ranging from Bayesian optimization,neuroevoltion, to differentiable search, and applications in various contexts. However, despite all great advances, few studies have presented insights on the difficulty of the problem itself, thus the success (or fail) of these methodologies remains unexplained. In this sense, the field of optimization has developed methods that highlight key aspects to describe optimization problems. The fitness landscape analysis stands out when it comes to characterize reliably and quantitatively search algorithms. In this paper, we propose to use fitness landscape analysis to study a neural architecture search problem. Particularly, we introduce the fitness landscape footprint, an aggregation of eight (8)general-purpose metrics to synthesize the landscape of an architecture search problem. We studied two problems, the classical image classification benchmark CIFAR-10, and the Remote-Sensing problem So2Sat LCZ42. The results present a quantitative appraisal of the problems, allowing to characterize the relative difficulty and other characteristics, such as the ruggedness or the persistence, that helps to tailor a search strategy to the problem. Also, the footprint is a tool that enables the comparison of multiple problems.



### Detect-and-Segment: a Deep Learning Approach to Automate Wound Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.01590v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.01590v1)
- **Published**: 2021-11-02 13:39:13+00:00
- **Updated**: 2021-11-02 13:39:13+00:00
- **Authors**: Gaetano Scebba, Jia Zhang, Sabrina Catanzaro, Carina Mihai, Oliver Distler, Martin Berli, Walter Karlen
- **Comment**: None
- **Journal**: None
- **Summary**: Chronic wounds significantly impact quality of life. If not properly managed, they can severely deteriorate. Image-based wound analysis could aid in objectively assessing the wound status by quantifying important features that are related to healing. However, the high heterogeneity of the wound types, image background composition, and capturing conditions challenge the robust segmentation of wound images. We present Detect-and-Segment (DS), a deep learning approach to produce wound segmentation maps with high generalization capabilities. In our approach, dedicated deep neural networks detected the wound position, isolated the wound from the uninformative background, and computed the wound segmentation map. We evaluated this approach using one data set with images of diabetic foot ulcers. For further testing, 4 supplemental independent data sets with larger variety of wound types from different body locations were used. The Matthews' correlation coefficient (MCC) improved from 0.29 when computing the segmentation on the full image to 0.85 when combining detection and segmentation in the same approach. When tested on the wound images drawn from the supplemental data sets, the DS approach increased the mean MCC from 0.17 to 0.85. Furthermore, the DS approach enabled the training of segmentation models with up to 90% less training data while maintaining the segmentation performance.



### Estimating 3D Motion and Forces of Human-Object Interactions from Internet Videos
- **Arxiv ID**: http://arxiv.org/abs/2111.01591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.01591v1)
- **Published**: 2021-11-02 13:40:18+00:00
- **Updated**: 2021-11-02 13:40:18+00:00
- **Authors**: Zongmian Li, Jiri Sedlar, Justin Carpentier, Ivan Laptev, Nicolas Mansard, Josef Sivic
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1904.02683
- **Journal**: None
- **Summary**: In this paper, we introduce a method to automatically reconstruct the 3D motion of a person interacting with an object from a single RGB video. Our method estimates the 3D poses of the person together with the object pose, the contact positions and the contact forces exerted on the human body. The main contributions of this work are three-fold. First, we introduce an approach to jointly estimate the motion and the actuation forces of the person on the manipulated object by modeling contacts and the dynamics of the interactions. This is cast as a large-scale trajectory optimization problem. Second, we develop a method to automatically recognize from the input video the 2D position and timing of contacts between the person and the object or the ground, thereby significantly simplifying the complexity of the optimization. Third, we validate our approach on a recent video+MoCap dataset capturing typical parkour actions, and demonstrate its performance on a new dataset of Internet videos showing people manipulating a variety of tools in unconstrained environments.



### Trajectory Prediction with Graph-based Dual-scale Context Fusion
- **Arxiv ID**: http://arxiv.org/abs/2111.01592v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.01592v2)
- **Published**: 2021-11-02 13:42:16+00:00
- **Updated**: 2022-07-30 15:59:18+00:00
- **Authors**: Lu Zhang, Peiliang Li, Jing Chen, Shaojie Shen
- **Comment**: Accepted by IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS) 2022. Code: https://github.com/HKUST-Aerial-Robotics/DSP
- **Journal**: None
- **Summary**: Motion prediction for traffic participants is essential for a safe and robust automated driving system, especially in cluttered urban environments. However, it is highly challenging due to the complex road topology as well as the uncertain intentions of the other agents. In this paper, we present a graph-based trajectory prediction network named the Dual Scale Predictor (DSP), which encodes both the static and dynamical driving context in a hierarchical manner. Different from methods based on a rasterized map or sparse lane graph, we consider the driving context as a graph with two layers, focusing on both geometrical and topological features. Graph neural networks (GNNs) are applied to extract features with different levels of granularity, and features are subsequently aggregated with attention-based inter-layer networks, realizing better local-global feature fusion. Following the recent goal-driven trajectory prediction pipeline, goal candidates with high likelihood for the target agent are extracted, and predicted trajectories are generated conditioned on these goals. Thanks to the proposed dual-scale context fusion network, our DSP is able to generate accurate and human-like multi-modal trajectories. We evaluate the proposed method on the large-scale Argoverse motion forecasting benchmark, and it achieves promising results, outperforming the recent state-of-the-art methods.



### A Critical Study on the Recent Deep Learning Based Semi-Supervised Video Anomaly Detection Methods
- **Arxiv ID**: http://arxiv.org/abs/2111.01604v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.01604v1)
- **Published**: 2021-11-02 14:00:33+00:00
- **Updated**: 2021-11-02 14:00:33+00:00
- **Authors**: Mohammad Baradaran, Robert Bergevin
- **Comment**: None
- **Journal**: None
- **Summary**: Video anomaly detection is one of the hot research topics in computer vision nowadays, as abnormal events contain a high amount of information. Anomalies are one of the main detection targets in surveillance systems, usually needing real-time actions. Regarding the availability of labeled data for training (i.e., there is not enough labeled data for abnormalities), semi-supervised anomaly detection approaches have gained interest recently. This paper introduces the researchers of the field to a new perspective and reviews the recent deep-learning based semi-supervised video anomaly detection approaches, based on a common strategy they use for anomaly detection. Our goal is to help researchers develop more effective video anomaly detection methods. As the selection of a right Deep Neural Network plays an important role for several parts of this task, a quick comparative review on DNNs is prepared first. Unlike previous surveys, DNNs are reviewed from a spatiotemporal feature extraction viewpoint, customized for video anomaly detection. This part of the review can help researchers in this field select suitable networks for different parts of their methods. Moreover, some of the state-of-the-art anomaly detection methods, based on their detection strategy, are critically surveyed. The review provides a novel and deep look at existing methods and results in stating the shortcomings of these approaches, which can be a hint for future works.



### PolyTrack: Tracking with Bounding Polygons
- **Arxiv ID**: http://arxiv.org/abs/2111.01606v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.01606v1)
- **Published**: 2021-11-02 14:05:41+00:00
- **Updated**: 2021-11-02 14:05:41+00:00
- **Authors**: Gaspar Faure, Hughes Perreault, Guillaume-Alexandre Bilodeau, Nicolas Saunier
- **Comment**: NeurIPS 2021 Machine Learning for Autonomous Driving Workshop
- **Journal**: None
- **Summary**: In this paper, we present a novel method called PolyTrack for fast multi-object tracking and segmentation using bounding polygons. Polytrack detects objects by producing heatmaps of their center keypoint. For each of them, a rough segmentation is done by computing a bounding polygon over each instance instead of the traditional bounding box. Tracking is done by taking two consecutive frames as input and computing a center offset for each object detected in the first frame to predict its location in the second frame. A Kalman filter is also applied to reduce the number of ID switches. Since our target application is automated driving systems, we apply our method on urban environment videos. We trained and evaluated PolyTrack on the MOTS and KITTIMOTS datasets. Results show that tracking polygons can be a good alternative to bounding box and mask tracking. The code of PolyTrack is available at https://github.com/gafaua/PolyTrack.



### StyleGAN of All Trades: Image Manipulation with Only Pretrained StyleGAN
- **Arxiv ID**: http://arxiv.org/abs/2111.01619v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.01619v1)
- **Published**: 2021-11-02 14:31:22+00:00
- **Updated**: 2021-11-02 14:31:22+00:00
- **Authors**: Min Jin Chong, Hsin-Ying Lee, David Forsyth
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, StyleGAN has enabled various image manipulation and editing tasks thanks to the high-quality generation and the disentangled latent space. However, additional architectures or task-specific training paradigms are usually required for different tasks. In this work, we take a deeper look at the spatial properties of StyleGAN. We show that with a pretrained StyleGAN along with some operations, without any additional architecture, we can perform comparably to the state-of-the-art methods on various tasks, including image blending, panorama generation, generation from a single image, controllable and local multimodal image to image translation, and attributes transfer. The proposed method is simple, effective, efficient, and applicable to any existing pretrained StyleGAN model.



### A Tri-attention Fusion Guided Multi-modal Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/2111.01623v1
- **DOI**: 10.1016/j.patcog.2021.108417
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.01623v1)
- **Published**: 2021-11-02 14:36:53+00:00
- **Updated**: 2021-11-02 14:36:53+00:00
- **Authors**: Tongxue Zhou, Su Ruan, Pierre Vera, Stéphane Canu
- **Comment**: 33 pages, 11 figures, accepted by Pattern Recognition on 01 November
  2021. arXiv admin note: substantial text overlap with arXiv:2102.03111
- **Journal**: Pattern Recognition 2021
- **Summary**: In the field of multimodal segmentation, the correlation between different modalities can be considered for improving the segmentation results. Considering the correlation between different MR modalities, in this paper, we propose a multi-modality segmentation network guided by a novel tri-attention fusion. Our network includes N model-independent encoding paths with N image sources, a tri-attention fusion block, a dual-attention fusion block, and a decoding path. The model independent encoding paths can capture modality-specific features from the N modalities. Considering that not all the features extracted from the encoders are useful for segmentation, we propose to use dual attention based fusion to re-weight the features along the modality and space paths, which can suppress less informative features and emphasize the useful ones for each modality at different positions. Since there exists a strong correlation between different modalities, based on the dual attention fusion block, we propose a correlation attention module to form the tri-attention fusion block. In the correlation attention module, a correlation description block is first used to learn the correlation between modalities and then a constraint based on the correlation is used to guide the network to learn the latent correlated features which are more relevant for segmentation. Finally, the obtained fused feature representation is projected by the decoder to obtain the segmentation results. Our experiment results tested on BraTS 2018 dataset for brain tumor segmentation demonstrate the effectiveness of our proposed method.



### Human Attention in Fine-grained Classification
- **Arxiv ID**: http://arxiv.org/abs/2111.01628v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.01628v1)
- **Published**: 2021-11-02 14:41:11+00:00
- **Updated**: 2021-11-02 14:41:11+00:00
- **Authors**: Yao Rong, Wenjia Xu, Zeynep Akata, Enkelejda Kasneci
- **Comment**: 19 pages, 9 figures
- **Journal**: British Machine Vision Conference (BMVC) 2021
- **Summary**: The way humans attend to, process and classify a given image has the potential to vastly benefit the performance of deep learning models. Exploiting where humans are focusing can rectify models when they are deviating from essential features for correct decisions. To validate that human attention contains valuable information for decision-making processes such as fine-grained classification, we compare human attention and model explanations in discovering important features. Towards this goal, we collect human gaze data for the fine-grained classification dataset CUB and build a dataset named CUB-GHA (Gaze-based Human Attention). Furthermore, we propose the Gaze Augmentation Training (GAT) and Knowledge Fusion Network (KFN) to integrate human gaze knowledge into classification models. We implement our proposals in CUB-GHA and the recently released medical dataset CXR-Eye of chest X-ray images, which includes gaze data collected from a radiologist. Our result reveals that integrating human attention knowledge benefits classification effectively, e.g. improving the baseline by 4.38% on CXR. Hence, our work provides not only valuable insights into understanding human attention in fine-grained classification, but also contributes to future research in integrating human gaze with computer vision tasks. CUB-GHA and code are available at https://github.com/yaorong0921/CUB-GHA.



### Explainable Medical Image Segmentation via Generative Adversarial Networks and Layer-wise Relevance Propagation
- **Arxiv ID**: http://arxiv.org/abs/2111.01665v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.01665v1)
- **Published**: 2021-11-02 15:22:28+00:00
- **Updated**: 2021-11-02 15:22:28+00:00
- **Authors**: Awadelrahman M. A. Ahmed, Leen A. M. Ali
- **Comment**: Nordic Machine Intelligence
- **Journal**: None
- **Summary**: This paper contributes to automating medical image segmentation by proposing generative adversarial network-based models to segment both polyps and instruments in endoscopy images. A major contribution of this work is to provide explanations for the predictions using a layer-wise relevance propagation approach designating which input image pixels are relevant to the predictions and to what extent. On the polyp segmentation task, the models achieved 0.84 of accuracy and 0.46 on Jaccard index. On the instrument segmentation task, the models achieved 0.96 of accuracy and 0.70 on Jaccard index. The code is available at https://github.com/Awadelrahman/MedAI.



### Relational Self-Attention: What's Missing in Attention for Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/2111.01673v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.01673v1)
- **Published**: 2021-11-02 15:36:11+00:00
- **Updated**: 2021-11-02 15:36:11+00:00
- **Authors**: Manjin Kim, Heeseung Kwon, Chunyu Wang, Suha Kwak, Minsu Cho
- **Comment**: Accepted to NeurIPS 2021
- **Journal**: None
- **Summary**: Convolution has been arguably the most important feature transform for modern neural networks, leading to the advance of deep learning. Recent emergence of Transformer networks, which replace convolution layers with self-attention blocks, has revealed the limitation of stationary convolution kernels and opened the door to the era of dynamic feature transforms. The existing dynamic transforms, including self-attention, however, are all limited for video understanding where correspondence relations in space and time, i.e., motion information, are crucial for effective representation. In this work, we introduce a relational feature transform, dubbed the relational self-attention (RSA), that leverages rich structures of spatio-temporal relations in videos by dynamically generating relational kernels and aggregating relational contexts. Our experiments and ablation studies show that the RSA network substantially outperforms convolution and self-attention counterparts, achieving the state of the art on the standard motion-centric benchmarks for video action recognition, such as Something-Something-V1 & V2, Diving48, and FineGym.



### Meta-Learning the Search Distribution of Black-Box Random Search Based Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2111.01714v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.01714v3)
- **Published**: 2021-11-02 16:28:08+00:00
- **Updated**: 2021-11-22 10:42:59+00:00
- **Authors**: Maksym Yatsura, Jan Hendrik Metzen, Matthias Hein
- **Comment**: accepted at NeurIPS 2021; updated the numbers in Table 5 and added
  references; added acknowledgements
- **Journal**: None
- **Summary**: Adversarial attacks based on randomized search schemes have obtained state-of-the-art results in black-box robustness evaluation recently. However, as we demonstrate in this work, their efficiency in different query budget regimes depends on manual design and heuristic tuning of the underlying proposal distributions. We study how this issue can be addressed by adapting the proposal distribution online based on the information obtained during the attack. We consider Square Attack, which is a state-of-the-art score-based black-box attack, and demonstrate how its performance can be improved by a learned controller that adjusts the parameters of the proposal distribution online during the attack. We train the controller using gradient-based end-to-end training on a CIFAR10 model with white box access. We demonstrate that plugging the learned controller into the attack consistently improves its black-box robustness estimate in different query regimes by up to 20% for a wide range of different models with black-box access. We further show that the learned adaptation principle transfers well to the other data distributions such as CIFAR100 or ImageNet and to the targeted attack setting.



### Absolute distance prediction based on deep learning object detection and monocular depth estimation models
- **Arxiv ID**: http://arxiv.org/abs/2111.01715v1
- **DOI**: 10.3233/FAIA210151
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.01715v1)
- **Published**: 2021-11-02 16:29:13+00:00
- **Updated**: 2021-11-02 16:29:13+00:00
- **Authors**: Armin Masoumian, David G. F. Marei, Saddam Abdulwahab, Julian Cristiano, Domenec Puig, Hatem A. Rashwan
- **Comment**: 10 pages, Submitted to 23rd International Conference of the Catalan
  Association for Artificial Intelligence (CCIA 2021)
- **Journal**: None
- **Summary**: Determining the distance between the objects in a scene and the camera sensor from 2D images is feasible by estimating depth images using stereo cameras or 3D cameras. The outcome of depth estimation is relative distances that can be used to calculate absolute distances to be applicable in reality. However, distance estimation is very challenging using 2D monocular cameras. This paper presents a deep learning framework that consists of two deep networks for depth estimation and object detection using a single image. Firstly, objects in the scene are detected and localized using the You Only Look Once (YOLOv5) network. In parallel, the estimated depth image is computed using a deep autoencoder network to detect the relative distances. The proposed object detection based YOLO was trained using a supervised learning technique, in turn, the network of depth estimation was self-supervised training. The presented distance estimation framework was evaluated on real images of outdoor scenes. The achieved results show that the proposed framework is promising and it yields an accuracy of 96% with RMSE of 0.203 of the correct absolute distance.



### MixFace: Improving Face Verification Focusing on Fine-grained Conditions
- **Arxiv ID**: http://arxiv.org/abs/2111.01717v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.01717v3)
- **Published**: 2021-11-02 16:34:54+00:00
- **Updated**: 2022-06-19 18:43:18+00:00
- **Authors**: Junuk Jung, Sungbin Son, Joochan Park, Yongjun Park, Seonhoon Lee, Heung-Seon Oh
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: The performance of face recognition has become saturated for public benchmark datasets such as LFW, CFP-FP, and AgeDB, owing to the rapid advances in CNNs. However, the effects of faces with various fine-grained conditions on FR models have not been investigated because of the absence of such datasets. This paper analyzes their effects in terms of different conditions and loss functions using K-FACE, a recently introduced FR dataset with fine-grained conditions. We propose a novel loss function, MixFace, that combines classification and metric losses. The superiority of MixFace in terms of effectiveness and robustness is demonstrated experimentally on various benchmark datasets.



### CPSeg: Cluster-free Panoptic Segmentation of 3D LiDAR Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2111.01723v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.01723v2)
- **Published**: 2021-11-02 16:44:06+00:00
- **Updated**: 2023-02-03 00:16:53+00:00
- **Authors**: Enxu Li, Ryan Razani, Yixuan Xu, Bingbing Liu
- **Comment**: Accepted at ICRA 2023
- **Journal**: None
- **Summary**: A fast and accurate panoptic segmentation system for LiDAR point clouds is crucial for autonomous driving vehicles to understand the surrounding objects and scenes. Existing approaches usually rely on proposals or clustering to segment foreground instances. As a result, they struggle to achieve real-time performance. In this paper, we propose a novel real-time end-to-end panoptic segmentation network for LiDAR point clouds, called CPSeg. In particular, CPSeg comprises a shared encoder, a dual-decoder, and a cluster-free instance segmentation head, which is able to dynamically pillarize foreground points according to the learned embedding. Then, it acquires instance labels by finding connected pillars with a pairwise embedding comparison. Thus, the conventional proposal-based or clustering-based instance segmentation is transformed into a binary segmentation problem on the pairwise embedding comparison matrix. To help the network regress instance embedding, a fast and deterministic depth completion algorithm is proposed to calculate the surface normal of each point cloud in real-time. The proposed method is benchmarked on two large-scale autonomous driving datasets: SemanticKITTI and nuScenes. Notably, extensive experimental results show that CPSeg achieves state-of-the-art results among real-time approaches on both datasets.



### Personalized One-Shot Lipreading for an ALS Patient
- **Arxiv ID**: http://arxiv.org/abs/2111.01740v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2111.01740v1)
- **Published**: 2021-11-02 17:03:29+00:00
- **Updated**: 2021-11-02 17:03:29+00:00
- **Authors**: Bipasha Sen, Aditya Agarwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, C V Jawahar
- **Comment**: None
- **Journal**: BMVC 2021
- **Summary**: Lipreading or visually recognizing speech from the mouth movements of a speaker is a challenging and mentally taxing task. Unfortunately, multiple medical conditions force people to depend on this skill in their day-to-day lives for essential communication. Patients suffering from Amyotrophic Lateral Sclerosis (ALS) often lose muscle control, consequently their ability to generate speech and communicate via lip movements. Existing large datasets do not focus on medical patients or curate personalized vocabulary relevant to an individual. Collecting a large-scale dataset of a patient, needed to train mod-ern data-hungry deep learning models is, however, extremely challenging. In this work, we propose a personalized network to lipread an ALS patient using only one-shot examples. We depend on synthetically generated lip movements to augment the one-shot scenario. A Variational Encoder based domain adaptation technique is used to bridge the real-synthetic domain gap. Our approach significantly improves and achieves high top-5accuracy with 83.2% accuracy compared to 62.6% achieved by comparable methods for the patient. Apart from evaluating our approach on the ALS patient, we also extend it to people with hearing impairment relying extensively on lip movements to communicate.



### LogAvgExp Provides a Principled and Performant Global Pooling Operator
- **Arxiv ID**: http://arxiv.org/abs/2111.01742v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.01742v1)
- **Published**: 2021-11-02 17:05:04+00:00
- **Updated**: 2021-11-02 17:05:04+00:00
- **Authors**: Scott C. Lowe, Thomas Trappenberg, Sageev Oore
- **Comment**: None
- **Journal**: None
- **Summary**: We seek to improve the pooling operation in neural networks, by applying a more theoretically justified operator. We demonstrate that LogSumExp provides a natural OR operator for logits. When one corrects for the number of elements inside the pooling operator, this becomes $\text{LogAvgExp} := \log(\text{mean}(\exp(x)))$. By introducing a single temperature parameter, LogAvgExp smoothly transitions from the max of its operands to the mean (found at the limiting cases $t \to 0^+$ and $t \to +\infty$). We experimentally tested LogAvgExp, both with and without a learnable temperature parameter, in a variety of deep neural network architectures for computer vision.



### PatchGame: Learning to Signal Mid-level Patches in Referential Games
- **Arxiv ID**: http://arxiv.org/abs/2111.01785v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.01785v1)
- **Published**: 2021-11-02 17:59:00+00:00
- **Updated**: 2021-11-02 17:59:00+00:00
- **Authors**: Kamal Gupta, Gowthami Somepalli, Anubhav Gupta, Vinoj Jayasundara, Matthias Zwicker, Abhinav Shrivastava
- **Comment**: To appear at NeurIPS 2021
- **Journal**: None
- **Summary**: We study a referential game (a type of signaling game) where two agents communicate with each other via a discrete bottleneck to achieve a common goal. In our referential game, the goal of the speaker is to compose a message or a symbolic representation of "important" image patches, while the task for the listener is to match the speaker's message to a different view of the same image. We show that it is indeed possible for the two agents to develop a communication protocol without explicit or implicit supervision. We further investigate the developed protocol and show the applications in speeding up recent Vision Transformers by using only important patches, and as pre-training for downstream recognition tasks (e.g., classification). Code available at https://github.com/kampta/PatchGame.



### 3-D PET Image Generation with tumour masks using TGAN
- **Arxiv ID**: http://arxiv.org/abs/2111.01866v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.01866v1)
- **Published**: 2021-11-02 19:53:13+00:00
- **Updated**: 2021-11-02 19:53:13+00:00
- **Authors**: Robert V Bergen, Jean-Francois Rajotte, Fereshteh Yousefirizi, Ivan S Klyuzhin, Arman Rahmim, Raymond T. Ng
- **Comment**: None
- **Journal**: None
- **Summary**: Training computer-vision related algorithms on medical images for disease diagnosis or image segmentation is difficult due to the lack of training data, labeled samples, and privacy concerns. For this reason, a robust generative method to create synthetic data is highly sought after. However, most three-dimensional image generators require additional image input or are extremely memory intensive. To address these issues we propose adapting video generation techniques for 3-D image generation. Using the temporal GAN (TGAN) architecture, we show we are able to generate realistic head and neck PET images. We also show that by conditioning the generator on tumour masks, we are able to control the geometry and location of the tumour in the generated images. To test the utility of the synthetic images, we train a segmentation model using the synthetic images. Synthetic images conditioned on real tumour masks are automatically segmented, and the corresponding real images are also segmented. We evaluate the segmentations using the Dice score and find the segmentation algorithm performs similarly on both datasets (0.65 synthetic data, 0.70 real data). Various radionomic features are then calculated over the segmented tumour volumes for each data set. A comparison of the real and synthetic feature distributions show that seven of eight feature distributions had statistically insignificant differences (p>0.05). Correlation coefficients were also calculated between all radionomic features and it is shown that all of the strong statistical correlations in the real data set are preserved in the synthetic data set.



### Body Size and Depth Disambiguation in Multi-Person Reconstruction from Single Images
- **Arxiv ID**: http://arxiv.org/abs/2111.01884v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.01884v2)
- **Published**: 2021-11-02 20:42:41+00:00
- **Updated**: 2021-12-08 21:00:42+00:00
- **Authors**: Nicolas Ugrinovic, Adria Ruiz, Antonio Agudo, Alberto Sanfeliu, Francesc Moreno-Noguer
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of multi-person 3D body pose and shape estimation from a single image. While this problem can be addressed by applying single-person approaches multiple times for the same scene, recent works have shown the advantages of building upon deep architectures that simultaneously reason about all people in the scene in a holistic manner by enforcing, e.g., depth order constraints or minimizing interpenetration among reconstructed bodies. However, existing approaches are still unable to capture the size variability of people caused by the inherent body scale and depth ambiguity. In this work, we tackle this challenge by devising a novel optimization scheme that learns the appropriate body scale and relative camera pose, by enforcing the feet of all people to remain on the ground floor. A thorough evaluation on MuPoTS-3D and 3DPW datasets demonstrates that our approach is able to robustly estimate the body translation and shape of multiple people while retrieving their spatial arrangement, consistently improving current state-of-the-art, especially in scenes with people of very different heights



### A dataset for multi-sensor drone detection
- **Arxiv ID**: http://arxiv.org/abs/2111.01888v1
- **DOI**: 10.1016/j.dib.2021.107521
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2111.01888v1)
- **Published**: 2021-11-02 20:52:03+00:00
- **Updated**: 2021-11-02 20:52:03+00:00
- **Authors**: Fredrik Svanström, Fernando Alonso-Fernandez, Cristofer Englund
- **Comment**: Published at Elsevier Data in Brief journal. arXiv admin note: text
  overlap with arXiv:2007.07396
- **Journal**: None
- **Summary**: The use of small and remotely controlled unmanned aerial vehicles (UAVs), or drones, has increased in recent years. This goes in parallel with misuse episodes, with an evident threat to the safety of people or facilities. As a result, the detection of UAV has also emerged as a research topic. Most studies on drone detection fail to specify the type of acquisition device, the drone type, the detection range, or the dataset. The lack of proper UAV detection studies employing thermal infrared cameras is also an issue, despite its success with other targets. Besides, we have not found any previous study that addresses the detection task as a function of distance to the target. Sensor fusion is indicated as an open research issue as well, although research in this direction is scarce too. To counteract the mentioned issues and allow fundamental studies with a common public benchmark, we contribute with an annotated multi-sensor database for drone detection that includes infrared and visible videos and audio files. The database includes three different drones, of different sizes and other flying objects that can be mistakenly detected as drones, such as birds, airplanes or helicopters. In addition to using several different sensors, the number of classes is higher than in previous studies. To allow studies as a function of the sensor-to-target distance, the dataset is divided into three categories (Close, Medium, Distant) according to the industry-standard Detect, Recognize and Identify (DRI) requirements, built on the Johnson criteria. Given that the drones must be flown within visual range due to regulations, the largest sensor-to-target distance for a drone is 200 m, and acquisitions are made in daylight. The data has been obtained at three airports in Sweden: Halmstad Airport (IATA code: HAD/ICAO code: ESMT), Gothenburg City Airport (GSE/ESGP) and Malm\"o Airport (MMX/ESMS).



### BiosecurID: a multimodal biometric database
- **Arxiv ID**: http://arxiv.org/abs/2111.03472v1
- **DOI**: 10.1007/s10044-009-0151-4
- **Categories**: **cs.CR**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03472v1)
- **Published**: 2021-11-02 21:01:33+00:00
- **Updated**: 2021-11-02 21:01:33+00:00
- **Authors**: Julian Fierrez, Javier Galbally, Javier Ortega-Garcia, Manuel R Freire, Fernando Alonso-Fernandez, Daniel Ramos, Doroteo Torre Toledano, Joaquin Gonzalez-Rodriguez, Juan A Siguenza, Javier Garrido-Salas, E Anguiano, Guillermo Gonzalez-de-Rivera, Ricardo Ribalda, Marcos Faundez-Zanuy, JA Ortega, Valentín Cardeñoso-Payo, A Viloria, Carlos E Vivaracho, Q Isaac Moro, Juan J Igarza, J Sanchez, Inmaculada Hernaez, Carlos Orrite-Urunuela, Francisco Martinez-Contreras, Juan José Gracia-Roche
- **Comment**: Published at Pattern Analysis and Applications journal
- **Journal**: None
- **Summary**: A new multimodal biometric database, acquired in the framework of the BiosecurID project, is presented together with the description of the acquisition setup and protocol. The database includes eight unimodal biometric traits, namely: speech, iris, face (still images, videos of talking faces), handwritten signature and handwritten text (on-line dynamic signals, off-line scanned images), fingerprints (acquired with two different sensors), hand (palmprint, contour-geometry) and keystroking. The database comprises 400 subjects and presents features such as: realistic acquisition scenario, balanced gender and population distributions, availability of information about particular demographic groups (age, gender, handedness), acquisition of replay attacks for speech and keystroking, skilled forgeries for signatures, and compatibility with other existing databases. All these characteristics make it very useful in research and development of unimodal and multimodal biometric systems.



### A high performance fingerprint liveness detection method based on quality related features
- **Arxiv ID**: http://arxiv.org/abs/2111.01898v1
- **DOI**: 10.1016/j.future.2010.11.024
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.01898v1)
- **Published**: 2021-11-02 21:09:39+00:00
- **Updated**: 2021-11-02 21:09:39+00:00
- **Authors**: Javier Galbally, Fernando Alonso-Fernandez, Julian Fierrez, Javier Ortega-Garcia
- **Comment**: Published at Elsevier Future Generation Computer Systems journal
- **Journal**: None
- **Summary**: A new software-based liveness detection approach using a novel fingerprint parameterization based on quality related features is proposed. The system is tested on a highly challenging database comprising over 10,500 real and fake images acquired with five sensors of different technologies and covering a wide range of direct attack scenarios in terms of materials and procedures followed to generate the gummy fingers. The proposed solution proves to be robust to the multi-scenario dataset, and presents an overall rate of 90% correctly classified samples. Furthermore, the liveness detection method presented has the added advantage over previously studied techniques of needing just one image from a finger to decide whether it is real or fake. This last characteristic provides the method with very valuable features as it makes it less intrusive, more user friendly, faster and reduces its implementation costs.



### Deep learning for identification and face, gender, expression recognition under constraints
- **Arxiv ID**: http://arxiv.org/abs/2111.01930v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.01930v1)
- **Published**: 2021-11-02 22:45:09+00:00
- **Updated**: 2021-11-02 22:45:09+00:00
- **Authors**: Ahmad B. Hassanat, Abeer Albustanji, Ahmad S. Tarawneh, Malek Alrashidi, Hani Alharbi, Mohammed Alanazi, Mansoor Alghamdi, Ibrahim S Alkhazi, V. B. Surya Prasath
- **Comment**: Submitted to International Journal of Biometrics
- **Journal**: None
- **Summary**: Biometric recognition based on the full face is an extensive research area. However, using only partially visible faces, such as in the case of veiled-persons, is a challenging task. Deep convolutional neural network (CNN) is used in this work to extract the features from veiled-person face images. We found that the sixth and the seventh fully connected layers, FC6 and FC7 respectively, in the structure of the VGG19 network provide robust features with each of these two layers containing 4096 features. The main objective of this work is to test the ability of deep learning based automated computer system to identify not only persons, but also to perform recognition of gender, age, and facial expressions such as eye smile. Our experimental results indicate that we obtain high accuracy for all the tasks. The best recorded accuracy values are up to 99.95% for identifying persons, 99.9% for gender recognition, 99.9% for age recognition and 80.9% for facial expression (eye smile) recognition.



### Revisiting spatio-temporal layouts for compositional action recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.01936v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.01936v1)
- **Published**: 2021-11-02 23:04:39+00:00
- **Updated**: 2021-11-02 23:04:39+00:00
- **Authors**: Gorjan Radevski, Marie-Francine Moens, Tinne Tuytelaars
- **Comment**: Published in BMVC 2021 (Oral)
- **Journal**: None
- **Summary**: Recognizing human actions is fundamentally a spatio-temporal reasoning problem, and should be, at least to some extent, invariant to the appearance of the human and the objects involved. Motivated by this hypothesis, in this work, we take an object-centric approach to action recognition. Multiple works have studied this setting before, yet it remains unclear (i) how well a carefully crafted, spatio-temporal layout-based method can recognize human actions, and (ii) how, and when, to fuse the information from layout and appearance-based models. The main focus of this paper is compositional/few-shot action recognition, where we advocate the usage of multi-head attention (proven to be effective for spatial reasoning) over spatio-temporal layouts, i.e., configurations of object bounding boxes. We evaluate different schemes to inject video appearance information to the system, and benchmark our approach on background cluttered action recognition. On the Something-Else and Action Genome datasets, we demonstrate (i) how to extend multi-head attention for spatio-temporal layout-based action recognition, (ii) how to improve the performance of appearance-based models by fusion with layout-based models, (iii) that even on non-compositional background-cluttered video datasets, a fusion between layout- and appearance-based models improves the performance.



