# Arxiv Papers in cs.CV on 2021-11-13
### A Central Difference Graph Convolutional Operator for Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.06995v1
- **DOI**: 10.1109/TCSVT.2021.3124562
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06995v1)
- **Published**: 2021-11-13 00:02:57+00:00
- **Updated**: 2021-11-13 00:02:57+00:00
- **Authors**: Shuangyan Miao, Yonghong Hou, Zhimin Gao, Mingliang Xu, Wanqing Li
- **Comment**: Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT)
- **Journal**: None
- **Summary**: This paper proposes a new graph convolutional operator called central difference graph convolution (CDGC) for skeleton based action recognition. It is not only able to aggregate node information like a vanilla graph convolutional operation but also gradient information. Without introducing any additional parameters, CDGC can replace vanilla graph convolution in any existing Graph Convolutional Networks (GCNs). In addition, an accelerated version of the CDGC is developed which greatly improves the speed of training. Experiments on two popular large-scale datasets NTU RGB+D 60 & 120 have demonstrated the efficacy of the proposed CDGC. Code is available at https://github.com/iesymiao/CD-GCN.



### Leveraging Unsupervised Image Registration for Discovery of Landmark Shape Descriptor
- **Arxiv ID**: http://arxiv.org/abs/2111.07009v1
- **DOI**: 10.1016/j.media.2021.102157
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07009v1)
- **Published**: 2021-11-13 01:02:10+00:00
- **Updated**: 2021-11-13 01:02:10+00:00
- **Authors**: Riddhish Bhalodia, Shireen Elhabian, Ladislav Kavan, Ross Whitaker
- **Comment**: Published in Medical Image Analysis
- **Journal**: None
- **Summary**: In current biological and medical research, statistical shape modeling (SSM) provides an essential framework for the characterization of anatomy/morphology. Such analysis is often driven by the identification of a relatively small number of geometrically consistent features found across the samples of a population. These features can subsequently provide information about the population shape variation. Dense correspondence models can provide ease of computation and yield an interpretable low-dimensional shape descriptor when followed by dimensionality reduction. However, automatic methods for obtaining such correspondences usually require image segmentation followed by significant preprocessing, which is taxing in terms of both computation as well as human resources. In many cases, the segmentation and subsequent processing require manual guidance and anatomy specific domain expertise. This paper proposes a self-supervised deep learning approach for discovering landmarks from images that can directly be used as a shape descriptor for subsequent analysis. We use landmark-driven image registration as the primary task to force the neural network to discover landmarks that register the images well. We also propose a regularization term that allows for robust optimization of the neural network and ensures that the landmarks uniformly span the image domain. The proposed method circumvents segmentation and preprocessing and directly produces a usable shape descriptor using just 2D or 3D images. In addition, we also propose two variants on the training loss function that allows for prior shape information to be integrated into the model. We apply this framework on several 2D and 3D datasets to obtain their shape descriptors, and analyze their utility for various applications.



### Improving the Otsu Thresholding Method of Global Binarization Using Ring Theory for Ultrasonographies of Congestive Heart Failure
- **Arxiv ID**: http://arxiv.org/abs/2111.07031v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, math.RA, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.07031v1)
- **Published**: 2021-11-13 04:08:02+00:00
- **Updated**: 2021-11-13 04:08:02+00:00
- **Authors**: Alisa Rahim, Esley Torres
- **Comment**: None
- **Journal**: None
- **Summary**: Ring Theory states that a ring is an algebraic structure where two binary operations can be performed among the elements addition and multiplication. Binarization is a method of image processing where values within pixels are reduced to a scale from zero to one, with zero representing the most absence of light and one representing the most presence of light. Currently, sonograms are implemented in scanning for congestive heart failure. However, the renowned Playboy Bunny symbol representing the ailment becomes increasingly difficult to isolate due to surrounding organs and lower quality image productions. This paper examines the Otsu thresholding method and incorporates new elements to account for different image features meant to better isolate congestive heart failure indicators in ultrasound images.



### UET-Headpose: A sensor-based top-view head pose dataset
- **Arxiv ID**: http://arxiv.org/abs/2111.07039v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2111.07039v1)
- **Published**: 2021-11-13 04:54:20+00:00
- **Updated**: 2021-11-13 04:54:20+00:00
- **Authors**: Linh Nguyen Viet, Tuan Nguyen Dinh, Hoang Nguyen Viet, Duc Tran Minh, Long Tran Quoc
- **Comment**: None
- **Journal**: None
- **Summary**: Head pose estimation is a challenging task that aims to solve problems related to predicting three dimensions vector, that serves for many applications in human-robot interaction or customer behavior. Previous researches have proposed some precise methods for collecting head pose data. But those methods require either expensive devices like depth cameras or complex laboratory environment setup. In this research, we introduce a new approach with efficient cost and easy setup to collecting head pose images, namely UET-Headpose dataset, with top-view head pose data. This method uses an absolute orientation sensor instead of Depth cameras to be set up quickly and small cost but still ensure good results. Through experiments, our dataset has been shown the difference between its distribution and available dataset like CMU Panoptic Dataset \cite{CMU}. Besides using the UET-Headpose dataset and other head pose datasets, we also introduce the full-range model called FSANet-Wide, which significantly outperforms head pose estimation results by the UET-Headpose dataset, especially on top-view images. Also, this model is very lightweight and takes small size images.



### Hyperspectral Mixed Noise Removal via Subspace Representation and Weighted Low-rank Tensor Regularization
- **Arxiv ID**: http://arxiv.org/abs/2111.07044v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07044v1)
- **Published**: 2021-11-13 05:30:56+00:00
- **Updated**: 2021-11-13 05:30:56+00:00
- **Authors**: Hang Zhou, Yanchi Su, Zhanshan Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, the low-rank property of different components extracted from the image has been considered in man hyperspectral image denoising methods. However, these methods usually unfold the 3D tensor to 2D matrix or 1D vector to exploit the prior information, such as nonlocal spatial self-similarity (NSS) and global spectral correlation (GSC), which break the intrinsic structure correlation of hyperspectral image (HSI) and thus lead to poor restoration quality. In addition, most of them suffer from heavy computational burden issues due to the involvement of singular value decomposition operation on matrix and tensor in the original high-dimensionality space of HSI. We employ subspace representation and the weighted low-rank tensor regularization (SWLRTR) into the model to remove the mixed noise in the hyperspectral image. Specifically, to employ the GSC among spectral bands, the noisy HSI is projected into a low-dimensional subspace which simplified calculation. After that, a weighted low-rank tensor regularization term is introduced to characterize the priors in the reduced image subspace. Moreover, we design an algorithm based on alternating minimization to solve the nonconvex problem. Experiments on simulated and real datasets demonstrate that the SWLRTR method performs better than other hyperspectral denoising methods quantitatively and visually.



### Facial Landmark Points Detection Using Knowledge Distillation-Based Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2111.07047v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07047v1)
- **Published**: 2021-11-13 05:45:14+00:00
- **Updated**: 2021-11-13 05:45:14+00:00
- **Authors**: Ali Pourramezan Fard, Mohammad H. Mahoor
- **Comment**: Accepted in Computer Vision and Image Understanding Journal
- **Journal**: None
- **Summary**: Facial landmark detection is a vital step for numerous facial image analysis applications. Although some deep learning-based methods have achieved good performances in this task, they are often not suitable for running on mobile devices. Such methods rely on networks with many parameters, which makes the training and inference time-consuming. Training lightweight neural networks such as MobileNets are often challenging, and the models might have low accuracy. Inspired by knowledge distillation (KD), this paper presents a novel loss function to train a lightweight Student network (e.g., MobileNetV2) for facial landmark detection. We use two Teacher networks, a Tolerant-Teacher and a Tough-Teacher in conjunction with the Student network. The Tolerant-Teacher is trained using Soft-landmarks created by active shape models, while the Tough-Teacher is trained using the ground truth (aka Hard-landmarks) landmark points. To utilize the facial landmark points predicted by the Teacher networks, we define an Assistive Loss (ALoss) for each Teacher network. Moreover, we define a loss function called KD-Loss that utilizes the facial landmark points predicted by the two pre-trained Teacher networks (EfficientNet-b3) to guide the lightweight Student network towards predicting the Hard-landmarks. Our experimental results on three challenging facial datasets show that the proposed architecture will result in a better-trained Student network that can extract facial landmark points with high accuracy.



### Image Classification with Consistent Supporting Evidence
- **Arxiv ID**: http://arxiv.org/abs/2111.07048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07048v1)
- **Published**: 2021-11-13 05:47:26+00:00
- **Updated**: 2021-11-13 05:47:26+00:00
- **Authors**: Peiqi Wang, Ruizhi Liao, Daniel Moyer, Seth Berkowitz, Steven Horng, Polina Golland
- **Comment**: 13 pages, 6 figures, proceedings of the Machine Learning for Health
  NeurIPS Workshop, 2021
- **Journal**: None
- **Summary**: Adoption of machine learning models in healthcare requires end users' trust in the system. Models that provide additional supportive evidence for their predictions promise to facilitate adoption. We define consistent evidence to be both compatible and sufficient with respect to model predictions. We propose measures of model inconsistency and regularizers that promote more consistent evidence. We demonstrate our ideas in the context of edema severity grading from chest radiographs. We demonstrate empirically that consistent models provide competitive performance while supporting interpretation.



### Factorial Convolution Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2111.07072v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07072v1)
- **Published**: 2021-11-13 09:02:53+00:00
- **Updated**: 2021-11-13 09:02:53+00:00
- **Authors**: Jaemo Sung, Eun-Sung Jung
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, GoogleNet has garnered substantial attention as one of the base convolutional neural networks (CNNs) to extract visual features for object detection. However, it experiences challenges of contaminated deep features when concatenating elements with different properties. Also, since GoogleNet is not an entirely lightweight CNN, it still has many execution overheads to apply to a resource-starved application domain. Therefore, a new CNNs, FactorNet, has been proposed to overcome these functional challenges. The FactorNet CNN is composed of multiple independent sub CNNs to encode different aspects of the deep visual features and has far fewer execution overheads in terms of weight parameters and floating-point operations. Incorporating FactorNet into the Faster-RCNN framework proved that FactorNet gives \ignore{a 5\%} better accuracy at a minimum and produces additional speedup over GoolgleNet throughout the KITTI object detection benchmark data set in a real-time object detection system.



### Memotion Analysis through the Lens of Joint Embedding
- **Arxiv ID**: http://arxiv.org/abs/2111.07074v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07074v3)
- **Published**: 2021-11-13 09:22:39+00:00
- **Updated**: 2021-12-03 07:15:14+00:00
- **Authors**: Nethra Gunti, Sathyanarayanan Ramamoorthy, Parth Patwa, Amitava Das
- **Comment**: Accepted as Student Abstract at AAAI-22
- **Journal**: None
- **Summary**: Joint embedding (JE) is a way to encode multi-modal data into a vector space where text remains as the grounding key and other modalities like image are to be anchored with such keys. Meme is typically an image with embedded text onto it. Although, memes are commonly used for fun, they could also be used to spread hate and fake information. That along with its growing ubiquity over several social platforms has caused automatic analysis of memes to become a widespread topic of research. In this paper, we report our initial experiments on Memotion Analysis problem through joint embeddings. Results are marginally yielding SOTA.



### D$^2$LV: A Data-Driven and Local-Verification Approach for Image Copy Detection
- **Arxiv ID**: http://arxiv.org/abs/2111.07090v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07090v2)
- **Published**: 2021-11-13 10:56:58+00:00
- **Updated**: 2021-12-04 17:24:04+00:00
- **Authors**: Wenhao Wang, Yifan Sun, Weipu Zhang, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Image copy detection is of great importance in real-life social media. In this paper, a data-driven and local-verification (D$^2$LV) approach is proposed to compete for Image Similarity Challenge: Matching Track at NeurIPS'21. In D$^2$LV, unsupervised pre-training substitutes the commonly-used supervised one. When training, we design a set of basic and six advanced transformations, and a simple but effective baseline learns robust representation. During testing, a global-local and local-global matching strategy is proposed. The strategy performs local-verification between reference and query images. Experiments demonstrate that the proposed method is effective. The proposed approach ranks first out of 1,103 participants on the Facebook AI Image Similarity Challenge: Matching Track. The code and trained models are available at https://github.com/WangWenhao0716/ISC-Track1-Submission.



### Deep Neural Networks for Automatic Grain-matrix Segmentation in Plane and Cross-polarized Sandstone Photomicrographs
- **Arxiv ID**: http://arxiv.org/abs/2111.07102v1
- **DOI**: 10.1007/s10489-021-02530-z
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.07102v1)
- **Published**: 2021-11-13 12:04:06+00:00
- **Updated**: 2021-11-13 12:04:06+00:00
- **Authors**: Rajdeep Das, Ajoy Mondal, Tapan Chakraborty, Kuntal Ghosh
- **Comment**: None
- **Journal**: None
- **Summary**: Grain segmentation of sandstone that is partitioning the grain from its surrounding matrix/cement in the thin section is the primary step for computer-aided mineral identification and sandstone classification. The microscopic images of sandstone contain many mineral grains and their surrounding matrix/cement. The distinction between adjacent grains and the matrix is often ambiguous, making grain segmentation difficult. Various solutions exist in literature to handle these problems; however, they are not robust against sandstone petrography's varied pattern. In this paper, we formulate grain segmentation as a pixel-wise two-class (i.e., grain and background) semantic segmentation task. We develop a deep learning-based end-to-end trainable framework named Deep Semantic Grain Segmentation network (DSGSN), a data-driven method, and provide a generic solution. As per the authors' knowledge, this is the first work where the deep neural network is explored to solve the grain segmentation problem. Extensive experiments on microscopic images highlight that our method obtains better segmentation accuracy than various segmentation architectures with more parameters.



### A strong baseline for image and video quality assessment
- **Arxiv ID**: http://arxiv.org/abs/2111.07104v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.07104v1)
- **Published**: 2021-11-13 12:24:08+00:00
- **Updated**: 2021-11-13 12:24:08+00:00
- **Authors**: Shaoguo Wen, Junle Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a simple yet effective unified model for perceptual quality assessment of image and video. In contrast to existing models which usually consist of complex network architecture, or rely on the concatenation of multiple branches of features, our model achieves a comparable performance by applying only one global feature derived from a backbone network (i.e. resnet18 in the presented work). Combined with some training tricks, the proposed model surpasses the current baselines of SOTA models on public and private datasets. Based on the architecture proposed, we release the models well trained for three common real-world scenarios: UGC videos in the wild, PGC videos with compression, Game videos with compression. These three pre-trained models can be directly applied for quality assessment, or be further fine-tuned for more customized usages. All the code, SDK, and the pre-trained weights of the proposed models are publicly available at https://github.com/Tencent/CenseoQoE.



### Learning Object-Centric Representations of Multi-Object Scenes from Multiple Views
- **Arxiv ID**: http://arxiv.org/abs/2111.07117v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.07117v1)
- **Published**: 2021-11-13 13:54:28+00:00
- **Updated**: 2021-11-13 13:54:28+00:00
- **Authors**: Li Nanbo, Cian Eastwood, Robert B. Fisher
- **Comment**: Accepted at NeurIPS 2020 (Spotlight)
- **Journal**: None
- **Summary**: Learning object-centric representations of multi-object scenes is a promising approach towards machine intelligence, facilitating high-level reasoning and control from visual sensory data. However, current approaches for unsupervised object-centric scene representation are incapable of aggregating information from multiple observations of a scene. As a result, these "single-view" methods form their representations of a 3D scene based only on a single 2D observation (view). Naturally, this leads to several inaccuracies, with these methods falling victim to single-view spatial ambiguities. To address this, we propose The Multi-View and Multi-Object Network (MulMON) -- a method for learning accurate, object-centric representations of multi-object scenes by leveraging multiple views. In order to sidestep the main technical difficulty of the multi-object-multi-view scenario -- maintaining object correspondences across views -- MulMON iteratively updates the latent object representations for a scene over multiple views. To ensure that these iterative updates do indeed aggregate spatial information to form a complete 3D scene understanding, MulMON is asked to predict the appearance of the scene from novel viewpoints during training. Through experiments, we show that MulMON better-resolves spatial ambiguities than single-view methods -- learning more accurate and disentangled object representations -- and also achieves new functionality in predicting object segmentations for novel viewpoints.



### Bag of Tricks and A Strong baseline for Image Copy Detection
- **Arxiv ID**: http://arxiv.org/abs/2111.08004v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08004v2)
- **Published**: 2021-11-13 13:58:43+00:00
- **Updated**: 2021-12-04 17:09:34+00:00
- **Authors**: Wenhao Wang, Weipu Zhang, Yifan Sun, Yi Yang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2111.07090
- **Journal**: None
- **Summary**: Image copy detection is of great importance in real-life social media. In this paper, a bag of tricks and a strong baseline are proposed for image copy detection. Unsupervised pre-training substitutes the commonly-used supervised one. Beyond that, we design a descriptor stretching strategy to stabilize the scores of different queries. Experiments demonstrate that the proposed method is effective. The proposed baseline ranks third out of 526 participants on the Facebook AI Image Similarity Challenge: Descriptor Track. The code and trained models are available at https://github.com/WangWenhao0716/ISC-Track2-Submission.



### Visual Understanding of Complex Table Structures from Document Images
- **Arxiv ID**: http://arxiv.org/abs/2111.07129v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.07129v1)
- **Published**: 2021-11-13 14:54:33+00:00
- **Updated**: 2021-11-13 14:54:33+00:00
- **Authors**: Sachin Raja, Ajoy Mondal, C V Jawahar
- **Comment**: None
- **Journal**: None
- **Summary**: Table structure recognition is necessary for a comprehensive understanding of documents. Tables in unstructured business documents are tough to parse due to the high diversity of layouts, varying alignments of contents, and the presence of empty cells. The problem is particularly difficult because of challenges in identifying individual cells using visual or linguistic contexts or both. Accurate detection of table cells (including empty cells) simplifies structure extraction and hence, it becomes the prime focus of our work. We propose a novel object-detection-based deep model that captures the inherent alignments of cells within tables and is fine-tuned for fast optimization. Despite accurate detection of cells, recognizing structures for dense tables may still be challenging because of difficulties in capturing long-range row/column dependencies in presence of multi-row/column spanning cells. Therefore, we also aim to improve structure recognition by deducing a novel rectilinear graph-based formulation. From a semantics perspective, we highlight the significance of empty cells in a table. To take these cells into account, we suggest an enhancement to a popular evaluation criterion. Finally, we introduce a modestly sized evaluation dataset with an annotation style inspired by human cognition to encourage new approaches to the problem. Our framework improves the previous state-of-the-art performance by a 2.7% average F1-score on benchmark datasets.



### Full-attention based Neural Architecture Search using Context Auto-regression
- **Arxiv ID**: http://arxiv.org/abs/2111.07139v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.07139v1)
- **Published**: 2021-11-13 16:07:37+00:00
- **Updated**: 2021-11-13 16:07:37+00:00
- **Authors**: Yuan Zhou, Haiyang Wang, Shuwei Huo, Boyu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Self-attention architectures have emerged as a recent advancement for improving the performance of vision tasks. Manual determination of the architecture for self-attention networks relies on the experience of experts and cannot automatically adapt to various scenarios. Meanwhile, neural architecture search (NAS) has significantly advanced the automatic design of neural architectures. Thus, it is appropriate to consider using NAS methods to discover a better self-attention architecture automatically. However, it is challenging to directly use existing NAS methods to search attention networks because of the uniform cell-based search space and the lack of long-term content dependencies. To address this issue, we propose a full-attention based NAS method. More specifically, a stage-wise search space is constructed that allows various attention operations to be adopted for different layers of a network. To extract global features, a self-supervised search algorithm is proposed that uses context auto-regression to discover the full-attention architecture. To verify the efficacy of the proposed methods, we conducted extensive experiments on various learning tasks, including image classification, fine-grained image recognition, and zero-shot image retrieval. The empirical results show strong evidence that our method is capable of discovering high-performance, full-attention architectures while guaranteeing the required search efficiency.



### New Performance Measures for Object Tracking under Complex Environments
- **Arxiv ID**: http://arxiv.org/abs/2111.07145v1
- **DOI**: 10.1007/s00530-021-00775-9.pdf
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.07145v1)
- **Published**: 2021-11-13 16:32:42+00:00
- **Updated**: 2021-11-13 16:32:42+00:00
- **Authors**: Ajoy Mondal
- **Comment**: None
- **Journal**: None
- **Summary**: Various performance measures based on the ground truth and without ground truth exist to evaluate the quality of a developed tracking algorithm. The existing popular measures - average center location error (ACLE) and average tracking accuracy (ATA) based on ground truth, may sometimes create confusion to quantify the quality of a developed algorithm for tracking an object under some complex environments (e.g., scaled or oriented or both scaled and oriented object). In this article, we propose three new auxiliary performance measures based on ground truth information to evaluate the quality of a developed tracking algorithm under such complex environments. Moreover, one performance measure is developed by combining both two existing measures ACLE and ATA and three new proposed measures for better quantifying the developed tracking algorithm under such complex conditions. Some examples and experimental results conclude that the proposed measure is better than existing measures to quantify one developed algorithm for tracking objects under such complex environments.



### Developing a Novel Approach for Periapical Dental Radiographs Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.07156v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07156v2)
- **Published**: 2021-11-13 17:25:35+00:00
- **Updated**: 2021-12-22 17:13:16+00:00
- **Authors**: Elaheh Hatamimajoumerd, Farshad Tajeripour
- **Comment**: Accepted in 2013 5th Conference on Information and Knowledge
  Technology (IKT) https://www.aconf.org/conf_32261.html
- **Journal**: None
- **Summary**: Image processing techniques has been widely used in dental researches such as human identification and forensic dentistry, teeth numbering, dental carries detection and periodontal disease analysis. One of the most challenging parts in dental imaging is teeth segmentation and how to separate them from each other. In this paper, an automated method for teeth segmentation of Periapical dental x-ray images which contain at least one root-canalled tooth is proposed. The result of this approach can be used as an initial step in bone lesion detection. The proposed algorithm is made of two stages. The first stage is pre-processing. The second and main part of this algorithm calculated rotation degree and uses the integral projection method for tooth isolation. Experimental results show that this algorithm is robust and achieves high accuracy.



### Where to Look: A Unified Attention Model for Visual Recognition with Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.07169v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T01, I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2111.07169v1)
- **Published**: 2021-11-13 18:44:50+00:00
- **Updated**: 2021-11-13 18:44:50+00:00
- **Authors**: Gang Chen
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: The idea of using the recurrent neural network for visual attention has gained popularity in computer vision community. Although the recurrent attention model (RAM) leverages the glimpses with more large patch size to increasing its scope, it may result in high variance and instability. For example, we need the Gaussian policy with high variance to explore object of interests in a large image, which may cause randomized search and unstable learning. In this paper, we propose to unify the top-down and bottom-up attention together for recurrent visual attention. Our model exploits the image pyramids and Q-learning to select regions of interests in the top-down attention mechanism, which in turn to guide the policy search in the bottom-up approach. In addition, we add another two constraints over the bottom-up recurrent neural networks for better exploration. We train our model in an end-to-end reinforcement learning framework, and evaluate our method on visual classification tasks. The experimental results outperform convolutional neural networks (CNNs) baseline and the bottom-up recurrent attention models on visual classification tasks.



### PhysXNet: A Customizable Approach for LearningCloth Dynamics on Dressed People
- **Arxiv ID**: http://arxiv.org/abs/2111.07195v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07195v1)
- **Published**: 2021-11-13 21:05:41+00:00
- **Updated**: 2021-11-13 21:05:41+00:00
- **Authors**: Jordi Sanchez-Riera, Albert Pumarola, Francesc Moreno-Noguer
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce PhysXNet, a learning-based approach to predict the dynamics of deformable clothes given 3D skeleton motion sequences of humans wearing these clothes. The proposed model is adaptable to a large variety of garments and changing topologies, without need of being retrained. Such simulations are typically carried out by physics engines that require manual human expertise and are subjectto computationally intensive computations. PhysXNet, by contrast, is a fully differentiable deep network that at inference is able to estimate the geometry of dense cloth meshes in a matter of milliseconds, and thus, can be readily deployed as a layer of a larger deep learning architecture. This efficiency is achieved thanks to the specific parameterization of the clothes we consider, based on 3D UV maps encoding spatial garment displacements. The problem is then formulated as a mapping between the human kinematics space (represented also by 3D UV maps of the undressed body mesh) into the clothes displacement UV maps, which we learn using a conditional GAN with a discriminator that enforces feasible deformations. We train simultaneously our model for three garment templates, tops, bottoms and dresses for which we simulate deformations under 50 different human actions. Nevertheless, the UV map representation we consider allows encapsulating many different cloth topologies, and at test we can simulate garments even if we did not specifically train for them. A thorough evaluation demonstrates that PhysXNet delivers cloth deformations very close to those computed with the physical engine, opening the door to be effectively integrated within deeplearning pipelines.



