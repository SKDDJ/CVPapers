# Arxiv Papers in cs.CV on 2021-11-21
### Self-Supervised Point Cloud Completion via Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2111.10701v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.10701v1)
- **Published**: 2021-11-21 00:03:11+00:00
- **Updated**: 2021-11-21 00:03:11+00:00
- **Authors**: Himangi Mittal, Brian Okorn, Arpit Jangid, David Held
- **Comment**: BMVC 2021 (Oral)
- **Journal**: None
- **Summary**: When navigating in urban environments, many of the objects that need to be tracked and avoided are heavily occluded. Planning and tracking using these partial scans can be challenging. The aim of this work is to learn to complete these partial point clouds, giving us a full understanding of the object's geometry using only partial observations. Previous methods achieve this with the help of complete, ground-truth annotations of the target objects, which are available only for simulated datasets. However, such ground truth is unavailable for real-world LiDAR data. In this work, we present a self-supervised point cloud completion algorithm, PointPnCNet, which is trained only on partial scans without assuming access to complete, ground-truth annotations. Our method achieves this via inpainting. We remove a portion of the input data and train the network to complete the missing region. As it is difficult to determine which regions were occluded in the initial cloud and which were synthetically removed, our network learns to complete the full cloud, including the missing regions in the initial partial cloud. We show that our method outperforms previous unsupervised and weakly-supervised methods on both the synthetic dataset, ShapeNet, and real-world LiDAR dataset, Semantic KITTI.



### Deep Probability Estimation
- **Arxiv ID**: http://arxiv.org/abs/2111.10734v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2111.10734v4)
- **Published**: 2021-11-21 03:55:50+00:00
- **Updated**: 2022-10-11 19:59:57+00:00
- **Authors**: Sheng Liu, Aakash Kaku, Weicheng Zhu, Matan Leibovich, Sreyas Mohan, Boyang Yu, Haoxiang Huang, Laure Zanna, Narges Razavian, Jonathan Niles-Weed, Carlos Fernandez-Granda
- **Comment**: SL, AK, WZ, ML, SM contributed equally to this work; 36 pages, 17
  figures, 12 tables
- **Journal**: Proceedings of the 39th International Conference on Machine
  Learning, PMLR 162:13746-13781, 2022
- **Summary**: Reliable probability estimation is of crucial importance in many real-world applications where there is inherent (aleatoric) uncertainty. Probability-estimation models are trained on observed outcomes (e.g. whether it has rained or not, or whether a patient has died or not), because the ground-truth probabilities of the events of interest are typically unknown. The problem is therefore analogous to binary classification, with the difference that the objective is to estimate probabilities rather than predicting the specific outcome. This work investigates probability estimation from high-dimensional data using deep neural networks. There exist several methods to improve the probabilities generated by these models but they mostly focus on model (epistemic) uncertainty. For problems with inherent uncertainty, it is challenging to evaluate performance without access to ground-truth probabilities. To address this, we build a synthetic dataset to study and compare different computable metrics. We evaluate existing methods on the synthetic data as well as on three real-world probability estimation tasks, all of which involve inherent uncertainty: precipitation forecasting from radar images, predicting cancer patient survival from histopathology images, and predicting car crashes from dashcam videos. We also give a theoretical analysis of a model for high-dimensional probability estimation which reproduces several of the phenomena evinced in our experiments. Finally, we propose a new method for probability estimation using neural networks, which modifies the training process to promote output probabilities that are consistent with empirical probabilities computed from the data. The method outperforms existing approaches on most metrics on the simulated as well as real-world data.



### 3D Visual Tracking Framework with Deep Learning for Asteroid Exploration
- **Arxiv ID**: http://arxiv.org/abs/2111.10737v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.10737v1)
- **Published**: 2021-11-21 04:14:45+00:00
- **Updated**: 2021-11-21 04:14:45+00:00
- **Authors**: Dong Zhou, Gunaghui Sun, Xiaopeng Hong
- **Comment**: None
- **Journal**: None
- **Summary**: 3D visual tracking is significant to deep space exploration programs, which can guarantee spacecraft to flexibly approach the target. In this paper, we focus on the studied accurate and real-time method for 3D tracking. Considering the fact that there are almost no public dataset for this topic, A new large-scale 3D asteroid tracking dataset is presented, including binocular video sequences, depth maps, and point clouds of diverse asteroids with various shapes and textures. Benefitting from the power and convenience of simulation platform, all the 2D and 3D annotations are automatically generated. Meanwhile, we propose a deep-learning based 3D tracking framework, named as Track3D, which involves 2D monocular tracker and a novel light-weight amodal axis-aligned bounding-box network, A3BoxNet. The evaluation results demonstrate that Track3D achieves state-of-the-art 3D tracking performance in both accuracy and precision, comparing to a baseline algorithm. Moreover, our framework has great generalization ability to 2D monocular tracking performance.



### MaIL: A Unified Mask-Image-Language Trimodal Network for Referring Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.10747v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.10747v2)
- **Published**: 2021-11-21 05:54:17+00:00
- **Updated**: 2021-11-25 03:43:26+00:00
- **Authors**: Zizhang Li, Mengmeng Wang, Jianbiao Mei, Yong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Referring image segmentation is a typical multi-modal task, which aims at generating a binary mask for referent described in given language expressions. Prior arts adopt a bimodal solution, taking images and languages as two modalities within an encoder-fusion-decoder pipeline. However, this pipeline is sub-optimal for the target task for two reasons. First, they only fuse high-level features produced by uni-modal encoders separately, which hinders sufficient cross-modal learning. Second, the uni-modal encoders are pre-trained independently, which brings inconsistency between pre-trained uni-modal tasks and the target multi-modal task. Besides, this pipeline often ignores or makes little use of intuitively beneficial instance-level features. To relieve these problems, we propose MaIL, which is a more concise encoder-decoder pipeline with a Mask-Image-Language trimodal encoder. Specifically, MaIL unifies uni-modal feature extractors and their fusion model into a deep modality interaction encoder, facilitating sufficient feature interaction across different modalities. Meanwhile, MaIL directly avoids the second limitation since no uni-modal encoders are needed anymore. Moreover, for the first time, we propose to introduce instance masks as an additional modality, which explicitly intensifies instance-level features and promotes finer segmentation results. The proposed MaIL set a new state-of-the-art on all frequently-used referring image segmentation datasets, including RefCOCO, RefCOCO+, and G-Ref, with significant gains, 3%-10% against previous best methods. Code will be released soon.



### Stochastic Variance Reduced Ensemble Adversarial Attack for Boosting the Adversarial Transferability
- **Arxiv ID**: http://arxiv.org/abs/2111.10752v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.10752v2)
- **Published**: 2021-11-21 06:33:27+00:00
- **Updated**: 2022-04-23 14:45:57+00:00
- **Authors**: Yifeng Xiong, Jiadong Lin, Min Zhang, John E. Hopcroft, Kun He
- **Comment**: 11 pages, 6 figures, accepted by CVPR 2022
- **Journal**: None
- **Summary**: The black-box adversarial attack has attracted impressive attention for its practical use in the field of deep learning security. Meanwhile, it is very challenging as there is no access to the network architecture or internal weights of the target model. Based on the hypothesis that if an example remains adversarial for multiple models, then it is more likely to transfer the attack capability to other models, the ensemble-based adversarial attack methods are efficient and widely used for black-box attacks. However, ways of ensemble attack are rather less investigated, and existing ensemble attacks simply fuse the outputs of all the models evenly. In this work, we treat the iterative ensemble attack as a stochastic gradient descent optimization process, in which the variance of the gradients on different models may lead to poor local optima. To this end, we propose a novel attack method called the stochastic variance reduced ensemble (SVRE) attack, which could reduce the gradient variance of the ensemble models and take full advantage of the ensemble attack. Empirical results on the standard ImageNet dataset demonstrate that the proposed method could boost the adversarial transferability and outperforms existing ensemble attacks significantly. Code is available at https://github.com/JHL-HUST/SVRE.



### Theoretical Foundations for Pseudo-Inversion of Nonlinear Operators
- **Arxiv ID**: http://arxiv.org/abs/2111.10755v2
- **DOI**: 10.1007/978-3-031-31975-4_3
- **Categories**: **math.ST**, cs.CV, cs.LG, eess.SP, math.FA, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/2111.10755v2)
- **Published**: 2021-11-21 07:15:37+00:00
- **Updated**: 2023-05-17 06:15:40+00:00
- **Authors**: Eyal Gofer, Guy Gilboa
- **Comment**: None
- **Journal**: L. Calatroni et al. (Eds.): SSVM 2023, LNCS 14009, pp. 29--41,
  2023
- **Summary**: The Moore-Penrose inverse is widely used in physics, statistics, and various fields of engineering. It captures well the notion of inversion of linear operators in the case of overcomplete data. In data science, nonlinear operators are extensively used. In this paper we characterize the fundamental properties of a pseudo-inverse (PI) for nonlinear operators.   The concept is defined broadly. First for general sets, and then a refinement for normed spaces. The PI for normed spaces yields the Moore-Penrose inverse when the operator is a matrix. We present conditions for existence and uniqueness of a PI and establish theoretical results investigating its properties, such as continuity, its value for operator compositions and projection operators, and others. Analytic expressions are given for the PI of some well-known, non-invertible, nonlinear operators, such as hard- or soft-thresholding and ReLU. Finally, we analyze a neural layer and discuss relations to wavelet thresholding.



### TraVLR: Now You See It, Now You Don't! A Bimodal Dataset for Evaluating Visio-Linguistic Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2111.10756v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.10756v3)
- **Published**: 2021-11-21 07:22:44+00:00
- **Updated**: 2023-04-15 09:48:44+00:00
- **Authors**: Keng Ji Chow, Samson Tan, Min-Yen Kan
- **Comment**: The first two authors contributed equally
- **Journal**: None
- **Summary**: Numerous visio-linguistic (V+L) representation learning methods have been developed, yet existing datasets do not adequately evaluate the extent to which they represent visual and linguistic concepts in a unified space. We propose several novel evaluation settings for V+L models, including cross-modal transfer. Furthermore, existing V+L benchmarks often report global accuracy scores on the entire dataset, making it difficult to pinpoint the specific reasoning tasks that models fail and succeed at. We present TraVLR, a synthetic dataset comprising four V+L reasoning tasks. TraVLR's synthetic nature allows us to constrain its training and testing distributions along task-relevant dimensions, enabling the evaluation of out-of-distribution generalisation. Each example in TraVLR redundantly encodes the scene in two modalities, allowing either to be dropped or added during training or testing without losing relevant information. We compare the performance of four state-of-the-art V+L models, finding that while they perform well on test examples from the same modality, they all fail at cross-modal transfer and have limited success accommodating the addition or deletion of one modality. We release TraVLR as an open challenge for the research community.



### Adversarial Mask: Real-World Universal Adversarial Attack on Face Recognition Model
- **Arxiv ID**: http://arxiv.org/abs/2111.10759v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.10759v3)
- **Published**: 2021-11-21 08:13:21+00:00
- **Updated**: 2022-09-07 06:06:58+00:00
- **Authors**: Alon Zolfi, Shai Avidan, Yuval Elovici, Asaf Shabtai
- **Comment**: 16 pages, 9 figures
- **Journal**: None
- **Summary**: Deep learning-based facial recognition (FR) models have demonstrated state-of-the-art performance in the past few years, even when wearing protective medical face masks became commonplace during the COVID-19 pandemic. Given the outstanding performance of these models, the machine learning research community has shown increasing interest in challenging their robustness. Initially, researchers presented adversarial attacks in the digital domain, and later the attacks were transferred to the physical domain. However, in many cases, attacks in the physical domain are conspicuous, and thus may raise suspicion in real-world environments (e.g., airports). In this paper, we propose Adversarial Mask, a physical universal adversarial perturbation (UAP) against state-of-the-art FR models that is applied on face masks in the form of a carefully crafted pattern. In our experiments, we examined the transferability of our adversarial mask to a wide range of FR model architectures and datasets. In addition, we validated our adversarial mask's effectiveness in real-world experiments (CCTV use case) by printing the adversarial pattern on a fabric face mask. In these experiments, the FR system was only able to identify 3.34% of the participants wearing the mask (compared to a minimum of 83.34% with other evaluated masks). A demo of our experiments can be found at: https://youtu.be/_TXkDO5z11w.



### COVID-19 Detection through Deep Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/2111.10762v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.10762v1)
- **Published**: 2021-11-21 08:32:08+00:00
- **Updated**: 2021-11-21 08:32:08+00:00
- **Authors**: Jash Dalvi, Aziz Bohra
- **Comment**: None
- **Journal**: None
- **Summary**: The SARS-CoV2 virus has caused a lot of tribulation to the human population. Predictive modeling that can accurately determine whether a person is infected with COVID-19 is imperative. The study proposes a novel approach that utilizes deep feature extraction technique, pre-trained ResNet50 acting as the backbone of the network, combined with Logistic Regression as the head model. The proposed model has been trained on Kaggle COVID-19 Radiography Dataset. The proposed model achieves a cross-validation accuracy of 100% on the COVID-19 and Normal X-Ray image classes. Similarly, when tested on combined three classes, the proposed model achieves 98.84% accuracy.



### Decentralized Unsupervised Learning of Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/2111.10763v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.10763v2)
- **Published**: 2021-11-21 08:36:31+00:00
- **Updated**: 2022-04-22 20:16:12+00:00
- **Authors**: Yawen Wu, Zhepeng Wang, Dewen Zeng, Meng Li, Yiyu Shi, Jingtong Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Collaborative learning enables distributed clients to learn a shared model for prediction while keeping the training data local on each client. However, existing collaborative learning methods require fully-labeled data for training, which is inconvenient or sometimes infeasible to obtain due to the high labeling cost and the requirement of expertise. The lack of labels makes collaborative learning impractical in many realistic settings. Self-supervised learning can address this challenge by learning from unlabeled data. Contrastive learning (CL), a self-supervised learning approach, can effectively learn visual representations from unlabeled image data. However, the distributed data collected on clients are usually not independent and identically distributed (non-IID) among clients, and each client may only have few classes of data, which degrades the performance of CL and learned representations. To tackle this problem, we propose a collaborative contrastive learning framework consisting of two approaches: feature fusion and neighborhood matching, by which a unified feature space among clients is learned for better data representations. Feature fusion provides remote features as accurate contrastive information to each client for better local learning. Neighborhood matching further aligns each client's local features to the remote features such that well-clustered features among clients can be learned. Extensive experiments show the effectiveness of the proposed framework. It outperforms other methods by 11% on IID data and matches the performance of centralized learning.



### One-shot Weakly-Supervised Segmentation in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2111.10773v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.10773v1)
- **Published**: 2021-11-21 09:14:13+00:00
- **Updated**: 2021-11-21 09:14:13+00:00
- **Authors**: Wenhui Lei, Qi Su, Ran Gu, Na Wang, Xinglong Liu, Guotai Wang, Xiaofan Zhang, Shaoting Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks usually require accurate and a large number of annotations to achieve outstanding performance in medical image segmentation. One-shot segmentation and weakly-supervised learning are promising research directions that lower labeling effort by learning a new class from only one annotated image and utilizing coarse labels instead, respectively. Previous works usually fail to leverage the anatomical structure and suffer from class imbalance and low contrast problems. Hence, we present an innovative framework for 3D medical image segmentation with one-shot and weakly-supervised settings. Firstly a propagation-reconstruction network is proposed to project scribbles from annotated volume to unlabeled 3D images based on the assumption that anatomical patterns in different human bodies are similar. Then a dual-level feature denoising module is designed to refine the scribbles based on anatomical- and pixel-level features. After expanding the scribbles to pseudo masks, we could train a segmentation model for the new class with the noisy label training strategy. Experiments on one abdomen and one head-and-neck CT dataset show the proposed method obtains significant improvement over the state-of-the-art methods and performs robustly even under severe class imbalance and low contrast.



### FCOSR: A Simple Anchor-free Rotated Detector for Aerial Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2111.10780v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.10780v2)
- **Published**: 2021-11-21 09:49:13+00:00
- **Updated**: 2021-12-01 02:50:41+00:00
- **Authors**: Zhonghua Li, Biao Hou, Zitong Wu, Licheng Jiao, Bo Ren, Chen Yang
- **Comment**: 10 pages, 6 tables, 7 figures
- **Journal**: None
- **Summary**: Existing anchor-base oriented object detection methods have achieved amazing results, but these methods require some manual preset boxes, which introduces additional hyperparameters and calculations. The existing anchor-free methods usually have complex architectures and are not easy to deploy. Our goal is to propose an algorithm which is simple and easy-to-deploy for aerial image detection. In this paper, we present a one-stage anchor-free rotated object detector (FCOSR) based on FCOS, which can be deployed on most platforms. The FCOSR has a simple architecture consisting of only convolution layers. Our work focuses on the label assignment strategy for the training phase. We use ellipse center sampling method to define a suitable sampling region for oriented bounding box (OBB). The fuzzy sample assignment strategy provides reasonable labels for overlapping objects. To solve the insufficient sampling problem, a multi-level sampling module is designed. These strategies allocate more appropriate labels to training samples. Our algorithm achieves 79.25, 75.41, and 90.15 mAP on DOTA1.0, DOTA1.5, and HRSC2016 datasets, respectively. FCOSR demonstrates superior performance to other methods in single-scale evaluation. We convert a lightweight FCOSR model to TensorRT format, which achieves 73.93 mAP on DOTA1.0 at a speed of 10.68 FPS on Jetson Xavier NX with single scale. The code is available at: https://github.com/lzh420202/FCOSR



### DuDoTrans: Dual-Domain Transformer Provides More Attention for Sinogram Restoration in Sparse-View CT Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2111.10790v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.10790v2)
- **Published**: 2021-11-21 10:41:07+00:00
- **Updated**: 2021-11-25 07:30:21+00:00
- **Authors**: Ce Wang, Kun Shang, Haimiao Zhang, Qian Li, Yuan Hui, S. Kevin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: While Computed Tomography (CT) reconstruction from X-ray sinograms is necessary for clinical diagnosis, iodine radiation in the imaging process induces irreversible injury, thereby driving researchers to study sparse-view CT reconstruction, that is, recovering a high-quality CT image from a sparse set of sinogram views. Iterative models are proposed to alleviate the appeared artifacts in sparse-view CT images, but the computation cost is too expensive. Then deep-learning-based methods have gained prevalence due to the excellent performances and lower computation. However, these methods ignore the mismatch between the CNN's \textbf{local} feature extraction capability and the sinogram's \textbf{global} characteristics. To overcome the problem, we propose \textbf{Du}al-\textbf{Do}main \textbf{Trans}former (\textbf{DuDoTrans}) to simultaneously restore informative sinograms via the long-range dependency modeling capability of Transformer and reconstruct CT image with both the enhanced and raw sinograms. With such a novel design, reconstruction performance on the NIH-AAPM dataset and COVID-19 dataset experimentally confirms the effectiveness and generalizability of DuDoTrans with fewer involved parameters. Extensive experiments also demonstrate its robustness with different noise-level scenarios for sparse-view CT reconstruction. The code and models are publicly available at https://github.com/DuDoTrans/CODE



### HoughCL: Finding Better Positive Pairs in Dense Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.10794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.10794v1)
- **Published**: 2021-11-21 11:23:12+00:00
- **Updated**: 2021-11-21 11:23:12+00:00
- **Authors**: Yunsung Lee, Teakgyu Hong, Han-Cheol Cho, Junbum Cha, Seungryong Kim
- **Comment**: Accepted to ICML 2021 Workshop: Self-Supervised Learning for
  Reasoning and Perception
- **Journal**: None
- **Summary**: Recently, self-supervised methods show remarkable achievements in image-level representation learning. Nevertheless, their image-level self-supervisions lead the learned representation to sub-optimal for dense prediction tasks, such as object detection, instance segmentation, etc. To tackle this issue, several recent self-supervised learning methods have extended image-level single embedding to pixel-level dense embeddings. Unlike image-level representation learning, due to the spatial deformation of augmentation, it is difficult to sample pixel-level positive pairs. Previous studies have sampled pixel-level positive pairs using the winner-takes-all among similarity or thresholding warped distance between dense embeddings. However, these naive methods can be struggled by background clutter and outliers problems. In this paper, we introduce Hough Contrastive Learning (HoughCL), a Hough space based method that enforces geometric consistency between two dense features. HoughCL achieves robustness against background clutter and outliers. Furthermore, compared to baseline, our dense positive pairing method has no additional learnable parameters and has a small extra computation cost. Compared to previous works, our method shows better or comparable performance on dense prediction fine-tuning tasks.



### FreqNet: A Frequency-domain Image Super-Resolution Network with Dicrete Cosine Transform
- **Arxiv ID**: http://arxiv.org/abs/2111.10800v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.10800v1)
- **Published**: 2021-11-21 11:49:12+00:00
- **Updated**: 2021-11-21 11:49:12+00:00
- **Authors**: Runyuan Cai, Yue Ding, Hongtao Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Single image super-resolution(SISR) is an ill-posed problem that aims to obtain high-resolution (HR) output from low-resolution (LR) input, during which extra high-frequency information is supposed to be added to improve the perceptual quality. Existing SISR works mainly operate in the spatial domain by minimizing the mean squared reconstruction error. Despite the high peak signal-to-noise ratios(PSNR) results, it is difficult to determine whether the model correctly adds desired high-frequency details. Some residual-based structures are proposed to guide the model to focus on high-frequency features implicitly. However, how to verify the fidelity of those artificial details remains a problem since the interpretation from spatial-domain metrics is limited. In this paper, we propose FreqNet, an intuitive pipeline from the frequency domain perspective, to solve this problem. Inspired by existing frequency-domain works, we convert images into discrete cosine transform (DCT) blocks, then reform them to obtain the DCT feature maps, which serve as the input and target of our model. A specialized pipeline is designed, and we further propose a frequency loss function to fit the nature of our frequency-domain task. Our SISR method in the frequency domain can learn the high-frequency information explicitly, provide fidelity and good perceptual quality for the SR images. We further observe that our model can be merged with other spatial super-resolution models to enhance the quality of their original SR output.



### Structure-Preserving Graph Kernel for Brain Network Classification
- **Arxiv ID**: http://arxiv.org/abs/2111.10803v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.10803v2)
- **Published**: 2021-11-21 12:03:19+00:00
- **Updated**: 2022-02-21 22:02:41+00:00
- **Authors**: Jun Yu, Zhaoming Kong, Aditya Kendre, Hao Peng, Carl Yang, Lichao Sun, Alex Leow, Lifang He
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel graph-based kernel learning approach for connectome analysis. Specifically, we demonstrate how to leverage the naturally available structure within the graph representation to encode prior knowledge in the kernel. We first proposed a matrix factorization to directly extract structural features from natural symmetric graph representations of connectome data. We then used them to derive a structure-persevering graph kernel to be fed into the support vector machine. The proposed approach has the advantage of being clinically interpretable. Quantitative evaluations on challenging HIV disease classification (DTI- and fMRI-derived connectome data) and emotion recognition (EEG-derived connectome data) tasks demonstrate the superior performance of our proposed methods against the state-of-the-art. Results showed that relevant EEG-connectome information is primarily encoded in the alpha band during the emotion regulation task.



### Customizing an Affective Tutoring System Based on Facial Expression and Head Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2111.14262v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.14262v1)
- **Published**: 2021-11-21 13:06:56+00:00
- **Updated**: 2021-11-21 13:06:56+00:00
- **Authors**: Mahdi Pourmirzaei, Gholam Ali Montazer, Ebrahim Mousavi
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the main problem in e-learning has shifted from analyzing content to personalization of learning environment by Intelligence Tutoring Systems (ITSs). Therefore, by designing personalized teaching models, learners are able to have a successful and satisfying experience in achieving their learning goals. Affective Tutoring Systems (ATSs) are some kinds of ITS that can recognize and respond to affective states of learner. In this study, we designed, implemented, and evaluated a system to personalize the learning environment based on the facial emotions recognition, head pose estimation, and cognitive style of learners. First, a unit called Intelligent Analyzer (AI) created which was responsible for recognizing facial expression and head angles of learners. Next, the ATS was built which mainly made of two units: ITS, IA. Results indicated that with the ATS, participants needed less efforts to pass the tests. In other words, we observed when the IA unit was activated, learners could pass the final tests in fewer attempts than those for whom the IA unit was deactivated. Additionally, they showed an improvement in terms of the mean passing score and academic satisfaction.



### Understanding Pixel-level 2D Image Semantics with 3D Keypoint Knowledge Engine
- **Arxiv ID**: http://arxiv.org/abs/2111.10817v1
- **DOI**: 10.1109/TPAMI.2021.3072659
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.10817v1)
- **Published**: 2021-11-21 13:25:20+00:00
- **Updated**: 2021-11-21 13:25:20+00:00
- **Authors**: Yang You, Chengkun Li, Yujing Lou, Zhoujun Cheng, Liangwei Li, Lizhuang Ma, Weiming Wang, Cewu Lu
- **Comment**: Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence; To appear in upcoming issues
- **Journal**: None
- **Summary**: Pixel-level 2D object semantic understanding is an important topic in computer vision and could help machine deeply understand objects (e.g. functionality and affordance) in our daily life. However, most previous methods directly train on correspondences in 2D images, which is end-to-end but loses plenty of information in 3D spaces. In this paper, we propose a new method on predicting image corresponding semantics in 3D domain and then projecting them back onto 2D images to achieve pixel-level understanding. In order to obtain reliable 3D semantic labels that are absent in current image datasets, we build a large scale keypoint knowledge engine called KeypointNet, which contains 103,450 keypoints and 8,234 3D models from 16 object categories. Our method leverages the advantages in 3D vision and can explicitly reason about objects self-occlusion and visibility. We show that our method gives comparative and even superior results on standard semantic benchmarks.



### Domain Generalization for Mammography Detection via Multi-style and Multi-view Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.10827v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.10827v1)
- **Published**: 2021-11-21 14:29:50+00:00
- **Updated**: 2021-11-21 14:29:50+00:00
- **Authors**: Zheren Li, Zhiming Cui, Sheng Wang, Yuji Qi, Xi Ouyang, Qitian Chen, Yuezhi Yang, Zhong Xue, Dinggang Shen, Jie-Zhi Cheng
- **Comment**: Pages 98-108
- **Journal**: International Conference on Medical Image Computing and
  Computer-Assisted Intervention 2021
- **Summary**: Lesion detection is a fundamental problem in the computer-aided diagnosis scheme for mammography. The advance of deep learning techniques have made a remarkable progress for this task, provided that the training data are large and sufficiently diverse in terms of image style and quality. In particular, the diversity of image style may be majorly attributed to the vendor factor. However, the collection of mammograms from vendors as many as possible is very expensive and sometimes impractical for laboratory-scale studies. Accordingly, to further augment the generalization capability of deep learning model to various vendors with limited resources, a new contrastive learning scheme is developed. Specifically, the backbone network is firstly trained with a multi-style and multi-view unsupervised self-learning scheme for the embedding of invariant features to various vendor-styles. Afterward, the backbone network is then recalibrated to the downstream task of lesion detection with the specific supervised learning. The proposed method is evaluated with mammograms from four vendors and one unseen public dataset. The experimental results suggest that our approach can effectively improve detection performance on both seen and unseen domains, and outperforms many state-of-the-art (SOTA) generalization methods.



### Denoised Internal Models: a Brain-Inspired Autoencoder against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2111.10844v4
- **DOI**: 10.1007/s11633-022-1375-7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.10844v4)
- **Published**: 2021-11-21 15:46:30+00:00
- **Updated**: 2023-03-05 09:05:46+00:00
- **Authors**: Kaiyuan Liu, Xingyu Li, Yurui Lai, Ge Zhang, Hang Su, Jiachen Wang, Chunxu Guo, Jisong Guan, Yi Zhou
- **Comment**: 16 pages, 3 figures
- **Journal**: Machine Intelligence Research, vol. 19, no. 5, pp.456-471, 2022
- **Summary**: Despite its great success, deep learning severely suffers from robustness; that is, deep neural networks are very vulnerable to adversarial attacks, even the simplest ones. Inspired by recent advances in brain science, we propose the Denoised Internal Models (DIM), a novel generative autoencoder-based model to tackle this challenge. Simulating the pipeline in the human brain for visual signal processing, DIM adopts a two-stage approach. In the first stage, DIM uses a denoiser to reduce the noise and the dimensions of inputs, reflecting the information pre-processing in the thalamus. Inspired from the sparse coding of memory-related traces in the primary visual cortex, the second stage produces a set of internal models, one for each category. We evaluate DIM over 42 adversarial attacks, showing that DIM effectively defenses against all the attacks and outperforms the SOTA on the overall robustness.



### XnODR and XnIDR: Two Accurate and Fast Fully Connected Layers For Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2111.10854v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2111.10854v2)
- **Published**: 2021-11-21 16:42:01+00:00
- **Updated**: 2022-06-13 01:35:46+00:00
- **Authors**: Jian Sun, Ali Pourramezan Fard, Mohammad H. Mahoor
- **Comment**: 17 pages, 9 figures, 10 tables
- **Journal**: None
- **Summary**: Although Capsule Network is powerful at defining the positional relationship between features in deep neural networks for visual recognition tasks, it is computationally expensive and not suitable for running on mobile devices. The bottleneck is in the computational complexity of the Dynamic Routing mechanism used between the capsules. On the other hand, XNOR-Net is fast and computationally efficient, though it suffers from low accuracy due to information loss in the binarization process. To address the computational burdens of the Dynamic Routing mechanism, this paper proposes new Fully Connected (FC) layers by xnorizing the linear projector outside or inside the Dynamic Routing within the CapsFC layer. Specifically, our proposed FC layers have two versions, XnODR (Xnorize the Linear Projection Linear Projector Outside Dynamic Routing) and XnIDR (Xnorize the Linear Projection Linear Projector Inside Dynamic Routing). To test the generalization of both XnODR and XnIDR, we insert them into two different networks, MobileNet V2 and ResNet-50. Our experiments on three datasets, MNIST, CIFAR-10, and MultiMNIST validate their effectiveness. The results demonstrate that both XnODR and XnIDR help networks to have high accuracy with lower FLOPs and fewer parameters (e.g., 95.32\% correctness with 2.99M parameters and 312.04M FLOPs on CIFAR-10).



### CpT: Convolutional Point Transformer for 3D Point Cloud Processing
- **Arxiv ID**: http://arxiv.org/abs/2111.10866v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.10866v1)
- **Published**: 2021-11-21 17:45:55+00:00
- **Updated**: 2021-11-21 17:45:55+00:00
- **Authors**: Chaitanya Kaul, Joshua Mitton, Hang Dai, Roderick Murray-Smith
- **Comment**: None
- **Journal**: None
- **Summary**: We present CpT: Convolutional point Transformer - a novel deep learning architecture for dealing with the unstructured nature of 3D point cloud data. CpT is an improvement over existing attention-based Convolutions Neural Networks as well as previous 3D point cloud processing transformers. It achieves this feat due to its effectiveness in creating a novel and robust attention-based point set embedding through a convolutional projection layer crafted for processing dynamically local point set neighbourhoods. The resultant point set embedding is robust to the permutations of the input points. Our novel CpT block builds over local neighbourhoods of points obtained via a dynamic graph computation at each layer of the networks' structure. It is fully differentiable and can be stacked just like convolutional layers to learn global properties of the points. We evaluate our model on standard benchmark datasets such as ModelNet40, ShapeNet Part Segmentation, and the S3DIS 3D indoor scene semantic segmentation dataset to show that our model can serve as an effective backbone for various point cloud processing tasks when compared to the existing state-of-the-art approaches.



### Geometry-Aware Multi-Task Learning for Binaural Audio Generation from Video
- **Arxiv ID**: http://arxiv.org/abs/2111.10882v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2111.10882v1)
- **Published**: 2021-11-21 19:26:45+00:00
- **Updated**: 2021-11-21 19:26:45+00:00
- **Authors**: Rishabh Garg, Ruohan Gao, Kristen Grauman
- **Comment**: Published in BMVC 2021, project page:
  http://vision.cs.utexas.edu/projects/geometry-aware-binaural/
- **Journal**: None
- **Summary**: Binaural audio provides human listeners with an immersive spatial sound experience, but most existing videos lack binaural audio recordings. We propose an audio spatialization method that draws on visual information in videos to convert their monaural (single-channel) audio to binaural audio. Whereas existing approaches leverage visual features extracted directly from video frames, our approach explicitly disentangles the geometric cues present in the visual stream to guide the learning process. In particular, we develop a multi-task framework that learns geometry-aware features for binaural audio generation by accounting for the underlying room impulse response, the visual stream's coherence with the sound source(s) positions, and the consistency in geometry of the sounding objects over time. Furthermore, we introduce a new large video dataset with realistic binaural audio simulated for real-world scanned environments. On two datasets, we demonstrate the efficacy of our method, which achieves state-of-the-art results.



### Dynamic imaging using motion-compensated smoothness regularization on manifolds (MoCo-SToRM)
- **Arxiv ID**: http://arxiv.org/abs/2111.10887v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.10887v1)
- **Published**: 2021-11-21 19:52:01+00:00
- **Updated**: 2021-11-21 19:52:01+00:00
- **Authors**: Qing Zou, Luis A. Torres, Sean B. Fain, Mathews Jacob
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce an unsupervised deep manifold learning algorithm for motion-compensated dynamic MRI. We assume that the motion fields in a free-breathing lung MRI dataset live on a manifold. The motion field at each time instant is modeled as the output of a deep generative model, driven by low-dimensional time-varying latent vectors that capture the temporal variability. The images at each time instant are modeled as the deformed version of an image template using the above motion fields. The template, the parameters of the deep generator, and the latent vectors are learned from the k-t space data in an unsupervised fashion. The manifold motion model serves as a regularizer, making the joint estimation of the motion fields and images from few radial spokes/frame well-posed. The utility of the algorithm is demonstrated in the context of motion-compensated high-resolution lung MRI.



### Joint alignment and reconstruction of multislice dynamic MRI using variational manifold learning
- **Arxiv ID**: http://arxiv.org/abs/2111.10889v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.10889v1)
- **Published**: 2021-11-21 20:03:41+00:00
- **Updated**: 2021-11-21 20:03:41+00:00
- **Authors**: Qing Zou, Abdul Haseeb Ahmed, Prashant Nagpal, Sarv Priya, Rolf F Schulte, Mathews Jacob
- **Comment**: None
- **Journal**: None
- **Summary**: Free-breathing cardiac MRI schemes are emerging as competitive alternatives to breath-held cine MRI protocols, enabling applicability to pediatric and other population groups that cannot hold their breath. Because the data from the slices are acquired sequentially, the cardiac/respiratory motion patterns may be different for each slice; current free-breathing approaches perform independent recovery of each slice. In addition to not being able to exploit the inter-slice redundancies, manual intervention or sophisticated post-processing methods are needed to align the images post-recovery for quantification. To overcome these challenges, we propose an unsupervised variational deep manifold learning scheme for the joint alignment and reconstruction of multislice dynamic MRI. The proposed scheme jointly learns the parameters of the deep network as well as the latent vectors for each slice, which capture the motion-induced dynamic variations, from the k-t space data of the specific subject. The variational framework minimizes the non-uniqueness in the representation, thus offering improved alignment and reconstructions.



### Deep Image Prior using Stein's Unbiased Risk Estimator: SURE-DIP
- **Arxiv ID**: http://arxiv.org/abs/2111.10892v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.10892v1)
- **Published**: 2021-11-21 20:11:56+00:00
- **Updated**: 2021-11-21 20:11:56+00:00
- **Authors**: Maneesh John, Hemant Kumar Aggarwal, Qing Zou, Mathews Jacob
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning algorithms that rely on extensive training data are revolutionizing image recovery from ill-posed measurements. Training data is scarce in many imaging applications, including ultra-high-resolution imaging. The deep image prior (DIP) algorithm was introduced for single-shot image recovery, completely eliminating the need for training data. A challenge with this scheme is the need for early stopping to minimize the overfitting of the CNN parameters to the noise in the measurements. We introduce a generalized Stein's unbiased risk estimate (GSURE) loss metric to minimize the overfitting. Our experiments show that the SURE-DIP approach minimizes the overfitting issues, thus offering significantly improved performance over classical DIP schemes. We also use the SURE-DIP approach with model-based unrolling architectures, which offers improved performance over direct inversion schemes.



### Video Content Swapping Using GAN
- **Arxiv ID**: http://arxiv.org/abs/2111.10916v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.10916v1)
- **Published**: 2021-11-21 23:01:58+00:00
- **Updated**: 2021-11-21 23:01:58+00:00
- **Authors**: Tingfung Lau, Sailun Xu, Xinze Wang
- **Comment**: 9 pages, 12 figures
- **Journal**: None
- **Summary**: Video generation is an interesting problem in computer vision. It is quite popular for data augmentation, special effect in move, AR/VR and so on. With the advances of deep learning, many deep generative models have been proposed to solve this task. These deep generative models provide away to utilize all the unlabeled images and videos online, since it can learn deep feature representations with unsupervised manner. These models can also generate different kinds of images, which have great value for visual application. However generating a video would be much more challenging since we need to model not only the appearances of objects in the video but also their temporal motion. In this work, we will break down any frame in the video into content and pose. We first extract the pose information from a video using a pre-trained human pose detection and use a generative model to synthesize the video based on the content code and pose code.



### Deep Reinforced Attention Regression for Partial Sketch Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2111.10917v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.10917v1)
- **Published**: 2021-11-21 23:12:51+00:00
- **Updated**: 2021-11-21 23:12:51+00:00
- **Authors**: Dingrong Wang, Hitesh Sapkota, Xumin Liu, Qi Yu
- **Comment**: 2021 IEEE International Conference on Data Mining (ICDM)
- **Journal**: None
- **Summary**: Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) aims at finding a specific image from a large gallery given a query sketch. Despite the widespread applicability of FG-SBIR in many critical domains (e.g., crime activity tracking), existing approaches still suffer from a low accuracy while being sensitive to external noises such as unnecessary strokes in the sketch. The retrieval performance will further deteriorate under a more practical on-the-fly setting, where only a partially complete sketch with only a few (noisy) strokes are available to retrieve corresponding images. We propose a novel framework that leverages a uniquely designed deep reinforcement learning model that performs a dual-level exploration to deal with partial sketch training and attention region selection. By enforcing the model's attention on the important regions of the original sketches, it remains robust to unnecessary stroke noises and improve the retrieval accuracy by a large margin. To sufficiently explore partial sketches and locate the important regions to attend, the model performs bootstrapped policy gradient for global exploration while adjusting a standard deviation term that governs a locator network for local exploration. The training process is guided by a hybrid loss that integrates a reinforcement loss and a supervised loss. A dynamic ranking reward is developed to fit the on-the-fly image retrieval process using partial sketches. The extensive experimentation performed on three public datasets shows that our proposed approach achieves the state-of-the-art performance on partial sketch based image retrieval.



