# Arxiv Papers in cs.CV on 2021-11-04
### Building Damage Mapping with Self-PositiveUnlabeled Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.02586v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.02586v1)
- **Published**: 2021-11-04 02:04:37+00:00
- **Updated**: 2021-11-04 02:04:37+00:00
- **Authors**: Junshi Xia, Naoto Yokoya, Bruno Adriano
- **Comment**: 7 pages, 1 figure, Artificial Intelligence for Humanitarian
  Assistance and Disaster Response Workshop, NeurIPS 2021
- **Journal**: None
- **Summary**: Humanitarian organizations must have fast and reliable data to respond to disasters. Deep learning approaches are difficult to implement in real-world disasters because it might be challenging to collect ground truth data of the damage situation (training data) soon after the event. The implementation of recent self-paced positive-unlabeled learning (PU) is demonstrated in this work by successfully applying to building damage assessment with very limited labeled data and a large amount of unlabeled data. Self-PU learning is compared with the supervised baselines and traditional PU learning using different datasets collected from the 2011 Tohoku earthquake, the 2018 Palu tsunami, and the 2018 Hurricane Michael. By utilizing only a portion of labeled damaged samples, we show how models trained with self-PU techniques may achieve comparable performance as supervised learning.



### Qimera: Data-free Quantization with Synthetic Boundary Supporting Samples
- **Arxiv ID**: http://arxiv.org/abs/2111.02625v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.02625v1)
- **Published**: 2021-11-04 04:52:50+00:00
- **Updated**: 2021-11-04 04:52:50+00:00
- **Authors**: Kanghyun Choi, Deokki Hong, Noseong Park, Youngsok Kim, Jinho Lee
- **Comment**: Accepted to Neurips 2021
- **Journal**: None
- **Summary**: Model quantization is known as a promising method to compress deep neural networks, especially for inferences on lightweight mobile or edge devices. However, model quantization usually requires access to the original training data to maintain the accuracy of the full-precision models, which is often infeasible in real-world scenarios for security and privacy issues. A popular approach to perform quantization without access to the original data is to use synthetically generated samples, based on batch-normalization statistics or adversarial learning. However, the drawback of such approaches is that they primarily rely on random noise input to the generator to attain diversity of the synthetic samples. We find that this is often insufficient to capture the distribution of the original data, especially around the decision boundaries. To this end, we propose Qimera, a method that uses superposed latent embeddings to generate synthetic boundary supporting samples. For the superposed embeddings to better reflect the original distribution, we also propose using an additional disentanglement mapping layer and extracting information from the full-precision model. The experimental results show that Qimera achieves state-of-the-art performances for various settings on data-free quantization. Code is available at https://github.com/iamkanghyunchoi/qimera.



### Temporal Fusion Based Mutli-scale Semantic Segmentation for Detecting Concealed Baggage Threats
- **Arxiv ID**: http://arxiv.org/abs/2111.02651v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.02651v2)
- **Published**: 2021-11-04 06:19:52+00:00
- **Updated**: 2021-11-07 05:26:25+00:00
- **Authors**: Muhammed Shafay, Taimur Hassan, Ernesto Damiani, Naoufel Werghi
- **Comment**: Accepted in IEEE SMC 2021
- **Journal**: None
- **Summary**: Detection of illegal and threatening items in baggage is one of the utmost security concern nowadays. Even for experienced security personnel, manual detection is a time-consuming and stressful task. Many academics have created automated frameworks for detecting suspicious and contraband data from X-ray scans of luggage. However, to our knowledge, no framework exists that utilizes temporal baggage X-ray imagery to effectively screen highly concealed and occluded objects which are barely visible even to the naked eye. To address this, we present a novel temporal fusion driven multi-scale residual fashioned encoder-decoder that takes series of consecutive scans as input and fuses them to generate distinct feature representations of the suspicious and non-suspicious baggage content, leading towards a more accurate extraction of the contraband data. The proposed methodology has been thoroughly tested using the publicly accessible GDXray dataset, which is the only dataset containing temporally linked grayscale X-ray scans showcasing extremely concealed contraband data. The proposed framework outperforms its competitors on the GDXray dataset on various metrics.



### LVIS Challenge Track Technical Report 1st Place Solution: Distribution Balanced and Boundary Refinement for Large Vocabulary Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.02668v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.02668v2)
- **Published**: 2021-11-04 07:23:56+00:00
- **Updated**: 2021-11-05 02:08:07+00:00
- **Authors**: WeiFu Fu, CongChong Nie, Ting Sun, Jun Liu, TianLiang Zhang, Yong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: This report introduces the technical details of the team FuXi-Fresher for LVIS Challenge 2021. Our method focuses on the problem in following two aspects: the long-tail distribution and the segmentation quality of mask and boundary. Based on the advanced HTC instance segmentation algorithm, we connect transformer backbone(Swin-L) through composite connections inspired by CBNetv2 to enhance the baseline results. To alleviate the problem of long-tail distribution, we design a Distribution Balanced method which includes dataset balanced and loss function balaced modules. Further, we use a Mask and Boundary Refinement method composed with mask scoring and refine-mask algorithms to improve the segmentation quality. In addition, we are pleasantly surprised to find that early stopping combined with EMA method can achieve a great improvement. Finally, by using multi-scale testing and increasing the upper limit of the number of objects detected per image, we achieved more than 45.4% boundary AP on the val set of LVIS Challenge 2021. On the test data of LVIS Challenge 2021, we rank 1st and achieve 48.1% AP. Notably, our APr 47.5% is very closed to the APf 48.0%.



### A semi-automatic ultrasound image analysis system for the grading diagnosis of COVID-19 pneumonia
- **Arxiv ID**: http://arxiv.org/abs/2111.02676v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.02676v1)
- **Published**: 2021-11-04 08:04:50+00:00
- **Updated**: 2021-11-04 08:04:50+00:00
- **Authors**: Yuanyuan Wang, Yao Zhang, Qiong He, Hongen Liao, Jianwen Luo
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a semi-automatic system based on quantitative characterization of the specific image patterns in lung ultrasound (LUS) images, in order to assess the lung conditions of patients with COVID-19 pneumonia, as well as to differentiate between the severe / and no-severe cases. Specifically, four parameters are extracted from each LUS image, namely the thickness (TPL) and roughness (RPL) of the pleural line, and the accumulated with (AWBL) and acoustic coefficient (ACBL) of B lines. 27 patients are enrolled in this study, which are grouped into 13 moderate patients, 7 severe patients and 7 critical patients. Furthermore, the severe and critical patients are regarded as the severe cases, and the moderate patients are regarded as the non-severe cases. Biomarkers among different groups are compared. Each single biomarker and a classifier with all the biomarkers as input are utilized for the binary diagnosis of severe case and non-severe case, respectively. The classifier achieves the best classification performance among all the compared methods (area under the receiver operating characteristics curve = 0.93, sensitivity = 0.93, specificity = 0.85). The proposed image analysis system could be potentially applied to the grading and prognosis evaluation of patients with COVID-19 pneumonia.



### MixSiam: A Mixture-based Approach to Self-supervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.02679v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.02679v1)
- **Published**: 2021-11-04 08:12:47+00:00
- **Updated**: 2021-11-04 08:12:47+00:00
- **Authors**: Xiaoyang Guo, Tianhao Zhao, Yutian Lin, Bo Du
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Recently contrastive learning has shown significant progress in learning visual representations from unlabeled data. The core idea is training the backbone to be invariant to different augmentations of an instance. While most methods only maximize the feature similarity between two augmented data, we further generate more challenging training samples and force the model to keep predicting discriminative representation on these hard samples. In this paper, we propose MixSiam, a mixture-based approach upon the traditional siamese network. On the one hand, we input two augmented images of an instance to the backbone and obtain the discriminative representation by performing an element-wise maximum of two features. On the other hand, we take the mixture of these augmented images as input, and expect the model prediction to be close to the discriminative representation. In this way, the model could access more variant data samples of an instance and keep predicting invariant discriminative representations for them. Thus the learned model is more robust compared to previous contrastive learning methods. Extensive experiments on large-scale datasets show that MixSiam steadily improves the baseline and achieves competitive results with state-of-the-art methods. Our code will be released soon.



### TimeMatch: Unsupervised Cross-Region Adaptation by Temporal Shift Estimation
- **Arxiv ID**: http://arxiv.org/abs/2111.02682v3
- **DOI**: 10.1016/j.isprsjprs.2022.04.018
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.02682v3)
- **Published**: 2021-11-04 08:32:59+00:00
- **Updated**: 2022-05-09 07:12:00+00:00
- **Authors**: Joachim Nyborg, Charlotte Pelletier, Sébastien Lefèvre, Ira Assent
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, Volume 188,
  June 2022, Pages 301-313
- **Summary**: The recent developments of deep learning models that capture complex temporal patterns of crop phenology have greatly advanced crop classification from Satellite Image Time Series (SITS). However, when applied to target regions spatially different from the training region, these models perform poorly without any target labels due to the temporal shift of crop phenology between regions. Although various unsupervised domain adaptation techniques have been proposed in recent years, no method explicitly learns the temporal shift of SITS and thus provides only limited benefits for crop classification. To address this, we propose TimeMatch, which explicitly accounts for the temporal shift for improved SITS-based domain adaptation. In TimeMatch, we first estimate the temporal shift from the target to the source region using the predictions of a source-trained model. Then, we re-train the model for the target region by an iterative algorithm where the estimated shift is used to generate accurate target pseudo-labels. Additionally, we introduce an open-access dataset for cross-region adaptation from SITS in four different regions in Europe. On our dataset, we demonstrate that TimeMatch outperforms all competing methods by 11% in average F1-score across five different adaptation scenarios, setting a new state-of-the-art in cross-region adaptation.



### Towards Smart Monitored AM: Open Source in-Situ Layer-wise 3D Printing Image Anomaly Detection Using Histograms of Oriented Gradients and a Physics-Based Rendering Engine
- **Arxiv ID**: http://arxiv.org/abs/2111.02703v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2111.02703v1)
- **Published**: 2021-11-04 09:27:10+00:00
- **Updated**: 2021-11-04 09:27:10+00:00
- **Authors**: Aliaksei Petsiuk, Joshua M. Pearce
- **Comment**: None
- **Journal**: None
- **Summary**: This study presents an open source method for detecting 3D printing anomalies by comparing images of printed layers from a stationary monocular camera with G-code-based reference images of an ideal process generated with Blender, a physics rendering engine. Recognition of visual deviations was accomplished by analyzing the similarity of histograms of oriented gradients (HOG) of local image areas. The developed technique requires preliminary modeling of the working environment to achieve the best match for orientation, color rendering, lighting, and other parameters of the printed part. The output of the algorithm is a level of mismatch between printed and synthetic reference layers. Twelve similarity and distance measures were implemented and compared for their effectiveness at detecting 3D printing errors on six different representative failure types and their control error-free print images. The results show that although Kendall tau, Jaccard, and Sorensen similarities are the most sensitive, Pearson r, Spearman rho, cosine, and Dice similarities produce the more reliable results. This open source method allows the program to notice critical errors in the early stages of their occurrence and either pause manufacturing processes for further investigation by an operator or in the future AI-controlled automatic error correction. The implementation of this novel method does not require preliminary data for training, and the greatest efficiency can be achieved with the mass production of parts by either additive or subtractive manufacturing of the same geometric shape. It can be concluded this open source method is a promising means of enabling smart distributed recycling for additive manufacturing using complex feedstocks as well as other challenging manufacturing environments.



### PDBL: Improving Histopathological Tissue Classification with Plug-and-Play Pyramidal Deep-Broad Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.03063v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2111.03063v1)
- **Published**: 2021-11-04 09:35:12+00:00
- **Updated**: 2021-11-04 09:35:12+00:00
- **Authors**: Jiatai Lin, Guoqiang Han, Xipeng Pan, Hao Chen, Danyi Li, Xiping Jia, Zhenwei Shi, Zhizhen Wang, Yanfen Cui, Haiming Li, Changhong Liang, Li Liang, Zaiyi Liu, Chu Han
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Histopathological tissue classification is a fundamental task in pathomics cancer research. Precisely differentiating different tissue types is a benefit for the downstream researches, like cancer diagnosis, prognosis and etc. Existing works mostly leverage the popular classification backbones in computer vision to achieve histopathological tissue classification. In this paper, we proposed a super lightweight plug-and-play module, named Pyramidal Deep-Broad Learning (PDBL), for any well-trained classification backbone to further improve the classification performance without a re-training burden. We mimic how pathologists observe pathology slides in different magnifications and construct an image pyramid for the input image in order to obtain the pyramidal contextual information. For each level in the pyramid, we extract the multi-scale deep-broad features by our proposed Deep-Broad block (DB-block). We equipped PDBL in three popular classification backbones, ShuffLeNetV2, EfficientNetb0, and ResNet50 to evaluate the effectiveness and efficiency of our proposed module on two datasets (Kather Multiclass Dataset and the LC25000 Dataset). Experimental results demonstrate the proposed PDBL can steadily improve the tissue-level classification performance for any CNN backbones, especially for the lightweight models when given a small among of training samples (less than 10%), which greatly saves the computational time and annotation efforts.



### Towards dynamic multi-modal phenotyping using chest radiographs and physiological data
- **Arxiv ID**: http://arxiv.org/abs/2111.02710v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.02710v1)
- **Published**: 2021-11-04 09:41:00+00:00
- **Updated**: 2021-11-04 09:41:00+00:00
- **Authors**: Nasir Hayat, Krzysztof J. Geras, Farah E. Shamout
- **Comment**: Accepted in medical imaging meets NeurIPS 2021
- **Journal**: None
- **Summary**: The healthcare domain is characterized by heterogeneous data modalities, such as imaging and physiological data. In practice, the variety of medical data assists clinicians in decision-making. However, most of the current state-of-the-art deep learning models solely rely upon carefully curated data of a single modality. In this paper, we propose a dynamic training approach to learn modality-specific data representations and to integrate auxiliary features, instead of solely relying on a single modality. Our preliminary experiments results for a patient phenotyping task using physiological data in MIMIC-IV & chest radiographs in the MIMIC- CXR dataset show that our proposed approach achieves the highest area under the receiver operating characteristic curve (AUROC) (0.764 AUROC) compared to the performance of the benchmark method in previous work, which only used physiological data (0.740 AUROC). For a set of five recurring or chronic diseases with periodic acute episodes, including cardiac dysrhythmia, conduction disorders, and congestive heart failure, the AUROC improves from 0.747 to 0.798. This illustrates the benefit of leveraging the chest imaging modality in the phenotyping task and highlights the potential of multi-modal learning in medical applications.



### Facial Emotion Recognition using Deep Residual Networks in Real-World Environments
- **Arxiv ID**: http://arxiv.org/abs/2111.02717v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2111.02717v1)
- **Published**: 2021-11-04 10:08:22+00:00
- **Updated**: 2021-11-04 10:08:22+00:00
- **Authors**: Panagiotis Tzirakis, Dénes Boros, Elnar Hajiyev, Björn W. Schuller
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic affect recognition using visual cues is an important task towards a complete interaction between humans and machines. Applications can be found in tutoring systems and human computer interaction. A critical step towards that direction is facial feature extraction. In this paper, we propose a facial feature extractor model trained on an in-the-wild and massively collected video dataset provided by the RealEyes company. The dataset consists of a million labelled frames and 2,616 thousand subjects. As temporal information is important to the emotion recognition domain, we utilise LSTM cells to capture the temporal dynamics in the data. To show the favourable properties of our pre-trained model on modelling facial affect, we use the RECOLA database, and compare with the current state-of-the-art approach. Our model provides the best results in terms of concordance correlation coefficient.



### Tea Chrysanthemum Detection under Unstructured Environments Using the TC-YOLO Model
- **Arxiv ID**: http://arxiv.org/abs/2111.02724v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.02724v1)
- **Published**: 2021-11-04 10:12:57+00:00
- **Updated**: 2021-11-04 10:12:57+00:00
- **Authors**: Chao Qi, Junfeng Gao, Simon Pearson, Helen Harman, Kunjie Chen, Lei Shu
- **Comment**: None
- **Journal**: None
- **Summary**: Tea chrysanthemum detection at its flowering stage is one of the key components for selective chrysanthemum harvesting robot development. However, it is a challenge to detect flowering chrysanthemums under unstructured field environments given the variations on illumination, occlusion and object scale. In this context, we propose a highly fused and lightweight deep learning architecture based on YOLO for tea chrysanthemum detection (TC-YOLO). First, in the backbone component and neck component, the method uses the Cross-Stage Partially Dense Network (CSPDenseNet) as the main network, and embeds custom feature fusion modules to guide the gradient flow. In the final head component, the method combines the recursive feature pyramid (RFP) multiscale fusion reflow structure and the Atrous Spatial Pyramid Pool (ASPP) module with cavity convolution to achieve the detection task. The resulting model was tested on 300 field images, showing that under the NVIDIA Tesla P100 GPU environment, if the inference speed is 47.23 FPS for each image (416 * 416), TC-YOLO can achieve the average precision (AP) of 92.49% on our own tea chrysanthemum dataset. In addition, this method (13.6M) can be deployed on a single mobile GPU, and it could be further developed as a perception system for a selective chrysanthemum harvesting robot in the future.



### When Neural Networks Using Different Sensors Create Similar Features
- **Arxiv ID**: http://arxiv.org/abs/2111.02732v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.02732v1)
- **Published**: 2021-11-04 10:35:57+00:00
- **Updated**: 2021-11-04 10:35:57+00:00
- **Authors**: Hugues Moreau, Andréa Vassilev, Liming Chen
- **Comment**: None
- **Journal**: EAI MobiCase 2021, Nov 2021, Online, China
- **Summary**: Multimodal problems are omnipresent in the real world: autonomous driving, robotic grasping, scene understanding, etc... We draw from the well-developed analysis of similarity to provide an example of a problem where neural networks are trained from different sensors, and where the features extracted from these sensors still carry similar information. More precisely, we demonstrate that for each sensor, the linear combination of the features from the last layer that correlates the most with other sensors corresponds to the classification components of the classification layer.



### Deep Learning Methods for Daily Wildfire Danger Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2111.02736v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.02736v1)
- **Published**: 2021-11-04 10:39:12+00:00
- **Updated**: 2021-11-04 10:39:12+00:00
- **Authors**: Ioannis Prapas, Spyros Kondylatos, Ioannis Papoutsis, Gustau Camps-Valls, Michele Ronco, Miguel-Ángel Fernández-Torres, Maria Piles Guillem, Nuno Carvalhais
- **Comment**: Accepted to the workshop on Artificial Intelligence for Humanitarian
  Assistance and Disaster Response at the 35th Conference on Neural Information
  Processing Systems (NeurIPS 2021)
- **Journal**: None
- **Summary**: Wildfire forecasting is of paramount importance for disaster risk reduction and environmental sustainability. We approach daily fire danger prediction as a machine learning task, using historical Earth observation data from the last decade to predict next-day's fire danger. To that end, we collect, pre-process and harmonize an open-access datacube, featuring a set of covariates that jointly affect the fire occurrence and spread, such as weather conditions, satellite-derived products, topography features and variables related to human activity. We implement a variety of Deep Learning (DL) models to capture the spatial, temporal or spatio-temporal context and compare them against a Random Forest (RF) baseline. We find that either spatial or temporal context is enough to surpass the RF, while a ConvLSTM that exploits the spatio-temporal context performs best with a test Area Under the Receiver Operating Characteristic of 0.926. Our DL-based proof-of-concept provides national-scale daily fire danger maps at a much higher spatial resolution than existing operational solutions.



### Multi-scale 2D Representation Learning for weakly-supervised moment retrieval
- **Arxiv ID**: http://arxiv.org/abs/2111.02741v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.02741v1)
- **Published**: 2021-11-04 10:48:37+00:00
- **Updated**: 2021-11-04 10:48:37+00:00
- **Authors**: Ding Li, Rui Wu, Yongqiang Tang, Zhizhong Zhang, Wensheng Zhang
- **Comment**: 8 pages, 4 figuers. Accepted for publication in 2020 25th
  International Conference on Pattern Recognition (ICPR)
- **Journal**: None
- **Summary**: Video moment retrieval aims to search the moment most relevant to a given language query. However, most existing methods in this community often require temporal boundary annotations which are expensive and time-consuming to label. Hence weakly supervised methods have been put forward recently by only using coarse video-level label. Despite effectiveness, these methods usually process moment candidates independently, while ignoring a critical issue that the natural temporal dependencies between candidates in different temporal scales. To cope with this issue, we propose a Multi-scale 2D Representation Learning method for weakly supervised video moment retrieval. Specifically, we first construct a two-dimensional map for each temporal scale to capture the temporal dependencies between candidates. Two dimensions in this map indicate the start and end time points of these candidates. Then, we select top-K candidates from each scale-varied map with a learnable convolutional neural network. With a newly designed Moments Evaluation Module, we obtain the alignment scores of the selected candidates. At last, the similarity between captions and language query is served as supervision for further training the candidates' selector. Experiments on two benchmark datasets Charades-STA and ActivityNet Captions demonstrate that our approach achieves superior performance to state-of-the-art results.



### FEAFA+: An Extended Well-Annotated Dataset for Facial Expression Analysis and 3D Facial Animation
- **Arxiv ID**: http://arxiv.org/abs/2111.02751v1
- **DOI**: 10.1117/12.2643588
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.02751v1)
- **Published**: 2021-11-04 11:03:19+00:00
- **Updated**: 2021-11-04 11:03:19+00:00
- **Authors**: Wei Gan, Jian Xue, Ke Lu, Yanfu Yan, Pengcheng Gao, Jiayi Lyu
- **Comment**: None
- **Journal**: Proc. SPIE 12342, Fourteenth International Conference on Digital
  Image Processing (ICDIP 2022), 1234211 (12 October 2022)
- **Summary**: Nearly all existing Facial Action Coding System-based datasets that include facial action unit (AU) intensity information annotate the intensity values hierarchically using A--E levels. However, facial expressions change continuously and shift smoothly from one state to another. Therefore, it is more effective to regress the intensity value of local facial AUs to represent whole facial expression changes, particularly in the fields of expression transfer and facial animation. We introduce an extension of FEAFA in combination with the relabeled DISFA database, which is available at https://www.iiplab.net/feafa+/ now. Extended FEAFA (FEAFA+) includes 150 video sequences from FEAFA and DISFA, with a total of 230,184 frames being manually annotated on floating-point intensity value of 24 redefined AUs using the Expression Quantitative Tool. We also list crude numerical results for posed and spontaneous subsets and provide a baseline comparison for the AU intensity regression task.



### Online Continual Learning via Multiple Deep Metric Learning and Uncertainty-guided Episodic Memory Replay -- 3rd Place Solution for ICCV 2021 Workshop SSLAD Track 3A Continual Object Classification
- **Arxiv ID**: http://arxiv.org/abs/2111.02757v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.02757v1)
- **Published**: 2021-11-04 11:16:42+00:00
- **Updated**: 2021-11-04 11:16:42+00:00
- **Authors**: Muhammad Rifki Kurniawan, Xing Wei, Yihong Gong
- **Comment**: 6 pages, 2 figures, 3 algorithms, 1 table
- **Journal**: None
- **Summary**: Online continual learning in the wild is a very difficult task in machine learning. Non-stationarity in online continual learning potentially brings about catastrophic forgetting in neural networks. Specifically, online continual learning for autonomous driving with SODA10M dataset exhibits extra problems on extremely long-tailed distribution with continuous distribution shift. To address these problems, we propose multiple deep metric representation learning via both contrastive and supervised contrastive learning alongside soft labels distillation to improve model generalization. Moreover, we exploit modified class-balanced focal loss for sensitive penalization in class imbalanced and hard-easy samples. We also store some samples under guidance of uncertainty metric for rehearsal and perform online and periodical memory updates. Our proposed method achieves considerable generalization with average mean class accuracy (AMCA) 64.01% on validation and 64.53% AMCA on test set.



### The role of MRI physics in brain segmentation CNNs: achieving acquisition invariance and instructive uncertainties
- **Arxiv ID**: http://arxiv.org/abs/2111.02771v1
- **DOI**: 10.1007/978-3-030-87592-3_7
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.02771v1)
- **Published**: 2021-11-04 11:52:49+00:00
- **Updated**: 2021-11-04 11:52:49+00:00
- **Authors**: Pedro Borges, Richard Shaw, Thomas Varsavsky, Kerstin Klaser, David Thomas, Ivana Drobnjak, Sebastien Ourselin, M Jorge Cardoso
- **Comment**: 10 pages, 3 figures, published in: Simulation and Synthesis in
  Medical Imaging 6th International Workshop, SASHIMI 2021, Held in Conjunction
  with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings
- **Journal**: None
- **Summary**: Being able to adequately process and combine data arising from different sites is crucial in neuroimaging, but is difficult, owing to site, sequence and acquisition-parameter dependent biases. It is important therefore to design algorithms that are not only robust to images of differing contrasts, but also be able to generalise well to unseen ones, with a quantifiable measure of uncertainty. In this paper we demonstrate the efficacy of a physics-informed, uncertainty-aware, segmentation network that employs augmentation-time MR simulations and homogeneous batch feature stratification to achieve acquisition invariance. We show that the proposed approach also accurately extrapolates to out-of-distribution sequence samples, providing well calibrated volumetric bounds on these. We demonstrate a significant improvement in terms of coefficients of variation, backed by uncertainty based volumetric validation.



### Stable and Compact Face Recognition via Unlabeled Data Driven Sparse Representation-Based Classification
- **Arxiv ID**: http://arxiv.org/abs/2111.02847v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.02847v1)
- **Published**: 2021-11-04 13:19:38+00:00
- **Updated**: 2021-11-04 13:19:38+00:00
- **Authors**: Xiaohui Yang, Zheng Wang, Huan Wu, Licheng Jiao, Yiming Xu, Haolin Chen
- **Comment**: 43 pages, 10 figures, 3 tables
- **Journal**: None
- **Summary**: Sparse representation-based classification (SRC) has attracted much attention by casting the recognition problem as simple linear regression problem. SRC methods, however, still is limited to enough labeled samples per category, insufficient use of unlabeled samples, and instability of representation. For tackling these problems, an unlabeled data driven inverse projection pseudo-full-space representation-based classification model is proposed with low-rank sparse constraints. The proposed model aims to mine the hidden semantic information and intrinsic structure information of all available data, which is suitable for few labeled samples and proportion imbalance between labeled samples and unlabeled samples problems in frontal face recognition. The mixed Gauss-Seidel and Jacobian ADMM algorithm is introduced to solve the model. The convergence, representation capability and stability of the model are analyzed. Experiments on three public datasets show that the proposed LR-S-PFSRC model achieves stable results, especially for proportion imbalance of samples.



### Testing using Privileged Information by Adapting Features with Statistical Dependence
- **Arxiv ID**: http://arxiv.org/abs/2111.02865v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.02865v1)
- **Published**: 2021-11-04 13:45:34+00:00
- **Updated**: 2021-11-04 13:45:34+00:00
- **Authors**: Kwang In Kim, James Tompkin
- **Comment**: Published at ICCV 2021. Webpage: http://www.jamestompkin.com/tupi
- **Journal**: None
- **Summary**: Given an imperfect predictor, we exploit additional features at test time to improve the predictions made, without retraining and without knowledge of the prediction function. This scenario arises if training labels or data are proprietary, restricted, or no longer available, or if training itself is prohibitively expensive. We assume that the additional features are useful if they exhibit strong statistical dependence to the underlying perfect predictor. Then, we empirically estimate and strengthen the statistical dependence between the initial noisy predictor and the additional features via manifold denoising. As an example, we show that this approach leads to improvement in real-world visual attribute ranking. Project webpage: http://www.jamestompkin.com/tupi



### Extended Abstract Version: CNN-based Human Detection System for UAVs in Search and Rescue
- **Arxiv ID**: http://arxiv.org/abs/2111.02870v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.02870v1)
- **Published**: 2021-11-04 13:57:20+00:00
- **Updated**: 2021-11-04 13:57:20+00:00
- **Authors**: Nikite Mesvan
- **Comment**: 3 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:2110.01930
- **Journal**: None
- **Summary**: This paper proposes an approach for the task of searching and detecting human using a convolutional neural network and a Quadcopter hardware platform. A pre-trained CNN model is applied to a Raspberry Pi B and a single camera is equipped at the bottom of the Quadcopter. The Quadcopter uses accelerometer-gyroscope sensor and ultrasonic sensor for balancing control. However, these sensors are susceptible to noise caused by the driving forces such as the vibration of the motors, thus, noise processing is implemented. Experiments proved that the system works well on the Raspberry Pi B with a processing speed of 3 fps.



### Generalized Radiograph Representation Learning via Cross-supervision between Images and Free-text Radiology Reports
- **Arxiv ID**: http://arxiv.org/abs/2111.03452v2
- **DOI**: 10.1038/s42256-021-00425-9
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.03452v2)
- **Published**: 2021-11-04 14:28:22+00:00
- **Updated**: 2022-01-27 03:41:54+00:00
- **Authors**: Hong-Yu Zhou, Xiaoyu Chen, Yinghao Zhang, Ruibang Luo, Liansheng Wang, Yizhou Yu
- **Comment**: Accepted by Nature Machine Intelligence. The official version is at
  https://www.nature.com/articles/s42256-021-00425-9. Codes are available at
  https://github.com/funnyzhou/REFERS
- **Journal**: None
- **Summary**: Pre-training lays the foundation for recent successes in radiograph analysis supported by deep learning. It learns transferable image representations by conducting large-scale fully-supervised or self-supervised learning on a source domain. However, supervised pre-training requires a complex and labor intensive two-stage human-assisted annotation process while self-supervised learning cannot compete with the supervised paradigm. To tackle these issues, we propose a cross-supervised methodology named REviewing FreE-text Reports for Supervision (REFERS), which acquires free supervision signals from original radiology reports accompanying the radiographs. The proposed approach employs a vision transformer and is designed to learn joint representations from multiple views within every patient study. REFERS outperforms its transfer learning and self-supervised learning counterparts on 4 well-known X-ray datasets under extremely limited supervision. Moreover, REFERS even surpasses methods based on a source domain of radiographs with human-assisted structured labels. Thus REFERS has the potential to replace canonical pre-training methodologies.



### Unsupervised Change Detection of Extreme Events Using ML On-Board
- **Arxiv ID**: http://arxiv.org/abs/2111.02995v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.02995v1)
- **Published**: 2021-11-04 16:45:15+00:00
- **Updated**: 2021-11-04 16:45:15+00:00
- **Authors**: Vít Růžička, Anna Vaughan, Daniele De Martini, James Fulton, Valentina Salvatelli, Chris Bridges, Gonzalo Mateo-Garcia, Valentina Zantedeschi
- **Comment**: 5 pages (+2 in appendix), 5 figures (+1 in appendix), 2 tables (+3 in
  appendix), NeurIPS Workshop on Artificial Intelligence for Humanitarian
  Assistance and Disaster Response Workshop (AI+HADR), 2021
- **Journal**: None
- **Summary**: In this paper, we introduce RaVAEn, a lightweight, unsupervised approach for change detection in satellite data based on Variational Auto-Encoders (VAEs) with the specific purpose of on-board deployment. Applications such as disaster management enormously benefit from the rapid availability of satellite observations. Traditionally, data analysis is performed on the ground after all data is transferred - downlinked - to a ground station. Constraint on the downlink capabilities therefore affects any downstream application. In contrast, RaVAEn pre-processes the sampled data directly on the satellite and flags changed areas to prioritise for downlink, shortening the response time. We verified the efficacy of our system on a dataset composed of time series of catastrophic events - which we plan to release alongside this publication - demonstrating that RaVAEn outperforms pixel-wise baselines. Finally we tested our approach on resource-limited hardware for assessing computational and memory limitations.



### Nondestructive Testing of Composite Fibre Materials with Hyperspectral Imaging : Evaluative Studies in the EU H2020 FibreEUse Project
- **Arxiv ID**: http://arxiv.org/abs/2111.03443v1
- **DOI**: 10.1109/TIM.2022.3155745
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03443v1)
- **Published**: 2021-11-04 17:01:38+00:00
- **Updated**: 2021-11-04 17:01:38+00:00
- **Authors**: Yijun Yan, Jinchang Ren, Huan Zhao, James F. C. Windmill, Winifred Ijomah, Jesper de Wit, Justus von Freeden
- **Comment**: 11 pages, 12 figures
- **Journal**: None
- **Summary**: Through capturing spectral data from a wide frequency range along with the spatial information, hyperspectral imaging (HSI) can detect minor differences in terms of temperature, moisture and chemical composition. Therefore, HSI has been successfully applied in various applications, including remote sensing for security and defense, precision agriculture for vegetation and crop monitoring, food/drink, and pharmaceuticals quality control. However, for condition monitoring and damage detection in carbon fibre reinforced polymer (CFRP), the use of HSI is a relatively untouched area, as existing non-destructive testing (NDT) techniques focus mainly on delivering information about physical integrity of structures but not on material composition. To this end, HSI can provide a unique way to tackle this challenge. In this paper, with the use of a near-infrared HSI camera, applications of HSI for the non-destructive inspection of CFRP products are introduced, taking the EU H2020 FibreEUse project as the background. Technical challenges and solutions on three case studies are presented in detail, including adhesive residues detection, surface damage detection and Cobot based automated inspection. Experimental results have fully demonstrated the great potential of HSI and related vision techniques for NDT of CFRP, especially the potential to satisfy the industrial manufacturing environment.



### Towards Panoptic 3D Parsing for Single Image in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2111.03039v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03039v2)
- **Published**: 2021-11-04 17:45:04+00:00
- **Updated**: 2021-11-29 23:56:43+00:00
- **Authors**: Sainan Liu, Vincent Nguyen, Yuan Gao, Subarna Tripathi, Zhuowen Tu
- **Comment**: None
- **Journal**: None
- **Summary**: Performing single image holistic understanding and 3D reconstruction is a central task in computer vision. This paper presents an integrated system that performs dense scene labeling, object detection, instance segmentation, depth estimation, 3D shape reconstruction, and 3D layout estimation for indoor and outdoor scenes from a single RGB image. We name our system panoptic 3D parsing (Panoptic3D) in which panoptic segmentation ("stuff" segmentation and "things" detection/segmentation) with 3D reconstruction is performed. We design a stage-wise system, Panoptic3D (stage-wise), where a complete set of annotations is absent. Additionally, we present an end-to-end pipeline, Panoptic3D (end-to-end), trained on a synthetic dataset with a full set of annotations. We show results on both indoor (3D-FRONT) and outdoor (COCO and Cityscapes) scenes. Our proposed panoptic 3D parsing framework points to a promising direction in computer vision. Panoptic3D can be applied to a variety of applications, including autonomous driving, mapping, robotics, design, computer graphics, robotics, human-computer interaction, and augmented reality.



### Unsupervised Learning of Compositional Energy Concepts
- **Arxiv ID**: http://arxiv.org/abs/2111.03042v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.03042v1)
- **Published**: 2021-11-04 17:46:12+00:00
- **Updated**: 2021-11-04 17:46:12+00:00
- **Authors**: Yilun Du, Shuang Li, Yash Sharma, Joshua B. Tenenbaum, Igor Mordatch
- **Comment**: NeurIPS 2021, website and code at
  https://energy-based-model.github.io/comet/
- **Journal**: None
- **Summary**: Humans are able to rapidly understand scenes by utilizing concepts extracted from prior experience. Such concepts are diverse, and include global scene descriptors, such as the weather or lighting, as well as local scene descriptors, such as the color or size of a particular object. So far, unsupervised discovery of concepts has focused on either modeling the global scene-level or the local object-level factors of variation, but not both. In this work, we propose COMET, which discovers and represents concepts as separate energy functions, enabling us to represent both global concepts as well as objects under a unified framework. COMET discovers energy functions through recomposing the input image, which we find captures independent factors without additional supervision. Sample generation in COMET is formulated as an optimization process on underlying energy functions, enabling us to generate images with permuted and composed concepts. Finally, discovered visual concepts in COMET generalize well, enabling us to compose concepts between separate modalities of images as well as with other concepts discovered by a separate instance of COMET trained on a different dataset. Code and data available at https://energy-based-model.github.io/comet/.



### A deep ensemble approach to X-ray polarimetry
- **Arxiv ID**: http://arxiv.org/abs/2111.03047v2
- **DOI**: None
- **Categories**: **astro-ph.IM**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.03047v2)
- **Published**: 2021-11-04 17:49:45+00:00
- **Updated**: 2021-11-30 17:29:25+00:00
- **Authors**: A. L. Peirson, R. W. Romani
- **Comment**: Fourth Workshop on Machine Learning and the Physical Sciences
  (NeurIPS 2021)
- **Journal**: None
- **Summary**: X-ray polarimetry will soon open a new window on the high energy universe with the launch of NASA's Imaging X-ray Polarimetry Explorer (IXPE). Polarimeters are currently limited by their track reconstruction algorithms, which typically use linear estimators and do not consider individual event quality. We present a modern deep learning method for maximizing the sensitivity of X-ray telescopic observations with imaging polarimeters, with a focus on the gas pixel detectors (GPDs) to be flown on IXPE. We use a weighted maximum likelihood combination of predictions from a deep ensemble of ResNets, trained on Monte Carlo event simulations. We derive and apply the optimal event weighting for maximizing the polarization signal-to-noise ratio (SNR) in track reconstruction algorithms. For typical power-law source spectra, our method improves on the current state of the art, providing a ~40% decrease in required exposure times for a given SNR.



### Bootstrap Your Object Detector via Mixed Training
- **Arxiv ID**: http://arxiv.org/abs/2111.03056v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03056v1)
- **Published**: 2021-11-04 17:58:26+00:00
- **Updated**: 2021-11-04 17:58:26+00:00
- **Authors**: Mengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin, Yue Cao, Stephen Lin, Han Hu, Xiang Bai
- **Comment**: None
- **Journal**: NeurIPS 2021, Spotlight
- **Summary**: We introduce MixTraining, a new training paradigm for object detection that can improve the performance of existing detectors for free. MixTraining enhances data augmentation by utilizing augmentations of different strengths while excluding the strong augmentations of certain training samples that may be detrimental to training. In addition, it addresses localization noise and missing labels in human annotations by incorporating pseudo boxes that can compensate for these errors. Both of these MixTraining capabilities are made possible through bootstrapping on the detector, which can be used to predict the difficulty of training on a strong augmentation, as well as to generate reliable pseudo boxes thanks to the robustness of neural networks to labeling error. MixTraining is found to bring consistent improvements across various detectors on the COCO dataset. In particular, the performance of Faster R-CNN \cite{ren2015faster} with a ResNet-50 \cite{he2016deep} backbone is improved from 41.7 mAP to 44.0 mAP, and the accuracy of Cascade-RCNN \cite{cai2018cascade} with a Swin-Small \cite{liu2021swin} backbone is raised from 50.9 mAP to 52.8 mAP. The code and models will be made publicly available at \url{https://github.com/MendelXu/MixTraining}.



### Generalization in Dexterous Manipulation via Geometry-Aware Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.03062v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2111.03062v1)
- **Published**: 2021-11-04 17:59:56+00:00
- **Updated**: 2021-11-04 17:59:56+00:00
- **Authors**: Wenlong Huang, Igor Mordatch, Pieter Abbeel, Deepak Pathak
- **Comment**: Website at https://huangwl18.github.io/geometry-dex
- **Journal**: None
- **Summary**: Dexterous manipulation of arbitrary objects, a fundamental daily task for humans, has been a grand challenge for autonomous robotic systems. Although data-driven approaches using reinforcement learning can develop specialist policies that discover behaviors to control a single object, they often exhibit poor generalization to unseen ones. In this work, we show that policies learned by existing reinforcement learning algorithms can in fact be generalist when combined with multi-task learning and a well-chosen object representation. We show that a single generalist policy can perform in-hand manipulation of over 100 geometrically-diverse real-world objects and generalize to new objects with unseen shape or size. Interestingly, we find that multi-task learning with object point cloud representations not only generalizes better but even outperforms the single-object specialist policies on both training as well as held-out test objects. Video results at https://huangwl18.github.io/geometry-dex



### Voxel-based 3D Detection and Reconstruction of Multiple Objects from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2111.03098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03098v1)
- **Published**: 2021-11-04 18:30:37+00:00
- **Updated**: 2021-11-04 18:30:37+00:00
- **Authors**: Feng Liu, Xiaoming Liu
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Inferring 3D locations and shapes of multiple objects from a single 2D image is a long-standing objective of computer vision. Most of the existing works either predict one of these 3D properties or focus on solving both for a single object. One fundamental challenge lies in how to learn an effective representation of the image that is well-suited for 3D detection and reconstruction. In this work, we propose to learn a regular grid of 3D voxel features from the input image which is aligned with 3D scene space via a 3D feature lifting operator. Based on the 3D voxel features, our novel CenterNet-3D detection head formulates the 3D detection as keypoint detection in the 3D space. Moreover, we devise an efficient coarse-to-fine reconstruction module, including coarse-level voxelization and a novel local PCA-SDF shape representation, which enables fine detail reconstruction and one order of magnitude faster inference than prior methods. With complementary supervision from both 3D detection and reconstruction, one enables the 3D voxel features to be geometry and context preserving, benefiting both tasks.The effectiveness of our approach is demonstrated through 3D detection and reconstruction in single object and multiple object scenarios.



### Skeleton-Split Framework using Spatial Temporal Graph Convolutional Networks for Action Recogntion
- **Arxiv ID**: http://arxiv.org/abs/2111.03106v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.03106v1)
- **Published**: 2021-11-04 18:59:02+00:00
- **Updated**: 2021-11-04 18:59:02+00:00
- **Authors**: Motasem Alsawadi, Miguel Rio
- **Comment**: None
- **Journal**: None
- **Summary**: There has been a dramatic increase in the volume of videos and their related content uploaded to the internet. Accordingly, the need for efficient algorithms to analyse this vast amount of data has attracted significant research interest. An action recognition system based upon human body motions has been proven to interpret videos contents accurately. This work aims to recognize activities of daily living using the ST-GCN model, providing a comparison between four different partitioning strategies: spatial configuration partitioning, full distance split, connection split, and index split. To achieve this aim, we present the first implementation of the ST-GCN framework upon the HMDB-51 dataset. We have achieved 48.88 % top-1 accuracy by using the connection split partitioning approach. Through experimental simulation, we show that our proposals have achieved the highest accuracy performance on the UCF-101 dataset using the ST-GCN framework than the state-of-the-art approach. Finally, accuracy of 73.25 % top-1 is achieved by using the index split partitioning strategy.



### Attention on Classification for Fire Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.03129v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03129v1)
- **Published**: 2021-11-04 19:52:49+00:00
- **Updated**: 2021-11-04 19:52:49+00:00
- **Authors**: Milad Niknejad, Alexandre Bernardino
- **Comment**: None
- **Journal**: None
- **Summary**: Detection and localization of fire in images and videos are important in tackling fire incidents. Although semantic segmentation methods can be used to indicate the location of pixels with fire in the images, their predictions are localized, and they often fail to consider global information of the existence of fire in the image which is implicit in the image labels. We propose a Convolutional Neural Network (CNN) for joint classification and segmentation of fire in images which improves the performance of the fire segmentation. We use a spatial self-attention mechanism to capture long-range dependency between pixels, and a new channel attention module which uses the classification probability as an attention weight. The network is jointly trained for both segmentation and classification, leading to improvement in the performance of the single-task image segmentation methods, and the previous methods proposed for fire segmentation.



### StyleCLIPDraw: Coupling Content and Style in Text-to-Drawing Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2111.03133v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2111.03133v2)
- **Published**: 2021-11-04 19:57:17+00:00
- **Updated**: 2022-03-01 02:31:48+00:00
- **Authors**: Peter Schaldenbrand, Zhixuan Liu, Jean Oh
- **Comment**: Superseded by arXiv:2202.12362
- **Journal**: None
- **Summary**: Generating images that fit a given text description using machine learning has improved greatly with the release of technologies such as the CLIP image-text encoder model; however, current methods lack artistic control of the style of image to be generated. We introduce StyleCLIPDraw which adds a style loss to the CLIPDraw text-to-drawing synthesis model to allow artistic control of the synthesized drawings in addition to control of the content via text. Whereas performing decoupled style transfer on a generated image only affects the texture, our proposed coupled approach is able to capture a style in both texture and shape, suggesting that the style of the drawing is coupled with the drawing process itself. More results and our code are available at https://github.com/pschaldenbrand/StyleCLIPDraw



### GraN-GAN: Piecewise Gradient Normalization for Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2111.03162v1
- **DOI**: 10.1109/WACV51458.2022.00249
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2111.03162v1)
- **Published**: 2021-11-04 21:13:02+00:00
- **Updated**: 2021-11-04 21:13:02+00:00
- **Authors**: Vineeth S. Bhaskara, Tristan Aumentado-Armstrong, Allan Jepson, Alex Levinshtein
- **Comment**: WACV 2022 Main Conference Paper (Submitted: 18 Aug 2021, Accepted: 4
  Oct 2021)
- **Journal**: 2022 IEEE/CVF Winter Conference on Applications of Computer Vision
  (WACV), 2022, pp. 2432-2441
- **Summary**: Modern generative adversarial networks (GANs) predominantly use piecewise linear activation functions in discriminators (or critics), including ReLU and LeakyReLU. Such models learn piecewise linear mappings, where each piece handles a subset of the input space, and the gradients per subset are piecewise constant. Under such a class of discriminator (or critic) functions, we present Gradient Normalization (GraN), a novel input-dependent normalization method, which guarantees a piecewise K-Lipschitz constraint in the input space. In contrast to spectral normalization, GraN does not constrain processing at the individual network layers, and, unlike gradient penalties, strictly enforces a piecewise Lipschitz constraint almost everywhere. Empirically, we demonstrate improved image generation performance across multiple datasets (incl. CIFAR-10/100, STL-10, LSUN bedrooms, and CelebA), GAN loss functions, and metrics. Further, we analyze altering the often untuned Lipschitz constant K in several standard GANs, not only attaining significant performance gains, but also finding connections between K and training dynamics, particularly in low-gradient loss plateaus, with the common Adam optimizer.



### EditGAN: High-Precision Semantic Image Editing
- **Arxiv ID**: http://arxiv.org/abs/2111.03186v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.03186v1)
- **Published**: 2021-11-04 22:36:33+00:00
- **Updated**: 2021-11-04 22:36:33+00:00
- **Authors**: Huan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, Antonio Torralba, Sanja Fidler
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have recently found applications in image editing. However, most GAN based image editing methods often require large scale datasets with semantic segmentation annotations for training, only provide high level control, or merely interpolate between different images. Here, we propose EditGAN, a novel method for high quality, high precision semantic image editing, allowing users to edit images by modifying their highly detailed part segmentation masks, e.g., drawing a new mask for the headlight of a car. EditGAN builds on a GAN framework that jointly models images and their semantic segmentations, requiring only a handful of labeled examples, making it a scalable tool for editing. Specifically, we embed an image into the GAN latent space and perform conditional latent code optimization according to the segmentation edit, which effectively also modifies the image. To amortize optimization, we find editing vectors in latent space that realize the edits. The framework allows us to learn an arbitrary number of editing vectors, which can then be directly applied on other images at interactive rates. We experimentally show that EditGAN can manipulate images with an unprecedented level of detail and freedom, while preserving full image quality.We can also easily combine multiple edits and perform plausible edits beyond EditGAN training data. We demonstrate EditGAN on a wide variety of image types and quantitatively outperform several previous editing methods on standard editing benchmark tasks.



### Addressing Multiple Salient Object Detection via Dual-Space Long-Range Dependencies
- **Arxiv ID**: http://arxiv.org/abs/2111.03195v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03195v1)
- **Published**: 2021-11-04 23:16:53+00:00
- **Updated**: 2021-11-04 23:16:53+00:00
- **Authors**: Bowen Deng, Andrew P. French, Michael P. Pound
- **Comment**: 10 pages, 9 figures
- **Journal**: None
- **Summary**: Salient object detection plays an important role in many downstream tasks. However, complex real-world scenes with varying scales and numbers of salient objects still pose a challenge. In this paper, we directly address the problem of detecting multiple salient objects across complex scenes. We propose a network architecture incorporating non-local feature information in both the spatial and channel spaces, capturing the long-range dependencies between separate objects. Traditional bottom-up and non-local features are combined with edge features within a feature fusion gate that progressively refines the salient object prediction in the decoder. We show that our approach accurately locates multiple salient regions even in complex scenarios. To demonstrate the efficacy of our approach to the multiple salient objects problem, we curate a new dataset containing only multiple salient objects. Our experiments demonstrate the proposed method presents state-of-the-art results on five widely used datasets without any pre-processing and post-processing. We obtain a further performance improvement against competing techniques on our multi-objects dataset. The dataset and source code are avaliable at: https://github.com/EricDengbowen/DSLRDNet.



