# Arxiv Papers in cs.CV on 2021-11-05
### Fast Camouflaged Object Detection via Edge-based Reversible Re-calibration Network
- **Arxiv ID**: http://arxiv.org/abs/2111.03216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03216v1)
- **Published**: 2021-11-05 02:03:54+00:00
- **Updated**: 2021-11-05 02:03:54+00:00
- **Authors**: Ge-Peng Ji, Lei Zhu, Mingchen Zhuge, Keren Fu
- **Comment**: 35 pages, 7 figures, 5 tables (Accepted by Pattern Recognition 2022)
- **Journal**: None
- **Summary**: Camouflaged Object Detection (COD) aims to detect objects with similar patterns (e.g., texture, intensity, colour, etc) to their surroundings, and recently has attracted growing research interest. As camouflaged objects often present very ambiguous boundaries, how to determine object locations as well as their weak boundaries is challenging and also the key to this task. Inspired by the biological visual perception process when a human observer discovers camouflaged objects, this paper proposes a novel edge-based reversible re-calibration network called ERRNet. Our model is characterized by two innovative designs, namely Selective Edge Aggregation (SEA) and Reversible Re-calibration Unit (RRU), which aim to model the visual perception behaviour and achieve effective edge prior and cross-comparison between potential camouflaged regions and background. More importantly, RRU incorporates diverse priors with more comprehensive information comparing to existing COD models. Experimental results show that ERRNet outperforms existing cutting-edge baselines on three COD datasets and five medical image segmentation datasets. Especially, compared with the existing top-1 model SINet, ERRNet significantly improves the performance by $\sim$6% (mean E-measure) with notably high speed (79.3 FPS), showing that ERRNet could be a general and robust solution for the COD task.



### Technical Report: Disentangled Action Parsing Networks for Accurate Part-level Action Parsing
- **Arxiv ID**: http://arxiv.org/abs/2111.03225v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03225v1)
- **Published**: 2021-11-05 02:29:32+00:00
- **Updated**: 2021-11-05 02:29:32+00:00
- **Authors**: Xuanhan Wang, Xiaojia Chen, Lianli Gao, Lechao Chen, Jingkuan Song
- **Comment**: None
- **Journal**: None
- **Summary**: Part-level Action Parsing aims at part state parsing for boosting action recognition in videos. Despite of dramatic progresses in the area of video classification research, a severe problem faced by the community is that the detailed understanding of human actions is ignored. Our motivation is that parsing human actions needs to build models that focus on the specific problem. We present a simple yet effective approach, named disentangled action parsing (DAP). Specifically, we divided the part-level action parsing into three stages: 1) person detection, where a person detector is adopted to detect all persons from videos as well as performs instance-level action recognition; 2) Part parsing, where a part-parsing model is proposed to recognize human parts from detected person images; and 3) Action parsing, where a multi-modal action parsing network is used to parse action category conditioning on all detection results that are obtained from previous stages. With these three major models applied, our approach of DAP records a global mean of $0.605$ score in 2021 Kinetics-TPS Challenge.



### Multi-Spectral Multi-Image Super-Resolution of Sentinel-2 with Radiometric Consistency Losses and Its Effect on Building Delineation
- **Arxiv ID**: http://arxiv.org/abs/2111.03231v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03231v1)
- **Published**: 2021-11-05 02:49:04+00:00
- **Updated**: 2021-11-05 02:49:04+00:00
- **Authors**: Muhammed Razzak, Gonzalo Mateo-Garcia, Luis GÃ³mez-Chova, Yarin Gal, Freddie Kalaitzis
- **Comment**: None
- **Journal**: None
- **Summary**: High resolution remote sensing imagery is used in broad range of tasks, including detection and classification of objects. High-resolution imagery is however expensive, while lower resolution imagery is often freely available and can be used by the public for range of social good applications. To that end, we curate a multi-spectral multi-image super-resolution dataset, using PlanetScope imagery from the SpaceNet 7 challenge as the high resolution reference and multiple Sentinel-2 revisits of the same imagery as the low-resolution imagery. We present the first results of applying multi-image super-resolution (MISR) to multi-spectral remote sensing imagery. We, additionally, introduce a radiometric consistency module into MISR model the to preserve the high radiometric resolution of the Sentinel-2 sensor. We show that MISR is superior to single-image super-resolution and other baselines on a range of image fidelity metrics. Furthermore, we conduct the first assessment of the utility of multi-image super-resolution on building delineation, showing that utilising multiple images results in better performance in these downstream tasks.



### Learning of Time-Frequency Attention Mechanism for Automatic Modulation Recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.03258v2
- **DOI**: 10.1109/LWC.2022.3140828
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03258v2)
- **Published**: 2021-11-05 04:52:52+00:00
- **Updated**: 2022-01-11 12:04:06+00:00
- **Authors**: Shangao Lin, Yuan Zeng, Yi Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Recent learning-based image classification and speech recognition approaches make extensive use of attention mechanisms to achieve state-of-the-art recognition power, which demonstrates the effectiveness of attention mechanisms. Motivated by the fact that the frequency and time information of modulated radio signals are crucial for modulation mode recognition, this paper proposes a time-frequency attention mechanism for a convolutional neural network (CNN)-based modulation recognition framework. The proposed time-frequency attention module is designed to learn which channel, frequency and time information is more meaningful in CNN for modulation recognition. We analyze the effectiveness of the proposed time-frequency attention mechanism and compare the proposed method with two existing learning-based methods. Experiments on an open-source modulation recognition dataset show that the recognition performance of the proposed framework is better than those of the framework without time-frequency attention and existing learning-based methods.



### Remote Sensing Image Super-resolution and Object Detection: Benchmark and State of the Art
- **Arxiv ID**: http://arxiv.org/abs/2111.03260v1
- **DOI**: 10.1016/j.eswa.2022.116793
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03260v1)
- **Published**: 2021-11-05 04:56:34+00:00
- **Updated**: 2021-11-05 04:56:34+00:00
- **Authors**: Yi Wang, Syed Muhammad Arsalan Bashir, Mahrukh Khan, Qudrat Ullah, Rui Wang, Yilin Song, Zhe Guo, Yilong Niu
- **Comment**: 39 pages, 15 figures, 5 tables. Submitted to Elsevier journal for
  review
- **Journal**: Expert Systems with Applications, 2022
- **Summary**: For the past two decades, there have been significant efforts to develop methods for object detection in Remote Sensing (RS) images. In most cases, the datasets for small object detection in remote sensing images are inadequate. Many researchers used scene classification datasets for object detection, which has its limitations; for example, the large-sized objects outnumber the small objects in object categories. Thus, they lack diversity; this further affects the detection performance of small object detectors in RS images. This paper reviews current datasets and object detection methods (deep learning-based) for remote sensing images. We also propose a large-scale, publicly available benchmark Remote Sensing Super-resolution Object Detection (RSSOD) dataset. The RSSOD dataset consists of 1,759 hand-annotated images with 22,091 instances of very high resolution (VHR) images with a spatial resolution of ~0.05 m. There are five classes with varying frequencies of labels per class. The image patches are extracted from satellite images, including real image distortions such as tangential scale distortion and skew distortion. We also propose a novel Multi-class Cyclic super-resolution Generative adversarial network with Residual feature aggregation (MCGR) and auxiliary YOLOv5 detector to benchmark image super-resolution-based object detection and compare with the existing state-of-the-art methods based on image super-resolution (SR). The proposed MCGR achieved state-of-the-art performance for image SR with an improvement of 1.2dB PSNR compared to the current state-of-the-art NLSN method. MCGR achieved best object detection mAPs of 0.758, 0.881, 0.841, and 0.983, respectively, for five-class, four-class, two-class, and single classes, respectively surpassing the performance of the state-of-the-art object detectors YOLOv5, EfficientDet, Faster RCNN, SSD, and RetinaNet.



### Pathological Analysis of Blood Cells Using Deep Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/2111.03274v1
- **DOI**: 10.2174/2666255813999200904113251
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2111.03274v1)
- **Published**: 2021-11-05 05:37:10+00:00
- **Updated**: 2021-11-05 05:37:10+00:00
- **Authors**: Virender Ranga, Shivam Gupta, Priyansh Agrawal, Jyoti Meena
- **Comment**: 6 Page, 3 Table and 6 Figures
- **Journal**: Recent Advances in Computer Science and Communications(Formerly
  Recent Patents on Computer Science),04 September,2020, Article ID
  e140921185564
- **Summary**: Pathology deals with the practice of discovering the reasons for disease by analyzing the body samples. The most used way in this field, is to use histology which is basically studying and viewing microscopic structures of cell and tissues. The slide viewing method is widely being used and converted into digital form to produce high resolution images. This enabled the area of deep learning and machine learning to deep dive into this field of medical sciences. In the present study, a neural based network has been proposed for classification of blood cells images into various categories. When input image is passed through the proposed architecture and all the hyper parameters and dropout ratio values are used in accordance with proposed algorithm, then model classifies the blood images with an accuracy of 95.24%. The performance of proposed model is better than existing standard architectures and work done by various researchers. Thus model will enable development of pathological system which will reduce human errors and daily load on laboratory men. This will in turn help pathologists in carrying out their work more efficiently and effectively.



### Recognizing Vector Graphics without Rasterization
- **Arxiv ID**: http://arxiv.org/abs/2111.03281v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03281v3)
- **Published**: 2021-11-05 06:16:17+00:00
- **Updated**: 2021-12-24 03:13:48+00:00
- **Authors**: Xinyang Jiang, Lu Liu, Caihua Shan, Yifei Shen, Xuanyi Dong, Dongsheng Li
- **Comment**: Accepted by NeurIPS2021
- **Journal**: None
- **Summary**: In this paper, we consider a different data format for images: vector graphics. In contrast to raster graphics which are widely used in image recognition, vector graphics can be scaled up or down into any resolution without aliasing or information loss, due to the analytic representation of the primitives in the document. Furthermore, vector graphics are able to give extra structural information on how low-level elements group together to form high level shapes or structures. These merits of graphic vectors have not been fully leveraged in existing methods. To explore this data format, we target on the fundamental recognition tasks: object localization and classification. We propose an efficient CNN-free pipeline that does not render the graphic into pixels (i.e. rasterization), and takes textual document of the vector graphics as input, called YOLaT (You Only Look at Text). YOLaT builds multi-graphs to model the structural and spatial information in vector graphics, and a dual-stream graph neural network is proposed to detect objects from the graph. Our experiments show that by directly operating on vector graphics, YOLaT out-performs raster-graphic based object detection baselines in terms of both average precision and efficiency.



### FBNet: Feature Balance Network for Urban-Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.03286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03286v1)
- **Published**: 2021-11-05 06:27:10+00:00
- **Updated**: 2021-11-05 06:27:10+00:00
- **Authors**: Lei Gan, Huabin Huang, Banghuai Li, Ye Yuan
- **Comment**: Tech Report
- **Journal**: None
- **Summary**: Image segmentation in the urban scene has recently attracted much attention due to its success in autonomous driving systems. However, the poor performance of concerned foreground targets, e.g., traffic lights and poles, still limits its further practical applications. In urban scenes, foreground targets are always concealed in their surrounding stuff because of the special camera position and 3D perspective projection. What's worse, it exacerbates the unbalance between foreground and background classes in high-level features due to the continuous expansion of the reception field. We call it Feature Camouflage. In this paper, we present a novel add-on module, named Feature Balance Network (FBNet), to eliminate the feature camouflage in urban-scene segmentation. FBNet consists of two key components, i.e., Block-wise BCE(BwBCE) and Dual Feature Modulator(DFM). BwBCE serves as an auxiliary loss to ensure uniform gradients for foreground classes and their surroundings during backpropagation. At the same time, DFM intends to enhance the deep representation of foreground classes in high-level features adaptively under the supervision of BwBCE. These two modules facilitate each other as a whole to ease feature camouflage effectively. Our proposed method achieves a new state-of-the-art segmentation performance on two challenging urban-scene benchmarks, i.e., Cityscapes and BDD100K. Code will be released for reproduction.



### SPANN: Highly-efficient Billion-scale Approximate Nearest Neighbor Search
- **Arxiv ID**: http://arxiv.org/abs/2111.08566v1
- **DOI**: None
- **Categories**: **cs.DB**, cs.AI, cs.CV, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.08566v1)
- **Published**: 2021-11-05 06:28:15+00:00
- **Updated**: 2021-11-05 06:28:15+00:00
- **Authors**: Qi Chen, Bing Zhao, Haidong Wang, Mingqin Li, Chuanjie Liu, Zengzhong Li, Mao Yang, Jingdong Wang
- **Comment**: Accepted to NeurIPS 2021
- **Journal**: None
- **Summary**: The in-memory algorithms for approximate nearest neighbor search (ANNS) have achieved great success for fast high-recall search, but are extremely expensive when handling very large scale database. Thus, there is an increasing request for the hybrid ANNS solutions with small memory and inexpensive solid-state drive (SSD). In this paper, we present a simple but efficient memory-disk hybrid indexing and search system, named SPANN, that follows the inverted index methodology. It stores the centroid points of the posting lists in the memory and the large posting lists in the disk. We guarantee both disk-access efficiency (low latency) and high recall by effectively reducing the disk-access number and retrieving high-quality posting lists. In the index-building stage, we adopt a hierarchical balanced clustering algorithm to balance the length of posting lists and augment the posting list by adding the points in the closure of the corresponding clusters. In the search stage, we use a query-aware scheme to dynamically prune the access of unnecessary posting lists. Experiment results demonstrate that SPANN is 2$\times$ faster than the state-of-the-art ANNS solution DiskANN to reach the same recall quality $90\%$ with same memory cost in three billion-scale datasets. It can reach $90\%$ recall@1 and recall@10 in just around one millisecond with only 32GB memory cost. Code is available at: {\footnotesize\color{blue}{\url{https://github.com/microsoft/SPTAG}}}.



### Frequency-Aware Physics-Inspired Degradation Model for Real-World Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2111.03301v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03301v2)
- **Published**: 2021-11-05 07:30:00+00:00
- **Updated**: 2022-02-11 06:41:54+00:00
- **Authors**: Zhenxing Dong, Hong Cao, Wang Shen, Yu Gan, Yuye Ling, Guangtao Zhai, Yikai Su
- **Comment**: 22 pages,12 figures
- **Journal**: None
- **Summary**: Current learning-based single image super-resolution (SISR) algorithms underperform on real data due to the deviation in the assumed degrada-tion process from that in the real-world scenario. Conventional degradation processes consider applying blur, noise, and downsampling (typicallybicubic downsampling) on high-resolution (HR) images to synthesize low-resolution (LR) counterparts. However, few works on degradation modelling have taken the physical aspects of the optical imaging system intoconsideration. In this paper, we analyze the imaging system optically andexploit the characteristics of the real-world LR-HR pairs in the spatial frequency domain. We formulate a real-world physics-inspired degradationmodel by considering bothopticsandsensordegradation; The physical degradation of an imaging system is modelled as a low-pass filter, whose cut-off frequency is dictated by the object distance, the focal length of thelens, and the pixel size of the image sensor. In particular, we propose to use a convolutional neural network (CNN) to learn the cutoff frequency of real-world degradation process. The learned network is then applied to synthesize LR images from unpaired HR images. The synthetic HR-LR image pairs are later used to train an SISR network. We evaluatethe effectiveness and generalization capability of the proposed degradation model on real-world images captured by different imaging systems. Experimental results showcase that the SISR network trained by using our synthetic data performs favorably against the network using the traditional degradation model. Moreover, our results are comparable to that obtained by the same network trained by using real-world LR-HR pairs, which are challenging to obtain in real scenes.



### Self-Supervised Intrinsic Image Decomposition Network Considering Reflectance Consistency
- **Arxiv ID**: http://arxiv.org/abs/2111.04506v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2111.04506v1)
- **Published**: 2021-11-05 07:52:06+00:00
- **Updated**: 2021-11-05 07:52:06+00:00
- **Authors**: Yuma Kinoshita, Hitoshi Kiya
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel intrinsic image decomposition network considering reflectance consistency. Intrinsic image decomposition aims to decompose an image into illumination-invariant and illumination-variant components, referred to as ``reflectance'' and ``shading,'' respectively. Although there are three consistencies that the reflectance and shading should satisfy, most conventional work does not sufficiently account for consistency with respect to reflectance, owing to the use of a white-illuminant decomposition model and the lack of training images capturing the same objects under various illumination-brightness and -color conditions. For this reason, the three consistencies are considered in the proposed network by using a color-illuminant model and training the network with losses calculated from images taken under various illumination conditions. In addition, the proposed network can be trained in a self-supervised manner because various illumination conditions can easily be simulated. Experimental results show that our network can decompose images into reflectance and shading components.



### KORSAL: Key-point Detection based Online Real-Time Spatio-Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2111.03319v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2111.03319v1)
- **Published**: 2021-11-05 08:39:36+00:00
- **Updated**: 2021-11-05 08:39:36+00:00
- **Authors**: Kalana Abeywardena, Shechem Sumanthiran, Sakuna Jayasundara, Sachira Karunasena, Ranga Rodrigo, Peshala Jayasekara
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Real-time and online action localization in a video is a critical yet highly challenging problem. Accurate action localization requires the utilization of both temporal and spatial information. Recent attempts achieve this by using computationally intensive 3D CNN architectures or highly redundant two-stream architectures with optical flow, making them both unsuitable for real-time, online applications. To accomplish activity localization under highly challenging real-time constraints, we propose utilizing fast and efficient key-point based bounding box prediction to spatially localize actions. We then introduce a tube-linking algorithm that maintains the continuity of action tubes temporally in the presence of occlusions. Further, we eliminate the need for a two-stream architecture by combining temporal and spatial information into a cascaded input to a single network, allowing the network to learn from both types of information. Temporal information is efficiently extracted using a structural similarity index map as opposed to computationally intensive optical flow. Despite the simplicity of our approach, our lightweight end-to-end architecture achieves state-of-the-art frame-mAP of 74.7% on the challenging UCF101-24 dataset, demonstrating a performance gain of 6.4% over the previous best online methods. We also achieve state-of-the-art video-mAP results compared to both online and offline methods. Moreover, our model achieves a frame rate of 41.8 FPS, which is a 10.7% improvement over contemporary real-time methods.



### Negative Sample is Negative in Its Own Way: Tailoring Negative Sentences for Image-Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2111.03349v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2111.03349v1)
- **Published**: 2021-11-05 09:36:41+00:00
- **Updated**: 2021-11-05 09:36:41+00:00
- **Authors**: Zhihao Fan, Zhongyu Wei, Zejun Li, Siyuan Wang, Jianqing Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Matching model is essential for Image-Text Retrieval framework. Existing research usually train the model with a triplet loss and explore various strategy to retrieve hard negative sentences in the dataset. We argue that current retrieval-based negative sample construction approach is limited in the scale of the dataset thus fail to identify negative sample of high difficulty for every image. We propose our TAiloring neGative Sentences with Discrimination and Correction (TAGS-DC) to generate synthetic sentences automatically as negative samples. TAGS-DC is composed of masking and refilling to generate synthetic negative sentences with higher difficulty. To keep the difficulty during training, we mutually improve the retrieval and generation through parameter sharing. To further utilize fine-grained semantic of mismatch in the negative sentence, we propose two auxiliary tasks, namely word discrimination and word correction to improve the training. In experiments, we verify the effectiveness of our model on MS-COCO and Flickr30K compared with current state-of-the-art models and demonstrates its robustness and faithfulness in the further analysis. Our code is available in https://github.com/LibertFan/TAGS.



### Hepatic vessel segmentation based on 3D swin-transformer with inductive biased multi-head self-attention
- **Arxiv ID**: http://arxiv.org/abs/2111.03368v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03368v2)
- **Published**: 2021-11-05 10:17:08+00:00
- **Updated**: 2021-11-22 02:02:41+00:00
- **Authors**: Mian Wu, Yinling Qian, Xiangyun Liao, Qiong Wang, Pheng-Ann Heng
- **Comment**: 20 pages, 6 figures
- **Journal**: None
- **Summary**: Purpose: Segmentation of liver vessels from CT images is indispensable prior to surgical planning and aroused broad range of interests in the medical image analysis community. Due to the complex structure and low contrast background, automatic liver vessel segmentation remains particularly challenging. Most of the related researches adopt FCN, U-net, and V-net variants as a backbone. However, these methods mainly focus on capturing multi-scale local features which may produce misclassified voxels due to the convolutional operator's limited locality reception field.   Methods: We propose a robust end-to-end vessel segmentation network called Inductive BIased Multi-Head Attention Vessel Net(IBIMHAV-Net) by expanding swin transformer to 3D and employing an effective combination of convolution and self-attention. In practice, we introduce the voxel-wise embedding rather than patch-wise embedding to locate precise liver vessel voxels, and adopt multi-scale convolutional operators to gain local spatial information. On the other hand, we propose the inductive biased multi-head self-attention which learns inductive biased relative positional embedding from initialized absolute position embedding. Based on this, we can gain a more reliable query and key matrix. To validate the generalization of our model, we test on samples which have different structural complexity.   Results: We conducted experiments on the 3DIRCADb datasets. The average dice and sensitivity of the four tested cases were 74.8% and 77.5%, which exceed results of existing deep learning methods and improved graph cuts method.   Conclusion: The proposed model IBIMHAV-Net provides an automatic, accurate 3D liver vessel segmentation with an interleaved architecture that better utilizes both global and local spatial features in CT volumes. It can be further extended for other clinical data.



### Segmentation of 2D Brain MR Images
- **Arxiv ID**: http://arxiv.org/abs/2111.03370v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.03370v1)
- **Published**: 2021-11-05 10:23:09+00:00
- **Updated**: 2021-11-05 10:23:09+00:00
- **Authors**: Angad Ripudaman Singh Bajwa
- **Comment**: None
- **Journal**: None
- **Summary**: Brain tumour segmentation is an essential task in medical image processing. Early diagnosis of brain tumours plays a crucial role in improving treatment possibilities and increases the survival rate of the patients. Manual segmentation of the brain tumours for cancer diagnosis, from large number of MRI images, is both a difficult and time-consuming task. There is a need for automatic brain tumour image segmentation. The purpose of this project is to provide an automatic brain tumour segmentation method of MRI images to help locate the tumour accurately and quickly.



### Seamless Satellite-image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2111.03384v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2111.03384v1)
- **Published**: 2021-11-05 10:42:24+00:00
- **Updated**: 2021-11-05 10:42:24+00:00
- **Authors**: Jialin Zhu, Tom Kelly
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: We introduce Seamless Satellite-image Synthesis (SSS), a novel neural architecture to create scale-and-space continuous satellite textures from cartographic data. While 2D map data is cheap and easily synthesized, accurate satellite imagery is expensive and often unavailable or out of date. Our approach generates seamless textures over arbitrarily large spatial extents which are consistent through scale-space. To overcome tile size limitations in image-to-image translation approaches, SSS learns to remove seams between tiled images in a semantically meaningful manner. Scale-space continuity is achieved by a hierarchy of networks conditioned on style and cartographic data. Our qualitative and quantitative evaluations show that our system improves over the state-of-the-art in several key areas. We show applications to texturing procedurally generation maps and interactive satellite image manipulation.



### Versatile Learned Video Compression
- **Arxiv ID**: http://arxiv.org/abs/2111.03386v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03386v2)
- **Published**: 2021-11-05 10:50:37+00:00
- **Updated**: 2022-01-05 11:01:46+00:00
- **Authors**: Runsen Feng, Zongyu Guo, Zhizheng Zhang, Zhibo Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Learned video compression methods have demonstrated great promise in catching up with traditional video codecs in their rate-distortion (R-D) performance. However, existing learned video compression schemes are limited by the binding of the prediction mode and the fixed network framework. They are unable to support various inter prediction modes and thus inapplicable for various scenarios. In this paper, to break this limitation, we propose a versatile learned video compression (VLVC) framework that uses one model to support all possible prediction modes. Specifically, to realize versatile compression, we first build a motion compensation module that applies multiple 3D motion vector fields (i.e., voxel flows) for weighted trilinear warping in spatial-temporal space. The voxel flows convey the information of temporal reference position that helps to decouple inter prediction modes away from framework designing. Secondly, in case of multiple-reference-frame prediction, we apply a flow prediction module to predict accurate motion trajectories with unified polynomial functions. We show that the flow prediction module can largely reduce the transmission cost of voxel flows. Experimental results demonstrate that our proposed VLVC not only supports versatile compression in various settings, but also is the first end-to-end learned video compression method that outperforms the latest VVC/H.266 standard reference software in terms of MS-SSIM.



### A Deep Learning Generative Model Approach for Image Synthesis of Plant Leaves
- **Arxiv ID**: http://arxiv.org/abs/2111.03388v1
- **DOI**: 10.1371/journal.pone.0276972
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.03388v1)
- **Published**: 2021-11-05 10:53:35+00:00
- **Updated**: 2021-11-05 10:53:35+00:00
- **Authors**: Alessandro Benfenati, Davide Bolzi, Paola Causin, Roberto Oberti
- **Comment**: None
- **Journal**: None
- **Summary**: Objectives. We generate via advanced Deep Learning (DL) techniques artificial leaf images in an automatized way. We aim to dispose of a source of training samples for AI applications for modern crop management. Such applications require large amounts of data and, while leaf images are not truly scarce, image collection and annotation remains a very time--consuming process. Data scarcity can be addressed by augmentation techniques consisting in simple transformations of samples belonging to a small dataset, but the richness of the augmented data is limited: this motivates the search for alternative approaches. Methods. Pursuing an approach based on DL generative models, we propose a Leaf-to-Leaf Translation (L2L) procedure structured in two steps: first, a residual variational autoencoder architecture generates synthetic leaf skeletons (leaf profile and veins) starting from companions binarized skeletons of real images. In a second step, we perform translation via a Pix2pix framework, which uses conditional generator adversarial networks to reproduce the colorization of leaf blades, preserving the shape and the venation pattern. Results. The L2L procedure generates synthetic images of leaves with a realistic appearance. We address the performance measurement both in a qualitative and a quantitative way; for this latter evaluation, we employ a DL anomaly detection strategy which quantifies the degree of anomaly of synthetic leaves with respect to real samples. Conclusions. Generative DL approaches have the potential to be a new paradigm to provide low-cost meaningful synthetic samples for computer-aided applications. The present L2L approach represents a step towards this goal, being able to generate synthetic samples with a relevant qualitative and quantitative resemblance to real leaves.



### SSA: Semantic Structure Aware Inference for Weakly Pixel-Wise Dense Predictions without Cost
- **Arxiv ID**: http://arxiv.org/abs/2111.03392v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03392v1)
- **Published**: 2021-11-05 11:07:21+00:00
- **Updated**: 2021-11-05 11:07:21+00:00
- **Authors**: Yanpeng Sun, Zechao Li
- **Comment**: None
- **Journal**: None
- **Summary**: The pixel-wise dense prediction tasks based on weakly supervisions currently use Class Attention Maps (CAM) to generate pseudo masks as ground-truth. However, the existing methods typically depend on the painstaking training modules, which may bring in grinding computational overhead and complex training procedures. In this work, the semantic structure aware inference (SSA) is proposed to explore the semantic structure information hidden in different stages of the CNN-based network to generate high-quality CAM in the model inference. Specifically, the semantic structure modeling module (SSM) is first proposed to generate the class-agnostic semantic correlation representation, where each item denotes the affinity degree between one category of objects and all the others. Then the structured feature representation is explored to polish an immature CAM via the dot product operation. Finally, the polished CAMs from different backbone stages are fused as the output. The proposed method has the advantage of no parameters and does not need to be trained. Therefore, it can be applied to a wide range of weakly-supervised pixel-wise dense prediction tasks. Experimental results on both weakly-supervised object localization and weakly-supervised semantic segmentation tasks demonstrate the effectiveness of the proposed method, which achieves the new state-of-the-art results on these two tasks.



### A bone suppression model ensemble to improve COVID-19 detection in chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/2111.03404v2
- **DOI**: 10.1371/journal.pone.0265691
- **Categories**: **eess.IV**, cs.CV, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2111.03404v2)
- **Published**: 2021-11-05 11:27:26+00:00
- **Updated**: 2021-12-04 18:21:36+00:00
- **Authors**: Sivaramakrishnan Rajaraman, Gregg Cohen, Lillian Spear, Les folio, Sameer Antani
- **Comment**: 29 pages, 10 figures, 4 tables
- **Journal**: None
- **Summary**: Chest X-ray (CXR) is a widely performed radiology examination that helps to detect abnormalities in the tissues and organs in the thoracic cavity. Detecting pulmonary abnormalities like COVID-19 may become difficult due to that they are obscured by the presence of bony structures like the ribs and the clavicles, thereby resulting in screening/diagnostic misinterpretations. Automated bone suppression methods would help suppress these bony structures and increase soft tissue visibility. In this study, we propose to build an ensemble of convolutional neural network models to suppress bones in frontal CXRs, improve classification performance, and reduce interpretation errors related to COVID-19 detection. The ensemble is constructed by (i) measuring the multi-scale structural similarity index (MS-SSIM) score between the sub-blocks of the bone-suppressed image predicted by each of the top-3 performing bone-suppression models and the corresponding sub-blocks of its respective ground truth soft-tissue image, and (ii) performing a majority voting of the MS-SSIM score computed in each sub-block to identify the sub-block with the maximum MS-SSIM score and use it in constructing the final bone-suppressed image. We empirically determine the sub-block size that delivers superior bone suppression performance. It is observed that the bone suppression model ensemble outperformed the individual models in terms of MS-SSIM and other metrics. A CXR modality-specific classification model is retrained and evaluated on the non-bone-suppressed and bone-suppressed images to classify them as showing normal lungs or other COVID-19-like manifestations. We observed that the bone-suppressed model training significantly outperformed the model trained on non-bone-suppressed images toward detecting COVID-19 manifestations.



### MSC-VO: Exploiting Manhattan and Structural Constraints for Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/2111.03408v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2111.03408v1)
- **Published**: 2021-11-05 11:29:52+00:00
- **Updated**: 2021-11-05 11:29:52+00:00
- **Authors**: Joan P. Company-Corcoles, Emilio Garcia-Fidalgo, Alberto Ortiz
- **Comment**: Submitted to RAL + ICRA 2022
- **Journal**: None
- **Summary**: Visual odometry algorithms tend to degrade when facing low-textured scenes -from e.g. human-made environments-, where it is often difficult to find a sufficient number of point features. Alternative geometrical visual cues, such as lines, which can often be found within these scenarios, can become particularly useful. Moreover, these scenarios typically present structural regularities, such as parallelism or orthogonality, and hold the Manhattan World assumption. Under these premises, in this work, we introduce MSC-VO, an RGB-D -based visual odometry approach that combines both point and line features and leverages, if exist, those structural regularities and the Manhattan axes of the scene. Within our approach, these structural constraints are initially used to estimate accurately the 3D position of the extracted lines. These constraints are also combined next with the estimated Manhattan axes and the reprojection errors of points and lines to refine the camera pose by means of local map optimization. Such a combination enables our approach to operate even in the absence of the aforementioned constraints, allowing the method to work for a wider variety of scenarios. Furthermore, we propose a novel multi-view Manhattan axes estimation procedure that mainly relies on line features. MSC-VO is assessed using several public datasets, outperforming other state-of-the-art solutions, and comparing favourably even with some SLAM methods.



### Structure-aware Image Inpainting with Two Parallel Streams
- **Arxiv ID**: http://arxiv.org/abs/2111.03414v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.03414v1)
- **Published**: 2021-11-05 11:34:00+00:00
- **Updated**: 2021-11-05 11:34:00+00:00
- **Authors**: Zhilin Huang, Chujun Qin, Ruixin Liu, Zhenyu Weng, Yuesheng Zhu
- **Comment**: 9 pages, 8 figures, rejected by IJCAI 2021
- **Journal**: None
- **Summary**: Recent works in image inpainting have shown that structural information plays an important role in recovering visually pleasing results. In this paper, we propose an end-to-end architecture composed of two parallel UNet-based streams: a main stream (MS) and a structure stream (SS). With the assistance of SS, MS can produce plausible results with reasonable structures and realistic details. Specifically, MS reconstructs detailed images by inferring missing structures and textures simultaneously, and SS restores only missing structures by processing the hierarchical information from the encoder of MS. By interacting with SS in the training process, MS can be implicitly encouraged to exploit structural cues. In order to help SS focus on structures and prevent textures in MS from being affected, a gated unit is proposed to depress structure-irrelevant activations in the information flow between MS and SS. Furthermore, the multi-scale structure feature maps in SS are utilized to explicitly guide the structure-reasonable image reconstruction in the decoder of MS through the fusion block. Extensive experiments on CelebA, Paris StreetView and Places2 datasets demonstrate that our proposed method outperforms state-of-the-art methods.



### Sampling Equivariant Self-attention Networks for Object Detection in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2111.03420v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03420v1)
- **Published**: 2021-11-05 11:48:04+00:00
- **Updated**: 2021-11-05 11:48:04+00:00
- **Authors**: Guo-Ye Yang, Xiang-Li Li, Ralph R. Martin, Shi-Min Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Objects in aerial images have greater variations in scale and orientation than in typical images, so detection is more difficult. Convolutional neural networks use a variety of frequency- and orientation-specific kernels to identify objects subject to different transformations; these require many parameters. Sampling equivariant networks can adjust sampling from input feature maps according to the transformation of the object, allowing a kernel to extract features of an object under different transformations. Doing so requires fewer parameters, and makes the network more suitable for representing deformable objects, like those in aerial images. However, methods like deformable convolutional networks can only provide sampling equivariance under certain circumstances, because of the locations used for sampling. We propose sampling equivariant self-attention networks which consider self-attention restricted to a local image patch as convolution sampling with masks instead of locations, and design a transformation embedding module to further improve the equivariant sampling ability. We also use a novel randomized normalization module to tackle overfitting due to limited aerial image data. We show that our model (i) provides significantly better sampling equivariance than existing methods, without additional supervision, (ii) provides improved classification on ImageNet, and (iii) achieves state-of-the-art results on the DOTA dataset, without increased computation.



### Solving Traffic4Cast Competition with U-Net and Temporal Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2111.03421v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.03421v1)
- **Published**: 2021-11-05 11:49:52+00:00
- **Updated**: 2021-11-05 11:49:52+00:00
- **Authors**: Vsevolod Konyakhin, Nina Lukashina, Aleksei Shpilman
- **Comment**: Conference on Neural Information Processing Systems (NeurIPS 2021)
  Traffic4cast Competition
- **Journal**: None
- **Summary**: In this technical report, we present our solution to the Traffic4Cast 2021 Core Challenge, in which participants were asked to develop algorithms for predicting a traffic state 60 minutes ahead, based on the information from the previous hour, in 4 different cities. In contrast to the previously held competitions, this year's challenge focuses on the temporal domain shift in traffic due to the COVID-19 pandemic. Following the past success of U-Net, we utilize it for predicting future traffic maps. Additionally, we explore the usage of pre-trained encoders such as DenseNet and EfficientNet and employ multiple domain adaptation techniques to fight the domain shift. Our solution has ranked third in the final competition. The code is available at https://github.com/jbr-ai-labs/traffic4cast-2021.



### DriveGuard: Robustification of Automated Driving Systems with Deep Spatio-Temporal Convolutional Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2111.03480v1
- **DOI**: 10.1109/WACVW52041.2021.00016
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2111.03480v1)
- **Published**: 2021-11-05 12:57:38+00:00
- **Updated**: 2021-11-05 12:57:38+00:00
- **Authors**: Andreas Papachristodoulou, Christos Kyrkou, Theocharis Theocharides
- **Comment**: 2021 IEEE Winter Conference on Applications of Computer Vision
  Workshops (WACVW)
- **Journal**: None
- **Summary**: Autonomous vehicles increasingly rely on cameras to provide the input for perception and scene understanding and the ability of these models to classify their environment and objects, under adverse conditions and image noise is crucial. When the input is, either unintentionally or through targeted attacks, deteriorated, the reliability of autonomous vehicle is compromised. In order to mitigate such phenomena, we propose DriveGuard, a lightweight spatio-temporal autoencoder, as a solution to robustify the image segmentation process for autonomous vehicles. By first processing camera images with DriveGuard, we offer a more universal solution than having to re-train each perception model with noisy input. We explore the space of different autoencoder architectures and evaluate them on a diverse dataset created with real and synthetic images demonstrating that by exploiting spatio-temporal information combined with multi-component loss we significantly increase robustness against adverse image effects reaching within 5-6% of that of the original model on clean images.



### Improving Visual Quality of Image Synthesis by A Token-based Generator with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2111.03481v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03481v2)
- **Published**: 2021-11-05 12:57:50+00:00
- **Updated**: 2021-12-18 14:10:48+00:00
- **Authors**: Yanhong Zeng, Huan Yang, Hongyang Chao, Jianbo Wang, Jianlong Fu
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: We present a new perspective of achieving image synthesis by viewing this task as a visual token generation problem. Different from existing paradigms that directly synthesize a full image from a single input (e.g., a latent code), the new formulation enables a flexible local manipulation for different image regions, which makes it possible to learn content-aware and fine-grained style control for image synthesis. Specifically, it takes as input a sequence of latent tokens to predict the visual tokens for synthesizing an image. Under this perspective, we propose a token-based generator (i.e.,TokenGAN). Particularly, the TokenGAN inputs two semantically different visual tokens, i.e., the learned constant content tokens and the style tokens from the latent space. Given a sequence of style tokens, the TokenGAN is able to control the image synthesis by assigning the styles to the content tokens by attention mechanism with a Transformer. We conduct extensive experiments and show that the proposed TokenGAN has achieved state-of-the-art results on several widely-used image synthesis benchmarks, including FFHQ and LSUN CHURCH with different resolutions. In particular, the generator is able to synthesize high-fidelity images with 1024x1024 size, dispensing with convolutions entirely.



### Event-based Motion Segmentation by Cascaded Two-Level Multi-Model Fitting
- **Arxiv ID**: http://arxiv.org/abs/2111.03483v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2111.03483v1)
- **Published**: 2021-11-05 12:59:41+00:00
- **Updated**: 2021-11-05 12:59:41+00:00
- **Authors**: Xiuyuan Lu, Yi Zhou, Shaojie Shen
- **Comment**: Accepted for presentation at the 2021 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2021)
- **Journal**: None
- **Summary**: Among prerequisites for a synthetic agent to interact with dynamic scenes, the ability to identify independently moving objects is specifically important. From an application perspective, nevertheless, standard cameras may deteriorate remarkably under aggressive motion and challenging illumination conditions. In contrast, event-based cameras, as a category of novel biologically inspired sensors, deliver advantages to deal with these challenges. Its rapid response and asynchronous nature enables it to capture visual stimuli at exactly the same rate of the scene dynamics. In this paper, we present a cascaded two-level multi-model fitting method for identifying independently moving objects (i.e., the motion segmentation problem) with a monocular event camera. The first level leverages tracking of event features and solves the feature clustering problem under a progressive multi-model fitting scheme. Initialized with the resulting motion model instances, the second level further addresses the event clustering problem using a spatio-temporal graph-cut method. This combination leads to efficient and accurate event-wise motion segmentation that cannot be achieved by any of them alone. Experiments demonstrate the effectiveness and versatility of our method in real-world scenes with different motion patterns and an unknown number of independently moving objects.



### Cross Modality 3D Navigation Using Reinforcement Learning and Neural Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2111.03485v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03485v1)
- **Published**: 2021-11-05 13:11:45+00:00
- **Updated**: 2021-11-05 13:11:45+00:00
- **Authors**: Cesare Magnetti, Hadrien Reynaud, Bernhard Kainz
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents the use of Multi-Agent Reinforcement Learning (MARL) to perform navigation in 3D anatomical volumes from medical imaging. We utilize Neural Style Transfer to create synthetic Computed Tomography (CT) agent gym environments and assess the generalization capabilities of our agents to clinical CT volumes. Our framework does not require any labelled clinical data and integrates easily with several image translation techniques, enabling cross modality applications. Further, we solely condition our agents on 2D slices, breaking grounds for 3D guidance in much more difficult imaging modalities, such as ultrasound imaging. This is an important step towards user guidance during the acquisition of standardised diagnostic view planes, improving diagnostic consistency and facilitating better case comparison.



### Visualizing the Emergence of Intermediate Visual Patterns in DNNs
- **Arxiv ID**: http://arxiv.org/abs/2111.03505v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.03505v1)
- **Published**: 2021-11-05 13:49:39+00:00
- **Updated**: 2021-11-05 13:49:39+00:00
- **Authors**: Mingjie Li, Shaobo Wang, Quanshi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a method to visualize the discrimination power of intermediate-layer visual patterns encoded by a DNN. Specifically, we visualize (1) how the DNN gradually learns regional visual patterns in each intermediate layer during the training process, and (2) the effects of the DNN using non-discriminative patterns in low layers to construct disciminative patterns in middle/high layers through the forward propagation. Based on our visualization method, we can quantify knowledge points (i.e., the number of discriminative visual patterns) learned by the DNN to evaluate the representation capacity of the DNN. Furthermore, this method also provides new insights into signal-processing behaviors of existing deep-learning techniques, such as adversarial attacks and knowledge distillation.



### First steps on Gamification of Lung Fluid Cells Annotations in the Flower Domain
- **Arxiv ID**: http://arxiv.org/abs/2111.03663v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.03663v2)
- **Published**: 2021-11-05 14:11:38+00:00
- **Updated**: 2022-01-17 14:41:34+00:00
- **Authors**: Sonja Kunzmann, Christian Marzahl, Felix Denzinger, Christof A. Bertram, Robert Klopfleisch, Katharina Breininger, Vincent Christlein, Andreas Maier
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: Annotating data, especially in the medical domain, requires expert knowledge and a lot of effort. This limits the amount and/or usefulness of available medical data sets for experimentation. Therefore, developing strategies to increase the number of annotations while lowering the needed domain knowledge is of interest. A possible strategy is the use of gamification, i.e. transforming the annotation task into a game. We propose an approach to gamify the task of annotating lung fluid cells from pathological whole slide images (WSIs). As the domain is unknown to non-expert annotators, we transform images of cells to the domain of flower images using a CycleGAN architecture. In this more assessable domain, non-expert annotators can be (t)asked to annotate different kinds of flowers in a playful setting. In order to provide a proof of concept, this work shows that the domain transfer is possible by evaluating an image classification network trained on real cell images and tested on the cell images generated by the CycleGAN network (reconstructed cell images) as well as real cell images. The classification network reaches an average accuracy of 94.73 % on the original lung fluid cells and 95.25 % on the transformed lung fluid cells, respectively. Our study lays the foundation for future research on gamification using CycleGANs.



### Semantically Consistent Image-to-Image Translation for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2111.03522v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03522v2)
- **Published**: 2021-11-05 14:22:20+00:00
- **Updated**: 2021-11-25 09:33:05+00:00
- **Authors**: Stephan Brehm, Sebastian Scherer, Rainer Lienhart
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (UDA) aims to adapt models trained on a source domain to a new target domain where no labelled data is available. In this work, we investigate the problem of UDA from a synthetic computer-generated domain to a similar but real-world domain for learning semantic segmentation. We propose a semantically consistent image-to-image translation method in combination with a consistency regularisation method for UDA. We overcome previous limitations on transferring synthetic images to real looking images. We leverage pseudo-labels in order to learn a generative image-to-image translation model that receives additional feedback from semantic labels on both domains. Our method outperforms state-of-the-art methods that combine image-to-image translation and semi-supervised learning on relevant domain adaptation benchmarks, i.e., on GTA5 to Cityscapes and SYNTHIA to Cityscapes.



### Joint Learning of Visual-Audio Saliency Prediction and Sound Source Localization on Multi-face Videos
- **Arxiv ID**: http://arxiv.org/abs/2111.08567v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08567v1)
- **Published**: 2021-11-05 14:35:08+00:00
- **Updated**: 2021-11-05 14:35:08+00:00
- **Authors**: Minglang Qiao, Yufan Liu, Mai Xu, Xin Deng, Bing Li, Weiming Hu, Ali Borji
- **Comment**: 21 pages, 15 figures
- **Journal**: None
- **Summary**: Visual and audio events simultaneously occur and both attract attention. However, most existing saliency prediction works ignore the influence of audio and only consider vision modality. In this paper, we propose a multitask learning method for visual-audio saliency prediction and sound source localization on multi-face video by leveraging visual, audio and face information. Specifically, we first introduce a large-scale database of multi-face video in visual-audio condition (MVVA), containing eye-tracking data and sound source annotations. Using this database, we find that sound influences human attention, and conversly attention offers a cue to determine sound source on multi-face video. Guided by these findings, a visual-audio multi-task network (VAM-Net) is introduced to predict saliency and locate sound source. VAM-Net consists of three branches corresponding to visual, audio and face modalities. Visual branch has a two-stream architecture to capture spatial and temporal information. Face and audio branches encode audio signals and faces, respectively. Finally, a spatio-temporal multi-modal graph (STMG) is constructed to model the interaction among multiple faces. With joint optimization of these branches, the intrinsic correlation of the tasks of saliency prediction and sound source localization is utilized and their performance is boosted by each other. Experiments show that the proposed method outperforms 12 state-of-the-art saliency prediction methods, and achieves competitive results in sound source localization.



### A Unified Game-Theoretic Interpretation of Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2111.03536v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03536v2)
- **Published**: 2021-11-05 14:57:49+00:00
- **Updated**: 2021-11-08 05:26:14+00:00
- **Authors**: Jie Ren, Die Zhang, Yisen Wang, Lu Chen, Zhanpeng Zhou, Yiting Chen, Xu Cheng, Xin Wang, Meng Zhou, Jie Shi, Quanshi Zhang
- **Comment**: the previous version is arXiv:2103.07364, but I mistakenly apply a
  new ID for the paper
- **Journal**: None
- **Summary**: This paper provides a unified view to explain different adversarial attacks and defense methods, \emph{i.e.} the view of multi-order interactions between input variables of DNNs. Based on the multi-order interaction, we discover that adversarial attacks mainly affect high-order interactions to fool the DNN. Furthermore, we find that the robustness of adversarially trained DNNs comes from category-specific low-order interactions. Our findings provide a potential method to unify adversarial perturbations and robustness, which can explain the existing defense methods in a principle way. Besides, our findings also make a revision of previous inaccurate understanding of the shape bias of adversarially learned features.



### Interpreting Representation Quality of DNNs for 3D Point Cloud Processing
- **Arxiv ID**: http://arxiv.org/abs/2111.03549v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.03549v1)
- **Published**: 2021-11-05 15:14:34+00:00
- **Updated**: 2021-11-05 15:14:34+00:00
- **Authors**: Wen Shen, Qihan Ren, Dongrui Liu, Quanshi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we evaluate the quality of knowledge representations encoded in deep neural networks (DNNs) for 3D point cloud processing. We propose a method to disentangle the overall model vulnerability into the sensitivity to the rotation, the translation, the scale, and local 3D structures. Besides, we also propose metrics to evaluate the spatial smoothness of encoding 3D structures, and the representation complexity of the DNN. Based on such analysis, experiments expose representation problems with classic DNNs, and explain the utility of the adversarial training.



### SmartDepthSync: Open Source Synchronized Video Recording System of Smartphone RGB and Depth Camera Range Image Frames with Sub-millisecond Precision
- **Arxiv ID**: http://arxiv.org/abs/2111.03552v3
- **DOI**: 10.1109/JSEN.2022.3150973
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2111.03552v3)
- **Published**: 2021-11-05 15:16:54+00:00
- **Updated**: 2022-09-13 12:10:30+00:00
- **Authors**: Marsel Faizullin, Anastasiia Kornilova, Azat Akhmetyanov, Konstantin Pakulev, Andrey Sadkov, Gonzalo Ferrer
- **Comment**: IEEE Sensors Journal paper
- **Journal**: None
- **Summary**: Nowadays, smartphones can produce a synchronized (synced) stream of high-quality data, including RGB images, inertial measurements, and other data. Therefore, smartphones are becoming appealing sensor systems in the robotics community. Unfortunately, there is still the need for external supporting sensing hardware, such as a depth camera precisely synced with the smartphone sensors.   In this paper, we propose a hardware-software recording system that presents a heterogeneous structure and contains a smartphone and an external depth camera for recording visual, depth, and inertial data that are mutually synchronized. The system is synced at the time and the frame levels: every RGB image frame from the smartphone camera is exposed at the same moment of time with a depth camera frame with sub-millisecond precision. We provide a method and a tool for sync performance evaluation that can be applied to any pair of depth and RGB cameras. Our system could be replicated, modified, or extended by employing our open-sourced materials.



### Spatial-Temporal Residual Aggregation for High Resolution Video Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2111.03574v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03574v1)
- **Published**: 2021-11-05 15:50:31+00:00
- **Updated**: 2021-11-05 15:50:31+00:00
- **Authors**: Vishnu Sanjay Ramiya Srinivasan, Rui Ma, Qiang Tang, Zili Yi, Zhan Xu
- **Comment**: Accepted by BMVC 2021. Project page:
  https://github.com/Ascend-Research/STRA_Net
- **Journal**: None
- **Summary**: Recent learning-based inpainting algorithms have achieved compelling results for completing missing regions after removing undesired objects in videos. To maintain the temporal consistency among the frames, 3D spatial and temporal operations are often heavily used in the deep networks. However, these methods usually suffer from memory constraints and can only handle low resolution videos. We propose STRA-Net, a novel spatial-temporal residual aggregation framework for high resolution video inpainting. The key idea is to first learn and apply a spatial and temporal inpainting network on the downsampled low resolution videos. Then, we refine the low resolution results by aggregating the learned spatial and temporal image residuals (details) to the upsampled inpainted frames. Both the quantitative and qualitative evaluations show that we can produce more temporal-coherent and visually appealing results than the state-of-the-art methods on inpainting high resolution videos.



### AGPCNet: Attention-Guided Pyramid Context Networks for Infrared Small Target Detection
- **Arxiv ID**: http://arxiv.org/abs/2111.03580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03580v1)
- **Published**: 2021-11-05 16:01:31+00:00
- **Updated**: 2021-11-05 16:01:31+00:00
- **Authors**: Tianfang Zhang, Siying Cao, Tian Pu, Zhenming Peng
- **Comment**: 12 pages, 13 figures, 8 tables
- **Journal**: None
- **Summary**: Infrared small target detection is an important problem in many fields such as earth observation, military reconnaissance, disaster relief, and has received widespread attention recently. This paper presents the Attention-Guided Pyramid Context Network (AGPCNet) algorithm. Its main components are an Attention-Guided Context Block (AGCB), a Context Pyramid Module (CPM), and an Asymmetric Fusion Module (AFM). AGCB divides the feature map into patches to compute local associations and uses Global Context Attention (GCA) to compute global associations between semantics, CPM integrates features from multi-scale AGCBs, and AFM integrates low-level and deep-level semantics from a feature-fusion perspective to enhance the utilization of features. The experimental results illustrate that AGPCNet has achieved new state-of-the-art performance on two available infrared small target datasets. The source codes are available at https://github.com/Tianfang-Zhang/AGPCNet.



### Edge Tracing using Gaussian Process Regression
- **Arxiv ID**: http://arxiv.org/abs/2111.03605v1
- **DOI**: 10.1109/TIP.2021.3128329
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03605v1)
- **Published**: 2021-11-05 16:43:14+00:00
- **Updated**: 2021-11-05 16:43:14+00:00
- **Authors**: Jamie Burke, Stuart King
- **Comment**: 15 pages, 6 figures. Accepted to be published in IEEE Transactions on
  Image Processing. Github repository:
  https://github.com/jaburke166/gaussian_process_edge_trace
- **Journal**: None
- **Summary**: We introduce a novel edge tracing algorithm using Gaussian process regression. Our edge-based segmentation algorithm models an edge of interest using Gaussian process regression and iteratively searches the image for edge pixels in a recursive Bayesian scheme. This procedure combines local edge information from the image gradient and global structural information from posterior curves, sampled from the model's posterior predictive distribution, to sequentially build and refine an observation set of edge pixels. This accumulation of pixels converges the distribution to the edge of interest. Hyperparameters can be tuned by the user at initialisation and optimised given the refined observation set. This tunable approach does not require any prior training and is not restricted to any particular type of imaging domain. Due to the model's uncertainty quantification, the algorithm is robust to artefacts and occlusions which degrade the quality and continuity of edges in images. Our approach also has the ability to efficiently trace edges in image sequences by using previous-image edge traces as a priori information for consecutive images. Various applications to medical imaging and satellite imaging are used to validate the technique and comparisons are made with two commonly used edge tracing algorithms.



### Single Image Deraining Network with Rain Embedding Consistency and Layered LSTM
- **Arxiv ID**: http://arxiv.org/abs/2111.03615v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03615v1)
- **Published**: 2021-11-05 17:03:08+00:00
- **Updated**: 2021-11-05 17:03:08+00:00
- **Authors**: Yizhou Li, Yusuke Monno, Masatoshi Okutomi
- **Comment**: Accepted by WACV2022, January 2022
- **Journal**: None
- **Summary**: Single image deraining is typically addressed as residual learning to predict the rain layer from an input rainy image. For this purpose, an encoder-decoder network draws wide attention, where the encoder is required to encode a high-quality rain embedding which determines the performance of the subsequent decoding stage to reconstruct the rain layer. However, most of existing studies ignore the significance of rain embedding quality, thus leading to limited performance with over/under-deraining. In this paper, with our observation of the high rain layer reconstruction performance by an rain-to-rain autoencoder, we introduce the idea of "Rain Embedding Consistency" by regarding the encoded embedding by the autoencoder as an ideal rain embedding and aim at enhancing the deraining performance by improving the consistency between the ideal rain embedding and the rain embedding derived by the encoder of the deraining network. To achieve this, a Rain Embedding Loss is applied to directly supervise the encoding process, with a Rectified Local Contrast Normalization (RLCN) as the guide that effectively extracts the candidate rain pixels. We also propose Layered LSTM for recurrent deraining and fine-grained encoder feature refinement considering different scales. Qualitative and quantitative experiments demonstrate that our proposed method outperforms previous state-of-the-art methods particularly on a real-world dataset. Our source code is available at http://www.ok.sc.e.titech.ac.jp/res/SIR/.



### BBC-Oxford British Sign Language Dataset
- **Arxiv ID**: http://arxiv.org/abs/2111.03635v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03635v1)
- **Published**: 2021-11-05 17:35:58+00:00
- **Updated**: 2021-11-05 17:35:58+00:00
- **Authors**: Samuel Albanie, GÃ¼l Varol, Liliane Momeni, Hannah Bull, Triantafyllos Afouras, Himel Chowdhury, Neil Fox, Bencie Woll, Rob Cooper, Andrew McParland, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we introduce the BBC-Oxford British Sign Language (BOBSL) dataset, a large-scale video collection of British Sign Language (BSL). BOBSL is an extended and publicly released dataset based on the BSL-1K dataset introduced in previous work. We describe the motivation for the dataset, together with statistics and available annotations. We conduct experiments to provide baselines for the tasks of sign recognition, sign language alignment, and sign language translation. Finally, we describe several strengths and limitations of the data from the perspectives of machine learning and linguistics, note sources of bias present in the dataset, and discuss potential applications of BOBSL in the context of sign language technology. The dataset is available at https://www.robots.ox.ac.uk/~vgg/data/bobsl/.



### TermiNeRF: Ray Termination Prediction for Efficient Neural Rendering
- **Arxiv ID**: http://arxiv.org/abs/2111.03643v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2111.03643v1)
- **Published**: 2021-11-05 17:50:44+00:00
- **Updated**: 2021-11-05 17:50:44+00:00
- **Authors**: Martin Piala, Ronald Clark
- **Comment**: 3DV 2021; Project page with videos:
  https://projects.mackopes.com/terminerf
- **Journal**: None
- **Summary**: Volume rendering using neural fields has shown great promise in capturing and synthesizing novel views of 3D scenes. However, this type of approach requires querying the volume network at multiple points along each viewing ray in order to render an image, resulting in very slow rendering times. In this paper, we present a method that overcomes this limitation by learning a direct mapping from camera rays to locations along the ray that are most likely to influence the pixel's final appearance. Using this approach we are able to render, train and fine-tune a volumetrically-rendered neural field model an order of magnitude faster than standard approaches. Unlike existing methods, our approach works with general volumes and can be trained end-to-end.



### Normalizing Flow as a Flexible Fidelity Objective for Photo-Realistic Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2111.03649v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03649v1)
- **Published**: 2021-11-05 17:56:51+00:00
- **Updated**: 2021-11-05 17:56:51+00:00
- **Authors**: Andreas Lugmayr, Martin Danelljan, Fisher Yu, Luc Van Gool, Radu Timofte
- **Comment**: None
- **Journal**: WACV 2022
- **Summary**: Super-resolution is an ill-posed problem, where a ground-truth high-resolution image represents only one possibility in the space of plausible solutions. Yet, the dominant paradigm is to employ pixel-wise losses, such as L_1, which drive the prediction towards a blurry average. This leads to fundamentally conflicting objectives when combined with adversarial losses, which degrades the final quality. We address this issue by revisiting the L_1 loss and show that it corresponds to a one-layer conditional flow. Inspired by this relation, we explore general flows as a fidelity-based alternative to the L_1 objective. We demonstrate that the flexibility of deeper flows leads to better visual quality and consistency when combined with adversarial losses. We conduct extensive user studies for three datasets and scale factors, where our approach is shown to outperform state-of-the-art methods for photo-realistic super-resolution. Code and trained models will be available at:   git.io/AdFlow



### The Curious Layperson: Fine-Grained Image Recognition without Expert Labels
- **Arxiv ID**: http://arxiv.org/abs/2111.03651v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2111.03651v1)
- **Published**: 2021-11-05 17:58:37+00:00
- **Updated**: 2021-11-05 17:58:37+00:00
- **Authors**: Subhabrata Choudhury, Iro Laina, Christian Rupprecht, Andrea Vedaldi
- **Comment**: To appear in BMVC 2021 (Oral). Project page:
  https://www.robots.ox.ac.uk/~vgg/research/clever/
- **Journal**: None
- **Summary**: Most of us are not experts in specific fields, such as ornithology. Nonetheless, we do have general image and language understanding capabilities that we use to match what we see to expert resources. This allows us to expand our knowledge and perform novel tasks without ad-hoc external supervision. On the contrary, machines have a much harder time consulting expert-curated knowledge bases unless trained specifically with that knowledge in mind. Thus, in this paper we consider a new problem: fine-grained image recognition without expert annotations, which we address by leveraging the vast knowledge available in web encyclopedias. First, we learn a model to describe the visual appearance of objects using non-expert image descriptions. We then train a fine-grained textual similarity model that matches image descriptions with documents on a sentence-level basis. We evaluate the method on two datasets and compare with several strong baselines and the state of the art in cross-modal retrieval. Code is available at: https://github.com/subhc/clever



### Do we still need ImageNet pre-training in remote sensing scene classification?
- **Arxiv ID**: http://arxiv.org/abs/2111.03690v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03690v3)
- **Published**: 2021-11-05 18:30:54+00:00
- **Updated**: 2022-05-25 16:21:35+00:00
- **Authors**: Vladimir RisojeviÄ, Vladan StojniÄ
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the scarcity of labeled data, using supervised models pre-trained on ImageNet is a de facto standard in remote sensing scene classification. Recently, the availability of larger high resolution remote sensing (HRRS) image datasets and progress in self-supervised learning have brought up the questions of whether supervised ImageNet pre-training is still necessary for remote sensing scene classification and would supervised pre-training on HRRS image datasets or self-supervised pre-training on ImageNet achieve better results on target remote sensing scene classification tasks. To answer these questions, in this paper we both train models from scratch and fine-tune supervised and self-supervised ImageNet models on several HRRS image datasets. We also evaluate the transferability of learned representations to HRRS scene classification tasks and show that self-supervised pre-training outperforms the supervised one, while the performance of HRRS pre-training is similar to self-supervised pre-training or slightly lower. Finally, we propose using an ImageNet pre-trained model combined with a second round of pre-training using in-domain HRRS images, i.e. domain-adaptive pre-training. The experimental results show that domain-adaptive pre-training results in models that achieve state-of-the-art results on HRRS scene classification benchmarks. The source code and pre-trained models are available at \url{https://github.com/risojevicv/RSSC-transfer}.



### Disaster mapping from satellites: damage detection with crowdsourced point labels
- **Arxiv ID**: http://arxiv.org/abs/2111.03693v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03693v1)
- **Published**: 2021-11-05 18:32:22+00:00
- **Updated**: 2021-11-05 18:32:22+00:00
- **Authors**: Danil Kuzin, Olga Isupova, Brooke D. Simmons, Steven Reece
- **Comment**: 3rd Workshop on Artificial Intelligence for Humanitarian Assistance
  and Disaster Response at NeurIPS 2021
- **Journal**: None
- **Summary**: High-resolution satellite imagery available immediately after disaster events is crucial for response planning as it facilitates broad situational awareness of critical infrastructure status such as building damage, flooding, and obstructions to access routes. Damage mapping at this scale would require hundreds of expert person-hours. However, a combination of crowdsourcing and recent advances in deep learning reduces the effort needed to just a few hours in real time. Asking volunteers to place point marks, as opposed to shapes of actual damaged areas, significantly decreases the required analysis time for response during the disaster. However, different volunteers may be inconsistent in their marking. This work presents methods for aggregating potentially inconsistent damage marks to train a neural network damage detector.



### Reconstructing Training Data from Diverse ML Models by Ensemble Inversion
- **Arxiv ID**: http://arxiv.org/abs/2111.03702v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03702v1)
- **Published**: 2021-11-05 18:59:01+00:00
- **Updated**: 2021-11-05 18:59:01+00:00
- **Authors**: Qian Wang, Daniel Kurz
- **Comment**: 9 pages, 8 figures, WACV 2022
- **Journal**: None
- **Summary**: Model Inversion (MI), in which an adversary abuses access to a trained Machine Learning (ML) model attempting to infer sensitive information about its original training data, has attracted increasing research attention. During MI, the trained model under attack (MUA) is usually frozen and used to guide the training of a generator, such as a Generative Adversarial Network (GAN), to reconstruct the distribution of the original training data of that model. This might cause leakage of original training samples, and if successful, the privacy of dataset subjects will be at risk if the training data contains Personally Identifiable Information (PII). Therefore, an in-depth investigation of the potentials of MI techniques is crucial for the development of corresponding defense techniques. High-quality reconstruction of training data based on a single model is challenging. However, existing MI literature does not explore targeting multiple models jointly, which may provide additional information and diverse perspectives to the adversary.   We propose the ensemble inversion technique that estimates the distribution of original training data by training a generator constrained by an ensemble (or set) of trained models with shared subjects or entities. This technique leads to noticeable improvements of the quality of the generated samples with distinguishable features of the dataset entities compared to MI of a single ML model. We achieve high quality results without any dataset and show how utilizing an auxiliary dataset that's similar to the presumed training data improves the results. The impact of model diversity in the ensemble is thoroughly investigated and additional constraints are utilized to encourage sharp predictions and high activations for the reconstructed samples, leading to more accurate reconstruction of training images.



### Damage Estimation and Localization from Sparse Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/2111.03708v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.03708v2)
- **Published**: 2021-11-05 19:12:15+00:00
- **Updated**: 2021-11-10 22:08:15+00:00
- **Authors**: Rene Garcia Franceschini, Jeffrey Liu, Saurabh Amin
- **Comment**: Version presented at NeurIPS 2021 AI+HADR workshop
- **Journal**: None
- **Summary**: Aerial images provide important situational awareness for responding to natural disasters such as hurricanes. They are well-suited for providing information for damage estimation and localization (DEL); i.e., characterizing the type and spatial extent of damage following a disaster. Despite recent advances in sensing and unmanned aerial systems technology, much of post-disaster aerial imagery is still taken by handheld DSLR cameras from small, manned, fixed-wing aircraft. However, these handheld cameras lack IMU information, and images are taken opportunistically post-event by operators. As such, DEL from such imagery is still a highly manual and time-consuming process. We propose an approach to both detect damage in aerial images and localize it in world coordinates, with specific focus on detecting and localizing flooding. The approach is based on using structure from motion to relate image coordinates to world coordinates via a projective transformation, using class activation mapping to detect the extent of damage in an image, and applying the projective transformation to localize damage in world coordinates. We evaluate the performance of our approach on post-event data from the 2016 Louisiana floods, and find that our approach achieves a precision of 88%. Given this high precision using limited data, we argue that this approach is currently viable for fast and effective DEL from handheld aerial imagery for disaster response.



### Explaining neural network predictions of material strength
- **Arxiv ID**: http://arxiv.org/abs/2111.03729v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.03729v1)
- **Published**: 2021-11-05 21:09:54+00:00
- **Updated**: 2021-11-05 21:09:54+00:00
- **Authors**: Ian A. Palmer, T. Nathan Mundhenk, Brian Gallagher, Yong Han
- **Comment**: None
- **Journal**: None
- **Summary**: We recently developed a deep learning method that can determine the critical peak stress of a material by looking at scanning electron microscope (SEM) images of the material's crystals. However, it has been somewhat unclear what kind of image features the network is keying off of when it makes its prediction. It is common in computer vision to employ an explainable AI saliency map to tell one what parts of an image are important to the network's decision. One can usually deduce the important features by looking at these salient locations. However, SEM images of crystals are more abstract to the human observer than natural image photographs. As a result, it is not easy to tell what features are important at the locations which are most salient. To solve this, we developed a method that helps us map features from important locations in SEM images to non-abstract textures that are easier to interpret.



### MQBench: Towards Reproducible and Deployable Model Quantization Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2111.03759v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03759v2)
- **Published**: 2021-11-05 23:38:44+00:00
- **Updated**: 2022-01-25 17:15:43+00:00
- **Authors**: Yuhang Li, Mingzhu Shen, Jian Ma, Yan Ren, Mingxin Zhao, Qi Zhang, Ruihao Gong, Fengwei Yu, Junjie Yan
- **Comment**: Accepted by 35th Conference on Neural Information Processing Systems
  (NeurIPS 2021) Track on Datasets and Benchmarks
- **Journal**: None
- **Summary**: Model quantization has emerged as an indispensable technique to accelerate deep learning inference. While researchers continue to push the frontier of quantization algorithms, existing quantization work is often unreproducible and undeployable. This is because researchers do not choose consistent training pipelines and ignore the requirements for hardware deployments. In this work, we propose Model Quantization Benchmark (MQBench), a first attempt to evaluate, analyze, and benchmark the reproducibility and deployability for model quantization algorithms. We choose multiple different platforms for real-world deployments, including CPU, GPU, ASIC, DSP, and evaluate extensive state-of-the-art quantization algorithms under a unified training pipeline. MQBench acts like a bridge to connect the algorithm and the hardware. We conduct a comprehensive analysis and find considerable intuitive or counter-intuitive insights. By aligning the training settings, we find existing algorithms have about the same performance on the conventional academic track. While for the hardware-deployable quantization, there is a huge accuracy gap which remains unsettled. Surprisingly, no existing algorithm wins every challenge in MQBench, and we hope this work could inspire future research directions.



