# Arxiv Papers in cs.CV on 2021-11-28
### Unsupervised Domain Adaptive Person Re-Identification via Human Learning Imitation
- **Arxiv ID**: http://arxiv.org/abs/2111.14014v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.14014v2)
- **Published**: 2021-11-28 01:14:29+00:00
- **Updated**: 2021-12-05 07:16:14+00:00
- **Authors**: Yang Peng, Ping Liu, Yawei Luo, Pan Zhou, Zichuan Xu, Jingen Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptive person re-identification has received significant attention due to its high practical value. In past years, by following the clustering and finetuning paradigm, researchers propose to utilize the teacher-student framework in their methods to decrease the domain gap between different person re-identification datasets. Inspired by recent teacher-student framework based methods, which try to mimic the human learning process either by making the student directly copy behavior from the teacher or selecting reliable learning materials, we propose to conduct further exploration to imitate the human learning process from different aspects, \textit{i.e.}, adaptively updating learning materials, selectively imitating teacher behaviors, and analyzing learning materials structures. The explored three components, collaborate together to constitute a new method for unsupervised domain adaptive person re-identification, which is called Human Learning Imitation framework. The experimental results on three benchmark datasets demonstrate the efficacy of our proposed method.



### AI-supported Framework of Semi-Automatic Monoplotting for Monocular Oblique Visual Data Analysis
- **Arxiv ID**: http://arxiv.org/abs/2111.14021v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.14021v1)
- **Published**: 2021-11-28 02:03:43+00:00
- **Updated**: 2021-11-28 02:03:43+00:00
- **Authors**: Behzad Golparvar, Ruo-Qian Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In the last decades, the development of smartphones, drones, aerial patrols, and digital cameras enabled high-quality photographs available to large populations and, thus, provides an opportunity to collect massive data of the nature and society with global coverage. However, the data collected with new photography tools is usually oblique - they are difficult to be georeferenced, and huge amounts of data is often obsolete. Georeferencing oblique imagery data may be solved by a technique called monoplotting, which only requires a single image and Digital Elevation Model (DEM). In traditional monoplotting, a human user has to manually choose a series of ground control point (GCP) pairs in the image and DEM and then determine the extrinsic and intrinsic parameters of the camera to establish a pixel-level correspondence between photos and the DEM to enable the mapping and georeferencing of objects in photos. This traditional method is difficult to scale due to several challenges including the labor-intensive inputs, the need of rich experience to identify well-defined GCPs, and limitations in camera pose estimation. Therefore, existing monoplotting methods are rarely used in analyzing large-scale databases or near-real-time warning systems. In this paper, we propose and demonstrate a novel semi-automatic monoplotting framework that provides pixel-level correspondence between photos and DEMs requiring minimal human interventions. A pipeline of analyses was developed including key point detection in images and DEM rasters, retrieving georeferenced 3D DEM GCPs, regularized gradient-based optimization, pose estimation, ray tracing, and the correspondence identification between image pixels and real world coordinates. Two numerical experiments show that the framework is superior in georeferencing visual data in 3-D coordinates, paving a way toward fully automatic monoplotting methodology.



### ESGN: Efficient Stereo Geometry Network for Fast 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2111.14055v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.14055v2)
- **Published**: 2021-11-28 05:25:36+00:00
- **Updated**: 2022-04-26 04:03:25+00:00
- **Authors**: Aqi Gao, Yanwei Pang, Jing Nie, Jiale Cao, Yishun Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Fast stereo based 3D object detectors have made great progress recently. However, they lag far behind high-precision stereo based methods in accuracy. We argue that the main reason is due to the poor geometry-aware feature representation in 3D space. To solve this problem, we propose an efficient stereo geometry network (ESGN). The key in our ESGN is an efficient geometry-aware feature generation (EGFG) module. Our EGFG module first uses a stereo correlation and reprojection module to construct multi-scale stereo volumes in camera frustum space, second employs a multi-scale BEV projection and fusion module to generate multiple geometry-aware features. In these two steps, we adopt deep multi-scale information fusion for discriminative geometry-aware feature generation, without any complex aggregation networks. In addition, we introduce a deep geometry-aware feature distillation scheme to guide stereo feature learning with a LiDAR-based detector. The experiments are performed on the classical KITTI dataset. On KITTI test set, our ESGN outperforms the fast state-of-art-art detector YOLOStereo3D by 5.14\% on mAP$_{3d}$ at 62$ms$. To the best of our knowledge, our ESGN achieves a best trade-off between accuracy and speed. We hope that our efficient stereo geometry network can provide more possible directions for fast 3D object detection. Our source code will be released.



### NoFADE: Analyzing Diminishing Returns on CO2 Investment
- **Arxiv ID**: http://arxiv.org/abs/2111.14059v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.14059v1)
- **Published**: 2021-11-28 05:48:48+00:00
- **Updated**: 2021-11-28 05:48:48+00:00
- **Authors**: Andre Fu, Justin Tran, Andy Xie, Jonathan Spraggett, Elisa Ding, Chang-Won Lee, Kanav Singla, Mahdi S. Hosseini, Konstantinos N. Plataniotis
- **Comment**: Climate Change with Machine Learning workshop at 35th Conference on
  Neural Information Processing Systems (NeurIPS2021-CCAI)
- **Journal**: None
- **Summary**: Climate change continues to be a pressing issue that currently affects society at-large. It is important that we as a society, including the Computer Vision (CV) community take steps to limit our impact on the environment. In this paper, we (a) analyze the effect of diminishing returns on CV methods, and (b) propose a \textit{``NoFADE''}: a novel entropy-based metric to quantify model--dataset--complexity relationships. We show that some CV tasks are reaching saturation, while others are almost fully saturated. In this light, NoFADE allows the CV community to compare models and datasets on a similar basis, establishing an agnostic platform.



### Detection of E-scooter Riders in Naturalistic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2111.14060v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.14060v1)
- **Published**: 2021-11-28 05:59:36+00:00
- **Updated**: 2021-11-28 05:59:36+00:00
- **Authors**: Kumar Apurv, Renran Tian, Rini Sherony
- **Comment**: None
- **Journal**: None
- **Summary**: E-scooters have become ubiquitous vehicles in major cities around the world.The numbers of e-scooters keep escalating, increasing their interactions with other cars on the road. Normal behavior of an e-scooter rider varies enormously to other vulnerable road users. This situation creates new challenges for vehicle active safety systems and automated driving functionalities, which require the detection of e-scooter riders as the first step. To our best knowledge, there is no existing computer vision model to detect these e-scooter riders. This paper presents a novel vision-based system to differentiate between e-scooter riders and regular pedestrians and a benchmark data set for e-scooter riders in natural scenes. We propose an efficient pipeline built over two existing state-of-the-art convolutional neural networks (CNN), You Only Look Once (YOLOv3) and MobileNetV2. We fine-tune MobileNetV2 over our dataset and train the model to classify e-scooter riders and pedestrians. We obtain a recall of around 0.75 on our raw test sample to classify e-scooter riders with the whole pipeline. Moreover, the classification accuracy of trained MobileNetV2 on top of YOLOv3 is over 91%, with precision and recall over 0.9.



### PAPooling: Graph-based Position Adaptive Aggregation of Local Geometry in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2111.14067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.14067v1)
- **Published**: 2021-11-28 07:26:55+00:00
- **Updated**: 2021-11-28 07:26:55+00:00
- **Authors**: Jie Wang, Jianan Li, Lihe Ding, Ying Wang, Tingfa Xu
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Fine-grained geometry, captured by aggregation of point features in local regions, is crucial for object recognition and scene understanding in point clouds. Nevertheless, existing preeminent point cloud backbones usually incorporate max/average pooling for local feature aggregation, which largely ignores points' positional distribution, leading to inadequate assembling of fine-grained structures. To mitigate this bottleneck, we present an efficient alternative to max pooling, Position Adaptive Pooling (PAPooling), that explicitly models spatial relations among local points using a novel graph representation, and aggregates features in a position adaptive manner, enabling position-sensitive representation of aggregated features. Specifically, PAPooling consists of two key steps, Graph Construction and Feature Aggregation, respectively in charge of constructing a graph with edges linking the center point with every neighboring point in a local region to map their relative positional information to channel-wise attentive weights, and adaptively aggregating local point features based on the generated weights through Graph Convolution Network (GCN). PAPooling is simple yet effective, and flexible enough to be ready to use for different popular backbones like PointNet++ and DGCNN, as a plug-andplay operator. Extensive experiments on various tasks ranging from 3D shape classification, part segmentation to scene segmentation well demonstrate that PAPooling can significantly improve predictive accuracy, while with minimal extra computational overhead. Code will be released.



### Image preprocessing and modified adaptive thresholding for improving OCR
- **Arxiv ID**: http://arxiv.org/abs/2111.14075v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6; I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/2111.14075v2)
- **Published**: 2021-11-28 08:13:20+00:00
- **Updated**: 2021-11-30 04:04:33+00:00
- **Authors**: Rohan Lal Kshetry
- **Comment**: 5 pages, 7 figues
- **Journal**: Stoli\'nski, Sebastian & Bieniecki, Wojciech. (2011). Application
  of OCR systems to processing and digitization of paper documents. WULS Press
  Warszawa 2011. 102-111
- **Summary**: In this paper I have proposed a method to find the major pixel intensity inside the text and thresholding an image accordingly to make it easier to be used for optical character recognition (OCR) models. In our method, instead of editing whole image, I are removing all other features except the text boundaries and the color filling them. In this approach, the grayscale intensity of the letters from the input image are used as one of thresholding parameters. The performance of the developed model is finally validated with input images, with and without image processing followed by OCR by PyTesseract. Based on the results obtained, it can be observed that this algorithm can be efficiently applied in the field of image processing for OCR.



### Deep MAGSAC++
- **Arxiv ID**: http://arxiv.org/abs/2111.14093v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.14093v2)
- **Published**: 2021-11-28 10:16:38+00:00
- **Updated**: 2022-03-11 13:01:09+00:00
- **Authors**: Wei Tong, Jiri Matas, Daniel Barath
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Deep MAGSAC++ combining the advantages of traditional and deep robust estimators. We introduce a novel loss function that exploits the orientation and scale from partially affine covariant features, e.g., SIFT, in a geometrically justifiable manner. The new loss helps in learning higher-order information about the underlying scene geometry. Moreover, we propose a new sampler for RANSAC that always selects the sample with the highest probability of consisting only of inliers. After every unsuccessful iteration, the probabilities are updated in a principled way via a Bayesian approach. The prediction of the deep network is exploited as prior inside the sampler. Benefiting from the new loss, the proposed sampler and a number of technical advancements, Deep MAGSAC++ is superior to the state-of-the-art both in terms of accuracy and run-time on thousands of image pairs from publicly available real-world datasets for essential and fundamental matrix estimation.



### Gated SwitchGAN for multi-domain facial image translation
- **Arxiv ID**: http://arxiv.org/abs/2111.14096v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.14096v1)
- **Published**: 2021-11-28 10:24:43+00:00
- **Updated**: 2021-11-28 10:24:43+00:00
- **Authors**: Xiaokang Zhang, Yuanlue Zhu, Wenting Chen, Wenshuang Liu, Linlin Shen
- **Comment**: Accepted in IEEE TRANSACTIONS ON MULTIMEDIA(TMM)
- **Journal**: None
- **Summary**: Recent studies on multi-domain facial image translation have achieved impressive results. The existing methods generally provide a discriminator with an auxiliary classifier to impose domain translation. However, these methods neglect important information regarding domain distribution matching. To solve this problem, we propose a switch generative adversarial network (SwitchGAN) with a more adaptive discriminator structure and a matched generator to perform delicate image translation among multiple domains. A feature-switching operation is proposed to achieve feature selection and fusion in our conditional modules. We demonstrate the effectiveness of our model. Furthermore, we also introduce a new capability of our generator that represents attribute intensity control and extracts content information without tailored training. Experiments on the Morph, RaFD and CelebA databases visually and quantitatively show that our extended SwitchGAN (i.e., Gated SwitchGAN) can achieve better translation results than StarGAN, AttGAN and STGAN. The attribute classification accuracy achieved using the trained ResNet-18 model and the FID score obtained using the ImageNet pretrained Inception-v3 model also quantitatively demonstrate the superior performance of our models.



### CHARTER: heatmap-based multi-type chart data extraction
- **Arxiv ID**: http://arxiv.org/abs/2111.14103v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.14103v1)
- **Published**: 2021-11-28 11:01:21+00:00
- **Updated**: 2021-11-28 11:01:21+00:00
- **Authors**: Joseph Shtok, Sivan Harary, Ophir Azulai, Adi Raz Goldfarb, Assaf Arbelle, Leonid Karlinsky
- **Comment**: Joseph Shtok, Sivan Harary and Leonid Karlinsky had equal
  contribution
- **Journal**: Document Intelligence workshop at KDD 2021 conference
- **Summary**: The digital conversion of information stored in documents is a great source of knowledge. In contrast to the documents text, the conversion of the embedded documents graphics, such as charts and plots, has been much less explored. We present a method and a system for end-to-end conversion of document charts into machine readable tabular data format, which can be easily stored and analyzed in the digital domain. Our approach extracts and analyses charts along with their graphical elements and supporting structures such as legends, axes, titles, and captions. Our detection system is based on neural networks, trained solely on synthetic data, eliminating the limiting factor of data collection. As opposed to previous methods, which detect graphical elements using bounding-boxes, our networks feature auxiliary domain specific heatmaps prediction enabling the precise detection of pie charts, line and scatter plots which do not fit the rectangular bounding-box presumption. Qualitative and quantitative results show high robustness and precision, improving upon previous works on popular benchmarks



### Cross-Task Consistency Learning Framework for Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.14122v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.14122v1)
- **Published**: 2021-11-28 11:55:19+00:00
- **Updated**: 2021-11-28 11:55:19+00:00
- **Authors**: Akihiro Nakano, Shi Chen, Kazuyuki Demachi
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-task learning (MTL) is an active field in deep learning in which we train a model to jointly learn multiple tasks by exploiting relationships between the tasks. It has been shown that MTL helps the model share the learned features between tasks and enhance predictions compared to when learning each task independently. We propose a new learning framework for 2-task MTL problem that uses the predictions of one task as inputs to another network to predict the other task. We define two new loss terms inspired by cycle-consistency loss and contrastive learning, alignment loss and cross-task consistency loss. Both losses are designed to enforce the model to align the predictions of multiple tasks so that the model predicts consistently. We theoretically prove that both losses help the model learn more efficiently and that cross-task consistency loss is better in terms of alignment with the straight-forward predictions. Experimental results also show that our proposed model achieves significant performance on the benchmark Cityscapes and NYU dataset.



### Learning a Weight Map for Weakly-Supervised Localization
- **Arxiv ID**: http://arxiv.org/abs/2111.14131v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.14131v1)
- **Published**: 2021-11-28 12:45:23+00:00
- **Updated**: 2021-11-28 12:45:23+00:00
- **Authors**: Tal Shaharabany, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: In the weakly supervised localization setting, supervision is given as an image-level label. We propose to employ an image classifier $f$ and to train a generative network $g$ that outputs, given the input image, a per-pixel weight map that indicates the location of the object within the image. Network $g$ is trained by minimizing the discrepancy between the output of the classifier $f$ on the original image and its output given the same image weighted by the output of $g$. The scheme requires a regularization term that ensures that $g$ does not provide a uniform weight, and an early stopping criterion in order to prevent $g$ from over-segmenting the image. Our results indicate that the method outperforms existing localization methods by a sizable margin on the challenging fine-grained classification datasets, as well as a generic image recognition dataset. Additionally, the obtained weight map is also state-of-the-art in weakly supervised segmentation in fine-grained categorization datasets.



### FashionSearchNet-v2: Learning Attribute Representations with Localization for Image Retrieval with Attribute Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2111.14145v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.14145v1)
- **Published**: 2021-11-28 13:50:20+00:00
- **Updated**: 2021-11-28 13:50:20+00:00
- **Authors**: Kenan E. Ak, Joo Hwee Lim, Ying Sun, Jo Yew Tham, Ashraf A. Kassim
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: The focus of this paper is on the problem of image retrieval with attribute manipulation. Our proposed work is able to manipulate the desired attributes of the query image while maintaining its other attributes. For example, the collar attribute of the query image can be changed from round to v-neck to retrieve similar images from a large dataset. A key challenge in e-commerce is that images have multiple attributes where users would like to manipulate and it is important to estimate discriminative feature representations for each of these attributes. The proposed FashionSearchNet-v2 architecture is able to learn attribute specific representations by leveraging on its weakly-supervised localization module, which ignores the unrelated features of attributes in the feature space, thus improving the similarity learning. The network is jointly trained with the combination of attribute classification and triplet ranking loss to estimate local representations. These local representations are then merged into a single global representation based on the instructed attribute manipulation where desired images can be retrieved with a distance metric. The proposed method also provides explainability for its retrieval process to help provide additional information on the attention of the network. Experiments performed on several datasets that are rich in terms of the number of attributes show that FashionSearchNet-v2 outperforms the other state-of-the-art attribute manipulation techniques. Different than our earlier work (FashionSearchNet), we propose several improvements in the learning procedure and show that the proposed FashionSearchNet-v2 can be generalized to different domains other than fashion.



### Multi-domain Integrative Swin Transformer network for Sparse-View Tomographic Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2111.14831v7
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.14831v7)
- **Published**: 2021-11-28 14:43:26+00:00
- **Updated**: 2022-04-15 09:18:51+00:00
- **Authors**: Jiayi Pan, Heye Zhang, Weifei Wu, Zhifan Gao, Weiwen Wu
- **Comment**: 30 pages, 11 figures, 10 tables, 54 references
- **Journal**: None
- **Summary**: Decreasing projection views to lower X-ray radiation dose usually leads to severe streak artifacts. To improve image quality from sparse-view data, a Multi-domain Integrative Swin Transformer network (MIST-net) was developed in this article. First, MIST-net incorporated lavish domain features from data, residual-data, image, and residual-image using flexible network architectures, where residual-data and residual-image sub-network was considered as data consistency module to eliminate interpolation and reconstruction errors. Second, a trainable edge enhancement filter was incorporated to detect and protect image edges. Third, a high-quality reconstruction Swin transformer (i.e., Recformer) was designed to capture image global features. The experiment results on numerical and real cardiac clinical datasets with 48-views demonstrated that our proposed MIST-net provided better image quality with more small features and sharp edges than other competitors.



### Implicit Equivariance in Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2111.14157v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.14157v1)
- **Published**: 2021-11-28 14:44:17+00:00
- **Updated**: 2021-11-28 14:44:17+00:00
- **Authors**: Naman Khetan, Tushar Arora, Samee Ur Rehman, Deepak K. Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks(CNN) are inherently equivariant under translations, however, they do not have an equivalent embedded mechanism to handle other transformations such as rotations and change in scale. Several approaches exist that make CNNs equivariant under other transformation groups by design. Among these, steerable CNNs have been especially effective. However, these approaches require redesigning standard networks with filters mapped from combinations of predefined basis involving complex analytical functions. We experimentally demonstrate that these restrictions in the choice of basis can lead to model weights that are sub-optimal for the primary deep learning task (e.g. classification). Moreover, such hard-baked explicit formulations make it difficult to design composite networks comprising heterogeneous feature groups. To circumvent such issues, we propose Implicitly Equivariant Networks (IEN) which induce equivariance in the different layers of a standard CNN model by optimizing a multi-objective loss function that combines the primary loss with an equivariance loss term. Through experiments with VGG and ResNet models on Rot-MNIST , Rot-TinyImageNet, Scale-MNIST and STL-10 datasets, we show that IEN, even with its simple formulation, performs better than steerable networks. Also, IEN facilitates construction of heterogeneous filter groups allowing reduction in number of channels in CNNs by a factor of over 30% while maintaining performance on par with baselines. The efficacy of IEN is further validated on the hard problem of visual object tracking. We show that IEN outperforms the state-of-the-art rotation equivariant tracking method while providing faster inference speed.



### Learning To Segment Dominant Object Motion From Watching Videos
- **Arxiv ID**: http://arxiv.org/abs/2111.14160v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.14160v1)
- **Published**: 2021-11-28 14:51:00+00:00
- **Updated**: 2021-11-28 14:51:00+00:00
- **Authors**: Sahir Shrestha, Mohammad Ali Armin, Hongdong Li, Nick Barnes
- **Comment**: DICTA 2021
- **Journal**: None
- **Summary**: Existing deep learning based unsupervised video object segmentation methods still rely on ground-truth segmentation masks to train. Unsupervised in this context only means that no annotated frames are used during inference. As obtaining ground-truth segmentation masks for real image scenes is a laborious task, we envision a simple framework for dominant moving object segmentation that neither requires annotated data to train nor relies on saliency priors or pre-trained optical flow maps. Inspired by a layered image representation, we introduce a technique to group pixel regions according to their affine parametric motion. This enables our network to learn segmentation of the dominant foreground object using only RGB image pairs as input for both training and inference. We establish a baseline for this novel task using a new MovingCars dataset and show competitive performance against recent methods that require annotated masks to train.



### CDGNet: Class Distribution Guided Network for Human Parsing
- **Arxiv ID**: http://arxiv.org/abs/2111.14173v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.14173v3)
- **Published**: 2021-11-28 15:18:53+00:00
- **Updated**: 2022-03-17 01:11:43+00:00
- **Authors**: Kunliang Liu, Ouk Choi, Jianming Wang, Wonjun Hwang
- **Comment**: Accepted at CVPR 2022
- **Journal**: None
- **Summary**: The objective of human parsing is to partition a human in an image into constituent parts. This task involves labeling each pixel of the human image according to the classes. Since the human body comprises hierarchically structured parts, each body part of an image can have its sole position distribution characteristic. Probably, a human head is less likely to be under the feet, and arms are more likely to be near the torso. Inspired by this observation, we make instance class distributions by accumulating the original human parsing label in the horizontal and vertical directions, which can be utilized as supervision signals. Using these horizontal and vertical class distribution labels, the network is guided to exploit the intrinsic position distribution of each class. We combine two guided features to form a spatial guidance map, which is then superimposed onto the baseline network by multiplication and concatenation to distinguish the human parts precisely. We conducted extensive experiments to demonstrate the effectiveness and superiority of our method on three well-known benchmarks: LIP, ATR, and CIHP databases.



### UAV-based Crowd Surveillance in Post COVID-19 Era
- **Arxiv ID**: http://arxiv.org/abs/2111.14176v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2111.14176v1)
- **Published**: 2021-11-28 15:28:31+00:00
- **Updated**: 2021-11-28 15:28:31+00:00
- **Authors**: Nizar Masmoudi, Wael Jaafar, Safa Cherif, Jihene Ben Abderrazak, Halim Yanikomeroglu
- **Comment**: Accepted for publication in IEEE Access; 14 pages, 13 figures, 5
  tables
- **Journal**: None
- **Summary**: To cope with the current pandemic situation and reinstate pseudo-normal daily life, several measures have been deployed and maintained, such as mask wearing, social distancing, hands sanitizing, etc. Since outdoor cultural events, concerts, and picnics, are gradually allowed, a close monitoring of the crowd activity is needed to avoid undesired contact and disease transmission. In this context, intelligent unmanned aerial vehicles (UAVs) can be occasionally deployed to ensure the surveillance of these activities, that health restriction measures are applied, and to trigger alerts when the latter are not respected. Consequently, we propose in this paper a complete UAV framework for intelligent monitoring of post COVID-19 outdoor activities. Specifically, we propose a three steps approach. In the first step, captured images by a UAV are analyzed using machine learning to detect and locate individuals. The second step consists of a novel coordinates mapping approach to evaluate distances among individuals, then cluster them, while the third step provides an energy-efficient and/or reliable UAV trajectory to inspect clusters for restrictions violation such as mask wearing. Obtained results provide the following insights: 1) Efficient detection of individuals depends on the angle from which the image was captured, 2) coordinates mapping is very sensitive to the estimation error in individuals' bounding boxes, and 3) UAV trajectory design algorithm 2-Opt is recommended for practical real-time deployments due to its low-complexity and near-optimal performance.



### Make an Omelette with Breaking Eggs: Zero-Shot Learning for Novel Attribute Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2111.14182v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.14182v6)
- **Published**: 2021-11-28 15:45:54+00:00
- **Updated**: 2022-11-19 12:18:18+00:00
- **Authors**: Yu-Hsuan Li, Tzu-Yin Chao, Ching-Chun Huang, Pin-Yu Chen, Wei-Chen Chiu
- **Comment**: Accepted by the 36th Conference on Neural Information Processing
  Systems (NeurIPS 2022). (* Yu-Hsuan Li and Tzu-Yin Chao contributed equally
  to this work.)
- **Journal**: None
- **Summary**: Most of the existing algorithms for zero-shot classification problems typically rely on the attribute-based semantic relations among categories to realize the classification of novel categories without observing any of their instances. However, training the zero-shot classification models still requires attribute labeling for each class (or even instance) in the training dataset, which is also expensive. To this end, in this paper, we bring up a new problem scenario: "Can we derive zero-shot learning for novel attribute detectors/classifiers and use them to automatically annotate the dataset for labeling efficiency?". Basically, given only a small set of detectors that are learned to recognize some manually annotated attributes (i.e., the seen attributes), we aim to synthesize the detectors of novel attributes in a zero-shot learning manner. Our proposed method, Zero-Shot Learning for Attributes (ZSLA), which is the first of its kind to the best of our knowledge, tackles this new research problem by applying the set operations to first decompose the seen attributes into their basic attributes and then recombine these basic attributes into the novel ones. Extensive experiments are conducted to verify the capacity of our synthesized detectors for accurately capturing the semantics of the novel attributes and show their superior performance in terms of detection and localization compared to other baseline approaches. Moreover, we demonstrate the application of automatic annotation using our synthesized detectors on Caltech-UCSD Birds-200-2011 dataset. Various generalized zero-shot classification algorithms trained upon the dataset re-annotated by ZSLA show comparable performance with those trained with the manual ground-truth annotations. Please refer to our project page for source code: https://yuhsuanli.github.io/ZSLA/



### Gram Barcodes for Histopathology Tissue Texture Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2111.15519v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.15519v1)
- **Published**: 2021-11-28 17:59:42+00:00
- **Updated**: 2021-11-28 17:59:42+00:00
- **Authors**: Shalev Lifshitz, Abtin Riasatian, H. R. Tizhoosh
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in digital pathology have led to the need for Histopathology Image Retrieval (HIR) systems that search through databases of biopsy images to find similar cases to a given query image. These HIR systems allow pathologists to effortlessly and efficiently access thousands of previously diagnosed cases in order to exploit the knowledge in the corresponding pathology reports. Since HIR systems may have to deal with millions of gigapixel images, the extraction of compact and expressive image features must be available to allow for efficient and accurate retrieval. In this paper, we propose the application of Gram barcodes as image features for HIR systems. Unlike most feature generation schemes, Gram barcodes are based on high-order statistics that describe tissue texture by summarizing the correlations between different feature maps in layers of convolutional neural networks. We run HIR experiments on three public datasets using a pre-trained VGG19 network for Gram barcode generation and showcase highly competitive results.



### Emergent Graphical Conventions in a Visual Communication Game
- **Arxiv ID**: http://arxiv.org/abs/2111.14210v4
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.14210v4)
- **Published**: 2021-11-28 18:59:57+00:00
- **Updated**: 2023-02-23 19:33:22+00:00
- **Authors**: Shuwen Qiu, Sirui Xie, Lifeng Fan, Tao Gao, Jungseock Joo, Song-Chun Zhu, Yixin Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Humans communicate with graphical sketches apart from symbolic languages. Primarily focusing on the latter, recent studies of emergent communication overlook the sketches; they do not account for the evolution process through which symbolic sign systems emerge in the trade-off between iconicity and symbolicity. In this work, we take the very first step to model and simulate this process via two neural agents playing a visual communication game; the sender communicates with the receiver by sketching on a canvas. We devise a novel reinforcement learning method such that agents are evolved jointly towards successful communication and abstract graphical conventions. To inspect the emerged conventions, we define three fundamental properties -- iconicity, symbolicity, and semanticity -- and design evaluation methods accordingly. Our experimental results under different controls are consistent with the observation in studies of human graphical conventions. Of note, we find that evolved sketches can preserve the continuum of semantics under proper environmental pressures. More interestingly, co-evolved agents can switch between conventionalized and iconic communication based on their familiarity with referents. We hope the present research can pave the path for studying emergent communication with the modality of sketches.



### Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.14213v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2111.14213v3)
- **Published**: 2021-11-28 19:03:39+00:00
- **Updated**: 2022-04-13 18:30:03+00:00
- **Authors**: Matias Mendieta, Taojiannan Yang, Pu Wang, Minwoo Lee, Zhengming Ding, Chen Chen
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Federated learning (FL) is a promising strategy for performing privacy-preserving, distributed learning with a network of clients (i.e., edge devices). However, the data distribution among clients is often non-IID in nature, making efficient optimization difficult. To alleviate this issue, many FL algorithms focus on mitigating the effects of data heterogeneity across clients by introducing a variety of proximal terms, some incurring considerable compute and/or memory overheads, to restrain local updates with respect to the global model. Instead, we consider rethinking solutions to data heterogeneity in FL with a focus on local learning generality rather than proximal restriction. To this end, we first present a systematic study informed by second-order indicators to better understand algorithm effectiveness in FL. Interestingly, we find that standard regularization methods are surprisingly strong performers in mitigating data heterogeneity effects. Based on our findings, we further propose a simple and effective method, FedAlign, to overcome data heterogeneity and the pitfalls of previous methods. FedAlign achieves competitive accuracy with state-of-the-art FL methods across a variety of settings while minimizing computation and memory overhead. Code is available at https://github.com/mmendiet/FedAlign



### Low-complexity Rounded KLT Approximation for Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2111.14239v1
- **DOI**: 10.1007/s11554-021-01173-0
- **Categories**: **eess.IV**, cs.CV, cs.NA, eess.SP, math.NA, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/2111.14239v1)
- **Published**: 2021-11-28 21:25:35+00:00
- **Updated**: 2021-11-28 21:25:35+00:00
- **Authors**: A. P. Radünz, F. M. Bayer, R. J. Cintra
- **Comment**: 10 pages, 7 figures, 3 tables
- **Journal**: J Real-Time Image Proc (2021)
- **Summary**: The Karhunen-Lo\`eve transform (KLT) is often used for data decorrelation and dimensionality reduction. Because its computation depends on the matrix of covariances of the input signal, the use of the KLT in real-time applications is severely constrained by the difficulty in developing fast algorithms to implement it. In this context, this paper proposes a new class of low-complexity transforms that are obtained through the application of the round function to the elements of the KLT matrix. The proposed transforms are evaluated considering figures of merit that measure the coding power and distance of the proposed approximations to the exact KLT and are also explored in image compression experiments. Fast algorithms are introduced for the proposed approximate transforms. It was shown that the proposed transforms perform well in image compression and require a low implementation cost.



### EffCNet: An Efficient CondenseNet for Image Classification on NXP BlueBox
- **Arxiv ID**: http://arxiv.org/abs/2111.14243v1
- **DOI**: 10.11648/j.ajece.20210502.15
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.14243v1)
- **Published**: 2021-11-28 21:32:31+00:00
- **Updated**: 2021-11-28 21:32:31+00:00
- **Authors**: Priyank Kalgaonkar, Mohamed El-Sharkawy
- **Comment**: 11 pages, 10 figures, published in American Journal of Electrical and
  Computer Engineering
- **Journal**: Vol. 5, No. 2, 2021, pp. 77-87
- **Summary**: Intelligent edge devices with built-in processors vary widely in terms of capability and physical form to perform advanced Computer Vision (CV) tasks such as image classification and object detection, for example. With constant advances in the field of autonomous cars and UAVs, embedded systems and mobile devices, there has been an ever-growing demand for extremely efficient Artificial Neural Networks (ANN) for real-time inference on these smart edge devices with constrained computational resources. With unreliable network connections in remote regions and an added complexity of data transmission, it is of an utmost importance to capture and process data locally instead of sending the data to cloud servers for remote processing. Edge devices on the other hand, offer limited processing power due to their inexpensive hardware, and limited cooling and computational resources. In this paper, we propose a novel deep convolutional neural network architecture called EffCNet which is an improved and an efficient version of CondenseNet Convolutional Neural Network (CNN) for edge devices utilizing self-querying data augmentation and depthwise separable convolutional strategies to improve real-time inference performance as well as reduce the final trained model size, trainable parameters, and Floating-Point Operations (FLOPs) of EffCNet CNN. Furthermore, extensive supervised image classification analyses are conducted on two benchmarking datasets: CIFAR-10 and CIFAR-100, to verify real-time inference performance of our proposed CNN. Finally, we deploy these trained weights on NXP BlueBox which is an intelligent edge development platform designed for self-driving vehicles and UAVs, and conclusions will be extrapolated accordingly.



### Learning a model of shape selectivity in V4 cells reveals shape encoding mechanisms in the brain
- **Arxiv ID**: http://arxiv.org/abs/2111.14250v3
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.14250v3)
- **Published**: 2021-11-28 22:24:12+00:00
- **Updated**: 2022-06-15 16:45:15+00:00
- **Authors**: Paria Mehrani, John K. Tsotsos
- **Comment**: 20 pages, 7 figures
- **Journal**: None
- **Summary**: The mechanisms involved in transforming early visual signals to curvature representations in V4 are unknown. We propose a hierarchical model that reveals V1/V2 encodings that are essential components for this transformation to the reported curvature representations in V4. Then, by relaxing the often-imposed prior of a single Gaussian, V4 shape selectivity is learned in the last layer of the hierarchy from Macaque V4 responses. We found that V4 cells integrate multiple shape parts from the full spatial extent of their receptive fields with similar excitatory and inhibitory contributions. Our results uncover new details in existing data about shape selectivity in V4 neurons that with further experiments can enhance our understanding of processing in this area. Accordingly, we propose designs for a stimulus set that allow removing shape parts without disturbing the curvature signal to isolate part contributions to V4 responses.



### 3D High-Quality Magnetic Resonance Image Restoration in Clinics Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.14259v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.14259v4)
- **Published**: 2021-11-28 22:58:00+00:00
- **Updated**: 2022-08-12 17:31:40+00:00
- **Authors**: Hao Li, Jianan Liu
- **Comment**: 16 pages, 10 figures
- **Journal**: None
- **Summary**: Shortening acquisition time and reducing the motion artifacts are two of the most essential concerns in magnetic resonance imaging. As a promising solution, deep learning-based high-quality MR image restoration has been investigated to generate highly-resolved and motion artifact-free MR images from lower resolution images acquired with shortened acquisition time or motion artifact-corrupted images. However, numerous problems still exist to prevent deep learning approaches from becoming practical in the clinic environment. Specifically, most of the prior works focus solely on the network but ignore the impact of various down-sampling strategies on the acquisition time. Besides, the long inference time and high GPU consumption are also the bottlenecks to deploy most of the prior works in clinics. Furthermore, prior studies employ random movement in retrospective motion artifact generation, resulting in uncontrollable severity of motion artifact. More importantly, doctors are unsure whether the generated MR images are trustworthy, making diagnosis difficult. To overcome all these problems, we adopted a unified framework of 2D deep learning neural network for both 3D MRI super-resolution and motion artifact reduction, demonstrating such a framework can achieve better performance in 3D MRI restoration tasks compared to other state-of-the-art methods and remain the GPU consumption and inference time significantly low, thus easier to deploy. We also analyzed several down-sampling strategies based on the acceleration factor, including multiple combinations of in-plane and through-plane down-sampling, and developed a controllable and quantifiable motion artifact generation method. At last, the pixel-wise uncertainty was calculated and used to estimate the accuracy of the generated image, providing additional information for a reliable diagnosis.



### Explore the Potential Performance of Vision-and-Language Navigation Model: a Snapshot Ensemble Method
- **Arxiv ID**: http://arxiv.org/abs/2111.14267v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2111.14267v1)
- **Published**: 2021-11-28 23:07:48+00:00
- **Updated**: 2021-11-28 23:07:48+00:00
- **Authors**: Wenda Qin, Teruhisa Misu, Derry Wijaya
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Vision-and-Language Navigation (VLN) is a challenging task in the field of artificial intelligence. Although massive progress has been made in this task over the past few years attributed to breakthroughs in deep vision and language models, it remains tough to build VLN models that can generalize as well as humans. In this paper, we provide a new perspective to improve VLN models. Based on our discovery that snapshots of the same VLN model behave significantly differently even when their success rates are relatively the same, we propose a snapshot-based ensemble solution that leverages predictions among multiple snapshots. Constructed on the snapshots of the existing state-of-the-art (SOTA) model $\circlearrowright$BERT and our past-action-aware modification, our proposed ensemble achieves the new SOTA performance in the R2R dataset challenge in Navigation Error (NE) and Success weighted by Path Length (SPL).



### Automated Detection of Patients in Hospital Video Recordings
- **Arxiv ID**: http://arxiv.org/abs/2111.14270v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.14270v1)
- **Published**: 2021-11-28 23:15:06+00:00
- **Updated**: 2021-11-28 23:15:06+00:00
- **Authors**: Siddharth Sharma, Florian Dubost, Christopher Lee-Messer, Daniel Rubin
- **Comment**: None
- **Journal**: None
- **Summary**: In a clinical setting, epilepsy patients are monitored via video electroencephalogram (EEG) tests. A video EEG records what the patient experiences on videotape while an EEG device records their brainwaves. Currently, there are no existing automated methods for tracking the patient's location during a seizure, and video recordings of hospital patients are substantially different from publicly available video benchmark datasets. For example, the camera angle can be unusual, and patients can be partially covered with bedding sheets and electrode sets. Being able to track a patient in real-time with video EEG would be a promising innovation towards improving the quality of healthcare. Specifically, an automated patient detection system could supplement clinical oversight and reduce the resource-intensive efforts of nurses and doctors who need to continuously monitor patients. We evaluate an ImageNet pre-trained Mask R-CNN, a standard deep learning model for object detection, on the task of patient detection using our own curated dataset of 45 videos of hospital patients. The dataset was aggregated and curated for this work. We show that without fine-tuning, ImageNet pre-trained Mask R-CNN models perform poorly on such data. By fine-tuning the models with a subset of our dataset, we observe a substantial improvement in patient detection performance, with a mean average precision of 0.64. We show that the results vary substantially depending on the video clip.



### ExCon: Explanation-driven Supervised Contrastive Learning for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2111.14271v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.14271v6)
- **Published**: 2021-11-28 23:15:26+00:00
- **Updated**: 2022-04-18 01:39:16+00:00
- **Authors**: Zhibo Zhang, Jongseong Jang, Chiheb Trabelsi, Ruiwen Li, Scott Sanner, Yeonjeong Jeong, Dongsub Shim
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive learning has led to substantial improvements in the quality of learned embedding representations for tasks such as image classification. However, a key drawback of existing contrastive augmentation methods is that they may lead to the modification of the image content which can yield undesired alterations of its semantics. This can affect the performance of the model on downstream tasks. Hence, in this paper, we ask whether we can augment image data in contrastive learning such that the task-relevant semantic content of an image is preserved. For this purpose, we propose to leverage saliency-based explanation methods to create content-preserving masked augmentations for contrastive learning. Our novel explanation-driven supervised contrastive learning (ExCon) methodology critically serves the dual goals of encouraging nearby image embeddings to have similar content and explanation. To quantify the impact of ExCon, we conduct experiments on the CIFAR-100 and the Tiny ImageNet datasets. We demonstrate that ExCon outperforms vanilla supervised contrastive learning in terms of classification, explanation quality, adversarial robustness as well as probabilistic calibration in the context of distributional shift.



