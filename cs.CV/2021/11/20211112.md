# Arxiv Papers in cs.CV on 2021-11-12
### Neuromuscular Control of the Face-Head-Neck Biomechanical Complex With Learning-Based Expression Transfer From Images and Videos
- **Arxiv ID**: http://arxiv.org/abs/2111.06517v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06517v1)
- **Published**: 2021-11-12 01:13:07+00:00
- **Updated**: 2021-11-12 01:13:07+00:00
- **Authors**: Xiao S. Zeng, Surya Dwarakanath, Wuyue Lu, Masaki Nakada, Demetri Terzopoulos
- **Comment**: 12 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: The transfer of facial expressions from people to 3D face models is a classic computer graphics problem. In this paper, we present a novel, learning-based approach to transferring facial expressions and head movements from images and videos to a biomechanical model of the face-head-neck complex. Leveraging the Facial Action Coding System (FACS) as an intermediate representation of the expression space, we train a deep neural network to take in FACS Action Units (AUs) and output suitable facial muscle and jaw activation signals for the musculoskeletal model. Through biomechanical simulation, the activations deform the facial soft tissues, thereby transferring the expression to the model. Our approach has advantages over previous approaches. First, the facial expressions are anatomically consistent as our biomechanical model emulates the relevant anatomy of the face, head, and neck. Second, by training the neural network using data generated from the biomechanical model itself, we eliminate the manual effort of data collection for expression transfer. The success of our approach is demonstrated through experiments involving the transfer onto our face-head-neck model of facial expressions and head poses from a range of facial images and videos.



### Self-supervised GAN Detector
- **Arxiv ID**: http://arxiv.org/abs/2111.06575v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.06575v2)
- **Published**: 2021-11-12 06:19:04+00:00
- **Updated**: 2022-03-07 15:35:57+00:00
- **Authors**: Yonghyun Jeong, Doyeon Kim, Pyounggeon Kim, Youngmin Ro, Jongwon Choi
- **Comment**: None
- **Journal**: None
- **Summary**: Although the recent advancement in generative models brings diverse advantages to society, it can also be abused with malicious purposes, such as fraud, defamation, and fake news. To prevent such cases, vigorous research is conducted to distinguish the generated images from the real images, but challenges still remain to distinguish the unseen generated images outside of the training settings. Such limitations occur due to data dependency arising from the model's overfitting issue to the training data generated by specific GANs. To overcome this issue, we adopt a self-supervised scheme to propose a novel framework. Our proposed method is composed of the artificial fingerprint generator reconstructing the high-quality artificial fingerprints of GAN images for detailed analysis, and the GAN detector distinguishing GAN images by learning the reconstructed artificial fingerprints. To improve the generalization of the artificial fingerprint generator, we build multiple autoencoders with different numbers of upconvolution layers. With numerous ablation studies, the robust generalization of our method is validated by outperforming the generalization of the previous state-of-the-art algorithms, even without utilizing the GAN images of the training dataset.



### Learning to Break Deep Perceptual Hashing: The Use Case NeuralHash
- **Arxiv ID**: http://arxiv.org/abs/2111.06628v4
- **DOI**: 10.1145/3531146.3533073
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06628v4)
- **Published**: 2021-11-12 09:49:27+00:00
- **Updated**: 2022-06-09 07:00:51+00:00
- **Authors**: Lukas Struppek, Dominik Hintersdorf, Daniel Neider, Kristian Kersting
- **Comment**: Accepted by ACM FAccT 2022 as Oral
- **Journal**: None
- **Summary**: Apple recently revealed its deep perceptual hashing system NeuralHash to detect child sexual abuse material (CSAM) on user devices before files are uploaded to its iCloud service. Public criticism quickly arose regarding the protection of user privacy and the system's reliability. In this paper, we present the first comprehensive empirical analysis of deep perceptual hashing based on NeuralHash. Specifically, we show that current deep perceptual hashing may not be robust. An adversary can manipulate the hash values by applying slight changes in images, either induced by gradient-based approaches or simply by performing standard image transformations, forcing or preventing hash collisions. Such attacks permit malicious actors easily to exploit the detection system: from hiding abusive material to framing innocent users, everything is possible. Moreover, using the hash values, inferences can still be made about the data stored on user devices. In our view, based on our results, deep perceptual hashing in its current form is generally not ready for robust client-side scanning and should not be used from a privacy perspective.



### Closed-Loop Data Transcription to an LDR via Minimaxing Rate Reduction
- **Arxiv ID**: http://arxiv.org/abs/2111.06636v3
- **DOI**: 10.3390/e24040456
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06636v3)
- **Published**: 2021-11-12 10:06:08+00:00
- **Updated**: 2022-03-03 07:13:43+00:00
- **Authors**: Xili Dai, Shengbang Tong, Mingyang Li, Ziyang Wu, Michael Psenka, Kwan Ho Ryan Chan, Pengyuan Zhai, Yaodong Yu, Xiaojun Yuan, Heung Yeung Shum, Yi Ma
- **Comment**: 41 pages
- **Journal**: None
- **Summary**: This work proposes a new computational framework for learning a structured generative model for real-world datasets. In particular, we propose to learn a closed-loop transcription between a multi-class multi-dimensional data distribution and a linear discriminative representation (LDR) in the feature space that consists of multiple independent multi-dimensional linear subspaces. In particular, we argue that the optimal encoding and decoding mappings sought can be formulated as the equilibrium point of a two-player minimax game between the encoder and decoder. A natural utility function for this game is the so-called rate reduction, a simple information-theoretic measure for distances between mixtures of subspace-like Gaussians in the feature space. Our formulation draws inspiration from closed-loop error feedback from control systems and avoids expensive evaluating and minimizing approximated distances between arbitrary distributions in either the data space or the feature space. To a large extent, this new formulation unifies the concepts and benefits of Auto-Encoding and GAN and naturally extends them to the settings of learning a both discriminative and generative representation for multi-class and multi-dimensional real-world data. Our extensive experiments on many benchmark imagery datasets demonstrate tremendous potential of this new closed-loop formulation: under fair comparison, visual quality of the learned decoder and classification performance of the encoder is competitive and often better than existing methods based on GAN, VAE, or a combination of both. Unlike existing generative models, the so learned features of the multiple classes are structured: different classes are explicitly mapped onto corresponding independent principal subspaces in the feature space. Source code can be found at https://github.com/Delay-Xili/LDR.



### Meta-Teacher For Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2111.06638v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.06638v1)
- **Published**: 2021-11-12 10:09:50+00:00
- **Updated**: 2021-11-12 10:09:50+00:00
- **Authors**: Yunxiao Qin, Zitong Yu, Longbin Yan, Zezheng Wang, Chenxu Zhao, Zhen Lei
- **Comment**: Accepted by IEEE TPAMI-2021
- **Journal**: None
- **Summary**: Face anti-spoofing (FAS) secures face recognition from presentation attacks (PAs). Existing FAS methods usually supervise PA detectors with handcrafted binary or pixel-wise labels. However, handcrafted labels may are not the most adequate way to supervise PA detectors learning sufficient and intrinsic spoofing cues. Instead of using the handcrafted labels, we propose a novel Meta-Teacher FAS (MT-FAS) method to train a meta-teacher for supervising PA detectors more effectively. The meta-teacher is trained in a bi-level optimization manner to learn the ability to supervise the PA detectors learning rich spoofing cues. The bi-level optimization contains two key components: 1) a lower-level training in which the meta-teacher supervises the detector's learning process on the training set; and 2) a higher-level training in which the meta-teacher's teaching performance is optimized by minimizing the detector's validation loss. Our meta-teacher differs significantly from existing teacher-student models because the meta-teacher is explicitly trained for better teaching the detector (student), whereas existing teachers are trained for outstanding accuracy neglecting teaching ability. Extensive experiments on five FAS benchmarks show that with the proposed MT-FAS, the trained meta-teacher 1) provides better-suited supervision than both handcrafted labels and existing teacher-student models; and 2) significantly improves the performances of PA detectors.



### Attention Guided Cosine Margin For Overcoming Class-Imbalance in Few-Shot Road Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2111.06639v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.06639v1)
- **Published**: 2021-11-12 10:11:56+00:00
- **Updated**: 2021-11-12 10:11:56+00:00
- **Authors**: Ashutosh Agarwal, Anay Majee, Anbumani Subramanian, Chetan Arora
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: Few-shot object detection (FSOD) localizes and classifies objects in an image given only a few data samples. Recent trends in FSOD research show the adoption of metric and meta-learning techniques, which are prone to catastrophic forgetting and class confusion. To overcome these pitfalls in metric learning based FSOD techniques, we introduce Attention Guided Cosine Margin (AGCM) that facilitates the creation of tighter and well separated class-specific feature clusters in the classification head of the object detector. Our novel Attentive Proposal Fusion (APF) module minimizes catastrophic forgetting by reducing the intra-class variance among co-occurring classes. At the same time, the proposed Cosine Margin Cross-Entropy loss increases the angular margin between confusing classes to overcome the challenge of class confusion between already learned (base) and newly added (novel) classes. We conduct our experiments on the challenging India Driving Dataset (IDD), which presents a real-world class-imbalanced setting alongside popular FSOD benchmark PASCAL-VOC. Our method outperforms State-of-the-Art (SoTA) approaches by up to 6.4 mAP points on the IDD-OS and up to 2.0 mAP points on the IDD-10 splits for the 10-shot setting. On the PASCAL-VOC dataset, we outperform existing SoTA approaches by up to 4.9 mAP points.



### Fully Automatic Page Turning on Real Scores
- **Arxiv ID**: http://arxiv.org/abs/2111.06643v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2111.06643v1)
- **Published**: 2021-11-12 10:23:14+00:00
- **Updated**: 2021-11-12 10:23:14+00:00
- **Authors**: Florian Henkel, Stephanie Schwaiger, Gerhard Widmer
- **Comment**: ISMIR 2021 Late Breaking/Demo
- **Journal**: None
- **Summary**: We present a prototype of an automatic page turning system that works directly on real scores, i.e., sheet images, without any symbolic representation. Our system is based on a multi-modal neural network architecture that observes a complete sheet image page as input, listens to an incoming musical performance, and predicts the corresponding position in the image. Using the position estimation of our system, we use a simple heuristic to trigger a page turning event once a certain location within the sheet image is reached. As a proof of concept we further combine our system with an actual machine that will physically turn the page on command.



### Frequency learning for structured CNN filters with Gaussian fractional derivatives
- **Arxiv ID**: http://arxiv.org/abs/2111.06660v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06660v1)
- **Published**: 2021-11-12 11:01:28+00:00
- **Updated**: 2021-11-12 11:01:28+00:00
- **Authors**: Nikhil Saldanha, Silvia L. Pintea, Jan C. van Gemert, Nergis Tomen
- **Comment**: Accepted at BMVC 2021
- **Journal**: None
- **Summary**: Frequency information lies at the base of discriminating between textures, and therefore between different objects. Classical CNN architectures limit the frequency learning through fixed filter sizes, and lack a way of explicitly controlling it. Here, we build on the structured receptive field filters with Gaussian derivative basis. Yet, rather than using predetermined derivative orders, which typically result in fixed frequency responses for the basis functions, we learn these. We show that by learning the order of the basis we can accurately learn the frequency of the filters, and hence adapt to the optimal frequencies for the underlying learning task. We investigate the well-founded mathematical formulation of fractional derivatives to adapt the filter frequencies during training. Our formulation leads to parameter savings and data efficiency when compared to the standard CNNs and the Gaussian derivative CNN filter networks that we build upon.



### A comprehensive study of clustering a class of 2D shapes
- **Arxiv ID**: http://arxiv.org/abs/2111.06662v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06662v1)
- **Published**: 2021-11-12 11:08:15+00:00
- **Updated**: 2021-11-12 11:08:15+00:00
- **Authors**: Agnieszka Kaliszewska, Monika Syga
- **Comment**: None
- **Journal**: None
- **Summary**: The paper concerns clustering with respect to the shape and size of 2D contours that are boundaries of cross-sections of 3D objects of revolution. We propose a number of similarity measures based on combined disparate Procrustes analysis (PA) and Dynamic Time Warping (DTW) distances. Motivation and the main application for this study comes from archaeology. The performed computational experiments refer to the clustering of archaeological pottery.



### DriverGym: Democratising Reinforcement Learning for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2111.06889v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06889v1)
- **Published**: 2021-11-12 11:47:08+00:00
- **Updated**: 2021-11-12 11:47:08+00:00
- **Authors**: Parth Kothari, Christian Perone, Luca Bergamini, Alexandre Alahi, Peter Ondruska
- **Comment**: Accepted to NeurIPS 2021 ML4AD Workshop
- **Journal**: None
- **Summary**: Despite promising progress in reinforcement learning (RL), developing algorithms for autonomous driving (AD) remains challenging: one of the critical issues being the absence of an open-source platform capable of training and effectively validating the RL policies on real-world data. We propose DriverGym, an open-source OpenAI Gym-compatible environment specifically tailored for developing RL algorithms for autonomous driving. DriverGym provides access to more than 1000 hours of expert logged data and also supports reactive and data-driven agent behavior. The performance of an RL policy can be easily validated on real-world data using our extensive and flexible closed-loop evaluation protocol. In this work, we also provide behavior cloning baselines using supervised learning and RL, trained in DriverGym. We make DriverGym code, as well as all the baselines publicly available to further stimulate development from the community.



### Robust Analytics for Video-Based Gait Biometrics
- **Arxiv ID**: http://arxiv.org/abs/2111.06670v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06670v1)
- **Published**: 2021-11-12 11:47:35+00:00
- **Updated**: 2021-11-12 11:47:35+00:00
- **Authors**: Ebenezer R. H. P. Isaac
- **Comment**: Ph.D. Thesis, Anna University, Chennai, Feb. 2018
- **Journal**: None
- **Summary**: Gait analysis is the study of the systematic methods that assess and quantify animal locomotion. Gait finds a unique importance among the many state-of-the-art biometric systems since it does not require the subject's cooperation to the extent required by other modalities. Hence by nature, it is an unobtrusive biometric.   This thesis discusses both hard and soft biometric characteristics of gait. It shows how to identify gender based on gait alone through the Posed-Based Voting scheme. It then describes improving gait recognition accuracy using Genetic Template Segmentation. Members of a wide population can be authenticated using Multiperson Signature Mapping. Finally, the mapping can be improved in a smaller population using Bayesian Thresholding. All methods proposed in this thesis have outperformed their existing state of the art with adequate experimentation and results.



### AlphaRotate: A Rotation Detection Benchmark using TensorFlow
- **Arxiv ID**: http://arxiv.org/abs/2111.06677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06677v1)
- **Published**: 2021-11-12 11:56:40+00:00
- **Updated**: 2021-11-12 11:56:40+00:00
- **Authors**: Xue Yang, Yue Zhou, Junchi Yan
- **Comment**: 7 pages, 1 figure, 1 table
- **Journal**: None
- **Summary**: AlphaRotate is an open-source Tensorflow benchmark for performing scalable rotation detection on various datasets. It currently provides more than 18 popular rotation detection models under a single, well-documented API designed for use by both practitioners and researchers. AlphaRotate regards high performance, robustness, sustainability and scalability as the core concept of design, and all models are covered by unit testing, continuous integration, code coverage, maintainability checks, and visual monitoring and analysis. AlphaRotate can be installed from PyPI and is released under the Apache-2.0 License. Source code is available at https://github.com/yangxue0827/RotationDetection.



### Deep-learning in the bioimaging wild: Handling ambiguous data with deepflash2
- **Arxiv ID**: http://arxiv.org/abs/2111.06693v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06693v1)
- **Published**: 2021-11-12 12:35:26+00:00
- **Updated**: 2021-11-12 12:35:26+00:00
- **Authors**: Matthias Griebel, Dennis Segebarth, Nikolai Stein, Nina Schukraft, Philip Tovote, Robert Blum, Christoph M. Flath
- **Comment**: None
- **Journal**: None
- **Summary**: We present deepflash2, a deep learning solution that facilitates the objective and reliable segmentation of ambiguous bioimages through multi-expert annotations and integrated quality assurance. Thereby, deepflash2 addresses typical challenges that arise during training, evaluation, and application of deep learning models in bioimaging. The tool is embedded in an easy-to-use graphical user interface and offers best-in-class predictive performance for semantic and instance segmentation under economical usage of computational resources.



### Transformer-based Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2111.06707v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06707v1)
- **Published**: 2021-11-12 13:13:20+00:00
- **Updated**: 2021-11-12 13:13:20+00:00
- **Authors**: Ming Lu, Peiyao Guo, Huiqing Shi, Chuntong Cao, Zhan Ma
- **Comment**: None
- **Journal**: None
- **Summary**: A Transformer-based Image Compression (TIC) approach is developed which reuses the canonical variational autoencoder (VAE) architecture with paired main and hyper encoder-decoders. Both main and hyper encoders are comprised of a sequence of neural transformation units (NTUs) to analyse and aggregate important information for more compact representation of input image, while the decoders mirror the encoder-side operations to generate pixel-domain image reconstruction from the compressed bitstream. Each NTU is consist of a Swin Transformer Block (STB) and a convolutional layer (Conv) to best embed both long-range and short-range information; In the meantime, a casual attention module (CAM) is devised for adaptive context modeling of latent features to utilize both hyper and autoregressive priors. The TIC rivals with state-of-the-art approaches including deep convolutional neural networks (CNNs) based learnt image coding (LIC) methods and handcrafted rules-based intra profile of recently-approved Versatile Video Coding (VVC) standard, and requires much less model parameters, e.g., up to 45% reduction to leading-performance LIC.



### Impact of loss functions on the performance of a deep neural network designed to restore low-dose digital mammography
- **Arxiv ID**: http://arxiv.org/abs/2111.06890v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.06890v1)
- **Published**: 2021-11-12 14:15:08+00:00
- **Updated**: 2021-11-12 14:15:08+00:00
- **Authors**: Hongming Shan, Rodrigo de Barros Vimieiro, Lucas Rodrigues Borges, Marcelo Andrade da Costa Vieira, Ge Wang
- **Comment**: 15 pages, 12 figures
- **Journal**: None
- **Summary**: Digital mammography is still the most common imaging tool for breast cancer screening. Although the benefits of using digital mammography for cancer screening outweigh the risks associated with the x-ray exposure, the radiation dose must be kept as low as possible while maintaining the diagnostic utility of the generated images, thus minimizing patient risks. Many studies investigated the feasibility of dose reduction by restoring low-dose images using deep neural networks. In these cases, choosing the appropriate training database and loss function is crucial and impacts the quality of the results. In this work, a modification of the ResNet architecture, with hierarchical skip connections, is proposed to restore low-dose digital mammography. We compared the restored images to the standard full-dose images. Moreover, we evaluated the performance of several loss functions for this task. For training purposes, we extracted 256,000 image patches from a dataset of 400 images of retrospective clinical mammography exams, where different dose levels were simulated to generate low and standard-dose pairs. To validate the network in a real scenario, a physical anthropomorphic breast phantom was used to acquire real low-dose and standard full-dose images in a commercially avaliable mammography system, which were then processed through our trained model. An analytical restoration model for low-dose digital mammography, previously presented, was used as a benchmark in this work. Objective assessment was performed through the signal-to-noise ratio (SNR) and mean normalized squared error (MNSE), decomposed into residual noise and bias. Results showed that the perceptual loss function (PL4) is able to achieve virtually the same noise levels of a full-dose acquisition, while resulting in smaller signal bias compared to other loss functions.



### Monte Carlo dropout increases model repeatability
- **Arxiv ID**: http://arxiv.org/abs/2111.06754v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06754v1)
- **Published**: 2021-11-12 15:03:20+00:00
- **Updated**: 2021-11-12 15:03:20+00:00
- **Authors**: Andreanne Lemay, Katharina Hoebel, Christopher P. Bridge, Didem Egemen, Ana Cecilia Rodriguez, Mark Schiffman, John Peter Campbell, Jayashree Kalpathy-Cramer
- **Comment**: Machine Learning for Health (ML4H) at NeurIPS 2021 - Extended
  Abstract
- **Journal**: None
- **Summary**: The integration of artificial intelligence into clinical workflows requires reliable and robust models. Among the main features of robustness is repeatability. Much attention is given to classification performance without assessing the model repeatability, leading to the development of models that turn out to be unusable in practice. In this work, we evaluate the repeatability of four model types on images from the same patient that were acquired during the same visit. We study the performance of binary, multi-class, ordinal, and regression models on three medical image analysis tasks: cervical cancer screening, breast density estimation, and retinopathy of prematurity classification. Moreover, we assess the impact of sampling Monte Carlo dropout predictions at test time on classification performance and repeatability. Leveraging Monte Carlo predictions significantly increased repeatability for all tasks on the binary, multi-class, and ordinal models leading to an average reduction of the 95% limits of agreement by 17% points.



### Diversity-Promoting Human Motion Interpolation via Conditional Variational Auto-Encoder
- **Arxiv ID**: http://arxiv.org/abs/2111.06762v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06762v1)
- **Published**: 2021-11-12 15:16:48+00:00
- **Updated**: 2021-11-12 15:16:48+00:00
- **Authors**: Chunzhi Gu, Shuofeng Zhao, Chao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a deep generative model based method to generate diverse human motion interpolation results. We resort to the Conditional Variational Auto-Encoder (CVAE) to learn human motion conditioned on a pair of given start and end motions, by leveraging the Recurrent Neural Network (RNN) structure for both the encoder and the decoder. Additionally, we introduce a regularization loss to further promote sample diversity. Once trained, our method is able to generate multiple plausible coherent motions by repetitively sampling from the learned latent space. Experiments on the publicly available dataset demonstrate the effectiveness of our method, in terms of sample plausibility and diversity.



### Identifying On-road Scenarios Predictive of ADHD usingDriving Simulator Time Series Data
- **Arxiv ID**: http://arxiv.org/abs/2111.06774v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.06774v1)
- **Published**: 2021-11-12 15:36:53+00:00
- **Updated**: 2021-11-12 15:36:53+00:00
- **Authors**: David Grethlein, Aleksanteri Sladek, Santiago Ontañón
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce a novel algorithm called Iterative Section Reduction (ISR) to automatically identify sub-intervals of spatiotemporal time series that are predictive of a target classification task. Specifically, using data collected from a driving simulator study, we identify which spatial regions (dubbed "sections") along the simulated routes tend to manifest driving behaviors that are predictive of the presence of Attention Deficit Hyperactivity Disorder (ADHD). Identifying these sections is important for two main reasons: (1) to improve predictive accuracy of the trained models by filtering out non-predictive time series sub-intervals, and (2) to gain insights into which on-road scenarios (dubbed events) elicit distinctly different driving behaviors from patients undergoing treatment for ADHD versus those that are not. Our experimental results show both improved performance over prior efforts (+10% accuracy) and good alignment between the predictive sections identified and scripted on-road events in the simulator (negotiating turns and curves).



### Sci-Net: Scale Invariant Model for Buildings Segmentation from Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/2111.06812v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06812v5)
- **Published**: 2021-11-12 16:45:20+00:00
- **Updated**: 2023-02-01 13:54:51+00:00
- **Authors**: Hasan Nasrallah, Mustafa Shukor, Ali J. Ghandour
- **Comment**: None
- **Journal**: None
- **Summary**: Buildings' segmentation is a fundamental task in the field of earth observation and aerial imagery analysis. Most existing deep learning-based methods in the literature can be applied to a fixed or narrow-range spatial resolution imagery. In practical scenarios, users deal with a broad spectrum of image resolutions. Thus, a given aerial image often needs to be re-sampled to match the spatial resolution of the dataset used to train the deep learning model, which results in a degradation in segmentation performance. To overcome this challenge, we propose, in this manuscript, Scale-invariant Neural Network (Sci-Net) architecture that segments buildings from wide-range spatial resolution aerial images. Specifically, our approach leverages UNet hierarchical representation and Dense Atrous Spatial Pyramid Pooling to extract fine-grained multi-scale representations. Sci-Net significantly outperforms state of the art models on the Open Cities AI and the Multi-Scale Building datasets with a steady improvement margin across different spatial resolutions.



### NRC-GAMMA: Introducing a Novel Large Gas Meter Image Dataset
- **Arxiv ID**: http://arxiv.org/abs/2111.06827v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.06827v1)
- **Published**: 2021-11-12 17:18:38+00:00
- **Updated**: 2021-11-12 17:18:38+00:00
- **Authors**: Ashkan Ebadi, Patrick Paul, Sofia Auer, Stéphane Tremblay
- **Comment**: 12 pages, 7 figures, 1 table
- **Journal**: None
- **Summary**: Automatic meter reading technology is not yet widespread. Gas, electricity, or water accumulation meters reading is mostly done manually on-site either by an operator or by the homeowner. In some countries, the operator takes a picture as reading proof to confirm the reading by checking offline with another operator and/or using it as evidence in case of conflicts or complaints. The whole process is time-consuming, expensive, and prone to errors. Automation can optimize and facilitate such labor-intensive and human error-prone processes. With the recent advances in the fields of artificial intelligence and computer vision, automatic meter reading systems are becoming more viable than ever. Motivated by the recent advances in the field of artificial intelligence and inspired by open-source open-access initiatives in the research community, we introduce a novel large benchmark dataset of real-life gas meter images, named the NRC-GAMMA dataset. The data were collected from an Itron 400A diaphragm gas meter on January 20, 2020, between 00:05 am and 11:59 pm. We employed a systematic approach to label the images, validate the labellings, and assure the quality of the annotations. The dataset contains 28,883 images of the entire gas meter along with 57,766 cropped images of the left and the right dial displays. We hope the NRC-GAMMA dataset helps the research community to design and implement accurate, innovative, intelligent, and reproducible automatic gas meter reading solutions.



### Small or Far Away? Exploiting Deep Super-Resolution and Altitude Data for Aerial Animal Surveillance
- **Arxiv ID**: http://arxiv.org/abs/2111.06830v1
- **DOI**: None
- **Categories**: **cs.CV**, 65D19
- **Links**: [PDF](http://arxiv.org/pdf/2111.06830v1)
- **Published**: 2021-11-12 17:30:55+00:00
- **Updated**: 2021-11-12 17:30:55+00:00
- **Authors**: Mowen Xue, Theo Greenslade, Majid Mirmehdi, Tilo Burghardt
- **Comment**: 11 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: Visuals captured by high-flying aerial drones are increasingly used to assess biodiversity and animal population dynamics around the globe. Yet, challenging acquisition scenarios and tiny animal depictions in airborne imagery, despite ultra-high resolution cameras, have so far been limiting factors for applying computer vision detectors successfully with high confidence. In this paper, we address the problem for the first time by combining deep object detectors with super-resolution techniques and altitude data. In particular, we show that the integration of a holistic attention network based super-resolution approach and a custom-built altitude data exploitation network into standard recognition pipelines can considerably increase the detection efficacy in real-world settings. We evaluate the system on two public, large aerial-capture animal datasets, SAVMAP and AED. We find that the proposed approach can consistently improve over ablated baselines and the state-of-the-art performance for both datasets. In addition, we provide a systematic analysis of the relationship between animal resolution and detection performance. We conclude that super-resolution and altitude knowledge exploitation techniques can significantly increase benchmarks across settings and, thus, should be used routinely when detecting minutely resolved animals in aerial imagery.



### Temporally-Consistent Surface Reconstruction using Metrically-Consistent Atlases
- **Arxiv ID**: http://arxiv.org/abs/2111.06838v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06838v1)
- **Published**: 2021-11-12 17:48:25+00:00
- **Updated**: 2021-11-12 17:48:25+00:00
- **Authors**: Jan Bednarik, Noam Aigerman, Vladimir G. Kim, Siddhartha Chaudhuri, Shaifali Parashar, Mathieu Salzmann, Pascal Fua
- **Comment**: 21 pages. arXiv admin note: substantial text overlap with
  arXiv:2104.06950
- **Journal**: None
- **Summary**: We propose a method for unsupervised reconstruction of a temporally-consistent sequence of surfaces from a sequence of time-evolving point clouds. It yields dense and semantically meaningful correspondences between frames. We represent the reconstructed surfaces as atlases computed by a neural network, which enables us to establish correspondences between frames. The key to making these correspondences semantically meaningful is to guarantee that the metric tensors computed at corresponding points are as similar as possible. We have devised an optimization strategy that makes our method robust to noise and global motions, without a priori correspondences or pre-alignment steps. As a result, our approach outperforms state-of-the-art ones on several challenging datasets. The code is available at https://github.com/bednarikjan/temporally_coherent_surface_reconstruction.



### The self-supervised spectral-spatial attention-based transformer network for automated, accurate prediction of crop nitrogen status from UAV imagery
- **Arxiv ID**: http://arxiv.org/abs/2111.06839v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06839v2)
- **Published**: 2021-11-12 17:48:44+00:00
- **Updated**: 2022-02-16 00:47:08+00:00
- **Authors**: Xin Zhang, Liangxiu Han, Tam Sobeih, Lewis Lappin, Mark Lee, Andew Howard, Aron Kisdi
- **Comment**: None
- **Journal**: None
- **Summary**: Nitrogen (N) fertilizer is routinely applied by farmers to increase crop yields. At present, farmers often over-apply N fertilizer in some locations or at certain times because they do not have high-resolution crop N status data. N-use efficiency can be low, with the remaining N lost to the environment, resulting in higher production costs and environmental pollution. Accurate and timely estimation of N status in crops is crucial to improving cropping systems' economic and environmental sustainability. Destructive approaches based on plant tissue analysis are time consuming and impractical over large fields. Recent advances in remote sensing and deep learning have shown promise in addressing the aforementioned challenges in a non-destructive way. In this work, we propose a novel deep learning framework: a self-supervised spectral-spatial attention-based vision transformer (SSVT). The proposed SSVT introduces a Spectral Attention Block (SAB) and a Spatial Interaction Block (SIB), which allows for simultaneous learning of both spatial and spectral features from UAV digital aerial imagery, for accurate N status prediction in wheat fields. Moreover, the proposed framework introduces local-to-global self-supervised learning to help train the model from unlabelled data. The proposed SSVT has been compared with five state-of-the-art models including: ResNet, RegNet, EfficientNet, EfficientNetV2 and the original vision transformer on both testing and independent datasets. The proposed approach achieved high accuracy (0.96) with good generalizability and reproducibility for wheat N status estimation.



### Deceive D: Adaptive Pseudo Augmentation for GAN Training with Limited Data
- **Arxiv ID**: http://arxiv.org/abs/2111.06849v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.06849v1)
- **Published**: 2021-11-12 18:13:45+00:00
- **Updated**: 2021-11-12 18:13:45+00:00
- **Authors**: Liming Jiang, Bo Dai, Wayne Wu, Chen Change Loy
- **Comment**: NeurIPS 2021. Code: https://github.com/EndlessSora/DeceiveD Project
  page: https://www.mmlab-ntu.com/project/apa/index.html
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) typically require ample data for training in order to synthesize high-fidelity images. Recent studies have shown that training GANs with limited data remains formidable due to discriminator overfitting, the underlying cause that impedes the generator's convergence. This paper introduces a novel strategy called Adaptive Pseudo Augmentation (APA) to encourage healthy competition between the generator and the discriminator. As an alternative method to existing approaches that rely on standard data augmentations or model regularization, APA alleviates overfitting by employing the generator itself to augment the real data distribution with generated images, which deceives the discriminator adaptively. Extensive experiments demonstrate the effectiveness of APA in improving synthesis quality in the low-data regime. We provide a theoretical analysis to examine the convergence and rationality of our new training strategy. APA is simple and effective. It can be added seamlessly to powerful contemporary GANs, such as StyleGAN2, with negligible computational cost.



### Convolutional Nets Versus Vision Transformers for Diabetic Foot Ulcer Classification
- **Arxiv ID**: http://arxiv.org/abs/2111.06894v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06894v1)
- **Published**: 2021-11-12 18:19:57+00:00
- **Updated**: 2021-11-12 18:19:57+00:00
- **Authors**: Adrian Galdran, Gustavo Carneiro, Miguel A. González Ballester
- **Comment**: None
- **Journal**: None
- **Summary**: This paper compares well-established Convolutional Neural Networks (CNNs) to recently introduced Vision Transformers for the task of Diabetic Foot Ulcer Classification, in the context of the DFUC 2021 Grand-Challenge, in which this work attained the first position. Comprehensive experiments demonstrate that modern CNNs are still capable of outperforming Transformers in a low-data regime, likely owing to their ability for better exploiting spatial correlations. In addition, we empirically demonstrate that the recent Sharpness-Aware Minimization (SAM) optimization algorithm considerably improves the generalization capability of both kinds of models. Our results demonstrate that for this task, the combination of CNNs and the SAM optimization process results in superior performance than any other of the considered approaches.



### Multimodal Virtual Point 3D Detection
- **Arxiv ID**: http://arxiv.org/abs/2111.06881v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2111.06881v1)
- **Published**: 2021-11-12 18:58:01+00:00
- **Updated**: 2021-11-12 18:58:01+00:00
- **Authors**: Tianwei Yin, Xingyi Zhou, Philipp Krähenbühl
- **Comment**: NeurIPS 2021, code available at https://tianweiy.github.io/mvp/
- **Journal**: None
- **Summary**: Lidar-based sensing drives current autonomous vehicles. Despite rapid progress, current Lidar sensors still lag two decades behind traditional color cameras in terms of resolution and cost. For autonomous driving, this means that large objects close to the sensors are easily visible, but far-away or small objects comprise only one measurement or two. This is an issue, especially when these objects turn out to be driving hazards. On the other hand, these same objects are clearly visible in onboard RGB sensors. In this work, we present an approach to seamlessly fuse RGB sensors into Lidar-based 3D recognition. Our approach takes a set of 2D detections to generate dense 3D virtual points to augment an otherwise sparse 3D point cloud. These virtual points naturally integrate into any standard Lidar-based 3D detectors along with regular Lidar measurements. The resulting multi-modal detector is simple and effective. Experimental results on the large-scale nuScenes dataset show that our framework improves a strong CenterPoint baseline by a significant 6.6 mAP, and outperforms competing fusion approaches. Code and more visualizations are available at https://tianweiy.github.io/mvp/



### Visual Intelligence through Human Interaction
- **Arxiv ID**: http://arxiv.org/abs/2111.06913v1
- **DOI**: 10.1007/978-3-030-82681-9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06913v1)
- **Published**: 2021-11-12 19:37:17+00:00
- **Updated**: 2021-11-12 19:37:17+00:00
- **Authors**: Ranjay Krishna, Mitchell Gordon, Li Fei-Fei, Michael Bernstein
- **Comment**: This is a preprint of the following chapter: Ranjay Krishna, Mitchell
  Gordon, Li Fei-Fei, Michael Bernstein, Visual Intelligence through Human
  Interaction, published in Artificial Intelligence for Human Computer
  Interaction: A Modern Approach, edited by Yang Li and Otmar Hilliges, 2021,
  Springer reproduced with permission of Springer Nature. arXiv admin note:
  substantial text overlap with arXiv:1602.04506, arXiv:1904.01121
- **Journal**: None
- **Summary**: Over the last decade, Computer Vision, the branch of Artificial Intelligence aimed at understanding the visual world, has evolved from simply recognizing objects in images to describing pictures, answering questions about images, aiding robots maneuver around physical spaces and even generating novel visual content. As these tasks and applications have modernized, so too has the reliance on more data, either for model training or for evaluation. In this chapter, we demonstrate that novel interaction strategies can enable new forms of data collection and evaluation for Computer Vision. First, we present a crowdsourcing interface for speeding up paid data collection by an order of magnitude, feeding the data-hungry nature of modern vision models. Second, we explore a method to increase volunteer contributions using automated social interventions. Third, we develop a system to ensure human evaluation of generative vision models are reliable, affordable and grounded in psychophysics theory. We conclude with future opportunities for Human-Computer Interaction to aid Computer Vision.



### Action2video: Generating Videos of Human 3D Actions
- **Arxiv ID**: http://arxiv.org/abs/2111.06925v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06925v2)
- **Published**: 2021-11-12 20:20:37+00:00
- **Updated**: 2021-12-19 00:18:34+00:00
- **Authors**: Chuan Guo, Xinxin Zuo, Sen Wang, Xinshuang Liu, Shihao Zou, Minglun Gong, Li Cheng
- **Comment**: Accepted by IJCV
- **Journal**: None
- **Summary**: We aim to tackle the interesting yet challenging problem of generating videos of diverse and natural human motions from prescribed action categories. The key issue lies in the ability to synthesize multiple distinct motion sequences that are realistic in their visual appearances. It is achieved in this paper by a two-step process that maintains internal 3D pose and shape representations, action2motion and motion2video. Action2motion stochastically generates plausible 3D pose sequences of a prescribed action category, which are processed and rendered by motion2video to form 2D videos. Specifically, the Lie algebraic theory is engaged in representing natural human motions following the physical law of human kinematics; a temporal variational auto-encoder (VAE) is developed that encourages diversity of output motions. Moreover, given an additional input image of a clothed human character, an entire pipeline is proposed to extract his/her 3D detailed shape, and to render in videos the plausible motions from different views. This is realized by improving existing methods to extract 3D human shapes and textures from single 2D images, rigging, animating, and rendering to form 2D videos of human motions. It also necessitates the curation and reannotation of 3D human motion datasets for training purpose. Thorough empirical experiments including ablation study, qualitative and quantitative evaluations manifest the applicability of our approach, and demonstrate its competitiveness in addressing related tasks, where components of our approach are compared favorably to the state-of-the-arts.



### Contrastive Feature Loss for Image Prediction
- **Arxiv ID**: http://arxiv.org/abs/2111.06934v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.06934v1)
- **Published**: 2021-11-12 20:39:52+00:00
- **Updated**: 2021-11-12 20:39:52+00:00
- **Authors**: Alex Andonian, Taesung Park, Bryan Russell, Phillip Isola, Jun-Yan Zhu, Richard Zhang
- **Comment**: Appeared in Advances in Image Manipulation Workshop at ICCV 2021.
  GitHub: https://github.com/alexandonian/contrastive-feature-loss
- **Journal**: None
- **Summary**: Training supervised image synthesis models requires a critic to compare two images: the ground truth to the result. Yet, this basic functionality remains an open problem. A popular line of approaches uses the L1 (mean absolute error) loss, either in the pixel or the feature space of pretrained deep networks. However, we observe that these losses tend to produce overly blurry and grey images, and other techniques such as GANs need to be employed to fight these artifacts. In this work, we introduce an information theory based approach to measuring similarity between two images. We argue that a good reconstruction should have high mutual information with the ground truth. This view enables learning a lightweight critic to "calibrate" a feature space in a contrastive manner, such that reconstructions of corresponding spatial patches are brought together, while other patches are repulsed. We show that our formulation immediately boosts the perceptual realism of output images when used as a drop-in replacement for the L1 loss, with or without an additional GAN loss.



### Through-Foliage Tracking with Airborne Optical Sectioning
- **Arxiv ID**: http://arxiv.org/abs/2111.06959v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06959v2)
- **Published**: 2021-11-12 21:54:25+00:00
- **Updated**: 2021-11-30 08:51:47+00:00
- **Authors**: Rakesh John Amala Arokia Nathan, Indrajit Kurmi, David C. Schedl, Oliver Bimber
- **Comment**: 9 Pages, 9 Figures, 1 Table and supplementary videos and material
- **Journal**: None
- **Summary**: Detecting and tracking moving targets through foliage is difficult, and for many cases even impossible in regular aerial images and videos. We present an initial light-weight and drone-operated 1D camera array that supports parallel synthetic aperture aerial imaging. Our main finding is that color anomaly detection benefits significantly from image integration when compared to conventional raw images or video frames (on average 97% vs. 42% in precision in our field experiments). We demonstrate, that these two contributions can lead to the detection and tracking of moving people through densely occluding forest.



### Scalable Diverse Model Selection for Accessible Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.06977v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06977v2)
- **Published**: 2021-11-12 22:53:28+00:00
- **Updated**: 2022-01-11 02:49:18+00:00
- **Authors**: Daniel Bolya, Rohit Mittapalli, Judy Hoffman
- **Comment**: NeurIPS 2021 camera ready v2 + Appendix. Added a missing citation and
  fixed Table 4 header. Table 1 is still purple. No, I do not know why
- **Journal**: None
- **Summary**: With the preponderance of pretrained deep learning models available off-the-shelf from model banks today, finding the best weights to fine-tune to your use-case can be a daunting task. Several methods have recently been proposed to find good models for transfer learning, but they either don't scale well to large model banks or don't perform well on the diversity of off-the-shelf models. Ideally the question we want to answer is, "given some data and a source model, can you quickly predict the model's accuracy after fine-tuning?" In this paper, we formalize this setting as "Scalable Diverse Model Selection" and propose several benchmarks for evaluating on this task. We find that existing model selection and transferability estimation methods perform poorly here and analyze why this is the case. We then introduce simple techniques to improve the performance and speed of these algorithms. Finally, we iterate on existing methods to create PARC, which outperforms all other methods on diverse model selection. We have released the benchmarks and method code in hope to inspire future work in model selection for accessible transfer learning.



### Learning Online for Unified Segmentation and Tracking Models
- **Arxiv ID**: http://arxiv.org/abs/2111.06994v1
- **DOI**: 10.1109/IJCNN52387.2021.9533455.
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.06994v1)
- **Published**: 2021-11-12 23:52:59+00:00
- **Updated**: 2021-11-12 23:52:59+00:00
- **Authors**: Tianyu Zhu, Rongkai Ma, Mehrtash Harandi, Tom Drummond
- **Comment**: None
- **Journal**: International Joint Conference on Neural Networks (IJCNN), 2021,
  pp. 1-8
- **Summary**: Tracking requires building a discriminative model for the target in the inference stage. An effective way to achieve this is online learning, which can comfortably outperform models that are only trained offline. Recent research shows that visual tracking benefits significantly from the unification of visual tracking and segmentation due to its pixel-level discrimination. However, it imposes a great challenge to perform online learning for such a unified model. A segmentation model cannot easily learn from prior information given in the visual tracking scenario. In this paper, we propose TrackMLP: a novel meta-learning method optimized to learn from only partial information to resolve the imposed challenge. Our model is capable of extensively exploiting limited prior information hence possesses much stronger target-background discriminability than other online learning methods. Empirically, we show that our model achieves state-of-the-art performance and tangible improvement over competing models. Our model achieves improved average overlaps of66.0%,67.1%, and68.5% in VOT2019, VOT2018, and VOT2016 datasets, which are 6.4%,7.3%, and6.4% higher than our baseline. Code will be made publicly available.



