# Arxiv Papers in cs.CV on 2021-11-20
### Discrete Representations Strengthen Vision Transformer Robustness
- **Arxiv ID**: http://arxiv.org/abs/2111.10493v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.10493v2)
- **Published**: 2021-11-20 01:49:56+00:00
- **Updated**: 2022-04-02 01:51:00+00:00
- **Authors**: Chengzhi Mao, Lu Jiang, Mostafa Dehghani, Carl Vondrick, Rahul Sukthankar, Irfan Essa
- **Comment**: None
- **Journal**: None
- **Summary**: Vision Transformer (ViT) is emerging as the state-of-the-art architecture for image recognition. While recent studies suggest that ViTs are more robust than their convolutional counterparts, our experiments find that ViTs trained on ImageNet are overly reliant on local textures and fail to make adequate use of shape information. ViTs thus have difficulties generalizing to out-of-distribution, real-world data. To address this deficiency, we present a simple and effective architecture modification to ViT's input layer by adding discrete tokens produced by a vector-quantized encoder. Different from the standard continuous pixel tokens, discrete tokens are invariant under small perturbations and contain less information individually, which promote ViTs to learn global information that is invariant. Experimental results demonstrate that adding discrete representation on four architecture variants strengthens ViT robustness by up to 12% across seven ImageNet robustness benchmarks while maintaining the performance on ImageNet.



### CamLiFlow: Bidirectional Camera-LiDAR Fusion for Joint Optical Flow and Scene Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/2111.10502v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.10502v4)
- **Published**: 2021-11-20 02:58:38+00:00
- **Updated**: 2022-04-12 04:32:52+00:00
- **Authors**: Haisong Liu, Tao Lu, Yihui Xu, Jia Liu, Wenjie Li, Lijun Chen
- **Comment**: Accepted to CVPR 2022 (Oral)
- **Journal**: None
- **Summary**: In this paper, we study the problem of jointly estimating the optical flow and scene flow from synchronized 2D and 3D data. Previous methods either employ a complex pipeline that splits the joint task into independent stages, or fuse 2D and 3D information in an "early-fusion" or "late-fusion" manner. Such one-size-fits-all approaches suffer from a dilemma of failing to fully utilize the characteristic of each modality or to maximize the inter-modality complementarity. To address the problem, we propose a novel end-to-end framework, called CamLiFlow. It consists of 2D and 3D branches with multiple bidirectional connections between them in specific layers. Different from previous work, we apply a point-based 3D branch to better extract the geometric features and design a symmetric learnable operator to fuse dense image features and sparse point features. Experiments show that CamLiFlow achieves better performance with fewer parameters. Our method ranks 1st on the KITTI Scene Flow benchmark, outperforming the previous art with 1/7 parameters. Code is available at https://github.com/MCG-NJU/CamLiFlow.



### StylePart: Image-based Shape Part Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2111.10520v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2111.10520v3)
- **Published**: 2021-11-20 05:30:08+00:00
- **Updated**: 2022-04-07 11:54:42+00:00
- **Authors**: I-Chao Shen, Li-Wen Su, Yu-Ting Wu, Bing-Yu Chen
- **Comment**: 10 pages, Project page:
  https://jdily.github.io/proj_site/stylepart_proj.html
- **Journal**: None
- **Summary**: Due to a lack of image-based "part controllers", shape manipulation of man-made shape images, such as resizing the backrest of a chair or replacing a cup handle is not intuitive. To tackle this problem, we present StylePart, a framework that enables direct shape manipulation of an image by leveraging generative models of both images and 3D shapes. Our key contribution is a shape-consistent latent mapping function that connects the image generative latent space and the 3D man-made shape attribute latent space. Our method "forwardly maps" the image content to its corresponding 3D shape attributes, where the shape part can be easily manipulated. The attribute codes of the manipulated 3D shape are then "backwardly mapped" to the image latent code to obtain the final manipulated image. We demonstrate our approach through various manipulation tasks, including part replacement, part resizing, and viewpoint manipulation, and evaluate its effectiveness through extensive ablation studies.



### ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2111.10524v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.10524v1)
- **Published**: 2021-11-20 06:15:30+00:00
- **Updated**: 2021-11-20 06:15:30+00:00
- **Authors**: Zhaoxin Fan, Zhengbo Song, Jian Xu, Zhicheng Wang, Kejian Wu, Hongyan Liu, Jun He
- **Comment**: 13 pages, 9 figures, 7 tables
- **Journal**: None
- **Summary**: Recently, category-level 6D object pose estimation has achieved significant improvements with the development of reconstructing canonical 3D representations. However, the reconstruction quality of existing methods is still far from excellent. In this paper, we propose a novel Adversarial Canonical Representation Reconstruction Network named ACR-Pose. ACR-Pose consists of a Reconstructor and a Discriminator. The Reconstructor is primarily composed of two novel sub-modules: Pose-Irrelevant Module (PIM) and Relational Reconstruction Module (RRM). PIM tends to learn canonical-related features to make the Reconstructor insensitive to rotation and translation, while RRM explores essential relational information between different input modalities to generate high-quality features. Subsequently, a Discriminator is employed to guide the Reconstructor to generate realistic canonical representations. The Reconstructor and the Discriminator learn to optimize through adversarial training. Experimental results on the prevalent NOCS-CAMERA and NOCS-REAL datasets demonstrate that our method achieves state-of-the-art performance.



### FAMINet: Learning Real-time Semi-supervised Video Object Segmentation with Steepest Optimized Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2111.10531v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.10531v1)
- **Published**: 2021-11-20 07:24:33+00:00
- **Updated**: 2021-11-20 07:24:33+00:00
- **Authors**: Ziyang Liu, Jingmeng Liu, Weihai Chen, Xingming Wu, Zhengguo Li
- **Comment**: Accepted by TIM (IEEE TRANSACTIONS ON INSTRUMENTATION AND
  MEASUREMENT)
- **Journal**: None
- **Summary**: Semi-supervised video object segmentation (VOS) aims to segment a few moving objects in a video sequence, where these objects are specified by annotation of first frame. The optical flow has been considered in many existing semi-supervised VOS methods to improve the segmentation accuracy. However, the optical flow-based semi-supervised VOS methods cannot run in real time due to high complexity of optical flow estimation. A FAMINet, which consists of a feature extraction network (F), an appearance network (A), a motion network (M), and an integration network (I), is proposed in this study to address the abovementioned problem. The appearance network outputs an initial segmentation result based on static appearances of objects. The motion network estimates the optical flow via very few parameters, which are optimized rapidly by an online memorizing algorithm named relaxed steepest descent. The integration network refines the initial segmentation result using the optical flow. Extensive experiments demonstrate that the FAMINet outperforms other state-of-the-art semi-supervised VOS methods on the DAVIS and YouTube-VOS benchmarks, and it achieves a good trade-off between accuracy and efficiency. Our code is available at https://github.com/liuziyang123/FAMINet.



### Temporal-MPI: Enabling Multi-Plane Images for Dynamic Scene Modelling via Temporal Basis Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.10533v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.10533v2)
- **Published**: 2021-11-20 07:34:28+00:00
- **Updated**: 2022-08-06 08:15:38+00:00
- **Authors**: Wenpeng Xing, Jie Chen
- **Comment**: In ECCV 2022
- **Journal**: None
- **Summary**: Novel view synthesis of static scenes has achieved remarkable advancements in producing photo-realistic results. However, key challenges remain for immersive rendering of dynamic scenes. One of the seminal image-based rendering method, the multi-plane image (MPI), produces high novel-view synthesis quality for static scenes. But modelling dynamic contents by MPI is not studied. In this paper, we propose a novel Temporal-MPI representation which is able to encode the rich 3D and dynamic variation information throughout the entire video as compact temporal basis and coefficients jointly learned. Time-instance MPI for rendering can be generated efficiently using mini-seconds by linear combinations of temporal basis and coefficients from Temporal-MPI. Thus novel-views at arbitrary time-instance will be able to be rendered via Temporal-MPI in real-time with high visual quality. Our method is trained and evaluated on Nvidia Dynamic Scene Dataset. We show that our proposed Temporal- MPI is much faster and more compact compared with other state-of-the-art dynamic scene modelling methods.



### Towards Scalable Unpaired Virtual Try-On via Patch-Routed Spatially-Adaptive GAN
- **Arxiv ID**: http://arxiv.org/abs/2111.10544v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.10544v1)
- **Published**: 2021-11-20 08:36:12+00:00
- **Updated**: 2021-11-20 08:36:12+00:00
- **Authors**: Zhenyu Xie, Zaiyu Huang, Fuwei Zhao, Haoye Dong, Michael Kampffmeyer, Xiaodan Liang
- **Comment**: 12 pages, 8 figures, 35th Conference on Neural Information Processing
  Systems
- **Journal**: None
- **Summary**: Image-based virtual try-on is one of the most promising applications of human-centric image generation due to its tremendous real-world potential. Yet, as most try-on approaches fit in-shop garments onto a target person, they require the laborious and restrictive construction of a paired training dataset, severely limiting their scalability. While a few recent works attempt to transfer garments directly from one person to another, alleviating the need to collect paired datasets, their performance is impacted by the lack of paired (supervised) information. In particular, disentangling style and spatial information of the garment becomes a challenge, which existing methods either address by requiring auxiliary data or extensive online optimization procedures, thereby still inhibiting their scalability. To achieve a \emph{scalable} virtual try-on system that can transfer arbitrary garments between a source and a target person in an unsupervised manner, we thus propose a texture-preserving end-to-end network, the PAtch-routed SpaTially-Adaptive GAN (PASTA-GAN), that facilitates real-world unpaired virtual try-on. Specifically, to disentangle the style and spatial information of each garment, PASTA-GAN consists of an innovative patch-routed disentanglement module for successfully retaining garment texture and shape characteristics. Guided by the source person keypoints, the patch-routed disentanglement module first decouples garments into normalized patches, thus eliminating the inherent spatial information of the garment, and then reconstructs the normalized patches to the warped garment complying with the target person pose. Given the warped garment, PASTA-GAN further introduces novel spatially-adaptive residual blocks that guide the generator to synthesize more realistic garment details.



### Delving into Rectifiers in Style-Based Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2111.10546v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2111.10546v2)
- **Published**: 2021-11-20 08:50:39+00:00
- **Updated**: 2021-11-23 09:13:44+00:00
- **Authors**: Yipeng Zhang, Bingliang Hu, Hailong Ning, Quang Wang
- **Comment**: 14 pages,14 figures
- **Journal**: None
- **Summary**: While modern image translation techniques can create photorealistic synthetic images, they have limited style controllability, thus could suffer from translation errors. In this work, we show that the activation function is one of the crucial components in controlling the direction of image synthesis. Specifically, we explicitly demonstrated that the slope parameters of the rectifier could change the data distribution and be used independently to control the direction of translation. To improve the style controllability, two simple but effective techniques are proposed, including Adaptive ReLU (AdaReLU) and structural adaptive function. The AdaReLU can dynamically adjust the slope parameters according to the target style and can be utilized to increase the controllability by combining with Adaptive Instance Normalization (AdaIN). Meanwhile, the structural adaptative function enables rectifiers to manipulate the structure of feature maps more effectively. It is composed of the proposed structural convolution (StruConv), an efficient convolutional module that can choose the area to be activated based on the mean and variance specified by AdaIN. Extensive experiments show that the proposed techniques can greatly increase the network controllability and output diversity in style-based image translation tasks.



### Teacher-Student Training and Triplet Loss to Reduce the Effect of Drastic Face Occlusion
- **Arxiv ID**: http://arxiv.org/abs/2111.10561v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.10561v1)
- **Published**: 2021-11-20 11:13:46+00:00
- **Updated**: 2021-11-20 11:13:46+00:00
- **Authors**: Mariana-Iuliana Georgescu, Georgian Duta, Radu Tudor Ionescu
- **Comment**: Accepted in Machine Vision and Applications. arXiv admin note: text
  overlap with arXiv:2008.01003
- **Journal**: None
- **Summary**: We study a series of recognition tasks in two realistic scenarios requiring the analysis of faces under strong occlusion. On the one hand, we aim to recognize facial expressions of people wearing Virtual Reality (VR) headsets. On the other hand, we aim to estimate the age and identify the gender of people wearing surgical masks. For all these tasks, the common ground is that half of the face is occluded. In this challenging setting, we show that convolutional neural networks (CNNs) trained on fully-visible faces exhibit very low performance levels. While fine-tuning the deep learning models on occluded faces is extremely useful, we show that additional performance gains can be obtained by distilling knowledge from models trained on fully-visible faces. To this end, we study two knowledge distillation methods, one based on teacher-student training and one based on triplet loss. Our main contribution consists in a novel approach for knowledge distillation based on triplet loss, which generalizes across models and tasks. Furthermore, we consider combining distilled models learned through conventional teacher-student training or through our novel teacher-student training based on triplet loss. We provide empirical evidence showing that, in most cases, both individual and combined knowledge distillation methods bring statistically significant performance improvements. We conduct experiments with three different neural models (VGG-f, VGG-face, ResNet-50) on various tasks (facial expression recognition, gender recognition, age estimation), showing consistent improvements regardless of the model or task.



### A Deeper Look into DeepCap
- **Arxiv ID**: http://arxiv.org/abs/2111.10563v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.10563v1)
- **Published**: 2021-11-20 11:34:33+00:00
- **Updated**: 2021-11-20 11:34:33+00:00
- **Authors**: Marc Habermann, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, Christian Theobalt
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2003.08325
- **Journal**: None
- **Summary**: Human performance capture is a highly important computer vision problem with many applications in movie production and virtual/augmented reality. Many previous performance capture approaches either required expensive multi-view setups or did not recover dense space-time coherent geometry with frame-to-frame correspondences. We propose a novel deep learning approach for monocular dense human performance capture. Our method is trained in a weakly supervised manner based on multi-view supervision completely removing the need for training data with 3D ground truth annotations. The network architecture is based on two separate networks that disentangle the task into a pose estimation and a non-rigid surface deformation step. Extensive qualitative and quantitative evaluations show that our approach outperforms the state of the art in terms of quality and robustness. This work is an extended version of DeepCap where we provide more detailed explanations, comparisons and results as well as applications.



### AGA-GAN: Attribute Guided Attention Generative Adversarial Network with U-Net for Face Hallucination
- **Arxiv ID**: http://arxiv.org/abs/2111.10591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.10591v1)
- **Published**: 2021-11-20 13:43:03+00:00
- **Updated**: 2021-11-20 13:43:03+00:00
- **Authors**: Abhishek Srivastava, Sukalpa Chanda, Umapada Pal
- **Comment**: 27 pages, 9 Figures
- **Journal**: None
- **Summary**: The performance of facial super-resolution methods relies on their ability to recover facial structures and salient features effectively. Even though the convolutional neural network and generative adversarial network-based methods deliver impressive performances on face hallucination tasks, the ability to use attributes associated with the low-resolution images to improve performance is unsatisfactory. In this paper, we propose an Attribute Guided Attention Generative Adversarial Network which employs novel attribute guided attention (AGA) modules to identify and focus the generation process on various facial features in the image. Stacking multiple AGA modules enables the recovery of both high and low-level facial structures. We design the discriminator to learn discriminative features exploiting the relationship between the high-resolution image and their corresponding facial attribute annotations. We then explore the use of U-Net based architecture to refine existing predictions and synthesize further facial details. Extensive experiments across several metrics show that our AGA-GAN and AGA-GAN+U-Net framework outperforms several other cutting-edge face hallucination state-of-the-art methods. We also demonstrate the viability of our method when every attribute descriptor is not known and thus, establishing its application in real-world scenarios.



### Unsupervised Domain Adaptation for Device-free Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.10602v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2111.10602v1)
- **Published**: 2021-11-20 14:25:35+00:00
- **Updated**: 2021-11-20 14:25:35+00:00
- **Authors**: Bin-Bin Zhang, Dongheng Zhang, Yadong Li, Yang Hu, Yan Chen
- **Comment**: The paper is submitted to the journal of IEEE Transactions on Mobile
  Computing. And it is still under review
- **Journal**: None
- **Summary**: Device free human gesture recognition with Radio Frequency signals has attained acclaim due to the omnipresence, privacy protection, and broad coverage nature of RF signals. However, neural network models trained for recognition with data collected from a specific domain suffer from significant performance degradation when applied to a new domain. To tackle this challenge, we propose an unsupervised domain adaptation framework for device free gesture recognition by making effective use of the unlabeled target domain data. Specifically, we apply pseudo labeling and consistency regularization with elaborated design on target domain data to produce pseudo labels and align instance feature of the target domain. Then, we design two data augmentation methods by randomly erasing the input data to enhance the robustness of the model. Furthermore, we apply a confidence control constraint to tackle the overconfidence problem. We conduct extensive experiments on a public WiFi dataset and a public millimeter wave radar dataset. The experimental results demonstrate the superior effectiveness of the proposed framework.



### Exploiting Multi-Scale Fusion, Spatial Attention and Patch Interaction Techniques for Text-Independent Writer Identification
- **Arxiv ID**: http://arxiv.org/abs/2111.10605v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.10605v1)
- **Published**: 2021-11-20 14:41:36+00:00
- **Updated**: 2021-11-20 14:41:36+00:00
- **Authors**: Abhishek Srivastava, Sukalpa Chanda, Umapada Pal
- **Comment**: 14 pages, 4 figures
- **Journal**: None
- **Summary**: Text independent writer identification is a challenging problem that differentiates between different handwriting styles to decide the author of the handwritten text. Earlier writer identification relied on handcrafted features to reveal pieces of differences between writers. Recent work with the advent of convolutional neural network, deep learning-based methods have evolved. In this paper, three different deep learning techniques - spatial attention mechanism, multi-scale feature fusion and patch-based CNN were proposed to effectively capture the difference between each writer's handwriting. Our methods are based on the hypothesis that handwritten text images have specific spatial regions which are more unique to a writer's style, multi-scale features propagate characteristic features with respect to individual writers and patch-based features give more general and robust representations that helps to discriminate handwriting from different writers. The proposed methods outperforms various state-of-the-art methodologies on word-level and page-level writer identification methods on three publicly available datasets - CVL, Firemaker, CERUG-EN datasets and give comparable performance on the IAM dataset.



### Constrained Deep One-Class Feature Learning For Classifying Imbalanced Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2111.10610v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.10610v2)
- **Published**: 2021-11-20 15:25:24+00:00
- **Updated**: 2022-04-14 08:17:46+00:00
- **Authors**: Long Gao, Chang Liu, Dooman Arefan, Ashok Panigrahy, Shandong Wu
- **Comment**: Corrected inaccurate information in affiliation and acknowledgment
- **Journal**: None
- **Summary**: Medical image data are usually imbalanced across different classes. One-class classification has attracted increasing attention to address the data imbalance problem by distinguishing the samples of the minority class from the majority class. Previous methods generally aim to either learn a new feature space to map training samples together or to fit training samples by autoencoder-like models. These methods mainly focus on capturing either compact or descriptive features, where the information of the samples of a given one class is not sufficiently utilized. In this paper, we propose a novel deep learning-based method to learn compact features by adding constraints on the bottleneck features, and to preserve descriptive features by training an autoencoder at the same time. Through jointly optimizing the constraining loss and the autoencoder's reconstruction loss, our method can learn more relevant features associated with the given class, making the majority and minority samples more distinguishable. Experimental results on three clinical datasets (including the MRI breast images, FFDM breast images and chest X-ray images) obtains state-of-art performance compared to previous methods.



### A photosensor employing data-driven binning for ultrafast image recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.10612v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.10612v1)
- **Published**: 2021-11-20 15:38:39+00:00
- **Updated**: 2021-11-20 15:38:39+00:00
- **Authors**: Lukas Mennel, Aday J. Molina-Mendoza, Matthias Paur, Dmitry K. Polyushkin, Dohyun Kwak, Miriam Giparakis, Maximilian Beiser, Aaron Maxwell Andrews, Thomas Mueller
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Pixel binning is a technique, widely used in optical image acquisition and spectroscopy, in which adjacent detector elements of an image sensor are combined into larger pixels. This reduces the amount of data to be processed as well as the impact of noise, but comes at the cost of a loss of information. Here, we push the concept of binning to its limit by combining a large fraction of the sensor elements into a single superpixel that extends over the whole face of the chip. For a given pattern recognition task, its optimal shape is determined from training data using a machine learning algorithm. We demonstrate the classification of optically projected images from the MNIST dataset on a nanosecond timescale, with enhanced sensitivity and without loss of classification accuracy. Our concept is not limited to imaging alone but can also be applied in optical spectroscopy or other sensing applications.



### GMSRF-Net: An improved generalizability with global multi-scale residual fusion network for polyp segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.10614v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.10614v1)
- **Published**: 2021-11-20 15:41:59+00:00
- **Updated**: 2021-11-20 15:41:59+00:00
- **Authors**: Abhishek Srivastava, Sukalpa Chanda, Debesh Jha, Umapada Pal, Sharib Ali
- **Comment**: None
- **Journal**: None
- **Summary**: Colonoscopy is a gold standard procedure but is highly operator-dependent. Efforts have been made to automate the detection and segmentation of polyps, a precancerous precursor, to effectively minimize missed rate. Widely used computer-aided polyp segmentation systems actuated by encoder-decoder have achieved high performance in terms of accuracy. However, polyp segmentation datasets collected from varied centers can follow different imaging protocols leading to difference in data distribution. As a result, most methods suffer from performance drop and require re-training for each specific dataset. We address this generalizability issue by proposing a global multi-scale residual fusion network (GMSRF-Net). Our proposed network maintains high-resolution representations while performing multi-scale fusion operations for all resolution scales. To further leverage scale information, we design cross multi-scale attention (CMSA) and multi-scale feature selection (MSFS) modules within the GMSRF-Net. The repeated fusion operations gated by CMSA and MSFS demonstrate improved generalizability of the network. Experiments conducted on two different polyp segmentation datasets show that our proposed GMSRF-Net outperforms the previous top-performing state-of-the-art method by 8.34% and 10.31% on unseen CVC-ClinicDB and unseen Kvasir-SEG, in terms of dice coefficient.



### Extracting Deformation-Aware Local Features by Learning to Deform
- **Arxiv ID**: http://arxiv.org/abs/2111.10617v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.10617v1)
- **Published**: 2021-11-20 15:46:33+00:00
- **Updated**: 2021-11-20 15:46:33+00:00
- **Authors**: Guilherme Potje, Renato Martins, Felipe Cadar, Erickson R. Nascimento
- **Comment**: To appear in Proceedings of the Thirty-fifth Annual Conference on
  Neural Information Processing Systems (NeurIPS) 2021
- **Journal**: None
- **Summary**: Despite the advances in extracting local features achieved by handcrafted and learning-based descriptors, they are still limited by the lack of invariance to non-rigid transformations. In this paper, we present a new approach to compute features from still images that are robust to non-rigid deformations to circumvent the problem of matching deformable surfaces and objects. Our deformation-aware local descriptor, named DEAL, leverages a polar sampling and a spatial transformer warping to provide invariance to rotation, scale, and image deformations. We train the model architecture end-to-end by applying isometric non-rigid deformations to objects in a simulated environment as guidance to provide highly discriminative local features. The experiments show that our method outperforms state-of-the-art handcrafted, learning-based image, and RGB-D descriptors in different datasets with both real and realistic synthetic deformable objects in still images. The source code and trained model of the descriptor are publicly available at https://www.verlab.dcc.ufmg.br/descriptors/neurips2021.



### PAANet: Progressive Alternating Attention for Automatic Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.10618v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.10618v1)
- **Published**: 2021-11-20 15:49:42+00:00
- **Updated**: 2021-11-20 15:49:42+00:00
- **Authors**: Abhishek Srivastava, Sukalpa Chanda, Debesh Jha, Michael A. Riegler, Pål Halvorsen, Dag Johansen, Umapada Pal
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image segmentation can provide detailed information for clinical analysis which can be useful for scenarios where the detailed location of a finding is important. Knowing the location of disease can play a vital role in treatment and decision-making. Convolutional neural network (CNN) based encoder-decoder techniques have advanced the performance of automated medical image segmentation systems. Several such CNN-based methodologies utilize techniques such as spatial- and channel-wise attention to enhance performance. Another technique that has drawn attention in recent years is residual dense blocks (RDBs). The successive convolutional layers in densely connected blocks are capable of extracting diverse features with varied receptive fields and thus, enhancing performance. However, consecutive stacked convolutional operators may not necessarily generate features that facilitate the identification of the target structures. In this paper, we propose a progressive alternating attention network (PAANet). We develop progressive alternating attention dense (PAAD) blocks, which construct a guiding attention map (GAM) after every convolutional layer in the dense blocks using features from all scales. The GAM allows the following layers in the dense blocks to focus on the spatial locations relevant to the target region. Every alternate PAAD block inverts the GAM to generate a reverse attention map which guides ensuing layers to extract boundary and edge-related information, refining the segmentation process. Our experiments on three different biomedical image segmentation datasets exhibit that our PAANet achieves favourable performance when compared to other state-of-the-art methods.



### Medical Knowledge-Guided Deep Learning for Imbalanced Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2111.10620v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.10620v2)
- **Published**: 2021-11-20 16:14:19+00:00
- **Updated**: 2022-04-14 08:11:04+00:00
- **Authors**: Long Gao, Chang Liu, Dooman Arefan, Ashok Panigrahy, Margarita L. Zuley, Shandong Wu
- **Comment**: Corrected inaccurate information in affiliation and acknowledgment
- **Journal**: None
- **Summary**: Deep learning models have gained remarkable performance on a variety of image classification tasks. However, many models suffer from limited performance in clinical or medical settings when data are imbalanced. To address this challenge, we propose a medical-knowledge-guided one-class classification approach that leverages domain-specific knowledge of classification tasks to boost the model's performance. The rationale behind our approach is that some existing prior medical knowledge can be incorporated into data-driven deep learning to facilitate model learning. We design a deep learning-based one-class classification pipeline for imbalanced image classification, and demonstrate in three use cases how we take advantage of medical knowledge of each specific classification task by generating additional middle classes to achieve higher classification performances. We evaluate our approach on three different clinical image classification tasks (a total of 8459 images) and show superior model performance when compared to six state-of-the-art methods. All codes of this work will be publicly available upon acceptance of the paper.



### FlowVOS: Weakly-Supervised Visual Warping for Detail-Preserving and Temporally Consistent Single-Shot Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.10621v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.10621v1)
- **Published**: 2021-11-20 16:17:10+00:00
- **Updated**: 2021-11-20 16:17:10+00:00
- **Authors**: Julia Gong, F. Christopher Holsinger, Serena Yeung
- **Comment**: To appear at BMVC 2021; 13 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: We consider the task of semi-supervised video object segmentation (VOS). Our approach mitigates shortcomings in previous VOS work by addressing detail preservation and temporal consistency using visual warping. In contrast to prior work that uses full optical flow, we introduce a new foreground-targeted visual warping approach that learns flow fields from VOS data. We train a flow module to capture detailed motion between frames using two weakly-supervised losses. Our object-focused approach of warping previous foreground object masks to their positions in the target frame enables detailed mask refinement with fast runtimes without using extra flow supervision. It can also be integrated directly into state-of-the-art segmentation networks. On the DAVIS17 and YouTubeVOS benchmarks, we outperform state-of-the-art offline methods that do not use extra data, as well as many online methods that use extra data. Qualitatively, we also show our approach produces segmentations with high detail and temporal consistency.



### SPINE: Soft Piecewise Interpretable Neural Equations
- **Arxiv ID**: http://arxiv.org/abs/2111.10622v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2111.10622v1)
- **Published**: 2021-11-20 16:18:00+00:00
- **Updated**: 2021-11-20 16:18:00+00:00
- **Authors**: Jasdeep Singh Grover, Harsh Minesh Domadia, Raj Anant Tapase, Grishma Sharma
- **Comment**: 31 pages, 23 figures, was submitted to NeurIPS 2020
- **Journal**: None
- **Summary**: Relu Fully Connected Networks are ubiquitous but uninterpretable because they fit piecewise linear functions emerging from multi-layered structures and complex interactions of model weights. This paper takes a novel approach to piecewise fits by using set operations on individual pieces(parts). This is done by approximating canonical normal forms and using the resultant as a model. This gives special advantages like (a)strong correspondence of parameters to pieces of the fit function(High Interpretability); (b)ability to fit any combination of continuous functions as pieces of the piecewise function(Ease of Design); (c)ability to add new non-linearities in a targeted region of the domain(Targeted Learning); (d)simplicity of an equation which avoids layering. It can also be expressed in the general max-min representation of piecewise linear functions which gives theoretical ease and credibility. This architecture is tested on simulated regression and classification tasks and benchmark datasets including UCI datasets, MNIST, FMNIST, and CIFAR 10. This performance is on par with fully connected architectures. It can find a variety of applications where fully connected layers must be replaced by interpretable layers.



### Sparse Tensor-based Multiscale Representation for Point Cloud Geometry Compression
- **Arxiv ID**: http://arxiv.org/abs/2111.10633v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.10633v2)
- **Published**: 2021-11-20 17:02:45+00:00
- **Updated**: 2022-10-21 14:12:13+00:00
- **Authors**: Jianqiang Wang, Dandan Ding, Zhu Li, Xiaoxing Feng, Chuntong Cao, Zhan Ma
- **Comment**: 17 pages, 15 figures
- **Journal**: None
- **Summary**: This study develops a unified Point Cloud Geometry (PCG) compression method through the processing of multiscale sparse tensor-based voxelized PCG. We call this compression method SparsePCGC. The proposed SparsePCGC is a low complexity solution because it only performs the convolutions on sparsely-distributed Most-Probable Positively-Occupied Voxels (MP-POV). The multiscale representation also allows us to compress scale-wise MP-POVs by exploiting cross-scale and same-scale correlations extensively and flexibly. The overall compression efficiency highly depends on the accuracy of estimated occupancy probability for each MP-POV. Thus, we first design the Sparse Convolution-based Neural Network (SparseCNN) which stacks sparse convolutions and voxel sampling to best characterize and embed spatial correlations. We then develop the SparseCNN-based Occupancy Probability Approximation (SOPA) model to estimate the occupancy probability either in a single-stage manner only using the cross-scale correlation, or in a multi-stage manner by exploiting stage-wise correlation among same-scale neighbors. Besides, we also suggest the SparseCNN based Local Neighborhood Embedding (SLNE) to aggregate local variations as spatial priors in feature attribute to improve the SOPA. Our unified approach not only shows state-of-the-art performance in both lossless and lossy compression modes across a variety of datasets including the dense object PCGs (8iVFB, Owlii, MUVB) and sparse LiDAR PCGs (KITTI, Ford) when compared with standardized MPEG G-PCC and other prevalent learning-based schemes, but also has low complexity which is attractive to practical applications.



### Identity-Preserving Pose-Robust Face Hallucination Through Face Subspace Prior
- **Arxiv ID**: http://arxiv.org/abs/2111.10634v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.10634v1)
- **Published**: 2021-11-20 17:08:38+00:00
- **Updated**: 2021-11-20 17:08:38+00:00
- **Authors**: Ali Abbasi, Mohammad Rahmati
- **Comment**: A shorter version of this paper has been submitted to IEEE
  Transactions on Image Processing
- **Journal**: None
- **Summary**: Over the past few decades, numerous attempts have been made to address the problem of recovering a high-resolution (HR) facial image from its corresponding low-resolution (LR) counterpart, a task commonly referred to as face hallucination. Despite the impressive performance achieved by position-patch and deep learning-based methods, most of these techniques are still unable to recover identity-specific features of faces. The former group of algorithms often produces blurry and oversmoothed outputs particularly in the presence of higher levels of degradation, whereas the latter generates faces which sometimes by no means resemble the individuals in the input images. In this paper, a novel face super-resolution approach will be introduced, in which the hallucinated face is forced to lie in a subspace spanned by the available training faces. Therefore, in contrast to the majority of existing face hallucination techniques and thanks to this face subspace prior, the reconstruction is performed in favor of recovering person-specific facial features, rather than merely increasing image quantitative scores. Furthermore, inspired by recent advances in the area of 3D face reconstruction, an efficient 3D dictionary alignment scheme is also presented, through which the algorithm becomes capable of dealing with low-resolution faces taken in uncontrolled conditions. In extensive experiments carried out on several well-known face datasets, the proposed algorithm shows remarkable performance by generating detailed and close to ground truth results which outperform the state-of-the-art face hallucination algorithms by significant margins both in quantitative and qualitative evaluations.



### Simulated LiDAR Repositioning: a novel point cloud data augmentation method
- **Arxiv ID**: http://arxiv.org/abs/2111.10650v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2111.10650v1)
- **Published**: 2021-11-20 18:35:39+00:00
- **Updated**: 2021-11-20 18:35:39+00:00
- **Authors**: Xavier Morin-Duchesne, Michael S Langer
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: We address a data augmentation problem for LiDAR. Given a LiDAR scan of a scene from some position, how can one simulate new scans of that scene from different, secondary positions? The method defines criteria for selecting valid secondary positions, and then estimates which points from the original point cloud would be acquired by a scanner from these positions. We validate the method using synthetic scenes, and examine how the similarity of generated point clouds depends on scanner distance, occlusion, and angular resolution. We show that the method is more accurate at short distances, and that having a high scanner resolution for the original point clouds has a strong impact on the similarity of generated point clouds. We also demonstrate how the method can be applied to natural scene statistics: in particular, we apply our method to reposition the scanner horizontally and vertically, separately consider points belonging to the ground and to non-ground objects, and describe the impact on the distributions of distances to these two classes of points.



### Real-time Human Detection Model for Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/2111.10653v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.10653v1)
- **Published**: 2021-11-20 18:42:17+00:00
- **Updated**: 2021-11-20 18:42:17+00:00
- **Authors**: Ali Farouk Khalifa, Hesham N. Elmahdy, Eman Badr
- **Comment**: 19 pages 6 figures 9 Tables
- **Journal**: None
- **Summary**: Building a small-sized fast surveillance system model to fit on limited resource devices is a challenging, yet an important task. Convolutional Neural Networks (CNNs) have replaced traditional feature extraction and machine learning models in detection and classification tasks. Various complex large CNN models are proposed that achieve significant improvement in the accuracy. Lightweight CNN models have been recently introduced for real-time tasks. This paper suggests a CNN-based lightweight model that can fit on a limited edge device such as Raspberry Pi. Our proposed model provides better performance time, smaller size and comparable accuracy with existing method. The model performance is evaluated on multiple benchmark datasets. It is also compared with existing models in terms of size, average processing time, and F-score. Other enhancements for future research are suggested.



### Are Vision Transformers Robust to Patch Perturbations?
- **Arxiv ID**: http://arxiv.org/abs/2111.10659v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.10659v2)
- **Published**: 2021-11-20 19:00:51+00:00
- **Updated**: 2022-07-18 17:24:18+00:00
- **Authors**: Jindong Gu, Volker Tresp, Yao Qin
- **Comment**: None
- **Journal**: European Conference on Computer Vision (ECCV) , 2022
- **Summary**: Recent advances in Vision Transformer (ViT) have demonstrated its impressive performance in image classification, which makes it a promising alternative to Convolutional Neural Network (CNN). Unlike CNNs, ViT represents an input image as a sequence of image patches. The patch-based input image representation makes the following question interesting: How does ViT perform when individual input image patches are perturbed with natural corruptions or adversarial perturbations, compared to CNNs? In this work, we study the robustness of ViT to patch-wise perturbations. Surprisingly, we find that ViTs are more robust to naturally corrupted patches than CNNs, whereas they are more vulnerable to adversarial patches. Furthermore, we discover that the attention mechanism greatly affects the robustness of vision transformers. Specifically, the attention module can help improve the robustness of ViT by effectively ignoring natural corrupted patches. However, when ViTs are attacked by an adversary, the attention mechanism can be easily fooled to focus more on the adversarially perturbed patches and cause a mistake. Based on our analysis, we propose a simple temperature-scaling based method to improve the robustness of ViT against adversarial patches. Extensive qualitative and quantitative experiments are performed to support our findings, understanding, and improvement of ViT robustness to patch-wise perturbations across a set of transformer-based architectures.



### VideoPose: Estimating 6D object pose from videos
- **Arxiv ID**: http://arxiv.org/abs/2111.10677v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.10677v1)
- **Published**: 2021-11-20 20:57:45+00:00
- **Updated**: 2021-11-20 20:57:45+00:00
- **Authors**: Apoorva Beedu, Zhile Ren, Varun Agrawal, Irfan Essa
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a simple yet effective algorithm that uses convolutional neural networks to directly estimate object poses from videos. Our approach leverages the temporal information from a video sequence, and is computationally efficient and robust to support robotic and AR domains. Our proposed network takes a pre-trained 2D object detector as input, and aggregates visual features through a recurrent neural network to make predictions at each frame. Experimental evaluation on the YCB-Video dataset show that our approach is on par with the state-of-the-art algorithms. Further, with a speed of 30 fps, it is also more efficient than the state-of-the-art, and therefore applicable to a variety of applications that require real-time object pose estimation.



### A Review on The Division of Magnetic Resonant Prostate Images with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.10683v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.NE, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.10683v1)
- **Published**: 2021-11-20 21:33:50+00:00
- **Updated**: 2021-11-20 21:33:50+00:00
- **Authors**: Elcin Huseyn, Emin Mammadov, Mohammad Hoseini
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning; it is often used in dividing processes on images in the biomedical field. In recent years, it has been observed that there is an increase in the division procedures performed on prostate images using deep learning compared to other methods of image division. Looking at the literature; It is seen that the process of dividing prostate images, which are carried out with deep learning, is an important step for the diagnosis and treatment of prostate cancer. For this reason, in this study; to be a source for future splitting operations; deep learning splitting procedures on prostate images obtained from magnetic resonance (MRI) imaging devices were examined.



### Representing Prior Knowledge Using Randomly, Weighted Feature Networks for Visual Relationship Detection
- **Arxiv ID**: http://arxiv.org/abs/2111.10686v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.10686v2)
- **Published**: 2021-11-20 21:56:45+00:00
- **Updated**: 2021-12-22 16:57:39+00:00
- **Authors**: Jinyung Hong, Theodore P. Pavlic
- **Comment**: 9 pages, 2 figures, Accepted to CLeaR2022 at AAAI2022
- **Journal**: None
- **Summary**: The single-hidden-layer Randomly Weighted Feature Network (RWFN) introduced by Hong and Pavlic (2021) was developed as an alternative to neural tensor network approaches for relational learning tasks. Its relatively small footprint combined with the use of two randomized input projections -- an insect-brain-inspired input representation and random Fourier features -- allow it to achieve rich expressiveness for relational learning with relatively low training cost. In particular, when Hong and Pavlic compared RWFN to Logic Tensor Networks (LTNs) for Semantic Image Interpretation (SII) tasks to extract structured semantic descriptions from images, they showed that the RWFN integration of the two hidden, randomized representations better captures relationships among inputs with a faster training process even though it uses far fewer learnable parameters. In this paper, we use RWFNs to perform Visual Relationship Detection (VRD) tasks, which are more challenging SII tasks. A zero-shot learning approach is used with RWFN that can exploit similarities with other seen relationships and background knowledge -- expressed with logical constraints between subjects, relations, and objects -- to achieve the ability to predict triples that do not appear in the training set. The experiments on the Visual Relationship Dataset to compare the performance between RWFNs and LTNs, one of the leading Statistical Relational Learning frameworks, show that RWFNs outperform LTNs for the predicate-detection task while using fewer number of adaptable parameters (1:56 ratio). Furthermore, background knowledge represented by RWFNs can be used to alleviate the incompleteness of training sets even though the space complexity of RWFNs is much smaller than LTNs (1:27 ratio).



