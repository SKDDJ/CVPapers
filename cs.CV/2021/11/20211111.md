# Arxiv Papers in cs.CV on 2021-11-11
### Synthetic Document Generator for Annotation-free Layout Recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.06016v3
- **DOI**: 10.1016/j.patcog.2022.108660
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06016v3)
- **Published**: 2021-11-11 01:58:44+00:00
- **Updated**: 2022-07-24 18:06:57+00:00
- **Authors**: Natraj Raman, Sameena Shah, Manuela Veloso
- **Comment**: None
- **Journal**: None
- **Summary**: Analyzing the layout of a document to identify headers, sections, tables, figures etc. is critical to understanding its content. Deep learning based approaches for detecting the layout structure of document images have been promising. However, these methods require a large number of annotated examples during training, which are both expensive and time consuming to obtain. We describe here a synthetic document generator that automatically produces realistic documents with labels for spatial positions, extents and categories of the layout elements. The proposed generative process treats every physical component of a document as a random variable and models their intrinsic dependencies using a Bayesian Network graph. Our hierarchical formulation using stochastic templates allow parameter sharing between documents for retaining broad themes and yet the distributional characteristics produces visually unique samples, thereby capturing complex and diverse layouts. We empirically illustrate that a deep layout detection model trained purely on the synthetic documents can match the performance of a model that uses real documents.



### csBoundary: City-scale Road-boundary Detection in Aerial Images for High-definition Maps
- **Arxiv ID**: http://arxiv.org/abs/2111.06020v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2111.06020v2)
- **Published**: 2021-11-11 02:04:36+00:00
- **Updated**: 2022-02-07 10:22:31+00:00
- **Authors**: Zhenhua Xu, Yuxuan Liu, Lu Gan, Xiangcheng Hu, Yuxiang Sun, Ming Liu, Lujia Wang
- **Comment**: Accepted by IEEE Robotics and Automation Letters and IEEE
  International Conference on Robotics and Automation (ICRA) 2022
- **Journal**: None
- **Summary**: High-Definition (HD) maps can provide precise geometric and semantic information of static traffic environments for autonomous driving. Road-boundary is one of the most important information contained in HD maps since it distinguishes between road areas and off-road areas, which can guide vehicles to drive within road areas. But it is labor-intensive to annotate road boundaries for HD maps at the city scale. To enable automatic HD map annotation, current work uses semantic segmentation or iterative graph growing for road-boundary detection. However, the former could not ensure topological correctness since it works at the pixel level, while the latter suffers from inefficiency and drifting issues. To provide a solution to the aforementioned problems, in this letter, we propose a novel system termed csBoundary to automatically detect road boundaries at the city scale for HD map annotation. Our network takes as input an aerial image patch, and directly infers the continuous road-boundary graph (i.e., vertices and edges) from this image. To generate the city-scale road-boundary graph, we stitch the obtained graphs from all the image patches. Our csBoundary is evaluated and compared on a public benchmark dataset. The results demonstrate our superiority. The accompanied demonstration video is available at our project page \url{https://sites.google.com/view/csboundary/}.



### Probabilistic Contrastive Learning for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2111.06021v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06021v5)
- **Published**: 2021-11-11 02:08:07+00:00
- **Updated**: 2023-07-16 14:07:07+00:00
- **Authors**: Junjie Li, Yixin Zhang, Zilei Wang, Keyu Tu, Saihui Hou
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive learning can largely enhance the feature discriminability in a self-supervised manner and has achieved remarkable success for various visual tasks. However, it is undesirably observed that the standard contrastive paradigm (features+$\ell_{2}$ normalization) only brings little help for domain adaptation. In this work, we delve into this phenomenon and find that the main reason is due to the class weights (weights of the final fully connected layer) which are vital for the recognition yet ignored in the optimization. To tackle this issue, we propose a simple yet powerful Probabilistic Contrastive Learning (PCL), which does not only assist in extracting discriminative features but also enforces them to be clustered around the class weights. Specifically, we break the standard contrastive paradigm by removing $\ell_{2}$ normalization and replacing the features with probabilities. In this way, PCL can enforce the probability to approximate the one-hot form, thereby reducing the deviation between the features and class weights. Benefiting from the conciseness, PCL can be well generalized to different settings. In this work, we conduct extensive experiments on five tasks and observe consistent performance gains, i.e., Unsupervised Domain Adaptation (UDA), Semi-Supervised Domain Adaptation (SSDA), Semi-Supervised Learning (SSL), UDA Detection, and UDA Semantic Segmentation. Notably, for UDA Semantic Segmentation on SYNTHIA, PCL surpasses the sophisticated CPSL-D by $>\!2\%$ in terms of mean IoU with a much smaller training cost (PCL: 1*3090, 5 days v.s. CPSL-D: 4*V100, 11 days). Code is available at https://github.com/ljjcoder/Probabilistic-Contrastive-Learning.



### FINO: Flow-based Joint Image and Noise Model
- **Arxiv ID**: http://arxiv.org/abs/2111.06031v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/2111.06031v1)
- **Published**: 2021-11-11 02:51:54+00:00
- **Updated**: 2021-11-11 02:51:54+00:00
- **Authors**: Lanqing Guo, Siyu Huang, Haosen Liu, Bihan Wen
- **Comment**: None
- **Journal**: None
- **Summary**: One of the fundamental challenges in image restoration is denoising, where the objective is to estimate the clean image from its noisy measurements. To tackle such an ill-posed inverse problem, the existing denoising approaches generally focus on exploiting effective natural image priors. The utilization and analysis of the noise model are often ignored, although the noise model can provide complementary information to the denoising algorithms. In this paper, we propose a novel Flow-based joint Image and NOise model (FINO) that distinctly decouples the image and noise in the latent space and losslessly reconstructs them via a series of invertible transformations. We further present a variable swapping strategy to align structural information in images and a noise correlation matrix to constrain the noise based on spatially minimized correlation information. Experimental results demonstrate FINO's capacity to remove both synthetic additive white Gaussian noise (AWGN) and real noise. Furthermore, the generalization of FINO to the removal of spatially variant noise and noise with inaccurate estimation surpasses that of the popular and state-of-the-art methods by large margins.



### Hybrid Saturation Restoration for LDR Images of HDR Scenes
- **Arxiv ID**: http://arxiv.org/abs/2111.06038v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06038v2)
- **Published**: 2021-11-11 03:20:32+00:00
- **Updated**: 2021-11-14 08:23:56+00:00
- **Authors**: Chaobing Zheng, Zhengguo Li, Shiqian Wu
- **Comment**: arXiv admin note: text overlap with arXiv:2007.02042
- **Journal**: None
- **Summary**: There are shadow and highlight regions in a low dynamic range (LDR) image which is captured from a high dynamic range (HDR) scene. It is an ill-posed problem to restore the saturated regions of the LDR image. In this paper, the saturated regions of the LDR image are restored by fusing model-based and data-driven approaches. With such a neural augmentation, two synthetic LDR images are first generated from the underlying LDR image via the model-based approach. One is brighter than the input image to restore the shadow regions and the other is darker than the input image to restore the high-light regions. Both synthetic images are then refined via a novel exposedness aware saturation restoration network (EASRN). Finally, the two synthetic images and the input image are combined together via an HDR synthesis algorithm or a multi-scale exposure fusion algorithm. The proposed algorithm can be embedded in any smart phones or digital cameras to produce an information-enriched LDR image.



### Fast T2w/FLAIR MRI Acquisition by Optimal Sampling of Information Complementary to Pre-acquired T1w MRI
- **Arxiv ID**: http://arxiv.org/abs/2111.06400v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.06400v1)
- **Published**: 2021-11-11 04:04:48+00:00
- **Updated**: 2021-11-11 04:04:48+00:00
- **Authors**: Junwei Yang, Xiao-Xin Li, Feihong Liu, Dong Nie, Pietro Lio, Haikun Qi, Dinggang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies on T1-assisted MRI reconstruction for under-sampled images of other modalities have demonstrated the potential of further accelerating MRI acquisition of other modalities. Most of the state-of-the-art approaches have achieved improvement through the development of network architectures for fixed under-sampling patterns, without fully exploiting the complementary information between modalities. Although existing under-sampling pattern learning algorithms can be simply modified to allow the fully-sampled T1-weighted MR image to assist the pattern learning, no significant improvement on the reconstruction task can be achieved. To this end, we propose an iterative framework to optimize the under-sampling pattern for MRI acquisition of another modality that can complement the fully-sampled T1-weighted MR image at different under-sampling factors, while jointly optimizing the T1-assisted MRI reconstruction model. Specifically, our proposed method exploits the difference of latent information between the two modalities for determining the sampling patterns that can maximize the assistance power of T1-weighted MR image in improving the MRI reconstruction. We have demonstrated superior performance of our learned under-sampling patterns on a public dataset, compared to commonly used under-sampling patterns and state-of-the-art methods that can jointly optimize both the reconstruction network and the under-sampling pattern, up to 8-fold under-sampling factor.



### Indian Licence Plate Dataset in the wild
- **Arxiv ID**: http://arxiv.org/abs/2111.06054v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06054v1)
- **Published**: 2021-11-11 05:04:10+00:00
- **Updated**: 2021-11-11 05:04:10+00:00
- **Authors**: Sanchit Tanwar, Ayush Tiwari, Ritesh Chowdhry
- **Comment**: 5 pages, 4 figures, 3 tables
- **Journal**: None
- **Summary**: Indian Licence Plate Detection is a problem that has not been explored much at an open-source level.There are proprietary solutions available for it, but there is no big open-source dataset that can be used to perform experiments and test different approaches.Most of the large datasets available are for countries like China, Brazil, but the model trained on these datasets does not perform well on Indian plates because the font styles and plate designs used vary significantly from country to country.This paper introduces an Indian license plate dataset with 16192 images and 21683 plate plates annotated with 4 points for each plate and each character in the corresponding plate.We present a benchmark model that uses semantic segmentation to solve number plate detection. We propose a two-stage approach in which the first stage is for localizing the plate, and the second stage is to read the text in cropped plate image.We tested benchmark object detection and semantic segmentation model, for the second stage, we used lprnet based OCR.



### On the Equivalence between Neural Network and Support Vector Machine
- **Arxiv ID**: http://arxiv.org/abs/2111.06063v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2111.06063v2)
- **Published**: 2021-11-11 06:05:00+00:00
- **Updated**: 2022-07-07 05:22:55+00:00
- **Authors**: Yilan Chen, Wei Huang, Lam M. Nguyen, Tsui-Wei Weng
- **Comment**: 35th Conference on Neural Information Processing Systems (NeurIPS
  2021)
- **Journal**: None
- **Summary**: Recent research shows that the dynamics of an infinitely wide neural network (NN) trained by gradient descent can be characterized by Neural Tangent Kernel (NTK) \citep{jacot2018neural}. Under the squared loss, the infinite-width NN trained by gradient descent with an infinitely small learning rate is equivalent to kernel regression with NTK \citep{arora2019exact}. However, the equivalence is only known for ridge regression currently \citep{arora2019harnessing}, while the equivalence between NN and other kernel machines (KMs), e.g. support vector machine (SVM), remains unknown. Therefore, in this work, we propose to establish the equivalence between NN and SVM, and specifically, the infinitely wide NN trained by soft margin loss and the standard soft margin SVM with NTK trained by subgradient descent. Our main theoretical results include establishing the equivalences between NNs and a broad family of $\ell_2$ regularized KMs with finite-width bounds, which cannot be handled by prior work, and showing that every finite-width NN trained by such regularized loss functions is approximately a KM. Furthermore, we demonstrate our theory can enable three practical applications, including (i) \textit{non-vacuous} generalization bound of NN via the corresponding KM; (ii) \textit{non-trivial} robustness certificate for the infinite-width NN (while existing robustness verification methods would provide vacuous bounds); (iii) intrinsically more robust infinite-width NNs than those from previous kernel regression. Our code for the experiments is available at \url{https://github.com/leslie-CH/equiv-nn-svm}.



### CodEx: A Modular Framework for Joint Temporal De-blurring and Tomographic Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2111.06069v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06069v3)
- **Published**: 2021-11-11 06:33:58+00:00
- **Updated**: 2022-07-31 00:59:09+00:00
- **Authors**: Soumendu Majee, Selin Aslan, Doga Gursoy, Charles A. Bouman
- **Comment**: None
- **Journal**: None
- **Summary**: In many computed tomography (CT) imaging applications, it is important to rapidly collect data from an object that is moving or changing with time. Tomographic acquisition is generally assumed to be step-and-shoot, where the object is rotated to each desired angle, and a view is taken. However, step-and-shoot acquisition is slow and can waste photons, so in practice fly-scanning is done where the object is continuously rotated while collecting data. However, this can result in motion-blurred views and consequently reconstructions with severe motion artifacts.   In this paper, we introduce CodEx, a modular framework for joint de-blurring and tomographic reconstruction that can effectively invert the motion blur introduced in sparse view fly-scanning. The method is a synergistic combination of a novel acquisition method with a novel non-convex Bayesian reconstruction algorithm. CodEx works by encoding the acquisition with a known binary code that the reconstruction algorithm then inverts. Using a well chosen binary code to encode the measurements can improve the accuracy of the inversion process. The CodEx reconstruction method uses the alternating direction method of multipliers (ADMM) to split the inverse problem into iterative deblurring and reconstruction sub-problems, making reconstruction practical to implement. We present reconstruction results on both simulated and binned experimental data to demonstrate the effectiveness of our method.



### Graph Relation Transformer: Incorporating pairwise object features into the Transformer architecture
- **Arxiv ID**: http://arxiv.org/abs/2111.06075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06075v1)
- **Published**: 2021-11-11 06:55:28+00:00
- **Updated**: 2021-11-11 06:55:28+00:00
- **Authors**: Michael Yang, Aditya Anantharaman, Zachary Kitowski, Derik Clive Robert
- **Comment**: Presented as poster in CVPR 2021 Visual Question Answering Workshop
- **Journal**: None
- **Summary**: Previous studies such as VizWiz find that Visual Question Answering (VQA) systems that can read and reason about text in images are useful in application areas such as assisting visually-impaired people. TextVQA is a VQA dataset geared towards this problem, where the questions require answering systems to read and reason about visual objects and text objects in images. One key challenge in TextVQA is the design of a system that effectively reasons not only about visual and text objects individually, but also about the spatial relationships between these objects. This motivates the use of 'edge features', that is, information about the relationship between each pair of objects. Some current TextVQA models address this problem but either only use categories of relations (rather than edge feature vectors) or do not use edge features within the Transformer architectures. In order to overcome these shortcomings, we propose a Graph Relation Transformer (GRT), which uses edge information in addition to node information for graph attention computation in the Transformer. We find that, without using any other optimizations, the proposed GRT method outperforms the accuracy of the M4C baseline model by 0.65% on the val set and 0.57% on the test set. Qualitatively, we observe that the GRT has superior spatial reasoning ability to M4C.



### A Survey of Visual Transformers
- **Arxiv ID**: http://arxiv.org/abs/2111.06091v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06091v4)
- **Published**: 2021-11-11 07:56:04+00:00
- **Updated**: 2022-12-06 16:26:56+00:00
- **Authors**: Yang Liu, Yao Zhang, Yixin Wang, Feng Hou, Jin Yuan, Jiang Tian, Yang Zhang, Zhongchao Shi, Jianping Fan, Zhiqiang He
- **Comment**: Accepted by IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS)
- **Journal**: None
- **Summary**: Transformer, an attention-based encoder-decoder model, has already revolutionized the field of natural language processing (NLP). Inspired by such significant achievements, some pioneering works have recently been done on employing Transformer-liked architectures in the computer vision (CV) field, which have demonstrated their effectiveness on three fundamental CV tasks (classification, detection, and segmentation) as well as multiple sensory data stream (images, point clouds, and vision-language data). Because of their competitive modeling capabilities, the visual Transformers have achieved impressive performance improvements over multiple benchmarks as compared with modern Convolution Neural Networks (CNNs). In this survey, we have reviewed over one hundred of different visual Transformers comprehensively according to three fundamental CV tasks and different data stream types, where a taxonomy is proposed to organize the representative methods according to their motivations, structures, and application scenarios. Because of their differences on training settings and dedicated vision tasks, we have also evaluated and compared all these existing visual Transformers under different configurations. Furthermore, we have revealed a series of essential but unexploited aspects that may empower such visual Transformers to stand out from numerous architectures, e.g., slack high-level semantic embeddings to bridge the gap between the visual Transformers and the sequential ones. Finally, three promising research directions are suggested for future investment. We will continue to update the latest articles and their released source codes at https://github.com/liuyang-ict/awesome-visual-transformers.



### Open surgery tool classification and hand utilization using a multi-camera system
- **Arxiv ID**: http://arxiv.org/abs/2111.06098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06098v1)
- **Published**: 2021-11-11 08:18:13+00:00
- **Updated**: 2021-11-11 08:18:13+00:00
- **Authors**: Kristina Basiev, Adam Goldbraikh, Carla M Pugh, Shlomi Laufer
- **Comment**: 12 pages, 3 figures, submitted to IPCAI 2022
- **Journal**: None
- **Summary**: Purpose: The goal of this work is to use multi-camera video to classify open surgery tools as well as identify which tool is held in each hand. Multi-camera systems help prevent occlusions in open surgery video data. Furthermore, combining multiple views such as a Top-view camera covering the full operative field and a Close-up camera focusing on hand motion and anatomy, may provide a more comprehensive view of the surgical workflow. However, multi-camera data fusion poses a new challenge: a tool may be visible in one camera and not the other. Thus, we defined the global ground truth as the tools being used regardless their visibility. Therefore, tools that are out of the image should be remembered for extensive periods of time while the system responds quickly to changes visible in the video.   Methods: Participants (n=48) performed a simulated open bowel repair. A Top-view and a Close-up cameras were used. YOLOv5 was used for tool and hand detection. A high frequency LSTM with a 1 second window at 30 frames per second (fps) and a low frequency LSTM with a 40 second window at 3 fps were used for spatial, temporal, and multi-camera integration.   Results: The accuracy and F1 of the six systems were: Top-view (0.88/0.88), Close-up (0.81,0.83), both cameras (0.9/0.9), high fps LSTM (0.92/0.93), low fps LSTM (0.9/0.91), and our final architecture the Multi-camera classifier(0.93/0.94).   Conclusion: By combining a system with a high fps and a low fps from the multiple camera array we improved the classification abilities of the global ground truth.



### Fine-Grained Image Analysis with Deep Learning: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2111.06119v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.06119v2)
- **Published**: 2021-11-11 09:43:56+00:00
- **Updated**: 2021-11-19 12:58:20+00:00
- **Authors**: Xiu-Shen Wei, Yi-Zhe Song, Oisin Mac Aodha, Jianxin Wu, Yuxin Peng, Jinhui Tang, Jian Yang, Serge Belongie
- **Comment**: Accepted by IEEE TPAMI
- **Journal**: None
- **Summary**: Fine-grained image analysis (FGIA) is a longstanding and fundamental problem in computer vision and pattern recognition, and underpins a diverse set of real-world applications. The task of FGIA targets analyzing visual objects from subordinate categories, e.g., species of birds or models of cars. The small inter-class and large intra-class variation inherent to fine-grained image analysis makes it a challenging problem. Capitalizing on advances in deep learning, in recent years we have witnessed remarkable progress in deep learning powered FGIA. In this paper we present a systematic survey of these advances, where we attempt to re-define and broaden the field of FGIA by consolidating two fundamental fine-grained research areas -- fine-grained image recognition and fine-grained image retrieval. In addition, we also review other key issues of FGIA, such as publicly available benchmark datasets and related domain-specific applications. We conclude by highlighting several research directions and open problems which need further exploration from the community.



### Spatio-Temporal Scene-Graph Embedding for Autonomous Vehicle Collision Prediction
- **Arxiv ID**: http://arxiv.org/abs/2111.06123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06123v1)
- **Published**: 2021-11-11 10:01:01+00:00
- **Updated**: 2021-11-11 10:01:01+00:00
- **Authors**: Arnav V. Malawade, Shih-Yuan Yu, Brandon Hsu, Deepan Muthirayan, Pramod P. Khargonekar, Mohammad A. Al Faruque
- **Comment**: None
- **Journal**: None
- **Summary**: In autonomous vehicles (AVs), early warning systems rely on collision prediction to ensure occupant safety. However, state-of-the-art methods using deep convolutional networks either fail at modeling collisions or are too expensive/slow, making them less suitable for deployment on AV edge hardware. To address these limitations, we propose sg2vec, a spatio-temporal scene-graph embedding methodology that uses Graph Neural Network (GNN) and Long Short-Term Memory (LSTM) layers to predict future collisions via visual scene perception. We demonstrate that sg2vec predicts collisions 8.11% more accurately and 39.07% earlier than the state-of-the-art method on synthesized datasets, and 29.47% more accurately on a challenging real-world collision dataset. We also show that sg2vec is better than the state-of-the-art at transferring knowledge from synthetic datasets to real-world driving datasets. Finally, we demonstrate that sg2vec performs inference 9.3x faster with an 88.0% smaller model, 32.4% less power, and 92.8% less energy than the state-of-the-art method on the industry-standard Nvidia DRIVE PX 2 platform, making it more suitable for implementation on the edge.



### Stacked U-Nets with Self-Assisted Priors Towards Robust Correction of Rigid Motion Artifact in Brain MRI
- **Arxiv ID**: http://arxiv.org/abs/2111.06401v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06401v1)
- **Published**: 2021-11-11 10:04:32+00:00
- **Updated**: 2021-11-11 10:04:32+00:00
- **Authors**: Mohammed A. Al-masni, Seul Lee, Jaeuk Yi, Sewook Kim, Sung-Min Gho, Young Hun Choi, Dong-Hyun Kim
- **Comment**: 24 pages, 10 figures, 3 tables
- **Journal**: None
- **Summary**: In this paper, we develop an efficient retrospective deep learning method called stacked U-Nets with self-assisted priors to address the problem of rigid motion artifacts in MRI. The proposed work exploits the usage of additional knowledge priors from the corrupted images themselves without the need for additional contrast data. The proposed network learns missed structural details through sharing auxiliary information from the contiguous slices of the same distorted subject. We further design a refinement stacked U-Nets that facilitates preserving of the image spatial details and hence improves the pixel-to-pixel dependency. To perform network training, simulation of MRI motion artifacts is inevitable. We present an intensive analysis using various types of image priors: the proposed self-assisted priors and priors from other image contrast of the same subject. The experimental analysis proves the effectiveness and feasibility of our self-assisted priors since it does not require any further data scans.



### A Novel Approach for Deterioration and Damage Identification in Building Structures Based on Stockwell-Transform and Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2111.06155v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.5 PATTERN RECOGNITION
- **Links**: [PDF](http://arxiv.org/pdf/2111.06155v2)
- **Published**: 2021-11-11 11:31:37+00:00
- **Updated**: 2021-11-16 17:12:51+00:00
- **Authors**: Vahidreza Gharehbaghi, Hashem Kalbkhani, Ehsan Noroozinejad Farsangi, T. Y. Yang, Andy Nguyen, Seyedali Mirjalili, C. Malaga-Chuquitaype
- **Comment**: 11 figures and 11 Tables, Accepted in Journal of Structural Integrity
  and Maintenance
- **Journal**: None
- **Summary**: In this paper, a novel deterioration and damage identification procedure (DIP) is presented and applied to building models. The challenge associated with applications on these types of structures is related to the strong correlation of responses, which gets further complicated when coping with real ambient vibrations with high levels of noise. Thus, a DIP is designed utilizing low-cost ambient vibrations to analyze the acceleration responses using the Stockwell transform (ST) to generate spectrograms. Subsequently, the ST outputs become the input of two series of Convolutional Neural Networks (CNNs) established for identifying deterioration and damage to the building models. To the best of our knowledge, this is the first time that both damage and deterioration are evaluated on building models through a combination of ST and CNN with high accuracy.



### Clicking Matters:Towards Interactive Human Parsing
- **Arxiv ID**: http://arxiv.org/abs/2111.06162v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06162v2)
- **Published**: 2021-11-11 11:47:53+00:00
- **Updated**: 2021-12-16 12:17:56+00:00
- **Authors**: Yutong Gao, Liqian Liang, Congyan Lang, Songhe Feng, Yidong Li, Yunchao Wei
- **Comment**: Human parsing, interactive segmentation, semantic segmentation
- **Journal**: None
- **Summary**: In this work, we focus on Interactive Human Parsing (IHP), which aims to segment a human image into multiple human body parts with guidance from users' interactions. This new task inherits the class-aware property of human parsing, which cannot be well solved by traditional interactive image segmentation approaches that are generally class-agnostic. To tackle this new task, we first exploit user clicks to identify different human parts in the given image. These clicks are subsequently transformed into semantic-aware localization maps, which are concatenated with the RGB image to form the input of the segmentation network and generate the initial parsing result. To enable the network to better perceive user's purpose during the correction process, we investigate several principal ways for the refinement, and reveal that random-sampling-based click augmentation is the best way for promoting the correction effectiveness. Furthermore, we also propose a semantic-perceiving loss (SP-loss) to augment the training, which can effectively exploit the semantic relationships of clicks for better optimization. To the best knowledge, this work is the first attempt to tackle the human parsing task under the interactive setting. Our IHP solution achieves 85\% mIoU on the benchmark LIP, 80\% mIoU on PASCAL-Person-Part and CIHP, 75\% mIoU on Helen with only 1.95, 3.02, 2.84 and 1.09 clicks per class respectively. These results demonstrate that we can simply acquire high-quality human parsing masks with only a few human effort. We hope this work can motivate more researchers to develop data-efficient solutions to IHP in the future.



### Graph-Guided Deformation for Point Cloud Completion
- **Arxiv ID**: http://arxiv.org/abs/2112.01840v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2112.01840v1)
- **Published**: 2021-11-11 12:55:26+00:00
- **Updated**: 2021-11-11 12:55:26+00:00
- **Authors**: Jieqi Shi, Lingyun Xu, Liang Heng, Shaojie Shen
- **Comment**: RAL with IROS 2021
- **Journal**: None
- **Summary**: For a long time, the point cloud completion task has been regarded as a pure generation task. After obtaining the global shape code through the encoder, a complete point cloud is generated using the shape priorly learnt by the networks. However, such models are undesirably biased towards prior average objects and inherently limited to fit geometry details. In this paper, we propose a Graph-Guided Deformation Network, which respectively regards the input data and intermediate generation as controlling and supporting points, and models the optimization guided by a graph convolutional network(GCN) for the point cloud completion task. Our key insight is to simulate the least square Laplacian deformation process via mesh deformation methods, which brings adaptivity for modeling variation in geometry details. By this means, we also reduce the gap between the completion task and the mesh deformation algorithms. As far as we know, we are the first to refine the point cloud completion task by mimicing traditional graphics algorithms with GCN-guided deformation. We have conducted extensive experiments on both the simulated indoor dataset ShapeNet, outdoor dataset KITTI, and our self-collected autonomous driving dataset Pandar40. The results show that our method outperforms the existing state-of-the-art algorithms in the 3D point cloud completion task.



### Towards Domain-Independent and Real-Time Gesture Recognition Using mmWave Signal
- **Arxiv ID**: http://arxiv.org/abs/2111.06195v3
- **DOI**: 10.1109/TMC.2022.3207570
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.06195v3)
- **Published**: 2021-11-11 13:28:28+00:00
- **Updated**: 2022-10-08 08:41:48+00:00
- **Authors**: Yadong Li, Dongheng Zhang, Jinbo Chen, Jinwei Wan, Dong Zhang, Yang Hu, Qibin Sun, Yan Chen
- **Comment**: This paper has been accepted by IEEE Transactions on Mobile Computing
  (2022)
- **Journal**: None
- **Summary**: Human gesture recognition using millimeter-wave (mmWave) signals provides attractive applications including smart home and in-car interfaces. While existing works achieve promising performance under controlled settings, practical applications are still limited due to the need of intensive data collection, extra training efforts when adapting to new domains, and poor performance for real-time recognition. In this paper, we propose DI-Gesture, a domain-independent and real-time mmWave gesture recognition system. Specifically, we first derive signal variations corresponding to human gestures with spatial-temporal processing. To enhance the robustness of the system and reduce data collecting efforts, we design a data augmentation framework for mmWave signals based on correlations between signal patterns and gesture variations. Furthermore, a spatial-temporal gesture segmentation algorithm is employed for real-time recognition. Extensive experimental results show DI-Gesture achieves an average accuracy of 97.92\%, 99.18\%, and 98.76\% for new users, environments, and locations, respectively. We also evaluate DI-Gesture in challenging scenarios like real-time recognition and sensing at extreme angles, all of which demonstrate the superior robustness and effectiveness of our system.



### Defining and Quantifying the Emergence of Sparse Concepts in DNNs
- **Arxiv ID**: http://arxiv.org/abs/2111.06206v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06206v6)
- **Published**: 2021-11-11 13:48:20+00:00
- **Updated**: 2023-04-03 12:02:02+00:00
- **Authors**: Jie Ren, Mingjie Li, Qirui Chen, Huiqi Deng, Quanshi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims to illustrate the concept-emerging phenomenon in a trained DNN. Specifically, we find that the inference score of a DNN can be disentangled into the effects of a few interactive concepts. These concepts can be understood as causal patterns in a sparse, symbolic causal graph, which explains the DNN. The faithfulness of using such a causal graph to explain the DNN is theoretically guaranteed, because we prove that the causal graph can well mimic the DNN's outputs on an exponential number of different masked samples. Besides, such a causal graph can be further simplified and re-written as an And-Or graph (AOG), without losing much explanation accuracy.



### Discovering and Explaining the Representation Bottleneck of DNNs
- **Arxiv ID**: http://arxiv.org/abs/2111.06236v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06236v4)
- **Published**: 2021-11-11 14:35:20+00:00
- **Updated**: 2022-11-07 16:47:51+00:00
- **Authors**: Huiqi Deng, Qihan Ren, Hao Zhang, Quanshi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper explores the bottleneck of feature representations of deep neural networks (DNNs), from the perspective of the complexity of interactions between input variables encoded in DNNs. To this end, we focus on the multi-order interaction between input variables, where the order represents the complexity of interactions. We discover that a DNN is more likely to encode both too simple interactions and too complex interactions, but usually fails to learn interactions of intermediate complexity. Such a phenomenon is widely shared by different DNNs for different tasks. This phenomenon indicates a cognition gap between DNNs and human beings, and we call it a representation bottleneck. We theoretically prove the underlying reason for the representation bottleneck. Furthermore, we propose a loss to encourage/penalize the learning of interactions of specific complexities, and analyze the representation capacities of interactions of different complexities.



### Dense Unsupervised Learning for Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.06265v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.06265v1)
- **Published**: 2021-11-11 15:15:11+00:00
- **Updated**: 2021-11-11 15:15:11+00:00
- **Authors**: Nikita Araslanov, Simone Schaub-Meyer, Stefan Roth
- **Comment**: To appear at NeurIPS*2021. Code:
  https://github.com/visinf/dense-ulearn-vos
- **Journal**: None
- **Summary**: We present a novel approach to unsupervised learning for video object segmentation (VOS). Unlike previous work, our formulation allows to learn dense feature representations directly in a fully convolutional regime. We rely on uniform grid sampling to extract a set of anchors and train our model to disambiguate between them on both inter- and intra-video levels. However, a naive scheme to train such a model results in a degenerate solution. We propose to prevent this with a simple regularisation scheme, accommodating the equivariance property of the segmentation task to similarity transformations. Our training objective admits efficient implementation and exhibits fast training convergence. On established VOS benchmarks, our approach exceeds the segmentation accuracy of previous work despite using significantly less training data and compute power.



### 6D Pose Estimation with Combined Deep Learning and 3D Vision Techniques for a Fast and Accurate Object Grasping
- **Arxiv ID**: http://arxiv.org/abs/2111.06276v1
- **DOI**: 10.1016/j.robot.2021.103775
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2111.06276v1)
- **Published**: 2021-11-11 15:36:55+00:00
- **Updated**: 2021-11-11 15:36:55+00:00
- **Authors**: Tuan-Tang Le, Trung-Son Le, Yu-Ru Chen, Joel Vidal, Chyi-Yeu Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Real-time robotic grasping, supporting a subsequent precise object-in-hand operation task, is a priority target towards highly advanced autonomous systems. However, such an algorithm which can perform sufficiently-accurate grasping with time efficiency is yet to be found. This paper proposes a novel method with a 2-stage approach that combines a fast 2D object recognition using a deep neural network and a subsequent accurate and fast 6D pose estimation based on Point Pair Feature framework to form a real-time 3D object recognition and grasping solution capable of multi-object class scenes. The proposed solution has a potential to perform robustly on real-time applications, requiring both efficiency and accuracy. In order to validate our method, we conducted extensive and thorough experiments involving laborious preparation of our own dataset. The experiment results show that the proposed method scores 97.37% accuracy in 5cm5deg metric and 99.37% in Average Distance metric. Experiment results have shown an overall 62% relative improvement (5cm5deg metric) and 52.48% (Average Distance metric) by using the proposed method. Moreover, the pose estimation execution also showed an average improvement of 47.6% in running time. Finally, to illustrate the overall efficiency of the system in real-time operations, a pick-and-place robotic experiment is conducted and has shown a convincing success rate with 90% of accuracy. This experiment video is available at https://sites.google.com/view/dl-ppf6dpose/.



### Related Work on Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2111.06291v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06291v2)
- **Published**: 2021-11-11 16:11:27+00:00
- **Updated**: 2022-04-13 05:55:28+00:00
- **Authors**: Dongxu Wang
- **Comment**: This is just my reading notes
- **Journal**: None
- **Summary**: Due to the existence of quality degradations introduced in various stages of visual signal acquisition, compression, transmission and display, image quality assessment (IQA) plays a vital role in image-based applications. According to whether the reference image is complete and available, image quality evaluation can be divided into three categories: Full-Reference(FR), Reduced- Reference(RR), and Non- Reference(NR). This article will review the state-of-the-art image quality assessment algorithms.



### Automatically identifying a mobile phone user's position within a vehicle
- **Arxiv ID**: http://arxiv.org/abs/2111.06306v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06306v1)
- **Published**: 2021-11-11 16:52:57+00:00
- **Updated**: 2021-11-11 16:52:57+00:00
- **Authors**: Matt Knutson, Kevin Kramer, Sara Seifert, Ryan Chamberlain
- **Comment**: 4 pages, 1 figure
- **Journal**: None
- **Summary**: Traffic-related injuries and fatalities are major health risks in the United States. Mobile phone use while driving quadruples the risk for a motor vehicle crash. This work demonstrates the feasibility of using the mobile phone camera to passively detect the location of the phone's user within a vehicle. In a large, varied dataset we were able correctly identify if the user was in the driver's seat or one of the passenger seats with 94.9% accuracy. This model could be used by application developers to selectively change or lock functionality while a user is driving, but not if the user is a passenger in a moving vehicle.



### Unsupervised Part Discovery from Contrastive Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2111.06349v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.06349v2)
- **Published**: 2021-11-11 17:59:42+00:00
- **Updated**: 2022-03-21 14:34:39+00:00
- **Authors**: Subhabrata Choudhury, Iro Laina, Christian Rupprecht, Andrea Vedaldi
- **Comment**: NeurIPS 2021. Project page:
  https://www.robots.ox.ac.uk/~vgg/research/unsup-parts/
- **Journal**: None
- **Summary**: The goal of self-supervised visual representation learning is to learn strong, transferable image representations, with the majority of research focusing on object or scene level. On the other hand, representation learning at part level has received significantly less attention. In this paper, we propose an unsupervised approach to object part discovery and segmentation and make three contributions. First, we construct a proxy task through a set of objectives that encourages the model to learn a meaningful decomposition of the image into its parts. Secondly, prior work argues for reconstructing or clustering pre-computed features as a proxy to parts; we show empirically that this alone is unlikely to find meaningful parts; mainly because of their low resolution and the tendency of classification networks to spatially smear out information. We suggest that image reconstruction at the level of pixels can alleviate this problem, acting as a complementary cue. Lastly, we show that the standard evaluation based on keypoint regression does not correlate well with segmentation quality and thus introduce different metrics, NMI and ARI, that better characterize the decomposition of objects into parts. Our method yields semantic parts which are consistent across fine-grained but visually distinct categories, outperforming the state of the art on three benchmark datasets. Code is available at the project page: https://www.robots.ox.ac.uk/~vgg/research/unsup-parts/.



### Masked Autoencoders Are Scalable Vision Learners
- **Arxiv ID**: http://arxiv.org/abs/2111.06377v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06377v3)
- **Published**: 2021-11-11 18:46:40+00:00
- **Updated**: 2021-12-19 19:23:25+00:00
- **Authors**: Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick
- **Comment**: Tech report. arXiv v2: add more transfer learning results; v3: add
  robustness evaluation
- **Journal**: None
- **Summary**: This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.



### Learning Signal-Agnostic Manifolds of Neural Fields
- **Arxiv ID**: http://arxiv.org/abs/2111.06387v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2111.06387v1)
- **Published**: 2021-11-11 18:57:40+00:00
- **Updated**: 2021-11-11 18:57:40+00:00
- **Authors**: Yilun Du, Katherine M. Collins, Joshua B. Tenenbaum, Vincent Sitzmann
- **Comment**: NeurIPS 2021, additional results and code at
  https://yilundu.github.io/gem/
- **Journal**: None
- **Summary**: Deep neural networks have been used widely to learn the latent structure of datasets, across modalities such as images, shapes, and audio signals. However, existing models are generally modality-dependent, requiring custom architectures and objectives to process different classes of signals. We leverage neural fields to capture the underlying structure in image, shape, audio and cross-modal audiovisual domains in a modality-independent manner. We cast our task as one of learning a manifold, where we aim to infer a low-dimensional, locally linear subspace in which our data resides. By enforcing coverage of the manifold, local linearity, and local isometry, our model -- dubbed GEM -- learns to capture the underlying structure of datasets across modalities. We can then travel along linear regions of our manifold to obtain perceptually consistent interpolations between samples, and can further use GEM to recover points on our manifold and glean not only diverse completions of input images, but cross-modal hallucinations of audio or image signals. Finally, we show that by walking across the underlying manifold of GEM, we may generate new samples in our signal domains. Code and additional results are available at https://yilundu.github.io/gem/.



### Full-Body Visual Self-Modeling of Robot Morphologies
- **Arxiv ID**: http://arxiv.org/abs/2111.06389v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2111.06389v2)
- **Published**: 2021-11-11 18:58:07+00:00
- **Updated**: 2021-11-21 19:10:14+00:00
- **Authors**: Boyuan Chen, Robert Kwiatkowski, Carl Vondrick, Hod Lipson
- **Comment**: Project website: https://robot-morphology.cs.columbia.edu/
- **Journal**: None
- **Summary**: Internal computational models of physical bodies are fundamental to the ability of robots and animals alike to plan and control their actions. These "self-models" allow robots to consider outcomes of multiple possible future actions, without trying them out in physical reality. Recent progress in fully data-driven self-modeling has enabled machines to learn their own forward kinematics directly from task-agnostic interaction data. However, forward-kinema\-tics models can only predict limited aspects of the morphology, such as the position of end effectors or velocity of joints and masses. A key challenge is to model the entire morphology and kinematics, without prior knowledge of what aspects of the morphology will be relevant to future tasks. Here, we propose that instead of directly modeling forward-kinematics, a more useful form of self-modeling is one that could answer space occupancy queries, conditioned on the robot's state. Such query-driven self models are continuous in the spatial domain, memory efficient, fully differentiable and kinematic aware. In physical experiments, we demonstrate how a visual self-model is accurate to about one percent of the workspace, enabling the robot to perform various motion planning and control tasks. Visual self-modeling can also allow the robot to detect, localize and recover from real-world damage, leading to improved machine resiliency. Our project website is at: https://robot-morphology.cs.columbia.edu/



### The Emergence of Objectness: Learning Zero-Shot Segmentation from Videos
- **Arxiv ID**: http://arxiv.org/abs/2111.06394v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.06394v1)
- **Published**: 2021-11-11 18:59:11+00:00
- **Updated**: 2021-11-11 18:59:11+00:00
- **Authors**: Runtao Liu, Zhirong Wu, Stella X. Yu, Stephen Lin
- **Comment**: This paper has been accepted to NeurIPS 2021
- **Journal**: None
- **Summary**: Humans can easily segment moving objects without knowing what they are. That objectness could emerge from continuous visual observations motivates us to model grouping and movement concurrently from unlabeled videos. Our premise is that a video has different views of the same scene related by moving components, and the right region segmentation and region flow would allow mutual view synthesis which can be checked from the data itself without any external supervision. Our model starts with two separate pathways: an appearance pathway that outputs feature-based region segmentation for a single image, and a motion pathway that outputs motion features for a pair of images. It then binds them in a conjoint representation called segment flow that pools flow offsets over each region and provides a gross characterization of moving regions for the entire scene. By training the model to minimize view synthesis errors based on segment flow, our appearance and motion pathways learn region segmentation and flow estimation automatically without building them up from low-level edges or optical flows respectively. Our model demonstrates the surprising emergence of objectness in the appearance pathway, surpassing prior works on zero-shot object segmentation from an image, moving object segmentation from a video with unsupervised test-time adaptation, and semantic image segmentation by supervised fine-tuning. Our work is the first truly end-to-end zero-shot object segmentation from videos. It not only develops generic objectness for segmentation and tracking, but also outperforms prevalent image-based contrastive learning methods without augmentation engineering.



### Multiple Hypothesis Hypergraph Tracking for Posture Identification in Embryonic Caenorhabditis elegans
- **Arxiv ID**: http://arxiv.org/abs/2111.06425v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.CB
- **Links**: [PDF](http://arxiv.org/pdf/2111.06425v2)
- **Published**: 2021-11-11 19:15:07+00:00
- **Updated**: 2022-07-09 00:33:02+00:00
- **Authors**: Andrew Lauziere, Evan Ardiel, Stephen Xu, Hari Shroff
- **Comment**: None
- **Journal**: None
- **Summary**: Current methods in multiple object tracking (MOT) rely on independent object trajectories undergoing predictable motion to effectively track large numbers of objects. Adversarial conditions such as volatile object motion and imperfect detections create a challenging tracking landscape in which established methods may yield inadequate results. Multiple hypothesis hypergraph tracking (MHHT) is developed to perform MOT among interdependent objects amid noisy detections. The method extends traditional multiple hypothesis tracking (MHT) via hypergraphs to model correlated object motion, allowing for robust tracking in challenging scenarios. MHHT is applied to perform seam cell tracking during late-stage embryogenesis in embryonic C. elegans.



### Expert Human-Level Driving in Gran Turismo Sport Using Deep Reinforcement Learning with Image-based Representation
- **Arxiv ID**: http://arxiv.org/abs/2111.06449v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06449v1)
- **Published**: 2021-11-11 20:31:19+00:00
- **Updated**: 2021-11-11 20:31:19+00:00
- **Authors**: Ryuji Imamura, Takuma Seno, Kenta Kawamoto, Michael Spranger
- **Comment**: Accepted at Deep Reinforcement Learning Workshop at Neural
  Information Processing Systems 2021
- **Journal**: None
- **Summary**: When humans play virtual racing games, they use visual environmental information on the game screen to understand the rules within the environments. In contrast, a state-of-the-art realistic racing game AI agent that outperforms human players does not use image-based environmental information but the compact and precise measurements provided by the environment. In this paper, a vision-based control algorithm is proposed and compared with human player performances under the same conditions in realistic racing scenarios using Gran Turismo Sport (GTS), which is known as a high-fidelity realistic racing simulator. In the proposed method, the environmental information that constitutes part of the observations in conventional state-of-the-art methods is replaced with feature representations extracted from game screen images. We demonstrate that the proposed method performs expert human-level vehicle control under high-speed driving scenarios even with game screen images as high-dimensional inputs. Additionally, it outperforms the built-in AI in GTS in a time trial task, and its score places it among the top 10% approximately 28,000 human players.



### Dynamic Iterative Refinement for Efficient 3D Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2111.06500v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06500v1)
- **Published**: 2021-11-11 23:31:34+00:00
- **Updated**: 2021-11-11 23:31:34+00:00
- **Authors**: John Yang, Yash Bhalgat, Simyung Chang, Fatih Porikli, Nojun Kwak
- **Comment**: None
- **Journal**: None
- **Summary**: While hand pose estimation is a critical component of most interactive extended reality and gesture recognition systems, contemporary approaches are not optimized for computational and memory efficiency. In this paper, we propose a tiny deep neural network of which partial layers are recursively exploited for refining its previous estimations. During its iterative refinements, we employ learned gating criteria to decide whether to exit from the weight-sharing loop, allowing per-sample adaptation in our model. Our network is trained to be aware of the uncertainty in its current predictions to efficiently gate at each iteration, estimating variances after each loop for its keypoint estimates. Additionally, we investigate the effectiveness of end-to-end and progressive training protocols for our recursive structure on maximizing the model capacity. With the proposed setting, our method consistently outperforms state-of-the-art 2D/3D hand pose estimation approaches in terms of both accuracy and efficiency for widely used benchmarks.



