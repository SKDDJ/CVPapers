# Arxiv Papers in cs.CV on 2021-11-10
### Are Transformers More Robust Than CNNs?
- **Arxiv ID**: http://arxiv.org/abs/2111.05464v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05464v1)
- **Published**: 2021-11-10 00:18:59+00:00
- **Updated**: 2021-11-10 00:18:59+00:00
- **Authors**: Yutong Bai, Jieru Mei, Alan Yuille, Cihang Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Transformer emerges as a powerful tool for visual recognition. In addition to demonstrating competitive performance on a broad range of visual benchmarks, recent works also argue that Transformers are much more robust than Convolutions Neural Networks (CNNs). Nonetheless, surprisingly, we find these conclusions are drawn from unfair experimental settings, where Transformers and CNNs are compared at different scales and are applied with distinct training frameworks. In this paper, we aim to provide the first fair & in-depth comparisons between Transformers and CNNs, focusing on robustness evaluations.   With our unified training setup, we first challenge the previous belief that Transformers outshine CNNs when measuring adversarial robustness. More surprisingly, we find CNNs can easily be as robust as Transformers on defending against adversarial attacks, if they properly adopt Transformers' training recipes. While regarding generalization on out-of-distribution samples, we show pre-training on (external) large-scale datasets is not a fundamental request for enabling Transformers to achieve better performance than CNNs. Moreover, our ablations suggest such stronger generalization is largely benefited by the Transformer's self-attention-like architectures per se, rather than by other training setups. We hope this work can help the community better understand and benchmark the robustness of Transformers and CNNs. The code and models are publicly available at https://github.com/ytongbai/ViTs-vs-CNNs.



### Sparse Adversarial Video Attacks with Spatial Transformations
- **Arxiv ID**: http://arxiv.org/abs/2111.05468v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05468v1)
- **Published**: 2021-11-10 00:37:35+00:00
- **Updated**: 2021-11-10 00:37:35+00:00
- **Authors**: Ronghui Mu, Wenjie Ruan, Leandro Soriano Marcolino, Qiang Ni
- **Comment**: The short version of this work will appear in the BMVC 2021
  conference
- **Journal**: None
- **Summary**: In recent years, a significant amount of research efforts concentrated on adversarial attacks on images, while adversarial video attacks have seldom been explored. We propose an adversarial attack strategy on videos, called DeepSAVA. Our model includes both additive perturbation and spatial transformation by a unified optimisation framework, where the structural similarity index (SSIM) measure is adopted to measure the adversarial distance. We design an effective and novel optimisation scheme which alternatively utilizes Bayesian optimisation to identify the most influential frame in a video and Stochastic gradient descent (SGD) based optimisation to produce both additive and spatial-transformed perturbations. Doing so enables DeepSAVA to perform a very sparse attack on videos for maintaining human imperceptibility while still achieving state-of-the-art performance in terms of both attack success rate and adversarial transferability. Our intensive experiments on various types of deep neural networks and video datasets confirm the superiority of DeepSAVA.



### Analysis of PDE-based binarization model for degraded document images
- **Arxiv ID**: http://arxiv.org/abs/2111.05471v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05471v1)
- **Published**: 2021-11-10 00:56:45+00:00
- **Updated**: 2021-11-10 00:56:45+00:00
- **Authors**: Uche A. Nnolim
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: This report presents the results of a PDE-based binarization model for degraded document images. The model utilizes an edge and binary source term in its formulation. Results indicate effectiveness for document images with bleed-through and faded text and stains to a lesser extent.



### Learning to Disentangle Scenes for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2111.05476v2
- **DOI**: 10.1016/j.imavis.2021.104330
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05476v2)
- **Published**: 2021-11-10 01:17:10+00:00
- **Updated**: 2022-02-28 09:50:03+00:00
- **Authors**: Xianghao Zang, Ge Li, Wei Gao, Xiujun Shu
- **Comment**: Preprint Version; Accepted by Image and Vision Computing
- **Journal**: Image and Vision Computing 2021
- **Summary**: There are many challenging problems in the person re-identification (ReID) task, such as the occlusion and scale variation. Existing works usually tried to solve them by employing a one-branch network. This one-branch network needs to be robust to various challenging problems, which makes this network overburdened. This paper proposes to divide-and-conquer the ReID task. For this purpose, we employ several self-supervision operations to simulate different challenging problems and handle each challenging problem using different networks. Concretely, we use the random erasing operation and propose a novel random scaling operation to generate new images with controllable characteristics. A general multi-branch network, including one master branch and two servant branches, is introduced to handle different scenes. These branches learn collaboratively and achieve different perceptive abilities. In this way, the complex scenes in the ReID task are effectively disentangled, and the burden of each branch is relieved. The results from extensive experiments demonstrate that the proposed method achieves state-of-the-art performances on three ReID benchmarks and two occluded ReID benchmarks. Ablation study also shows that the proposed scheme and operations significantly improve the performance in various scenes. The code is available at https://git.openi.org.cn/zangxh/LDS.git.



### Handwritten Digit Recognition Using Improved Bounding Box Recognition Technique
- **Arxiv ID**: http://arxiv.org/abs/2111.05483v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05483v1)
- **Published**: 2021-11-10 01:53:34+00:00
- **Updated**: 2021-11-10 01:53:34+00:00
- **Authors**: Arkaprabha Basu, M. Sathya
- **Comment**: 41 pages, 12 figures
- **Journal**: None
- **Summary**: The project comes with the technique of OCR (Optical Character Recognition) which includes various research sides of computer science. The project is to take a picture of a character and process it up to recognize the image of that character like a human brain recognize the various digits. The project contains the deep idea of the Image Processing techniques and the big research area of machine learning and the building block of the machine learning called Neural Network. There are two different parts of the project. Training part comes with the idea of to train a child by giving various sets of similar characters but not the totally same and to say them the output of this is this. Like this idea one has to train the newly built neural network with so many characters. This part contains some new algorithm which is self-created and upgraded as the project need. The testing part contains the testing of a new dataset .This part always comes after the part of the training .At first one has to teach the child how to recognize the character .Then one has to take the test whether he has given right answer or not. If not, one has to train him harder by giving new dataset and new entries. Just like that one has to test the algorithm also. There are many parts of statistical modeling and optimization techniques which come into the project requiring a lot of modeling concept of statistics like optimizer technique and filtering process, that how the mathematics and prediction behind that filtering or the algorithms comes after or which result one actually needs to and ultimately for the prediction of a predictive model creation. Machine learning algorithm is built by concepts of prediction and programming.



### A Structure Feature Algorithm for Multi-modal Forearm Registration
- **Arxiv ID**: http://arxiv.org/abs/2111.05485v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05485v1)
- **Published**: 2021-11-10 01:58:39+00:00
- **Updated**: 2021-11-10 01:58:39+00:00
- **Authors**: Jiaxin Li, Yan Ding, Weizhong Zhang, Yifan Zhao, Lingxi Guo, Zhe Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Augmented reality technology based on image registration is becoming increasingly popular for the convenience of pre-surgery preparation and medical education. This paper focuses on the registration of forearm images and digital anatomical models. Due to the difference in texture features of forearm multi-modal images, this paper proposes a forearm feature representation curve (FFRC) based on structure compliant multi-modal image registration framework (FAM) for the forearm.



### Automated Pulmonary Embolism Detection from CTPA Images Using an End-to-End Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2111.05506v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05506v1)
- **Published**: 2021-11-10 03:01:55+00:00
- **Updated**: 2021-11-10 03:01:55+00:00
- **Authors**: Yi Lin, Jianchao Su, Xiang Wang, Xiang Li, Jingen Liu, Kwang-Ting Cheng, Xin Yang
- **Comment**: Accepted to MICCAI 2019
- **Journal**: None
- **Summary**: Automated methods for detecting pulmonary embolisms (PEs) on CT pulmonary angiography (CTPA) images are of high demand. Existing methods typically employ separate steps for PE candidate detection and false positive removal, without considering the ability of the other step. As a result, most existing methods usually suffer from a high false positive rate in order to achieve an acceptable sensitivity. This study presents an end-to-end trainable convolutional neural network (CNN) where the two steps are optimized jointly. The proposed CNN consists of three concatenated subnets: 1) a novel 3D candidate proposal network for detecting cubes containing suspected PEs, 2) a 3D spatial transformation subnet for generating fixed-sized vessel-aligned image representation for candidates, and 3) a 2D classification network which takes the three cross-sections of the transformed cubes as input and eliminates false positives. We have evaluated our approach using the 20 CTPA test dataset from the PE challenge, achieving a sensitivity of 78.9%, 80.7% and 80.7% at 2 false positives per volume at 0mm, 2mm and 5mm localization error, which is superior to the state-of-the-art methods. We have further evaluated our system on our own dataset consisting of 129 CTPA data with a total of 269 emboli. Our system achieves a sensitivity of 63.2%, 78.9% and 86.8% at 2 false positives per volume at 0mm, 2mm and 5mm localization error.



### Space-Time Memory Network for Sounding Object Localization in Videos
- **Arxiv ID**: http://arxiv.org/abs/2111.05526v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05526v1)
- **Published**: 2021-11-10 04:40:12+00:00
- **Updated**: 2021-11-10 04:40:12+00:00
- **Authors**: Sizhe Li, Yapeng Tian, Chenliang Xu
- **Comment**: Accepted to BMVC2021. Project page:
  https://sites.google.com/view/bmvc2021stm
- **Journal**: None
- **Summary**: Leveraging temporal synchronization and association within sight and sound is an essential step towards robust localization of sounding objects. To this end, we propose a space-time memory network for sounding object localization in videos. It can simultaneously learn spatio-temporal attention over both uni-modal and cross-modal representations from audio and visual modalities. We show and analyze both quantitatively and qualitatively the effectiveness of incorporating spatio-temporal learning in localizing audio-visual objects. We demonstrate that our approach generalizes over various complex audio-visual scenes and outperforms recent state-of-the-art methods.



### Understanding the Generalization Benefit of Model Invariance from a Data Perspective
- **Arxiv ID**: http://arxiv.org/abs/2111.05529v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2111.05529v2)
- **Published**: 2021-11-10 04:53:07+00:00
- **Updated**: 2023-02-23 17:09:33+00:00
- **Authors**: Sicheng Zhu, Bang An, Furong Huang
- **Comment**: Accepted to NeurIPS 2021. Version 2 includes several content
  clarifications and image format revisions
- **Journal**: None
- **Summary**: Machine learning models that are developed with invariance to certain types of data transformations have demonstrated superior generalization performance in practice. However, the underlying mechanism that explains why invariance leads to better generalization is not well-understood, limiting our ability to select appropriate data transformations for a given dataset. This paper studies the generalization benefit of model invariance by introducing the sample cover induced by transformations, i.e., a representative subset of a dataset that can approximately recover the whole dataset using transformations. Based on this notion, we refine the generalization bound for invariant models and characterize the suitability of a set of data transformations by the sample covering number induced by transformations, i.e., the smallest size of its induced sample covers. We show that the generalization bound can be tightened for suitable transformations that have a small sample covering number. Moreover, our proposed sample covering number can be empirically evaluated, providing a practical guide for selecting transformations to develop model invariance for better generalization. We evaluate the sample covering numbers for commonly used transformations on multiple datasets and demonstrate that the smaller sample covering number for a set of transformations indicates a smaller gap between the test and training error for invariant models, thus validating our propositions.



### 3D modelling of survey scene from images enhanced with a multi-exposure fusion
- **Arxiv ID**: http://arxiv.org/abs/2111.05541v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05541v1)
- **Published**: 2021-11-10 06:02:44+00:00
- **Updated**: 2021-11-10 06:02:44+00:00
- **Authors**: Kwok-Leung Chan, Liping Li, Arthur Wing-Tak Leung, Ho-Yin Chan
- **Comment**: None
- **Journal**: None
- **Summary**: In current practice, scene survey is carried out by workers using total stations. The method has high accuracy, but it incurs high costs if continuous monitoring is needed. Techniques based on photogrammetry, with the relatively cheaper digital cameras, have gained wide applications in many fields. Besides point measurement, photogrammetry can also create a three-dimensional (3D) model of the scene. Accurate 3D model reconstruction depends on high quality images. Degraded images will result in large errors in the reconstructed 3D model. In this paper, we propose a method that can be used to improve the visibility of the images, and eventually reduce the errors of the 3D scene model. The idea is inspired by image dehazing. Each original image is first transformed into multiple exposure images by means of gamma-correction operations and adaptive histogram equalization. The transformed images are analyzed by the computation of the local binary patterns. The image is then enhanced, with each pixel generated from the set of transformed image pixels weighted by a function of the local pattern feature and image saturation. Performance evaluation has been performed on benchmark image dehazing datasets. Experimentations have been carried out on outdoor and indoor surveys. Our analysis finds that the method works on different types of degradation that exist in both outdoor and indoor images. When fed into the photogrammetry software, the enhanced images can reconstruct 3D scene models with sub-millimeter mean errors.



### ICDAR 2021 Competition on Document VisualQuestion Answering
- **Arxiv ID**: http://arxiv.org/abs/2111.05547v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.05547v1)
- **Published**: 2021-11-10 06:52:07+00:00
- **Updated**: 2021-11-10 06:52:07+00:00
- **Authors**: Rubèn Tito, Minesh Mathew, C. V. Jawahar, Ernest Valveny, Dimosthenis Karatzas
- **Comment**: None
- **Journal**: None
- **Summary**: In this report we present results of the ICDAR 2021 edition of the Document Visual Question Challenges. This edition complements the previous tasks on Single Document VQA and Document Collection VQA with a newly introduced on Infographics VQA. Infographics VQA is based on a new dataset of more than 5,000 infographics images and 30,000 question-answer pairs. The winner methods have scored 0.6120 ANLS in Infographics VQA task, 0.7743 ANLSL in Document Collection VQA task and 0.8705 ANLS in Single Document VQA. We present a summary of the datasets used for each task, description of each of the submitted methods and the results and analysis of their performance. A summary of the progress made on Single Document VQA since the first edition of the DocVQA 2020 challenge is also presented.



### Deep Attention-guided Graph Clustering with Dual Self-supervision
- **Arxiv ID**: http://arxiv.org/abs/2111.05548v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.05548v3)
- **Published**: 2021-11-10 06:53:03+00:00
- **Updated**: 2022-12-26 05:45:31+00:00
- **Authors**: Zhihao Peng, Hui Liu, Yuheng Jia, Junhui Hou
- **Comment**: Accepted by IEEE TCSVT
- **Journal**: None
- **Summary**: Existing deep embedding clustering works only consider the deepest layer to learn a feature embedding and thus fail to well utilize the available discriminative information from cluster assignments, resulting performance limitation. To this end, we propose a novel method, namely deep attention-guided graph clustering with dual self-supervision (DAGC). Specifically, DAGC first utilizes a heterogeneity-wise fusion module to adaptively integrate the features of an auto-encoder and a graph convolutional network in each layer and then uses a scale-wise fusion module to dynamically concatenate the multi-scale features in different layers. Such modules are capable of learning a discriminative feature embedding via an attention-based mechanism. In addition, we design a distribution-wise fusion module that leverages cluster assignments to acquire clustering results directly. To better explore the discriminative information from the cluster assignments, we develop a dual self-supervision solution consisting of a soft self-supervision strategy with a triplet Kullback-Leibler divergence loss and a hard self-supervision strategy with a pseudo supervision loss. Extensive experiments validate that our method consistently outperforms state-of-the-art methods on six benchmark datasets. Especially, our method improves the ARI by more than 18.14% over the best baseline.



### TomoSLAM: factor graph optimization for rotation angle refinement in microtomography
- **Arxiv ID**: http://arxiv.org/abs/2111.05562v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2111.05562v1)
- **Published**: 2021-11-10 08:00:46+00:00
- **Updated**: 2021-11-10 08:00:46+00:00
- **Authors**: Mark Griguletskii, Mikhail Chekanov, Oleg Shipitko
- **Comment**: None
- **Journal**: None
- **Summary**: In computed tomography (CT), the relative trajectories of a sample, a detector, and a signal source are traditionally considered to be known, since they are caused by the intentional preprogrammed movement of the instrument parts. However, due to the mechanical backlashes, rotation sensor measurement errors, thermal deformations real trajectory differs from desired ones. This negatively affects the resulting quality of tomographic reconstruction. Neither the calibration nor preliminary adjustments of the device completely eliminates the inaccuracy of the trajectory but significantly increase the cost of instrument maintenance. A number of approaches to this problem are based on an automatic refinement of the source and sensor position estimate relative to the sample for each projection (at each time step) during the reconstruction process. A similar problem of position refinement while observing different images of an object from different angles is well known in robotics (particularly, in mobile robots and self-driving vehicles) and is called Simultaneous Localization And Mapping (SLAM). The scientific novelty of this work is to consider the problem of trajectory refinement in microtomography as a SLAM problem. This is achieved by extracting Speeded Up Robust Features (SURF) features from X-ray projections, filtering matches with Random Sample Consensus (RANSAC), calculating angles between projections, and using them in factor graph in combination with stepper motor control signals in order to refine rotation angles.



### CLIP2TV: Align, Match and Distill for Video-Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2111.05610v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2111.05610v2)
- **Published**: 2021-11-10 10:05:11+00:00
- **Updated**: 2022-07-21 17:19:19+00:00
- **Authors**: Zijian Gao, Jingyu Liu, Weiqi Sun, Sheng Chen, Dedan Chang, Lili Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Modern video-text retrieval frameworks basically consist of three parts: video encoder, text encoder and the similarity head. With the success on both visual and textual representation learning, transformer based encoders and fusion methods have also been adopted in the field of video-text retrieval. In this report, we present CLIP2TV, aiming at exploring where the critical elements lie in transformer based methods. To achieve this, We first revisit some recent works on multi-modal learning, then introduce some techniques into video-text retrieval, finally evaluate them through extensive experiments in different configurations. Notably, CLIP2TV achieves 52.9@R1 on MSR-VTT dataset, outperforming the previous SOTA result by 4.1%.



### Leveraging Geometry for Shape Estimation from a Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/2111.05615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05615v1)
- **Published**: 2021-11-10 10:17:56+00:00
- **Updated**: 2021-11-10 10:17:56+00:00
- **Authors**: Florian Langer, Ignas Budvytis, Roberto Cipolla
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting 3D shapes and poses of static objects from a single RGB image is an important research area in modern computer vision. Its applications range from augmented reality to robotics and digital content creation. Typically this task is performed through direct object shape and pose predictions which is inaccurate. A promising research direction ensures meaningful shape predictions by retrieving CAD models from large scale databases and aligning them to the objects observed in the image. However, existing work does not take the object geometry into account, leading to inaccurate object pose predictions, especially for unseen objects. In this work we demonstrate how cross-domain keypoint matches from an RGB image to a rendered CAD model allow for more precise object pose predictions compared to ones obtained through direct predictions. We further show that keypoint matches can not only be used to estimate the pose of an object, but also to modify the shape of the object itself. This is important as the accuracy that can be achieved with object retrieval alone is inherently limited to the available CAD models. Allowing shape adaptation bridges the gap between the retrieved CAD model and the observed shape. We demonstrate our approach on the challenging Pix3D dataset. The proposed geometric shape prediction improves the AP mesh over the state-of-the-art from 33.2 to 37.8 on seen objects and from 8.2 to 17.1 on unseen objects. Furthermore, we demonstrate more accurate shape predictions without closely matching CAD models when following the proposed shape adaptation. Code is publicly available at https://github.com/florianlanger/leveraging_geometry_for_shape_estimation .



### FabricFlowNet: Bimanual Cloth Manipulation with a Flow-based Policy
- **Arxiv ID**: http://arxiv.org/abs/2111.05623v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05623v3)
- **Published**: 2021-11-10 10:29:38+00:00
- **Updated**: 2022-04-10 23:16:35+00:00
- **Authors**: Thomas Weng, Sujay Bajracharya, Yufei Wang, Khush Agrawal, David Held
- **Comment**: CoRL 2021
- **Journal**: None
- **Summary**: We address the problem of goal-directed cloth manipulation, a challenging task due to the deformability of cloth. Our insight is that optical flow, a technique normally used for motion estimation in video, can also provide an effective representation for corresponding cloth poses across observation and goal images. We introduce FabricFlowNet (FFN), a cloth manipulation policy that leverages flow as both an input and as an action representation to improve performance. FabricFlowNet also elegantly switches between bimanual and single-arm actions based on the desired goal. We show that FabricFlowNet significantly outperforms state-of-the-art model-free and model-based cloth manipulation policies that take image input. We also present real-world experiments on a bimanual system, demonstrating effective sim-to-real transfer. Finally, we show that our method generalizes when trained on a single square cloth to other cloth shapes, such as T-shirts and rectangular cloths. Video and other supplementary materials are available at: https://sites.google.com/view/fabricflownet.



### The Impact of Changes in Resolution on the Persistent Homology of Images
- **Arxiv ID**: http://arxiv.org/abs/2111.05663v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV, math.AT, 68T09, 68U03, 55N31, 62R40, 54H30, 55-08
- **Links**: [PDF](http://arxiv.org/pdf/2111.05663v1)
- **Published**: 2021-11-10 12:07:54+00:00
- **Updated**: 2021-11-10 12:07:54+00:00
- **Authors**: Teresa Heiss, Sarah Tymochko, Brittany Story, Adélie Garin, Hoa Bui, Bea Bleile, Vanessa Robins
- **Comment**: accepted for the IEEE Big Data 2021 workshop: Applications of
  Topological Data Analysis to 'Big Data'
- **Journal**: None
- **Summary**: Digital images enable quantitative analysis of material properties at micro and macro length scales, but choosing an appropriate resolution when acquiring the image is challenging. A high resolution means longer image acquisition and larger data requirements for a given sample, but if the resolution is too low, significant information may be lost. This paper studies the impact of changes in resolution on persistent homology, a tool from topological data analysis that provides a signature of structure in an image across all length scales. Given prior information about a function, the geometry of an object, or its density distribution at a given resolution, we provide methods to select the coarsest resolution yielding results within an acceptable tolerance. We present numerical case studies for an illustrative synthetic example and samples from porous materials where the theoretical bounds are unknown.



### Explanatory Analysis and Rectification of the Pitfalls in COVID-19 Datasets
- **Arxiv ID**: http://arxiv.org/abs/2111.05679v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2111.05679v1)
- **Published**: 2021-11-10 13:38:51+00:00
- **Updated**: 2021-11-10 13:38:51+00:00
- **Authors**: Samyak Prajapati, Japman Singh Monga, Shaanya Singh, Amrit Raj, Yuvraj Singh Champawat, Chandra Prakash
- **Comment**: None
- **Journal**: None
- **Summary**: Since the onset of the COVID-19 pandemic in 2020, millions of people have succumbed to this deadly virus. Many attempts have been made to devise an automated method of testing that could detect the virus. Various researchers around the globe have proposed deep learning based methodologies to detect the COVID-19 using Chest X-Rays. However, questions have been raised on the presence of bias in the publicly available Chest X-Ray datasets which have been used by the majority of the researchers. In this paper, we propose a 2 staged methodology to address this topical issue. Two experiments have been conducted as a part of stage 1 of the methodology to exhibit the presence of bias in the datasets. Subsequently, an image segmentation, super-resolution and CNN based pipeline along with different image augmentation techniques have been proposed in stage 2 of the methodology to reduce the effect of bias. InceptionResNetV2 trained on Chest X-Ray images that were augmented with Histogram Equalization followed by Gamma Correction when passed through the pipeline proposed in stage 2, yielded a top accuracy of 90.47% for 3-class (Normal, Pneumonia, and COVID-19) classification task.



### Learning to ignore: rethinking attention in CNNs
- **Arxiv ID**: http://arxiv.org/abs/2111.05684v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.05684v1)
- **Published**: 2021-11-10 13:47:37+00:00
- **Updated**: 2021-11-10 13:47:37+00:00
- **Authors**: Firas Laakom, Kateryna Chumachenko, Jenni Raitoharju, Alexandros Iosifidis, Moncef Gabbouj
- **Comment**: accepted to BMVC 2021
- **Journal**: None
- **Summary**: Recently, there has been an increasing interest in applying attention mechanisms in Convolutional Neural Networks (CNNs) to solve computer vision tasks. Most of these methods learn to explicitly identify and highlight relevant parts of the scene and pass the attended image to further layers of the network. In this paper, we argue that such an approach might not be optimal. Arguably, explicitly learning which parts of the image are relevant is typically harder than learning which parts of the image are less relevant and, thus, should be ignored. In fact, in vision domain, there are many easy-to-identify patterns of irrelevant features. For example, image regions close to the borders are less likely to contain useful information for a classification task. Based on this idea, we propose to reformulate the attention mechanism in CNNs to learn to ignore instead of learning to attend. Specifically, we propose to explicitly learn irrelevant information in the scene and suppress it in the produced representation, keeping only important attributes. This implicit attention scheme can be incorporated into any existing attention mechanism. In this work, we validate this idea using two recent attention methods Squeeze and Excitation (SE) block and Convolutional Block Attention Module (CBAM). Experimental results on different datasets and model architectures show that learning to ignore, i.e., implicit attention, yields superior performance compared to the standard approaches.



### Efficient Neural Network Training via Forward and Backward Propagation Sparsification
- **Arxiv ID**: http://arxiv.org/abs/2111.05685v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05685v1)
- **Published**: 2021-11-10 13:49:47+00:00
- **Updated**: 2021-11-10 13:49:47+00:00
- **Authors**: Xiao Zhou, Weizhong Zhang, Zonghao Chen, Shizhe Diao, Tong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse training is a natural idea to accelerate the training speed of deep neural networks and save the memory usage, especially since large modern neural networks are significantly over-parameterized. However, most of the existing methods cannot achieve this goal in practice because the chain rule based gradient (w.r.t. structure parameters) estimators adopted by previous methods require dense computation at least in the backward propagation step. This paper solves this problem by proposing an efficient sparse training method with completely sparse forward and backward passes. We first formulate the training process as a continuous minimization problem under global sparsity constraint. We then separate the optimization process into two steps, corresponding to weight update and structure parameter update. For the former step, we use the conventional chain rule, which can be sparse via exploiting the sparse structure. For the latter step, instead of using the chain rule based gradient estimators as in existing methods, we propose a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation, thus achieving completely sparse training. We prove that the variance of our gradient estimator is bounded. Extensive experimental results on real-world datasets demonstrate that compared to previous methods, our algorithm is much more effective in accelerating the training process, up to an order of magnitude faster.



### Analysis of Multiscale Wavelet-based Fractional Gradient-Anisotropic Diffusion Fusion for single hazy and underwater image enhancement
- **Arxiv ID**: http://arxiv.org/abs/2111.15479v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.15479v1)
- **Published**: 2021-11-10 13:54:37+00:00
- **Updated**: 2021-11-10 13:54:37+00:00
- **Authors**: Uche A. Nnolim
- **Comment**: 15 pages, 10 figures
- **Journal**: None
- **Summary**: This report presents the results of a multi-scale wavelet based scheme for single image de-hazing and underwater image enhancement. The scheme is fast and highly localized in addition to global enhancement of hazy images. A PDE-based formulation enables additional versatility as the iterative nature allows more flexibility for various types of images. Visual and objective results from experiments indicate that the proposed approach competes favourably or surpasses most of the state-of-the-art approaches.



### Robust reconstructions by multi-scale/irregular tangential covering
- **Arxiv ID**: http://arxiv.org/abs/2111.05688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05688v1)
- **Published**: 2021-11-10 14:02:05+00:00
- **Updated**: 2021-11-10 14:02:05+00:00
- **Authors**: Antoine Vacavant, Bertrand Kerautret, Fabien Feschet
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an original manner to employ a tangential cover algorithm - minDSS - in order to geometrically reconstruct noisy digital contours. To do so, we exploit the representation of graphical objects by maximal primitives we have introduced in previous works. By calculating multi-scale and irregular isothetic representations of the contour, we obtained 1-D (one-dimensional) intervals, and achieved afterwards a decomposition into maximal line segments or circular arcs. By adapting minDSS to this sparse and irregular data of 1-D intervals supporting the maximal primitives, we are now able to reconstruct the input noisy objects into cyclic contours made of lines or arcs with a minimal number of primitives. In this work, we explain our novel complete pipeline, and present its experimental evaluation by considering both synthetic and real image data. We also show that this is a robust approach, with respect to selected references from state-of-the-art, and by considering a multi-scale noise evaluation process.



### Multi-Scale Single Image Dehazing Using Laplacian and Gaussian Pyramids
- **Arxiv ID**: http://arxiv.org/abs/2111.05700v2
- **DOI**: 10.1109/TIP.2021.3123551
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05700v2)
- **Published**: 2021-11-10 14:17:58+00:00
- **Updated**: 2021-11-14 08:18:52+00:00
- **Authors**: Zhengguo Li, Haiyan Shu, Chaobing Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Model driven single image dehazing was widely studied on top of different priors due to its extensive applications. Ambiguity between object radiance and haze and noise amplification in sky regions are two inherent problems of model driven single image dehazing. In this paper, a dark direct attenuation prior (DDAP) is proposed to address the former problem. A novel haze line averaging is proposed to reduce the morphological artifacts caused by the DDAP which enables a weighted guided image filter with a smaller radius to further reduce the morphological artifacts while preserve the fine structure in the image. A multi-scale dehazing algorithm is then proposed to address the latter problem by adopting Laplacian and Guassian pyramids to decompose the hazy image into different levels and applying different haze removal and noise reduction approaches to restore the scene radiance at different levels of the pyramid. The resultant pyramid is collapsed to restore a haze-free image. Experiment results demonstrate that the proposed algorithm outperforms state of the art dehazing algorithms and the noise is indeed prevented from being amplified in the sky region.



### Single image dehazing via combining the prior knowledge and CNNs
- **Arxiv ID**: http://arxiv.org/abs/2111.05701v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05701v2)
- **Published**: 2021-11-10 14:18:25+00:00
- **Updated**: 2021-11-14 08:32:29+00:00
- **Authors**: Yuwen Li, Chaobing Zheng, Shiqian Wu, Wangming Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Aiming at the existing single image haze removal algorithms, which are based on prior knowledge and assumptions, subject to many limitations in practical applications, and could suffer from noise and halo amplification. An end-to-end system is proposed in this paper to reduce defects by combining the prior knowledge and deep learning method. The haze image is decomposed into the base layer and detail layers through a weighted guided image filter (WGIF) firstly, and the airlight is estimated from the base layer. Then, the base layer image is passed to the efficient deep convolutional network for estimating the transmission map. To restore object close to the camera completely without amplifying noise in sky or heavily hazy scene, an adaptive strategy is proposed based on the value of the transmission map. If the transmission map of a pixel is small, the base layer of the haze image is used to recover a haze-free image via atmospheric scattering model, finally. Otherwise, the haze image is used. Experiments show that the proposed method achieves superior performance over existing methods.



### An Underexplored Dilemma between Confidence and Calibration in Quantized Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2111.08163v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08163v2)
- **Published**: 2021-11-10 14:37:16+00:00
- **Updated**: 2021-12-02 16:58:06+00:00
- **Authors**: Guoxuan Xia, Sangwon Ha, Tiago Azevedo, Partha Maji
- **Comment**: Accepted at I (Still) Can't Believe It's Not Better Workshop at
  NeurIPS 2021
- **Journal**: None
- **Summary**: Modern convolutional neural networks (CNNs) are known to be overconfident in terms of their calibration on unseen input data. That is to say, they are more confident than they are accurate. This is undesirable if the probabilities predicted are to be used for downstream decision making. When considering accuracy, CNNs are also surprisingly robust to compression techniques, such as quantization, which aim to reduce computational and memory costs. We show that this robustness can be partially explained by the calibration behavior of modern CNNs, and may be improved with overconfidence. This is due to an intuitive result: low confidence predictions are more likely to change post-quantization, whilst being less accurate. High confidence predictions will be more accurate, but more difficult to change. Thus, a minimal drop in post-quantization accuracy is incurred. This presents a potential conflict in neural network design: worse calibration from overconfidence may lead to better robustness to quantization. We perform experiments applying post-training quantization to a variety of CNNs, on the CIFAR-100 and ImageNet datasets.



### Multimodal Approach for Metadata Extraction from German Scientific Publications
- **Arxiv ID**: http://arxiv.org/abs/2111.05736v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.05736v1)
- **Published**: 2021-11-10 15:19:04+00:00
- **Updated**: 2021-11-10 15:19:04+00:00
- **Authors**: Azeddine Bouabdallah, Jorge Gavilan, Jennifer Gerbl, Prayuth Patumcharoenpol
- **Comment**: 8 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: Nowadays, metadata information is often given by the authors themselves upon submission. However, a significant part of already existing research papers have missing or incomplete metadata information. German scientific papers come in a large variety of layouts which makes the extraction of metadata a non-trivial task that requires a precise way to classify the metadata extracted from the documents. In this paper, we propose a multimodal deep learning approach for metadata extraction from scientific papers in the German language. We consider multiple types of input data by combining natural language processing and image vision processing. This model aims to increase the overall accuracy of metadata extraction compared to other state-of-the-art approaches. It enables the utilization of both spatial and contextual features in order to achieve a more reliable extraction. Our model for this approach was trained on a dataset consisting of around 8800 documents and is able to obtain an overall F1-score of 0.923.



### Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2111.05759v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05759v2)
- **Published**: 2021-11-10 16:04:49+00:00
- **Updated**: 2022-07-18 14:11:51+00:00
- **Authors**: Chuang Lin, Yi Jiang, Jianfei Cai, Lizhen Qu, Gholamreza Haffari, Zehuan Yuan
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Vision-and-Language Navigation (VLN) is a task that an agent is required to follow a language instruction to navigate to the goal position, which relies on the ongoing interactions with the environment during moving. Recent Transformer-based VLN methods have made great progress benefiting from the direct connections between visual observations and the language instruction via the multimodal cross-attention mechanism. However, these methods usually represent temporal context as a fixed-length vector by using an LSTM decoder or using manually designed hidden states to build a recurrent Transformer. Considering a single fixed-length vector is often insufficient to capture long-term temporal context, in this paper, we introduce Multimodal Transformer with Variable-length Memory (MTVM) for visually-grounded natural language navigation by modelling the temporal context explicitly. Specifically, MTVM enables the agent to keep track of the navigation trajectory by directly storing previous activations in a memory bank. To further boost the performance, we propose a memory-aware consistency loss to help learn a better joint representation of temporal context with random masked instructions. We evaluate MTVM on popular R2R and CVDN datasets, and our model improves Success Rate on R2R unseen validation and test set by 2% each, and reduce Goal Process by 1.6m on CVDN test set.



### Theoretical and empirical analysis of a fast algorithm for extracting polygons from signed distance bounds
- **Arxiv ID**: http://arxiv.org/abs/2111.05778v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CG, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05778v1)
- **Published**: 2021-11-10 16:31:27+00:00
- **Updated**: 2021-11-10 16:31:27+00:00
- **Authors**: Nenad Markuš
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate an asymptotically fast method for transforming signed distance bounds into polygon meshes. This is achieved by combining sphere tracing (also known as ray marching) and one of the traditional polygonization schemes (e.g., Marching cubes). Let us call this approach Gridhopping. We provide theoretical and experimental evidence that it is of the $O(N^2\log N)$ computational complexity for a polygonization grid with $N^3$ cells. The algorithm is tested on both a set of primitive shapes as well as signed distance fields generated from point clouds by machine learning. Given its speed, simplicity and portability, we argue that it could prove useful during the modelling stage as well as in shape compression for storage.   The code is available here: https://github.com/nenadmarkus/gridhopping



### Evaluation of Deep Learning Topcoders Method for Neuron Individualization in Histological Macaque Brain Section
- **Arxiv ID**: http://arxiv.org/abs/2111.05789v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05789v2)
- **Published**: 2021-11-10 16:38:35+00:00
- **Updated**: 2022-08-08 12:08:28+00:00
- **Authors**: Huaqian Wu, Nicolas Souedet, Zhenzhen You, Caroline Jan, Cédric Clouchoux, Thierry Delzescaux
- **Comment**: None
- **Journal**: None
- **Summary**: Cell individualization has a vital role in digital pathology image analysis. Deep Learning is considered as an efficient tool for instance segmentation tasks, including cell individualization. However, the precision of the Deep Learning model relies on massive unbiased dataset and manual pixel-level annotations, which is labor intensive. Moreover, most applications of Deep Learning have been developed for processing oncological data. To overcome these challenges, i) we established a pipeline to synthesize pixel-level labels with only point annotations provided; ii) we tested an ensemble Deep Learning algorithm to perform cell individualization on neurological data. Results suggest that the proposed method successfully segments neuronal cells in both object-level and pixel-level, with an average detection accuracy of 0.93.



### Towards Live Video Analytics with On-Drone Deeper-yet-Compatible Compression
- **Arxiv ID**: http://arxiv.org/abs/2111.06263v1
- **DOI**: None
- **Categories**: **cs.NI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06263v1)
- **Published**: 2021-11-10 16:53:06+00:00
- **Updated**: 2021-11-10 16:53:06+00:00
- **Authors**: Junpeng Guo, Chunyi Peng
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present DCC(Deeper-yet-Compatible Compression), one enabling technique for real-time drone-sourced edge-assisted video analytics built on top of the existing codec. DCC tackles an important technical problem to compress streamed video from the drone to the edge without scarifying accuracy and timeliness of video analytical tasks performed at the edge. DCC is inspired by the fact that not every bit in streamed video is equally valuable to video analytics, which opens new compression room over the conventional analytics-oblivious video codec technology. We exploit drone-specific context and intermediate hints from object detection to pursue adaptive fidelity needed to retain analytical quality. We have prototyped DCC in one showcase application of vehicle detection and validated its efficiency in representative scenarios. DCC has reduced transmission volume by 9.5-fold over the baseline approach and 19-683% over the state-of-the-art with comparable detection accuracy.



### SwAMP: Swapped Assignment of Multi-Modal Pairs for Cross-Modal Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2111.05814v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05814v2)
- **Published**: 2021-11-10 17:17:09+00:00
- **Updated**: 2022-10-11 19:23:06+00:00
- **Authors**: Minyoung Kim
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the cross-modal retrieval problem, where learning is only supervised by relevant multi-modal pairs in the data. Although the contrastive learning is the most popular approach for this task, it makes potentially wrong assumption that the instances in different pairs are automatically irrelevant. To address the issue, we propose a novel loss function that is based on self-labeling of the unknown semantic classes. Specifically, we aim to predict class labels of the data instances in each modality, and assign those labels to the corresponding instances in the other modality (i.e., swapping the pseudo labels). With these swapped labels, we learn the data embedding for each modality using the supervised cross-entropy loss. This way, cross-modal instances from different pairs that are semantically related can be aligned to each other by the class predictor. We tested our approach on several real-world cross-modal retrieval problems, including text-based video retrieval, sketch-based image retrieval, and image-text retrieval. For all these tasks our method achieves significant performance improvement over the contrastive learning.



### Palette: Image-to-Image Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2111.05826v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.05826v2)
- **Published**: 2021-11-10 17:49:29+00:00
- **Updated**: 2022-05-03 22:24:28+00:00
- **Authors**: Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, Mohammad Norouzi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io for an overview of the results.



### Structure from Silence: Learning Scene Structure from Ambient Sound
- **Arxiv ID**: http://arxiv.org/abs/2111.05846v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.MM, cs.RO, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2111.05846v1)
- **Published**: 2021-11-10 18:55:14+00:00
- **Updated**: 2021-11-10 18:55:14+00:00
- **Authors**: Ziyang Chen, Xixi Hu, Andrew Owens
- **Comment**: Accepted to CoRL 2021 (Oral Presentation)
- **Journal**: None
- **Summary**: From whirling ceiling fans to ticking clocks, the sounds that we hear subtly vary as we move through a scene. We ask whether these ambient sounds convey information about 3D scene structure and, if so, whether they provide a useful learning signal for multimodal models. To study this, we collect a dataset of paired audio and RGB-D recordings from a variety of quiet indoor scenes. We then train models that estimate the distance to nearby walls, given only audio as input. We also use these recordings to learn multimodal representations through self-supervision, by training a network to associate images with their corresponding sounds. These results suggest that ambient sound conveys a surprising amount of information about scene structure, and that it is a useful signal for learning multimodal features.



### Advances in Neural Rendering
- **Arxiv ID**: http://arxiv.org/abs/2111.05849v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05849v2)
- **Published**: 2021-11-10 18:57:01+00:00
- **Updated**: 2022-03-30 16:39:58+00:00
- **Authors**: Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Yifan Wang, Christoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, Tomas Simon, Christian Theobalt, Matthias Niessner, Jonathan T. Barron, Gordon Wetzstein, Michael Zollhoefer, Vladislav Golyanik
- **Comment**: 33 pages, 14 figures, 5 tables; State of the Art Report at
  EUROGRAPHICS 2022
- **Journal**: None
- **Summary**: Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline. This state-of-the-art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non-rigidly deforming objects...



### A Histopathology Study Comparing Contrastive Semi-Supervised and Fully Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.05882v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05882v1)
- **Published**: 2021-11-10 19:04:08+00:00
- **Updated**: 2021-11-10 19:04:08+00:00
- **Authors**: Lantian Zhang, Mohamed Amgad, Lee A. D. Cooper
- **Comment**: 7 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: Data labeling is often the most challenging task when developing computational pathology models. Pathologist participation is necessary to generate accurate labels, and the limitations on pathologist time and demand for large, labeled datasets has led to research in areas including weakly supervised learning using patient-level labels, machine assisted annotation and active learning. In this paper we explore self-supervised learning to reduce labeling burdens in computational pathology. We explore this in the context of classification of breast cancer tissue using the Barlow Twins approach, and we compare self-supervision with alternatives like pre-trained networks in low-data scenarios. For the task explored in this paper, we find that ImageNet pre-trained networks largely outperform the self-supervised representations obtained using Barlow Twins.



### Multimodal End-to-End Group Emotion Recognition using Cross-Modal Attention
- **Arxiv ID**: http://arxiv.org/abs/2111.05890v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2111.05890v1)
- **Published**: 2021-11-10 19:19:26+00:00
- **Updated**: 2021-11-10 19:19:26+00:00
- **Authors**: Lev Evtodienko
- **Comment**: None
- **Journal**: None
- **Summary**: Classifying group-level emotions is a challenging task due to complexity of video, in which not only visual, but also audio information should be taken into consideration. Existing works on multimodal emotion recognition are using bulky approach, where pretrained neural networks are used as a feature extractors and then extracted features are being fused. However, this approach does not consider attributes of multimodal data and feature extractors cannot be fine-tuned for specific task which can be disadvantageous for overall model accuracy. To this end, our impact is twofold: (i) we train model end-to-end, which allows early layers of neural network to be adapted with taking into account later, fusion layers, of two modalities; (ii) all layers of our model was fine-tuned for downstream task of emotion recognition, so there were no need to train neural networks from scratch. Our model achieves best validation accuracy of 60.37% which is approximately 8.5% higher, than VGAF dataset baseline and is competitive with existing works, audio and video modalities.



### An Extensive Study of User Identification via Eye Movements across Multiple Datasets
- **Arxiv ID**: http://arxiv.org/abs/2111.05901v1
- **DOI**: 10.1016/j.image.2022.116804
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.05901v1)
- **Published**: 2021-11-10 19:43:48+00:00
- **Updated**: 2021-11-10 19:43:48+00:00
- **Authors**: Sahar Mahdie Klim Al Zaidawi, Martin H. U. Prinzler, Jonas Lührs, Sebastian Maneth
- **Comment**: 11 pages, 5 figures, submitted to Signal Processing: Image
  Communication
- **Journal**: Signal Processing: Image Communication, 2022
- **Summary**: Several studies have reported that biometric identification based on eye movement characteristics can be used for authentication. This paper provides an extensive study of user identification via eye movements across multiple datasets based on an improved version of method originally proposed by George and Routray. We analyzed our method with respect to several factors that affect the identification accuracy, such as the type of stimulus, the IVT parameters (used for segmenting the trajectories into fixation and saccades), adding new features such as higher-order derivatives of eye movements, the inclusion of blink information, template aging, age and gender.We find that three methods namely selecting optimal IVT parameters, adding higher-order derivatives features and including an additional blink classifier have a positive impact on the identification accuracy. The improvements range from a few percentage points, up to an impressive 9 % increase on one of the datasets.



### Dance In the Wild: Monocular Human Animation with Neural Dynamic Appearance Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2111.05916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05916v1)
- **Published**: 2021-11-10 20:18:57+00:00
- **Updated**: 2021-11-10 20:18:57+00:00
- **Authors**: Tuanfeng Y. Wang, Duygu Ceylan, Krishna Kumar Singh, Niloy J. Mitra
- **Comment**: None
- **Journal**: None
- **Summary**: Synthesizing dynamic appearances of humans in motion plays a central role in applications such as AR/VR and video editing. While many recent methods have been proposed to tackle this problem, handling loose garments with complex textures and high dynamic motion still remains challenging. In this paper, we propose a video based appearance synthesis method that tackles such challenges and demonstrates high quality results for in-the-wild videos that have not been shown before. Specifically, we adopt a StyleGAN based architecture to the task of person specific video based motion retargeting. We introduce a novel motion signature that is used to modulate the generator weights to capture dynamic appearance changes as well as regularizing the single frame based pose estimates to improve temporal coherency. We evaluate our method on a set of challenging videos and show that our approach achieves state-of-the art performance both qualitatively and quantitatively.



### A soft thumb-sized vision-based sensor with accurate all-round force perception
- **Arxiv ID**: http://arxiv.org/abs/2111.05934v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2111.05934v1)
- **Published**: 2021-11-10 20:46:23+00:00
- **Updated**: 2021-11-10 20:46:23+00:00
- **Authors**: Huanbo Sun, Katherine J. Kuchenbecker, Georg Martius
- **Comment**: 1 table, 5 figures, 24 pages for the main manuscript. 5 tables, 12
  figures, 27 pages for the supplementary material. 8 supplementary videos
- **Journal**: None
- **Summary**: Vision-based haptic sensors have emerged as a promising approach to robotic touch due to affordable high-resolution cameras and successful computer-vision techniques. However, their physical design and the information they provide do not yet meet the requirements of real applications. We present a robust, soft, low-cost, vision-based, thumb-sized 3D haptic sensor named Insight: it continually provides a directional force-distribution map over its entire conical sensing surface. Constructed around an internal monocular camera, the sensor has only a single layer of elastomer over-molded on a stiff frame to guarantee sensitivity, robustness, and soft contact. Furthermore, Insight is the first system to combine photometric stereo and structured light using a collimator to detect the 3D deformation of its easily replaceable flexible outer shell. The force information is inferred by a deep neural network that maps images to the spatial distribution of 3D contact force (normal and shear). Insight has an overall spatial resolution of 0.4 mm, force magnitude accuracy around 0.03 N, and force direction accuracy around 5 degrees over a range of 0.03--2 N for numerous distinct contacts with varying contact area. The presented hardware and software design concepts can be transferred to a wide variety of robot parts.



### Self-Supervised Multi-Object Tracking with Cross-Input Consistency
- **Arxiv ID**: http://arxiv.org/abs/2111.05943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05943v1)
- **Published**: 2021-11-10 21:00:34+00:00
- **Updated**: 2021-11-10 21:00:34+00:00
- **Authors**: Favyen Bastani, Songtao He, Sam Madden
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: In this paper, we propose a self-supervised learning procedure for training a robust multi-object tracking (MOT) model given only unlabeled video. While several self-supervisory learning signals have been proposed in prior work on single-object tracking, such as color propagation and cycle-consistency, these signals cannot be directly applied for training RNN models, which are needed to achieve accurate MOT: they yield degenerate models that, for instance, always match new detections to tracks with the closest initial detections. We propose a novel self-supervisory signal that we call cross-input consistency: we construct two distinct inputs for the same sequence of video, by hiding different information about the sequence in each input. We then compute tracks in that sequence by applying an RNN model independently on each input, and train the model to produce consistent tracks across the two inputs. We evaluate our unsupervised method on MOT17 and KITTI -- remarkably, we find that, despite training only on unlabeled video, our unsupervised approach outperforms four supervised methods published in the last 1--2 years, including Tracktor++, FAMNet, GSM, and mmMOT.



### Self-Compression in Bayesian Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2111.05950v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05950v1)
- **Published**: 2021-11-10 21:19:40+00:00
- **Updated**: 2021-11-10 21:19:40+00:00
- **Authors**: Giuseppina Carannante, Dimah Dera, Ghulam Rasool, Nidhal C. Bouaynaya
- **Comment**: submitted to 2020 IEEE International Workshop on Machine Learning for
  Signal Processing
- **Journal**: None
- **Summary**: Machine learning models have achieved human-level performance on various tasks. This success comes at a high cost of computation and storage overhead, which makes machine learning algorithms difficult to deploy on edge devices. Typically, one has to partially sacrifice accuracy in favor of an increased performance quantified in terms of reduced memory usage and energy consumption. Current methods compress the networks by reducing the precision of the parameters or by eliminating redundant ones. In this paper, we propose a new insight into network compression through the Bayesian framework. We show that Bayesian neural networks automatically discover redundancy in model parameters, thus enabling self-compression, which is linked to the propagation of uncertainty through the layers of the network. Our experimental results show that the network architecture can be successfully compressed by deleting parameters identified by the network itself while retaining the same level of accuracy.



### Robust Learning via Ensemble Density Propagation in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2111.05953v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, math.PR
- **Links**: [PDF](http://arxiv.org/pdf/2111.05953v1)
- **Published**: 2021-11-10 21:26:08+00:00
- **Updated**: 2021-11-10 21:26:08+00:00
- **Authors**: Giuseppina Carannante, Dimah Dera, Ghulam Rasool, Nidhal C. Bouaynaya, Lyudmila Mihaylova
- **Comment**: submitted to 2020 IEEE International Workshop on Machine Learning for
  Signal Processing
- **Journal**: None
- **Summary**: Learning in uncertain, noisy, or adversarial environments is a challenging task for deep neural networks (DNNs). We propose a new theoretically grounded and efficient approach for robust learning that builds upon Bayesian estimation and Variational Inference. We formulate the problem of density propagation through layers of a DNN and solve it using an Ensemble Density Propagation (EnDP) scheme. The EnDP approach allows us to propagate moments of the variational probability distribution across the layers of a Bayesian DNN, enabling the estimation of the mean and covariance of the predictive distribution at the output of the model. Our experiments using MNIST and CIFAR-10 datasets show a significant improvement in the robustness of the trained models to random noise and adversarial attacks.



### Keys to Accurate Feature Extraction Using Residual Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2111.05955v4
- **DOI**: 10.1088/2634-4386/ac8bef
- **Categories**: **cs.LG**, cs.CV, I.2.6; I.2.10; I.4.8; I.5.2; D.2.13
- **Links**: [PDF](http://arxiv.org/pdf/2111.05955v4)
- **Published**: 2021-11-10 21:29:19+00:00
- **Updated**: 2022-06-23 15:01:24+00:00
- **Authors**: Alex Vicente-Sola, Davide L. Manna, Paul Kirkland, Gaetano Di Caterina, Trevor Bihl
- **Comment**: 17 pages, 6 figures, 17 tables
- **Journal**: None
- **Summary**: Spiking neural networks (SNNs) have become an interesting alternative to conventional artificial neural networks (ANN) thanks to their temporal processing capabilities and energy efficient implementations in neuromorphic hardware. However the challenges involved in training SNNs have limited their performance in terms of accuracy and thus their applications. Improving learning algorithms and neural architectures for a more accurate feature extraction is therefore one of the current priorities in SNN research. In this paper we present a study on the key components of modern spiking architectures. We design a spiking version of the successful residual network architecture and provide an in-depth study on the possible implementations of spiking residual connections. This study shows how, depending on the use case, the optimal residual connection implementation may vary. Additionally, we empirically compare different techniques in image classification datasets taken from the best performing networks. Our results provide a state of the art guide to SNN design, which allows to make informed choices when trying to build the optimal visual feature extractor. Finally, our network outperforms previous SNN architectures in CIFAR-10 (94.14%) and CIFAR-100 (74.65%) datasets and matches the state of the art in DVS-CIFAR10 (72.98%), with less parameters than the previous state of the art and without the need for ANN-SNN conversion. Code available at https://github.com/VicenteAlex/Spiking_ResNet



### Feature Generation for Long-tail Classification
- **Arxiv ID**: http://arxiv.org/abs/2111.05956v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.05956v1)
- **Published**: 2021-11-10 21:34:29+00:00
- **Updated**: 2021-11-10 21:34:29+00:00
- **Authors**: Rahul Vigneswaran, Marc T. Law, Vineeth N. Balasubramanian, Makarand Tapaswi
- **Comment**: Accepted at ICVGIP'21. Code available at
  https://github.com/rahulvigneswaran/TailCalibX
- **Journal**: None
- **Summary**: The visual world naturally exhibits an imbalance in the number of object or scene instances resulting in a \emph{long-tailed distribution}. This imbalance poses significant challenges for classification models based on deep learning. Oversampling instances of the tail classes attempts to solve this imbalance. However, the limited visual diversity results in a network with poor representation ability. A simple counter to this is decoupling the representation and classifier networks and using oversampling only to train the classifier. In this paper, instead of repeatedly re-sampling the same image (and thereby features), we explore a direction that attempts to generate meaningful features by estimating the tail category's distribution. Inspired by ideas from recent work on few-shot learning, we create calibrated distributions to sample additional features that are subsequently used to train the classifier. Through several experiments on the CIFAR-100-LT (long-tail) dataset with varying imbalance factors and on mini-ImageNet-LT (long-tail), we show the efficacy of our approach and establish a new state-of-the-art. We also present a qualitative analysis of generated features using t-SNE visualizations and analyze the nearest neighbors used to calibrate the tail class distributions. Our code is available at https://github.com/rahulvigneswaran/TailCalibX.



### Advancing Brain Metastases Detection in T1-Weighted Contrast-Enhanced 3D MRI using Noisy Student-based Training
- **Arxiv ID**: http://arxiv.org/abs/2111.05959v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.05959v2)
- **Published**: 2021-11-10 21:44:57+00:00
- **Updated**: 2021-11-19 16:10:30+00:00
- **Authors**: Engin Dikici, Xuan V. Nguyen, Matthew Bigelow, John. L. Ryu, Luciano M. Prevedello
- **Comment**: None
- **Journal**: None
- **Summary**: The detection of brain metastases (BM) in their early stages could have a positive impact on the outcome of cancer patients. We previously developed a framework for detecting small BM (with diameters of less than 15mm) in T1-weighted Contrast-Enhanced 3D Magnetic Resonance images (T1c) to assist medical experts in this time-sensitive and high-stakes task. The framework utilizes a dedicated convolutional neural network (CNN) trained using labeled T1c data, where the ground truth BM segmentations were provided by a radiologist. This study aims to advance the framework with a noisy student-based self-training strategy to make use of a large corpus of unlabeled T1c data (i.e., data without BM segmentations or detections). Accordingly, the work (1) describes the student and teacher CNN architectures, (2) presents data and model noising mechanisms, and (3) introduces a novel pseudo-labeling strategy factoring in the learned BM detection sensitivity of the framework. Finally, it describes a semi-supervised learning strategy utilizing these components. We performed the validation using 217 labeled and 1247 unlabeled T1c exams via 2-fold cross-validation. The framework utilizing only the labeled exams produced 9.23 false positives for 90% BM detection sensitivity; whereas, the framework using the introduced learning strategy led to ~9% reduction in false detections (i.e., 8.44) for the same sensitivity level. Furthermore, while experiments utilizing 75% and 50% of the labeled datasets resulted in algorithm performance degradation (12.19 and 13.89 false positives respectively), the impact was less pronounced with the noisy student-based training strategy (10.79 and 12.37 false positives respectively).



### Fast Computation of Hahn Polynomials for High Order Moments
- **Arxiv ID**: http://arxiv.org/abs/2111.07749v1
- **DOI**: 10.1109/ACCESS.2022.3170893
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07749v1)
- **Published**: 2021-11-10 22:28:21+00:00
- **Updated**: 2021-11-10 22:28:21+00:00
- **Authors**: Basheera M. Mahmmod, Sadiq H. Abdulhussain, Tomáš Suk, Abir Hussain
- **Comment**: This first of this paper is submitted to Pattern Recognition at June
  15, 2020 This version of the manuscript is submitted to PLOS ONE
- **Journal**: IEEE Access, 10 (2022) 48719-48732
- **Summary**: Discrete Hahn polynomials (DHPs) and their moments are considered to be one of the efficient orthogonal moments and they are applied in various scientific areas such as image processing and feature extraction. Commonly, DHPs are used as object representation; however, they suffer from the problem of numerical instability when the moment order becomes large. In this paper, an efficient method for computation of Hahn orthogonal basis is proposed and applied to high orders. This paper developed a new mathematical model for computing the initial value of the DHP and for different values of DHP parameters ($\alpha$ and $\beta$). In addition, the proposed method is composed of two recurrence algorithms with an adaptive threshold to stabilize the generation of the DHP coefficients. It is compared with state-of-the-art algorithms in terms of computational cost and the maximum size that can be correctly generated. The experimental results show that the proposed algorithm performs better in both parameters for wide ranges of parameter values of ($\alpha$ and $\beta$) and polynomial sizes.



### SUPER-Net: Trustworthy Medical Image Segmentation with Uncertainty Propagation in Encoder-Decoder Networks
- **Arxiv ID**: http://arxiv.org/abs/2111.05978v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.05978v3)
- **Published**: 2021-11-10 22:46:05+00:00
- **Updated**: 2023-01-21 17:26:07+00:00
- **Authors**: Giuseppina Carannante, Dimah Dera, Nidhal C. Bouaynaya, Hassan M. Fathallah-Shaykh, Ghulam Rasool
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Learning (DL) holds great promise in reshaping the healthcare industry owing to its precision, efficiency, and objectivity. However, the brittleness of DL models to noisy and out-of-distribution inputs is ailing their deployment in the clinic. Most models produce point estimates without further information about model uncertainty or confidence. This paper introduces a new Bayesian DL framework for uncertainty quantification in segmentation neural networks: SUPER-Net: trustworthy medical image Segmentation with Uncertainty Propagation in Encoder-decodeR Networks. SUPER-Net analytically propagates, using Taylor series approximations, the first two moments (mean and covariance) of the posterior distribution of the model parameters across the nonlinear layers. In particular, SUPER-Net simultaneously learns the mean and covariance without expensive post-hoc Monte Carlo sampling or model ensembling. The output consists of two simultaneous maps: the segmented image and its pixelwise uncertainty map, which corresponds to the covariance matrix of the predictive distribution. We conduct an extensive evaluation of SUPER-Net on medical image segmentation of Magnetic Resonances Imaging and Computed Tomography scans under various noisy and adversarial conditions. Our experiments on multiple benchmark datasets demonstrate that SUPER-Net is more robust to noise and adversarial attacks than state-of-the-art segmentation models. Moreover, the uncertainty map of the proposed SUPER-Net associates low confidence (or equivalently high uncertainty) to patches in the test input images that are corrupted with noise, artifacts, or adversarial attacks. Perhaps more importantly, the model exhibits the ability of self-assessment of its segmentation decisions, notably when making erroneous predictions due to noise or adversarial examples.



### A Multi-attribute Controllable Generative Model for Histopathology Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2111.06398v1
- **DOI**: 10.1007/978-3-030-87237-3_59
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06398v1)
- **Published**: 2021-11-10 22:48:55+00:00
- **Updated**: 2021-11-10 22:48:55+00:00
- **Authors**: Jiarong Ye, Yuan Xue, Peter Liu, Richard Zaino, Keith Cheng, Xiaolei Huang
- **Comment**: MICCAI 2021
- **Journal**: None
- **Summary**: Generative models have been applied in the medical imaging domain for various image recognition and synthesis tasks. However, a more controllable and interpretable image synthesis model is still lacking yet necessary for important applications such as assisting in medical training. In this work, we leverage the efficient self-attention and contrastive learning modules and build upon state-of-the-art generative adversarial networks (GANs) to achieve an attribute-aware image synthesis model, termed AttributeGAN, which can generate high-quality histopathology images based on multi-attribute inputs. In comparison to existing single-attribute conditional generative models, our proposed model better reflects input attributes and enables smoother interpolation among attribute values. We conduct experiments on a histopathology dataset containing stained H&E images of urothelial carcinoma and demonstrate the effectiveness of our proposed model via comprehensive quantitative and qualitative comparisons with state-of-the-art models as well as different variants of our model. Code is available at https://github.com/karenyyy/MICCAI2021AttributeGAN.



### Self-Supervised Real-time Video Stabilization
- **Arxiv ID**: http://arxiv.org/abs/2111.05980v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05980v1)
- **Published**: 2021-11-10 22:49:56+00:00
- **Updated**: 2021-11-10 22:49:56+00:00
- **Authors**: Jinsoo Choi, Jaesik Park, In So Kweon
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: Videos are a popular media form, where online video streaming has recently gathered much popularity. In this work, we propose a novel method of real-time video stabilization - transforming a shaky video to a stabilized video as if it were stabilized via gimbals in real-time. Our framework is trainable in a self-supervised manner, which does not require data captured with special hardware setups (i.e., two cameras on a stereo rig or additional motion sensors). Our framework consists of a transformation estimator between given frames for global stability adjustments, followed by scene parallax reduction module via spatially smoothed optical flow for further stability. Then, a margin inpainting module fills in the missing margin regions created during stabilization to reduce the amount of post-cropping. These sequential steps reduce distortion and margin cropping to a minimum while enhancing stability. Hence, our approach outperforms state-of-the-art real-time video stabilization methods as well as offline methods that require camera trajectory optimization. Our method procedure takes approximately 24.3 ms yielding 41 fps regardless of resolution (e.g., 480p or 1080p).



### Improving Structured Text Recognition with Regular Expression Biasing
- **Arxiv ID**: http://arxiv.org/abs/2111.06738v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.06738v1)
- **Published**: 2021-11-10 23:12:05+00:00
- **Updated**: 2021-11-10 23:12:05+00:00
- **Authors**: Baoguang Shi, Wenfeng Cheng, Yijuan Lu, Cha Zhang, Dinei Florencio
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of recognizing structured text, i.e. text that follows certain formats, and propose to improve the recognition accuracy of structured text by specifying regular expressions (regexes) for biasing. A biased recognizer recognizes text that matches the specified regexes with significantly improved accuracy, at the cost of a generally small degradation on other text. The biasing is realized by modeling regexes as a Weighted Finite-State Transducer (WFST) and injecting it into the decoder via dynamic replacement. A single hyperparameter controls the biasing strength. The method is useful for recognizing text lines with known formats or containing words from a domain vocabulary. Examples include driver license numbers, drug names in prescriptions, etc. We demonstrate the efficacy of regex biasing on datasets of printed and handwritten structured text and measures its side effects.



### Selective Synthetic Augmentation with HistoGAN for Improved Histopathology Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2111.06399v1
- **DOI**: 10.1016/j.media.2020.101816
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.06399v1)
- **Published**: 2021-11-10 23:25:39+00:00
- **Updated**: 2021-11-10 23:25:39+00:00
- **Authors**: Yuan Xue, Jiarong Ye, Qianying Zhou, Rodney Long, Sameer Antani, Zhiyun Xue, Carl Cornwell, Richard Zaino, Keith Cheng, Xiaolei Huang
- **Comment**: Elsevier Medical Image Analysis Best Paper Award runner up. arXiv
  admin note: substantial text overlap with arXiv:1912.03837
- **Journal**: Medical Image Analysis 67 (2021): 101816
- **Summary**: Histopathological analysis is the present gold standard for precancerous lesion diagnosis. The goal of automated histopathological classification from digital images requires supervised training, which requires a large number of expert annotations that can be expensive and time-consuming to collect. Meanwhile, accurate classification of image patches cropped from whole-slide images is essential for standard sliding window based histopathology slide classification methods. To mitigate these issues, we propose a carefully designed conditional GAN model, namely HistoGAN, for synthesizing realistic histopathology image patches conditioned on class labels. We also investigate a novel synthetic augmentation framework that selectively adds new synthetic image patches generated by our proposed HistoGAN, rather than expanding directly the training set with synthetic images. By selecting synthetic images based on the confidence of their assigned labels and their feature similarity to real labeled images, our framework provides quality assurance to synthetic augmentation. Our models are evaluated on two datasets: a cervical histopathology image dataset with limited annotations, and another dataset of lymph node histopathology images with metastatic cancer. Here, we show that leveraging HistoGAN generated images with selective augmentation results in significant and consistent improvements of classification performance (6.7% and 2.8% higher accuracy, respectively) for cervical histopathology and metastatic cancer datasets.



### Traffic4cast -- Large-scale Traffic Prediction using 3DResNet and Sparse-UNet
- **Arxiv ID**: http://arxiv.org/abs/2111.05990v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.05990v1)
- **Published**: 2021-11-10 23:40:52+00:00
- **Updated**: 2021-11-10 23:40:52+00:00
- **Authors**: Bo Wang, Reza Mohajerpoor, Chen Cai, Inhi Kim, Hai L. Vu
- **Comment**: None
- **Journal**: None
- **Summary**: The IARAI competition Traffic4cast 2021 aims to predict short-term city-wide high-resolution traffic states given the static and dynamic traffic information obtained previously. The aim is to build a machine learning model for predicting the normalized average traffic speed and flow of the subregions of multiple large-scale cities using historical data points. The model is supposed to be generic, in a way that it can be applied to new cities. By considering spatiotemporal feature learning and modeling efficiency, we explore 3DResNet and Sparse-UNet approaches for the tasks in this competition. The 3DResNet based models use 3D convolution to learn the spatiotemporal features and apply sequential convolutional layers to enhance the temporal relationship of the outputs. The Sparse-UNet model uses sparse convolutions as the backbone for spatiotemporal feature learning. Since the latter algorithm mainly focuses on non-zero data points of the inputs, it dramatically reduces the computation time, while maintaining a competitive accuracy. Our results show that both of the proposed models achieve much better performance than the baseline algorithms. The codes and pretrained models are available at https://github.com/resuly/Traffic4Cast-2021.



