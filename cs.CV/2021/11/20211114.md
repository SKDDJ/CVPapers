# Arxiv Papers in cs.CV on 2021-11-14
### Local Multi-Head Channel Self-Attention for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.07224v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.07224v2)
- **Published**: 2021-11-14 02:38:01+00:00
- **Updated**: 2021-11-18 17:09:03+00:00
- **Authors**: Roberto Pecoraro, Valerio Basile, Viviana Bono, Sara Gallo
- **Comment**: https://github.com/Bodhis4ttva/LHC_Net
- **Journal**: None
- **Summary**: Since the Transformer architecture was introduced in 2017 there has been many attempts to bring the self-attention paradigm in the field of computer vision. In this paper we propose a novel self-attention module that can be easily integrated in virtually every convolutional neural network and that is specifically designed for computer vision, the LHC: Local (multi) Head Channel (self-attention). LHC is based on two main ideas: first, we think that in computer vision the best way to leverage the self-attention paradigm is the channel-wise application instead of the more explored spatial attention and that convolution will not be replaced by attention modules like recurrent networks were in NLP; second, a local approach has the potential to better overcome the limitations of convolution than global attention. With LHC-Net we managed to achieve a new state of the art in the famous FER2013 dataset with a significantly lower complexity and impact on the "host" architecture in terms of computational cost when compared with the previous SOTA.



### Curriculum Learning for Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2111.07228v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07228v1)
- **Published**: 2021-11-14 03:02:07+00:00
- **Updated**: 2021-11-14 03:02:07+00:00
- **Authors**: Jiwen Zhang, Zhongyu Wei, Jianqing Fan, Jiajie Peng
- **Comment**: Accepted by NeurIPS 2021
- **Journal**: None
- **Summary**: Vision-and-Language Navigation (VLN) is a task where an agent navigates in an embodied indoor environment under human instructions. Previous works ignore the distribution of sample difficulty and we argue that this potentially degrade their agent performance. To tackle this issue, we propose a novel curriculum-based training paradigm for VLN tasks that can balance human prior knowledge and agent learning progress about training samples. We develop the principle of curriculum design and re-arrange the benchmark Room-to-Room (R2R) dataset to make it suitable for curriculum training. Experiments show that our method is model-agnostic and can significantly improve the performance, the generalizability, and the training efficiency of current state-of-the-art navigation agents without increasing model complexity.



### Visual design intuition: Predicting dynamic properties of beams from raw cross-section images
- **Arxiv ID**: http://arxiv.org/abs/2111.09701v1
- **DOI**: 10.1098/rsif.2021.0571
- **Categories**: **eess.IV**, cs.AI, cs.CV, 74H15, 49S99, 74S05, 74S60, 74S99, 65Z99, J.6; J.7
- **Links**: [PDF](http://arxiv.org/pdf/2111.09701v1)
- **Published**: 2021-11-14 03:10:15+00:00
- **Updated**: 2021-11-14 03:10:15+00:00
- **Authors**: Philippe M. Wyder, Hod Lipson
- **Comment**: Accepted for publication in Journal Of The Royal Society Interface
- **Journal**: None
- **Summary**: In this work we aim to mimic the human ability to acquire the intuition to estimate the performance of a design from visual inspection and experience alone. We study the ability of convolutional neural networks to predict static and dynamic properties of cantilever beams directly from their raw cross-section images. Using pixels as the only input, the resulting models learn to predict beam properties such as volume maximum deflection and eigenfrequencies with 4.54% and 1.43% Mean Average Percentage Error (MAPE) respectively, compared to the Finite Element Analysis (FEA) approach. Training these models doesn't require prior knowledge of theory or relevant geometric properties, but rather relies solely on simulated or empirical data, thereby making predictions based on "experience" as opposed to theoretical knowledge. Since this approach is over 1000 times faster than FEA, it can be adopted to create surrogate models that could speed up the preliminary optimization studies where numerous consecutive evaluations of similar geometries are required. We suggest that this modeling approach would aid in addressing challenging optimization problems involving complex structures and physical phenomena for which theoretical models are unavailable.



### Robust and Accurate Object Detection via Self-Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2111.07239v1
- **DOI**: 10.1109/ICIP46576.2022.9898031
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07239v1)
- **Published**: 2021-11-14 04:40:15+00:00
- **Updated**: 2021-11-14 04:40:15+00:00
- **Authors**: Weipeng Xu, Pengzhi Chu, Renhao Xie, Xiongziyan Xiao, Hongcheng Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection has achieved promising performance on clean datasets, but how to achieve better tradeoff between the adversarial robustness and clean precision is still under-explored. Adversarial training is the mainstream method to improve robustness, but most of the works will sacrifice clean precision to gain robustness than standard training. In this paper, we propose Unified Decoupled Feature Alignment (UDFA), a novel fine-tuning paradigm which achieves better performance than existing methods, by fully exploring the combination between self-knowledge distillation and adversarial training for object detection. We first use decoupled fore/back-ground features to construct self-knowledge distillation branch between clean feature representation from pretrained detector (served as teacher) and adversarial feature representation from student detector. Then we explore the self-knowledge distillation from a new angle by decoupling original branch into a self-supervised learning branch and a new self-knowledge distillation branch. With extensive experiments on the PASCAL-VOC and MS-COCO benchmarks, the evaluation results show that UDFA can surpass the standard training and state-of-the-art adversarial training methods for object detection. For example, compared with teacher detector, our approach on GFLV2 with ResNet-50 improves clean precision by 2.2 AP on PASCAL-VOC; compared with SOTA adversarial training methods, our approach improves clean precision by 1.6 AP, while improving adversarial robustness by 0.5 AP. Our code will be available at https://github.com/grispeut/udfa.



### Background-Aware 3D Point Cloud Segmentationwith Dynamic Point Feature Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2111.07248v1
- **DOI**: 10.1109/TGRS.2022.3168555
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07248v1)
- **Published**: 2021-11-14 05:46:05+00:00
- **Updated**: 2021-11-14 05:46:05+00:00
- **Authors**: Jiajing Chen, Burak Kakillioglu, Senem Velipasalar
- **Comment**: None
- **Journal**: None
- **Summary**: With the proliferation of Lidar sensors and 3D vision cameras, 3D point cloud analysis has attracted significant attention in recent years. After the success of the pioneer work PointNet, deep learning-based methods have been increasingly applied to various tasks, including 3D point cloud segmentation and 3D object classification. In this paper, we propose a novel 3D point cloud learning network, referred to as Dynamic Point Feature Aggregation Network (DPFA-Net), by selectively performing the neighborhood feature aggregation with dynamic pooling and an attention mechanism. DPFA-Net has two variants for semantic segmentation and classification of 3D point clouds. As the core module of the DPFA-Net, we propose a Feature Aggregation layer, in which features of the dynamic neighborhood of each point are aggregated via a self-attention mechanism. In contrast to other segmentation models, which aggregate features from fixed neighborhoods, our approach can aggregate features from different neighbors in different layers providing a more selective and broader view to the query points, and focusing more on the relevant features in a local neighborhood. In addition, to further improve the performance of the proposed semantic segmentation model, we present two novel approaches, namely Two-Stage BF-Net and BF-Regularization to exploit the background-foreground information. Experimental results show that the proposed DPFA-Net achieves the state-of-the-art overall accuracy score for semantic segmentation on the S3DIS dataset, and provides a consistently satisfactory performance across different tasks of semantic segmentation, part segmentation, and 3D object classification. It is also computationally more efficient compared to other methods.



### Moment Transform-Based Compressive Sensing in Image Processing
- **Arxiv ID**: http://arxiv.org/abs/2111.07254v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2111.07254v1)
- **Published**: 2021-11-14 06:19:45+00:00
- **Updated**: 2021-11-14 06:19:45+00:00
- **Authors**: T. Kalampokas, G. A. Papakostas
- **Comment**: 12 pages, 13 figures
- **Journal**: None
- **Summary**: Over the last decades, images have become an important source of information in many domains, thus their high quality has become necessary to acquire better information. One of the important issues that arise is image denoising, which means recovering a signal from inaccurately and/or partially measured samples. This interpretation is highly correlated to the compressive sensing theory, which is a revolutionary technology and implies that if a signal is sparse then the original signal can be obtained from a few measured values, which are much less, than the ones suggested by other used theories like Shannon's sampling theories. A strong factor in Compressive Sensing (CS) theory to achieve the sparsest solution and the noise removal from the corrupted image is the selection of the basis dictionary. In this paper, Discrete Cosine Transform (DCT) and moment transform (Tchebichef, Krawtchouk) are compared in order to achieve image denoising of Gaussian additive white noise based on compressive sensing and sparse approximation theory. The experimental results revealed that the basis dictionaries constructed by the moment transform perform competitively to the traditional DCT. The latter transform shows a higher PSNR of 30.82 dB and the same 0.91 SSIM value as the Tchebichef transform. Moreover, from the sparsity point of view, Krawtchouk moments provide approximately 20-30% more sparse results than DCT.



### Sign Language Translation with Hierarchical Spatio-TemporalGraph Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2111.07258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07258v1)
- **Published**: 2021-11-14 07:02:28+00:00
- **Updated**: 2021-11-14 07:02:28+00:00
- **Authors**: Jichao Kan, Kun Hu, Markus Hagenbuchner, Ah Chung Tsoi, Mohammed Bennamounm, Zhiyong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Sign language translation (SLT), which generates text in a spoken language from visual content in a sign language, is important to assist the hard-of-hearing community for their communications. Inspired by neural machine translation (NMT), most existing SLT studies adopted a general sequence to sequence learning strategy. However, SLT is significantly different from general NMT tasks since sign languages convey messages through multiple visual-manual aspects. Therefore, in this paper, these unique characteristics of sign languages are formulated as hierarchical spatio-temporal graph representations, including high-level and fine-level graphs of which a vertex characterizes a specified body part and an edge represents their interactions. Particularly, high-level graphs represent the patterns in the regions such as hands and face, and fine-level graphs consider the joints of hands and landmarks of facial regions. To learn these graph patterns, a novel deep learning architecture, namely hierarchical spatio-temporal graph neural network (HST-GNN), is proposed. Graph convolutions and graph self-attentions with neighborhood context are proposed to characterize both the local and the global graph properties. Experimental results on benchmark datasets demonstrated the effectiveness of the proposed method.



### Auxiliary Loss Reweighting for Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2111.07279v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07279v4)
- **Published**: 2021-11-14 08:45:49+00:00
- **Updated**: 2022-04-22 12:35:16+00:00
- **Authors**: Siqi Hui, Sanping Zhou, Ye Deng, Wenli Huang, Jinjun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Image Inpainting is a task that aims to fill in missing regions of corrupted images with plausible contents. Recent inpainting methods have introduced perceptual and style losses as auxiliary losses to guide the learning of inpainting generators. Perceptual and style losses help improve the perceptual quality of inpainted results by supervising deep features of generated regions. However, two challenges have emerged with the usage of auxiliary losses: (i) the time-consuming grid search is required to decide weights for perceptual and style losses to properly perform, and (ii) loss terms with different auxiliary abilities are equally weighted by perceptual and style losses. To meet these two challenges, we propose a novel framework that independently weights auxiliary loss terms and adaptively adjusts their weights within a single training process, without a time-consuming grid search. Specifically, to release the auxiliary potential of perceptual and style losses, we propose two auxiliary losses, Tunable Perceptual Loss (TPL) and Tunable Style Loss (TSL) by using different tunable weights to consider the contributions of different loss terms. TPL and TSL are supersets of perceptual and style losses and release the auxiliary potential of standard perceptual and style losses. We further propose the Auxiliary Weights Adaptation (AWA) algorithm, which efficiently reweights TPL and TSL in a single training process. AWA is based on the principle that the best auxiliary weights would lead to the most improvement in inpainting performance. We conduct experiments on publically available datasets and find that our framework helps current SOTA methods achieve better results.



### Deep Joint Demosaicing and High Dynamic Range Imaging within a Single Shot
- **Arxiv ID**: http://arxiv.org/abs/2111.07281v1
- **DOI**: 10.1109/TCSVT.2021.3129691
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07281v1)
- **Published**: 2021-11-14 08:54:26+00:00
- **Updated**: 2021-11-14 08:54:26+00:00
- **Authors**: Yilun Xu, Ziyang Liu, Xingming Wu, Weihai Chen, Changyun Wen, Zhengguo Li
- **Comment**: 15 pages, 17 figures
- **Journal**: None
- **Summary**: Spatially varying exposure (SVE) is a promising choice for high-dynamic-range (HDR) imaging (HDRI). The SVE-based HDRI, which is called single-shot HDRI, is an efficient solution to avoid ghosting artifacts. However, it is very challenging to restore a full-resolution HDR image from a real-world image with SVE because: a) only one-third of pixels with varying exposures are captured by camera in a Bayer pattern, b) some of the captured pixels are over- and under-exposed. For the former challenge, a spatially varying convolution (SVC) is designed to process the Bayer images carried with varying exposures. For the latter one, an exposure-guidance method is proposed against the interference from over- and under-exposed pixels. Finally, a joint demosaicing and HDRI deep learning framework is formalized to include the two novel components and to realize an end-to-end single-shot HDRI. Experiments indicate that the proposed end-to-end framework avoids the problem of cumulative errors and surpasses the related state-of-the-art methods.



### Novel Intensity Mapping Functions: Weighted Histogram Averaging
- **Arxiv ID**: http://arxiv.org/abs/2111.07283v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07283v5)
- **Published**: 2021-11-14 09:10:32+00:00
- **Updated**: 2023-01-27 08:08:52+00:00
- **Authors**: Yilun Xu, Zhengguo Li, Weihai Chen, Changyun Wen
- **Comment**: None
- **Journal**: None
- **Summary**: It is challenging to align the brightness distribution of the images with different exposures due to possible color distortion and loss of details in the brightest and darkest regions of input images. In this paper, a novel intensity mapping algorithm is first proposed by introducing a new concept of weighted histogram averaging (WHA). The proposed WHA algorithm leverages the correspondence between the histogram bins of two images which are built up by using the non-decreasing property of the intensity mapping functions (IMFs). Extensive experiments indicate that the proposed WHA algorithm significantly surpasses the related state-of-the-art intensity mapping methods.



### Towards Privacy-Preserving Affect Recognition: A Two-Level Deep Learning Architecture
- **Arxiv ID**: http://arxiv.org/abs/2111.07344v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.07344v1)
- **Published**: 2021-11-14 13:52:57+00:00
- **Updated**: 2021-11-14 13:52:57+00:00
- **Authors**: Jimiama M. Mase, Natalie Leesakul, Fan Yang, Grazziela P. Figueredo, Mercedes Torres Torres
- **Comment**: 8 pages, 6 figures, 4 tables
- **Journal**: None
- **Summary**: Automatically understanding and recognising human affective states using images and computer vision can improve human-computer and human-robot interaction. However, privacy has become an issue of great concern, as the identities of people used to train affective models can be exposed in the process. For instance, malicious individuals could exploit images from users and assume their identities. In addition, affect recognition using images can lead to discriminatory and algorithmic bias, as certain information such as race, gender, and age could be assumed based on facial features. Possible solutions to protect the privacy of users and avoid misuse of their identities are to: (1) extract anonymised facial features, namely action units (AU) from a database of images, discard the images and use AUs for processing and training, and (2) federated learning (FL) i.e. process raw images in users' local machines (local processing) and send the locally trained models to the main processing machine for aggregation (central processing). In this paper, we propose a two-level deep learning architecture for affect recognition that uses AUs in level 1 and FL in level 2 to protect users' identities. The architecture consists of recurrent neural networks to capture the temporal relationships amongst the features and predict valence and arousal affective states. In our experiments, we evaluate the performance of our privacy-preserving architecture using different variations of recurrent neural networks on RECOLA, a comprehensive multimodal affective database. Our results show state-of-the-art performance of $0.426$ for valence and $0.401$ for arousal using the Concordance Correlation Coefficient evaluation metric, demonstrating the feasibility of developing models for affect recognition that are both accurate and ensure privacy.



### A Study on the Efficient Product Search Service for the Damaged Image Information
- **Arxiv ID**: http://arxiv.org/abs/2111.07346v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07346v1)
- **Published**: 2021-11-14 13:58:48+00:00
- **Updated**: 2021-11-14 13:58:48+00:00
- **Authors**: Yonghyun Kim
- **Comment**: 5 pages, 8 figures
- **Journal**: None
- **Summary**: With the development of Information and Communication Technologies and the dissemination of smartphones, especially now that image search is possible through the internet, e-commerce markets are more activating purchasing services for a wide variety of products. However, it often happens that the image of the desired product is impaired and that the search engine does not recognize it properly. The idea of this study is to help search for products through image restoration using an image pre-processing and image inpainting algorithm for damaged images. It helps users easily purchase the items they want by providing a more accurate image search system. Besides, the system has the advantage of efficiently showing information by category, so that enables efficient sales of registered information.



### Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models
- **Arxiv ID**: http://arxiv.org/abs/2111.07355v3
- **DOI**: 10.3390/s22031285
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.07355v3)
- **Published**: 2021-11-14 14:21:24+00:00
- **Updated**: 2022-02-08 18:03:37+00:00
- **Authors**: Fırat Hardalaç, Fatih Uysal, Ozan Peker, Murat Çiçeklidağ, Tolga Tolunay, Nil Tokgöz, Uğurhan Kutbay, Boran Demirciler, Fatih Mert
- **Comment**: This paper is accepted at Sensors, MDPI, 2022, 22, 1285. Section:
  "Sensing and Imaging"
- **Journal**: Sensors, MDPI, 2022, 22, 1285. Section: "Sensing and Imaging"
- **Summary**: Hospitals, especially their emergency services, receive a high number of wrist fracture cases. For correct diagnosis and proper treatment of these, images obtained from various medical equipment must be viewed by physicians, along with the patients medical records and physical examination. The aim of this study is to perform fracture detection by use of deep learning on wrist Xray images to support physicians in the diagnosis of these fractures, particularly in the emergency services. Using SABL, RegNet, RetinaNet, PAA, Libra R_CNN, FSAF, Faster R_CNN, Dynamic R_CNN and DCN deep learning based object detection models with various backbones, 20 different fracture detection procedures were performed on Gazi University Hospitals dataset of wrist Xray images. To further improve these procedures, five different ensemble models were developed and then used to reform an ensemble model to develop a unique detection model, wrist fracture detection_combo (WFD_C). From 26 different models for fracture detection, the highest detection result obtained was 0.8639 average precision (AP50) in the WFD-C model. Huawei Turkey R&D Center supports this study within the scope of the ongoing cooperation project coded 071813 between Gazi University, Huawei and Medskor. Code is available at https://github.com/fatihuysal88/wrist-d



### A layer-stress learning framework universally augments deep neural network tasks
- **Arxiv ID**: http://arxiv.org/abs/2111.08597v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08597v1)
- **Published**: 2021-11-14 15:14:13+00:00
- **Updated**: 2021-11-14 15:14:13+00:00
- **Authors**: Shihao Shao, Yong Liu, Qinghua Cui
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNN) such as Multi-Layer Perception (MLP) and Convolutional Neural Networks (CNN) represent one of the most established deep learning algorithms. Given the tremendous effects of the number of hidden layers on network architecture and performance, it is very important to choose the number of hidden layers but still a serious challenge. More importantly, the current network architectures can only process the information from the last layer of the feature extractor, which greatly limited us to further improve its performance. Here we presented a layer-stress deep learning framework (x-NN) which implemented automatic and wise depth decision on shallow or deep feature map in a deep network through firstly designing enough number of layers and then trading off them by Multi-Head Attention Block. The x-NN can make use of features from various depth layers through attention allocation and then help to make final decision as well. As a result, x-NN showed outstanding prediction ability in the Alzheimer's Disease Classification Technique Challenge PRCV 2021, in which it won the top laurel and outperformed all other AI models. Moreover, the performance of x-NN was verified by one more AD neuroimaging dataset and other AI tasks.



### Estimation of Acetabular Version from Anteroposterior Pelvic Radiograph Employing Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.07369v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.07369v1)
- **Published**: 2021-11-14 15:34:26+00:00
- **Updated**: 2021-11-14 15:34:26+00:00
- **Authors**: Ata Jodeiri, Hadi Seyedarabi, Fatemeh Shahbazi, Seyed Mohammad Mahdi Hashemi, Seyyedhossein Shafiei
- **Comment**: 12 pages, 8 figures
- **Journal**: None
- **Summary**: Background and Objective: The Acetabular version, an essential factor in total hip arthroplasty, is measured by CT scan as the gold standard. The dose of radiation and expensiveness of CT make anterior-posterior pelvic radiograph an appropriate alternative procedure. In this study, we applied a deep learning approach on anteroposterior pelvic X-rays to measure anatomical version, eliminating the necessity of using Computed tomography scan. Methods: The right and left acetabular version angles of the hips of 300 patients are computed using their CT images. The proposed deep learning model, Attention on Pretrained-VGG16 for Bone Age, is applied to the AP images of the included population. The age and gender of these people are added as two other inputs to the last fully connected layer of attention mechanism. As the output, the angles of both hips are predicted. Results: The angles of hips computed on CT increase as people get older with the mean values of 16.54 and 16.11 (right and left angles) for men and 20.61 and 19.55 for women in our dataset. The predicted errors in the estimation of right and left angles using the proposed method of deep learning are in the accurate region of error (<=3 degrees) which shows the ability of the proposed method in measuring anatomical version based on AP images. Conclusion: The suggested algorithm, applying pre-trained vgg16 on the AP images of the pelvis of patients followed by an attention model considering age and gender of patients, can assess version accurately using only AP radiographs while obviating the need for CT scan. The applied technique of estimation of anatomical acetabular version based on AP pelvic images using DL approaches, to the best of authors' knowledge, has not been published yet.



### Co-segmentation Inspired Attention Module for Video-based Computer Vision Tasks
- **Arxiv ID**: http://arxiv.org/abs/2111.07370v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07370v3)
- **Published**: 2021-11-14 15:35:37+00:00
- **Updated**: 2022-08-01 22:14:39+00:00
- **Authors**: Arulkumar Subramaniam, Jayesh Vaidya, Muhammed Abdul Majeed Ameen, Athira Nambiar, Anurag Mittal
- **Comment**: 26 pages, 14 figures, Preprint submitted to CVIU journal
- **Journal**: None
- **Summary**: Video-based computer vision tasks can benefit from estimation of the salient regions and interactions between those regions. Traditionally, this has been done by identifying the object regions in the images by utilizing pre-trained models to perform object detection, object segmentation and/or object pose estimation. Although using pre-trained models is a viable approach, it has several limitations in the need for an exhaustive annotation of object categories, a possible domain gap between datasets, and a bias that is typically present in pre-trained models. In this work, we propose to utilize the common rationale that a sequence of video frames capture a set of common objects and interactions between them, thus a notion of co-segmentation between the video frame features may equip the model with the ability to automatically focus on task-specific salient regions and improve the underlying task's performance in an end-to-end manner. In this regard, we propose a generic module called ``Co-Segmentation inspired Attention Module'' (COSAM) that can be plugged in to any CNN model to promote the notion of co-segmentation based attention among a sequence of video frame features. We show the application of COSAM in three video-based tasks namely: 1) Video-based person re-ID, 2) Video captioning, & 3) Video action classification and demonstrate that COSAM is able to capture the task-specific salient regions in video frames, thus leading to notable performance improvements along with interpretable attention maps for a variety of video-based vision tasks, with possible application to other video-based vision tasks as well.



### Sparse Steerable Convolutions: An Efficient Learning of SE(3)-Equivariant Features for Estimation and Tracking of Object Poses in 3D Space
- **Arxiv ID**: http://arxiv.org/abs/2111.07383v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07383v1)
- **Published**: 2021-11-14 16:28:30+00:00
- **Updated**: 2021-11-14 16:28:30+00:00
- **Authors**: Jiehong Lin, Hongyang Li, Ke Chen, Jiangbo Lu, Kui Jia
- **Comment**: Accepted by NeurIPS 2021
- **Journal**: None
- **Summary**: As a basic component of SE(3)-equivariant deep feature learning, steerable convolution has recently demonstrated its advantages for 3D semantic analysis. The advantages are, however, brought by expensive computations on dense, volumetric data, which prevent its practical use for efficient processing of 3D data that are inherently sparse. In this paper, we propose a novel design of Sparse Steerable Convolution (SS-Conv) to address the shortcoming; SS-Conv greatly accelerates steerable convolution with sparse tensors, while strictly preserving the property of SE(3)-equivariance. Based on SS-Conv, we propose a general pipeline for precise estimation of object poses, wherein a key design is a Feature-Steering module that takes the full advantage of SE(3)-equivariance and is able to conduct an efficient pose refinement. To verify our designs, we conduct thorough experiments on three tasks of 3D object semantic analysis, including instance-level 6D pose estimation, category-level 6D pose and size estimation, and category-level 6D pose tracking. Our proposed pipeline based on SS-Conv outperforms existing methods on almost all the metrics evaluated by the three tasks. Ablation studies also show the superiority of our SS-Conv over alternative convolutions in terms of both accuracy and efficiency. Our code is released publicly at https://github.com/Gorilla-Lab-SCUT/SS-Conv.



### TANDEM: Tracking and Dense Mapping in Real-time using Deep Multi-view Stereo
- **Arxiv ID**: http://arxiv.org/abs/2111.07418v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2111.07418v1)
- **Published**: 2021-11-14 19:01:02+00:00
- **Updated**: 2021-11-14 19:01:02+00:00
- **Authors**: Lukas Koestler, Nan Yang, Niclas Zeller, Daniel Cremers
- **Comment**: CoRL 2021. The manuscript contains the main paper and the
  supplementary materials. Project page: https://go.vision.in.tum.de/tandem
- **Journal**: None
- **Summary**: In this paper, we present TANDEM a real-time monocular tracking and dense mapping framework. For pose estimation, TANDEM performs photometric bundle adjustment based on a sliding window of keyframes. To increase the robustness, we propose a novel tracking front-end that performs dense direct image alignment using depth maps rendered from a global model that is built incrementally from dense depth predictions. To predict the dense depth maps, we propose Cascade View-Aggregation MVSNet (CVA-MVSNet) that utilizes the entire active keyframe window by hierarchically constructing 3D cost volumes with adaptive view aggregation to balance the different stereo baselines between the keyframes. Finally, the predicted depth maps are fused into a consistent global map represented as a truncated signed distance function (TSDF) voxel grid. Our experimental results show that TANDEM outperforms other state-of-the-art traditional and learning-based monocular visual odometry (VO) methods in terms of camera tracking. Moreover, TANDEM shows state-of-the-art real-time 3D reconstruction performance.



### Generating Band-Limited Adversarial Surfaces Using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2111.07424v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.07424v1)
- **Published**: 2021-11-14 19:16:05+00:00
- **Updated**: 2021-11-14 19:16:05+00:00
- **Authors**: Roee Ben-Shlomo, Yevgeniy Men, Ido Imanuel
- **Comment**: None
- **Journal**: None
- **Summary**: Generating adversarial examples is the art of creating a noise that is added to an input signal of a classifying neural network, and thus changing the network's classification, while keeping the noise as tenuous as possible. While the subject is well-researched in the 2D regime, it is lagging behind in the 3D regime, i.e. attacking a classifying network that works on 3D point-clouds or meshes and, for example, classifies the pose of people's 3D scans. As of now, the vast majority of papers that describe adversarial attacks in this regime work by methods of optimization. In this technical report we suggest a neural network that generates the attacks. This network utilizes PointNet's architecture with some alterations. While the previous articles on which we based our work on have to optimize each shape separately, i.e. tailor an attack from scratch for each individual input without any learning, we attempt to create a unified model that can deduce the needed adversarial example with a single forward run.



### Unsupervised Action Localization Crop in Video Retargeting for 3D ConvNets
- **Arxiv ID**: http://arxiv.org/abs/2111.07426v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2111.07426v2)
- **Published**: 2021-11-14 19:27:13+00:00
- **Updated**: 2021-11-22 09:17:07+00:00
- **Authors**: Prithwish Jana, Swarnabja Bhaumik, Partha Pratim Mohanta
- **Comment**: Accepted for Publication in Proceedings of 2021 IEEE Region 10
  Conference (TENCON), 7-10 December 2021, Auckland, New Zealand
- **Journal**: None
- **Summary**: Untrimmed videos on social media or those captured by robots and surveillance cameras are of varied aspect ratios. However, 3D CNNs usually require as input a square-shaped video, whose spatial dimension is smaller than the original. Random- or center-cropping may leave out the video's subject altogether. To address this, we propose an unsupervised video cropping approach by shaping this as a retargeting and video-to-video synthesis problem. The synthesized video maintains a 1:1 aspect ratio, is smaller in size and is targeted at video-subject(s) throughout the entire duration. First, action localization is performed on each frame by identifying patches with homogeneous motion patterns. Thus, a single salient patch is pinpointed per frame. But to avoid viewpoint jitters and flickering, any inter-frame scale or position changes among the patches should be performed gradually over time. This issue is addressed with a polyBezier fitting in 3D space that passes through some chosen pivot timestamps and whose shape is influenced by the in-between control timestamps. To corroborate the effectiveness of the proposed method, we evaluate the video classification task by comparing our dynamic cropping technique with random cropping on three benchmark datasets, viz. UCF-101, HMDB-51 and ActivityNet v1.3. The clip and top-1 accuracy for video classification after our cropping, outperform 3D CNN performances for same-sized random-crop inputs, also surpassing some larger random-crop sizes.



### A Comparative Study of Fingerprint Image-Quality Estimation Methods
- **Arxiv ID**: http://arxiv.org/abs/2111.07432v1
- **DOI**: 10.1109/TIFS.2007.908228
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.07432v1)
- **Published**: 2021-11-14 19:53:12+00:00
- **Updated**: 2021-11-14 19:53:12+00:00
- **Authors**: Fernando Alonso-Fernandez, Julian Fierrez, Javier Ortega-Garcia, Joaquin Gonzalez-Rodriguez, Hartwig Fronthaler, Klaus Kollreider, Josef Bigun
- **Comment**: Published at IEEE Transactions on Information Forensics and Security
- **Journal**: None
- **Summary**: One of the open issues in fingerprint verification is the lack of robustness against image-quality degradation. Poor-quality images result in spurious and missing features, thus degrading the performance of the overall system. Therefore, it is important for a fingerprint recognition system to estimate the quality and validity of the captured fingerprint images. In this work, we review existing approaches for fingerprint image-quality estimation, including the rationale behind the published measures and visual examples showing their behavior under different quality conditions. We have also tested a selection of fingerprint image-quality estimation algorithms. For the experiments, we employ the BioSec multimodal baseline corpus, which includes 19200 fingerprint images from 200 individuals acquired in two sessions with three different sensors. The behavior of the selected quality measures is compared, showing high correlation between them in most cases. The effect of low-quality samples in the verification performance is also studied for a widely available minutiae-based fingerprint matching system.



### Impact of Benign Modifications on Discriminative Performance of Deepfake Detectors
- **Arxiv ID**: http://arxiv.org/abs/2111.07468v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.07468v1)
- **Published**: 2021-11-14 22:50:39+00:00
- **Updated**: 2021-11-14 22:50:39+00:00
- **Authors**: Yuhang Lu, Evgeniy Upenik, Touradj Ebrahimi
- **Comment**: None
- **Journal**: None
- **Summary**: Deepfakes are becoming increasingly popular in both good faith applications such as in entertainment and maliciously intended manipulations such as in image and video forgery. Primarily motivated by the latter, a large number of deepfake detectors have been proposed recently in order to identify such content. While the performance of such detectors still need further improvements, they are often assessed in simple if not trivial scenarios. In particular, the impact of benign processing operations such as transcoding, denoising, resizing and enhancement are not sufficiently studied. This paper proposes a more rigorous and systematic framework to assess the performance of deepfake detectors in more realistic situations. It quantitatively measures how and to which extent each benign processing approach impacts a state-of-the-art deepfake detection method. By illustrating it in a popular deepfake detector, our benchmark proposes a framework to assess robustness of detectors and provides valuable insights to design more efficient deepfake detectors.



