# Arxiv Papers in cs.CV on 2021-11-17
### Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities
- **Arxiv ID**: http://arxiv.org/abs/2111.08851v5
- **DOI**: 10.1007/s10044-023-01181-9
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2111.08851v5)
- **Published**: 2021-11-17 01:10:23+00:00
- **Updated**: 2023-06-01 00:40:22+00:00
- **Authors**: Xintong Shi, Wenzhi Cao, Sebastian Raschka
- **Comment**: Accepted for publication in Pattern Analysis and Applications
- **Journal**: Pattern Analysis and Applications 2023
- **Summary**: In recent times, deep neural networks achieved outstanding predictive performance on various classification and pattern recognition tasks. However, many real-world prediction problems have ordinal response variables, and this ordering information is ignored by conventional classification losses such as the multi-category cross-entropy. Ordinal regression methods for deep neural networks address this. One such method is the CORAL method, which is based on an earlier binary label extension framework and achieves rank consistency among its output layer tasks by imposing a weight-sharing constraint. However, while earlier experiments showed that CORAL's rank consistency is beneficial for performance, it is limited by a weight-sharing constraint in a neural network's fully connected output layer, which may restrict the expressiveness and capacity of a network trained using CORAL. We propose a new method for rank-consistent ordinal regression without this limitation. Our rank-consistent ordinal regression framework (CORN) achieves rank consistency by a novel training scheme. This training scheme uses conditional training sets to obtain the unconditional rank probabilities through applying the chain rule for conditional probability distributions. Experiments on various datasets demonstrate the efficacy of the proposed method to utilize the ordinal target information, and the absence of the weight-sharing restriction improves the performance substantially compared to the CORAL reference approach. Additionally, the suggested CORN method is not tied to any specific architecture and can be utilized with any deep neural network classifier to train it for ordinal regression tasks.



### TYolov5: A Temporal Yolov5 Detector Based on Quasi-Recurrent Neural Networks for Real-Time Handgun Detection in Video
- **Arxiv ID**: http://arxiv.org/abs/2111.08867v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08867v2)
- **Published**: 2021-11-17 02:35:29+00:00
- **Updated**: 2021-11-19 03:49:25+00:00
- **Authors**: Mario Alberto Duran-Vega, Miguel Gonzalez-Mendoza, Leonardo Chang, Cuauhtemoc Daniel Suarez-Ramirez
- **Comment**: None
- **Journal**: None
- **Summary**: Timely handgun detection is a crucial problem to improve public safety; nevertheless, the effectiveness of many surveillance systems still depends of finite human attention. Much of the previous research on handgun detection is based on static image detectors, leaving aside valuable temporal information that could be used to improve object detection in videos. To improve the performance of surveillance systems, a real-time temporal handgun detection system should be built. Using Temporal Yolov5, an architecture based on Quasi-Recurrent Neural Networks, temporal information is extracted from video to improve the results of handgun detection. Moreover, two publicly available datasets are proposed, labeled with hands, guns, and phones. One containing 2199 static images to train static detectors, and another with 5960 frames of videos to train temporal modules. Additionally, we explore two temporal data augmentation techniques based on Mosaic and Mixup. The resulting systems are three temporal architectures: one focused in reducing inference with a mAP$_{50:95}$ of 55.9, another in having a good balance between inference and accuracy with a mAP$_{50:95}$ of 59, and a last one specialized in accuracy with a mAP$_{50:95}$ of 60.2. Temporal Yolov5 achieves real-time detection in the small and medium architectures. Moreover, it takes advantage of temporal features contained in videos to perform better than Yolov5 in our temporal dataset, making TYolov5 suitable for real-world applications. The source code is publicly available at https://github.com/MarioDuran/TYolov5.



### Enhanced Correlation Matching based Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2111.08869v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08869v1)
- **Published**: 2021-11-17 02:43:45+00:00
- **Updated**: 2021-11-17 02:43:45+00:00
- **Authors**: Sungho Lee, Narae Choi, Woong Il Choi
- **Comment**: Accepted to WACV 2022, equal contribution from first two authors
- **Journal**: None
- **Summary**: We propose a novel DNN based framework called the Enhanced Correlation Matching based Video Frame Interpolation Network to support high resolution like 4K, which has a large scale of motion and occlusion. Considering the extensibility of the network model according to resolution, the proposed scheme employs the recurrent pyramid architecture that shares the parameters among each pyramid layer for optical flow estimation. In the proposed flow estimation, the optical flows are recursively refined by tracing the location with maximum correlation. The forward warping based correlation matching enables to improve the accuracy of flow update by excluding incorrectly warped features around the occlusion area. Based on the final bi-directional flows, the intermediate frame at arbitrary temporal position is synthesized using the warping and blending network and it is further improved by refinement network. Experiment results demonstrate that the proposed scheme outperforms the previous works at 4K video data and low-resolution benchmark datasets as well in terms of objective and subjective quality with the smallest number of model parameters.



### TorchGeo: Deep Learning With Geospatial Data
- **Arxiv ID**: http://arxiv.org/abs/2111.08872v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.08872v4)
- **Published**: 2021-11-17 02:47:33+00:00
- **Updated**: 2022-09-17 18:12:21+00:00
- **Authors**: Adam J. Stewart, Caleb Robinson, Isaac A. Corley, Anthony Ortiz, Juan M. Lavista Ferres, Arindam Banerjee
- **Comment**: None
- **Journal**: None
- **Summary**: Remotely sensed geospatial data are critical for applications including precision agriculture, urban planning, disaster monitoring and response, and climate change research, among others. Deep learning methods are particularly promising for modeling many remote sensing tasks given the success of deep neural networks in similar computer vision tasks and the sheer volume of remotely sensed imagery available. However, the variance in data collection methods and handling of geospatial metadata make the application of deep learning methodology to remotely sensed data nontrivial. For example, satellite imagery often includes additional spectral bands beyond red, green, and blue and must be joined to other geospatial data sources that can have differing coordinate systems, bounds, and resolutions. To help realize the potential of deep learning for remote sensing applications, we introduce TorchGeo, a Python library for integrating geospatial data into the PyTorch deep learning ecosystem. TorchGeo provides data loaders for a variety of benchmark datasets, composable datasets for generic geospatial data sources, samplers for geospatial data, and transforms that work with multispectral imagery. TorchGeo is also the first library to provide pre-trained models for multispectral satellite imagery (e.g., models that use all bands from the Sentinel-2 satellites), allowing for advances in transfer learning on downstream remote sensing tasks with limited labeled data. We use TorchGeo to create reproducible benchmark results on existing datasets and benchmark our proposed method for preprocessing geospatial imagery on the fly. TorchGeo is open source and available on GitHub: https://github.com/microsoft/torchgeo.



### SAPNet: Segmentation-Aware Progressive Network for Perceptual Contrastive Deraining
- **Arxiv ID**: http://arxiv.org/abs/2111.08892v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08892v2)
- **Published**: 2021-11-17 03:57:11+00:00
- **Updated**: 2021-11-26 16:13:57+00:00
- **Authors**: Shen Zheng, Changjie Lu, Yuxiong Wu, Gaurav Gupta
- **Comment**: Accepted by WACV2022
- **Journal**: None
- **Summary**: Deep learning algorithms have recently achieved promising deraining performances on both the natural and synthetic rainy datasets. As an essential low-level pre-processing stage, a deraining network should clear the rain streaks and preserve the fine semantic details. However, most existing methods only consider low-level image restoration. That limits their performances at high-level tasks requiring precise semantic information. To address this issue, in this paper, we present a segmentation-aware progressive network (SAPNet) based upon contrastive learning for single image deraining. We start our method with a lightweight derain network formed with progressive dilated units (PDU). The PDU can significantly expand the receptive field and characterize multi-scale rain streaks without the heavy computation on multi-scale images. A fundamental aspect of this work is an unsupervised background segmentation (UBS) network initialized with ImageNet and Gaussian weights. The UBS can faithfully preserve an image's semantic information and improve the generalization ability to unseen photos. Furthermore, we introduce a perceptual contrastive loss (PCL) and a learned perceptual image similarity loss (LPISL) to regulate model learning. By exploiting the rainy image and groundtruth as the negative and the positive sample in the VGG-16 latent space, we bridge the fine semantic details between the derained image and the groundtruth in a fully constrained manner. Comprehensive experiments on synthetic and real-world rainy images show our model surpasses top-performing methods and aids object detection and semantic segmentation with considerable efficacy. A Pytorch Implementation is available at https://github.com/ShenZheng2000/SAPNet-for-image-deraining.



### Achieving Human Parity on Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2111.08896v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08896v3)
- **Published**: 2021-11-17 04:25:11+00:00
- **Updated**: 2021-11-19 07:22:08+00:00
- **Authors**: Ming Yan, Haiyang Xu, Chenliang Li, Junfeng Tian, Bin Bi, Wei Wang, Weihua Chen, Xianzhe Xu, Fan Wang, Zheng Cao, Zhicheng Zhang, Qiyu Zhang, Ji Zhang, Songfang Huang, Fei Huang, Luo Si, Rong Jin
- **Comment**: None
- **Journal**: None
- **Summary**: The Visual Question Answering (VQA) task utilizes both visual image and language analysis to answer a textual question with respect to an image. It has been a popular research topic with an increasing number of real-world applications in the last decade. This paper describes our recent research of AliceMind-MMU (ALIbaba's Collection of Encoder-decoders from Machine IntelligeNce lab of Damo academy - MultiMedia Understanding) that obtains similar or even slightly better results than human being does on VQA. This is achieved by systematically improving the VQA pipeline including: (1) pre-training with comprehensive visual and textual feature representation; (2) effective cross-modal interaction with learning to attend; and (3) A novel knowledge mining framework with specialized expert modules for the complex VQA task. Treating different types of visual questions with corresponding expertise needed plays an important role in boosting the performance of our VQA architecture up to the human level. An extensive set of experiments and analysis are conducted to demonstrate the effectiveness of the new research work.



### ARKitScenes: A Diverse Real-World Dataset For 3D Indoor Scene Understanding Using Mobile RGB-D Data
- **Arxiv ID**: http://arxiv.org/abs/2111.08897v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.08897v3)
- **Published**: 2021-11-17 04:27:01+00:00
- **Updated**: 2022-01-12 08:19:29+00:00
- **Authors**: Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, Elad Shulman
- **Comment**: None
- **Journal**: None
- **Summary**: Scene understanding is an active research area. Commercial depth sensors, such as Kinect, have enabled the release of several RGB-D datasets over the past few years which spawned novel methods in 3D scene understanding. More recently with the launch of the LiDAR sensor in Apple's iPads and iPhones, high quality RGB-D data is accessible to millions of people on a device they commonly use. This opens a whole new era in scene understanding for the Computer Vision community as well as app developers. The fundamental research in scene understanding together with the advances in machine learning can now impact people's everyday experiences. However, transforming these scene understanding methods to real-world experiences requires additional innovation and development. In this paper we introduce ARKitScenes. It is not only the first RGB-D dataset that is captured with a now widely available depth sensor, but to our best knowledge, it also is the largest indoor scene understanding data released. In addition to the raw and processed data from the mobile device, ARKitScenes includes high resolution depth maps captured using a stationary laser scanner, as well as manually labeled 3D oriented bounding boxes for a large taxonomy of furniture. We further analyze the usefulness of the data for two downstream tasks: 3D object detection and color-guided depth upsampling. We demonstrate that our dataset can help push the boundaries of existing state-of-the-art methods and it introduces new challenges that better represent real-world scenarios.



### Hierarchical Knowledge Guided Learning for Real-world Retinal Diseases Recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.08913v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08913v2)
- **Published**: 2021-11-17 05:44:39+00:00
- **Updated**: 2023-03-21 06:07:43+00:00
- **Authors**: Lie Ju, Zhen Yu, Lin Wang, Xin Zhao, Xin Wang, Paul Bonnington, Zongyuan Ge
- **Comment**: None
- **Journal**: None
- **Summary**: In the real world, medical datasets often exhibit a long-tailed data distribution (i.e., a few classes occupy the majority of the data, while most classes have only a limited number of samples), which results in a challenging long-tailed learning scenario. Some recently published datasets in ophthalmology AI consist of more than 40 kinds of retinal diseases with complex abnormalities and variable morbidity. Nevertheless, more than 30 conditions are rarely seen in global patient cohorts. From a modeling perspective, most deep learning models trained on these datasets may lack the ability to generalize to rare diseases where only a few available samples are presented for training. In addition, there may be more than one disease for the presence of the retina, resulting in a challenging label co-occurrence scenario, also known as \textit{multi-label}, which can cause problems when some re-sampling strategies are applied during training. To address the above two major challenges, this paper presents a novel method that enables the deep neural network to learn from a long-tailed fundus database for various retinal disease recognition. Firstly, we exploit the prior knowledge in ophthalmology to improve the feature representation using a hierarchy-aware pre-training. Secondly, we adopt an instance-wise class-balanced sampling strategy to address the label co-occurrence issue under the long-tailed medical dataset scenario. Thirdly, we introduce a novel hybrid knowledge distillation to train a less biased representation and classifier. We conducted extensive experiments on four databases, including two public datasets and two in-house databases with more than one million fundus images. The experimental results demonstrate the superiority of our proposed methods with recognition accuracy outperforming the state-of-the-art competitors, especially for these rare diseases.



### Local Texture Estimator for Implicit Representation Function
- **Arxiv ID**: http://arxiv.org/abs/2111.08918v6
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08918v6)
- **Published**: 2021-11-17 06:01:17+00:00
- **Updated**: 2022-03-29 05:46:56+00:00
- **Authors**: Jaewon Lee, Kyong Hwan Jin
- **Comment**: CVPR 2022 camera-ready version (https://ipl.dgist.ac.kr/LTE_cvpr.pdf)
- **Journal**: None
- **Summary**: Recent works with an implicit neural function shed light on representing images in arbitrary resolution. However, a standalone multi-layer perceptron shows limited performance in learning high-frequency components. In this paper, we propose a Local Texture Estimator (LTE), a dominant-frequency estimator for natural images, enabling an implicit function to capture fine details while reconstructing images in a continuous manner. When jointly trained with a deep super-resolution (SR) architecture, LTE is capable of characterizing image textures in 2D Fourier space. We show that an LTE-based neural function achieves favorable performance against existing deep SR methods within an arbitrary-scale factor. Furthermore, we demonstrate that our implementation takes the shortest running time compared to previous works.



### EMScore: Evaluating Video Captioning via Coarse-Grained and Fine-Grained Embedding Matching
- **Arxiv ID**: http://arxiv.org/abs/2111.08919v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08919v2)
- **Published**: 2021-11-17 06:02:43+00:00
- **Updated**: 2022-07-17 04:35:18+00:00
- **Authors**: Yaya Shi, Xu Yang, Haiyang Xu, Chunfeng Yuan, Bing Li, Weiming Hu, Zheng-Jun Zha
- **Comment**: cvpr2022
- **Journal**: None
- **Summary**: Current metrics for video captioning are mostly based on the text-level comparison between reference and candidate captions. However, they have some insuperable drawbacks, e.g., they cannot handle videos without references, and they may result in biased evaluation due to the one-to-many nature of video-to-text and the neglect of visual relevance. From the human evaluator's viewpoint, a high-quality caption should be consistent with the provided video, but not necessarily be similar to the reference in literal or semantics. Inspired by human evaluation, we propose EMScore (Embedding Matching-based score), a novel reference-free metric for video captioning, which directly measures similarity between video and candidate captions. Benefit from the recent development of large-scale pre-training models, we exploit a well pre-trained vision-language model to extract visual and linguistic embeddings for computing EMScore. Specifically, EMScore combines matching scores of both coarse-grained (video and caption) and fine-grained (frames and words) levels, which takes the overall understanding and detailed characteristics of the video into account. Furthermore, considering the potential information gain, EMScore can be flexibly extended to the conditions where human-labeled references are available. Last but not least, we collect VATEX-EVAL and ActivityNet-FOIl datasets to systematically evaluate the existing metrics. VATEX-EVAL experiments demonstrate that EMScore has higher human correlation and lower reference dependency. ActivityNet-FOIL experiment verifies that EMScore can effectively identify "hallucinating" captions. The datasets will be released to facilitate the development of video captioning metrics. The code is available at: https://github.com/ShiYaya/emscore.



### Protection of SVM Model with Secret Key from Unauthorized Access
- **Arxiv ID**: http://arxiv.org/abs/2111.08927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08927v1)
- **Published**: 2021-11-17 06:41:51+00:00
- **Updated**: 2021-11-17 06:41:51+00:00
- **Authors**: Ryota Iijima, AprilPyone MaungMaung, Hitoshi Kiya
- **Comment**: To appear in IWAIT 2022
- **Journal**: None
- **Summary**: In this paper, we propose a block-wise image transformation method with a secret key for support vector machine (SVM) models. Models trained by using transformed images offer a poor performance to unauthorized users without a key, while they can offer a high performance to authorized users with a key. The proposed method is demonstrated to be robust enough against unauthorized access even under the use of kernel functions in a facial recognition experiment.



### Transparent Human Evaluation for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2111.08940v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08940v2)
- **Published**: 2021-11-17 07:09:59+00:00
- **Updated**: 2022-05-18 23:17:18+00:00
- **Authors**: Jungo Kasai, Keisuke Sakaguchi, Lavinia Dunagan, Jacob Morrison, Ronan Le Bras, Yejin Choi, Noah A. Smith
- **Comment**: Proc. of NAACL 2022
- **Journal**: None
- **Summary**: We establish THumB, a rubric-based human evaluation protocol for image captioning models. Our scoring rubrics and their definitions are carefully developed based on machine- and human-generated captions on the MSCOCO dataset. Each caption is evaluated along two main dimensions in a tradeoff (precision and recall) as well as other aspects that measure the text quality (fluency, conciseness, and inclusive language). Our evaluations demonstrate several critical problems of the current evaluation practice. Human-generated captions show substantially higher quality than machine-generated ones, especially in coverage of salient information (i.e., recall), while most automatic metrics say the opposite. Our rubric-based results reveal that CLIPScore, a recent metric that uses image features, better correlates with human judgments than conventional text-only metrics because it is more sensitive to recall. We hope that this work will promote a more transparent evaluation protocol for image captioning and its automatic metrics.



### Tracklet-Switch Adversarial Attack against Pedestrian Multi-Object Tracking Trackers
- **Arxiv ID**: http://arxiv.org/abs/2111.08954v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08954v3)
- **Published**: 2021-11-17 07:53:45+00:00
- **Updated**: 2023-04-04 06:43:35+00:00
- **Authors**: Delv Lin, Qi Chen, Chengyu Zhou, Kun He
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-Object Tracking (MOT) has achieved aggressive progress and derived many excellent deep learning trackers. Meanwhile, most deep learning models are known to be vulnerable to adversarial examples that are crafted with small perturbations but could mislead the model prediction. In this work, we observe that the robustness on the MOT trackers is rarely studied, and it is challenging to attack the MOT system since its mature association algorithms are designed to be robust against errors during the tracking. To this end, we analyze the vulnerability of popular MOT trackers and propose a novel adversarial attack method called Tracklet-Switch (TraSw) against the complete tracking pipeline of MOT. The proposed TraSw can fool the advanced deep pedestrian trackers (i.e., FairMOT and ByteTrack), causing them fail to track the targets in the subsequent frames by perturbing very few frames. Experiments on the MOT-Challenge datasets (i.e., 2DMOT15, MOT17, and MOT20) show that TraSw can achieve an extraordinarily high success attack rate of over 95% by attacking only four frames on average. To our knowledge, this is the first work on the adversarial attack against the pedestrian MOT trackers. Code is available at https://github.com/JHL-HUST/TraSw .



### Compositional Transformers for Scene Generation
- **Arxiv ID**: http://arxiv.org/abs/2111.08960v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.08960v1)
- **Published**: 2021-11-17 08:11:42+00:00
- **Updated**: 2021-11-17 08:11:42+00:00
- **Authors**: Drew A. Hudson, C. Lawrence Zitnick
- **Comment**: Published as a conference paper at NeurIPS 2021
- **Journal**: None
- **Summary**: We introduce the GANformer2 model, an iterative object-oriented transformer, explored for the task of generative modeling. The network incorporates strong and explicit structural priors, to reflect the compositional nature of visual scenes, and synthesizes images through a sequential process. It operates in two stages: a fast and lightweight planning phase, where we draft a high-level scene layout, followed by an attention-based execution phase, where the layout is being refined, evolving into a rich and detailed picture. Our model moves away from conventional black-box GAN architectures that feature a flat and monolithic latent space towards a transparent design that encourages efficiency, controllability and interpretability. We demonstrate GANformer2's strengths and qualities through a careful evaluation over a range of datasets, from multi-object CLEVR scenes to the challenging COCO images, showing it successfully achieves state-of-the-art performance in terms of visual quality, diversity and consistency. Further experiments demonstrate the model's disentanglement and provide a deeper insight into its generative process, as it proceeds step-by-step from a rough initial sketch, to a detailed layout that accounts for objects' depths and dependencies, and up to the final high-resolution depiction of vibrant and intricate real-world scenes. See https://github.com/dorarad/gansformer for model implementation.



### Generating Unrestricted 3D Adversarial Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2111.08973v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08973v2)
- **Published**: 2021-11-17 08:30:18+00:00
- **Updated**: 2021-11-19 04:24:42+00:00
- **Authors**: Xuelong Dai, Yanjie Li, Hua Dai, Bin Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: Utilizing 3D point cloud data has become an urgent need for the deployment of artificial intelligence in many areas like facial recognition and self-driving. However, deep learning for 3D point clouds is still vulnerable to adversarial attacks, e.g., iterative attacks, point transformation attacks, and generative attacks. These attacks need to restrict perturbations of adversarial examples within a strict bound, leading to the unrealistic adversarial 3D point clouds. In this paper, we propose an Adversarial Graph-Convolutional Generative Adversarial Network (AdvGCGAN) to generate visually realistic adversarial 3D point clouds from scratch. Specifically, we use a graph convolutional generator and a discriminator with an auxiliary classifier to generate realistic point clouds, which learn the latent distribution from the real 3D data. The unrestricted adversarial attack loss is incorporated in the special adversarial training of GAN, which enables the generator to generate the adversarial examples to spoof the target network. Compared with the existing state-of-art attack methods, the experiment results demonstrate the effectiveness of our unrestricted adversarial attack methods with a higher attack success rate and visual quality. Additionally, the proposed AdvGCGAN can achieve better performance against defense models and better transferability than existing attack methods with strong camouflage.



### Pedestrian Detection by Exemplar-Guided Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.08974v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08974v3)
- **Published**: 2021-11-17 08:34:40+00:00
- **Updated**: 2022-07-09 07:22:53+00:00
- **Authors**: Zebin Lin, Wenjie Pei, Fanglin Chen, David Zhang, Guangming Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Typical methods for pedestrian detection focus on either tackling mutual occlusions between crowded pedestrians, or dealing with the various scales of pedestrians. Detecting pedestrians with substantial appearance diversities such as different pedestrian silhouettes, different viewpoints or different dressing, remains a crucial challenge. Instead of learning each of these diverse pedestrian appearance features individually as most existing methods do, we propose to perform contrastive learning to guide the feature learning in such a way that the semantic distance between pedestrians with different appearances in the learned feature space is minimized to eliminate the appearance diversities, whilst the distance between pedestrians and background is maximized. To facilitate the efficiency and effectiveness of contrastive learning, we construct an exemplar dictionary with representative pedestrian appearances as prior knowledge to construct effective contrastive training pairs and thus guide contrastive learning. Besides, the constructed exemplar dictionary is further leveraged to evaluate the quality of pedestrian proposals during inference by measuring the semantic distance between the proposal and the exemplar dictionary. Extensive experiments on both daytime and nighttime pedestrian detection validate the effectiveness of the proposed method.



### Nonlinear Intensity Sonar Image Matching based on Deep Convolution Features
- **Arxiv ID**: http://arxiv.org/abs/2111.08994v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08994v3)
- **Published**: 2021-11-17 09:30:43+00:00
- **Updated**: 2021-12-15 02:53:03+00:00
- **Authors**: Xiaoteng Zhou, Changli Yu, Xin Yuan, Yi Wu, Haijun Feng, Citong Luo
- **Comment**: 5 pages, 9 figures. The final manuscript we submitted is a research
  under the original title. Compared with the previous papers, we adopted a
  more novel research method and experimental design
- **Journal**: None
- **Summary**: With the continuous development of underwater vision technology, more and more remote sensing images could be obtained. In the underwater scene, sonar sensors are currently the most effective remote perception devices, and the sonar images captured by them could provide rich environment information. In order to analyze a certain scene, we often need to merge the sonar images from different periods, various sonar frequencies and distinctive viewpoints. However, the above scenes will bring nonlinear intensity differences to the sonar images, which will make traditional matching methods almost ineffective. This paper proposes a non-linear intensity sonar image matching method that combines local feature points and deep convolution features. This method has two key advantages: (i) we generate data samples related to local feature points based on the self-learning idea; (ii) we use the convolutional neural network (CNN) and Siamese network architecture to measure the similarity of the local position in the sonar image pair. Our method encapsulates the feature extraction and feature matching stage in a model, and directly learns the mapping function from image patch pairs to matching labels, and achieves matching tasks in a near-end-to-end manner. Feature matching experiments are carried out on the sonar images acquired by autonomous underwater vehicle (AUV) in the real underwater environment. Experiment results show that our method has better matching effects and strong robustness.



### Probabilistic Spatial Distribution Prior Based Attentional Keypoints Matching Network
- **Arxiv ID**: http://arxiv.org/abs/2111.09006v2
- **DOI**: 10.1109/TCSVT.2021.3068761
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.09006v2)
- **Published**: 2021-11-17 09:52:03+00:00
- **Updated**: 2021-11-23 05:20:11+00:00
- **Authors**: Xiaoming Zhao, Jingmeng Liu, Xingming Wu, Weihai Chen, Fanghong Guo, Zhengguo Li
- **Comment**: None
- **Journal**: None
- **Summary**: Keypoints matching is a pivotal component for many image-relevant applications such as image stitching, visual simultaneous localization and mapping (SLAM), and so on. Both handcrafted-based and recently emerged deep learning-based keypoints matching methods merely rely on keypoints and local features, while losing sight of other available sensors such as inertial measurement unit (IMU) in the above applications. In this paper, we demonstrate that the motion estimation from IMU integration can be used to exploit the spatial distribution prior of keypoints between images. To this end, a probabilistic perspective of attention formulation is proposed to integrate the spatial distribution prior into the attentional graph neural network naturally. With the assistance of spatial distribution prior, the effort of the network for modeling the hidden features can be reduced. Furthermore, we present a projection loss for the proposed keypoints matching network, which gives a smooth edge between matching and un-matching keypoints. Image matching experiments on visual SLAM datasets indicate the effectiveness and efficiency of the presented method.



### Image Super-Resolution Using T-Tetromino Pixels
- **Arxiv ID**: http://arxiv.org/abs/2111.09013v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.09013v2)
- **Published**: 2021-11-17 10:11:03+00:00
- **Updated**: 2023-03-22 19:13:46+00:00
- **Authors**: Simon Grosche, Andy Regensky, Jürgen Seiler, André Kaup
- **Comment**: 10 pages, 9 figures, 4 tables. This work has been submitted to the
  IEEE for possible publication. Copyright may be transferred without notice,
  after which this version may no longer be accessible
- **Journal**: None
- **Summary**: For modern high-resolution imaging sensors, pixel binning is performed in low-lighting conditions and in case high frame rates are required. To recover the original spatial resolution, single-image super-resolution techniques can be applied for upscaling. To achieve a higher image quality after upscaling, we propose a novel binning concept using tetromino-shaped pixels. It is embedded into the field of compressed sensing and the coherence is calculated to motivate the sensor layouts used. Next, we investigate the reconstruction quality using tetromino pixels for the first time in literature. Instead of using different types of tetrominoes as proposed elsewhere, we show that using a small repeating cell consisting of only four T-tetrominoes is sufficient. For reconstruction, we use a locally fully connected reconstruction (LFCR) network as well as two classical reconstruction methods from the field of compressed sensing. Using the LFCR network in combination with the proposed tetromino layout, we achieve superior image quality in terms of PSNR, SSIM, and visually compared to conventional single-image super-resolution using the very deep super-resolution (VDSR) network. For PSNR, a gain of up to \SI[retain-explicit-plus]{+1.92}{dB} is achieved.



### Discriminative Dictionary Learning based on Statistical Methods
- **Arxiv ID**: http://arxiv.org/abs/2111.09027v1
- **DOI**: 10.1016/B978-0-323-91776-6.00004-X
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.09027v1)
- **Published**: 2021-11-17 10:45:10+00:00
- **Updated**: 2021-11-17 10:45:10+00:00
- **Authors**: G. Madhuri, Atul Negi
- **Comment**: None
- **Journal**: Statistical Modeling in Machine Learning Concepts and Applications
  2023, Pages 55-77, Academic Press
- **Summary**: Sparse Representation (SR) of signals or data has a well founded theory with rigorous mathematical error bounds and proofs. SR of a signal is given by superposition of very few columns of a matrix called Dictionary, implicitly reducing dimensionality. Training dictionaries such that they represent each class of signals with minimal loss is called Dictionary Learning (DL). Dictionary learning methods like Method of Optimal Directions (MOD) and K-SVD have been successfully used in reconstruction based applications in image processing like image "denoising", "inpainting" and others. Other dictionary learning algorithms such as Discriminative K-SVD and Label Consistent K-SVD are supervised learning methods based on K-SVD. In our experience, one of the drawbacks of current methods is that the classification performance is not impressive on datasets like Telugu OCR datasets, with large number of classes and high dimensionality. There is scope for improvement in this direction and many researchers have used statistical methods to design dictionaries for classification. This chapter presents a review of statistical techniques and their application to learning discriminative dictionaries. The objective of the methods described here is to improve classification using sparse representation. In this chapter a hybrid approach is described, where sparse coefficients of input data are generated. We use a simple three layer Multi Layer Perceptron with back-propagation training as a classifier with those sparse codes as input. The results are quite comparable with other computation intensive methods.   Keywords: Statistical modeling, Dictionary Learning, Discriminative Dictionary, Sparse representation, Gaussian prior, Cauchy prior, Entropy, Hidden Markov model, Hybrid Dictionary Learning



### Trustworthy Long-Tailed Classification
- **Arxiv ID**: http://arxiv.org/abs/2111.09030v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.09030v2)
- **Published**: 2021-11-17 10:52:36+00:00
- **Updated**: 2022-03-25 05:24:58+00:00
- **Authors**: Bolian Li, Zongbo Han, Haining Li, Huazhu Fu, Changqing Zhang
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition 2022
- **Journal**: None
- **Summary**: Classification on long-tailed distributed data is a challenging problem, which suffers from serious class-imbalance and accordingly unpromising performance especially on tail classes. Recently, the ensembling based methods achieve the state-of-the-art performance and show great potential. However, there are two limitations for current methods. First, their predictions are not trustworthy for failure-sensitive applications. This is especially harmful for the tail classes where the wrong predictions is basically frequent. Second, they assign unified numbers of experts to all samples, which is redundant for easy samples with excessive computational cost. To address these issues, we propose a Trustworthy Long-tailed Classification (TLC) method to jointly conduct classification and uncertainty estimation to identify hard samples in a multi-expert framework. Our TLC obtains the evidence-based uncertainty (EvU) and evidence for each expert, and then combines these uncertainties and evidences under the Dempster-Shafer Evidence Theory (DST). Moreover, we propose a dynamic expert engagement to reduce the number of engaged experts for easy samples and achieve efficiency while maintaining promising performances. Finally, we conduct comprehensive experiments on the tasks of classification, tail detection, OOD detection and failure prediction. The experimental results show that the proposed TLC outperforms existing methods and is trustworthy with reliable uncertainty.



### Using Convolutional Neural Networks to Detect Compression Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2111.09034v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.09034v2)
- **Published**: 2021-11-17 11:03:16+00:00
- **Updated**: 2022-01-12 16:52:29+00:00
- **Authors**: Shubham Bharadwaj
- **Comment**: Pre-print Under Review
- **Journal**: None
- **Summary**: Machine learning is penetrating various domains virtually, thereby proliferating excellent results. It has also found an outlet in digital forensics, wherein it is becoming the prime driver of computational efficiency. A prominent feature that exhibits the effectiveness of ML algorithms is feature extraction that can be instrumental in the applications for digital forensics. Convolutional Neural Networks are further used to identify parts of the file. To this end, we observed that the literature does not include sufficient information about the identification of the algorithms used to compress file fragments. With this research, we attempt to address this gap as compression algorithms are beneficial in generating higher entropy comparatively as they make the data more compact. We used a base dataset, compressed every file with various algorithms, and designed a model based on that. The used model was accurately able to identify files compressed using compress, lzip and bzip2.



### Improving Person Re-Identification with Temporal Constraints
- **Arxiv ID**: http://arxiv.org/abs/2111.09056v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2111.09056v1)
- **Published**: 2021-11-17 11:56:07+00:00
- **Updated**: 2021-11-17 11:56:07+00:00
- **Authors**: Julia Dietlmeier, Feiyan Hu, Frances Ryan, Noel E. O'Connor, Kevin McGuinness
- **Comment**: 10 pages, RWS @ WACV2022
- **Journal**: None
- **Summary**: In this paper we introduce an image-based person re-identification dataset collected across five non-overlapping camera views in the large and busy airport in Dublin, Ireland. Unlike all publicly available image-based datasets, our dataset contains timestamp information in addition to frame number, and camera and person IDs. Also our dataset has been fully anonymized to comply with modern data privacy regulations. We apply state-of-the-art person re-identification models to our dataset and show that by leveraging the available timestamp information we are able to achieve a significant gain of 37.43% in mAP and a gain of 30.22% in Rank1 accuracy. We also propose a Bayesian temporal re-ranking post-processing step, which further adds a 10.03% gain in mAP and 9.95% gain in Rank1 accuracy metrics. This work on combining visual and temporal information is not possible on other image-based person re-identification datasets. We believe that the proposed new dataset will enable further development of person re-identification research for challenging real-world applications. DAA dataset can be downloaded from https://bit.ly/3AtXTd6



### To Trust or Not To Trust Prediction Scores for Membership Inference Attacks
- **Arxiv ID**: http://arxiv.org/abs/2111.09076v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.09076v3)
- **Published**: 2021-11-17 12:39:04+00:00
- **Updated**: 2023-01-24 14:56:46+00:00
- **Authors**: Dominik Hintersdorf, Lukas Struppek, Kristian Kersting
- **Comment**: 15 pages, 8 figures, 10 tables
- **Journal**: None
- **Summary**: Membership inference attacks (MIAs) aim to determine whether a specific sample was used to train a predictive model. Knowing this may indeed lead to a privacy breach. Most MIAs, however, make use of the model's prediction scores - the probability of each output given some input - following the intuition that the trained model tends to behave differently on its training data. We argue that this is a fallacy for many modern deep network architectures. Consequently, MIAs will miserably fail since overconfidence leads to high false-positive rates not only on known domains but also on out-of-distribution data and implicitly acts as a defense against MIAs. Specifically, using generative adversarial networks, we are able to produce a potentially infinite number of samples falsely classified as part of the training data. In other words, the threat of MIAs is overestimated, and less information is leaked than previously assumed. Moreover, there is actually a trade-off between the overconfidence of models and their susceptibility to MIAs: the more classifiers know when they do not know, making low confidence predictions, the more they reveal the training data.



### Motion Detection using CSI from Raspberry Pi 4
- **Arxiv ID**: http://arxiv.org/abs/2111.09091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.09091v1)
- **Published**: 2021-11-17 13:17:02+00:00
- **Updated**: 2021-11-17 13:17:02+00:00
- **Authors**: Glenn Forbes, Stewart Massie, Susan Craw, Christopher Clare
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: Monitoring behaviour in smart homes using sensors can offer insights into changes in the independent ability and long-term health of residents. Passive Infrared motion sensors (PIRs) are standard, however may not accurately track the full duration of movement. They also require line-of-sight to detect motion which can restrict performance and ensures they must be visible to residents. Channel State Information (CSI) is a low cost, unintrusive form of radio sensing which can monitor movement but also offers opportunities to generate rich data. We have developed a novel, self-calibrating motion detection system which uses CSI data collected and processed on a stock Raspberry Pi 4. This system exploits the correlation between CSI frames, on which we perform variance analysis using our algorithm to accurately measure the full period of a resident's movement. We demonstrate the effectiveness of this approach in several real-world environments. Experiments conducted demonstrate that activity start and end time can be accurately detected for motion examples of different intensities at different locations.



### STEEX: Steering Counterfactual Explanations with Semantics
- **Arxiv ID**: http://arxiv.org/abs/2111.09094v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.09094v3)
- **Published**: 2021-11-17 13:20:29+00:00
- **Updated**: 2022-07-18 19:45:56+00:00
- **Authors**: Paul Jacob, Éloi Zablocki, Hédi Ben-Younes, Mickaël Chen, Patrick Pérez, Matthieu Cord
- **Comment**: ECCV 2022 --- 14 pages + supplementary
- **Journal**: None
- **Summary**: As deep learning models are increasingly used in safety-critical applications, explainability and trustworthiness become major concerns. For simple images, such as low-resolution face portraits, synthesizing visual counterfactual explanations has recently been proposed as a way to uncover the decision mechanisms of a trained classification model. In this work, we address the problem of producing counterfactual explanations for high-quality images and complex scenes. Leveraging recent semantic-to-image models, we propose a new generative counterfactual explanation framework that produces plausible and sparse modifications which preserve the overall scene structure. Furthermore, we introduce the concept of "region-targeted counterfactual explanations", and a corresponding framework, where users can guide the generation of counterfactuals by specifying a set of semantic regions of the query image the explanation must be about. Extensive experiments are conducted on challenging datasets including high-quality portraits (CelebAMask-HQ) and driving scenes (BDD100k). Code is available at https://github.com/valeoai/STEEX



### Quality Measures in Biometric Systems
- **Arxiv ID**: http://arxiv.org/abs/2111.08704v1
- **DOI**: 10.1109/MSP.2011.178
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2111.08704v1)
- **Published**: 2021-11-17 13:28:07+00:00
- **Updated**: 2021-11-17 13:28:07+00:00
- **Authors**: Fernando Alonso-Fernandez, Julian Fierrez, Javier Ortega-Garcia
- **Comment**: Published at IEEE Security & Privacy journal
- **Journal**: None
- **Summary**: Biometric technology has been increasingly deployed in the past decade, offering greater security and convenience than traditional methods of personal recognition. Although biometric signals' quality heavily affects a biometric system's performance, prior research on evaluating quality is limited. Quality is a critical issue in security, especially in adverse scenarios involving surveillance cameras, forensics, portable devices, or remote access through the Internet. This article analyzes what factors negatively impact biometric quality, how to overcome them, and how to incorporate quality measures into biometric systems. A review of the state of the art in these matters gives an overall framework for the challenges of biometric quality.



### Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2111.09099v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.09099v6)
- **Published**: 2021-11-17 13:30:31+00:00
- **Updated**: 2022-03-14 09:41:35+00:00
- **Authors**: Nicolae-Catalin Ristea, Neelu Madan, Radu Tudor Ionescu, Kamal Nasrollahi, Fahad Shahbaz Khan, Thomas B. Moeslund, Mubarak Shah
- **Comment**: Accepted at CVPR 2022. Paper + supplementary (14 pages, 9 figures)
- **Journal**: None
- **Summary**: Anomaly detection is commonly pursued as a one-class classification problem, where models can only learn from normal training samples, while being evaluated on both normal and abnormal test samples. Among the successful approaches for anomaly detection, a distinguished category of methods relies on predicting masked information (e.g. patches, future frames, etc.) and leveraging the reconstruction error with respect to the masked information as an abnormality score. Different from related methods, we propose to integrate the reconstruction-based functionality into a novel self-supervised predictive architectural building block. The proposed self-supervised block is generic and can easily be incorporated into various state-of-the-art anomaly detection methods. Our block starts with a convolutional layer with dilated filters, where the center area of the receptive field is masked. The resulting activation maps are passed through a channel attention module. Our block is equipped with a loss that minimizes the reconstruction error with respect to the masked area in the receptive field. We demonstrate the generality of our block by integrating it into several state-of-the-art frameworks for anomaly detection on image and video, providing empirical evidence that shows considerable performance improvements on MVTec AD, Avenue, and ShanghaiTech. We release our code as open source at https://github.com/ristea/sspcab.



### Fast and Light-Weight Network for Single Frame Structured Illumination Microscopy Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2111.09103v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.09103v1)
- **Published**: 2021-11-17 13:39:41+00:00
- **Updated**: 2021-11-17 13:39:41+00:00
- **Authors**: Xi Cheng, Jun Li, Qiang Dai, Zhenyong Fu, Jian Yang
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Structured illumination microscopy (SIM) is an important super-resolution based microscopy technique that breaks the diffraction limit and enhances optical microscopy systems. With the development of biology and medical engineering, there is a high demand for real-time and robust SIM imaging under extreme low light and short exposure environments. Existing SIM techniques typically require multiple structured illumination frames to produce a high-resolution image. In this paper, we propose a single-frame structured illumination microscopy (SF-SIM) based on deep learning. Our SF-SIM only needs one shot of a structured illumination frame and generates similar results compared with the traditional SIM systems that typically require 15 shots. In our SF-SIM, we propose a noise estimator which can effectively suppress the noise in the image and enable our method to work under the low light and short exposure environment, without the need for stacking multiple frames for non-local denoising. We also design a bandpass attention module that makes our deep network more sensitive to the change of frequency and enhances the imaging quality. Our proposed SF-SIM is almost 14 times faster than traditional SIM methods when achieving similar results. Therefore, our method is significantly valuable for the development of microbiology and medicine.



### Benchmarking Quality-Dependent and Cost-Sensitive Score-Level Multimodal Biometric Fusion Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2111.08703v1
- **DOI**: 10.1109/TIFS.2009.2034885
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2111.08703v1)
- **Published**: 2021-11-17 13:39:48+00:00
- **Updated**: 2021-11-17 13:39:48+00:00
- **Authors**: Norman Poh, Thirimachos Bourlai, Josef Kittler, Lorene Allano, Fernando Alonso-Fernandez, Onkar Ambekar, John Baker, Bernadette Dorizzi, Omolara Fatukasi, Julian Fierrez, Harald Ganster, Javier Ortega-Garcia, Donald Maurer, Albert Ali Salah, Tobias Scheidat, Claus Vielhauer
- **Comment**: Published at IEEE Transactions on Information Forensics and Security
  journal
- **Journal**: None
- **Summary**: Automatically verifying the identity of a person by means of biometrics is an important application in day-to-day activities such as accessing banking services and security control in airports. To increase the system reliability, several biometric devices are often used. Such a combined system is known as a multimodal biometric system. This paper reports a benchmarking study carried out within the framework of the BioSecure DS2 (Access Control) evaluation campaign organized by the University of Surrey, involving face, fingerprint, and iris biometrics for person authentication, targeting the application of physical access control in a medium-size establishment with some 500 persons. While multimodal biometrics is a well-investigated subject, there exists no benchmark for a fusion algorithm comparison. Working towards this goal, we designed two sets of experiments: quality-dependent and cost-sensitive evaluation. The quality-dependent evaluation aims at assessing how well fusion algorithms can perform under changing quality of raw images principally due to change of devices. The cost-sensitive evaluation, on the other hand, investigates how well a fusion algorithm can perform given restricted computation and in the presence of software and hardware failures, resulting in errors such as failure-to-acquire and failure-to-match. Since multiple capturing devices are available, a fusion algorithm should be able to handle this nonideal but nevertheless realistic scenario. In both evaluations, each fusion algorithm is provided with scores from each biometric comparison subsystem as well as the quality measures of both template and query data. The response to the call of the campaign proved very encouraging, with the submission of 22 fusion systems. To the best of our knowledge, this is the first attempt to benchmark quality-based multimodal fusion algorithms.



### Cryo-shift: Reducing domain shift in cryo-electron subtomograms with unsupervised domain adaptation and randomization
- **Arxiv ID**: http://arxiv.org/abs/2111.09114v1
- **DOI**: 10.1093/bioinformatics/btab794
- **Categories**: **q-bio.QM**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.09114v1)
- **Published**: 2021-11-17 13:43:36+00:00
- **Updated**: 2021-11-17 13:43:36+00:00
- **Authors**: Hmrishav Bandyopadhyay, Zihao Deng, Leiting Ding, Sinuo Liu, Mostofa Rafid Uddin, Xiangrui Zeng, Sima Behpour, Min Xu
- **Comment**: 14 pages
- **Journal**: Bioinformatics 2021
- **Summary**: Cryo-Electron Tomography (cryo-ET) is a 3D imaging technology that enables the visualization of subcellular structures in situ at near-atomic resolution. Cellular cryo-ET images help in resolving the structures of macromolecules and determining their spatial relationship in a single cell, which has broad significance in cell and structural biology. Subtomogram classification and recognition constitute a primary step in the systematic recovery of these macromolecular structures. Supervised deep learning methods have been proven to be highly accurate and efficient for subtomogram classification, but suffer from limited applicability due to scarcity of annotated data. While generating simulated data for training supervised models is a potential solution, a sizeable difference in the image intensity distribution in generated data as compared to real experimental data will cause the trained models to perform poorly in predicting classes on real subtomograms. In this work, we present Cryo-Shift, a fully unsupervised domain adaptation and randomization framework for deep learning-based cross-domain subtomogram classification. We use unsupervised multi-adversarial domain adaption to reduce the domain shift between features of simulated and experimental data. We develop a network-driven domain randomization procedure with `warp' modules to alter the simulated data and help the classifier generalize better on experimental data. We do not use any labeled experimental data to train our model, whereas some of the existing alternative approaches require labeled experimental samples for cross-domain classification. Nevertheless, Cryo-Shift outperforms the existing alternative approaches in cross-domain subtomogram classification in extensive evaluation studies demonstrated herein using both simulated and experimental data.



### The Multiscenario Multienvironment BioSecure Multimodal Database (BMDB)
- **Arxiv ID**: http://arxiv.org/abs/2111.08702v1
- **DOI**: 10.1109/TPAMI.2009.76
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2111.08702v1)
- **Published**: 2021-11-17 13:50:23+00:00
- **Updated**: 2021-11-17 13:50:23+00:00
- **Authors**: Javier Ortega-Garcia, Julian Fierrez, Fernando Alonso-Fernandez, Javier Galbally, Manuel R Freire, Joaquin Gonzalez-Rodriguez, Carmen Garcia-Mateo, Jose-Luis Alba-Castro, Elisardo Gonzalez-Agulla, Enrique Otero-Muras, Sonia Garcia-Salicetti, Lorene Allano, Bao Ly-Van, Bernadette Dorizzi, Josef Kittler, Thirimachos Bourlai, Norman Poh, Farzin Deravi, Ming NR Ng, Michael Fairhurst, Jean Hennebert, Andreas Humm, Massimo Tistarelli, Linda Brodo, Jonas Richiardi, Andrezj Drygajlo, Harald Ganster, Federico M Sukno, Sri-Kaushik Pavani, Alejandro Frangi, Lale Akarun, Arman Savran
- **Comment**: Published at IEEE Transactions on Pattern Analysis and Machine
  Intelligence journal
- **Journal**: None
- **Summary**: A new multimodal biometric database designed and acquired within the framework of the European BioSecure Network of Excellence is presented. It is comprised of more than 600 individuals acquired simultaneously in three scenarios: 1) over the Internet, 2) in an office environment with desktop PC, and 3) in indoor/outdoor environments with mobile portable hardware. The three scenarios include a common part of audio/video data. Also, signature and fingerprint data have been acquired both with desktop PC and mobile portable hardware. Additionally, hand and iris data were acquired in the second scenario using desktop PC. Acquisition has been conducted by 11 European institutions. Additional features of the BioSecure Multimodal Database (BMDB) are: two acquisition sessions, several sensors in certain modalities, balanced gender and age distributions, multimodal realistic scenarios with simple and quick tasks per modality, cross-European diversity, availability of demographic data, and compatibility with other multimodal databases. The novel acquisition conditions of the BMDB allow us to perform new challenging research and evaluation of either monomodal or multimodal biometric systems, as in the recent BioSecure Multimodal Evaluation campaign. A description of this campaign including baseline results of individual modalities from the new database is also given. The database is expected to be available for research purposes through the BioSecure Association during 2008



### IntraQ: Learning Synthetic Images with Intra-Class Heterogeneity for Zero-Shot Network Quantization
- **Arxiv ID**: http://arxiv.org/abs/2111.09136v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.09136v5)
- **Published**: 2021-11-17 14:17:19+00:00
- **Updated**: 2022-03-10 06:48:25+00:00
- **Authors**: Yunshan Zhong, Mingbao Lin, Gongrui Nan, Jianzhuang Liu, Baochang Zhang, Yonghong Tian, Rongrong Ji
- **Comment**: CVPR2022
- **Journal**: None
- **Summary**: Learning to synthesize data has emerged as a promising direction in zero-shot quantization (ZSQ), which represents neural networks by low-bit integer without accessing any of the real data. In this paper, we observe an interesting phenomenon of intra-class heterogeneity in real data and show that existing methods fail to retain this property in their synthetic images, which causes a limited performance increase. To address this issue, we propose a novel zero-shot quantization method referred to as IntraQ. First, we propose a local object reinforcement that locates the target objects at different scales and positions of the synthetic images. Second, we introduce a marginal distance constraint to form class-related features distributed in a coarse area. Lastly, we devise a soft inception loss which injects a soft prior label to prevent the synthetic images from being overfitting to a fixed object. Our IntraQ is demonstrated to well retain the intra-class heterogeneity in the synthetic images and also observed to perform state-of-the-art. For example, compared to the advanced ZSQ, our IntraQ obtains 9.17\% increase of the top-1 accuracy on ImageNet when all layers of MobileNetV1 are quantized to 4-bit. Code is at https://github.com/zysxmu/IntraQ.



### Two-Face: Adversarial Audit of Commercial Face Recognition Systems
- **Arxiv ID**: http://arxiv.org/abs/2111.09137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.09137v1)
- **Published**: 2021-11-17 14:21:23+00:00
- **Updated**: 2021-11-17 14:21:23+00:00
- **Authors**: Siddharth D Jaiswal, Karthikeya Duggirala, Abhisek Dash, Animesh Mukherjee
- **Comment**: This work has been accepted for publication at ICWSM 2022
- **Journal**: None
- **Summary**: Computer vision applications like automated face detection are used for a variety of purposes ranging from unlocking smart devices to tracking potential persons of interest for surveillance. Audits of these applications have revealed that they tend to be biased against minority groups which result in unfair and concerning societal and political outcomes. Despite multiple studies over time, these biases have not been mitigated completely and have in fact increased for certain tasks like age prediction. While such systems are audited over benchmark datasets, it becomes necessary to evaluate their robustness for adversarial inputs. In this work, we perform an extensive adversarial audit on multiple systems and datasets, making a number of concerning observations - there has been a drop in accuracy for some tasks on CELEBSET dataset since a previous audit. While there still exists a bias in accuracy against individuals from minority groups for multiple datasets, a more worrying observation is that these biases tend to get exorbitantly pronounced with adversarial inputs toward the minority group. We conclude with a discussion on the broader societal impacts in light of these observations and a few suggestions on how to collectively deal with this issue.



### It's About Time: Analog Clock Reading in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2111.09162v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.09162v4)
- **Published**: 2021-11-17 14:52:02+00:00
- **Updated**: 2022-04-05 22:07:34+00:00
- **Authors**: Charig Yang, Weidi Xie, Andrew Zisserman
- **Comment**: CVPR 2022. Project page:
  https://www.robots.ox.ac.uk/~vgg/research/time
- **Journal**: None
- **Summary**: In this paper, we present a framework for reading analog clocks in natural images or videos. Specifically, we make the following contributions: First, we create a scalable pipeline for generating synthetic clocks, significantly reducing the requirements for the labour-intensive annotations; Second, we introduce a clock recognition architecture based on spatial transformer networks (STN), which is trained end-to-end for clock alignment and recognition. We show that the model trained on the proposed synthetic dataset generalises towards real clocks with good accuracy, advocating a Sim2Real training regime; Third, to further reduce the gap between simulation and real data, we leverage the special property of "time", i.e.uniformity, to generate reliable pseudo-labels on real unlabelled clock videos, and show that training on these videos offers further improvements while still requiring zero manual annotations. Lastly, we introduce three benchmark datasets based on COCO, Open Images, and The Clock movie, with full annotations for time, accurate to the minute.



### Automated Approach for Computer Vision-based Vehicle Movement Classification at Traffic Intersections
- **Arxiv ID**: http://arxiv.org/abs/2111.09171v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.09171v1)
- **Published**: 2021-11-17 15:02:43+00:00
- **Updated**: 2021-11-17 15:02:43+00:00
- **Authors**: Udita Jana, Jyoti Prakash Das Karmakar, Pranamesh Chakraborty, Tingting Huang, Dave Ness, Duane Ritcher, Anuj Sharma
- **Comment**: None
- **Journal**: None
- **Summary**: Movement specific vehicle classification and counting at traffic intersections is a crucial component for various traffic management activities. In this context, with recent advancements in computer-vision based techniques, cameras have emerged as a reliable data source for extracting vehicular trajectories from traffic scenes. However, classifying these trajectories by movement type is quite challenging as characteristics of motion trajectories obtained this way vary depending on camera calibrations. Although some existing methods have addressed such classification tasks with decent accuracies, the performance of these methods significantly relied on manual specification of several regions of interest. In this study, we proposed an automated classification method for movement specific classification (such as right-turn, left-turn and through movements) of vision-based vehicle trajectories. Our classification framework identifies different movement patterns observed in a traffic scene using an unsupervised hierarchical clustering technique Thereafter a similarity-based assignment strategy is adopted to assign incoming vehicle trajectories to identified movement groups. A new similarity measure was designed to overcome the inherent shortcomings of vision-based trajectories. Experimental results demonstrated the effectiveness of the proposed classification approach and its ability to adapt to different traffic scenarios without any manual intervention.



### End-to-end optimized image compression with competition of prior distributions
- **Arxiv ID**: http://arxiv.org/abs/2111.09172v1
- **DOI**: 10.1109/CVPRW53098.2021.00212
- **Categories**: **eess.IV**, cs.CV, 68T07 (Primary), 68P30 (Secondary), I.4.2
- **Links**: [PDF](http://arxiv.org/pdf/2111.09172v1)
- **Published**: 2021-11-17 15:04:01+00:00
- **Updated**: 2021-11-17 15:04:01+00:00
- **Authors**: Benoit Brummer, Christophe De Vleeschouwer
- **Comment**: None
- **Journal**: 2021 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW)
- **Summary**: Convolutional autoencoders are now at the forefront of image compression research. To improve their entropy coding, encoder output is typically analyzed with a second autoencoder to generate per-variable parametrized prior probability distributions. We instead propose a compression scheme that uses a single convolutional autoencoder and multiple learned prior distributions working as a competition of experts. Trained prior distributions are stored in a static table of cumulative distribution functions. During inference, this table is used by an entropy coder as a look-up-table to determine the best prior for each spatial location. Our method offers rate-distortion performance comparable to that obtained with a predicted parametrized prior with only a fraction of its entropy coding and decoding complexity.



### Augmentation of base classifier performance via HMMs on a handwritten character data set
- **Arxiv ID**: http://arxiv.org/abs/2111.10204v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.10204v1)
- **Published**: 2021-11-17 15:22:47+00:00
- **Updated**: 2021-11-17 15:22:47+00:00
- **Authors**: Hélder Campos, Nuno Paulino
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents results of a study of the performance of several base classifiers for recognition of handwritten characters of the modern Latin alphabet. Base classification performance is further enhanced by utilizing Viterbi error correction by determining the Viterbi sequence. Hidden Markov Models (HMMs) models exploit relationships between letters within a word to determine the most likely sequence of characters. Four base classifiers are studied along with eight feature sets extracted from the handwritten dataset. The best classification performance after correction was 89.8%, and the average was 68.1%



### Tiny Obstacle Discovery by Occlusion-Aware Multilayer Regression
- **Arxiv ID**: http://arxiv.org/abs/2111.09204v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.09204v1)
- **Published**: 2021-11-17 15:55:27+00:00
- **Updated**: 2021-11-17 15:55:27+00:00
- **Authors**: Feng Xue, Anlong Ming, Yu Zhou
- **Comment**: Published in Transaction on Image Processing 2021
- **Journal**: None
- **Summary**: Edges are the fundamental visual element for discovering tiny obstacles using a monocular camera. Nevertheless, tiny obstacles often have weak and inconsistent edge cues due to various properties such as small size and similar appearance to the free space, making it hard to capture them. ...



### Single-pass Object-adaptive Data Undersampling and Reconstruction for MRI
- **Arxiv ID**: http://arxiv.org/abs/2111.09212v3
- **DOI**: 10.1109/TCI.2022.3167454
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2111.09212v3)
- **Published**: 2021-11-17 16:06:06+00:00
- **Updated**: 2022-05-18 20:53:42+00:00
- **Authors**: Zhishen Huang, Saiprasad Ravishankar
- **Comment**: None
- **Journal**: in IEEE Transactions on Computational Imaging, vol. 8, pp.
  333-345, 2022
- **Summary**: There is much recent interest in techniques to accelerate the data acquisition process in MRI by acquiring limited measurements. Often sophisticated reconstruction algorithms are deployed to maintain high image quality in such settings. In this work, we propose a data-driven sampler using a convolutional neural network, MNet, to provide object-specific sampling patterns adaptive to each scanned object. The network observes very limited low-frequency k-space data for each object and rapidly predicts the desired undersampling pattern in one go that achieves high image reconstruction quality. We propose an accompanying alternating-type training framework with a mask-backward procedure that efficiently generates training labels for the sampler network and jointly trains an image reconstruction network. Experimental results on the fastMRI knee dataset demonstrate the ability of the proposed learned undersampling network to generate object-specific masks at fourfold and eightfold acceleration that achieve superior image reconstruction performance than several existing schemes. The source code for the proposed joint sampling and reconstruction learning framework is available at https://github.com/zhishenhuang/mri.



### Segmentation of Lung Tumor from CT Images using Deep Supervision
- **Arxiv ID**: http://arxiv.org/abs/2111.09262v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.09262v1)
- **Published**: 2021-11-17 17:50:18+00:00
- **Updated**: 2021-11-17 17:50:18+00:00
- **Authors**: Farhanaz Farheen, Md. Salman Shamil, Nabil Ibtehaz, M. Sohel Rahman
- **Comment**: None
- **Journal**: None
- **Summary**: Lung cancer is a leading cause of death in most countries of the world. Since prompt diagnosis of tumors can allow oncologists to discern their nature, type and the mode of treatment, tumor detection and segmentation from CT Scan images is a crucial field of study worldwide. This paper approaches lung tumor segmentation by applying two-dimensional discrete wavelet transform (DWT) on the LOTUS dataset for more meticulous texture analysis whilst integrating information from neighboring CT slices before feeding them to a Deeply Supervised MultiResUNet model. Variations in learning rates, decay and optimization algorithms while training the network have led to different dice co-efficients, the detailed statistics of which have been included in this paper. We also discuss the challenges in this dataset and how we opted to overcome them. In essence, this study aims to maximize the success rate of predicting tumor regions from two dimensional CT Scan slices by experimenting with a number of adequate networks, resulting in a dice co-efficient of 0.8472.



### DiverGAN: An Efficient and Effective Single-Stage Framework for Diverse Text-to-Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2111.09267v1
- **DOI**: 10.1016/j.neucom.2021.12.005
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.09267v1)
- **Published**: 2021-11-17 17:59:56+00:00
- **Updated**: 2021-11-17 17:59:56+00:00
- **Authors**: Zhenxing Zhang, Lambert Schomaker
- **Comment**: None
- **Journal**: Neurocomputing 2022
- **Summary**: In this paper, we present an efficient and effective single-stage framework (DiverGAN) to generate diverse, plausible and semantically consistent images according to a natural-language description. DiverGAN adopts two novel word-level attention modules, i.e., a channel-attention module (CAM) and a pixel-attention module (PAM), which model the importance of each word in the given sentence while allowing the network to assign larger weights to the significant channels and pixels semantically aligning with the salient words. After that, Conditional Adaptive Instance-Layer Normalization (CAdaILN) is introduced to enable the linguistic cues from the sentence embedding to flexibly manipulate the amount of change in shape and texture, further improving visual-semantic representation and helping stabilize the training. Also, a dual-residual structure is developed to preserve more original visual features while allowing for deeper networks, resulting in faster convergence speed and more vivid details. Furthermore, we propose to plug a fully-connected layer into the pipeline to address the lack-of-diversity problem, since we observe that a dense layer will remarkably enhance the generative capability of the network, balancing the trade-off between a low-dimensional random latent code contributing to variants and modulation modules that use high-dimensional and textual contexts to strength feature maps. Inserting a linear layer after the second residual block achieves the best variety and quality. Both qualitative and quantitative results on benchmark data sets demonstrate the superiority of our DiverGAN for realizing diversity, without harming quality and semantic consistency.



### Induce, Edit, Retrieve: Language Grounded Multimodal Schema for Instructional Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2111.09276v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2111.09276v1)
- **Published**: 2021-11-17 18:20:04+00:00
- **Updated**: 2021-11-17 18:20:04+00:00
- **Authors**: Yue Yang, Joongwon Kim, Artemis Panagopoulou, Mark Yatskar, Chris Callison-Burch
- **Comment**: None
- **Journal**: None
- **Summary**: Schemata are structured representations of complex tasks that can aid artificial intelligence by allowing models to break down complex tasks into intermediate steps. We propose a novel system that induces schemata from web videos and generalizes them to capture unseen tasks with the goal of improving video retrieval performance. Our system proceeds in three major phases: (1) Given a task with related videos, we construct an initial schema for a task using a joint video-text model to match video segments with text representing steps from wikiHow; (2) We generalize schemata to unseen tasks by leveraging language models to edit the text within existing schemata. Through generalization, we can allow our schemata to cover a more extensive range of tasks with a small amount of learning data; (3) We conduct zero-shot instructional video retrieval with the unseen task names as the queries. Our schema-guided approach outperforms existing methods for video retrieval, and we demonstrate that the schemata induced by our system are better than those generated by other models.



### Learning to Compose Visual Relations
- **Arxiv ID**: http://arxiv.org/abs/2111.09297v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2111.09297v1)
- **Published**: 2021-11-17 18:51:29+00:00
- **Updated**: 2021-11-17 18:51:29+00:00
- **Authors**: Nan Liu, Shuang Li, Yilun Du, Joshua B. Tenenbaum, Antonio Torralba
- **Comment**: NeurIPS 2021 (Spotlight), first three authors contributed equally,
  Website: https://composevisualrelations.github.io/
- **Journal**: None
- **Summary**: The visual world around us can be described as a structured set of objects and their associated relations. An image of a room may be conjured given only the description of the underlying objects and their associated relations. While there has been significant work on designing deep neural networks which may compose individual objects together, less work has been done on composing the individual relations between objects. A principal difficulty is that while the placement of objects is mutually independent, their relations are entangled and dependent on each other. To circumvent this issue, existing works primarily compose relations by utilizing a holistic encoder, in the form of text or graphs. In this work, we instead propose to represent each relation as an unnormalized density (an energy-based model), enabling us to compose separate relations in a factorized manner. We show that such a factorized decomposition allows the model to both generate and edit scenes that have multiple sets of relations more faithfully. We further show that decomposition enables our model to effectively understand the underlying relational scene structure. Project page at: https://composevisualrelations.github.io/.



### SeCGAN: Parallel Conditional Generative Adversarial Networks for Face Editing via Semantic Consistency
- **Arxiv ID**: http://arxiv.org/abs/2111.09298v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.09298v4)
- **Published**: 2021-11-17 18:54:58+00:00
- **Updated**: 2022-06-16 13:13:46+00:00
- **Authors**: Jiaze Sun, Binod Bhattarai, Zhixiang Chen, Tae-Kyun Kim
- **Comment**: Accepted by AI for Content Creation (AI4CC) workshop at CVPR 2022
- **Journal**: None
- **Summary**: Semantically guided conditional Generative Adversarial Networks (cGANs) have become a popular approach for face editing in recent years. However, most existing methods introduce semantic masks as direct conditional inputs to the generator and often require the target masks to perform the corresponding translation in the RGB space. We propose SeCGAN, a novel label-guided cGAN for editing face images utilising semantic information without the need to specify target semantic masks. During training, SeCGAN has two branches of generators and discriminators operating in parallel, with one trained to translate RGB images and the other for semantic masks. To bridge the two branches in a mutually beneficial manner, we introduce a semantic consistency loss which constrains both branches to have consistent semantic outputs. Whilst both branches are required during training, the RGB branch is our primary network and the semantic branch is not needed for inference. Our results on CelebA and CelebA-HQ demonstrate that our approach is able to generate facial images with more accurate attributes, outperforming competitive baselines in terms of Target Attribute Recognition Rate whilst maintaining quality metrics such as self-supervised Fr\'{e}chet Inception Distance and Inception Score.



### Learning to Align Sequential Actions in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2111.09301v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.09301v1)
- **Published**: 2021-11-17 18:55:36+00:00
- **Updated**: 2021-11-17 18:55:36+00:00
- **Authors**: Weizhe Liu, Bugra Tekin, Huseyin Coskun, Vibhav Vineet, Pascal Fua, Marc Pollefeys
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art methods for self-supervised sequential action alignment rely on deep networks that find correspondences across videos in time. They either learn frame-to-frame mapping across sequences, which does not leverage temporal information, or assume monotonic alignment between each video pair, which ignores variations in the order of actions. As such, these methods are not able to deal with common real-world scenarios that involve background frames or videos that contain non-monotonic sequence of actions.   In this paper, we propose an approach to align sequential actions in the wild that involve diverse temporal variations. To this end, we propose an approach to enforce temporal priors on the optimal transport matrix, which leverages temporal consistency, while allowing for variations in the order of actions. Our model accounts for both monotonic and non-monotonic sequences and handles background frames that should not be aligned. We demonstrate that our approach consistently outperforms the state-of-the-art in self-supervised sequential action representation learning on four different benchmark datasets.



### Facial Information Analysis Technology for Gender and Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/2111.09303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.09303v1)
- **Published**: 2021-11-17 18:56:43+00:00
- **Updated**: 2021-11-17 18:56:43+00:00
- **Authors**: Gilheum Park, Sua Jung
- **Comment**: None
- **Journal**: None
- **Summary**: This is a study on facial information analysis technology for estimating gender and age, and poses are estimated using a transformation relationship matrix between the camera coordinate system and the world coordinate system for estimating the pose of a face image. Gender classification was relatively simple compared to age estimation, and age estimation was made possible using deep learning-based facial recognition technology. A comparative CNN was proposed to calculate the experimental results using the purchased database and the public database, and deep learning-based gender classification and age estimation performed at a significant level and was more robust to environmental changes compared to the existing machine learning techniques.



### Temporally Consistent Online Depth Estimation in Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2111.09337v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.09337v3)
- **Published**: 2021-11-17 19:00:51+00:00
- **Updated**: 2022-12-08 20:06:59+00:00
- **Authors**: Zhaoshuo Li, Wei Ye, Dilin Wang, Francis X. Creighton, Russell H. Taylor, Ganesh Venkatesh, Mathias Unberath
- **Comment**: WACV 2023, project page: https://mli0603.github.io/codd/
- **Journal**: None
- **Summary**: Temporally consistent depth estimation is crucial for online applications such as augmented reality. While stereo depth estimation has received substantial attention as a promising way to generate 3D information, there is relatively little work focused on maintaining temporal stability. Indeed, based on our analysis, current techniques still suffer from poor temporal consistency. Stabilizing depth temporally in dynamic scenes is challenging due to concurrent object and camera motion. In an online setting, this process is further aggravated because only past frames are available. We present a framework named Consistent Online Dynamic Depth (CODD) to produce temporally consistent depth estimates in dynamic scenes in an online setting. CODD augments per-frame stereo networks with novel motion and fusion networks. The motion network accounts for dynamics by predicting a per-pixel SE3 transformation and aligning the observations. The fusion network improves temporal depth consistency by aggregating the current and past estimates. We conduct extensive experiments and demonstrate quantitatively and qualitatively that CODD outperforms competing methods in terms of temporal consistency and performs on par in terms of per-frame accuracy.



### MPF6D: Masked Pyramid Fusion 6D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2111.09378v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.09378v2)
- **Published**: 2021-11-17 20:23:54+00:00
- **Updated**: 2022-02-04 21:29:56+00:00
- **Authors**: Nuno Pereira, Luís A. Alexandre
- **Comment**: None
- **Journal**: None
- **Summary**: Object pose estimation has multiple important applications, such as robotic grasping and augmented reality. We present a new method to estimate the 6D pose of objects that improves upon the accuracy of current proposals and can still be used in real-time. Our method uses RGB-D data as input to segment objects and estimate their pose. It uses a neural network with multiple heads to identify the objects in the scene, generate the appropriate masks and estimate the values of the translation vectors and the quaternion that represents the objects' rotation. These heads leverage a pyramid architecture used during feature extraction and feature fusion. We conduct an empirical evaluation using the two most common datasets in the area, and compare against state-of-the-art approaches, illustrating the capabilities of MPF6D. Our method can be used in real-time with its low inference time and high accuracy.



### DeepCurrents: Learning Implicit Representations of Shapes with Boundaries
- **Arxiv ID**: http://arxiv.org/abs/2111.09383v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2111.09383v2)
- **Published**: 2021-11-17 20:34:20+00:00
- **Updated**: 2022-03-21 23:07:53+00:00
- **Authors**: David Palmer, Dmitriy Smirnov, Stephanie Wang, Albert Chern, Justin Solomon
- **Comment**: None
- **Journal**: None
- **Summary**: Recent techniques have been successful in reconstructing surfaces as level sets of learned functions (such as signed distance fields) parameterized by deep neural networks. Many of these methods, however, learn only closed surfaces and are unable to reconstruct shapes with boundary curves. We propose a hybrid shape representation that combines explicit boundary curves with implicit learned interiors. Using machinery from geometric measure theory, we parameterize currents using deep networks and use stochastic gradient descent to solve a minimal surface problem. By modifying the metric according to target geometry coming, e.g., from a mesh or point cloud, we can use this approach to represent arbitrary surfaces, learning implicitly defined shapes with explicitly defined boundary curves. We further demonstrate learning families of shapes jointly parameterized by boundary curves and latent codes.



### Low Precision Decentralized Distributed Training over IID and non-IID Data
- **Arxiv ID**: http://arxiv.org/abs/2111.09389v3
- **DOI**: 10.1016/j.neunet.2022.08.032
- **Categories**: **cs.LG**, cs.CV, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2111.09389v3)
- **Published**: 2021-11-17 20:48:09+00:00
- **Updated**: 2022-09-11 18:38:26+00:00
- **Authors**: Sai Aparna Aketi, Sangamesh Kodge, Kaushik Roy
- **Comment**: 11 pages, 7 figures, 9 tables
- **Journal**: Neural Networks 2022
- **Summary**: Decentralized distributed learning is the key to enabling large-scale machine learning (training) on edge devices utilizing private user-generated local data, without relying on the cloud. However, the practical realization of such on-device training is limited by the communication and compute bottleneck. In this paper, we propose and show the convergence of low precision decentralized training that aims to reduce the computational complexity and communication cost of decentralized training. Many feedback-based compression techniques have been proposed in the literature to reduce communication costs. To the best of our knowledge, there is no work that applies and shows compute efficient training techniques such as quantization, pruning, etc., for peer-to-peer decentralized learning setups. Since real-world applications have a significant skew in the data distribution, we design "Range-EvoNorm" as the normalization activation layer which is better suited for low precision training over non-IID data. Moreover, we show that the proposed low precision training can be used in synergy with other communication compression methods decreasing the communication cost further. Our experiments indicate that 8-bit decentralized training has minimal accuracy loss compared to its full precision counterpart even with non-IID data. However, when low precision training is accompanied by communication compression through sparsification we observe a 1-2% drop in accuracy. The proposed low precision decentralized training decreases computational complexity, memory usage, and communication cost by 4x and compute energy by a factor of ~20x, while trading off less than a $1\%$ accuracy for both IID and non-IID data. In particular, with higher skew values, we observe an increase in accuracy (by ~ 0.5%) with low precision training, indicating the regularization effect of the quantization.



### Fine-Grained Vehicle Classification in Urban Traffic Scenes using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.09403v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.09403v1)
- **Published**: 2021-11-17 21:19:03+00:00
- **Updated**: 2021-11-17 21:19:03+00:00
- **Authors**: Syeda Aneeba Najeeb, Rana Hammad Raza, Adeel Yusuf, Zamra Sultan
- **Comment**: None
- **Journal**: None
- **Summary**: The increasingly dense traffic is becoming a challenge in our local settings, urging the need for a better traffic monitoring and management system. Fine-grained vehicle classification appears to be a challenging task as compared to vehicle coarse classification. Exploring a robust approach for vehicle detection and classification into fine-grained categories is therefore essentially required. Existing Vehicle Make and Model Recognition (VMMR) systems have been developed on synchronized and controlled traffic conditions. Need for robust VMMR in complex, urban, heterogeneous, and unsynchronized traffic conditions still remain an open research area. In this paper, vehicle detection and fine-grained classification are addressed using deep learning. To perform fine-grained classification with related complexities, local dataset THS-10 having high intra-class and low interclass variation is exclusively prepared. The dataset consists of 4250 vehicle images of 10 vehicle models, i.e., Honda City, Honda Civic, Suzuki Alto, Suzuki Bolan, Suzuki Cultus, Suzuki Mehran, Suzuki Ravi, Suzuki Swift, Suzuki Wagon R and Toyota Corolla. This dataset is available online. Two approaches have been explored and analyzed for classification of vehicles i.e, fine-tuning, and feature extraction from deep neural networks. A comparative study is performed, and it is demonstrated that simpler approaches can produce good results in local environment to deal with complex issues such as dense occlusion and lane departures. Hence reducing computational load and time, e.g. fine-tuning Inception-v3 produced highest accuracy of 97.4% with lowest misclassification rate of 2.08%. Fine-tuning MobileNet-v2 and ResNet-18 produced 96.8% and 95.7% accuracies, respectively. Extracting features from fc6 layer of AlexNet produces an accuracy of 93.5% with a misclassification rate of 6.5%.



### Rethinking Drone-Based Search and Rescue with Aerial Person Detection
- **Arxiv ID**: http://arxiv.org/abs/2111.09406v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.09406v1)
- **Published**: 2021-11-17 21:48:31+00:00
- **Updated**: 2021-11-17 21:48:31+00:00
- **Authors**: Pasi Pyrrö, Hassan Naseri, Alexander Jung
- **Comment**: 10 pages, 5 figures, 3 tables, 1 algorithm
- **Journal**: None
- **Summary**: The visual inspection of aerial drone footage is an integral part of land search and rescue (SAR) operations today. Since this inspection is a slow, tedious and error-prone job for humans, we propose a novel deep learning algorithm to automate this aerial person detection (APD) task. We experiment with model architecture selection, online data augmentation, transfer learning, image tiling and several other techniques to improve the test performance of our method. We present the novel Aerial Inspection RetinaNet (AIR) algorithm as the combination of these contributions. The AIR detector demonstrates state-of-the-art performance on a commonly used SAR test data set in terms of both precision (~21 percentage point increase) and speed. In addition, we provide a new formal definition for the APD problem in SAR missions. That is, we propose a novel evaluation scheme that ranks detectors in terms of real-world SAR localization requirements. Finally, we propose a novel postprocessing method for robust, approximate object localization: the merging of overlapping bounding boxes (MOB) algorithm. This final processing stage used in the AIR detector significantly improves its performance and usability in the face of real-world aerial SAR missions.



### See Eye to Eye: A Lidar-Agnostic 3D Detection Framework for Unsupervised Multi-Target Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2111.09450v2
- **DOI**: 10.1109/LRA.2022.3185783
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.09450v2)
- **Published**: 2021-11-17 23:46:47+00:00
- **Updated**: 2023-04-10 21:32:35+00:00
- **Authors**: Darren Tsai, Julie Stephany Berrio, Mao Shan, Stewart Worrall, Eduardo Nebot
- **Comment**: Published in RAL and presented in IROS 2022. Code is available at
  https://github.com/darrenjkt/SEE-MTDA
- **Journal**: IEEE Robotics and Automation Letters (2022)
- **Summary**: Sampling discrepancies between different manufacturers and models of lidar sensors result in inconsistent representations of objects. This leads to performance degradation when 3D detectors trained for one lidar are tested on other types of lidars. Remarkable progress in lidar manufacturing has brought about advances in mechanical, solid-state, and recently, adjustable scan pattern lidars. For the latter, existing works often require fine-tuning the model each time scan patterns are adjusted, which is infeasible. We explicitly deal with the sampling discrepancy by proposing a novel unsupervised multi-target domain adaptation framework, SEE, for transferring the performance of state-of-the-art 3D detectors across both fixed and flexible scan pattern lidars without requiring fine-tuning of models by end-users. Our approach interpolates the underlying geometry and normalizes the scan pattern of objects from different lidars before passing them to the detection network. We demonstrate the effectiveness of SEE on public datasets, achieving state-of-the-art results, and additionally provide quantitative results on a novel high-resolution lidar to prove the industry applications of our framework.



