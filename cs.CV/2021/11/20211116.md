# Arxiv Papers in cs.CV on 2021-11-16
### Postdisaster image-based damage detection and repair cost estimation of reinforced concrete buildings using dual convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2111.09862v1
- **DOI**: 10.1111/mice.12549
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2111.09862v1)
- **Published**: 2021-11-16 00:15:39+00:00
- **Updated**: 2021-11-16 00:15:39+00:00
- **Authors**: Xiao Pan, T. Y. Yang
- **Comment**: 16 pages, 21 figures
- **Journal**: Computer Aided Civil and Infrastructure Engineering, 2020
- **Summary**: Reinforced concrete buildings are commonly used around the world. With recent earthquakes worldwide, rapid structural damage inspection and repair cost evaluation are crucial for building owners and policy makers to make informed risk management decisions. To improve the efficiency of such inspection, advanced computer vision techniques based on convolution neural networks have been adopted in recent research to rapidly quantify the damage state of structures. In this paper, an advanced object detection neural network, named YOLO-v2, is implemented which achieves 98.2% and 84.5% average precision in training and testing, respectively. The proposed YOLO-v2 is used in combination with the classification neural network, which improves the identification accuracy for critical damage state of reinforced concrete structures by 7.5%. The improved classification procedures allow engineers to rapidly and more accurately quantify the damage states of the structure, and also localize the critical damage features. The identified damage state can then be integrated with the state-of-the-art performance evaluation framework to quantify the financial losses of critical reinforced concrete buildings. The results can be used by the building owners and decision makers to make informed risk management decisions immediately after the strong earthquake shaking. Hence, resources can be allocated rapidly to improve the resiliency of the community.



### Image-based monitoring of bolt loosening through deep-learning-based integrated detection and tracking
- **Arxiv ID**: http://arxiv.org/abs/2111.09117v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2111.09117v1)
- **Published**: 2021-11-16 00:47:00+00:00
- **Updated**: 2021-11-16 00:47:00+00:00
- **Authors**: Xiao Pan, T. Y. Yang
- **Comment**: 15 pages, 11 figures, accepted in Journal of Computer Aided Civil and
  Infrastructure Engineering, 2021
- **Journal**: None
- **Summary**: Structural bolts are critical components used in different structural elements, such as beam-column connections and friction damping devices. The clamping force in structural bolts is highly influenced by the bolt rotation. Much of the existing vision-based research about bolt rotation estimation relies on traditional computer vision algorithms such as Hough Transform to assess static images of bolts. This requires careful image preprocessing, and it may not perform well in the situation of complicated bolt assemblies, or in the presence of surrounding objects and background noise, thus hindering their real-world applications. In this study, an integrated real-time detect-track method, namely RTDT-Bolt, is proposed to monitor the bolt rotation angle. First, a real-time convolutional-neural-networks-based object detector, named YOLOv3-tiny, is established and trained to localize structural bolts. Then, the target-free object tracking algorithm based on optical flow is implemented, to continuously monitor and quantify the rotation of structural bolts. In order to enhance the tracking performance against background noise and potential illumination changes during tracking, the YOLOv3-tiny is integrated with the optical flow tracking algorithm to re-detect the bolts when the tracking gets lost. Extensive parameter studies were conducted to identify optimal tracking performance and examine the potential limitations. The results indicate the RTDT-Bolt method can greatly enhance the tracking performance of bolt rotation, which can achieve over 90% accuracy using the recommended range for the parameters.



### The Neural Correlates of Image Texture in the Human Vision Using Magnetoencephalography
- **Arxiv ID**: http://arxiv.org/abs/2111.09118v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.09118v1)
- **Published**: 2021-11-16 01:09:51+00:00
- **Updated**: 2021-11-16 01:09:51+00:00
- **Authors**: Elaheh Hatamimajoumerd, Alireza Talebpour
- **Comment**: None
- **Journal**: None
- **Summary**: Undoubtedly, textural property of an image is one of the most important features in object recognition task in both human and computer vision applications. Here, we investigated the neural signatures of four well-known statistical texture features including contrast, homogeneity, energy, and correlation computed from the gray level co-occurrence matrix (GLCM) of the images viewed by the participants in the process of magnetoencephalography (MEG) data collection. To trace these features in the human visual system, we used multivariate pattern analysis (MVPA) and trained a linear support vector machine (SVM) classifier on every timepoint of MEG data representing the brain activity and compared it with the textural descriptors of images using the Spearman correlation. The result of this study demonstrates that hierarchical structure in the processing of these four texture descriptors in the human brain with the order of contrast, homogeneity, energy, and correlation. Additionally, we found that energy, which carries broad texture property of the images, shows a more sustained statistically meaningful correlation with the brain activity in the course of time.



### ShapeY: Measuring Shape Recognition Capacity Using Nearest Neighbor Matching
- **Arxiv ID**: http://arxiv.org/abs/2111.08174v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.08174v1)
- **Published**: 2021-11-16 01:21:54+00:00
- **Updated**: 2021-11-16 01:21:54+00:00
- **Authors**: Jong Woo Nam, Amanda S. Rios, Bartlett W. Mel
- **Comment**: 6 pages, 5 figures, Accepted to NeurIPS: ImageNet Past, Present, and
  Future
- **Journal**: None
- **Summary**: Object recognition in humans depends primarily on shape cues. We have developed a new approach to measuring the shape recognition performance of a vision system based on nearest neighbor view matching within the system's embedding space. Our performance benchmark, ShapeY, allows for precise control of task difficulty, by enforcing that view matching span a specified degree of 3D viewpoint change and/or appearance change. As a first test case we measured the performance of ResNet50 pre-trained on ImageNet. Matching error rates were high. For example, a 27 degree change in object pitch led ResNet50 to match the incorrect object 45% of the time. Appearance changes were also highly disruptive. Examination of false matches indicates that ResNet50's embedding space is severely "tangled". These findings suggest ShapeY can be a useful tool for charting the progress of artificial vision systems towards human-level shape recognition capabilities.



### Coarse-to-fine Animal Pose and Shape Estimation
- **Arxiv ID**: http://arxiv.org/abs/2111.08176v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.08176v1)
- **Published**: 2021-11-16 01:27:20+00:00
- **Updated**: 2021-11-16 01:27:20+00:00
- **Authors**: Chen Li, Gim Hee Lee
- **Comment**: Accepted by Neurips2021
- **Journal**: None
- **Summary**: Most existing animal pose and shape estimation approaches reconstruct animal meshes with a parametric SMAL model. This is because the low-dimensional pose and shape parameters of the SMAL model makes it easier for deep networks to learn the high-dimensional animal meshes. However, the SMAL model is learned from scans of toy animals with limited pose and shape variations, and thus may not be able to represent highly varying real animals well. This may result in poor fittings of the estimated meshes to the 2D evidences, e.g. 2D keypoints or silhouettes. To mitigate this problem, we propose a coarse-to-fine approach to reconstruct 3D animal mesh from a single image. The coarse estimation stage first estimates the pose, shape and translation parameters of the SMAL model. The estimated meshes are then used as a starting point by a graph convolutional network (GCN) to predict a per-vertex deformation in the refinement stage. This combination of SMAL-based and vertex-based representations benefits from both parametric and non-parametric representations. We design our mesh refinement GCN (MRGCN) as an encoder-decoder structure with hierarchical feature representations to overcome the limited receptive field of traditional GCNs. Moreover, we observe that the global image feature used by existing animal mesh reconstruction works is unable to capture detailed shape information for mesh refinement. We thus introduce a local feature extractor to retrieve a vertex-level feature and use it together with the global feature as the input of the MRGCN. We test our approach on the StanfordExtra dataset and achieve state-of-the-art results. Furthermore, we test the generalization capacity of our approach on the Animal Pose and BADJA datasets. Our code is available at the project website.



### $AIR^2$ for Interaction Prediction
- **Arxiv ID**: http://arxiv.org/abs/2111.08184v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08184v1)
- **Published**: 2021-11-16 01:58:30+00:00
- **Updated**: 2021-11-16 01:58:30+00:00
- **Authors**: David Wu, Yunnan Wu
- **Comment**: None
- **Journal**: None
- **Summary**: The 2021 Waymo Interaction Prediction Challenge introduced a problem of predicting the future trajectories and confidences of two interacting agents jointly. We developed a solution that takes an anchored marginal motion prediction model with rasterization and augments it to model agent interaction. We do this by predicting the joint confidences using a rasterized image that highlights the ego agent and the interacting agent. Our solution operates on the cartesian product space of the anchors; hence the $"^2"$ in $AIR^2$. Our model achieved the highest mAP (the primary metric) on the leaderboard.



### Comparative Analysis of Machine Learning Models for Predicting Travel Time
- **Arxiv ID**: http://arxiv.org/abs/2111.08226v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08226v1)
- **Published**: 2021-11-16 04:55:21+00:00
- **Updated**: 2021-11-16 04:55:21+00:00
- **Authors**: Armstrong Aboah, Elizabeth Arthur
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, five different deep learning models are being compared for predicting travel time. These models are autoregressive integrated moving average (ARIMA) model, recurrent neural network (RNN) model, autoregressive (AR) model, Long-short term memory (LSTM) model, and gated recurrent units (GRU) model. The aim of this study is to investigate the performance of each developed model for forecasting travel time. The dataset used in this paper consists of travel time and travel speed information from the state of Missouri. The learning rate used for building each model was varied from 0.0001-0.01. The best learning rate was found to be 0.001. The study concluded that the ARIMA model was the best model architecture for travel time prediction and forecasting.



### Identifying the Factors that Influence Urban Public Transit Demand
- **Arxiv ID**: http://arxiv.org/abs/2111.09126v1
- **DOI**: None
- **Categories**: **stat.AP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.09126v1)
- **Published**: 2021-11-16 05:37:51+00:00
- **Updated**: 2021-11-16 05:37:51+00:00
- **Authors**: Armstrong Aboah, Lydia Johnson, Setul Shah
- **Comment**: None
- **Journal**: None
- **Summary**: The rise in urbanization throughout the United States (US) in recent years has required urban planners and transportation engineers to have greater consideration for the transportation services available to residents of a metropolitan region. This compels transportation authorities to provide better and more reliable modes of public transit through improved technologies and increased service quality. These improvements can be achieved by identifying and understanding the factors that influence urban public transit demand. Common factors that can influence urban public transit demand can be internal and/or external factors. Internal factors include policy measures such as transit fares, service headways, and travel times. External factors can include geographic, socioeconomic, and highway facility characteristics. There is inherent simultaneity between transit supply and demand, thus a two-stage least squares (2SLS) regression modeling procedure should be conducted to forecast urban transit supply and demand. As such, two multiple linear regression models should be developed: one to predict transit supply and a second to predict transit demand. It was found that service area density, total average cost per trip, and the average number of vehicles operated in maximum service can be used to forecast transit supply, expressed as vehicle revenue hours. Furthermore, estimated vehicle revenue hours and total average fares per trip can be used to forecast transit demand, expressed as unlinked passenger trips. Additional data such as socioeconomic information of the surrounding areas for each transit agency and travel time information of the various transit systems would be useful to improve upon the models developed.



### CAR -- Cityscapes Attributes Recognition A Multi-category Attributes Dataset for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2111.08243v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2111.08243v1)
- **Published**: 2021-11-16 06:00:43+00:00
- **Updated**: 2021-11-16 06:00:43+00:00
- **Authors**: Kareem Metwaly, Aerin Kim, Elliot Branson, Vishal Monga
- **Comment**: None
- **Journal**: None
- **Summary**: Self-driving vehicles are the future of transportation. With current advancements in this field, the world is getting closer to safe roads with almost zero probability of having accidents and eliminating human errors. However, there is still plenty of research and development necessary to reach a level of robustness. One important aspect is to understand a scene fully including all details. As some characteristics (attributes) of objects in a scene (drivers' behavior for instance) could be imperative for correct decision making. However, current algorithms suffer from low-quality datasets with such rich attributes. Therefore, in this paper, we present a new dataset for attributes recognition -- Cityscapes Attributes Recognition (CAR). The new dataset extends the well-known dataset Cityscapes by adding an additional yet important annotation layer of attributes of objects in each image. Currently, we have annotated more than 32k instances of various categories (Vehicles, Pedestrians, etc.). The dataset has a structured and tailored taxonomy where each category has its own set of possible attributes. The tailored taxonomy focuses on attributes that is of most beneficent for developing better self-driving algorithms that depend on accurate computer vision and scene comprehension. We have also created an API for the dataset to ease the usage of CAR. The API can be accessed through https://github.com/kareem-metwaly/CAR-API.



### Bengali Handwritten Grapheme Classification: Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2111.08249v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.08249v1)
- **Published**: 2021-11-16 06:14:59+00:00
- **Updated**: 2021-11-16 06:14:59+00:00
- **Authors**: Tarun Roy, Hasib Hasan, Kowsar Hossain, Masuma Akter Rumi
- **Comment**: 8 pages, 15 figures, pre-print
- **Journal**: None
- **Summary**: Despite being one of the most spoken languages in the world ($6^{th}$ based on population), research regarding Bengali handwritten grapheme (smallest functional unit of a writing system) classification has not been explored widely compared to other prominent languages. Moreover, the large number of combinations of graphemes in the Bengali language makes this classification task very challenging. With an effort to contribute to this research problem, we participate in a Kaggle competition \cite{kaggle_link} where the challenge is to separately classify three constituent elements of a Bengali grapheme in the image: grapheme root, vowel diacritics, and consonant diacritics. We explore the performances of some existing neural network models such as Multi-Layer Perceptron (MLP) and state of the art ResNet50. To further improve the performance we propose our own convolution neural network (CNN) model for Bengali grapheme classification with validation root accuracy 95.32\%, vowel accuracy 98.61\%, and consonant accuracy 98.76\%. We also explore Region Proposal Network (RPN) using VGGNet with a limited setting that can be a potential future direction to improve the performance.



### Enabling equivariance for arbitrary Lie groups
- **Arxiv ID**: http://arxiv.org/abs/2111.08251v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2111.08251v2)
- **Published**: 2021-11-16 06:23:44+00:00
- **Updated**: 2022-03-30 09:25:40+00:00
- **Authors**: Lachlan Ewen MacDonald, Sameera Ramasinghe, Simon Lucey
- **Comment**: Oral presentation at the Conference on Computer Vision and Pattern
  Recognition (CVPR), 2022
- **Journal**: None
- **Summary**: Although provably robust to translational perturbations, convolutional neural networks (CNNs) are known to suffer from extreme performance degradation when presented at test time with more general geometric transformations of inputs. Recently, this limitation has motivated a shift in focus from CNNs to Capsule Networks (CapsNets). However, CapsNets suffer from admitting relatively few theoretical guarantees of invariance. We introduce a rigourous mathematical framework to permit invariance to any Lie group of warps, exclusively using convolutions (over Lie groups), without the need for capsules. Previous work on group convolutions has been hampered by strong assumptions about the group, which precludes the application of such techniques to common warps in computer vision such as affine and homographic. Our framework enables the implementation of group convolutions over any finite-dimensional Lie group. We empirically validate our approach on the benchmark affine-invariant classification task, where we achieve 30% improvement in accuracy against conventional CNNs while outperforming most CapsNets. As further illustration of the generality of our framework, we train a homography-convolutional model which achieves superior robustness on a homography-perturbed dataset, where CapsNet results degrade.



### Online Meta Adaptation for Variable-Rate Learned Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2111.08256v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08256v1)
- **Published**: 2021-11-16 06:46:23+00:00
- **Updated**: 2021-11-16 06:46:23+00:00
- **Authors**: Wei Jiang, Wei Wang, Songnan Li, Shan Liu
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: This work addresses two major issues of end-to-end learned image compression (LIC) based on deep neural networks: variable-rate learning where separate networks are required to generate compressed images with varying qualities, and the train-test mismatch between differentiable approximate quantization and true hard quantization. We introduce an online meta-learning (OML) setting for LIC, which combines ideas from meta learning and online learning in the conditional variational auto-encoder (CVAE) framework. By treating the conditional variables as meta parameters and treating the generated conditional features as meta priors, the desired reconstruction can be controlled by the meta parameters to accommodate compression with variable qualities. The online learning framework is used to update the meta parameters so that the conditional reconstruction is adaptively tuned for the current image. Through the OML mechanism, the meta parameters can be effectively updated through SGD. The conditional reconstruction is directly based on the quantized latent representation in the decoder network, and therefore helps to bridge the gap between the training estimation and true quantized latent distribution. Experiments demonstrate that our OML approach can be flexibly applied to different state-of-the-art LIC methods to achieve additional performance improvements with little computation and transmission overhead.



### Pose Recognition in the Wild: Animal pose estimation using Agglomerative Clustering and Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.08259v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.08259v1)
- **Published**: 2021-11-16 07:00:31+00:00
- **Updated**: 2021-11-16 07:00:31+00:00
- **Authors**: Samayan Bhattacharya, Sk Shahnawaz
- **Comment**: 9 pages, 4 figures, 3 tables
- **Journal**: None
- **Summary**: Animal pose estimation has recently come into the limelight due to its application in biology, zoology, and aquaculture. Deep learning methods have effectively been applied to human pose estimation. However, the major bottleneck to the application of these methods to animal pose estimation is the unavailability of sufficient quantities of labeled data. Though there are ample quantities of unlabelled data publicly available, it is economically impractical to label large quantities of data for each animal. In addition, due to the wide variety of body shapes in the animal kingdom, the transfer of knowledge across domains is ineffective. Given the fact that the human brain is able to recognize animal pose without requiring large amounts of labeled data, it is only reasonable that we exploit unsupervised learning to tackle the problem of animal pose recognition from the available, unlabelled data. In this paper, we introduce a novel architecture that is able to recognize the pose of multiple animals fromunlabelled data. We do this by (1) removing background information from each image and employing an edge detection algorithm on the body of the animal, (2) Tracking motion of the edge pixels and performing agglomerative clustering to segment body parts, (3) employing contrastive learning to discourage grouping of distant body parts together. Hence we are able to distinguish between body parts of the animal, based on their visual behavior, instead of the underlying anatomy. Thus, we are able to achieve a more effective classification of the data than their human-labeled counterparts. We test our model on the TigDog and WLD (WildLife Documentary) datasets, where we outperform state-of-the-art approaches by a significant margin. We also study the performance of our model on other public data to demonstrate the generalization ability of our model.



### Data Augmentation using Random Image Cropping for High-resolution Virtual Try-On (VITON-CROP)
- **Arxiv ID**: http://arxiv.org/abs/2111.08270v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08270v1)
- **Published**: 2021-11-16 07:40:16+00:00
- **Updated**: 2021-11-16 07:40:16+00:00
- **Authors**: Taewon Kang, Sunghyun Park, Seunghwan Choi, Jaegul Choo
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: Image-based virtual try-on provides the capacity to transfer a clothing item onto a photo of a given person, which is usually accomplished by warping the item to a given human pose and adjusting the warped item to the person. However, the results of real-world synthetic images (e.g., selfies) from the previous method is not realistic because of the limitations which result in the neck being misrepresented and significant changes to the style of the garment. To address these challenges, we propose a novel method to solve this unique issue, called VITON-CROP. VITON-CROP synthesizes images more robustly when integrated with random crop augmentation compared to the existing state-of-the-art virtual try-on models. In the experiments, we demonstrate that VITON-CROP is superior to VITON-HD both qualitatively and quantitatively.



### Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts
- **Arxiv ID**: http://arxiv.org/abs/2111.08276v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08276v3)
- **Published**: 2021-11-16 07:55:26+00:00
- **Updated**: 2022-06-01 16:45:09+00:00
- **Authors**: Yan Zeng, Xinsong Zhang, Hang Li
- **Comment**: ICML 2022
- **Journal**: None
- **Summary**: Most existing methods in vision language pre-training rely on object-centric features extracted through object detection and make fine-grained alignments between the extracted features and texts. It is challenging for these methods to learn relations among multiple objects. To this end, we propose a new method called X-VLM to perform `multi-grained vision language pre-training.' The key to learning multi-grained alignments is to locate visual concepts in the image given the associated texts, and in the meantime align the texts with the visual concepts, where the alignments are in multi-granularity. Experimental results show that X-VLM effectively leverages the learned multi-grained alignments to many downstream vision language tasks and consistently outperforms state-of-the-art methods.



### Keypoint Message Passing for Video-based Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2111.08279v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08279v2)
- **Published**: 2021-11-16 08:01:16+00:00
- **Updated**: 2021-12-13 06:27:54+00:00
- **Authors**: Di Chen, Andreas Doering, Shanshan Zhang, Jian Yang, Juergen Gall, Bernt Schiele
- **Comment**: To appear in AAAI 2022
- **Journal**: None
- **Summary**: Video-based person re-identification (re-ID) is an important technique in visual surveillance systems which aims to match video snippets of people captured by different cameras. Existing methods are mostly based on convolutional neural networks (CNNs), whose building blocks either process local neighbor pixels at a time, or, when 3D convolutions are used to model temporal information, suffer from the misalignment problem caused by person movement. In this paper, we propose to overcome the limitations of normal convolutions with a human-oriented graph method. Specifically, features located at person joint keypoints are extracted and connected as a spatial-temporal graph. These keypoint features are then updated by message passing from their connected nodes with a graph convolutional network (GCN). During training, the GCN can be attached to any CNN-based person re-ID model to assist representation learning on feature maps, whilst it can be dropped after training for better inference speed. Our method brings significant improvements over the CNN-based baseline model on the MARS dataset with generated person keypoints and a newly annotated dataset: PoseTrackReID. It also defines a new state-of-the-art method in terms of top-1 accuracy and mean average precision in comparison to prior works.



### Self-supervised Re-renderable Facial Albedo Reconstruction from Single Image
- **Arxiv ID**: http://arxiv.org/abs/2111.08282v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2111.08282v2)
- **Published**: 2021-11-16 08:10:24+00:00
- **Updated**: 2022-06-06 08:52:25+00:00
- **Authors**: Mingxin Yang, Jianwei Guo, Zhanglin Cheng, Xiaopeng Zhang, Dong-Ming Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstructing high-fidelity 3D facial texture from a single image is a quite challenging task due to the lack of complete face information and the domain gap between the 3D face and 2D image. Further, obtaining re-renderable 3D faces has become a strongly desired property in many applications, where the term 're-renderable' demands the facial texture to be spatially complete and disentangled with environmental illumination. In this paper, we propose a new self-supervised deep learning framework for reconstructing high-quality and re-renderable facial albedos from single-view images in-the-wild. Our main idea is to first utilize a prior generation module based on the 3DMM proxy model to produce an unwrapped texture and a globally parameterized prior albedo. Then we apply a detail refinement module to synthesize the final texture with both high-frequency details and completeness. To further make facial textures disentangled with illumination, we propose a novel detailed illumination representation which is reconstructed with the detailed albedo together. We also design several novel regularization losses on both the albedo and illumination maps to facilitate the disentanglement of these two factors. Finally, by leveraging a differentiable renderer, each face attribute can be jointly trained in a self-supervised manner without requiring ground-truth facial reflectance. Extensive comparisons and ablation studies on challenging datasets demonstrate that our framework outperforms state-of-the-art approaches.



### NENet: Monocular Depth Estimation via Neural Ensembles
- **Arxiv ID**: http://arxiv.org/abs/2111.08313v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08313v1)
- **Published**: 2021-11-16 09:09:05+00:00
- **Updated**: 2021-11-16 09:09:05+00:00
- **Authors**: Shuwei Shao, Ran Li, Zhongcai Pei, Zhong Liu, Weihai Chen, Wentao Zhu, Xingming Wu, Baochang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Depth estimation is getting a widespread popularity in the computer vision community, and it is still quite difficult to recover an accurate depth map using only one single RGB image. In this work, we observe a phenomenon that existing methods tend to exhibit asymmetric errors, which might open up a new direction for accurate and robust depth estimation. We carefully investigate into the phenomenon, and construct a two-level ensemble scheme, NENet, to integrate multiple predictions from diverse base predictors. The NENet forms a more reliable depth estimator, which substantially boosts the performance over base predictors. Notably, this is the first attempt to introduce ensemble learning and evaluate its utility for monocular depth estimation to the best of our knowledge. Extensive experiments demonstrate that the proposed NENet achieves better results than previous state-of-the-art approaches on the NYU-Depth-v2 and KITTI datasets. In particular, our method improves previous state-of-the-art methods from 0.365 to 0.349 on the metric RMSE on the NYU dataset. To validate the generalizability across cameras, we directly apply the models trained on the NYU dataset to the SUN RGB-D dataset without any fine-tuning, and achieve the superior results, which indicate its strong generalizability. The source code and trained models will be publicly available upon the acceptance.



### TRIG: Transformer-Based Text Recognizer with Initial Embedding Guidance
- **Arxiv ID**: http://arxiv.org/abs/2111.08314v1
- **DOI**: 10.3390/electronics10222780
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08314v1)
- **Published**: 2021-11-16 09:10:39+00:00
- **Updated**: 2021-11-16 09:10:39+00:00
- **Authors**: Yue Tao, Zhiwei Jia, Runze Ma, Shugong Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Scene text recognition (STR) is an important bridge between images and text, attracting abundant research attention. While convolutional neural networks (CNNS) have achieved remarkable progress in this task, most of the existing works need an extra module (context modeling module) to help CNN to capture global dependencies to solve the inductive bias and strengthen the relationship between text features. Recently, the transformer has been proposed as a promising network for global context modeling by self-attention mechanism, but one of the main shortcomings, when applied to recognition, is the efficiency. We propose a 1-D split to address the challenges of complexity and replace the CNN with the transformer encoder to reduce the need for a context modeling module. Furthermore, recent methods use a frozen initial embedding to guide the decoder to decode the features to text, leading to a loss of accuracy. We propose to use a learnable initial embedding learned from the transformer encoder to make it adaptive to different input images. Above all, we introduce a novel architecture for text recognition, named TRansformer-based text recognizer with Initial embedding Guidance (TRIG), composed of three stages (transformation, feature extraction, and prediction). Extensive experiments show that our approach can achieve state-of-the-art on text recognition benchmarks.



### DRINet++: Efficient Voxel-as-point Point Cloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.08318v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2111.08318v1)
- **Published**: 2021-11-16 09:22:15+00:00
- **Updated**: 2021-11-16 09:22:15+00:00
- **Authors**: Maosheng Ye, Rui Wan, Shuangjie Xu, Tongyi Cao, Qifeng Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, many approaches have been proposed through single or multiple representations to improve the performance of point cloud semantic segmentation. However, these works do not maintain a good balance among performance, efficiency, and memory consumption. To address these issues, we propose DRINet++ that extends DRINet by enhancing the sparsity and geometric properties of a point cloud with a voxel-as-point principle. To improve efficiency and performance, DRINet++ mainly consists of two modules: Sparse Feature Encoder and Sparse Geometry Feature Enhancement. The Sparse Feature Encoder extracts the local context information for each point, and the Sparse Geometry Feature Enhancement enhances the geometric properties of a sparse point cloud via multi-scale sparse projection and attentive multi-scale fusion. In addition, we propose deep sparse supervision in the training phase to help convergence and alleviate the memory consumption problem. Our DRINet++ achieves state-of-the-art outdoor point cloud segmentation on both SemanticKITTI and Nuscenes datasets while running significantly faster and consuming less memory.



### Which CNNs and Training Settings to Choose for Action Unit Detection? A Study Based on a Large-Scale Dataset
- **Arxiv ID**: http://arxiv.org/abs/2111.08320v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08320v1)
- **Published**: 2021-11-16 09:28:32+00:00
- **Updated**: 2021-11-16 09:28:32+00:00
- **Authors**: Mina Bishay, Ahmed Ghoneim, Mohamed Ashraf, Mohammad Mavadati
- **Comment**: FG 2021
- **Journal**: None
- **Summary**: In this paper we explore the influence of some frequently used Convolutional Neural Networks (CNNs), training settings, and training set structures, on Action Unit (AU) detection. Specifically, we first compare 10 different shallow and deep CNNs in AU detection. Second, we investigate how the different training settings (i.e. centering/normalizing the inputs, using different augmentation severities, and balancing the data) impact the performance in AU detection. Third, we explore the effect of increasing the number of labelled subjects and frames in the training set on the AU detection performance. These comparisons provide the research community with useful tips about the choice of different CNNs and training settings in AU detection. In our analysis, we use a large-scale naturalistic dataset, consisting of ~55K videos captured in the wild. To the best of our knowledge, there is no work that had investigated the impact of such settings on a large-scale AU dataset.



### Choose Settings Carefully: Comparing Action Unit detection at Different Settings Using a Large-Scale Dataset
- **Arxiv ID**: http://arxiv.org/abs/2111.08324v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08324v1)
- **Published**: 2021-11-16 09:44:41+00:00
- **Updated**: 2021-11-16 09:44:41+00:00
- **Authors**: Mina Bishay, Ahmed Ghoneim, Mohamed Ashraf, Mohammad Mavadati
- **Comment**: ICIP 2021
- **Journal**: None
- **Summary**: In this paper, we investigate the impact of some of the commonly used settings for (a) preprocessing face images, and (b) classification and training, on Action Unit (AU) detection performance and complexity. We use in our investigation a large-scale dataset, consisting of ~55K videos collected in the wild for participants watching commercial ads. The preprocessing settings include scaling the face to a fixed resolution, changing the color information (RGB to gray-scale), aligning the face, and cropping AU regions, while the classification and training settings include the kind of classifier (multi-label vs. binary) and the amount of data used for training models. To the best of our knowledge, no work had investigated the effect of those settings on AU detection. In our analysis we use CNNs as our baseline classification model.



### Pansharpening by convolutional neural networks in the full resolution framework
- **Arxiv ID**: http://arxiv.org/abs/2111.08334v3
- **DOI**: 10.1109/TGRS.2022.3163887
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08334v3)
- **Published**: 2021-11-16 10:03:11+00:00
- **Updated**: 2022-04-04 13:02:58+00:00
- **Authors**: Matteo Ciotola, Sergio Vitale, Antonio Mazza, Giovanni Poggi, Giuseppe Scarpa
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing
- **Summary**: In recent years, there has been a growing interest in deep learning-based pansharpening. Thus far, research has mainly focused on architectures. Nonetheless, model training is an equally important issue. A first problem is the absence of ground truths, unavoidable in pansharpening. This is often addressed by training networks in a reduced resolution domain and using the original data as ground truth, relying on an implicit scale invariance assumption. However, on full resolution images results are often disappointing, suggesting such invariance not to hold. A further problem is the scarcity of training data, which causes a limited generalization ability and a poor performance on off-training test images. In this paper, we propose a full-resolution training framework for deep learning-based pansharpening. The framework is fully general and can be used for any deep learning-based pansharpening model. Training takes place in the high-resolution domain, relying only on the original data, thus avoiding any loss of information. To ensure spectral and spatial fidelity, a suitable two-component loss is defined. The spectral component enforces consistency between the pansharpened output and the low-resolution multispectral input. The spatial component, computed at high-resolution, maximizes the local correlation between each pansharpened band and the panchromatic input. At testing time, the target-adaptive operating modality is adopted, achieving good generalization with a limited computational overhead. Experiments carried out on WorldView-3, WorldView-2, and GeoEye-1 images show that methods trained with the proposed framework guarantee a pretty good performance in terms of both full-resolution numerical indexes and visual quality.



### SEnSeI: A Deep Learning Module for Creating Sensor Independent Cloud Masks
- **Arxiv ID**: http://arxiv.org/abs/2111.08349v1
- **DOI**: 10.1109/TGRS.2021.3128280
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2111.08349v1)
- **Published**: 2021-11-16 10:47:10+00:00
- **Updated**: 2021-11-16 10:47:10+00:00
- **Authors**: Alistair Francis, John Mrziglod, Panagiotis Sidiropoulos, Jan-Peter Muller
- **Comment**: 22 pages, 7 figures. This is an accepted version of work to be
  published in the IEEE Transactions on Geoscience and Remote Sensing
- **Journal**: None
- **Summary**: We introduce a novel neural network architecture -- Spectral ENcoder for SEnsor Independence (SEnSeI) -- by which several multispectral instruments, each with different combinations of spectral bands, can be used to train a generalised deep learning model. We focus on the problem of cloud masking, using several pre-existing datasets, and a new, freely available dataset for Sentinel-2. Our model is shown to achieve state-of-the-art performance on the satellites it was trained on (Sentinel-2 and Landsat 8), and is able to extrapolate to sensors it has not seen during training such as Landsat 7, Per\'uSat-1, and Sentinel-3 SLSTR. Model performance is shown to improve when multiple satellites are used in training, approaching or surpassing the performance of specialised, single-sensor models. This work is motivated by the fact that the remote sensing community has access to data taken with a hugely variety of sensors. This has inevitably led to labelling efforts being undertaken separately for different sensors, which limits the performance of deep learning models, given their need for huge training sets to perform optimally. Sensor independence can enable deep learning models to utilise multiple datasets for training simultaneously, boosting performance and making them much more widely applicable. This may lead to deep learning approaches being used more frequently for on-board applications and in ground segment data processing, which generally require models to be ready at launch or soon afterwards.



### Image-specific Convolutional Kernel Modulation for Single Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2111.08362v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08362v1)
- **Published**: 2021-11-16 11:05:10+00:00
- **Updated**: 2021-11-16 11:05:10+00:00
- **Authors**: Yuanfei Huang, Jie Li, Yanting Hu, Xinbo Gao, Hua Huang
- **Comment**: 13 pages, submitted to IEEE Transactions, codes are available at
  https://github.com/YuanfeiHuang/IKM
- **Journal**: None
- **Summary**: Recently, deep-learning-based super-resolution methods have achieved excellent performances, but mainly focus on training a single generalized deep network by feeding numerous samples. Yet intuitively, each image has its representation, and is expected to acquire an adaptive model. For this issue, we propose a novel image-specific convolutional kernel modulation (IKM) by exploiting the global contextual information of image or feature to generate an attention weight for adaptively modulating the convolutional kernels, which outperforms the vanilla convolution and several existing attention mechanisms while embedding into the state-of-the-art architectures without any additional parameters. Particularly, to optimize our IKM in mini-batch training, we introduce an image-specific optimization (IsO) algorithm, which is more effective than the conventional mini-batch SGD optimization. Furthermore, we investigate the effect of IKM on the state-of-the-art architectures and exploit a new backbone with U-style residual learning and hourglass dense block learning, terms U-Hourglass Dense Network (U-HDN), which is an appropriate architecture to utmost improve the effectiveness of IKM theoretically and experimentally. Extensive experiments on single image super-resolution show that the proposed methods achieve superior performances over state-of-the-art methods. Code is available at github.com/YuanfeiHuang/IKM.



### Fight Detection from Still Images in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2111.08370v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08370v2)
- **Published**: 2021-11-16 11:16:11+00:00
- **Updated**: 2021-11-17 09:49:06+00:00
- **Authors**: Şeymanur Aktı, Ferda Ofli, Muhammad Imran, Hazım Kemal Ekenel
- **Comment**: Accepted for publication at Winter Conference of Applications on
  Computer Vision Workshops (WACV-W 2022), Workshop on Real-World Surveillance:
  Applications and Challenges
- **Journal**: None
- **Summary**: Detecting fights from still images shared on social media is an important task required to limit the distribution of violent scenes in order to prevent their negative effects. For this reason, in this study, we address the problem of fight detection from still images collected from the web and social media. We explore how well one can detect fights from just a single still image. We also propose a new dataset, named Social Media Fight Images (SMFI), comprising real-world images of fight actions. Results of the extensive experiments on the proposed dataset show that fight actions can be recognized successfully from still images. That is, even without exploiting the temporal information, it is possible to detect fights with high accuracy by utilizing appearance only. We also perform cross-dataset experiments to evaluate the representation capacity of the collected dataset. These experiments indicate that, as in the other computer vision problems, there exists a dataset bias for the fight recognition problem. Although the methods achieve close to 100% accuracy when trained and tested on the same fight dataset, the cross-dataset accuracies are significantly lower, i.e., around 70% when more representative datasets are used for training. SMFI dataset is found to be one of the two most representative datasets among the utilized five fight datasets.



### Single Image Object Counting and Localizing using Active-Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.08383v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08383v1)
- **Published**: 2021-11-16 11:29:21+00:00
- **Updated**: 2021-11-16 11:29:21+00:00
- **Authors**: Inbar Huberman-Spiegelglas, Raanan Fattal
- **Comment**: Published in IEEE Winter Conference on Applications of Computer
  Vision (WACV) 2022
- **Journal**: None
- **Summary**: The need to count and localize repeating objects in an image arises in different scenarios, such as biological microscopy studies, production lines inspection, and surveillance recordings analysis. The use of supervised Convoutional Neural Networks (CNNs) achieves accurate object detection when trained over large class-specific datasets. The labeling effort in this approach does not pay-off when the counting is required over few images of a unique object class.   We present a new method for counting and localizing repeating objects in single-image scenarios, assuming no pre-trained classifier is available. Our method trains a CNN over a small set of labels carefully collected from the input image in few active-learning iterations. At each iteration, the latent space of the network is analyzed to extract a minimal number of user-queries that strives to both sample the in-class manifold as thoroughly as possible as well as avoid redundant labels.   Compared with existing user-assisted counting methods, our active-learning iterations achieve state-of-the-art performance in terms of counting and localizing accuracy, number of user mouse clicks, and running-time. This evaluation was performed through a large user study over a wide range of image classes with diverse conditions of illumination and occlusions.



### 2.5D Vehicle Odometry Estimation
- **Arxiv ID**: http://arxiv.org/abs/2111.08398v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08398v1)
- **Published**: 2021-11-16 11:54:34+00:00
- **Updated**: 2021-11-16 11:54:34+00:00
- **Authors**: Ciaran Eising, Leroy-Francisco Pereira, Jonathan Horgan, Anbuchezhiyan Selvaraju, John McDonald, Paul Moran
- **Comment**: 13 pages, 16 figures, 2 tables
- **Journal**: IET Intelligent Transport Systems, 2020
- **Summary**: It is well understood that in ADAS applications, a good estimate of the pose of the vehicle is required. This paper proposes a metaphorically named 2.5D odometry, whereby the planar odometry derived from the yaw rate sensor and four wheel speed sensors is augmented by a linear model of suspension. While the core of the planar odometry is a yaw rate model that is already understood in the literature, we augment this by fitting a quadratic to the incoming signals, enabling interpolation, extrapolation, and a finer integration of the vehicle position. We show, by experimental results with a DGPS/IMU reference, that this model provides highly accurate odometry estimates, compared with existing methods. Utilising sensors that return the change in height of vehicle reference points with changing suspension configurations, we define a planar model of the vehicle suspension, thus augmenting the odometry model. We present an experimental framework and evaluations criteria by which the goodness of the odometry is evaluated and compared with existing methods. This odometry model has been designed to support low-speed surround-view camera systems that are well-known. Thus, we present some application results that show a performance boost for viewing and computer vision applications using the proposed odometry



### Weakly-supervised fire segmentation by visualizing intermediate CNN layers
- **Arxiv ID**: http://arxiv.org/abs/2111.08401v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08401v1)
- **Published**: 2021-11-16 11:56:28+00:00
- **Updated**: 2021-11-16 11:56:28+00:00
- **Authors**: Milad Niknejad, Alexandre Bernardino
- **Comment**: None
- **Journal**: None
- **Summary**: Fire localization in images and videos is an important step for an autonomous system to combat fire incidents. State-of-art image segmentation methods based on deep neural networks require a large number of pixel-annotated samples to train Convolutional Neural Networks (CNNs) in a fully-supervised manner. In this paper, we consider weakly supervised segmentation of fire in images, in which only image labels are used to train the network. We show that in the case of fire segmentation, which is a binary segmentation problem, the mean value of features in a mid-layer of classification CNN can perform better than conventional Class Activation Mapping (CAM) method. We also propose to further improve the segmentation accuracy by adding a rotation equivariant regularization loss on the features of the last convolutional layer. Our results show noticeable improvements over baseline method for weakly-supervised fire segmentation.



### Grounding Psychological Shape Space in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2111.08409v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08409v1)
- **Published**: 2021-11-16 12:21:07+00:00
- **Updated**: 2021-11-16 12:21:07+00:00
- **Authors**: Lucas Bechberger, Kai-Uwe Kühnberger
- **Comment**: accepted at CIFMA2021 (https://cifma.github.io/)
- **Journal**: None
- **Summary**: Shape information is crucial for human perception and cognition, and should therefore also play a role in cognitive AI systems. We employ the interdisciplinary framework of conceptual spaces, which proposes a geometric representation of conceptual knowledge through low-dimensional interpretable similarity spaces. These similarity spaces are often based on psychological dissimilarity ratings for a small set of stimuli, which are then transformed into a spatial representation by a technique called multidimensional scaling. Unfortunately, this approach is incapable of generalizing to novel stimuli. In this paper, we use convolutional neural networks to learn a generalizable mapping between perceptual inputs (pixels of grayscale line drawings) and a recently proposed psychological similarity space for the shape domain. We investigate different network architectures (classification network vs. autoencoder) and different training regimes (transfer learning vs. multi-task learning). Our results indicate that a classification-based multi-task learning scenario yields the best results, but that its performance is relatively sensitive to the dimensionality of the similarity space.



### Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding
- **Arxiv ID**: http://arxiv.org/abs/2111.08413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08413v1)
- **Published**: 2021-11-16 12:32:03+00:00
- **Updated**: 2021-11-16 12:32:03+00:00
- **Authors**: Bum Jun Kim, Hyeyeon Choi, Hyeonah Jang, Dong Gu Lee, Wonseok Jeong, Sang Woo Kim
- **Comment**: 7 pages, 8 figures. Work in Progress
- **Journal**: None
- **Summary**: Vision transformers (ViTs) have recently demonstrated state-of-the-art performance in a variety of vision tasks, replacing convolutional neural networks (CNNs). Meanwhile, since ViT has a different architecture than CNN, it may behave differently. To investigate the reliability of ViT, this paper studies the behavior and robustness of ViT. We compared the robustness of CNN and ViT by assuming various image corruptions that may appear in practical vision tasks. We confirmed that for most image transformations, ViT showed robustness comparable to CNN or more improved. However, for contrast enhancement, severe performance degradations were consistently observed in ViT. From a detailed analysis, we identified a potential problem: positional embedding in ViT's patch embedding could work improperly when the color scale changes. Here we claim the use of PreLayerNorm, a modified patch embedding structure to ensure scale-invariant behavior of ViT. ViT with PreLayerNorm showed improved robustness in various corruptions including contrast-varying environments.



### Automated Atlas-based Segmentation of Single Coronal Mouse Brain Slices using Linear 2D-2D Registration
- **Arxiv ID**: http://arxiv.org/abs/2111.08705v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08705v1)
- **Published**: 2021-11-16 12:33:09+00:00
- **Updated**: 2021-11-16 12:33:09+00:00
- **Authors**: Sébastien Piluso, Nicolas Souedet, Caroline Jan, Cédric Clouchoux, Thierry Delzescaux
- **Comment**: None
- **Journal**: None
- **Summary**: A significant challenge for brain histological data analysis is to precisely identify anatomical regions in order to perform accurate local quantifications and evaluate therapeutic solutions. Usually, this task is performed manually, becoming therefore tedious and subjective. Another option is to use automatic or semi-automatic methods, among which segmentation using digital atlases co-registration. However, most available atlases are 3D, whereas digitized histological data are 2D. Methods to perform such 2D-3D segmentation from an atlas are required. This paper proposes a strategy to automatically and accurately segment single 2D coronal slices within a 3D volume of atlas, using linear registration. We validated its robustness and performance using an exploratory approach at whole-brain scale.



### Delta-GAN-Encoder: Encoding Semantic Changes for Explicit Image Editing, using Few Synthetic Samples
- **Arxiv ID**: http://arxiv.org/abs/2111.08419v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08419v2)
- **Published**: 2021-11-16 12:42:04+00:00
- **Updated**: 2021-11-17 11:02:33+00:00
- **Authors**: Nir Diamant, Nitsan Sandor, Alex M Bronstein
- **Comment**: 8 pages, 13 figures
- **Journal**: None
- **Summary**: Understating and controlling generative models' latent space is a complex task.   In this paper, we propose a novel method for learning to control any desired attribute in a pre-trained GAN's latent space, for the purpose of editing synthesized and real-world data samples accordingly.   We perform Sim2Real learning, relying on minimal samples to achieve an unlimited amount of continuous precise edits.   We present an Autoencoder-based model that learns to encode the semantics of changes between images as a basis for editing new samples later on, achieving precise desired results - example shown in Fig. 1.   While previous editing methods rely on a known structure of latent spaces (e.g., linearity of some semantics in StyleGAN), our method inherently does not require any structural constraints.   We demonstrate our method in the domain of facial imagery: editing different expressions, poses, and lighting attributes, achieving state-of-the-art results.



### An Overview of Backdoor Attacks Against Deep Neural Networks and Possible Defences
- **Arxiv ID**: http://arxiv.org/abs/2111.08429v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08429v1)
- **Published**: 2021-11-16 13:06:31+00:00
- **Updated**: 2021-11-16 13:06:31+00:00
- **Authors**: Wei Guo, Benedetta Tondi, Mauro Barni
- **Comment**: None
- **Journal**: None
- **Summary**: Together with impressive advances touching every aspect of our society, AI technology based on Deep Neural Networks (DNN) is bringing increasing security concerns. While attacks operating at test time have monopolised the initial attention of researchers, backdoor attacks, exploiting the possibility of corrupting DNN models by interfering with the training process, represents a further serious threat undermining the dependability of AI techniques. In a backdoor attack, the attacker corrupts the training data so to induce an erroneous behaviour at test time. Test time errors, however, are activated only in the presence of a triggering event corresponding to a properly crafted input sample. In this way, the corrupted network continues to work as expected for regular inputs, and the malicious behaviour occurs only when the attacker decides to activate the backdoor hidden within the network. In the last few years, backdoor attacks have been the subject of an intense research activity focusing on both the development of new classes of attacks, and the proposal of possible countermeasures. The goal of this overview paper is to review the works published until now, classifying the different types of attacks and defences proposed so far. The classification guiding the analysis is based on the amount of control that the attacker has on the training process, and the capability of the defender to verify the integrity of the data used for training, and to monitor the operations of the DNN at training and test time. As such, the proposed analysis is particularly suited to highlight the strengths and weaknesses of both attacks and defences with reference to the application scenarios they are operating in.



### Code-free development and deployment of deep segmentation models for digital pathology
- **Arxiv ID**: http://arxiv.org/abs/2111.08430v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, eess.IV, I.4.6; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2111.08430v1)
- **Published**: 2021-11-16 13:08:05+00:00
- **Updated**: 2021-11-16 13:08:05+00:00
- **Authors**: Henrik Sahlin Pettersen, Ilya Belevich, Elin Synnøve Røyset, Erik Smistad, Eija Jokitalo, Ingerid Reinertsen, Ingunn Bakke, André Pedersen
- **Comment**: 18 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: Application of deep learning on histopathological whole slide images (WSIs) holds promise of improving diagnostic efficiency and reproducibility but is largely dependent on the ability to write computer code or purchase commercial solutions. We present a code-free pipeline utilizing free-to-use, open-source software (QuPath, DeepMIB, and FastPathology) for creating and deploying deep learning-based segmentation models for computational pathology. We demonstrate the pipeline on a use case of separating epithelium from stroma in colonic mucosa. A dataset of 251 annotated WSIs, comprising 140 hematoxylin-eosin (HE)-stained and 111 CD3 immunostained colon biopsy WSIs, were developed through active learning using the pipeline. On a hold-out test set of 36 HE and 21 CD3-stained WSIs a mean intersection over union score of 96.6% and 95.3% was achieved on epithelium segmentation. We demonstrate pathologist-level segmentation accuracy and clinical acceptable runtime performance and show that pathologists without programming experience can create near state-of-the-art segmentation solutions for histopathological WSIs using only free-to-use software. The study further demonstrates the strength of open-source solutions in its ability to create generalizable, open pipelines, of which trained models and predictions can seamlessly be exported in open formats and thereby used in external solutions. All scripts, trained models, a video tutorial, and the full dataset of 251 WSIs with ~31k epithelium annotations are made openly available at https://github.com/andreped/NoCodeSeg to accelerate research in the field.



### Robust 3D Scene Segmentation through Hierarchical and Learnable Part-Fusion
- **Arxiv ID**: http://arxiv.org/abs/2111.08434v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08434v1)
- **Published**: 2021-11-16 13:14:47+00:00
- **Updated**: 2021-11-16 13:14:47+00:00
- **Authors**: Anirud Thyagharajan, Benjamin Ummenhofer, Prashant Laddha, Om J Omer, Sreenivas Subramoney
- **Comment**: None
- **Journal**: None
- **Summary**: 3D semantic segmentation is a fundamental building block for several scene understanding applications such as autonomous driving, robotics and AR/VR. Several state-of-the-art semantic segmentation models suffer from the part misclassification problem, wherein parts of the same object are labelled incorrectly. Previous methods have utilized hierarchical, iterative methods to fuse semantic and instance information, but they lack learnability in context fusion, and are computationally complex and heuristic driven. This paper presents Segment-Fusion, a novel attention-based method for hierarchical fusion of semantic and instance information to address the part misclassifications. The presented method includes a graph segmentation algorithm for grouping points into segments that pools point-wise features into segment-wise features, a learnable attention-based network to fuse these segments based on their semantic and instance features, and followed by a simple yet effective connected component labelling algorithm to convert segment features to instance labels. Segment-Fusion can be flexibly employed with any network architecture for semantic/instance segmentation. It improves the qualitative and quantitative performance of several semantic segmentation backbones by upto 5% when evaluated on the ScanNet and S3DIS datasets.



### Point detection through multi-instance deep heatmap regression for sutures in endoscopy
- **Arxiv ID**: http://arxiv.org/abs/2111.08468v1
- **DOI**: 10.1007/s11548-021-02523-w
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.08468v1)
- **Published**: 2021-11-16 13:45:23+00:00
- **Updated**: 2021-11-16 13:45:23+00:00
- **Authors**: Lalith Sharan, Gabriele Romano, Julian Brand, Halvar Kelm, Matthias Karck, Raffaele De Simone, Sandy Engelhardt
- **Comment**: Accepted to International Journal of Computer Assisted Radiology and
  Surgery, 15 pages, 5 figures
- **Journal**: Int J CARS (2021) 1861-6429
- **Summary**: Purpose: Mitral valve repair is a complex minimally invasive surgery of the heart valve. In this context, suture detection from endoscopic images is a highly relevant task that provides quantitative information to analyse suturing patterns, assess prosthetic configurations and produce augmented reality visualisations. Facial or anatomical landmark detection tasks typically contain a fixed number of landmarks, and use regression or fixed heatmap-based approaches to localize the landmarks. However in endoscopy, there are a varying number of sutures in every image, and the sutures may occur at any location in the annulus, as they are not semantically unique. Method: In this work, we formulate the suture detection task as a multi-instance deep heatmap regression problem, to identify entry and exit points of sutures. We extend our previous work, and introduce the novel use of a 2D Gaussian layer followed by a differentiable 2D spatial Soft-Argmax layer to function as a local non-maximum suppression. Results: We present extensive experiments with multiple heatmap distribution functions and two variants of the proposed model. In the intra-operative domain, Variant 1 showed a mean F1 of +0.0422 over the baseline. Similarly, in the simulator domain, Variant 1 showed a mean F1 of +0.0865 over the baseline. Conclusion: The proposed model shows an improvement over the baseline in the intra-operative and the simulator domains. The data is made publicly available within the scope of the MICCAI AdaptOR2021 Challenge https://adaptor2021.github.io/, and the code at https://github.com/Cardio-AI/suture-detection-pytorch/. DOI:10.1007/s11548-021-02523-w. The link to the open access article can be found here: https://link.springer.com/article/10.1007%2Fs11548-021-02523-w



### Consistent Semantic Attacks on Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2111.08485v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08485v1)
- **Published**: 2021-11-16 14:05:07+00:00
- **Updated**: 2021-11-16 14:05:07+00:00
- **Authors**: Tom Koren, Lior Talker, Michael Dinerstein, Roy J Jevnisek
- **Comment**: Paper and supplementary material
- **Journal**: None
- **Summary**: We present a novel approach for semantically targeted adversarial attacks on Optical Flow. In such attacks the goal is to corrupt the flow predictions of a specific object category or instance. Usually, an attacker seeks to hide the adversarial perturbations in the input. However, a quick scan of the output reveals the attack. In contrast, our method helps to hide the attackers intent in the output as well. We achieve this thanks to a regularization term that encourages off-target consistency. We perform extensive tests on leading optical flow models to demonstrate the benefits of our approach in both white-box and black-box settings. Also, we demonstrate the effectiveness of our attack on subsequent tasks that depend on the optical flow.



### Automated skin lesion segmentation using multi-scale feature extraction scheme and dual-attention mechanism
- **Arxiv ID**: http://arxiv.org/abs/2111.08708v3
- **DOI**: 10.1109/ICAC3N53548.2021.9725739
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08708v3)
- **Published**: 2021-11-16 14:08:35+00:00
- **Updated**: 2022-06-07 06:22:53+00:00
- **Authors**: G Jignesh Chowdary, G V S N Durga Yathisha, Suganya G, Premalatha M
- **Comment**: None
- **Journal**: In 2021 3rd International Conference on Advances in Computing,
  Communication Control and Networking (ICAC3N) (pp. 1763-1771). IEEE (2021)
- **Summary**: Segmenting skin lesions from dermoscopic images is essential for diagnosing skin cancer. But the automatic segmentation of these lesions is complicated due to the poor contrast between the background and the lesion, image artifacts, and unclear lesion boundaries. In this work, we present a deep learning model for the segmentation of skin lesions from dermoscopic images. To deal with the challenges of skin lesion characteristics, we designed a multi-scale feature extraction module for extracting the discriminative features. Further in this work, two attention mechanisms are developed to refine the post-upsampled features and the features extracted by the encoder. This model is evaluated using the ISIC2018 and ISBI2017 datasets. The proposed model outperformed all the existing works and the top-ranked models in two competitions.



### SequentialPointNet: A strong frame-level parallel point cloud sequence network for 3D action recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.08492v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08492v2)
- **Published**: 2021-11-16 14:13:32+00:00
- **Updated**: 2022-03-10 13:55:29+00:00
- **Authors**: Xing Li, Qian Huang, Zhijian Wang, Zhenjie Hou, Tianjin Yang
- **Comment**: None
- **Journal**: None
- **Summary**: The point cloud sequence of 3D human actions consists of a set of ordered point cloud frames. Compared to static point clouds, point cloud sequences have huge data sizes proportional to the time dimension. Therefore, developing an efficient and lightweight point cloud sequence model is pivotal for 3D action recognition. In this paper, we propose a strong frame-level parallel point cloud sequence network referred to as SequentialPointNet for 3D action recognition. The key to our approach is to divide the main modeling operations into frame-level units executed in parallel, which greatly improves the efficiency of modeling point cloud sequences.Moreover, we propose to flatten the point cloud sequence into a new point data type named hyperpoint sequence that preserves the complete spatial structure of each frame. Then, a novel Hyperpoint-Mixer module is introduced to mix intra-frame spatial features and inter-frame temporal features of the hyperpoint sequence. By doing so, SequentialPointNet maximizes the appearance encoding ability and extracts sufficient motion information for effective human action recognition. Extensive experiments show that SequentialPointNet achieves up to 10X faster than existing point cloud sequence models. Additionally, our SequentialPointNet surpasses state-of-the-art approaches for human action recognition on both large-scale datasets (i.e., NTU RGB+D 60 and NTU RGB+D 120) and small-scale datasets (i.e., MSR Action3D and UTD-MHAD).



### Learning Intrinsic Images for Clothing
- **Arxiv ID**: http://arxiv.org/abs/2111.08521v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08521v1)
- **Published**: 2021-11-16 14:43:12+00:00
- **Updated**: 2021-11-16 14:43:12+00:00
- **Authors**: Kuo Jiang, Zian Wang, Xiaodong Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstruction of human clothing is an important task and often relies on intrinsic image decomposition. With a lack of domain-specific data and coarse evaluation metrics, existing models failed to produce satisfying results for graphics applications. In this paper, we focus on intrinsic image decomposition for clothing images and have comprehensive improvements. We collected CloIntrinsics, a clothing intrinsic image dataset, including a synthetic training set and a real-world testing set. A more interpretable edge-aware metric and an annotation scheme is designed for the testing set, which allows diagnostic evaluation for intrinsic models. Finally, we propose ClothInNet model with carefully designed loss terms and an adversarial module. It utilizes easy-to-acquire labels to learn from real-world shading, significantly improves performance with only minor additional annotation effort. We show that our proposed model significantly reduce texture-copying artifacts while retaining surprisingly tiny details, outperforming existing state-of-the-art methods.



### Language bias in Visual Question Answering: A Survey and Taxonomy
- **Arxiv ID**: http://arxiv.org/abs/2111.08531v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.08531v1)
- **Published**: 2021-11-16 15:01:24+00:00
- **Updated**: 2021-11-16 15:01:24+00:00
- **Authors**: Desen Yuan
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Visual question answering (VQA) is a challenging task, which has attracted more and more attention in the field of computer vision and natural language processing. However, the current visual question answering has the problem of language bias, which reduces the robustness of the model and has an adverse impact on the practical application of visual question answering. In this paper, we conduct a comprehensive review and analysis of this field for the first time, and classify the existing methods according to three categories, including enhancing visual information, weakening language priors, data enhancement and training strategies. At the same time, the relevant representative methods are introduced, summarized and analyzed in turn. The causes of language bias are revealed and classified. Secondly, this paper introduces the datasets mainly used for testing, and reports the experimental results of various existing methods. Finally, we discuss the possible future research directions in this field.



### CNN Filter Learning from Drawn Markers for the Detection of Suggestive Signs of COVID-19 in CT Images
- **Arxiv ID**: http://arxiv.org/abs/2111.08710v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, J.3, J.6
- **Links**: [PDF](http://arxiv.org/pdf/2111.08710v1)
- **Published**: 2021-11-16 15:03:42+00:00
- **Updated**: 2021-11-16 15:03:42+00:00
- **Authors**: Azael M. Sousa, Fabiano Reis, Rachel Zerbini, João L. D. Comba, Alexandre X. Falcão
- **Comment**: 4 pages. To be published in the 43rd Annual International Conference
  of the IEEE Engineering in Medicine and Biology Society
- **Journal**: None
- **Summary**: Early detection of COVID-19 is vital to control its spread. Deep learning methods have been presented to detect suggestive signs of COVID-19 from chest CT images. However, due to the novelty of the disease, annotated volumetric data are scarce. Here we propose a method that does not require either large annotated datasets or backpropagation to estimate the filters of a convolutional neural network (CNN). For a few CT images, the user draws markers at representative normal and abnormal regions. The method generates a feature extractor composed of a sequence of convolutional layers, whose kernels are specialized in enhancing regions similar to the marked ones, and the decision layer of our CNN is a support vector machine. As we have no control over the CT image acquisition, we also propose an intensity standardization approach. Our method can achieve mean accuracy and kappa values of $0.97$ and $0.93$, respectively, on a dataset with 117 CT images extracted from different sites, surpassing its counterpart in all scenarios.



### Rethinking Keypoint Representations: Modeling Keypoints and Poses as Objects for Multi-Person Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2111.08557v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.08557v4)
- **Published**: 2021-11-16 15:36:44+00:00
- **Updated**: 2022-07-19 03:53:28+00:00
- **Authors**: William McNally, Kanav Vats, Alexander Wong, John McPhee
- **Comment**: None
- **Journal**: None
- **Summary**: In keypoint estimation tasks such as human pose estimation, heatmap-based regression is the dominant approach despite possessing notable drawbacks: heatmaps intrinsically suffer from quantization error and require excessive computation to generate and post-process. Motivated to find a more efficient solution, we propose to model individual keypoints and sets of spatially related keypoints (i.e., poses) as objects within a dense single-stage anchor-based detection framework. Hence, we call our method KAPAO (pronounced "Ka-Pow"), for Keypoints And Poses As Objects. KAPAO is applied to the problem of single-stage multi-person human pose estimation by simultaneously detecting human pose and keypoint objects and fusing the detections to exploit the strengths of both object representations. In experiments, we observe that KAPAO is faster and more accurate than previous methods, which suffer greatly from heatmap post-processing. The accuracy-speed trade-off is especially favourable in the practical setting when not using test-time augmentation. Source code: https://github.com/wmcnally/kapao.



### Two-step adversarial debiasing with partial learning -- medical image case-studies
- **Arxiv ID**: http://arxiv.org/abs/2111.08711v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.08711v1)
- **Published**: 2021-11-16 15:41:33+00:00
- **Updated**: 2021-11-16 15:41:33+00:00
- **Authors**: Ramon Correa, Jiwoong Jason Jeong, Bhavik Patel, Hari Trivedi, Judy W. Gichoya, Imon Banerjee
- **Comment**: None
- **Journal**: None
- **Summary**: The use of artificial intelligence (AI) in healthcare has become a very active research area in the last few years. While significant progress has been made in image classification tasks, only a few AI methods are actually being deployed in hospitals. A major hurdle in actively using clinical AI models currently is the trustworthiness of these models. More often than not, these complex models are black boxes in which promising results are generated. However, when scrutinized, these models begin to reveal implicit biases during the decision making, such as detecting race and having bias towards ethnic groups and subpopulations. In our ongoing study, we develop a two-step adversarial debiasing approach with partial learning that can reduce the racial disparity while preserving the performance of the targeted task. The methodology has been evaluated on two independent medical image case-studies - chest X-ray and mammograms, and showed promises in bias reduction while preserving the targeted performance.



### Tracking Blobs in the Turbulent Edge Plasma of a Tokamak Fusion Device
- **Arxiv ID**: http://arxiv.org/abs/2111.08570v3
- **DOI**: None
- **Categories**: **physics.plasm-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08570v3)
- **Published**: 2021-11-16 15:46:05+00:00
- **Updated**: 2022-08-10 13:00:25+00:00
- **Authors**: Woonghee Han, Randall A. Pietersen, Rafael Villamor-Lora, Matthew Beveridge, Nicola Offeddu, Theodore Golfinopoulos, Christian Theiler, James L. Terry, Earl S. Marmar, Iddo Drori
- **Comment**: 14 pages, 9 figures
- **Journal**: None
- **Summary**: The analysis of turbulence in plasmas is fundamental in fusion research. Despite extensive progress in theoretical modeling in the past 15 years, we still lack a complete and consistent understanding of turbulence in magnetic confinement devices, such as tokamaks. Experimental studies are challenging due to the diverse processes that drive the high-speed dynamics of turbulent phenomena. This work presents a novel application of motion tracking to identify and track turbulent filaments in fusion plasmas, called blobs, in a high-frequency video obtained from Gas Puff Imaging diagnostics. We compare four baseline methods (RAFT, Mask R-CNN, GMA, and Flow Walk) trained on synthetic data and then test on synthetic and real-world data obtained from plasmas in the Tokamak `a Configuration Variable (TCV). The blob regime identified from an analysis of blob trajectories agrees with state-of-the-art conditional averaging methods for each of the baseline methods employed, giving confidence in the accuracy of these techniques. High entry barriers traditionally limit tokamak plasma research to a small community of researchers in the field. By making a dataset and benchmark publicly available, we hope to open the field to a broad community in science and engineering.



### GRI: General Reinforced Imitation and its Application to Vision-Based Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2111.08575v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08575v2)
- **Published**: 2021-11-16 15:52:54+00:00
- **Updated**: 2022-05-17 15:38:30+00:00
- **Authors**: Raphael Chekroun, Marin Toromanoff, Sascha Hornauer, Fabien Moutarde
- **Comment**: None
- **Journal**: None
- **Summary**: Deep reinforcement learning (DRL) has been demonstrated to be effective for several complex decision-making applications such as autonomous driving and robotics. However, DRL is notoriously limited by its high sample complexity and its lack of stability. Prior knowledge, e.g. as expert demonstrations, is often available but challenging to leverage to mitigate these issues. In this paper, we propose General Reinforced Imitation (GRI), a novel method which combines benefits from exploration and expert data and is straightforward to implement over any off-policy RL algorithm. We make one simplifying hypothesis: expert demonstrations can be seen as perfect data whose underlying policy gets a constant high reward. Based on this assumption, GRI introduces the notion of offline demonstration agents. This agent sends expert data which are processed both concurrently and indistinguishably with the experiences coming from the online RL exploration agent. We show that our approach enables major improvements on vision-based autonomous driving in urban environments. We further validate the GRI method on Mujoco continuous control tasks with different off-policy RL algorithms. Our method ranked first on the CARLA Leaderboard and outperforms World on Rails, the previous state-of-the-art, by 17%.



### Robustness of Bayesian Neural Networks to White-Box Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2111.08591v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08591v1)
- **Published**: 2021-11-16 16:14:44+00:00
- **Updated**: 2021-11-16 16:14:44+00:00
- **Authors**: Adaku Uchendu, Daniel Campoy, Christopher Menart, Alexandra Hildenbrandt
- **Comment**: Accepted at the fourth IEEE International Conference on Artificial
  Intelligence and Knowledge Engineering (AIKE 2021)
- **Journal**: None
- **Summary**: Bayesian Neural Networks (BNNs), unlike Traditional Neural Networks (TNNs) are robust and adept at handling adversarial attacks by incorporating randomness. This randomness improves the estimation of uncertainty, a feature lacking in TNNs. Thus, we investigate the robustness of BNNs to white-box attacks using multiple Bayesian neural architectures. Furthermore, we create our BNN model, called BNN-DenseNet, by fusing Bayesian inference (i.e., variational Bayes) to the DenseNet architecture, and BDAV, by combining this intervention with adversarial training. Experiments are conducted on the CIFAR-10 and FGVC-Aircraft datasets. We attack our models with strong white-box attacks ($l_\infty$-FGSM, $l_\infty$-PGD, $l_2$-PGD, EOT $l_\infty$-FGSM, and EOT $l_\infty$-PGD). In all experiments, at least one BNN outperforms traditional neural networks during adversarial attack scenarios. An adversarially-trained BNN outperforms its non-Bayesian, adversarially-trained counterpart in most experiments, and often by significant margins. Lastly, we investigate network calibration and find that BNNs do not make overconfident predictions, providing evidence that BNNs are also better at measuring uncertainty.



### Advancement of Deep Learning in Pneumonia and Covid-19 Classification and Localization: A Qualitative and Quantitative Analysis
- **Arxiv ID**: http://arxiv.org/abs/2111.08606v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08606v1)
- **Published**: 2021-11-16 16:40:39+00:00
- **Updated**: 2021-11-16 16:40:39+00:00
- **Authors**: Aakash Shah, Manan Shah
- **Comment**: 20 pages, 5 figures, 5 tables
- **Journal**: None
- **Summary**: Around 450 million people are affected by pneumonia every year which results in 2.5 million deaths. Covid-19 has also affected 181 million people which has lead to 3.92 million casualties. The chances of death in both of these diseases can be significantly reduced if they are diagnosed early. However, the current methods of diagnosing pneumonia (complaints + chest X-ray) and covid-19 (RT-PCR) require the presence of expert radiologists and time, respectively. With the help of Deep Learning models, pneumonia and covid-19 can be detected instantly from Chest X-rays or CT scans. This way, the process of diagnosing Pneumonia/Covid-19 can be made more efficient and widespread. In this paper, we aim to elicit, explain, and evaluate, qualitatively and quantitatively, major advancements in deep learning methods aimed at detecting or localizing community-acquired pneumonia (CAP), viral pneumonia, and covid-19 from images of chest X-rays and CT scans. Being a systematic review, the focus of this paper lies in explaining deep learning model architectures which have either been modified or created from scratch for the task at hand wiwth focus on generalizability. For each model, this paper answers the question of why the model is designed the way it is, the challenges that a particular model overcomes, and the tradeoffs that come with modifying a model to the required specifications. A quantitative analysis of all models described in the paper is also provided to quantify the effectiveness of different models with a similar goal. Some tradeoffs cannot be quantified, and hence they are mentioned explicitly in the qualitative analysis, which is done throughout the paper. By compiling and analyzing a large quantum of research details in one place with all the datasets, model architectures, and results, we aim to provide a one-stop solution to beginners and current researchers interested in this field.



### IKEA Object State Dataset: A 6DoF object pose estimation dataset and benchmark for multi-state assembly objects
- **Arxiv ID**: http://arxiv.org/abs/2111.08614v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08614v1)
- **Published**: 2021-11-16 16:57:57+00:00
- **Updated**: 2021-11-16 16:57:57+00:00
- **Authors**: Yongzhi Su, Mingxin Liu, Jason Rambach, Antonia Pehrson, Anton Berg, Didier Stricker
- **Comment**: None
- **Journal**: None
- **Summary**: Utilizing 6DoF(Degrees of Freedom) pose information of an object and its components is critical for object state detection tasks. We present IKEA Object State Dataset, a new dataset that contains IKEA furniture 3D models, RGBD video of the assembly process, the 6DoF pose of furniture parts and their bounding box. The proposed dataset will be available at https://github.com/mxllmx/IKEAObjectStateDataset.



### A Data-Driven Approach for Linear and Nonlinear Damage Detection Using Variational Mode Decomposition and GARCH Model
- **Arxiv ID**: http://arxiv.org/abs/2111.08620v1
- **DOI**: 10.1007/s00366-021-01568-4
- **Categories**: **cs.CV**, I.5
- **Links**: [PDF](http://arxiv.org/pdf/2111.08620v1)
- **Published**: 2021-11-16 17:01:26+00:00
- **Updated**: 2021-11-16 17:01:26+00:00
- **Authors**: Vahid Reza Gharehbaghi, Hashem Kalbkhani, Ehsan Noroozinejad Farsangi, T. Y. Yang, Seyedali Mirjalili
- **Comment**: 30 Pages, 12 Figures and 8 Tables, Submitted Journal: Engineering
  with Computers, Springer
- **Journal**: Engineering with Computers (2022)
- **Summary**: In this article, an original data-driven approach is proposed to detect both linear and nonlinear damage in structures using output-only responses. The method deploys variational mode decomposition (VMD) and a generalised autoregressive conditional heteroscedasticity (GARCH) model for signal processing and feature extraction. To this end, VMD decomposes the response signals into intrinsic mode functions (IMFs). Afterwards, the GARCH model is utilised to represent the statistics of IMFs. The model coefficients of IMFs construct the primary feature vector. Kernel-based principal component analysis (PCA) and linear discriminant analysis (LDA) are utilised to reduce the redundancy of the primary features by mapping them to the new feature space. The informative features are then fed separately into three supervised classifiers, namely support vector machine (SVM), k-nearest neighbour (kNN), and fine tree. The performance of the proposed method is evaluated on two experimentally scaled models in terms of linear and nonlinear damage assessment. Kurtosis and ARCH tests proved the compatibility of the GARCH model.



### UBnormal: New Benchmark for Supervised Open-Set Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2111.08644v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.08644v3)
- **Published**: 2021-11-16 17:28:46+00:00
- **Updated**: 2023-04-07 12:31:31+00:00
- **Authors**: Andra Acsintoae, Andrei Florescu, Mariana-Iuliana Georgescu, Tudor Mare, Paul Sumedrea, Radu Tudor Ionescu, Fahad Shahbaz Khan, Mubarak Shah
- **Comment**: Accepted at CVPR 2022. Paper + supplementary (15 pages, 9 figures)
- **Journal**: None
- **Summary**: Detecting abnormal events in video is commonly framed as a one-class classification task, where training videos contain only normal events, while test videos encompass both normal and abnormal events. In this scenario, anomaly detection is an open-set problem. However, some studies assimilate anomaly detection to action recognition. This is a closed-set scenario that fails to test the capability of systems at detecting new anomaly types. To this end, we propose UBnormal, a new supervised open-set benchmark composed of multiple virtual scenes for video anomaly detection. Unlike existing data sets, we introduce abnormal events annotated at the pixel level at training time, for the first time enabling the use of fully-supervised learning methods for abnormal event detection. To preserve the typical open-set formulation, we make sure to include disjoint sets of anomaly types in our training and test collections of videos. To our knowledge, UBnormal is the first video anomaly detection benchmark to allow a fair head-to-head comparison between one-class open-set models and supervised closed-set models, as shown in our experiments. Moreover, we provide empirical evidence showing that UBnormal can enhance the performance of a state-of-the-art anomaly detection framework on two prominent data sets, Avenue and ShanghaiTech. Our benchmark is freely available at https://github.com/lilygeorgescu/UBnormal.



### Automatic Semantic Segmentation of the Lumbar Spine: Clinical Applicability in a Multi-parametric and Multi-centre Study on Magnetic Resonance Images
- **Arxiv ID**: http://arxiv.org/abs/2111.08712v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 92B20, 92C50, 68T07, 68T45, 68U10, 92B10
- **Links**: [PDF](http://arxiv.org/pdf/2111.08712v3)
- **Published**: 2021-11-16 17:33:05+00:00
- **Updated**: 2022-11-08 08:59:10+00:00
- **Authors**: Jhon Jairo Saenz-Gamboa, Julio Domenech, Antonio Alonso-Manjarrés, Jon A. Gómez, Maria de la Iglesia-Vayá
- **Comment**: 19 pages, 9 Figures, 8 Tables; Supplementary Material: 6 pages, 8
  Tables
- **Journal**: None
- **Summary**: One of the major difficulties in medical image segmentation is the high variability of these images, which is caused by their origin (multi-centre), the acquisition protocols (multi-parametric), as well as the variability of human anatomy, the severity of the illness, the effect of age and gender, among others. The problem addressed in this work is the automatic semantic segmentation of lumbar spine Magnetic Resonance images using convolutional neural networks. The purpose is to assign a class label to each pixel of an image. Classes were defined by radiologists and correspond to different structural elements like vertebrae, intervertebral discs, nerves, blood vessels, and other tissues. The proposed network topologies are variants of the U-Net architecture. Several complementary blocks were used to define the variants: Three types of convolutional blocks, spatial attention models, deep supervision and multilevel feature extractor. This document describes the topologies and analyses the results of the neural network designs that obtained the most accurate segmentations. Several of the proposed designs outperform the standard U-Net used as baseline, especially when used in ensembles where the output of multiple neural networks is combined according to different strategies.



### Diversified Multi-prototype Representation for Semi-supervised Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.08651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08651v1)
- **Published**: 2021-11-16 17:33:58+00:00
- **Updated**: 2021-11-16 17:33:58+00:00
- **Authors**: Jizong Peng, Christian Desrosiers, Marco Pedersoli
- **Comment**: None
- **Journal**: None
- **Summary**: This work considers semi-supervised segmentation as a dense prediction problem based on prototype vector correlation and proposes a simple way to represent each segmentation class with multiple prototypes. To avoid degenerate solutions, two regularization strategies are applied on unlabeled images. The first one leverages mutual information maximization to ensure that all prototype vectors are considered by the network. The second explicitly enforces prototypes to be orthogonal by minimizing their cosine distance. Experimental results on two benchmark medical segmentation datasets reveal our method's effectiveness in improving segmentation performance when few annotated images are available.



### A Latent Encoder Coupled Generative Adversarial Network (LE-GAN) for Efficient Hyperspectral Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2111.08685v1
- **DOI**: 10.1109/TGRS.2022.3193441
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.08685v1)
- **Published**: 2021-11-16 18:40:19+00:00
- **Updated**: 2021-11-16 18:40:19+00:00
- **Authors**: Yue Shi, Liangxiu Han, Lianghao Han, Sheng Chang, Tongle Hu, Darren Dancey
- **Comment**: 18 pages, 10 figures
- **Journal**: None
- **Summary**: Realistic hyperspectral image (HSI) super-resolution (SR) techniques aim to generate a high-resolution (HR) HSI with higher spectral and spatial fidelity from its low-resolution (LR) counterpart. The generative adversarial network (GAN) has proven to be an effective deep learning framework for image super-resolution. However, the optimisation process of existing GAN-based models frequently suffers from the problem of mode collapse, leading to the limited capacity of spectral-spatial invariant reconstruction. This may cause the spectral-spatial distortion on the generated HSI, especially with a large upscaling factor. To alleviate the problem of mode collapse, this work has proposed a novel GAN model coupled with a latent encoder (LE-GAN), which can map the generated spectral-spatial features from the image space to the latent space and produce a coupling component to regularise the generated samples. Essentially, we treat an HSI as a high-dimensional manifold embedded in a latent space. Thus, the optimisation of GAN models is converted to the problem of learning the distributions of high-resolution HSI samples in the latent space, making the distributions of the generated super-resolution HSIs closer to those of their original high-resolution counterparts. We have conducted experimental evaluations on the model performance of super-resolution and its capability in alleviating mode collapse. The proposed approach has been tested and validated based on two real HSI datasets with different sensors (i.e. AVIRIS and UHD-185) for various upscaling factors and added noise levels, and compared with the state-of-the-art super-resolution models (i.e. HyCoNet, LTTR, BAGAN, SR- GAN, WGAN).



### INTERN: A New Learning Paradigm Towards General Vision
- **Arxiv ID**: http://arxiv.org/abs/2111.08687v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.08687v2)
- **Published**: 2021-11-16 18:42:50+00:00
- **Updated**: 2022-02-24 18:57:20+00:00
- **Authors**: Jing Shao, Siyu Chen, Yangguang Li, Kun Wang, Zhenfei Yin, Yinan He, Jianing Teng, Qinghong Sun, Mengya Gao, Jihao Liu, Gengshi Huang, Guanglu Song, Yichao Wu, Yuming Huang, Fenggang Liu, Huan Peng, Shuo Qin, Chengyu Wang, Yujie Wang, Conghui He, Ding Liang, Yu Liu, Fengwei Yu, Junjie Yan, Dahua Lin, Xiaogang Wang, Yu Qiao
- **Comment**: None
- **Journal**: None
- **Summary**: Enormous waves of technological innovations over the past several years, marked by the advances in AI technologies, are profoundly reshaping the industry and the society. However, down the road, a key challenge awaits us, that is, our capability of meeting rapidly-growing scenario-specific demands is severely limited by the cost of acquiring a commensurate amount of training data. This difficult situation is in essence due to limitations of the mainstream learning paradigm: we need to train a new model for each new scenario, based on a large quantity of well-annotated data and commonly from scratch. In tackling this fundamental problem, we move beyond and develop a new learning paradigm named INTERN. By learning with supervisory signals from multiple sources in multiple stages, the model being trained will develop strong generalizability. We evaluate our model on 26 well-known datasets that cover four categories of tasks in computer vision. In most cases, our models, adapted with only 10% of the training data in the target domain, outperform the counterparts trained with the full set of data, often by a significant margin. This is an important step towards a promising prospect where such a model with general vision capability can dramatically reduce our reliance on data, thus expediting the adoption of AI technologies. Furthermore, revolving around our new paradigm, we also introduce a new data system, a new architecture, and a new benchmark, which, together, form a general vision ecosystem to support its future development in an open and inclusive manner. See project website at https://opengvlab.shlab.org.cn .



### Synthesis-Guided Feature Learning for Cross-Spectral Periocular Recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.08738v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.08738v1)
- **Published**: 2021-11-16 19:22:20+00:00
- **Updated**: 2021-11-16 19:22:20+00:00
- **Authors**: Domenick Poster, Nasser Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: A common yet challenging scenario in periocular biometrics is cross-spectral matching - in particular, the matching of visible wavelength against near-infrared (NIR) periocular images. We propose a novel approach to cross-spectral periocular verification that primarily focuses on learning a mapping from visible and NIR periocular images to a shared latent representational subspace, and supports this effort by simultaneously learning intra-spectral image reconstruction. We show the auxiliary image reconstruction task (and in particular the reconstruction of high-level, semantic features) results in learning a more discriminative, domain-invariant subspace compared to the baseline while incurring no additional computational or memory costs at test-time. The proposed Coupled Conditional Generative Adversarial Network (CoGAN) architecture uses paired generator networks (one operating on visible images and the other on NIR) composed of U-Nets with ResNet-18 encoders trained for feature learning via contrastive loss and for intra-spectral image reconstruction with adversarial, pixel-based, and perceptual reconstruction losses. Moreover, the proposed CoGAN model beats the current state-of-art (SotA) in cross-spectral periocular recognition. On the Hong Kong PolyU benchmark dataset, we achieve 98.65% AUC and 5.14% EER compared to the SotA EER of 8.02%. On the Cross-Eyed dataset, we achieve 99.31% AUC and 3.99% EER versus SotA EER of 4.39%.



### Learning Scene Dynamics from Point Cloud Sequences
- **Arxiv ID**: http://arxiv.org/abs/2111.08755v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.08755v1)
- **Published**: 2021-11-16 19:52:46+00:00
- **Updated**: 2021-11-16 19:52:46+00:00
- **Authors**: Pan He, Patrick Emami, Sanjay Ranka, Anand Rangarajan
- **Comment**: Accepted for publication in International Journal of Computer Vision,
  Special Issue on 3D Computer Vision. Code and data:
  https://github.com/BestSonny/SPCM
- **Journal**: None
- **Summary**: Understanding 3D scenes is a critical prerequisite for autonomous agents. Recently, LiDAR and other sensors have made large amounts of data available in the form of temporal sequences of point cloud frames. In this work, we propose a novel problem -- sequential scene flow estimation (SSFE) -- that aims to predict 3D scene flow for all pairs of point clouds in a given sequence. This is unlike the previously studied problem of scene flow estimation which focuses on two frames.   We introduce the SPCM-Net architecture, which solves this problem by computing multi-scale spatiotemporal correlations between neighboring point clouds and then aggregating the correlation across time with an order-invariant recurrent unit. Our experimental evaluation confirms that recurrent processing of point cloud sequences results in significantly better SSFE compared to using only two frames. Additionally, we demonstrate that this approach can be effectively modified for sequential point cloud forecasting (SPF), a related problem that demands forecasting future point cloud frames.   Our experimental results are evaluated using a new benchmark for both SSFE and SPF consisting of synthetic and real datasets. Previously, datasets for scene flow estimation have been limited to two frames. We provide non-trivial extensions to these datasets for multi-frame estimation and prediction. Due to the difficulty of obtaining ground truth motion for real-world datasets, we use self-supervised training and evaluation metrics. We believe that this benchmark will be pivotal to future research in this area. All code for benchmark and models will be made accessible.



### Computer Vision for Supporting Image Search
- **Arxiv ID**: http://arxiv.org/abs/2111.08772v1
- **DOI**: 10.1007/978-3-030-90235-3_1
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2111.08772v1)
- **Published**: 2021-11-16 20:50:32+00:00
- **Updated**: 2021-11-16 20:50:32+00:00
- **Authors**: Alan F. Smeaton
- **Comment**: 10 pages
- **Journal**: Advances in Visual Informatics. H. Badioze Zaman et al (Eds). IVIC
  2021, LNCS 13051, pp1-10, 2021
- **Summary**: Computer vision and multimedia information processing have made extreme progress within the last decade and many tasks can be done with a level of accuracy as if done by humans, or better. This is because we leverage the benefits of huge amounts of data available for training, we have enormous computer processing available and we have seen the evolution of machine learning as a suite of techniques to process data and deliver accurate vision-based systems. What kind of applications do we use this processing for ? We use this in autonomous vehicle navigation or in security applications, searching CCTV for example, and in medical image analysis for healthcare diagnostics. One application which is not widespread is image or video search directly by users. In this paper we present the need for such image finding or re-finding by examining human memory and when it fails, thus motivating the need for a different approach to image search which is outlined, along with the requirements of computer vision to support it.



### Film Trailer Generation via Task Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2111.08774v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08774v1)
- **Published**: 2021-11-16 20:50:52+00:00
- **Updated**: 2021-11-16 20:50:52+00:00
- **Authors**: Pinelopi Papalampidi, Frank Keller, Mirella Lapata
- **Comment**: None
- **Journal**: None
- **Summary**: Movie trailers perform multiple functions: they introduce viewers to the story, convey the mood and artistic style of the film, and encourage audiences to see the movie. These diverse functions make automatic trailer generation a challenging endeavor. We decompose it into two subtasks: narrative structure identification and sentiment prediction. We model movies as graphs, where nodes are shots and edges denote semantic relations between them. We learn these relations using joint contrastive training which leverages privileged textual information (e.g., characters, actions, situations) from screenplays. An unsupervised algorithm then traverses the graph and generates trailers that human judges prefer to ones generated by competitive supervised approaches.



### Detecting AutoAttack Perturbations in the Frequency Domain
- **Arxiv ID**: http://arxiv.org/abs/2111.08785v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2111.08785v1)
- **Published**: 2021-11-16 21:20:37+00:00
- **Updated**: 2021-11-16 21:20:37+00:00
- **Authors**: Peter Lorenz, Paula Harder, Dominik Strassel, Margret Keuper, Janis Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, adversarial attacks on image classification networks by the AutoAttack (Croce and Hein, 2020b) framework have drawn a lot of attention. While AutoAttack has shown a very high attack success rate, most defense approaches are focusing on network hardening and robustness enhancements, like adversarial training. This way, the currently best-reported method can withstand about 66% of adversarial examples on CIFAR10. In this paper, we investigate the spatial and frequency domain properties of AutoAttack and propose an alternative defense. Instead of hardening a network, we detect adversarial attacks during inference, rejecting manipulated inputs. Based on a rather simple and fast analysis in the frequency domain, we introduce two different detection algorithms. First, a black box detector that only operates on the input images and achieves a detection accuracy of 100% on the AutoAttack CIFAR10 benchmark and 99.3% on ImageNet, for epsilon = 8/255 in both cases. Second, a whitebox detector using an analysis of CNN feature maps, leading to a detection rate of also 100% and 98.7% on the same benchmarks.



### DeltaConv: Anisotropic Operators for Geometric Deep Learning on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2111.08799v5
- **DOI**: 10.1145/3528223.3530166
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08799v5)
- **Published**: 2021-11-16 21:58:55+00:00
- **Updated**: 2022-05-12 20:38:25+00:00
- **Authors**: Ruben Wiersma, Ahmad Nasikun, Elmar Eisemann, Klaus Hildebrandt
- **Comment**: 8 pages, 5 figures, 7 tables; ACM Transactions on Graphics 41, 4,
  Article 105 (SIGGRAPH 2022)
- **Journal**: None
- **Summary**: Learning from 3D point-cloud data has rapidly gained momentum, motivated by the success of deep learning on images and the increased availability of 3D~data. In this paper, we aim to construct anisotropic convolution layers that work directly on the surface derived from a point cloud. This is challenging because of the lack of a global coordinate system for tangential directions on surfaces. We introduce DeltaConv, a convolution layer that combines geometric operators from vector calculus to enable the construction of anisotropic filters on point clouds. Because these operators are defined on scalar- and vector-fields, we separate the network into a scalar- and a vector-stream, which are connected by the operators. The vector stream enables the network to explicitly represent, evaluate, and process directional information. Our convolutions are robust and simple to implement and match or improve on state-of-the-art approaches on several benchmarks, while also speeding up training and inference.



### A Benchmark for Modeling Violation-of-Expectation in Physical Reasoning Across Event Categories
- **Arxiv ID**: http://arxiv.org/abs/2111.08826v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2111.08826v1)
- **Published**: 2021-11-16 22:59:25+00:00
- **Updated**: 2021-11-16 22:59:25+00:00
- **Authors**: Arijit Dasgupta, Jiafei Duan, Marcelo H. Ang Jr, Yi Lin, Su-hua Wang, Renée Baillargeon, Cheston Tan
- **Comment**: arXiv admin note: text overlap with arXiv:2110.05836
- **Journal**: None
- **Summary**: Recent work in computer vision and cognitive reasoning has given rise to an increasing adoption of the Violation-of-Expectation (VoE) paradigm in synthetic datasets. Inspired by infant psychology, researchers are now evaluating a model's ability to label scenes as either expected or surprising with knowledge of only expected scenes. However, existing VoE-based 3D datasets in physical reasoning provide mainly vision data with little to no heuristics or inductive biases. Cognitive models of physical reasoning reveal infants create high-level abstract representations of objects and interactions. Capitalizing on this knowledge, we established a benchmark to study physical reasoning by curating a novel large-scale synthetic 3D VoE dataset armed with ground-truth heuristic labels of causally relevant features and rules. To validate our dataset in five event categories of physical reasoning, we benchmarked and analyzed human performance. We also proposed the Object File Physical Reasoning Network (OFPR-Net) which exploits the dataset's novel heuristics to outperform our baseline and ablation models. The OFPR-Net is also flexible in learning an alternate physical reality, showcasing its ability to learn universal causal relationships in physical reasoning to create systems with better interpretability.



### HARA: A Hierarchical Approach for Robust Rotation Averaging
- **Arxiv ID**: http://arxiv.org/abs/2111.08831v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.08831v2)
- **Published**: 2021-11-16 23:25:24+00:00
- **Updated**: 2022-03-29 22:45:14+00:00
- **Authors**: Seong Hun Lee, Javier Civera
- **Comment**: Accepted to CVPR2022
- **Journal**: None
- **Summary**: We propose a novel hierarchical approach for multiple rotation averaging, dubbed HARA. Our method incrementally initializes the rotation graph based on a hierarchy of triplet support. The key idea is to build a spanning tree by prioritizing the edges with many strong triplet supports and gradually adding those with weaker and fewer supports. This reduces the risk of adding outliers in the spanning tree. As a result, we obtain a robust initial solution that enables us to filter outliers prior to nonlinear optimization. With minimal modification, our approach can also integrate the knowledge of the number of valid 2D-2D correspondences. We perform extensive evaluations on both synthetic and real datasets, demonstrating state-of-the-art results.



