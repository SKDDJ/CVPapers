# Arxiv Papers in cs.CV on 2021-11-06
### Artifact- and content-specific quality assessment for MRI with image rulers
- **Arxiv ID**: http://arxiv.org/abs/2111.03780v1
- **DOI**: 10.1016/j.media.2021.102344
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03780v1)
- **Published**: 2021-11-06 02:17:12+00:00
- **Updated**: 2021-11-06 02:17:12+00:00
- **Authors**: Ke Lei, John M. Pauly, Shreyas S. Vasanawala
- **Comment**: None
- **Journal**: None
- **Summary**: In clinical practice MR images are often first seen by radiologists long after the scan. If image quality is inadequate either patients have to return for an additional scan, or a suboptimal interpretation is rendered. An automatic image quality assessment (IQA) would enable real-time remediation. Existing IQA works for MRI give only a general quality score, agnostic to the cause of and solution to low-quality scans. Furthermore, radiologists' image quality requirements vary with the scan type and diagnostic task. Therefore, the same score may have different implications for different scans. We propose a framework with multi-task CNN model trained with calibrated labels and inferenced with image rulers. Labels calibrated by human inputs follow a well-defined and efficient labeling task. Image rulers address varying quality standards and provide a concrete way of interpreting raw scores from the CNN. The model supports assessments of two of the most common artifacts in MRI: noise and motion. It achieves accuracies of around 90%, 6% better than the best previous method examined, and 3% better than human experts on noise assessment. Our experiments show that label calibration, image rulers, and multi-task training improve the model's performance and generalizability.



### Generation of microbial colonies dataset with deep learning style transfer
- **Arxiv ID**: http://arxiv.org/abs/2111.03789v2
- **DOI**: 10.1038/s41598-022-09264-z
- **Categories**: **cs.CV**, cs.AI, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2111.03789v2)
- **Published**: 2021-11-06 03:11:01+00:00
- **Updated**: 2022-01-19 02:54:06+00:00
- **Authors**: Jarosław Pawłowski, Sylwia Majchrowska, Tomasz Golan
- **Comment**: 13 pages, 9 figures, 2 tables
- **Journal**: Scientific Reports 12, 5212 (2022)
- **Summary**: We introduce an effective strategy to generate an annotated synthetic dataset of microbiological images of Petri dishes that can be used to train deep learning models in a fully supervised fashion. The developed generator employs traditional computer vision algorithms together with a neural style transfer method for data augmentation. We show that the method is able to synthesize a dataset of realistic looking images that can be used to train a neural network model capable of localising, segmenting, and classifying five different microbial species. Our method requires significantly fewer resources to obtain a useful dataset than collecting and labeling a whole large set of real images with annotations. We show that starting with only 100 real images, we can generate data to train a detector that achieves comparable results (detection mAP = 0.416, and counting MAE = 4.49) to the same detector but trained on a real, several dozen times bigger dataset (mAP = 0.520, MAE = 4.31), containing over 7k images. We prove the usefulness of the method in microbe detection and segmentation, but we expect that it is general and flexible and can also be applicable in other domains of science and industry to detect various objects.



### Neural BRDFs: Representation and Operations
- **Arxiv ID**: http://arxiv.org/abs/2111.03797v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03797v2)
- **Published**: 2021-11-06 03:50:02+00:00
- **Updated**: 2021-11-14 05:59:28+00:00
- **Authors**: Jiahui Fan, Beibei Wang, Miloš Hašan, Jian Yang, Ling-Qi Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Bidirectional reflectance distribution functions (BRDFs) are pervasively used in computer graphics to produce realistic physically-based appearance. In recent years, several works explored using neural networks to represent BRDFs, taking advantage of neural networks' high compression rate and their ability to fit highly complex functions. However, once represented, the BRDFs will be fixed and therefore lack flexibility to take part in follow-up operations. In this paper, we present a form of "Neural BRDF algebra", and focus on both representation and operations of BRDFs at the same time. We propose a representation neural network to compress BRDFs into latent vectors, which is able to represent BRDFs accurately. We further propose several operations that can be applied solely in the latent space, such as layering and interpolation. Spatial variation is straightforward to achieve by using textures of latent vectors. Furthermore, our representation can be efficiently evaluated and sampled, providing a competitive solution to more expensive Monte Carlo layering approaches.



### Order-Guided Disentangled Representation Learning for Ulcerative Colitis Classification with Limited Labels
- **Arxiv ID**: http://arxiv.org/abs/2111.03815v2
- **DOI**: 10.1007/978-3-030-87196-3_44
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03815v2)
- **Published**: 2021-11-06 06:53:40+00:00
- **Updated**: 2023-03-02 14:18:57+00:00
- **Authors**: Shota Harada, Ryoma Bise, Hideaki Hayashi, Kiyohito Tanaka, Seiichi Uchida
- **Comment**: Accepted by MICCAI 2021
- **Journal**: None
- **Summary**: Ulcerative colitis (UC) classification, which is an important task for endoscopic diagnosis, involves two main difficulties. First, endoscopic images with the annotation about UC (positive or negative) are usually limited. Second, they show a large variability in their appearance due to the location in the colon. Especially, the second difficulty prevents us from using existing semi-supervised learning techniques, which are the common remedy for the first difficulty. In this paper, we propose a practical semi-supervised learning method for UC classification by newly exploiting two additional features, the location in a colon (e.g., left colon) and image capturing order, both of which are often attached to individual images in endoscopic image sequences. The proposed method can extract the essential information of UC classification efficiently by a disentanglement process with those features. Experimental results demonstrate that the proposed method outperforms several existing semi-supervised learning methods in the classification task, even with a small number of annotated images.



### Will You Ever Become Popular? Learning to Predict Virality of Dance Clips
- **Arxiv ID**: http://arxiv.org/abs/2111.03819v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2111.03819v1)
- **Published**: 2021-11-06 07:26:28+00:00
- **Updated**: 2021-11-06 07:26:28+00:00
- **Authors**: Jiahao Wang, Yunhong Wang, Nina Weng, Tianrui Chai, Annan Li, Faxi Zhang, Sansi Yu
- **Comment**: Accepted by TOMM
- **Journal**: None
- **Summary**: Dance challenges are going viral in video communities like TikTok nowadays. Once a challenge becomes popular, thousands of short-form videos will be uploaded in merely a couple of days. Therefore, virality prediction from dance challenges is of great commercial value and has a wide range of applications, such as smart recommendation and popularity promotion. In this paper, a novel multi-modal framework which integrates skeletal, holistic appearance, facial and scenic cues is proposed for comprehensive dance virality prediction. To model body movements, we propose a pyramidal skeleton graph convolutional network (PSGCN) which hierarchically refines spatio-temporal skeleton graphs. Meanwhile, we introduce a relational temporal convolutional network (RTCN) to exploit appearance dynamics with non-local temporal relations. An attentive fusion approach is finally proposed to adaptively aggregate predictions from different modalities. To validate our method, we introduce a large-scale viral dance video (VDV) dataset, which contains over 4,000 dance clips of eight viral dance challenges. Extensive experiments on the VDV dataset demonstrate the efficacy of our model. Extensive experiments on the VDV dataset well demonstrate the effectiveness of our approach. Furthermore, we show that short video applications like multi-dimensional recommendation and action feedback can be derived from our model.



### ROFT: Real-Time Optical Flow-Aided 6D Object Pose and Velocity Tracking
- **Arxiv ID**: http://arxiv.org/abs/2111.03821v1
- **DOI**: 10.1109/LRA.2021.3119379
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2111.03821v1)
- **Published**: 2021-11-06 07:30:00+00:00
- **Updated**: 2021-11-06 07:30:00+00:00
- **Authors**: Nicola A. Piga, Yuriy Onyshchuk, Giulia Pasquale, Ugo Pattacini, Lorenzo Natale
- **Comment**: To cite this work, please refer to the journal reference entry. For
  more information, code, pictures and video please visit
  https://github.com/hsp-iit/roft
- **Journal**: IEEE Robotics and Automation Letters Volume 7, Issue 1, Jan. 2022,
  pp 159-166
- **Summary**: 6D object pose tracking has been extensively studied in the robotics and computer vision communities. The most promising solutions, leveraging on deep neural networks and/or filtering and optimization, exhibit notable performance on standard benchmarks. However, to our best knowledge, these have not been tested thoroughly against fast object motions. Tracking performance in this scenario degrades significantly, especially for methods that do not achieve real-time performance and introduce non negligible delays. In this work, we introduce ROFT, a Kalman filtering approach for 6D object pose and velocity tracking from a stream of RGB-D images. By leveraging real-time optical flow, ROFT synchronizes delayed outputs of low frame rate Convolutional Neural Networks for instance segmentation and 6D object pose estimation with the RGB-D input stream to achieve fast and precise 6D object pose and velocity tracking. We test our method on a newly introduced photorealistic dataset, Fast-YCB, which comprises fast moving objects from the YCB model set, and on the dataset for object and hand pose estimation HO-3D. Results demonstrate that our approach outperforms state-of-the-art methods for 6D object pose tracking, while also providing 6D object velocity tracking. A video showing the experiments is provided as supplementary material.



### Neural Implicit Event Generator for Motion Tracking
- **Arxiv ID**: http://arxiv.org/abs/2111.03824v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03824v1)
- **Published**: 2021-11-06 07:38:52+00:00
- **Updated**: 2021-11-06 07:38:52+00:00
- **Authors**: Mana Masuda, Yusuke Sekikawa, Ryo Fujii, Hideo Saito
- **Comment**: Submitted to ICRA 2022
- **Journal**: None
- **Summary**: We present a novel framework of motion tracking from event data using implicit expression. Our framework use pre-trained event generation MLP named implicit event generator (IEG) and does motion tracking by updating its state (position and velocity) based on the difference between the observed event and generated event from the current state estimate. The difference is computed implicitly by the IEG. Unlike the conventional explicit approach, which requires dense computation to evaluate the difference, our implicit approach realizes efficient state update directly from sparse event data. Our sparse algorithm is especially suitable for mobile robotics applications where computational resources and battery life are limited. To verify the effectiveness of our method on real-world data, we applied it to the AR marker tracking application. We have confirmed that our framework works well in real-world environments in the presence of noise and background clutter.



### Multi-modal land cover mapping of remote sensing images using pyramid attention and gated fusion networks
- **Arxiv ID**: http://arxiv.org/abs/2111.03845v1
- **DOI**: 10.1080/01431161.2022.2098078
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03845v1)
- **Published**: 2021-11-06 10:01:01+00:00
- **Updated**: 2021-11-06 10:01:01+00:00
- **Authors**: Qinghui Liu, Michael Kampffmeyer, Robert Jenssen, Arnt-Børre Salberg
- **Comment**: 24 pages, 11 figures, submitted to IJRS
- **Journal**: None
- **Summary**: Multi-modality data is becoming readily available in remote sensing (RS) and can provide complementary information about the Earth's surface. Effective fusion of multi-modal information is thus important for various applications in RS, but also very challenging due to large domain differences, noise, and redundancies. There is a lack of effective and scalable fusion techniques for bridging multiple modality encoders and fully exploiting complementary information. To this end, we propose a new multi-modality network (MultiModNet) for land cover mapping of multi-modal remote sensing data based on a novel pyramid attention fusion (PAF) module and a gated fusion unit (GFU). The PAF module is designed to efficiently obtain rich fine-grained contextual representations from each modality with a built-in cross-level and cross-view attention fusion mechanism, and the GFU module utilizes a novel gating mechanism for early merging of features, thereby diminishing hidden redundancies and noise. This enables supplementary modalities to effectively extract the most valuable and complementary information for late feature fusion. Extensive experiments on two representative RS benchmark datasets demonstrate the effectiveness, robustness, and superiority of the MultiModNet for multi-modal land cover classification.



### Multimodal PET/CT Tumour Segmentation and Prediction of Progression-Free Survival using a Full-Scale UNet with Attention
- **Arxiv ID**: http://arxiv.org/abs/2111.03848v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03848v1)
- **Published**: 2021-11-06 10:28:48+00:00
- **Updated**: 2021-11-06 10:28:48+00:00
- **Authors**: Emmanuelle Bourigault, Daniel R. McGowan, Abolfazl Mehranian, Bartłomiej W. Papież
- **Comment**: 13 pages, 3 figures, 2 tables. To appear in Head and Neck Tumor
  Segmentation in PET/CT: The HECKTOR Challenge,Valentin Oreiller et al.,
  Medical Image Analysis,2021, HECKTOR 2021, Lecture Notes in Computer Science,
  Springer
- **Journal**: None
- **Summary**: Segmentation of head and neck (H\&N) tumours and prediction of patient outcome are crucial for patient's disease diagnosis and treatment monitoring. Current developments of robust deep learning models are hindered by the lack of large multi-centre, multi-modal data with quality annotations. The MICCAI 2021 HEad and neCK TumOR (HECKTOR) segmentation and outcome prediction challenge creates a platform for comparing segmentation methods of the primary gross target volume on fluoro-deoxyglucose (FDG)-PET and Computed Tomography images and prediction of progression-free survival in H\&N oropharyngeal cancer.For the segmentation task, we proposed a new network based on an encoder-decoder architecture with full inter- and intra-skip connections to take advantage of low-level and high-level semantics at full scales. Additionally, we used Conditional Random Fields as a post-processing step to refine the predicted segmentation maps. We trained multiple neural networks for tumor volume segmentation, and these segmentations were ensembled achieving an average Dice Similarity Coefficient of 0.75 in cross-validation, and 0.76 on the challenge testing data set. For prediction of patient progression free survival task, we propose a Cox proportional hazard regression combining clinical, radiomic, and deep learning features. Our survival prediction model achieved a concordance index of 0.82 in cross-validation, and 0.62 on the challenge testing data set.



### A new baseline for retinal vessel segmentation: Numerical identification and correction of methodological inconsistencies affecting 100+ papers
- **Arxiv ID**: http://arxiv.org/abs/2111.03853v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.03853v1)
- **Published**: 2021-11-06 11:09:11+00:00
- **Updated**: 2021-11-06 11:09:11+00:00
- **Authors**: György Kovács, Attila Fazekas
- **Comment**: None
- **Journal**: None
- **Summary**: In the last 15 years, the segmentation of vessels in retinal images has become an intensively researched problem in medical imaging, with hundreds of algorithms published. One of the de facto benchmarking data sets of vessel segmentation techniques is the DRIVE data set. Since DRIVE contains a predefined split of training and test images, the published performance results of the various segmentation techniques should provide a reliable ranking of the algorithms. Including more than 100 papers in the study, we performed a detailed numerical analysis of the coherence of the published performance scores. We found inconsistencies in the reported scores related to the use of the field of view (FoV), which has a significant impact on the performance scores. We attempted to eliminate the biases using numerical techniques to provide a more realistic picture of the state of the art. Based on the results, we have formulated several findings, most notably: despite the well-defined test set of DRIVE, most rankings in published papers are based on non-comparable figures; in contrast to the near-perfect accuracy scores reported in the literature, the highest accuracy score achieved to date is 0.9582 in the FoV region, which is 1% higher than that of human annotators. The methods we have developed for identifying and eliminating the evaluation biases can be easily applied to other domains where similar problems may arise.



### What augmentations are sensitive to hyper-parameters and why?
- **Arxiv ID**: http://arxiv.org/abs/2111.03861v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.03861v1)
- **Published**: 2021-11-06 11:56:40+00:00
- **Updated**: 2021-11-06 11:56:40+00:00
- **Authors**: Ch Muhammad Awais, Imad Eddine Ibrahim Bekkouch
- **Comment**: 10 pages, 17 figures
- **Journal**: None
- **Summary**: We apply augmentations to our dataset to enhance the quality of our predictions and make our final models more resilient to noisy data and domain drifts. Yet the question remains, how are these augmentations going to perform with different hyper-parameters? In this study we evaluate the sensitivity of augmentations with regards to the model's hyper parameters along with their consistency and influence by performing a Local Surrogate (LIME) interpretation on the impact of hyper-parameters when different augmentations are applied to a machine learning model. We have utilized Linear regression coefficients for weighing each augmentation. Our research has proved that there are some augmentations which are highly sensitive to hyper-parameters and others which are more resilient and reliable.



### Towards Calibrated Model for Long-Tailed Visual Recognition from Prior Perspective
- **Arxiv ID**: http://arxiv.org/abs/2111.03874v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.03874v1)
- **Published**: 2021-11-06 12:53:34+00:00
- **Updated**: 2021-11-06 12:53:34+00:00
- **Authors**: Zhengzhuo Xu, Zenghao Chai, Chun Yuan
- **Comment**: Accepted at NeurIPS 2021
- **Journal**: None
- **Summary**: Real-world data universally confronts a severe class-imbalance problem and exhibits a long-tailed distribution, i.e., most labels are associated with limited instances. The na\"ive models supervised by such datasets would prefer dominant labels, encounter a serious generalization challenge and become poorly calibrated. We propose two novel methods from the prior perspective to alleviate this dilemma. First, we deduce a balance-oriented data augmentation named Uniform Mixup (UniMix) to promote mixup in long-tailed scenarios, which adopts advanced mixing factor and sampler in favor of the minority. Second, motivated by the Bayesian theory, we figure out the Bayes Bias (Bayias), an inherent bias caused by the inconsistency of prior, and compensate it as a modification on standard cross-entropy loss. We further prove that both the proposed methods ensure the classification calibration theoretically and empirically. Extensive experiments verify that our strategies contribute to a better-calibrated model, and their combination achieves state-of-the-art performance on CIFAR-LT, ImageNet-LT, and iNaturalist 2018.



### Action Recognition using Transfer Learning and Majority Voting for CSGO
- **Arxiv ID**: http://arxiv.org/abs/2111.03882v1
- **DOI**: 10.1109/ICTS52701.2021.9608407
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03882v1)
- **Published**: 2021-11-06 13:33:20+00:00
- **Updated**: 2021-11-06 13:33:20+00:00
- **Authors**: Tasnim Sakib Apon, Abrar Islam, MD. Golam Rabiul Alam
- **Comment**: None
- **Journal**: None
- **Summary**: Presently online video games have become a progressively favorite source of recreation and Counter Strike: Global Offensive (CS: GO) is one of the top-listed online first-person shooting games. Numerous competitive games are arranged every year by Esports. Nonetheless, (i) No study has been conducted on video analysis and action recognition of CS: GO game-play which can play a substantial role in the gaming industry for prediction model (ii) No work has been done on the real-time application on the actions and results of a CS: GO match (iii) Game data of a match is usually available in the HLTV as a CSV formatted file however it does not have open access and HLTV tends to prevent users from taking data. This manuscript aims to develop a model for accurate prediction of 4 different actions and compare the performance among the five different transfer learning models with our self-developed deep neural network and identify the best-fitted model and also including major voting later on, which is qualified to provide real time prediction and the result of this model aids to the construction of the automated system of gathering and processing more data alongside solving the issue of collecting data from HLTV.



### Demystifying Deep Learning Models for Retinal OCT Disease Classification using Explainable AI
- **Arxiv ID**: http://arxiv.org/abs/2111.03890v1
- **DOI**: 10.1109/CSDE53843.2021.9718400
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03890v1)
- **Published**: 2021-11-06 13:54:07+00:00
- **Updated**: 2021-11-06 13:54:07+00:00
- **Authors**: Tasnim Sakib Apon, Mohammad Mahmudul Hasan, Abrar Islam, MD. Golam Rabiul Alam
- **Comment**: None
- **Journal**: None
- **Summary**: In the world of medical diagnostics, the adoption of various deep learning techniques is quite common as well as effective, and its statement is equally true when it comes to implementing it into the retina Optical Coherence Tomography (OCT) sector, but (i)These techniques have the black box characteristics that prevent the medical professionals to completely trust the results generated from them (ii)Lack of precision of these methods restricts their implementation in clinical and complex cases (iii)The existing works and models on the OCT classification are substantially large and complicated and they require a considerable amount of memory and computational power, reducing the quality of classifiers in real-time applications. To meet these problems, in this paper a self-developed CNN model has been proposed which is comparatively smaller and simpler along with the use of Lime that introduces Explainable AI to the study and helps to increase the interpretability of the model. This addition will be an asset to the medical experts for getting major and detailed information and will help them in making final decisions and will also reduce the opacity and vulnerability of the conventional deep learning models.



### EEGEyeNet: a Simultaneous Electroencephalography and Eye-tracking Dataset and Benchmark for Eye Movement Prediction
- **Arxiv ID**: http://arxiv.org/abs/2111.05100v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.05100v2)
- **Published**: 2021-11-06 15:41:27+00:00
- **Updated**: 2021-11-10 08:22:39+00:00
- **Authors**: Ard Kastrati, Martyna Beata Płomecka, Damián Pascual, Lukas Wolf, Victor Gillioz, Roger Wattenhofer, Nicolas Langer
- **Comment**: Published at NeurIPS 2021 Datasets and Benchmarks Track
- **Journal**: None
- **Summary**: We present a new dataset and benchmark with the goal of advancing research in the intersection of brain activities and eye movements. Our dataset, EEGEyeNet, consists of simultaneous Electroencephalography (EEG) and Eye-tracking (ET) recordings from 356 different subjects collected from three different experimental paradigms. Using this dataset, we also propose a benchmark to evaluate gaze prediction from EEG measurements. The benchmark consists of three tasks with an increasing level of difficulty: left-right, angle-amplitude and absolute position. We run extensive experiments on this benchmark in order to provide solid baselines, both based on classical machine learning models and on large neural networks. We release our complete code and data and provide a simple and easy-to-use interface to evaluate new methods.



### Domain Attention Consistency for Multi-Source Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2111.03911v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.03911v1)
- **Published**: 2021-11-06 15:56:53+00:00
- **Updated**: 2021-11-06 15:56:53+00:00
- **Authors**: Zhongying Deng, Kaiyang Zhou, Yongxin Yang, Tao Xiang
- **Comment**: Accepted to BMVC 2021 as oral presentation
- **Journal**: None
- **Summary**: Most existing multi-source domain adaptation (MSDA) methods minimize the distance between multiple source-target domain pairs via feature distribution alignment, an approach borrowed from the single source setting. However, with diverse source domains, aligning pairwise feature distributions is challenging and could even be counter-productive for MSDA. In this paper, we introduce a novel approach: transferable attribute learning. The motivation is simple: although different domains can have drastically different visual appearances, they contain the same set of classes characterized by the same set of attributes; an MSDA model thus should focus on learning the most transferable attributes for the target domain. Adopting this approach, we propose a domain attention consistency network, dubbed DAC-Net. The key design is a feature channel attention module, which aims to identify transferable features (attributes). Importantly, the attention module is supervised by a consistency loss, which is imposed on the distributions of channel attention weights between source and target domains. Moreover, to facilitate discriminative feature learning on the target data, we combine pseudo-labeling with a class compactness loss to minimize the distance between the target features and the classifier's weight vectors. Extensive experiments on three MSDA benchmarks show that our DAC-Net achieves new state of the art performance on all of them.



### Detecting COVID-19 from Chest Computed Tomography Scans using AI-Driven Android Application
- **Arxiv ID**: http://arxiv.org/abs/2111.06254v1
- **DOI**: 10.1016/j.compbiomed.2022.105298
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.06254v1)
- **Published**: 2021-11-06 16:35:24+00:00
- **Updated**: 2021-11-06 16:35:24+00:00
- **Authors**: Aryan Verma, Sagar B. Amin, Muhammad Naeem, Monjoy Saha
- **Comment**: None
- **Journal**: Computers in Biology and Medicine, 143 (2022), 105298
- **Summary**: The COVID-19 (coronavirus disease 2019) pandemic affected more than 186 million people with over 4 million deaths worldwide by June 2021. The magnitude of which has strained global healthcare systems. Chest Computed Tomography (CT) scans have a potential role in the diagnosis and prognostication of COVID-19. Designing a diagnostic system which is cost-efficient and convenient to operate on resource-constrained devices like mobile phones would enhance the clinical usage of chest CT scans and provide swift, mobile, and accessible diagnostic capabilities. This work proposes developing a novel Android application that detects COVID-19 infection from chest CT scans using a highly efficient and accurate deep learning algorithm. It further creates an attention heatmap, augmented on the segmented lung parenchyma region in the CT scans through an algorithm developed as a part of this work, which shows the regions of infection in the lungs. We propose a selection approach combined with multi-threading for a faster generation of heatmaps on Android Device, which reduces the processing time by about 93%. The neural network trained to detect COVID-19 in this work is tested with F1 score and accuracy, both of 99.58% and sensitivity of 99.69%, which is better than most of the results in the domain of COVID diagnosis from CT scans. This work will be beneficial in high volume practices and help doctors triage patients in the early diagnosis of the COVID-19 quickly and efficiently.



### Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling
- **Arxiv ID**: http://arxiv.org/abs/2111.03930v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2111.03930v2)
- **Published**: 2021-11-06 18:09:22+00:00
- **Updated**: 2021-11-15 04:58:28+00:00
- **Authors**: Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, Hongsheng Li
- **Comment**: preprints
- **Journal**: None
- **Summary**: Contrastive Vision-Language Pre-training, known as CLIP, has provided a new paradigm for learning visual representations by using large-scale contrastive image-text pairs. It shows impressive performance on zero-shot knowledge transfer to downstream tasks. To further enhance CLIP's few-shot capability, CLIP-Adapter proposed to fine-tune a lightweight residual feature adapter and significantly improves the performance for few-shot classification. However, such a process still needs extra training and computational resources. In this paper, we propose \textbf{T}raining-Free CL\textbf{IP}-\textbf{Adapter} (\textbf{Tip-Adapter}), which not only inherits CLIP's training-free advantage but also performs comparably or even better than CLIP-Adapter. Tip-Adapter does not require any back propagation for training the adapter, but creates the weights by a key-value cache model constructed from the few-shot training set. In this non-parametric manner, Tip-Adapter acquires well-performed adapter weights without any training, which is both efficient and effective. Moreover, the performance of Tip-Adapter can be further boosted by fine-tuning such properly initialized adapter for only a few epochs with super-fast convergence speed. We conduct extensive experiments of few-shot classification on ImageNet and other 10 datasets to demonstrate the superiority of proposed Tip-Adapter. The code will be released at \url{https://github.com/gaopengcuhk/Tip-Adapter}.



### Convolutional Gated MLP: Combining Convolutions & gMLP
- **Arxiv ID**: http://arxiv.org/abs/2111.03940v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2111.03940v1)
- **Published**: 2021-11-06 19:11:24+00:00
- **Updated**: 2021-11-06 19:11:24+00:00
- **Authors**: A. Rajagopal, V. Nirmala
- **Comment**: Conference
- **Journal**: None
- **Summary**: To the best of our knowledge, this is the first paper to introduce Convolutions to Gated MultiLayer Perceptron and contributes an implementation of this novel Deep Learning architecture. Google Brain introduced the gMLP in May 2021. Microsoft introduced Convolutions in Vision Transformer in Mar 2021. Inspired by both gMLP and CvT, we introduce convolutional layers in gMLP. CvT combined the power of Convolutions and Attention. Our implementation combines the best of Convolutional learning along with spatial gated MLP. Further, the paper visualizes how CgMLP learns. Visualizations show how CgMLP learns from features such as outline of a car. While Attention was the basis of much of recent progress in Deep Learning, gMLP proposed an approach that doesn't use Attention computation. In Transformer based approaches, a whole lot of Attention matrixes need to be learnt using vast amount of training data. In gMLP, the fine tunning for new tasks can be challenging by transfer learning with smaller datasets. We implement CgMLP and compares it with gMLP on CIFAR dataset. Experimental results explore the power of generaliza-tion of CgMLP, while gMLP tend to drastically overfit the training data.   To summarize, the paper contributes a novel Deep Learning architecture and demonstrates the learning mechanism of CgMLP through visualizations, for the first time in literature.



### CALText: Contextual Attention Localization for Offline Handwritten Text
- **Arxiv ID**: http://arxiv.org/abs/2111.03952v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, 68T07 (Primary), 68U10 (Secondary), I.2.6; I.7.5
- **Links**: [PDF](http://arxiv.org/pdf/2111.03952v1)
- **Published**: 2021-11-06 19:54:21+00:00
- **Updated**: 2021-11-06 19:54:21+00:00
- **Authors**: Tayaba Anjum, Nazar Khan
- **Comment**: 25 pages, 15 figures and 6 tables
- **Journal**: None
- **Summary**: Recognition of Arabic-like scripts such as Persian and Urdu is more challenging than Latin-based scripts. This is due to the presence of a two-dimensional structure, context-dependent character shapes, spaces and overlaps, and placement of diacritics. Not much research exists for offline handwritten Urdu script which is the 10th most spoken language in the world. We present an attention based encoder-decoder model that learns to read Urdu in context. A novel localization penalty is introduced to encourage the model to attend only one location at a time when recognizing the next character. In addition, we comprehensively refine the only complete and publicly available handwritten Urdu dataset in terms of ground-truth annotations. We evaluate the model on both Urdu and Arabic datasets and show that contextual attention localization outperforms both simple attention and multi-directional LSTM models.



