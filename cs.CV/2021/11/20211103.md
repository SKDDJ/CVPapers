# Arxiv Papers in cs.CV on 2021-11-03
### Adversarially Perturbed Wavelet-based Morphed Face Generation
- **Arxiv ID**: http://arxiv.org/abs/2111.01965v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.01965v1)
- **Published**: 2021-11-03 01:18:29+00:00
- **Updated**: 2021-11-03 01:18:29+00:00
- **Authors**: Kelsey O'Haire, Sobhan Soleymani, Baaria Chaudhary, Poorya Aghdaie, Jeremy Dawson, Nasser M. Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: Morphing is the process of combining two or more subjects in an image in order to create a new identity which contains features of both individuals. Morphed images can fool Facial Recognition Systems (FRS) into falsely accepting multiple people, leading to failures in national security. As morphed image synthesis becomes easier, it is vital to expand the research community's available data to help combat this dilemma. In this paper, we explore combination of two methods for morphed image generation, those of geometric transformation (warping and blending to create morphed images) and photometric perturbation. We leverage both methods to generate high-quality adversarially perturbed morphs from the FERET, FRGC, and FRLL datasets. The final images retain high similarity to both input subjects while resulting in minimal artifacts in the visual domain. Images are synthesized by fusing the wavelet sub-bands from the two look-alike subjects, and then adversarially perturbed to create highly convincing imagery to deceive both humans and deep morph detectors.



### Skin Cancer Classification using Inception Network and Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.02402v1
- **DOI**: 10.1007/978-3-030-58799-4_39
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.02402v1)
- **Published**: 2021-11-03 01:50:52+00:00
- **Updated**: 2021-11-03 01:50:52+00:00
- **Authors**: Priscilla Benedetti, Damiano Perri, Marco Simonetti, Osvaldo Gervasi, Gianluca Reali, Mauro Femminella
- **Comment**: International Conference on Computational Science and Its
  Applications, ICCSA 2020
- **Journal**: LNCS, volume 12249, 2020
- **Summary**: Medical data classification is typically a challenging task due to imbalance between classes. In this paper, we propose an approach to classify dermatoscopic images from HAM10000 (Human Against Machine with 10000 training images) dataset, consisting of seven imbalanced types of skin lesions, with good precision and low resources requirements. Classification is done by using a pretrained convolutional neural network. We evaluate the accuracy and performance of the proposal and illustrate possible extensions.



### WORD: A large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from CT image
- **Arxiv ID**: http://arxiv.org/abs/2111.02403v5
- **DOI**: 10.1016/j.media.2022.102642
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.02403v5)
- **Published**: 2021-11-03 02:26:14+00:00
- **Updated**: 2023-02-13 03:24:50+00:00
- **Authors**: Xiangde Luo, Wenjun Liao, Jianghong Xiao, Jieneng Chen, Tao Song, Xiaofan Zhang, Kang Li, Dimitris N. Metaxas, Guotai Wang, Shaoting Zhang
- **Comment**: Accepted to Medical Image Analysis, dataset at:
  https://github.com/HiLab-git/WORD (we corrected the results or description in
  this version.)
- **Journal**: None
- **Summary**: Whole abdominal organ segmentation is important in diagnosing abdomen lesions, radiotherapy, and follow-up. However, oncologists' delineating all abdominal organs from 3D volumes is time-consuming and very expensive. Deep learning-based medical image segmentation has shown the potential to reduce manual delineation efforts, but it still requires a large-scale fine annotated dataset for training, and there is a lack of large-scale datasets covering the whole abdomen region with accurate and detailed annotations for the whole abdominal organ segmentation. In this work, we establish a new large-scale \textit{W}hole abdominal \textit{OR}gan \textit{D}ataset (\textit{WORD}) for algorithm research and clinical application development. This dataset contains 150 abdominal CT volumes (30495 slices). Each volume has 16 organs with fine pixel-level annotations and scribble-based sparse annotations, which may be the largest dataset with whole abdominal organ annotation. Several state-of-the-art segmentation methods are evaluated on this dataset. And we also invited three experienced oncologists to revise the model predictions to measure the gap between the deep learning method and oncologists. Afterwards, we investigate the inference-efficient learning on the WORD, as the high-resolution image requires large GPU memory and a long inference time in the test stage. We further evaluate the scribble-based annotation-efficient learning on this dataset, as the pixel-wise manual annotation is time-consuming and expensive. The work provided a new benchmark for the abdominal multi-organ segmentation task, and these experiments can serve as the baseline for future research and clinical application development.



### Multi-Glimpse Network: A Robust and Efficient Classification Architecture based on Recurrent Downsampled Attention
- **Arxiv ID**: http://arxiv.org/abs/2111.02018v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.02018v2)
- **Published**: 2021-11-03 04:46:26+00:00
- **Updated**: 2023-04-12 14:36:38+00:00
- **Authors**: Sia Huat Tan, Runpei Dong, Kaisheng Ma
- **Comment**: Accepted at BMVC 2021
- **Journal**: The British Machine Vision Conference (BMVC) 2021
- **Summary**: Most feedforward convolutional neural networks spend roughly the same efforts for each pixel. Yet human visual recognition is an interaction between eye movements and spatial attention, which we will have several glimpses of an object in different regions. Inspired by this observation, we propose an end-to-end trainable Multi-Glimpse Network (MGNet) which aims to tackle the challenges of high computation and the lack of robustness based on recurrent downsampled attention mechanism. Specifically, MGNet sequentially selects task-relevant regions of an image to focus on and then adaptively combines all collected information for the final prediction. MGNet expresses strong resistance against adversarial attacks and common corruptions with less computation. Also, MGNet is inherently more interpretable as it explicitly informs us where it focuses during each iteration. Our experiments on ImageNet100 demonstrate the potential of recurrent downsampled attention mechanisms to improve a single feedforward manner. For example, MGNet improves 4.76% accuracy on average in common corruptions with only 36.9% computational cost. Moreover, while the baseline incurs an accuracy drop to 7.6%, MGNet manages to maintain 44.2% accuracy in the same PGD attack strength with ResNet-50 backbone. Our code is available at https://github.com/siahuat0727/MGNet.



### Recent Advancements in Self-Supervised Paradigms for Visual Feature Representation
- **Arxiv ID**: http://arxiv.org/abs/2111.02042v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.02042v1)
- **Published**: 2021-11-03 07:02:34+00:00
- **Updated**: 2021-11-03 07:02:34+00:00
- **Authors**: Mrinal Anand, Aditya Garg
- **Comment**: None
- **Journal**: None
- **Summary**: We witnessed a massive growth in the supervised learning paradigm in the past decade. Supervised learning requires a large amount of labeled data to reach state-of-the-art performance. However, labeling the samples requires a lot of human annotation. To avoid the cost of labeling data, self-supervised methods were proposed to make use of largely available unlabeled data. This study conducts a comprehensive and insightful survey and analysis of recent developments in the self-supervised paradigm for feature representation. In this paper, we investigate the factors affecting the usefulness of self-supervision under different settings. We present some of the key insights concerning two different approaches in self-supervision, generative and contrastive methods. We also investigate the limitations of supervised adversarial training and how self-supervision can help overcome those limitations. We then move on to discuss the limitations and challenges in effectively using self-supervision for visual tasks. Finally, we highlight some open problems and point out future research directions.



### Categorical Difference and Related Brain Regions of the Attentional Blink Effect
- **Arxiv ID**: http://arxiv.org/abs/2111.02044v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2111.02044v1)
- **Published**: 2021-11-03 07:11:52+00:00
- **Updated**: 2021-11-03 07:11:52+00:00
- **Authors**: Renzhou Gui, Xiaohong Ji
- **Comment**: Accepted in PhotonIcs and Electromagnetics Research Symposium (PIERS)
  2021
- **Journal**: None
- **Summary**: Attentional blink (AB) is a biological effect, showing that for 200 to 500ms after paying attention to one visual target, it is difficult to notice another target that appears next, and attentional blink magnitude (ABM) is a indicating parameter to measure the degree of this effect. Researchers have shown that different categories of images can access the consciousness of human mind differently, and produce different ranges of ABM values. So in this paper, we compare two different types of images, categorized as animal and object, by predicting ABM values directly from image features extracted from convolutional neural network (CNN), and indirectly from functional magnetic resonance imaging (fMRI) data. First, for two sets of images, we separately extract their average features from layers of Alexnet, a classic model of CNN, then input the features into a trained linear regression model to predict ABM values, and we find higher-level instead of lower-level image features determine the categorical difference in AB effect, and mid-level image features predict ABM values more correctly than low-level and high-level image features. Then we employ fMRI data from different brain regions collected when the subjects viewed 50 test images to predict ABM values, and conclude that brain regions covering relatively broader areas, like LVC, HVC and VC, perform better than other smaller brain regions, which means AB effect is more related to synthetic impact of several visual brain regions than only one particular visual regions.



### Deep Point Set Resampling via Gradient Fields
- **Arxiv ID**: http://arxiv.org/abs/2111.02045v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.02045v1)
- **Published**: 2021-11-03 07:20:35+00:00
- **Updated**: 2021-11-03 07:20:35+00:00
- **Authors**: Haolan Chen, Bi'an Du, Shitong Luo, Wei Hu
- **Comment**: arXiv admin note: text overlap with arXiv:2107.10981
- **Journal**: None
- **Summary**: 3D point clouds acquired by scanning real-world objects or scenes have found a wide range of applications including immersive telepresence, autonomous driving, surveillance, etc. They are often perturbed by noise or suffer from low density, which obstructs downstream tasks such as surface reconstruction and understanding. In this paper, we propose a novel paradigm of point set resampling for restoration, which learns continuous gradient fields of point clouds that converge points towards the underlying surface. In particular, we represent a point cloud via its gradient field -- the gradient of the log-probability density function, and enforce the gradient field to be continuous, thus guaranteeing the continuity of the model for solvable optimization. Based on the continuous gradient fields estimated via a proposed neural network, resampling a point cloud amounts to performing gradient-based Markov Chain Monte Carlo (MCMC) on the input noisy or sparse point cloud. Further, we propose to introduce regularization into the gradient-based MCMC during point cloud restoration, which essentially refines the intermediate resampled point cloud iteratively and accommodates various priors in the resampling process. Extensive experimental results demonstrate that the proposed point set resampling achieves the state-of-the-art performance in representative restoration tasks including point cloud denoising and upsampling.



### Rethinking the Image Feature Biases Exhibited by Deep CNN Models
- **Arxiv ID**: http://arxiv.org/abs/2111.02058v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.02058v1)
- **Published**: 2021-11-03 08:04:06+00:00
- **Updated**: 2021-11-03 08:04:06+00:00
- **Authors**: Dawei Dai, Yutang Li, Huanan Bao, Sy Xia, Guoyin Wang, Xiaoli Ma
- **Comment**: 15 pages, 15 figures
- **Journal**: None
- **Summary**: In recent years, convolutional neural networks (CNNs) have been applied successfully in many fields. However, such deep neural models are still regarded as black box in most tasks. One of the fundamental issues underlying this problem is understanding which features are most influential in image recognition tasks and how they are processed by CNNs. It is widely accepted that CNN models combine low-level features to form complex shapes until the object can be readily classified, however, several recent studies have argued that texture features are more important than other features. In this paper, we assume that the importance of certain features varies depending on specific tasks, i.e., specific tasks exhibit a feature bias. We designed two classification tasks based on human intuition to train deep neural models to identify anticipated biases. We devised experiments comprising many tasks to test these biases for the ResNet and DenseNet models. From the results, we conclude that (1) the combined effect of certain features is typically far more influential than any single feature; (2) in different tasks, neural models can perform different biases, that is, we can design a specific task to make a neural model biased toward a specific anticipated feature.



### Deep-Learning-Based Single-Image Height Reconstruction from Very-High-Resolution SAR Intensity Data
- **Arxiv ID**: http://arxiv.org/abs/2111.02061v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2111.02061v2)
- **Published**: 2021-11-03 08:20:03+00:00
- **Updated**: 2021-11-19 13:49:54+00:00
- **Authors**: Michael Recla, Michael Schmitt
- **Comment**: 19 pages, 14 figures
- **Journal**: None
- **Summary**: Originally developed in fields such as robotics and autonomous driving with image-based navigation in mind, deep learning-based single-image depth estimation (SIDE) has found great interest in the wider image analysis community. Remote sensing is no exception, as the possibility to estimate height maps from single aerial or satellite imagery bears great potential in the context of topographic reconstruction. A few pioneering investigations have demonstrated the general feasibility of single image height prediction from optical remote sensing images and motivate further studies in that direction. With this paper, we present the first-ever demonstration of deep learning-based single image height prediction for the other important sensor modality in remote sensing: synthetic aperture radar (SAR) data. Besides the adaptation of a convolutional neural network (CNN) architecture for SAR intensity images, we present a workflow for the generation of training data, and extensive experimental results for different SAR imaging modes and test sites. Since we put a particular emphasis on transferability, we are able to confirm that deep learning-based single-image height estimation is not only possible, but also transfers quite well to unseen data, even if acquired by different imaging modes and imaging parameters.



### Event and Activity Recognition in Video Surveillance for Cyber-Physical Systems
- **Arxiv ID**: http://arxiv.org/abs/2111.02064v1
- **DOI**: 10.1007/978-3-030-66222-6_4
- **Categories**: **cs.CV**, cs.LG, cs.RO, I.2.9; I.2.10; I.4.7; I.4.8; I.5.1; I.5.2; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2111.02064v1)
- **Published**: 2021-11-03 08:30:38+00:00
- **Updated**: 2021-11-03 08:30:38+00:00
- **Authors**: Swarnabja Bhaumik, Prithwish Jana, Partha Pratim Mohanta
- **Comment**: This is a preprint of the chapter:S Bhaumik, P Jana, PP Mohanta,
  Event and Activity Recognition in Video Surveillance for Cyber-Physical
  Systems, published in Emergence of Cyber Physical System.., edited by KK
  Singh et al, 2021, Springer reproduced with permission of Springer Nature
  Switzerland AG. The final authenticated version is available online at
  http://dx.doi.org/10.1007/978-3-030-66222-6_4
- **Journal**: Emergence of Cyber Physical System and IoT in Smart Automation and
  Robotics. Springer, Cham, 2021. 51-68
- **Summary**: This chapter aims to aid the development of Cyber-Physical Systems (CPS) in automated understanding of events and activities in various applications of video-surveillance. These events are mostly captured by drones, CCTVs or novice and unskilled individuals on low-end devices. Being unconstrained, these videos are immensely challenging due to a number of quality factors. We present an extensive account of the various approaches taken to solve the problem over the years. This ranges from methods as early as Structure from Motion (SFM) based approaches to recent solution frameworks involving deep neural networks. We show that the long-term motion patterns alone play a pivotal role in the task of recognizing an event. Consequently each video is significantly represented by a fixed number of key-frames using a graph-based approach. Only the temporal features are exploited using a hybrid Convolutional Neural Network (CNN) + Recurrent Neural Network (RNN) architecture. The results we obtain are encouraging as they outperform standard temporal CNNs and are at par with those using spatial information along with motion cues. Further exploring multistream models, we conceive a multi-tier fusion strategy for the spatial and temporal wings of a network. A consolidated representation of the respective individual prediction vectors on video and frame levels is obtained using a biased conflation technique. The fusion strategy endows us with greater rise in precision on each stage as compared to the state-of-the-art methods, and thus a powerful consensus is achieved in classification. Results are recorded on four benchmark datasets widely used in the domain of action recognition, namely CCV, HMDB, UCF-101 and KCV. It is inferable that focusing on better classification of the video sequences certainly leads to robust actuation of a system designed for event surveillance and object cum activity tracking.



### Dual Progressive Prototype Network for Generalized Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.02073v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.02073v2)
- **Published**: 2021-11-03 08:43:29+00:00
- **Updated**: 2021-11-22 11:00:51+00:00
- **Authors**: Chaoqun Wang, Shaobo Min, Xuejin Chen, Xiaoyan Sun, Houqiang Li
- **Comment**: Accepted by NeurIPS 2021
- **Journal**: None
- **Summary**: Generalized Zero-Shot Learning (GZSL) aims to recognize new categories with auxiliary semantic information,e.g., category attributes. In this paper, we handle the critical issue of domain shift problem, i.e., confusion between seen and unseen categories, by progressively improving cross-domain transferability and category discriminability of visual representations. Our approach, named Dual Progressive Prototype Network (DPPN), constructs two types of prototypes that record prototypical visual patterns for attributes and categories, respectively. With attribute prototypes, DPPN alternately searches attribute-related local regions and updates corresponding attribute prototypes to progressively explore accurate attribute-region correspondence. This enables DPPN to produce visual representations with accurate attribute localization ability, which benefits the semantic-visual alignment and representation transferability. Besides, along with progressive attribute localization, DPPN further projects category prototypes into multiple spaces to progressively repel visual representations from different categories, which boosts category discriminability. Both attribute and category prototypes are collaboratively learned in a unified framework, which makes visual representations of DPPN transferable and distinctive. Experiments on four benchmarks prove that DPPN effectively alleviates the domain shift problem in GZSL.



### FaceQvec: Vector Quality Assessment for Face Biometrics based on ISO Compliance
- **Arxiv ID**: http://arxiv.org/abs/2111.02078v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.02078v1)
- **Published**: 2021-11-03 09:07:41+00:00
- **Updated**: 2021-11-03 09:07:41+00:00
- **Authors**: Javier Hernandez-Ortega, Julian Fierrez, Luis F. Gomez, Aythami Morales, Jose Luis Gonzalez-de-Suso, Francisco Zamora-Martinez
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we develop FaceQvec, a software component for estimating the conformity of facial images with each of the points contemplated in the ISO/IEC 19794-5, a quality standard that defines general quality guidelines for face images that would make them acceptable or unacceptable for use in official documents such as passports or ID cards. This type of tool for quality assessment can help to improve the accuracy of face recognition, as well as to identify which factors are affecting the quality of a given face image and to take actions to eliminate or reduce those factors, e.g., with postprocessing techniques or re-acquisition of the image. FaceQvec consists of the automation of 25 individual tests related to different points contemplated in the aforementioned standard, as well as other characteristics of the images that have been considered to be related to facial quality. We first include the results of the quality tests evaluated on a development dataset captured under realistic conditions. We used those results to adjust the decision threshold of each test. Then we checked again their accuracy on a evaluation database that contains new face images not seen during development. The evaluation results demonstrate the accuracy of the individual tests for checking compliance with ISO/IEC 19794-5. FaceQvec is available online (https://github.com/uam-biometrics/FaceQvec).



### Influence of image noise on crack detection performance of deep convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2111.02079v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.02079v1)
- **Published**: 2021-11-03 09:08:54+00:00
- **Updated**: 2021-11-03 09:08:54+00:00
- **Authors**: Riccardo Chianese, Andy Nguyen, Vahidreza Gharehbaghi, Thiru Aravinthan, Mohammad Noori
- **Comment**: 8 pages, 16 figures, 4 tables
- **Journal**: 10th International Conference on Structural Health Monitoring of
  Intelligent Infrastructure, SHMII 10, 2021
- **Summary**: Development of deep learning techniques to analyse image data is an expansive and emerging field. The benefits of tracking, identifying, measuring, and sorting features of interest from image data has endless applications for saving cost, time, and improving safety. Much research has been conducted on classifying cracks from image data using deep convolutional neural networks; however, minimal research has been conducted to study the efficacy of network performance when noisy images are used. This paper will address the problem and is dedicated to investigating the influence of image noise on network accuracy. The methods used incorporate a benchmark image data set, which is purposely deteriorated with two types of noise, followed by treatment with image enhancement pre-processing techniques. These images, including their native counterparts, are then used to train and validate two different networks to study the differences in accuracy and performance. Results from this research reveal that noisy images have a moderate to high impact on the network's capability to accurately classify images despite the application of image pre-processing. A new index has been developed for finding the most efficient method for classification in terms of computation timing and accuracy. Consequently, AlexNet was selected as the most efficient model based on the proposed index.



### LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs
- **Arxiv ID**: http://arxiv.org/abs/2111.02114v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.02114v1)
- **Published**: 2021-11-03 10:16:39+00:00
- **Updated**: 2021-11-03 10:16:39+00:00
- **Authors**: Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki
- **Comment**: Short version. Accepted at Data Centric AI NeurIPS Workshop 2021
- **Journal**: None
- **Summary**: Multi-modal language-vision models trained on hundreds of millions of image-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable capability to perform zero- or few-shot learning and transfer even in absence of per-sample labels on target image data. Despite this trend, to date there has been no publicly available datasets of sufficient scale for training such models from scratch. To address this issue, in a community effort we build and release for public LAION-400M, a dataset with CLIP-filtered 400 million image-text pairs, their CLIP embeddings and kNN indices that allow efficient similarity search.



### Efficient 3D Deep LiDAR Odometry
- **Arxiv ID**: http://arxiv.org/abs/2111.02135v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.02135v2)
- **Published**: 2021-11-03 11:09:49+00:00
- **Updated**: 2022-09-14 20:42:06+00:00
- **Authors**: Guangming Wang, Xinrui Wu, Shuyang Jiang, Zhe Liu, Hesheng Wang
- **Comment**: 17 pages, 13 figures. Accepted by PAMI 2022. arXiv admin note:
  substantial text overlap with arXiv:2012.00972
- **Journal**: None
- **Summary**: An efficient 3D point cloud learning architecture, named EfficientLO-Net, for LiDAR odometry is first proposed in this paper. In this architecture, the projection-aware representation of the 3D point cloud is proposed to organize the raw 3D point cloud into an ordered data form to achieve efficiency. The Pyramid, Warping, and Cost volume (PWC) structure for the LiDAR odometry task is built to estimate and refine the pose in a coarse-to-fine approach. A projection-aware attentive cost volume is built to directly associate two discrete point clouds and obtain embedding motion patterns. Then, a trainable embedding mask is proposed to weigh the local motion patterns to regress the overall pose and filter outlier points. The trainable pose warp-refinement module is iteratively used with embedding mask optimized hierarchically to make the pose estimation more robust for outliers. The entire architecture is holistically optimized end-to-end to achieve adaptive learning of cost volume and mask, and all operations involving point cloud sampling and grouping are accelerated by projection-aware 3D feature learning methods. The superior performance and effectiveness of our LiDAR odometry architecture are demonstrated on KITTI, M2DGR, and Argoverse datasets. Our method outperforms all recent learning-based methods and even the geometry-based approach, LOAM with mapping optimization, on most sequences of KITTI odometry dataset. We open sourced our codes at: https://github.com/IRMVLab/EfficientLO-Net.



### An Entropy-guided Reinforced Partial Convolutional Network for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2111.02139v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.02139v1)
- **Published**: 2021-11-03 11:13:13+00:00
- **Updated**: 2021-11-03 11:13:13+00:00
- **Authors**: Yun Li, Zhe Liu, Lina Yao, Xianzhi Wang, Julian McAuley, Xiaojun Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-Shot Learning (ZSL) aims to transfer learned knowledge from observed classes to unseen classes via semantic correlations. A promising strategy is to learn a global-local representation that incorporates global information with extra localities (i.e., small parts/regions of inputs). However, existing methods discover localities based on explicit features without digging into the inherent properties and relationships among regions. In this work, we propose a novel Entropy-guided Reinforced Partial Convolutional Network (ERPCNet), which extracts and aggregates localities progressively based on semantic relevance and visual correlations without human-annotated regions. ERPCNet uses reinforced partial convolution and entropy guidance; it not only discovers global-cooperative localities dynamically but also converges faster for policy gradient optimization. We conduct extensive experiments to demonstrate ERPCNet's performance through comparisons with state-of-the-art methods under ZSL and Generalized Zero-Shot Learning (GZSL) settings on four benchmark datasets. We also show ERPCNet is time efficient and explainable through visualization analysis.



### Certainty Volume Prediction for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2111.02901v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.02901v1)
- **Published**: 2021-11-03 11:22:55+00:00
- **Updated**: 2021-11-03 11:22:55+00:00
- **Authors**: Tobias Ringwald, Rainer Stiefelhagen
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) deals with the problem of classifying unlabeled target domain data while labeled data is only available for a different source domain. Unfortunately, commonly used classification methods cannot fulfill this task adequately due to the domain gap between the source and target data. In this paper, we propose a novel uncertainty-aware domain adaptation setup that models uncertainty as a multivariate Gaussian distribution in feature space. We show that our proposed uncertainty measure correlates with other common uncertainty quantifications and relates to smoothing the classifier's decision boundary, therefore improving the generalization capabilities. We evaluate our proposed pipeline on challenging UDA datasets and achieve state-of-the-art results. Code for our method is available at https://gitlab.com/tringwald/cvp.



### Beyond PRNU: Learning Robust Device-Specific Fingerprint for Source Camera Identification
- **Arxiv ID**: http://arxiv.org/abs/2111.02144v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.02144v1)
- **Published**: 2021-11-03 11:25:19+00:00
- **Updated**: 2021-11-03 11:25:19+00:00
- **Authors**: Manisha, Chang-Tsun Li, Xufeng Lin, Karunakar A. Kotegar
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Source camera identification tools assist image forensic investigators to associate an image in question with a suspect camera. Various techniques have been developed based on the analysis of the subtle traces left in the images during the acquisition. The Photo Response Non Uniformity (PRNU) noise pattern caused by sensor imperfections has been proven to be an effective way to identify the source camera. The existing literature suggests that the PRNU is the only fingerprint that is device-specific and capable of identifying the exact source device. However, the PRNU is susceptible to camera settings, image content, image processing operations, and counter-forensic attacks. A forensic investigator unaware of counter-forensic attacks or incidental image manipulations is at the risk of getting misled. The spatial synchronization requirement during the matching of two PRNUs also represents a major limitation of the PRNU. In recent years, deep learning based approaches have been successful in identifying source camera models. However, the identification of individual cameras of the same model through these data-driven approaches remains unsatisfactory. In this paper, we bring to light the existence of a new robust data-driven device-specific fingerprint in digital images which is capable of identifying the individual cameras of the same model. It is discovered that the new device fingerprint is location-independent, stochastic, and globally available, which resolve the spatial synchronization issue. Unlike the PRNU, which resides in the high-frequency band, the new device fingerprint is extracted from the low and mid-frequency bands, which resolves the fragility issue that the PRNU is unable to contend with. Our experiments on various datasets demonstrate that the new fingerprint is highly resilient to image manipulations such as rotation, gamma correction, and aggressive JPEG compression.



### Graph Neural Networks for Nomination and Representation Learning of Web Elements
- **Arxiv ID**: http://arxiv.org/abs/2111.02168v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.HC, cs.IR, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2111.02168v3)
- **Published**: 2021-11-03 12:13:52+00:00
- **Updated**: 2022-10-25 14:27:11+00:00
- **Authors**: Alexandra Hotti, Riccardo Sven Risuleo, Stefan Magureanu, Aref Moradi, Jens Lagergren
- **Comment**: 12 pages, 8 figures, 3 tables, under review
- **Journal**: None
- **Summary**: This paper tackles the under-explored problem of DOM element nomination and representation learning with three important contributions. First, we present a large-scale and realistic dataset of webpages, far richer and more diverse than other datasets proposed for element representation learning, classification and nomination on the web. The dataset contains $51,701$ manually labeled product pages from $8,175$ real e-commerce websites. Second, we adapt several Graph Neural Network (GNN) architectures to website DOM trees and benchmark their performance on a diverse set of element nomination tasks using our proposed dataset. In element nomination, a single element on a page is selected for a given class. We show that on our challenging dataset a simple Convolutional GNN outperforms state-of-the-art methods on web element nomination. Finally, we propose a new training method that further boosts the element nomination accuracy. In nomination for the web, classification (assigning a class to a given element) is usually used as a surrogate objective for nomination during training. Our novel training methodology steers the classification objective towards the more complex and useful nomination objective.



### ProSTformer: Pre-trained Progressive Space-Time Self-attention Model for Traffic Flow Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2111.03459v1
- **DOI**: None
- **Categories**: **physics.soc-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.03459v1)
- **Published**: 2021-11-03 12:20:08+00:00
- **Updated**: 2021-11-03 12:20:08+00:00
- **Authors**: Xiao Yan, Xianghua Gan, Jingjing Tang, Rui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Traffic flow forecasting is essential and challenging to intelligent city management and public safety. Recent studies have shown the potential of convolution-free Transformer approach to extract the dynamic dependencies among complex influencing factors. However, two issues prevent the approach from being effectively applied in traffic flow forecasting. First, it ignores the spatiotemporal structure of the traffic flow videos. Second, for a long sequence, it is hard to focus on crucial attention due to the quadratic times dot-product computation. To address the two issues, we first factorize the dependencies and then design a progressive space-time self-attention mechanism named ProSTformer. It has two distinctive characteristics: (1) corresponding to the factorization, the self-attention mechanism progressively focuses on spatial dependence from local to global regions, on temporal dependence from inside to outside fragment (i.e., closeness, period, and trend), and finally on external dependence such as weather, temperature, and day-of-week; (2) by incorporating the spatiotemporal structure into the self-attention mechanism, each block in ProSTformer highlights the unique dependence by aggregating the regions with spatiotemporal positions to significantly decrease the computation. We evaluate ProSTformer on two traffic datasets, and each dataset includes three separate datasets with big, medium, and small scales. Despite the radically different design compared to the convolutional architectures for traffic flow forecasting, ProSTformer performs better or the same on the big scale datasets than six state-of-the-art baseline methods by RMSE. When pre-trained on the big scale datasets and transferred to the medium and small scale datasets, ProSTformer achieves a significant enhancement and behaves best.



### A cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.02172v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2111.02172v1)
- **Published**: 2021-11-03 12:24:03+00:00
- **Updated**: 2021-11-03 12:24:03+00:00
- **Authors**: Ziwang Fu, Feng Liu, Hanyang Wang, Jiayin Qi, Xiangling Fu, Aimin Zhou, Zhibin Li
- **Comment**: 5 pages, 1 figure, 2 tables
- **Journal**: None
- **Summary**: The audio-video based multimodal emotion recognition has attracted a lot of attention due to its robust performance. Most of the existing methods focus on proposing different cross-modal fusion strategies. However, these strategies introduce redundancy in the features of different modalities without fully considering the complementary properties between modal information, and these approaches do not guarantee the non-loss of original semantic information during intra- and inter-modal interactions. In this paper, we propose a novel cross-modal fusion network based on self-attention and residual structure (CFN-SR) for multimodal emotion recognition. Firstly, we perform representation learning for audio and video modalities to obtain the semantic features of the two modalities by efficient ResNeXt and 1D CNN, respectively. Secondly, we feed the features of the two modalities into the cross-modal blocks separately to ensure efficient complementarity and completeness of information through the self-attention mechanism and residual structure. Finally, we obtain the output of emotions by splicing the obtained fused representation with the original representation. To verify the effectiveness of the proposed method, we conduct experiments on the RAVDESS dataset. The experimental results show that the proposed CFN-SR achieves the state-of-the-art and obtains 75.76% accuracy with 26.30M parameters. Our code is available at https://github.com/skeletonNN/CFN-SR.



### Discriminator Synthesis: On reusing the other half of Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2111.02175v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2111.02175v2)
- **Published**: 2021-11-03 12:30:58+00:00
- **Updated**: 2021-11-12 10:13:00+00:00
- **Authors**: Diego Porres
- **Comment**: 7 pages, 4 figures, NeurIPS Workshop on Machine Learning for
  Creativity and Design 2021
- **Journal**: None
- **Summary**: Generative Adversarial Networks have long since revolutionized the world of computer vision and, tied to it, the world of art. Arduous efforts have gone into fully utilizing and stabilizing training so that outputs of the Generator network have the highest possible fidelity, but little has gone into using the Discriminator after training is complete. In this work, we propose to use the latter and show a way to use the features it has learned from the training dataset to both alter an image and generate one from scratch. We name this method Discriminator Dreaming, and the full code can be found at https://github.com/PDillis/stylegan3-fun.



### Learned Image Compression for Machine Perception
- **Arxiv ID**: http://arxiv.org/abs/2111.02249v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.02249v1)
- **Published**: 2021-11-03 14:39:09+00:00
- **Updated**: 2021-11-03 14:39:09+00:00
- **Authors**: Felipe Codevilla, Jean Gabriel Simard, Ross Goroshin, Chris Pal
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Recent work has shown that learned image compression strategies can outperform standard hand-crafted compression algorithms that have been developed over decades of intensive research on the rate-distortion trade-off. With growing applications of computer vision, high quality image reconstruction from a compressible representation is often a secondary objective. Compression that ensures high accuracy on computer vision tasks such as image segmentation, classification, and detection therefore has the potential for significant impact across a wide variety of settings. In this work, we develop a framework that produces a compression format suitable for both human perception and machine perception. We show that representations can be learned that simultaneously optimize for compression and performance on core vision tasks. Our approach allows models to be trained directly from compressed representations, and this approach yields increased performance on new tasks and in low-shot learning settings. We present results that improve upon segmentation and detection performance compared to standard high quality JPGs, but with representations that are four to ten times smaller in terms of bits per pixel. Further, unlike naive compression methods, at a level ten times smaller than standard JEPGs, segmentation and detection models trained from our format suffer only minor degradation in performance.



### Multi-Cue Adaptive Emotion Recognition Network
- **Arxiv ID**: http://arxiv.org/abs/2111.02273v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2111.02273v2)
- **Published**: 2021-11-03 15:08:55+00:00
- **Updated**: 2021-12-14 12:33:05+00:00
- **Authors**: Willams Costa, David Macêdo, Cleber Zanchettin, Lucas S. Figueiredo, Veronica Teichrieb
- **Comment**: None
- **Journal**: None
- **Summary**: Expressing and identifying emotions through facial and physical expressions is a significant part of social interaction. Emotion recognition is an essential task in computer vision due to its various applications and mainly for allowing a more natural interaction between humans and machines. The common approaches for emotion recognition focus on analyzing facial expressions and requires the automatic localization of the face in the image. Although these methods can correctly classify emotion in controlled scenarios, such techniques are limited when dealing with unconstrained daily interactions. We propose a new deep learning approach for emotion recognition based on adaptive multi-cues that extract information from context and body poses, which humans commonly use in social interaction and communication. We compare the proposed approach with the state-of-art approaches in the CAER-S dataset, evaluating different components in a pipeline that reached an accuracy of 89.30%



### A Comparison of Deep Learning Models for the Prediction of Hand Hygiene Videos
- **Arxiv ID**: http://arxiv.org/abs/2111.02322v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.02322v1)
- **Published**: 2021-11-03 16:15:55+00:00
- **Updated**: 2021-11-03 16:15:55+00:00
- **Authors**: Rashmi Bakshi
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2110.02842
- **Journal**: None
- **Summary**: This paper presents a comparison of various deep learning models such as Exception, Resnet-50, and Inception V3 for the classification and prediction of hand hygiene gestures, which were recorded in accordance with the World Health Organization (WHO) guidelines. The dataset consists of six hand hygiene movements in a video format, gathered for 30 participants. The network consists of pre-trained models with image net weights and a modified head of the model. An accuracy of 37% (Xception model), 33% (Inception V3), and 72% (ResNet-50) is achieved in the classification report after the training of the models for 25 epochs. ResNet-50 model clearly outperforms with correct class predictions. The major speed limitation can be overcome with the use of fast processing GPU for future work. A complete hand hygiene dataset along with other generic gestures such as one-hand movements (linear hand motion; circular hand rotation) will be tested with ResNet-50 architecture and the variants for health care workers.



### ML-PersRef: A Machine Learning-based Personalized Multimodal Fusion Approach for Referencing Outside Objects From a Moving Vehicle
- **Arxiv ID**: http://arxiv.org/abs/2111.02327v1
- **DOI**: 10.1145/3462244.3479910
- **Categories**: **cs.HC**, cs.CV, H.5.2; H.1.2
- **Links**: [PDF](http://arxiv.org/pdf/2111.02327v1)
- **Published**: 2021-11-03 16:22:17+00:00
- **Updated**: 2021-11-03 16:22:17+00:00
- **Authors**: Amr Gomaa, Guillermo Reyes, Michael Feld
- **Comment**: None
- **Journal**: In Proceedings of the 2021 International Conference on Multimodal
  Interaction, pp. 318-327. 2021
- **Summary**: Over the past decades, the addition of hundreds of sensors to modern vehicles has led to an exponential increase in their capabilities. This allows for novel approaches to interaction with the vehicle that go beyond traditional touch-based and voice command approaches, such as emotion recognition, head rotation, eye gaze, and pointing gestures. Although gaze and pointing gestures have been used before for referencing objects inside and outside vehicles, the multimodal interaction and fusion of these gestures have so far not been extensively studied. We propose a novel learning-based multimodal fusion approach for referencing outside-the-vehicle objects while maintaining a long driving route in a simulated environment. The proposed multimodal approaches outperform single-modality approaches in multiple aspects and conditions. Moreover, we also demonstrate possible ways to exploit behavioral differences between users when completing the referencing task to realize an adaptable personalized system for each driver. We propose a personalization technique based on the transfer-of-learning concept for exceedingly small data sizes to enhance prediction and adapt to individualistic referencing behavior. Our code is publicly available at https://github.com/amr-gomaa/ML-PersRef.



### LTD: Low Temperature Distillation for Robust Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2111.02331v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.02331v3)
- **Published**: 2021-11-03 16:26:00+00:00
- **Updated**: 2023-06-30 06:56:18+00:00
- **Authors**: Erh-Chung Chen, Che-Rung Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial training has been widely used to enhance the robustness of neural network models against adversarial attacks. Despite the popularity of neural network models, a significant gap exists between the natural and robust accuracy of these models. In this paper, we identify one of the primary reasons for this gap is the common use of one-hot vectors as labels, which hinders the learning process for image recognition. Representing ambiguous images with one-hot vectors is imprecise and may lead the model to suboptimal solutions. To overcome this issue, we propose a novel method called Low Temperature Distillation (LTD) that generates soft labels using the modified knowledge distillation framework. Unlike previous approaches, LTD uses a relatively low temperature in the teacher model and fixed, but different temperatures for the teacher and student models. This modification boosts the model's robustness without encountering the gradient masking problem that has been addressed in defensive distillation. The experimental results demonstrate the effectiveness of the proposed LTD method combined with previous techniques, achieving robust accuracy rates of 58.19%, 31.13%, and 42.08% on CIFAR-10, CIFAR-100, and ImageNet data sets, respectively, without additional unlabeled data.



### HS3: Learning with Proper Task Complexity in Hierarchically Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2111.02333v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.02333v1)
- **Published**: 2021-11-03 16:33:29+00:00
- **Updated**: 2021-11-03 16:33:29+00:00
- **Authors**: Shubhankar Borse, Hong Cai, Yizhe Zhang, Fatih Porikli
- **Comment**: Accepted to BMVC 2021
- **Journal**: None
- **Summary**: While deeply supervised networks are common in recent literature, they typically impose the same learning objective on all transitional layers despite their varying representation powers.   In this paper, we propose Hierarchically Supervised Semantic Segmentation (HS3), a training scheme that supervises intermediate layers in a segmentation network to learn meaningful representations by varying task complexity. To enforce a consistent performance vs. complexity trade-off throughout the network, we derive various sets of class clusters to supervise each transitional layer of the network. Furthermore, we devise a fusion framework, HS3-Fuse, to aggregate the hierarchical features generated by these layers, which can provide rich semantic contexts and further enhance the final segmentation. Extensive experiments show that our proposed HS3 scheme considerably outperforms vanilla deep supervision with no added inference cost. Our proposed HS3-Fuse framework further improves segmentation predictions and achieves state-of-the-art results on two large segmentation benchmarks: NYUD-v2 and Cityscapes.



### Partial supervision for the FeTA challenge 2021
- **Arxiv ID**: http://arxiv.org/abs/2111.02408v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.02408v1)
- **Published**: 2021-11-03 16:48:39+00:00
- **Updated**: 2021-11-03 16:48:39+00:00
- **Authors**: Lucas Fidon, Michael Aertsen, Suprosanna Shit, Philippe Demaerel, Sébastien Ourselin, Jan Deprest, Tom Vercauteren
- **Comment**: Accepted as a poster at the MICCAI 2021 Perinatal, Preterm and
  Paediatric Image Analysis (PIPPI) workshop
- **Journal**: None
- **Summary**: This paper describes our method for our participation in the FeTA challenge2021 (team name: TRABIT). The performance of convolutional neural networks for medical image segmentation is thought to correlate positively with the number of training data. The FeTA challenge does not restrict participants to using only the provided training data but also allows for using other publicly available sources. Yet, open access fetal brain data remains limited. An advantageous strategy could thus be to expand the training data to cover broader perinatal brain imaging sources. Perinatal brain MRIs, other than the FeTA challenge data, that are currently publicly available, span normal and pathological fetal atlases as well as neonatal scans. However, perinatal brain MRIs segmented in different datasets typically come with different annotation protocols. This makes it challenging to combine those datasets to train a deep neural network. We recently proposed a family of loss functions, the label-set loss functions, for partially supervised learning. Label-set loss functions allow to train deep neural networks with partially segmented images, i.e. segmentations in which some classes may be grouped into super-classes. We propose to use label-set loss functions to improve the segmentation performance of a state-of-the-art deep learning pipeline for multi-class fetal brain segmentation by merging several publicly available datasets. To promote generalisability, our approach does not introduce any additional hyper-parameters tuning.



### Breast Cancer Classification Using: Pixel Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2111.02409v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.02409v1)
- **Published**: 2021-11-03 16:58:17+00:00
- **Updated**: 2021-11-03 16:58:17+00:00
- **Authors**: Osama Rezq Shahin, Hamdy Mohammed Kelash, Gamal Mahrous Attiya, Osama Slah Farg Allah
- **Comment**: 9 pages, 9 figures, Acta Scientific Computer Sciences
- **Journal**: Acta Scientific Open Access, Publication date 2021/10/28
- **Summary**: Image Processing represents the backbone research area within engineering and computer science specialization. It is promptly growing technologies today, and its applications founded in various aspects of biomedical fields especially in cancer disease. Breast cancer is considered the fatal one of all cancer types according to recent statistics all over the world. It is the most commonly cancer in women and the second reason of cancer death between females. About 23% of the total cancer cases in both developing and developed countries. In this work, an interpolation process was used to classify the breast cancer into main types, benign and malignant. This scheme dependent on the morphologic spectrum of mammographic masses. Malignant tumors had irregular shape percent higher than the benign tumors. By this way the boundary of the tumor will be interpolated by additional pixels to make the boundary smoothen as possible, these needed pixels is proportional with irregularity shape of the tumor, so that the increasing in interpolated pixels meaning the tumor goes toward the malignant case. The proposed system is implemented using MATLAB programming and tested over several images taken from the Mammogram Image Analysis Society (MIAS) image database. The MIAS offers a regular classification for mammographic studies. The system works faster so that any radiologist can take a clear decision about the appearance of calcifications by visual inspection.



### VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts
- **Arxiv ID**: http://arxiv.org/abs/2111.02358v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.02358v2)
- **Published**: 2021-11-03 17:20:36+00:00
- **Updated**: 2022-05-27 15:44:26+00:00
- **Authors**: Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti Aggarwal, Subhojit Som, Furu Wei
- **Comment**: Work in progress
- **Journal**: None
- **Summary**: We present a unified Vision-Language pretrained Model (VLMo) that jointly learns a dual encoder and a fusion encoder with a modular Transformer network. Specifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer, where each block contains a pool of modality-specific experts and a shared self-attention layer. Because of the modeling flexibility of MoME, pretrained VLMo can be fine-tuned as a fusion encoder for vision-language classification tasks, or used as a dual encoder for efficient image-text retrieval. Moreover, we propose a stagewise pre-training strategy, which effectively leverages large-scale image-only and text-only data besides image-text pairs. Experimental results show that VLMo achieves state-of-the-art results on various vision-language tasks, including VQA, NLVR2 and image-text retrieval. The code and pretrained models are available at https://aka.ms/vlmo.



### Subpixel Heatmap Regression for Facial Landmark Localization
- **Arxiv ID**: http://arxiv.org/abs/2111.02360v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.02360v1)
- **Published**: 2021-11-03 17:21:28+00:00
- **Updated**: 2021-11-03 17:21:28+00:00
- **Authors**: Adrian Bulat, Enrique Sanchez, Georgios Tzimiropoulos
- **Comment**: Accepted at BMVC 2021
- **Journal**: None
- **Summary**: Deep Learning models based on heatmap regression have revolutionized the task of facial landmark localization with existing models working robustly under large poses, non-uniform illumination and shadows, occlusions and self-occlusions, low resolution and blur. However, despite their wide adoption, heatmap regression approaches suffer from discretization-induced errors related to both the heatmap encoding and decoding process. In this work we show that these errors have a surprisingly large negative impact on facial alignment accuracy. To alleviate this problem, we propose a new approach for the heatmap encoding and decoding process by leveraging the underlying continuous distribution. To take full advantage of the newly proposed encoding-decoding mechanism, we also introduce a Siamese-based training that enforces heatmap consistency across various geometric image transformations. Our approach offers noticeable gains across multiple datasets setting a new state-of-the-art result in facial landmark localization. Code alongside the pretrained models will be made available at https://www.adrianbulat.com/face-alignment



### Video Salient Object Detection via Contrastive Features and Attention Modules
- **Arxiv ID**: http://arxiv.org/abs/2111.02368v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.02368v1)
- **Published**: 2021-11-03 17:40:32+00:00
- **Updated**: 2021-11-03 17:40:32+00:00
- **Authors**: Yi-Wen Chen, Xiaojie Jin, Xiaohui Shen, Ming-Hsuan Yang
- **Comment**: Accepted in WACV 2022
- **Journal**: None
- **Summary**: Video salient object detection aims to find the most visually distinctive objects in a video. To explore the temporal dependencies, existing methods usually resort to recurrent neural networks or optical flow. However, these approaches require high computational cost, and tend to accumulate inaccuracies over time. In this paper, we propose a network with attention modules to learn contrastive features for video salient object detection without the high computational temporal modeling techniques. We develop a non-local self-attention scheme to capture the global information in the video frame. A co-attention formulation is utilized to combine the low-level and high-level features. We further apply the contrastive learning to improve the feature representations, where foreground region pairs from the same video are pulled together, and foreground-background region pairs are pushed away in the latent space. The intra-frame contrastive loss helps separate the foreground and background features, and the inter-frame contrastive loss improves the temporal consistency. We conduct extensive experiments on several benchmark datasets for video salient object detection and unsupervised video object segmentation, and show that the proposed method requires less computation, and performs favorably against the state-of-the-art approaches.



### An Empirical Study of Training End-to-End Vision-and-Language Transformers
- **Arxiv ID**: http://arxiv.org/abs/2111.02387v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.02387v3)
- **Published**: 2021-11-03 17:55:36+00:00
- **Updated**: 2022-03-18 03:29:10+00:00
- **Authors**: Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, Zicheng Liu, Michael Zeng
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Vision-and-language (VL) pre-training has proven to be highly effective on various VL downstream tasks. While recent work has shown that fully transformer-based VL models can be more efficient than previous region-feature-based methods, their performance on downstream tasks often degrades significantly. In this paper, we present METER, a Multimodal End-to-end TransformER framework, through which we investigate how to design and pre-train a fully transformer-based VL model in an end-to-end manner. Specifically, we dissect the model designs along multiple dimensions: vision encoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa, DeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention), architectural design (e.g., encoder-only vs. encoder-decoder), and pre-training objectives (e.g., masked image modeling). We conduct comprehensive experiments and provide insights on how to train a performant VL transformer. METER achieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images for pre-training, surpassing the state-of-the-art region-feature-based model by 1.04%, and outperforming the previous best fully transformer-based model by 1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy of 80.54%. Code and pre-trained models are released at https://github.com/zdou0830/METER.



### FAST: Faster Arbitrarily-Shaped Text Detector with Minimalist Kernel Representation
- **Arxiv ID**: http://arxiv.org/abs/2111.02394v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.02394v2)
- **Published**: 2021-11-03 17:58:47+00:00
- **Updated**: 2023-01-11 14:04:01+00:00
- **Authors**: Zhe Chen, Jiahao Wang, Wenhai Wang, Guo Chen, Enze Xie, Ping Luo, Tong Lu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an accurate and efficient scene text detection framework, termed FAST (i.e., faster arbitrarily-shaped text detector). Different from recent advanced text detectors that used complicated post-processing and hand-crafted network architectures, resulting in low inference speed, FAST has two new designs. (1) We design a minimalist kernel representation (only has 1-channel output) to model text with arbitrary shape, as well as a GPU-parallel post-processing to efficiently assemble text lines with a negligible time overhead. (2) We search the network architecture tailored for text detection, leading to more powerful features than most networks that are searched for image classification. Benefiting from these two designs, FAST achieves an excellent trade-off between accuracy and efficiency on several challenging datasets, including Total Text, CTW1500, ICDAR 2015, and MSRA-TD500. For example, FAST-T yields 81.6% F-measure at 152 FPS on Total-Text, outperforming the previous fastest method by 1.7 points and 70 FPS in terms of accuracy and speed. With TensorRT optimization, the inference speed can be further accelerated to over 600 FPS. Code and models will be released at https://github.com/czczup/FAST.



### Panoptic 3D Scene Reconstruction From a Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/2111.02444v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.02444v2)
- **Published**: 2021-11-03 18:06:38+00:00
- **Updated**: 2022-05-16 15:51:09+00:00
- **Authors**: Manuel Dahnert, Ji Hou, Matthias Nießner, Angela Dai
- **Comment**: Video: https://youtu.be/YVxRNHmd5SA, Project Page:
  https://manuel-dahnert.com/research/panoptic-reconstruction
- **Journal**: None
- **Summary**: Understanding 3D scenes from a single image is fundamental to a wide variety of tasks, such as for robotics, motion planning, or augmented reality. Existing works in 3D perception from a single RGB image tend to focus on geometric reconstruction only, or geometric reconstruction with semantic segmentation or instance segmentation. Inspired by 2D panoptic segmentation, we propose to unify the tasks of geometric reconstruction, 3D semantic segmentation, and 3D instance segmentation into the task of panoptic 3D scene reconstruction - from a single RGB image, predicting the complete geometric reconstruction of the scene in the camera frustum of the image, along with semantic and instance segmentations. We thus propose a new approach for holistic 3D scene understanding from a single RGB image which learns to lift and propagate 2D features from an input image to a 3D volumetric scene representation. We demonstrate that this holistic view of joint scene reconstruction, semantic, and instance segmentation is beneficial over treating the tasks independently, thus outperforming alternative approaches.



### On the Frequency Bias of Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2111.02447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.02447v1)
- **Published**: 2021-11-03 18:12:11+00:00
- **Updated**: 2021-11-03 18:12:11+00:00
- **Authors**: Katja Schwarz, Yiyi Liao, Andreas Geiger
- **Comment**: None
- **Journal**: None
- **Summary**: The key objective of Generative Adversarial Networks (GANs) is to generate new data with the same statistics as the provided training data. However, multiple recent works show that state-of-the-art architectures yet struggle to achieve this goal. In particular, they report an elevated amount of high frequencies in the spectral statistics which makes it straightforward to distinguish real and generated images. Explanations for this phenomenon are controversial: While most works attribute the artifacts to the generator, other works point to the discriminator. We take a sober look at those explanations and provide insights on what makes proposed measures against high-frequency artifacts effective. To achieve this, we first independently assess the architectures of both the generator and discriminator and investigate if they exhibit a frequency bias that makes learning the distribution of high-frequency content particularly problematic. Based on these experiments, we make the following four observations: 1) Different upsampling operations bias the generator towards different spectral properties. 2) Checkerboard artifacts introduced by upsampling cannot explain the spectral discrepancies alone as the generator is able to compensate for these artifacts. 3) The discriminator does not struggle with detecting high frequencies per se but rather struggles with frequencies of low magnitude. 4) The downsampling operations in the discriminator can impair the quality of the training signal it provides. In light of these findings, we analyze proposed measures against high-frequency artifacts in state-of-the-art GAN training but find that none of the existing approaches can fully resolve spectral artifacts yet. Our results suggest that there is great potential in improving the discriminator and that this could be key to match the distribution of the training data more closely.



### Unified 3D Mesh Recovery of Humans and Animals by Learning Animal Exercise
- **Arxiv ID**: http://arxiv.org/abs/2111.02450v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2111.02450v1)
- **Published**: 2021-11-03 18:15:50+00:00
- **Updated**: 2021-11-03 18:15:50+00:00
- **Authors**: Kim Youwang, Kim Ji-Yeon, Kyungdon Joo, Tae-Hyun Oh
- **Comment**: BMVC 2021, 10 pages excluding reference page
- **Journal**: None
- **Summary**: We propose an end-to-end unified 3D mesh recovery of humans and quadruped animals trained in a weakly-supervised way. Unlike recent work focusing on a single target class only, we aim to recover 3D mesh of broader classes with a single multi-task model. However, there exists no dataset that can directly enable multi-task learning due to the absence of both human and animal annotations for a single object, e.g., a human image does not have animal pose annotations; thus, we have to devise a new way to exploit heterogeneous datasets. To make the unstable disjoint multi-task learning jointly trainable, we propose to exploit the morphological similarity between humans and animals, motivated by animal exercise where humans imitate animal poses. We realize the morphological similarity by semantic correspondences, called sub-keypoint, which enables joint training of human and animal mesh regression branches. Besides, we propose class-sensitive regularization methods to avoid a mean-shape bias and to improve the distinctiveness across multi-classes. Our method performs favorably against recent uni-modal models on various human and animal datasets while being far more compact.



### Slapping Cats, Bopping Heads, and Oreo Shakes: Understanding Indicators of Virality in TikTok Short Videos
- **Arxiv ID**: http://arxiv.org/abs/2111.02452v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2111.02452v1)
- **Published**: 2021-11-03 18:17:16+00:00
- **Updated**: 2021-11-03 18:17:16+00:00
- **Authors**: Chen Ling, Jeremy Blackburn, Emiliano De Cristofaro, Gianluca Stringhini
- **Comment**: None
- **Journal**: None
- **Summary**: Short videos have become one of the leading media used by younger generations to express themselves online and thus a driving force in shaping online culture. In this context, TikTok has emerged as a platform where viral videos are often posted first. In this paper, we study what elements of short videos posted on TikTok contribute to their virality. We apply a mixed-method approach to develop a codebook and identify important virality features. We do so vis-\`a-vis three research hypotheses; namely, that: 1) the video content, 2) TikTok's recommendation algorithm, and 3) the popularity of the video creator contribute to virality.   We collect and label a dataset of 400 TikTok videos and train classifiers to help us identify the features that influence virality the most. While the number of followers is the most powerful predictor, close-up and medium-shot scales also play an essential role. So does the lifespan of the video, the presence of text, and the point of view. Our research highlights the characteristics that distinguish viral from non-viral TikTok videos, laying the groundwork for developing additional approaches to create more engaging online content and proactively identify possibly risky content that is likely to reach a large audience.



### Automatic ultrasound vessel segmentation with deep spatiotemporal context learning
- **Arxiv ID**: http://arxiv.org/abs/2111.02461v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2111.02461v1)
- **Published**: 2021-11-03 18:30:15+00:00
- **Updated**: 2021-11-03 18:30:15+00:00
- **Authors**: Baichuan Jiang, Alvin Chen, Shyam Bharat, Mingxin Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate, real-time segmentation of vessel structures in ultrasound image sequences can aid in the measurement of lumen diameters and assessment of vascular diseases. This, however, remains a challenging task, particularly for extremely small vessels that are difficult to visualize. We propose to leverage the rich spatiotemporal context available in ultrasound to improve segmentation of small-scale lower-extremity arterial vasculature. We describe efficient deep learning methods that incorporate temporal, spatial, and feature-aware contextual embeddings at multiple resolution scales while jointly utilizing information from B-mode and Color Doppler signals. Evaluating on femoral and tibial artery scans performed on healthy subjects by an expert ultrasonographer, and comparing to consensus expert ground-truth annotations of inner lumen boundaries, we demonstrate real-time segmentation using the context-aware models and show that they significantly outperform comparable baseline approaches.



### Roadmap on Signal Processing for Next Generation Measurement Systems
- **Arxiv ID**: http://arxiv.org/abs/2111.02493v3
- **DOI**: 10.1088/1361-6501/ac2dbd
- **Categories**: **eess.SP**, cs.AI, cs.CV, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/2111.02493v3)
- **Published**: 2021-11-03 19:39:34+00:00
- **Updated**: 2022-01-28 16:42:18+00:00
- **Authors**: D. K. Iakovidis, M. Ooi, Y. C. Kuang, S. Demidenko, A. Shestakov, V. Sinitsin, M. Henry, A. Sciacchitano, A. Discetti, S. Donati, M. Norgia, A. Menychtas, I. Maglogiannis, S. C. Wriessnegger, L. A. Barradas Chacon, G. Dimas, D. Filos, A. H. Aletras, J. Töger, F. Dong, S. Ren, A. Uhl, J. Paziewski, J. Geng, F. Fioranelli, R. M. Narayanan, C. Fernandez, C. Stiller, K. Malamousi, S. Kamnis, K. Delibasis, D. Wang, J. Zhang, R. X. Gao
- **Comment**: 48 pages, https://iopscience.iop.org/article/10.1088/1361-6501/ac2dbd
- **Journal**: Measurement Science and Technology 33(1) (2022) 1-48
- **Summary**: Signal processing is a fundamental component of almost any sensor-enabled system, with a wide range of applications across different scientific disciplines. Time series data, images, and video sequences comprise representative forms of signals that can be enhanced and analysed for information extraction and quantification. The recent advances in artificial intelligence and machine learning are shifting the research attention towards intelligent, data-driven, signal processing. This roadmap presents a critical overview of the state-of-the-art methods and applications aiming to highlight future challenges and research opportunities towards next generation measurement systems. It covers a broad spectrum of topics ranging from basic to industrial research, organized in concise thematic sections that reflect the trends and the impacts of current and future developments per research field. Furthermore, it offers guidance to researchers and funding agencies in identifying new prospects.



### Improving Pose Estimation through Contextual Activity Fusion
- **Arxiv ID**: http://arxiv.org/abs/2111.02500v1
- **DOI**: 10.1007/978-3-030-95070-5_5
- **Categories**: **cs.CV**, cs.LG, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2111.02500v1)
- **Published**: 2021-11-03 19:58:13+00:00
- **Updated**: 2021-11-03 19:58:13+00:00
- **Authors**: David Poulton, Richard Klein
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: This research presents the idea of activity fusion into existing Pose Estimation architectures to enhance their predictive ability. This is motivated by the rise in higher level concepts found in modern machine learning architectures, and the belief that activity context is a useful piece of information for the problem of pose estimation. To analyse this concept we take an existing deep learning architecture and augment it with an additional 1x1 convolution to fuse activity information into the model. We perform evaluation and comparison on a common pose estimation dataset, and show a performance improvement over our baseline model, especially in uncommon poses and on typically difficult joints. Additionally, we perform an ablative analysis to indicate that the performance improvement does in fact draw from the activity information.



### Resampling and super-resolution of hexagonally sampled images using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2111.02520v1
- **DOI**: 10.1117/1.OE.60.10.103105
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, 68T45, I.4.4; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2111.02520v1)
- **Published**: 2021-11-03 20:58:33+00:00
- **Updated**: 2021-11-03 20:58:33+00:00
- **Authors**: Dylan Flaute, Russell C. Hardie, Hamed Elwarfalli
- **Comment**: 31 pages, 16 figures, 5 tables. \c{opyright} 2021 Society of
  Photo-Optical Instrumentation Engineers (SPIE)
- **Journal**: Optical Engineering 60(10), 103105 (29 October 2021)
- **Summary**: Super-resolution (SR) aims to increase the resolution of imagery. Applications include security, medical imaging, and object recognition. We propose a deep learning-based SR system that takes a hexagonally sampled low-resolution image as an input and generates a rectangularly sampled SR image as an output. For training and testing, we use a realistic observation model that includes optical degradation from diffraction and sensor degradation from detector integration. Our SR approach first uses non-uniform interpolation to partially upsample the observed hexagonal imagery and convert it to a rectangular grid. We then leverage a state-of-the-art convolutional neural network (CNN) architecture designed for SR known as Residual Channel Attention Network (RCAN). In particular, we use RCAN to further upsample and restore the imagery to produce the final SR image estimate. We demonstrate that this system is superior to applying RCAN directly to rectangularly sampled LR imagery with equivalent sample density. The theoretical advantages of hexagonal sampling are well known. However, to the best of our knowledge, the practical benefit of hexagonal sampling in light of modern processing techniques such as RCAN SR is heretofore untested. Our SR system demonstrates a notable advantage of hexagonally sampled imagery when employing a modified RCAN for hexagonal SR.



### Sequence-to-Sequence Modeling for Action Identification at High Temporal Resolution
- **Arxiv ID**: http://arxiv.org/abs/2111.02521v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2111.02521v1)
- **Published**: 2021-11-03 21:06:36+00:00
- **Updated**: 2021-11-03 21:06:36+00:00
- **Authors**: Aakash Kaku, Kangning Liu, Avinash Parnandi, Haresh Rengaraj Rajamohan, Kannan Venkataramanan, Anita Venkatesan, Audre Wirtanen, Natasha Pandit, Heidi Schambra, Carlos Fernandez-Granda
- **Comment**: Under review as a conference paper at ICLR 2022
- **Journal**: None
- **Summary**: Automatic action identification from video and kinematic data is an important machine learning problem with applications ranging from robotics to smart health. Most existing works focus on identifying coarse actions such as running, climbing, or cutting a vegetable, which have relatively long durations. This is an important limitation for applications that require the identification of subtle motions at high temporal resolution. For example, in stroke recovery, quantifying rehabilitation dose requires differentiating motions with sub-second durations. Our goal is to bridge this gap. To this end, we introduce a large-scale, multimodal dataset, StrokeRehab, as a new action-recognition benchmark that includes subtle short-duration actions labeled at a high temporal resolution. These short-duration actions are called functional primitives, and consist of reaches, transports, repositions, stabilizations, and idles. The dataset consists of high-quality Inertial Measurement Unit sensors and video data of 41 stroke-impaired patients performing activities of daily living like feeding, brushing teeth, etc. We show that current state-of-the-art models based on segmentation produce noisy predictions when applied to these data, which often leads to overcounting of actions. To address this, we propose a novel approach for high-resolution action identification, inspired by speech-recognition techniques, which is based on a sequence-to-sequence model that directly predicts the sequence of actions. This approach outperforms current state-of-the-art methods on the StrokeRehab dataset, as well as on the standard benchmark datasets 50Salads, Breakfast, and Jigsaws.



### Understanding Cross Domain Presentation Attack Detection for Visible Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2111.02548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2111.02548v1)
- **Published**: 2021-11-03 22:25:45+00:00
- **Updated**: 2021-11-03 22:25:45+00:00
- **Authors**: Jennifer Hamblin, Kshitij Nikhal, Benjamin S. Riggan
- **Comment**: None
- **Journal**: None
- **Summary**: Face signatures, including size, shape, texture, skin tone, eye color, appearance, and scars/marks, are widely used as discriminative, biometric information for access control. Despite recent advancements in facial recognition systems, presentation attacks on facial recognition systems have become increasingly sophisticated. The ability to detect presentation attacks or spoofing attempts is a pressing concern for the integrity, security, and trust of facial recognition systems. Multi-spectral imaging has been previously introduced as a way to improve presentation attack detection by utilizing sensors that are sensitive to different regions of the electromagnetic spectrum (e.g., visible, near infrared, long-wave infrared). Although multi-spectral presentation attack detection systems may be discriminative, the need for additional sensors and computational resources substantially increases complexity and costs. Instead, we propose a method that exploits information from infrared imagery during training to increase the discriminability of visible-based presentation attack detection systems. We introduce (1) a new cross-domain presentation attack detection framework that increases the separability of bonafide and presentation attacks using only visible spectrum imagery, (2) an inverse domain regularization technique for added training stability when optimizing our cross-domain presentation attack detection framework, and (3) a dense domain adaptation subnetwork to transform representations between visible and non-visible domains.



