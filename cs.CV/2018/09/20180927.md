# Arxiv Papers in cs.CV on 2018-09-27
### Semantically Invariant Text-to-Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1809.10274v1
- **DOI**: 10.1109/ICIP.2018.8451656
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.10274v1)
- **Published**: 2018-09-27 00:11:25+00:00
- **Updated**: 2018-09-27 00:11:25+00:00
- **Authors**: Shagan Sah, Dheeraj Peri, Ameya Shringi, Chi Zhang, Miguel Dominguez, Andreas Savakis, Ray Ptucha
- **Comment**: 5 papers, 5 figures, Published in 2018 25th IEEE International
  Conference on Image Processing (ICIP)
- **Journal**: None
- **Summary**: Image captioning has demonstrated models that are capable of generating plausible text given input images or videos. Further, recent work in image generation has shown significant improvements in image quality when text is used as a prior. Our work ties these concepts together by creating an architecture that can enable bidirectional generation of images and text. We call this network Multi-Modal Vector Representation (MMVR). Along with MMVR, we propose two improvements to the text conditioned image generation. Firstly, a n-gram metric based cost function is introduced that generalizes the caption with respect to the image. Secondly, multiple semantically similar sentences are shown to help in generating better images. Qualitative and quantitative evaluations demonstrate that MMVR improves upon existing text conditioned image generation results by over 20%, while integrating visual and text modalities.



### Unsupervised Person Image Synthesis in Arbitrary Poses
- **Arxiv ID**: http://arxiv.org/abs/1809.10280v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10280v1)
- **Published**: 2018-09-27 00:32:22+00:00
- **Updated**: 2018-09-27 00:32:22+00:00
- **Authors**: Albert Pumarola, Antonio Agudo, Alberto Sanfeliu, Francesc Moreno-Noguer
- **Comment**: Accepted as Spotlight at CVPR 2018
- **Journal**: None
- **Summary**: We present a novel approach for synthesizing photo-realistic images of people in arbitrary poses using generative adversarial learning. Given an input image of a person and a desired pose represented by a 2D skeleton, our model renders the image of the same person under the new pose, synthesizing novel views of the parts visible in the input image and hallucinating those that are not seen. This problem has recently been addressed in a supervised manner, i.e., during training the ground truth images under the new poses are given to the network. We go beyond these approaches by proposing a fully unsupervised strategy. We tackle this challenging scenario by splitting the problem into two principal subtasks. First, we consider a pose conditioned bidirectional generator that maps back the initially rendered image to the original pose, hence being directly comparable to the input image without the need to resort to any training image. Second, we devise a novel loss function that incorporates content and style terms, and aims at producing images of high perceptual quality. Extensive experiments conducted on the DeepFashion dataset demonstrate that the images rendered by our model are very close in appearance to those obtained by fully supervised approaches.



### Geometry-Aware Network for Non-Rigid Shape Prediction from a Single View
- **Arxiv ID**: http://arxiv.org/abs/1809.10305v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10305v1)
- **Published**: 2018-09-27 01:44:11+00:00
- **Updated**: 2018-09-27 01:44:11+00:00
- **Authors**: Albert Pumarola, Antonio Agudo, Lorenzo Porzi, Alberto Sanfeliu, Vincent Lepetit, Francesc Moreno-Noguer
- **Comment**: Accepted at CVPR 2018
- **Journal**: None
- **Summary**: We propose a method for predicting the 3D shape of a deformable surface from a single view. By contrast with previous approaches, we do not need a pre-registered template of the surface, and our method is robust to the lack of texture and partial occlusions. At the core of our approach is a {\it geometry-aware} deep architecture that tackles the problem as usually done in analytic solutions: first perform 2D detection of the mesh and then estimate a 3D shape that is geometrically consistent with the image. We train this architecture in an end-to-end manner using a large dataset of synthetic renderings of shapes under different levels of deformation, material properties, textures and lighting conditions. We evaluate our approach on a test split of this dataset and available real benchmarks, consistently improving state-of-the-art solutions with a significantly lower computational time.



### Vision-based Navigation of Autonomous Vehicle in Roadway Environments with Unexpected Hazards
- **Arxiv ID**: http://arxiv.org/abs/1810.03967v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1810.03967v3)
- **Published**: 2018-09-27 02:08:21+00:00
- **Updated**: 2019-05-20 17:31:59+00:00
- **Authors**: Mhafuzul Islam, Mahsrur Chowdhury, Hongda Li, Hongxin Hu
- **Comment**: 17 pages, 12 images
- **Journal**: None
- **Summary**: Vision-based navigation of autonomous vehicles primarily depends on the Deep Neural Network (DNN) based systems in which the controller obtains input from sensors/detectors, such as cameras and produces a vehicle control output, such as a steering wheel angle to navigate the vehicle safely in a roadway traffic environment. Typically, these DNN-based systems of the autonomous vehicle are trained through supervised learning; however, recent studies show that a trained DNN-based system can be compromised by perturbation or adversarial inputs. Similarly, this perturbation can be introduced into the DNN-based systems of autonomous vehicle by unexpected roadway hazards, such as debris and roadblocks. In this study, we first introduce a roadway hazardous environment (both intentional and unintentional roadway hazards) that can compromise the DNN-based navigational system of an autonomous vehicle, and produces an incorrect steering wheel angle, which can cause crashes resulting in fatality and injury. Then, we develop a DNN-based autonomous vehicle driving system using object detection and semantic segmentation to mitigate the adverse effect of this type of hazardous environment, which helps the autonomous vehicle to navigate safely around such hazards. We find that our developed DNN-based autonomous vehicle driving system including hazardous object detection and semantic segmentation improves the navigational ability of an autonomous vehicle to avoid a potential hazard by 21% compared to the traditional DNN-based autonomous vehicle driving system.



### Diagnostics in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1809.10328v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10328v1)
- **Published**: 2018-09-27 03:24:49+00:00
- **Updated**: 2018-09-27 03:24:49+00:00
- **Authors**: Vladimir Nekrasov, Chunhua Shen, Ian Reid
- **Comment**: Supplementary Material: https://cv-conf.shinyapps.io/diag-sem-segm/
- **Journal**: None
- **Summary**: Over the past years, computer vision community has contributed to enormous progress in semantic image segmentation, a per-pixel classification task, crucial for dense scene understanding and rapidly becoming vital in lots of real-world applications, including driverless cars and medical imaging. Most recent models are now reaching previously unthinkable numbers (e.g., 89% mean iou on PASCAL VOC, 83% on CityScapes), and, while intersection-over-union and a range of other metrics provide the general picture of model performance, in this paper we aim to extend them into other meaningful and important for applications characteristics, answering such questions as 'how accurate the model segmentation is on small objects in the general scene?', or 'what are the sources of uncertainty that cause the model to make an erroneous prediction?'. Besides establishing a methodology that covers the performance of a single model from different perspectives, we also showcase several extensions that can be worth pursuing in order to further improve current results in semantic segmentation.



### Multi-View Frame Reconstruction with Conditional GAN
- **Arxiv ID**: http://arxiv.org/abs/1809.10352v1
- **DOI**: 10.1109/GlobalSIP.2018.8646380
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1809.10352v1)
- **Published**: 2018-09-27 05:49:52+00:00
- **Updated**: 2018-09-27 05:49:52+00:00
- **Authors**: Tahmida Mahmud, Mohammad Billah, Amit K. Roy-Chowdhury
- **Comment**: 5 pages, 4 figures, 3 tables, Accepted at IEEE Global Conference on
  Signal and Information Processing, 2018
- **Journal**: None
- **Summary**: Multi-view frame reconstruction is an important problem particularly when multiple frames are missing and past and future frames within the camera are far apart from the missing ones. Realistic coherent frames can still be reconstructed using corresponding frames from other overlapping cameras. We propose an adversarial approach to learn the spatio-temporal representation of the missing frame using conditional Generative Adversarial Network (cGAN). The conditional input to each cGAN is the preceding or following frames within the camera or the corresponding frames in other overlapping cameras, all of which are merged together using a weighted average. Representations learned from frames within the camera are given more weight compared to the ones learned from other cameras when they are close to the missing frames and vice versa. Experiments on two challenging datasets demonstrate that our framework produces comparable results with the state-of-the-art reconstruction method in a single camera and achieves promising performance in multi-camera scenario.



### An Intelligent Extraversion Analysis Scheme from Crowd Trajectories for Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1809.10398v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10398v2)
- **Published**: 2018-09-27 08:28:58+00:00
- **Updated**: 2020-01-21 02:29:23+00:00
- **Authors**: Wenxi Liu, Yuanlong Yu, Chun-Yang Zhang, Genggeng Liu, Naixue Xiong
- **Comment**: require modification
- **Journal**: None
- **Summary**: In recent years, crowd analysis is important for applications such as smart cities, intelligent transportation system, customer behavior prediction, and visual surveillance. Understanding the characteristics of the individual motion in a crowd can be beneficial for social event detection and abnormal detection, but it has rarely been studied. In this paper, we focus on the extraversion measure of individual motions in crowds based on trajectory data. Extraversion is one of typical personalities that is often observed in human crowd behaviors and it can reflect not only the characteristics of the individual motion, but also the that of the holistic crowd motions. To our best knowledge, this is the first attempt to analyze individual extraversion of crowd motions based on trajectories. To accomplish this, we first present a effective composite motion descriptor, which integrates the basic individual motion information and social metrics, to describe the extraversion of each individual in a crowd. The social metrics consider both the neighboring distribution and their interaction pattern. Since our major goal is to learn a universal scoring function that can measure the degrees of extraversion across varied crowd scenes, we incorporate and adapt the active learning technique to the relative attribute approach. Specifically, we assume the social groups in any crowds contain individuals with the similar degree of extraversion. Based on such assumption, we significantly reduce the computation cost by clustering and ranking the trajectories actively. Finally, we demonstrate the performance of our proposed method by measuring the degree of extraversion for real individual trajectories in crowds and analyzing crowd scenes from a real-world dataset.



### Image Reconstruction Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.10410v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10410v1)
- **Published**: 2018-09-27 08:58:38+00:00
- **Updated**: 2018-09-27 08:58:38+00:00
- **Authors**: Po-Yu Liu, Edmund Y. Lam
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a deep learning architecture that attains statistically significant improvements over traditional algorithms in Poisson image denoising espically when the noise is strong. Poisson noise commonly occurs in low-light and photon- limited settings, where the noise can be most accurately modeled by the Poission distribution. Poisson noise traditionally prevails only in specific fields such as astronomical imaging. However, with the booming market of surveillance cameras, which commonly operate in low-light environments, or mobile phones, which produce noisy night scene pictures due to lower-grade sensors, the necessity for an advanced Poisson image denoising algorithm has increased. Deep learning has achieved amazing breakthroughs in other imaging problems, such image segmentation and recognition, and this paper proposes a deep learning denoising network that outperforms traditional algorithms in Poisson denoising especially when the noise is strong. The architecture incorporates a hybrid of convolutional and deconvolutional layers along with symmetric connections. The denoising network achieved statistically significant 0.38dB, 0.68dB, and 1.04dB average PSNR gains over benchmark traditional algorithms in experiments with image peak values 4, 2, and 1. The denoising network can also operate with shorter computational time while still outperforming the benchmark algorithm by tuning the reconstruction stride sizes.



### Deformable Object Tracking with Gated Fusion
- **Arxiv ID**: http://arxiv.org/abs/1809.10417v2
- **DOI**: 10.1109/TIP.2019.2902784
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10417v2)
- **Published**: 2018-09-27 09:15:27+00:00
- **Updated**: 2019-04-11 07:34:35+00:00
- **Authors**: Wenxi Liu, Yibing Song, Dengsheng Chen, Shengfeng He, Yuanlong Yu, Tao Yan, Gerhard P. Hancke, Rynson W. H. Lau
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, 2019
- **Summary**: The tracking-by-detection framework receives growing attentions through the integration with the Convolutional Neural Networks (CNNs). Existing tracking-by-detection based methods, however, fail to track objects with severe appearance variations. This is because the traditional convolutional operation is performed on fixed grids, and thus may not be able to find the correct response while the object is changing pose or under varying environmental conditions. In this paper, we propose a deformable convolution layer to enrich the target appearance representations in the tracking-by-detection framework. We aim to capture the target appearance variations via deformable convolution, which adaptively enhances its original features. In addition, we also propose a gated fusion scheme to control how the variations captured by the deformable convolution affect the original appearance. The enriched feature representation through deformable convolution facilitates the discrimination of the CNN classifier on the target object and background. Extensive experiments on the standard benchmarks show that the proposed tracker performs favorably against state-of-the-art methods.



### Towards increased trustworthiness of deep learning segmentation methods on cardiac MRI
- **Arxiv ID**: http://arxiv.org/abs/1809.10430v3
- **DOI**: 10.1117/12.2511699
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10430v3)
- **Published**: 2018-09-27 09:49:46+00:00
- **Updated**: 2019-01-28 09:30:59+00:00
- **Authors**: Jörg Sander, Bob D. de Vos, Jelmer M. Wolterink, Ivana Išgum
- **Comment**: This work has been submitted to SPIE 2019 conference
- **Journal**: None
- **Summary**: Current state-of-the-art deep learning segmentation methods have not yet made a broad entrance into the clinical setting in spite of high demand for such automatic methods. One important reason is the lack of reliability caused by models that fail unnoticed and often locally produce anatomically implausible results that medical experts would not make. This paper presents an automatic image segmentation method based on (Bayesian) dilated convolutional networks (DCNN) that generate segmentation masks and spatial uncertainty maps for the input image at hand. The method was trained and evaluated using segmentation of the left ventricle (LV) cavity, right ventricle (RV) endocardium and myocardium (Myo) at end-diastole (ED) and end-systole (ES) in 100 cardiac 2D MR scans from the MICCAI 2017 Challenge (ACDC). Combining segmentations and uncertainty maps and employing a human-in-the-loop setting, we provide evidence that image areas indicated as highly uncertain regarding the obtained segmentation almost entirely cover regions of incorrect segmentations. The fused information can be harnessed to increase segmentation performance. Our results reveal that we can obtain valuable spatial uncertainty maps with low computational effort using DCNNs.



### A Generative Adversarial Model for Right Ventricle Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.03969v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.03969v1)
- **Published**: 2018-09-27 09:52:10+00:00
- **Updated**: 2018-09-27 09:52:10+00:00
- **Authors**: Nicoló Savioli, Miguel Silva Vieira, Pablo Lamata, Giovanni Montana
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: The clinical management of several cardiovascular conditions, such as pulmonary hypertension, require the assessment of the right ventricular (RV) function. This work addresses the fully automatic and robust access to one of the key RV biomarkers, its ejection fraction, from the gold standard imaging modality, MRI. The problem becomes the accurate segmentation of the RV blood pool from cine MRI sequences. This work proposes a solution based on Fully Convolutional Neural Networks (FCNN), where our first contribution is the optimal combination of three concepts (the convolution Gated Recurrent Units (GRU), the Generative Adversarial Networks (GAN), and the L1 loss function) that achieves an improvement of 0.05 and 3.49 mm in Dice Index and Hausdorff Distance respectively with respect to the baseline FCNN. This improvement is then doubled by our second contribution, the ROI-GAN, that sets two GANs to cooperate working at two fields of view of the image, its full resolution and the region of interest (ROI). Our rationale here is to better guide the FCNN learning by combining global (full resolution) and local Region Of Interest (ROI) features. The study is conducted in a large in-house dataset of $\sim$ 23.000 segmented MRI slices, and its generality is verified in a publicly available dataset.



### CNN Based Posture-Free Hand Detection
- **Arxiv ID**: http://arxiv.org/abs/1809.10432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10432v1)
- **Published**: 2018-09-27 10:00:31+00:00
- **Updated**: 2018-09-27 10:00:31+00:00
- **Authors**: Richard Adiguna, Yustinus Eko Soelistio
- **Comment**: 4 pages, 5 figures, in The 10th International Conference on
  Information Technology and Electrical Engineering 2018, ISBN:
  978-1-5386-4739-4
- **Journal**: None
- **Summary**: Although many studies suggest high performance hand detection methods, those methods are likely to be overfitting. Fortunately, the Convolution Neural Network (CNN) based approach provides a better way that is less sensitive to translation and hand poses. However the CNN approach is complex and can increase computational time, which at the end reduce its effectiveness on a system where the speed is essential.In this study we propose a shallow CNN network which is fast, and insensitive to translation and hand poses. It is tested on two different domains of hand datasets, and performs in relatively comparable performance and faster than the other state-of-the-art hand CNN-based hand detection method. Our evaluation shows that the proposed shallow CNN network performs at 93.9% accuracy and reaches much faster speed than its competitors.



### Compressing the Input for CNNs with the First-Order Scattering Transform
- **Arxiv ID**: http://arxiv.org/abs/1809.10200v1
- **DOI**: 10.1007/978-3-030-01240-3_19
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.10200v1)
- **Published**: 2018-09-27 10:14:46+00:00
- **Updated**: 2018-09-27 10:14:46+00:00
- **Authors**: Edouard Oyallon, Eugene Belilovsky, Sergey Zagoruyko, Michal Valko
- **Comment**: None
- **Journal**: ECCV 2018
- **Summary**: We study the first-order scattering transform as a candidate for reducing the signal processed by a convolutional neural network (CNN). We show theoretical and empirical evidence that in the case of natural images and sufficiently small translation invariance, this transform preserves most of the signal information needed for classification while substantially reducing the spatial resolution and total signal size. We demonstrate that cascading a CNN with this representation performs on par with ImageNet classification models, commonly used in downstream tasks, such as the ResNet-50. We subsequently apply our trained hybrid ImageNet model as a base model on a detection system, which has typically larger image inputs. On Pascal VOC and COCO detection tasks we demonstrate improvements in the inference speed and training memory consumption compared to models trained directly on the input image.



### A Simple Framework to Leverage State-Of-The-Art Single-Image Super-Resolution Methods to Restore Light Fields
- **Arxiv ID**: http://arxiv.org/abs/1809.10449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10449v1)
- **Published**: 2018-09-27 10:54:07+00:00
- **Updated**: 2018-09-27 10:54:07+00:00
- **Authors**: Reuben A. Farrugia, C. Guillemot
- **Comment**: None
- **Journal**: None
- **Summary**: Plenoptic cameras offer a cost effective solution to capture light fields by multiplexing multiple views on a single image sensor. However, the high angular resolution is achieved at the expense of reducing the spatial resolution of each view by orders of magnitude compared to the raw sensor image. While light field super-resolution is still at an early stage, the field of single image super-resolution (SISR) has recently known significant advances with the use of deep learning techniques. This paper describes a simple framework allowing us to leverage state-of-the-art SISR techniques into light fields, while taking into account specific light field geometrical constraints. The idea is to first compute a representation compacting most of the light field energy into as few components as possible. This is achieved by aligning the light field using optical flows and then by decomposing the aligned light field using singular value decomposition (SVD). The principal basis captures the information that is coherent across all the views, while the other basis contain the high angular frequencies. Super-resolving this principal basis using an SISR method allows us to super-resolve all the information that is coherent across the entire light field. This framework allows the proposed light field super-resolution method to inherit the benefits of the SISR method used. Experimental results show that the proposed method is competitive, and most of the time superior, to recent light field super-resolution methods in terms of both PSNR and SSIM quality metrics, with a lower complexity.



### Learning to Train a Binary Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1809.10463v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.10463v1)
- **Published**: 2018-09-27 11:40:03+00:00
- **Updated**: 2018-09-27 11:40:03+00:00
- **Authors**: Joseph Bethge, Haojin Yang, Christian Bartz, Christoph Meinel
- **Comment**: Code: https://github.com/Jopyth/BMXNet
- **Journal**: None
- **Summary**: Convolutional neural networks have achieved astonishing results in different application areas. Various methods which allow us to use these models on mobile and embedded devices have been proposed. Especially binary neural networks seem to be a promising approach for these devices with low computational power. However, understanding binary neural networks and training accurate models for practical applications remains a challenge. In our work, we focus on increasing our understanding of the training process and making it accessible to everyone. We publish our code and models based on BMXNet for everyone to use. Within this framework, we systematically evaluated different network architectures and hyperparameters to provide useful insights on how to train a binary neural network. Further, we present how we improved accuracy by increasing the number of connections in the network.



### Edge and Corner Detection for Unorganized 3D Point Clouds with Application to Robotic Welding
- **Arxiv ID**: http://arxiv.org/abs/1809.10468v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10468v1)
- **Published**: 2018-09-27 11:50:58+00:00
- **Updated**: 2018-09-27 11:50:58+00:00
- **Authors**: Syeda Mariam Ahmed, Yan Zhi Tan, Chee Meng Chew, Abdullah Al Mamun, Fook Seng Wong
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose novel edge and corner detection algorithms for unorganized point clouds. Our edge detection method evaluates symmetry in a local neighborhood and uses an adaptive density based threshold to differentiate 3D edge points. We extend this algorithm to propose a novel corner detector that clusters curvature vectors and uses their geometrical statistics to classify a point as corner. We perform rigorous evaluation of the algorithms on RGB-D semantic segmentation and 3D washer models from the ShapeNet dataset and report higher precision and recall scores. Finally, we also demonstrate how our edge and corner detectors can be used as a novel approach towards automatic weld seam detection for robotic welding. We propose to generate weld seams directly from a point cloud as opposed to using 3D models for offline planning of welding paths. For this application, we show a comparison between Harris 3D and our proposed approach on a panel workpiece.



### No New-Net
- **Arxiv ID**: http://arxiv.org/abs/1809.10483v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10483v2)
- **Published**: 2018-09-27 12:24:27+00:00
- **Updated**: 2019-01-31 07:54:38+00:00
- **Authors**: Fabian Isensee, Philipp Kickingereder, Wolfgang Wick, Martin Bendszus, Klaus H. Maier-Hein
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we demonstrate the effectiveness of a well trained U-Net in the context of the BraTS 2018 challenge. This endeavour is particularly interesting given that researchers are currently besting each other with architectural modifications that are intended to improve the segmentation performance. We instead focus on the training process arguing that a well trained U-Net is hard to beat. Our baseline U-Net, which has only minor modifications and is trained with a large patch size and a Dice loss function indeed achieved competitive Dice scores on the BraTS2018 validation data. By incorporating additional measures such as region based training, additional training data, a simple postprocessing technique and a combination of loss functions, we obtain Dice scores of 77.88, 87.81 and 80.62, and Hausdorff Distances (95th percentile) of 2.90, 6.03 and 5.08 for the enhancing tumor, whole tumor and tumor core, respectively on the test data. This setup achieved rank two in BraTS2018, with more than 60 teams participating in the challenge.



### nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1809.10486v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10486v1)
- **Published**: 2018-09-27 12:25:52+00:00
- **Updated**: 2018-09-27 12:25:52+00:00
- **Authors**: Fabian Isensee, Jens Petersen, Andre Klein, David Zimmerer, Paul F. Jaeger, Simon Kohl, Jakob Wasserthal, Gregor Koehler, Tobias Norajitra, Sebastian Wirkert, Klaus H. Maier-Hein
- **Comment**: None
- **Journal**: None
- **Summary**: The U-Net was presented in 2015. With its straight-forward and successful architecture it quickly evolved to a commonly used benchmark in medical image segmentation. The adaptation of the U-Net to novel problems, however, comprises several degrees of freedom regarding the exact architecture, preprocessing, training and inference. These choices are not independent of each other and substantially impact the overall performance. The present paper introduces the nnU-Net ('no-new-Net'), which refers to a robust and self-adapting framework on the basis of 2D and 3D vanilla U-Nets. We argue the strong case for taking away superfluous bells and whistles of many proposed network designs and instead focus on the remaining aspects that make out the performance and generalizability of a method. We evaluate the nnU-Net in the context of the Medical Segmentation Decathlon challenge, which measures segmentation performance in ten disciplines comprising distinct entities, image modalities, image geometries and dataset sizes, with no manual adjustments between datasets allowed. At the time of manuscript submission, nnU-Net achieves the highest mean dice scores across all classes and seven phase 1 tasks (except class 1 in BrainTumour) in the online leaderboard of the challenge.



### Improving Myocardium Segmentation in Cardiac CT Angiography using Spectral Information
- **Arxiv ID**: http://arxiv.org/abs/1810.03968v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03968v2)
- **Published**: 2018-09-27 12:34:42+00:00
- **Updated**: 2019-01-28 08:46:09+00:00
- **Authors**: Steffen Bruns, Jelmer M. Wolterink, Robbert W. van Hamersvelt, Majd Zreik, Tim Leiner, Ivana Išgum
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate segmentation of the left ventricle myocardium in cardiac CT angiography (CCTA) is essential for e.g. the assessment of myocardial perfusion. Automatic deep learning methods for segmentation in CCTA might suffer from differences in contrast-agent attenuation between training and test data due to non-standardized contrast administration protocols and varying cardiac output. We propose augmentation of the training data with virtual mono-energetic reconstructions from a spectral CT scanner which show different attenuation levels of the contrast agent. We compare this to an augmentation by linear scaling of all intensity values, and combine both types of augmentation. We train a 3D fully convolutional network (FCN) with 10 conventional CCTA images and corresponding virtual mono-energetic reconstructions acquired on a spectral CT scanner, and evaluate on 40 CCTA scans acquired on a conventional CT scanner. We show that training with data augmentation using virtual mono-energetic images improves upon training with only conventional images (Dice similarity coefficient (DSC) 0.895 $\pm$ 0.039 vs. 0.846 $\pm$ 0.125). In comparison, training with data augmentation using linear scaling improves the DSC to 0.890 $\pm$ 0.039. Moreover, combining the results of both augmentation methods leads to a DSC of 0.901 $\pm$ 0.036, showing that both augmentations lead to different local improvements of the segmentations. Our results indicate that virtual mono-energetic images improve the generalization of an FCN used for myocardium segmentation in CCTA images.



### Collective behavior recognition using compact descriptors
- **Arxiv ID**: http://arxiv.org/abs/1809.10499v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10499v1)
- **Published**: 2018-09-27 13:01:48+00:00
- **Updated**: 2018-09-27 13:01:48+00:00
- **Authors**: Gustavo Fuhr, Claudio Rosito Jung
- **Comment**: 8 pages, 6 figures. Paper submitted do Pattern Recognition Letters
- **Journal**: None
- **Summary**: This paper presents a novel hierarchical approach for collective behavior recognition based solely on ground-plane trajectories. In the first layer of our classifier, we introduce a novel feature called Personal Interaction Descriptor (PID), which combines the spatial distribution of a pair of pedestrians within a temporal window with a pyramidal representation of the relative speed to detect pairwise interactions. These interactions are then combined with higher level features related to the mean speed and shape formed by the pedestrians in the scene, generating a Collective Behavior Descriptor (CBD) that is used to identify collective behaviors in a second stage. In both layers, Random Forests were used as classifiers, since they allow features of different natures to be combined seamlessly. Our experimental results indicate that the proposed method achieves results on par with state of the art techniques with a better balance of class errors. Moreover, we show that our method can generalize well across different camera setups through cross-dataset experiments.



### Rate-Accuracy Trade-Off In Video Classification With Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.03964v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.03964v2)
- **Published**: 2018-09-27 14:33:43+00:00
- **Updated**: 2019-01-02 13:08:19+00:00
- **Authors**: Mohammad Jubran, Alhabib Abbas, Aaron Chadha, Yiannis Andreopoulos
- **Comment**: None
- **Journal**: None
- **Summary**: Advanced video classification systems decode video frames to derive the necessary texture and motion representations for ingestion and analysis by spatio-temporal deep convolutional neural networks (CNNs). However, when considering visual Internet-of-Things applications, surveillance systems and semantic crawlers of large video repositories, the video capture and the CNN-based semantic analysis parts do not tend to be co-located. This necessitates the transport of compressed video over networks and incurs significant overhead in bandwidth and energy consumption, thereby significantly undermining the deployment potential of such systems. In this paper, we investigate the trade-off between the encoding bitrate and the achievable accuracy of CNN-based video classification models that directly ingest AVC/H.264 and HEVC encoded videos. Instead of retaining entire compressed video bitstreams and applying complex optical flow calculations prior to CNN processing, we only retain motion vector and select texture information at significantly-reduced bitrates and apply no additional processing prior to CNN ingestion. Based on three CNN architectures and two action recognition datasets, we achieve 11%-94% saving in bitrate with marginal effect on classification accuracy. A model-based selection between multiple CNNs increases these savings further, to the point where, if up to 7% loss of accuracy can be tolerated, video classification can take place with as little as 3 kbps for the transport of the required compressed video information to the system implementing the CNN models.



### Novel Sparse Recovery Algorithms for 3D Debris Localization using Rotating Point Spread Function Imagery
- **Arxiv ID**: http://arxiv.org/abs/1809.10541v1
- **DOI**: None
- **Categories**: **astro-ph.IM**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1809.10541v1)
- **Published**: 2018-09-27 14:35:09+00:00
- **Updated**: 2018-09-27 14:35:09+00:00
- **Authors**: Chao Wang, Robert Plemmons, Sudhakar Prasad, Raymond Chan, Mila Nikolova
- **Comment**: 16 pages. arXiv admin note: substantial text overlap with
  arXiv:1804.04000
- **Journal**: Proc. 2018 AMOS Technical Conference
- **Summary**: An optical imager that exploits off-center image rotation to encode both the lateral and depth coordinates of point sources in a single snapshot can perform 3D localization and tracking of space debris. When actively illuminated, unresolved space debris, which can be regarded as a swarm of point sources, can scatter a fraction of laser irradiance back into the imaging sensor. Determining the source locations and fluxes is a large-scale sparse 3D inverse problem, for which we have developed efficient and effective algorithms based on sparse recovery using non-convex optimization. Numerical simulations illustrate the efficiency and stability of the algorithms.



### Real-time 3D Pose Estimation with a Monocular Camera Using Deep Learning and Object Priors On an Autonomous Racecar
- **Arxiv ID**: http://arxiv.org/abs/1809.10548v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1809.10548v1)
- **Published**: 2018-09-27 14:45:19+00:00
- **Updated**: 2018-09-27 14:45:19+00:00
- **Authors**: Ankit Dhall
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a complete pipeline that allows object detection and simultaneously estimate the pose of these multiple object instances using just a single image. A novel "keypoint regression" scheme with a cross-ratio term is introduced that exploits prior information about the object's shape and size to regress and find specific feature points. Further, a priori 3D information about the object is used to match 2D-3D correspondences and accurately estimate object positions up to a distance of 15m. A detailed discussion of the results and an in-depth analysis of the pipeline is presented. The pipeline runs efficiently on a low-powered Jetson TX2 and is deployed as part of the perception pipeline on a real-time autonomous vehicle cruising at a top speed of 54 km/hr.



### Dropout Distillation for Efficiently Estimating Model Confidence
- **Arxiv ID**: http://arxiv.org/abs/1809.10562v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10562v1)
- **Published**: 2018-09-27 15:02:56+00:00
- **Updated**: 2018-09-27 15:02:56+00:00
- **Authors**: Corina Gurau, Alex Bewley, Ingmar Posner
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an efficient way to output better calibrated uncertainty scores from neural networks. The Distilled Dropout Network (DDN) makes standard (non-Bayesian) neural networks more introspective by adding a new training loss which prevents them from being overconfident. Our method is more efficient than Bayesian neural networks or model ensembles which, despite providing more reliable uncertainty scores, are more cumbersome to train and slower to test. We evaluate DDN on the the task of image classification on the CIFAR-10 dataset and show that our calibration results are competitive even when compared to 100 Monte Carlo samples from a dropout network while they also increase the classification accuracy. We also propose better calibration within the state of the art Faster R-CNN object detection framework and show, using the COCO dataset, that DDN helps train better calibrated object detectors.



### Scalar Arithmetic Multiple Data: Customizable Precision for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.10572v2
- **DOI**: 10.1109/ARITH.2019.00018
- **Categories**: **cs.PF**, cs.CV, cs.MS
- **Links**: [PDF](http://arxiv.org/pdf/1809.10572v2)
- **Published**: 2018-09-27 15:25:02+00:00
- **Updated**: 2019-12-12 15:39:59+00:00
- **Authors**: Andrew Anderson, David Gregg
- **Comment**: None
- **Journal**: None
- **Summary**: Quantization of weights and activations in Deep Neural Networks (DNNs) is a powerful technique for network compression, and has enjoyed significant attention and success. However, much of the inference-time benefit of quantization is accessible only through the use of customized hardware accelerators or by providing an FPGA implementation of quantized arithmetic.   Building on prior work, we show how to construct arbitrary bit-precise signed and unsigned integer operations using a software technique which logically \emph{embeds} a vector architecture with custom bit-width lanes in universally available fixed-width scalar arithmetic.   We evaluate our approach on a high-end Intel Haswell processor, and an embedded ARM processor. Our approach yields very fast implementations of bit-precise custom DNN operations, which often match or exceed the performance of operations quantized to the sizes supported in native arithmetic. At the strongest level of quantization, our approach yields a maximum speedup of $\thicksim6\times$ on the Intel platform, and $\thicksim10\times$ on the ARM platform versus quantization to native 8-bit integers.



### Adaptive Image Stream Classification via Convolutional Neural Network with Intrinsic Similarity Metrics
- **Arxiv ID**: http://arxiv.org/abs/1810.03966v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.03966v1)
- **Published**: 2018-09-27 15:27:26+00:00
- **Updated**: 2018-09-27 15:27:26+00:00
- **Authors**: Yang Gao, Swarup Chandra, Zhuoyi Wang, Latifur Khan
- **Comment**: 10 pages; KDD'18 Deep Learning Day, August 2018, London, UK
- **Journal**: None
- **Summary**: When performing data classification over a stream of continuously occurring instances, a key challenge is to develop an open-world classifier that anticipates instances from an unknown class. Studies addressing this problem, typically called novel class detection, have considered classification methods that reactively adapt to such changes along the stream. Importantly, they rely on the property of cohesion and separation among instances in feature space. Instances belonging to the same class are assumed to be closer to each other (cohesion) than those belonging to different classes (separation). Unfortunately, this assumption may not have large support when dealing with high dimensional data such as images. In this paper, we address this key challenge by proposing a semisupervised multi-task learning framework called CSIM which aims to intrinsically search for a latent space suitable for detecting labels of instances from both known and unknown classes. Particularly, we utilize a convolution neural network layer that aids in the learning of a latent feature space suitable for novel class detection. We empirically measure the performance of CSIM over multiple realworld image datasets and demonstrate its superiority by comparing its performance with existing semi-supervised methods.



### Kernel based low-rank sparse model for single image super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1809.10582v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10582v1)
- **Published**: 2018-09-27 15:41:41+00:00
- **Updated**: 2018-09-27 15:41:41+00:00
- **Authors**: Jiahe Shi, Chun Qi
- **Comment**: 27 pages, Keywords: low-rank, sparse representation, kernel method,
  self-similarity learning, super-resolution
- **Journal**: None
- **Summary**: Self-similarity learning has been recognized as a promising method for single image super-resolution (SR) to produce high-resolution (HR) image in recent years. The performance of learning based SR reconstruction, however, highly depends on learned representation coeffcients. Due to the degradation of input image, conventional sparse coding is prone to produce unfaithful representation coeffcients. To this end, we propose a novel kernel based low-rank sparse model with self-similarity learning for single image SR which incorporates nonlocalsimilarity prior to enforce similar patches having similar representation weights. We perform a gradual magnification scheme, using self-examples extracted from the degraded input image and up-scaled versions. To exploit nonlocal-similarity, we concatenate the vectorized input patch and its nonlocal neighbors at different locations into a data matrix which consists of similar components. Then we map the nonlocal data matrix into a high-dimensional feature space by kernel method to capture their nonlinear structures. Under the assumption that the sparse coeffcients for the nonlocal data in the kernel space should be low-rank, we impose low-rank constraint on sparse coding to share similarities among representation coeffcients and remove outliers in order that stable weights for SR reconstruction can be obtained. Experimental results demonstrate the advantage of our proposed method in both visual quality and reconstruction error.



### A Deep Learning Approach to Denoise Optical Coherence Tomography Images of the Optic Nerve Head
- **Arxiv ID**: http://arxiv.org/abs/1809.10589v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10589v1)
- **Published**: 2018-09-27 15:53:36+00:00
- **Updated**: 2018-09-27 15:53:36+00:00
- **Authors**: Sripad Krishna Devalla, Giridhar Subramanian, Tan Hung Pham, Xiaofei Wang, Shamira Perera, Tin A. Tun, Tin Aung, Leopold Schmetterer, Alexandre H. Thiery, Michael J. A. Girard
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: To develop a deep learning approach to de-noise optical coherence tomography (OCT) B-scans of the optic nerve head (ONH).   Methods: Volume scans consisting of 97 horizontal B-scans were acquired through the center of the ONH using a commercial OCT device (Spectralis) for both eyes of 20 subjects. For each eye, single-frame (without signal averaging), and multi-frame (75x signal averaging) volume scans were obtained. A custom deep learning network was then designed and trained with 2,328 "clean B-scans" (multi-frame B-scans), and their corresponding "noisy B-scans" (clean B-scans + gaussian noise) to de-noise the single-frame B-scans. The performance of the de-noising algorithm was assessed qualitatively, and quantitatively on 1,552 B-scans using the signal to noise ratio (SNR), contrast to noise ratio (CNR), and mean structural similarity index metrics (MSSIM).   Results: The proposed algorithm successfully denoised unseen single-frame OCT B-scans. The denoised B-scans were qualitatively similar to their corresponding multi-frame B-scans, with enhanced visibility of the ONH tissues. The mean SNR increased from $4.02 \pm 0.68$ dB (single-frame) to $8.14 \pm 1.03$ dB (denoised). For all the ONH tissues, the mean CNR increased from $3.50 \pm 0.56$ (single-frame) to $7.63 \pm 1.81$ (denoised). The MSSIM increased from $0.13 \pm 0.02$ (single frame) to $0.65 \pm 0.03$ (denoised) when compared with the corresponding multi-frame B-scans.   Conclusions: Our deep learning algorithm can denoise a single-frame OCT B-scan of the ONH in under 20 ms, thus offering a framework to obtain superior quality OCT B-scans with reduced scanning times and minimal patient discomfort.



### Generative replay with feedback connections as a general strategy for continual learning
- **Arxiv ID**: http://arxiv.org/abs/1809.10635v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.10635v2)
- **Published**: 2018-09-27 16:55:58+00:00
- **Updated**: 2019-04-17 09:20:24+00:00
- **Authors**: Gido M. van de Ven, Andreas S. Tolias
- **Comment**: 17 pages, 8 figures, 4 tables
- **Journal**: None
- **Summary**: A major obstacle to developing artificial intelligence applications capable of true lifelong learning is that artificial neural networks quickly or catastrophically forget previously learned tasks when trained on a new one. Numerous methods for alleviating catastrophic forgetting are currently being proposed, but differences in evaluation protocols make it difficult to directly compare their performance. To enable more meaningful comparisons, here we identified three distinct scenarios for continual learning based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as "soft targets") achieved superior performance in all three scenarios. Addressing the issue of efficiency, we reduced the computational cost of generative replay by integrating the generative model into the main model by equipping it with generative feedback or backward connections. This Replay-through-Feedback approach substantially shortened training time with no or negligible loss in performance. We believe this to be an important first step towards making the powerful technique of generative replay scalable to real-world continual learning applications.



### Conditional WaveGAN
- **Arxiv ID**: http://arxiv.org/abs/1809.10636v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.10636v1)
- **Published**: 2018-09-27 16:56:23+00:00
- **Updated**: 2018-09-27 16:56:23+00:00
- **Authors**: Chae Young Lee, Anoop Toffy, Gue Jun Jung, Woo-Jin Han
- **Comment**: Preprint
- **Journal**: None
- **Summary**: Generative models are successfully used for image synthesis in the recent years. But when it comes to other modalities like audio, text etc little progress has been made. Recent works focus on generating audio from a generative model in an unsupervised setting. We explore the possibility of using generative models conditioned on class labels. Concatenation based conditioning and conditional scaling were explored in this work with various hyper-parameter tuning methods. In this paper we introduce Conditional WaveGANs (cWaveGAN). Find our implementation at https://github.com/acheketa/cwavegan



### Weakly-Supervised Localization and Classification of Proximal Femur Fractures
- **Arxiv ID**: http://arxiv.org/abs/1809.10692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10692v1)
- **Published**: 2018-09-27 18:00:11+00:00
- **Updated**: 2018-09-27 18:00:11+00:00
- **Authors**: Amelia Jiménez-Sánchez, Anees Kazi, Shadi Albarqouni, Sonja Kirchhoff, Alexandra Sträter, Peter Biberthaler, Diana Mateus, Nassir Navab
- **Comment**: 7 pages, 4 figures
- **Journal**: None
- **Summary**: In this paper, we target the problem of fracture classification from clinical X-Ray images towards an automated Computer Aided Diagnosis (CAD) system. Although primarily dealing with an image classification problem, we argue that localizing the fracture in the image is crucial to make good class predictions. Therefore, we propose and thoroughly analyze several schemes for simultaneous fracture localization and classification. We show that using an auxiliary localization task, in general, improves the classification performance. Moreover, it is possible to avoid the need for additional localization annotations thanks to recent advancements in weakly-supervised deep learning approaches. Among such approaches, we investigate and adapt Spatial Transformers (ST), Self-Transfer Learning (STL), and localization from global pooling layers. We provide a detailed quantitative and qualitative validation on a dataset of 1347 femur fractures images and report high accuracy with regard to inter-expert correlation values reported in the literature. Our investigations show that i) lesion localization improves the classification outcome, ii) weakly-supervised methods improve baseline classification without any additional cost, iii) STL guides feature activations and boost performance. We plan to make both the dataset and code available.



### Learning Pose Estimation for High-Precision Robotic Assembly Using Simulated Depth Images
- **Arxiv ID**: http://arxiv.org/abs/1809.10699v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.10699v2)
- **Published**: 2018-09-27 18:02:24+00:00
- **Updated**: 2019-03-23 12:57:02+00:00
- **Authors**: Yuval Litvak, Armin Biess, Aharon Bar-Hillel
- **Comment**: 8 pages, 5 figures. This work has been accepted to the International
  Conference on Robotics and Automation (ICRA 2019). For associated video, see
  https://youtu.be/uMvq2-Tg-9g
- **Journal**: None
- **Summary**: Most of industrial robotic assembly tasks today require fixed initial conditions for successful assembly. These constraints induce high production costs and low adaptability to new tasks. In this work we aim towards flexible and adaptable robotic assembly by using 3D CAD models for all parts to be assembled. We focus on a generic assembly task - the Siemens Innovation Challenge - in which a robot needs to assemble a gear-like mechanism with high precision into an operating system. To obtain the millimeter-accuracy required for this task and industrial settings alike, we use a depth camera mounted near the robot end-effector. We present a high-accuracy two-stage pose estimation procedure based on deep convolutional neural networks, which includes detection, pose estimation, refinement, and handling of near- and full symmetries of parts. The networks are trained on simulated depth images with means to ensure successful transfer to the real robot. We obtain an average pose estimation error of 2.16 millimeters and 0.64 degree leading to 91% success rate for robotic assembly of randomly distributed parts. To the best of our knowledge, this is the first time that the Siemens Innovation Challenge is fully addressed, with all the parts assembled with high success rates.



### Semantic Topic Analysis of Traffic Camera Images
- **Arxiv ID**: http://arxiv.org/abs/1809.10707v1
- **DOI**: 10.1109/ITSC.2018.8569449
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10707v1)
- **Published**: 2018-09-27 18:13:04+00:00
- **Updated**: 2018-09-27 18:13:04+00:00
- **Authors**: Jeffrey Liu, Andrew Weinert, Saurabh Amin
- **Comment**: To be presented at IEEE-ITSC 2018, Nov 3-7 2018
- **Journal**: None
- **Summary**: Traffic cameras are commonly deployed monitoring components in road infrastructure networks, providing operators visual information about conditions at critical points in the network. However, human observers are often limited in their ability to process simultaneous information sources. Recent advancements in computer vision, driven by deep learning methods, have enabled general object recognition, unlocking opportunities for camera-based sensing beyond the existing human observer paradigm. In this paper, we present a Natural Language Processing (NLP)-inspired approach, entitled Bag-of-Label-Words (BoLW), for analyzing image data sets using exclusively textual labels. The BoLW model represents the data in a conventional matrix form, enabling data compression and decomposition techniques, while preserving semantic interpretability. We apply the Latent Dirichlet Allocation (LDA) topic model to decompose the label data into a small number of semantic topics. To illustrate our approach, we use freeway camera images collected from the Boston area between December 2017-January 2018. We analyze the cameras' sensitivity to weather events; identify temporal traffic patterns; and analyze the impact of infrequent events, such as the winter holidays and the "bomb cyclone" winter storm. This study demonstrates the flexibility of our approach, which allows us to analyze weather events and freeway traffic using only traffic camera image labels.



### Multi-Scale Recursive and Perception-Distortion Controllable Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1809.10711v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1809.10711v2)
- **Published**: 2018-09-27 18:26:09+00:00
- **Updated**: 2019-01-29 03:07:47+00:00
- **Authors**: Pablo Navarrete Michelini, Dan Zhu, Hanwen Liu
- **Comment**: In ECCV 2018 Workshops. Won 2nd place in Region 3 of PIRM-SR
  Challenge 2018. Code and models are available at
  https://github.com/pnavarre/pirm-sr-2018
- **Journal**: None
- **Summary**: We describe our solution for the PIRM Super-Resolution Challenge 2018 where we achieved the 2nd best perceptual quality for average RMSE<=16, 5th best for RMSE<=12.5, and 7th best for RMSE<=11.5. We modify a recently proposed Multi-Grid Back-Projection (MGBP) architecture to work as a generative system with an input parameter that can control the amount of artificial details in the output. We propose a discriminator for adversarial training with the following novel properties: it is multi-scale that resembles a progressive-GAN; it is recursive that balances the architecture of the generator; and it includes a new layer to capture significant statistics of natural images. Finally, we propose a training strategy that avoids conflicts between reconstruction and perceptual losses. Our configuration uses only 281k parameters and upscales each image of the competition in 0.2s in average.



### On the loss landscape of a class of deep neural networks with no bad local valleys
- **Arxiv ID**: http://arxiv.org/abs/1809.10749v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.10749v2)
- **Published**: 2018-09-27 20:09:59+00:00
- **Updated**: 2018-12-24 00:58:29+00:00
- **Authors**: Quynh Nguyen, Mahesh Chandra Mukkamala, Matthias Hein
- **Comment**: Accepted at ICLR 2019
- **Journal**: None
- **Summary**: We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley, in the sense that from any point in parameter space there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. This implies that these networks have no sub-optimal strict local minima.



### Interactive Surveillance Technologies for Dense Crowds
- **Arxiv ID**: http://arxiv.org/abs/1810.03965v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1810.03965v1)
- **Published**: 2018-09-27 20:18:25+00:00
- **Updated**: 2018-09-27 20:18:25+00:00
- **Authors**: Aniket Bera, Dinesh Manocha
- **Comment**: Presented at AAAI FSS-18: Artificial Intelligence in Government and
  Public Sector, Arlington, Virginia, USA
- **Journal**: None
- **Summary**: We present an algorithm for realtime anomaly detection in low to medium density crowd videos using trajectory-level behavior learning. Our formulation combines online tracking algorithms from computer vision, non-linear pedestrian motion models from crowd simulation, and Bayesian learning techniques to automatically compute the trajectory-level pedestrian behaviors for each agent in the video. These learned behaviors are used to segment the trajectories and motions of different pedestrians or agents and detect anomalies. We demonstrate the interactive performance on the PETS ARENA dataset as well as indoor and outdoor crowd video benchmarks consisting of tens of human agents. We also discuss the implications of recent public policy and law enforcement issues relating to surveillance and our research.



### Cursive Scene Text Analysis by Deep Convolutional Linear Pyramids
- **Arxiv ID**: http://arxiv.org/abs/1809.10792v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10792v1)
- **Published**: 2018-09-27 22:51:56+00:00
- **Updated**: 2018-09-27 22:51:56+00:00
- **Authors**: Saad Bin Ahmed, Saeeda Naz, Muhammad Imran Razzak, Rubiyah Yusof
- **Comment**: 09 pages,6 figures, 25th International Conference on Neural
  Information Processing (ICONIP 2018)
- **Journal**: None
- **Summary**: The camera captured images have various aspects to investigate. Generally, the emphasis of research depends on the interesting regions. Sometimes the focus could be on color segmentation, object detection or scene text analysis. The image analysis, visibility and layout analysis are the tasks easier for humans as suggested by behavioral trait of humans, but in contrast when these same tasks are supposed to perform by machines then it seems to be challenging. The learning machines always learn from the properties associated to provided samples. The numerous approaches are designed in recent years for scene text extraction and recognition and the efforts are underway to improve the accuracy. The convolutional approach provided reasonable results on non-cursive text analysis appeared in natural images. The work presented in this manuscript exploited the strength of linear pyramids by considering each pyramid as a feature of the provided sample. Each pyramid image process through various empirically selected kernels. The performance was investigated by considering Arabic text on each image pyramid of EASTR-42k dataset. The error rate of 0.17% was reported on Arabic scene text recognition.



### Effective Cloud Detection and Segmentation using a Gradient-Based Algorithm for Satellite Imagery; Application to improve PERSIANN-CCS
- **Arxiv ID**: http://arxiv.org/abs/1809.10801v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10801v1)
- **Published**: 2018-09-27 23:59:24+00:00
- **Updated**: 2018-09-27 23:59:24+00:00
- **Authors**: Negin Hayatbini, Kuo-lin Hsu, Soroosh Sorooshian, Yunji Zhang, Fuqing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Being able to effectively identify clouds and monitor their evolution is one important step toward more accurate quantitative precipitation estimation and forecast. In this study, a new gradient-based cloud-image segmentation technique is developed using tools from image processing techniques. This method integrates morphological image gradient magnitudes to separable cloud systems and patches boundaries. A varying scale-kernel is implemented to reduce the sensitivity of image segmentation to noise and capture objects with various finenesses of the edges in remote-sensing images. The proposed method is flexible and extendable from single- to multi-spectral imagery. Case studies were carried out to validate the algorithm by applying the proposed segmentation algorithm to synthetic radiances for channels of the Geostationary Operational Environmental Satellites (GOES-R) simulated by a high-resolution weather prediction model. The proposed method compares favorably with the existing cloud-patch-based segmentation technique implemented in the PERSIANN-CCS (Precipitation Estimation from Remotely Sensed Information using Artificial Neural Network - Cloud Classification System) rainfall retrieval algorithm. Evaluation of event-based images indicates that the proposed algorithm has potential to improve rain detection and estimation skills with an average of more than 45% gain comparing to the segmentation technique used in PERSIANN-CCS and identifying cloud regions as objects with accuracy rates up to 98%.



