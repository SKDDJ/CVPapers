# Arxiv Papers in cs.CV on 2018-09-11
### ManifoldNet: A Deep Network Framework for Manifold-valued Data
- **Arxiv ID**: http://arxiv.org/abs/1809.06211v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06211v3)
- **Published**: 2018-09-11 00:27:48+00:00
- **Updated**: 2018-09-20 18:58:35+00:00
- **Authors**: Rudrasis Chakraborty, Jose Bouza, Jonathan Manton, Baba C. Vemuri
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have become the main work horse for many tasks involving learning from data in a variety of applications in Science and Engineering. Traditionally, the input to these networks lie in a vector space and the operations employed within the network are well defined on vector-spaces. In the recent past, due to technological advances in sensing, it has become possible to acquire manifold-valued data sets either directly or indirectly. Examples include but are not limited to data from omnidirectional cameras on automobiles, drones etc., synthetic aperture radar imaging, diffusion magnetic resonance imaging, elastography and conductance imaging in the Medical Imaging domain and others. Thus, there is need to generalize the deep neural networks to cope with input data that reside on curved manifolds where vector space operations are not naturally admissible. In this paper, we present a novel theoretical framework to generalize the widely popular convolutional neural networks (CNNs) to high dimensional manifold-valued data inputs. We call these networks, ManifoldNets.   In ManifoldNets, convolution operation on data residing on Riemannian manifolds is achieved via a provably convergent recursive computation of the weighted Fr\'{e}chet Mean (wFM) of the given data, where the weights makeup the convolution mask, to be learned. Further, we prove that the proposed wFM layer achieves a contraction mapping and hence ManifoldNet does not need the non-linear ReLU unit used in standard CNNs. We present experiments, using the ManifoldNet framework, to achieve dimensionality reduction by computing the principal linear subspaces that naturally reside on a Grassmannian. The experimental results demonstrate the efficacy of ManifoldNets in the context of classification and reconstruction accuracy.



### Context-Dependent Diffusion Network for Visual Relationship Detection
- **Arxiv ID**: http://arxiv.org/abs/1809.06213v1
- **DOI**: 10.1145/3240508.3240668
- **Categories**: **cs.CV**, cs.CL, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.06213v1)
- **Published**: 2018-09-11 02:13:45+00:00
- **Updated**: 2018-09-11 02:13:45+00:00
- **Authors**: Zhen Cui, Chunyan Xu, Wenming Zheng, Jian Yang
- **Comment**: 8 pages, 3 figures, 2018 ACM Multimedia Conference (MM'18)
- **Journal**: None
- **Summary**: Visual relationship detection can bridge the gap between computer vision and natural language for scene understanding of images. Different from pure object recognition tasks, the relation triplets of subject-predicate-object lie on an extreme diversity space, such as \textit{person-behind-person} and \textit{car-behind-building}, while suffering from the problem of combinatorial explosion. In this paper, we propose a context-dependent diffusion network (CDDN) framework to deal with visual relationship detection. To capture the interactions of different object instances, two types of graphs, word semantic graph and visual scene graph, are constructed to encode global context interdependency. The semantic graph is built through language priors to model semantic correlations across objects, whilst the visual scene graph defines the connections of scene objects so as to utilize the surrounding scene information. For the graph-structured data, we design a diffusion network to adaptively aggregate information from contexts, which can effectively learn latent representations of visual relationships and well cater to visual relationship detection in view of its isomorphic invariance to graphs. Experiments on two widely-used datasets demonstrate that our proposed method is more effective and achieves the state-of-the-art performance.



### Neural Rendering and Reenactment of Human Actor Videos
- **Arxiv ID**: http://arxiv.org/abs/1809.03658v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03658v3)
- **Published**: 2018-09-11 02:29:26+00:00
- **Updated**: 2019-05-09 19:11:51+00:00
- **Authors**: Lingjie Liu, Weipeng Xu, Michael Zollhoefer, Hyeongwoo Kim, Florian Bernard, Marc Habermann, Wenping Wang, Christian Theobalt
- **Comment**: ACM ToG paper. Project page:
  http://gvv.mpi-inf.mpg.de/projects/wxu/HumanReenactment/
- **Journal**: None
- **Summary**: We propose a method for generating video-realistic animations of real humans under user control. In contrast to conventional human character rendering, we do not require the availability of a production-quality photo-realistic 3D model of the human, but instead rely on a video sequence in conjunction with a (medium-quality) controllable 3D template model of the person. With that, our approach significantly reduces production cost compared to conventional rendering approaches based on production-quality 3D models, and can also be used to realistically edit existing videos. Technically, this is achieved by training a neural network that translates simple synthetic images of a human character into realistic imagery. For training our networks, we first track the 3D motion of the person in the video using the template model, and subsequently generate a synthetically rendered version of the video. These images are then used to train a conditional generative adversarial network that translates synthetic images of the 3D model into realistic imagery of the human. We evaluate our method for the reenactment of another person that is tracked in order to obtain the motion data, and show video results generated from artist-designed skeleton motion. Our results outperform the state-of-the-art in learning-based human image synthesis. Project page: http://gvv.mpi-inf.mpg.de/projects/wxu/HumanReenactment/



### Comparing Computing Platforms for Deep Learning on a Humanoid Robot
- **Arxiv ID**: http://arxiv.org/abs/1809.03668v2
- **DOI**: 10.1007/978-3-030-04239-4_11
- **Categories**: **cs.LG**, cs.CV, cs.RO, 68T40, 68T45, 68U10, I.2.10; I.2.9; C.3
- **Links**: [PDF](http://arxiv.org/pdf/1809.03668v2)
- **Published**: 2018-09-11 03:25:36+00:00
- **Updated**: 2019-01-20 23:38:47+00:00
- **Authors**: Alexander Biddulph, Trent Houlistion, Alexandre Mendes, Stephan K. Chalup
- **Comment**: 12 pages, 5 figures
- **Journal**: Neural Information Processing, 11307 (2018), 120-131
- **Summary**: The goal of this study is to test two different computing platforms with respect to their suitability for running deep networks as part of a humanoid robot software system. One of the platforms is the CPU-centered Intel NUC7i7BNH and the other is a NVIDIA Jetson TX2 system that puts more emphasis on GPU processing. The experiments addressed a number of benchmarking tasks including pedestrian detection using deep neural networks. Some of the results were unexpected but demonstrate that platforms exhibit both advantages and disadvantages when taking computational performance and electrical power requirements of such a system into account.



### Temporal-Spatial Mapping for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.03669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03669v1)
- **Published**: 2018-09-11 03:29:28+00:00
- **Updated**: 2018-09-11 03:29:28+00:00
- **Authors**: Xiaolin Song, Cuiling Lan, Wenjun Zeng, Junliang Xing, Jingyu Yang, Xiaoyan Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models have enjoyed great success for image related computer vision tasks like image classification and object detection. For video related tasks like human action recognition, however, the advancements are not as significant yet. The main challenge is the lack of effective and efficient models in modeling the rich temporal spatial information in a video. We introduce a simple yet effective operation, termed Temporal-Spatial Mapping (TSM), for capturing the temporal evolution of the frames by jointly analyzing all the frames of a video. We propose a video level 2D feature representation by transforming the convolutional features of all frames to a 2D feature map, referred to as VideoMap. With each row being the vectorized feature representation of a frame, the temporal-spatial features are compactly represented, while the temporal dynamic evolution is also well embedded. Based on the VideoMap representation, we further propose a temporal attention model within a shallow convolutional neural network to efficiently exploit the temporal-spatial dynamics. The experiment results show that the proposed scheme achieves the state-of-the-art performance, with 4.2% accuracy gain over Temporal Segment Network (TSN), a competing baseline method, on the challenging human action benchmark dataset HMDB51.



### Unbiasing Semantic Segmentation For Robot Perception using Synthetic Data Feature Transfer
- **Arxiv ID**: http://arxiv.org/abs/1809.03676v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1809.03676v1)
- **Published**: 2018-09-11 04:27:40+00:00
- **Updated**: 2018-09-11 04:27:40+00:00
- **Authors**: Jonathan C Balloch, Varun Agrawal, Irfan Essa, Sonia Chernova
- **Comment**: None
- **Journal**: None
- **Summary**: Robot perception systems need to perform reliable image segmentation in real-time on noisy, raw perception data. State-of-the-art segmentation approaches use large CNN models and carefully constructed datasets; however, these models focus on accuracy at the cost of real-time inference. Furthermore, the standard semantic segmentation datasets are not large enough for training CNNs without augmentation and are not representative of noisy, uncurated robot perception data. We propose improving the performance of real-time segmentation frameworks on robot perception data by transferring features learned from synthetic segmentation data. We show that pretraining real-time segmentation architectures with synthetic segmentation data instead of ImageNet improves fine-tuning performance by reducing the bias learned in pretraining and closing the \textit{transfer gap} as a result. Our experiments show that our real-time robot perception models pretrained on synthetic data outperform those pretrained on ImageNet for every scale of fine-tuning data examined. Moreover, the degree to which synthetic pretraining outperforms ImageNet pretraining increases as the availability of robot data decreases, making our approach attractive for robotics domains where dataset collection is hard and/or expensive.



### An Automatic Method for Complete Brain Matter Segmentation from Multislice CT scan
- **Arxiv ID**: http://arxiv.org/abs/1809.06215v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06215v2)
- **Published**: 2018-09-11 06:35:37+00:00
- **Updated**: 2018-10-22 07:11:06+00:00
- **Authors**: Soumi Ray, Vinod Kumar, Chirag Ahuja, Niranjan Khandelwal
- **Comment**: None
- **Journal**: None
- **Summary**: Computed tomography imaging is well accepted for its imaging speed, image contrast & resolution and cost. Thus it has wide use in detection and diagnosis of brain diseases. But unfortunately reported works on CT segmentation is not very significant. In this paper, a robust automatic segmentation system is presented which is capable of segment complete brain matter from CT slices, without any lose in information. The proposed method is simple, fast, accurate and completely automatic. It can handle multislice CT scan in single run. From a given multislice CT dataset, one slice is selected automatically to form masks for segmentation. Two types of masks are created to handle nasal slices in a better way. Masks are created from selected reference slice using automatic seed point selection and region growing technique. One mask is designed for brain matter and another includes the skull of the reference slice. This second mask is used as global reference mask for all slices whereas the brain matter mask is implemented on only adjacent slices and continuously modified for better segmentation. Slices in given dataset are divided into two batches, before reference slice and after reference slice. Each batch segmented separately. Successive propagation of brain matter mask has demonstrated very high potential in reported segmentation. Presented result shows highest sensitivity and more than 96% accuracy in all cases. Resulted segmented images can be used for any brain disease diagnosis or further image analysis.



### Bio-LSTM: A Biomechanically Inspired Recurrent Neural Network for 3D Pedestrian Pose and Gait Prediction
- **Arxiv ID**: http://arxiv.org/abs/1809.03705v3
- **DOI**: 10.1109/LRA.2019.2895266
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.03705v3)
- **Published**: 2018-09-11 07:11:32+00:00
- **Updated**: 2019-09-13 15:28:14+00:00
- **Authors**: Xiaoxiao Du, Ram Vasudevan, Matthew Johnson-Roberson
- **Comment**: Typo corrected after Eq.(2)
- **Journal**: IEEE Robotics and Automation Letters, vol. 4, no. 2, pp.
  1501-1508, April 2019
- **Summary**: In applications such as autonomous driving, it is important to understand, infer, and anticipate the intention and future behavior of pedestrians. This ability allows vehicles to avoid collisions and improve ride safety and quality. This paper proposes a biomechanically inspired recurrent neural network (Bio-LSTM) that can predict the location and 3D articulated body pose of pedestrians in a global coordinate frame, given 3D poses and locations estimated in prior frames with inaccuracy. The proposed network is able to predict poses and global locations for multiple pedestrians simultaneously, for pedestrians up to 45 meters from the cameras (urban intersection scale). The outputs of the proposed network are full-body 3D meshes represented in Skinned Multi-Person Linear (SMPL) model parameters. The proposed approach relies on a novel objective function that incorporates the periodicity of human walking (gait), the mirror symmetry of the human body, and the change of ground reaction forces in a human gait cycle. This paper presents prediction results on the PedX dataset, a large-scale, in-the-wild data set collected at real urban intersections with heavy pedestrian traffic. Results show that the proposed network can successfully learn the characteristics of pedestrian gait and produce accurate and consistent 3D pose predictions.



### The Visual QA Devil in the Details: The Impact of Early Fusion and Batch Norm on CLEVR
- **Arxiv ID**: http://arxiv.org/abs/1809.04482v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.04482v1)
- **Published**: 2018-09-11 07:14:30+00:00
- **Updated**: 2018-09-11 07:14:30+00:00
- **Authors**: Mateusz Malinowski, Carl Doersch
- **Comment**: Presented at ECCV'18 Workshop on Shortcomings in Vision and Language
- **Journal**: None
- **Summary**: Visual QA is a pivotal challenge for higher-level reasoning, requiring understanding language, vision, and relationships between many objects in a scene. Although datasets like CLEVR are designed to be unsolvable without such complex relational reasoning, some surprisingly simple feed-forward, "holistic" models have recently shown strong performance on this dataset. These models lack any kind of explicit iterative, symbolic reasoning procedure, which are hypothesized to be necessary for counting objects, narrowing down the set of relevant objects based on several attributes, etc. The reason for this strong performance is poorly understood. Hence, our work analyzes such models, and finds that minor architectural elements are crucial to performance. In particular, we find that \textit{early fusion} of language and vision provides large performance improvements. This contrasts with the late fusion approaches popular at the dawn of Visual QA. We propose a simple module we call Multimodal Core, which we hypothesize performs the fundamental operations for multimodal tasks. We believe that understanding why these elements are so important to complex question answering will aid the design of better-performing algorithms for Visual QA while minimizing hand-engineering effort.



### Answering Visual What-If Questions: From Actions to Predicted Scene Descriptions
- **Arxiv ID**: http://arxiv.org/abs/1809.03707v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, 68
- **Links**: [PDF](http://arxiv.org/pdf/1809.03707v2)
- **Published**: 2018-09-11 07:22:28+00:00
- **Updated**: 2018-11-21 16:39:39+00:00
- **Authors**: M. Wagner, H. Basevi, R. Shetty, W. Li, M. Malinowski, M. Fritz, A. Leonardis
- **Comment**: Paper: 18 pages, 5 figures, 5 tables. Supplementary material: 3
  pages, 1 figure, 1 table. To be published in VLEASE ECCV 2018 workshop
- **Journal**: None
- **Summary**: In-depth scene descriptions and question answering tasks have greatly increased the scope of today's definition of scene understanding. While such tasks are in principle open ended, current formulations primarily focus on describing only the current state of the scenes under consideration. In contrast, in this paper, we focus on the future states of the scenes which are also conditioned on actions. We posit this as a question answering task, where an answer has to be given about a future scene state, given observations of the current scene, and a question that includes a hypothetical action. Our solution is a hybrid model which integrates a physics engine into a question answering architecture in order to anticipate future scene states resulting from object-object interactions caused by an action. We demonstrate first results on this challenging new problem and compare to baselines, where we outperform fully data-driven end-to-end learning approaches.



### Deep Asymmetric Networks with a Set of Node-wise Variant Activation Functions
- **Arxiv ID**: http://arxiv.org/abs/1809.03721v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.03721v2)
- **Published**: 2018-09-11 08:09:25+00:00
- **Updated**: 2019-05-17 07:24:15+00:00
- **Authors**: Jinhyeok Jang, Hyunjoong Cho, Jaehong Kim, Jaeyeon Lee, Seungjoon Yang
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents deep asymmetric networks with a set of node-wise variant activation functions. The nodes' sensitivities are affected by activation function selections such that the nodes with smaller indices become increasingly more sensitive. As a result, features learned by the nodes are sorted by the node indices in the order of their importance. Asymmetric networks not only learn input features but also the importance of those features. Nodes of lesser importance in asymmetric networks can be pruned to reduce the complexity of the networks, and the pruned networks can be retrained without incurring performance losses. We validate the feature-sorting property using both shallow and deep asymmetric networks as well as deep asymmetric networks transferred from famous networks.



### Non-blind Image Restoration Based on Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1809.03757v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03757v1)
- **Published**: 2018-09-11 09:05:21+00:00
- **Updated**: 2018-09-11 09:05:21+00:00
- **Authors**: Kazutaka Uchida, Masayuki Tanaka, Masatoshi Okutomi
- **Comment**: Accepted by IEEE 7th Global Conference on Consumer Electronics, 2018
- **Journal**: None
- **Summary**: Blind image restoration processors based on convolutional neural network (CNN) are intensively researched because of their high performance. However, they are too sensitive to the perturbation of the degradation model. They easily fail to restore the image whose degradation model is slightly different from the trained degradation model. In this paper, we propose a non-blind CNN-based image restoration processor, aiming to be robust against a perturbation of the degradation model compared to the blind restoration processor. Experimental comparisons demonstrate that the proposed non-blind CNN-based image restoration processor can robustly restore images compared to existing blind CNN-based image restoration processors.



### 3D Human Body Reconstruction from a Single Image via Volumetric Regression
- **Arxiv ID**: http://arxiv.org/abs/1809.03770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03770v1)
- **Published**: 2018-09-11 09:53:07+00:00
- **Updated**: 2018-09-11 09:53:07+00:00
- **Authors**: Aaron S. Jackson, Chris Manafas, Georgios Tzimiropoulos
- **Comment**: Accepted to ECCV Workshops (PeopleCap) 2018
- **Journal**: None
- **Summary**: This paper proposes the use of an end-to-end Convolutional Neural Network for direct reconstruction of the 3D geometry of humans via volumetric regression. The proposed method does not require the fitting of a shape model and can be trained to work from a variety of input types, whether it be landmarks, images or segmentation masks. Additionally, non-visible parts, either self-occluded or otherwise, are still reconstructed, which is not the case with depth map regression. We present results that show that our method can handle both pose variation and detailed reconstruction given appropriate datasets for training.



### Probabilistic approach to limited-data computed tomography reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1809.03779v3
- **DOI**: 10.1088/1361-6420/ab2e2a
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.03779v3)
- **Published**: 2018-09-11 10:16:44+00:00
- **Updated**: 2019-07-03 08:30:06+00:00
- **Authors**: Zenith Purisha, Carl Jidling, Niklas Wahlström, Simo Särkkä, Thomas B. Schön
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we consider the inverse problem of reconstructing the internal structure of an object from limited x-ray projections. We use a Gaussian process prior to model the target function and estimate its (hyper)parameters from measured data. In contrast to other established methods, this comes with the advantage of not requiring any manual parameter tuning, which usually arises in classical regularization strategies. Our method uses a basis function expansion technique for the Gaussian process which significantly reduces the computational complexity and avoids the need for numerical integration. The approach also allows for reformulation of come classical regularization methods as Laplacian and Tikhonov regularization as Gaussian process regression, and hence provides an efficient algorithm and principled means for their parameter tuning. Results from simulated and real data indicate that this approach is less sensitive to streak artifacts as compared to the commonly used method of filtered backprojection.



### Long-Term Occupancy Grid Prediction Using Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.03782v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03782v2)
- **Published**: 2018-09-11 10:21:39+00:00
- **Updated**: 2019-06-07 12:21:02+00:00
- **Authors**: Marcel Schreiber, Stefan Hoermann, Klaus Dietmayer
- **Comment**: 8 pages, 10 figures
- **Journal**: None
- **Summary**: We tackle the long-term prediction of scene evolution in a complex downtown scenario for automated driving based on Lidar grid fusion and recurrent neural networks (RNNs). A bird's eye view of the scene, including occupancy and velocity, is fed as a sequence to a RNN which is trained to predict future occupancy. The nature of prediction allows generation of multiple hours of training data without the need of manual labeling. Thus, the training strategy and loss function is designed for long sequences of real-world data (unbalanced, continuously changing situations, false labels, etc.). The deep CNN architecture comprises convolutional long short-term memories (ConvLSTMs) to separate static from dynamic regions and to predict dynamic objects in future frames. Novel recurrent skip connections show the ability to predict small occluded objects, i.e. pedestrians, and occluded static regions. Spatio-temporal correlations between grid cells are exploited to predict multimodal future paths and interactions between objects. Experiments also quantify improvements to our previous network, a Monte Carlo approach, and literature.



### Normalization in Training U-Net for 2D Biomedical Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1809.03783v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03783v3)
- **Published**: 2018-09-11 10:27:45+00:00
- **Updated**: 2019-01-12 21:12:14+00:00
- **Authors**: Xiao-Yun Zhou, Guang-Zhong Yang
- **Comment**: 5 figures, 5 tables, published on IEEE Robotics and Automation
  Letters 2019
- **Journal**: None
- **Summary**: 2D biomedical semantic segmentation is important for robotic vision in surgery. Segmentation methods based on Deep Convolutional Neural Network (DCNN) can out-perform conventional methods in terms of both accuracy and levels of automation. One common issue in training a DCNN for biomedical semantic segmentation is the internal covariate shift where the training of convolutional kernels is encumbered by the distribution change of input features, hence both the training speed and performance are decreased. Batch Normalization (BN) is the first proposed method for addressing internal covariate shift and is widely used. Instance Normalization (IN) and Layer Normalization (LN) have also been proposed. Group Normalization (GN) is proposed more recently and has not yet been applied to 2D biomedical semantic segmentation, however, no specific validations on GN were given. Most DCNNs for biomedical semantic segmentation adopt BN as the normalization method by default, without reviewing its performance. In this paper, four normalization methods - BN, IN, LN and GN are compared in details, specifically for 2D biomedical semantic segmentation. U-Net is adopted as the basic DCNN structure. Three datasets regarding the Right Ventricle (RV), aorta, and Left Ventricle (LV) are used for the validation. The results show that detailed subdivision of the feature map, i.e. GN with a large group number or IN, achieves higher accuracy. This accuracy improvement mainly comes from better model generalization. Codes are uploaded and maintained at Xiao-Yun Zhou's Github.



### Convolutional Neural Networks for the segmentation of microcalcification in Mammography Imaging
- **Arxiv ID**: http://arxiv.org/abs/1809.03788v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03788v1)
- **Published**: 2018-09-11 10:51:19+00:00
- **Updated**: 2018-09-11 10:51:19+00:00
- **Authors**: Gabriele Valvano, Gianmarco Santini, Nicola Martini, Andrea Ripoli, Chiara Iacconi, Dante Chiappino, Daniele Della Latta
- **Comment**: 13 pages, 7 figures
- **Journal**: None
- **Summary**: Cluster of microcalcifications can be an early sign of breast cancer. In this paper we propose a novel approach based on convolutional neural networks for the detection and segmentation of microcalcification clusters. In this work we used 283 mammograms to train and validate our model, obtaining an accuracy of 98.22% in the detection of preliminary suspect regions and of 97.47% in the segmentation task. Our results show how deep learning could be an effective tool to effectively support radiologists during mammograms examination.



### Unsupervised Stylish Image Description Generation via Domain Layer Norm
- **Arxiv ID**: http://arxiv.org/abs/1809.06214v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.06214v1)
- **Published**: 2018-09-11 11:07:26+00:00
- **Updated**: 2018-09-11 11:07:26+00:00
- **Authors**: Cheng Kuan Chen, Zhu Feng Pan, Min Sun, Ming-Yu Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the existing works on image description focus on generating expressive descriptions. The only few works that are dedicated to generating stylish (e.g., romantic, lyric, etc.) descriptions suffer from limited style variation and content digression. To address these limitations, we propose a controllable stylish image description generation model. It can learn to generate stylish image descriptions that are more related to image content and can be trained with the arbitrary monolingual corpus without collecting new paired image and stylish descriptions. Moreover, it enables users to generate various stylish descriptions by plugging in style-specific parameters to include new styles into the existing model. We achieve this capability via a novel layer normalization layer design, which we will refer to as the Domain Layer Norm (DLN). Extensive experimental validation and user study on various stylish image description generation tasks are conducted to show the competitive advantages of the proposed model.



### A Dataset and Preliminary Results for Umpire Pose Detection Using SVM Classification of Deep Features
- **Arxiv ID**: http://arxiv.org/abs/1809.06217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06217v1)
- **Published**: 2018-09-11 12:44:57+00:00
- **Updated**: 2018-09-11 12:44:57+00:00
- **Authors**: Aravind Ravi, Harshwin Venugopal, Sruthy Paul, Hamid R. Tizhoosh
- **Comment**: To be published at the 2018 IEEE Symposium Series on Computational
  Intelligence (IEEE SSCI 2018), 18-21 NOV, 2018, BENGALURU, INDIA
- **Journal**: None
- **Summary**: In recent years, there has been increased interest in video summarization and automatic sports highlights generation. In this work, we introduce a new dataset, called SNOW, for umpire pose detection in the game of cricket. The proposed dataset is evaluated as a preliminary aid for developing systems to automatically generate cricket highlights. In cricket, the umpire has the authority to make important decisions about events on the field. The umpire signals important events using unique hand signals and gestures. We identify four such events for classification namely SIX, NO BALL, OUT and WIDE based on detecting the pose of the umpire from the frames of a cricket video. Pre-trained convolutional neural networks such as Inception V3 and VGG19 net-works are selected as primary candidates for feature extraction. The results are obtained using a linear SVM classifier. The highest classification performance was achieved for the SVM trained on features extracted from the VGG19 network. The preliminary results suggest that the proposed system is an effective solution for the application of cricket highlights generation.



### Facial Recognition with Encoded Local Projections
- **Arxiv ID**: http://arxiv.org/abs/1809.06218v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06218v1)
- **Published**: 2018-09-11 13:11:07+00:00
- **Updated**: 2018-09-11 13:11:07+00:00
- **Authors**: Dhruv Sharma, Sarim Zafar, Morteza Babaie, H. R. Tizhoosh
- **Comment**: To be published at the 2018 IEEE Symp. Series on Comp. Intelligence
  (IEEE SSCI 2018), 18-21 NOV, 2018, BENGALURU, INDIA
- **Journal**: None
- **Summary**: Encoded Local Projections (ELP) is a recently introduced dense sampling image descriptor which uses projections in small neighbourhoods to construct a histogram/descriptor for the entire image. ELP has shown to be as accurate as other state-of-the-art features in searching medical images while being time and resource efficient. This paper attempts for the first time to utilize ELP descriptor as primary features for facial recognition and compare the results with LBP histogram on the Labeled Faces in the Wild dataset. We have evaluated descriptors by comparing the chi-squared distance of each image descriptor versus all others as well as training Support Vector Machines (SVM) with each feature vector. In both cases, the results of ELP were better than LBP in the same sub-image configuration.



### Visualizing Convolutional Neural Networks to Improve Decision Support for Skin Lesion Classification
- **Arxiv ID**: http://arxiv.org/abs/1809.03851v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.03851v1)
- **Published**: 2018-09-11 13:17:38+00:00
- **Updated**: 2018-09-11 13:17:38+00:00
- **Authors**: Pieter Van Molle, Miguel De Strooper, Tim Verbelen, Bert Vankeirsbilck, Pieter Simoens, Bart Dhoedt
- **Comment**: 8 pages, 6 figures, Workshop on Interpretability of Machine
  Intelligence in Medical Image Computing at MICCAI 2018
- **Journal**: None
- **Summary**: Because of their state-of-the-art performance in computer vision, CNNs are becoming increasingly popular in a variety of fields, including medicine. However, as neural networks are black box function approximators, it is difficult, if not impossible, for a medical expert to reason about their output. This could potentially result in the expert distrusting the network when he or she does not agree with its output. In such a case, explaining why the CNN makes a certain decision becomes valuable information. In this paper, we try to open the black box of the CNN by inspecting and visualizing the learned feature maps, in the field of dermatology. We show that, to some extent, CNNs focus on features similar to those used by dermatologists to make a diagnosis. However, more research is required for fully explaining their output.



### A Detection and Segmentation Architecture for Skin Lesion Segmentation on Dermoscopy Images
- **Arxiv ID**: http://arxiv.org/abs/1809.03917v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03917v2)
- **Published**: 2018-09-11 14:20:15+00:00
- **Updated**: 2018-09-30 06:02:20+00:00
- **Authors**: Chengyao Qian, Ting Liu, Hao Jiang, Zhe Wang, Pengfei Wang, Mingxin Guan, Biao Sun
- **Comment**: 5 pages, 9 figures, Ranked 1st place in ISIC 2018 task1, title
  updated and results added
- **Journal**: None
- **Summary**: This report summarises our method and validation results for the ISIC Challenge 2018 - Skin Lesion Analysis Towards Melanoma Detection - Task 1: Lesion Segmentation. We present a two-stage method for lesion segmentation with optimised training method and ensemble post-process. Our method achieves state-of-the-art performance on lesion segmentation and we win the first place in ISIC 2018 task1.



### Hubless keypoint-based 3D deformable groupwise registration
- **Arxiv ID**: http://arxiv.org/abs/1809.03951v3
- **DOI**: 10.1016/j.media.2019.101564
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03951v3)
- **Published**: 2018-09-11 14:59:03+00:00
- **Updated**: 2019-09-26 12:31:51+00:00
- **Authors**: Rémi Agier, Sébastien Valette, Razmig Kéchichian, Laurent Fanton, Rémy Prost
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel algorithm for Fast Registration Of image Groups (FROG), applied to large 3D image groups. Our approach extracts 3D SURF keypoints from images, computes matched pairs of keypoints and registers the group by minimizing pair distances in a hubless way i.e. without computing any central mean image. Using keypoints significantly reduces the problem complexity compared to voxel-based approaches, and enables us to provide an in-core global optimization, similar to the Bundle Adjustment for 3D reconstruction. As we aim to register images of different patients, the matching step yields many outliers. Then we propose a new EM-weighting algorithm which efficiently discards outliers. Global optimization is carried out with a fast gradient descent algorithm. This allows our approach to robustly register large datasets. The result is a set of diffeomorphic half transforms which link the volumes together and can be subsequently exploited for computational anatomy and landmark detection. We show experimental results on whole-body CT scans, with groups of up to 103 volumes. On a benchmark based on anatomical landmarks, our algorithm compares favorably with the star-groupwise voxel-based ANTs and NiftyReg approaches while being much faster. We also discuss the limitations of our approach for lower resolution images such as brain MRI.



### Efficient Road Lane Marking Detection with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.03994v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03994v1)
- **Published**: 2018-09-11 15:58:48+00:00
- **Updated**: 2018-09-11 15:58:48+00:00
- **Authors**: Ping-Rong Chen, Shao-Yuan Lo, Hsueh-Ming Hang, Sheng-Wei Chan, Jing-Jhih Lin
- **Comment**: Accepted at International Conference on Digital Signal Processing
  (DSP) 2018
- **Journal**: None
- **Summary**: Lane mark detection is an important element in the road scene analysis for Advanced Driver Assistant System (ADAS). Limited by the onboard computing power, it is still a challenge to reduce system complexity and maintain high accuracy at the same time. In this paper, we propose a Lane Marking Detector (LMD) using a deep convolutional neural network to extract robust lane marking features. To improve its performance with a target of lower complexity, the dilated convolution is adopted. A shallower and thinner structure is designed to decrease the computational cost. Moreover, we also design post-processing algorithms to construct 3rd-order polynomial models to fit into the curved lanes. Our system shows promising results on the captured road scenes.



### Capsule Deep Neural Network for Recognition of Historical Graffiti Handwriting
- **Arxiv ID**: http://arxiv.org/abs/1809.06693v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.06693v1)
- **Published**: 2018-09-11 17:02:13+00:00
- **Updated**: 2018-09-11 17:02:13+00:00
- **Authors**: Nikita Gordienko, Yuriy Kochura, Vlad Taran, Gang Peng, Yuri Gordienko, Sergii Stirenko
- **Comment**: 6 pages, 8 figures, accepted for 2018 IEEE Ukraine Student, Young
  Professional and Women in Engineering Congress (UKRSYW), October 2-6, 2018
  (Kyiv, Ukraine). arXiv admin note: text overlap with arXiv:1808.10862
- **Journal**: None
- **Summary**: Automatic recognition of the historical letters (XI-XVIII centuries) carved on the stoned walls of St.Sophia cathedral in Kyiv (Ukraine) was demonstrated by means of capsule deep learning neural network. It was applied to the image dataset of the carved Glagolitic and Cyrillic letters (CGCL), which was assembled and pre-processed recently for recognition and prediction by machine learning methods (https://www.kaggle.com/yoctoman/graffiti-st-sophia-cathedral-kyiv). CGCL dataset contains >4000 images for glyphs of 34 letters which are hardly recognized by experts even in contrast to notMNIST dataset with the better images of 10 letters taken from different fonts. Despite the much worse quality of CGCL dataset and extremely low number of samples (in comparison to notMNIST dataset) the capsule network model demonstrated much better results than the previously used convolutional neural network (CNN). The validation accuracy (and validation loss) was higher (lower) for capsule network model than for CNN without data augmentation even. The area under curve (AUC) values for receiver operating characteristic (ROC) were also higher for the capsule network model than for CNN model: 0.88-0.93 (capsule network) and 0.50 (CNN) without data augmentation, 0.91-0.95 (capsule network) and 0.51 (CNN) with lossless data augmentation, and similar results of 0.91-0.93 (capsule network) and 0.9 (CNN) in the regime of lossless data augmentation only. The confusion matrixes were much better for capsule network than for CNN model and gave the much lower type I (false positive) and type II (false negative) values in all three regimes of data augmentation. These results supports the previous claims that capsule-like networks allow to reduce error rates not only on MNIST digit dataset, but on the other notMNIST letter dataset and the more complex CGCL handwriting graffiti letter dataset also.



### Ensemble learning with 3D convolutional neural networks for connectome-based prediction
- **Arxiv ID**: http://arxiv.org/abs/1809.06219v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.06219v2)
- **Published**: 2018-09-11 17:55:10+00:00
- **Updated**: 2019-06-04 21:55:28+00:00
- **Authors**: Meenakshi Khosla, Keith Jamison, Amy Kuceyeski, Mert R. Sabuncu
- **Comment**: 45 pages, 9 figures, 4 supplementary figures (To appear in
  Neuroimage)
- **Journal**: None
- **Summary**: The specificty and sensitivity of resting state functional MRI (rs-fMRI) measurements depend on pre-processing choices, such as the parcellation scheme used to define regions of interest (ROIs). In this study, we critically evaluate the effect of brain parcellations on machine learning models applied to rs-fMRI data. Our experiments reveal a remarkable trend: On average, models with stochastic parcellations consistently perform as well as models with widely used atlases at the same spatial scale. We thus propose an ensemble learning strategy to combine the predictions from models trained on connectivity data extracted using different (e.g., stochastic) parcellations. We further present an implementation of our ensemble learning strategy with a novel 3D Convolutional Neural Network (CNN) approach. The proposed CNN approach takes advantage of the full-resolution 3D spatial structure of rs-fMRI data and fits non-linear predictive models. Our ensemble CNN framework overcomes the limitations of traditional machine learning models for connectomes that often rely on region-based summary statistics and/or linear models. We showcase our approach on a classification (autism patients versus healthy controls) and a regression problem (prediction of subject's age), and report promising results.



### FIVR: Fine-grained Incident Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1809.04094v2
- **DOI**: 10.1109/TMM.2019.2905741
- **Categories**: **cs.MM**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1809.04094v2)
- **Published**: 2018-09-11 18:09:44+00:00
- **Updated**: 2019-03-24 08:59:05+00:00
- **Authors**: Giorgos Kordopatis-Zilos, Symeon Papadopoulos, Ioannis Patras, Ioannis Kompatsiaris
- **Comment**: None
- **Journal**: IEEE Transactions on Multimedia 2019
- **Summary**: This paper introduces the problem of Fine-grained Incident Video Retrieval (FIVR). Given a query video, the objective is to retrieve all associated videos, considering several types of associations that range from duplicate videos to videos from the same incident. FIVR offers a single framework that contains several retrieval tasks as special cases. To address the benchmarking needs of all such tasks, we construct and present a large-scale annotated video dataset, which we call FIVR-200K, and it comprises 225,960 videos. To create the dataset, we devise a process for the collection of YouTube videos based on major news events from recent years crawled from Wikipedia and deploy a retrieval pipeline for the automatic selection of query videos based on their estimated suitability as benchmarks. We also devise a protocol for the annotation of the dataset with respect to the four types of video associations defined by FIVR. Finally, we report the results of an experimental study on the dataset comparing five state-of-the-art methods developed based on a variety of visual descriptors, highlighting the challenges of the current problem.



### Parallel Separable 3D Convolution for Video and Volumetric Data Understanding
- **Arxiv ID**: http://arxiv.org/abs/1809.04096v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04096v1)
- **Published**: 2018-09-11 18:15:20+00:00
- **Updated**: 2018-09-11 18:15:20+00:00
- **Authors**: Felix Gonda, Donglai Wei, Toufiq Parag, Hanspeter Pfister
- **Comment**: None
- **Journal**: None
- **Summary**: For video and volumetric data understanding, 3D convolution layers are widely used in deep learning, however, at the cost of increasing computation and training time. Recent works seek to replace the 3D convolution layer with convolution blocks, e.g. structured combinations of 2D and 1D convolution layers. In this paper, we propose a novel convolution block, Parallel Separable 3D Convolution (PmSCn), which applies m parallel streams of n 2D and one 1D convolution layers along different dimensions. We first mathematically justify the need of parallel streams (Pm) to replace a single 3D convolution layer through tensor decomposition. Then we jointly replace consecutive 3D convolution layers, common in modern network architectures, with the multiple 2D convolution layers (Cn). Lastly, we empirically show that PmSCn is applicable to different backbone architectures, such as ResNet, DenseNet, and UNet, for different applications, such as video action recognition, MRI brain segmentation, and electron microscopy segmentation. In all three applications, we replace the 3D convolution layers in state-of-the art models with PmSCn and achieve around 14% improvement in test performance and 40% reduction in model size and on average.



### On the Structural Sensitivity of Deep Convolutional Networks to the Directions of Fourier Basis Functions
- **Arxiv ID**: http://arxiv.org/abs/1809.04098v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04098v2)
- **Published**: 2018-09-11 18:23:09+00:00
- **Updated**: 2019-04-18 03:45:23+00:00
- **Authors**: Yusuke Tsuzuku, Issei Sato
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Data-agnostic quasi-imperceptible perturbations on inputs are known to degrade recognition accuracy of deep convolutional networks severely. This phenomenon is considered to be a potential security issue. Moreover, some results on statistical generalization guarantees indicate that the phenomenon can be a key to improve the networks' generalization. However, the characteristics of the shared directions of such harmful perturbations remain unknown. Our primal finding is that convolutional networks are sensitive to the directions of Fourier basis functions. We derived the property by specializing a hypothesis of the cause of the sensitivity, known as the linearity of neural networks, to convolutional networks and empirically validated it. As a by-product of the analysis, we propose an algorithm to create shift-invariant universal adversarial perturbations available in black-box settings.



### Humans can decipher adversarial images
- **Arxiv ID**: http://arxiv.org/abs/1809.04120v3
- **DOI**: 10.1038/s41467-019-08931-6
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.04120v3)
- **Published**: 2018-09-11 19:39:51+00:00
- **Updated**: 2019-03-03 18:37:06+00:00
- **Authors**: Zhenglong Zhou, Chaz Firestone
- **Comment**: 14 pages, 4 figures
- **Journal**: Nature Communications, 10, 1334 (2019)
- **Summary**: How similar is the human mind to the sophisticated machine-learning systems that mirror its performance? Models of object categorization based on convolutional neural networks (CNNs) have achieved human-level benchmarks in assigning known labels to novel images. These advances promise to support transformative technologies such as autonomous vehicles and machine diagnosis; beyond this, they also serve as candidate models for the visual system itself -- not only in their output but perhaps even in their underlying mechanisms and principles. However, unlike human vision, CNNs can be "fooled" by adversarial examples -- carefully crafted images that appear as nonsense patterns to humans but are recognized as familiar objects by machines, or that appear as one object to humans and a different object to machines. This seemingly extreme divergence between human and machine classification challenges the promise of these new advances, both as applied image-recognition systems and also as models of the human mind. Surprisingly, however, little work has empirically investigated human classification of such adversarial stimuli: Does human and machine performance fundamentally diverge? Or could humans decipher such images and predict the machine's preferred labels? Here, we show that human and machine classification of adversarial stimuli are robustly related: In eight experiments on five prominent and diverse adversarial imagesets, human subjects reliably identified the machine's chosen label over relevant foils. This pattern persisted for images with strong antecedent identities, and even for images described as "totally unrecognizable to human eyes". We suggest that human intuition may be a more reliable guide to machine (mis)classification than has typically been imagined, and we explore the consequences of this result for minds and machines alike.



### Magnetically Guided Capsule Endoscopy
- **Arxiv ID**: http://arxiv.org/abs/1809.04130v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04130v1)
- **Published**: 2018-09-11 19:55:52+00:00
- **Updated**: 2018-09-11 19:55:52+00:00
- **Authors**: Thomas Kruezer
- **Comment**: None
- **Journal**: None
- **Summary**: The following research undertakes a historical review of this technology with specific highlighting of its advancement in medical diagnostics as well as the therapeutic functionality of wireless capsule endoscopy. Without restriction to the gastrointestinal tract alone, the review will additionally investigate the developments in the technology of micro-robots guided through the magnetic power and are capable of navigating through multiple forms of air and fluid filled lumina as well as cavities within the body. All these capabilities are of use in the utilization of minimally invasive medicine.



### Simultaneous Localization and Layout Model Selection in Manhattan Worlds
- **Arxiv ID**: http://arxiv.org/abs/1809.04135v3
- **DOI**: 10.1109/LRA.2019.2893417
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.04135v3)
- **Published**: 2018-09-11 20:04:15+00:00
- **Updated**: 2018-12-13 21:38:50+00:00
- **Authors**: Armon Shariati, Bernd Pfrommer, Camillo J. Taylor
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we will demonstrate how Manhattan structure can be exploited to transform the Simultaneous Localization and Mapping (SLAM) problem, which is typically solved by a nonlinear optimization over feature positions, into a model selection problem solved by a convex optimization over higher order layout structures, namely walls, floors, and ceilings. Furthermore, we show how our novel formulation leads to an optimization procedure that automatically performs data association and loop closure and which ultimately produces the simplest model of the environment that is consistent with the available measurements. We verify our method on real world data sets collected with various sensing modalities.



### JigsawNet: Shredded Image Reassembly using Convolutional Neural Network and Loop-based Composition
- **Arxiv ID**: http://arxiv.org/abs/1809.04137v1
- **DOI**: 10.1109/TIP.2019.2903298
- **Categories**: **cs.CV**, I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/1809.04137v1)
- **Published**: 2018-09-11 20:07:27+00:00
- **Updated**: 2018-09-11 20:07:27+00:00
- **Authors**: Canyu Le, Xin Li
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel algorithm to reassemble an arbitrarily shredded image to its original status. Existing reassembly pipelines commonly consist of a local matching stage and a global compositions stage. In the local stage, a key challenge in fragment reassembly is to reliably compute and identify correct pairwise matching, for which most existing algorithms use handcrafted features, and hence, cannot reliably handle complicated puzzles. We build a deep convolutional neural network to detect the compatibility of a pairwise stitching, and use it to prune computed pairwise matches. To improve the network efficiency and accuracy, we transfer the calculation of CNN to the stitching region and apply a boost training strategy. In the global composition stage, we modify the commonly adopted greedy edge selection strategies to two new loop closure based searching algorithms. Extensive experiments show that our algorithm significantly outperforms existing methods on solving various puzzles, especially those challenging ones with many fragment pieces.



### End-to-end Image Captioning Exploits Multimodal Distributional Similarity
- **Arxiv ID**: http://arxiv.org/abs/1809.04144v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04144v1)
- **Published**: 2018-09-11 20:32:21+00:00
- **Updated**: 2018-09-11 20:32:21+00:00
- **Authors**: Pranava Madhyastha, Josiah Wang, Lucia Specia
- **Comment**: Published in BMVC 2018
- **Journal**: None
- **Summary**: We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn `distributional similarity' in a multimodal feature space by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the `image' side of image captioning, and vary the input image representation but keep the RNN text generation component of a CNN-RNN model constant. Our analysis indicates that image captioning models (i) are capable of separating structure from noisy input representations; (ii) suffer virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together. Our findings indicate that our distributional similarity hypothesis holds. We conclude that regardless of the image representation used image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.



### Intensity and Rescale Invariant Copy Move Forgery Detection Techniques
- **Arxiv ID**: http://arxiv.org/abs/1809.04154v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04154v1)
- **Published**: 2018-09-11 20:53:01+00:00
- **Updated**: 2018-09-11 20:53:01+00:00
- **Authors**: Tejas K, Swathi C, Rajesh Kumar M
- **Comment**: Further research is active on this paper in VIT University. Hence,
  the paper is yet not published
- **Journal**: None
- **Summary**: In this contemporary world digital media such as videos and images behave as an active medium to carry valuable information across the globe on all fronts. However there are several techniques evolved to tamper the image which has made their authenticity untrustworthy. CopyMove Forgery CMF is one of the most common forgeries present in an image where a cluster of pixels are duplicated in the same image with potential postprocessing techniques. Various state-of-art techniques are developed in the recent years which are effective in detecting passive image forgery. However most methods do fail when the copied image is rescaled or added with certain intensity before being pasted due to de-synchronization of pixels in the searching process. To tackle this problem the paper proposes distinct novel algorithms which recognize a unique approach of using Hus invariant moments and Discreet Cosine Transformations DCT to attain the desired rescale invariant and intensity invariant CMF detection techniques respectively. The experiments conducted quantitatively and qualitatively demonstrate the effectiveness of the algorithm.



### Heated-Up Softmax Embedding
- **Arxiv ID**: http://arxiv.org/abs/1809.04157v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.04157v1)
- **Published**: 2018-09-11 20:56:02+00:00
- **Updated**: 2018-09-11 20:56:02+00:00
- **Authors**: Xu Zhang, Felix Xinnan Yu, Svebor Karaman, Wei Zhang, Shih-Fu Chang
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Metric learning aims at learning a distance which is consistent with the semantic meaning of the samples. The problem is generally solved by learning an embedding for each sample such that the embeddings of samples of the same category are compact while the embeddings of samples of different categories are spread-out in the feature space. We study the features extracted from the second last layer of a deep neural network based classifier trained with the cross entropy loss on top of the softmax layer. We show that training classifiers with different temperature values of softmax function leads to features with different levels of compactness. Leveraging these insights, we propose a "heating-up" strategy to train a classifier with increasing temperatures, leading the corresponding embeddings to achieve state-of-the-art performance on a variety of metric learning benchmarks.



### Iterative Segmentation from Limited Training Data: Applications to Congenital Heart Disease
- **Arxiv ID**: http://arxiv.org/abs/1809.04182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04182v1)
- **Published**: 2018-09-11 22:17:14+00:00
- **Updated**: 2018-09-11 22:17:14+00:00
- **Authors**: Danielle F. Pace, Adrian V. Dalca, Tom Brosch, Tal Geva, Andrew J. Powell, Jürgen Weese, Mehdi H. Moghari, Polina Golland
- **Comment**: Presented at the Deep Learning in Medical Image Analysis Workshop,
  MICCAI 2018
- **Journal**: None
- **Summary**: We propose a new iterative segmentation model which can be accurately learned from a small dataset. A common approach is to train a model to directly segment an image, requiring a large collection of manually annotated images to capture the anatomical variability in a cohort. In contrast, we develop a segmentation model that recursively evolves a segmentation in several steps, and implement it as a recurrent neural network. We learn model parameters by optimizing the interme- diate steps of the evolution in addition to the final segmentation. To this end, we train our segmentation propagation model by presenting incom- plete and/or inaccurate input segmentations paired with a recommended next step. Our work aims to alleviate challenges in segmenting heart structures from cardiac MRI for patients with congenital heart disease (CHD), which encompasses a range of morphological deformations and topological changes. We demonstrate the advantages of this approach on a dataset of 20 images from CHD patients, learning a model that accurately segments individual heart chambers and great vessels. Com- pared to direct segmentation, the iterative method yields more accurate segmentation for patients with the most severe CHD malformations.



### Searching for Efficient Multi-Scale Architectures for Dense Image Prediction
- **Arxiv ID**: http://arxiv.org/abs/1809.04184v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.04184v1)
- **Published**: 2018-09-11 22:36:01+00:00
- **Updated**: 2018-09-11 22:36:01+00:00
- **Authors**: Liang-Chieh Chen, Maxwell D. Collins, Yukun Zhu, George Papandreou, Barret Zoph, Florian Schroff, Hartwig Adam, Jonathon Shlens
- **Comment**: Accepted by NIPS 2018
- **Journal**: None
- **Summary**: The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Much work has endeavored to design and build architectures automatically through clever construction of a search space paired with simple learning algorithms. Recent progress has demonstrated that such meta-learning methods may exceed scalable human-invented architectures on image classification tasks. An open question is the degree to which such methods may generalize to new domains. In this work we explore the construction of meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation. Constructing viable search spaces in this domain is challenging because of the multi-scale representation of visual information and the necessity to operate on high resolution imagery. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architectures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7\% on Cityscapes (street scene parsing), 71.3\% on PASCAL-Person-Part (person-part segmentation), and 87.9\% on PASCAL VOC 2012 (semantic image segmentation). Additionally, the resulting architecture is more computationally efficient, requiring half the parameters and half the computational cost as previous state of the art systems.



### Deep Micro-Dictionary Learning and Coding Network
- **Arxiv ID**: http://arxiv.org/abs/1809.04185v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04185v2)
- **Published**: 2018-09-11 22:36:36+00:00
- **Updated**: 2018-12-25 13:03:42+00:00
- **Authors**: Hao Tang, Heng Wei, Wei Xiao, Wei Wang, Dan Xu, Yan Yan, Nicu Sebe
- **Comment**: 10 page, 8 figures, accepted to WACV 2019
- **Journal**: None
- **Summary**: In this paper, we propose a novel Deep Micro-Dictionary Learning and Coding Network (DDLCN). DDLCN has most of the standard deep learning layers (pooling, fully, connected, input/output, etc.) but the main difference is that the fundamental convolutional layers are replaced by novel compound dictionary learning and coding layers. The dictionary learning layer learns an over-complete dictionary for the input training data. At the deep coding layer, a locality constraint is added to guarantee that the activated dictionary bases are close to each other. Next, the activated dictionary atoms are assembled together and passed to the next compound dictionary learning and coding layers. In this way, the activated atoms in the first layer can be represented by the deeper atoms in the second dictionary. Intuitively, the second dictionary is designed to learn the fine-grained components which are shared among the input dictionary atoms. In this way, a more informative and discriminative low-level representation of the dictionary atoms can be obtained. We empirically compare the proposed DDLCN with several dictionary learning methods and deep learning architectures. The experimental results on four popular benchmark datasets demonstrate that the proposed DDLCN achieves competitive results compared with state-of-the-art approaches.



### Fourier-Domain Optimization for Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1809.04187v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04187v1)
- **Published**: 2018-09-11 22:38:00+00:00
- **Updated**: 2018-09-11 22:38:00+00:00
- **Authors**: Majed El Helou, Frederike Dümbgen, Radhakrishna Achanta, Sabine Süsstrunk
- **Comment**: None
- **Journal**: None
- **Summary**: Image optimization problems encompass many applications such as spectral fusion, deblurring, deconvolution, dehazing, matting, reflection removal and image interpolation, among others. With current image sizes in the order of megabytes, it is extremely expensive to run conventional algorithms such as gradient descent, making them unfavorable especially when closed-form solutions can be derived and computed efficiently. This paper explains in detail the framework for solving convex image optimization and deconvolution in the Fourier domain. We begin by explaining the mathematical background and motivating why the presented setups can be transformed and solved very efficiently in the Fourier domain. We also show how to practically use these solutions, by providing the corresponding implementations. The explanations are aimed at a broad audience with minimal knowledge of convolution and image optimization. The eager reader can jump to Section 3 for a footprint of how to solve and implement a sample optimization function, and Section 5 for the more complex cases.



### Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference
- **Arxiv ID**: http://arxiv.org/abs/1809.04191v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04191v2)
- **Published**: 2018-09-11 22:51:55+00:00
- **Updated**: 2019-02-25 01:49:04+00:00
- **Authors**: Jeffrey L. McKinstry, Steven K. Esser, Rathinakumar Appuswamy, Deepika Bablani, John V. Arthur, Izzet B. Yildiz, Dharmendra S. Modha
- **Comment**: None
- **Journal**: None
- **Summary**: To realize the promise of ubiquitous embedded deep network inference, it is essential to seek limits of energy and area efficiency. To this end, low-precision networks offer tremendous promise because both energy and area scale down quadratically with the reduction in precision. Here we demonstrate ResNet-18, -34, -50, -152, Inception-v3, Densenet-161, and VGG-16bn networks on the ImageNet classification benchmark that, at 8-bit precision exceed the accuracy of the full-precision baseline networks after one epoch of finetuning, thereby leveraging the availability of pretrained models. We also demonstrate ResNet-18, -34, -50, -152, Densenet-161, and VGG-16bn 4-bit models that match the accuracy of the full-precision baseline networks -- the highest scores to date. Surprisingly, the weights of the low-precision networks are very close (in cosine similarity) to the weights of the corresponding baseline networks, making training from scratch unnecessary.   We find that gradient noise due to quantization during training increases with reduced precision, and seek ways to overcome this noise. The number of iterations required by SGD to achieve a given training error is related to the square of (a) the distance of the initial solution from the final plus (b) the maximum variance of the gradient estimates. Therefore, we (a) reduce solution distance by starting with pretrained fp32 precision baseline networks and fine-tuning, and (b) combat gradient noise introduced by quantization by training longer and reducing learning rates. Sensitivity analysis indicates that these simple techniques, coupled with proper activation function range calibration to take full advantage of the limited precision, are sufficient to discover low-precision networks, if they exist, close to fp32 precision baseline networks. The results herein provide evidence that 4-bits suffice for classification.



### Clinically Deployed Distributed Magnetic Resonance Imaging Reconstruction: Application to Pediatric Knee Imaging
- **Arxiv ID**: http://arxiv.org/abs/1809.04195v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, cs.DC, 68W15, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1809.04195v1)
- **Published**: 2018-09-11 23:21:48+00:00
- **Updated**: 2018-09-11 23:21:48+00:00
- **Authors**: Michael J. Anderson, Jonathan I. Tamir, Javier S. Turek, Marcus T. Alley, Theodore L. Willke, Shreyas S. Vasanawala, Michael Lustig
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic resonance imaging is capable of producing volumetric images without ionizing radiation. Nonetheless, long acquisitions lead to prohibitively long exams. Compressed sensing (CS) can enable faster scanning via sub-sampling with reduced artifacts. However, CS requires significantly higher reconstruction computation, limiting current clinical applications to 2D/3D or limited-resolution dynamic imaging. Here we analyze the practical limitations to T2 Shuffling, a four-dimensional CS-based acquisition, which provides sharp 3D-isotropic-resolution and multi-contrast images in a single scan. Our improvements to the pipeline on a single machine provide a 3x overall reconstruction speedup, which allowed us to add algorithmic changes improving image quality. Using four machines, we achieved additional 2.1x improvement through distributed parallelization. Our solution reduced the reconstruction time in the hospital to 90 seconds on a 4-node cluster, enabling its use clinically. To understand the implications of scaling this application, we simulated running our reconstructions with a multiple scanner setup typical in hospitals.



### Zoom: SSD-based Vector Search for Optimizing Accuracy, Latency and Memory
- **Arxiv ID**: http://arxiv.org/abs/1809.04067v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1809.04067v1)
- **Published**: 2018-09-11 23:46:33+00:00
- **Updated**: 2018-09-11 23:46:33+00:00
- **Authors**: Minjia Zhang, Yuxiong He
- **Comment**: None
- **Journal**: None
- **Summary**: With the advancement of machine learning and deep learning, vector search becomes instrumental to many information retrieval systems, to search and find best matches to user queries based on their semantic similarities.These online services require the search architecture to be both effective with high accuracy and efficient with low latency and memory footprint, which existing work fails to offer. We develop, Zoom, a new vector search solution that collaboratively optimizes accuracy, latency and memory based on a multiview approach. (1) A "preview" step generates a small set of good candidates, leveraging compressed vectors in memory for reduced footprint and fast lookup. (2) A "fullview" step on SSDs reranks those candidates with their full-length vector, striking high accuracy. Our evaluation shows that, Zoom achieves an order of magnitude improvements on efficiency while attaining equal or higher accuracy, comparing with the state-of-the-art.



