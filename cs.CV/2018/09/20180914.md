# Arxiv Papers in cs.CV on 2018-09-14
### Enhanced Optic Disk and Cup Segmentation with Glaucoma Screening from Fundus Images using Position encoded CNNs
- **Arxiv ID**: http://arxiv.org/abs/1809.05216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05216v1)
- **Published**: 2018-09-14 01:31:15+00:00
- **Updated**: 2018-09-14 01:31:15+00:00
- **Authors**: Vismay Agrawal, Avinash Kori, Varghese Alex, Ganapathy Krishnamurthi
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: In this manuscript, we present a robust method for glaucoma screening from fundus images using an ensemble of convolutional neural networks (CNNs). The pipeline comprises of first segmenting the optic disk and optic cup from the fundus image, then extracting a patch centered around the optic disk and subsequently feeding to the classification network to differentiate the image as diseased or healthy. In the segmentation network, apart from the image, we make use of spatial co-ordinate (X \& Y) space so as to learn the structure of interest better. The classification network is composed of a DenseNet201 and a ResNet18 which were pre-trained on a large cohort of natural images. On the REFUGE validation data (n=400), the segmentation network achieved a dice score of 0.88 and 0.64 for optic disc and optic cup respectively. For the tasking differentiating images affected with glaucoma from healthy images, the area under the ROC curve was observed to be 0.85.



### A Variational Observation Model of 3D Object for Probabilistic Semantic SLAM
- **Arxiv ID**: http://arxiv.org/abs/1809.05225v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.05225v1)
- **Published**: 2018-09-14 02:27:58+00:00
- **Updated**: 2018-09-14 02:27:58+00:00
- **Authors**: H. W. Yu, B. H. Le
- **Comment**: will be submitted to ICRA 2018
- **Journal**: None
- **Summary**: We present a Bayesian object observation model for complete probabilistic semantic SLAM. Recent studies on object detection and feature extraction have become important for scene understanding and 3D mapping. However, 3D shape of the object is too complex to formulate the probabilistic observation model; therefore, performing the Bayesian inference of the object-oriented features as well as their pose is less considered. Besides, when the robot equipped with an RGB mono camera only observes the projected single view of an object, a significant amount of the 3D shape information is abandoned. Due to these limitations, semantic SLAM and viewpoint-independent loop closure using volumetric 3D object shape is challenging. In order to enable the complete formulation of probabilistic semantic SLAM, we approximate the observation model of a 3D object with a tractable distribution. We also estimate the variational likelihood from the 2D image of the object to exploit its observed single view. In order to evaluate the proposed method, we perform pose and feature estimation, and demonstrate that the automatic loop closure works seamlessly without additional loop detector in various environments.



### VoxelMorph: A Learning Framework for Deformable Medical Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1809.05231v3
- **DOI**: 10.1109/TMI.2019.2897538
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05231v3)
- **Published**: 2018-09-14 02:46:16+00:00
- **Updated**: 2019-09-01 21:57:19+00:00
- **Authors**: Guha Balakrishnan, Amy Zhao, Mert R. Sabuncu, John Guttag, Adrian V. Dalca
- **Comment**: Accepted to IEEE TMI ( (c) IEEE). This manuscript expands the CVPR
  2018 paper (arXiv:1802.02604) by introducing an auxiliary model that uses
  segmentation maps during training, an amortized optimization analysis, and
  extensive model analysis. Code available at http://voxelmorph.csail.mit.edu
- **Journal**: None
- **Summary**: We present VoxelMorph, a fast learning-based framework for deformable, pairwise medical image registration. Traditional registration methods optimize an objective function for each pair of images, which can be time-consuming for large datasets or rich deformation models. In contrast to this approach, and building on recent learning-based methods, we formulate registration as a function that maps an input image pair to a deformation field that aligns these images. We parameterize the function via a convolutional neural network (CNN), and optimize the parameters of the neural network on a set of images. Given a new pair of scans, VoxelMorph rapidly computes a deformation field by directly evaluating the function. In this work, we explore two different training strategies. In the first (unsupervised) setting, we train the model to maximize standard image matching objective functions that are based on the image intensities. In the second setting, we leverage auxiliary segmentations available in the training data. We demonstrate that the unsupervised model's accuracy is comparable to state-of-the-art methods, while operating orders of magnitude faster. We also show that VoxelMorph trained with auxiliary data improves registration accuracy at test time, and evaluate the effect of training set size on registration. Our method promises to speed up medical image analysis and processing pipelines, while facilitating novel directions in learning-based registration and its applications. Our code is freely available at voxelmorph.csail.mit.edu.



### Network Recasting: A Universal Method for Network Architecture Transformation
- **Arxiv ID**: http://arxiv.org/abs/1809.05262v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.05262v2)
- **Published**: 2018-09-14 05:39:15+00:00
- **Updated**: 2019-06-19 09:38:15+00:00
- **Authors**: Joonsang Yu, Sungbum Kang, Kiyoung Choi
- **Comment**: AAAI 2019 Oral presentation, source codes are available on github:
  https://github.com/joonsang-yu/Network-Recasting
- **Journal**: None
- **Summary**: This paper proposes network recasting as a general method for network architecture transformation. The primary goal of this method is to accelerate the inference process through the transformation, but there can be many other practical applications. The method is based on block-wise recasting; it recasts each source block in a pre-trained teacher network to a target block in a student network. For the recasting, a target block is trained such that its output activation approximates that of the source block. Such a block-by-block recasting in a sequential manner transforms the network architecture while preserving the accuracy. This method can be used to transform an arbitrary teacher network type to an arbitrary student network type. It can even generate a mixed-architecture network that consists of two or more types of block. The network recasting can generate a network with fewer parameters and/or activations, which reduce the inference time significantly. Naturally, it can be used for network compression by recasting a trained network into a smaller network of the same type. Our experiments show that it outperforms previous compression approaches in terms of actual speedup on a GPU.



### Detection-by-Localization: Maintenance-Free Change Object Detector
- **Arxiv ID**: http://arxiv.org/abs/1809.05267v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.05267v1)
- **Published**: 2018-09-14 06:25:55+00:00
- **Updated**: 2018-09-14 06:25:55+00:00
- **Authors**: Tanaka Kanji
- **Comment**: 7 pages, 3 figures, Technical report
- **Journal**: None
- **Summary**: Recent researches demonstrate that self-localization performance is a very useful measure of likelihood-of-change (LoC) for change detection. In this paper, this "detection-by-localization" scheme is studied in a novel generalized task of object-level change detection. In our framework, a given query image is segmented into object-level subimages (termed "scene parts"), which are then converted to subimage-level pixel-wise LoC maps via the detection-by-localization scheme. Our approach models a self-localization system as a ranking function, outputting a ranked list of reference images, without requiring relevance score. Thanks to this new setting, we can generalize our approach to a broad class of self-localization systems. Our ranking based self-localization model allows to fuse self-localization results from different modalities via an unsupervised rank fusion derived from a field of multi-modal information retrieval (MMR).



### Left Ventricle Segmentation and Volume Estimation on Cardiac MRI using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.06247v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.06247v2)
- **Published**: 2018-09-14 06:40:07+00:00
- **Updated**: 2018-11-21 16:58:27+00:00
- **Authors**: Ehab Abdelmaguid, Jolene Huang, Sanjay Kenchareddy, Disha Singla, Laura Wilke, Mai H. Nguyen, Ilkay Altintas
- **Comment**: 42 pages
- **Journal**: None
- **Summary**: In the United States, heart disease is the leading cause of death for both men and women, accounting for 610,000 deaths each year [1]. Physicians use Magnetic Resonance Imaging (MRI) scans to take images of the heart in order to non-invasively estimate its structural and functional parameters for cardiovascular diagnosis and disease management. The end-systolic volume (ESV) and end-diastolic volume (EDV) of the left ventricle (LV), and the ejection fraction (EF) are indicators of heart disease. These measures can be derived from the segmented contours of the LV; thus, consistent and accurate segmentation of the LV from MRI images are critical to the accuracy of the ESV, EDV, and EF, and to non-invasive cardiac disease detection.   In this work, various image preprocessing techniques, model configurations using the U-Net deep learning architecture, postprocessing methods, and approaches for volume estimation are investigated. An end-to-end analytics pipeline with multiple stages is provided for automated LV segmentation and volume estimation. First, image data are reformatted and processed from DICOM and NIfTI formats to raw images in array format. Secondly, raw images are processed with multiple image preprocessing methods and cropped to include only the Region of Interest (ROI). Thirdly, preprocessed images are segmented using U-Net models. Lastly, post processing of segmented images to remove extra contours along with intelligent slice and frame selection are applied, followed by calculation of the ESV, EDV, and EF. This analytics pipeline is implemented and runs on a distributed computing environment with a GPU cluster at the San Diego Supercomputer Center at UCSD.



### Keypoint Based Weakly Supervised Human Parsing
- **Arxiv ID**: http://arxiv.org/abs/1809.05285v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05285v1)
- **Published**: 2018-09-14 07:32:43+00:00
- **Updated**: 2018-09-14 07:32:43+00:00
- **Authors**: Zhonghua Wu, Guosheng Lin, Jianfei Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Fully convolutional networks (FCN) have achieved great success in human parsing in recent years. In conventional human parsing tasks, pixel-level labeling is required for guiding the training, which usually involves enormous human labeling efforts. To ease the labeling efforts, we propose a novel weakly supervised human parsing method which only requires simple object keypoint annotations for learning. We develop an iterative learning method to generate pseudo part segmentation masks from keypoint labels. With these pseudo masks, we train an FCN network to output pixel-level human parsing predictions. Furthermore, we develop a correlation network to perform joint prediction of part and object segmentation masks and improve the segmentation performance. The experiment results show that our weakly supervised method is able to achieve very competitive human parsing results. Despite our method only uses simple keypoint annotations for learning, we are able to achieve comparable performance with fully supervised methods which use the expensive pixel-level annotations.



### Deep CNN Frame Interpolation with Lessons Learned from Natural Language Processing
- **Arxiv ID**: http://arxiv.org/abs/1809.05286v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05286v2)
- **Published**: 2018-09-14 07:44:46+00:00
- **Updated**: 2018-09-17 00:43:58+00:00
- **Authors**: Kian Ghodoussi, Nihar Sheth, Zane Durante, Markie Wagner
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: A major area of growth within deep learning has been the study and implementation of convolutional neural networks. The general explanation within the deep learning community of the robustness of convolutional neural networks (CNNs) within image recognition rests upon the idea that CNNs are able to extract localized features. However, recent developments in fields such as Natural Language Processing are demonstrating that this paradigm may be incorrect. In this paper, we analyze the current state of the field concerning CNN's and present a hypothesis that provides a novel explanation for the robustness of CNN models. From there, we demonstrate the effectiveness of our approach by presenting novel deep CNN frame interpolation architecture that is comparable to the state of the art interpolation models with a fraction of the complexity.



### Efficient Rank Minimization via Solving Non-convexPenalties by Iterative Shrinkage-Thresholding Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1809.05292v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.05292v1)
- **Published**: 2018-09-14 07:58:03+00:00
- **Updated**: 2018-09-14 07:58:03+00:00
- **Authors**: Zaiyi Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Rank minimization (RM) is a wildly investigated task of finding solutions by exploiting low-rank structure of parameter matrices. Recently, solving RM problem by leveraging non-convex relaxations has received significant attention. It has been demonstrated by some theoretical and experimental work that non-convex relaxation, e.g. Truncated Nuclear Norm Regularization (TNNR) and Reweighted Nuclear Norm Regularization (RNNR), can provide a better approximation of original problems than convex relaxations. However, designing an efficient algorithm with theoretical guarantee remains a challenging problem. In this paper, we propose a simple but efficient proximal-type method, namely Iterative Shrinkage-Thresholding Algorithm(ISTA), with concrete analysis to solve rank minimization problems with both non-convex weighted and reweighted nuclear norm as low-rank regularizers. Theoretically, the proposed method could converge to the critical point under very mild assumptions with the rate in the order of $O(1/T)$. Moreover, the experimental results on both synthetic data and real world data sets show that proposed algorithm outperforms state-of-arts in both efficiency and accuracy.



### A Domain Agnostic Normalization Layer for Unsupervised Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1809.05298v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05298v1)
- **Published**: 2018-09-14 08:15:52+00:00
- **Updated**: 2018-09-14 08:15:52+00:00
- **Authors**: Rob Romijnders, Panagiotis Meletis, Gijs Dubbelman
- **Comment**: None
- **Journal**: IEEE WACV 2019
- **Summary**: We propose a normalization layer for unsupervised domain adaption in semantic scene segmentation. Normalization layers are known to improve convergence and generalization and are part of many state-of-the-art fully-convolutional neural networks. We show that conventional normalization layers worsen the performance of current Unsupervised Adversarial Domain Adaption (UADA), which is a method to improve network performance on unlabeled datasets and the focus of our research. Therefore, we propose a novel Domain Agnostic Normalization layer and thereby unlock the benefits of normalization layers for unsupervised adversarial domain adaptation. In our evaluation, we adapt from the synthetic GTA5 data set to the real Cityscapes data set, a common benchmark experiment, and surpass the state-of-the-art. As our normalization layer is domain agnostic at test time, we furthermore demonstrate that UADA using Domain Agnostic Normalization improves performance on unseen domains, specifically on Apolloscape and Mapillary.



### Adaptive Sampling Towards Fast Graph Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.05343v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05343v3)
- **Published**: 2018-09-14 10:33:27+00:00
- **Updated**: 2018-11-19 07:50:26+00:00
- **Authors**: Wenbing Huang, Tong Zhang, Yu Rong, Junzhou Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Graph Convolutional Networks (GCNs) have become a crucial tool on learning representations of graph vertices. The main challenge of adapting GCNs on large-scale graphs is the scalability issue that it incurs heavy cost both in computation and memory due to the uncontrollable neighborhood expansion across layers. In this paper, we accelerate the training of GCNs through developing an adaptive layer-wise sampling method. By constructing the network layer by layer in a top-down passway, we sample the lower layer conditioned on the top one, where the sampled neighborhoods are shared by different parent nodes and the over expansion is avoided owing to the fixed-size sampling. More importantly, the proposed sampler is adaptive and applicable for explicit variance reduction, which in turn enhances the training of our method. Furthermore, we propose a novel and economical approach to promote the message passing over distant nodes by applying skip connections. Intensive experiments on several benchmarks verify the effectiveness of our method regarding the classification accuracy while enjoying faster convergence speed.



### Multi-Kernel Diffusion CNNs for Graph-Based Learning on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1809.05370v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05370v1)
- **Published**: 2018-09-14 12:15:37+00:00
- **Updated**: 2018-09-14 12:15:37+00:00
- **Authors**: Lasse Hansen, Jasper Diesel, Mattias P. Heinrich
- **Comment**: accepted for ECCV 2018 Workshop Geometry Meets Deep Learning
- **Journal**: None
- **Summary**: Graph convolutional networks are a new promising learning approach to deal with data on irregular domains. They are predestined to overcome certain limitations of conventional grid-based architectures and will enable efficient handling of point clouds or related graphical data representations, e.g. superpixel graphs. Learning feature extractors and classifiers on 3D point clouds is still an underdeveloped area and has potential restrictions to equal graph topologies. In this work, we derive a new architectural design that combines rotationally and topologically invariant graph diffusion operators and node-wise feature learning through 1x1 convolutions. By combining multiple isotropic diffusion operations based on the Laplace-Beltrami operator, we can learn an optimal linear combination of diffusion kernels for effective feature propagation across nodes on an irregular graph. We validated our approach for learning point descriptors as well as semantic classification on real 3D point clouds of human poses and demonstrate an improvement from 85% to 95% in Dice overlap with our multi-kernel approach.



### Style Augmentation: Data Augmentation via Style Randomization
- **Arxiv ID**: http://arxiv.org/abs/1809.05375v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05375v2)
- **Published**: 2018-09-14 12:34:36+00:00
- **Updated**: 2019-04-12 16:58:20+00:00
- **Authors**: Philip T. Jackson, Amir Atapour-Abarghouei, Stephen Bonner, Toby Breckon, Boguslaw Obara
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce style augmentation, a new form of data augmentation based on random style transfer, for improving the robustness of convolutional neural networks (CNN) over both classification and regression based tasks. During training, our style augmentation randomizes texture, contrast and color, while preserving shape and semantic content. This is accomplished by adapting an arbitrary style transfer network to perform style randomization, by sampling input style embeddings from a multivariate normal distribution instead of inferring them from a style image. In addition to standard classification experiments, we investigate the effect of style augmentation (and data augmentation generally) on domain transfer tasks. We find that data augmentation significantly improves robustness to domain shift, and can be used as a simple, domain agnostic alternative to domain adaptation. Comparing style augmentation against a mix of seven traditional augmentation techniques, we find that it can be readily combined with them to improve network performance. We validate the efficacy of our technique with domain transfer experiments in classification and monocular depth estimation, illustrating consistent improvements in generalization.



### SCORES: Shape Composition with Recursive Substructure Priors
- **Arxiv ID**: http://arxiv.org/abs/1809.05398v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.05398v1)
- **Published**: 2018-09-14 13:20:06+00:00
- **Updated**: 2018-09-14 13:20:06+00:00
- **Authors**: Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, Renjiao Yi, Hao Zhang
- **Comment**: Accepted to SIGGRAPH Asia 2018. Corresponding Author: Kai Xu
  (kevin.kai.xu@gmail.com)
- **Journal**: ACM Transactions on Graphics, 2018
- **Summary**: We introduce SCORES, a recursive neural network for shape composition. Our network takes as input sets of parts from two or more source 3D shapes and a rough initial placement of the parts. It outputs an optimized part structure for the composed shape, leading to high-quality geometry construction. A unique feature of our composition network is that it is not merely learning how to connect parts. Our goal is to produce a coherent and plausible 3D shape, despite large incompatibilities among the input parts. The network may significantly alter the geometry and structure of the input parts and synthesize a novel shape structure based on the inputs, while adding or removing parts to minimize a structure plausibility loss. We design SCORES as a recursive autoencoder network. During encoding, the input parts are recursively grouped to generate a root code. During synthesis, the root code is decoded, recursively, to produce a new, coherent part assembly. Assembled shape structures may be novel, with little global resemblance to training exemplars, yet have plausible substructures. SCORES therefore learns a hierarchical substructure shape prior based on per-node losses. It is trained on structured shapes from ShapeNet, and is applied iteratively to reduce the plausibility loss.We showresults of shape composition from multiple sources over different categories of man-made shapes and compare with state-of-the-art alternatives, demonstrating that our network can significantly expand the range of composable shapes for assembly-based modeling.



### Socially Aware Kalman Neural Networks for Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/1809.05408v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, I.2.9; I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/1809.05408v4)
- **Published**: 2018-09-14 13:27:53+00:00
- **Updated**: 2019-06-26 14:06:42+00:00
- **Authors**: Ce Ju, Zheng Wang, Xiaoyu Zhang
- **Comment**: Superceded by arXiv:1902.10928
- **Journal**: None
- **Summary**: Trajectory prediction is a critical technique in the navigation of robots and autonomous vehicles. However, the complex traffic and dynamic uncertainties yield challenges in the effectiveness and robustness in modeling. We purpose a data-driven approach socially aware Kalman neural networks (SAKNN) where the interaction layer and the Kalman layer are embedded in the architecture, resulting in a class of architectures with huge potential to directly learn from high variance sensor input and robustly generate low variance outcomes. The evaluation of our approach on NGSIM dataset demonstrates that SAKNN performs state-of-the-art on prediction effectiveness in a relatively long-term horizon and significantly improves the signal-to-noise ratio of the predicted signal.



### Real Time System for Facial Analysis
- **Arxiv ID**: http://arxiv.org/abs/1809.05474v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05474v1)
- **Published**: 2018-09-14 15:45:29+00:00
- **Updated**: 2018-09-14 15:45:29+00:00
- **Authors**: Janne Tommola, Pedram Ghazi, Bishwo Adhikari, Heikki Huttunen
- **Comment**: Submitted to EUVIP2018 conference
- **Journal**: None
- **Summary**: In this paper we describe the anatomy of a real-time facial analysis system. The system recognizes the age, gender and facial expression from users in appearing in front of the camera. All components are based on convolutional neural networks, whose accuracy we study on commonly used training and evaluation sets. A key contribution of the work is the description of the interplay between processing threads for frame grabbing, face detection and the three types of recognition. The python code for executing the system uses common libraries--keras/tensorflow, opencv and dlib--and is available for download.



### MoSculp: Interactive Visualization of Shape and Time
- **Arxiv ID**: http://arxiv.org/abs/1809.05491v2
- **DOI**: 10.1145/3242587.3242592
- **Categories**: **cs.HC**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1809.05491v2)
- **Published**: 2018-09-14 16:27:08+00:00
- **Updated**: 2019-01-02 17:56:47+00:00
- **Authors**: Xiuming Zhang, Tali Dekel, Tianfan Xue, Andrew Owens, Qiurui He, Jiajun Wu, Stefanie Mueller, William T. Freeman
- **Comment**: UIST 2018. Project page: http://mosculp.csail.mit.edu/
- **Journal**: None
- **Summary**: We present a system that allows users to visualize complex human motion via 3D motion sculptures---a representation that conveys the 3D structure swept by a human body as it moves through space. Given an input video, our system computes the motion sculptures and provides a user interface for rendering it in different styles, including the options to insert the sculpture back into the original video, render it in a synthetic scene or physically print it.   To provide this end-to-end workflow, we introduce an algorithm that estimates that human's 3D geometry over time from a set of 2D images and develop a 3D-aware image-based rendering approach that embeds the sculpture back into the scene. By automating the process, our system takes motion sculpture creation out of the realm of professional artists, and makes it applicable to a wide range of existing video material.   By providing viewers with 3D information, motion sculptures reveal space-time motion information that is difficult to perceive with the naked eye, and allow viewers to interpret how different parts of the object interact over time. We validate the effectiveness of this approach with user studies, finding that our motion sculpture visualizations are significantly more informative about motion than existing stroboscopic and space-time visualization methods.



### Elastic Registration of Geodesic Vascular Graphs
- **Arxiv ID**: http://arxiv.org/abs/1809.05499v1
- **DOI**: 10.1007/978-3-030-00928-1_91
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05499v1)
- **Published**: 2018-09-14 16:43:25+00:00
- **Updated**: 2018-09-14 16:43:25+00:00
- **Authors**: Stefano Moriconi, Maria A. Zuluaga, H. Rolf Jager, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso
- **Comment**: None
- **Journal**: Medical Image Computing and Computer Assisted Intervention --
  MICCAI 2018
- **Summary**: Vascular graphs can embed a number of high-level features, from morphological parameters, to functional biomarkers, and represent an invaluable tool for longitudinal and cross-sectional clinical inference. This, however, is only feasible when graphs are co-registered together, allowing coherent multiple comparisons. The robust registration of vascular topologies stands therefore as key enabling technology for group-wise analyses. In this work, we present an end-to-end vascular graph registration approach, that aligns networks with non-linear geometries and topological deformations, by introducing a novel overconnected geodesic vascular graph formulation, and without enforcing any anatomical prior constraint. The 3D elastic graph registration is then performed with state-of-the-art graph matching methods used in computer vision. Promising results of vascular matching are found using graphs from synthetic and real angiographies. Observations and future designs are discussed towards potential clinical applications.



### BPE and computer-extracted parenchymal enhancement for breast cancer risk, response monitoring, and prognosis
- **Arxiv ID**: http://arxiv.org/abs/1809.05510v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.05510v1)
- **Published**: 2018-09-14 17:23:12+00:00
- **Updated**: 2018-09-14 17:23:12+00:00
- **Authors**: Bas H. M. van der Velden
- **Comment**: This work has been distributed as part of the syllabus at the ISMRM
  Workshop on Breast MRI: Advancing the State of the Art
- **Journal**: None
- **Summary**: Functional behavior of breast cancer - representing underlying biology - can be analyzed using MRI. The most widely used breast MR imaging protocol is dynamic contrast-enhanced T1-weighted imaging. The cancer enhances on dynamic contrast-enhanced MR imaging because the contrast agent leaks from the leaky vessels into the interstitial space. The contrast agent subsequently leaks back into the vascular space, creating a washout effect. The normal parenchymal tissue of the breast can also enhance after contrast injection. This enhancement generally increases over time. Typically, a radiologist assesses this background parenchymal enhancement (BPE) using the Breast Imaging Reporting and Data System (BI-RADS). According to the BI-RADS, BPE refers to the volume of enhancement and the intensity of enhancement and is divided in four incremental categories: minimal, mild, moderate, and marked.   Researchers have developed semi-automatic and automatic methods to extract properties of BPE from MR images. For clarity, in this syllabus the BI-RADS definition will be referred to as BPE, whereas the computer-extracted properties will not. Both BPE and computer-extracted parenchymal enhancement properties have been linked to screening and diagnosis, hormone status and age, risk of development of breast cancer, response monitoring, and prognosis.



### Identification of multi-scale hierarchical brain functional networks using deep matrix factorization
- **Arxiv ID**: http://arxiv.org/abs/1809.05557v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05557v1)
- **Published**: 2018-09-14 18:48:49+00:00
- **Updated**: 2018-09-14 18:48:49+00:00
- **Authors**: Hongming Li, Xiaofeng Zhu, Yong Fan
- **Comment**: Accepted by MICCAI 2018
- **Journal**: None
- **Summary**: We present a deep semi-nonnegative matrix factorization method for identifying subject-specific functional networks (FNs) at multiple spatial scales with a hierarchical organization from resting state fMRI data. Our method is built upon a deep semi-nonnegative matrix factorization framework to jointly detect the FNs at multiple scales with a hierarchical organization, enhanced by group sparsity regularization that helps identify subject-specific FNs without loss of inter-subject comparability. The proposed method has been validated for predicting subject-specific functional activations based on functional connectivity measures of the hierarchical multi-scale FNs of the same subjects. Experimental results have demonstrated that our method could obtain subject-specific multi-scale hierarchical FNs and their functional connectivity measures across different scales could better predict subject-specific functional activations than those obtained by alternative techniques.



### Visual Diagnostics for Deep Reinforcement Learning Policy Development
- **Arxiv ID**: http://arxiv.org/abs/1809.06781v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.06781v2)
- **Published**: 2018-09-14 18:59:12+00:00
- **Updated**: 2018-09-26 22:21:09+00:00
- **Authors**: Jieliang Luo, Sam Green, Peter Feghali, George Legrady, Çetin Kaya Koç
- **Comment**: 4 pages, 5 figures
- **Journal**: None
- **Summary**: Modern vision-based reinforcement learning techniques often use convolutional neural networks (CNN) as universal function approximators to choose which action to take for a given visual input. Until recently, CNNs have been treated like black-box functions, but this mindset is especially dangerous when used for control in safety-critical settings. In this paper, we present our extensions of CNN visualization algorithms to the domain of vision-based reinforcement learning. We use a simulated drone environment as an example scenario. These visualization algorithms are an important tool for behavior introspection and provide insight into the qualities and flaws of trained policies when interacting with the physical world. A video may be seen at https://sites.google.com/view/drlvisual .



### Identification of temporal transition of functional states using recurrent neural networks from functional MRI
- **Arxiv ID**: http://arxiv.org/abs/1809.05560v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1809.05560v1)
- **Published**: 2018-09-14 18:59:32+00:00
- **Updated**: 2018-09-14 18:59:32+00:00
- **Authors**: Hongming Li, Yong Fan
- **Comment**: Accepted by MICCAI 2018
- **Journal**: None
- **Summary**: Dynamic functional connectivity analysis provides valuable information for understanding brain functional activity underlying different cognitive processes. Besides sliding window based approaches, a variety of methods have been developed to automatically split the entire functional MRI scan into segments by detecting change points of functional signals to facilitate better characterization of temporally dynamic functional connectivity patterns. However, these methods are based on certain assumptions for the functional signals, such as Gaussian distribution, which are not necessarily suitable for the fMRI data. In this study, we develop a deep learning based framework for adaptively detecting temporally dynamic functional state transitions in a data-driven way without any explicit modeling assumptions, by leveraging recent advances in recurrent neural networks (RNNs) for sequence modeling. Particularly, we solve this problem in an anomaly detection framework with an assumption that the functional profile of one single time point could be reliably predicted based on its preceding profiles within stable functional state, while large prediction errors would occur around change points of functional states. We evaluate the proposed method using both task and resting-state fMRI data obtained from the human connectome project and experimental results have demonstrated that the proposed change point detection method could effectively identify change points between different task events and split the resting-state fMRI into segments with distinct functional connectivity patterns.



### Brain decoding from functional MRI using long short-term memory recurrent neural networks
- **Arxiv ID**: http://arxiv.org/abs/1809.05561v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05561v1)
- **Published**: 2018-09-14 19:06:58+00:00
- **Updated**: 2018-09-14 19:06:58+00:00
- **Authors**: Hongming Li, Yong Fan
- **Comment**: Accepted by MICCAI 2018
- **Journal**: None
- **Summary**: Decoding brain functional states underlying different cognitive processes using multivariate pattern recognition techniques has attracted increasing interests in brain imaging studies. Promising performance has been achieved using brain functional connectivity or brain activation signatures for a variety of brain decoding tasks. However, most of existing studies have built decoding models upon features extracted from imaging data at individual time points or temporal windows with a fixed interval, which might not be optimal across different cognitive processes due to varying temporal durations and dependency of different cognitive processes. In this study, we develop a deep learning based framework for brain decoding by leveraging recent advances in sequence modeling using long short-term memory (LSTM) recurrent neural networks (RNNs). Particularly, functional profiles extracted from task functional imaging data based on their corresponding subject-specific intrinsic functional networks are used as features to build brain decoding models, and LSTM RNNs are adopted to learn decoding mappings between functional profiles and brain states. We evaluate the proposed method using task fMRI data from the HCP dataset, and experimental results have demonstrated that the proposed method could effectively distinguish brain states under different task events and obtain higher accuracy than conventional decoding models.



### Models Matter, So Does Training: An Empirical Study of CNNs for Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1809.05571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05571v1)
- **Published**: 2018-09-14 20:27:49+00:00
- **Updated**: 2018-09-14 20:27:49+00:00
- **Authors**: Deqing Sun, Xiaodong Yang, Ming-Yu Liu, Jan Kautz
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate two crucial and closely related aspects of CNNs for optical flow estimation: models and training. First, we design a compact but effective CNN model, called PWC-Net, according to simple and well-established principles: pyramidal processing, warping, and cost volume processing. PWC-Net is 17 times smaller in size, 2 times faster in inference, and 11\% more accurate on Sintel final than the recent FlowNet2 model. It is the winning entry in the optical flow competition of the robust vision challenge. Next, we experimentally analyze the sources of our performance gains. In particular, we use the same training procedure of PWC-Net to retrain FlowNetC, a sub-network of FlowNet2. The retrained FlowNetC is 56\% more accurate on Sintel final than the previously trained one and even 5\% more accurate than the FlowNet2 model. We further improve the training procedure and increase the accuracy of PWC-Net on Sintel by 10\% and on KITTI 2012 and 2015 by 20\%. Our newly trained model parameters and training protocols will be available on https://github.com/NVlabs/PWC-Net



### A study on the use of Boundary Equilibrium GAN for Approximate Frontalization of Unconstrained Faces to aid in Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1809.05611v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05611v1)
- **Published**: 2018-09-14 23:02:03+00:00
- **Updated**: 2018-09-14 23:02:03+00:00
- **Authors**: Wazeer Zulfikar, Sebastin Santy, Sahith Dambekodi, Tirtharaj Dash
- **Comment**: None
- **Journal**: None
- **Summary**: Face frontalization is the process of synthesizing frontal facing views of faces given its angled poses. We implement a generative adversarial network (GAN) with spherical linear interpolation (Slerp) for frontalization of unconstrained facial images. Our special focus is intended towards the generation of approximate frontal faces of the side posed images captured from surveillance cameras. Specifically, the present work is a comprehensive study on the implementation of an auto-encoder based Boundary Equilibrium GAN (BEGAN) to generate frontal faces using an interpolation of a side view face and its mirrored view. To increase the quality of the interpolated output we implement a BEGAN with Slerp. This approach could produce a promising output along with a faster and more stable training for the model. The BEGAN model additionally has a balanced generator-discriminator combination, which prevents mode collapse along with a global convergence measure. It is expected that such an approximate face generation model would be able to replace face composites used in surveillance and crime detection.



