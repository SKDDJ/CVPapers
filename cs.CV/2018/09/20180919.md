# Arxiv Papers in cs.CV on 2018-09-19
### Wearable-based Mediation State Detection in Individuals with Parkinson's Disease
- **Arxiv ID**: http://arxiv.org/abs/1809.06973v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1809.06973v1)
- **Published**: 2018-09-19 01:01:37+00:00
- **Updated**: 2018-09-19 01:01:37+00:00
- **Authors**: Murtadha D. Hssayeni, Michelle A. Burack, M. D., Joohi Jimenez-Shahed, M. D., Behnaz Ghoraani, Ph. D
- **Comment**: None
- **Journal**: None
- **Summary**: One of the most prevalent complaints of individuals with mid-stage and advanced Parkinson's disease (PD) is the fluctuating response to their medication (i.e., ON state with maximum benefit from medication and OFF state with no benefit from medication). In order to address these motor fluctuations, the patients go through periodic clinical examination where the treating physician reviews the patients' self-report about duration in different medication states and optimize therapy accordingly. Unfortunately, the patients' self-report can be unreliable and suffer from recall bias. There is a need to a technology-based system that can provide objective measures about the duration in different medication states that can be used by the treating physician to successfully adjust the therapy. In this paper, we developed a medication state detection algorithm to detect medication states using two wearable motion sensors. A series of significant features are extracted from the motion data and used in a classifier that is based on a support vector machine with fuzzy labeling. The developed algorithm is evaluated using a dataset with 19 PD subjects and a total duration of 1,052.24 minutes (17.54 hours). The algorithm resulted in an average classification accuracy of 90.5%, sensitivity of 94.2%, and specificity of 85.4%.



### Deep-learning models improve on community-level diagnosis for common congenital heart disease lesions
- **Arxiv ID**: http://arxiv.org/abs/1809.06993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06993v1)
- **Published**: 2018-09-19 03:16:26+00:00
- **Updated**: 2018-09-19 03:16:26+00:00
- **Authors**: Rima Arnaout, Lara Curran, Erin Chinn, Yili Zhao, Anita Moon-Grady
- **Comment**: rima.arnaout@ucsf.edu
- **Journal**: None
- **Summary**: Prenatal diagnosis of tetralogy of Fallot (TOF) and hypoplastic left heart syndrome (HLHS), two serious congenital heart defects, improves outcomes and can in some cases facilitate in utero interventions. In practice, however, the fetal diagnosis rate for these lesions is only 30-50 percent in community settings. Improving fetal diagnosis of congenital heart disease is therefore critical. Deep learning is a cutting-edge machine learning technique for finding patterns in images but has not yet been applied to prenatal diagnosis of congenital heart disease. Using 685 retrospectively collected echocardiograms from fetuses 18-24 weeks of gestational age from 2000-2018, we trained convolutional and fully-convolutional deep learning models in a supervised manner to (i) identify the five canonical screening views of the fetal heart and (ii) segment cardiac structures to calculate fetal cardiac biometrics. We then trained models to distinguish by view between normal hearts, TOF, and HLHS. In a holdout test set of images, F-score for identification of the five most important fetal cardiac views was 0.95. Binary classification of unannotated cardiac views of normal heart vs. TOF reached an overall sensitivity of 75% and a specificity of 76%, while normal vs. HLHS reached a sensitivity of 100% and specificity of 90%, both well above average diagnostic rates for these lesions. Furthermore, segmentation-based measurements for cardiothoracic ratio (CTR), cardiac axis (CA), and ventricular fractional area change (FAC) were compatible with clinically measured metrics for normal, TOF, and HLHS hearts. Thus, using guideline-recommended imaging, deep learning models can significantly improve detection of fetal congenital heart disease compared to the common standard of care.



### Generating 3D Adversarial Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1809.07016v4
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.07016v4)
- **Published**: 2018-09-19 05:01:06+00:00
- **Updated**: 2019-07-12 12:35:10+00:00
- **Authors**: Chong Xiang, Charles R. Qi, Bo Li
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Deep neural networks are known to be vulnerable to adversarial examples which are carefully crafted instances to cause the models to make wrong predictions. While adversarial examples for 2D images and CNNs have been extensively studied, less attention has been paid to 3D data such as point clouds. Given many safety-critical 3D applications such as autonomous driving, it is important to study how adversarial point clouds could affect current deep 3D models. In this work, we propose several novel algorithms to craft adversarial point clouds against PointNet, a widely used deep neural network for point cloud processing. Our algorithms work in two ways: adversarial point perturbation and adversarial point generation. For point perturbation, we shift existing points negligibly. For point generation, we generate either a set of independent and scattered points or a small number (1-3) of point clusters with meaningful shapes such as balls and airplanes which could be hidden in the human psyche. In addition, we formulate six perturbation measurement metrics tailored to the attacks in point clouds and conduct extensive experiments to evaluate the proposed algorithms on the ModelNet40 3D shape classification dataset. Overall, our attack algorithms achieve a success rate higher than 99% for all targeted attacks



### Exploring Visual Relationship for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1809.07041v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07041v1)
- **Published**: 2018-09-19 07:50:17+00:00
- **Updated**: 2018-09-19 07:50:17+00:00
- **Authors**: Ting Yao, Yingwei Pan, Yehao Li, Tao Mei
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: It is always well believed that modeling relationships between objects would be helpful for representing and eventually describing an image. Nevertheless, there has not been evidence in support of the idea on image description generation. In this paper, we introduce a new design to explore the connections between objects for image captioning under the umbrella of attention-based encoder-decoder framework. Specifically, we present Graph Convolutional Networks plus Long Short-Term Memory (dubbed as GCN-LSTM) architecture that novelly integrates both semantic and spatial object relationships into image encoder. Technically, we build graphs over the detected objects in an image based on their spatial and semantic connections. The representations of each region proposed on objects are then refined by leveraging graph structure through GCN. With the learnt region-level features, our GCN-LSTM capitalizes on LSTM-based captioning framework with attention mechanism for sentence generation. Extensive experiments are conducted on COCO image captioning dataset, and superior results are reported when comparing to state-of-the-art approaches. More remarkably, GCN-LSTM increases CIDEr-D performance from 120.1% to 128.7% on COCO testing set.



### New approach for solar tracking systems based on computer vision, low cost hardware and deep learning
- **Arxiv ID**: http://arxiv.org/abs/1809.07048v1
- **DOI**: 10.1016/j.renene.2018.08.101
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1809.07048v1)
- **Published**: 2018-09-19 08:09:04+00:00
- **Updated**: 2018-09-19 08:09:04+00:00
- **Authors**: Jose A. Carballo, Javier Bonilla, Manuel Berenguel, Jesús Fernández-Reche, Ginés García
- **Comment**: 12 pages, 10 figures,
- **Journal**: Carballo, J. A., Bonilla, J., Berenguel, M., Fernandez-Reche, J.,
  & Garcia, G. (2018). New approach for solar tracking systems based on
  computer vision, low cost hardware and deep learning. Renewable Energy
- **Summary**: In this work, a new approach for Sun tracking systems is presented. Due to the current system limitations regarding costs and operational problems, a new approach based on low cost, computer vision open hardware and deep learning has been developed. The preliminary tests carried out successfully in Plataforma solar de Almeria (PSA), reveal the great potential and show the new approach as a good alternative to traditional systems. The proposed approach can provide key variables for the Sun tracking system control like cloud movements prediction, block and shadow detection, atmospheric attenuation or measures of concentrated solar radiation, which can improve the control strategies of the system and therefore the system performance.



### Multi-Scale Fully Convolutional Network for Cardiac Left Ventricle Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1809.10203v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.10203v1)
- **Published**: 2018-09-19 08:13:05+00:00
- **Updated**: 2018-09-19 08:13:05+00:00
- **Authors**: Han Kang, Defeng Chen
- **Comment**: 7 pages, 9 figures
- **Journal**: None
- **Summary**: The morphological structure of left ventricle segmented from cardiac magnetic resonance images can be used to calculate key clinical parameters, and it is of great significance to the accurate and efficient diagnosis of cardiovascular diseases. Compared with traditional methods, the segmentation algorithms based on fully convolutional neural network greatly improve the accuracy of semantic segmentation. For the problem of left ventricular segmentation, a new fully convolutional neural network structure named MS-FCN is proposed in this paper. The MS-FCN network employs a multi-scale pooling module to ensure that the network maximises the feature extraction ability and uses a dense connectivity decoder to refine the boundaries of the object. Based on the Sunnybrook cine-MR dataset provided by the MICCAI 2009 challenge, numerical experiments demonstrate that our proposed model has obtained state-of-the-art segmentation results: the Dice score of our method reaches 0.93 on the endocardium, and 0.96 on the epicardium.



### Faster Training of Mask R-CNN by Focusing on Instance Boundaries
- **Arxiv ID**: http://arxiv.org/abs/1809.07069v4
- **DOI**: 10.1016/j.cviu.2019.102795
- **Categories**: **cs.CV**, cs.LG, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1809.07069v4)
- **Published**: 2018-09-19 08:54:18+00:00
- **Updated**: 2019-08-10 08:55:37+00:00
- **Authors**: Roland S. Zimmermann, Julien N. Siems
- **Comment**: 9 pages, 7 figures, 5 tables
- **Journal**: None
- **Summary**: We present an auxiliary task to Mask R-CNN, an instance segmentation network, which leads to faster training of the mask head. Our addition to Mask R-CNN is a new prediction head, the Edge Agreement Head, which is inspired by the way human annotators perform instance segmentation. Human annotators copy the contour of an object instance and only indirectly the occupied instance area. Hence, the edges of instance masks are particularly useful as they characterize the instance well. The Edge Agreement Head therefore encourages predicted masks to have similar image gradients to the ground-truth mask using edge detection filters. We provide a detailed survey of loss combinations and show improvements on the MS COCO Mask metrics compared to using no additional loss. Our approach marginally increases the model size and adds no additional trainable model variables. While the computational costs are increased slightly, the increment is negligible considering the high computational cost of the Mask R-CNN architecture. As the additional network head is only relevant during training, inference speed remains unchanged compared to Mask R-CNN. In a default Mask R-CNN setup, we achieve a training speed-up and a relative overall improvement of 8.1% on the MS COCO metrics compared to the baseline.



### Detect, anticipate and generate: Semi-supervised recurrent latent variable models for human activity modeling
- **Arxiv ID**: http://arxiv.org/abs/1809.07075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07075v1)
- **Published**: 2018-09-19 09:04:21+00:00
- **Updated**: 2018-09-19 09:04:21+00:00
- **Authors**: Judith Bütepage, Danica Kragic
- **Comment**: This paper has been accepted at the IROS 2018 workshop "Human-Robot
  Cooperation and Collaboration in Manipulation: Advancements and Challenges"
- **Journal**: None
- **Summary**: Successful Human-Robot collaboration requires a predictive model of human behavior. The robot needs to be able to recognize current goals and actions and to predict future activities in a given context. However, the spatio-temporal sequence of human actions is difficult to model since latent factors such as intention, task, knowledge, intuition and preference determine the action choices of each individual. In this work we introduce semi-supervised variational recurrent neural networks which are able to a) model temporal distributions over latent factors and the observable feature space, b) incorporate discrete labels such as activity type when available, and c) generate possible future action sequences on both feature and label level. We evaluate our model on the Cornell Activity Dataset CAD-120 dataset. Our model outperforms state-of-the-art approaches in both activity and affordance detection and anticipation. Additionally, we show how samples of possible future action sequences are in line with past observations.



### The Aqualoc Dataset: Towards Real-Time Underwater Localization from a Visual-Inertial-Pressure Acquisition System
- **Arxiv ID**: http://arxiv.org/abs/1809.07076v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.07076v1)
- **Published**: 2018-09-19 09:04:22+00:00
- **Updated**: 2018-09-19 09:04:22+00:00
- **Authors**: Maxime Ferrera, Julien Moras, Pauline Trouvé-Peloux, Vincent Creuze, Denis Dégez
- **Comment**: None
- **Journal**: IROS Workshop - New Horizons for Underwater Intervention Missions:
  from Current Technologies to Future Applications, Oct 2018, Madrid, Spain
- **Summary**: This paper presents a new underwater dataset acquired from a visual-inertial-pressure acquisition system and meant to be used to benchmark visual odometry, visual SLAM and multi-sensors SLAM solutions. The dataset is publicly available and contains ground-truth trajectories for evaluation.



### Deep Learning Based Rib Centerline Extraction and Labeling
- **Arxiv ID**: http://arxiv.org/abs/1809.07082v2
- **DOI**: 10.1007/978-3-030-11166-3_9
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.07082v2)
- **Published**: 2018-09-19 09:09:23+00:00
- **Updated**: 2019-01-14 20:56:28+00:00
- **Authors**: Matthias Lenga, Tobias Klinder, Christian Bürger, Jens von Berg, Astrid Franz, Cristian Lorenz
- **Comment**: This paper was accepted for presentation at the MICCAI MSKI 2018
  Workshop
- **Journal**: None
- **Summary**: Automated extraction and labeling of rib centerlines is a typically needed prerequisite for more advanced assisted reading tools that help the radiologist to efficiently inspect all 24 ribs in a CT volume. In this paper, we combine a deep learning-based rib detection with a dedicated centerline extraction algorithm applied to the detection result for the purpose of fast, robust and accurate rib centerline extraction and labeling from CT volumes. More specifically, we first apply a fully convolutional neural network (FCNN) to generate a probability map for detecting the first rib pair, the twelfth rib pair, and the collection of all intermediate ribs. In a second stage, a newly designed centerline extraction algorithm is applied to this multi-label probability map. Finally, the distinct detection of first and twelfth rib separately, allows to derive individual rib labels by simple sorting and counting the detected centerlines. We applied our method to CT volumes from 116 patients which included a variety of different challenges and achieved a centerline accuracy of 0.787 mm with respect to manual centerline annotations.   This article is a preprint version of: Lenga M., Klinder T., B\"urger C., von Berg J., Franz A., Lorenz C. (2019) Deep Learning Based Rib Centerline Extraction and Labeling. In: Vrtovec T., Yao J., Zheng G., Pozo J. (eds) Computational Methods and Clinical Applications in Musculoskeletal Imaging. MSKI 2018. Lecture Notes in Computer Science, vol 11404. Springer, Cham



### Counting the uncountable: deep semantic density estimation from Space
- **Arxiv ID**: http://arxiv.org/abs/1809.07091v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.07091v2)
- **Published**: 2018-09-19 09:29:35+00:00
- **Updated**: 2018-09-20 07:15:02+00:00
- **Authors**: Andres C. Rodriguez, Jan D. Wegner
- **Comment**: Accepted in GCPR 2018
- **Journal**: None
- **Summary**: We propose a new method to count objects of specific categories that are significantly smaller than the ground sampling distance of a satellite image. This task is hard due to the cluttered nature of scenes where different object categories occur. Target objects can be partially occluded, vary in appearance within the same class and look alike to different categories. Since traditional object detection is infeasible due to the small size of objects with respect to the pixel size, we cast object counting as a density estimation problem. To distinguish objects of different classes, our approach combines density estimation with semantic segmentation in an end-to-end learnable convolutional neural network (CNN). Experiments show that deep semantic density estimation can robustly count objects of various classes in cluttered scenes. Experiments also suggest that we need specific CNN architectures in remote sensing instead of blindly applying existing ones from computer vision.



### Dual Reconstruction Nets for Image Super-Resolution with Gradient Sensitive Loss
- **Arxiv ID**: http://arxiv.org/abs/1809.07099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07099v1)
- **Published**: 2018-09-19 09:39:41+00:00
- **Updated**: 2018-09-19 09:39:41+00:00
- **Authors**: Yong Guo, Qi Chen, Jian Chen, Junzhou Huang, Yanwu Xu, Jiezhang Cao, Peilin Zhao, Mingkui Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have exhibited promising performance in image super-resolution (SR) due to the power in learning the non-linear mapping from low-resolution (LR) images to high-resolution (HR) images. However, most deep learning methods employ feed-forward architectures, and thus the dependencies between LR and HR images are not fully exploited, leading to limited learning performance. Moreover, most deep learning based SR methods apply the pixel-wise reconstruction error as the loss, which, however, may fail to capture high-frequency information and produce perceptually unsatisfying results, whilst the recent perceptual loss relies on some pre-trained deep model and they may not generalize well. In this paper, we introduce a mask to separate the image into low- and high-frequency parts based on image gradient magnitude, and then devise a gradient sensitive loss to well capture the structures in the image without sacrificing the recovery of low-frequency content. Moreover, by investigating the duality in SR, we develop a dual reconstruction network (DRN) to improve the SR performance. We provide theoretical analysis on the generalization performance of our method and demonstrate its effectiveness and superiority with thorough experiments.



### Characterising Across-Stack Optimisations for Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.07196v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1809.07196v1)
- **Published**: 2018-09-19 13:52:49+00:00
- **Updated**: 2018-09-19 13:52:49+00:00
- **Authors**: Jack Turner, José Cano, Valentin Radu, Elliot J. Crowley, Michael O'Boyle, Amos Storkey
- **Comment**: IISWC 2018
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are extremely computationally demanding, presenting a large barrier to their deployment on resource-constrained devices. Since such systems are where some of their most useful applications lie (e.g. obstacle detection for mobile robots, vision-based medical assistive technology), significant bodies of work from both machine learning and systems communities have attempted to provide optimisations that will make CNNs available to edge devices. In this paper we unify the two viewpoints in a Deep Learning Inference Stack and take an across-stack approach by implementing and evaluating the most common neural network compression techniques (weight pruning, channel pruning, and quantisation) and optimising their parallel execution with a range of programming approaches (OpenMP, OpenCL) and hardware architectures (CPU, GPU). We provide comprehensive Pareto curves to instruct trade-offs under constraints of accuracy, execution time, and memory space.



### 3D Human Pose Estimation with Siamese Equivariant Embedding
- **Arxiv ID**: http://arxiv.org/abs/1809.07217v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07217v2)
- **Published**: 2018-09-19 14:26:14+00:00
- **Updated**: 2019-02-16 16:33:41+00:00
- **Authors**: Márton Véges, Viktor Varga, András Lőrincz
- **Comment**: Accepted to Neurocomputing
- **Journal**: None
- **Summary**: In monocular 3D human pose estimation a common setup is to first detect 2D positions and then lift the detection into 3D coordinates. Many algorithms suffer from overfitting to camera positions in the training set. We propose a siamese architecture that learns a rotation equivariant hidden representation to reduce the need for data augmentation. Our method is evaluated on multiple databases with different base networks and shows a consistent improvement of error metrics. It achieves state-of-the-art cross-camera error rate among algorithms that use estimated 2D joint coordinates only.



### Pose Estimation for Non-Cooperative Spacecraft Rendezvous Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.07238v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07238v1)
- **Published**: 2018-09-19 15:19:45+00:00
- **Updated**: 2018-09-19 15:19:45+00:00
- **Authors**: Sumant Sharma, Connor Beierle, Simone D'Amico
- **Comment**: Presented at the 2018 IEEE Aerospace Conference, Big Sky, MT
- **Journal**: None
- **Summary**: On-board estimation of the pose of an uncooperative target spacecraft is an essential task for future on-orbit servicing and close-proximity formation flying missions. However, two issues hinder reliable on-board monocular vision based pose estimation: robustness to illumination conditions due to a lack of reliable visual features and scarcity of image datasets required for training and benchmarking. To address these two issues, this work details the design and validation of a monocular vision based pose determination architecture for spaceborne applications. The primary contribution to the state-of-the-art of this work is the introduction of a novel pose determination method based on Convolutional Neural Networks (CNN) to provide an initial guess of the pose in real-time on-board. The method involves discretizing the pose space and training the CNN with images corresponding to the resulting pose labels. Since reliable training of the CNN requires massive image datasets and computational resources, the parameters of the CNN must be determined prior to the mission with synthetic imagery. Moreover, reliable training of the CNN requires datasets that appropriately account for noise, color, and illumination characteristics expected in orbit. Therefore, the secondary contribution of this work is the introduction of an image synthesis pipeline, which is tailored to generate high fidelity images of any spacecraft 3D model. The proposed technique is scalable to spacecraft of different structural and physical properties as well as robust to the dynamic illumination conditions of space. Through metrics measuring classification and pose accuracy, it is shown that the presented architecture has desirable robustness and scalable properties.



### MTLE: A Multitask Learning Encoder of Visual Feature Representations for Video and Movie Description
- **Arxiv ID**: http://arxiv.org/abs/1809.07257v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.07257v1)
- **Published**: 2018-09-19 15:50:18+00:00
- **Updated**: 2018-09-19 15:50:18+00:00
- **Authors**: Oliver Nina, Washington Garcia, Scott Clouse, Alper Yilmaz
- **Comment**: This is a pre-print version of our soon to be released paper
- **Journal**: None
- **Summary**: Learning visual feature representations for video analysis is a daunting task that requires a large amount of training samples and a proper generalization framework. Many of the current state of the art methods for video captioning and movie description rely on simple encoding mechanisms through recurrent neural networks to encode temporal visual information extracted from video data. In this paper, we introduce a novel multitask encoder-decoder framework for automatic semantic description and captioning of video sequences. In contrast to current approaches, our method relies on distinct decoders that train a visual encoder in a multitask fashion. Our system does not depend solely on multiple labels and allows for a lack of training data working even with datasets where only one single annotation is viable per video. Our method shows improved performance over current state of the art methods in several metrics on multi-caption and single-caption datasets. To the best of our knowledge, our method is the first method to use a multitask approach for encoding video features. Our method demonstrates its robustness on the Large Scale Movie Description Challenge (LSMDC) 2017 where our method won the movie description task and its results were ranked among other competitors as the most helpful for the visually impaired.



### Generative Adversarial Network in Medical Imaging: A Review
- **Arxiv ID**: http://arxiv.org/abs/1809.07294v4
- **DOI**: 10.1016/j.media.2019.101552
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.07294v4)
- **Published**: 2018-09-19 16:44:36+00:00
- **Updated**: 2019-09-04 01:01:58+00:00
- **Authors**: Xin Yi, Ekta Walia, Paul Babyn
- **Comment**: 24 pages; v4; added missing references from before Jan 1st 2019;
  accepted to MedIA
- **Journal**: None
- **Summary**: Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique.



### Towards Large-Scale Video Video Object Mining
- **Arxiv ID**: http://arxiv.org/abs/1809.07316v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07316v1)
- **Published**: 2018-09-19 17:49:35+00:00
- **Updated**: 2018-09-19 17:49:35+00:00
- **Authors**: Aljosa Osep, Paul Voigtlaender, Jonathon Luiten, Stefan Breuers, Bastian Leibe
- **Comment**: 4 pages, 3 figures, 1 table. ECCV 2018 Workshop on Interactive and
  Adaptive Learning in an Open World
- **Journal**: None
- **Summary**: We propose to leverage a generic object tracker in order to perform object mining in large-scale unlabeled videos, captured in a realistic automotive setting. We present a dataset of more than 360'000 automatically mined object tracks from 10+ hours of video data (560'000 frames) and propose a method for automated novel category discovery and detector learning. In addition, we show preliminary results on using the mined tracks for object detector adaptation.



### Combined Image- and World-Space Tracking in Traffic Scenes
- **Arxiv ID**: http://arxiv.org/abs/1809.07357v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07357v1)
- **Published**: 2018-09-19 18:16:42+00:00
- **Updated**: 2018-09-19 18:16:42+00:00
- **Authors**: Aljosa Osep, Wolfgang Mehner, Markus Mathias, Bastian Leibe
- **Comment**: 8 pages, 7 figures, 2 tables. ICRA 2017 paper
- **Journal**: None
- **Summary**: Tracking in urban street scenes plays a central role in autonomous systems such as self-driving cars. Most of the current vision-based tracking methods perform tracking in the image domain. Other approaches, eg based on LIDAR and radar, track purely in 3D. While some vision-based tracking methods invoke 3D information in parts of their pipeline, and some 3D-based methods utilize image-based information in components of their approach, we propose to use image- and world-space information jointly throughout our method. We present our tracking pipeline as a 3D extension of image-based tracking. From enhancing the detections with 3D measurements to the reported positions of every tracked object, we use world-space 3D information at every stage of processing. We accomplish this by our novel coupled 2D-3D Kalman filter, combined with a conceptually clean and extendable hypothesize-and-select framework. Our approach matches the current state-of-the-art on the official KITTI benchmark, which performs evaluation in the 2D image domain only. Further experiments show significant improvements in 3D localization precision by enabling our coupled 2D-3D tracking.



### Visual Diver Recognition for Underwater Human-Robot Collaboration
- **Arxiv ID**: http://arxiv.org/abs/1809.10201v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1809.10201v1)
- **Published**: 2018-09-19 20:21:46+00:00
- **Updated**: 2018-09-19 20:21:46+00:00
- **Authors**: Youya Xia, Junaed Sattar
- **Comment**: submitted for ICRA 2019
- **Journal**: None
- **Summary**: This paper presents an approach for autonomous underwater robots to visually detect and identify divers. The proposed approach enables an autonomous underwater robot to detect multiple divers in a visual scene and distinguish between them. Such methods are useful for robots to identify a human leader, for example, in multi-human/robot teams where only designated individuals are allowed to command or lean a team of robots. Initial diver identification is performed using the Faster R-CNN algorithm with a region proposal network which produces bounding boxes around the divers' locations. Subsequently, a suite of spatial and frequency domain descriptors are extracted from the bounding boxes to create a feature vector. A K-Means clustering algorithm, with k set to the number of detected bounding boxes, thereafter identifies the detected divers based on these feature vectors. We evaluate the performance of the proposed approach on video footage of divers swimming in front of a mobile robot and demonstrate its accuracy.



### Nonisometric Surface Registration via Conformal Laplace-Beltrami Basis Pursuit
- **Arxiv ID**: http://arxiv.org/abs/1809.07399v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, 65K10, 58J50, 68U05
- **Links**: [PDF](http://arxiv.org/pdf/1809.07399v1)
- **Published**: 2018-09-19 20:30:24+00:00
- **Updated**: 2018-09-19 20:30:24+00:00
- **Authors**: Stefan C. Schonsheck, Michael M. Bronstein, Rongjie Lai
- **Comment**: 21 pages, 7 figures
- **Journal**: None
- **Summary**: Surface registration is one of the most fundamental problems in geometry processing. Many approaches have been developed to tackle this problem in cases where the surfaces are nearly isometric. However, it is much more challenging to compute correspondence between surfaces which are intrinsically less similar. In this paper, we propose a variational model to align the Laplace-Beltrami (LB) eigensytems of two non-isometric genus zero shapes via conformal deformations. This method enables us compute to geometric meaningful point-to-point maps between non-isometric shapes. Our model is based on a novel basis pursuit scheme whereby we simultaneously compute a conformal deformation of a 'target shape' and its deformed LB eigensytem. We solve the model using an proximal alternating minimization algorithm hybridized with the augmented Lagrangian method which produces accurate correspondences given only a few landmark points. We also propose a reinitialization scheme to overcome some of the difficulties caused by the non-convexity of the variational problem. Intensive numerical experiments illustrate the effectiveness and robustness of the proposed method to handle non-isometric surfaces with large deformation with respect to both noise on the underlying manifolds and errors within the given landmarks.



### Distances for WiFi Based Topological Indoor Mapping
- **Arxiv ID**: http://arxiv.org/abs/1809.07405v1
- **DOI**: 10.1145/3360774.3360780
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.07405v1)
- **Published**: 2018-09-19 20:45:59+00:00
- **Updated**: 2018-09-19 20:45:59+00:00
- **Authors**: Bastian Schäfermeier, Tom Hanika, Gerd Stumme
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: For localization and mapping of indoor environments through WiFi signals, locations are often represented as likelihoods of the received signal strength indicator. In this work we compare various measures of distance between such likelihoods in combination with different methods for estimation and representation. In particular, we show that among the considered distance measures the Earth Mover's Distance seems the most beneficial for the localization task. Combined with kernel density estimation we were able to retain the topological structure of rooms in a real-world office scenario.



### DSVO: Direct Stereo Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/1810.03963v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1810.03963v2)
- **Published**: 2018-09-19 20:56:57+00:00
- **Updated**: 2019-09-16 14:52:24+00:00
- **Authors**: Jiawei Mo, Junaed Sattar
- **Comment**: Rewritten to "Extending Monocular Visual Odometry to Stereo Camera
  Systems by Scale Optimization" arXiv:1905.12723
- **Journal**: None
- **Summary**: This paper proposes a novel approach to stereo visual odometry without stereo matching. It is particularly robust in scenes of repetitive high-frequency textures. Referred to as DSVO (Direct Stereo Visual Odometry), it operates directly on pixel intensities, without any explicit feature matching, and is thus efficient and more accurate than the state-of-the-art stereo-matching-based methods. It applies a semi-direct monocular visual odometry running on one camera of the stereo pair, tracking the camera pose and mapping the environment simultaneously; the other camera is used to optimize the scale of monocular visual odometry. We evaluate DSVO in a number of challenging scenes to evaluate its performance and present comparisons with the state-of-the-art stereo visual odometry algorithms.



### Egocentric Vision-based Future Vehicle Localization for Intelligent Driving Assistance Systems
- **Arxiv ID**: http://arxiv.org/abs/1809.07408v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1809.07408v2)
- **Published**: 2018-09-19 20:58:04+00:00
- **Updated**: 2019-03-03 15:24:57+00:00
- **Authors**: Yu Yao, Mingze Xu, Chiho Choi, David J. Crandall, Ella M. Atkins, Behzad Dariush
- **Comment**: To appear on ICRA 2019
- **Journal**: None
- **Summary**: Predicting the future location of vehicles is essential for safety-critical applications such as advanced driver assistance systems (ADAS) and autonomous driving. This paper introduces a novel approach to simultaneously predict both the location and scale of target vehicles in the first-person (egocentric) view of an ego-vehicle. We present a multi-stream recurrent neural network (RNN) encoder-decoder model that separately captures both object location and scale and pixel-level observations for future vehicle localization. We show that incorporating dense optical flow improves prediction results significantly since it captures information about motion as well as appearance change. We also find that explicitly modeling future motion of the ego-vehicle improves the prediction accuracy, which could be especially beneficial in intelligent and automated vehicles that have motion planning capability. To evaluate the performance of our approach, we present a new dataset of first-person videos collected from a variety of scenarios at road intersections, which are particularly challenging moments for prediction because vehicle trajectories are diverse and dynamic.



### Learning to Interpret Satellite Images Using Wikipedia
- **Arxiv ID**: http://arxiv.org/abs/1809.10236v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1809.10236v1)
- **Published**: 2018-09-19 21:58:14+00:00
- **Updated**: 2018-09-19 21:58:14+00:00
- **Authors**: Evan Sheehan, Burak Uzkent, Chenlin Meng, Zhongyi Tang, Marshall Burke, David Lobell, Stefano Ermon
- **Comment**: None
- **Journal**: None
- **Summary**: Despite recent progress in computer vision, fine-grained interpretation of satellite images remains challenging because of a lack of labeled training data. To overcome this limitation, we propose using Wikipedia as a previously untapped source of rich, georeferenced textual information with global coverage. We construct a novel large-scale, multi-modal dataset by pairing geo-referenced Wikipedia articles with satellite imagery of their corresponding locations. To prove the efficacy of this dataset, we focus on the African continent and train a deep network to classify images based on labels extracted from articles. We then fine-tune the model on a human annotated dataset and demonstrate that this weak form of supervision can drastically reduce the quantity of human annotated labels and time required for downstream tasks.



### Deep Part Induction from Articulated Object Pairs
- **Arxiv ID**: http://arxiv.org/abs/1809.07417v1
- **DOI**: 10.1145/3272127.3275027
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1809.07417v1)
- **Published**: 2018-09-19 22:07:18+00:00
- **Updated**: 2018-09-19 22:07:18+00:00
- **Authors**: Li Yi, Haibin Huang, Difan Liu, Evangelos Kalogerakis, Hao Su, Leonidas Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: Object functionality is often expressed through part articulation -- as when the two rigid parts of a scissor pivot against each other to perform the cutting function. Such articulations are often similar across objects within the same functional category. In this paper, we explore how the observation of different articulation states provides evidence for part structure and motion of 3D objects. Our method takes as input a pair of unsegmented shapes representing two different articulation states of two functionally related objects, and induces their common parts along with their underlying rigid motion. This is a challenging setting, as we assume no prior shape structure, no prior shape category information, no consistent shape orientation, the articulation states may belong to objects of different geometry, plus we allow inputs to be noisy and partial scans, or point clouds lifted from RGB images. Our method learns a neural network architecture with three modules that respectively propose correspondences, estimate 3D deformation flows, and perform segmentation. To achieve optimal performance, our architecture alternates between correspondence, deformation flow, and segmentation prediction iteratively in an ICP-like fashion. Our results demonstrate that our method significantly outperforms state-of-the-art techniques in the task of discovering articulated parts of objects. In addition, our part induction is object-class agnostic and successfully generalizes to new and unseen objects.



