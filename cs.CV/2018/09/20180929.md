# Arxiv Papers in cs.CV on 2018-09-29
### Visual Object Tracking based on Adaptive Siamese and Motion Estimation Network
- **Arxiv ID**: http://arxiv.org/abs/1810.00119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00119v1)
- **Published**: 2018-09-29 00:34:36+00:00
- **Updated**: 2018-09-29 00:34:36+00:00
- **Authors**: Hossein Kashiani, Shahriar B. Shokouhi
- **Comment**: 28 pages, 1 algorithm, 7 figures, 2 table, Submitted to Elsevier,
  Image and Vision Computing
- **Journal**: None
- **Summary**: Recently, convolutional neural network (CNN) has attracted much attention in different areas of computer vision, due to its powerful abstract feature representation. Visual object tracking is one of the interesting and important areas in computer vision that achieves remarkable improvements in recent years. In this work, we aim to improve both the motion and observation models in visual object tracking by leveraging representation power of CNNs. To this end, a motion estimation network (named MEN) is utilized to seek the most likely locations of the target and prepare a further clue in addition to the previous target position. Hence the motion estimation would be enhanced by generating a small number of candidates near two plausible positions. The generated candidates are then fed into a trained Siamese network to detect the most probable candidate. Each candidate is compared to an adaptable buffer, which is updated under a predefined condition. To take into account the target appearance changes, a weighting CNN (called WCNN) adaptively assigns weights to the final similarity scores of the Siamese network using sequence-specific information. Evaluation results on well-known benchmark datasets (OTB100, OTB50 and OTB2013) prove that the proposed tracker outperforms the state-of-the-art competitors.



### Robot Vision: Calibration of Wide-Angle Lens Cameras Using Collinearity Condition and K-Nearest Neighbour Regression
- **Arxiv ID**: http://arxiv.org/abs/1810.00128v1
- **DOI**: 10.5194/isprs-archives-XLII-1-93-2018
- **Categories**: **cs.RO**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1810.00128v1)
- **Published**: 2018-09-29 01:16:27+00:00
- **Updated**: 2018-09-29 01:16:27+00:00
- **Authors**: Jacky C. K. Chow, Ivan Detchev, Kathleen Ang, Kristian Morin, Karthik Mahadevan, Nicholas Louie
- **Comment**: ISPRS TC I Mid-term Symposium "Innovative Sensing - From Sensors to
  Methods and Applications", 10-12 October 2018. Karlsruhe, Germany
- **Journal**: The International Archives of the Photogrammetry, Remote Sensing
  and Spatial Information Sciences, Volume XLII-1, 2018, pp. 93-99
- **Summary**: Visual perception is regularly used by humans and robots for navigation. By either implicitly or explicitly mapping the environment, ego-motion can be determined and a path of actions can be planned. The process of mapping and navigation are delicately intertwined; therefore, improving one can often lead to an improvement of the other. Both processes are sensitive to the interior orientation parameters of the camera system and mathematically modelling these systematic errors can often improve the precision and accuracy of the overall solution. This paper presents an automatic camera calibration method suitable for any lens, without having prior knowledge about the sensor. Statistical inference is performed to map the environment and localize the camera simultaneously. K-nearest neighbour regression is used to model the geometric distortions of the images. A normal-angle lens Nikon camera and wide-angle lens GoPro camera were calibrated using the proposed method, as well as the conventional bundle adjustment with self-calibration method (for comparison). Results showed that the mapping error was reduced from an average of 14.9 mm to 1.2 mm (i.e. a 92% improvement) and 66.6 mm to 1.5 mm (i.e. a 98% improvement) using the proposed method for the Nikon and GoPro cameras, respectively. In contrast, the conventional approach achieved an average 3D error of 0.9 mm (i.e. 94% improvement) and 3.3 mm (i.e. 95% improvement) for the Nikon and GoPro cameras, respectively. Thus, the proposed method performs well irrespective of the lens/sensor used: it yields results that are comparable to the conventional approach for normal-angle lens cameras, and it has the additional benefit of improving calibration results for wide-angle lens cameras.



### FusedLSTM: Fusing frame-level and video-level features for Content-based Video Relevance Prediction
- **Arxiv ID**: http://arxiv.org/abs/1810.00136v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00136v1)
- **Published**: 2018-09-29 02:22:42+00:00
- **Updated**: 2018-09-29 02:22:42+00:00
- **Authors**: Yash Bhalgat
- **Comment**: Submission report for the ACMMM CBVRP challenge 2018
- **Journal**: None
- **Summary**: This paper describes two of my best performing approaches on the Content-based Video Relevance Prediction challenge. In the FusedLSTM based approach, the inception-pool3 and the C3D-pool5 features are combined using an LSTM and a dense layer to form embeddings with the objective to minimize the triplet loss function. In the second approach, an Online Kernel Similarity Learning method is proposed to learn a non-linear similarity measure to adhere the relevance training data. The last section gives a complete comparison of all the approaches implemented during this challenge, including the one presented in the baseline paper.



### Modelling Errors in X-ray Fluoroscopic Imaging Systems Using Photogrammetric Bundle Adjustment With a Data-Driven Self-Calibration Approach
- **Arxiv ID**: http://arxiv.org/abs/1810.00138v2
- **DOI**: 10.5194/isprs-archives-XLII-1-101-2018
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.00138v2)
- **Published**: 2018-09-29 03:17:15+00:00
- **Updated**: 2018-10-19 06:11:05+00:00
- **Authors**: Jacky C. K. Chow, Derek Lichti, Kathleen Ang, Gregor Kuntze, Gulshan Sharma, Janet Ronsky
- **Comment**: ISPRS TC I Mid-term Symposium "Innovative Sensing - From Sensors to
  Methods and Applications", 10-12 October 2018. Karlsruhe, Germany
- **Journal**: The International Archives of the Photogrammetry, Remote Sensing
  and Spatial Information Sciences, Volume XLII-1, 2018
- **Summary**: X-ray imaging is a fundamental tool of routine clinical diagnosis. Fluoroscopic imaging can further acquire X-ray images at video frame rates, thus enabling non-invasive in-vivo motion studies of joints, gastrointestinal tract, etc. For both the qualitative and quantitative analysis of static and dynamic X-ray images, the data should be free of systematic biases. Besides precise fabrication of hardware, software-based calibration solutions are commonly used for modelling the distortions. In this primary research study, a robust photogrammetric bundle adjustment was used to model the projective geometry of two fluoroscopic X-ray imaging systems. However, instead of relying on an expert photogrammetrist's knowledge and judgement to decide on a parametric model for describing the systematic errors, a self-tuning data-driven approach is used to model the complex non-linear distortion profile of the sensors. Quality control from the experiment showed that 0.06 mm to 0.09 mm 3D reconstruction accuracy was achievable post-calibration using merely 15 X-ray images. As part of the bundle adjustment, the location of the virtual fluoroscopic system relative to the target field can also be spatially resected with an RMSE between 3.10 mm and 3.31 mm.



### NICE: Noise Injection and Clamping Estimation for Neural Network Quantization
- **Arxiv ID**: http://arxiv.org/abs/1810.00162v2
- **DOI**: 10.3390/math9172144
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.00162v2)
- **Published**: 2018-09-29 06:56:33+00:00
- **Updated**: 2018-10-02 20:07:32+00:00
- **Authors**: Chaim Baskin, Natan Liss, Yoav Chai, Evgenii Zheltonozhskii, Eli Schwartz, Raja Giryes, Avi Mendelson, Alexander M. Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) are very popular in many fields including computer vision, speech recognition, natural language processing, to name a few. Though deep learning leads to groundbreaking performance in these domains, the networks used are very demanding computationally and are far from real-time even on a GPU, which is not power efficient and therefore does not suit low power systems such as mobile devices. To overcome this challenge, some solutions have been proposed for quantizing the weights and activations of these networks, which accelerate the runtime significantly. Yet, this acceleration comes at the cost of a larger error. The \uniqname method proposed in this work trains quantized neural networks by noise injection and a learned clamping, which improve the accuracy. This leads to state-of-the-art results on various regression and classification tasks, e.g., ImageNet classification with architectures such as ResNet-18/34/50 with low as 3-bit weights and activations. We implement the proposed solution on an FPGA to demonstrate its applicability for low power real-time applications. The implementation of the paper is available at https://github.com/Lancer555/NICE



### Continual Learning of Context-dependent Processing in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1810.01256v3
- **DOI**: 10.1038/s42256-019-0080-x
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.01256v3)
- **Published**: 2018-09-29 09:45:08+00:00
- **Updated**: 2021-06-27 13:38:39+00:00
- **Authors**: Guanxiong Zeng, Yang Chen, Bo Cui, Shan Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are powerful tools in learning sophisticated but fixed mapping rules between inputs and outputs, thereby limiting their application in more complex and dynamic situations in which the mapping rules are not kept the same but changing according to different contexts. To lift such limits, we developed a novel approach involving a learning algorithm, called orthogonal weights modification (OWM), with the addition of a context-dependent processing (CDP) module. We demonstrated that with OWM to overcome the problem of catastrophic forgetting, and the CDP module to learn how to reuse a feature representation and a classifier for different contexts, a single network can acquire numerous context-dependent mapping rules in an online and continual manner, with as few as $\sim$10 samples to learn each. This should enable highly compact systems to gradually learn myriad regularities of the real world and eventually behave appropriately within it.



### Non-local NetVLAD Encoding for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1810.00207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00207v1)
- **Published**: 2018-09-29 13:07:38+00:00
- **Updated**: 2018-09-29 13:07:38+00:00
- **Authors**: Yongyi Tang, Xing Zhang, Jingwen Wang, Shaoxiang Chen, Lin Ma, Yu-Gang Jiang
- **Comment**: ECCV2018 workshop on YouTube-8M Large-Scale Video Understanding
- **Journal**: None
- **Summary**: This paper describes our solution for the 2$^\text{nd}$ YouTube-8M video understanding challenge organized by Google AI. Unlike the video recognition benchmarks, such as Kinetics and Moments, the YouTube-8M challenge provides pre-extracted visual and audio features instead of raw videos. In this challenge, the submitted model is restricted to 1GB, which encourages participants focus on constructing one powerful single model rather than incorporating of the results from a bunch of models. Our system fuses six different sub-models into one single computational graph, which are categorized into three families. More specifically, the most effective family is the model with non-local operations following the NetVLAD encoding. The other two family models are Soft-BoF and GRU, respectively. In order to further boost single models performance, the model parameters of different checkpoints are averaged. Experimental results demonstrate that our proposed system can effectively perform the video classification task, achieving 0.88763 on the public test set and 0.88704 on the private set in terms of GAP@20, respectively. We finally ranked at the fourth place in the YouTube-8M video understanding challenge.



### Parameter Estimation for the Single-Look $\mathcal{G}^0$ Distribution
- **Arxiv ID**: http://arxiv.org/abs/1810.00216v1
- **DOI**: None
- **Categories**: **stat.AP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.00216v1)
- **Published**: 2018-09-29 14:31:09+00:00
- **Updated**: 2018-09-29 14:31:09+00:00
- **Authors**: Débora Chan, Andrea Rey, Juliana Gambini, Alejandro C. Frery
- **Comment**: None
- **Journal**: None
- **Summary**: The statistical properties of Synthetic Aperture Radar (SAR) image texture reveals useful target characteristics. It is well-known that these images are affected by speckle, and prone to contamination as double bounce and corner reflectors. The $\mathcal{G}^0$ distribution is flexible enough to model different degrees of texture in speckled data. It is indexed by three parameters: $\alpha$, related to the texture, $\gamma$, a scale parameter, and $L$, the number of looks which is related to the signal-to-noise ratio. Quality estimation of $\alpha$ is essential due to its immediate interpretability. In this article, we compare the behavior of a number of parameter estimation techniques in the noisiest case, namely single look data. We evaluate them using Monte Carlo methods for non-contaminated and contaminated data, considering convergence rate, bias, mean squared error (MSE) and computational cost. The results are verified with simulated and actual SAR images.



### Deep Adversarial Training for Multi-Organ Nuclei Segmentation in Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/1810.00236v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00236v2)
- **Published**: 2018-09-29 17:06:48+00:00
- **Updated**: 2018-10-19 20:59:31+00:00
- **Authors**: Faisal Mahmood, Daniel Borders, Richard Chen, Gregory N. McKay, Kevan J. Salimian, Alexander Baras, Nicholas J. Durr
- **Comment**: None
- **Journal**: None
- **Summary**: Nuclei segmentation is a fundamental task that is critical for various computational pathology applications including nuclei morphology analysis, cell type classification, and cancer grading. Conventional vision-based methods for nuclei segmentation struggle in challenging cases and deep learning approaches have proven to be more robust and generalizable. However, CNNs require large amounts of labeled histopathology data. Moreover, conventional CNN-based approaches lack structured prediction capabilities which are required to distinguish overlapping and clumped nuclei. Here, we present an approach to nuclei segmentation that overcomes these challenges by utilizing a conditional generative adversarial network (cGAN) trained with synthetic and real data. We generate a large dataset of H&E training images with perfect nuclei segmentation labels using an unpaired GAN framework. This synthetic data along with real histopathology data from six different organs are used to train a conditional GAN with spectral normalization and gradient penalty for nuclei segmentation. This adversarial regression framework enforces higher order consistency when compared to conventional CNN models. We demonstrate that this nuclei segmentation approach generalizes across different organs, sites, patients and disease states, and outperforms conventional approaches, especially in isolating individual and overlapping nuclei.



### Pulsed Schlieren Imaging of Ultrasonic Haptics and Levitation using Phased Arrays
- **Arxiv ID**: http://arxiv.org/abs/1810.00258v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/1810.00258v1)
- **Published**: 2018-09-29 19:42:25+00:00
- **Updated**: 2018-09-29 19:42:25+00:00
- **Authors**: Michele Iodice, William Frier, James Wilcox, Ben Long, Orestis Georgiou
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: Ultrasonic acoustic fields have recently been used to generate haptic effects on the human skin as well as to levitate small sub-wavelength size particles. Schlieren imaging and background-oriented schlieren techniques can be used for acoustic wave pattern and beam shape visualization. These techniques exploit variations in the refractive index of a propagation medium by applying refractive optics or cross-correlation algorithms of photographs of illuminated background patterns. Here both background-oriented and traditional schlieren systems are used to visualize the regions of the acoustic power involved in creating dynamic haptic sensations and dynamic levitation traps. We demonstrate for the first time the application of back-ground-oriented schlieren for imaging ultrasonic fields in air. We detail our imaging apparatus and present improved algorithms used to visualize these phenomena that we have produced using multiple phased arrays. Moreover, to improve imaging, we leverage an electronically controlled, high-output LED which is pulsed in synchrony with the ultrasonic carrier frequency.



