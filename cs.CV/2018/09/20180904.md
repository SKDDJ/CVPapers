# Arxiv Papers in cs.CV on 2018-09-04
### End-to-end Multimodal Emotion and Gender Recognition with Dynamic Joint Loss Weights
- **Arxiv ID**: http://arxiv.org/abs/1809.00758v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SD, eess.AS, stat.ML, 68T05
- **Links**: [PDF](http://arxiv.org/pdf/1809.00758v3)
- **Published**: 2018-09-04 00:52:25+00:00
- **Updated**: 2018-10-02 04:16:54+00:00
- **Authors**: Myungsu Chae, Tae-Ho Kim, Young Hoon Shin, June-Woo Kim, Soo-Young Lee
- **Comment**: IROS 2018 Workshop on Crossmodal Learning for Intelligent Robotics
- **Journal**: None
- **Summary**: Multi-task learning is a method for improving the generalizability of multiple tasks. In order to perform multiple classification tasks with one neural network model, the losses of each task should be combined. Previous studies have mostly focused on multiple prediction tasks using joint loss with static weights for training models, choosing the weights between tasks without making sufficient considerations by setting them uniformly or empirically. In this study, we propose a method to calculate joint loss using dynamic weights to improve the total performance, instead of the individual performance, of tasks. We apply this method to design an end-to-end multimodal emotion and gender recognition model using audio and video data. This approach provides proper weights for the loss of each task when the training process ends. In our experiments, emotion and gender recognition with the proposed method yielded a lower joint loss, which is computed as the negative log-likelihood, than using static weights for joint loss. Moreover, our proposed model has better generalizability than other models. To the best of our knowledge, this research is the first to demonstrate the strength of using dynamic weights for joint loss for maximizing overall performance in emotion and gender recognition tasks.



### Spatial-Spectral Fusion by Combining Deep Learning and Variation Model
- **Arxiv ID**: http://arxiv.org/abs/1809.00764v1
- **DOI**: 10.1109/TGRS.2019.2904659
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00764v1)
- **Published**: 2018-09-04 01:39:04+00:00
- **Updated**: 2018-09-04 01:39:04+00:00
- **Authors**: Huanfeng Shen, Menghui Jiang, Jie Li, Qiangqiang Yuan, Yanchong Wei, Liangpei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In the field of spatial-spectral fusion, the model-based method and the deep learning (DL)-based method are state-of-the-art. This paper presents a fusion method that incorporates the deep neural network into the model-based method for the most common case in the spatial-spectral fusion: PAN/multispectral (MS) fusion. Specifically, we first map the gradient of the high spatial resolution panchromatic image (HR-PAN) and the low spatial resolution multispectral image (LR-MS) to the gradient of the high spatial resolution multispectral image (HR-MS) via a deep residual convolutional neural network (CNN). Then we construct a fusion framework by the LR-MS image, the gradient prior learned from the gradient network, and the ideal fused image. Finally, an iterative optimization algorithm is used to solve the fusion model. Both quantitative and visual assessments on high-quality images from various sources demonstrate that the proposed fusion method is superior to all the mainstream algorithms included in the comparison in terms of overall fusion accuracy.



### Robust Iris Segmentation Based on Fully Convolutional Networks and Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.00769v1
- **DOI**: 10.1109/SIBGRAPI.2018.00043
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00769v1)
- **Published**: 2018-09-04 02:10:41+00:00
- **Updated**: 2018-09-04 02:10:41+00:00
- **Authors**: Cides S. Bezerra, Rayson Laroca, Diego R. Lucio, Evair Severo, Lucas F. Oliveira, Alceu S. Britto Jr., David Menotti
- **Comment**: Accepted for presentation at the Conference on Graphics, Patterns and
  Images (SIBGRAPI) 2018
- **Journal**: None
- **Summary**: The iris can be considered as one of the most important biometric traits due to its high degree of uniqueness. Iris-based biometrics applications depend mainly on the iris segmentation whose suitability is not robust for different environments such as near-infrared (NIR) and visible (VIS) ones. In this paper, two approaches for robust iris segmentation based on Fully Convolutional Networks (FCNs) and Generative Adversarial Networks (GANs) are described. Similar to a common convolutional network, but without the fully connected layers (i.e., the classification layers), an FCN employs at its end a combination of pooling layers from different convolutional layers. Based on the game theory, a GAN is designed as two networks competing with each other to generate the best segmentation. The proposed segmentation networks achieved promising results in all evaluated datasets (i.e., BioSec, CasiaI3, CasiaT4, IITD-1) of NIR images and (NICE.I, CrEye-Iris and MICHE-I) of VIS images in both non-cooperative and cooperative domains, outperforming the baselines techniques which are the best ones found so far in the literature, i.e., a new state of the art for these datasets. Furthermore, we manually labeled 2,431 images from CasiaT4, CrEye-Iris and MICHE-I datasets, making the masks available for research purposes.



### Hierarchical Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/1809.03316v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.03316v1)
- **Published**: 2018-09-04 02:29:06+00:00
- **Updated**: 2018-09-04 02:29:06+00:00
- **Authors**: Farzaneh Mahdisoltani, Roland Memisevic, David Fleet
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a hierarchical architecture for video understanding that exploits the structure of real world actions by capturing targets at different levels of granularity. We design the model such that it first learns simpler coarse-grained tasks, and then moves on to learn more fine-grained targets. The model is trained with a joint loss on different granularity levels. We demonstrate empirical results on the recent release of Something-Something dataset, which provides a hierarchy of targets, namely coarse-grained action groups, fine-grained action categories, and captions. Experiments suggest that models that exploit targets at different levels of granularity achieve better performance on all levels.



### Deep Smoke Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1809.00774v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1809.00774v1)
- **Published**: 2018-09-04 02:48:28+00:00
- **Updated**: 2018-09-04 02:48:28+00:00
- **Authors**: Feiniu Yuan, Lin Zhang, Xue Xia, Boyang Wan, Qinghua Huang, Xuelong Li
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: Inspired by the recent success of fully convolutional networks (FCN) in semantic segmentation, we propose a deep smoke segmentation network to infer high quality segmentation masks from blurry smoke images. To overcome large variations in texture, color and shape of smoke appearance, we divide the proposed network into a coarse path and a fine path. The first path is an encoder-decoder FCN with skip structures, which extracts global context information of smoke and accordingly generates a coarse segmentation mask. To retain fine spatial details of smoke, the second path is also designed as an encoder-decoder FCN with skip structures, but it is shallower than the first path network. Finally, we propose a very small network containing only add, convolution and activation layers to fuse the results of the two paths. Thus, we can easily train the proposed network end to end for simultaneous optimization of network parameters. To avoid the difficulty in manually labelling fuzzy smoke objects, we propose a method to generate synthetic smoke images. According to results of our deep segmentation method, we can easily and accurately perform smoke detection from videos. Experiments on three synthetic smoke datasets and a realistic smoke dataset show that our method achieves much better performance than state-of-the-art segmentation algorithms based on FCNs. Test results of our method on videos are also appealing.



### PFDet: 2nd Place Solution to Open Images Challenge 2018 Object Detection Track
- **Arxiv ID**: http://arxiv.org/abs/1809.00778v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00778v1)
- **Published**: 2018-09-04 02:59:07+00:00
- **Updated**: 2018-09-04 02:59:07+00:00
- **Authors**: Takuya Akiba, Tommi Kerola, Yusuke Niitani, Toru Ogawa, Shotaro Sano, Shuji Suzuki
- **Comment**: Technical report for Open Images Challenge 2018 Object Detection
  Track
- **Journal**: None
- **Summary**: We present a large-scale object detection system by team PFDet. Our system enables training with huge datasets using 512 GPUs, handles sparsely verified classes, and massive class imbalance. Using our method, we achieved 2nd place in the Google AI Open Images Object Detection Track 2018 on Kaggle.



### RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes
- **Arxiv ID**: http://arxiv.org/abs/1809.00812v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.00812v1)
- **Published**: 2018-09-04 07:04:55+00:00
- **Updated**: 2018-09-04 07:04:55+00:00
- **Authors**: Semih Yagcioglu, Aykut Erdem, Erkut Erdem, Nazli Ikizler-Cinbis
- **Comment**: EMNLP 2018
- **Journal**: None
- **Summary**: Understanding and reasoning about cooking recipes is a fruitful research direction towards enabling machines to interpret procedural text. In this work, we introduce RecipeQA, a dataset for multimodal comprehension of cooking recipes. It comprises of approximately 20K instructional recipes with multiple modalities such as titles, descriptions and aligned set of images. With over 36K automatically generated question-answer pairs, we design a set of comprehension and reasoning tasks that require joint understanding of images and text, capturing the temporal flow of events and making sense of procedural knowledge. Our preliminary results indicate that RecipeQA will serve as a challenging test bed and an ideal benchmark for evaluating machine comprehension systems. The data and leaderboard are available at http://hucvl.github.io/recipeqa.



### Metabolize Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1809.00837v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.00837v1)
- **Published**: 2018-09-04 08:42:52+00:00
- **Updated**: 2018-09-04 08:42:52+00:00
- **Authors**: Dan Dai, Zhiwen Yu, Yang Hu, Wenming Cao, Mingnan Luo
- **Comment**: None
- **Journal**: None
- **Summary**: The metabolism of cells is the most basic and important part of human function. Neural networks in deep learning stem from neuronal activity. It is self-evident that the significance of metabolize neuronal network(MetaNet) in model construction. In this study, we explore neuronal metabolism for shallow network from proliferation and autophagy two aspects. First, we propose different neuron proliferate methods that constructive the selfgrowing network in metabolism cycle. Proliferate neurons alleviate resources wasting and insufficient model learning problem when network initializes more or less parameters. Then combined with autophagy mechanism in the process of model self construction to ablate under-expressed neurons. The MetaNet can automatically determine the number of neurons during training, further, save more resource consumption. We verify the performance of the proposed methods on datasets: MNIST, Fashion-MNIST and CIFAR-10.



### Towards Understanding Regularization in Batch Normalization
- **Arxiv ID**: http://arxiv.org/abs/1809.00846v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SY, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.00846v4)
- **Published**: 2018-09-04 09:01:10+00:00
- **Updated**: 2019-04-24 05:23:45+00:00
- **Authors**: Ping Luo, Xinjiang Wang, Wenqi Shao, Zhanglin Peng
- **Comment**: International Conference on Learning Representations (ICLR)
- **Journal**: None
- **Summary**: Batch Normalization (BN) improves both convergence and generalization in training neural networks. This work understands these phenomena theoretically. We analyze BN by using a basic block of neural networks, consisting of a kernel layer, a BN layer, and a nonlinear activation function. This basic network helps us understand the impacts of BN in three aspects. First, by viewing BN as an implicit regularizer, BN can be decomposed into population normalization (PN) and gamma decay as an explicit regularization. Second, learning dynamics of BN and the regularization show that training converged with large maximum and effective learning rate. Third, generalization of BN is explored by using statistical mechanics. Experiments demonstrate that BN in convolutional neural networks share the same traits of regularization as the above analyses.



### Soft-PHOC Descriptor for End-to-End Word Spotting in Egocentric Scene Images
- **Arxiv ID**: http://arxiv.org/abs/1809.00854v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00854v2)
- **Published**: 2018-09-04 09:21:32+00:00
- **Updated**: 2019-10-11 17:01:28+00:00
- **Authors**: Dena Bazazian, Dimosthenis Karatzas, Andrew D. Bagdanov
- **Comment**: 9 pages, 10 figures, The Third International Workshop on Egocentric
  Perception, Interaction and Computing (EPIC) at ECCV2018
- **Journal**: None
- **Summary**: Word spotting in natural scene images has many applications in scene understanding and visual assistance. In this paper we propose a technique to create and exploit an intermediate representation of images based on text attributes which are character probability maps. Our representation extends the concept of the Pyramidal Histogram Of Characters (PHOC) by exploiting Fully Convolutional Networks to derive a pixel-wise mapping of the character distribution within candidate word regions. We call this representation the Soft-PHOC. Furthermore, we show how to use Soft-PHOC descriptors for word spotting tasks in egocentric camera streams through an efficient text line proposal algorithm. This is based on the Hough Transform over character attribute maps followed by scoring using Dynamic Time Warping (DTW). We evaluate our results on ICDAR 2015 Challenge 4 dataset of incidental scene text captured by an egocentric camera.



### Handwriting styles: benchmarks and evaluation metrics
- **Arxiv ID**: http://arxiv.org/abs/1809.00862v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.00862v1)
- **Published**: 2018-09-04 09:54:25+00:00
- **Updated**: 2018-09-04 09:54:25+00:00
- **Authors**: Omar Mohammed, Gerard Bailly, Damien Pellier
- **Comment**: Submitted to IEEE International Workshop on Deep and Transfer
  Learning (DTL 2018)
- **Journal**: None
- **Summary**: Evaluating the style of handwriting generation is a challenging problem, since it is not well defined. It is a key component in order to develop in developing systems with more personalized experiences with humans. In this paper, we propose baseline benchmarks, in order to set anchors to estimate the relative quality of different handwriting style methods. This will be done using deep learning techniques, which have shown remarkable results in different machine learning tasks, learning classification, regression, and most relevant to our work, generating temporal sequences. We discuss the challenges associated with evaluating our methods, which is related to evaluation of generative models in general. We then propose evaluation metrics, which we find relevant to this problem, and we discuss how we evaluate the evaluation metrics. In this study, we use IRON-OFF dataset. To the best of our knowledge, there is no work done before in generating handwriting (either in terms of methodology or the performance metrics), our in exploring styles using this dataset.



### MesoNet: a Compact Facial Video Forgery Detection Network
- **Arxiv ID**: http://arxiv.org/abs/1809.00888v1
- **DOI**: 10.1109/WIFS.2018.8630761
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1809.00888v1)
- **Published**: 2018-09-04 10:59:22+00:00
- **Updated**: 2018-09-04 10:59:22+00:00
- **Authors**: Darius Afchar, Vincent Nozick, Junichi Yamagishi, Isao Echizen
- **Comment**: accepted to WIFS 2018
- **Journal**: None
- **Summary**: This paper presents a method to automatically and efficiently detect face tampering in videos, and particularly focuses on two recent techniques used to generate hyper-realistic forged videos: Deepfake and Face2Face. Traditional image forensics techniques are usually not well suited to videos due to the compression that strongly degrades the data. Thus, this paper follows a deep learning approach and presents two networks, both with a low number of layers to focus on the mesoscopic properties of images. We evaluate those fast networks on both an existing dataset and a dataset we have constituted from online videos. The tests demonstrate a very successful detection rate with more than 98% for Deepfake and 95% for Face2Face.



### Image Reassembly Combining Deep Learning and Shortest Path Problem
- **Arxiv ID**: http://arxiv.org/abs/1809.00898v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00898v1)
- **Published**: 2018-09-04 11:39:03+00:00
- **Updated**: 2018-09-04 11:39:03+00:00
- **Authors**: M. -M. Paumard, D. Picard, H. Tabia
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: This paper addresses the problem of reassembling images from disjointed fragments. More specifically, given an unordered set of fragments, we aim at reassembling one or several possibly incomplete images. The main contributions of this work are: 1) several deep neural architectures to predict the relative position of image fragments that outperform the previous state of the art; 2) casting the reassembly problem into the shortest path in a graph problem for which we provide several construction algorithms depending on available information; 3) a new dataset of images taken from the Metropolitan Museum of Art (MET) dedicated to image reassembly for which we provide a clear setup and a strong baseline.



### Penalizing Top Performers: Conservative Loss for Semantic Segmentation Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1809.00903v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00903v2)
- **Published**: 2018-09-04 11:50:09+00:00
- **Updated**: 2019-04-02 03:31:48+00:00
- **Authors**: Xinge Zhu, Hui Zhou, Ceyuan Yang, Jianping Shi, Dahua Lin
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Due to the expensive and time-consuming annotations (e.g., segmentation) for real-world images, recent works in computer vision resort to synthetic data. However, the performance on the real image often drops significantly because of the domain shift between the synthetic data and the real images. In this setting, domain adaptation brings an appealing option. The effective approaches of domain adaptation shape the representations that (1) are discriminative for the main task and (2) have good generalization capability for domain shift. To this end, we propose a novel loss function, i.e., Conservative Loss, which penalizes the extreme good and bad cases while encouraging the moderate examples. More specifically, it enables the network to learn features that are discriminative by gradient descent and are invariant to the change of domains via gradient ascend method. Extensive experiments on synthetic to real segmentation adaptation show our proposed method achieves state of the art results. Ablation studies give more insights into properties of the Conservative Loss. Exploratory experiments and discussion demonstrate that our Conservative Loss has good flexibility rather than restricting an exact form.



### Bangla License Plate Recognition Using Convolutional Neural Networks (CNN)
- **Arxiv ID**: http://arxiv.org/abs/1809.00905v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00905v1)
- **Published**: 2018-09-04 11:55:34+00:00
- **Updated**: 2018-09-04 11:55:34+00:00
- **Authors**: M M Shaifur Rahman, Mst Shamima Nasrin, Moin Mostakim, Md Zahangir Alom
- **Comment**: 6 pages,10 figures
- **Journal**: None
- **Summary**: In the last few years, the deep learning technique in particular Convolutional Neural Networks (CNNs) is using massively in the field of computer vision and machine learning. This deep learning technique provides state-of-the-art accuracy in different classification, segmentation, and detection tasks on different benchmarks such as MNIST, CIFAR-10, CIFAR-100, Microsoft COCO, and ImageNet. However, there are a lot of research has been conducted for Bangla License plate recognition with traditional machine learning approaches in last decade. None of them are used to deploy a physical system for Bangla License Plate Recognition System (BLPRS) due to their poor recognition accuracy. In this paper, we have implemented CNNs based Bangla license plate recognition system with better accuracy that can be applied for different purposes including roadside assistance, automatic parking lot management system, vehicle license status detection and so on. Along with that, we have also created and released a very first and standard database for BLPRS.



### OCNet: Object Context Network for Scene Parsing
- **Arxiv ID**: http://arxiv.org/abs/1809.00916v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00916v4)
- **Published**: 2018-09-04 12:22:10+00:00
- **Updated**: 2021-03-15 03:27:54+00:00
- **Authors**: Yuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang, Xilin Chen, Jingdong Wang
- **Comment**: Accepted at IJCV 2021
- **Journal**: None
- **Summary**: In this paper, we address the semantic segmentation task with a new context aggregation scheme named \emph{object context}, which focuses on enhancing the role of object information. Motivated by the fact that the category of each pixel is inherited from the object it belongs to, we define the object context for each pixel as the set of pixels that belong to the same category as the given pixel in the image. We use a binary relation matrix to represent the relationship between all pixels, where the value one indicates the two selected pixels belong to the same category and zero otherwise.   We propose to use a dense relation matrix to serve as a surrogate for the binary relation matrix. The dense relation matrix is capable to emphasize the contribution of object information as the relation scores tend to be larger on the object pixels than the other pixels. Considering that the dense relation matrix estimation requires quadratic computation overhead and memory consumption w.r.t. the input size, we propose an efficient interlaced sparse self-attention scheme to model the dense relations between any two of all pixels via the combination of two sparse relation matrices.   To capture richer context information, we further combine our interlaced sparse self-attention scheme with the conventional multi-scale context schemes including pyramid pooling~\citep{zhao2017pyramid} and atrous spatial pyramid pooling~\citep{chen2018deeplab}. We empirically show the advantages of our approach with competitive performances on five challenging benchmarks including: Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff



### Compressive Hyperspectral Imaging: Fourier Transform Interferometry meets Single Pixel Camera
- **Arxiv ID**: http://arxiv.org/abs/1809.00950v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00950v1)
- **Published**: 2018-09-04 13:40:39+00:00
- **Updated**: 2018-09-04 13:40:39+00:00
- **Authors**: Amirafshar Moshtaghpour, José M. Bioucas-Dias, Laurent Jacques
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: This paper introduces a single-pixel HyperSpectral (HS) imaging framework based on Fourier Transform Interferometry (FTI). By combining a space-time coding of the light illumination with partial interferometric observations of a collimated light beam (observed by a single pixel), our system benefits from (i) reduced measurement rate and light-exposure of the observed object compared to common (Nyquist) FTI imagers, and (ii) high spectral resolution as desirable in, e.g., Fluorescence Spectroscopy (FS). From the principles of compressive sensing with multilevel sampling, our method leverages the sparsity "in level" of FS data, both in the spectral and the spatial domains. This allows us to optimize the space-time light coding using time-modulated Hadamard patterns. We confirm the effectiveness of our approach by a few numerical experiments.



### Towards Efficient Convolutional Neural Network for Domain-Specific Applications on FPGA
- **Arxiv ID**: http://arxiv.org/abs/1809.03318v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03318v1)
- **Published**: 2018-09-04 13:58:22+00:00
- **Updated**: 2018-09-04 13:58:22+00:00
- **Authors**: Ruizhe Zhao, Ho-Cheung Ng, Wayne Luk, Xinyu Niu
- **Comment**: None
- **Journal**: None
- **Summary**: FPGA becomes a popular technology for implementing Convolutional Neural Network (CNN) in recent years. Most CNN applications on FPGA are domain-specific, e.g., detecting objects from specific categories, in which commonly-used CNN models pre-trained on general datasets may not be efficient enough. This paper presents TuRF, an end-to-end CNN acceleration framework to efficiently deploy domain-specific applications on FPGA by transfer learning that adapts pre-trained models to specific domains, replacing standard convolution layers with efficient convolution blocks, and applying layer fusion to enhance hardware design performance. We evaluate TuRF by deploying a pre-trained VGG-16 model for a domain-specific image recognition task onto a Stratix V FPGA. Results show that designs generated by TuRF achieve better performance than prior methods for the original VGG-16 and ResNet-50 models, while for the optimised VGG-16 model TuRF designs are more accurate and easier to process.



### Geometric Operator Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1809.01016v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01016v1)
- **Published**: 2018-09-04 14:19:39+00:00
- **Updated**: 2018-09-04 14:19:39+00:00
- **Authors**: Yangling Ma, Yixin Luo, Zhouwang Yang
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: The Convolutional Neural Network (CNN) has been successfully applied in many fields during recent decades; however it lacks the ability to utilize prior domain knowledge when dealing with many realistic problems. We present a framework called Geometric Operator Convolutional Neural Network (GO-CNN) that uses domain knowledge, wherein the kernel of the first convolutional layer is replaced with a kernel generated by a geometric operator function. This framework integrates many conventional geometric operators, which allows it to adapt to a diverse range of problems. Under certain conditions, we theoretically analyze the convergence and the bound of the generalization errors between GO-CNNs and common CNNs. Although the geometric operator convolution kernels have fewer trainable parameters than common convolution kernels, the experimental results indicate that GO-CNN performs more accurately than common CNN on CIFAR-10/100. Furthermore, GO-CNN reduces dependence on the amount of training examples and enhances adversarial stability. In the practical task of medically diagnosing bone fractures, GO-CNN obtains 3% improvement in terms of the recall.



### Leveraging Deep Visual Descriptors for Hierarchical Efficient Localization
- **Arxiv ID**: http://arxiv.org/abs/1809.01019v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01019v2)
- **Published**: 2018-09-04 14:25:17+00:00
- **Updated**: 2018-09-18 20:51:28+00:00
- **Authors**: Paul-Edouard Sarlin, Frédéric Debraine, Marcin Dymczyk, Roland Siegwart, Cesar Cadena
- **Comment**: CoRL 2018 Camera-ready (fix typos and update citations)
- **Journal**: None
- **Summary**: Many robotics applications require precise pose estimates despite operating in large and changing environments. This can be addressed by visual localization, using a pre-computed 3D model of the surroundings. The pose estimation then amounts to finding correspondences between 2D keypoints in a query image and 3D points in the model using local descriptors. However, computational power is often limited on robotic platforms, making this task challenging in large-scale environments. Binary feature descriptors significantly speed up this 2D-3D matching, and have become popular in the robotics community, but also strongly impair the robustness to perceptual aliasing and changes in viewpoint, illumination and scene structure. In this work, we propose to leverage recent advances in deep learning to perform an efficient hierarchical localization. We first localize at the map level using learned image-wide global descriptors, and subsequently estimate a precise pose from 2D-3D matches computed in the candidate places only. This restricts the local search and thus allows to efficiently exploit powerful non-binary descriptors usually dismissed on resource-constrained devices. Our approach results in state-of-the-art localization performance while running in real-time on a popular mobile platform, enabling new prospects for robotics research.



### Iris recognition in cases of eye pathology
- **Arxiv ID**: http://arxiv.org/abs/1809.01040v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01040v1)
- **Published**: 2018-09-04 15:22:42+00:00
- **Updated**: 2018-09-04 15:22:42+00:00
- **Authors**: Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz
- **Comment**: Accepted for publication as a chapter in A. Nait-Ali (Ed.),
  "Biometrics under Biomedical Considerations", Springer, 2019, ISBN
  978-981-13-1143-7
- **Journal**: None
- **Summary**: This chapter provides insight on how iris recognition, one of the leading biometric identification technologies in the world, can be impacted by pathologies and illnesses present in the eye, what are the possible repercussions of this influence, and what are the possible means for taking such effects into account when matching iris samples.   To make this study possible, a special database of iris images has been used, representing more than 20 different medical conditions of the ocular region (including cataract, glaucoma, rubeosis iridis, synechiae, iris defects, corneal pathologies and other) and containing almost 3000 samples collected from 230 distinct irises. Then, with the use of four different iris recognition methods, a series of experiments has been conducted, concluding in several important observations.   One of the most popular ocular disorders worldwide - the cataract - is shown to worsen genuine comparison scores when results obtained from cataract-affected eyes are compared to those coming from healthy irises. An analysis devoted to different types of impact on eye structures caused by diseases is also carried out with significant results. The enrollment process is highly sensitive to those eye conditions that make the iris obstructed or introduce geometrical distortions. Disorders affecting iris geometry, or producing obstructions are exceptionally capable of degrading the genuine comparison scores, so that the performance of the entire biometric system can be influenced. Experiments also reveal that imperfect execution of the image segmentation stage is the most prominent contributor to recognition errors.



### Out-of-Distribution Detection Using an Ensemble of Self Supervised Leave-out Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1809.03576v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.03576v1)
- **Published**: 2018-09-04 16:00:08+00:00
- **Updated**: 2018-09-04 16:00:08+00:00
- **Authors**: Apoorv Vyas, Nataraj Jammalamadaka, Xia Zhu, Dipankar Das, Bharat Kaul, Theodore L. Willke
- **Comment**: None
- **Journal**: None
- **Summary**: As deep learning methods form a critical part in commercially important applications such as autonomous driving and medical diagnostics, it is important to reliably detect out-of-distribution (OOD) inputs while employing these algorithms. In this work, we propose an OOD detection algorithm which comprises of an ensemble of classifiers. We train each classifier in a self-supervised manner by leaving out a random subset of training data as OOD data and the rest as in-distribution (ID) data. We propose a novel margin-based loss over the softmax output which seeks to maintain at least a margin $m$ between the average entropy of the OOD and in-distribution samples. In conjunction with the standard cross-entropy loss, we minimize the novel loss to train an ensemble of classifiers. We also propose a novel method to combine the outputs of the ensemble of classifiers to obtain OOD detection score and class prediction. Overall, our method convincingly outperforms Hendrycks et al.[7] and the current state-of-the-art ODIN[13] on several OOD detection benchmarks.



### Text2Scene: Generating Compositional Scenes from Textual Descriptions
- **Arxiv ID**: http://arxiv.org/abs/1809.01110v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1809.01110v3)
- **Published**: 2018-09-04 17:31:13+00:00
- **Updated**: 2019-06-09 16:22:47+00:00
- **Authors**: Fuwen Tan, Song Feng, Vicente Ordonez
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we propose Text2Scene, a model that generates various forms of compositional scene representations from natural language descriptions. Unlike recent works, our method does NOT use Generative Adversarial Networks (GANs). Text2Scene instead learns to sequentially generate objects and their attributes (location, size, appearance, etc) at every time step by attending to different parts of the input text and the current status of the generated scene. We show that under minor modifications, the proposed framework can handle the generation of different forms of scene representations, including cartoon-like scenes, object layouts corresponding to real images, and synthetic images. Our method is not only competitive when compared with state-of-the-art GAN-based methods using automatic metrics and superior based on human judgments but also has the advantage of producing interpretable results.



### VideoMatch: Matching based Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1809.01123v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.01123v1)
- **Published**: 2018-09-04 17:59:53+00:00
- **Updated**: 2018-09-04 17:59:53+00:00
- **Authors**: Yuan-Ting Hu, Jia-Bin Huang, Alexander G. Schwing
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: Video object segmentation is challenging yet important in a wide variety of applications for video analysis. Recent works formulate video object segmentation as a prediction task using deep nets to achieve appealing state-of-the-art performance. Due to the formulation as a prediction task, most of these methods require fine-tuning during test time, such that the deep nets memorize the appearance of the objects of interest in the given video. However, fine-tuning is time-consuming and computationally expensive, hence the algorithms are far from real time. To address this issue, we develop a novel matching based algorithm for video object segmentation. In contrast to memorization based classification techniques, the proposed approach learns to match extracted features to a provided template without memorizing the appearance of the objects. We validate the effectiveness and the robustness of the proposed method on the challenging DAVIS-16, DAVIS-17, Youtube-Objects and JumpCut datasets. Extensive results show that our method achieves comparable performance without fine-tuning and is much more favorable in terms of computational time.



### Straight to the Facts: Learning Knowledge Base Retrieval for Factual Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1809.01124v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1809.01124v1)
- **Published**: 2018-09-04 17:59:55+00:00
- **Updated**: 2018-09-04 17:59:55+00:00
- **Authors**: Medhini Narasimhan, Alexander G. Schwing
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: Question answering is an important task for autonomous agents and virtual assistants alike and was shown to support the disabled in efficiently navigating an overwhelming environment. Many existing methods focus on observation-based questions, ignoring our ability to seamlessly combine observed content with general knowledge. To understand interactions with a knowledge base, a dataset has been introduced recently and keyword matching techniques were shown to yield compelling results despite being vulnerable to misconceptions due to synonyms and homographs. To address this issue, we develop a learning-based approach which goes straight to the facts via a learned embedding space. We demonstrate state-of-the-art results on the challenging recently introduced fact-based visual question answering dataset, outperforming competing methods by more than 5%.



### Unsupervised Video Object Segmentation using Motion Saliency-Guided Spatio-Temporal Propagation
- **Arxiv ID**: http://arxiv.org/abs/1809.01125v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01125v1)
- **Published**: 2018-09-04 17:59:55+00:00
- **Updated**: 2018-09-04 17:59:55+00:00
- **Authors**: Yuan-Ting Hu, Jia-Bin Huang, Alexander G. Schwing
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: Unsupervised video segmentation plays an important role in a wide variety of applications from object identification to compression. However, to date, fast motion, motion blur and occlusions pose significant challenges. To address these challenges for unsupervised video segmentation, we develop a novel saliency estimation technique as well as a novel neighborhood graph, based on optical flow and edge cues. Our approach leads to significantly better initial foreground-background estimates and their robust as well as accurate diffusion across time. We evaluate our proposed algorithm on the challenging DAVIS, SegTrack v2 and FBMS-59 datasets. Despite the usage of only a standard edge detector trained on 200 images, our method achieves state-of-the-art results outperforming deep learning based methods in the unsupervised setting. We even demonstrate competitive results comparable to deep learning based methods in the semi-supervised setting on the DAVIS dataset.



### Deep Priority Hashing
- **Arxiv ID**: http://arxiv.org/abs/1809.01238v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01238v1)
- **Published**: 2018-09-04 20:48:18+00:00
- **Updated**: 2018-09-04 20:48:18+00:00
- **Authors**: Zhangjie Cao, Ziping Sun, Mingsheng Long, Jianmin Wang, Philip S. Yu
- **Comment**: ACMMM 2018 Poster. arXiv admin note: text overlap with
  arXiv:1702.00758
- **Journal**: None
- **Summary**: Deep hashing enables image retrieval by end-to-end learning of deep representations and hash codes from training data with pairwise similarity information. Subject to the distribution skewness underlying the similarity information, most existing deep hashing methods may underperform for imbalanced data due to misspecified loss functions. This paper presents Deep Priority Hashing (DPH), an end-to-end architecture that generates compact and balanced hash codes in a Bayesian learning framework. The main idea is to reshape the standard cross-entropy loss for similarity-preserving learning such that it down-weighs the loss associated to highly-confident pairs. This idea leads to a novel priority cross-entropy loss, which prioritizes the training on uncertain pairs over confident pairs. Also, we propose another priority quantization loss, which prioritizes hard-to-quantize examples for generation of nearly lossless hash codes. Extensive experiments demonstrate that DPH can generate high-quality hash codes and yield state-of-the-art image retrieval results on three datasets, ImageNet, NUS-WIDE, and MS-COCO.



### Multi-Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1809.02176v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02176v1)
- **Published**: 2018-09-04 20:54:48+00:00
- **Updated**: 2018-09-04 20:54:48+00:00
- **Authors**: Zhongyi Pei, Zhangjie Cao, Mingsheng Long, Jianmin Wang
- **Comment**: AAAI 2018 Oral. arXiv admin note: substantial text overlap with
  arXiv:1705.10667, arXiv:1707.07901
- **Journal**: None
- **Summary**: Recent advances in deep domain adaptation reveal that adversarial learning can be embedded into deep networks to learn transferable features that reduce distribution discrepancy between the source and target domains. Existing domain adversarial adaptation methods based on single domain discriminator only align the source and target data distributions without exploiting the complex multimode structures. In this paper, we present a multi-adversarial domain adaptation (MADA) approach, which captures multimode structures to enable fine-grained alignment of different data distributions based on multiple domain discriminators. The adaptation can be achieved by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Empirical evidence demonstrates that the proposed model outperforms state of the art methods on standard domain adaptation datasets.



### An Efficient Approach for Polyps Detection in Endoscopic Videos Based on Faster R-CNN
- **Arxiv ID**: http://arxiv.org/abs/1809.01263v1
- **DOI**: None
- **Categories**: **q-bio.TO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.01263v1)
- **Published**: 2018-09-04 22:43:13+00:00
- **Updated**: 2018-09-04 22:43:13+00:00
- **Authors**: Xi Mo, Ke Tao, Quan Wang, Guanghui Wang
- **Comment**: 6 pages, 10 figures,2018 International Conference on Pattern
  Recognition
- **Journal**: None
- **Summary**: Polyp has long been considered as one of the major etiologies to colorectal cancer which is a fatal disease around the world, thus early detection and recognition of polyps plays a crucial role in clinical routines. Accurate diagnoses of polyps through endoscopes operated by physicians becomes a challenging task not only due to the varying expertise of physicians, but also the inherent nature of endoscopic inspections. To facilitate this process, computer-aid techniques that emphasize fully-conventional image processing and novel machine learning enhanced approaches have been dedicatedly designed for polyp detection in endoscopic videos or images. Among all proposed algorithms, deep learning based methods take the lead in terms of multiple metrics in evolutions for algorithmic performance. In this work, a highly effective model, namely the faster region-based convolutional neural network (Faster R-CNN) is implemented for polyp detection. In comparison with the reported results of the state-of-the-art approaches on polyps detection, extensive experiments demonstrate that the Faster R-CNN achieves very competing results, and it is an efficient approach for clinical practice.



### Developing a Purely Visual Based Obstacle Detection using Inverse Perspective Mapping
- **Arxiv ID**: http://arxiv.org/abs/1809.01268v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.01268v1)
- **Published**: 2018-09-04 23:09:45+00:00
- **Updated**: 2018-09-04 23:09:45+00:00
- **Authors**: Julian Nubert, Niklas Funk, Fabio Meier, Fabrice Oehler
- **Comment**: Project report and analysis for the Duckietown Project
  (https://www.duckietown.org/). 17 pages and 38 figures
- **Journal**: None
- **Summary**: Our solution is implemented in and for the frame of Duckietown. The goal of Duckietown is to provide a relatively simple platform to explore, tackle and solve many problems linked to autonomous driving. "Duckietown" is simple in the basics, but an infinitely expandable environment. From controlling single driving Duckiebots until complete fleet management, every scenario is possible and can be put into practice. So far, none of the existing modules was capable of reliably detecting obstacles and reacting to them in real time. We faced the general problem of detecting obstacles given images from a monocular RGB camera mounted at the front of our Duckiebot and reacting to them properly without crashing or erroneously stopping the Duckiebot. Both, the detection as well as the reaction have to be implemented and have to run on a Raspberry Pi in real time. Due to the strong hardware limitations, we decided to not use any learning algorithms for the obstacle detection part. As it later transpired, a working "hard coded" software needs thorough analysis and understanding of the given problem. In layman's terms, we simply seek to make Duckietown a safer place.



