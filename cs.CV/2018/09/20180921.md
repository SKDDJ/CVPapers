# Arxiv Papers in cs.CV on 2018-09-21
### Large-Scale Video Classification with Feature Space Augmentation coupled with Learned Label Relations and Ensembling
- **Arxiv ID**: http://arxiv.org/abs/1809.07895v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07895v1)
- **Published**: 2018-09-21 00:10:18+00:00
- **Updated**: 2018-09-21 00:10:18+00:00
- **Authors**: Choongyeun Cho, Benjamin Antin, Sanchit Arora, Shwan Ashrafi, Peilin Duan, Dang The Huynh, Lee James, Hang Tuan Nguyen, Mojtaba Solgi, Cuong Van Than
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents the Axon AI's solution to the 2nd YouTube-8M Video Understanding Challenge, achieving the final global average precision (GAP) of 88.733% on the private test set (ranked 3rd among 394 teams, not considering the model size constraint), and 87.287% using a model that meets size requirement. Two sets of 7 individual models belonging to 3 different families were trained separately. Then, the inference results on a training data were aggregated from these multiple models and fed to train a compact model that meets the model size requirement. In order to further improve performance we explored and employed data over/sub-sampling in feature space, an additional regularization term during training exploiting label relationship, and learned weights for ensembling different individual models.



### Adaptive O-CNN: A Patch-based Deep Representation of 3D Shapes
- **Arxiv ID**: http://arxiv.org/abs/1809.07917v1
- **DOI**: 10.1145/3272127.3275050
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1809.07917v1)
- **Published**: 2018-09-21 02:24:48+00:00
- **Updated**: 2018-09-21 02:24:48+00:00
- **Authors**: Peng-Shuai Wang, Chun-Yu Sun, Yang Liu, Xin Tong
- **Comment**: None
- **Journal**: ACM Transactions on Graphics, 2018
- **Summary**: We present an Adaptive Octree-based Convolutional Neural Network (Adaptive O-CNN) for efficient 3D shape encoding and decoding. Different from volumetric-based or octree-based CNN methods that represent a 3D shape with voxels in the same resolution, our method represents a 3D shape adaptively with octants at different levels and models the 3D shape within each octant with a planar patch. Based on this adaptive patch-based representation, we propose an Adaptive O-CNN encoder and decoder for encoding and decoding 3D shapes. The Adaptive O-CNN encoder takes the planar patch normal and displacement as input and performs 3D convolutions only at the octants at each level, while the Adaptive O-CNN decoder infers the shape occupancy and subdivision status of octants at each level and estimates the best plane normal and displacement for each leaf octant. As a general framework for 3D shape analysis and generation, the Adaptive O-CNN not only reduces the memory and computational cost, but also offers better shape generation capability than the existing 3D-CNN approaches. We validate Adaptive O-CNN in terms of efficiency and effectiveness on different shape analysis and generation tasks, including shape classification, 3D autoencoding, shape prediction from a single image, and shape completion for noisy and incomplete point clouds.



### Adversarial 3D Human Pose Estimation via Multimodal Depth Supervision
- **Arxiv ID**: http://arxiv.org/abs/1809.07921v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07921v1)
- **Published**: 2018-09-21 02:48:34+00:00
- **Updated**: 2018-09-21 02:48:34+00:00
- **Authors**: Kun Zhou, Jinmiao Cai, Yao Li, Yulong Shi, Xiaoguang Han, Nianjuan Jiang, Kui Jia, Jiangbo Lu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a novel deep-learning based framework is proposed to infer 3D human poses from a single image. Specifically, a two-phase approach is developed. We firstly utilize a generator with two branches for the extraction of explicit and implicit depth information respectively. During the training process, an adversarial scheme is also employed to further improve the performance. The implicit and explicit depth information with the estimated 2D joints generated by a widely used estimator, in the second step, are together fed into a deep 3D pose regressor for the final pose generation. Our method achieves MPJPE of 58.68mm on the ECCV2018 3D Human Pose Estimation Challenge.



### LIDAR-Camera Fusion for Road Detection Using Fully Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.07941v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07941v1)
- **Published**: 2018-09-21 04:31:40+00:00
- **Updated**: 2018-09-21 04:31:40+00:00
- **Authors**: Luca Caltagirone, Mauro Bellone, Lennart Svensson, Mattias Wahde
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, a deep learning approach has been developed to carry out road detection by fusing LIDAR point clouds and camera images. An unstructured and sparse point cloud is first projected onto the camera image plane and then upsampled to obtain a set of dense 2D images encoding spatial information. Several fully convolutional neural networks (FCNs) are then trained to carry out road detection, either by using data from a single sensor, or by using three fusion strategies: early, late, and the newly proposed cross fusion. Whereas in the former two fusion approaches, the integration of multimodal information is carried out at a predefined depth level, the cross fusion FCN is designed to directly learn from data where to integrate information; this is accomplished by using trainable cross connections between the LIDAR and the camera processing branches.   To further highlight the benefits of using a multimodal system for road detection, a data set consisting of visually challenging scenes was extracted from driving sequences of the KITTI raw data set. It was then demonstrated that, as expected, a purely camera-based FCN severely underperforms on this data set. A multimodal system, on the other hand, is still able to provide high accuracy. Finally, the proposed cross fusion FCN was evaluated on the KITTI road benchmark where it achieved excellent performance, with a MaxF score of 96.03%, ranking it among the top-performing approaches.



### Classifying Mammographic Breast Density by Residual Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.10241v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.10241v1)
- **Published**: 2018-09-21 06:29:50+00:00
- **Updated**: 2018-09-21 06:29:50+00:00
- **Authors**: Jingxu Xu, Cheng Li, Yongjin Zhou, Lisha Mou, Hairong Zheng, Shanshan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Mammographic breast density, a parameter used to describe the proportion of breast tissue fibrosis, is widely adopted as an evaluation characteristic of the likelihood of breast cancer incidence. In this study, we present a radiomics approach based on residual learning for the classification of mammographic breast densities. Our method possesses several encouraging properties such as being almost fully automatic, possessing big model capacity and flexibility. It can obtain outstanding classification results without the necessity of result compensation using mammographs taken from different views. The proposed method was instantiated with the INbreast dataset and classification accuracies of 92.6% and 96.8% were obtained for the four BI-RADS (Breast Imaging and Reporting Data System) category task and the two BI-RADS category task,respectively. The superior performances achieved compared to the existing state-of-the-art methods along with its encouraging properties indicate that our method has a great potential to be applied as a computer-aided diagnosis tool.



### Real-Time Stereo Vision on FPGAs with SceneScan
- **Arxiv ID**: http://arxiv.org/abs/1809.07977v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07977v1)
- **Published**: 2018-09-21 08:16:50+00:00
- **Updated**: 2018-09-21 08:16:50+00:00
- **Authors**: Konstantin Schauwecker
- **Comment**: 12 pages, 3 figures; accepted for publication at Forum
  Bildverarbeitung 2018
- **Journal**: None
- **Summary**: We present a flexible FPGA stereo vision implementation that is capable of processing up to 100 frames per second or image resolutions up to 3.4 megapixels, while consuming only 8 W of power. The implementation uses a variation of the Semi-Global Matching (SGM) algorithm, which provides superior results compared to many simpler approaches. The stereo matching results are improved significantly through a post-processing chain that operates on the computed cost cube and the disparity map. With this implementation we have created two stand-alone hardware systems for stereo vision, called SceneScan and SceneScan Pro. Both systems have been developed to market maturity and are available from Nerian Vision GmbH.



### On Variational Methods for Motion Compensated Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1809.07983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07983v1)
- **Published**: 2018-09-21 08:26:05+00:00
- **Updated**: 2018-09-21 08:26:05+00:00
- **Authors**: Francois Lauze, Mads Nielsen
- **Comment**: DIKU Technical report 2009 with some small corrections
- **Journal**: None
- **Summary**: We develop in this paper a generic Bayesian framework for the joint estimation of motion and recovery of missing data in a damaged video sequence. Using standard maximum a posteriori to variational formulation rationale, we derive generic minimum energy formulations for the estimation of a reconstructed sequence as well as motion recovery. We instantiate these energy formulations and from their Euler-Lagrange Equations, we propose a full multiresolution algorithms in order to compute good local minimizers for our energies and discuss their numerical implementations, focusing on the missing data recovery part, i.e. inpainting. Experimental results for synthetic as well as real sequences are presented. Image sequences and extra material is available at http://image.diku.dk/francois/seqinp.php.



### Minimal Paths for Tubular Structure Segmentation with Coherence Penalty and Adaptive Anisotropy
- **Arxiv ID**: http://arxiv.org/abs/1809.07987v4
- **DOI**: 10.1109/TIP.2018.2874282
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07987v4)
- **Published**: 2018-09-21 08:35:59+00:00
- **Updated**: 2018-10-25 15:13:20+00:00
- **Authors**: Da Chen, Jiong Zhang, Laurent D. Cohen
- **Comment**: This manuscript has been accepted by IEEE Trans. Image Processing,
  2018
- **Journal**: None
- **Summary**: The minimal path method has proven to be particularly useful and efficient in tubular structure segmentation applications. In this paper, we propose a new minimal path model associated with a dynamic Riemannian metric embedded with an appearance feature coherence penalty and an adaptive anisotropy enhancement term. The features that characterize the appearance and anisotropy properties of a tubular structure are extracted through the associated orientation score. The proposed dynamic Riemannian metric is updated in the course of the geodesic distance computation carried out by the efficient single-pass fast marching method. Compared to state-of-the-art minimal path models, the proposed minimal path model is able to extract the desired tubular structures from a complicated vessel tree structure. In addition, we propose an efficient prior path-based method to search for vessel radius value at each centerline position of the target. Finally, we perform the numerical experiments on both synthetic and real images. The quantitive validation is carried out on retinal vessel images. The results indicate that the proposed model indeed achieves a promising performance.



### SG-FCN: A Motion and Memory-Based Deep Learning Model for Video Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/1809.07988v1
- **DOI**: 10.1109/TCYB.2018.2832053
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07988v1)
- **Published**: 2018-09-21 08:36:15+00:00
- **Updated**: 2018-09-21 08:36:15+00:00
- **Authors**: Meijun Sun, Ziqi Zhou, QinGhua Hu, Zheng Wang, Jianmin Jiang
- **Comment**: None
- **Journal**: IEEE Transactions on Cybernetics ( Volume: PP, Issue: 99 ),2018
- **Summary**: Data-driven saliency detection has attracted strong interest as a result of applying convolutional neural networks to the detection of eye fixations. Although a number of imagebased salient object and fixation detection models have been proposed, video fixation detection still requires more exploration. Different from image analysis, motion and temporal information is a crucial factor affecting human attention when viewing video sequences. Although existing models based on local contrast and low-level features have been extensively researched, they failed to simultaneously consider interframe motion and temporal information across neighboring video frames, leading to unsatisfactory performance when handling complex scenes. To this end, we propose a novel and efficient video eye fixation detection model to improve the saliency detection performance. By simulating the memory mechanism and visual attention mechanism of human beings when watching a video, we propose a step-gained fully convolutional network by combining the memory information on the time axis with the motion information on the space axis while storing the saliency information of the current frame. The model is obtained through hierarchical training, which ensures the accuracy of the detection. Extensive experiments in comparison with 11 state-of-the-art methods are carried out, and the results show that our proposed model outperforms all 11 methods across a number of publicly available datasets.



### Multimodal Dual Attention Memory for Video Story Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1809.07999v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1809.07999v1)
- **Published**: 2018-09-21 09:19:12+00:00
- **Updated**: 2018-09-21 09:19:12+00:00
- **Authors**: Kyung-Min Kim, Seong-Ho Choi, Jin-Hwa Kim, Byoung-Tak Zhang
- **Comment**: Accepted for ECCV 2018
- **Journal**: None
- **Summary**: We propose a video story question-answering (QA) architecture, Multimodal Dual Attention Memory (MDAM). The key idea is to use a dual attention mechanism with late fusion. MDAM uses self-attention to learn the latent concepts in scene frames and captions. Given a question, MDAM uses the second attention over these latent concepts. Multimodal fusion is performed after the dual attention processes (late fusion). Using this processing pipeline, MDAM learns to infer a high-level vision-language joint representation from an abstraction of the full video content. We evaluate MDAM on PororoQA and MovieQA datasets which have large-scale QA annotations on cartoon videos and movies, respectively. For both datasets, MDAM achieves new state-of-the-art results with significant margins compared to the runner-up models. We confirm the best performance of the dual attention mechanism combined with late fusion by ablation studies. We also perform qualitative analysis by visualizing the inference mechanisms of MDAM.



### Perfect match: Improved cross-modal embeddings for audio-visual synchronisation
- **Arxiv ID**: http://arxiv.org/abs/1809.08001v2
- **DOI**: 10.1109/ICASSP.2019.8682524
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1809.08001v2)
- **Published**: 2018-09-21 09:24:37+00:00
- **Updated**: 2018-11-02 07:41:21+00:00
- **Authors**: Soo-Whan Chung, Joon Son Chung, Hong-Goo Kang
- **Comment**: Preprint. Work in progress
- **Journal**: None
- **Summary**: This paper proposes a new strategy for learning powerful cross-modal embeddings for audio-to-video synchronization. Here, we set up the problem as one of cross-modal retrieval, where the objective is to find the most relevant audio segment given a short video clip. The method builds on the recent advances in learning representations from cross-modal self-supervision.   The main contributions of this paper are as follows: (1) we propose a new learning strategy where the embeddings are learnt via a multi-way matching problem, as opposed to a binary classification (matching or non-matching) problem as proposed by recent papers; (2) we demonstrate that performance of this method far exceeds the existing baselines on the synchronization task; (3) we use the learnt embeddings for visual speech recognition in self-supervision, and show that the performance matches the representations learnt end-to-end in a fully-supervised manner.



### On-field player workload exposure and knee injury risk monitoring via deep learning
- **Arxiv ID**: http://arxiv.org/abs/1809.08016v3
- **DOI**: 10.1016/j.jbiomech.2019.07.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08016v3)
- **Published**: 2018-09-21 10:09:48+00:00
- **Updated**: 2019-07-11 14:34:31+00:00
- **Authors**: William R. Johnson, Ajmal Mian, David G. Lloyd, Jacqueline A. Alderson
- **Comment**: None
- **Journal**: None
- **Summary**: In sports analytics, an understanding of accurate on-field 3D knee joint moments (KJM) could provide an early warning system for athlete workload exposure and knee injury risk. Traditionally, this analysis has relied on captive laboratory force plates and associated downstream biomechanical modeling, and many researchers have approached the problem of portability by extrapolating models built on linear statistics. An alternative approach would be to capitalize on recent advances in deep learning. In this study, using the pre-trained CaffeNet convolutional neural network (CNN) model, multivariate regression of marker-based motion capture to 3D KJM for three sports-related movement types were compared. The strongest overall mean correlation to source modeling of 0.8895 was achieved over the initial 33 % of stance phase for sidestepping. The accuracy of these mean predictions of the three critical KJM associated with anterior cruciate ligament (ACL) injury demonstrate the feasibility of on-field knee injury assessment using deep learning in lieu of laboratory embedded force plates. This multidisciplinary research approach significantly advances machine representation of real-world physical models with practical application for both community and professional level athletes.



### Non-Line-of-Sight Reconstruction using Efficient Transient Rendering
- **Arxiv ID**: http://arxiv.org/abs/1809.08044v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, I.3.3; I.3.5; I.3.7; I.3.8; I.4.5; I.4.8; I.6.3
- **Links**: [PDF](http://arxiv.org/pdf/1809.08044v2)
- **Published**: 2018-09-21 11:34:44+00:00
- **Updated**: 2019-10-23 11:41:21+00:00
- **Authors**: Julian Iseringhausen, Matthias B. Hullin
- **Comment**: None
- **Journal**: None
- **Summary**: Being able to see beyond the direct line of sight is an intriguing prospective and could benefit a wide variety of important applications. Recent work has demonstrated that time-resolved measurements of indirect diffuse light contain valuable information for reconstructing shape and reflectance properties of objects located around a corner. In this paper, we introduce a novel reconstruction scheme that, by design, produces solutions that are consistent with state-of-the-art physically-based rendering. Our method combines an efficient forward model (a custom renderer for time-resolved three-bounce indirect light transport) with an optimization framework to reconstruct object geometry in an analysis-by-synthesis sense. We evaluate our algorithm on a variety of synthetic and experimental input data, and show that it gracefully handles uncooperative scenes with high levels of noise or non-diffuse material reflectance.



### From 2D to 3D Geodesic-based Garment Matching
- **Arxiv ID**: http://arxiv.org/abs/1809.08064v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08064v1)
- **Published**: 2018-09-21 12:37:56+00:00
- **Updated**: 2018-09-21 12:37:56+00:00
- **Authors**: Meysam Madadi, Egils Avots, Sergio Escalera, Jordi Gonzalez, Xavier Baro, Gholamreza Anbarjafari
- **Comment**: None
- **Journal**: None
- **Summary**: A new approach for 2D to 3D garment retexturing is proposed based on Gaussian mixture models and thin plate splines (TPS). An automatically segmented garment of an individual is matched to a new source garment and rendered, resulting in augmented images in which the target garment has been retextured by using the texture of the source garment. We divide the problem into garment boundary matching based on Gaussian mixture models and then interpolate inner points using surface topology extracted through geodesic paths, which leads to a more realistic result than standard approaches. We evaluated and compared our system quantitatively by mean square error (MSE) and qualitatively using the mean opinion score (MOS), showing the benefits of the proposed methodology on our gathered dataset.



### Analysing object detectors from the perspective of co-occurring object categories
- **Arxiv ID**: http://arxiv.org/abs/1809.08132v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08132v1)
- **Published**: 2018-09-21 14:08:08+00:00
- **Updated**: 2018-09-21 14:08:08+00:00
- **Authors**: Csaba Nemes, Sandor Jordan
- **Comment**: accepted to 9th IEEE International Conference on Cognitive
  InfoCommunications
- **Journal**: None
- **Summary**: The accuracy of state-of-the-art Faster R-CNN and YOLO object detectors are evaluated and compared on a special masked MS COCO dataset to measure how much their predictions rely on contextual information encoded at object category level. Category level representation of context is motivated by the fact that it could be an adequate way to transfer knowledge between visual and non-visual domains. According to our measurements, current detectors usually do not build strong dependency on contextual information at category level, however, when they does, they does it in a similar way, suggesting that contextual dependence of object categories is an independent property that is relevant to be transferred.



### Dynamic Environment Mapping for Augmented Reality Applications on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1809.08134v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08134v1)
- **Published**: 2018-09-21 14:10:55+00:00
- **Updated**: 2018-09-21 14:10:55+00:00
- **Authors**: Rafael Monroy, Matis Hudon, Aljosa Smolic
- **Comment**: To be presented at VMV 2018 (Eurographics Digital Library)
- **Journal**: None
- **Summary**: Augmented Reality is a topic of foremost interest nowadays. Its main goal is to seamlessly blend virtual content in real-world scenes. Due to the lack of computational power in mobile devices, rendering a virtual object with high-quality, coherent appearance and in real-time, remains an area of active research. In this work, we present a novel pipeline that allows for coupled environment acquisition and virtual object rendering on a mobile device equipped with a depth sensor. While keeping human interaction to a minimum, our system can scan a real scene and project it onto a two-dimensional environment map containing RGB+Depth data. Furthermore, we define a set of criteria that allows for an adaptive update of the environment map to account for dynamic changes in the scene. Then, under the assumption of diffuse surfaces and distant illumination, our method exploits an analytic expression for the irradiance in terms of spherical harmonic coefficients, which leads to a very efficient rendering algorithm. We show that all the processes in our pipeline can be executed while maintaining an average frame rate of 31Hz on a mobile device.



### Exclusive Independent Probability Estimation using Deep 3D Fully Convolutional DenseNets: Application to IsoIntense Infant Brain MRI Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1809.08168v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08168v3)
- **Published**: 2018-09-21 15:08:31+00:00
- **Updated**: 2018-12-10 05:55:31+00:00
- **Authors**: Seyed Raein Hashemi, Sanjay P. Prabhu, Simon K. Warfield, Ali Gholipour
- **Comment**: None
- **Journal**: None
- **Summary**: The most recent fast and accurate image segmentation methods are built upon fully convolutional deep neural networks. In this paper, we propose new deep learning strategies for DenseNets to improve segmenting images with subtle differences in intensity values and features. We aim to segment brain tissue on infant brain MRI at about 6 months of age where white matter and gray matter of the developing brain show similar T1 and T2 relaxation times, thus appear to have similar intensity values on both T1- and T2-weighted MRI scans. Brain tissue segmentation at this age is, therefore, very challenging. To this end, we propose an exclusive multi-label training strategy to segment the mutually exclusive brain tissues with similarity loss functions that automatically balance the training based on class prevalence. Using our proposed training strategy based on similarity loss functions and patch prediction fusion we decrease the number of parameters in the network, reduce the complexity of the training process focusing the attention on less number of tasks, while mitigating the effects of data imbalance between labels and inaccuracies near patch borders. By taking advantage of these strategies we were able to perform fast image segmentation (90 seconds per 3D volume), using a network with less parameters than many state-of-the-art networks, overcoming issues such as 3Dvs2D training and large vs small patch size selection, while achieving the top performance in segmenting brain tissue among all methods tested in first and second round submissions of the isointense infant brain MRI segmentation (iSeg) challenge according to the official challenge test results. Our proposed strategy improves the training process through balanced training and by reducing its complexity while providing a trained model that works for any size input image and is fast and more accurate than many state-of-the-art methods.



### Image Denoising and Super-Resolution using Residual Learning of Deep Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1809.08229v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1809.08229v1)
- **Published**: 2018-09-21 17:58:22+00:00
- **Updated**: 2018-09-21 17:58:22+00:00
- **Authors**: Rohit Pardasani, Utkarsh Shreemali
- **Comment**: 5 pages, 12 figures
- **Journal**: None
- **Summary**: Image super-resolution and denoising are two important tasks in image processing that can lead to improvement in image quality. Image super-resolution is the task of mapping a low resolution image to a high resolution image whereas denoising is the task of learning a clean image from a noisy input. We propose and train a single deep learning network that we term as SuRDCNN (super-resolution and denoising convolutional neural network), to perform these two tasks simultaneously . Our model nearly replicates the architecture of existing state-of-the-art deep learning models for super-resolution and denoising. We use the proven strategy of residual learning, as supported by state-of-the-art networks in this domain. Our trained SuRDCNN is capable of super-resolving image in the presence of Gaussian noise, Poisson noise or any random combination of both of these noises.



### Global Weighted Average Pooling Bridges Pixel-level Localization and Image-level Classification
- **Arxiv ID**: http://arxiv.org/abs/1809.08264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08264v1)
- **Published**: 2018-09-21 18:29:13+00:00
- **Updated**: 2018-09-21 18:29:13+00:00
- **Authors**: Suo Qiu
- **Comment**: technical report
- **Journal**: None
- **Summary**: In this work, we first tackle the problem of simultaneous pixel-level localization and image-level classification with only image-level labels for fully convolutional network training. We investigate the global pooling method which plays a vital role in this task. Classical global max pooling and average pooling methods are hard to indicate the precise regions of objects. Therefore, we revisit the global weighted average pooling (GWAP) method for this task and propose the class-agnostic GWAP module and the class-specific GWAP module in this paper. We evaluate the classification and pixel-level localization ability on the ILSVRC benchmark dataset. Experimental results show that the proposed GWAP module can better capture the regions of the foreground objects. We further explore the knowledge transfer between the image classification task and the region-based object detection task. We propose a multi-task framework that combines our class-specific GWAP module with R-FCN. The framework is trained with few ground truth bounding boxes and large-scale image-level labels. We evaluate this framework on PASCAL VOC dataset. Experimental results show that this framework can use the data with only image-level labels to improve the generalization of the object detection model.



### GAPLE: Generalizable Approaching Policy LEarning for Robotic Object Searching in Indoor Environment
- **Arxiv ID**: http://arxiv.org/abs/1809.08287v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.08287v2)
- **Published**: 2018-09-21 19:54:27+00:00
- **Updated**: 2019-03-07 02:31:04+00:00
- **Authors**: Xin Ye, Zhe Lin, Joon-Young Lee, Jianming Zhang, Shibin Zheng, Yezhou Yang
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of learning a generalizable action policy for an intelligent agent to actively approach an object of interest in an indoor environment solely from its visual inputs. While scene-driven or recognition-driven visual navigation has been widely studied, prior efforts suffer severely from the limited generalization capability. In this paper, we first argue the object searching task is environment dependent while the approaching ability is general. To learn a generalizable approaching policy, we present a novel solution dubbed as GAPLE which adopts two channels of visual features: depth and semantic segmentation, as the inputs to the policy learning module. The empirical studies conducted on the House3D dataset as well as on a physical platform in a real world scenario validate our hypothesis, and we further provide in-depth qualitative analysis.



### Temporal Interpolation as an Unsupervised Pretraining Task for Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1809.08317v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08317v1)
- **Published**: 2018-09-21 21:30:18+00:00
- **Updated**: 2018-09-21 21:30:18+00:00
- **Authors**: Jonas Wulff, Michael J. Black
- **Comment**: 16 pages, 7 figures; accepted for publication at the German
  Conference for Pattern Recognition (GCPR) 2018
- **Journal**: None
- **Summary**: The difficulty of annotating training data is a major obstacle to using CNNs for low-level tasks in video. Synthetic data often does not generalize to real videos, while unsupervised methods require heuristic losses. Proxy tasks can overcome these issues, and start by training a network for a task for which annotation is easier or which can be trained unsupervised. The trained network is then fine-tuned for the original task using small amounts of ground truth data. Here, we investigate frame interpolation as a proxy task for optical flow. Using real movies, we train a CNN unsupervised for temporal interpolation. Such a network implicitly estimates motion, but cannot handle untextured regions. By fine-tuning on small amounts of ground truth flow, the network can learn to fill in homogeneous regions and compute full optical flow fields. Using this unsupervised pre-training, our network outperforms similar architectures that were trained supervised using synthetic optical flow.



### Recurrent Flow-Guided Semantic Forecasting
- **Arxiv ID**: http://arxiv.org/abs/1809.08318v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08318v2)
- **Published**: 2018-09-21 21:31:25+00:00
- **Updated**: 2018-11-21 18:13:47+00:00
- **Authors**: Adam M. Terwilliger, Garrick Brazil, Xiaoming Liu
- **Comment**: 10 pages, 5 figures, 8 tables, Accepted to WACV 2019
- **Journal**: None
- **Summary**: Understanding the world around us and making decisions about the future is a critical component to human intelligence. As autonomous systems continue to develop, their ability to reason about the future will be the key to their success. Semantic anticipation is a relatively under-explored area for which autonomous vehicles could take advantage of (e.g., forecasting pedestrian trajectories). Motivated by the need for real-time prediction in autonomous systems, we propose to decompose the challenging semantic forecasting task into two subtasks: current frame segmentation and future optical flow prediction. Through this decomposition, we built an efficient, effective, low overhead model with three main components: flow prediction network, feature-flow aggregation LSTM, and end-to-end learnable warp layer. Our proposed method achieves state-of-the-art accuracy on short-term and moving objects semantic forecasting while simultaneously reducing model parameters by up to 95% and increasing efficiency by greater than 40x.



### Unsupervised Image to Sequence Translation with Canvas-Drawer Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.08340v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08340v2)
- **Published**: 2018-09-21 23:16:44+00:00
- **Updated**: 2018-09-26 17:58:24+00:00
- **Authors**: Kevin Frans, Chin-Yi Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Encoding images as a series of high-level constructs, such as brush strokes or discrete shapes, can often be key to both human and machine understanding. In many cases, however, data is only available in pixel form. We present a method for generating images directly in a high-level domain (e.g. brush strokes), without the need for real pairwise data. Specifically, we train a "canvas" network to imitate the mapping of high-level constructs to pixels, followed by a high-level "drawing" network which is optimized through this mapping towards solving a desired image recreation or translation task. We successfully discover sequential vector representations of symbols, large sketches, and 3D objects, utilizing only pixel data. We display applications of our method in image segmentation, and present several ablation studies comparing various configurations.



