# Arxiv Papers in cs.CV on 2018-09-13
### A Less Biased Evaluation of Out-of-distribution Sample Detectors
- **Arxiv ID**: http://arxiv.org/abs/1809.04729v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.04729v2)
- **Published**: 2018-09-13 01:15:49+00:00
- **Updated**: 2019-08-20 17:46:05+00:00
- **Authors**: Alireza Shafaei, Mark Schmidt, James J. Little
- **Comment**: to appear in BMVC 2019; v2 is more compact, with more results
- **Journal**: None
- **Summary**: In the real world, a learning system could receive an input that is unlike anything it has seen during training. Unfortunately, out-of-distribution samples can lead to unpredictable behaviour. We need to know whether any given input belongs to the population distribution of the training/evaluation data to prevent unpredictable behaviour in deployed systems. A recent surge of interest in this problem has led to the development of sophisticated techniques in the deep learning literature. However, due to the absence of a standard problem definition or an exhaustive evaluation, it is not evident if we can rely on these methods. What makes this problem different from a typical supervised learning setting is that the distribution of outliers used in training may not be the same as the distribution of outliers encountered in the application. Classical approaches that learn inliers vs. outliers with only two datasets can yield optimistic results. We introduce OD-test, a three-dataset evaluation scheme as a more reliable strategy to assess progress on this problem. We present an exhaustive evaluation of a broad set of methods from related areas on image classification tasks. Contrary to the existing results, we show that for realistic applications of high-dimensional images the previous techniques have low accuracy and are not reliable in practice.



### Adapting Semantic Segmentation Models for Changes in Illumination and Camera Perspective
- **Arxiv ID**: http://arxiv.org/abs/1809.04730v1
- **DOI**: 10.1109/LRA.2019.2891027
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04730v1)
- **Published**: 2018-09-13 01:17:54+00:00
- **Updated**: 2018-09-13 01:17:54+00:00
- **Authors**: Wei Zhou, Alex Zyner, Stewart Worrall, Eduardo Nebot
- **Comment**: Submitted to IEEE Robotics and Automation Letters (RA-L) and 2019
  IEEE International Conference on Robotics and Automation (ICRA)
- **Journal**: None
- **Summary**: Semantic segmentation using deep neural networks has been widely explored to generate high-level contextual information for autonomous vehicles. To acquire a complete $180^\circ$ semantic understanding of the forward surroundings, we propose to stitch semantic images from multiple cameras with varying orientations. However, previously trained semantic segmentation models showed unacceptable performance after significant changes to the camera orientations and the lighting conditions. To avoid time-consuming hand labeling, we explore and evaluate the use of data augmentation techniques, specifically skew and gamma correction, from a practical real-world standpoint to extend the existing model and provide more robust performance. The presented experimental results have shown significant improvements with varying illumination and camera perspective changes.



### DispSegNet: Leveraging Semantics for End-to-End Learning of Disparity Estimation from Stereo Imagery
- **Arxiv ID**: http://arxiv.org/abs/1809.04734v2
- **DOI**: 10.1109/LRA.2019.2894913
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04734v2)
- **Published**: 2018-09-13 01:36:55+00:00
- **Updated**: 2019-01-15 19:07:31+00:00
- **Authors**: Junming Zhang, Katherine A. Skinner, Ram Vasudevan, Matthew Johnson-Roberson
- **Comment**: Add more description on the architecture of the model. Add more
  discussion on section IV-C. Fix typo in formula 6
- **Journal**: IEEE Robotics and Automation Letters, vol. 4, no. 2, pp.
  1162-1169, April 2019
- **Summary**: Recent work has shown that convolutional neural networks (CNNs) can be applied successfully in disparity estimation, but these methods still suffer from errors in regions of low-texture, occlusions and reflections. Concurrently, deep learning for semantic segmentation has shown great progress in recent years. In this paper, we design a CNN architecture that combines these two tasks to improve the quality and accuracy of disparity estimation with the help of semantic segmentation. Specifically, we propose a network structure in which these two tasks are highly coupled. One key novelty of this approach is the two-stage refinement process. Initial disparity estimates are refined with an embedding learned from the semantic segmentation branch of the network. The proposed model is trained using an unsupervised approach, in which images from one half of the stereo pair are warped and compared against images from the other camera. Another key advantage of the proposed approach is that a single network is capable of outputting disparity estimates and semantic labels. These outputs are of great use in autonomous vehicle operation; with real-time constraints being key, such performance improvements increase the viability of driving applications. Experiments on KITTI and Cityscapes datasets show that our model can achieve state-of-the-art results and that leveraging embedding learned from semantic segmentation improves the performance of disparity estimation.



### Adversarial Feature Sampling Learning for Efficient Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1809.04741v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04741v2)
- **Published**: 2018-09-13 02:06:18+00:00
- **Updated**: 2018-09-15 10:25:21+00:00
- **Authors**: Yingjie Yin, Lei Zhang, De Xu, Xingang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The tracking-by-detection framework usually consist of two stages: drawing samples around the target object in the first stage and classifying each sample as the target object or background in the second stage. Current popular trackers based on tracking-by-detection framework typically draw samples in the raw image as the inputs of deep convolution networks in the first stage, which usually results in high computational burden and low running speed. In this paper, we propose a new visual tracking method using sampling deep convolutional features to address this problem. Only one cropped image around the target object is input into the designed deep convolution network and the samples is sampled on the feature maps of the network by spatial bilinear resampling. In addition, a generative adversarial network is integrated into our network framework to augment positive samples and improve the tracking performance. Extensive experiments on benchmark datasets demonstrate that the proposed method achieves a comparable performance to state-of-the-art trackers and accelerates tracking-by-detection trackers based on raw-image samples effectively.



### Head Reconstruction from Internet Photos
- **Arxiv ID**: http://arxiv.org/abs/1809.04763v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04763v1)
- **Published**: 2018-09-13 04:13:26+00:00
- **Updated**: 2018-09-13 04:13:26+00:00
- **Authors**: Shu Liang, Linda G. Shapiro, Ira Kemelmacher-Shlizerman
- **Comment**: Published on ECCV 2016
- **Journal**: None
- **Summary**: 3D face reconstruction from Internet photos has recently produced exciting results. A person's face, e.g., Tom Hanks, can be modeled and animated in 3D from a completely uncalibrated photo collection. Most methods, however, focus solely on face area and mask out the rest of the head. This paper proposes that head modeling from the Internet is a problem we can solve. We target reconstruction of the rough shape of the head. Our method is to gradually "grow" the head mesh starting from the frontal face and extending to the rest of views using photometric stereo constraints. We call our method boundary-value growing algorithm. Results on photos of celebrities downloaded from the Internet are presented.



### 3D Face Hallucination from a Single Depth Frame
- **Arxiv ID**: http://arxiv.org/abs/1809.04764v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04764v1)
- **Published**: 2018-09-13 04:13:56+00:00
- **Updated**: 2018-09-13 04:13:56+00:00
- **Authors**: Shu Liang, Ira Kemelmacher-Shlizerman, Linda G. Shapiro
- **Comment**: published on 3Dv 2014
- **Journal**: None
- **Summary**: We present an algorithm that takes a single frame of a person's face from a depth camera, e.g., Kinect, and produces a high-resolution 3D mesh of the input face. We leverage a dataset of 3D face meshes of 1204 distinct individuals ranging from age 3 to 40, captured in a neutral expression. We divide the input depth frame into semantically significant regions (eyes, nose, mouth, cheeks) and search the database for the best matching shape per region. We further combine the input depth frame with the matched database shapes into a single mesh that results in a high-resolution shape of the input person. Our system is fully automatic and uses only depth data for matching, making it invariant to imaging conditions. We evaluate our results using ground truth shapes, as well as compare to state-of-the-art shape estimation methods. We demonstrate the robustness of our local matching approach with high-quality reconstruction of faces that fall outside of the dataset span, e.g., faces older than 40 years old, facial expressions, and different ethnicities.



### Video to Fully Automatic 3D Hair Model
- **Arxiv ID**: http://arxiv.org/abs/1809.04765v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1809.04765v1)
- **Published**: 2018-09-13 04:14:53+00:00
- **Updated**: 2018-09-13 04:14:53+00:00
- **Authors**: Shu Liang, Xiufeng Huang, Xianyu Meng, Kunyao Chen, Linda G. Shapiro, Ira Kemelmacher-Shlizerman
- **Comment**: supplementary video: https://www.youtube.com/watch?v=so_CMv7Xd40
- **Journal**: None
- **Summary**: Imagine taking a selfie video with your mobile phone and getting as output a 3D model of your head (face and 3D hair strands) that can be later used in VR, AR, and any other domain. State of the art hair reconstruction methods allow either a single photo (thus compromising 3D quality) or multiple views, but they require manual user interaction (manual hair segmentation and capture of fixed camera views that span full 360 degree). In this paper, we describe a system that can completely automatically create a reconstruction from any video (even a selfie video), and we don't require specific views, since taking your -90 degree, 90 degree, and full back views is not feasible in a selfie capture.   In the core of our system, in addition to the automatization components, hair strands are estimated and deformed in 3D (rather than 2D as in state of the art) thus enabling superior results. We provide qualitative, quantitative, and Mechanical Turk human studies that support the proposed system, and show results on a diverse variety of videos (8 different celebrity videos, 9 selfie mobile videos, spanning age, gender, hair length, type, and styling).



### Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations
- **Arxiv ID**: http://arxiv.org/abs/1809.04766v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04766v2)
- **Published**: 2018-09-13 04:19:26+00:00
- **Updated**: 2019-02-27 05:53:59+00:00
- **Authors**: Vladimir Nekrasov, Thanuja Dharmasiri, Andrew Spek, Tom Drummond, Chunhua Shen, Ian Reid
- **Comment**: The models are available here -
  https://github.com/drsleep/multi-task-refinenet; supplementary video here -
  https://youtu.be/qwShIBhaq8Y
- **Journal**: None
- **Summary**: Deployment of deep learning models in robotics as sensory information extractors can be a daunting task to handle, even using generic GPU cards. Here, we address three of its most prominent hurdles, namely, i) the adaptation of a single model to perform multiple tasks at once (in this work, we consider depth estimation and semantic segmentation crucial for acquiring geometric and semantic understanding of the scene), while ii) doing it in real-time, and iii) using asymmetric datasets with uneven numbers of annotations per each modality. To overcome the first two issues, we adapt a recently proposed real-time semantic segmentation network, making changes to further reduce the number of floating point operations. To approach the third issue, we embrace a simple solution based on hard knowledge distillation under the assumption of having access to a powerful `teacher' network. We showcase how our system can be easily extended to handle more tasks, and more datasets, all at once, performing depth estimation and segmentation both indoors and outdoors with a single model. Quantitatively, we achieve results equivalent to (or better than) current state-of-the-art approaches with one forward pass costing just 13ms and 6.5 GFLOPs on 640x480 inputs. This efficiency allows us to directly incorporate the raw predictions of our network into the SemanticFusion framework for dense 3D semantic reconstruction of the scene.



### Generative adversarial network-based image super-resolution using perceptual content losses
- **Arxiv ID**: http://arxiv.org/abs/1809.04783v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04783v2)
- **Published**: 2018-09-13 05:23:54+00:00
- **Updated**: 2018-09-21 07:30:56+00:00
- **Authors**: Manri Cheon, Jun-Hyuk Kim, Jun-Ho Choi, Jong-Seok Lee
- **Comment**: To appear in ECCV 2018 workshop. Won the 2nd place for Region 1 in
  the PIRM Challenge on Perceptual Super Resolution at ECCV 2018. Github at
  https://github.com/manricheon/eusr-pcl-tf
- **Journal**: None
- **Summary**: In this paper, we propose a deep generative adversarial network for super-resolution considering the trade-off between perception and distortion. Based on good performance of a recently developed model for super-resolution, i.e., deep residual network using enhanced upscale modules (EUSR), the proposed model is trained to improve perceptual performance with only slight increase of distortion. For this purpose, together with the conventional content loss, i.e., reconstruction loss such as L1 or L2, we consider additional losses in the training phase, which are the discrete cosine transform coefficients loss and differential content loss. These consider perceptual part in the content loss, i.e., consideration of proper high frequency components is helpful for the trade-off problem in super-resolution. The experimental results show that our proposed model has good performance for both perception and distortion, and is effective in perceptual super-resolution applications.



### Deep Learning-based Image Super-Resolution Considering Quantitative and Perceptual Quality
- **Arxiv ID**: http://arxiv.org/abs/1809.04789v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04789v2)
- **Published**: 2018-09-13 06:03:56+00:00
- **Updated**: 2019-04-19 05:23:55+00:00
- **Authors**: Jun-Ho Choi, Jun-Hyuk Kim, Manri Cheon, Jong-Seok Lee
- **Comment**: Won the 2nd place for Region 2 in the PIRM Challenge on Perceptual
  Super Resolution at ECCV 2018. GitHub at
  https://github.com/idearibosome/tf-perceptual-eusr
- **Journal**: None
- **Summary**: Recently, it has been shown that in super-resolution, there exists a tradeoff relationship between the quantitative and perceptual quality of super-resolved images, which correspond to the similarity to the ground-truth images and the naturalness, respectively. In this paper, we propose a novel super-resolution method that can improve the perceptual quality of the upscaled images while preserving the conventional quantitative performance. The proposed method employs a deep network for multi-pass upscaling in company with a discriminator network and two quantitative score predictor networks. Experimental results demonstrate that the proposed method achieves a good balance of the quantitative and perceptual quality, showing more satisfactory results than existing methods.



### Canonical and Compact Point Cloud Representation for Shape Classification
- **Arxiv ID**: http://arxiv.org/abs/1809.04820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04820v1)
- **Published**: 2018-09-13 08:11:18+00:00
- **Updated**: 2018-09-13 08:11:18+00:00
- **Authors**: Kent Fujiwara, Ikuro Sato, Mitsuru Ambai, Yuichi Yoshida, Yoshiaki Sakakura
- **Comment**: 16 pages, 5 figures
- **Journal**: None
- **Summary**: We present a novel compact point cloud representation that is inherently invariant to scale, coordinate change and point permutation. The key idea is to parametrize a distance field around an individual shape into a unique, canonical, and compact vector in an unsupervised manner. We firstly project a distance field to a $4$D canonical space using singular value decomposition. We then train a neural network for each instance to non-linearly embed its distance field into network parameters. We employ a bias-free Extreme Learning Machine (ELM) with ReLU activation units, which has scale-factor commutative property between layers. We demonstrate the descriptiveness of the instance-wise, shape-embedded network parameters by using them to classify shapes in $3$D datasets. Our learning-based representation requires minimal augmentation and simple neural networks, where previous approaches demand numerous representations to handle coordinate change and point permutation.



### Image Captioning based on Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.04835v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.04835v1)
- **Published**: 2018-09-13 08:40:21+00:00
- **Updated**: 2018-09-13 08:40:21+00:00
- **Authors**: Haichao Shi, Peng Li, Bo Wang, Zhenyu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently it has shown that the policy-gradient methods for reinforcement learning have been utilized to train deep end-to-end systems on natural language processing tasks. What's more, with the complexity of understanding image content and diverse ways of describing image content in natural language, image captioning has been a challenging problem to deal with. To the best of our knowledge, most state-of-the-art methods follow a pattern of sequential model, such as recurrent neural networks (RNN). However, in this paper, we propose a novel architecture for image captioning with deep reinforcement learning to optimize image captioning tasks. We utilize two networks called "policy network" and "value network" to collaboratively generate the captions of images. The experiments are conducted on Microsoft COCO dataset, and the experimental results have verified the effectiveness of the proposed method.



### Investigation of Multimodal Features, Classifiers and Fusion Methods for Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.06225v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1809.06225v1)
- **Published**: 2018-09-13 08:56:25+00:00
- **Updated**: 2018-09-13 08:56:25+00:00
- **Authors**: Zheng Lian, Ya Li, Jianhua Tao, Jian Huang
- **Comment**: 9 pages, 11 figures and 4 Tables. EmotiW2018 challenge
- **Journal**: None
- **Summary**: Automatic emotion recognition is a challenging task. In this paper, we present our effort for the audio-video based sub-challenge of the Emotion Recognition in the Wild (EmotiW) 2018 challenge, which requires participants to assign a single emotion label to the video clip from the six universal emotions (Anger, Disgust, Fear, Happiness, Sad and Surprise) and Neutral. The proposed multimodal emotion recognition system takes audio, video and text information into account. Except for handcraft features, we also extract bottleneck features from deep neutral networks (DNNs) via transfer learning. Both temporal classifiers and non-temporal classifiers are evaluated to obtain the best unimodal emotion classification result. Then possibilities are extracted and passed into the Beam Search Fusion (BS-Fusion). We test our method in the EmotiW 2018 challenge and we gain promising results. Compared with the baseline system, there is a significant improvement. We achieve 60.34% accuracy on the testing dataset, which is only 1.5% lower than the winner. It shows that our method is very competitive.



### On Offline Evaluation of Vision-based Driving Models
- **Arxiv ID**: http://arxiv.org/abs/1809.04843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04843v1)
- **Published**: 2018-09-13 09:03:47+00:00
- **Updated**: 2018-09-13 09:03:47+00:00
- **Authors**: Felipe Codevilla, Antonio M. López, Vladlen Koltun, Alexey Dosovitskiy
- **Comment**: Published at the ECCV 2018 conference
- **Journal**: None
- **Summary**: Autonomous driving models should ideally be evaluated by deploying them on a fleet of physical vehicles in the real world. Unfortunately, this approach is not practical for the vast majority of researchers. An attractive alternative is to evaluate models offline, on a pre-collected validation dataset with ground truth annotation. In this paper, we investigate the relation between various online and offline metrics for evaluation of autonomous driving models. We find that offline prediction error is not necessarily correlated with driving quality, and two models with identical prediction error can differ dramatically in their driving performance. We show that the correlation of offline evaluation with driving quality can be significantly improved by selecting an appropriate validation dataset and suitable offline metrics. The supplementary video can be viewed at https://www.youtube.com/watch?v=P8K8Z-iF0cY



### Sparse Label Smoothing Regularization for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1809.04976v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04976v3)
- **Published**: 2018-09-13 14:04:58+00:00
- **Updated**: 2019-03-05 06:50:44+00:00
- **Authors**: Jean-Paul Ainam, Ke Qin, Guisong Liu, Guangchun Luo
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Person re-identification (re-id) is a cross-camera retrieval task which establishes a correspondence between images of a person from multiple cameras. Deep Learning methods have been successfully applied to this problem and have achieved impressive results. However, these methods require a large amount of labeled training data. Currently labeled datasets in person re-id are limited in their scale and manual acquisition of such large-scale datasets from surveillance cameras is a tedious and labor-intensive task. In this paper, we propose a framework that performs intelligent data augmentation and assigns partial smoothing label to generated data. Our approach first exploits the clustering property of existing person re-id datasets to create groups of similar objects that model cross-view variations. Each group is then used to generate realistic images through adversarial training. Our aim is to emphasize feature similarity between generated samples and the original samples. Finally, we assign a non-uniform label distribution to the generated samples and define a regularized loss function for training. The proposed approach tackles two problems (1) how to efficiently use the generated data and (2) how to address the over-smoothness problem found in current regularization methods. Extensive experiments on four larges cale datasets show that our regularization method significantly improves the Re-ID accuracy compared to existing methods.



### Part-based Graph Convolutional Network for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.04983v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1809.04983v1)
- **Published**: 2018-09-13 14:22:58+00:00
- **Updated**: 2018-09-13 14:22:58+00:00
- **Authors**: Kalpit Thakkar, P J Narayanan
- **Comment**: Main: 13 pages, 3 figures, 2 tables. Supplementary: 5 pages, 3
  figures, 1 table. Accepted at BMVC 2018
- **Journal**: None
- **Summary**: Human actions comprise of joint motion of articulated body parts or `gestures'. Human skeleton is intuitively represented as a sparse graph with joints as nodes and natural connections between them as edges. Graph convolutional networks have been used to recognize actions from skeletal videos. We introduce a part-based graph convolutional network (PB-GCN) for this task, inspired by Deformable Part-based Models (DPMs). We divide the skeleton graph into four subgraphs with joints shared across them and learn a recognition model using a part-based graph convolutional network. We show that such a model improves performance of recognition, compared to a model using entire skeleton graph. Instead of using 3D joint coordinates as node features, we show that using relative coordinates and temporal displacements boosts performance. Our model achieves state-of-the-art performance on two challenging benchmark datasets NTURGB+D and HDM05, for skeletal action recognition.



### SiftingGAN: Generating and Sifting Labeled Samples to Improve the Remote Sensing Image Scene Classification Baseline in vitro
- **Arxiv ID**: http://arxiv.org/abs/1809.04985v4
- **DOI**: 10.1109/LGRS.2018.2890413
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04985v4)
- **Published**: 2018-09-13 14:26:12+00:00
- **Updated**: 2018-11-30 16:28:26+00:00
- **Authors**: Dongao Ma, Ping Tang, Lijun Zhao
- **Comment**: 5 pages, 5 figures, 1 tables
- **Journal**: None
- **Summary**: Lack of annotated samples greatly restrains the direct application of deep learning in remote sensing image scene classification. Although researches have been done to tackle this issue by data augmentation with various image transformation operations, they are still limited in quantity and diversity. Recently, the advent of the unsupervised learning based generative adversarial networks (GANs) bring us a new way to generate augmented samples. However, such GAN-generated samples are currently only served for training GANs model itself and for improving the performance of the discriminator in GANs internally (in vivo). It becomes a question of serious doubt whether the GAN-generated samples can help better improve the scene classification performance of other deep learning networks (in vitro), compared with the widely used transformed samples. To answer this question, this paper proposes a SiftingGAN approach to generate more numerous, more diverse and more authentic labeled samples for data augmentation. SiftingGAN extends traditional GAN framework with an Online-Output method for sample generation, a Generative-Model-Sifting method for model sifting, and a Labeled-Sample-Discriminating method for sample sifting. Experiments on the well-known AID dataset demonstrate that the proposed SiftingGAN method can not only effectively improve the performance of the scene classification baseline that is achieved without data augmentation, but also significantly excels the comparison methods based on traditional geometric/radiometric transformation operations.



### Synthetic Occlusion Augmentation with Volumetric Heatmaps for the 2018 ECCV PoseTrack Challenge on 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1809.04987v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1809.04987v3)
- **Published**: 2018-09-13 14:27:15+00:00
- **Updated**: 2018-11-06 15:02:54+00:00
- **Authors**: István Sárándi, Timm Linder, Kai O. Arras, Bastian Leibe
- **Comment**: Extended abstract for the 2018 ECCV PoseTrack Workshop, updated with
  full result tables
- **Journal**: None
- **Summary**: In this paper we present our winning entry at the 2018 ECCV PoseTrack Challenge on 3D human pose estimation. Using a fully-convolutional backbone architecture, we obtain volumetric heatmaps per body joint, which we convert to coordinates using soft-argmax. Absolute person center depth is estimated by a 1D heatmap prediction head. The coordinates are back-projected to 3D camera space, where we minimize the L1 loss. Key to our good results is the training data augmentation with randomly placed occluders from the Pascal VOC dataset. In addition to reaching first place in the Challenge, our method also surpasses the state-of-the-art on the full Human3.6M benchmark among methods that use no additional pose datasets in training. Code for applying synthetic occlusions is availabe at https://github.com/isarandi/synthetic-occlusion.



### Efficient Graph Cut Optimization for Full CRFs with Quantized Edges
- **Arxiv ID**: http://arxiv.org/abs/1809.04995v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04995v1)
- **Published**: 2018-09-13 14:46:23+00:00
- **Updated**: 2018-09-13 14:46:23+00:00
- **Authors**: Olga Veksler
- **Comment**: None
- **Journal**: None
- **Summary**: Fully connected pairwise Conditional Random Fields (Full-CRF) with Gaussian edge weights can achieve superior results compared to sparsely connected CRFs. However, traditional methods for Full-CRFs are too expensive. Previous work develops efficient approximate optimization based on mean field inference, which is a local optimization method and can be far from the optimum. We propose efficient and effective optimization based on graph cuts for Full-CRFs with quantized edge weights. To quantize edge weights, we partition the image into superpixels and assume that the weight of an edge between any two pixels depends only on the superpixels these pixels belong to. Our quantized edge CRF is an approximation to the Gaussian edge CRF, and gets closer to it as superpixel size decreases. Being an approximation, our model offers an intuition about the regularization properties of the Guassian edge Full-CRF. For efficient inference, we first consider the two-label case and develop an approximate method based on transforming the original problem into a smaller domain. Then we handle multi-label CRF by showing how to implement expansion moves. In both binary and multi-label cases, our solutions have significantly lower energy compared to that of mean field inference. We also show the effectiveness of our approach on semantic segmentation task.



### Linear and Deformable Image Registration with 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.06226v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.06226v1)
- **Published**: 2018-09-13 14:56:44+00:00
- **Updated**: 2018-09-13 14:56:44+00:00
- **Authors**: Stergios Christodoulidis, Mihir Sahasrabudhe, Maria Vakalopoulou, Guillaume Chassagnon, Marie-Pierre Revel, Stavroula Mougiakakou, Nikos Paragios
- **Comment**: None
- **Journal**: None
- **Summary**: Image registration and in particular deformable registration methods are pillars of medical imaging. Inspired by the recent advances in deep learning, we propose in this paper, a novel convolutional neural network architecture that couples linear and deformable registration within a unified architecture endowed with near real-time performance. Our framework is modular with respect to the global transformation component, as well as with respect to the similarity function while it guarantees smooth displacement fields. We evaluate the performance of our network on the challenging problem of MRI lung registration, and demonstrate superior performance with respect to state of the art elastic registration methods. The proposed deformation (between inspiration & expiration) was considered within a clinically relevant task of interstitial lung disease (ILD) classification and showed promising results.



### Discovering Features in Sr$_{14}$Cu$_{24}$O$_{41}$ Neutron Single Crystal Diffraction Data by Cluster Analysis
- **Arxiv ID**: http://arxiv.org/abs/1809.05039v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05039v1)
- **Published**: 2018-09-13 16:16:36+00:00
- **Updated**: 2018-09-13 16:16:36+00:00
- **Authors**: Yawei Hui, Yaohua Liu, Byung-Hoon Park
- **Comment**: None
- **Journal**: None
- **Summary**: To address the SMC'18 data challenge, "Discovering Features in Sr$_{14}$Cu$_{24}$O$_{41}$", we have used the clustering algorithm "DBSCAN" to separate the diffuse scattering features from the Bragg peaks, which takes into account both spatial and photometric information in the dataset during in the clustering process. We find that, in additional to highly localized Bragg peaks, there exists broad diffuse scattering patterns consisting of distinguishable geometries. Besides these two distinctive features, we also identify a third distinguishable feature submerged in the low signal-to-noise region in the reciprocal space, whose origin remains an open question.



### Learning to Group and Label Fine-Grained Shape Components
- **Arxiv ID**: http://arxiv.org/abs/1809.05050v1
- **DOI**: 10.1145/3272127.3275009
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.05050v1)
- **Published**: 2018-09-13 16:31:43+00:00
- **Updated**: 2018-09-13 16:31:43+00:00
- **Authors**: Xiaogang Wang, Bin Zhou, Haiyue Fang, Xiaowu Chen, Qinping Zhao, Kai Xu
- **Comment**: Accepted to SIGGRAPH Asia 2018. Corresponding Author: Kai Xu
  (kevin.kai.xu@gmail.com)
- **Journal**: ACM Transactions on Graphics, 2018
- **Summary**: A majority of stock 3D models in modern shape repositories are assembled with many fine-grained components. The main cause of such data form is the component-wise modeling process widely practiced by human modelers. These modeling components thus inherently reflect some function-based shape decomposition the artist had in mind during modeling. On the other hand, modeling components represent an over-segmentation since a functional part is usually modeled as a multi-component assembly. Based on these observations, we advocate that labeled segmentation of stock 3D models should not overlook the modeling components and propose a learning solution to grouping and labeling of the fine-grained components. However, directly characterizing the shape of individual components for the purpose of labeling is unreliable, since they can be arbitrarily tiny and semantically meaningless. We propose to generate part hypotheses from the components based on a hierarchical grouping strategy, and perform labeling on those part groups instead of directly on the components. Part hypotheses are mid-level elements which are more probable to carry semantic information. A multiscale 3D convolutional neural network is trained to extract context-aware features for the hypotheses. To accomplish a labeled segmentation of the whole shape, we formulate higher-order conditional random fields (CRFs) to infer an optimal label assignment for all components. Extensive experiments demonstrate that our method achieves significantly robust labeling results on raw 3D models from public shape repositories. Our work also contributes the first benchmark for component-wise labeling.



### Improving Reinforcement Learning Based Image Captioning with Natural Language Prior
- **Arxiv ID**: http://arxiv.org/abs/1809.06227v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.06227v1)
- **Published**: 2018-09-13 17:21:56+00:00
- **Updated**: 2018-09-13 17:21:56+00:00
- **Authors**: Tszhang Guo, Shiyu Chang, Mo Yu, Kun Bai
- **Comment**: 8 pages, 5 figures, EMNLP2018
- **Journal**: None
- **Summary**: Recently, Reinforcement Learning (RL) approaches have demonstrated advanced performance in image captioning by directly optimizing the metric used for testing. However, this shaped reward introduces learning biases, which reduces the readability of generated text. In addition, the large sample space makes training unstable and slow. To alleviate these issues, we propose a simple coherent solution that constrains the action space using an n-gram language prior. Quantitative and qualitative evaluations on benchmarks show that RL with the simple add-on module performs favorably against its counterpart in terms of both readability and speed of convergence. Human evaluation results show that our model is more human readable and graceful. The implementation will become publicly available upon the acceptance of the paper.



### Seeing Tree Structure from Vibration
- **Arxiv ID**: http://arxiv.org/abs/1809.05067v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1809.05067v1)
- **Published**: 2018-09-13 17:23:08+00:00
- **Updated**: 2018-09-13 17:23:08+00:00
- **Authors**: Tianfan Xue, Jiajun Wu, Zhoutong Zhang, Chengkai Zhang, Joshua B. Tenenbaum, William T. Freeman
- **Comment**: ECCV 2018. The first two authors contributed equally to this work.
  Project page: http://tree.csail.mit.edu/
- **Journal**: None
- **Summary**: Humans recognize object structure from both their appearance and motion; often, motion helps to resolve ambiguities in object structure that arise when we observe object appearance only. There are particular scenarios, however, where neither appearance nor spatial-temporal motion signals are informative: occluding twigs may look connected and have almost identical movements, though they belong to different, possibly disconnected branches. We propose to tackle this problem through spectrum analysis of motion signals, because vibrations of disconnected branches, though visually similar, often have distinctive natural frequencies. We propose a novel formulation of tree structure based on a physics-based link model, and validate its effectiveness by theoretical analysis, numerical simulation, and empirical experiments. With this formulation, we use nonparametric Bayesian inference to reconstruct tree structure from both spectral vibration signals and appearance cues. Our model performs well in recognizing hierarchical tree structure from real-world videos of trees and vessels.



### Learning Shape Priors for Single-View 3D Completion and Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1809.05068v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1809.05068v1)
- **Published**: 2018-09-13 17:23:13+00:00
- **Updated**: 2018-09-13 17:23:13+00:00
- **Authors**: Jiajun Wu, Chengkai Zhang, Xiuming Zhang, Zhoutong Zhang, William T. Freeman, Joshua B. Tenenbaum
- **Comment**: ECCV 2018. The first two authors contributed equally to this work.
  Project page: http://shapehd.csail.mit.edu/
- **Journal**: None
- **Summary**: The problem of single-view 3D shape completion or reconstruction is challenging, because among the many possible shapes that explain an observation, most are implausible and do not correspond to natural objects. Recent research in the field has tackled this problem by exploiting the expressiveness of deep convolutional networks. In fact, there is another level of ambiguity that is often overlooked: among plausible shapes, there are still multiple shapes that fit the 2D image equally well; i.e., the ground truth shape is non-deterministic given a single-view input. Existing fully supervised approaches fail to address this issue, and often produce blurry mean shapes with smooth surfaces but no fine details.   In this paper, we propose ShapeHD, pushing the limit of single-view shape completion and reconstruction by integrating deep generative models with adversarially learned shape priors. The learned priors serve as a regularizer, penalizing the model only if its output is unrealistic, not if it deviates from the ground truth. Our design thus overcomes both levels of ambiguity aforementioned. Experiments demonstrate that ShapeHD outperforms state of the art by a large margin in both shape completion and shape reconstruction on multiple real datasets.



### Physical Primitive Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1809.05070v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1809.05070v1)
- **Published**: 2018-09-13 17:23:20+00:00
- **Updated**: 2018-09-13 17:23:20+00:00
- **Authors**: Zhijian Liu, William T. Freeman, Joshua B. Tenenbaum, Jiajun Wu
- **Comment**: ECCV 2018. Project page: http://ppd.csail.mit.edu/
- **Journal**: None
- **Summary**: Objects are made of parts, each with distinct geometry, physics, functionality, and affordances. Developing such a distributed, physical, interpretable representation of objects will facilitate intelligent agents to better explore and interact with the world. In this paper, we study physical primitive decomposition---understanding an object through its components, each with physical and geometric attributes. As annotated data for object parts and physics are rare, we propose a novel formulation that learns physical primitives by explaining both an object's appearance and its behaviors in physical events. Our model performs well on block towers and tools in both synthetic and real scenarios; we also demonstrate that visual and physical observations often provide complementary signals. We further present ablation and behavioral studies to better understand our model and contrast it with human performance.



### Computer Vision-aided Atom Tracking in STEM Imaging
- **Arxiv ID**: http://arxiv.org/abs/1809.05076v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05076v1)
- **Published**: 2018-09-13 17:33:18+00:00
- **Updated**: 2018-09-13 17:33:18+00:00
- **Authors**: Yawei Hui, Yaohua Liu
- **Comment**: None
- **Journal**: None
- **Summary**: To address the SMC'17 data challenge -- "Data mining atomically resolved images for material properties", we first used the classic "blob detection" algorithms developed in computer vision to identify all atom centers in each STEM image frame. With the help of nearest neighbor analysis, we then found and labeled every atom center common to all the STEM frames and tracked their movements through the given time interval for both Molybdenum or Selenium atoms.



### Defensive Dropout for Hardening Deep Neural Networks under Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1809.05165v1
- **DOI**: 10.1145/3240765.3264699
- **Categories**: **cs.CR**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.05165v1)
- **Published**: 2018-09-13 20:26:32+00:00
- **Updated**: 2018-09-13 20:26:32+00:00
- **Authors**: Siyue Wang, Xiao Wang, Pu Zhao, Wujie Wen, David Kaeli, Peter Chin, Xue Lin
- **Comment**: Accepted as conference paper on ICCAD 2018
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are known vulnerable to adversarial attacks. That is, adversarial examples, obtained by adding delicately crafted distortions onto original legal inputs, can mislead a DNN to classify them as any target labels. This work provides a solution to hardening DNNs under adversarial attacks through defensive dropout. Besides using dropout during training for the best test accuracy, we propose to use dropout also at test time to achieve strong defense effects. We consider the problem of building robust DNNs as an attacker-defender two-player game, where the attacker and the defender know each others' strategies and try to optimize their own strategies towards an equilibrium. Based on the observations of the effect of test dropout rate on test accuracy and attack success rate, we propose a defensive dropout algorithm to determine an optimal test dropout rate given the neural network model and the attacker's strategy for generating adversarial examples.We also investigate the mechanism behind the outstanding defense effects achieved by the proposed defensive dropout. Comparing with stochastic activation pruning (SAP), another defense method through introducing randomness into the DNN model, we find that our defensive dropout achieves much larger variances of the gradients, which is the key for the improved defense effects (much lower attack success rate). For example, our defensive dropout can reduce the attack success rate from 100% to 13.89% under the currently strongest attack i.e., C&W attack on MNIST dataset.



### GANs for Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1809.06222v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.06222v3)
- **Published**: 2018-09-13 21:38:29+00:00
- **Updated**: 2019-10-09 06:31:07+00:00
- **Authors**: Salome Kazeminia, Christoph Baur, Arjan Kuijper, Bram van Ginneken, Nassir Navab, Shadi Albarqouni, Anirban Mukhopadhyay
- **Comment**: Salome Kazeminia and Christoph Baur contributed equally to this work
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) and their extensions have carved open many exciting ways to tackle well known and challenging medical image analysis problems such as medical image de-noising, reconstruction, segmentation, data simulation, detection or classification. Furthermore, their ability to synthesize images at unprecedented levels of realism also gives hope that the chronic scarcity of labeled data in the medical field can be resolved with the help of these generative models. In this review paper, a broad overview of recent literature on GANs for medical applications is given, the shortcomings and opportunities of the proposed methods are thoroughly discussed and potential future work is elaborated. We review the most relevant papers published until the submission date. For quick access, important details such as the underlying method, datasets and performance are tabulated. An interactive visualization which categorizes all papers to keep the review alive, is available at http://livingreview.in.tum.de/GANs_for_Medical_Applications.



### A Time Series Graph Cut Image Segmentation Scheme for Liver Tumors
- **Arxiv ID**: http://arxiv.org/abs/1809.05210v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05210v1)
- **Published**: 2018-09-13 23:56:24+00:00
- **Updated**: 2018-09-13 23:56:24+00:00
- **Authors**: Laramie Paxton, Yufeng Cao, Kevin R. Vixie, Yuan Wang, Brian Hobbs, Chaan Ng
- **Comment**: Image processing; image analysis; medical imaging
- **Journal**: None
- **Summary**: Tumor detection in biomedical imaging is a time-consuming process for medical professionals and is not without errors. Thus in recent decades, researchers have developed algorithmic techniques for image processing using a wide variety of mathematical methods, such as statistical modeling, variational techniques, and machine learning. In this paper, we propose a semi-automatic method for liver segmentation of 2D CT scans into three labels denoting healthy, vessel, or tumor tissue based on graph cuts. First, we create a feature vector for each pixel in a novel way that consists of the 59 intensity values in the time series data and propose a simplified perimeter cost term in the energy functional. We normalize the data and perimeter terms in the functional to expedite the graph cut without having to optimize the scaling parameter $\lambda$. In place of a training process, predetermined tissue means are computed based on sample regions identified by expert radiologists. The proposed method also has the advantage of being relatively simple to implement computationally. It was evaluated against the ground truth on a clinical CT dataset of 10 tumors and yielded segmentations with a mean Dice similarity coefficient (DSC) of .77 and mean volume overlap error (VOE) of 36.7%. The average processing time was 1.25 minutes per slice.



