# Arxiv Papers in cs.CV on 2018-09-05
### BOLD5000: A public fMRI dataset of 5000 images
- **Arxiv ID**: http://arxiv.org/abs/1809.01281v1
- **DOI**: 10.1038/s41597-019-0052-3
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.01281v1)
- **Published**: 2018-09-05 00:50:34+00:00
- **Updated**: 2018-09-05 00:50:34+00:00
- **Authors**: Nadine Chang, John A. Pyles, Abhinav Gupta, Michael J. Tarr, Elissa M. Aminoff
- **Comment**: Currently in submission to Scientific Data
- **Journal**: None
- **Summary**: Vision science, particularly machine vision, has been revolutionized by introducing large-scale image datasets and statistical learning approaches. Yet, human neuroimaging studies of visual perception still rely on small numbers of images (around 100) due to time-constrained experimental procedures. To apply statistical learning approaches that integrate neuroscience, the number of images used in neuroimaging must be significantly increased. We present BOLD5000, a human functional MRI (fMRI) study that includes almost 5,000 distinct images depicting real-world scenes. Beyond dramatically increasing image dataset size relative to prior fMRI studies, BOLD5000 also accounts for image diversity, overlapping with standard computer vision datasets by incorporating images from the Scene UNderstanding (SUN), Common Objects in Context (COCO), and ImageNet datasets. The scale and diversity of these image datasets, combined with a slow event-related fMRI design, enable fine-grained exploration into the neural representation of a wide range of visual features, categories, and semantics. Concurrently, BOLD5000 brings us closer to realizing Marr's dream of a singular vision science - the intertwined study of biological and computer vision.



### A Robotic Auto-Focus System based on Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.03314v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1809.03314v1)
- **Published**: 2018-09-05 02:14:50+00:00
- **Updated**: 2018-09-05 02:14:50+00:00
- **Authors**: Xiaofan Yu, Runze Yu, Jingsong Yang, Xiaohui Duan
- **Comment**: To Appear at ICARCV 2018
- **Journal**: None
- **Summary**: Considering its advantages in dealing with high-dimensional visual input and learning control policies in discrete domain, Deep Q Network (DQN) could be an alternative method of traditional auto-focus means in the future. In this paper, based on Deep Reinforcement Learning, we propose an end-to-end approach that can learn auto-focus policies from visual input and finish at a clear spot automatically. We demonstrate that our method - discretizing the action space with coarse to fine steps and applying DQN is not only a solution to auto-focus but also a general approach towards vision-based control problems. Separate phases of training in virtual and real environments are applied to obtain an effective model. Virtual experiments, which are carried out after the virtual training phase, indicates that our method could achieve 100% accuracy on a certain view with different focus range. Further training on real robots could eliminate the deviation between the simulator and real scenario, leading to reliable performances in real applications.



### Reconstruction and Registration of Large-Scale Medical Scene Using Point Clouds Data from Different Modalities
- **Arxiv ID**: http://arxiv.org/abs/1809.01318v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01318v1)
- **Published**: 2018-09-05 04:32:42+00:00
- **Updated**: 2018-09-05 04:32:42+00:00
- **Authors**: Ke Wang, Han Song, Jiahui Zhang, Xinran Zhang, Hongen Liao
- **Comment**: 2 pages, 4 figures, submitted to ACCAS
- **Journal**: None
- **Summary**: Sensing the medical scenario can ensure the safety during the surgical operations. So, in this regard, a monitor platform which can obtain the accurate location information of the surgery room is desperately needed. Compared to 2D camera image, 3D data contains more information of distance and direction. Therefore, 3D sensors are more suitable to be used in surgical scene monitoring. However, each 3D sensor has its own limitations. For example, Lidar (Light Detection and Ranging) can detect large-scale environment with high precision, but the point clouds or depth maps are very sparse. As for commodity RGBD sensors, such as Kinect, can accurately capture denser data, but limited to a small range from 0.5 to 4.5m. So, a proper method which can address these problems for fusing different modalities data is important. In this paper, we proposed a method which can fuse different modalities 3D data to get a large-scale and dense point cloud. The key contributions of our work are as follows. First, we proposed a 3D data collecting system to reconstruct the medical scenes. By fusing the Lidar and Kinect data, a large-scale medical scene with more details can be reconstructed. Second, we proposed a location-based fast point clouds registration algorithm to deal with different modality datasets.



### ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1809.01330v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01330v1)
- **Published**: 2018-09-05 05:15:14+00:00
- **Updated**: 2018-09-05 05:15:14+00:00
- **Authors**: Hongyang Gao, Zhengyang Wang, Shuiwang Ji
- **Comment**: 10 pages, NIPS18
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have shown great capability of solving various artificial intelligence tasks. However, the increasing model size has raised challenges in employing them in resource-limited applications. In this work, we propose to compress deep models by using channel-wise convolutions, which re- place dense connections among feature maps with sparse ones in CNNs. Based on this novel operation, we build light-weight CNNs known as ChannelNets. Channel- Nets use three instances of channel-wise convolutions; namely group channel-wise convolutions, depth-wise separable channel-wise convolutions, and the convolu- tional classification layer. Compared to prior CNNs designed for mobile devices, ChannelNets achieve a significant reduction in terms of the number of parameters and computational cost without loss in accuracy. Notably, our work represents the first attempt to compress the fully-connected classification layer, which usually accounts for about 25% of total parameters in compact CNNs. Experimental results on the ImageNet dataset demonstrate that ChannelNets achieve consistently better performance compared to prior methods.



### Localizing Moments in Video with Temporal Language
- **Arxiv ID**: http://arxiv.org/abs/1809.01337v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1809.01337v1)
- **Published**: 2018-09-05 05:58:47+00:00
- **Updated**: 2018-09-05 05:58:47+00:00
- **Authors**: Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, Bryan Russell
- **Comment**: EMNLP 2018
- **Journal**: None
- **Summary**: Localizing moments in a longer video via natural language queries is a new, challenging task at the intersection of language and video understanding. Though moment localization with natural language is similar to other language and vision tasks like natural language object retrieval in images, moment localization offers an interesting opportunity to model temporal dependencies and reasoning in text. We propose a new model that explicitly reasons about different temporal segments in a video, and shows that temporal context is important for localizing phrases which include temporal language. To benchmark whether our model, and other recent video localization models, can effectively reason about temporal language, we collect the novel TEMPOral reasoning in video and language (TEMPO) dataset. Our dataset consists of two parts: a dataset with real videos and template sentences (TEMPO - Template Language) which allows for controlled studies on temporal language, and a human language dataset which consists of temporal sentences annotated by humans (TEMPO - Human Language).



### Retinal Vessel Segmentation under Extreme Low Annotation: A Generative Adversarial Network Approach
- **Arxiv ID**: http://arxiv.org/abs/1809.01348v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01348v1)
- **Published**: 2018-09-05 06:31:56+00:00
- **Updated**: 2018-09-05 06:31:56+00:00
- **Authors**: Avisek Lahiri, Vineet Jain, Arnab Mondal, Prabir Kumar Biswas
- **Comment**: * First 3 authors contributed equally
- **Journal**: None
- **Summary**: Contemporary deep learning based medical image segmentation algorithms require hours of annotation labor by domain experts. These data hungry deep models perform sub-optimally in the presence of limited amount of labeled data. In this paper, we present a data efficient learning framework using the recent concept of Generative Adversarial Networks; this allows a deep neural network to perform significantly better than its fully supervised counterpart in low annotation regime. The proposed method is an extension of our previous work with the addition of a new unsupervised adversarial loss and a structured prediction based architecture. To the best of our knowledge, this work is the first demonstration of an adversarial framework based structured prediction model for medical image segmentation. Though generic, we apply our method for segmentation of blood vessels in retinal fundus images. We experiment with extreme low annotation budget (0.8 - 1.6% of contemporary annotation size). On DRIVE and STARE datasets, the proposed method outperforms our previous method and other fully supervised benchmark models by significant margins especially with very low number of annotated examples. In addition, our systematic ablation studies suggest some key recipes for successfully training GAN based semi-supervised algorithms with an encoder-decoder style network architecture.



### Semantic Human Matting
- **Arxiv ID**: http://arxiv.org/abs/1809.01354v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.01354v2)
- **Published**: 2018-09-05 06:50:24+00:00
- **Updated**: 2018-09-18 12:36:31+00:00
- **Authors**: Quan Chen, Tiezheng Ge, Yanyu Xu, Zhiqiang Zhang, Xinxin Yang, Kun Gai
- **Comment**: ACM Multimedia 2018
- **Journal**: None
- **Summary**: Human matting, high quality extraction of humans from natural images, is crucial for a wide variety of applications. Since the matting problem is severely under-constrained, most previous methods require user interactions to take user designated trimaps or scribbles as constraints. This user-in-the-loop nature makes them difficult to be applied to large scale data or time-sensitive scenarios. In this paper, instead of using explicit user input constraints, we employ implicit semantic constraints learned from data and propose an automatic human matting algorithm (SHM). SHM is the first algorithm that learns to jointly fit both semantic information and high quality details with deep networks. In practice, simultaneously learning both coarse semantics and fine details is challenging. We propose a novel fusion strategy which naturally gives a probabilistic estimation of the alpha matte. We also construct a very large dataset with high quality annotations consisting of 35,513 unique foregrounds to facilitate the learning and evaluation of human matting. Extensive experiments on this dataset and plenty of real images show that SHM achieves comparable results with state-of-the-art interactive matting methods.



### A Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1809.01361v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01361v3)
- **Published**: 2018-09-05 07:39:59+00:00
- **Updated**: 2018-10-28 05:08:09+00:00
- **Authors**: Alexander H. Liu, Yen-Cheng Liu, Yu-Ying Yeh, Yu-Chiang Frank Wang
- **Comment**: NIPS 2018
- **Journal**: None
- **Summary**: We present a novel and unified deep learning framework which is capable of learning domain-invariant representation from data across multiple domains. Realized by adversarial training with additional ability to exploit domain-specific information, the proposed network is able to perform continuous cross-domain image translation and manipulation, and produces desirable output images accordingly. In addition, the resulting feature representation exhibits superior performance of unsupervised domain adaptation, which also verifies the effectiveness of the proposed model in learning disentangled features for describing cross-domain data.



### Towards a Better Match in Siamese Network Based Visual Object Tracker
- **Arxiv ID**: http://arxiv.org/abs/1809.01368v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01368v1)
- **Published**: 2018-09-05 07:53:37+00:00
- **Updated**: 2018-09-05 07:53:37+00:00
- **Authors**: Anfeng He, Chong Luo, Xinmei Tian, Wenjun Zeng
- **Comment**: This paper is accepted by ECCV Visual Object Tracking Challenge
  Workshop VOT2018
- **Journal**: None
- **Summary**: Recently, Siamese network based trackers have received tremendous interest for their fast tracking speed and high performance. Despite the great success, this tracking framework still suffers from several limitations. First, it cannot properly handle large object rotation. Second, tracking gets easily distracted when the background contains salient objects. In this paper, we propose two simple yet effective mechanisms, namely angle estimation and spatial masking, to address these issues. The objective is to extract more representative features so that a better match can be obtained between the same object from different frames. The resulting tracker, named Siam-BM, not only significantly improves the tracking performance, but more importantly maintains the realtime capability. Evaluations on the VOT2017 dataset show that Siam-BM achieves an EAO of 0.335, which makes it the best-performing realtime tracker to date.



### Temporally Coherent Video Harmonization Using Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.01372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01372v1)
- **Published**: 2018-09-05 08:01:15+00:00
- **Updated**: 2018-09-05 08:01:15+00:00
- **Authors**: Haozhi Huang, Senzhe Xu, Junxiong Cai, Wei Liu, Shimin Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Compositing is one of the most important editing operations for images and videos. The process of improving the realism of composite results is often called harmonization. Previous approaches for harmonization mainly focus on images. In this work, we take one step further to attack the problem of video harmonization. Specifically, we train a convolutional neural network in an adversarial way, exploiting a pixel-wise disharmony discriminator to achieve more realistic harmonized results and introducing a temporal loss to increase temporal consistency between consecutive harmonized frames. Thanks to the pixel-wise disharmony discriminator, we are also able to relieve the need of input foreground masks. Since existing video datasets which have ground-truth foreground masks and optical flows are not sufficiently large, we propose a simple yet efficient method to build up a synthetic dataset supporting supervised training of the proposed adversarial network. Experiments show that training on our synthetic dataset generalizes well to the real-world composite dataset. Also, our method successfully incorporates temporal consistency during training and achieves more harmonious results than previous methods.



### Image Manipulation with Perceptual Discriminators
- **Arxiv ID**: http://arxiv.org/abs/1809.01396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01396v1)
- **Published**: 2018-09-05 09:17:05+00:00
- **Updated**: 2018-09-05 09:17:05+00:00
- **Authors**: Diana Sungatullina, Egor Zakharov, Dmitry Ulyanov, Victor Lempitsky
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Systems that perform image manipulation using deep convolutional networks have achieved remarkable realism. Perceptual losses and losses based on adversarial discriminators are the two main classes of learning objectives behind these advances. In this work, we show how these two ideas can be combined in a principled and non-additive manner for unaligned image translation tasks. This is accomplished through a special architecture of the discriminator network inside generative adversarial learning framework. The new architecture, that we call a perceptual discriminator, embeds the convolutional parts of a pre-trained deep classification network inside the discriminator network. The resulting architecture can be trained on unaligned image datasets while benefiting from the robustness and efficiency of perceptual losses. We demonstrate the merits of the new architecture in a series of qualitative and quantitative comparisons with baseline approaches and state-of-the-art frameworks for unaligned image translation.



### Consensus-Driven Propagation in Massive Unlabeled Data for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.01407v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.01407v2)
- **Published**: 2018-09-05 09:41:16+00:00
- **Updated**: 2019-01-01 08:43:56+00:00
- **Authors**: Xiaohang Zhan, Ziwei Liu, Junjie Yan, Dahua Lin, Chen Change Loy
- **Comment**: In ECCV 2018. More details at the project page:
  http://mmlab.ie.cuhk.edu.hk/projects/CDP/
- **Journal**: None
- **Summary**: Face recognition has witnessed great progress in recent years, mainly attributed to the high-capacity model designed and the abundant labeled data collected. However, it becomes more and more prohibitive to scale up the current million-level identity annotations. In this work, we show that unlabeled face data can be as effective as the labeled ones. Here, we consider a setting closely mimicking the real-world scenario, where the unlabeled data are collected from unconstrained environments and their identities are exclusive from the labeled ones. Our main insight is that although the class information is not available, we can still faithfully approximate these semantic relationships by constructing a relational graph in a bottom-up manner. We propose Consensus-Driven Propagation (CDP) to tackle this challenging problem with two modules, the "committee" and the "mediator", which select positive face pairs robustly by carefully aggregating multi-view information. Extensive experiments validate the effectiveness of both modules to discard outliers and mine hard positives. With CDP, we achieve a compelling accuracy of 78.18% on MegaFace identification challenge by using only 9% of the labels, comparing to 61.78% when no unlabeled data are used and 78.52% when all labels are employed.



### Generating Highly Realistic Images of Skin Lesions with GANs
- **Arxiv ID**: http://arxiv.org/abs/1809.01410v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1809.01410v2)
- **Published**: 2018-09-05 09:47:13+00:00
- **Updated**: 2018-09-06 09:39:44+00:00
- **Authors**: Christoph Baur, Shadi Albarqouni, Nassir Navab
- **Comment**: Accepted at the MICCAI 2018 ISIC Skin Lesion Workshop
- **Journal**: None
- **Summary**: As many other machine learning driven medical image analysis tasks, skin image analysis suffers from a chronic lack of labeled data and skewed class distributions, which poses problems for the training of robust and well-generalizing models. The ability to synthesize realistic looking images of skin lesions could act as a reliever for the aforementioned problems. Generative Adversarial Networks (GANs) have been successfully used to synthesize realistically looking medical images, however limited to low resolution, whereas machine learning models for challenging tasks such as skin lesion segmentation or classification benefit from much higher resolution data. In this work, we successfully synthesize realistically looking images of skin lesions with GANs at such high resolution. Therefore, we utilize the concept of progressive growing, which we both quantitatively and qualitatively compare to other GAN architectures such as the DCGAN and the LAPGAN. Our results show that with the help of progressive growing, we can synthesize highly realistic dermoscopic images of skin lesions that even expert dermatologists find hard to distinguish from real ones.



### Modified Diversity of Class Probability Estimation Co-training for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1809.01436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01436v1)
- **Published**: 2018-09-05 11:12:48+00:00
- **Updated**: 2018-09-05 11:12:48+00:00
- **Authors**: Yan Ju, Lingling Li, Licheng Jiao, Zhongle Ren, Biao Hou, Shuyuan Yang
- **Comment**: 13 pages, 10 figures and 8 tables
- **Journal**: None
- **Summary**: Due to the limited amount and imbalanced classes of labeled training data, the conventional supervised learning can not ensure the discrimination of the learned feature for hyperspectral image (HSI) classification. In this paper, we propose a modified diversity of class probability estimation (MDCPE) with two deep neural networks to learn spectral-spatial feature for HSI classification. In co-training phase, recurrent neural network (RNN) and convolutional neural network (CNN) are utilized as two learners to extract features from labeled and unlabeled data. Based on the extracted features, MDCPE selects most credible samples to update initial labeled data by combining k-means clustering with the traditional diversity of class probability estimation (DCPE) co-training. In this way, MDCPE can keep new labeled data class-balanced and extract discriminative features for both the minority and majority classes. During testing process, classification results are acquired by co-decision of the two learners. Experimental results demonstrate that the proposed semi-supervised co-training method can make full use of unlabeled information to enhance generality of the learners and achieve favorable accuracies on all three widely used data sets: Salinas, Pavia University and Pavia Center.



### How is Contrast Encoded in Deep Neural Networks?
- **Arxiv ID**: http://arxiv.org/abs/1809.01438v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01438v1)
- **Published**: 2018-09-05 11:17:09+00:00
- **Updated**: 2018-09-05 11:17:09+00:00
- **Authors**: Arash Akbarinia, Karl R. Gegenfurtner
- **Comment**: None
- **Journal**: None
- **Summary**: Contrast is a crucial factor in visual information processing. It is desired for a visual system - irrespective of being biological or artificial - to "perceive" the world robustly under large potential changes in illumination. In this work, we studied the responses of deep neural networks (DNN) to identical images at different levels of contrast. We analysed the activation of kernels in the convolutional layers of eight prominent networks with distinct architectures (e.g. VGG and Inception). The results of our experiments indicate that those networks with a higher tolerance to alteration of contrast have more than one convolutional layer prior to the first max-pooling operator. It appears that the last convolutional layer before the first max-pooling acts as a mitigator of contrast variation in input images. In our investigation, interestingly, we observed many similarities between the mechanisms of these DNNs and biological visual systems. These comparisons allow us to understand more profoundly the underlying mechanisms of a visual system that is grounded on the basis of "data-analysis".



### Data Augmentation for Skin Lesion Analysis
- **Arxiv ID**: http://arxiv.org/abs/1809.01442v1
- **DOI**: 10.1007/978-3-030-01201-4_33
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01442v1)
- **Published**: 2018-09-05 11:35:41+00:00
- **Updated**: 2018-09-05 11:35:41+00:00
- **Authors**: Fábio Perez, Cristina Vasconcelos, Sandra Avila, Eduardo Valle
- **Comment**: 8 pages, 3 figures, to be presented on ISIC Skin Image Analysis
  Workshop
- **Journal**: None
- **Summary**: Deep learning models show remarkable results in automated skin lesion analysis. However, these models demand considerable amounts of data, while the availability of annotated skin lesion images is often limited. Data augmentation can expand the training dataset by transforming input images. In this work, we investigate the impact of 13 data augmentation scenarios for melanoma classification trained on three CNNs (Inception-v4, ResNet, and DenseNet). Scenarios include traditional color and geometric transforms, and more unusual augmentations such as elastic transforms, random erasing and a novel augmentation that mixes different lesions. We also explore the use of data augmentation at test-time and the impact of data augmentation on various dataset sizes. Our results confirm the importance of data augmentation in both training and testing and show that it can lead to more performance gains than obtaining new images. The best scenario results in an AUC of 0.882 for melanoma classification without using external data, outperforming the top-ranked submission (0.874) for the ISIC Challenge 2017, which was trained with additional data.



### Conditional Transfer with Dense Residual Attention: Synthesizing traffic signs from street-view imagery
- **Arxiv ID**: http://arxiv.org/abs/1809.01444v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01444v1)
- **Published**: 2018-09-05 11:40:47+00:00
- **Updated**: 2018-09-05 11:40:47+00:00
- **Authors**: Clint Sebastian, Ries Uittenbogaard, Julien Vijverberg, Bas Boom, Peter H. N. de With
- **Comment**: The first two authors have equal contribution. Accepted at
  International Conference on Pattern Recognition 2018 (ICPR)
- **Journal**: None
- **Summary**: Object detection and classification of traffic signs in street-view imagery is an essential element for asset management, map making and autonomous driving. However, some traffic signs occur rarely and consequently, they are difficult to recognize automatically. To improve the detection and classification rates, we propose to generate images of traffic signs, which are then used to train a detector/classifier. In this research, we present an end-to-end framework that generates a realistic image of a traffic sign from a given image of a traffic sign and a pictogram of the target class. We propose a residual attention mechanism with dense concatenation called Dense Residual Attention, that preserves the background information while transferring the object information. We also propose to utilize multi-scale discriminators, so that the smaller scales of the output guide the higher resolution output. We have performed detection and classification tests across a large number of traffic sign classes, by training the detector using the combination of real and generated data. The newly trained model reduces the number of false positives by 1.2 - 1.5% at 99% recall in the detection tests and an absolute improvement of 4.65% (top-1 accuracy) in the classification tests.



### Blur-Countering Keypoint Detection via Eigenvalue Asymmetry
- **Arxiv ID**: http://arxiv.org/abs/1809.01456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01456v1)
- **Published**: 2018-09-05 12:24:18+00:00
- **Updated**: 2018-09-05 12:24:18+00:00
- **Authors**: Chao Zhang, Xuequan Lu, Takuya Akashi
- **Comment**: Contact us for high-resolution clear PDF file if you are interested
  in this paper
- **Journal**: None
- **Summary**: Well-known corner or local extrema feature based detectors such as FAST and DoG have achieved noticeable successes. However, detecting keypoints in the presence of blur has remained to be an unresolved issue. As a matter of fact, various kinds of blur (e.g., motion blur, out-of-focus, and space-variant) remarkably increase challenges for keypoint detection. As a result, those methods have limited performance. To settle this issue, we propose a blur-countering method for detecting valid keypoints for various types and degrees of blurred images. Specifically, we first present a distance metric for derivative distributions, which preserves the distinctiveness of patch pairs well under blur. We then model the asymmetry by utilizing the difference of squared eigenvalues based on the distance metric. To make it scale-robust, we also extend it to scale space. The proposed detector is efficient as the main computational cost is the square of derivatives at each pixel. Extensive visual and quantitative results show that our method outperforms current approaches under different types and degrees of blur. Without any parallelization, our implementation\footnote{We will make our code publicly available upon the acceptance.} achieves real-time performance for low-resolution images (e.g., $320\times240$ pixel).



### Deep Bilevel Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.01465v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.01465v1)
- **Published**: 2018-09-05 12:50:24+00:00
- **Updated**: 2018-09-05 12:50:24+00:00
- **Authors**: Simon Jenni, Paolo Favaro
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: We present a novel regularization approach to train neural networks that enjoys better generalization and test error than standard stochastic gradient descent. Our approach is based on the principles of cross-validation, where a validation set is used to limit the model overfitting. We formulate such principles as a bilevel optimization problem. This formulation allows us to define the optimization of a cost on the validation set subject to another optimization on the training set. The overfitting is controlled by introducing weights on each mini-batch in the training set and by choosing their values so that they minimize the error on the validation set. In practice, these weights define mini-batch learning rates in a gradient descent update equation that favor gradients with better generalization capabilities. Because of its simplicity, this approach can be integrated with other regularization methods and training schemes. We evaluate extensively our proposed algorithm on several neural network architectures and datasets, and find that it consistently improves the generalization of the model, especially when labels are noisy.



### CNNs-based Acoustic Scene Classification using Multi-Spectrogram Fusion and Label Expansions
- **Arxiv ID**: http://arxiv.org/abs/1809.01543v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01543v1)
- **Published**: 2018-09-05 14:34:08+00:00
- **Updated**: 2018-09-05 14:34:08+00:00
- **Authors**: Weiping Zheng, Zhenyao Mo, Xiaotao Xing, Gansen Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Spectrograms have been widely used in Convolutional Neural Networks based schemes for acoustic scene classification, such as the STFT spectrogram and the MFCC spectrogram, etc. They have different time-frequency characteristics, contributing to their own advantages and disadvantages in recognizing acoustic scenes. In this letter, a novel multi-spectrogram fusion framework is proposed, making the spectrograms complement each other. In the framework, a single CNN architecture is applied onto multiple spectrograms for feature extraction. The deep features extracted from multiple spectrograms are then fused to discriminate the acoustic scenes. Moreover, motivated by the inter-class similarities in acoustic scene datasets, a label expansion method is further proposed in which super-class labels are constructed upon the original classes. On the help of the expanded labels, the CNN models are transformed into the multitask learning form to improve the acoustic scene classification by appending the auxiliary task of super-class classification. To verify the effectiveness of the proposed methods, intensive experiments have been performed on the DCASE2017 and the LITIS Rouen datasets. Experimental results show that the proposed method can achieve promising accuracies on both datasets. Specifically, accuracies of 0.9744, 0.8865 and 0.7778 are obtained for the LITIS Rouen dataset, the DCASE Development set and Evaluation set respectively.



### Traffic Density Estimation using a Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1809.01564v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.01564v1)
- **Published**: 2018-09-05 15:03:23+00:00
- **Updated**: 2018-09-05 15:03:23+00:00
- **Authors**: Julian Nubert, Nicholas Giai Truong, Abel Lim, Herbert Ilhan Tanujaya, Leah Lim, Mai Anh Vu
- **Comment**: Machine Learning Project National University of Singapore. 6 pages, 5
  figures
- **Journal**: None
- **Summary**: The goal of this project is to introduce and present a machine learning application that aims to improve the quality of life of people in Singapore. In particular, we investigate the use of machine learning solutions to tackle the problem of traffic congestion in Singapore. In layman's terms, we seek to make Singapore (or any other city) a smoother place. To accomplish this aim, we present an end-to-end system comprising of 1. A traffic density estimation algorithm at traffic lights/junctions and 2. a suitable traffic signal control algorithms that make use of the density information for better traffic control. Traffic density estimation can be obtained from traffic junction images using various machine learning techniques (combined with CV tools). After research into various advanced machine learning methods, we decided on convolutional neural networks (CNNs). We conducted experiments on our algorithms, using the publicly available traffic camera dataset published by the Land Transport Authority (LTA) to demonstrate the feasibility of this approach. With these traffic density estimates, different traffic algorithms can be applied to minimize congestion at traffic junctions in general.



### Deep Depth from Defocus: how can defocus blur improve 3D estimation using dense neural networks?
- **Arxiv ID**: http://arxiv.org/abs/1809.01567v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01567v2)
- **Published**: 2018-09-05 15:09:20+00:00
- **Updated**: 2018-09-06 10:02:02+00:00
- **Authors**: Marcela Carvalho, Bertrand Le Saux, Pauline Trouvé-Peloux, Andrés Almansa, Frédéric Champagnat
- **Comment**: 3DRW Workshop ECCV 2018. Code:
  https://github.com/marcelampc/d3net_depth_estimation
- **Journal**: None
- **Summary**: Depth estimation is of critical interest for scene understanding and accurate 3D reconstruction. Most recent approaches in depth estimation with deep learning exploit geometrical structures of standard sharp images to predict corresponding depth maps. However, cameras can also produce images with defocus blur depending on the depth of the objects and camera settings. Hence, these features may represent an important hint for learning to predict depth. In this paper, we propose a full system for single-image depth prediction in the wild using depth-from-defocus and neural networks. We carry out thorough experiments to test deep convolutional networks on real and simulated defocused images using a realistic model of blur variation with respect to depth. We also investigate the influence of blur on depth prediction observing model uncertainty with a Bayesian neural network approach. From these studies, we show that out-of-focus blur greatly improves the depth-prediction network performances. Furthermore, we transfer the ability learned on a synthetic, indoor dataset to real, indoor and outdoor images. For this purpose, we present a new dataset containing real all-focus and defocused images from a Digital Single-Lens Reflex (DSLR) camera, paired with ground truth depth maps obtained with an active 3D sensor for indoor scenes. The proposed approach is successfully validated on both this new dataset and standard ones as NYUv2 or Depth-in-the-Wild. Code and new datasets are available at https://github.com/marcelampc/d3net_depth_estimation



### Modelling Point Spread Function in Fluorescence Microscopy with a Sparse Combination of Gaussian Mixture: Trade-off between Accuracy and Efficiency
- **Arxiv ID**: http://arxiv.org/abs/1809.01579v2
- **DOI**: 10.1109/TIP.2019.2898843
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.01579v2)
- **Published**: 2018-09-05 15:32:37+00:00
- **Updated**: 2019-02-28 22:34:53+00:00
- **Authors**: Denis K. Samuylov, Prateek Purwar, Gábor Székely, Grégory Paul
- **Comment**: This paper has been accepted in the IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: Deblurring is a fundamental inverse problem in bioimaging. It requires modelling the point spread function (PSF), which captures the optical distortions entailed by the image formation process. The PSF limits the spatial resolution attainable for a given microscope. However, recent applications require a higher resolution, and have prompted the development of super-resolution techniques to achieve sub-pixel accuracy. This requirement restricts the class of suitable PSF models to analog ones. In addition, deblurring is computationally intensive, hence further requiring computationally efficient models. A custom candidate fitting both requirements is the Gaussian model. However, this model cannot capture the rich tail structures found in both theoretical and empirical PSFs. In this paper, we aim at improving the reconstruction accuracy beyond the Gaussian model, while preserving its computational efficiency. We introduce a new class of analog PSF models based on Gaussian mixtures. The number of Gaussian kernels controls both the modelling accuracy and the computational efficiency of the model: the lower the number of kernels, the lower accuracy and the higher efficiency. To explore the accuracy--efficiency trade-off, we propose a variational formulation of the PSF calibration problem, where a convex sparsity-inducing penalty on the number of Gaussian kernels allows trading accuracy for efficiency. We derive an efficient algorithm based on a fully-split formulation of alternating split Bregman. We assess our framework on synthetic and real data and demonstrate a better reconstruction accuracy in both geometry and photometry in point source localisation---a fundamental inverse problem in fluorescence microscopy.



### A Bayesian framework for the analog reconstruction of kymographs from fluorescence microscopy data
- **Arxiv ID**: http://arxiv.org/abs/1809.01590v1
- **DOI**: 10.1109/TIP.2018.2867946
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.01590v1)
- **Published**: 2018-09-05 15:59:05+00:00
- **Updated**: 2018-09-05 15:59:05+00:00
- **Authors**: Denis K. Samuylov, Gábor Székely, Grégory Paul
- **Comment**: This paper has been accepted in the IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: Kymographs are widely used to represent and anal- yse spatio-temporal dynamics of fluorescence markers along curvilinear biological compartments. These objects have a sin- gular geometry, thus kymograph reconstruction is inherently an analog image processing task. However, the existing approaches are essentially digital: the kymograph photometry is sampled directly from the time-lapse images. As a result, such kymographs rely on raw image data that suffer from the degradations entailed by the image formation process and the spatio-temporal resolution of the imaging setup. In this work, we address these limitations and introduce a well-grounded Bayesian framework for the analog reconstruction of kymographs. To handle the movement of the object, we introduce an intrinsic description of kymographs using differential geometry: a kymograph is a photometry defined on a parameter space that is embedded in physical space by a time-varying map that follows the object geometry. We model the kymograph photometry as a L\'evy innovation process, a flexible class of non-parametric signal priors. We account for the image formation process using the virtual microscope framework. We formulate a computationally tractable representation of the associated maximum a posteriori problem and solve it using a class of efficient and modular algorithms based on the alternating split Bregman. We assess the performance of our Bayesian framework on synthetic data and apply it to reconstruct the fluorescence dynamics along microtubules in vivo in the budding yeast S. cerevisiae. We demonstrate that our framework allows revealing patterns from single time-lapse data that are invisible on standard digital kymographs.



### Bimodal network architectures for automatic generation of image annotation from text
- **Arxiv ID**: http://arxiv.org/abs/1809.01610v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1809.01610v1)
- **Published**: 2018-09-05 16:30:08+00:00
- **Updated**: 2018-09-05 16:30:08+00:00
- **Authors**: Mehdi Moradi, Ali Madani, Yaniv Gur, Yufan Guo, Tanveer Syeda-Mahmood
- **Comment**: Accepted to MICCAI 2018, LNCS 11070
- **Journal**: Lecture Notes in Computer Science (LNCS 11070), Proceedings of
  Medical Image Computing & Computer Assisted Intervention (MICCAI 2018)
- **Summary**: Medical image analysis practitioners have embraced big data methodologies. This has created a need for large annotated datasets. The source of big data is typically large image collections and clinical reports recorded for these images. In many cases, however, building algorithms aimed at segmentation and detection of disease requires a training dataset with markings of the areas of interest on the image that match with the described anomalies. This process of annotation is expensive and needs the involvement of clinicians. In this work we propose two separate deep neural network architectures for automatic marking of a region of interest (ROI) on the image best representing a finding location, given a textual report or a set of keywords. One architecture consists of LSTM and CNN components and is trained end to end with images, matching text, and markings of ROIs for those images. The output layer estimates the coordinates of the vertices of a polygonal region. The second architecture uses a network pre-trained on a large dataset of the same image types for learning feature representations of the findings of interest. We show that for a variety of findings from chest X-ray images, both proposed architectures learn to estimate the ROI, as validated by clinical annotations. There is a clear advantage obtained from the architecture with pre-trained imaging network. The centroids of the ROIs marked by this network were on average at a distance equivalent to 5.1% of the image width from the centroids of the ground truth ROIs.



### Efficient Egocentric Visual Perception Combining Eye-tracking, a Software Retina and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.01633v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01633v1)
- **Published**: 2018-09-05 17:19:07+00:00
- **Updated**: 2018-09-05 17:19:07+00:00
- **Authors**: Nina Hristozova, Piotr Ozimek, Jan Paul Siebert
- **Comment**: Accepted for: EPIC Workshop at the European Conference on Computer
  Vision, ECCV2018
- **Journal**: None
- **Summary**: We present ongoing work to harness biological approaches to achieving highly efficient egocentric perception by combining the space-variant imaging architecture of the mammalian retina with Deep Learning methods. By pre-processing images collected by means of eye-tracking glasses to control the fixation locations of a software retina model, we demonstrate that we can reduce the input to a DCNN by a factor of 3, reduce the required number of training epochs and obtain over 98% classification rates when training and validating the system on a database of over 26,000 images of 9 object classes.



### DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency
- **Arxiv ID**: http://arxiv.org/abs/1809.01649v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01649v1)
- **Published**: 2018-09-05 17:58:25+00:00
- **Updated**: 2018-09-05 17:58:25+00:00
- **Authors**: Yuliang Zou, Zelun Luo, Jia-Bin Huang
- **Comment**: ECCV 2018. Project website: http://yuliang.vision/DF-Net/ Code:
  https://github.com/vt-vl-lab/DF-Net
- **Journal**: None
- **Summary**: We present an unsupervised learning framework for simultaneously training single-view depth prediction and optical flow estimation models using unlabeled video sequences. Existing unsupervised methods often exploit brightness constancy and spatial smoothness priors to train depth or flow models. In this paper, we propose to leverage geometric consistency as additional supervisory signals. Our core idea is that for rigid regions we can use the predicted scene depth and camera motion to synthesize 2D optical flow by backprojecting the induced 3D scene flow. The discrepancy between the rigid flow (from depth prediction and camera motion) and the estimated flow (from optical flow model) allows us to impose a cross-task consistency loss. While all the networks are jointly optimized during training, they can be applied independently at test time. Extensive experiments demonstrate that our depth and flow models compare favorably with state-of-the-art unsupervised methods.



### Breast Tumor Segmentation and Shape Classification in Mammograms using Generative Adversarial and Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1809.01687v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01687v3)
- **Published**: 2018-09-05 18:40:04+00:00
- **Updated**: 2018-10-23 17:08:49+00:00
- **Authors**: Vivek Kumar Singh, Hatem A. Rashwan, Santiago Romani, Farhan Akram, Nidhi Pandey, Md. Mostafa Kamal Sarker, Adel Saleh, Meritexell Arenas, Miguel Arquez, Domenec Puig, Jordina Torrents-Barrena
- **Comment**: 33 pages, Submitted to Expert Systems with Applications
- **Journal**: None
- **Summary**: Mammogram inspection in search of breast tumors is a tough assignment that radiologists must carry out frequently. Therefore, image analysis methods are needed for the detection and delineation of breast masses, which portray crucial morphological information that will support reliable diagnosis. In this paper, we proposed a conditional Generative Adversarial Network (cGAN) devised to segment a breast mass within a region of interest (ROI) in a mammogram. The generative network learns to recognize the breast mass area and to create the binary mask that outlines the breast mass. In turn, the adversarial network learns to distinguish between real (ground truth) and synthetic segmentations, thus enforcing the generative network to create binary masks as realistic as possible. The cGAN works well even when the number of training samples are limited. Therefore, the proposed method outperforms several state-of-the-art approaches. This hypothesis is corroborated by diverse experiments performed on two datasets, the public INbreast and a private in-house dataset. The proposed segmentation model provides a high Dice coefficient and Intersection over Union (IoU) of 94% and 87%, respectively. In addition, a shape descriptor based on a Convolutional Neural Network (CNN) is proposed to classify the generated masks into four mass shapes: irregular, lobular, oval and round. The proposed shape descriptor was trained on Digital Database for Screening Mammography (DDSM) yielding an overall accuracy of 80%, which outperforms the current state-of-the-art.



### TVQA: Localized, Compositional Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1809.01696v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.01696v2)
- **Published**: 2018-09-05 19:14:11+00:00
- **Updated**: 2019-05-07 21:34:05+00:00
- **Authors**: Jie Lei, Licheng Yu, Mohit Bansal, Tamara L. Berg
- **Comment**: EMNLP 2018 (13 pages; Data and Leaderboard at:
  http://tvqa.cs.unc.edu). Updated with test-public results
- **Journal**: None
- **Summary**: Recent years have witnessed an increasing interest in image-based question-answering (QA) tasks. However, due to data limitations, there has been much less work on video-based QA. In this paper, we present TVQA, a large-scale video QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairs from 21,793 clips, spanning over 460 hours of video. Questions are designed to be compositional in nature, requiring systems to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts. We provide analyses of this new dataset as well as several baselines and a multi-stream end-to-end trainable neural network framework for the TVQA task. The dataset is publicly available at http://tvqa.cs.unc.edu.



### Pack and Detect: Fast Object Detection in Videos Using Region-of-Interest Packing
- **Arxiv ID**: http://arxiv.org/abs/1809.01701v4
- **DOI**: 10.1145/3297001.3297020
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.01701v4)
- **Published**: 2018-09-05 19:29:34+00:00
- **Updated**: 2021-03-22 04:44:14+00:00
- **Authors**: Athindran Ramesh Kumar, Balaraman Ravindran, Anand Raghunathan
- **Comment**: Proceedings of the ACM India Joint International Conference on Data
  Science and Management of Data. 2019
- **Journal**: None
- **Summary**: Object detection in videos is an important task in computer vision for various applications such as object tracking, video summarization and video search. Although great progress has been made in improving the accuracy of object detection in recent years due to the rise of deep neural networks, the state-of-the-art algorithms are highly computationally intensive. In order to address this challenge, we make two important observations in the context of videos: (i) Objects often occupy only a small fraction of the area in each video frame, and (ii) There is a high likelihood of strong temporal correlation between consecutive frames. Based on these observations, we propose Pack and Detect (PaD), an approach to reduce the computational requirements of object detection in videos. In PaD, only selected video frames called anchor frames are processed at full size. In the frames that lie between anchor frames (inter-anchor frames), regions of interest (ROIs) are identified based on the detections in the previous frame. We propose an algorithm to pack the ROIs of each inter-anchor frame together into a reduced-size frame. The computational requirements of the detector are reduced due to the lower size of the input. In order to maintain the accuracy of object detection, the proposed algorithm expands the ROIs greedily to provide additional background around each object to the detector. PaD can use any underlying neural network architecture to process the full-size and reduced-size frames. Experiments using the ImageNet video object detection dataset indicate that PaD can potentially reduce the number of FLOPS required for a frame by $4\times$. This leads to an overall increase in throughput of $1.25\times$ on a 2.1 GHz Intel Xeon server with a NVIDIA Titan X GPU at the cost of $1.1\%$ drop in accuracy.



### Neural Comic Style Transfer: Case Study
- **Arxiv ID**: http://arxiv.org/abs/1809.01726v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.01726v2)
- **Published**: 2018-09-05 20:37:53+00:00
- **Updated**: 2018-09-11 08:27:48+00:00
- **Authors**: Maciej Pęśko, Tomasz Trzciński
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: The work by Gatys et al. [1] recently showed a neural style algorithm that can produce an image in the style of another image. Some further works introduced various improvements regarding generalization, quality and efficiency, but each of them was mostly focused on styles such as paintings, abstract images or photo-realistic style. In this paper, we present a comparison of how state-of-the-art style transfer methods cope with transferring various comic styles on different images. We select different combinations of Adaptive Instance Normalization [11] and Universal Style Transfer [16] models and confront them to find their advantages and disadvantages in terms of qualitative and quantitative analysis. Finally, we present the results of a survey conducted on over 100 people that aims at validating the evaluation results in a real-life application of comic style transfer.



