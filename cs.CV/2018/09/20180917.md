# Arxiv Papers in cs.CV on 2018-09-17
### FermiNets: Learning generative machines to generate efficient neural networks via generative synthesis
- **Arxiv ID**: http://arxiv.org/abs/1809.05989v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.05989v2)
- **Published**: 2018-09-17 01:26:57+00:00
- **Updated**: 2018-11-13 19:14:50+00:00
- **Authors**: Alexander Wong, Mohammad Javad Shafiee, Brendan Chwyl, Francis Li
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: The tremendous potential exhibited by deep learning is often offset by architectural and computational complexity, making widespread deployment a challenge for edge scenarios such as mobile and other consumer devices. To tackle this challenge, we explore the following idea: Can we learn generative machines to automatically generate deep neural networks with efficient network architectures? In this study, we introduce the idea of generative synthesis, which is premised on the intricate interplay between a generator-inquisitor pair that work in tandem to garner insights and learn to generate highly efficient deep neural networks that best satisfies operational requirements. What is most interesting is that, once a generator has been learned through generative synthesis, it can be used to generate not just one but a large variety of different, unique highly efficient deep neural networks that satisfy operational requirements. Experimental results for image classification, semantic segmentation, and object detection tasks illustrate the efficacy of generative synthesis in producing generators that automatically generate highly efficient deep neural networks (which we nickname FermiNets) with higher model efficiency and lower computational costs (reaching >10x more efficient and fewer multiply-accumulate operations than several tested state-of-the-art networks), as well as higher energy efficiency (reaching >4x improvements in image inferences per joule consumed on a Nvidia Tegra X2 mobile processor). As such, generative synthesis can be a powerful, generalized approach for accelerating and improving the building of deep neural networks for on-device edge scenarios.



### Highly-Economized Multi-View Binary Compression for Scalable Image Clustering
- **Arxiv ID**: http://arxiv.org/abs/1809.05992v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DM, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.05992v1)
- **Published**: 2018-09-17 01:35:42+00:00
- **Updated**: 2018-09-17 01:35:42+00:00
- **Authors**: Zheng Zhang, Li Liu, Jie Qin, Fan Zhu, Fumin Shen, Yong Xu, Ling Shao, Heng Tao Shen
- **Comment**: European Conference on Computer Vision (ECCV) 2018
- **Journal**: None
- **Summary**: How to economically cluster large-scale multi-view images is a long-standing problem in computer vision. To tackle this challenge, we introduce a novel approach named Highly-economized Scalable Image Clustering (HSIC) that radically surpasses conventional image clustering methods via binary compression. We intuitively unify the binary representation learning and efficient binary cluster structure learning into a joint framework. In particular, common binary representations are learned by exploiting both sharable and individual information across multiple views to capture their underlying correlations. Meanwhile, cluster assignment with robust binary centroids is also performed via effective discrete optimization under L21-norm constraint. By this means, heavy continuous-valued Euclidean distance computations can be successfully reduced by efficient binary XOR operations during the clustering procedure. To our best knowledge, HSIC is the first binary clustering work specifically designed for scalable multi-view image clustering. Extensive experimental results on four large-scale image datasets show that HSIC consistently outperforms the state-of-the-art approaches, whilst significantly reducing computational time and memory footprint.



### Development of spatial suppression surrounding the focus of visual attention
- **Arxiv ID**: http://arxiv.org/abs/1809.09700v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.09700v1)
- **Published**: 2018-09-17 01:35:56+00:00
- **Updated**: 2018-09-17 01:35:56+00:00
- **Authors**: Audrey M. B. Wong-Kee-You, John K. Tsotsos, Scott A. Adler
- **Comment**: None
- **Journal**: None
- **Summary**: The capacity to filter out irrelevant information from our environment is critical to efficient processing. Yet, during development, when building a knowledge base of the world is occurring, the ability to selectively allocate attentional resources is limited (e.g., Amso & Scerif, 2015). In adulthood, research has demonstrated that surrounding the spatial location of attentional focus is a suppressive field, resulting from top-down attention promoting the processing of relevant stimuli and inhibiting surrounding distractors (e.g., Hopf et al., 2006). It is not fully known, however, whether this phenomenon manifests in development. In the current study, we examined whether spatial suppression surrounding the focus of visual attention is exhibited in developmental age groups. Participants between 12 and 27 years of age exhibited spatial suppression surrounding their focus of visual attention. Their accuracy increased as a function of the separation distance between a spatially cued (and attended) target and a second target, suggesting that a ring of suppression surrounded the attended target. When a central cue was instead presented and therefore attention was no longer spatially cued, surround suppression was not observed, indicating that our initial findings of suppression were indeed related to the focus of attention. Attentional surround suppression was not observed in 8- to 11-years-olds, even with a longer spatial cue presentation time, demonstrating that the lack of the effect at these ages is not due to slowed attentional feedback processes. Our findings demonstrate that top-down attentional processes are still immature until approximately 12 years of age, and that they continue to be refined throughout adolescence, converging well with previous research on attentional development.



### Devil in the Details: Towards Accurate Single and Multiple Human Parsing
- **Arxiv ID**: http://arxiv.org/abs/1809.05996v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05996v3)
- **Published**: 2018-09-17 02:28:49+00:00
- **Updated**: 2018-11-29 06:58:05+00:00
- **Authors**: Tao Ruan, Ting Liu, Zilong Huang, Yunchao Wei, Shikui Wei, Yao Zhao, Thomas Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Human parsing has received considerable interest due to its wide application potentials. Nevertheless, it is still unclear how to develop an accurate human parsing system in an efficient and elegant way. In this paper, we identify several useful properties, including feature resolution, global context information and edge details, and perform rigorous analyses to reveal how to leverage them to benefit the human parsing task. The advantages of these useful properties finally result in a simple yet effective Context Embedding with Edge Perceiving (CE2P) framework for single human parsing. Our CE2P is end-to-end trainable and can be easily adopted for conducting multiple human parsing. Benefiting the superiority of CE2P, we achieved the 1st places on all three human parsing benchmarks. Without any bells and whistles, we achieved 56.50\% (mIoU), 45.31\% (mean $AP^r$) and 33.34\% ($AP^p_{0.5}$) in LIP, CIHP and MHP v2.0, which outperform the state-of-the-arts more than 2.06\%, 3.81\% and 1.87\%, respectively. We hope our CE2P will serve as a solid baseline and help ease future research in single/multiple human parsing. Code has been made available at \url{https://github.com/liutinglt/CE2P}.



### Incomplete Multi-view Clustering via Graph Regularized Matrix Factorization
- **Arxiv ID**: http://arxiv.org/abs/1809.05998v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.05998v1)
- **Published**: 2018-09-17 02:46:48+00:00
- **Updated**: 2018-09-17 02:46:48+00:00
- **Authors**: Jie Wen, Zheng Zhang, Yong Xu, Zuofeng Zhong
- **Comment**: ECCV 2018 International Workshop on Compact and Efficient Feature
  Representation and Learning in Computer Vision (CEFRL)
- **Journal**: None
- **Summary**: Clustering with incomplete views is a challenge in multi-view clustering. In this paper, we provide a novel and simple method to address this issue. Specifically, the proposed method simultaneously exploits the local information of each view and the complementary information among views to learn the common latent representation for all samples, which can greatly improve the compactness and discriminability of the obtained representation. Compared with the conventional graph embedding methods, the proposed method does not introduce any extra regularization term and corresponding penalty parameter to preserve the local structure of data, and thus does not increase the burden of extra parameter selection. By imposing the orthogonal constraint on the basis matrix of each view, the proposed method is able to handle the out-of-sample. Moreover, the proposed method can be viewed as a unified framework for multi-view learning since it can handle both incomplete and complete multi-view clustering and classification tasks. Extensive experiments conducted on several multi-view datasets prove that the proposed method can significantly improve the clustering performance.



### Evaluating Merging Strategies for Sampling-based Uncertainty Techniques in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1809.06006v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06006v3)
- **Published**: 2018-09-17 03:16:03+00:00
- **Updated**: 2019-03-07 00:02:33+00:00
- **Authors**: Dimity Miller, Feras Dayoub, Michael Milford, Niko Sünderhauf
- **Comment**: to appear in IEEE International Conference on Robotics and Automation
  2019 (ICRA 2019)
- **Journal**: None
- **Summary**: There has been a recent emergence of sampling-based techniques for estimating epistemic uncertainty in deep neural networks. While these methods can be applied to classification or semantic segmentation tasks by simply averaging samples, this is not the case for object detection, where detection sample bounding boxes must be accurately associated and merged. A weak merging strategy can significantly degrade the performance of the detector and yield an unreliable uncertainty measure. This paper provides the first in-depth investigation of the effect of different association and merging strategies. We compare different combinations of three spatial and two semantic affinity measures with four clustering methods for MC Dropout with a Single Shot Multi-Box Detector. Our results show that the correct choice of affinity-clustering combination can greatly improve the effectiveness of the classification and spatial uncertainty estimation and the resulting object detection performance. We base our evaluation on a new mix of datasets that emulate near open-set conditions (semantically similar unknown classes), distant open-set conditions (semantically dissimilar unknown classes) and the common closed-set conditions (only known classes).



### DASNet: Reducing Pixel-level Annotations for Instance and Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1809.06013v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1809.06013v2)
- **Published**: 2018-09-17 04:23:20+00:00
- **Updated**: 2020-01-31 18:07:53+00:00
- **Authors**: Chuang Niu, Shenghan Ren, Jimin Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Pixel-level annotation demands expensive human efforts and limits the performance of deep networks that usually benefits from more such training data. In this work we aim to achieve high quality instance and semantic segmentation results over a small set of pixel-level mask annotations and a large set of box annotations. The basic idea is exploring detection models to simplify the pixel-level supervised learning task and thus reduce the required amount of mask annotations. Our architecture, named DASNet, consists of three modules: detection, attention, and segmentation. The detection module detects all classes of objects, the attention module generates multi-scale class-specific features, and the segmentation module recovers the binary masks. Our method demonstrates substantially improved performance compared to existing semi-supervised approaches on PASCAL VOC 2012 dataset.



### Extracting representations of cognition across neuroimaging studies improves brain decoding
- **Arxiv ID**: http://arxiv.org/abs/1809.06035v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1809.06035v3)
- **Published**: 2018-09-17 06:19:11+00:00
- **Updated**: 2021-05-19 08:37:14+00:00
- **Authors**: Arthur Mensch, Julien Mairal, Bertrand Thirion, Gaël Varoquaux
- **Comment**: None
- **Journal**: PLoS Computational Biology, Public Library of Science, 2021
- **Summary**: Cognitive brain imaging is accumulating datasets about the neural substrate of many different mental processes. Yet, most studies are based on few subjects and have low statistical power. Analyzing data across studies could bring more statistical power; yet the current brain-imaging analytic framework cannot be used at scale as it requires casting all cognitive tasks in a unified theoretical framework. We introduce a new methodology to analyze brain responses across tasks without a joint model of the psychological processes. The method boosts statistical power in small studies with specific cognitive focus by analyzing them jointly with large studies that probe less focal mental processes. Our approach improves decoding performance for 80% of 35 widely-different functional-imaging studies. It finds commonalities across tasks in a data-driven way, via common brain representations that predict mental processes. These are brain networks tuned to psychological manipulations. They outline interpretable and plausible brain structures. The extracted networks have been made available; they can be readily reused in new neuro-imaging studies. We provide a multi-study decoding tool to adapt to new data.



### Binocular Tone Mapping with Improved Overall Contrast and Local Details
- **Arxiv ID**: http://arxiv.org/abs/1809.06036v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06036v1)
- **Published**: 2018-09-17 06:19:54+00:00
- **Updated**: 2018-09-17 06:19:54+00:00
- **Authors**: Zhuming Zhang, Xinghong Hu, Xueting Liu, Tien-Tsin Wong
- **Comment**: Accepted by Pacific Graphics 2018
- **Journal**: Computer Graphics Forum (Pacific Graphics issue) 37, 7 (2018)
- **Summary**: Tone mapping is a commonly used technique that maps the set of colors in high-dynamic-range (HDR) images to another set of colors in low-dynamic-range (LDR) images, to fit the need for print-outs, LCD monitors and projectors. Unfortunately, during the compression of dynamic range, the overall contrast and local details generally cannot be preserved simultaneously. Recently, with the increased use of stereoscopic devices, the notion of binocular tone mapping has been proposed in the existing research study. However, the existing research lacks the binocular perception study and is unable to generate the optimal binocular pair that presents the most visual content. In this paper, we propose a novel perception-based binocular tone mapping method, that can generate an optimal binocular image pair (generating left and right images simultaneously) from an HDR image that presents the most visual content by designing a binocular perception metric. Our method outperforms the existing method in terms of both visual and time performance.



### Scattering Networks for Hybrid Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.06367v1
- **DOI**: 10.1109/TPAMI.2018.2855738
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.06367v1)
- **Published**: 2018-09-17 06:27:40+00:00
- **Updated**: 2018-09-17 06:27:40+00:00
- **Authors**: Edouard Oyallon, Sergey Zagoruyko, Gabriel Huang, Nikos Komodakis, Simon Lacoste-Julien, Matthew Blaschko, Eugene Belilovsky
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1703.08961
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  Institute of Electrical and Electronics Engineers, 2018, pp.11
- **Summary**: Scattering networks are a class of designed Convolutional Neural Networks (CNNs) with fixed weights. We argue they can serve as generic representations for modelling images. In particular, by working in scattering space, we achieve competitive results both for supervised and unsupervised learning tasks, while making progress towards constructing more interpretable CNNs. For supervised learning, we demonstrate that the early layers of CNNs do not necessarily need to be learned, and can be replaced with a scattering network instead. Indeed, using hybrid architectures, we achieve the best results with predefined representations to-date, while being competitive with end-to-end learned CNNs. Specifically, even applying a shallow cascade of small-windowed scattering coefficients followed by 1$\times$1-convolutions results in AlexNet accuracy on the ILSVRC2012 classification task. Moreover, by combining scattering networks with deep residual networks, we achieve a single-crop top-5 error of 11.4% on ILSVRC2012. Also, we show they can yield excellent performance in the small sample regime on CIFAR-10 and STL-10 datasets, exceeding their end-to-end counterparts, through their ability to incorporate geometrical priors. For unsupervised learning, scattering coefficients can be a competitive representation that permits image recovery. We use this fact to train hybrid GANs to generate images. Finally, we empirically analyze several properties related to stability and reconstruction of images from scattering coefficients.



### Building Prior Knowledge: A Markov Based Pedestrian Prediction Model Using Urban Environmental Data
- **Arxiv ID**: http://arxiv.org/abs/1809.06045v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.06045v1)
- **Published**: 2018-09-17 07:06:44+00:00
- **Updated**: 2018-09-17 07:06:44+00:00
- **Authors**: Pavan Vasishta, Dominique Vaufreydaz, Anne Spalanzani
- **Comment**: 15 th International Conference on Control, Automation, Robotics and
  Vision (ICARCV 2018), Nov 2018, Singapore, Singapore
- **Journal**: None
- **Summary**: Autonomous Vehicles navigating in urban areas have a need to understand and predict future pedestrian behavior for safer navigation. This high level of situational awareness requires observing pedestrian behavior and extrapolating their positions to know future positions. While some work has been done in this field using Hidden Markov Models (HMMs), one of the few observed drawbacks of the method is the need for informed priors for learning behavior. In this work, an extension to the Growing Hidden Markov Model (GHMM) method is proposed to solve some of these drawbacks. This is achieved by building on existing work using potential cost maps and the principle of Natural Vision. As a consequence, the proposed model is able to predict pedestrian positions more precisely over a longer horizon compared to the state of the art. The method is tested over "legal" and "illegal" behavior of pedestrians, having trained the model with sparse observations and partial trajectories. The method, with no training data, is compared against a trained state of the art model. It is observed that the proposed method is robust even in new, previously unseen areas.



### Object-sensitive Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.06064v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.06064v1)
- **Published**: 2018-09-17 07:59:36+00:00
- **Updated**: 2018-09-17 07:59:36+00:00
- **Authors**: Yuezhang Li, Katia Sycara, Rahul Iyer
- **Comment**: 15 pages, 6 figures, Accepted at 3rd Global Conference on Artificial
  Intelligence (GCAI-17), Miami, 2017
- **Journal**: None
- **Summary**: Deep reinforcement learning has become popular over recent years, showing superiority on different visual-input tasks such as playing Atari games and robot navigation. Although objects are important image elements, few work considers enhancing deep reinforcement learning with object characteristics. In this paper, we propose a novel method that can incorporate object recognition processing to deep reinforcement learning models. This approach can be adapted to any existing deep reinforcement learning frameworks. State-of-the-art results are shown in experiments on Atari games. We also propose a new approach called "object saliency maps" to visually explain the actions made by deep reinforcement learning agents.



### Focal Loss in 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1809.06065v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06065v3)
- **Published**: 2018-09-17 08:02:00+00:00
- **Updated**: 2019-01-16 09:59:12+00:00
- **Authors**: Peng Yun, Lei Tai, Yuan Wang, Chengju Liu, Ming Liu
- **Comment**: IEEE RA-L 2019 to appear. Codes and trained weights are available on
  the project page(https://goo.gl/2hFbmL)
- **Journal**: None
- **Summary**: 3D object detection is still an open problem in autonomous driving scenes. When recognizing and localizing key objects from sparse 3D inputs, autonomous vehicles suffer from a larger continuous searching space and higher fore-background imbalance compared to image-based object detection. In this paper, we aim to solve this fore-background imbalance in 3D object detection. Inspired by the recent use of focal loss in image-based object detection, we extend this hard-mining improvement of binary cross entropy to point-cloud-based object detection and conduct experiments to show its performance based on two different 3D detectors: 3D-FCN and VoxelNet. The evaluation results show up to 11.2AP gains through the focal loss in a wide range of hyperparameters for 3D object detection.



### Seuillage par hystérésis pour le test de photo-consistance des voxels dans le cadre de la reconstruction 3D
- **Arxiv ID**: http://arxiv.org/abs/1809.06070v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06070v1)
- **Published**: 2018-09-17 08:29:37+00:00
- **Updated**: 2018-09-17 08:29:37+00:00
- **Authors**: Mohamed Chafik Bakkay, Walid Barhoumi, Ezzeddine Zagrouba
- **Comment**: in French
- **Journal**: None
- **Summary**: Voxel coloring is a popular method of reconstructing a three-dimensional surface model from a set of calibrated 2D images. However, the reconstruction quality is largely dependent on a thresholding procedure allowing the authors to decide, for each voxel, whether it is photo-consistent or not. Even so, this method is widely used because of its simplicity and low computational cost. We have returned to this method in order to propose an improvement in the thresholding step which will be fully automated. Indeed, the geometrical information is implicitly integrated using an hysteresis thresholding which takes into account the spatial coherence of color voxels. Moreover, the ambiguity of choosing the thresholds is extremely minimized by defining a fuzzy degree of membership of each voxel into the class of consistent voxels. Also, there is no need for preset thresholds since the hysteresis ones are defined automatically and adaptively depending on the number of images that the voxel isprojected onto. Preliminary results are very promising and demonstrate that the proposed method performs automatically precise and smooth volumetric scene reconstruction.



### An Integral Pose Regression System for the ECCV2018 PoseTrack Challenge
- **Arxiv ID**: http://arxiv.org/abs/1809.06079v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06079v1)
- **Published**: 2018-09-17 08:59:22+00:00
- **Updated**: 2018-09-17 08:59:22+00:00
- **Authors**: Xiao Sun, Chuankang Li, Stephen Lin
- **Comment**: None
- **Journal**: None
- **Summary**: For the ECCV 2018 PoseTrack Challenge, we present a 3D human pose estimation system based mainly on the integral human pose regression method. We show a comprehensive ablation study to examine the key performance factors of the proposed system. Our system obtains 47mm MPJPE on the CHALL_H80K test dataset, placing second in the ECCV2018 3D human pose estimation challenge. Code will be released to facilitate future work.



### A Deep Learning Framework for Unsupervised Affine and Deformable Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1809.06130v2
- **DOI**: 10.1016/j.media.2018.11.010
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06130v2)
- **Published**: 2018-09-17 11:14:54+00:00
- **Updated**: 2018-12-05 14:38:00+00:00
- **Authors**: Bob D. de Vos, Floris F. Berendsen, Max A. Viergever, Hessam Sokooti, Marius Staring, Ivana Isgum
- **Comment**: Accepted: Medical Image Analysis - Elsevier
- **Journal**: Medical Image Analysis, Volume 52, February 2019, Pages 128-143
- **Summary**: Image registration, the process of aligning two or more images, is the core technique of many (semi-)automatic medical image analysis tasks. Recent studies have shown that deep learning methods, notably convolutional neural networks (ConvNets), can be used for image registration. Thus far training of ConvNets for registration was supervised using predefined example registrations. However, obtaining example registrations is not trivial. To circumvent the need for predefined examples, and thereby to increase convenience of training ConvNets for image registration, we propose the Deep Learning Image Registration (DLIR) framework for \textit{unsupervised} affine and deformable image registration. In the DLIR framework ConvNets are trained for image registration by exploiting image similarity analogous to conventional intensity-based image registration. After a ConvNet has been trained with the DLIR framework, it can be used to register pairs of unseen images in one shot. We propose flexible ConvNets designs for affine image registration and for deformable image registration. By stacking multiple of these ConvNets into a larger architecture, we are able to perform coarse-to-fine image registration. We show for registration of cardiac cine MRI and registration of chest CT that performance of the DLIR framework is comparable to conventional image registration while being several orders of magnitude faster.



### Revisit Multinomial Logistic Regression in Deep Learning: Data Dependent Model Initialization for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.06131v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.06131v1)
- **Published**: 2018-09-17 11:23:33+00:00
- **Updated**: 2018-09-17 11:23:33+00:00
- **Authors**: Bowen Cheng, Rong Xiao, Yandong Guo, Yuxiao Hu, Jianfeng Wang, Lei Zhang
- **Comment**: tech report
- **Journal**: None
- **Summary**: We study in this paper how to initialize the parameters of multinomial logistic regression (a fully connected layer followed with softmax and cross entropy loss), which is widely used in deep neural network (DNN) models for classification problems. As logistic regression is widely known not having a closed-form solution, it is usually randomly initialized, leading to several deficiencies especially in transfer learning where all the layers except for the last task-specific layer are initialized using a pre-trained model. The deficiencies include slow convergence speed, possibility of stuck in local minimum, and the risk of over-fitting. To address those deficiencies, we first study the properties of logistic regression and propose a closed-form approximate solution named regularized Gaussian classifier (RGC). Then we adopt this approximate solution to initialize the task-specific linear layer and demonstrate superior performance over random initialization in terms of both accuracy and convergence speed on various tasks and datasets. For example, for image classification, our approach can reduce the training time by 10 times and achieve 3.2% gain in accuracy for Flickr-style classification. For object detection, our approach can also be 10 times faster in training for the same accuracy, or 5% better in terms of mAP for VOC 2007 with slightly longer training.



### Automatic Electrodes Detection during simultaneous EEG/fMRI acquisition
- **Arxiv ID**: http://arxiv.org/abs/1809.06139v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1809.06139v1)
- **Published**: 2018-09-17 11:37:49+00:00
- **Updated**: 2018-09-17 11:37:49+00:00
- **Authors**: Mathis Fleury, Pierre Maurel, Marsel Mano, Elise Bannier, Christian Barillot
- **Comment**: ISMRM, Jun 2018, Paris, France. 2018, https://www.ismrm.org/
- **Journal**: None
- **Summary**: Simultaneous EEG/fMRI acquisition allows to measure brain activity at high spatial-temporal resolution. The localisation of EEG sources depends on several parameters including the position of the electrodes on the scalp. The position of the MR electrodes during its acquisitions is obtained with the use of the UTE sequence allowing their visualisation. The retrieval of the electrodes consists in obtaining the volume where the electrodes are located by applying a sphere detection algorithm. We detect around 90% of electrodes for each subject, and our UTE-based electrode detection showed an average position error of 3.7mm for all subjects.



### Feature2Mass: Visual Feature Processing in Latent Space for Realistic Labeled Mass Generation
- **Arxiv ID**: http://arxiv.org/abs/1809.06147v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06147v2)
- **Published**: 2018-09-17 12:01:47+00:00
- **Updated**: 2018-11-12 06:09:00+00:00
- **Authors**: Jae-Hyeok Lee, Seong Tae Kim, Hakmin Lee, Yong Man Ro
- **Comment**: This paper presented at ECCV 2018 Workshop
- **Journal**: None
- **Summary**: This paper deals with a method for generating realistic labeled masses. Recently, there have been many attempts to apply deep learning to various bio-image computing fields including computer-aided detection and diagnosis. In order to learn deep network model to be well-behaved in bio-image computing fields, a lot of labeled data is required. However, in many bioimaging fields, the large-size of labeled dataset is scarcely available. Although a few researches have been dedicated to solving this problem through generative model, there are some problems as follows: 1) The generated bio-image does not seem realistic; 2) the variation of generated bio-image is limited; and 3) additional label annotation task is needed. In this study, we propose a realistic labeled bio-image generation method through visual feature processing in latent space. Experimental results have shown that mass images generated by the proposed method were realistic and had wide expression range of targeted mass characteristics.



### Periocular Recognition Using CNN Features Off-the-Shelf
- **Arxiv ID**: http://arxiv.org/abs/1809.06157v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06157v1)
- **Published**: 2018-09-17 12:33:18+00:00
- **Updated**: 2018-09-17 12:33:18+00:00
- **Authors**: Kevin Hernandez-Diaz, Fernando Alonso-Fernandez, Josef Bigun
- **Comment**: Accepted to BIOSIG 2018: 17th International Conference of the
  Biometrics Special Interest Group
- **Journal**: Proc. 17th International Conference of the Biometrics Special
  Interest Group, BIOSIG, Darmstadt, Germany, 26-28 September 2018
- **Summary**: Periocular refers to the region around the eye, including sclera, eyelids, lashes, brows and skin. With a surprisingly high discrimination ability, it is the ocular modality requiring the least constrained acquisition. Here, we apply existing pre-trained architectures, proposed in the context of the ImageNet Large Scale Visual Recognition Challenge, to the task of periocular recognition. These have proven to be very successful for many other computer vision tasks apart from the detection and classification tasks for which they were designed. Experiments are done with a database of periocular images captured with a digital camera. We demonstrate that these off-the-shelf CNN features can effectively recognize individuals based on periocular images, despite being trained to classify generic objects. Compared against reference periocular features, they show an EER reduction of up to ~40%, with the fusion of CNN and traditional features providing additional improvements.



### Dual Encoding for Zero-Example Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1809.06181v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06181v3)
- **Published**: 2018-09-17 13:13:14+00:00
- **Updated**: 2019-03-19 14:32:13+00:00
- **Authors**: Jianfeng Dong, Xirong Li, Chaoxi Xu, Shouling Ji, Yuan He, Gang Yang, Xun Wang
- **Comment**: Accepted by CVPR 2019. Code and data are available at
  https://github.com/danieljf24/dual_encoding
- **Journal**: None
- **Summary**: This paper attacks the challenging problem of zero-example video retrieval. In such a retrieval paradigm, an end user searches for unlabeled videos by ad-hoc queries described in natural language text with no visual example provided. Given videos as sequences of frames and queries as sequences of words, an effective sequence-to-sequence cross-modal matching is required. The majority of existing methods are concept based, extracting relevant concepts from queries and videos and accordingly establishing associations between the two modalities. In contrast, this paper takes a concept-free approach, proposing a dual deep encoding network that encodes videos and queries into powerful dense representations of their own. Dual encoding is conceptually simple, practically effective and end-to-end. As experiments on three benchmarks, i.e. MSR-VTT, TRECVID 2016 and 2017 Ad-hoc Video Search show, the proposed solution establishes a new state-of-the-art for zero-example video retrieval.



### Study and Observation of the Variation of Accuracies of KNN, SVM, LMNN, ENN Algorithms on Eleven Different Datasets from UCI Machine Learning Repository
- **Arxiv ID**: http://arxiv.org/abs/1809.06186v3
- **DOI**: 10.1109/CEEICT.2018.8628041
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.06186v3)
- **Published**: 2018-09-17 13:27:43+00:00
- **Updated**: 2018-11-29 16:57:47+00:00
- **Authors**: Mohammad Mahmudur Rahman Khan, Rezoana Bente Arif, Md. Abu Bakr Siddique, Mahjabin Rahman Oishe
- **Comment**: To be published in the 4th IEEE International Conference on
  Electrical Engineering and Information & Communication Technology (iCEEiCT
  2018)
- **Journal**: 2018 4th International Conference on Electrical Engineering and
  Information & Communication Technology (iCEEiCT)
- **Summary**: Machine learning qualifies computers to assimilate with data, without being solely programmed [1, 2]. Machine learning can be classified as supervised and unsupervised learning. In supervised learning, computers learn an objective that portrays an input to an output hinged on training input-output pairs [3]. Most efficient and widely used supervised learning algorithms are K-Nearest Neighbors (KNN), Support Vector Machine (SVM), Large Margin Nearest Neighbor (LMNN), and Extended Nearest Neighbor (ENN). The main contribution of this paper is to implement these elegant learning algorithms on eleven different datasets from the UCI machine learning repository to observe the variation of accuracies for each of the algorithms on all datasets. Analyzing the accuracy of the algorithms will give us a brief idea about the relationship of the machine learning algorithms and the data dimensionality. All the algorithms are developed in Matlab. Upon such accuracy observation, the comparison can be built among KNN, SVM, LMNN, and ENN regarding their performances on each dataset.



### Study and Observation of the Variations of Accuracies for Handwritten Digits Recognition with Various Hidden Layers and Epochs using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1809.06187v3
- **DOI**: 10.1109/CEEICT.2018.8628078
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.06187v3)
- **Published**: 2018-09-17 13:28:02+00:00
- **Updated**: 2018-11-29 16:56:11+00:00
- **Authors**: Rezoana Bente Arif, Md. Abu Bakr Siddique, Mohammad Mahmudur Rahman Khan, Mahjabin Rahman Oishe
- **Comment**: To be published in the 4th IEEE International Conference on
  Electrical Engineering and Information & Communication Technology (iCEEiCT
  2018)
- **Journal**: 2018 4th International Conference on Electrical Engineering and
  Information & Communication Technology (iCEEiCT)
- **Summary**: Nowadays, deep learning can be employed to a wide ranges of fields including medicine, engineering, etc. In deep learning, Convolutional Neural Network (CNN) is extensively used in the pattern and sequence recognition, video analysis, natural language processing, spam detection, topic categorization, regression analysis, speech recognition, image classification, object detection, segmentation, face recognition, robotics, and control. The benefits associated with its near human level accuracies in large applications lead to the growing acceptance of CNN in recent years. The primary contribution of this paper is to analyze the impact of the pattern of the hidden layers of a CNN over the overall performance of the network. To demonstrate this influence, we applied neural network with different layers on the Modified National Institute of Standards and Technology (MNIST) dataset. Also, is to observe the variations of accuracies of the network for various numbers of hidden layers and epochs and to make comparison and contrast among them. The system is trained utilizing stochastic gradient and backpropagation algorithm and tested with feedforward algorithm.



### Study and Observation of the Variations of Accuracies for Handwritten Digits Recognition with Various Hidden Layers and Epochs using Neural Network Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1809.06188v3
- **DOI**: 10.1109/CEEICT.2018.8628144
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1809.06188v3)
- **Published**: 2018-09-17 13:28:19+00:00
- **Updated**: 2018-11-29 16:54:07+00:00
- **Authors**: Md. Abu Bakr Siddique, Mohammad Mahmudur Rahman Khan, Rezoana Bente Arif, Zahidun Ashrafi
- **Comment**: To be published in the 4th IEEE International Conference on
  Electrical Engineering and Information & Communication Technology (iCEEiCT
  2018)
- **Journal**: 2018 4th International Conference on Electrical Engineering and
  Information & Communication Technology (iCEEiCT)
- **Summary**: In recent days, Artificial Neural Network (ANN) can be applied to a vast majority of fields including business, medicine, engineering, etc. The most popular areas where ANN is employed nowadays are pattern and sequence recognition, novelty detection, character recognition, regression analysis, speech recognition, image compression, stock market prediction, Electronic nose, security, loan applications, data processing, robotics, and control. The benefits associated with its broad applications leads to increasing popularity of ANN in the era of 21st Century. ANN confers many benefits such as organic learning, nonlinear data processing, fault tolerance, and self-repairing compared to other conventional approaches. The primary objective of this paper is to analyze the influence of the hidden layers of a neural network over the overall performance of the network. To demonstrate this influence, we applied neural network with different layers on the MNIST dataset. Also, another goal is to observe the variations of accuracies of ANN for different numbers of hidden layers and epochs and to compare and contrast among them.



### ADBSCAN: Adaptive Density-Based Spatial Clustering of Applications with Noise for Identifying Clusters with Varying Densities
- **Arxiv ID**: http://arxiv.org/abs/1809.06189v3
- **DOI**: 10.1109/CEEICT.2018.8628138
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.06189v3)
- **Published**: 2018-09-17 13:28:35+00:00
- **Updated**: 2018-11-29 16:51:05+00:00
- **Authors**: Mohammad Mahmudur Rahman Khan, Md. Abu Bakr Siddique, Rezoana Bente Arif, Mahjabin Rahman Oishe
- **Comment**: To be published in the 4th IEEE International Conference on
  Electrical Engineering and Information & Communication Technology (iCEEiCT
  2018)
- **Journal**: 2018 4th International Conference on Electrical Engineering and
  Information & Communication Technology (iCEEiCT)
- **Summary**: Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm which has the high-performance rate for dataset where clusters have the constant density of data points. One of the significant attributes of this algorithm is noise cancellation. However, DBSCAN demonstrates reduced performances for clusters with different densities. Therefore, in this paper, an adaptive DBSCAN is proposed which can work significantly well for identifying clusters with varying densities.



### Multi Modal Convolutional Neural Networks for Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1809.06191v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06191v2)
- **Published**: 2018-09-17 13:33:21+00:00
- **Updated**: 2018-09-20 07:27:38+00:00
- **Authors**: Mehmet Aygün, Yusuf Hüseyin Şahin, Gözde Ünal
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a multi-modal Convolutional Neural Network (CNN) approach for brain tumor segmentation. We investigate how to combine different modalities efficiently in the CNN framework.We adapt various fusion methods, which are previously employed on video recognition problem, to the brain tumor segmentation problem,and we investigate their efficiency in terms of memory and performance.Our experiments, which are performed on BRATS dataset, lead us to the conclusion that learning separate representations for each modality and combining them for brain tumor segmentation could increase the performance of CNN systems.



### From Same Photo: Cheating on Visual Kinship Challenges
- **Arxiv ID**: http://arxiv.org/abs/1809.06200v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06200v2)
- **Published**: 2018-09-17 13:49:44+00:00
- **Updated**: 2018-11-05 12:09:39+00:00
- **Authors**: Mitchell Dawson, Andrew Zisserman, Christoffer Nellåker
- **Comment**: Accepted to ACCV 2018
- **Journal**: None
- **Summary**: With the propensity for deep learning models to learn unintended signals from data sets there is always the possibility that the network can `cheat' in order to solve a task. In the instance of data sets for visual kinship verification, one such unintended signal could be that the faces are cropped from the same photograph, since faces from the same photograph are more likely to be from the same family. In this paper we investigate the influence of this artefactual data inference in published data sets for kinship verification.   To this end, we obtain a large dataset, and train a CNN classifier to determine if two faces are from the same photograph or not. Using this classifier alone as a naive classifier of kinship, we demonstrate near state of the art results on five public benchmark data sets for kinship verification - achieving over 90% accuracy on one of them. Thus, we conclude that faces derived from the same photograph are a strong inadvertent signal in all the data sets we examined, and it is likely that the fraction of kinship explained by existing kinship models is small.



### Sensor Transfer: Learning Optimal Sensor Effect Image Augmentation for Sim-to-Real Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1809.06256v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06256v2)
- **Published**: 2018-09-17 14:44:42+00:00
- **Updated**: 2019-01-07 17:01:45+00:00
- **Authors**: Alexandra Carlson, Katherine A. Skinner, Ram Vasudevan, Matthew Johnson-Roberson
- **Comment**: None
- **Journal**: None
- **Summary**: Performance on benchmark datasets has drastically improved with advances in deep learning. Still, cross-dataset generalization performance remains relatively low due to the domain shift that can occur between two different datasets. This domain shift is especially exaggerated between synthetic and real datasets. Significant research has been done to reduce this gap, specifically via modeling variation in the spatial layout of a scene, such as occlusions, and scene environmental factors, such as time of day and weather effects. However, few works have addressed modeling the variation in the sensor domain as a means of reducing the synthetic to real domain gap. The camera or sensor used to capture a dataset introduces artifacts into the image data that are unique to the sensor model, suggesting that sensor effects may also contribute to domain shift. To address this, we propose a learned augmentation network composed of physically-based augmentation functions. Our proposed augmentation pipeline transfers specific effects of the sensor model -- chromatic aberration, blur, exposure, noise, and color temperature -- from a real dataset to a synthetic dataset. We provide experiments that demonstrate that augmenting synthetic training datasets with the proposed learned augmentation framework reduces the domain gap between synthetic and real domains for object detection in urban driving scenes.



### Industrial Smoke Detection and Visualization
- **Arxiv ID**: http://arxiv.org/abs/1809.06263v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06263v1)
- **Published**: 2018-09-17 14:51:56+00:00
- **Updated**: 2018-09-17 14:51:56+00:00
- **Authors**: Yen-Chia Hsu, Paul Dille, Randy Sargent, Illah Nourbakhsh
- **Comment**: None
- **Journal**: None
- **Summary**: As sensing technology proliferates and becomes affordable to the general public, there is a growing trend in citizen science where scientists and volunteers form a strong partnership in conducting scientific research including problem finding, data collection, analysis, visualization, and storytelling. Providing easy-to-use computational tools to support citizen science has become an important issue. To raise the public awareness of environmental science and improve the air quality in local areas, we are currently collaborating with a local community in monitoring and documenting fugitive emissions from a coke refinery. We have helped the community members build a live camera system which captures and visualizes high resolution timelapse imagery starting from November 2014. However, searching and documenting smoke emissions manually from all video frames requires manpower and takes an impractical investment of time. This paper describes a software tool which integrates four features: (1) an algorithm based on change detection and texture segmentation for identifying smoke emissions; (2) an interactive timeline visualization providing indicators for seeking to interesting events; (3) an autonomous fast-forwarding mode for skipping uninteresting timelapse frames; and (4) a collection of animated smoke images generated automatically according to the algorithm for documentation, presentation, storytelling, and sharing. With the help of this tool, citizen scientists can now focus on the content of the story instead of time-consuming and laborious works.



### Learning Effective RGB-D Representations for Scene Recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.06269v1
- **DOI**: 10.1109/TIP.2018.2872629
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06269v1)
- **Published**: 2018-09-17 15:07:47+00:00
- **Updated**: 2018-09-17 15:07:47+00:00
- **Authors**: Xinhang Song, Shuqiang Jiang, Luis Herranz, Chengpeng Chen
- **Comment**: Accepted at IEEE Transactions on Image Processing
- **Journal**: IEEE Transactions on Image Processing, vol. 28, no. 2, pp.
  980-993, Feb. 2019
- **Summary**: Deep convolutional networks (CNN) can achieve impressive results on RGB scene recognition thanks to large datasets such as Places. In contrast, RGB-D scene recognition is still underdeveloped in comparison, due to two limitations of RGB-D data we address in this paper. The first limitation is the lack of depth data for training deep learning models. Rather than fine tuning or transferring RGB-specific features, we address this limitation by proposing an architecture and a two-step training approach that directly learns effective depth-specific features using weak supervision via patches. The resulting RGB-D model also benefits from more complementary multimodal features. Another limitation is the short range of depth sensors (typically 0.5m to 5.5m), resulting in depth images not capturing distant objects in the scenes that RGB images can. We show that this limitation can be addressed by using RGB-D videos, where more comprehensive depth information is accumulated as the camera travels across the scene. Focusing on this scenario, we introduce the ISIA RGB-D video dataset to evaluate RGB-D scene recognition with videos. Our video recognition architecture combines convolutional and recurrent neural networks (RNNs) that are trained in three steps with increasingly complex data to learn effective features (i.e. patches, frames and sequences). Our approach obtains state-of-the-art performances on RGB-D image (NYUD2 and SUN RGB-D) and video (ISIA RGB-D) scene recognition.



### Retrospective correction of Rigid and Non-Rigid MR motion artifacts using GANs
- **Arxiv ID**: http://arxiv.org/abs/1809.06276v2
- **DOI**: 10.1109/ISBI.2019.8759509
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06276v2)
- **Published**: 2018-09-17 15:30:17+00:00
- **Updated**: 2018-10-06 19:11:08+00:00
- **Authors**: Karim Armanious, Sergios Gatidis, Konstantin Nikolaou, Bin Yang, Thomas Küstner
- **Comment**: 5 pages, 2 figures, under review for the IEEE International Symposium
  for Biomedical Images
- **Journal**: None
- **Summary**: Motion artifacts are a primary source of magnetic resonance (MR) image quality deterioration with strong repercussions on diagnostic performance. Currently, MR motion correction is carried out either prospectively, with the help of motion tracking systems, or retrospectively by mainly utilizing computationally expensive iterative algorithms. In this paper, we utilize a new adversarial framework, titled MedGAN, for the joint retrospective correction of rigid and non-rigid motion artifacts in different body regions and without the need for a reference image. MedGAN utilizes a unique combination of non-adversarial losses and a new generator architecture to capture the textures and fine-detailed structures of the desired artifact-free MR images. Quantitative and qualitative comparisons with other adversarial techniques have illustrated the proposed model performance.



### Computer-Aided Diagnosis of Label-Free 3-D Optical Coherence Microscopy Images of Human Cervical Tissue
- **Arxiv ID**: http://arxiv.org/abs/1809.10196v1
- **DOI**: 10.1109/TBME.2018.2890167
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10196v1)
- **Published**: 2018-09-17 16:45:13+00:00
- **Updated**: 2018-09-17 16:45:13+00:00
- **Authors**: Yutao Ma, Tao Xu, Xiaolei Huang, Xiaofang Wang, Canyu Li, Jason Jerwick, Yuan Ning, Xianxu Zeng, Baojin Wang, Yihong Wang, Zhan Zhang, Xiaoan Zhang, Chao Zhou
- **Comment**: 9 pages, 5 figures, and 2 tables
- **Journal**: IEEE Transactions on Biomedical Engineering, 2019, 66(9):
  2447-2456
- **Summary**: Objective: Ultrahigh-resolution optical coherence microscopy (OCM) has recently demonstrated its potential for accurate diagnosis of human cervical diseases. One major challenge for clinical adoption, however, is the steep learning curve clinicians need to overcome to interpret OCM images. Developing an intelligent technique for computer-aided diagnosis (CADx) to accurately interpret OCM images will facilitate clinical adoption of the technology and improve patient care. Methods: 497 high-resolution 3-D OCM volumes (600 cross-sectional images each) were collected from 159 ex vivo specimens of 92 female patients. OCM image features were extracted using a convolutional neural network (CNN) model, concatenated with patient information (e.g., age, HPV results), and classified using a support vector machine classifier. Ten-fold cross-validations were utilized to test the performance of the CADx method in a five-class classification task and a binary classification task. Results: An 88.3 plus or minus 4.9% classification accuracy was achieved for five fine-grained classes of cervical tissue, namely normal, ectropion, low-grade and high-grade squamous intraepithelial lesions (LSIL and HSIL), and cancer. In the binary classification task (low-risk [normal, ectropion and LSIL] vs. high-risk [HSIL and cancer]), the CADx method achieved an area-under-the-curve (AUC) value of 0.959 with an 86.7 plus or minus 11.4% sensitivity and 93.5 plus or minus 3.8% specificity. Conclusion: The proposed deep-learning based CADx method outperformed three human experts. It was also able to identify morphological characteristics in OCM images that were consistent with histopathological interpretations. Significance: Label-free OCM imaging, combined with deep-learning based CADx methods, hold a great promise to be used in clinical settings for the effective screening and diagnosis of cervical diseases.



### Efficient Dense Modules of Asymmetric Convolution for Real-Time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1809.06323v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06323v3)
- **Published**: 2018-09-17 16:52:46+00:00
- **Updated**: 2019-12-28 06:11:57+00:00
- **Authors**: Shao-Yuan Lo, Hsueh-Ming Hang, Sheng-Wei Chan, Jing-Jhih Lin
- **Comment**: Accepted in ACM International Conference on Multimedia in Asia
  (MMAsia) 2019 [Best Paper Award]
- **Journal**: None
- **Summary**: Real-time semantic segmentation plays an important role in practical applications such as self-driving and robots. Most semantic segmentation research focuses on improving estimation accuracy with little consideration on efficiency. Several previous studies that emphasize high-speed inference often fail to produce high-accuracy segmentation results. In this paper, we propose a novel convolutional network named Efficient Dense modules with Asymmetric convolution (EDANet), which employs an asymmetric convolution structure and incorporates dilated convolution and dense connectivity to achieve high efficiency at low computational cost and model size. EDANet is 2.7 times faster than the existing fast segmentation network, ICNet, while it achieves a similar mIoU score without any additional context module, post-processing scheme, and pretrained model. We evaluate EDANet on Cityscapes and CamVid datasets, and compare it with the other state-of-art systems. Our network can run with the high-resolution inputs at the speed of 108 FPS on one GTX 1080Ti.



### Apple Flower Detection using Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.06357v1
- **DOI**: 10.1016/j.compind.2018.03.010
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06357v1)
- **Published**: 2018-09-17 17:46:40+00:00
- **Updated**: 2018-09-17 17:46:40+00:00
- **Authors**: Philipe A. Dias, Amy Tabb, Henry Medeiros
- **Comment**: 14 pages
- **Journal**: Computers in Industry, vol. 99, pp. 17-28, Aug. 2018
- **Summary**: To optimize fruit production, a portion of the flowers and fruitlets of apple trees must be removed early in the growing season. The proportion to be removed is determined by the bloom intensity, i.e., the number of flowers present in the orchard. Several automated computer vision systems have been proposed to estimate bloom intensity, but their overall performance is still far from satisfactory even in relatively controlled environments. With the goal of devising a technique for flower identification which is robust to clutter and to changes in illumination, this paper presents a method in which a pre-trained convolutional neural network is fine-tuned to become specially sensitive to flowers. Experimental results on a challenging dataset demonstrate that our method significantly outperforms three approaches that represent the state of the art in flower detection, with recall and precision rates higher than $90\%$. Moreover, a performance assessment on three additional datasets previously unseen by the network, which consist of different flower species and were acquired under different conditions, reveals that the proposed method highly surpasses baseline approaches in terms of generalization capability.



### Déjà Vu: an empirical evaluation of the memorization properties of ConvNets
- **Arxiv ID**: http://arxiv.org/abs/1809.06396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06396v1)
- **Published**: 2018-09-17 18:25:08+00:00
- **Updated**: 2018-09-17 18:25:08+00:00
- **Authors**: Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Hervé Jégou
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks memorize part of their training data, which is why strategies such as data augmentation and drop-out are employed to mitigate overfitting. This paper considers the related question of "membership inference", where the goal is to determine if an image was used during training. We consider it under three complementary angles. We show how to detect which dataset was used to train a model, and in particular whether some validation images were used at train time. We then analyze explicit memorization and extend classical random label experiments to the problem of learning a model that predicts if an image belongs to an arbitrary set. Finally, we propose a new approach to infer membership when a few of the top layers are not available or have been fine-tuned, and show that lower layers still carry information about the training samples. To support our findings, we conduct large-scale experiments on Imagenet and subsets of YFCC-100M with modern architectures such as VGG and Resnet.



### Segmenting root systems in X-ray computed tomography images using level sets
- **Arxiv ID**: http://arxiv.org/abs/1809.06398v1
- **DOI**: 10.1109/WACV.2018.00070
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06398v1)
- **Published**: 2018-09-17 18:31:51+00:00
- **Updated**: 2018-09-17 18:31:51+00:00
- **Authors**: Amy Tabb, Keith E. Duncan, Christopher N. Topp
- **Comment**: 11 pages
- **Journal**: 2018 IEEE Winter Conference on Applications of Computer Vision
  (WACV), Lake Tahoe, NV/CA. pp. 586-595
- **Summary**: The segmentation of plant roots from soil and other growing media in X-ray computed tomography images is needed to effectively study the root system architecture without excavation. However, segmentation is a challenging problem in this context because the root and non-root regions share similar features. In this paper, we describe a method based on level sets and specifically adapted for this segmentation problem. In particular, we deal with the issues of using a level sets approach on large image volumes for root segmentation, and track active regions of the front using an occupancy grid. This method allows for straightforward modifications to a narrow-band algorithm such that excessive forward and backward movements of the front can be avoided, distance map computations in a narrow band context can be done in linear time through modification of Meijster et al.'s distance transform algorithm, and regions of the image volume are iteratively used to estimate distributions for root versus non-root classes. Results are shown of three plant species of different maturity levels, grown in three different media. Our method compares favorably to a state-of-the-art method for root segmentation in X-ray CT image volumes.



### Crowdsourcing Lung Nodules Detection and Annotation
- **Arxiv ID**: http://arxiv.org/abs/1809.06402v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06402v1)
- **Published**: 2018-09-17 18:41:58+00:00
- **Updated**: 2018-09-17 18:41:58+00:00
- **Authors**: Saeed Boorboor, Saad Nadeem, Ji Hwan Park, Kevin Baker, Arie Kaufman
- **Comment**: 7 pages, SPIE Medical Imaging 2018
- **Journal**: None
- **Summary**: We present crowdsourcing as an additional modality to aid radiologists in the diagnosis of lung cancer from clinical chest computed tomography (CT) scans. More specifically, a complete workflow is introduced which can help maximize the sensitivity of lung nodule detection by utilizing the collective intelligence of the crowd. We combine the concept of overlapping thin-slab maximum intensity projections (TS-MIPs) and cine viewing to render short videos that can be outsourced as an annotation task to the crowd. These videos are generated by linearly interpolating overlapping TS-MIPs of CT slices through the depth of each quadrant of a patient's lung. The resultant videos are outsourced to an online community of non-expert users who, after a brief tutorial, annotate suspected nodules in these video segments. Using our crowdsourcing workflow, we achieved a lung nodule detection sensitivity of over 90% for 20 patient CT datasets (containing 178 lung nodules with sizes between 1-30mm), and only 47 false positives from a total of 1021 annotations on nodules of all sizes (96% sensitivity for nodules$>$4mm). These results show that crowdsourcing can be a robust and scalable modality to aid radiologists in screening for lung cancer, directly or in combination with computer-aided detection (CAD) algorithms. For CAD algorithms, the presented workflow can provide highly accurate training data to overcome the high false-positive rate (per scan) problem. We also provide, for the first time, analysis on nodule size and position which can help improve CAD algorithms.



### Crowd-Assisted Polyp Annotation of Virtual Colonoscopy Videos
- **Arxiv ID**: http://arxiv.org/abs/1809.06408v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06408v1)
- **Published**: 2018-09-17 18:58:44+00:00
- **Updated**: 2018-09-17 18:58:44+00:00
- **Authors**: Ji Hwan Park, Saad Nadeem, Joseph Marino, Kevin Baker, Matthew Barish, Arie Kaufman
- **Comment**: 7 pages, SPIE Medical Imaging 2018
- **Journal**: None
- **Summary**: Virtual colonoscopy (VC) allows a radiologist to navigate through a 3D colon model reconstructed from a computed tomography scan of the abdomen, looking for polyps, the precursors of colon cancer. Polyps are seen as protrusions on the colon wall and haustral folds, visible in the VC fly-through videos. A complete review of the colon surface requires full navigation from the rectum to the cecum in antegrade and retrograde directions, which is a tedious task that takes an average of 30 minutes. Crowdsourcing is a technique for non-expert users to perform certain tasks, such as image or video annotation. In this work, we use crowdsourcing for the examination of complete VC fly-through videos for polyp annotation by non-experts. The motivation for this is to potentially help the radiologist reach a diagnosis in a shorter period of time, and provide a stronger confirmation of the eventual diagnosis. The crowdsourcing interface includes an interactive tool for the crowd to annotate suspected polyps in the video with an enclosing box. Using our workflow, we achieve an overall polyps-per-patient sensitivity of 87.88% (95.65% for polyps $\geq$5mm and 70% for polyps $<$5mm). We also demonstrate the efficacy and effectiveness of a non-expert user in detecting and annotating polyps and discuss their possibility in aiding radiologists in VC examinations.



### Radiative Transport Based Flame Volume Reconstruction from Videos
- **Arxiv ID**: http://arxiv.org/abs/1809.06417v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06417v1)
- **Published**: 2018-09-17 19:57:06+00:00
- **Updated**: 2018-09-17 19:57:06+00:00
- **Authors**: Liang Shen, Dengming Zhu, Saad Nadeem, Zhaoqi Wang, Arie Kaufman
- **Comment**: IEEE Transactions on Visualization and Computer Graphics, 24(7):
  2209-2222, 2018
- **Journal**: None
- **Summary**: We introduce a novel approach for flame volume reconstruction from videos using inexpensive charge-coupled device (CCD) consumer cameras. The approach includes an economical data capture technique using inexpensive CCD cameras. Leveraging the smear feature of the CCD chip, we present a technique for synchronizing CCD cameras while capturing flame videos from different views. Our reconstruction is based on the radiative transport equation which enables complex phenomena such as emission, extinction, and scattering to be used in the rendering process. Both the color intensity and temperature reconstructions are implemented using the CUDA parallel computing framework, which provides real-time performance and allows visualization of reconstruction results after every iteration. We present the results of our approach using real captured data and physically-based simulated data. Finally, we also compare our approach against the other state-of-the-art flame volume reconstruction methods and demonstrate the efficacy and efficiency of our approach in four different applications: (1) rendering of reconstructed flames in virtual environments, (2) rendering of reconstructed flames in augmented reality, (3) flame stylization, and (4) reconstruction of other semitransparent phenomena.



### Toward Bridging the Simulated-to-Real Gap: Benchmarking Super-Resolution on Real Data
- **Arxiv ID**: http://arxiv.org/abs/1809.06420v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06420v2)
- **Published**: 2018-09-17 19:59:12+00:00
- **Updated**: 2019-06-16 20:21:55+00:00
- **Authors**: Thomas Köhler, Michel Bätz, Farzad Naderi, André Kaup, Andreas Maier, Christian Riess
- **Comment**: To appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence; data and source code available at
  https://superresolution.tf.fau.de/
- **Journal**: None
- **Summary**: Capturing ground truth data to benchmark super-resolution (SR) is challenging. Therefore, current quantitative studies are mainly evaluated on simulated data artificially sampled from ground truth images. We argue that such evaluations overestimate the actual performance of SR methods compared to their behavior on real images. Toward bridging this simulated-to-real gap, we introduce the Super-Resolution Erlangen (SupER) database, the first comprehensive laboratory SR database of all-real acquisitions with pixel-wise ground truth. It consists of more than 80k images of 14 scenes combining different facets: CMOS sensor noise, real sampling at four resolution levels, nine scene motion types, two photometric conditions, and lossy video coding at five levels. As such, the database exceeds existing benchmarks by an order of magnitude in quality and quantity. This paper also benchmarks 19 popular single-image and multi-frame algorithms on our data. The benchmark comprises a quantitative study by exploiting ground truth data and qualitative evaluations in a large-scale observer study. We also rigorously investigate agreements between both evaluations from a statistical perspective. One interesting result is that top-performing methods on simulated data may be surpassed by others on real data. Our insights can spur further algorithm development, and the publicy available dataset can foster future evaluations.



### LMap: Shape-Preserving Local Mappings for Biomedical Visualization
- **Arxiv ID**: http://arxiv.org/abs/1809.06442v2
- **DOI**: 10.1109/TVCG.2017.2772237
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06442v2)
- **Published**: 2018-09-17 20:56:17+00:00
- **Updated**: 2018-10-25 20:24:11+00:00
- **Authors**: Saad Nadeem, Xianfeng Gu, Arie Kaufman
- **Comment**: IEEE Transactions on Visualization and Computer Graphics, 24(12):
  3111-3122, 2018 (12 pages, 11 figures)
- **Journal**: S. Nadeem, X. Gu, and A. Kaufman. LMap: Shape-Preserving Local
  Mappings for Biomedical Visualization. IEEE Transactions on Visualization and
  Computer Graphics, 24(12):3111-3122, 2018
- **Summary**: Visualization of medical organs and biological structures is a challenging task because of their complex geometry and the resultant occlusions. Global spherical and planar mapping techniques simplify the complex geometry and resolve the occlusions to aid in visualization. However, while resolving the occlusions these techniques do not preserve the geometric context, making them less suitable for mission-critical biomedical visualization tasks. In this paper, we present a shape-preserving local mapping technique for resolving occlusions locally while preserving the overall geometric context. More specifically, we present a novel visualization algorithm, LMap, for conformally parameterizing and deforming a selected local region-of-interest (ROI) on an arbitrary surface. The resultant shape-preserving local mappings help to visualize complex surfaces while preserving the overall geometric context. The algorithm is based on the robust and efficient extrinsic Ricci flow technique, and uses the dynamic Ricci flow algorithm to guarantee the existence of a local map for a selected ROI on an arbitrary surface. We show the effectiveness and efficacy of our method in three challenging use cases: (1) multimodal brain visualization, (2) optimal coverage of virtual colonoscopy centerline flythrough, and (3) molecular surface visualization.



### Mask Editor : an Image Annotation Tool for Image Segmentation Tasks
- **Arxiv ID**: http://arxiv.org/abs/1809.06461v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06461v1)
- **Published**: 2018-09-17 22:25:55+00:00
- **Updated**: 2018-09-17 22:25:55+00:00
- **Authors**: Chuanhai Zhang, Kurt Loken, Zhiyu Chen, Zhiyong Xiao, Gary Kunkel
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural network (DCNN) is the state-of-the-art method for image segmentation, which is one of key challenging computer vision tasks. However, DCNN requires a lot of training images with corresponding image masks to get a good segmentation result. Image annotation software which is easy to use and allows fast image mask generation is in great demand. To the best of our knowledge, all existing image annotation software support only drawing bounding polygons, bounding boxes, or bounding ellipses to mark target objects. These existing software are inefficient when targeting objects that have irregular shapes (e.g., defects in fabric images or tire images). In this paper we design an easy-to-use image annotation software called Mask Editor for image mask generation. Mask Editor allows drawing any bounding curve to mark objects and improves efficiency to mark objects with irregular shapes. Mask Editor also supports drawing bounding polygons, drawing bounding boxes, drawing bounding ellipses, painting, erasing, super-pixel-marking, image cropping, multi-class masks, mask loading, and mask modifying.



