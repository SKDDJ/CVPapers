# Arxiv Papers in cs.CV on 2018-09-22
### Unrestricted Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1809.08352v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.08352v1)
- **Published**: 2018-09-22 00:16:18+00:00
- **Updated**: 2018-09-22 00:16:18+00:00
- **Authors**: Tom B. Brown, Nicholas Carlini, Chiyuan Zhang, Catherine Olsson, Paul Christiano, Ian Goodfellow
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a two-player contest for evaluating the safety and robustness of machine learning systems, with a large prize pool. Unlike most prior work in ML robustness, which studies norm-constrained adversaries, we shift our focus to unconstrained adversaries. Defenders submit machine learning models, and try to achieve high accuracy and coverage on non-adversarial data while making no confident mistakes on adversarial inputs. Attackers try to subvert defenses by finding arbitrary unambiguous inputs where the model assigns an incorrect label with high confidence. We propose a simple unambiguous dataset ("bird-or- bicycle") to use as part of this contest. We hope this contest will help to more comprehensively evaluate the worst-case adversarial risk of machine learning models.



### Focus On What's Important: Self-Attention Model for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1809.08371v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08371v2)
- **Published**: 2018-09-22 02:45:10+00:00
- **Updated**: 2019-04-04 13:51:51+00:00
- **Authors**: Guanxiong Sun, Chengqin Ye, Kuanquan Wang
- **Comment**: some errors on it
- **Journal**: None
- **Summary**: Human pose estimation is an essential yet challenging task in computer vision. One of the reasons for this difficulty is that there are many redundant regions in the images. In this work, we proposed a convolutional network architecture combined with the novel attention model. We named it attention convolutional neural network (ACNN). ACNN learns to focus on specific regions of different input features. It's a multi-stage architecture. Early stages filtrate the "nothing-regions", such as background and redundant body parts. And then, they submit the important regions which contain the joints of the human body to the following stages to get a more accurate result. What's more, it does not require extra manual annotations and self-learning is one of our intentions. We separately trained the network because the attention learning task and the pose estimation task are not independent. State-of-the-art performance is obtained on the MPII benchmarks.



### Galaxy morphology prediction using capsule networks
- **Arxiv ID**: http://arxiv.org/abs/1809.08377v1
- **DOI**: 10.1093/mnras/stz915
- **Categories**: **astro-ph.IM**, astro-ph.GA, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1809.08377v1)
- **Published**: 2018-09-22 03:41:05+00:00
- **Updated**: 2018-09-22 03:41:05+00:00
- **Authors**: Reza Katebi, Yadi Zhou, Ryan Chornock, Razvan Bunescu
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Understanding morphological types of galaxies is a key parameter for studying their formation and evolution. Neural networks that have been used previously for galaxy morphology classification have some disadvantages, such as not being invariant under rotation. In this work, we studied the performance of Capsule Network, a recently introduced neural network architecture that is rotationally invariant and spatially aware, on the task of galaxy morphology classification. We designed two evaluation scenarios based on the answers from the question tree in the Galaxy Zoo project. In the first scenario, we used Capsule Network for regression and predicted probabilities for all of the questions. In the second scenario, we chose the answer to the first morphology question that had the highest user agreement as the class of the object and trained a Capsule Network classifier, where we also reconstructed galaxy images. We achieved promising results in both of these scenarios. Automated approaches such as the one introduced here will greatly decrease the workload of astronomers and will play a critical role in the upcoming large sky surveys.



### Learning to Localize and Align Fine-Grained Actions to Sparse Instructions
- **Arxiv ID**: http://arxiv.org/abs/1809.08381v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08381v1)
- **Published**: 2018-09-22 04:41:08+00:00
- **Updated**: 2018-09-22 04:41:08+00:00
- **Authors**: Meera Hahn, Nataniel Ruiz, Jean-Baptiste Alayrac, Ivan Laptev, James M. Rehg
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic generation of textual video descriptions that are time-aligned with video content is a long-standing goal in computer vision. The task is challenging due to the difficulty of bridging the semantic gap between the visual and natural language domains. This paper addresses the task of automatically generating an alignment between a set of instructions and a first person video demonstrating an activity. The sparse descriptions and ambiguity of written instructions create significant alignment challenges. The key to our approach is the use of egocentric cues to generate a concise set of action proposals, which are then matched to recipe steps using object recognition and computational linguistic techniques. We obtain promising results on both the Extended GTEA Gaze+ dataset and the Bristol Egocentric Object Interactions Dataset.



### Understanding Fake Faces
- **Arxiv ID**: http://arxiv.org/abs/1809.08391v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1809.08391v1)
- **Published**: 2018-09-22 06:42:34+00:00
- **Updated**: 2018-09-22 06:42:34+00:00
- **Authors**: Ryota Natsume, Kazuki Inoue, Yoshihiro Fukuhara, Shintaro Yamamoto, Shigeo Morishima, Hirokatsu Kataoka
- **Comment**: 11 pages, 3 figures, ECCV 2018 Workshop on Brain-Driven Computer
  Vision (BDCV)
- **Journal**: None
- **Summary**: Face recognition research is one of the most active topics in computer vision (CV), and deep neural networks (DNN) are now filling the gap between human-level and computer-driven performance levels in face verification algorithms. However, although the performance gap appears to be narrowing in terms of accuracy-based expectations, a curious question has arisen; specifically, "Face understanding of AI is really close to that of human?" In the present study, in an effort to confirm the brain-driven concept, we conduct image-based detection, classification, and generation using an in-house created fake face database. This database has two configurations: (i) false positive face detections produced using both the Viola Jones (VJ) method and convolutional neural networks (CNN), and (ii) simulacra that have fundamental characteristics that resemble faces but are completely artificial. The results show a level of suggestive knowledge that indicates the continuing existence of a gap between the capabilities of recent vision-based face recognition algorithms and human-level performance. On a positive note, however, we have obtained knowledge that will advance the progress of face-understanding models.



### Geometric Multi-Model Fitting by Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.08397v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.08397v2)
- **Published**: 2018-09-22 07:13:05+00:00
- **Updated**: 2018-12-27 12:47:47+00:00
- **Authors**: Zongliang Zhang, Hongbin Zeng, Jonathan Li, Yiping Chen, Chenhui Yang, Cheng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper deals with the geometric multi-model fitting from noisy, unstructured point set data (e.g., laser scanned point clouds). We formulate multi-model fitting problem as a sequential decision making process. We then use a deep reinforcement learning algorithm to learn the optimal decisions towards the best fitting result. In this paper, we have compared our method against the state-of-the-art on simulated data. The results demonstrated that our approach significantly reduced the number of fitting iterations.



### RPNet: an End-to-End Network for Relative Camera Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1809.08402v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08402v1)
- **Published**: 2018-09-22 08:03:38+00:00
- **Updated**: 2018-09-22 08:03:38+00:00
- **Authors**: Sovann En, Alexis Lechervy, Frédéric Jurie
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the task of relative camera pose estimation from raw image pixels, by means of deep neural networks. The proposed RPNet network takes pairs of images as input and directly infers the relative poses, without the need of camera intrinsic/extrinsic. While state-of-the-art systems based on SIFT + RANSAC, are able to recover the translation vector only up to scale, RPNet is trained to produce the full translation vector, in an end-to-end way. Experimental results on the Cambridge Landmark dataset show very promising results regarding the recovery of the full translation vector. They also show that RPNet produces more accurate and more stable results than traditional approaches, especially for hard images (repetitive textures, textureless images, etc). To the best of our knowledge, RPNet is the first attempt to recover full translation vectors in relative pose estimation.



### Active image restoration
- **Arxiv ID**: http://arxiv.org/abs/1809.08406v1
- **DOI**: 10.1103/PhysRevE.98.052108
- **Categories**: **cond-mat.stat-mech**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.08406v1)
- **Published**: 2018-09-22 08:32:43+00:00
- **Updated**: 2018-09-22 08:32:43+00:00
- **Authors**: Rongrong Xie, Shengfeng Deng, Weibing Deng, Armen E. Allahverdyan
- **Comment**: 17 pages, 6 figures
- **Journal**: Phys. Rev. E 98, 052108 (2018)
- **Summary**: We study active restoration of noise-corrupted images generated via the Gibbs probability of an Ising ferromagnet in external magnetic field. Ferromagnetism accounts for the prior expectation of data smoothness, i.e. a positive correlation between neighbouring pixels (Ising spins), while the magnetic field refers to the bias. The restoration is actively supervised by requesting the true values of certain pixels after a noisy observation. This additional information improves restoration of other pixels. The optimal strategy of active inference is not known for realistic (two-dimensional) images. We determine this strategy for the mean-field version of the model and show that it amounts to supervising the values of spins (pixels) that do not agree with the sign of the average magnetization. The strategy leads to a transparent analytical expression for the minimal Bayesian risk, and shows that there is a maximal number of pixels beyond of which the supervision is useless. We show numerically that this strategy applies for two-dimensional images away from the critical regime. Within this regime the strategy is outperformed by its local (adaptive) version, which supervises pixels that do not agree with their Bayesian estimate. We show on transparent examples how active supervising can be essential in recovering noise-corrupted images and advocate for a wider usage of active methods in image restoration.



### Implementation of Fuzzy C-Means and Possibilistic C-Means Clustering Algorithms, Cluster Tendency Analysis and Cluster Validation
- **Arxiv ID**: http://arxiv.org/abs/1809.08417v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.08417v3)
- **Published**: 2018-09-22 09:45:05+00:00
- **Updated**: 2019-05-12 06:10:21+00:00
- **Authors**: Md. Abu Bakr Siddique, Rezoana Bente Arif, Mohammad Mahmudur Rahman Khan, Zahidun Ashrafi
- **Comment**: 8 pages, 13 figures, 8 tables
- **Journal**: None
- **Summary**: In this paper, several two-dimensional clustering scenarios are given. In those scenarios, soft partitioning clustering algorithms (Fuzzy C-means (FCM) and Possibilistic c-means (PCM)) are applied. Afterward, VAT is used to investigate the clustering tendency visually, and then in order of checking cluster validation, three types of indices (e.g., PC, DI, and DBI) were used. After observing the clustering algorithms, it was evident that each of them has its limitations; however, PCM is more robust to noise than FCM as in case of FCM a noise point has to be considered as a member of any of the cluster.



### Pose-Guided Multi-Granularity Attention Network for Text-Based Person Search
- **Arxiv ID**: http://arxiv.org/abs/1809.08440v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08440v3)
- **Published**: 2018-09-22 14:18:41+00:00
- **Updated**: 2019-11-27 03:03:21+00:00
- **Authors**: Ya Jing, Chenyang Si, Junbo Wang, Wei Wang, Liang Wang, Tieniu Tan
- **Comment**: published in AAAI2020(oral)
- **Journal**: None
- **Summary**: Text-based person search aims to retrieve the corresponding person images in an image database by virtue of a describing sentence about the person, which poses great potential for various applications such as video surveillance. Extracting visual contents corresponding to the human description is the key to this cross-modal matching problem. Moreover, correlated images and descriptions involve different granularities of semantic relevance, which is usually ignored in previous methods. To exploit the multilevel corresponding visual contents, we propose a pose-guided multi-granularity attention network (PMA). Firstly, we propose a coarse alignment network (CA) to select the related image regions to the global description by a similarity-based attention. To further capture the phrase-related visual body part, a fine-grained alignment network (FA) is proposed, which employs pose information to learn latent semantic alignment between visual body part and textual noun phrase. To verify the effectiveness of our model, we perform extensive experiments on the CUHK Person Description Dataset (CUHK-PEDES) which is currently the only available dataset for text-based person search. Experimental results show that our approach outperforms the state-of-the-art methods by 15 \% in terms of the top-1 metric.



### Artistic Instance-Aware Image Filtering by Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.08448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08448v1)
- **Published**: 2018-09-22 15:40:45+00:00
- **Updated**: 2018-09-22 15:40:45+00:00
- **Authors**: Milad Tehrani, Mahnoosh Bagheri, Mahdi Ahmadi, Alireza Norouzi, Nader Karimi, Shadrokh Samavi
- **Comment**: 4 pages, 5 figures
- **Journal**: None
- **Summary**: In the recent years, public use of artistic effects for editing and beautifying images has encouraged researchers to look for new approaches to this task. Most of the existing methods apply artistic effects to the whole image. Exploitation of neural network vision technologies like object detection and semantic segmentation could be a new viewpoint in this area. In this paper, we utilize an instance segmentation neural network to obtain a class mask for separately filtering the background and foreground of an image. We implement a top prior-mask selection to let us select an object class for filtering purpose. Different artistic effects are used in the filtering process to meet the requirements of a vast variety of users. Also, our method is flexible enough to allow the addition of new filters. We use pre-trained Mask R-CNN instance segmentation on the COCO dataset as the segmentation network. Experimental results on the use of different filters are performed. System's output results show that this novel approach can create satisfying artistic images with fast operation and simple interface.



### Shift-based Primitives for Efficient Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.08458v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.08458v2)
- **Published**: 2018-09-22 17:43:28+00:00
- **Updated**: 2018-09-25 02:59:32+00:00
- **Authors**: Huasong Zhong, Xianggen Liu, Yihui He, Yuchun Ma
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a collection of three shift-based primitives for building efficient compact CNN-based networks. These three primitives (channel shift, address shift, shortcut shift) can reduce the inference time on GPU while maintains the prediction accuracy. These shift-based primitives only moves the pointer but avoids memory copy, thus very fast. For example, the channel shift operation is 12.7x faster compared to channel shuffle in ShuffleNet but achieves the same accuracy. The address shift and channel shift can be merged into the point-wise group convolution and invokes only a single kernel call, taking little time to perform spatial convolution and channel shift. Shortcut shift requires no time to realize residual connection through allocating space in advance. We blend these shift-based primitives with point-wise group convolution and built two inference-efficient CNN architectures named AddressNet and Enhanced AddressNet. Experiments on CIFAR100 and ImageNet datasets show that our models are faster and achieve comparable or better accuracy.



### Addressing Training Bias via Automated Image Annotation
- **Arxiv ID**: http://arxiv.org/abs/1809.10242v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.10242v2)
- **Published**: 2018-09-22 19:47:01+00:00
- **Updated**: 2018-10-10 12:09:25+00:00
- **Authors**: Zhujun Xiao, Yanzi Zhu, Yuxin Chen, Ben Y. Zhao, Junchen Jiang, Haitao Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Build accurate DNN models requires training on large labeled, context specific datasets, especially those matching the target scenario. We believe advances in wireless localization, working in unison with cameras, can produce automated annotation of targets on images and videos captured in the wild. Using pedestrian and vehicle detection as examples, we demonstrate the feasibility, benefits, and challenges of an automatic image annotation system. Our work calls for new technical development on passive localization, mobile data analytics, and error-resilient ML models, as well as design issues in user privacy policies.



### Parametric Synthesis of Text on Stylized Backgrounds using PGGANs
- **Arxiv ID**: http://arxiv.org/abs/1809.08488v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08488v1)
- **Published**: 2018-09-22 21:10:06+00:00
- **Updated**: 2018-09-22 21:10:06+00:00
- **Authors**: Mayank Gupta, Abhinav Kumar, Sriganesh Madhvanath
- **Comment**: None
- **Journal**: None
- **Summary**: We describe a novel method of generating high-resolution real-world images of text where the style and textual content of the images are described parametrically. Our method combines text to image retrieval techniques with progressive growing of Generative Adversarial Networks (PGGANs) to achieve conditional generation of photo-realistic images that reflect specific styles, as well as artifacts seen in real-world images. We demonstrate our method in the context of automotive license plates. We assess the impact of varying the number of training images of each style on the fidelity of the generated style, and demonstrate the quality of the generated images using license plate recognition systems.



### SelfKin: Self Adjusted Deep Model For Kinship Verification
- **Arxiv ID**: http://arxiv.org/abs/1809.08493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08493v1)
- **Published**: 2018-09-22 21:52:18+00:00
- **Updated**: 2018-09-22 21:52:18+00:00
- **Authors**: Eran Dahan, Yosi Keller
- **Comment**: None
- **Journal**: None
- **Summary**: One of the unsolved challenges in the field of biometrics and face recognition is Kinship Verification. This problem aims to understand if two people are family-related and how (sisters, brothers, etc.) Solving this problem can give rise to varied tasks and applications. In the area of homeland security (HLS) it is crucial to auto-detect if the person questioned is related to a wanted suspect, In the field of biometrics, kinship-verification can help to discriminate between families by photos and in the field of predicting or fashion it can help to predict an older or younger model of people faces. Lately, and with the advanced deep learning technology, this problem has gained focus from the research community in matters of data and research. In this article, we propose using a Deep Learning approach for solving the Kinship-Verification problem. Further, we offer a novel self-learning deep model, which learns the essential features from different faces. We show that our model wins the Recognize Families In the Wild(RFIW2018,FG2018) challenge and obtains state-of-the-art results. Moreover, we show that our proposed model can reduce the size of the network by half without loss in performance.



### SqueezeSegV2: Improved Model Structure and Unsupervised Domain Adaptation for Road-Object Segmentation from a LiDAR Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1809.08495v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08495v1)
- **Published**: 2018-09-22 22:04:49+00:00
- **Updated**: 2018-09-22 22:04:49+00:00
- **Authors**: Bichen Wu, Xuanyu Zhou, Sicheng Zhao, Xiangyu Yue, Kurt Keutzer
- **Comment**: Bichen Wu, Xuanyu Zhou, and Sicheng Zhao contributed equally to this
  paper
- **Journal**: None
- **Summary**: Earlier work demonstrates the promise of deep-learning-based approaches for point cloud segmentation; however, these approaches need to be improved to be practically useful. To this end, we introduce a new model SqueezeSegV2 that is more robust to dropout noise in LiDAR point clouds. With improved model structure, training loss, batch normalization and additional input channel, SqueezeSegV2 achieves significant accuracy improvement when trained on real data. Training models for point cloud segmentation requires large amounts of labeled point-cloud data, which is expensive to obtain. To sidestep the cost of collection and annotation, simulators such as GTA-V can be used to create unlimited amounts of labeled, synthetic data. However, due to domain shift, models trained on synthetic data often do not generalize well to the real world. We address this problem with a domain-adaptation training pipeline consisting of three major components: 1) learned intensity rendering, 2) geodesic correlation alignment, and 3) progressive domain calibration. When trained on real data, our new model exhibits segmentation accuracy improvements of 6.0-8.6% over the original SqueezeSeg. When training our new model on synthetic data using the proposed domain adaptation pipeline, we nearly double test accuracy on real-world data, from 29.0% to 57.4%. Our source code and synthetic dataset will be open-sourced.



