# Arxiv Papers in cs.CV on 2018-09-28
### Inverse Transport Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.10820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10820v1)
- **Published**: 2018-09-28 01:50:51+00:00
- **Updated**: 2018-09-28 01:50:51+00:00
- **Authors**: Chengqian Che, Fujun Luan, Shuang Zhao, Kavita Bala, Ioannis Gkioulekas
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce inverse transport networks as a learning architecture for inverse rendering problems where, given input image measurements, we seek to infer physical scene parameters such as shape, material, and illumination. During training, these networks are evaluated not only in terms of how close they can predict groundtruth parameters, but also in terms of whether the parameters they produce can be used, together with physically-accurate graphics renderers, to reproduce the input image measurements. To en- able training of inverse transport networks using stochastic gradient descent, we additionally create a general-purpose, physically-accurate differentiable renderer, which can be used to estimate derivatives of images with respect to arbitrary physical scene parameters. Our experiments demonstrate that inverse transport networks can be trained efficiently using differentiable rendering, and that they generalize to scenes with completely unseen geometry and illumination better than networks trained without appearance- matching regularization.



### Boundary-guided Feature Aggregation Network for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1809.10821v1
- **DOI**: 10.1109/LSP.2018.2875586
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10821v1)
- **Published**: 2018-09-28 01:50:51+00:00
- **Updated**: 2018-09-28 01:50:51+00:00
- **Authors**: Yunzhi Zhuge, Pingping Zhang, Huchuan Lu
- **Comment**: To appear in Signal Processing Letters (SPL), 5 pages, 5 figures and
  3 tables
- **Journal**: None
- **Summary**: Fully convolutional networks (FCN) has significantly improved the performance of many pixel-labeling tasks, such as semantic segmentation and depth estimation. However, it still remains non-trivial to thoroughly utilize the multi-level convolutional feature maps and boundary information for salient object detection. In this paper, we propose a novel FCN framework to integrate multi-level convolutional features recurrently with the guidance of object boundary information. First, a deep convolutional network is used to extract multi-level feature maps and separately aggregate them into multiple resolutions, which can be used to generate coarse saliency maps. Meanwhile, another boundary information extraction branch is proposed to generate boundary features. Finally, an attention-based feature fusion module is designed to fuse boundary information into salient regions to achieve accurate boundary inference and semantic enhancement. The final saliency maps are the combination of the predicted boundary maps and integrated saliency maps, which are more closer to the ground truths. Experiments and analysis on four large-scale benchmarks verify that our framework achieves new state-of-the-art results.



### Semantic Segmentation for Urban Planning Maps based on U-Net
- **Arxiv ID**: http://arxiv.org/abs/1809.10862v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.10862v2)
- **Published**: 2018-09-28 05:32:45+00:00
- **Updated**: 2018-10-01 03:00:11+00:00
- **Authors**: Zhiling Guo, Hiroaki Shengoku, Guangming Wu, Qi Chen, Wei Yuan, Xiaodan Shi, Xiaowei Shao, Yongwei Xu, Ryosuke Shibasaki
- **Comment**: 4 pages, 3 figures, conference, International Geoscience and Remote
  Sensing Symposium (IGARSS 2018), Jul 2018, Valencia, Spain
- **Journal**: None
- **Summary**: The automatic digitizing of paper maps is a significant and challenging task for both academia and industry. As an important procedure of map digitizing, the semantic segmentation section mainly relies on manual visual interpretation with low efficiency. In this study, we select urban planning maps as a representative sample and investigate the feasibility of utilizing U-shape fully convolutional based architecture to perform end-to-end map semantic segmentation. The experimental results obtained from the test area in Shibuya district, Tokyo, demonstrate that our proposed method could achieve a very high Jaccard similarity coefficient of 93.63% and an overall accuracy of 99.36%. For implementation on GPGPU and cuDNN, the required processing time for the whole Shibuya district can be less than three minutes. The results indicate the proposed method can serve as a viable tool for urban planning map semantic segmentation task with high accuracy and efficiency.



### Depth Reconstruction of Translucent Objects from a Single Time-of-Flight Camera using Deep Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.10917v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10917v1)
- **Published**: 2018-09-28 08:42:35+00:00
- **Updated**: 2018-09-28 08:42:35+00:00
- **Authors**: Seongjong Song, Hyunjung Shim
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel approach to recovering the translucent objects from a single time-of-flight (ToF) depth camera using deep residual networks. When recording the translucent objects using the ToF depth camera, their depth values are severely contaminated due to complex light interactions with the surrounding environment. While existing methods suggested new capture systems or developed the depth distortion models, their solutions were less practical because of strict assumptions or heavy computational complexity. In this paper, we adopt the deep residual networks for modeling the ToF depth distortion caused by translucency. To fully utilize both the local and semantic information of objects, multi-scale patches are used to predict the depth value. Based on the quantitative and qualitative evaluation on our benchmark database, we show the effectiveness and robustness of the proposed algorithm.



### Deep Adaptive Learning for Writer Identification based on Single Handwritten Word Images
- **Arxiv ID**: http://arxiv.org/abs/1809.10954v1
- **DOI**: 10.1016/j.patcog.2018.11.003
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10954v1)
- **Published**: 2018-09-28 10:40:23+00:00
- **Updated**: 2018-09-28 10:40:23+00:00
- **Authors**: Sheng He, Lambert Schomaker
- **Comment**: Under view of Pattern Recognition
- **Journal**: None
- **Summary**: There are two types of information in each handwritten word image: explicit information which can be easily read or derived directly, such as lexical content or word length, and implicit attributes such as the author's identity. Whether features learned by a neural network for one task can be used for another task remains an open question. In this paper, we present a deep adaptive learning method for writer identification based on single-word images using multi-task learning. An auxiliary task is added to the training process to enforce the emergence of reusable features. Our proposed method transfers the benefits of the learned features of a convolutional neural network from an auxiliary task such as explicit content recognition to the main task of writer identification in a single procedure. Specifically, we propose a new adaptive convolutional layer to exploit the learned deep features. A multi-task neural network with one or several adaptive convolutional layers is trained end-to-end, to exploit robust generic features for a specific main task, i.e., writer identification. Three auxiliary tasks, corresponding to three explicit attributes of handwritten word images (lexical content, word length and character attributes), are evaluated. Experimental results on two benchmark datasets show that the proposed deep adaptive learning method can improve the performance of writer identification based on single-word images, compared to non-adaptive and simple linear-adaptive approaches.



### Variational Bayesian Inference for Audio-Visual Tracking of Multiple Speakers
- **Arxiv ID**: http://arxiv.org/abs/1809.10961v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.10961v2)
- **Published**: 2018-09-28 11:03:03+00:00
- **Updated**: 2019-10-29 16:54:55+00:00
- **Authors**: Yutong Ban, Xavier Alameda-Pineda, Laurent Girin, Radu Horaud
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we address the problem of tracking multiple speakers via the fusion of visual and auditory information. We propose to exploit the complementary nature of these two modalities in order to accurately estimate smooth trajectories of the tracked persons, to deal with the partial or total absence of one of the modalities over short periods of time, and to estimate the acoustic status -- either speaking or silent -- of each tracked person along time. We propose to cast the problem at hand into a generative audio-visual fusion (or association) model formulated as a latent-variable temporal graphical model. This may well be viewed as the problem of maximizing the posterior joint distribution of a set of continuous and discrete latent variables given the past and current observations, which is intractable. We propose a variational inference model which amounts to approximate the joint distribution with a factorized distribution. The solution takes the form of a closed-form expectation maximization procedure. We describe in detail the inference algorithm, we evaluate its performance and we compare it with several baseline methods. These experiments show that the proposed audio-visual tracker performs well in informal meetings involving a time-varying number of people.



### Domain Generalization with Domain-Specific Aggregation Modules
- **Arxiv ID**: http://arxiv.org/abs/1809.10966v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10966v1)
- **Published**: 2018-09-28 11:24:02+00:00
- **Updated**: 2018-09-28 11:24:02+00:00
- **Authors**: Antonio D'Innocente, Barbara Caputo
- **Comment**: None
- **Journal**: None
- **Summary**: Visual recognition systems are meant to work in the real world. For this to happen, they must work robustly in any visual domain, and not only on the data used during training. Within this context, a very realistic scenario deals with domain generalization, i.e. the ability to build visual recognition algorithms able to work robustly in several visual domains, without having access to any information about target data statistic. This paper contributes to this research thread, proposing a deep architecture that maintains separated the information about the available source domains data while at the same time leveraging over generic perceptual information. We achieve this by introducing domain-specific aggregation modules that through an aggregation layer strategy are able to merge generic and specific information in an effective manner. Experiments on two different benchmark databases show the power of our approach, reaching the new state of the art in domain generalization.



### CNNs Fusion for Building Detection in Aerial Images for the Building Detection Challenge
- **Arxiv ID**: http://arxiv.org/abs/1809.10976v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10976v1)
- **Published**: 2018-09-28 12:04:33+00:00
- **Updated**: 2018-09-28 12:04:33+00:00
- **Authors**: Rémi Delassus, Romain Giot
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents our contribution to the DeepGlobe Building Detection Challenge. We enhanced the SpaceNet Challenge winning solution by proposing a new fusion strategy based on a deep combiner using segmentation both results of different CNN and input data to segment. Segmentation results for all cities have been significantly improved (between 1% improvement over the baseline for the smallest one to more than 7% for the largest one). The separation of adjacent buildings should be the next enhancement made to the solution.



### Local Frequency Interpretation and Non-Local Self-Similarity on Graph for Point Cloud Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1810.03973v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1810.03973v1)
- **Published**: 2018-09-28 13:08:49+00:00
- **Updated**: 2018-09-28 13:08:49+00:00
- **Authors**: Zeqing Fu, Wei Hu, Zongming Guo
- **Comment**: 11 pages, 11 figures, submitted to IEEE Transactions on Image
  Processing at 2018.09.04
- **Journal**: None
- **Summary**: As 3D scanning devices and depth sensors mature, point clouds have attracted increasing attention as a format for 3D object representation, with applications in various fields such as tele-presence, navigation and heritage reconstruction. However, point clouds usually exhibit holes of missing data, mainly due to the limitation of acquisition techniques and complicated structure. Further, point clouds are defined on irregular non-Euclidean domains, which is challenging to address especially with conventional signal processing tools. Hence, leveraging on recent advances in graph signal processing, we propose an efficient point cloud inpainting method, exploiting both the local smoothness and the non-local self-similarity in point clouds. Specifically, we first propose a frequency interpretation in graph nodal domain, based on which we introduce the local graph-signal smoothness prior in order to describe the local smoothness of point clouds. Secondly, we explore the characteristics of non-local self-similarity, by globally searching for the most similar area to the missing region. The similarity metric between two areas is defined based on the direct component and the anisotropic graph total variation of normals in each area. Finally, we formulate the hole-filling step as an optimization problem based on the selected most similar area and regularized by the graph-signal smoothness prior. Besides, we propose voxelization and automatic hole detection methods for the point cloud prior to inpainting. Experimental results show that the proposed approach outperforms four competing methods significantly, both in objective and subjective quality.



### Real-time Dynamic Object Detection for Autonomous Driving using Prior 3D-Maps
- **Arxiv ID**: http://arxiv.org/abs/1809.11036v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.11036v2)
- **Published**: 2018-09-28 14:04:31+00:00
- **Updated**: 2019-07-05 14:17:26+00:00
- **Authors**: B Ravi Kiran, Luis Roldão, Benat Irastorza, Renzo Verastegui, Sebastian Suss, Senthil Yogamani, Victor Talpaert, Alexandre Lepoutre, Guillaume Trehard
- **Comment**: Preprint Submission to ECCVW AutoNUE 2018 - v2 author name accent
  correction
- **Journal**: None
- **Summary**: Lidar has become an essential sensor for autonomous driving as it provides reliable depth estimation. Lidar is also the primary sensor used in building 3D maps which can be used even in the case of low-cost systems which do not use Lidar. Computation on Lidar point clouds is intensive as it requires processing of millions of points per second. Additionally there are many subsequent tasks such as clustering, detection, tracking and classification which makes real-time execution challenging. In this paper, we discuss real-time dynamic object detection algorithms which leverages previously mapped Lidar point clouds to reduce processing. The prior 3D maps provide a static background model and we formulate dynamic object detection as a background subtraction problem. Computation and modeling challenges in the mapping and online execution pipeline are described. We propose a rejection cascade architecture to subtract road regions and other 3D regions separately. We implemented an initial version of our proposed algorithm and evaluated the accuracy on CARLA simulator.



### Interest point detectors stability evaluation on ApolloScape dataset
- **Arxiv ID**: http://arxiv.org/abs/1809.11039v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.11039v1)
- **Published**: 2018-09-28 14:06:30+00:00
- **Updated**: 2018-09-28 14:06:30+00:00
- **Authors**: Jacek Komorowski, Konrad Czarnota, Tomasz Trzcinski, Lukasz Dabala, Simon Lynen
- **Comment**: None
- **Journal**: None
- **Summary**: In the recent years, a number of novel, deep-learning based, interest point detectors, such as LIFT, DELF, Superpoint or LF-Net was proposed. However there's a lack of a standard benchmark to evaluate suitability of these novel keypoint detectors for real-live applications such as autonomous driving. Traditional benchmarks (e.g. Oxford VGG) are rather limited, as they consist of relatively few images of mostly planar scenes taken in favourable conditions. In this paper we verify if the recent, deep-learning based interest point detectors have the advantage over the traditional, hand-crafted keypoint detectors. To this end, we evaluate stability of a number of hand crafted and recent, learning-based interest point detectors on the street-level view ApolloScape dataset.



### A Symmetric Keyring Encryption Scheme for Biometric Cryptosystems
- **Arxiv ID**: http://arxiv.org/abs/1809.11045v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.11045v1)
- **Published**: 2018-09-28 14:10:51+00:00
- **Updated**: 2018-09-28 14:10:51+00:00
- **Authors**: Yen-Lung Lai, Jung-Yeon Hwang, Zhe Jin, Soohyong Kim, Sangrae Cho, Andrew Beng Jin Teoh
- **Comment**: 15 pages, 5 figures, 5 tables
- **Journal**: None
- **Summary**: In this paper, we propose a novel biometric cryptosystem for vectorial biometrics named symmetric keyring encryption (SKE) inspired by Rivest's keyring model (2016). Unlike conventional biometric secret-binding primitives, such as fuzzy commitment and fuzzy vault, the proposed scheme reframes the biometric secret-binding problem as a fuzzy symmetric encryption problem with a notion called resilient vector pair. In this study, the pair resembles the encryption-decryption key pair in symmetric key cryptosystems. This notion is realized using the index of maximum hashed vectors - a special instance of the ranking-based locality-sensitive hashing function. With a simple filtering mechanism and [m,k] Shamir's secret-sharing scheme, we show that SKE, both in theoretical and empirical evaluation, can retrieve the exact secret with overwhelming probability for a genuine input yet negligible probability for an imposter input. Though SKE can be applied to any vectorial biometrics, we adopt the fingerprint vector as a case of study in this work. The experiments have been performed under several subsets of FVC 2002, 2004, and 2006 datasets. We formalize and analyze the threat model of SKE that encloses several major security attacks.



### SConE: Siamese Constellation Embedding Descriptor for Image Matching
- **Arxiv ID**: http://arxiv.org/abs/1809.11054v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.11054v1)
- **Published**: 2018-09-28 14:24:30+00:00
- **Updated**: 2018-09-28 14:24:30+00:00
- **Authors**: Tomasz Trzcinski, Jacek Komorowski, Lukasz Dabala, Konrad Czarnota, Grzegorz Kurzejamski, Simon Lynen
- **Comment**: None
- **Journal**: None
- **Summary**: Numerous computer vision applications rely on local feature descriptors, such as SIFT, SURF or FREAK, for image matching. Although their local character makes image matching processes more robust to occlusions, it often leads to geometrically inconsistent keypoint matches that need to be filtered out, e.g. using RANSAC. In this paper we propose a novel, more discriminative, descriptor that includes not only local feature representation, but also information about the geometric layout of neighbouring keypoints. To that end, we use a Siamese architecture that learns a low-dimensional feature embedding of keypoint constellation by maximizing the distances between non-corresponding pairs of matched image patches, while minimizing it for correct matches. The 48-dimensional oating point descriptor that we train is built on top of the state-of-the-art FREAK descriptor achieves significant performance improvement over the competitors on a challenging TUM dataset.



### Aggregation of binary feature descriptors for compact scene model representation in large scale structure-from-motion applications
- **Arxiv ID**: http://arxiv.org/abs/1809.11062v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.11062v1)
- **Published**: 2018-09-28 14:35:29+00:00
- **Updated**: 2018-09-28 14:35:29+00:00
- **Authors**: Jacek Komorowski, Tomasz Trzcinski
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present an efficient method for aggregating binary feature descriptors to allow compact representation of 3D scene model in incremental structure-from-motion and SLAM applications. All feature descriptors linked with one 3D scene point or landmark are represented by a single low-dimensional real-valued vector called a \emph{prototype}. The method allows significant reduction of memory required to store and process feature descriptors in large-scale structure-from-motion applications. An efficient approximate nearest neighbours search methods suited for real-valued descriptors, such as FLANN, can be used on the resulting prototypes to speed up matching processed frames.



### Camera Pose Estimation from Sequence of Calibrated Images
- **Arxiv ID**: http://arxiv.org/abs/1809.11066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.11066v1)
- **Published**: 2018-09-28 14:42:29+00:00
- **Updated**: 2018-09-28 14:42:29+00:00
- **Authors**: Jacek Komorowski, Przemyslaw Rokita
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper a method for camera pose estimation from a sequence of images is presented. The method assumes camera is calibrated (intrinsic parameters are known) which allows to decrease a number of required pairs of corresponding points compared to uncalibrated case. Our algorithm can be used as a first stage in a structure from motion stereo reconstruction system.



### Face Recognition Based on Sequence of Images
- **Arxiv ID**: http://arxiv.org/abs/1809.11069v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.11069v1)
- **Published**: 2018-09-28 14:54:12+00:00
- **Updated**: 2018-09-28 14:54:12+00:00
- **Authors**: Jacek Komorowski, Przemyslaw Rokita
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a face recognition method based on a sequence of images. Face shape is reconstructed from images using a combination of structure-from-motion and multi-view stereo methods. The reconstructed 3D face model is compared against models held in a gallery. The novel element in the presented approach is the fact, that the reconstruction is based only on input images and doesn't require a generic, deformable face model. Experimental verification of the proposed method is also included.



### Extrinsic camera calibration method and its performance evaluation
- **Arxiv ID**: http://arxiv.org/abs/1809.11073v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.11073v1)
- **Published**: 2018-09-28 15:00:46+00:00
- **Updated**: 2018-09-28 15:00:46+00:00
- **Authors**: Jacek Komorowski, Przemyslaw Rokita
- **Comment**: arXiv admin note: text overlap with arXiv:1809.11066
- **Journal**: None
- **Summary**: This paper presents a method for extrinsic camera calibration (estimation of camera rotation and translation matrices) from a sequence of images. It is assumed camera intrinsic matrix and distortion coefficients are known and fixed during the entire sequence. %This allows to decrease a number of pairs of corresponding keypoints between images needed to estimate epipolar geometry compared to uncalibrated case. Performance of the presented method is evaluated on a number of multi-view stereo test datasets. Presented algorithm can be used as a first stage in a dense stereo reconstruction system.



### Rethinking Self-driving: Multi-task Knowledge for Better Generalization and Accident Explanation Ability
- **Arxiv ID**: http://arxiv.org/abs/1809.11100v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.11100v1)
- **Published**: 2018-09-28 15:45:32+00:00
- **Updated**: 2018-09-28 15:45:32+00:00
- **Authors**: Zhihao Li, Toshiyuki Motoyoshi, Kazuma Sasaki, Tetsuya Ogata, Shigeki Sugano
- **Comment**: 11 pages, 4 figures, 3 tables
- **Journal**: None
- **Summary**: Current end-to-end deep learning driving models have two problems: (1) Poor generalization ability of unobserved driving environment when diversity of training driving dataset is limited (2) Lack of accident explanation ability when driving models don't work as expected. To tackle these two problems, rooted on the believe that knowledge of associated easy task is benificial for addressing difficult task, we proposed a new driving model which is composed of perception module for \textit{see and think} and driving module for \textit{behave}, and trained it with multi-task perception-related basic knowledge and driving knowledge stepwisely.   Specifically segmentation map and depth map (pixel level understanding of images) were considered as \textit{what \& where} and \textit{how far} knowledge for tackling easier driving-related perception problems before generating final control commands for difficult driving task. The results of experiments demonstrated the effectiveness of multi-task perception knowledge for better generalization and accident explanation ability. With our method the average sucess rate of finishing most difficult navigation tasks in untrained city of CoRL test surpassed current benchmark method for 15 percent in trained weather and 20 percent in untrained weathers. Demonstration video link is: https://www.youtube.com/watch?v=N7ePnnZZwdE



### Channel-wise and Spatial Feature Modulation Network for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1809.11130v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.11130v1)
- **Published**: 2018-09-28 16:29:31+00:00
- **Updated**: 2018-09-28 16:29:31+00:00
- **Authors**: Yanting Hu, Jie Li, Yuanfei Huang, Xinbo Gao
- **Comment**: 14 pages,14 figures
- **Journal**: None
- **Summary**: The performance of single image super-resolution has achieved significant improvement by utilizing deep convolutional neural networks (CNNs). The features in deep CNN contain different types of information which make different contributions to image reconstruction. However, most CNN-based models lack discriminative ability for different types of information and deal with them equally, which results in the representational capacity of the models being limited. On the other hand, as the depth of neural networks grows, the long-term information coming from preceding layers is easy to be weaken or lost in late layers, which is adverse to super-resolving image. To capture more informative features and maintain long-term information for image super-resolution, we propose a channel-wise and spatial feature modulation (CSFM) network in which a sequence of feature-modulation memory (FMM) modules is cascaded with a densely connected structure to transform low-resolution features to high informative features. In each FMM module, we construct a set of channel-wise and spatial attention residual (CSAR) blocks and stack them in a chain structure to dynamically modulate multi-level features in a global-and-local manner. This feature modulation strategy enables the high contribution information to be enhanced and the redundant information to be suppressed. Meanwhile, for long-term information persistence, a gated fusion (GF) node is attached at the end of the FMM module to adaptively fuse hierarchical features and distill more effective information via the dense skip connections and the gating mechanism. Extensive quantitative and qualitative evaluations on benchmark datasets illustrate the superiority of our proposed method over the state-of-the-art methods.



### Deep Residual Network for Off-Resonance Artifact Correction with Application to Pediatric Body Magnetic Resonance Angiography with 3D Cones
- **Arxiv ID**: http://arxiv.org/abs/1810.00072v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1810.00072v1)
- **Published**: 2018-09-28 20:17:23+00:00
- **Updated**: 2018-09-28 20:17:23+00:00
- **Authors**: David Y Zeng, Jamil Shaikh, Dwight G Nishimura, Shreyas S Vasanawala, Joseph Y Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Off-resonance artifact correction by deep-learning, to facilitate rapid pediatric body imaging with a scan time efficient 3D cones trajectory. Methods: A residual convolutional neural network to correct off-resonance artifacts (Off-ResNet) was trained with a prospective study of 30 pediatric magnetic resonance angiography exams. Each exam acquired a short-readout scan (1.18 ms +- 0.38) and a long-readout scan (3.35 ms +- 0.74) at 3T. Short-readout scans, with longer scan times but negligible off-resonance blurring, were used as reference images and augmented with additional off-resonance for supervised training examples. Long-readout scans, with greater off-resonance artifacts but shorter scan time, were corrected by autofocus and Off-ResNet and compared to short-readout scans by normalized root-mean-square error (NRMSE), structural similarity index (SSIM), and peak signal-to-noise ratio (PSNR). Scans were also compared by scoring on eight anatomical features by two radiologists, using analysis of variance with post-hoc Tukey's test. Reader agreement was determined with intraclass correlation. Results: Long-readout scans were on average 59.3% shorter than short-readout scans. Images from Off-ResNet had superior NRMSE, SSIM, and PSNR compared to uncorrected images across +-1kHz off-resonance (P<0.01). The proposed method had superior NRMSE over -677Hz to +1kHz and superior SSIM and PSNR over +-1kHz compared to autofocus (P<0.01). Radiologic scoring demonstrated that long-readout scans corrected with Off-ResNet were non-inferior to short-readout scans (P<0.01). Conclusion: The proposed method can correct off-resonance artifacts from rapid long-readout 3D cones scans to a non-inferior image quality compared to diagnostically standard short-readout scans.



### Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout
- **Arxiv ID**: http://arxiv.org/abs/1810.00091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00091v1)
- **Published**: 2018-09-28 21:42:38+00:00
- **Updated**: 2018-09-28 21:42:38+00:00
- **Authors**: Kun Wan, Boyuan Feng, Lingwei Xie, Yufei Ding
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Recently convolutional neural networks (CNNs) achieve great accuracy in visual recognition tasks. DenseNet becomes one of the most popular CNN models due to its effectiveness in feature-reuse. However, like other CNN models, DenseNets also face overfitting problem if not severer. Existing dropout method can be applied but not as effective due to the introduced nonlinear connections. In particular, the property of feature-reuse in DenseNet will be impeded, and the dropout effect will be weakened by the spatial correlation inside feature maps. To address these problems, we craft the design of a specialized dropout method from three aspects, dropout location, dropout granularity, and dropout probability. The insights attained here could potentially be applied as a general approach for boosting the accuracy of other CNN models with similar nonlinear connections. Experimental results show that DenseNets with our specialized dropout method yield better accuracy compared to vanilla DenseNet and state-of-the-art CNN models, and such accuracy boost increases with the model depth.



### Superimposition-guided Facial Reconstruction from Skull
- **Arxiv ID**: http://arxiv.org/abs/1810.00107v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.00107v1)
- **Published**: 2018-09-28 22:24:07+00:00
- **Updated**: 2018-09-28 22:24:07+00:00
- **Authors**: Celong Liu, Xin Li
- **Comment**: 14 pages; 14 figures
- **Journal**: None
- **Summary**: We develop a new algorithm to perform facial reconstruction from a given skull. This technique has forensic application in helping the identification of skeletal remains when other information is unavailable. Unlike most existing strategies that directly reconstruct the face from the skull, we utilize a database of portrait photos to create many face candidates, then perform a superimposition to get a well matched face, and then revise it according to the superimposition. To support this pipeline, we build an effective autoencoder for image-based facial reconstruction, and a generative model for constrained face inpainting. Our experiments have demonstrated that the proposed pipeline is stable and accurate.



### Audio-Visual Speech Recognition With A Hybrid CTC/Attention Architecture
- **Arxiv ID**: http://arxiv.org/abs/1810.00108v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00108v1)
- **Published**: 2018-09-28 22:41:59+00:00
- **Updated**: 2018-09-28 22:41:59+00:00
- **Authors**: Stavros Petridis, Themos Stafylakis, Pingchuan Ma, Georgios Tzimiropoulos, Maja Pantic
- **Comment**: Accepted to IEEE SLT 2018
- **Journal**: None
- **Summary**: Recent works in speech recognition rely either on connectionist temporal classification (CTC) or sequence-to-sequence models for character-level recognition. CTC assumes conditional independence of individual characters, whereas attention-based models can provide nonsequential alignments. Therefore, we could use a CTC loss in combination with an attention-based model in order to force monotonic alignments and at the same time get rid of the conditional independence assumption. In this paper, we use the recently proposed hybrid CTC/attention architecture for audio-visual recognition of speech in-the-wild. To the best of our knowledge, this is the first time that such a hybrid architecture architecture is used for audio-visual recognition of speech. We use the LRS2 database and show that the proposed audio-visual model leads to an 1.3% absolute decrease in word error rate over the audio-only model and achieves the new state-of-the-art performance on LRS2 database (7% word error rate). We also observe that the audio-visual model significantly outperforms the audio-based model (up to 32.9% absolute improvement in word error rate) for several different types of noise as the signal-to-noise ratio decreases.



### DeepSSM: A Deep Learning Framework for Statistical Shape Modeling from Raw Images
- **Arxiv ID**: http://arxiv.org/abs/1810.00111v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.00111v1)
- **Published**: 2018-09-28 22:53:49+00:00
- **Updated**: 2018-09-28 22:53:49+00:00
- **Authors**: Riddhish Bhalodia, Shireen Y. Elhabian, Ladislav Kavan, Ross T. Whitaker
- **Comment**: Accepted to ShapeMI MICCAI 2018 (oral): Workshop on Shape in Medical
  Imaging
- **Journal**: None
- **Summary**: Statistical shape modeling is an important tool to characterize variation in anatomical morphology. Typical shapes of interest are measured using 3D imaging and a subsequent pipeline of registration, segmentation, and some extraction of shape features or projections onto some lower-dimensional shape space, which facilitates subsequent statistical analysis. Many methods for constructing compact shape representations have been proposed, but are often impractical due to the sequence of image preprocessing operations, which involve significant parameter tuning, manual delineation, and/or quality control by the users. We propose DeepSSM: a deep learning approach to extract a low-dimensional shape representation directly from 3D images, requiring virtually no parameter tuning or user assistance. DeepSSM uses a convolutional neural network (CNN) that simultaneously localizes the biological structure of interest, establishes correspondences, and projects these points onto a low-dimensional shape representation in the form of PCA loadings within a point distribution model. To overcome the challenge of the limited availability of training images, we present a novel data augmentation procedure that uses existing correspondences on a relatively small set of processed images with shape statistics to create plausible training samples with known shape parameters. Hence, we leverage the limited CT/MRI scans (40-50) into thousands of images needed to train a CNN. After the training, the CNN automatically produces accurate low-dimensional shape representations for unseen images. We validate DeepSSM for three different applications pertaining to modeling pediatric cranial CT for characterization of metopic craniosynostosis, femur CT scans identifying morphologic deformities of the hip due to femoroacetabular impingement, and left atrium MRI scans for atrial fibrillation recurrence prediction.



