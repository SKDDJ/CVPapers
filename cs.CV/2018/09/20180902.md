# Arxiv Papers in cs.CV on 2018-09-02
### A Comparison of Handcrafted and Deep Neural Network Feature Extraction for Classifying Optical Coherence Tomography (OCT) Images
- **Arxiv ID**: http://arxiv.org/abs/1809.03306v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.03306v1)
- **Published**: 2018-09-02 03:18:17+00:00
- **Updated**: 2018-09-02 03:18:17+00:00
- **Authors**: Kuntoro Adi Nugroho
- **Comment**: None
- **Journal**: None
- **Summary**: Optical Coherence Tomography allows ophthalmologist to obtain cross-section imaging of eye retina. Assisted with digital image analysis methods, effective disease detection could be performed. Various methods exist to extract feature from OCT images. The proposed study aims to compare the effectiveness of handcrafted and deep neural network features. The evaluated dataset consist of 32339 instances distributed in four classes, namely CNV, DME, DRUSEN, and NORMAL. The methods are Histogram of Oriented Gradient (HOG), Local Binary Pattern (LBP), DenseNet-169, and ResNet50. As a result, the deep neural network based methods outperformed the handcrafted feature with 88% and 89% accuracy for DenseNet and ResNet compared to 50 % and 42 % for HOG and LBP respectively. The deep neural network based methods also demonstrated better result on the under represented class.



### Learning to Navigate for Fine-grained Classification
- **Arxiv ID**: http://arxiv.org/abs/1809.00287v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00287v1)
- **Published**: 2018-09-02 03:40:08+00:00
- **Updated**: 2018-09-02 03:40:08+00:00
- **Authors**: Ze Yang, Tiange Luo, Dong Wang, Zhiqiang Hu, Jun Gao, Liwei Wang
- **Comment**: Accepted by ECCV 2018
- **Journal**: None
- **Summary**: Fine-grained classification is challenging due to the difficulty of finding discriminative features. Finding those subtle traits that fully characterize the object is not straightforward. To handle this circumstance, we propose a novel self-supervision mechanism to effectively localize informative regions without the need of bounding-box/part annotations. Our model, termed NTS-Net for Navigator-Teacher-Scrutinizer Network, consists of a Navigator agent, a Teacher agent and a Scrutinizer agent. In consideration of intrinsic consistency between informativeness of the regions and their probability being ground-truth class, we design a novel training paradigm, which enables Navigator to detect most informative regions under the guidance from Teacher. After that, the Scrutinizer scrutinizes the proposed regions from Navigator and makes predictions. Our model can be viewed as a multi-agent cooperation, wherein agents benefit from each other, and make progress together. NTS-Net can be trained end-to-end, while provides accurate fine-grained classification predictions as well as highly informative regions during inference. We achieve state-of-the-art performance in extensive benchmark datasets.



### Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.00338v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.00338v2)
- **Published**: 2018-09-02 13:58:37+00:00
- **Updated**: 2018-10-04 01:53:17+00:00
- **Authors**: Jian Zhao, Yu Cheng, Yi Cheng, Yang Yang, Haochong Lan, Fang Zhao, Lin Xiong, Yan Xu, Jianshu Li, Sugiri Pranata, Shengmei Shen, Junliang Xing, Hengzhu Liu, Shuicheng Yan, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the remarkable progress in face recognition related technologies, reliably recognizing faces across ages still remains a big challenge. The appearance of a human face changes substantially over time, resulting in significant intra-class variations. As opposed to current techniques for age-invariant face recognition, which either directly extract age-invariant features for recognition, or first synthesize a face that matches target age before feature extraction, we argue that it is more desirable to perform both tasks jointly so that they can leverage each other. To this end, we propose a deep Age-Invariant Model (AIM) for face recognition in the wild with three distinct novelties. First, AIM presents a novel unified deep architecture jointly performing cross-age face synthesis and recognition in a mutual boosting way. Second, AIM achieves continuous face rejuvenation/aging with remarkable photorealistic and identity-preserving properties, avoiding the requirement of paired data and the true age of testing samples. Third, we develop effective and novel training strategies for end-to-end learning the whole deep architecture, which generates powerful age-invariant face representations explicitly disentangled from the age variation. Moreover, we propose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset to facilitate existing efforts and push the frontiers of age-invariant face recognition research. Extensive experiments on both our CAFR and several other cross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the proposed AIM model over the state-of-the-arts. Benchmarking our model on one of the most popular unconstrained face recognition datasets IJB-C additionally verifies the promising generalizability of AIM in recognizing faces in the wild.



### Chittron: An Automatic Bangla Image Captioning System
- **Arxiv ID**: http://arxiv.org/abs/1809.00339v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.00339v1)
- **Published**: 2018-09-02 14:03:30+00:00
- **Updated**: 2018-09-02 14:03:30+00:00
- **Authors**: Motiur Rahman, Nabeel Mohammed, Nafees Mansoor, Sifat Momen
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic image caption generation aims to produce an accurate description of an image in natural language automatically. However, Bangla, the fifth most widely spoken language in the world, is lagging considerably in the research and development of such domain. Besides, while there are many established data sets to related to image annotation in English, no such resource exists for Bangla yet. Hence, this paper outlines the development of "Chittron", an automatic image captioning system in Bangla. Moreover, to address the data set availability issue, a collection of 16,000 Bangladeshi contextual images has been accumulated and manually annotated in Bangla. This data set is then used to train a model which integrates a pre-trained VGG16 image embedding model with stacked LSTM layers. The model is trained to predict the caption when the input is an image, one word at a time. The results show that the model has successfully been able to learn a working language model and to generate captions of images quite accurately in many cases. The results are evaluated mainly qualitatively. However, BLEU scores are also reported. It is expected that a better result can be obtained with a bigger and more varied data set.



### Identifying Land Patterns from Satellite Imagery in Amazon Rainforest using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.00340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00340v1)
- **Published**: 2018-09-02 14:06:39+00:00
- **Updated**: 2018-09-02 14:06:39+00:00
- **Authors**: Somnath Rakshit, Soumyadeep Debnath, Dhiman Mondal
- **Comment**: None
- **Journal**: None
- **Summary**: The Amazon rainforests have been suffering widespread damage, both via natural and artificial means. Every minute, it is estimated that the world loses forest cover the size of 48 football fields. Deforestation in the Amazon rainforest has led to drastically reduced biodiversity, loss of habitat, climate change, and other biological losses. In this respect, it has become essential to track how the nature of these forests change over time. Image classification using deep learning can help speed up this process by removing the manual task of classifying each image. Here, it is shown how convolutional neural networks can be used to track changes in land patterns in the Amazon rainforests. In this work, a testing accuracy of 96.71% was obtained. This can help governments and other agencies to track changes in land patterns more effectively and accurately.



### Natural Language Person Search Using Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.00365v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1809.00365v1)
- **Published**: 2018-09-02 16:19:20+00:00
- **Updated**: 2018-09-02 16:19:20+00:00
- **Authors**: Ankit Shah, Tyler Vuong
- **Comment**: Equal Contribution - Work in Progress. Preprint results
- **Journal**: None
- **Summary**: Recent success in deep reinforcement learning is having an agent learn how to play Go and beat the world champion without any prior knowledge of the game. In that task, the agent has to make a decision on what action to take based on the positions of the pieces. Person Search is recently explored using natural language based text description of images for video surveillance applications (S.Li et.al). We see (Fu.et al) provides an end to end approach for object-based retrieval using deep reinforcement learning without constraints placed on which objects are being detected. However, we believe for real-world applications such as person search defining specific constraints which identify a person as opposed to starting with a general object detection will have benefits in terms of performance and computational resources required. In our task, Deep reinforcement learning would localize the person in an image by reshaping the sizes of the bounding boxes. Deep Reinforcement learning with appropriate constraints would look only for the relevant person in the image as opposed to an unconstrained approach where each individual objects in the image are ranked. For person search, the agent is trying to form a tight bounding box around the person in the image who matches the description. The bounding box is initialized to the full image and at each time step, the agent makes a decision on how to change the current bounding box so that it has a tighter bound around the person based on the description of the person and the pixel values of the current bounding box. After the agent takes an action, it will be given a reward based on the Intersection over Union (IoU) of the current bounding box and the ground truth box. Once the agent believes that the bounding box is covering the person, it will indicate that the person is found.



### Visual Transfer between Atari Games using Competitive Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.00397v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.00397v1)
- **Published**: 2018-09-02 21:34:28+00:00
- **Updated**: 2018-09-02 21:34:28+00:00
- **Authors**: Akshita Mittel, Sowmya Munukutla, Himanshi Yadav
- **Comment**: None
- **Journal**: None
- **Summary**: This paper explores the use of deep reinforcement learning agents to transfer knowledge from one environment to another. More specifically, the method takes advantage of asynchronous advantage actor critic (A3C) architecture to generalize a target game using an agent trained on a source game in Atari. Instead of fine-tuning a pre-trained model for the target game, we propose a learning approach to update the model using multiple agents trained in parallel with different representations of the target game. Visual mapping between video sequences of transfer pairs is used to derive new representations of the target game; training on these visual representations of the target game improves model updates in terms of performance, data efficiency and stability. In order to demonstrate the functionality of the architecture, Atari games Pong-v0 and Breakout-v0 are being used from the OpenAI gym environment; as the source and target environment.



### MANTIS: Model-Augmented Neural neTwork with Incoherent k-space Sampling for efficient MR T2 mapping
- **Arxiv ID**: http://arxiv.org/abs/1809.03308v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.03308v1)
- **Published**: 2018-09-02 21:43:49+00:00
- **Updated**: 2018-09-02 21:43:49+00:00
- **Authors**: Fang Liu, Li Feng, Richard Kijowski
- **Comment**: None
- **Journal**: None
- **Summary**: Quantitative mapping of magnetic resonance (MR) parameters have been shown as valuable methods for improved assessment of a range of diseases. Due to the need to image an anatomic structure multiple times, parameter mapping usually requires long scan times compared to conventional static imaging. Therefore, accelerated parameter mapping is highly-desirable and remains a topic of great interest in the MR research community. While many recent deep learning methods have focused on highly efficient image reconstruction for conventional static MR imaging, applications of deep learning for dynamic imaging and in particular accelerated parameter mapping have been limited. The purpose of this work was to develop and evaluate a novel deep learning-based reconstruction framework called Model-Augmented Neural neTwork with Incoherent k-space Sampling (MANTIS) for efficient MR parameter mapping. Our approach combines end-to-end CNN mapping with k-space consistency using the concept of cyclic loss to further enforce data and model fidelity. Incoherent k-space sampling is used to improve reconstruction performance. A physical model is incorporated into the proposed framework, so that the parameter maps can be efficiently estimated directly from undersampled images. The performance of MANTIS was demonstrated for the spin-spin relaxation time (T2) mapping of the knee joint. Compared to conventional reconstruction approaches that exploited image sparsity, MANTIS yielded lower errors and higher similarity with respect to the reference in the T2 estimation. Our study demonstrated that the proposed MANTIS framework, with a combination of end-to-end CNN mapping, signal model-augmented data consistency, and incoherent k-space sampling, represents a promising approach for efficient MR parameter mapping. MANTIS can potentially be extended to other types of parameter mapping with appropriate models.



### On the Role of Event Boundaries in Egocentric Activity Recognition from Photostreams
- **Arxiv ID**: http://arxiv.org/abs/1809.00402v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00402v2)
- **Published**: 2018-09-02 22:09:26+00:00
- **Updated**: 2018-09-06 13:50:26+00:00
- **Authors**: Alejandro Cartas, Estefania Talavera, Petia Radeva, Mariella Dimiccoli
- **Comment**: Presented as a short abstract in the EPIC workshop at ECCV 2018
- **Journal**: None
- **Summary**: Event boundaries play a crucial role as a pre-processing step for detection, localization, and recognition tasks of human activities in videos. Typically, although their intrinsic subjectiveness, temporal bounds are provided manually as input for training action recognition algorithms. However, their role for activity recognition in the domain of egocentric photostreams has been so far neglected. In this paper, we provide insights of how automatically computed boundaries can impact activity recognition results in the emerging domain of egocentric photostreams. Furthermore, we collected a new annotated dataset acquired by 15 people by a wearable photo-camera and we used it to show the generalization capabilities of several deep learning based architectures to unseen users.



