# Arxiv Papers in cs.CV on 2018-09-26
### PhotoShape: Photorealistic Materials for Large-Scale Shape Collections
- **Arxiv ID**: http://arxiv.org/abs/1809.09761v1
- **DOI**: 10.1145/3272127.3275066
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.09761v1)
- **Published**: 2018-09-26 00:01:03+00:00
- **Updated**: 2018-09-26 00:01:03+00:00
- **Authors**: Keunhong Park, Konstantinos Rematas, Ali Farhadi, Steven M. Seitz
- **Comment**: To be presented at SIGGRAPH Asia 2018. Project page:
  https://keunhong.com/publications/photoshape/
- **Journal**: None
- **Summary**: Existing online 3D shape repositories contain thousands of 3D models but lack photorealistic appearance. We present an approach to automatically assign high-quality, realistic appearance models to large scale 3D shape collections. The key idea is to jointly leverage three types of online data -- shape collections, material collections, and photo collections, using the photos as reference to guide assignment of materials to shapes. By generating a large number of synthetic renderings, we train a convolutional neural network to classify materials in real photos, and employ 3D-2D alignment techniques to transfer materials to different parts of each shape model. Our system produces photorealistic, relightable, 3D shapes (PhotoShapes).



### DSR: Direct Self-rectification for Uncalibrated Dual-lens Cameras
- **Arxiv ID**: http://arxiv.org/abs/1809.09763v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09763v1)
- **Published**: 2018-09-26 00:10:42+00:00
- **Updated**: 2018-09-26 00:10:42+00:00
- **Authors**: Ruichao Xiao, Wenxiu Sun, Jiahao Pang, Qiong Yan, Jimmy Ren
- **Comment**: Accepted at 3DV2018
- **Journal**: None
- **Summary**: With the developments of dual-lens camera modules,depth information representing the third dimension of thecaptured scenes becomes available for smartphones. It isestimated by stereo matching algorithms, taking as input thetwo views captured by dual-lens cameras at slightly differ-ent viewpoints. Depth-of-field rendering (also be referred toas synthetic defocus or bokeh) is one of the trending depth-based applications. However, to achieve fast depth estima-tion on smartphones, the stereo pairs need to be rectified inthe first place. In this paper, we propose a cost-effective so-lution to perform stereo rectification for dual-lens camerascalled direct self-rectification, short for DSR1. It removesthe need of individual offline calibration for every pair ofdual-lens cameras. In addition, the proposed solution isrobust to the slight movements, e.g., due to collisions, ofthe dual-lens cameras after fabrication. Different with ex-isting self-rectification approaches, our approach computesthe homography in a novel way with zero geometric distor-tions introduced to the master image. It is achieved by di-rectly minimizing the vertical displacements of correspond-ing points between the original master image and the trans-formed slave image. Our method is evaluated on both real-istic and synthetic stereo image pairs, and produces supe-rior results compared to the calibrated rectification or otherself-rectification approaches



### Night-to-Day Image Translation for Retrieval-based Localization
- **Arxiv ID**: http://arxiv.org/abs/1809.09767v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09767v2)
- **Published**: 2018-09-26 00:33:43+00:00
- **Updated**: 2019-03-04 11:09:08+00:00
- **Authors**: Asha Anoosheh, Torsten Sattler, Radu Timofte, Marc Pollefeys, Luc Van Gool
- **Comment**: Published in ICRA 2019
- **Journal**: None
- **Summary**: Visual localization is a key step in many robotics pipelines, allowing the robot to (approximately) determine its position and orientation in the world. An efficient and scalable approach to visual localization is to use image retrieval techniques. These approaches identify the image most similar to a query photo in a database of geo-tagged images and approximate the query's pose via the pose of the retrieved database image. However, image retrieval across drastically different illumination conditions, e.g. day and night, is still a problem with unsatisfactory results, even in this age of powerful neural models. This is due to a lack of a suitably diverse dataset with true correspondences to perform end-to-end learning. A recent class of neural models allows for realistic translation of images among visual domains with relatively little training data and, most importantly, without ground-truth pairings. In this paper, we explore the task of accurately localizing images captured from two traversals of the same area in both day and night. We propose ToDayGAN - a modified image-translation model to alter nighttime driving images to a more useful daytime representation. We then compare the daytime and translated night images to obtain a pose estimate for the night image using the known 6-DOF position of the closest day image. Our approach improves localization performance by over 250% compared the current state-of-the-art, in the context of standard metrics in multiple categories.



### A Problem Reduction Approach for Visual Relationships Detection
- **Arxiv ID**: http://arxiv.org/abs/1809.09828v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09828v1)
- **Published**: 2018-09-26 07:08:41+00:00
- **Updated**: 2018-09-26 07:08:41+00:00
- **Authors**: Toshiyuki Fukuzawa
- **Comment**: ECCV 2018 Workshop
- **Journal**: None
- **Summary**: Identifying different objects (man and cup) is an important problem on its own, but identifying the relationship between them (holding) is critical for many real world use cases. This paper describes an approach to reduce a visual relationship detection problem to object detection problems. The method was applied to Google AI Open Images V4 Visual Relationship Track Challenge, which was held in conjunction with 2018 European Conference on Computer Vision (ECCV 2018) and it finished as a prize winner. The challenge was to build an algorithm that detects pairs of objects in particular relations: things like "woman playing guitar," "beer on table," or "dog inside car.".



### Graph Laplacian Regularized Graph Convolutional Networks for Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.09839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09839v1)
- **Published**: 2018-09-26 07:39:29+00:00
- **Updated**: 2018-09-26 07:39:29+00:00
- **Authors**: Bo Jiang, Doudou Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, graph convolutional network (GCN) has been widely used for semi-supervised classification and deep feature representation on graph-structured data. However, existing GCN generally fails to consider the local invariance constraint in learning and representation process. That is, if two data points Xi and Xj are close in the intrinsic geometry of the data distribution, then their labels/representations should also be close to each other. This is known as local invariance assumption which plays an essential role in the development of various kinds of traditional algorithms, such as dimensionality reduction and semi-supervised learning, in machine learning area. To overcome this limitation, we introduce a graph Laplacian GCN (gLGCN) approach for graph data representation and semi-supervised classification. The proposed gLGCN model is capable of encoding both graph structure and node features together while maintains the local invariance constraint naturally for robust data representation and semi-supervised classification. Experiments show the benefit of the benefits the proposed gLGCN network.



### Active Learning for Deep Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1809.09875v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.09875v1)
- **Published**: 2018-09-26 09:47:42+00:00
- **Updated**: 2018-09-26 09:47:42+00:00
- **Authors**: Clemens-Alexander Brust, Christoph Käding, Joachim Denzler
- **Comment**: None
- **Journal**: None
- **Summary**: The great success that deep models have achieved in the past is mainly owed to large amounts of labeled training data. However, the acquisition of labeled data for new tasks aside from existing benchmarks is both challenging and costly. Active learning can make the process of labeling new data more efficient by selecting unlabeled samples which, when labeled, are expected to improve the model the most. In this paper, we combine a novel method of active learning for object detection with an incremental learning scheme to enable continuous exploration of new unlabeled datasets. We propose a set of uncertainty-based active learning metrics suitable for most object detectors. Furthermore, we present an approach to leverage class imbalances during sample selection. All methods are evaluated systematically in a continuous exploration context on the PASCAL VOC 2012 dataset.



### Hierarchy-based Image Embeddings for Semantic Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1809.09924v4
- **DOI**: 10.1109/WACV.2019.00073
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.09924v4)
- **Published**: 2018-09-26 11:58:19+00:00
- **Updated**: 2019-03-19 15:13:18+00:00
- **Authors**: Björn Barz, Joachim Denzler
- **Comment**: Accepted at WACV 2019. Source code:
  https://github.com/cvjena/semantic-embeddings
- **Journal**: 2019 IEEE Winter Conference on Applications of Computer Vision
  (WACV), Waikoloa Village, HI, USA, 2019, pp. 638-647
- **Summary**: Deep neural networks trained for classification have been found to learn powerful image representations, which are also often used for other tasks such as comparing images w.r.t. their visual similarity. However, visual similarity does not imply semantic similarity. In order to learn semantically discriminative features, we propose to map images onto class embeddings whose pair-wise dot products correspond to a measure of semantic similarity between classes. Such an embedding does not only improve image retrieval results, but could also facilitate integrating semantics for other tasks, e.g., novelty detection or few-shot learning. We introduce a deterministic algorithm for computing the class centroids directly based on prior world-knowledge encoded in a hierarchy of classes such as WordNet. Experiments on CIFAR-100, NABirds, and ImageNet show that our learned semantic image embeddings improve the semantic consistency of image retrieval results by a large margin.



### Vision-based Semantic Mapping and Localization for Autonomous Indoor Parking
- **Arxiv ID**: http://arxiv.org/abs/1809.09929v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09929v1)
- **Published**: 2018-09-26 12:03:45+00:00
- **Updated**: 2018-09-26 12:03:45+00:00
- **Authors**: Yewei Huang, Junqiao Zhao, Xudong He, Shaoming Zhang, Tiantian Feng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we proposed a novel and practical solution for the real-time indoor localization of autonomous driving in parking lots. High-level landmarks, the parking slots, are extracted and enriched with labels to avoid the aliasing of low-level visual features. We then proposed a robust method for detecting incorrect data associations between parking slots and further extended the optimization framework by dynamically eliminating suboptimal data associations. Visual fiducial markers are introduced to improve the overall precision. As a result, a semantic map of the parking lot can be established fully automatically and robustly. We experimented the performance of real-time localization based on the map using our autonomous driving platform TiEV, and the average accuracy of 0.3m track tracing can be achieved at a speed of 10kph.



### Random Occlusion-recovery for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1809.09970v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09970v3)
- **Published**: 2018-09-26 13:29:26+00:00
- **Updated**: 2019-03-12 11:34:15+00:00
- **Authors**: Di Wu, Kun Zhang, Fei Cheng, Yang Zhao, Qi Liu, Chang-An Yuan, De-Shuang Huang
- **Comment**: None
- **Journal**: None
- **Summary**: As a basic task of multi-camera surveillance system, person re-identification aims to re-identify a query pedestrian observed from non-overlapping multiple cameras or across different time with a single camera. Recently, deep learning-based person re-identification models have achieved great success in many benchmarks. However, these supervised models require a large amount of labeled image data, and the process of manual labeling spends much manpower and time. In this study, we introduce a method to automatically synthesize labeled person images and adopt them to increase the sample number per identity for person re-identification datasets. To be specific, we use block rectangles to randomly occlude pedestrian images. Then, a generative adversarial network (GAN) model is proposed to use paired occluded and original images to synthesize the de-occluded images that similar but not identical to the original image. Afterwards, we annotate the de-occluded images with the same labels of their corresponding raw images and use them to augment the number of samples per identity. Finally, we use the augmented datasets to train baseline model. The experiment results on CUHK03, Market-1501 and DukeMTMC-reID datasets show that the effectiveness of the proposed method.



### Residuum-Condition Diagram and Reduction of Over-Complete Endmember-Sets
- **Arxiv ID**: http://arxiv.org/abs/1809.10089v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10089v1)
- **Published**: 2018-09-26 16:01:11+00:00
- **Updated**: 2018-09-26 16:01:11+00:00
- **Authors**: Christoph Schikora, Markus Plack, Andreas Kolb
- **Comment**: None
- **Journal**: None
- **Summary**: Extracting reference spectra, or endmembers (EMs) from a given multi- or hyperspectral image, as well as estimating the size of the EM set, plays an important role in multispectral image processing. In this paper, we present condition-residuum-diagrams. By plotting the residuum resulting from the unmixing and reconstruction and the condition number of various EM sets, the resulting diagram provides insight into the behavior of the spectral unmixing under a varying amount of endmembers (EMs). Furthermore, we utilize condition-residuum-diagrams to realize an EM reduction algorithm that starts with an initially extracted, over-complete EM set. An over-complete EM set commonly exhibits a good unmixing result, i.e. a lower reconstruction residuum, but due to its partial redundancy, the unmixing gets numerically unstable, i.e. the unmixed abundances values are less reliable. Our greedy reduction scheme improves the EM set by reducing the condition number, i.e. enhancing the set's stability, while keeping the reconstruction error as low as possible. The resulting set sequence gives hint to the optimal EM set and its size. We demonstrate the benefit of our condition-residuum-diagram and reduction scheme on well-studied datasets with known reference EM set sizes for several well-known EE algorithms.



### Photometric Depth Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1809.10097v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10097v2)
- **Published**: 2018-09-26 16:09:26+00:00
- **Updated**: 2019-06-25 07:09:00+00:00
- **Authors**: Bjoern Haefner, Songyou Peng, Alok Verma, Yvain Quéau, Daniel Cremers
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (T-PAMI), 2019. First three authors contribute equally
- **Journal**: None
- **Summary**: This study explores the use of photometric techniques (shape-from-shading and uncalibrated photometric stereo) for upsampling the low-resolution depth map from an RGB-D sensor to the higher resolution of the companion RGB image. A single-shot variational approach is first put forward, which is effective as long as the target's reflectance is piecewise-constant. It is then shown that this dependency upon a specific reflectance model can be relaxed by focusing on a specific class of objects (e.g., faces), and delegate reflectance estimation to a deep neural network. A multi-shot strategy based on randomly varying lighting conditions is eventually discussed. It requires no training or prior on the reflectance, yet this comes at the price of a dedicated acquisition setup. Both quantitative and qualitative evaluations illustrate the effectiveness of the proposed methods on synthetic and real-world scenarios.



### Redundant Perception and State Estimation for Reliable Autonomous Racing
- **Arxiv ID**: http://arxiv.org/abs/1809.10099v1
- **DOI**: 10.1109/ICRA.2019.8794155
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.10099v1)
- **Published**: 2018-09-26 16:14:25+00:00
- **Updated**: 2018-09-26 16:14:25+00:00
- **Authors**: Nikhil Bharadwaj Gosala, Andreas Bühler, Manish Prajapat, Claas Ehmke, Mehak Gupta, Ramya Sivanesan, Abel Gawel, Mark Pfeiffer, Mathias Bürki, Inkyu Sa, Renaud Dubé, Roland Siegwart
- **Comment**: 7 pages, 21 figures, submitted to the International Conference on
  Robotics and Automation 2019, for accompanying video visit
  https://www.youtube.com/watch?v=ir_uqEYuT84
- **Journal**: None
- **Summary**: In autonomous racing, vehicles operate close to the limits of handling and a sensor failure can have critical consequences. To limit the impact of such failures, this paper presents the redundant perception and state estimation approaches developed for an autonomous race car. Redundancy in perception is achieved by estimating the color and position of the track delimiting objects using two sensor modalities independently. Specifically, learning-based approaches are used to generate color and pose estimates, from LiDAR and camera data respectively. The redundant perception inputs are fused by a particle filter based SLAM algorithm that operates in real-time. Velocity is estimated using slip dynamics, with reliability being ensured through a probabilistic failure detection algorithm. The sub-modules are extensively evaluated in real-world racing conditions using the autonomous race car "gotthard driverless", achieving lateral accelerations up to 1.7G and a top speed of 90km/h.



### Convolutional Neural Networks for Video Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1809.10117v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.10117v1)
- **Published**: 2018-09-26 16:55:54+00:00
- **Updated**: 2018-09-26 16:55:54+00:00
- **Authors**: Michalis Giannopoulos, Grigorios Tsagkatakis, Saverio Blasi, Farzad Toutounchi, Athanasios Mouchtaris, Panagiotis Tsakalides, Marta Mrak, Ebroul Izquierdo
- **Comment**: Number of Pages: 12, Number of Figures: 17, Submitted to: Signal
  Processing: Image Communication (Elsevier)
- **Journal**: None
- **Summary**: Video Quality Assessment (VQA) is a very challenging task due to its highly subjective nature. Moreover, many factors influence VQA. Compression of video content, while necessary for minimising transmission and storage requirements, introduces distortions which can have detrimental effects on the perceived quality. Especially when dealing with modern video coding standards, it is extremely difficult to model the effects of compression due to the unpredictability of encoding on different content types. Moreover, transmission also introduces delays and other distortion types which affect the perceived quality. Therefore, it would be highly beneficial to accurately predict the perceived quality of video to be distributed over modern content distribution platforms, so that specific actions could be undertaken to maximise the Quality of Experience (QoE) of the users. Traditional VQA techniques based on feature extraction and modelling may not be sufficiently accurate. In this paper, a novel Deep Learning (DL) framework is introduced for effectively predicting VQA of video content delivery mechanisms based on end-to-end feature learning. The proposed framework is based on Convolutional Neural Networks, taking into account compression distortion as well as transmission delays. Training and evaluation of the proposed framework are performed on a user annotated VQA dataset specifically created to undertake this work. The experiments show that the proposed methods can lead to high accuracy of the quality estimation, showcasing the potential of using DL in complex VQA scenarios.



### Open Source Presentation Attack Detection Baseline for Iris Recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.10172v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10172v2)
- **Published**: 2018-09-26 18:11:01+00:00
- **Updated**: 2019-05-09 03:46:52+00:00
- **Authors**: Joseph McGrath, Kevin W. Bowyer, Adam Czajka
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes the first, known to us, open source presentation attack detection (PAD) solution to distinguish between authentic iris images (possibly wearing clear contact lenses) and irises with textured contact lenses. This software can serve as a baseline in various PAD evaluations, and also as an open-source platform with an up-to-date reference method for iris PAD. The software is written in C++ and Python and uses only open source resources, such as OpenCV. This method does not incorporate iris image segmentation, which may be problematic for unknown fake samples. Instead, it makes a best guess to localize the rough position of the iris. The PAD-related features are extracted with the Binary Statistical Image Features (BSIF), which are classified by an ensemble of classifiers incorporating support vector machine, random forest and multilayer perceptron. The models attached to the current software have been trained with the NDCLD'15 database and evaluated on the independent datasets included in the LivDet-Iris 2017 competition. The software implements the functionality of retraining the classifiers with any database of authentic and attack images. The accuracy of the current version offered with this paper exceeds 99% when tested on subject-disjoint subsets of NDCLD'15, and oscillates around 85% when tested on the LivDet-Iris 2017 benchmarks, which is on par with the results obtained by the LivDet-Iris 2017 winner.



### Content Based Image Retrieval from AWiFS Images Repository of IRS Resourcesat-2 Satellite Based on Water Bodies and Burnt Areas
- **Arxiv ID**: http://arxiv.org/abs/1809.10190v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.10190v1)
- **Published**: 2018-09-26 18:58:24+00:00
- **Updated**: 2018-09-26 18:58:24+00:00
- **Authors**: Suraj Kothawade, Kunjan Mhaske, Sahil Sharma, Furkhan Shaikh
- **Comment**: None
- **Journal**: None
- **Summary**: Satellite Remote Sensing Technology is becoming a major milestone in the prediction of weather anomalies, natural disasters as well as finding alternative resources in proximity using multiple multi-spectral sensors emitting electromagnetic waves at distinct wavelengths. Hence, it is imperative to extract water bodies and burnt areas from orthorectified tiles and correspondingly rank them using similarity measures. Different objects in all the spheres of the earth have the inherent capability of absorbing electromagnetic waves of distant wavelengths. This creates various unique masks in terms of reflectance on the receptor. We propose Dynamic Semantic Segmentation (DSS) algorithms that utilized the mentioned capability to extract and rank Advanced Wide Field Sensor (AWiFS) images according to various features. This system stores data intelligently in the form of a sparse feature vector which drastically mitigates the computational and spatial costs incurred for further analysis. The compressed source image is divided into chunks and stored in the database for quicker retrieval. This work is intended to utilize readily available and cost effective resources like AWiFS dataset instead of depending on advanced technologies like Moderate Resolution Imaging Spectroradiometer (MODIS) for data which is scarce.



### Left Ventricle Segmentation and Quantification from Cardiac Cine MR Images via Multi-task Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.10221v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10221v1)
- **Published**: 2018-09-26 20:37:40+00:00
- **Updated**: 2018-09-26 20:37:40+00:00
- **Authors**: Shusil Dangi, Ziv Yaniv, Cristian A. Linte
- **Comment**: STACOM 2018 Workshop, MICCAI 2018
- **Journal**: None
- **Summary**: Segmentation of the left ventricle and quantification of various cardiac contractile functions is crucial for the timely diagnosis and treatment of cardiovascular diseases. Traditionally, the two tasks have been tackled independently. Here we propose a convolutional neural network based multi-task learning approach to perform both tasks simultaneously, such that, the network learns better representation of the data with improved generalization performance. Probabilistic formulation of the problem enables learning the task uncertainties during the training, which are used to automatically compute the weights for the tasks. We performed a five fold cross-validation of the myocardium segmentation obtained from the proposed multi-task network on 97 patient 4-dimensional cardiac cine-MRI datasets available through the STACOM LV segmentation challenge against the provided gold-standard myocardium segmentation, obtaining a Dice overlap of $0.849 \pm 0.036$ and mean surface distance of $0.274 \pm 0.083$ mm, while simultaneously estimating the myocardial area with mean absolute difference error of $205\pm198$ mm$^2$.



### Automatic Dataset Annotation to Learn CNN Pore Description for Fingerprint Recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.10229v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.10229v4)
- **Published**: 2018-09-26 20:50:23+00:00
- **Updated**: 2018-11-22 13:13:02+00:00
- **Authors**: Gabriel Dahia, Maurício Pamplona Segundo
- **Comment**: None
- **Journal**: None
- **Summary**: High-resolution fingerprint recognition often relies on sophisticated matching algorithms based on hand-crafted keypoint descriptors, with pores being the most common keypoint choice. Our method is the opposite of the prevalent approach: we use instead a simple matching algorithm based on robust local pore descriptors that are learned from the data using a CNN. In order to train this CNN in a fully supervised manner, we describe how the automatic alignment of fingerprint images can be used to obtain the required training annotations, which are otherwise missing in all publicly available datasets. This improves the state-of-the-art recognition results for both partial and full fingerprints in a public benchmark. To confirm that the observed improvement is due to the adoption of learned descriptors, we conduct an ablation study using the most successful pore descriptors previously used in the literature. All our code is available at https://github.com/gdahia/high-res-fingerprint-recognition



### A Coarse-To-Fine Framework For Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1809.10260v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.10260v1)
- **Published**: 2018-09-26 22:55:10+00:00
- **Updated**: 2018-09-26 22:55:10+00:00
- **Authors**: Chi Zhang, Alexander Loui
- **Comment**: 6 pages, 11 figures, to appear in Electronic Imaging, Visual
  Information Processing and Communication VIII, pp. 32-37(6)
- **Journal**: None
- **Summary**: In this study, we develop an unsupervised coarse-to-fine video analysis framework and prototype system to extract a salient object in a video sequence. This framework starts from tracking grid-sampled points along temporal frames, typically using KLT tracking method. The tracking points could be divided into several groups due to their inconsistent movements. At the same time, the SLIC algorithm is extended into 3D space to generate supervoxels. Coarse segmentation is achieved by combining the categorized tracking points and supervoxels of the corresponding frame in the video sequence. Finally, a graph-based fine segmentation algorithm is used to extract the moving object in the scene. Experimental results reveal that this method outperforms the previous approaches in terms of accuracy and robustness.



