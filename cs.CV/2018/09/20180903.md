# Arxiv Papers in cs.CV on 2018-09-03
### Hierarchically Learned View-Invariant Representations for Cross-View Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.00421v2
- **DOI**: 10.1109/TCSVT.2018.2868123
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00421v2)
- **Published**: 2018-09-03 01:31:05+00:00
- **Updated**: 2019-09-18 08:14:46+00:00
- **Authors**: Yang Liu, Zhaoyang Lu, Jing Li, Tao Yang
- **Comment**: Published in IEEE Transactions on Circuits and Systems for Video
  Technology, codes can be found at https://yangliu9208.github.io/JSRDA/
- **Journal**: None
- **Summary**: Recognizing human actions from varied views is challenging due to huge appearance variations in different views. The key to this problem is to learn discriminant view-invariant representations generalizing well across views. In this paper, we address this problem by learning view-invariant representations hierarchically using a novel method, referred to as Joint Sparse Representation and Distribution Adaptation (JSRDA). To obtain robust and informative feature representations, we first incorporate a sample-affinity matrix into the marginalized stacked denoising Autoencoder (mSDA) to obtain shared features, which are then combined with the private features. In order to make the feature representations of videos across views transferable, we then learn a transferable dictionary pair simultaneously from pairs of videos taken at different views to encourage each action video across views to have the same sparse representation. However, the distribution difference across views still exists because a unified subspace where the sparse representations of one action across views are the same may not exist when the view difference is large. Therefore, we propose a novel unsupervised distribution adaptation method that learns a set of projections that project the source and target views data into respective low-dimensional subspaces where the marginal and conditional distribution differences are reduced simultaneously. Therefore, the finally learned feature representation is view-invariant and robust for substantial distribution difference across views even the view difference is large. Experimental results on four multiview datasets show that our approach outperforms the state-ofthe-art approaches.



### Unsupervised Image Super-Resolution using Cycle-in-Cycle Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.00437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00437v1)
- **Published**: 2018-09-03 03:07:00+00:00
- **Updated**: 2018-09-03 03:07:00+00:00
- **Authors**: Yuan Yuan, Siyuan Liu, Jiawei Zhang, Yongbing Zhang, Chao Dong, Liang Lin
- **Comment**: 10 pages (reference included), 6 figures
- **Journal**: None
- **Summary**: We consider the single image super-resolution problem in a more general case that the low-/high-resolution pairs and the down-sampling process are unavailable. Different from traditional super-resolution formulation, the low-resolution input is further degraded by noises and blurring. This complicated setting makes supervised learning and accurate kernel estimation impossible. To solve this problem, we resort to unsupervised learning without paired data, inspired by the recent successful image-to-image translation applications. With generative adversarial networks (GAN) as the basic component, we propose a Cycle-in-Cycle network structure to tackle the problem within three steps. First, the noisy and blurry input is mapped to a noise-free low-resolution space. Then the intermediate image is up-sampled with a pre-trained deep model. Finally, we fine-tune the two modules in an end-to-end manner to get the high-resolution output. Experiments on NTIRE2018 datasets demonstrate that the proposed unsupervised method achieves comparable results as the state-of-the-art supervised models.



### YouTube-VOS: Sequence-to-Sequence Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1809.00461v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00461v1)
- **Published**: 2018-09-03 06:16:13+00:00
- **Updated**: 2018-09-03 06:16:13+00:00
- **Authors**: Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang, Dingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen, Thomas Huang
- **Comment**: ECCV 2018 accepted paper
- **Journal**: None
- **Summary**: Learning long-term spatial-temporal features are critical for many video analysis tasks. However, existing video segmentation methods predominantly rely on static image segmentation techniques, and methods capturing temporal dependency for segmentation have to depend on pretrained optical flow models, leading to suboptimal solutions for the problem. End-to-end sequential learning to explore spatial-temporal features for video segmentation is largely limited by the scale of available video segmentation datasets, i.e., even the largest video segmentation dataset only contains 90 short video clips. To solve this problem, we build a new large-scale video object segmentation dataset called YouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains 3,252 YouTube video clips and 78 categories including common objects and human activities. This is by far the largest video object segmentation dataset to our knowledge and we have released it at https://youtube-vos.org. Based on this dataset, we propose a novel sequence-to-sequence network to fully exploit long-term spatial-temporal information in videos for segmentation. We demonstrate that our method is able to achieve the best results on our YouTube-VOS test set and comparable results on DAVIS 2016 compared to the current state-of-the-art methods. Experiments show that the large scale dataset is indeed a key factor to the success of our model.



### Image Segmentation with Pseudo-marginal MCMC Sampling and Nonparametric Shape Priors
- **Arxiv ID**: http://arxiv.org/abs/1809.00488v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00488v1)
- **Published**: 2018-09-03 08:24:43+00:00
- **Updated**: 2018-09-03 08:24:43+00:00
- **Authors**: Ertunc Erdil, Sinan Yildirim, Tolga Tasdizen, Mujdat Cetin
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an efficient pseudo-marginal Markov chain Monte Carlo (MCMC) sampling approach to draw samples from posterior shape distributions for image segmentation. The computation time of the proposed approach is independent from the size of the training set used to learn the shape prior distribution nonparametrically. Therefore, it scales well for very large data sets. Our approach is able to characterize the posterior probability density in the space of shapes through its samples, and to return multiple solutions, potentially from different modes of a multimodal probability density, which would be encountered, e.g., in segmenting objects from multiple shape classes. Experimental results demonstrate the potential of the proposed approach.



### Prediction of Electric Multiple Unit Fleet Size Based on Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1809.00491v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00491v1)
- **Published**: 2018-09-03 08:30:17+00:00
- **Updated**: 2018-09-03 08:30:17+00:00
- **Authors**: Boliang Lin
- **Comment**: 13 pages,7 figures
- **Journal**: None
- **Summary**: With the expansion of high-speed railway network and growth of passenger transportation demands, the fleet size of electric multiple unit (EMU) in China needs to be adjusted accordingly. Generally, an EMU train costs tens of millions of dollars which constitutes a significant portion of capital investment. Thus, the prediction of EMU fleet size has attracted increasing attention from associated railway departments. First, this paper introduces a typical architecture of convolutional neural network (CNN) and its basic theory. Then, some data of nine indices, such as passenger traffic volume and length of high-speed railways in operation, is collected and preprocessed. Next, a CNN and a backpropagation neural network (BPNN) are constructed and trained aiming to predict EMU fleet size in the following years. The differences and performances of these two networks in computation experiments are analyzed in-depth. The results indicate that the CNN is superior to the BPNN both in generalization ability and fitting accuracy, and CNN can serve as an aid in EMU fleet size prediction.



### LRS3-TED: a large-scale dataset for visual speech recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.00496v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00496v2)
- **Published**: 2018-09-03 08:38:34+00:00
- **Updated**: 2018-10-28 14:29:46+00:00
- **Authors**: Triantafyllos Afouras, Joon Son Chung, Andrew Zisserman
- **Comment**: The audio-visual dataset can be downloaded from
  http://www.robots.ox.ac.uk/~vgg/data/lip_reading/
- **Journal**: None
- **Summary**: This paper introduces a new multi-modal dataset for visual and audio-visual speech recognition. It includes face tracks from over 400 hours of TED and TEDx videos, along with the corresponding subtitles and word alignment boundaries. The new dataset is substantially larger in scale compared to other public datasets that are available for general research.



### Learning Vision-based Cohesive Flight in Drone Swarms
- **Arxiv ID**: http://arxiv.org/abs/1809.00543v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/1809.00543v1)
- **Published**: 2018-09-03 10:44:28+00:00
- **Updated**: 2018-09-03 10:44:28+00:00
- **Authors**: Fabian Schilling, Julien Lecoeur, Fabrizio Schiano, Dario Floreano
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a data-driven approach to learning vision-based collective behavior from a simple flocking algorithm. We simulate a swarm of quadrotor drones and formulate the controller as a regression problem in which we generate 3D velocity commands directly from raw camera images. The dataset is created by simultaneously acquiring omnidirectional images and computing the corresponding control command from the flocking algorithm. We show that a convolutional neural network trained on the visual inputs of the drone can learn not only robust collision avoidance but also coherence of the flock in a sample-efficient manner. The neural controller effectively learns to localize other agents in the visual input, which we show by visualizing the regions with the most influence on the motion of an agent. This weakly supervised saliency map can be computed efficiently and may be used as a prior for subsequent detection and relative localization of other agents. We remove the dependence on sharing positions among flock members by taking only local visual information into account for control. Our work can therefore be seen as the first step towards a fully decentralized, vision-based flock without the need for communication or visual markers to aid detection of other agents.



### Object Pose Estimation from Monocular Image using Multi-View Keypoint Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1809.00553v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00553v1)
- **Published**: 2018-09-03 11:16:11+00:00
- **Updated**: 2018-09-03 11:16:11+00:00
- **Authors**: Jogendra Nath Kundu, Rahul M. V., Aditya Ganeshan, R. Venkatesh Babu
- **Comment**: Accepted in ECCV-W; Code available at this http url:
  https://github.com/val-iisc/pose_estimation
- **Journal**: None
- **Summary**: Understanding the geometry and pose of objects in 2D images is a fundamental necessity for a wide range of real world applications. Driven by deep neural networks, recent methods have brought significant improvements to object pose estimation. However, they suffer due to scarcity of keypoint/pose-annotated real images and hence can not exploit the object's 3D structural information effectively. In this work, we propose a data-efficient method which utilizes the geometric regularity of intraclass objects for pose estimation. First, we learn pose-invariant local descriptors of object parts from simple 2D RGB images. These descriptors, along with keypoints obtained from renders of a fixed 3D template model are then used to generate keypoint correspondence maps for a given monocular real image. Finally, a pose estimation network predicts 3D pose of the object using these correspondence maps. This pipeline is further extended to a multi-view approach, which assimilates keypoint information from correspondence sets generated from multiple views of the 3D template model. Fusion of multi-view information significantly improves geometric comprehension of the system which in turn enhances the pose estimation performance. Furthermore, use of correspondence framework responsible for the learning of pose invariant keypoint descriptor also allows us to effectively alleviate the data-scarcity problem. This enables our method to achieve state-of-the-art performance on multiple real-image viewpoint estimation datasets, such as Pascal3D+ and ObjectNet3D. To encourage reproducible research, we have released the codes for our proposed approach.



### PathGAN: Visual Scanpath Prediction with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.00567v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1809.00567v1)
- **Published**: 2018-09-03 11:57:38+00:00
- **Updated**: 2018-09-03 11:57:38+00:00
- **Authors**: Marc Assens, Xavier Giro-i-Nieto, Kevin McGuinness, Noel E. O'Connor
- **Comment**: ECCV 2018 Workshop on Egocentric Perception, Interaction and
  Computing (EPIC). This work obtained the 2nd award in Prediction of Head-gaze
  Scan-paths for Images, and the 2nd award in Prediction of Eye-gaze Scan-paths
  for Images at the IEEE ICME 2018 Salient360! Challenge
- **Journal**: None
- **Summary**: We introduce PathGAN, a deep neural network for visual scanpath prediction trained on adversarial examples. A visual scanpath is defined as the sequence of fixation points over an image defined by a human observer with its gaze. PathGAN is composed of two parts, the generator and the discriminator. Both parts extract features from images using off-the-shelf networks, and train recurrent layers to generate or discriminate scanpaths accordingly. In scanpath prediction, the stochastic nature of the data makes it very difficult to generate realistic predictions using supervised learning strategies, but we adopt adversarial training as a suitable alternative. Our experiments prove how PathGAN improves the state of the art of visual scanpath prediction on the iSUN and Salient360! datasets. Source code and models are available at https://imatge-upc.github.io/pathgan/



### Optical Flow Super-Resolution Based on Image Guidence Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1809.00588v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00588v1)
- **Published**: 2018-09-03 13:03:21+00:00
- **Updated**: 2018-09-03 13:03:21+00:00
- **Authors**: Liping Zhang, Zongqing Lu, Qingmin Liao
- **Comment**: 20 pages,7 figures
- **Journal**: None
- **Summary**: The convolutional neural network model for optical flow estimation usually outputs a low-resolution(LR) optical flow field. To obtain the corresponding full image resolution,interpolation and variational approach are the most common options, which do not effectively improve the results. With the motivation of various convolutional neural network(CNN) structures succeeded in single image super-resolution(SISR) task, an end-to-end convolutional neural network is proposed to reconstruct the high resolution(HR) optical flow field from initial LR optical flow with the guidence of the first frame used in optical flow estimation. Our optical flow super-resolution(OFSR) problem differs from the general SISR problem in two main aspects. Firstly, the optical flow includes less texture information than image so that the SISR CNN structures can't be directly used in our OFSR problem. Secondly, the initial LR optical flow data contains estimation error, while the LR image data for SISR is generally a bicubic downsampled, blurred, and noisy version of HR ground truth. We evaluate the proposed approach on two different optical flow estimation mehods and show that it can not only obtain the full image resolution, but generate more accurate optical flow field (Accuracy improve 15% on FlyingChairs, 13% on MPI Sintel) with sharper edges than the estimation result of original method.



### Adversarial Attack Type I: Cheat Classifiers by Significant Changes
- **Arxiv ID**: http://arxiv.org/abs/1809.00594v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.00594v2)
- **Published**: 2018-09-03 13:25:06+00:00
- **Updated**: 2019-07-22 13:00:58+00:00
- **Authors**: Sanli Tang, Xiaolin Huang, Mingjian Chen, Chengjin Sun, Jie Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the great success of deep neural networks, the adversarial attack can cheat some well-trained classifiers by small permutations. In this paper, we propose another type of adversarial attack that can cheat classifiers by significant changes. For example, we can significantly change a face but well-trained neural networks still recognize the adversarial and the original example as the same person. Statistically, the existing adversarial attack increases Type II error and the proposed one aims at Type I error, which are hence named as Type II and Type I adversarial attack, respectively. The two types of attack are equally important but are essentially different, which are intuitively explained and numerically evaluated. To implement the proposed attack, a supervised variation autoencoder is designed and then the classifier is attacked by updating the latent variables using gradient information. {Besides, with pre-trained generative models, Type I attack on latent spaces is investigated as well.} Experimental results show that our method is practical and effective to generate Type I adversarial examples on large-scale image datasets. Most of these generated examples can pass detectors designed for defending Type II attack and the strengthening strategy is only efficient with a specific type attack, both implying that the underlying reasons for Type I and Type II attack are different.



### Image computing for fibre-bundle endomicroscopy: A review
- **Arxiv ID**: http://arxiv.org/abs/1809.00604v1
- **DOI**: 10.1016/j.media.2019.101620
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00604v1)
- **Published**: 2018-09-03 13:57:47+00:00
- **Updated**: 2018-09-03 13:57:47+00:00
- **Authors**: Antonios Perperidis, Kevin Dhaliwal, Stephen McLaughlin, Tom Vercauteren
- **Comment**: 38 Pages, 2 Figures, 6 Tables
- **Journal**: None
- **Summary**: Endomicroscopy is an emerging imaging modality, that facilitates the acquisition of in vivo, in situ optical biopsies, assisting diagnostic and potentially therapeutic interventions. While there is a diverse and constantly expanding range of commercial and experimental optical biopsy platforms available, fibre-bundle endomicroscopy is currently the most widely used platform and is approved for clinical use in a range of clinical indications. Miniaturised, flexible fibre-bundles, guided through the working channel of endoscopes, needles and catheters, enable high-resolution imaging across a variety of organ systems. Yet, the nature of image acquisition though a fibre-bundle gives rise to several inherent characteristics and limitations necessitating novel and effective image pre- and post-processing algorithms, ranging from image formation, enhancement and mosaicing to pathology detection and quantification. This paper introduces the underlying technology and most prevalent clinical applications of fibre-bundle endomicroscopy, and provides a comprehensive, up-to-date, review of relevant image reconstruction, analysis and understanding/inference methodologies. Furthermore, current limitations as well as future challenges and opportunities in fibre-bundle endomicroscopy computing are identified and discussed.



### Learning Saliency Prediction From Sparse Fixation Pixel Map
- **Arxiv ID**: http://arxiv.org/abs/1809.00644v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1809.00644v1)
- **Published**: 2018-09-03 16:31:11+00:00
- **Updated**: 2018-09-03 16:31:11+00:00
- **Authors**: Shanghua Xiao
- **Comment**: 7 pages, 7 figures
- **Journal**: None
- **Summary**: Ground truth for saliency prediction datasets consists of two types of map data: fixation pixel map which records the human eye movements on sample images, and fixation blob map generated by performing gaussian blurring on the corresponding fixation pixel map. Current saliency approaches perform prediction by directly pixel-wise regressing the input image into saliency map with fixation blob as ground truth, yet learning saliency from fixation pixel map is not explored. In this work, we propose a first-of-its-kind approach of learning saliency prediction from sparse fixation pixel map, and a novel loss function for training from such sparse fixation. We utilize clustering to extract sparse fixation pixel from the raw fixation pixel map, and add a max-pooling transformation on the output to avoid false penalty between sparse outputs and labels caused by nearby but non-overlapping saliency pixels when calculating loss. This approach provides a novel perspective for achieving saliency prediction. We evaluate our approach over multiple benchmark datasets, and achieve competitive performance in terms of multiple metrics comparing with state-of-the-art saliency methods.



### Detail Preserving Depth Estimation from a Single Image Using Attention Guided Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.00646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00646v1)
- **Published**: 2018-09-03 16:33:30+00:00
- **Updated**: 2018-09-03 16:33:30+00:00
- **Authors**: Zhixiang Hao, Yu Li, Shaodi You, Feng Lu
- **Comment**: Published at IEEE International Conference on 3D Vision (3DV) 2018
- **Journal**: None
- **Summary**: Convolutional Neural Networks have demonstrated superior performance on single image depth estimation in recent years. These works usually use stacked spatial pooling or strided convolution to get high-level information which are common practices in classification task. However, depth estimation is a dense prediction problem and low-resolution feature maps usually generate blurred depth map which is undesirable in application. In order to produce high quality depth map, say clean and accurate, we propose a network consists of a Dense Feature Extractor (DFE) and a Depth Map Generator (DMG). The DFE combines ResNet and dilated convolutions. It extracts multi-scale information from input image while keeping the feature maps dense. As for DMG, we use attention mechanism to fuse multi-scale features produced in DFE. Our Network is trained end-to-end and does not need any post-processing. Hence, it runs fast and can predict depth map in about 15 fps. Experiment results show that our method is competitive with the state-of-the-art in quantitative evaluation, but can preserve better structural details of the scene depth.



### Context-Patch Face Hallucination Based on Thresholding Locality-constrained Representation and Reproducing Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.00665v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00665v2)
- **Published**: 2018-09-03 17:23:38+00:00
- **Updated**: 2018-09-15 03:15:30+00:00
- **Authors**: Junjun Jiang, Yi Yu, Suhua Tang, Jiayi Ma, Akiko Aizawa, Kiyoharu Aizawa
- **Comment**: 13 pages, 15 figures, Accepted by IEEE TCYB
- **Journal**: None
- **Summary**: Face hallucination is a technique that reconstruct high-resolution (HR) faces from low-resolution (LR) faces, by using the prior knowledge learned from HR/LR face pairs. Most state-of-the-arts leverage position-patch prior knowledge of human face to estimate the optimal representation coefficients for each image patch. However, they focus only the position information and usually ignore the context information of image patch. In addition, when they are confronted with misalignment or the Small Sample Size (SSS) problem, the hallucination performance is very poor. To this end, this study incorporates the contextual information of image patch and proposes a powerful and efficient context-patch based face hallucination approach, namely Thresholding Locality-constrained Representation and Reproducing learning (TLcR-RL). Under the context-patch based framework, we advance a thresholding based representation method to enhance the reconstruction accuracy and reduce the computational complexity. To further improve the performance of the proposed algorithm, we propose a promotion strategy called reproducing learning. By adding the estimated HR face to the training set, which can simulates the case that the HR version of the input LR face is present in the training set, thus iteratively enhancing the final hallucination result. Experiments demonstrate that the proposed TLcR-RL method achieves a substantial increase in the hallucinated results, both subjectively and objectively. Additionally, the proposed framework is more robust to face misalignment and the SSS problem, and its hallucinated HR face is still very good when the LR test face is from the real-world. The MATLAB source code is available at https://github.com/junjun-jiang/TLcR-RL



### Diverse and Coherent Paragraph Generation from Images
- **Arxiv ID**: http://arxiv.org/abs/1809.00681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00681v1)
- **Published**: 2018-09-03 18:16:08+00:00
- **Updated**: 2018-09-03 18:16:08+00:00
- **Authors**: Moitreya Chatterjee, Alexander G. Schwing
- **Comment**: Camera Ready Version of ECCV 2018 paper; Coupled with supplementary
- **Journal**: None
- **Summary**: Paragraph generation from images, which has gained popularity recently, is an important task for video summarization, editing, and support of the disabled. Traditional image captioning methods fall short on this front, since they aren't designed to generate long informative descriptions. Moreover, the vanilla approach of simply concatenating multiple short sentences, possibly synthesized from a classical image captioning system, doesn't embrace the intricacies of paragraphs: coherent sentences, globally consistent structure, and diversity. To address those challenges, we propose to augment paragraph generation techniques with 'coherence vectors', 'global topic vectors', and modeling of the inherent ambiguity of associating paragraphs with images, via a variational auto-encoder formulation. We demonstrate the effectiveness of the developed approach on two datasets, outperforming existing state-of-the-art techniques on both.



### Convolutional Neural Network for Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/1809.00696v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00696v2)
- **Published**: 2018-09-03 19:30:13+00:00
- **Updated**: 2018-11-26 05:39:29+00:00
- **Authors**: Nishant Nikhil, Brendan Tran Morris
- **Comment**: Accepted at ECCV 2018 workshop - Anticipating Human Behavior
- **Journal**: None
- **Summary**: Predicting trajectories of pedestrians is quintessential for autonomous robots which share the same environment with humans. In order to effectively and safely interact with humans, trajectory prediction needs to be both precise and computationally efficient. In this work, we propose a convolutional neural network (CNN) based human trajectory prediction approach. Unlike more recent LSTM-based moles which attend sequentially to each frame, our model supports increased parallelism and effective temporal representation. The proposed compact CNN model is faster than the current approaches yet still yields competitive results.



### A Global Alignment Kernel based Approach for Group-level Happiness Intensity Estimation
- **Arxiv ID**: http://arxiv.org/abs/1809.03313v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03313v1)
- **Published**: 2018-09-03 20:23:30+00:00
- **Updated**: 2018-09-03 20:23:30+00:00
- **Authors**: Xiaohua Huang, Abhinav Dhall, Roland Goecke, Matti Pietikainen, Guoying Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: With the progress in automatic human behavior understanding, analysing the perceived affect of multiple people has been recieved interest in affective computing community. Unlike conventional facial expression analysis, this paper primarily focuses on analysing the behaviour of multiple people in an image. The proposed method is based on support vector regression with the combined global alignment kernels (GAKs) to estimate the happiness intensity of a group of people. We first exploit Riesz-based volume local binary pattern (RVLBP) and deep convolutional neural network (CNN) based features for characterizing facial images. Furthermore, we propose to use the GAK for RVLBP and deep CNN features, respectively for explicitly measuring the similarity of two group-level images. Specifically, we exploit the global weight sort scheme to sort the face images from group-level image according to their spatial weights, making an efficient data structure to GAK. Lastly, we propose Multiple kernel learning based on three combination strategies for combining two respective GAKs based on RVLBP and deep CNN features, such that enhancing the discriminative ability of each GAK. Intensive experiments are performed on the challenging group-level happiness intensity database, namely HAPPEI. Our experimental results demonstrate that the proposed approach achieves promising performance for group happiness intensity analysis, when compared with the recent state-of-the-art methods.



### InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes Dataset
- **Arxiv ID**: http://arxiv.org/abs/1809.00716v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1809.00716v1)
- **Published**: 2018-09-03 20:42:27+00:00
- **Updated**: 2018-09-03 20:42:27+00:00
- **Authors**: Wenbin Li, Sajad Saeedi, John McCormac, Ronald Clark, Dimos Tzoumanikas, Qing Ye, Yuzhong Huang, Rui Tang, Stefan Leutenegger
- **Comment**: British Machine Vision Conference (BMVC) 2018
- **Journal**: None
- **Summary**: Datasets have gained an enormous amount of popularity in the computer vision community, from training and evaluation of Deep Learning-based methods to benchmarking Simultaneous Localization and Mapping (SLAM). Without a doubt, synthetic imagery bears a vast potential due to scalability in terms of amounts of data obtainable without tedious manual ground truth annotations or measurements. Here, we present a dataset with the aim of providing a higher degree of photo-realism, larger scale, more variability as well as serving a wider range of purposes compared to existing datasets. Our dataset leverages the availability of millions of professional interior designs and millions of production-level furniture and object assets -- all coming with fine geometric details and high-resolution texture. We render high-resolution and high frame-rate video sequences following realistic trajectories while supporting various camera types as well as providing inertial measurements. Together with the release of the dataset, we will make executable program of our interactive simulator software as well as our renderer available at https://interiornetdataset.github.io. To showcase the usability and uniqueness of our dataset, we show benchmarking results of both sparse and dense SLAM algorithms.



### Estimating Small Differences in Car-Pose from Orbits
- **Arxiv ID**: http://arxiv.org/abs/1809.00720v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00720v1)
- **Published**: 2018-09-03 21:17:59+00:00
- **Updated**: 2018-09-03 21:17:59+00:00
- **Authors**: Berkay Kicanaoglu, Ran Tao, Arnold W. M. Smeulders
- **Comment**: to appear in BMVC2018
- **Journal**: None
- **Summary**: Distinction among nearby poses and among symmetries of an object is challenging. In this paper, we propose a unified, group-theoretic approach to tackle both. Different from existing works which directly predict absolute pose, our method measures the pose of an object relative to another pose, i.e., the pose difference. The proposed method generates the complete orbit of an object from a single view of the object with respect to the subgroup of SO(3) of rotations around the z-axis, and compares the orbit of the object with another orbit using a novel orbit metric to estimate the pose difference. The generated orbit in the latent space records all the differences in pose in the original observational space, and as a result, the method is capable of finding subtle differences in pose. We demonstrate the effectiveness of the proposed method on cars, where identifying the subtle pose differences is vital.



