# Arxiv Papers in cs.CV on 2018-09-15
### DocFace+: ID Document to Selfie Matching
- **Arxiv ID**: http://arxiv.org/abs/1809.05620v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05620v2)
- **Published**: 2018-09-15 00:31:44+00:00
- **Updated**: 2018-09-18 05:15:51+00:00
- **Authors**: Yichun Shi, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Numerous activities in our daily life require us to verify who we are by showing our ID documents containing face images, such as passports and driver licenses, to human operators. However, this process is slow, labor intensive and unreliable. As such, an automated system for matching ID document photos to live face images (selfies) in real time and with high accuracy is required. In this paper, we propose DocFace+ to meet this objective. We first show that gradient-based optimization methods converge slowly (due to the underfitting of classifier weights) when many classes have very few samples, a characteristic of existing ID-selfie datasets. To overcome this shortcoming, we propose a method, called dynamic weight imprinting (DWI), to update the classifier weights, which allows faster convergence and more generalizable representations. Next, a pair of sibling networks with partially shared parameters are trained to learn a unified face representation with domain-specific parameters. Cross-validation on an ID-selfie dataset shows that while a publicly available general face matcher (SphereFace) only achieves a True Accept Rate (TAR) of 59.29+-1.55% at a False Accept Rate (FAR) of 0.1% on the problem, DocFace+ improves the TAR to 97.51+-0.40%.



### DLO: Direct LiDAR Odometry for 2.5D Outdoor Environment
- **Arxiv ID**: http://arxiv.org/abs/1809.10199v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1809.10199v1)
- **Published**: 2018-09-15 01:20:06+00:00
- **Updated**: 2018-09-15 01:20:06+00:00
- **Authors**: Lu Sun, Junqiao Zhao, Xudong He, Chen Ye
- **Comment**: None
- **Journal**: None
- **Summary**: For autonomous vehicles, high-precision real-time localization is the guarantee of stable driving. Compared with the visual odometry (VO), the LiDAR odometry (LO) has the advantages of higher accuracy and better stability. However, 2D LO is only suitable for the indoor environment, and 3D LO has less efficiency in general. Both are not suitable for the online localization of an autonomous vehicle in an outdoor driving environment. In this paper, a direct LO method based on the 2.5D grid map is proposed. The fast semi-dense direct method proposed for VO is employed to register two 2.5D maps. Experiments show that this method is superior to both the 3D-NDT and LOAM in the outdoor environment.



### OffsetNet: Deep Learning for Localization in the Lung using Rendered Images
- **Arxiv ID**: http://arxiv.org/abs/1809.05645v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05645v1)
- **Published**: 2018-09-15 04:15:16+00:00
- **Updated**: 2018-09-15 04:15:16+00:00
- **Authors**: Jake Sganga, David Eng, Chauncey Graetzel, David Camarillo
- **Comment**: 7 pages, 10 figures
- **Journal**: None
- **Summary**: Navigating surgical tools in the dynamic and tortuous anatomy of the lung's airways requires accurate, real-time localization of the tools with respect to the preoperative scan of the anatomy. Such localization can inform human operators or enable closed-loop control by autonomous agents, which would require accuracy not yet reported in the literature. In this paper, we introduce a deep learning architecture, called OffsetNet, to accurately localize a bronchoscope in the lung in real-time. After training on only 30 minutes of recorded camera images in conserved regions of a lung phantom, OffsetNet tracks the bronchoscope's motion on a held-out recording through these same regions at an update rate of 47 Hz and an average position error of 1.4 mm. Because this model performs poorly in less conserved regions, we augment the training dataset with simulated images from these regions. To bridge the gap between camera and simulated domains, we implement domain randomization and a generative adversarial network (GAN). After training on simulated images, OffsetNet tracks the bronchoscope's motion in less conserved regions at an average position error of 2.4 mm, which meets conservative thresholds required for successful tracking.



### A New Multi-vehicle Trajectory Generator to Simulate Vehicle-to-Vehicle Encounters
- **Arxiv ID**: http://arxiv.org/abs/1809.05680v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1809.05680v5)
- **Published**: 2018-09-15 09:16:34+00:00
- **Updated**: 2019-02-24 14:53:07+00:00
- **Authors**: Wenhao Ding, Wenshuo Wang, Ding Zhao
- **Comment**: 6 pages, accepted by ICRA 2019
- **Journal**: None
- **Summary**: Generating multi-vehicle trajectories from existing limited data can provide rich resources for autonomous vehicle development and testing. This paper introduces a multi-vehicle trajectory generator (MTG) that can encode multi-vehicle interaction scenarios (called driving encounters) into an interpretable representation from which new driving encounter scenarios are generated by sampling. The MTG consists of a bi-directional encoder and a multi-branch decoder. A new disentanglement metric is then developed for model analyses and comparisons in terms of model robustness and the independence of the latent codes. Comparison of our proposed MTG with $\beta$-VAE and InfoGAN demonstrates that the MTG has stronger capability to purposely generate rational vehicle-to-vehicle encounters through operating the disentangled latent codes. Thus the MTG could provide more data for engineers and researchers to develop testing and evaluation scenarios for autonomous vehicles.



### Multi-Scale Deep Compressive Sensing Network
- **Arxiv ID**: http://arxiv.org/abs/1809.05717v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.05717v2)
- **Published**: 2018-09-15 14:05:27+00:00
- **Updated**: 2018-09-18 10:51:07+00:00
- **Authors**: Thuong Nguyen Canh, Byeungwoo Jeon
- **Comment**: 4 pages, 4 figures, 2 tables, IEEE International Conference on Visual
  Communication and Image Processing (VCIP)
- **Journal**: None
- **Summary**: With joint learning of sampling and recovery, the deep learning-based compressive sensing (DCS) has shown significant improvement in performance and running time reduction. Its reconstructed image, however, losses high-frequency content especially at low subrates. This happens similarly in the multi-scale sampling scheme which also samples more low-frequency components. In this paper, we propose a multi-scale DCS convolutional neural network (MS-DCSNet) in which we convert image signal using multiple scale-based wavelet transform, then capture it through convolution block by block across scales. The initial reconstructed image is directly recovered from multi-scale measurements. Multi-scale wavelet convolution is utilized to enhance the final reconstruction quality. The network is able to learn both multi-scale sampling and multi-scale reconstruction, thus results in better reconstruction quality.



