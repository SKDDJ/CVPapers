# Arxiv Papers in cs.CV on 2018-09-30
### Automatic Skin Lesion Segmentation Using GrabCut in HSV Colour Space
- **Arxiv ID**: http://arxiv.org/abs/1810.00871v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00871v1)
- **Published**: 2018-09-30 01:30:43+00:00
- **Updated**: 2018-09-30 01:30:43+00:00
- **Authors**: Fakrul Islam Tushar
- **Comment**: None
- **Journal**: None
- **Summary**: Skin lesion segmentation is one of the first steps towards automatic Computer-Aided Diagnosis of skin cancer. Vast variety in the appearance of the skin lesion makes this task very challenging. The contribution of this paper is to apply a power foreground extraction technique called GrabCut for automatic skin lesion segmentation with minimal human interaction in HSV color space. Preprocessing was performed for removing the outer black border. Jaccard Index was measured to evaluate the performance of the segmentation method. On average, 0.71 Jaccard Index was achieved on 1000 images from ISIC challenge 2017 Training Dataset.



### DIMENSION: Dynamic MR Imaging with Both K-space and Spatial Prior Knowledge Obtained via Multi-Supervised Network Training
- **Arxiv ID**: http://arxiv.org/abs/1810.00302v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00302v4)
- **Published**: 2018-09-30 03:01:14+00:00
- **Updated**: 2018-11-06 04:59:23+00:00
- **Authors**: Shanshan Wang, Ziwen Ke, Huitao Cheng, Sen Jia, Ying Leslie, Hairong Zheng, Dong Liang
- **Comment**: 11 pages, 12 figures
- **Journal**: None
- **Summary**: Dynamic MR image reconstruction from incomplete k-space data has generated great research interest due to its capability in reducing scan time. Nevertheless, the reconstruction problem is still challenging due to its ill-posed nature. Most existing methods either suffer from long iterative reconstruction time or explore limited prior knowledge. This paper proposes a dynamic MR imaging method with both k-space and spatial prior knowledge integrated via multi-supervised network training, dubbed as DIMENSION. Specifically, the DIMENSION architecture consists of a frequential prior network for updating the k-space with its network prediction and a spatial prior network for capturing image structures and details. Furthermore, a multisupervised network training technique is developed to constrain the frequency domain information and reconstruction results at different levels. The comparisons with classical k-t FOCUSS, k-t SLR, L+S and the state-of-the-art CNN-based method on in vivo datasets show our method can achieve improved reconstruction results in shorter time.



### Correlation Propagation Networks for Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/1810.00304v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00304v1)
- **Published**: 2018-09-30 03:14:41+00:00
- **Updated**: 2018-09-30 03:14:41+00:00
- **Authors**: Zichuan Liu, Guosheng Lin, Wang Ling Goh, Fayao Liu, Chunhua Shen, Xiaokang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a novel hybrid method for scene text detection namely Correlation Propagation Network (CPN). It is an end-to-end trainable framework engined by advanced Convolutional Neural Networks. Our CPN predicts text objects according to both top-down observations and the bottom-up cues. Multiple candidate boxes are assembled by a spatial communication mechanism call Correlation Propagation (CP). The extracted spatial features by CNN are regarded as node features in a latticed graph and Correlation Propagation algorithm runs distributively on each node to update the hypothesis of corresponding object centers. The CP process can flexibly handle scale-varying and rotated text objects without using predefined bounding box templates. Benefit from its distributive nature, CPN is computationally efficient and enjoys a high level of parallelism. Moreover, we introduce deformable convolution to the backbone network to enhance the adaptability to long texts. The evaluation on public benchmarks shows that the proposed method achieves state-of-art performance, and it significantly outperforms the existing methods for handling multi-scale and multi-oriented text objects with much lower computation cost.



### Posture recognition using an RGB-D camera : exploring 3D body modeling and deep learning approaches
- **Arxiv ID**: http://arxiv.org/abs/1810.00308v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00308v2)
- **Published**: 2018-09-30 03:50:09+00:00
- **Updated**: 2018-10-13 00:07:11+00:00
- **Authors**: Mohamed El Amine Elforaici, Ismail Chaaraoui, Wassim Bouachir, Youssef Ouakrim, Neila Mezghani
- **Comment**: IEEE Life Sciences Conference 2018
- **Journal**: None
- **Summary**: The emergence of RGB-D sensors offered new possibilities for addressing complex artificial vision problems efficiently. Human posture recognition is among these computer vision problems, with a wide range of applications such as ambient assisted living and intelligent health care systems. In this context, our paper presents novel methods and ideas to design automatic posture recognition systems using an RGB-D camera. More specifically, we introduce two supervised methods to learn and recognize human postures using the main types of visual data provided by an RGB-D camera. The first method is based on convolutional features extracted from 2D images. Convolutional Neural Networks (CNNs) are trained to recognize human postures using transfer learning on RGB and depth images. Secondly, we propose to model the posture using the body joint configuration in the 3D space. Posture recognition is then performed through SVM classification of 3D skeleton-based features. To evaluate the proposed methods, we created a challenging posture recognition dataset with a considerable variability regarding the acquisition conditions. The experimental results demonstrated comparable performances and high precision for both methods in recognizing human postures, with a slight superiority for the CNN-based method when applied on depth images. Moreover, the two approaches demonstrated a high robustness to several perturbation factors, such as scale and orientation change.



### Modeling Uncertainty with Hedged Instance Embedding
- **Arxiv ID**: http://arxiv.org/abs/1810.00319v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.00319v6)
- **Published**: 2018-09-30 04:51:27+00:00
- **Updated**: 2019-08-27 00:31:41+00:00
- **Authors**: Seong Joon Oh, Kevin Murphy, Jiyan Pan, Joseph Roth, Florian Schroff, Andrew Gallagher
- **Comment**: 15 pages, 11 figures, updated version of ICLR'19
- **Journal**: None
- **Summary**: Instance embeddings are an efficient and versatile image representation that facilitates applications like recognition, verification, retrieval, and clustering. Many metric learning methods represent the input as a single point in the embedding space. Often the distance between points is used as a proxy for match confidence. However, this can fail to represent uncertainty arising when the input is ambiguous, e.g., due to occlusion or blurriness. This work addresses this issue and explicitly models the uncertainty by hedging the location of each input in the embedding space. We introduce the hedged instance embedding (HIB) in which embeddings are modeled as random variables and the model is trained under the variational information bottleneck principle. Empirical results on our new N-digit MNIST dataset show that our method leads to the desired behavior of hedging its bets across the embedding space upon encountering ambiguous inputs. This results in improved performance for image matching and classification tasks, more structure in the learned embedding space, and an ability to compute a per-exemplar uncertainty measure that is correlated with downstream performance.



### Multi-Level Contextual Network for Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1810.00327v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00327v1)
- **Published**: 2018-09-30 06:45:16+00:00
- **Updated**: 2018-09-30 06:45:16+00:00
- **Authors**: Amirhossein Dadashzadeh, Alireza Tavakoli Targhi
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate and reliable image segmentation is an essential part of biomedical image analysis. In this paper, we consider the problem of biomedical image segmentation using deep convolutional neural networks. We propose a new end-to-end network architecture that effectively integrates local and global contextual patterns of histologic primitives to obtain a more reliable segmentation result. Specifically, we introduce a deep fully convolution residual network with a new skip connection strategy to control the contextual information passed forward. Moreover, our trained model is also computationally inexpensive due to its small number of network parameters. We evaluate our method on two public datasets for epithelium segmentation and tubule segmentation tasks. Our experimental results show that the proposed method provides a fast and effective way of producing a pixel-wise dense prediction of biomedical images.



### A Multi-Face Challenging Dataset for Robust Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1810.01898v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01898v2)
- **Published**: 2018-09-30 07:04:59+00:00
- **Updated**: 2019-03-28 01:03:01+00:00
- **Authors**: Shiv Ram Dubey, Snehasis Mukherjee
- **Comment**: 15th International Conference on Control, Automation, Robotics and
  Vision (ICARCV 2018)
- **Journal**: None
- **Summary**: Face recognition in images is an active area of interest among the computer vision researchers. However, recognizing human face in an unconstrained environment, is a relatively less-explored area of research. Multiple face recognition in unconstrained environment is a challenging task, due to the variation of view-point, scale, pose, illumination and expression of the face images. Partial occlusion of faces makes the recognition task even more challenging. The contribution of this paper is two-folds: introducing a challenging multiface dataset (i.e., IIITS MFace Dataset) for face recognition in unconstrained environment and evaluating the performance of state-of-the-art hand-designed and deep learning based face descriptors on the dataset. The proposed IIITS MFace dataset contains faces with challenges like pose variation, occlusion, mask, spectacle, expressions, change of illumination, etc. We experiment with several state-of-the-art face descriptors, including recent deep learning based face descriptors like VGGFace, and compare with the existing benchmark face datasets. Results of the experiments clearly show that the difficulty level of the proposed dataset is much higher compared to the benchmark datasets.



### RCCNet: An Efficient Convolutional Neural Network for Histological Routine Colon Cancer Nuclei Classification
- **Arxiv ID**: http://arxiv.org/abs/1810.02797v3
- **DOI**: 10.1109/ICARCV.2018.8581147
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.02797v3)
- **Published**: 2018-09-30 07:18:58+00:00
- **Updated**: 2019-06-08 05:19:12+00:00
- **Authors**: S H Shabbeer Basha, Soumen Ghosh, Kancharagunta Kishan Babu, Shiv Ram Dubey, Viswanath Pulabaigari, Snehasis Mukherjee
- **Comment**: Published in ICARCV 2018
- **Journal**: None
- **Summary**: Efficient and precise classification of histological cell nuclei is of utmost importance due to its potential applications in the field of medical image analysis. It would facilitate the medical practitioners to better understand and explore various factors for cancer treatment. The classification of histological cell nuclei is a challenging task due to the cellular heterogeneity. This paper proposes an efficient Convolutional Neural Network (CNN) based architecture for classification of histological routine colon cancer nuclei named as RCCNet. The main objective of this network is to keep the CNN model as simple as possible. The proposed RCCNet model consists of only 1,512,868 learnable parameters which are significantly less compared to the popular CNN models such as AlexNet, CIFARVGG, GoogLeNet, and WRN. The experiments are conducted over publicly available routine colon cancer histological dataset "CRCHistoPhenotypes". The results of the proposed RCCNet model are compared with five state-of-the-art CNN models in terms of the accuracy, weighted average F1 score and training time. The proposed method has achieved a classification accuracy of 80.61% and 0.7887 weighted average F1 score. The proposed RCCNet is more efficient and generalized terms of the training time and data over-fitting, respectively.



### Pixel and Feature Level Based Domain Adaption for Object Detection in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1810.00345v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00345v2)
- **Published**: 2018-09-30 09:26:40+00:00
- **Updated**: 2019-08-11 08:19:13+00:00
- **Authors**: Yuhu Shan, Wen Feng Lu, Chee Meng Chew
- **Comment**: None
- **Journal**: None
- **Summary**: Annotating large scale datasets to train modern convolutional neural networks is prohibitively expensive and time-consuming for many real tasks. One alternative is to train the model on labeled synthetic datasets and apply it in the real scenes. However, this straightforward method often fails to generalize well mainly due to the domain bias between the synthetic and real datasets. Many unsupervised domain adaptation (UDA) methods are introduced to address this problem but most of them only focus on the simple classification task. In this paper, we present a novel UDA model to solve the more complex object detection problem in the context of autonomous driving. Our model integrates both pixel level and feature level based transformtions to fulfill the cross domain detection task and can be further trained end-to-end to pursue better performance. We employ objectives of the generative adversarial network and the cycle consistency loss for image translation in the pixel space. To address the potential semantic inconsistency problem, we propose region proposal based feature adversarial training to preserve the semantics of our target objects as well as further minimize the domain shifts. Extensive experiments are conducted on several different datasets, and the results demonstrate the robustness and superiority of our method.



### Improving Bag-of-Visual-Words Towards Effective Facial Expressive Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1810.00360v1
- **DOI**: 10.5220/0006537601450152
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00360v1)
- **Published**: 2018-09-30 11:28:24+00:00
- **Updated**: 2018-09-30 11:28:24+00:00
- **Authors**: Dawood Al Chanti, Alice Caplier
- **Comment**: 8 pages, 6 figures, Volume 5: VISAPPm year 2018,
  publisher=SciTePress, organization=INSTICC, isbn=978-989-758-290-5
- **Journal**: In Proceedings of the 13th International Joint Conference on
  Computer Vision, Imaging and Computer Graphics Theory and Applications -
  Volume 5: VISAPP, ISBN 978-989-758-290-5, pages 145-152, year=2018
- **Summary**: Bag-of-Visual-Words (BoVW) approach has been widely used in the recent years for image classification purposes. However, the limitations regarding optimal feature selection, clustering technique, the lack of spatial organization of the data and the weighting of visual words are crucial. These factors affect the stability of the model and reduce performance. We propose to develop an algorithm based on BoVW for facial expression analysis which goes beyond those limitations. Thus the visual codebook is built by using k-Means++ method to avoid poor clustering. To exploit reliable low level features, we search for the best feature detector that avoids locating a large number of keypoints which do not contribute to the classification process. Then, we propose to compute the relative conjunction matrix in order to preserve the spatial order of the data by coding the relationships among visual words. In addition, a weighting scheme that reflects how important a visual word is with respect to a given image is introduced. We speed up the learning process by using histogram intersection kernel by Support Vector Machine to learn a discriminative classifier. The efficiency of the proposed algorithm is compared with standard bag of visual words method and with bag of visual words method with spatial pyramid. Extensive experiments on the CK+, the MMI and the JAFFE databases show good average recognition rates. Likewise, the ability to recognize spontaneous and non-basic expressive states is investigated using the DynEmo database.



### Spontaneous Facial Expression Recognition using Sparse Representation
- **Arxiv ID**: http://arxiv.org/abs/1810.00362v1
- **DOI**: 10.5220/0006118000640074
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00362v1)
- **Published**: 2018-09-30 11:38:34+00:00
- **Updated**: 2018-09-30 11:38:34+00:00
- **Authors**: Dawood Al Chanti, Alice Caplier
- **Comment**: 11 pages, 9 figures, VISAPP 2017, publisher=SciTePress,
  organization=INSTICC, isbn=978-989-758-226-4, Proceedings of the 12th
  International Joint Conference on Computer Vision, Imaging and Computer
  Graphics Theory and Applications - Volume 5: VISAPP, (VISIGRAPP 2017)}, 2017
- **Journal**: None
- **Summary**: Facial expression is the most natural means for human beings to communicate their emotions. Most facial expression analysis studies consider the case of acted expressions. Spontaneous facial expression recognition is significantly more challenging since each person has a different way to react to a given emotion. We consider the problem of recognizing spontaneous facial expression by learning discriminative dictionaries for sparse representation. Facial images are represented as a sparse linear combination of prototype atoms via Orthogonal Matching Pursuit algorithm. Sparse codes are then used to train an SVM classifier dedicated to the recognition task. The dictionary that sparsifies the facial images (feature points with the same class labels should have similar sparse codes) is crucial for robust classification. Learning sparsifying dictionaries heavily relies on the initialization process of the dictionary. To improve the performance of dictionaries, a random face feature descriptor based on the Random Projection concept is developed. The effectiveness of the proposed method is evaluated through several experiments on the spontaneous facial expressions DynEmo database. It is also estimated on the well-known acted facial expressions JAFFE database for a purpose of comparison with state-of-the-art methods.



### Marrying Tracking with ELM: A Metric Constraint Guided Multiple Feature Fusion Method
- **Arxiv ID**: http://arxiv.org/abs/1810.01271v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.01271v2)
- **Published**: 2018-09-30 13:43:12+00:00
- **Updated**: 2018-10-06 13:16:22+00:00
- **Authors**: Jing Zhang, Yonggong Ren
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1807.10211
- **Journal**: None
- **Summary**: Object Tracking is one important problem in computer vision and surveillance system. The existing models mainly exploit the single-view feature (i.e. color, texture, shape) to solve the problem, failing to describe the objects comprehensively. In this paper, we solve the problem from multi-view perspective by leveraging multi-view complementary and latent information, so as to be robust to the partial occlusion and background clutter especially when the objects are similar to the target, meanwhile addressing tracking drift. However, one big problem is that multi-view fusion strategy can inevitably result tracking into non-efficiency. To this end, we propose to marry ELM (Extreme learning machine) to multi-view fusion to train the global hidden output weight, to effectively exploit the local information from each view. Following this principle, we propose a novel method to obtain the optimal sample as the target object, which avoids tracking drift resulting from noisy samples. Our method is evaluated over 12 challenge image sequences challenged with different attributes including illumination, occlusion, deformation, etc., which demonstrates better performance than several state-of-the-art methods in terms of effectiveness and robustness.



### Quantification of Trabeculae Inside the Heart from MRI Using Fractal Analysis
- **Arxiv ID**: http://arxiv.org/abs/1810.04637v2
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1810.04637v2)
- **Published**: 2018-09-30 15:05:48+00:00
- **Updated**: 2018-10-14 17:00:09+00:00
- **Authors**: Md. Kamrul Hasan, Fakrul Islam Tushar
- **Comment**: None
- **Journal**: None
- **Summary**: Left ventricular non-compaction (LVNC) is a rare cardiomyopathy (CMP) that should be considered as a possible diagnosis because of its potential complications which are heart failure, ventricular arrhythmias, and embolic events. For analysis cardiac functionality, extracting information from the Left ventricular (LV) is already a broad field of Medical Imaging. Different algorithms and strategies ranging that is semiautomated or automated has already been developed to get useful information from such a critical structure of heart. Trabeculae in the heart undergoes difference changes like solid from spongy. Due to failure of this process left ventricle non-compaction occurred. In this project, we will demonstrate the fractal dimension (FD) and manual segmentation of the Magnetic Resonance Imaging (MRI) of the heart that quantify amount of trabeculae inside the heart. The greater the value of fractal dimension inside the heart indicates the greater complex pattern of the trabeculae in the heart.



### Benchmarks of ResNet Architecture for Atrial Fibrillation Classification
- **Arxiv ID**: http://arxiv.org/abs/1810.00396v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.00396v1)
- **Published**: 2018-09-30 15:09:42+00:00
- **Updated**: 2018-09-30 15:09:42+00:00
- **Authors**: Roman Khudorozhkov, Dmitry Podvyaznikov
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we apply variations of ResNet architecture to the task of atrial fibrillation classification. Variations differ in number of filter after first convolution, ResNet block layout, number of filters in block convolutions and number of ResNet blocks between downsampling operations. We have found a range of model size in which models with quite different configurations show similar performance. It is likely that overall number of parameters plays dominant role in model performance. However, configuration parameters like layout have values that constantly lead to better results, which allows to suggest that these parameters should be defined and fixed in the first place, while others may be varied in a reasonable range to satisfy any existing constraints.



### Modelling local phase of images and textures with applications in phase denoising and phase retrieval
- **Arxiv ID**: http://arxiv.org/abs/1810.00403v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00403v1)
- **Published**: 2018-09-30 15:31:28+00:00
- **Updated**: 2018-09-30 15:31:28+00:00
- **Authors**: Ido Zachevsky, Yehoshua Y. Zeevi
- **Comment**: None
- **Journal**: None
- **Summary**: The Fourier magnitude has been studied extensively, but less effort has been devoted to the Fourier phase, despite its well-established importance in image representation. Global phase was shown to be more important for image representation than the magnitude, whereas local phase, exhibited in Gabor filters, has been used for analysis purposes in detecting image contours and edges. Neither global nor local phase has been modelled in closed form, suitable for Bayesian estimation.   In this work, we analyze the local phase of textured images and propose a local (Markovian) model for local phase coefficients. This model is Gaussian-mixture-based, learned from the graph representation of images, based on their complex wavelet decomposition. We demonstrate the applicability of the model in restoration of images with noisy local phase and in image retrieval, where we show superior performance to the well-known hybrid input-output (HIO) method. We also provide a framework for application of the model in a general setup of image processing.



### Optical Illusions Images Dataset
- **Arxiv ID**: http://arxiv.org/abs/1810.00415v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00415v2)
- **Published**: 2018-09-30 16:14:48+00:00
- **Updated**: 2018-10-16 15:21:43+00:00
- **Authors**: Robert Max Williams, Roman V. Yampolskiy
- **Comment**: None
- **Journal**: None
- **Summary**: Human vision is capable of performing many tasks not optimized for in its long evolution. Reading text and identifying artificial objects such as road signs are both tasks that mammalian brains never encountered in the wild but are very easy for us to perform. However, humans have discovered many very specific tricks that cause us to misjudge color, size, alignment and movement of what we are looking at. A better understanding of these phenomenon could reveal insights into how human perception achieves these feats. In this paper we present a dataset of 6725 illusion images gathered from two websites, and a smaller dataset of 500 hand-picked images. We will discuss the process of collecting this data, models trained on it, and the work that needs to be done to make it of value to computer vision researchers.



### CaTDet: Cascaded Tracked Detector for Efficient Object Detection from Video
- **Arxiv ID**: http://arxiv.org/abs/1810.00434v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1810.00434v2)
- **Published**: 2018-09-30 17:59:42+00:00
- **Updated**: 2019-02-19 18:57:18+00:00
- **Authors**: Huizi Mao, Taeyoung Kong, William J. Dally
- **Comment**: Accepted to SysML 2019
- **Journal**: None
- **Summary**: Detecting objects in a video is a compute-intensive task. In this paper we propose CaTDet, a system to speedup object detection by leveraging the temporal correlation in video. CaTDet consists of two DNN models that form a cascaded detector, and an additional tracker to predict regions of interests based on historic detections. We also propose a new metric, mean Delay(mD), which is designed for latency-critical video applications. Experiments on the KITTI dataset show that CaTDet reduces operation count by 5.1-8.7x with the same mean Average Precision(mAP) as the single-model Faster R-CNN detector and incurs additional delay of 0.3 frame. On CityPersons dataset, CaTDet achieves 13.0x reduction in operations with 0.8% mAP loss.



### AgriColMap: Aerial-Ground Collaborative 3D Mapping for Precision Farming
- **Arxiv ID**: http://arxiv.org/abs/1810.00457v2
- **DOI**: 10.1109/LRA.2019.2894468
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1810.00457v2)
- **Published**: 2018-09-30 20:10:52+00:00
- **Updated**: 2019-03-14 10:26:36+00:00
- **Authors**: Ciro Potena, Raghav Khanna, Juan Nieto, Roland Siegwart, Daniele Nardi, Alberto Pretto
- **Comment**: Published in IEEE Robotics and Automation Letters, 2019
- **Journal**: IEEE Robotics and Automation Letters, Vol: 4, Issue: 2, April
  2019, pages 1085-1092
- **Summary**: The combination of aerial survey capabilities of Unmanned Aerial Vehicles with targeted intervention abilities of agricultural Unmanned Ground Vehicles can significantly improve the effectiveness of robotic systems applied to precision agriculture. In this context, building and updating a common map of the field is an essential but challenging task. The maps built using robots of different types show differences in size, resolution and scale, the associated geolocation data may be inaccurate and biased, while the repetitiveness of both visual appearance and geometric structures found within agricultural contexts render classical map merging techniques ineffective. In this paper we propose AgriColMap, a novel map registration pipeline that leverages a grid-based multimodal environment representation which includes a vegetation index map and a Digital Surface Model. We cast the data association problem between maps built from UAVs and UGVs as a multimodal, large displacement dense optical flow estimation. The dominant, coherent flows, selected using a voting scheme, are used as point-to-point correspondences to infer a preliminary non-rigid alignment between the maps. A final refinement is then performed, by exploiting only meaningful parts of the registered maps. We evaluate our system using real world data for 3 fields with different crop species. The results show that our method outperforms several state of the art map registration and matching techniques by a large margin, and has a higher tolerance to large initial misalignments. We release an implementation of the proposed approach along with the acquired datasets with this paper.



### 3D-PSRNet: Part Segmented 3D Point Cloud Reconstruction From a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1810.00461v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.00461v1)
- **Published**: 2018-09-30 20:36:58+00:00
- **Updated**: 2018-09-30 20:36:58+00:00
- **Authors**: Priyanka Mandikal, Navaneet K L, R. Venkatesh Babu
- **Comment**: Accepted at ECCV Workshop 2018. Codes are available at
  https://github.com/val-iisc/3d-psrnet
- **Journal**: None
- **Summary**: We propose a mechanism to reconstruct part annotated 3D point clouds of objects given just a single input image. We demonstrate that jointly training for both reconstruction and segmentation leads to improved performance in both the tasks, when compared to training for each task individually. The key idea is to propagate information from each task so as to aid the other during the training procedure. Towards this end, we introduce a location-aware segmentation loss in the training regime. We empirically show the effectiveness of the proposed loss in generating more faithful part reconstructions while also improving segmentation accuracy. We thoroughly evaluate the proposed approach on different object categories from the ShapeNet dataset to obtain improved results in reconstruction as well as segmentation.



### Deep Learning for End-to-End Atrial Fibrillation Recurrence Estimation
- **Arxiv ID**: http://arxiv.org/abs/1810.00475v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.00475v1)
- **Published**: 2018-09-30 22:10:28+00:00
- **Updated**: 2018-09-30 22:10:28+00:00
- **Authors**: Riddhish Bhalodia, Anupama Goparaju, Tim Sodergren, Alan Morris, Evgueni Kholmovski, Nassir Marrouche, Joshua Cates, Ross Whitaker, Shireen Elhabian
- **Comment**: Presented at Computing in Cardiology (CinC) 2018
- **Journal**: None
- **Summary**: Left atrium shape has been shown to be an independent predictor of recurrence after atrial fibrillation (AF) ablation. Shape-based representation is imperative to such an estimation process, where correspondence-based representation offers the most flexibility and ease-of-computation for population-level shape statistics. Nonetheless, population-level shape representations in the form of image segmentation and correspondence models derived from cardiac MRI require significant human resources with sufficient anatomy-specific expertise. In this paper, we propose a machine learning approach that uses deep networks to estimate AF recurrence by predicting shape descriptors directly from MRI images, with NO image pre-processing involved. We also propose a novel data augmentation scheme to effectively train a deep network in a limited training data setting. We compare this new method of estimating shape descriptors from images with the state-of-the-art correspondence-based shape modeling that requires image segmentation and correspondence optimization. Results show that the proposed method and the current state-of-the-art produce statistically similar outcomes on AF recurrence, eliminating the need for expensive pre-processing pipelines and associated human labor.



### Few-Shot Goal Inference for Visuomotor Learning and Planning
- **Arxiv ID**: http://arxiv.org/abs/1810.00482v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1810.00482v1)
- **Published**: 2018-09-30 22:57:58+00:00
- **Updated**: 2018-09-30 22:57:58+00:00
- **Authors**: Annie Xie, Avi Singh, Sergey Levine, Chelsea Finn
- **Comment**: Videos available at https://sites.google.com/view/few-shot-goals
- **Journal**: None
- **Summary**: Reinforcement learning and planning methods require an objective or reward function that encodes the desired behavior. Yet, in practice, there is a wide range of scenarios where an objective is difficult to provide programmatically, such as tasks with visual observations involving unknown object positions or deformable objects. In these cases, prior methods use engineered problem-specific solutions, e.g., by instrumenting the environment with additional sensors to measure a proxy for the objective. Such solutions require a significant engineering effort on a per-task basis, and make it impractical for robots to continuously learn complex skills outside of laboratory settings. We aim to find a more general and scalable solution for specifying goals for robot learning in unconstrained environments. To that end, we formulate the few-shot objective learning problem, where the goal is to learn a task objective from only a few example images of successful end states for that task. We propose a simple solution to this problem: meta-learn a classifier that can recognize new goals from a few examples. We show how this approach can be used with both model-free reinforcement learning and visual model-based planning and show results in three domains: rope manipulation from images in simulation, visual navigation in a simulated 3D environment, and object arrangement into user-specified configurations on a real robot.



