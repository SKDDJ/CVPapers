# Arxiv Papers in cs.CV on 2018-09-25
### MedAL: Deep Active Learning Sampling Method for Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1809.09287v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09287v2)
- **Published**: 2018-09-25 02:30:24+00:00
- **Updated**: 2018-09-28 19:59:58+00:00
- **Authors**: Asim Smailagic, Hae Young Noh, Pedro Costa, Devesh Walawalkar, Kartik Khandelwal, Mostafa Mirshekari, Jonathon Fagert, Adrián Galdrán, Susu Xu
- **Comment**: Accepted as conference paper for ICMLA 2018
- **Journal**: None
- **Summary**: Deep learning models have been successfully used in medical image analysis problems but they require a large amount of labeled images to obtain good performance.Deep learning models have been successfully used in medical image analysis problems but they require a large amount of labeled images to obtain good performance. However, such large labeled datasets are costly to acquire. Active learning techniques can be used to minimize the number of required training labels while maximizing the model's performance.In this work, we propose a novel sampling method that queries the unlabeled examples that maximize the average distance to all training set examples in a learned feature space. We then extend our sampling method to define a better initial training set, without the need for a trained model, by using ORB feature descriptors. We validate MedAL on 3 medical image datasets and show that our method is robust to different dataset properties. MedAL is also efficient, achieving 80% accuracy on the task of Diabetic Retinopathy detection using only 425 labeled images, corresponding to a 32% reduction in the number of required labeled examples compared to the standard uncertainty sampling technique, and a 40% reduction compared to random sampling.



### Covfefe: A Computer Vision Approach For Estimating Force Exertion
- **Arxiv ID**: http://arxiv.org/abs/1809.09293v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.09293v1)
- **Published**: 2018-09-25 02:45:19+00:00
- **Updated**: 2018-09-25 02:45:19+00:00
- **Authors**: Vaneet Aggarwal, Hamed Asadi, Mayank Gupta, Jae Joong Lee, Denny Yu
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Cumulative exposure to repetitive and forceful activities may lead to musculoskeletal injuries which not only reduce workers' efficiency and productivity, but also affect their quality of life. Thus, widely accessible techniques for reliable detection of unsafe muscle force exertion levels for human activity is necessary for their well-being. However, measurement of force exertion levels is challenging and the existing techniques pose a great challenge as they are either intrusive, interfere with human-machine interface, and/or subjective in the nature, thus are not scalable for all workers. In this work, we use face videos and the photoplethysmography (PPG) signals to classify force exertion levels of 0\%, 50\%, and 100\% (representing rest, moderate effort, and high effort), thus providing a non-intrusive and scalable approach. Efficient feature extraction approaches have been investigated, including standard deviation of the movement of different landmarks of the face, distances between peaks and troughs in the PPG signals. We note that the PPG signals can be obtained from the face videos, thus giving an efficient classification algorithm for the force exertion levels using face videos. Based on the data collected from 20 subjects, features extracted from the face videos give 90\% accuracy in classification among the 100\% and the combination of 0\% and 50\% datasets. Further combining the PPG signals provide 81.7\% accuracy. The approach is also shown to be robust to the correctly identify force level when the person is talking, even though such datasets are not included in the training.



### Object Detection from Scratch with Deep Supervision
- **Arxiv ID**: http://arxiv.org/abs/1809.09294v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09294v2)
- **Published**: 2018-09-25 02:56:44+00:00
- **Updated**: 2019-03-19 01:34:46+00:00
- **Authors**: Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang, Yurong Chen, Xiangyang Xue
- **Comment**: More results and analysis in this version. This is an extension of
  our previous conference paper: arXiv:1708.01241
- **Journal**: None
- **Summary**: We propose Deeply Supervised Object Detectors (DSOD), an object detection framework that can be trained from scratch. Recent advances in object detection heavily depend on the off-the-shelf models pre-trained on large-scale classification datasets like ImageNet and OpenImage. However, one problem is that adopting pre-trained models from classification to detection task may incur learning bias due to the different objective function and diverse distributions of object categories. Techniques like fine-tuning on detection task could alleviate this issue to some extent but are still not fundamental. Furthermore, transferring these pre-trained models across discrepant domains will be more difficult (e.g., from RGB to depth images). Thus, a better solution to handle these critical problems is to train object detectors from scratch, which motivates our proposed method. Previous efforts on this direction mainly failed by reasons of the limited training data and naive backbone network structures for object detection. In DSOD, we contribute a set of design principles for learning object detectors from scratch. One of the key principles is the deep supervision, enabled by layer-wise dense connections in both backbone networks and prediction layers, plays a critical role in learning good detectors from scratch. After involving several other principles, we build our DSOD based on the single-shot detection framework (SSD). We evaluate our method on PASCAL VOC 2007, 2012 and COCO datasets. DSOD achieves consistently better results than the state-of-the-art methods with much more compact models. Specifically, DSOD outperforms baseline method SSD on all three benchmarks, while requiring only 1/2 parameters. We also observe that DSOD can achieve comparable/slightly better results than Mask RCNN + FPN (under similar input size) with only 1/3 parameters, using no extra data or pre-trained models.



### Satellite Imagery Multiscale Rapid Detection with Windowed Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.09978v1
- **DOI**: 10.1109/WACV.2019.00083
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09978v1)
- **Published**: 2018-09-25 03:00:05+00:00
- **Updated**: 2018-09-25 03:00:05+00:00
- **Authors**: Adam Van Etten
- **Comment**: 8 pages, 7 figures, 2 tables, 1 appendix. arXiv admin note:
  substantial text overlap with arXiv:1805.09512
- **Journal**: None
- **Summary**: Detecting small objects over large areas remains a significant challenge in satellite imagery analytics. Among the challenges is the sheer number of pixels and geographical extent per image: a single DigitalGlobe satellite image encompasses over 64 km2 and over 250 million pixels. Another challenge is that objects of interest are often minuscule (~pixels in extent even for the highest resolution imagery), which complicates traditional computer vision techniques. To address these issues, we propose a pipeline (SIMRDWN) that evaluates satellite images of arbitrarily large size at native resolution at a rate of > 0.2 km2/s. Building upon the tensorflow object detection API paper, this pipeline offers a unified approach to multiple object detection frameworks that can run inference on images of arbitrary size. The SIMRDWN pipeline includes a modified version of YOLO (known as YOLT), along with the models of the tensorflow object detection API: SSD, Faster R-CNN, and R-FCN. The proposed approach allows comparison of the performance of these four frameworks, and can rapidly detect objects of vastly different scales with relatively little training data over multiple sensors. For objects of very different scales (e.g. airplanes versus airports) we find that using two different detectors at different scales is very effective with negligible runtime cost.We evaluate large test images at native resolution and find mAP scores of 0.2 to 0.8 for vehicle localization, with the YOLT architecture achieving both the highest mAP and fastest inference speed.



### Gradient-Based Low-Light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1809.09297v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09297v1)
- **Published**: 2018-09-25 03:05:43+00:00
- **Updated**: 2018-09-25 03:05:43+00:00
- **Authors**: Masayuki Tanaka, Takashi Shibata, Masatoshi Okutomi
- **Comment**: None
- **Journal**: None
- **Summary**: A low-light image enhancement is a highly demanded image processing technique, especially for consumer digital cameras and cameras on mobile phones. In this paper, a gradient-based low-light image enhancement algorithm is proposed. The key is to enhance the gradients of dark region, because the gradients are more sensitive for human visual system than absolute values. In addition, we involve the intensity-range constraints for the image integration. By using the intensity-range constraints, we can integrate the output image with enhanced gradients preserving the given gradient information while enforcing the intensity range of the output image within a certain intensity range. Experiments demonstrate that the proposed gradient-based low-light image enhancement can effectively enhance the low-light images.



### Triply Supervised Decoder Networks for Joint Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1809.09299v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09299v1)
- **Published**: 2018-09-25 03:11:55+00:00
- **Updated**: 2018-09-25 03:11:55+00:00
- **Authors**: Jiale Cao, Yanwei Pang, Xuelong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Joint object detection and semantic segmentation can be applied to many fields, such as self-driving cars and unmanned surface vessels. An initial and important progress towards this goal has been achieved by simply sharing the deep convolutional features for the two tasks. However, this simple scheme is unable to make full use of the fact that detection and segmentation are mutually beneficial. To overcome this drawback, we propose a framework called TripleNet where triple supervisions including detection-oriented supervision, class-aware segmentation supervision, and class-agnostic segmentation supervision are imposed on each layer of the decoder network. Class-agnostic segmentation supervision provides an objectness prior knowledge for both semantic segmentation and object detection. Besides the three types of supervisions, two light-weight modules (i.e., inner-connected module and attention skip-layer fusion) are also incorporated into each layer of the decoder. In the proposed framework, detection and segmentation can sufficiently boost each other. Moreover, class-agnostic and class-aware segmentation on each decoder layer are not performed at the test stage. Therefore, no extra computational costs are introduced at the test stage. Experimental results on the VOC2007 and VOC2012 datasets demonstrate that the proposed TripleNet is able to improve both the detection and segmentation accuracies without adding extra computational costs.



### Scenic: A Language for Scenario Specification and Scene Generation
- **Arxiv ID**: http://arxiv.org/abs/1809.09310v2
- **DOI**: 10.1145/3314221.3314633
- **Categories**: **cs.PL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.09310v2)
- **Published**: 2018-09-25 03:57:00+00:00
- **Updated**: 2019-06-21 01:12:24+00:00
- **Authors**: Daniel J. Fremont, Tommaso Dreossi, Shromona Ghosh, Xiangyu Yue, Alberto L. Sangiovanni-Vincentelli, Sanjit A. Seshia
- **Comment**: 41 pages, 36 figures. Full version of a PLDI 2019 paper (extending UC
  Berkeley EECS Department Tech Report No. UCB/EECS-2018-8)
- **Journal**: None
- **Summary**: We propose a new probabilistic programming language for the design and analysis of perception systems, especially those based on machine learning. Specifically, we consider the problems of training a perception system to handle rare events, testing its performance under different conditions, and debugging failures. We show how a probabilistic programming language can help address these problems by specifying distributions encoding interesting types of inputs and sampling these to generate specialized training and test sets. More generally, such languages can be used for cyber-physical systems and robotics to write environment models, an essential prerequisite to any formal analysis. In this paper, we focus on systems like autonomous cars and robots, whose environment is a "scene", a configuration of physical objects and agents. We design a domain-specific language, Scenic, for describing "scenarios" that are distributions over scenes. As a probabilistic programming language, Scenic allows assigning distributions to features of the scene, as well as declaratively imposing hard and soft constraints over the scene. We develop specialized techniques for sampling from the resulting distribution, taking advantage of the structure provided by Scenic's domain-specific syntax. Finally, we apply Scenic in a case study on a convolutional neural network designed to detect cars in road images, improving its performance beyond that achieved by state-of-the-art synthetic data generation methods.



### Multigrid Backprojection Super-Resolution and Deep Filter Visualization
- **Arxiv ID**: http://arxiv.org/abs/1809.09326v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1809.09326v3)
- **Published**: 2018-09-25 05:36:51+00:00
- **Updated**: 2019-01-29 02:04:15+00:00
- **Authors**: Pablo Navarrete Michelini, Hanwen Liu, Dan Zhu
- **Comment**: Spotlight paper in the Thirty-Third AAAI Conference on Artificial
  Intelligence (AAAI-19)
- **Journal**: None
- **Summary**: We introduce a novel deep-learning architecture for image upscaling by large factors (e.g. 4x, 8x) based on examples of pristine high-resolution images. Our target is to reconstruct high-resolution images from their downscale versions. The proposed system performs a multi-level progressive upscaling, starting from small factors (2x) and updating for higher factors (4x and 8x). The system is recursive as it repeats the same procedure at each level. It is also residual since we use the network to update the outputs of a classic upscaler. The network residuals are improved by Iterative Back-Projections (IBP) computed in the features of a convolutional network. To work in multiple levels we extend the standard back-projection algorithm using a recursion analogous to Multi-Grid algorithms commonly used as solvers of large systems of linear equations. We finally show how the network can be interpreted as a standard upsampling-and-filter upscaler with a space-variant filter that adapts to the geometry. This approach allows us to visualize how the network learns to upscale. Finally, our system reaches state of the art quality for models with relatively few number of parameters.



### Geometric-based Line Segment Tracking for HDR Stereo Sequences
- **Arxiv ID**: http://arxiv.org/abs/1809.09368v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09368v1)
- **Published**: 2018-09-25 09:10:30+00:00
- **Updated**: 2018-09-25 09:10:30+00:00
- **Authors**: Ruben Gomez-Ojeda, Javier Gonzalez-Jimenez
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a purely geometrical approach for the robust matching of line segments for challenging stereo streams with severe illumination changes or High Dynamic Range (HDR) environments. To that purpose, we exploit the univocal nature of the matching problem, i.e. every observation must be corresponded with a single feature or not corresponded at all. We state the problem as a sparse, convex, L1-minimization of the matching vector regularized by the geometric constraints. This formulation allows for the robust tracking of line segments along sequences where traditional appearance-based matching techniques tend to fail due to dynamic changes in illumination conditions. Moreover, the proposed matching algorithm also results in a considerable speed-up of previous state of the art techniques making it suitable for real-time applications such as Visual Odometry (VO). This, of course, comes at expense of a slightly lower number of matches in comparison with appearance based methods, and also limits its application to continuous video sequences, as it is rather constrained to small pose increments between consecutive frames. We validate the claimed advantages by first evaluating the matching performance in challenging video sequences, and then testing the method in a benchmarked point and line based VO algorithm.



### Pre and Post-hoc Diagnosis and Interpretation of Malignancy from Breast DCE-MRI
- **Arxiv ID**: http://arxiv.org/abs/1809.09404v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09404v2)
- **Published**: 2018-09-25 10:48:10+00:00
- **Updated**: 2019-02-04 04:59:37+00:00
- **Authors**: Gabriel Maicas, Andrew P. Bradley, Jacinto C. Nascimento, Ian Reid, Gustavo Carneiro
- **Comment**: Submitted to Medical Image Analysis
- **Journal**: None
- **Summary**: We propose a new method for breast cancer screening from DCE-MRI based on a post-hoc approach that is trained using weakly annotated data (i.e., labels are available only at the image level without any lesion delineation). Our proposed post-hoc method automatically diagnosis the whole volume and, for positive cases, it localizes the malignant lesions that led to such diagnosis. Conversely, traditional approaches follow a pre-hoc approach that initially localises suspicious areas that are subsequently classified to establish the breast malignancy -- this approach is trained using strongly annotated data (i.e., it needs a delineation and classification of all lesions in an image). Another goal of this paper is to establish the advantages and disadvantages of both approaches when applied to breast screening from DCE-MRI. Relying on experiments on a breast DCE-MRI dataset that contains scans of 117 patients, our results show that the post-hoc method is more accurate for diagnosing the whole volume per patient, achieving an AUC of 0.91, while the pre-hoc method achieves an AUC of 0.81. However, the performance for localising the malignant lesions remains challenging for the post-hoc method due to the weakly labelled dataset employed during training.



### Vehicle Re-Identification in Context
- **Arxiv ID**: http://arxiv.org/abs/1809.09409v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09409v2)
- **Published**: 2018-09-25 11:21:13+00:00
- **Updated**: 2018-09-26 09:49:16+00:00
- **Authors**: Aytaç Kanacı, Xiatian Zhu, Shaogang Gong
- **Comment**: Dataset available at: http://qmul-vric.github.io. To appear on German
  Conference on Pattern Recognition (GCPR) 2018
- **Journal**: None
- **Summary**: Existing vehicle re-identification (re-id) evaluation benchmarks consider strongly artificial test scenarios by assuming the availability of high quality images and fine-grained appearance at an almost constant image scale, reminiscent to images required for Automatic Number Plate Recognition, e.g. VeRi-776. Such assumptions are often invalid in realistic vehicle re-id scenarios where arbitrarily changing image resolutions (scales) are the norm. This makes the existing vehicle re-id benchmarks limited for testing the true performance of a re-id method. In this work, we introduce a more realistic and challenging vehicle re-id benchmark, called Vehicle Re-Identification in Context (VRIC). In contrast to existing datasets, VRIC is uniquely characterised by vehicle images subject to more realistic and unconstrained variations in resolution (scale), motion blur, illumination, occlusion, and viewpoint. It contains 60,430 images of 5,622 vehicle identities captured by 60 different cameras at heterogeneous road traffic scenes in both day-time and night-time.



### Automatic brain tumor grading from MRI data using convolutional neural networks and quality assessment
- **Arxiv ID**: http://arxiv.org/abs/1809.09468v1
- **DOI**: 10.1007/978-3-030-02628-8
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1809.09468v1)
- **Published**: 2018-09-25 13:32:41+00:00
- **Updated**: 2018-09-25 13:32:41+00:00
- **Authors**: Sergio Pereira, Raphael Meier, Victor Alves, Mauricio Reyes, Carlos A. Silva
- **Comment**: Accepted and presented at iMIMIC - Workshop on Interpretability of
  Machine Intelligence in Medical Image Computing
- **Journal**: None
- **Summary**: Glioblastoma Multiforme is a high grade, very aggressive, brain tumor, with patients having a poor prognosis. Lower grade gliomas are less aggressive, but they can evolve into higher grade tumors over time. Patient management and treatment can vary considerably with tumor grade, ranging from tumor resection followed by a combined radio- and chemotherapy to a "wait and see" approach. Hence, tumor grading is important for adequate treatment planning and monitoring. The gold standard for tumor grading relies on histopathological diagnosis of biopsy specimens. However, this procedure is invasive, time consuming, and prone to sampling error. Given these disadvantages, automatic tumor grading from widely used MRI protocols would be clinically important, as a way to expedite treatment planning and assessment of tumor evolution. In this paper, we propose to use Convolutional Neural Networks for predicting tumor grade directly from imaging data. In this way, we overcome the need for expert annotations of regions of interest. We evaluate two prediction approaches: from the whole brain, and from an automatically defined tumor region. Finally, we employ interpretability methodologies as a quality assurance stage to check if the method is using image regions indicative of tumor grade for classification.



### Taking A Closer Look at Domain Shift: Category-level Adversaries for Semantics Consistent Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1809.09478v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1809.09478v3)
- **Published**: 2018-09-25 13:43:25+00:00
- **Updated**: 2019-04-01 14:25:06+00:00
- **Authors**: Yawei Luo, Liang Zheng, Tao Guan, Junqing Yu, Yi Yang
- **Comment**: CVPR2019 Oral
- **Journal**: None
- **Summary**: We consider the problem of unsupervised domain adaptation in semantic segmentation. The key in this campaign consists in reducing the domain shift, i.e., enforcing the data distributions of the two domains to be similar. A popular strategy is to align the marginal distribution in the feature space through adversarial learning. However, this global alignment strategy does not consider the local category-level feature distribution. A possible consequence of the global movement is that some categories which are originally well aligned between the source and target may be incorrectly mapped. To address this problem, this paper introduces a category-level adversarial network, aiming to enforce local semantic consistency during the trend of global alignment. Our idea is to take a close look at the category-level data distribution and align each class with an adaptive adversarial loss. Specifically, we reduce the weight of the adversarial loss for category-level aligned features while increasing the adversarial force for those poorly aligned. In this process, we decide how well a feature is category-level aligned between source and target by a co-training approach. In two domain adaptation tasks, i.e., GTA5 -> Cityscapes and SYNTHIA -> Cityscapes, we validate that the proposed method matches the state of the art in segmentation accuracy.



### Human-Machine Interface for Remote Training of Robot Tasks
- **Arxiv ID**: http://arxiv.org/abs/1809.09558v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.09558v1)
- **Published**: 2018-09-25 15:44:35+00:00
- **Updated**: 2018-09-25 15:44:35+00:00
- **Authors**: Jordi Spranger, Roxana Buzatoiu, Athanasios Polydoros, Lazaros Nalpantidis, Evangelos Boukas
- **Comment**: Accepted in IEEE International Conference on Imaging Systems and
  Techniques - IST2018
- **Journal**: None
- **Summary**: Regardless of their industrial or research application, the streamlining of robot operations is limited by the proximity of experienced users to the actual hardware. Be it massive open online robotics courses, crowd-sourcing of robot task training, or remote research on massive robot farms for machine learning, the need to create an apt remote Human-Machine Interface is quite prevalent. The paper at hand proposes a novel solution to the programming/training of remote robots employing an intuitive and accurate user-interface which offers all the benefits of working with real robots without imposing delays and inefficiency. The system includes: a vision-based 3D hand detection and gesture recognition subsystem, a simulated digital twin of a robot as visual feedback, and the "remote" robot learning/executing trajectories using dynamic motion primitives. Our results indicate that the system is a promising solution to the problem of remote training of robot tasks.



### MPRAD: A Multiparametric Radiomics Framework
- **Arxiv ID**: http://arxiv.org/abs/1809.09973v1
- **DOI**: 10.1007/s10549-020-05533-5
- **Categories**: **cs.CV**, physics.bio-ph, physics.med-ph, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1809.09973v1)
- **Published**: 2018-09-25 16:47:05+00:00
- **Updated**: 2018-09-25 16:47:05+00:00
- **Authors**: Vishwa S. Parekh, Michael A. Jacobs
- **Comment**: 32 pages, 7 figures
- **Journal**: Breast Cancer Res Treat (2020)
- **Summary**: Multiparametric radiological imaging is vital for detection, characterization and diagnosis of many different diseases. The use of radiomics for quantitative extraction of textural features from radiological imaging is increasing moving towards clinical decision support. However, current methods in radiomics are limited to using single images for the extraction of these textural features and may limit the applicable scope of radiomics in different clinical settings. Thus, in the current form, they are not capable of capturing the true underlying tissue characteristics in high dimensional multiparametric imaging space. To overcome this challenge, we have developed a multiparametric imaging radiomic framework termed MPRAD for extraction of radiomic features from high dimensional datasets. MPRAD was tested on two different organs and diseases; breast cancer and cerebrovascular accidents in brain, commonly referred to as stroke. The MPRAD framework classified malignant from benign breast lesions with excellent sensitivity and specificity of 87% and 80.5% respectively with an AUC of 0.88 providing a 9%-28% increase in AUC over single radiomic parameters. More importantly, in breast, the glandular tissue MPRAD were similar between each group with no significance differences. Similarly, the MPRAD features in brain stroke demonstrated increased performance in distinguishing the perfusion-diffusion mismatch compared to single parameter radiomics and there were no differences within the white and gray matter tissue. In conclusion, we have introduced the use of multiparametric radiomics into a clinical setting



### Structural and object detection for phosphene images
- **Arxiv ID**: http://arxiv.org/abs/1809.09607v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09607v2)
- **Published**: 2018-09-25 17:38:16+00:00
- **Updated**: 2018-09-26 17:09:14+00:00
- **Authors**: Melani Sanchez-Garcia, Ruben Martinez-Cantin, Jose J. Guerrero
- **Comment**: None
- **Journal**: None
- **Summary**: Prosthetic vision based on phosphenes is a promising way to provide visual perception to some blind people. However, phosphenic images are very limited in terms of spatial resolution (e.g.: 32 x 32 phosphene array) and luminance levels (e.g.: 8 gray levels), which results in the subject receiving very limited information about the scene. This requires using high-level processing to extract more information from the scene and present it to the subject with the phosphenes limitations. In this work, we study the recognition of indoor environments under simulated prosthetic vision. Most research in simulated prosthetic vision is performed based on static images, while very few researchers have addressed the problem of scene recognition through video sequences. We propose a new approach to build a schematic representation of indoor environments for phosphene images. Our schematic representation relies on two parallel CNNs for the extraction of structural informative edges of the room and the relevant object silhouettes based on mask segmentation. We have performed a study with twelve normally sighted subjects to evaluate how our methods were able to the room recognition by presenting phosphenic images and videos. We show how our method is able to increase the recognition ability of the user from 75% using alternative methods to 90% using our approach.



### Deep Neural Networks for Pattern Recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.09645v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.09645v1)
- **Published**: 2018-09-25 18:23:49+00:00
- **Updated**: 2018-09-25 18:23:49+00:00
- **Authors**: Kyongsik Yun, Alexander Huyen, Thomas Lu
- **Comment**: None
- **Journal**: None
- **Summary**: In the field of pattern recognition research, the method of using deep neural networks based on improved computing hardware recently attracted attention because of their superior accuracy compared to conventional methods. Deep neural networks simulate the human visual system and achieve human equivalent accuracy in image classification, object detection, and segmentation. This chapter introduces the basic structure of deep neural networks that simulate human neural networks. Then we identify the operational processes and applications of conditional generative adversarial networks, which are being actively researched based on the bottom-up and top-down mechanisms, the most important functions of the human visual perception process. Finally, recent developments in training strategies for effective learning of complex deep neural networks are addressed.



### Surface Type Estimation from GPS Tracked Bicycle Activities
- **Arxiv ID**: http://arxiv.org/abs/1809.09745v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1809.09745v1)
- **Published**: 2018-09-25 22:24:12+00:00
- **Updated**: 2018-09-25 22:24:12+00:00
- **Authors**: Nitish Nag, Vaibhav Pandey, Aishwarya Manjunath, Avinash Vaka, Ramesh Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Road conditions affect both machine and human powered modes of transportation. In the case of human powered transportation, poor road conditions increase the work for the individual to travel. Previous estimates for these parameters have used computationally expensive analysis of satellite images. In this work, we use a computationally inexpensive and simple method by using only GPS data from a human powered cyclist. By estimating if the road taken by the user has high or low variations in their directional vector, we classify if the user is on a paved road or on an unpaved trail. In order to do this, three methods were adopted, changes in frequency of the direction of slope in a given path segment, fitting segments of the path, and finding the first derivative and the number of points of zero crossings of each segment. Machine learning models such as support vector machines, K-nearest neighbors, and decision trees were used for the classification of the path. We show in our methods, the decision trees performed the best with an accuracy of 86\%. Estimation of the type of surface can be used for many applications such as understanding rolling resistance for power estimation estimation or building exercise recommendation systems by user profiling as described in detail in the paper.



### Confidence Inference for Focused Learning in Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/1809.09758v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09758v1)
- **Published**: 2018-09-25 23:34:39+00:00
- **Updated**: 2018-09-25 23:34:39+00:00
- **Authors**: Ruichao Xiao, Wenxiu Sun, Chengxi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present confidence inference approachin an unsupervised way in stereo matching. Deep Neu-ral Networks (DNNs) have recently been achieving state-of-the-art performance. However, it is often hard to tellwhether the trained model was making sensible predictionsor just guessing at random. To address this problem, westart from a probabilistic interpretation of theL1loss usedin stereo matching, which inherently assumes an indepen-dent and identical (aka i.i.d.) Laplacian distribution. Weshow that with the newly introduced dense confidence map,the identical assumption is relaxed. Intuitively, the vari-ance in the Laplacian distribution is large for low confidentpixels while small for high-confidence pixels. In practice,the network learns toattenuatelow-confidence pixels (e.g.,noisy input, occlusions, featureless regions) andfocusonhigh-confidence pixels. Moreover, it can be observed fromexperiments that the focused learning is very helpful in find-ing a better convergence state of the trained model, reduc-ing over-fitting on a given dataset.



