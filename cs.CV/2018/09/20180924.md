# Arxiv Papers in cs.CV on 2018-09-24
### Give me a hint! Navigating Image Databases using Human-in-the-loop Feedback
- **Arxiv ID**: http://arxiv.org/abs/1809.08714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08714v1)
- **Published**: 2018-09-24 01:16:55+00:00
- **Updated**: 2018-09-24 01:16:55+00:00
- **Authors**: Bryan A. Plummer, M. Hadi Kiapour, Shuai Zheng, Robinson Piramuthu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce an attribute-based interactive image search which can leverage human-in-the-loop feedback to iteratively refine image search results. We study active image search where human feedback is solicited exclusively in visual form, without using relative attribute annotations used by prior work which are not typically found in many datasets. In order to optimize the image selection strategy, a deep reinforcement model is trained to learn what images are informative rather than rely on hand-crafted measures typically leveraged in prior work. Additionally, we extend the recently introduced Conditional Similarity Network to incorporate global similarity in training visual embeddings, which results in more natural transitions as the user explores the learned similarity embeddings. Our experiments demonstrate the effectiveness of our approach, producing compelling results on both active image search and image attribute representation tasks.



### Modern Convex Optimization to Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1809.08734v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08734v1)
- **Published**: 2018-09-24 03:03:38+00:00
- **Updated**: 2018-09-24 03:03:38+00:00
- **Authors**: Jing Yuan, Aaron Fenster
- **Comment**: Book chapter, in submission
- **Journal**: None
- **Summary**: Recently, diagnosis, therapy and monitoring of human diseases involve a variety of imaging modalities, such as magnetic resonance imaging(MRI), computed tomography(CT), Ultrasound(US) and Positron-emission tomography(PET) as well as a variety of modern optical techniques. Over the past two decade, it has been recognized that advanced image processing techniques provide valuable information to physicians for diagnosis, image guided therapy and surgery, and monitoring of the treated organ to the therapy. Many researchers and companies have invested significant efforts in the developments of advanced medical image analysis methods; especially in the two core studies of medical image segmentation and registration, segmentations of organs and lesions are used to quantify volumes and shapes used in diagnosis and monitoring treatment; registration of multimodality images of organs improves detection, diagnosis and staging of diseases as well as image-guided surgery and therapy, registration of images obtained from the same modality are used to monitor progression of therapy. These challenging clinical-motivated applications introduce novel and sophisticated mathematical problems which stimulate developments of advanced optimization and computing methods, especially convex optimization attaining optimum in a global sense, hence, bring an enormous spread of research topics for recent computational medical image analysis. Particularly, distinct from the usual image processing, most medical images have a big volume of acquired data, often in 3D or 4D (3D + t) along with great noises or incomplete image information, and form the challenging large-scale optimization problems; how to process such poor 'big data' of medical images efficiently and solve the corresponding optimization problems robustly are the key factors of modern medical image analysis.



### Learning to Detect Fake Face Images in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1809.08754v3
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.08754v3)
- **Published**: 2018-09-24 04:45:24+00:00
- **Updated**: 2018-10-18 14:55:42+00:00
- **Authors**: Chih-Chung Hsu, Chia-Yen Lee, Yi-Xiu Zhuang
- **Comment**: 4 pages to appear in IEEE IS3C Conference (IEEE International
  Symposium on Computer, Consumer and Control Conference), Dec. 2018
- **Journal**: None
- **Summary**: Although Generative Adversarial Network (GAN) can be used to generate the realistic image, improper use of these technologies brings hidden concerns. For example, GAN can be used to generate a tampered video for specific people and inappropriate events, creating images that are detrimental to a particular person, and may even affect that personal safety. In this paper, we will develop a deep forgery discriminator (DeepFD) to efficiently and effectively detect the computer-generated images. Directly learning a binary classifier is relatively tricky since it is hard to find the common discriminative features for judging the fake images generated from different GANs. To address this shortcoming, we adopt contrastive loss in seeking the typical features of the synthesized images generated by different GANs and follow by concatenating a classifier to detect such computer-generated images. Experimental results demonstrate that the proposed DeepFD successfully detected 94.7% fake images generated by several state-of-the-art GANs.



### Low Frequency Adversarial Perturbation
- **Arxiv ID**: http://arxiv.org/abs/1809.08758v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08758v2)
- **Published**: 2018-09-24 04:54:36+00:00
- **Updated**: 2019-07-22 21:45:56+00:00
- **Authors**: Chuan Guo, Jared S. Frank, Kilian Q. Weinberger
- **Comment**: 9 pages, 9 figures. Accepted to UAI 2019
- **Journal**: None
- **Summary**: Adversarial images aim to change a target model's decision by minimally perturbing a target image. In the black-box setting, the absence of gradient information often renders this search problem costly in terms of query complexity. In this paper we propose to restrict the search for adversarial images to a low frequency domain. This approach is readily compatible with many existing black-box attack frameworks and consistently reduces their query cost by 2 to 4 times. Further, we can circumvent image transformation defenses even when both the model and the defense strategy are unknown. Finally, we demonstrate the efficacy of this technique by fooling the Google Cloud Vision platform with an unprecedented low number of model queries.



### Speaker Naming in Movies
- **Arxiv ID**: http://arxiv.org/abs/1809.08761v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.08761v1)
- **Published**: 2018-09-24 05:00:05+00:00
- **Updated**: 2018-09-24 05:00:05+00:00
- **Authors**: Mahmoud Azab, Mingzhe Wang, Max Smith, Noriyuki Kojima, Jia Deng, Rada Mihalcea
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new model for speaker naming in movies that leverages visual, textual, and acoustic modalities in an unified optimization framework. To evaluate the performance of our model, we introduce a new dataset consisting of six episodes of the Big Bang Theory TV show and eighteen full movies covering different genres. Our experiments show that our multimodal model significantly outperforms several competitive baselines on the average weighted F-score metric. To demonstrate the effectiveness of our framework, we design an end-to-end memory network model that leverages our speaker naming model and achieves state-of-the-art results on the subtitles task of the MovieQA 2017 Challenge.



### FCHD: Fast and accurate head detection in crowded scenes
- **Arxiv ID**: http://arxiv.org/abs/1809.08766v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08766v3)
- **Published**: 2018-09-24 05:31:16+00:00
- **Updated**: 2019-05-05 05:18:29+00:00
- **Authors**: Aditya Vora, Vinay Chilaka
- **Comment**: 5 pages, 4 figures, accepted for publication at International
  Conference on Image Processing, 2019
- **Journal**: None
- **Summary**: In this paper, we propose FCHD-Fully Convolutional Head Detector, an end-to-end trainable head detection model. Our proposed architecture is a single fully convolutional network which is responsible for both bounding box prediction and classification. This makes our model lightweight with low inference time and memory requirements. Along with run-time, our model has better overall average precision (AP) which is achieved by selection of anchor sizes based on the effective receptive field of the network. This can be concluded from our experiments on several head detection datasets with varying head counts. We achieve an AP of 0.70 on a challenging head detection dataset which is comparable to some standard benchmarks. Along with this our model runs at 5 FPS on Nvidia Quadro M1000M for VGA resolution images. Code is available at https://github.com/aditya-vora/FCHD-Fully-Convolutional-Head-Detector.



### Person Retrieval in Surveillance Video using Height, Color and Gender
- **Arxiv ID**: http://arxiv.org/abs/1810.05080v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05080v1)
- **Published**: 2018-09-24 07:21:23+00:00
- **Updated**: 2018-09-24 07:21:23+00:00
- **Authors**: Hiren Galiyawala, Kenil Shah, Vandit Gajjar, Mehul S. Raval
- **Comment**: 6 Pages, 6 Figures, Accepted to Semantic Person Retrieval in
  Surveillance Using Soft Biometrics challenge in Conjunction with AVSS-2018
- **Journal**: None
- **Summary**: A person is commonly described by attributes like height, build, cloth color, cloth type, and gender. Such attributes are known as soft biometrics. They bridge the semantic gap between human description and person retrieval in surveillance video. The paper proposes a deep learning-based linear filtering approach for person retrieval using height, cloth color, and gender. The proposed approach uses Mask R-CNN for pixel-wise person segmentation. It removes background clutter and provides precise boundary around the person. Color and gender models are fine-tuned using AlexNet and the algorithm is tested on SoftBioSearch dataset. It achieves good accuracy for person retrieval using the semantic query in challenging conditions.



### Chargrid: Towards Understanding 2D Documents
- **Arxiv ID**: http://arxiv.org/abs/1809.08799v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1809.08799v1)
- **Published**: 2018-09-24 08:37:02+00:00
- **Updated**: 2018-09-24 08:37:02+00:00
- **Authors**: Anoop Raveendra Katti, Christian Reisswig, Cordula Guder, Sebastian Brarda, Steffen Bickel, Johannes Höhne, Jean Baptiste Faddoul
- **Comment**: To be published at EMNLP 2018
- **Journal**: None
- **Summary**: We introduce a novel type of text representation that preserves the 2D layout of a document. This is achieved by encoding each document page as a two-dimensional grid of characters. Based on this representation, we present a generic document understanding pipeline for structured documents. This pipeline makes use of a fully convolutional encoder-decoder network that predicts a segmentation mask and bounding boxes. We demonstrate its capabilities on an information extraction task from invoices and show that it significantly outperforms approaches based on sequential text or document images.



### Beyond Binomial and Negative Binomial: Adaptation in Bernoulli Parameter Estimation
- **Arxiv ID**: http://arxiv.org/abs/1809.08801v2
- **DOI**: None
- **Categories**: **stat.AP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.08801v2)
- **Published**: 2018-09-24 08:38:34+00:00
- **Updated**: 2019-04-21 19:08:11+00:00
- **Authors**: Safa C. Medin, John Murray-Bruce, David Castañón, Vivek K Goyal
- **Comment**: 13 pages, 16 figures
- **Journal**: None
- **Summary**: Estimating the parameter of a Bernoulli process arises in many applications, including photon-efficient active imaging where each illumination period is regarded as a single Bernoulli trial. Motivated by acquisition efficiency when multiple Bernoulli processes are of interest, we formulate the allocation of trials under a constraint on the mean as an optimal resource allocation problem. An oracle-aided trial allocation demonstrates that there can be a significant advantage from varying the allocation for different processes and inspires a simple trial allocation gain quantity. Motivated by realizing this gain without an oracle, we present a trellis-based framework for representing and optimizing stopping rules. Considering the convenient case of Beta priors, three implementable stopping rules with similar performances are explored, and the simplest of these is shown to asymptotically achieve the oracle-aided trial allocation. These approaches are further extended to estimating functions of a Bernoulli parameter. In simulations inspired by realistic active imaging scenarios, we demonstrate significant mean-squared error improvements: up to 4.36 dB for the estimation of p and up to 1.80 dB for the estimation of log p.



### MobileFace: 3D Face Reconstruction with Efficient CNN Regression
- **Arxiv ID**: http://arxiv.org/abs/1809.08809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08809v1)
- **Published**: 2018-09-24 09:15:08+00:00
- **Updated**: 2018-09-24 09:15:08+00:00
- **Authors**: Nikolai Chinaev, Alexander Chigorin, Ivan Laptev
- **Comment**: ECCV Workshops (PeopleCap) 2018
- **Journal**: None
- **Summary**: Estimation of facial shapes plays a central role for face transfer and animation. Accurate 3D face reconstruction, however, often deploys iterative and costly methods preventing real-time applications. In this work we design a compact and fast CNN model enabling real-time face reconstruction on mobile devices. For this purpose, we first study more traditional but slow morphable face models and use them to automatically annotate a large set of images for CNN training. We then investigate a class of efficient MobileNet CNNs and adapt such models for the task of shape regression. Our evaluation on three datasets demonstrates significant improvements in the speed and the size of our model while maintaining state-of-the-art reconstruction accuracy.



### Vis-DSS: An Open-Source toolkit for Visual Data Selection and Summarization
- **Arxiv ID**: http://arxiv.org/abs/1809.08846v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.08846v1)
- **Published**: 2018-09-24 11:15:14+00:00
- **Updated**: 2018-09-24 11:15:14+00:00
- **Authors**: Rishabh Iyer, Pratik Dubal, Kunal Dargan, Suraj Kothawade, Rohan Mahadev, Vishal Kaushal
- **Comment**: Vis-DSS is available at https://github.com/rishabhk108/vis-dss
- **Journal**: None
- **Summary**: With increasing amounts of visual data being created in the form of videos and images, visual data selection and summarization are becoming ever increasing problems. We present Vis-DSS, an open-source toolkit for Visual Data Selection and Summarization. Vis-DSS implements a framework of models for summarization and data subset selection using submodular functions, which are becoming increasingly popular today for these problems. We present several classes of models, capturing notions of diversity, coverage, representation and importance, along with optimization/inference and learning algorithms. Vis-DSS is the first open source toolkit for several Data selection and summarization tasks including Image Collection Summarization, Video Summarization, Training Data selection for Classification and Diversified Active Learning. We demonstrate state-of-the art performance on all these tasks, and also show how we can scale to large problems. Vis-DSS allows easy integration for applications to be built on it, also can serve as a general skeleton that can be extended to several use cases, including video and image sharing platforms for creating GIFs, image montage creation, or as a component to surveillance systems and we demonstrate this by providing a graphical user-interface (GUI) desktop app built over Qt framework. Vis-DSS is available at https://github.com/rishabhk108/vis-dss



### A Framework towards Domain Specific Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1809.08854v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.08854v2)
- **Published**: 2018-09-24 11:36:53+00:00
- **Updated**: 2018-12-28 08:54:33+00:00
- **Authors**: Vishal Kaushal, Sandeep Subramanian, Suraj Kothawade, Rishabh Iyer, Ganesh Ramakrishnan
- **Comment**: Accepted to WACV 2019
- **Journal**: None
- **Summary**: In the light of exponentially increasing video content, video summarization has attracted a lot of attention recently due to its ability to optimize time and storage. Characteristics of a good summary of a video depend on the particular domain under question. We propose a novel framework for domain specific video summarization. Given a video of a particular domain, our system can produce a summary based on what is important for that domain in addition to possessing other desired characteristics like representativeness, coverage, diversity etc. as suitable to that domain. Past related work has focused either on using supervised approaches for ranking the snippets to produce summary or on using unsupervised approaches of generating the summary as a subset of snippets with the above characteristics. We look at the joint problem of learning domain specific importance of segments as well as the desired summary characteristic for that domain. Our studies show that the more efficient way of incorporating domain specific relevances into a summary is by obtaining ratings of shots as opposed to binary inclusion/exclusion information. We also argue that ratings can be seen as unified representation of all possible ground truth summaries of a video, taking us one step closer in dealing with challenges associated with multiple ground truth summaries of a video. We also propose a novel evaluation measure which is more naturally suited in assessing the quality of video summary for the task at hand than F1 like measures. It leverages the ratings information and is richer in appropriately modeling desirable and undesirable characteristics of a summary. Lastly, we release a gold standard dataset for furthering research in domain specific video summarization, which to our knowledge is the first dataset with long videos across several domains with rating annotations.



### A Probabilistic Semi-Supervised Approach to Multi-Task Human Activity Modeling
- **Arxiv ID**: http://arxiv.org/abs/1809.08875v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08875v3)
- **Published**: 2018-09-24 12:39:21+00:00
- **Updated**: 2019-03-14 12:11:48+00:00
- **Authors**: Judith Bütepage, Hedvig Kjellström, Danica Kragic
- **Comment**: None
- **Journal**: None
- **Summary**: Human behavior is a continuous stochastic spatio-temporal process which is governed by semantic actions and affordances as well as latent factors. Therefore, video-based human activity modeling is concerned with a number of tasks such as inferring current and future semantic labels, predicting future continuous observations as well as imagining possible future label and feature sequences. In this paper we present a semi-supervised probabilistic deep latent variable model that can represent both discrete labels and continuous observations as well as latent dynamics over time. This allows the model to solve several tasks at once without explicit fine-tuning. We focus here on the tasks of action classification, detection, prediction and anticipation as well as motion prediction and synthesis based on 3D human activity data recorded with Kinect. We further extend the model to capture hierarchical label structure and to model the dependencies between multiple entities, such as a human and objects. Our experiments demonstrate that our principled approach to human activity modeling can be used to detect current and anticipate future semantic labels and to predict and synthesize future label and feature sequences. When comparing our model to state-of-the-art approaches, which are specifically designed for e.g. action classification, we find that our probabilistic formulation outperforms or is comparable to these task specific models.



### Vision-based Control of a Quadrotor in User Proximity: Mediated vs End-to-End Learning Approaches
- **Arxiv ID**: http://arxiv.org/abs/1809.08881v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.08881v2)
- **Published**: 2018-09-24 12:49:38+00:00
- **Updated**: 2019-02-25 15:02:06+00:00
- **Authors**: Dario Mantegazza, Jérôme Guzzi, Luca M. Gambardella, Alessandro Giusti
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the task of controlling a quadrotor to hover in front of a freely moving user, using input data from an onboard camera. On this specific task we compare two widespread learning paradigms: a mediated approach, which learns an high-level state from the input and then uses it for deriving control signals; and an end-to-end approach, which skips high-level state estimation altogether. We show that despite their fundamental difference, both approaches yield equivalent performance on this task. We finally qualitatively analyze the behavior of a quadrotor implementing such approaches.



### On The Utility of Conditional Generation Based Mutual Information for Characterizing Adversarial Subspaces
- **Arxiv ID**: http://arxiv.org/abs/1809.08986v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.08986v1)
- **Published**: 2018-09-24 15:05:01+00:00
- **Updated**: 2018-09-24 15:05:01+00:00
- **Authors**: Chia-Yi Hsu, Pei-Hsuan Lu, Pin-Yu Chen, Chia-Mu Yu
- **Comment**: Accepted to IEEE GlobalSIP 2018
- **Journal**: None
- **Summary**: Recent studies have found that deep learning systems are vulnerable to adversarial examples; e.g., visually unrecognizable adversarial images can easily be crafted to result in misclassification. The robustness of neural networks has been studied extensively in the context of adversary detection, which compares a metric that exhibits strong discriminate power between natural and adversarial examples. In this paper, we propose to characterize the adversarial subspaces through the lens of mutual information (MI) approximated by conditional generation methods. We use MI as an information-theoretic metric to strengthen existing defenses and improve the performance of adversary detection. Experimental results on MagNet defense demonstrate that our proposed MI detector can strengthen its robustness against powerful adversarial attacks.



### Improved Semantic Stixels via Multimodal Sensor Fusion
- **Arxiv ID**: http://arxiv.org/abs/1809.08993v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08993v2)
- **Published**: 2018-09-24 15:18:10+00:00
- **Updated**: 2018-09-27 13:47:28+00:00
- **Authors**: Florian Piewak, Peter Pinggera, Markus Enzweiler, David Pfeiffer, Marius Zöllner
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a compact and accurate representation of 3D scenes that are observed by a LiDAR sensor and a monocular camera. The proposed method is based on the well-established Stixel model originally developed for stereo vision applications. We extend this Stixel concept to incorporate data from multiple sensor modalities. The resulting mid-level fusion scheme takes full advantage of the geometric accuracy of LiDAR measurements as well as the high resolution and semantic detail of RGB images. The obtained environment model provides a geometrically and semantically consistent representation of the 3D scene at a significantly reduced amount of data while minimizing information loss at the same time. Since the different sensor modalities are considered as input to a joint optimization problem, the solution is obtained with only minor computational overhead. We demonstrate the effectiveness of the proposed multimodal Stixel algorithm on a manually annotated ground truth dataset. Our results indicate that the proposed mid-level fusion of LiDAR and camera data improves both the geometric and semantic accuracy of the Stixel model significantly while reducing the computational overhead as well as the amount of generated data in comparison to using a single modality on its own.



### Fast Geometrically-Perturbed Adversarial Faces
- **Arxiv ID**: http://arxiv.org/abs/1809.08999v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.08999v2)
- **Published**: 2018-09-24 15:26:13+00:00
- **Updated**: 2018-09-28 17:20:54+00:00
- **Authors**: Ali Dabouei, Sobhan Soleymani, Jeremy Dawson, Nasser M. Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: The state-of-the-art performance of deep learning algorithms has led to a considerable increase in the utilization of machine learning in security-sensitive and critical applications. However, it has recently been shown that a small and carefully crafted perturbation in the input space can completely fool a deep model. In this study, we explore the extent to which face recognition systems are vulnerable to geometrically-perturbed adversarial faces. We propose a fast landmark manipulation method for generating adversarial faces, which is approximately 200 times faster than the previous geometric attacks and obtains 99.86% success rate on the state-of-the-art face recognition models. To further force the generated samples to be natural, we introduce a second attack constrained on the semantic structure of the face which has the half speed of the first attack with the success rate of 99.96%. Both attacks are extremely robust against the state-of-the-art defense methods with the success rate of equal or greater than 53.59%. Code is available at https://github.com/alldbi/FLM



### Weakly-Supervised Learning of Metric Aggregations for Deformable Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1809.09004v1
- **DOI**: 10.1109/JBHI.2018.2869700
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09004v1)
- **Published**: 2018-09-24 15:32:48+00:00
- **Updated**: 2018-09-24 15:32:48+00:00
- **Authors**: Enzo Ferrante, Puneet K. Dokania, Rafael Marini Silva, Nikos Paragios
- **Comment**: Accepted for publication in IEEE Journal of Biomedical and Health
  Informatics, 2018
- **Journal**: None
- **Summary**: Deformable registration has been one of the pillars of biomedical image computing. Conventional approaches refer to the definition of a similarity criterion that, once endowed with a deformation model and a smoothness constraint, determines the optimal transformation to align two given images. The definition of this metric function is among the most critical aspects of the registration process. We argue that incorporating semantic information (in the form of anatomical segmentation maps) into the registration process will further improve the accuracy of the results. In this paper, we propose a novel weakly supervised approach to learn domain specific aggregations of conventional metrics using anatomical segmentations. This combination is learned using latent structured support vector machines (LSSVM). The learned matching criterion is integrated within a metric free optimization framework based on graphical models, resulting in a multi-metric algorithm endowed with a spatially varying similarity metric function conditioned on the anatomical structures. We provide extensive evaluation on three different datasets of CT and MRI images, showing that learned multi-metric registration outperforms single-metric approaches based on conventional similarity measures.



### Autonomously and Simultaneously Refining Deep Neural Network Parameters by a Bi-Generative Adversarial Network Aided Genetic Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1809.10244v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1809.10244v1)
- **Published**: 2018-09-24 15:43:17+00:00
- **Updated**: 2018-09-24 15:43:17+00:00
- **Authors**: Yantao Lu, Burak Kakillioglu, Senem Velipasalar
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1805.09712
- **Journal**: None
- **Summary**: The choice of parameters, and the design of the network architecture are important factors affecting the performance of deep neural networks. Genetic Algorithms (GA) have been used before to determine parameters of a network. Yet, GAs perform a finite search over a discrete set of pre-defined candidates, and cannot, in general, generate unseen configurations. In this paper, to move from exploration to exploitation, we propose a novel and systematic method that autonomously and simultaneously optimizes multiple parameters of any deep neural network by using a GA aided by a bi-generative adversarial network (Bi-GAN). The proposed Bi-GAN allows the autonomous exploitation and choice of the number of neurons, for fully-connected layers, and number of filters, for convolutional layers, from a large range of values. Our proposed Bi-GAN involves two generators, and two different models compete and improve each other progressively with a GAN-based strategy to optimize the networks during GA evolution. Our proposed approach can be used to autonomously refine the number of convolutional layers and dense layers, number and size of kernels, and the number of neurons for the dense layers; choose the type of the activation function; and decide whether to use dropout and batch normalization or not, to improve the accuracy of different deep neural network architectures. Without loss of generality, the proposed method has been tested with the ModelNet database, and compared with the 3D Shapenets and two GA-only methods. The results show that the presented approach can simultaneously and successfully optimize multiple neural network parameters, and achieve higher accuracy even with shallower networks.



### Sparse-to-Continuous: Enhancing Monocular Depth Estimation using Occupancy Maps
- **Arxiv ID**: http://arxiv.org/abs/1809.09061v3
- **DOI**: 10.1109/ICAR46387.2019.8981652
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09061v3)
- **Published**: 2018-09-24 17:10:10+00:00
- **Updated**: 2019-10-21 22:01:23+00:00
- **Authors**: Nícolas Rosa, Vitor Guizilini, Valdir Grassi Jr
- **Comment**: Accepted. (c) 2019 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: This paper addresses the problem of single image depth estimation (SIDE), focusing on improving the quality of deep neural network predictions. In a supervised learning scenario, the quality of predictions is intrinsically related to the training labels, which guide the optimization process. For indoor scenes, structured-light-based depth sensors (e.g. Kinect) are able to provide dense, albeit short-range, depth maps. On the other hand, for outdoor scenes, LiDARs are considered the standard sensor, which comparatively provides much sparser measurements, especially in areas further away. Rather than modifying the neural network architecture to deal with sparse depth maps, this article introduces a novel densification method for depth maps, using the Hilbert Maps framework. A continuous occupancy map is produced based on 3D points from LiDAR scans, and the resulting reconstructed surface is projected into a 2D depth map with arbitrary resolution. Experiments conducted with various subsets of the KITTI dataset show a significant improvement produced by the proposed Sparse-to-Continuous technique, without the introduction of extra information into the training stage.



### Incorporating Luminance, Depth and Color Information by a Fusion-based Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1809.09077v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09077v3)
- **Published**: 2018-09-24 17:45:35+00:00
- **Updated**: 2019-05-19 18:07:56+00:00
- **Authors**: Shang-Wei Hung, Shao-Yuan Lo, Hsueh-Ming Hang
- **Comment**: Accepted in IEEE International Conference on Image Processing (ICIP)
  2019
- **Journal**: None
- **Summary**: Semantic segmentation has made encouraging progress due to the success of deep convolutional networks in recent years. Meanwhile, depth sensors become prevalent nowadays, so depth maps can be acquired more easily. However, there are few studies that focus on the RGB-D semantic segmentation task. Exploiting the depth information effectiveness to improve performance is a challenge. In this paper, we propose a novel solution named LDFNet, which incorporates Luminance, Depth and Color information by a fusion-based network. It includes a sub-network to process depth maps and employs luminance images to assist the depth information in processes. LDFNet outperforms the other state-of-art systems on the Cityscapes dataset, and its inference speed is faster than most of the existing networks. The experimental results show the effectiveness of the proposed multi-modal fusion network and its potential for practical applications.



### Cylindrical Transform: 3D Semantic Segmentation of Kidneys With Limited Annotated Images
- **Arxiv ID**: http://arxiv.org/abs/1809.10245v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1809.10245v1)
- **Published**: 2018-09-24 17:58:54+00:00
- **Updated**: 2018-09-24 17:58:54+00:00
- **Authors**: Hojjat Salehinejad, Sumeya Naqvi, Errol Colak, Joseph Barfett, Shahrokh Valaee
- **Comment**: This paper is accepted for presentation at IEEE Global Conference on
  Signal and Information Processing (IEEE GlobalSIP), California, USA, 2018
- **Journal**: None
- **Summary**: In this paper, we propose a novel technique for sampling sequential images using a cylindrical transform in a cylindrical coordinate system for kidney semantic segmentation in abdominal computed tomography (CT). The images generated from a cylindrical transform augment a limited annotated set of images in three dimensions. This approach enables us to train contemporary classification deep convolutional neural networks (DCNNs) instead of fully convolutional networks (FCNs) for semantic segmentation. Typical semantic segmentation models segment a sequential set of images (e.g. CT or video) by segmenting each image independently. However, the proposed method not only considers the spatial dependency in the x-y plane, but also the spatial sequential dependency along the z-axis. The results show that classification DCNNs, trained on cylindrical transformed images, can achieve a higher segmentation performance value than FCNs using a limited number of annotated images.



### Real-Time Monocular Object-Model Aware Sparse SLAM
- **Arxiv ID**: http://arxiv.org/abs/1809.09149v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.09149v2)
- **Published**: 2018-09-24 18:23:32+00:00
- **Updated**: 2019-03-06 06:24:40+00:00
- **Authors**: Mehdi Hosseinzadeh, Kejie Li, Yasir Latif, Ian Reid
- **Comment**: Accepted to ICRA 2019 (for video demo look at
  https://youtu.be/UMWXd4sHONw and https://youtu.be/QPQqVrvP0dE)
- **Journal**: None
- **Summary**: Simultaneous Localization And Mapping (SLAM) is a fundamental problem in mobile robotics. While sparse point-based SLAM methods provide accurate camera localization, the generated maps lack semantic information. On the other hand, state of the art object detection methods provide rich information about entities present in the scene from a single image. This work incorporates a real-time deep-learned object detector to the monocular SLAM framework for representing generic objects as quadrics that permit detections to be seamlessly integrated while allowing the real-time performance. Finer reconstruction of an object, learned by a CNN network, is also incorporated and provides a shape prior for the quadric leading further refinement. To capture the dominant structure of the scene, additional planar landmarks are detected by a CNN-based plane detector and modeled as independent landmarks in the map. Extensive experiments support our proposed inclusion of semantic objects and planar structures directly in the bundle-adjustment of SLAM - Semantic SLAM - that enriches the reconstructed map semantically, while significantly improving the camera localization. The performance of our SLAM system is demonstrated in https://youtu.be/UMWXd4sHONw and https://youtu.be/QPQqVrvP0dE .



### Zoom-RNN: A Novel Method for Person Recognition Using Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.09189v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09189v2)
- **Published**: 2018-09-24 19:45:13+00:00
- **Updated**: 2018-09-26 01:01:50+00:00
- **Authors**: Sina Mokhtarzadeh Azar, Sajjad Azami, Mina Ghadimi Atigh, Mohammad Javadi, Ahmad Nickabadi
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: The overwhelming popularity of social media has resulted in bulk amounts of personal photos being uploaded to the internet every day. Since these photos are taken in unconstrained settings, recognizing the identities of people among the photos remains a challenge. Studies have indicated that utilizing evidence other than face appearance improves the performance of person recognition systems. In this work, we aim to take advantage of additional cues obtained from different body regions in a zooming in fashion for person recognition. Hence, we present Zoom-RNN, a novel method based on recurrent neural networks for combining evidence extracted from the whole body, upper body, and head regions. Our model is evaluated on a challenging dataset, namely People In Photo Albums (PIPA), and we demonstrate that employing our system improves the performance of conventional fusion methods by a noticeable margin.



### Towards Automated Post-Earthquake Inspections with Deep Learning-based Condition-Aware Models
- **Arxiv ID**: http://arxiv.org/abs/1809.09195v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.09195v1)
- **Published**: 2018-09-24 19:59:07+00:00
- **Updated**: 2018-09-24 19:59:07+00:00
- **Authors**: Vedhus Hoskere, Yasutaka Narazaki, Tu A. Hoang, Billie F. Spencer Jr
- **Comment**: None
- **Journal**: None
- **Summary**: In the aftermath of an earthquake, rapid structural inspections are required to get citizens back in to their homes and offices in a safe and timely manner. These inspections gfare typically conducted by municipal authorities through structural engineer volunteers. As manual inspec-tions can be time consuming, laborious and dangerous, research has been underway to develop methods to help speed up and increase the automation of the entire process. Researchers typi-cally envisage the use of unmanned aerial vehicles (UAV) for data acquisition and computer vision for data processing to extract actionable information. In this work we propose a new framework to generate vision-based condition-aware models that can serve as the basis for speeding up or automating higher level inspection decisions. The condition-aware models are generated by projecting the inference of trained deep-learning models on a set of images of a structure onto a 3D mesh model generated through multi-view stereo from the same image set. Deep fully convolutional residual networks are used for semantic segmentation of images of buildings to provide (i) damage information such as cracks and spalling (ii) contextual infor-mation such as the presence of a building and visually identifiable components like windows and doors. The proposed methodology was implemented on a damaged building that was sur-veyed by the authors after the Central Mexico Earthquake in September 2017 and qualitative-ly evaluated. Results demonstrate the promise of the proposed method towards the ultimate goal of rapid and automated post-earthquake inspections.



