# Arxiv Papers in cs.CV on 2018-09-16
### CADP: A Novel Dataset for CCTV Traffic Camera based Accident Analysis
- **Arxiv ID**: http://arxiv.org/abs/1809.05782v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1809.05782v2)
- **Published**: 2018-09-16 00:01:39+00:00
- **Updated**: 2018-11-16 06:28:36+00:00
- **Authors**: Ankit Shah, Jean Baptiste Lamare, Tuan Nguyen Anh, Alexander Hauptmann
- **Comment**: Accepted at IEEE International Workshop on Traffic and Street
  Surveillance for Safety and Security, First three authors contributed
  equally, 7 pages + 1 References
- **Journal**: None
- **Summary**: This paper presents a novel dataset for traffic accidents analysis. Our goal is to resolve the lack of public data for research about automatic spatio-temporal annotations for traffic safety in the roads. Through the analysis of the proposed dataset, we observed a significant degradation of object detection in pedestrian category in our dataset, due to the object sizes and complexity of the scenes. To this end, we propose to integrate contextual information into conventional Faster R-CNN using Context Mining (CM) and Augmented Context Mining (ACM) to complement the accuracy for small pedestrian detection. Our experiments indicate a considerable improvement in object detection accuracy: +8.51% for CM and +6.20% for ACM. Finally, we demonstrate the performance of accident forecasting in our dataset using Faster R-CNN and an Accident LSTM architecture. We achieved an average of 1.684 seconds in terms of Time-To-Accident measure with an Average Precision of 47.25%. Our Webpage for the paper is https://goo.gl/cqK2wE



### GANVO: Unsupervised Deep Monocular Visual Odometry and Depth Estimation with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.05786v3
- **DOI**: 10.1109/ICRA.2019.8793512
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.05786v3)
- **Published**: 2018-09-16 00:27:09+00:00
- **Updated**: 2019-03-05 12:45:15+00:00
- **Authors**: Yasin Almalioglu, Muhamad Risqi U. Saputra, Pedro P. B. de Gusmao, Andrew Markham, Niki Trigoni
- **Comment**: ICRA 2019 - accepted
- **Journal**: 2019 International Conference on Robotics and Automation (ICRA)
- **Summary**: In the last decade, supervised deep learning approaches have been extensively employed in visual odometry (VO) applications, which is not feasible in environments where labelled data is not abundant. On the other hand, unsupervised deep learning approaches for localization and mapping in unknown environments from unlabelled data have received comparatively less attention in VO research. In this study, we propose a generative unsupervised learning framework that predicts 6-DoF pose camera motion and monocular depth map of the scene from unlabelled RGB image sequences, using deep convolutional Generative Adversarial Networks (GANs). We create a supervisory signal by warping view sequences and assigning the re-projection minimization to the objective loss function that is adopted in multi-view pose estimation and single-view depth generation network. Detailed quantitative and qualitative evaluations of the proposed framework on the KITTI and Cityscapes datasets show that the proposed method outperforms both existing traditional and unsupervised deep VO methods providing better results for both pose estimation and depth recovery.



### Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/1809.05825v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1809.05825v2)
- **Published**: 2018-09-16 07:08:58+00:00
- **Updated**: 2019-03-03 01:29:13+00:00
- **Authors**: Michael Danielczuk, Matthew Matl, Saurabh Gupta, Andrew Li, Andrew Lee, Jeffrey Mahler, Ken Goldberg
- **Comment**: In proceedings of ICRA 2019/
- **Journal**: None
- **Summary**: The ability to segment unknown objects in depth images has potential to enhance robot skills in grasping and object tracking. Recent computer vision research has demonstrated that Mask R-CNN can be trained to segment specific categories of objects in RGB images when massive hand-labeled datasets are available. As generating these datasets is time consuming, we instead train with synthetic depth images. Many robots now use depth sensors, and recent results suggest training on synthetic depth data can transfer successfully to the real world. We present a method for automated dataset generation and rapidly generate a synthetic training dataset of 50,000 depth images and 320,000 object masks using simulated heaps of 3D CAD models. We train a variant of Mask R-CNN with domain randomization on the generated dataset to perform category-agnostic instance segmentation without any hand-labeled data and we evaluate the trained network, which we refer to as Synthetic Depth (SD) Mask R-CNN, on a set of real, high-resolution depth images of challenging, densely-cluttered bins containing objects with highly-varied geometry. SD Mask R-CNN outperforms point cloud clustering baselines by an absolute 15% in Average Precision and 20% in Average Recall on COCO benchmarks, and achieves performance levels similar to a Mask R-CNN trained on a massive, hand-labeled RGB dataset and fine-tuned on real images from the experimental setup. We deploy the model in an instance-specific grasping pipeline to demonstrate its usefulness in a robotics application. Code, the synthetic training dataset, and supplementary material are available at https://bit.ly/2letCuE.



### Real-Time, Highly Accurate Robotic Grasp Detection using Fully Convolutional Neural Networks with High-Resolution Images
- **Arxiv ID**: http://arxiv.org/abs/1809.05828v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1809.05828v2)
- **Published**: 2018-09-16 07:36:02+00:00
- **Updated**: 2019-09-16 08:05:22+00:00
- **Authors**: Dongwon Park, Yonghyeok Seo, Se Young Chun
- **Comment**: This work was superceded by arXiv:1812.07762
- **Journal**: None
- **Summary**: Robotic grasp detection for novel objects is a challenging task, but for the last few years, deep learning based approaches have achieved remarkable performance improvements, up to 96.1% accuracy, with RGB-D data. In this paper, we propose fully convolutional neural network (FCNN) based methods for robotic grasp detection. Our methods also achieved state-of-the-art detection accuracy (up to 96.6%) with state-of- the-art real-time computation time for high-resolution images (6-20ms per 360x360 image) on Cornell dataset. Due to FCNN, our proposed method can be applied to images with any size for detecting multigrasps on multiobjects. Proposed methods were evaluated using 4-axis robot arm with small parallel gripper and RGB-D camera for grasping challenging small, novel objects. With accurate vision-robot coordinate calibration through our proposed learning-based, fully automatic approach, our proposed method yielded 90% success rate.



### Multiple Abnormality Detection for Automatic Medical Image Diagnosis Using Bifurcated Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1809.05831v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05831v2)
- **Published**: 2018-09-16 07:55:51+00:00
- **Updated**: 2018-10-15 20:02:15+00:00
- **Authors**: Mohsen Hajabdollahi, Reza Esfandiarpoor, Elyas Sabeti, Nader Karimi, Kayvan Najarian, S. M. Reza Soroushmehr, Shadrokh Samavi
- **Comment**: None
- **Journal**: None
- **Summary**: Automating classification and segmentation process of abnormal regions in different body organs has a crucial role in most of medical imaging applications such as funduscopy, endoscopy, and dermoscopy. Detecting multiple abnormalities in each type of images is necessary for better and more accurate diagnosis procedure and medical decisions. In recent years portable medical imaging devices such as capsule endoscopy and digital dermatoscope have been introduced and made the diagnosis procedure easier and more efficient. However, these portable devices have constrained power resources and limited computational capability. To address this problem, we propose a bifurcated structure for convolutional neural networks performing both classification and segmentation of multiple abnormalities simultaneously. The proposed network is first trained by each abnormality separately. Then the network is trained using all abnormalities. In order to reduce the computational complexity, the network is redesigned to share some features which are common among all abnormalities. Later, these shared features are used in different settings (directions) to segment and classify the abnormal region of the image. Finally, results of the classification and segmentation directions are fused to obtain the classified segmentation map. Proposed framework is simulated using four frequent gastrointestinal abnormalities as well as three dermoscopic lesions and for evaluation of the proposed framework the results are compared with the corresponding ground truth map. Properties of the bifurcated network like low complexity and resource sharing make it suitable to be implemented as a part of portable medical imaging devices.



### Towards Good Practices for Multi-modal Fusion in Large-scale Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1809.05848v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05848v4)
- **Published**: 2018-09-16 10:17:37+00:00
- **Updated**: 2018-09-28 02:12:57+00:00
- **Authors**: Jinlai Liu, Zehuan Yuan, Changhu Wang
- **Comment**: ECCV YouTube-8M workshop general paper
- **Journal**: None
- **Summary**: Leveraging both visual frames and audio has been experimentally proven effective to improve large-scale video classification. Previous research on video classification mainly focuses on the analysis of visual content among extracted video frames and their temporal feature aggregation. In contrast, multimodal data fusion is achieved by simple operators like average and concatenation. Inspired by the success of bilinear pooling in the visual and language fusion, we introduce multi-modal factorized bilinear pooling (MFB) to fuse visual and audio representations. We combine MFB with different video-level features and explore its effectiveness in video classification. Experimental results on the challenging Youtube-8M v2 dataset demonstrate that MFB significantly outperforms simple fusion methods in large-scale video classification.



### Geometry-Consistent Generative Adversarial Networks for One-Sided Unsupervised Domain Mapping
- **Arxiv ID**: http://arxiv.org/abs/1809.05852v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05852v2)
- **Published**: 2018-09-16 10:42:29+00:00
- **Updated**: 2018-11-26 00:49:30+00:00
- **Authors**: Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, Kun Zhang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain mapping aims to learn a function to translate domain X to Y by a function GXY in the absence of paired examples. Finding the optimal GXY without paired data is an ill-posed problem, so appropriate constraints are required to obtain reasonable solutions. One of the most prominent constraints is cycle consistency, which enforces the translated image by GXY to be translated back to the input image by an inverse mapping GYX. While cycle consistency requires the simultaneous training of GXY and GY X, recent studies have shown that one-sided domain mapping can be achieved by preserving pairwise distances between images. Although cycle consistency and distance preservation successfully constrain the solution space, they overlook the special properties that simple geometric transformations do not change the semantic structure of images. Based on this special property, we develop a geometry-consistent generative adversarial network (GcGAN), which enables one-sided unsupervised domain mapping. GcGAN takes the original image and its counterpart image transformed by a predefined geometric transformation as inputs and generates two images in the new domain coupled with the corresponding geometry-consistency constraint. The geometry-consistency constraint reduces the space of possible solutions while keep the correct solutions in the search space. Quantitative and qualitative comparisons with the baseline (GAN alone) and the state-of-the-art methods including CycleGAN and DistanceGAN demonstrate the effectiveness of our method.



### f-VAEs: Improve VAEs with Conditional Flows
- **Arxiv ID**: http://arxiv.org/abs/1809.05861v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.05861v1)
- **Published**: 2018-09-16 12:23:09+00:00
- **Updated**: 2018-09-16 12:23:09+00:00
- **Authors**: Jianlin Su, Guang Wu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we integrate VAEs and flow-based generative models successfully and get f-VAEs. Compared with VAEs, f-VAEs generate more vivid images, solved the blurred-image problem of VAEs. Compared with flow-based models such as Glow, f-VAE is more lightweight and converges faster, achieving the same performance under smaller-size architecture.



### In Defense of the Classification Loss for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1809.05864v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05864v2)
- **Published**: 2018-09-16 12:35:53+00:00
- **Updated**: 2018-11-29 02:54:49+00:00
- **Authors**: Yao Zhai, Xun Guo, Yan Lu, Houqiang Li
- **Comment**: None
- **Journal**: None
- **Summary**: The recent research for person re-identification has been focused on two trends. One is learning the part-based local features to form more informative feature descriptors. The other is designing effective metric learning loss functions such as the triplet loss family. We argue that learning global features with classification loss could achieve the same goal, even with some simple and cost-effective architecture design. In this paper, we first explain why the person re-id framework with standard classification loss usually has inferior performance compared to metric learning. Based on that, we further propose a person re-id framework featured by channel grouping and multi-branch strategy, which divides global features into multiple channel groups and learns the discriminative channel group features by multi-branch classification layers. The extensive experiments show that our framework outperforms prior state-of-the-arts in terms of both accuracy and inference speed.



### Road Detection Technique Using Filters with Application to Autonomous Driving System
- **Arxiv ID**: http://arxiv.org/abs/1809.05878v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1809.05878v1)
- **Published**: 2018-09-16 14:14:36+00:00
- **Updated**: 2018-09-16 14:14:36+00:00
- **Authors**: Y. O. Agunbiade, J. O. Dehinbo, T. Zuva, A. K. Akanbi
- **Comment**: 7 pages, 7 figures, International Journal of Computing,
  Communications & Instrumentation Engg. (IJCCIE) 2016
- **Journal**: None
- **Summary**: Autonomous driving systems are broadly used equipment in the industries and in our daily lives, they assist in production, but are majorly used for exploration in dangerous or unfamiliar locations. Thus, for a successful exploration, navigation plays a significant role. Road detection is an essential factor that assists autonomous robots achieved perfect navigation. Various techniques using camera sensors have been proposed by numerous scholars with inspiring results, but their techniques are still vulnerable to these environmental noises: rain, snow, light intensity and shadow. In addressing these problems, this paper proposed to enhance the road detection system with filtering algorithm to overcome these limitations. Normalized Differences Index (NDI) and morphological operation are the filtering algorithms used to address the effect of shadow and guidance and re-guidance image filtering algorithms are used to address the effect of rain and/or snow, while dark channel image and specular-to-diffuse are the filters used to address light intensity effects. The experimental performance of the road detection system with filtering algorithms was tested qualitatively and quantitatively using the following evaluation schemes: False Negative Rate (FNR) and False Positive Rate (FPR). Comparison results of the road detection system with and without filtering algorithm shows the filtering algorithm's capability to suppress the effect of environmental noises because better road/non-road classification is achieved by the road detection system. with filtering algorithm. This achievement has further improved path planning/region classification for autonomous driving system



### An FPGA-Accelerated Design for Deep Learning Pedestrian Detection in Self-Driving Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1809.05879v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.05879v1)
- **Published**: 2018-09-16 14:16:33+00:00
- **Updated**: 2018-09-16 14:16:33+00:00
- **Authors**: Abdallah Moussawi, Kamal Haddad, Anthony Chahine
- **Comment**: 7 pages. American University of Beirut, Faculty of Engineering and
  Architecture Student and Alumni Conference 2017 FEASAC
- **Journal**: None
- **Summary**: With the rise of self-driving vehicles comes the risk of accidents and the need for higher safety, and protection for pedestrian detection in the following scenarios: imminent crashes, thus the car should crash into an object and avoid the pedestrian, and in the case of road intersections, where it is important for the car to stop when pedestrians are crossing. Currently, a special topology of deep neural networks called Fused Deep Neural Network (F-DNN) is considered to be the state of the art in pedestrian detection, as it has the lowest miss rate, yet it is very slow. Therefore, acceleration is needed to speed up the performance. This project proposes two contributions to address this problem, by using a deep neural network used for object detection, called Single Shot Multi-Box Detector (SSD). The first contribution is training and tuning the hyperparameters of SSD to improve pedestrian detection. The second contribution is a new FPGA design for accelerating the model on the Altera Arria 10 platform. The final system will be used in self-driving vehicles in real-time. Preliminary results of the improved SSD shows 3% higher miss-rate than F-DNN on Caltech pedestrian detection benchmark, but 4x performance improvement. The acceleration design is expected to achieve an additional performance improvement significantly outweighing the minimal difference in accuracy.



### Multi-Label Image Classification via Knowledge Distillation from Weakly-Supervised Detection
- **Arxiv ID**: http://arxiv.org/abs/1809.05884v2
- **DOI**: 10.1145/3240508.3240567
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1809.05884v2)
- **Published**: 2018-09-16 14:35:03+00:00
- **Updated**: 2019-02-21 12:28:58+00:00
- **Authors**: Yongcheng Liu, Lu Sheng, Jing Shao, Junjie Yan, Shiming Xiang, Chunhong Pan
- **Comment**: accepted by ACM Multimedia 2018, 9 pages, 4 figures, 5 tables
- **Journal**: None
- **Summary**: Multi-label image classification is a fundamental but challenging task towards general visual understanding. Existing methods found the region-level cues (e.g., features from RoIs) can facilitate multi-label classification. Nevertheless, such methods usually require laborious object-level annotations (i.e., object labels and bounding boxes) for effective learning of the object-level visual features. In this paper, we propose a novel and efficient deep framework to boost multi-label classification by distilling knowledge from weakly-supervised detection task without bounding box annotations. Specifically, given the image-level annotations, (1) we first develop a weakly-supervised detection (WSD) model, and then (2) construct an end-to-end multi-label image classification framework augmented by a knowledge distillation module that guides the classification model by the WSD model according to the class-level predictions for the whole image and the object-level visual features for object RoIs. The WSD model is the teacher model and the classification model is the student model. After this cross-task knowledge distillation, the performance of the classification model is significantly improved and the efficiency is maintained since the WSD model can be safely discarded in the test phase. Extensive experiments on two large-scale datasets (MS-COCO and NUS-WIDE) show that our framework achieves superior performances over the state-of-the-art methods on both performance and efficiency.



### MeshCNN: A Network with an Edge
- **Arxiv ID**: http://arxiv.org/abs/1809.05910v2
- **DOI**: 10.1145/3306346.3322959
- **Categories**: **cs.LG**, cs.CV, cs.GR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.05910v2)
- **Published**: 2018-09-16 16:32:29+00:00
- **Updated**: 2019-02-13 11:30:57+00:00
- **Authors**: Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman, Daniel Cohen-Or
- **Comment**: For a two-minute explanation video see https://bit.ly/meshcnnvideo
- **Journal**: None
- **Summary**: Polygonal meshes provide an efficient representation for 3D shapes. They explicitly capture both shape surface and topology, and leverage non-uniformity to represent large flat regions as well as sharp, intricate features. This non-uniformity and irregularity, however, inhibits mesh analysis efforts using neural networks that combine convolution and pooling operations. In this paper, we utilize the unique properties of the mesh for a direct analysis of 3D shapes using MeshCNN, a convolutional neural network designed specifically for triangular meshes. Analogous to classic CNNs, MeshCNN combines specialized convolution and pooling layers that operate on the mesh edges, by leveraging their intrinsic geodesic connections. Convolutions are applied on edges and the four edges of their incident triangles, and pooling is applied via an edge collapse operation that retains surface topology, thereby, generating new mesh connectivity for the subsequent convolutions. MeshCNN learns which edges to collapse, thus forming a task-driven process where the network exposes and expands the important features while discarding the redundant ones. We demonstrate the effectiveness of our task-driven pooling on various learning tasks applied to 3D meshes.



### Maximum-Entropy Fine-Grained Classification
- **Arxiv ID**: http://arxiv.org/abs/1809.05934v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.05934v2)
- **Published**: 2018-09-16 18:58:22+00:00
- **Updated**: 2018-09-20 21:11:37+00:00
- **Authors**: Abhimanyu Dubey, Otkrist Gupta, Ramesh Raskar, Nikhil Naik
- **Comment**: Camera-ready, accepted to NIPS 2018, v2 has minor typo updates and
  small changes in text
- **Journal**: None
- **Summary**: Fine-Grained Visual Classification (FGVC) is an important computer vision problem that involves small diversity within the different classes, and often requires expert annotators to collect data. Utilizing this notion of small visual diversity, we revisit Maximum-Entropy learning in the context of fine-grained classification, and provide a training routine that maximizes the entropy of the output probability distribution for training convolutional neural networks on FGVC tasks. We provide a theoretical as well as empirical justification of our approach, and achieve state-of-the-art performance across a variety of classification tasks in FGVC, that can potentially be extended to any fine-tuning task. Our method is robust to different hyperparameter values, amount of training data and amount of training label noise and can hence be a valuable tool in many similar problems.



### 3D Path Planning from a Single 2D Fluoroscopic Image for Robot Assisted Fenestrated Endovascular Aortic Repair
- **Arxiv ID**: http://arxiv.org/abs/1809.05955v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05955v1)
- **Published**: 2018-09-16 21:01:16+00:00
- **Updated**: 2018-09-16 21:01:16+00:00
- **Authors**: Jian-Qing Zheng, Xiao-Yun Zhou, Celia Riga, Guang-Zhong Yang
- **Comment**: None
- **Journal**: None
- **Summary**: The current standard of intra-operative navigation during Fenestrated Endovascular Aortic Repair (FEVAR) calls for need of 3D alignments between inserted devices and aortic branches. The navigation commonly via 2D fluoroscopic images, lacks anatomical information, resulting in longer operation hours and radiation exposure. In this paper, a framework for real-time 3D robotic path planning from a single 2D fluoroscopic image of Abdominal Aortic Aneurysm (AAA) is introduced. A graph matching method is proposed to establish the correspondence between the 3D preoperative and 2D intra-operative AAA skeletons, and then the two skeletons are registered by skeleton deformation and regularization in respect to skeleton length and smoothness. Furthermore, deep learning was used to segment 3D pre-operative AAA from Computed Tomography (CT) scans to facilitate the framework automation. Simulation, phantom and patient AAA data sets have been used to validate the proposed framework. 3D distance error of 2mm was achieved in the phantom setup. Performance advantages were also achieved in terms of accuracy, robustness and time-efficiency. All the code will be open source.



### Robust Adversarial Perturbation on Deep Proposal-based Models
- **Arxiv ID**: http://arxiv.org/abs/1809.05962v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05962v2)
- **Published**: 2018-09-16 21:49:14+00:00
- **Updated**: 2019-11-03 15:18:03+00:00
- **Authors**: Yuezun Li, Daniel Tian, Ming-Ching Chang, Xiao Bian, Siwei Lyu
- **Comment**: To appear in BMVC 2018
- **Journal**: None
- **Summary**: Adversarial noises are useful tools to probe the weakness of deep learning based computer vision algorithms. In this paper, we describe a robust adversarial perturbation (R-AP) method to attack deep proposal-based object detectors and instance segmentation algorithms. Our method focuses on attacking the common component in these algorithms, namely Region Proposal Network (RPN), to universally degrade their performance in a black-box fashion. To do so, we design a loss function that combines a label loss and a novel shape loss, and optimize it with respect to image using a gradient based iterative algorithm. Evaluations are performed on the MS COCO 2014 dataset for the adversarial attacking of 6 state-of-the-art object detectors and 2 instance segmentation algorithms. Experimental results demonstrate the efficacy of the proposed method.



### Exploring the Vulnerability of Single Shot Module in Object Detectors via Imperceptible Background Patches
- **Arxiv ID**: http://arxiv.org/abs/1809.05966v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.05966v3)
- **Published**: 2018-09-16 22:06:17+00:00
- **Updated**: 2019-07-02 01:38:04+00:00
- **Authors**: Yuezun Li, Xiao Bian, Ming-ching Chang, Siwei Lyu
- **Comment**: To appear in BMVC 2019
- **Journal**: None
- **Summary**: Recent works succeeded to generate adversarial perturbations on the entire image or the object of interests to corrupt CNN based object detectors. In this paper, we focus on exploring the vulnerability of the Single Shot Module (SSM) commonly used in recent object detectors, by adding small perturbations to patches in the background outside the object. The SSM is referred to the Region Proposal Network used in a two-stage object detector or the single-stage object detector itself. The SSM is typically a fully convolutional neural network which generates output in a single forward pass. Due to the excessive convolutions used in SSM, the actual receptive field is larger than the object itself. As such, we propose a novel method to corrupt object detectors by generating imperceptible patches only in the background. Our method can find a few background patches for perturbation, which can effectively decrease true positives and dramatically increase false positives. Efficacy is demonstrated on 5 two-stage object detectors and 8 single-stage object detectors on the MS COCO 2014 dataset. Results indicate that perturbations with small distortions outside the bounding box of object region can still severely damage the detection performance.



