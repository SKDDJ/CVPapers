# Arxiv Papers in cs.CV on 2018-09-18
### Scene Text Recognition from Two-Dimensional Perspective
- **Arxiv ID**: http://arxiv.org/abs/1809.06508v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06508v2)
- **Published**: 2018-09-18 02:34:04+00:00
- **Updated**: 2018-11-17 06:26:25+00:00
- **Authors**: Minghui Liao, Jian Zhang, Zhaoyi Wan, Fengming Xie, Jiajun Liang, Pengyuan Lyu, Cong Yao, Xiang Bai
- **Comment**: To appear in AAAI 2019
- **Journal**: None
- **Summary**: Inspired by speech recognition, recent state-of-the-art algorithms mostly consider scene text recognition as a sequence prediction problem. Though achieving excellent performance, these methods usually neglect an important fact that text in images are actually distributed in two-dimensional space. It is a nature quite different from that of speech, which is essentially a one-dimensional signal. In principle, directly compressing features of text into a one-dimensional form may lose useful information and introduce extra noise. In this paper, we approach scene text recognition from a two-dimensional perspective. A simple yet effective model, called Character Attention Fully Convolutional Network (CA-FCN), is devised for recognizing the text of arbitrary shapes. Scene text recognition is realized with a semantic segmentation network, where an attention mechanism for characters is adopted. Combined with a word formation module, CA-FCN can simultaneously recognize the script and predict the position of each character. Experiments demonstrate that the proposed algorithm outperforms previous methods on both regular and irregular text datasets. Moreover, it is proven to be more robust to imprecise localizations in the text detection phase, which are very common in practice.



### Multimodal Trajectory Predictions for Autonomous Driving using Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.10732v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.10732v2)
- **Published**: 2018-09-18 04:07:13+00:00
- **Updated**: 2019-03-01 14:07:02+00:00
- **Authors**: Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou, Tsung-Han Lin, Thi Nguyen, Tzu-Kuo Huang, Jeff Schneider, Nemanja Djuric
- **Comment**: Accepted for publication at IEEE International Conference on Robotics
  and Automation (ICRA) 2019
- **Journal**: None
- **Summary**: Autonomous driving presents one of the largest problems that the robotics and artificial intelligence communities are facing at the moment, both in terms of difficulty and potential societal impact. Self-driving vehicles (SDVs) are expected to prevent road accidents and save millions of lives while improving the livelihood and life quality of many more. However, despite large interest and a number of industry players working in the autonomous domain, there still remains more to be done in order to develop a system capable of operating at a level comparable to best human drivers. One reason for this is high uncertainty of traffic behavior and large number of situations that an SDV may encounter on the roads, making it very difficult to create a fully generalizable system. To ensure safe and efficient operations, an autonomous vehicle is required to account for this uncertainty and to anticipate a multitude of possible behaviors of traffic actors in its surrounding. We address this critical problem and present a method to predict multiple possible trajectories of actors while also estimating their probabilities. The method encodes each actor's surrounding context into a raster image, used as input by deep convolutional networks to automatically derive relevant features for the task. Following extensive offline evaluation and comparison to state-of-the-art baselines, the method was successfully tested on SDVs in closed-course tests.



### Deep Textured 3D Reconstruction of Human Bodies
- **Arxiv ID**: http://arxiv.org/abs/1809.06547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06547v1)
- **Published**: 2018-09-18 06:19:55+00:00
- **Updated**: 2018-09-18 06:19:55+00:00
- **Authors**: Abbhinav Venkat, Sai Sagar Jinka, Avinash Sharma
- **Comment**: None
- **Journal**: BRITISH MACHINE VISION CONFERENCE (BMVC), 2018
- **Summary**: Recovering textured 3D models of non-rigid human body shapes is challenging due to self-occlusions caused by complex body poses and shapes, clothing obstructions, lack of surface texture, background clutter, sparse set of cameras with non-overlapping fields of view, etc. Further, a calibration-free environment adds additional complexity to both - reconstruction and texture recovery. In this paper, we propose a deep learning based solution for textured 3D reconstruction of human body shapes from a single view RGB image. This is achieved by first recovering the volumetric grid of the non-rigid human body given a single view RGB image followed by orthographic texture view synthesis using the respective depth projection of the reconstructed (volumetric) shape and input RGB image. We propose to co-learn the depth information readily available with affordable RGBD sensors (e.g., Kinect) while showing multiple views of the same object during the training phase. We show superior reconstruction performance in terms of quantitative and qualitative results, on both, publicly available datasets (by simulating the depth channel with virtual Kinect) as well as real RGBD data collected with our calibrated multi Kinect setup.



### Image Super-Resolution via Deterministic-Stochastic Synthesis and Local Statistical Rectification
- **Arxiv ID**: http://arxiv.org/abs/1809.06557v1
- **DOI**: 10.1145/3272127.3275060
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1809.06557v1)
- **Published**: 2018-09-18 06:50:56+00:00
- **Updated**: 2018-09-18 06:50:56+00:00
- **Authors**: Weifeng Ge, Bingchen Gong, Yizhou Yu
- **Comment**: to appear in SIGGRAPH Asia 2018
- **Journal**: None
- **Summary**: Single image superresolution has been a popular research topic in the last two decades and has recently received a new wave of interest due to deep neural networks. In this paper, we approach this problem from a different perspective. With respect to a downsampled low resolution image, we model a high resolution image as a combination of two components, a deterministic component and a stochastic component. The deterministic component can be recovered from the low-frequency signals in the downsampled image. The stochastic component, on the other hand, contains the signals that have little correlation with the low resolution image. We adopt two complementary methods for generating these two components. While generative adversarial networks are used for the stochastic component, deterministic component reconstruction is formulated as a regression problem solved using deep neural networks. Since the deterministic component exhibits clearer local orientations, we design novel loss functions tailored for such properties for training the deep regression network. These two methods are first applied to the entire input image to produce two distinct high-resolution images. Afterwards, these two images are fused together using another deep neural network that also performs local statistical rectification, which tries to make the local statistics of the fused image match the same local statistics of the groundtruth image. Quantitative results and a user study indicate that the proposed method outperforms existing state-of-the-art algorithms with a clear margin.



### U-Net for MAV-based Penstock Inspection: an Investigation of Focal Loss in Multi-class Segmentation for Corrosion Identification
- **Arxiv ID**: http://arxiv.org/abs/1809.06576v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06576v1)
- **Published**: 2018-09-18 08:04:42+00:00
- **Updated**: 2018-09-18 08:04:42+00:00
- **Authors**: Ty Nguyen, Tolga Ozaslan, Ian D. Miller, James Keller, Giuseppe Loianno, Camillo J. Taylor, Daniel D. Lee, Vijay Kumar, Joseph H. Harwood, Jennifer Wozencraft
- **Comment**: 8 Pages, 4 figures
- **Journal**: None
- **Summary**: Periodical inspection and maintenance of critical infrastructure such as dams, penstocks, and locks are of significant importance to prevent catastrophic failures. Conventional manual inspection methods require inspectors to climb along a penstock to spot corrosion, rust and crack formation which is unsafe, labor-intensive, and requires intensive training. This work presents an alternative approach using a Micro Aerial Vehicle (MAV) that autonomously flies to collect imagery which is then fed into a pretrained deep-learning model to identify corrosion. Our simplified U-Net trained with less than 40 image samples can do inference at 12 fps on a single GPU. We analyze different loss functions to solve the class imbalance problem, followed by a discussion on choosing proper metrics and weights for object classes. Results obtained with the dataset collected from Center Hill Dam, TN show that focal loss function, combined with a proper set of class weights yield better segmentation results than the base loss, Softmax cross entropy. Our method can be used in combination with planning algorithm to offer a complete, safe and cost-efficient solution to autonomous infrastructure inspection.



### Symbolic Tensor Neural Networks for Digital Media - from Tensor Processing via BNF Graph Rules to CREAMS Applications
- **Arxiv ID**: http://arxiv.org/abs/1809.06582v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1809.06582v2)
- **Published**: 2018-09-18 08:25:01+00:00
- **Updated**: 2018-12-12 09:16:30+00:00
- **Authors**: Wladyslaw Skarbek
- **Comment**: None
- **Journal**: None
- **Summary**: This tutorial material on Convolutional Neural Networks (CNN) and its applications in digital media research is based on the concept of Symbolic Tensor Neural Networks. The set of STNN expressions is specified in Backus-Naur Form (BNF) which is annotated by constraints typical for labeled acyclic directed graphs (DAG). The BNF induction begins from a collection of neural unit symbols with extra (up to five) decoration fields (including tensor depth and sharing fields). The inductive rules provide not only the general graph structure but also the specific shortcuts for residual blocks of units. A syntactic mechanism for network fragments modularization is introduced via user defined units and their instances. Moreover, the dual BNF rules are specified in order to generate the Dual Symbolic Tensor Neural Network (DSTNN). The joined interpretation of STNN and DSTNN provides the correct flow of gradient tensors, back propagated at the training stage. The proposed symbolic representation of CNNs is illustrated for six generic digital media applications (CREAMS): Compression, Recognition, Embedding, Annotation, 3D Modeling for human-computer interfacing, and data Security based on digital media objects. In order to make the CNN description and its gradient flow complete, for all presented applications, the symbolic representations of mathematically defined loss/gain functions and gradient flow equations for all used core units, are given. The tutorial is to convince the reader that STNN is not only a convenient symbolic notation for public presentations of CNN based solutions for CREAMS problems but also that it is a design blueprint with a potential for automatic generation of application source code.



### Enhanced 3DTV Regularization and Its Applications on Hyper-spectral Image Denoising and Compressed Sensing
- **Arxiv ID**: http://arxiv.org/abs/1809.06591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06591v1)
- **Published**: 2018-09-18 08:35:18+00:00
- **Updated**: 2018-09-18 08:35:18+00:00
- **Authors**: Jiangjun Peng, Qi Xie, Qian Zhao, Yao Wang, Deyu Meng, Yee Leung
- **Comment**: 25 pages, 27 figures, 3 tables, machine learning, regularization
- **Journal**: None
- **Summary**: The 3-D total variation (3DTV) is a powerful regularization term, which encodes the local smoothness prior structure underlying a hyper-spectral image (HSI), for general HSI processing tasks. This term is calculated by assuming identical and independent sparsity structures on all bands of gradient maps calculated along spatial and spectral HSI modes. This, however, is always largely deviated from the real cases, where the gradient maps are generally with different while correlated sparsity structures across all their bands. Such deviation tends to hamper the performance of the related method by adopting such prior term. To this is- sue, this paper proposes an enhanced 3DTV (E-3DTV) regularization term beyond conventional 3DTV. Instead of imposing sparsity on gradient maps themselves, the new term calculated sparsity on the subspace bases on the gradient maps along their bands, which naturally encode the correlation and difference across these bands, and more faithfully reflect the insightful configurations of an HSI. The E-3DTV term can easily replace the previous 3DTV term and be em- bedded into an HSI processing model to ameliorate its performance. The superiority of the proposed methods is substantiated by extensive experiments on two typical related tasks: HSI denoising and compressed sensing, as compared with state-of-the-arts designed for both tasks.



### Attribute-aware Face Aging with Wavelet-based Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.06647v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06647v3)
- **Published**: 2018-09-18 11:23:12+00:00
- **Updated**: 2019-04-15 10:52:44+00:00
- **Authors**: Yunfan Liu, Qi Li, Zhenan Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Since it is difficult to collect face images of the same subject over a long range of age span, most existing face aging methods resort to unpaired datasets to learn age mappings. However, the matching ambiguity between young and aged face images inherent to unpaired training data may lead to unnatural changes of facial attributes during the aging process, which could not be solved by only enforcing identity consistency like most existing studies do. In this paper, we propose a attribute-aware face aging model with wavelet-based Generative Adversarial Networks (GANs) to address the above issues. To be specific, we embed facial attribute vectors into both generator and discriminator of the model to encourage each synthesized elderly face image to be faithful to the attribute of its corresponding input. In addition, a wavelet packet transform (WPT) module is incorporated to improve the visual fidelity of generated images by capturing age-related texture details at multiple scales in the frequency space. Qualitative results demonstrate the ability of our model to synthesize visually plausible face images, and extensive quantitative evaluation results show that the proposed method achieves state-of-the-art performance on existing datasets.



### Support Vector Machine (SVM) Recognition Approach adapted to Individual and Touching Moths Counting in Trap Images
- **Arxiv ID**: http://arxiv.org/abs/1809.06663v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06663v1)
- **Published**: 2018-09-18 12:19:01+00:00
- **Updated**: 2018-09-18 12:19:01+00:00
- **Authors**: Mohamed Chafik Bakkay, Sylvie Chambon, Hatem A. Rashwan, Christian Lubat, Sébastien Barsotti
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims at developing an automatic algorithm for moth recognition from trap images in real-world conditions. This method uses our previous work for detection [1] and introduces an adapted classification step. More precisely, SVM classifier is trained with a multi-scale descriptor, Histogram Of Curviness Saliency (HCS). This descriptor is robust to illumination changes and is able to detect and to describe the external and the internal contours of the target insect in multi-scale. The proposed classification method can be trained with a small set of images. Quantitative evaluations show that the proposed method is able to classify insects with higher accuracy (rate of 95.8%) than the state-of-the art approaches.



### A Simple Approach to Intrinsic Correspondence Learning on Unstructured 3D Meshes
- **Arxiv ID**: http://arxiv.org/abs/1809.06664v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1809.06664v2)
- **Published**: 2018-09-18 12:19:20+00:00
- **Updated**: 2018-09-26 19:16:19+00:00
- **Authors**: Isaak Lim, Alexander Dielen, Marcel Campen, Leif Kobbelt
- **Comment**: Presented at the ECCV workshop on Geometry meets Deep Learning
- **Journal**: None
- **Summary**: The question of representation of 3D geometry is of vital importance when it comes to leveraging the recent advances in the field of machine learning for geometry processing tasks. For common unstructured surface meshes state-of-the-art methods rely on patch-based or mapping-based techniques that introduce resampling operations in order to encode neighborhood information in a structured and regular manner. We investigate whether such resampling can be avoided, and propose a simple and direct encoding approach. It does not only increase processing efficiency due to its simplicity - its direct nature also avoids any loss in data fidelity. To evaluate the proposed method, we perform a number of experiments in the challenging domain of intrinsic, non-rigid shape correspondence estimation. In comparisons to current methods we observe that our approach is able to achieve highly competitive results.



### Compressed Sensing Parallel MRI with Adaptive Shrinkage TV Regularization
- **Arxiv ID**: http://arxiv.org/abs/1809.06665v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06665v1)
- **Published**: 2018-09-18 12:23:55+00:00
- **Updated**: 2018-09-18 12:23:55+00:00
- **Authors**: Raji Susan Mathew, Joseph Suresh Paul
- **Comment**: 27 pages,9 figures
- **Journal**: None
- **Summary**: Compressed sensing (CS) methods in magnetic resonance imaging (MRI) offer rapid acquisition and improved image quality but require iterative reconstruction schemes with regularization to enforce sparsity. Regardless of the difficulty in obtaining a fast numerical solution, the total variation (TV) regularization is a preferred choice due to its edge-preserving and structure recovery capabilities. While many approaches have been proposed to overcome the non-differentiability of the TV cost term, an iterative shrinkage based formulation allows recovering an image through recursive application of linear filtering and soft thresholding. However, providing an optimal setting for the regularization parameter is critical due to its direct impact on the rate of convergence as well as steady state error. In this paper, a regularizer adaptively varying in the derivative space is proposed, that follows the generalized discrepancy principle (GDP). The implementation proceeds by adaptively reducing the discrepancy level expressed as the absolute difference between TV norms of the consistency error and the sparse approximation error. A criterion based on the absolute difference between TV norms of consistency and sparse approximation errors is used to update the threshold. Application of the adaptive shrinkage TV regularizer to CS recovery of parallel MRI (pMRI) and temporal gradient adaptation in dynamic MRI are shown to result in improved image quality with accelerated convergence. In addition, the adaptive TV-based iterative shrinkage (ATVIS) provides a significant speed advantage over the fast iterative shrinkage-thresholding algorithm (FISTA).



### Adding Cues to Binary Feature Descriptors for Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.06690v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.06690v1)
- **Published**: 2018-09-18 13:19:33+00:00
- **Updated**: 2018-09-18 13:19:33+00:00
- **Authors**: Dominik Schlegel, Giorgio Grisetti
- **Comment**: 8 pages, 8 figures, source: www.gitlab.com/srrg-software/srrg_bench,
  submitted to ICRA 2019
- **Journal**: None
- **Summary**: In this paper we propose an approach to embed continuous and selector cues in binary feature descriptors used for visual place recognition. The embedding is achieved by extending each feature descriptor with a binary string that encodes a cue and supports the Hamming distance metric. Augmenting the descriptors in such a way has the advantage of being transparent to the procedure used to compare them. We present two concrete applications of our methodology, demonstrating the two considered types of cues. In addition to that, we conducted on these applications a broad quantitative and comparative evaluation covering five benchmark datasets and several state-of-the-art image retrieval approaches in combination with various binary descriptor types.



### Multiple Combined Constraints for Image Stitching
- **Arxiv ID**: http://arxiv.org/abs/1809.06706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06706v1)
- **Published**: 2018-09-18 13:34:25+00:00
- **Updated**: 2018-09-18 13:34:25+00:00
- **Authors**: Kai Chen, Jingmin Tu, Binbin Xiang, Li Li, Jian Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Several approaches to image stitching use different constraints to estimate the motion model between image pairs. These constraints can be roughly divided into two categories: geometric constraints and photometric constraints. In this paper, geometric and photometric constraints are combined to improve the alignment quality, which is based on the observation that these two kinds of constraints are complementary. On the one hand, geometric constraints (e.g., point and line correspondences) are usually spatially biased and are insufficient in some extreme scenes, while photometric constraints are always evenly and densely distributed. On the other hand, photometric constraints are sensitive to displacements and are not suitable for images with large parallaxes, while geometric constraints are usually imposed by feature matching and are more robust to handle parallaxes. The proposed method therefore combines them together in an efficient mesh-based image warping framework. It achieves better alignment quality than methods only with geometric constraints, and can handle larger parallax than photometric-constraint-based method. Experimental results on various images illustrate that the proposed method outperforms representative state-of-the-art image stitching methods reported in the literature.



### 3D segmentation of mandible from multisectional CT scans by convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1809.06752v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06752v1)
- **Published**: 2018-09-18 14:08:05+00:00
- **Updated**: 2018-09-18 14:08:05+00:00
- **Authors**: Bingjiang Qiu, Jiapan Guo, J. Kraeima, R. J. H. Borra, M. J. H. Witjes, P. M. A. van Ooijen
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation of mandibles in CT scans during virtual surgical planning is crucial for 3D surgical planning in order to obtain a detailed surface representation of the patients bone. Automatic segmentation of mandibles in CT scans is a challenging task due to large variation in their shape and size between individuals. In order to address this challenge we propose a convolutional neural network approach for mandible segmentation in CT scans by considering the continuum of anatomical structures through different planes. The proposed convolutional neural network adopts the architecture of the U-Net and then combines the resulting 2D segmentations from three different planes into a 3D segmentation. We implement such a segmentation approach on 11 neck CT scans and then evaluate the performance. We achieve an average dice coefficient of $ 0.89 $ on two testing mandible segmentation. Experimental results show that our proposed approach for mandible segmentation in CT scans exhibits high accuracy.



### Generalized Content-Preserving Warps for Image Stitching
- **Arxiv ID**: http://arxiv.org/abs/1809.06783v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06783v1)
- **Published**: 2018-09-18 14:59:50+00:00
- **Updated**: 2018-09-18 14:59:50+00:00
- **Authors**: Kai Chen, Jingmin Tu, Jian Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Local misalignment caused by global homography is a common issue in image stitching task. Content-Preserving Warping (CPW) is a typical method to deal with this issue, in which geometric and photometric constraints are imposed to guide the warping process. One of its essential condition however, is colour consistency, and an elusive goal in real world applications. In this paper, we propose a Generalized Content-Preserving Warping (GCPW) method to alleviate this problem. GCPW extends the original CPW by applying a colour model that expresses the colour transformation between images locally, thus meeting the photometric constraint requirements for effective image stitching. We combine the photometric and geometric constraints and jointly estimate the colour transformation and the warped mesh vertexes, simultaneously. We align images locally with an optimal grid mesh generated by our GCPW method. Experiments on both synthetic and real images demonstrate that our new method is robust to colour variations, outperforming other state-of-the-art CPW-based image stitching methods.



### Albumentations: fast and flexible image augmentations
- **Arxiv ID**: http://arxiv.org/abs/1809.06839v1
- **DOI**: 10.3390/info11020125
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06839v1)
- **Published**: 2018-09-18 17:28:08+00:00
- **Updated**: 2018-09-18 17:28:08+00:00
- **Authors**: Alexander Buslaev, Alex Parinov, Eugene Khvedchenya, Vladimir I. Iglovikov, Alexandr A. Kalinin
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation is a commonly used technique for increasing both the size and the diversity of labeled training sets by leveraging input transformations that preserve output labels. In computer vision domain, image augmentations have become a common implicit regularization technique to combat overfitting in deep convolutional neural networks and are ubiquitously used to improve performance. While most deep learning frameworks implement basic image transformations, the list is typically limited to some variations and combinations of flipping, rotating, scaling, and cropping. Moreover, the image processing speed varies in existing tools for image augmentation. We present Albumentations, a fast and flexible library for image augmentations with many various image transform operations available, that is also an easy-to-use wrapper around other augmentation libraries. We provide examples of image augmentations for different computer vision tasks and show that Albumentations is faster than other commonly used image augmentation tools on the most of commonly used image transformations. The source code for Albumentations is made publicly available online at https://github.com/albu/albumentations



### MNIST Dataset Classification Utilizing k-NN Classifier with Modified Sliding-window Metric
- **Arxiv ID**: http://arxiv.org/abs/1809.06846v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06846v4)
- **Published**: 2018-09-18 17:55:53+00:00
- **Updated**: 2019-03-12 14:09:28+00:00
- **Authors**: Divas Grover, Behrad Toghi
- **Comment**: Accepted to the Computer Vision Conference (CVC2019), Las Vegas, NV
- **Journal**: None
- **Summary**: The MNIST dataset of the handwritten digits is known as one of the commonly used datasets for machine learning and computer vision research. We aim to study a widely applicable classification problem and apply a simple yet efficient K-nearest neighbor classifier with an enhanced heuristic. We evaluate the performance of the K-nearest neighbor classification algorithm on the MNIST dataset where the $L2$ Euclidean distance metric is compared to a modified distance metric which utilizes the sliding window technique in order to avoid performance degradation due to slight spatial misalignments. The accuracy metric and confusion matrices are used as the performance indicators to compare the performance of the baseline algorithm versus the enhanced sliding window method and results show significant improvement using this proposed method.



### SilhoNet: An RGB Method for 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1809.06893v4
- **DOI**: 10.1109/LRA.2019.2928776
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1809.06893v4)
- **Published**: 2018-09-18 19:15:38+00:00
- **Updated**: 2020-05-07 17:27:25+00:00
- **Authors**: Gideon Billings, Matthew Johnson-Roberson
- **Comment**: 8 pages, 3 figures
- **Journal**: IEEE Robotics and Automation Letters 4.4 (2019): 3727-3734
- **Summary**: Autonomous robot manipulation involves estimating the translation and orientation of the object to be manipulated as a 6-degree-of-freedom (6D) pose. Methods using RGB-D data have shown great success in solving this problem. However, there are situations where cost constraints or the working environment may limit the use of RGB-D sensors. When limited to monocular camera data only, the problem of object pose estimation is very challenging. In this work, we introduce a novel method called SilhoNet that predicts 6D object pose from monocular images. We use a Convolutional Neural Network (CNN) pipeline that takes in Region of Interest (ROI) proposals to simultaneously predict an intermediate silhouette representation for objects with an associated occlusion mask and a 3D translation vector. The 3D orientation is then regressed from the predicted silhouettes. We show that our method achieves better overall performance on the YCB-Video dataset than two state-of-the art networks for 6D pose estimation from monocular image input.



### Language Identification with Deep Bottleneck Features
- **Arxiv ID**: http://arxiv.org/abs/1809.08909v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1809.08909v2)
- **Published**: 2018-09-18 19:34:54+00:00
- **Updated**: 2020-02-02 09:57:14+00:00
- **Authors**: Zhanyu Ma, Hong Yu
- **Comment**: Preliminary work report
- **Journal**: None
- **Summary**: In this paper we proposed an end-to-end short utterances speech language identification(SLD) approach based on a Long Short Term Memory (LSTM) neural network which is special suitable for SLD application in intelligent vehicles. Features used for LSTM learning are generated by a transfer learning method. Bottle-neck features of a deep neural network (DNN) which are trained for mandarin acoustic-phonetic classification are used for LSTM training. In order to improve the SLD accuracy of short utterances a phase vocoder based time-scale modification(TSM) method is used to reduce and increase speech rated of the test utterance. By splicing the normal, speech rate reduced and increased utterances, we can extend length of test utterances so as to improved improved the performance of the SLD system. The experimental results on AP17-OLR database shows that the proposed methods can improve the performance of SLD, especially on short utterance with 1s and 3s durations.



### A Study on Deep Learning Based Sauvegrain Method for Measurement of Puberty Bone Age
- **Arxiv ID**: http://arxiv.org/abs/1809.06965v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.06965v1)
- **Published**: 2018-09-18 23:47:08+00:00
- **Updated**: 2018-09-18 23:47:08+00:00
- **Authors**: Seung Bin Baik, Keum Gang Cha
- **Comment**: 5 pages, 6 figures, 1 table
- **Journal**: None
- **Summary**: This study applies a technique to expand the number of images to a level that allows deep learning. And the applicability of the Sauvegrain method through deep learning with relatively few elbow X-rays is studied. The study was composed of processes similar to the physicians' bone age assessment procedures. The selected reference images were learned without being included in the evaluation data, and at the same time, the data was extended to accommodate the number of cases. In addition, we adjusted the X-ray images to better images using U-Net and selected the ROI with RPN + so as to be able to perform bone age estimation through CNN. The mean absolute error of the Sauvegrain method based on deep learning is 2.8 months and the Mean Absolute Percentage Error (MAPE) is 0.018. This result shows that X - ray analysis using the Sauvegrain method shows higher accuracy than that of the age group of puberty even in the deep learning base. This means that deep learning of the Suvegrain method can be measured at a level similar to that of an expert, based on the extended X-ray image with the image data extension technique. Finally, we applied the Sauvegrain method to deep learning for accurate measurement of bone age at puberty. As a result, the present study is based on deep learning, and compared with the evaluation results of experts, it is possible to overcome limitations of the method of measuring bone age based on machine learning which was in TW3 or Greulich & Pyle due to lack of X- I confirmed the fact. And we also presented the Sauvegrain method, which is applicable to adolescents as well.



