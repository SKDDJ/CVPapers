# Arxiv Papers in cs.CV on 2018-09-20
### Multispecies fruit flower detection using a refined semantic segmentation network
- **Arxiv ID**: http://arxiv.org/abs/1809.10080v1
- **DOI**: 10.1109/LRA.2018.2849498
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1809.10080v1)
- **Published**: 2018-09-20 00:08:42+00:00
- **Updated**: 2018-09-20 00:08:42+00:00
- **Authors**: Philipe A. Dias, Amy Tabb, Henry Medeiros
- **Comment**: 8 pages
- **Journal**: IEEE Robotics and Automation Letters, vol. 3, no. 4, pp.
  3003-3010, Oct. 2018
- **Summary**: In fruit production, critical crop management decisions are guided by bloom intensity, i.e., the number of flowers present in an orchard. Despite its importance, bloom intensity is still typically estimated by means of human visual inspection. Existing automated computer vision systems for flower identification are based on hand-engineered techniques that work only under specific conditions and with limited performance. This work proposes an automated technique for flower identification that is robust to uncontrolled environments and applicable to different flower species. Our method relies on an end-to-end residual convolutional neural network (CNN) that represents the state-of-the-art in semantic segmentation. To enhance its sensitivity to flowers, we fine-tune this network using a single dataset of apple flower images. Since CNNs tend to produce coarse segmentations, we employ a refinement method to better distinguish between individual flower instances. Without any pre-processing or dataset-specific training, experimental results on images of apple, peach and pear flowers, acquired under different conditions demonstrate the robustness and broad applicability of our method.



### Deep Generative Classifiers for Thoracic Disease Diagnosis with Chest X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/1809.07436v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.07436v2)
- **Published**: 2018-09-20 00:13:50+00:00
- **Updated**: 2018-11-08 18:41:48+00:00
- **Authors**: Chengsheng Mao, Yiheng Pan, Zexian Zeng, Liang Yao, Yuan Luo
- **Comment**: BIBM 2018 accepted
- **Journal**: None
- **Summary**: Thoracic diseases are very serious health problems that plague a large number of people. Chest X-ray is currently one of the most popular methods to diagnose thoracic diseases, playing an important role in the healthcare workflow. However, reading the chest X-ray images and giving an accurate diagnosis remain challenging tasks for expert radiologists. With the success of deep learning in computer vision, a growing number of deep neural network architectures were applied to chest X-ray image classification. However, most of the previous deep neural network classifiers were based on deterministic architectures which are usually very noise-sensitive and are likely to aggravate the overfitting issue. In this paper, to make a deep architecture more robust to noise and to reduce overfitting, we propose using deep generative classifiers to automatically diagnose thorax diseases from the chest X-ray images. Unlike the traditional deterministic classifier, a deep generative classifier has a distribution middle layer in the deep neural network. A sampling layer then draws a random sample from the distribution layer and input it to the following layer for classification. The classifier is generative because the class label is generated from samples of a related distribution. Through training the model with a certain amount of randomness, the deep generative classifiers are expected to be robust to noise and can reduce overfitting and then achieve good performances. We implemented our deep generative classifiers based on a number of well-known deterministic neural network architectures, and tested our models on the chest X-ray14 dataset. The results demonstrated the superiority of deep generative classifiers compared with the corresponding deep deterministic classifiers.



### A Coupled Evolutionary Network for Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/1809.07447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07447v1)
- **Published**: 2018-09-20 01:55:56+00:00
- **Updated**: 2018-09-20 01:55:56+00:00
- **Authors**: Peipei Li, Yibo Hu, Ran He, Zhenan Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Age estimation of unknown persons is a challenging pattern analysis task due to the lacking of training data and various aging mechanisms for different people. Label distribution learning-based methods usually make distribution assumptions to simplify age estimation. However, age label distributions are often complex and difficult to be modeled in a parameter way. Inspired by the biological evolutionary mechanism, we propose a Coupled Evolutionary Network (CEN) with two concurrent evolutionary processes: evolutionary label distribution learning and evolutionary slack regression. Evolutionary network learns and refines age label distributions in an iteratively learning way. Evolutionary label distribution learning adaptively learns and constantly refines the age label distributions without making strong assumptions on the distribution patterns. To further utilize the ordered and continuous information of age labels, we accordingly propose an evolutionary slack regression to convert the discrete age label regression into the continuous age interval regression. Experimental results on Morph, ChaLearn15 and MegaAge-Asian datasets show the superiority of our method.



### Global and Local Consistent Wavelet-domain Age Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1809.07764v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07764v2)
- **Published**: 2018-09-20 02:44:58+00:00
- **Updated**: 2019-04-01 01:10:24+00:00
- **Authors**: Peipei Li, Yibo Hu, Ran He, Zhenan Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Age synthesis is a challenging task due to the complicated and non-linear transformation in human aging process. Aging information is usually reflected in local facial parts, such as wrinkles at the eye corners. However, these local facial parts contribute less in previous GAN based methods for age synthesis. To address this issue, we propose a Wavelet-domain Global and Local Consistent Age Generative Adversarial Network (WaveletGLCA-GAN), in which one global specific network and three local specific networks are integrated together to capture both global topology information and local texture details of human faces. Different from the most existing methods that modeling age synthesis in image-domain, we adopt wavelet transform to depict the textual information in frequency-domain. %Moreover, to achieve accurate age generation under the premise of preserving the identity information, age estimation network and face verification network are employed. Moreover, five types of losses are adopted: 1) adversarial loss aims to generate realistic wavelets; 2) identity preserving loss aims to better preserve identity information; 3) age preserving loss aims to enhance the accuracy of age synthesis; 4) pixel-wise loss aims to preserve the background information of the input face; 5) the total variation regularization aims to remove ghosting artifacts. Our method is evaluated on three face aging datasets, including CACD2000, Morph and FG-NET. Qualitative and quantitative experiments show the superiority of the proposed method over other state-of-the-arts.



### Towards Discrete Solution: A Sparse Preserving Method for Correspondence Problem
- **Arxiv ID**: http://arxiv.org/abs/1809.07456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07456v1)
- **Published**: 2018-09-20 02:46:40+00:00
- **Updated**: 2018-09-20 02:46:40+00:00
- **Authors**: Bo Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Many problems of interest in computer vision can be formulated as a problem of finding consistent correspondences between two feature sets. Feature correspondence (matching) problem with one-to-one mapping constraint is usually formulated as an Integral Quadratic Programming (IQP) problem with permutation (or orthogonal) constraint. Since it is NP-hard, relaxation models are required. One main challenge for optimizing IQP matching problem is how to incorporate the discrete one-to-one mapping (permutation) constraint in its quadratic objective optimization. In this paper, we present a new relaxation model, called Sparse Constraint Preserving Matching (SPM), for IQP matching problem. SPM is motivated by our observation that the discrete permutation constraint can be well encoded via a sparse constraint. Comparing with traditional relaxation models, SPM can incorporate the discrete one-to-one mapping constraint straightly via a sparse constraint and thus provides a tighter relaxation for original IQP matching problem. A simple yet effective update algorithm has been derived to solve the proposed SPM model. Experimental results on several feature matching tasks demonstrate the effectiveness and efficiency of SPM method.



### Recent progress in semantic image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1809.10198v1
- **DOI**: 10.1007/s10462-018-9641-3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.10198v1)
- **Published**: 2018-09-20 03:20:42+00:00
- **Updated**: 2018-09-20 03:20:42+00:00
- **Authors**: Xiaolong Liu, Zhidong Deng, Yuhan Yang
- **Comment**: Pubulished at Artificial Intelligence review
- **Journal**: Liu, Xiaolong, Zhidong Deng, and Yuhan Yang. "Recent progress in
  semantic image segmentation." Artificial Intelligence Review (2018): 1-18
- **Summary**: Semantic image segmentation, which becomes one of the key applications in image processing and computer vision domain, has been used in multiple domains such as medical area and intelligent transportation. Lots of benchmark datasets are released for researchers to verify their algorithms. Semantic segmentation has been studied for many years. Since the emergence of Deep Neural Network (DNN), segmentation has made a tremendous progress. In this paper, we divide semantic image segmentation methods into two categories: traditional and recent DNN method. Firstly, we briefly summarize the traditional method as well as datasets released for segmentation, then we comprehensively investigate recent methods based on DNN which are described in the eight aspects: fully convolutional network, upsample ways, FCN joint with CRF methods, dilated convolution approaches, progresses in backbone network, pyramid methods, Multi-level feature and multi-stage method, supervised, weakly-supervised and unsupervised methods. Finally, a conclusion in this area is drawn.



### Generic Vehicle Tracking Framework Capable of Handling Occlusions Based on Modified Mixture Particle Filter
- **Arxiv ID**: http://arxiv.org/abs/1809.10237v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.10237v1)
- **Published**: 2018-09-20 05:27:47+00:00
- **Updated**: 2018-09-20 05:27:47+00:00
- **Authors**: Jiachen Li, Wei Zhan, Masayoshi Tomizuka
- **Comment**: Presented in 2018 IEEE Intelligent Vehicles Symposium (IV)
- **Journal**: None
- **Summary**: Accurate and robust tracking of surrounding road participants plays an important role in autonomous driving. However, there is usually no prior knowledge of the number of tracking targets due to object emergence, object disappearance and false alarms. To overcome this challenge, we propose a generic vehicle tracking framework based on modified mixture particle filter, which can make the number of tracking targets adaptive to real-time observations and track all the vehicles within sensor range simultaneously in a uniform architecture without explicit data association. Each object corresponds to a mixture component whose distribution is non-parametric and approximated by particle hypotheses. Most tracking approaches employ vehicle kinematic models as the prediction model. However, it is hard for these models to make proper predictions when sensor measurements are lost or become low quality due to partial or complete occlusions. Moreover, these models are incapable of forecasting sudden maneuvers. To address these problems, we propose to incorporate learning-based behavioral models instead of pure vehicle kinematic models to realize prediction in the prior update of recursive Bayesian state estimation. Two typical driving scenarios including lane keeping and lane change are demonstrated to verify the effectiveness and accuracy of the proposed framework as well as the advantages of employing learning-based models.



### OxIOD: The Dataset for Deep Inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/1809.07491v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.07491v1)
- **Published**: 2018-09-20 06:48:37+00:00
- **Updated**: 2018-09-20 06:48:37+00:00
- **Authors**: Changhao Chen, Peijun Zhao, Chris Xiaoxuan Lu, Wei Wang, Andrew Markham, Niki Trigoni
- **Comment**: None
- **Journal**: None
- **Summary**: Advances in micro-electro-mechanical (MEMS) techniques enable inertial measurements units (IMUs) to be small, cheap, energy efficient, and widely used in smartphones, robots, and drones. Exploiting inertial data for accurate and reliable navigation and localization has attracted significant research and industrial interest, as IMU measurements are completely ego-centric and generally environment agnostic. Recent studies have shown that the notorious issue of drift can be significantly alleviated by using deep neural networks (DNNs), e.g. IONet. However, the lack of sufficient labelled data for training and testing various architectures limits the proliferation of adopting DNNs in IMU-based tasks. In this paper, we propose and release the Oxford Inertial Odometry Dataset (OxIOD), a first-of-its-kind data collection for inertial-odometry research, with all sequences having ground-truth labels. Our dataset contains 158 sequences totalling more than 42 km in total distance, much larger than previous inertial datasets. Another notable feature of this dataset lies in its diversity, which can reflect the complex motions of phone-based IMUs in various everyday usage. The measurements were collected with four different attachments (handheld, in the pocket, in the handbag and on the trolley), four motion modes (halting, walking slowly, walking normally, and running), five different users, four types of off-the-shelf consumer phones, and large-scale localization from office buildings. Deep inertial tracking experiments were conducted to show the effectiveness of our dataset in training deep neural network models and evaluate learning-based and model-based algorithms. The OxIOD Dataset is available at: http://deepio.cs.ox.ac.uk



### Learning a Local Feature Descriptor for 3D LiDAR Scans
- **Arxiv ID**: http://arxiv.org/abs/1809.07494v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1809.07494v1)
- **Published**: 2018-09-20 06:52:16+00:00
- **Updated**: 2018-09-20 06:52:16+00:00
- **Authors**: Ayush Dewan, Tim Caselitz, Wolfram Burgard
- **Comment**: Accepted for IROS-2018. Project details and code:
  http://deep3d-descriptor.informatik.uni-freiburg.de/
- **Journal**: None
- **Summary**: Robust data association is necessary for virtually every SLAM system and finding corresponding points is typically a preprocessing step for scan alignment algorithms. Traditionally, handcrafted feature descriptors were used for these problems but recently learned descriptors have been shown to perform more robustly. In this work, we propose a local feature descriptor for 3D LiDAR scans. The descriptor is learned using a Convolutional Neural Network (CNN). Our proposed architecture consists of a Siamese network for learning a feature descriptor and a metric learning network for matching the descriptors. We also present a method for estimating local surface patches and obtaining ground-truth correspondences. In extensive experiments, we compare our learned feature descriptor with existing 3D local descriptors and report highly competitive results for multiple experiments in terms of matching accuracy and computation time. \end{abstract}



### MASON: A Model AgnoStic ObjectNess Framework
- **Arxiv ID**: http://arxiv.org/abs/1809.07499v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.07499v1)
- **Published**: 2018-09-20 07:08:38+00:00
- **Updated**: 2018-09-20 07:08:38+00:00
- **Authors**: K J Joseph, Vineeth N Balasubramanian
- **Comment**: Accepted at AutoNUE Workshop, 15th European Conference on Computer
  Vision (ECCV), September 2018, Munich, Germany
- **Journal**: None
- **Summary**: This paper proposes a simple, yet very effective method to localize dominant foreground objects in an image, to pixel-level precision. The proposed method 'MASON' (Model-AgnoStic ObjectNess) uses a deep convolutional network to generate category-independent and model-agnostic heat maps for any image. The network is not explicitly trained for the task, and hence, can be used off-the-shelf in tandem with any other network or task. We show that this framework scales to a wide variety of images, and illustrate the effectiveness of MASON in three varied application contexts.



### C4Synth: Cross-Caption Cycle-Consistent Text-to-Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1809.10238v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.10238v1)
- **Published**: 2018-09-20 07:18:57+00:00
- **Updated**: 2018-09-20 07:18:57+00:00
- **Authors**: K J Joseph, Arghya Pal, Sailaja Rajanala, Vineeth N Balasubramanian
- **Comment**: To appear in the proceedings of IEEE Winter Conference on
  Applications of Computer Vision, WACV-2019
- **Journal**: None
- **Summary**: Generating an image from its description is a challenging task worth solving because of its numerous practical applications ranging from image editing to virtual reality. All existing methods use one single caption to generate a plausible image. A single caption by itself, can be limited, and may not be able to capture the variety of concepts and behavior that may be present in the image. We propose two deep generative models that generate an image by making use of multiple captions describing it. This is achieved by ensuring 'Cross-Caption Cycle Consistency' between the multiple captions and the generated image(s). We report quantitative and qualitative results on the standard Caltech-UCSD Birds (CUB) and Oxford-102 Flowers datasets to validate the efficacy of the proposed approach.



### The 2018 PIRM Challenge on Perceptual Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1809.07517v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07517v3)
- **Published**: 2018-09-20 08:00:42+00:00
- **Updated**: 2019-01-31 07:44:49+00:00
- **Authors**: Yochai Blau, Roey Mechrez, Radu Timofte, Tomer Michaeli, Lihi Zelnik-Manor
- **Comment**: Workshop and Challenge on Perceptual Image Restoration and
  Manipulation in conjunction with ECCV 2018 webpage: https://www.pirm2018.org/
- **Journal**: Proceedings of the European Conference on Computer Vision (ECCV)
  Workshops, 2018
- **Summary**: This paper reports on the 2018 PIRM challenge on perceptual super-resolution (SR), held in conjunction with the Perceptual Image Restoration and Manipulation (PIRM) workshop at ECCV 2018. In contrast to previous SR challenges, our evaluation methodology jointly quantifies accuracy and perceptual quality, therefore enabling perceptual-driven methods to compete alongside algorithms that target PSNR maximization. Twenty-one participating teams introduced algorithms which well-improved upon the existing state-of-the-art methods in perceptual SR, as confirmed by a human opinion study. We also analyze popular image quality measures and draw conclusions regarding which of them correlates best with human opinion scores. We conclude with an analysis of the current trends in perceptual SR, as reflected from the leading submissions.



### Empty Cities: Image Inpainting for a Dynamic-Object-Invariant Space
- **Arxiv ID**: http://arxiv.org/abs/1809.10239v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.10239v2)
- **Published**: 2018-09-20 08:13:52+00:00
- **Updated**: 2019-02-15 09:36:18+00:00
- **Authors**: Berta Bescos, José Neira, Roland Siegwart, Cesar Cadena
- **Comment**: Accepted for Publication at IEEE International Conference on Robotics
  and Automation (ICRA) 2019
- **Journal**: None
- **Summary**: In this paper we present an end-to-end deep learning framework to turn images that show dynamic content, such as vehicles or pedestrians, into realistic static frames. This objective encounters two main challenges: detecting all the dynamic objects, and inpainting the static occluded background with plausible imagery. The second problem is approached with a conditional generative adversarial model that, taking as input the original dynamic image and its dynamic/static binary mask, is capable of generating the final static image. The former challenge is addressed by the use of a convolutional network that learns a multi-class semantic segmentation of the image.   These generated images can be used for applications such as augmented reality or vision-based robot localization purposes. To validate our approach, we show both qualitative and quantitative comparisons against other state-of-the-art inpainting methods by removing the dynamic objects and hallucinating the static structure behind them. Furthermore, to demonstrate the potential of our results, we carry out pilot experiments that show the benefits of our proposal for visual place recognition.



### On the self-similarity of line segments in decaying homogeneous isotropic turbulence
- **Arxiv ID**: http://arxiv.org/abs/1809.07539v1
- **DOI**: 10.1016/j.compfluid.2018.08.001
- **Categories**: **physics.flu-dyn**, cs.CV, nlin.CD
- **Links**: [PDF](http://arxiv.org/pdf/1809.07539v1)
- **Published**: 2018-09-20 09:07:29+00:00
- **Updated**: 2018-09-20 09:07:29+00:00
- **Authors**: Michael Gauding, Lipo Wang, Jens Henrik Goebbert, Mathis Bode, Luminita Danaila, Emilien Varea
- **Comment**: None
- **Journal**: None
- **Summary**: The self-similarity of a passive scalar in homogeneous isotropic decaying turbulence is investigated by the method of line segments (M. Gauding et al., Physics of Fluids 27.9 (2015): 095102). The analysis is based on a highly resolved direct numerical simulation of decaying turbulence. The method of line segments is used to perform a decomposition of the scalar field into smaller sub-units based on the extremal points of the scalar along a straight line. These sub-units (the so-called line segments) are parameterized by their length $\ell$ and the difference $\Delta\phi$ of the scalar field between the ending points. Line segments can be understood as thin local convective-diffusive structures in which diffusive processes are enhanced by compressive strain. From DNS, it is shown that the marginal distribution function of the length~$\ell$ assumes complete self-similarity when re-scaled by the mean length $\ell_m$. The joint statistics of $\Delta\phi$ and $\ell$, from which the local gradient $g=\Delta\phi/\ell$ can be defined, play an important role in understanding the turbulence mixing and flow structure. Large values of $g$ occur at a small but finite length scale. Statistics of $g$ are characterized by rare but strong deviations that exceed the standard deviation by more than one order of magnitude. It is shown that these events break complete self-similarity of line segments, which confirms the standard paradigm of turbulence that intense events (which are known as internal intermittency) are not self-similar.



### RGBD2lux: Dense light intensity estimation with an RGBD sensor
- **Arxiv ID**: http://arxiv.org/abs/1809.07558v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07558v3)
- **Published**: 2018-09-20 10:30:09+00:00
- **Updated**: 2018-12-07 16:47:08+00:00
- **Authors**: Theodore Tsesmelis, Irtiza Hasan, Marco Cristani, Fabio Galasso, Alessio Del Bue
- **Comment**: 10 pages, 9 figures, this manuscript is accepted in WACV 2019
- **Journal**: None
- **Summary**: Lighting design and modelling or industrial applications like luminaire planning and commissioning rely heavily on time consuming manual measurements or on physically coherent computational simulations. Regarding the latter,standard approaches are based on CAD modeling simulations and offline rendering, with long processing times and therefore inflexible workflows. Thus, in this paper we pro-pose a computer vision based system to measure lighting with just a single RGBD camera. The proposed method uses both depth data and images from the sensor to provide a dense measure of light intensity in the field of view of the camera. We evaluate our system on novel ground truth data and compare it to state-of-the-art commercial light-planning software. Our system provides improved performance, while being completely automated, given that the CAD model is extracted from the depth and the albedo estimated with the support of RGB images. To the best of our knowledge, this is the first automatic framework for the estimation of lighting in general indoor scenarios from RGBDinput.



### A Fast and Accurate System for Face Detection, Identification, and Verification
- **Arxiv ID**: http://arxiv.org/abs/1809.07586v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07586v1)
- **Published**: 2018-09-20 12:14:20+00:00
- **Updated**: 2018-09-20 12:14:20+00:00
- **Authors**: Rajeev Ranjan, Ankan Bansal, Jingxiao Zheng, Hongyu Xu, Joshua Gleason, Boyu Lu, Anirudh Nanduri, Jun-Cheng Chen, Carlos D. Castillo, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: The availability of large annotated datasets and affordable computation power have led to impressive improvements in the performance of CNNs on various object detection and recognition benchmarks. These, along with a better understanding of deep learning methods, have also led to improved capabilities of machine understanding of faces. CNNs are able to detect faces, locate facial landmarks, estimate pose, and recognize faces in unconstrained images and videos. In this paper, we describe the details of a deep learning pipeline for unconstrained face identification and verification which achieves state-of-the-art performance on several benchmark datasets. We propose a novel face detector, Deep Pyramid Single Shot Face Detector (DPSSD), which is fast and capable of detecting faces with large scale variations (especially tiny faces). We give design details of the various modules involved in automatic face recognition: face detection, landmark localization and alignment, and face identification/verification. We provide evaluation results of the proposed face detector on challenging unconstrained face detection datasets. Then, we present experimental results for IARPA Janus Benchmarks A, B and C (IJB-A, IJB-B, IJB-C), and the Janus Challenge Set 5 (CS5).



### DuPLO: A DUal view Point deep Learning architecture for time series classificatiOn
- **Arxiv ID**: http://arxiv.org/abs/1809.07589v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07589v1)
- **Published**: 2018-09-20 12:19:35+00:00
- **Updated**: 2018-09-20 12:19:35+00:00
- **Authors**: Roberto Interdonato, Dino Ienco, Raffaele Gaetano, Kenji Ose
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, modern Earth Observation systems continuously generate huge amounts of data. A notable example is represented by the Sentinel-2 mission, which provides images at high spatial resolution (up to 10m) with high temporal revisit period (every 5 days), which can be organized in Satellite Image Time Series (SITS). While the use of SITS has been proved to be beneficial in the context of Land Use/Land Cover (LULC) map generation, unfortunately, machine learning approaches commonly leveraged in remote sensing field fail to take advantage of spatio-temporal dependencies present in such data.   Recently, new generation deep learning methods allowed to significantly advance research in this field. These approaches have generally focused on a single type of neural network, i.e., Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), which model different but complementary information: spatial autocorrelation (CNNs) and temporal dependencies (RNNs). In this work, we propose the first deep learning architecture for the analysis of SITS data, namely \method{} (DUal view Point deep Learning architecture for time series classificatiOn), that combines Convolutional and Recurrent neural networks to exploit their complementarity. Our hypothesis is that, since CNNs and RNNs capture different aspects of the data, a combination of both models would produce a more diverse and complete representation of the information for the underlying land cover classification task. Experiments carried out on two study sites characterized by different land cover characteristics (i.e., the \textit{Gard} site in France and the \textit{Reunion Island} in the Indian Ocean), demonstrate the significance of our proposal.



### ConvPath: A Software Tool for Lung Adenocarcinoma Digital Pathological Image Analysis Aided by Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1809.10240v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.10240v1)
- **Published**: 2018-09-20 13:31:51+00:00
- **Updated**: 2018-09-20 13:31:51+00:00
- **Authors**: Shidan Wang, Tao Wang, Lin Yang, Faliu Yi, Xin Luo, Yikun Yang, Adi Gazdar, Junya Fujimoto, Ignacio I. Wistuba, Bo Yao, ShinYi Lin, Yang Xie, Yousheng Mao, Guanghua Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: The spatial distributions of different types of cells could reveal a cancer cell growth pattern, its relationships with the tumor microenvironment and the immune response of the body, all of which represent key hallmarks of cancer. However, manually recognizing and localizing all the cells in pathology slides are almost impossible. In this study, we developed an automated cell type classification pipeline, ConvPath, which includes nuclei segmentation, convolutional neural network-based tumor, stromal and lymphocytes classification, and extraction of tumor microenvironment related features for lung cancer pathology images. The overall classification accuracy is 92.9% and 90.1% in training and independent testing datasets, respectively. By identifying cells and classifying cell types, this pipeline can convert a pathology image into a spatial map of tumor, stromal and lymphocyte cells. From this spatial map, we can extracted features that characterize the tumor micro-environment. Based on these features, we developed an image feature-based prognostic model and validated the model in two independent cohorts. The predicted risk group serves as an independent prognostic factor, after adjusting for clinical variables that include age, gender, smoking status, and stage.



### Faster RER-CNN: application to the detection of vehicles in aerial images
- **Arxiv ID**: http://arxiv.org/abs/1809.07628v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07628v2)
- **Published**: 2018-09-20 13:56:20+00:00
- **Updated**: 2020-09-16 10:08:37+00:00
- **Authors**: Jean Ogier du Terrail, Frédéric Jurie
- **Comment**: technical report v2 fixes formatting issues and add acknowledgments
- **Journal**: None
- **Summary**: Detecting small vehicles in aerial images is a difficult job that can be challenging even for humans. Rotating objects, low resolution, small inter-class variability and very large images comprising complicated backgrounds render the work of photo-interpreters tedious and wearisome. Unfortunately even the best classical detection pipelines like Faster R-CNN cannot be used off-the-shelf with good results because they were built to process object centric images from day-to-day life with multi-scale vertical objects. In this work we build on the Faster R-CNN approach to turn it into a detection framework that deals appropriately with the rotation equivariance inherent to any aerial image task. This new pipeline (Faster Rotation Equivariant Regions CNN) gives, without any bells and whistles, state-of-the-art results on one of the most challenging aerial imagery datasets: VeDAI and give good results w.r.t. the baseline Faster R-CNN on two others: Munich and GoogleEarth .



### Real Time Dense Depth Estimation by Fusing Stereo with Sparse Depth Measurements
- **Arxiv ID**: http://arxiv.org/abs/1809.07677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07677v1)
- **Published**: 2018-09-20 15:39:49+00:00
- **Updated**: 2018-09-20 15:39:49+00:00
- **Authors**: Shreyas S. Shivakumar, Kartik Mohta, Bernd Pfrommer, Vijay Kumar, Camillo J. Taylor
- **Comment**: 7 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: We present an approach to depth estimation that fuses information from a stereo pair with sparse range measurements derived from a LIDAR sensor or a range camera. The goal of this work is to exploit the complementary strengths of the two sensor modalities, the accurate but sparse range measurements and the ambiguous but dense stereo information. These two sources are effectively and efficiently fused by combining ideas from anisotropic diffusion and semi-global matching.   We evaluate our approach on the KITTI 2015 and Middlebury 2014 datasets, using randomly sampled ground truth range measurements as our sparse depth input. We achieve significant performance improvements with a small fraction of range measurements on both datasets. We also provide qualitative results from our platform using the PMDTec Monstar sensor. Our entire pipeline runs on an NVIDIA TX-2 platform at 5Hz on 1280x1024 stereo images with 128 disparity levels.



### Exemplar-based synthesis of geology using kernel discrepancies and generative neural networks
- **Arxiv ID**: http://arxiv.org/abs/1809.07748v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1809.07748v2)
- **Published**: 2018-09-20 17:33:20+00:00
- **Updated**: 2018-09-21 09:31:45+00:00
- **Authors**: Shing Chan, Ahmed H. Elsheikh
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a framework for synthesis of geological images based on an exemplar image. We synthesize new realizations such that the discrepancy in the patch distribution between the realizations and the exemplar image is minimized. Such discrepancy is quantified using a kernel method for two-sample test called maximum mean discrepancy. To enable fast synthesis, we train a generative neural network in an offline phase to sample realizations efficiently during deployment, while also providing a parametrization of the synthesis process. We assess the framework on a classical binary image representing channelized subsurface reservoirs, finding that the method reproduces the visual patterns and spatial statistics (image histogram and two-point probability functions) of the exemplar image.



### Implementing Adaptive Separable Convolution for Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/1809.07759v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07759v1)
- **Published**: 2018-09-20 17:48:27+00:00
- **Updated**: 2018-09-20 17:48:27+00:00
- **Authors**: Mart Kartašev, Carlo Rapisarda, Dominik Fay
- **Comment**: All authors contributed equally
- **Journal**: None
- **Summary**: As Deep Neural Networks are becoming more popular, much of the attention is being devoted to Computer Vision problems that used to be solved with more traditional approaches. Video frame interpolation is one of such challenges that has seen new research involving various techniques in deep learning. In this paper, we replicate the work of Niklaus et al. on Adaptive Separable Convolution, which claims high quality results on the video frame interpolation task. We apply the same network structure trained on a smaller dataset and experiment with various different loss functions, in order to determine the optimal approach in data-scarce scenarios. The best resulting model is still able to provide visually pleasing videos, although achieving lower evaluation scores.



### Brain Tumor Segmentation Using Deep Learning by Type Specific Sorting of Images
- **Arxiv ID**: http://arxiv.org/abs/1809.07786v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1809.07786v1)
- **Published**: 2018-09-20 18:24:09+00:00
- **Updated**: 2018-09-20 18:24:09+00:00
- **Authors**: Zahra Sobhaninia, Safiyeh Rezaei, Alireza Noroozi, Mehdi Ahmadi, Hamidreza Zarrabi, Nader Karimi, Ali Emami, Shadrokh Samavi
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: Recently deep learning has been playing a major role in the field of computer vision. One of its applications is the reduction of human judgment in the diagnosis of diseases. Especially, brain tumor diagnosis requires high accuracy, where minute errors in judgment may lead to disaster. For this reason, brain tumor segmentation is an important challenge for medical purposes. Currently several methods exist for tumor segmentation but they all lack high accuracy. Here we present a solution for brain tumor segmenting by using deep learning. In this work, we studied different angles of brain MR images and applied different networks for segmentation. The effect of using separate networks for segmentation of MR images is evaluated by comparing the results with a single network. Experimental evaluations of the networks show that Dice score of 0.73 is achieved for a single network and 0.79 in obtained for multiple networks.



### Playing the Game of Universal Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/1809.07802v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.07802v2)
- **Published**: 2018-09-20 18:48:36+00:00
- **Updated**: 2018-09-25 20:16:45+00:00
- **Authors**: Julien Perolat, Mateusz Malinowski, Bilal Piot, Olivier Pietquin
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set. By observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play, to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.



### LaSOT: A High-quality Benchmark for Large-scale Single Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1809.07845v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.07845v2)
- **Published**: 2018-09-20 20:37:24+00:00
- **Updated**: 2019-03-27 02:17:15+00:00
- **Authors**: Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, Haibin Ling
- **Comment**: 18 pages, including supplementary material, adding minor revisions
  and correcting typos
- **Journal**: None
- **Summary**: In this paper, we present LaSOT, a high-quality benchmark for Large-scale Single Object Tracking. LaSOT consists of 1,400 sequences with more than 3.5M frames in total. Each frame in these sequences is carefully and manually annotated with a bounding box, making LaSOT the largest, to the best of our knowledge, densely annotated tracking benchmark. The average video length of LaSOT is more than 2,500 frames, and each sequence comprises various challenges deriving from the wild where target objects may disappear and re-appear again in the view. By releasing LaSOT, we expect to provide the community with a large-scale dedicated benchmark with high quality for both the training of deep trackers and the veritable evaluation of tracking algorithms. Moreover, considering the close connections of visual appearance and natural language, we enrich LaSOT by providing additional language specification, aiming at encouraging the exploration of natural linguistic feature for tracking. A thorough experimental evaluation of 35 tracking algorithms on LaSOT is presented with detailed analysis, and the results demonstrate that there is still a big room for improvements.



