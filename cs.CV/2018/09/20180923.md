# Arxiv Papers in cs.CV on 2018-09-23
### Bounding Box Regression with Uncertainty for Accurate Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1809.08545v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08545v3)
- **Published**: 2018-09-23 07:07:01+00:00
- **Updated**: 2019-04-16 23:26:16+00:00
- **Authors**: Yihui He, Chenchen Zhu, Jianren Wang, Marios Savvides, Xiangyu Zhang
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Large-scale object detection datasets (e.g., MS-COCO) try to define the ground truth bounding boxes as clear as possible. However, we observe that ambiguities are still introduced when labeling the bounding boxes. In this paper, we propose a novel bounding box regression loss for learning bounding box transformation and localization variance together. Our loss greatly improves the localization accuracies of various architectures with nearly no additional computation. The learned localization variance allows us to merge neighboring bounding boxes during non-maximum suppression (NMS), which further improves the localization performance. On MS-COCO, we boost the Average Precision (AP) of VGG-16 Faster R-CNN from 23.6% to 29.1%. More importantly, for ResNet-50-FPN Mask R-CNN, our method improves the AP and AP90 by 1.8% and 6.2% respectively, which significantly outperforms previous state-of-the-art bounding box refinement methods. Our code and models are available at: github.com/yihui-he/KL-Loss



### Self Attention Grid for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1809.08556v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08556v1)
- **Published**: 2018-09-23 08:50:57+00:00
- **Updated**: 2018-09-23 08:50:57+00:00
- **Authors**: Jean-Paul Ainam, Ke Qin, Guisong Liu
- **Comment**: 10 pages, 4 figures, under review
- **Journal**: None
- **Summary**: In this paper, we present an attention mechanism scheme to improve person re-identification task. Inspired by biology, we propose Self Attention Grid (SAG) to discover the most informative parts from a high-resolution image using its internal representation. In particular, given an input image, the proposed model is fed with two copies of the same image and consists of two branches. The upper branch processes the high-resolution image and learns high dimensional feature representation while the lower branch processes the low-resolution image and learn a filtering attention grid. We apply a max filter operation to non-overlapping sub-regions on the high feature representation before element-wise multiplied with the output of the second branch. The feature maps of the second branch are subsequently weighted to reflect the importance of each patch of the grid using a softmax operation. Our attention module helps the network learn the most discriminative visual features of multiple image regions and is specifically optimized to attend feature representation at different levels. Extensive experiments on three large-scale datasets show that our self-attention mechanism significantly improves the baseline model and outperforms various state-of-art models by a large margin.



### Learning for Video Super-Resolution through HR Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1809.08573v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08573v2)
- **Published**: 2018-09-23 10:30:28+00:00
- **Updated**: 2018-10-25 08:22:06+00:00
- **Authors**: Longguang Wang, Yulan Guo, Zaiping Lin, Xinpu Deng, Wei An
- **Comment**: To appear in ACCV 2018
- **Journal**: None
- **Summary**: Video super-resolution (SR) aims to generate a sequence of high-resolution (HR) frames with plausible and temporally consistent details from their low-resolution (LR) counterparts. The generation of accurate correspondence plays a significant role in video SR. It is demonstrated by traditional video SR methods that simultaneous SR of both images and optical flows can provide accurate correspondences and better SR results. However, LR optical flows are used in existing deep learning based methods for correspondence generation. In this paper, we propose an end-to-end trainable video SR framework to super-resolve both images and optical flows. Specifically, we first propose an optical flow reconstruction network (OFRnet) to infer HR optical flows in a coarse-to-fine manner. Then, motion compensation is performed according to the HR optical flows. Finally, compensated LR inputs are fed to a super-resolution network (SRnet) to generate the SR results. Extensive experiments demonstrate that HR optical flows provide more accurate correspondences than their LR counterparts and improve both accuracy and consistency performance. Comparative results on the Vid4 and DAVIS-10 datasets show that our framework achieves the state-of-the-art performance.



### Accelerate CU Partition in HEVC using Large-Scale Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1809.08617v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08617v1)
- **Published**: 2018-09-23 15:39:19+00:00
- **Updated**: 2018-09-23 15:39:19+00:00
- **Authors**: Chenying Wang, Li Yu, Shengwei Wang
- **Comment**: 4 pages, 2 figures
- **Journal**: None
- **Summary**: High efficiency video coding (HEVC) suffers high encoding computational complexity, partly attributed to the rate-distortion optimization quad-tree search in CU partition decision. Therefore, we propose a novel two-stage CU partition decision approach in HEVC intra-mode. In the proposed approach, CNN-based algorithm is designed to decide CU partition mode precisely in three depths. In order to alleviate computational complexity further, an auxiliary earl-termination mechanism is also proposed to filter obvious homogeneous CUs out of the subsequent CNN-based algorithm. Experimental results show that the proposed approach achieves about 37% encoding time saving on average and insignificant BD-Bitrate rise compared with the original HEVC encoder.



### Unsupervised Learning of Dense Optical Flow, Depth and Egomotion from Sparse Event Data
- **Arxiv ID**: http://arxiv.org/abs/1809.08625v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1809.08625v2)
- **Published**: 2018-09-23 16:27:58+00:00
- **Updated**: 2019-02-25 18:17:25+00:00
- **Authors**: Chengxi Ye, Anton Mitrokhin, Cornelia Ferm√ºller, James A. Yorke, Yiannis Aloimonos
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we present a lightweight, unsupervised learning pipeline for \textit{dense} depth, optical flow and egomotion estimation from sparse event output of the Dynamic Vision Sensor (DVS). To tackle this low level vision task, we use a novel encoder-decoder neural network architecture - ECN.   Our work is the first monocular pipeline that generates dense depth and optical flow from sparse event data only. The network works in self-supervised mode and has just 150k parameters. We evaluate our pipeline on the MVSEC self driving dataset and present results for depth, optical flow and and egomotion estimation. Due to the lightweight design, the inference part of the network runs at 250 FPS on a single GPU, making the pipeline ready for realtime robotics applications. Our experiments demonstrate significant improvements upon previous works that used deep learning on event data, as well as the ability of our pipeline to perform well during both day and night.



### Domain Adaptation for Robot Predictive Maintenance Systems
- **Arxiv ID**: http://arxiv.org/abs/1809.08626v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.08626v3)
- **Published**: 2018-09-23 16:29:29+00:00
- **Updated**: 2020-02-24 19:37:38+00:00
- **Authors**: Arash Golibagh Mahyari, Thomas Locker
- **Comment**: None
- **Journal**: None
- **Summary**: Industrial robots play an increasingly important role in a growing number of fields. For example, robotics is used to increase productivity while reducing costs in various aspects of manufacturing. Since robots are often set up in production lines, the breakdown of a single robot has a negative impact on the entire process, in the worst case bringing the whole line to a halt until the issue is resolved, leading to substantial financial losses due to the unforeseen downtime. Therefore, predictive maintenance systems based on the internal signals of robots have gained attention as an essential component of robotics service offerings. The main shortcoming of existing predictive maintenance algorithms is that the extracted features typically differ significantly from the learnt model when the operation of the robot changes, incurring false alarms. In order to mitigate this problem, predictive maintenance algorithms require the model to be retrained with normal data of the new operation. In this paper, we propose a novel solution based on transfer learning to pass the knowledge of the trained model from one operation to another in order to prevent the need for retraining and to eliminate such false alarms. The deployment of the proposed unsupervised transfer learning algorithm on real-world datasets demonstrates that the algorithm can not only distinguish between operation and mechanical condition change, it further yields a sharper deviation from the trained model in case of a mechanical condition change and thus detects mechanical issues with higher confidence.



### Segmentation of Skin Lesions and their Attributes Using Multi-Scale Convolutional Neural Networks and Domain Specific Augmentations
- **Arxiv ID**: http://arxiv.org/abs/1809.10243v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.10243v3)
- **Published**: 2018-09-23 18:01:14+00:00
- **Updated**: 2019-03-29 09:12:34+00:00
- **Authors**: Mostafa Jahanifar, Neda Zamani Tajeddin, Navid Alemi Koohbanani, Ali Gooya, Nasir Rajpoot
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: Computer-aided diagnosis systems for classification of different type of skin lesions have been an active field of research in recent decades. It has been shown that introducing lesions and their attributes masks into lesion classification pipeline can greatly improve the performance. In this paper, we propose a framework by incorporating transfer learning for segmenting lesions and their attributes based on the convolutional neural networks. The proposed framework is based on the encoder-decoder architecture which utilizes a variety of pre-trained networks in the encoding path and generates the prediction map by combining multi-scale information in decoding path using a pyramid pooling manner. To address the lack of training data and increase the proposed model generalization, an extensive set of novel domain-specific augmentation routines have been applied to simulate the real variations in dermoscopy images. Finally, by performing broad experiments on three different data sets obtained from International Skin Imaging Collaboration archive (ISIC2016, ISIC2017, and ISIC2018 challenges data sets), we show that the proposed method outperforms other state-of-the-art approaches for ISIC2016 and ISIC2017 segmentation task and achieved the first rank on the leader-board of ISIC2018 attribute detection task.



### Learning to Read by Spelling: Towards Unsupervised Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.08675v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08675v2)
- **Published**: 2018-09-23 20:53:41+00:00
- **Updated**: 2018-12-09 19:57:06+00:00
- **Authors**: Ankush Gupta, Andrea Vedaldi, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents a method for visual text recognition without using any paired supervisory data. We formulate the text recognition task as one of aligning the conditional distribution of strings predicted from given text images, with lexically valid strings sampled from target corpora. This enables fully automated, and unsupervised learning from just line-level text-images, and unpaired text-string samples, obviating the need for large aligned datasets. We present detailed analysis for various aspects of the proposed method, namely - (1) impact of the length of training sequences on convergence, (2) relation between character frequencies and the order in which they are learnt, (3) generalisation ability of our recognition network to inputs of arbitrary lengths, and (4) impact of varying the text corpus on recognition accuracy. Finally, we demonstrate excellent text recognition accuracy on both synthetically generated text images, and scanned images of real printed books, using no labelled training examples.



### Curvilinear Structure Enhancement by Multiscale Top-Hat Tensor in 2D/3D Images
- **Arxiv ID**: http://arxiv.org/abs/1809.08678v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.08678v2)
- **Published**: 2018-09-23 20:58:58+00:00
- **Updated**: 2019-01-04 20:55:27+00:00
- **Authors**: Shuaa S. Alharbi, Cigdem Sazak, Carl J. Nelson, Boguslaw Obara
- **Comment**: None
- **Journal**: None
- **Summary**: A wide range of biomedical applications requires enhancement, detection, quantification and modelling of curvilinear structures in 2D and 3D images. Curvilinear structure enhancement is a crucial step for further analysis, but many of the enhancement approaches still suffer from contrast variations and noise. This can be addressed using a multiscale approach that produces a better quality enhancement for low contrast and noisy images compared with a single-scale approach in a wide range of biomedical images. Here, we propose the Multiscale Top-Hat Tensor (MTHT) approach, which combines multiscale morphological filtering with a local tensor representation of curvilinear structures in 2D and 3D images. The proposed approach is validated on synthetic and real data and is also compared to the state-of-the-art approaches. Our results show that the proposed approach achieves high-quality curvilinear structure enhancement in synthetic examples and in a wide range of 2D and 3D images.



### Unsupervised parameter selection for denoising with the elastic net
- **Arxiv ID**: http://arxiv.org/abs/1809.08696v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.08696v3)
- **Published**: 2018-09-23 23:31:17+00:00
- **Updated**: 2019-05-29 14:56:15+00:00
- **Authors**: Ernesto de Vito, Zeljko Kereta, Valeria Naumova
- **Comment**: 27 pages, 6 figures
- **Journal**: None
- **Summary**: Despite recent advances in regularisation theory, the issue of parameter selection still remains a challenge for most applications. In a recent work the framework of statistical learning was used to approximate the optimal Tikhonov regularisation parameter from noisy data. In this work, we improve their results and extend the analysis to the elastic net regularisation, providing explicit error bounds on the accuracy of the approximated parameter and the corresponding regularisation solution in a simplified case. Furthermore, in the general case we design a data-driven, automated algorithm for the computation of an approximate regularisation parameter. Our analysis combines statistical learning theory with insights from regularisation theory. We compare our approach with state-of-the-art parameter selection criteria and illustrate its superiority in terms of accuracy and computational time on simulated and real data sets.



### Textually Enriched Neural Module Networks for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1809.08697v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.08697v1)
- **Published**: 2018-09-23 23:45:54+00:00
- **Updated**: 2018-09-23 23:45:54+00:00
- **Authors**: Khyathi Raghavi Chandu, Mary Arpita Pyreddy, Matthieu Felix, Narendra Nath Joshi
- **Comment**: None
- **Journal**: None
- **Summary**: Problems at the intersection of language and vision, like visual question answering, have recently been gaining a lot of attention in the field of multi-modal machine learning as computer vision research moves beyond traditional recognition tasks. There has been recent success in visual question answering using deep neural network models which use the linguistic structure of the questions to dynamically instantiate network layouts. In the process of converting the question to a network layout, the question is simplified, which results in loss of information in the model. In this paper, we enrich the image information with textual data using image captions and external knowledge bases to generate more coherent answers. We achieve 57.1% overall accuracy on the test-dev open-ended questions from the visual question answering (VQA 1.0) real image dataset.



