# Arxiv Papers in cs.CV on 2018-09-09
### Binary Classification of Alzheimer Disease using sMRI Imaging modality and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.06209v3
- **DOI**: 10.1007/s10278-019-00265-5
- **Categories**: **cs.CV**, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1809.06209v3)
- **Published**: 2018-09-09 01:46:37+00:00
- **Updated**: 2020-04-03 18:04:27+00:00
- **Authors**: Ahsan Bin Tufail, Qiu-Na Zhang, Yong-Kui Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Alzheimer's disease (AD) is an irreversible devastative neurodegenerative disorder associated with progressive impairment of memory and cognitive functions. Its early diagnosis is crucial for the development of possible future treatment option(s). Structural magnetic resonance images (sMRI) plays an important role to help in understanding the anatomical changes related to AD especially in its early stages. Conventional methods require the expertise of domain experts and extract hand-picked features such as gray matter substructures and train a classifier to distinguish AD subjects from healthy subjects. Different from these methods, this paper proposes to construct multiple deep 2D convolutional neural networks (2D-CNNs) to learn the various features from local brain images which are combined to make the final classification for AD diagnosis. The whole brain image was passed through two transfer learning architectures; Inception version 3 and Xception; as well as custom Convolutional Neural Network (CNN) built with the help of separable convolutional layers which can automatically learn the generic features from imaging data for classification. Our study is conducted using cross-sectional T1-weighted structural MRI brain images from Open Access Series of Imaging Studies (OASIS) database to maintain the size and contrast over different MRI scans. Experimental results show that the transfer learning approaches exceed the performance of non-transfer learning based approaches demonstrating the effectiveness of these approaches for the binary AD classification task.



### Towards Query Efficient Black-box Attacks: An Input-free Perspective
- **Arxiv ID**: http://arxiv.org/abs/1809.02918v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.02918v1)
- **Published**: 2018-09-09 03:49:06+00:00
- **Updated**: 2018-09-09 03:49:06+00:00
- **Authors**: Yali Du, Meng Fang, Jinfeng Yi, Jun Cheng, Dacheng Tao
- **Comment**: Accepted by 11th ACM Workshop on Artificial Intelligence and Security
  (AISec) with the 25th ACM Conference on Computer and Communications Security
  (CCS)
- **Journal**: None
- **Summary**: Recent studies have highlighted that deep neural networks (DNNs) are vulnerable to adversarial attacks, even in a black-box scenario. However, most of the existing black-box attack algorithms need to make a huge amount of queries to perform attacks, which is not practical in the real world. We note one of the main reasons for the massive queries is that the adversarial example is required to be visually similar to the original image, but in many cases, how adversarial examples look like does not matter much. It inspires us to introduce a new attack called \emph{input-free} attack, under which an adversary can choose an arbitrary image to start with and is allowed to add perceptible perturbations on it. Following this approach, we propose two techniques to significantly reduce the query complexity. First, we initialize an adversarial example with a gray color image on which every pixel has roughly the same importance for the target model. Then we shrink the dimension of the attack space by perturbing a small region and tiling it to cover the input image. To make our algorithm more effective, we stabilize a projected gradient ascent algorithm with momentum, and also propose a heuristic approach for region size selection. Through extensive experiments, we show that with only 1,701 queries on average, we can perturb a gray image to any target class of ImageNet with a 100\% success rate on InceptionV3. Besides, our algorithm has successfully defeated two real-world systems, the Clarifai food detection API and the Baidu Animal Identification API.



### Automated Strabismus Detection for Telemedicine Applications
- **Arxiv ID**: http://arxiv.org/abs/1809.02940v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02940v3)
- **Published**: 2018-09-09 08:46:59+00:00
- **Updated**: 2018-12-03 02:06:46+00:00
- **Authors**: Jiewei Lu, Zhun Fan, Ce Zheng, Jingan Feng, Longtao Huang, Wenji Li, Erik D. Goodman
- **Comment**: 8 page, 10 figures
- **Journal**: None
- **Summary**: Strabismus is one of the most influential ophthalmologic diseases in human's life. Timely detection of strabismus contributes to its prognosis and treatment. Telemedicine, which has great potential to alleviate the growing demand of the diagnosis of ophthalmologic diseases, is an effective method to achieve timely strabismus detection. In this paper, a tele strabismus dataset is established by the ophthalmologists. Then an end-to-end framework named as RF-CNN is proposed to achieve automated strabismus detection on the established tele strabismus dataset. RF-CNN first performs eye region segmentation on each individual image, and further classifies the segmented eye regions with deep neural networks. The experimental results on the established tele strabismus dataset demonstrates that the proposed RF-CNN can have a good performance on automated strabismus detection for telemedicine application. Code is made publicly available at: https://github.com/jieWeiLu/Strabismus-Detection-for-Telemedicine-Application.



### Visual Relationship Prediction via Label Clustering and Incorporation of Depth Information
- **Arxiv ID**: http://arxiv.org/abs/1809.02945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02945v1)
- **Published**: 2018-09-09 09:23:53+00:00
- **Updated**: 2018-09-09 09:23:53+00:00
- **Authors**: Hsuan-Kung Yang, An-Chieh Cheng, Kuan-Wei Ho, Tsu-Jui Fu, Chun-Yi Lee
- **Comment**: Won 2nd place in Person In Context Challenge (ECCV 2018 workshop)
- **Journal**: None
- **Summary**: In this paper, we investigate the use of an unsupervised label clustering technique and demonstrate that it enables substantial improvements in visual relationship prediction accuracy on the Person in Context (PIC) dataset. We propose to group object labels with similar patterns of relationship distribution in the dataset into fewer categories. Label clustering not only mitigates both the large classification space and class imbalance issues, but also potentially increases data samples for each clustered category. We further propose to incorporate depth information as an additional feature into the instance segmentation model. The additional depth prediction path supplements the relationship prediction model in a way that bounding boxes or segmentation masks are unable to deliver. We have rigorously evaluated the proposed techniques and performed various ablation analysis to validate the benefits of them.



### LS-Net: Learning to Solve Nonlinear Least Squares for Monocular Stereo
- **Arxiv ID**: http://arxiv.org/abs/1809.02966v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02966v1)
- **Published**: 2018-09-09 13:20:00+00:00
- **Updated**: 2018-09-09 13:20:00+00:00
- **Authors**: Ronald Clark, Michael Bloesch, Jan Czarnowski, Stefan Leutenegger, Andrew J. Davison
- **Comment**: ECCV 2018. Video: https://youtu.be/5bZbMm8UqbA
- **Journal**: None
- **Summary**: Sum-of-squares objective functions are very popular in computer vision algorithms. However, these objective functions are not always easy to optimize. The underlying assumptions made by solvers are often not satisfied and many problems are inherently ill-posed. In this paper, we propose LS-Net, a neural nonlinear least squares optimization algorithm which learns to effectively optimize these cost functions even in the presence of adversities. Unlike traditional approaches, the proposed solver requires no hand-crafted regularizers or priors as these are implicitly learned from the data. We apply our method to the problem of motion stereo ie. jointly estimating the motion and scene geometry from pairs of images of a monocular sequence. We show that our learned optimizer is able to efficiently and effectively solve this challenging optimization problem.



### Geometry-Aware Face Completion and Editing
- **Arxiv ID**: http://arxiv.org/abs/1809.02967v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02967v2)
- **Published**: 2018-09-09 13:29:40+00:00
- **Updated**: 2019-02-13 07:30:04+00:00
- **Authors**: Linsen Song, Jie Cao, Linxiao Song, Yibo Hu, Ran He
- **Comment**: None
- **Journal**: None
- **Summary**: Face completion is a challenging generation task because it requires generating visually pleasing new pixels that are semantically consistent with the unmasked face region. This paper proposes a geometry-aware Face Completion and Editing NETwork (FCENet) by systematically studying facial geometry from the unmasked region. Firstly, a facial geometry estimator is learned to estimate facial landmark heatmaps and parsing maps from the unmasked face image. Then, an encoder-decoder structure generator serves to complete a face image and disentangle its mask areas conditioned on both the masked face image and the estimated facial geometry images. Besides, since low-rank property exists in manually labeled masks, a low-rank regularization term is imposed on the disentangled masks, enforcing our completion network to manage occlusion area with various shape and size. Furthermore, our network can generate diverse results from the same masked input by modifying estimated facial geometry, which provides a flexible mean to edit the completed face appearance. Extensive experimental results qualitatively and quantitatively demonstrate that our network is able to generate visually pleasing face completion results and edit face attributes as well.



### Dual Attention Network for Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1809.02983v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.02983v4)
- **Published**: 2018-09-09 14:48:22+00:00
- **Updated**: 2019-04-21 08:10:54+00:00
- **Authors**: Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, Hanqing Lu
- **Comment**: Accepted by CVPR2019
- **Journal**: None
- **Summary**: In this paper, we address the scene segmentation task by capturing rich contextual dependencies based on the selfattention mechanism. Unlike previous works that capture contexts by multi-scale features fusion, we propose a Dual Attention Networks (DANet) to adaptively integrate local features with their global dependencies. Specifically, we append two types of attention modules on top of traditional dilated FCN, which model the semantic interdependencies in spatial and channel dimensions respectively. The position attention module selectively aggregates the features at each position by a weighted sum of the features at all positions. Similar features would be related to each other regardless of their distances. Meanwhile, the channel attention module selectively emphasizes interdependent channel maps by integrating associated features among all channel maps. We sum the outputs of the two attention modules to further improve feature representation which contributes to more precise segmentation results. We achieve new state-of-the-art segmentation performance on three challenging scene segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5% on Cityscapes test set is achieved without using coarse data. We make the code and trained model publicly available at https://github.com/junfu1115/DANet



### Fingertip Detection and Tracking for Recognition of Air-Writing in Videos
- **Arxiv ID**: http://arxiv.org/abs/1809.03016v1
- **DOI**: 10.1016/j.eswa.2019.06.034
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03016v1)
- **Published**: 2018-09-09 18:10:59+00:00
- **Updated**: 2018-09-09 18:10:59+00:00
- **Authors**: Sohom Mukherjee, Arif Ahmed, Debi Prosad Dogra, Samarjit Kar, Partha Pratim Roy
- **Comment**: 32 pages, 10 figures, 2 tables. Submitted to Journal of Expert
  Systems with Applications
- **Journal**: Expert Systems with Applications Volume 136, 1 December 2019,
  Pages 217-229
- **Summary**: Air-writing is the process of writing characters or words in free space using finger or hand movements without the aid of any hand-held device. In this work, we address the problem of mid-air finger writing using web-cam video as input. In spite of recent advances in object detection and tracking, accurate and robust detection and tracking of the fingertip remains a challenging task, primarily due to small dimension of the fingertip. Moreover, the initialization and termination of mid-air finger writing is also challenging due to the absence of any standard delimiting criterion. To solve these problems, we propose a new writing hand pose detection algorithm for initialization of air-writing using the Faster R-CNN framework for accurate hand detection followed by hand segmentation and finally counting the number of raised fingers based on geometrical properties of the hand. Further, we propose a robust fingertip detection and tracking approach using a new signature function called distance-weighted curvature entropy. Finally, a fingertip velocity-based termination criterion is used as a delimiter to mark the completion of the air-writing gesture. Experiments show the superiority of the proposed fingertip detection and tracking algorithm over state-of-the-art approaches giving a mean precision of 73.1 % while achieving real-time performance at 18.5 fps, a condition which is of vital importance to air-writing. Character recognition experiments give a mean accuracy of 96.11 % using the proposed air-writing system, a result which is comparable to that of existing handwritten character recognition systems.



### Crack-pot: Autonomous Road Crack and Pothole Detection
- **Arxiv ID**: http://arxiv.org/abs/1810.05107v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1810.05107v1)
- **Published**: 2018-09-09 18:46:27+00:00
- **Updated**: 2018-09-09 18:46:27+00:00
- **Authors**: Sukhad Anand, Saksham Gupta, Vaibhav Darbari, Shivam Kohli
- **Comment**: Submitted at DICTA 2018
- **Journal**: None
- **Summary**: With the advent of self-driving cars and autonomous robots, it is imperative to detect road impairments like cracks and potholes and to perform necessary evading maneuvers to ensure fluid journey for on-board passengers or equipment. We propose a fully autonomous robust real-time road crack and pothole detection algorithm which can be deployed on any GPU based conventional processing boards with an associated camera. The approach is based on a deep neural net architecture which detects cracks and potholes using texture and spatial features. We also propose pre-processing methods which ensure real-time performance. The novelty of the approach lies in using texture- based features to differentiate between crack surfaces and sound roads. The approach performs well in large viewpoint changes, background noise, shadows, and occlusion. The efficacy of the system is shown on standard road crack datasets.



### A Neural Temporal Model for Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/1809.03036v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03036v5)
- **Published**: 2018-09-09 20:12:32+00:00
- **Updated**: 2019-11-22 14:08:15+00:00
- **Authors**: Anand Gopalakrishnan, Ankur Mali, Dan Kifer, C. Lee Giles, Alexander G. Ororbia
- **Comment**: accepted to cvpr 2019
- **Journal**: In Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition, pp. 12116-12125. 2019
- **Summary**: We propose novel neural temporal models for predicting and synthesizing human motion, achieving state-of-the-art in modeling long-term motion trajectories while being competitive with prior work in short-term prediction and requiring significantly less computation. Key aspects of our proposed system include: 1) a novel, two-level processing architecture that aids in generating planned trajectories, 2) a simple set of easily computable features that integrate derivative information, and 3) a novel multi-objective loss function that helps the model to slowly progress from simple next-step prediction to the harder task of multi-step, closed-loop prediction. Our results demonstrate that these innovations improve the modeling of long-term motion trajectories. Finally, we propose a novel metric, called Normalized Power Spectrum Similarity (NPSS), to evaluate the long-term predictive ability of motion synthesis models, complementing the popular mean-squared error (MSE) measure of Euler joint angles over time. We conduct a user study to determine if the proposed NPSS correlates with human evaluation of long-term motion more strongly than MSE and find that it indeed does. We release code and additional results (visualizations) for this paper at: https://github.com/cr7anand/neural_temporal_models



### How clever is the FiLM model, and how clever can it be?
- **Arxiv ID**: http://arxiv.org/abs/1809.03044v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.03044v1)
- **Published**: 2018-09-09 21:08:57+00:00
- **Updated**: 2018-09-09 21:08:57+00:00
- **Authors**: Alexander Kuhnle, Huiyuan Xie, Ann Copestake
- **Comment**: None
- **Journal**: None
- **Summary**: The FiLM model achieves close-to-perfect performance on the diagnostic CLEVR dataset and is distinguished from other such models by having a comparatively simple and easily transferable architecture. In this paper, we investigate in more detail the ability of FiLM to learn various linguistic constructions. Our main results show that (a) FiLM is not able to learn relational statements straight away except for very simple instances, (b) training on a broader set of instances as well as pretraining on simpler instance types can help alleviate these learning difficulties, (c) mixing is less robust than pretraining and very sensitive to the compositional structure of the dataset. Overall, our results suggest that the approach of big all-encompassing datasets and the paradigm of "the effectiveness of data" may have fundamental limitations.



### TextContourNet: a Flexible and Effective Framework for Improving Scene Text Detection Architecture with a Multi-task Cascade
- **Arxiv ID**: http://arxiv.org/abs/1809.03050v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03050v2)
- **Published**: 2018-09-09 22:31:37+00:00
- **Updated**: 2018-12-03 04:42:39+00:00
- **Authors**: Dafang He, Xiao Yang, Daniel Kifer, C. Lee Giles
- **Comment**: 9 pages(including references); WACV 2019
- **Journal**: None
- **Summary**: We study the problem of extracting text instance contour information from images and use it to assist scene text detection. We propose a novel and effective framework for this and experimentally demonstrate that: (1) A CNN that can be effectively used to extract instance-level text contour from natural images. (2) The extracted contour information can be used for better scene text detection. We propose two ways for learning the contour task together with the scene text detection: (1) as an auxiliary task and (2) as multi-task cascade. Extensive experiments with different benchmark datasets demonstrate that both designs improve the performance of a state-of-the-art scene text detector and that a multi-task cascade design achieves the best performance.



### LDW-SCSA: Logistic Dynamic Weight based Sine Cosine Search Algorithm for Numerical Functions Optimization
- **Arxiv ID**: http://arxiv.org/abs/1809.03055v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.03055v1)
- **Published**: 2018-09-09 22:41:57+00:00
- **Updated**: 2018-09-09 22:41:57+00:00
- **Authors**: Turker Tuncer
- **Comment**: None
- **Journal**: None
- **Summary**: Particle swarm optimization (PSO) and Sine Cosine algorithm (SCA) have been widely used optimization methods but these methods have some disadvantages such as trapped local optimum point. In order to solve this problem and obtain more successful results than others, a novel logistic dynamic weight based sine cosine search algorithm (LDW-SCSA) is presented in this paper. In the LDW-SCSA method, logistic map is used as dynamic weight generator. Logistic map is one of the famous and widely used chaotic map in the literature. Search process of SCA is modified in the LDW-SCSA. To evaluate performance of the LDW-SCSA, the widely used numerical benchmark functions were utilized as test suite and other swarm optimization methods were used to obtain the comparison results. Superior performances of the LDW-SCSA are proved success of this method.



