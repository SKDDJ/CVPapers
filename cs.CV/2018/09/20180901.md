# Arxiv Papers in cs.CV on 2018-09-01
### Learning Sparse Low-Precision Neural Networks With Learnable Regularization
- **Arxiv ID**: http://arxiv.org/abs/1809.00095v2
- **DOI**: 10.1109/ACCESS.2020.2996936
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1809.00095v2)
- **Published**: 2018-09-01 01:28:21+00:00
- **Updated**: 2020-05-24 00:41:54+00:00
- **Authors**: Yoojin Choi, Mostafa El-Khamy, Jungwon Lee
- **Comment**: IEEE Access
- **Journal**: None
- **Summary**: We consider learning deep neural networks (DNNs) that consist of low-precision weights and activations for efficient inference of fixed-point operations. In training low-precision networks, gradient descent in the backward pass is performed with high-precision weights while quantized low-precision weights and activations are used in the forward pass to calculate the loss function for training. Thus, the gradient descent becomes suboptimal, and accuracy loss follows. In order to reduce the mismatch in the forward and backward passes, we utilize mean squared quantization error (MSQE) regularization. In particular, we propose using a learnable regularization coefficient with the MSQE regularizer to reinforce the convergence of high-precision weights to their quantized values. We also investigate how partial L2 regularization can be employed for weight pruning in a similar manner. Finally, combining weight pruning, quantization, and entropy coding, we establish a low-precision DNN compression pipeline. In our experiments, the proposed method yields low-precision MobileNet and ShuffleNet models on ImageNet classification with the state-of-the-art compression ratios of 7.13 and 6.79, respectively. Moreover, we examine our method for image super resolution networks to produce 8-bit low-precision models at negligible performance loss.



### Attentive Crowd Flow Machines
- **Arxiv ID**: http://arxiv.org/abs/1809.00101v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.00101v1)
- **Published**: 2018-09-01 02:22:57+00:00
- **Updated**: 2018-09-01 02:22:57+00:00
- **Authors**: Lingbo Liu, Ruimao Zhang, Jiefeng Peng, Guanbin Li, Bowen Du, Liang Lin
- **Comment**: ACM MM, full paper
- **Journal**: None
- **Summary**: Traffic flow prediction is crucial for urban traffic management and public safety. Its key challenges lie in how to adaptively integrate the various factors that affect the flow changes. In this paper, we propose a unified neural network module to address this problem, called Attentive Crowd Flow Machine~(ACFM), which is able to infer the evolution of the crowd flow by learning dynamic representations of temporally-varying data with an attention mechanism. Specifically, the ACFM is composed of two progressive ConvLSTM units connected with a convolutional layer for spatial weight prediction. The first LSTM takes the sequential flow density representation as input and generates a hidden state at each time-step for attention map inference, while the second LSTM aims at learning the effective spatial-temporal feature expression from attentionally weighted crowd flow features. Based on the ACFM, we further build a deep architecture with the application to citywide crowd flow prediction, which naturally incorporates the sequential and periodic data as well as other external influences. Extensive experiments on two standard benchmarks (i.e., crowd flow in Beijing and New York City) show that the proposed method achieves significant improvements over the state-of-the-art methods.



### DAC-SDC Low Power Object Detection Challenge for UAV Applications
- **Arxiv ID**: http://arxiv.org/abs/1809.00110v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00110v1)
- **Published**: 2018-09-01 03:37:37+00:00
- **Updated**: 2018-09-01 03:37:37+00:00
- **Authors**: Xiaowei Xu, Xinyi Zhang, Bei Yu, X. Sharon Hu, Christopher Rowen, Jingtong Hu, Yiyu Shi
- **Comment**: 12 pages, 21 figures
- **Journal**: None
- **Summary**: The 55th Design Automation Conference (DAC) held its first System Design Contest (SDC) in 2018. SDC'18 features a lower power object detection challenge (LPODC) on designing and implementing novel algorithms based object detection in images taken from unmanned aerial vehicles (UAV). The dataset includes 95 categories and 150k images, and the hardware platforms include Nvidia's TX2 and Xilinx's PYNQ Z1. DAC-SDC'18 attracted more than 110 entries from 12 countries. This paper presents in detail the dataset and evaluation procedure. It further discusses the methods developed by some of the entries as well as representative results. The paper concludes with directions for future improvements.



### Landslide Monitoring based on Terrestrial Laser Scanning: A Novel Semi-automatic Workflow
- **Arxiv ID**: http://arxiv.org/abs/1809.03305v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03305v1)
- **Published**: 2018-09-01 06:04:53+00:00
- **Updated**: 2018-09-01 06:04:53+00:00
- **Authors**: Yue Pan
- **Comment**: 10 pages, 13 figures ,3 tables
- **Journal**: None
- **Summary**: In this paper, we propose a workflow that uses Terrestrial Laser Scanning(TLS) to semi-automatically monitor landslide and then test it in practice. Firstly, several groups of TLS stations are set on different time to collect the raw point cloud of the object mountain. Next, Hierarchical Merging Based Multi-view (HMMR) registration algorithm is adapted to accomplish single-phase multi-view registration.In order to analyze deformation between multiple periods, Iterative Global Similarity Point (IGSP) algorithm is applied to accomplish multiple-phase registration, which outperforms ICP in experiments. Then the cloth simulation filtering (CSF) algorithm was used together with manual post-processing to remove vegetation on the slope. After that, the mountain slope's digital terrain model (DTM) is generated for each period, and the distance between adjacent DTMs are calculated as the landslide deformation mass. Furthermore, average deformation rate of the landslide surface is calculated and analyzed.To validate the effectiveness of proposed workflow, we uses the TLS data of five periods of the landslide in the Shanhou village of northern Changshan Island from 2013 to 2015. The results indicate that the method can obtain centimeter-level deformation monitoring accuracy which can effectively monitor and analyze long-term landslide morphology and trend as well as position the significant deformation area and determine the type of landslide. In addition, the process can be automated to provide end-to-end TLS based long-term landslide monitoring applications, providing reference for monitoring and early warning of potential landslides.



### Implications of Ocular Pathologies for Iris Recognition Reliability
- **Arxiv ID**: http://arxiv.org/abs/1809.00168v1
- **DOI**: 10.1016/j.imavis.2016.08.001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00168v1)
- **Published**: 2018-09-01 13:03:48+00:00
- **Updated**: 2018-09-01 13:03:48+00:00
- **Authors**: Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz
- **Comment**: Image and Vision Computing (2016)
- **Journal**: None
- **Summary**: This paper presents an analysis of how iris recognition is influenced by eye disease and an appropriate dataset comprising 2996 images of irises taken from 230 distinct eyes (including 184 affected by more than 20 different eye conditions). The images were collected in near infrared and visible light during routine ophthalmological examination. The experimental study carried out utilizing four independent iris recognition algorithms (MIRLIN, VeriEye, OSIRIS and IriCore) renders four valuable results. First, the enrollment process is highly sensitive to those eye conditions that obstruct the iris or cause geometrical distortions. Second, even those conditions that do not produce visible changes to the structure of the iris may increase the dissimilarity between samples of the same eyes. Third, eye conditions affecting the geometry or the tissue structure of the iris or otherwise producing obstructions significantly decrease same-eye similarity and have a lower, yet still statistically significant, influence on impostor comparison scores. Fourth, for unhealthy eyes, the most prominent effect of disease on iris recognition is to cause segmentation errors. To our knowledge this paper describes the largest database of iris images for disease-affected eyes made publicly available to researchers and offers the most comprehensive study of what we can expect when iris recognition is employed for diseased eyes.



### Linear regression analysis of template aging in iris biometrics
- **Arxiv ID**: http://arxiv.org/abs/1809.00170v1
- **DOI**: 10.1109/IWBF.2015.7110233
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00170v1)
- **Published**: 2018-09-01 13:12:32+00:00
- **Updated**: 2018-09-01 13:12:32+00:00
- **Authors**: Mateusz Trokielewicz
- **Comment**: Accepted manuscript version of the IEEE IWBF2015 paper
- **Journal**: 3rd International Workshop on Biometrics and Forensics (IWBF
  2015), Gjovik, 2015, pp. 1-6
- **Summary**: The aim of this work is to determine how vulnerable different iris coding methods are in relation to biometric template aging phenomenon. This is considered to be particularly important when the time lapse between gallery and probe samples extends significantly, to more than a few years.   Our experiments employ iris aging analysis conducted using three different iris recognition algorithms and a database of 583 samples from 58 irises collected up to nine years apart. To determine the degradation rates of similarity scores with extending time lapse and also in relation to multiple image quality and geometrical factors of sample images, a linear regression analysis was performed. 29 regression models have been tested with both the time parameter and geometrical factors being statistically significant in every model. Quality measures that showed statistically significant influence on the predicted variable were, depending on the method, image sharpness and local contrast or their mutual relations.   To our best knowledge, this is the first paper describing aging analysis using multiple regression models with data covering such a wide time period. Results presented suggest that template aging effect occurs in iris biometrics to a statistically significant extent. Image quality and geometrical factors may contribute to the degradation of similarity score. However, the estimate of time parameter showed statistical significance and similar value in each of the tested models. This reveals that the aging phenomenon may as well be unrelated to quality and geometrical measures of the image.



### Human Iris Recognition in Post-mortem Subjects: Study and Database
- **Arxiv ID**: http://arxiv.org/abs/1809.00174v1
- **DOI**: 10.1109/BTAS.2016.7791175
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00174v1)
- **Published**: 2018-09-01 13:26:18+00:00
- **Updated**: 2018-09-01 13:26:18+00:00
- **Authors**: Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz
- **Comment**: Accepted manuscript version of the BTAS2016 paper
- **Journal**: 2016 IEEE 8th International Conference on Biometrics Theory,
  Applications and Systems (BTAS), Niagara Falls, NY, 2016, pp. 1-6
- **Summary**: This paper presents a unique study of post-mortem human iris recognition and the first known to us database of near-infrared and visible-light iris images of deceased humans collected up to almost 17 days after death. We used four different iris recognition methods to analyze the dynamics of iris quality decay in short-term comparisons (samples collected up to 60 hours after death) and long-term comparisons (for samples acquired up to 407 hours after demise). This study shows that post-mortem iris recognition is possible and occasionally works even 17 days after death. These conclusions contradict a promulgated rumor that iris is unusable shortly after decease. We make this dataset publicly available to let others verify our findings and to research new aspects of this important and unfamiliar topic. We are not aware of any earlier papers offering post-mortem human iris images and such comprehensive analysis employing four different matchers.



### Iris Recognition Under Biologically Troublesome Conditions - Effects of Aging, Diseases and Post-mortem Changes
- **Arxiv ID**: http://arxiv.org/abs/1809.00182v1
- **DOI**: 10.5220/0006251702530258
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00182v1)
- **Published**: 2018-09-01 13:51:07+00:00
- **Updated**: 2018-09-01 13:51:07+00:00
- **Authors**: Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz
- **Comment**: Accepted manuscript version of the BIOSIGNALS 2017 paper
- **Journal**: Proceedings of the 10th International Joint Conference on
  Biomedical Engineering Systems and Technologies - Volume 4: BIOSIGNALS, pages
  253-258, 2017, Porto, Portugal
- **Summary**: This paper presents the most comprehensive analysis of iris recognition reliability in the occurrence of various biological processes happening naturally and pathologically in the human body, including aging, illnesses, and post-mortem changes to date. Insightful conclusions are offered in relation to all three of these aspects. Extensive regression analysis of the template aging phenomenon shows that differences in pupil dilation, combined with certain quality factors of the sample image and the progression of time itself can significantly degrade recognition accuracy. Impactful effects can also be observed when iris recognition is employed with eyes affected by certain eye pathologies or (even more) with eyes of the deceased subjects. Notably, appropriate databases are delivered to the biometric community to stimulate further research in these utterly important areas of iris biometrics studies. Finally, some open questions are stated to inspire further discussions and research on these important topics. To Authors' best knowledge, this is the only scientific study of iris recognition reliability of such a broad scope and novelty.



### Data Dropout: Optimizing Training Data for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.00193v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00193v2)
- **Published**: 2018-09-01 14:24:36+00:00
- **Updated**: 2018-09-07 10:09:13+00:00
- **Authors**: Tianyang Wang, Jun Huan, Bo Li
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models learn to fit training data while they are highly expected to generalize well to testing data. Most works aim at finding such models by creatively designing architectures and fine-tuning parameters. To adapt to particular tasks, hand-crafted information such as image prior has also been incorporated into end-to-end learning. However, very little progress has been made on investigating how an individual training sample will influence the generalization ability of a model. In other words, to achieve high generalization accuracy, do we really need all the samples in a training dataset? In this paper, we demonstrate that deep learning models such as convolutional neural networks may not favor all training samples, and generalization accuracy can be further improved by dropping those unfavorable samples. Specifically, the influence of removing a training sample is quantifiable, and we propose a Two-Round Training approach, aiming to achieve higher generalization accuracy. We locate unfavorable samples after the first round of training, and then retrain the model from scratch with the reduced training dataset in the second round. Since our approach is essentially different from fine-tuning or further training, the computational cost should not be a concern. Our extensive experimental results indicate that, with identical settings, the proposed approach can boost performance of the well-known networks on both high-level computer vision problems such as image classification, and low-level vision problems such as image denoising.



### Improving Visual Relationship Detection using Semantic Modeling of Scene Descriptions
- **Arxiv ID**: http://arxiv.org/abs/1809.00204v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.00204v1)
- **Published**: 2018-09-01 15:11:12+00:00
- **Updated**: 2018-09-01 15:11:12+00:00
- **Authors**: Stephan Baier, Yunpu Ma, Volker Tresp
- **Comment**: None
- **Journal**: None
- **Summary**: Structured scene descriptions of images are useful for the automatic processing and querying of large image databases. We show how the combination of a semantic and a visual statistical model can improve on the task of mapping images to their associated scene description. In this paper we consider scene descriptions which are represented as a set of triples (subject, predicate, object), where each triple consists of a pair of visual objects, which appear in the image, and the relationship between them (e.g. man-riding-elephant, man-wearing-hat). We combine a standard visual model for object detection, based on convolutional neural networks, with a latent variable model for link prediction. We apply multiple state-of-the-art link prediction methods and compare their capability for visual relationship detection. One of the main advantages of link prediction methods is that they can also generalize to triples, which have never been observed in the training data. Our experimental results on the recently published Stanford Visual Relationship dataset, a challenging real world dataset, show that the integration of a semantic model using link prediction methods can significantly improve the results for visual relationship detection. Our combined approach achieves superior performance compared to the state-of-the-art method from the Stanford computer vision group.



### Assessment of iris recognition reliability for eyes affected by ocular pathologies
- **Arxiv ID**: http://arxiv.org/abs/1809.00206v1
- **DOI**: 10.1109/BTAS.2015.7358747
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00206v1)
- **Published**: 2018-09-01 15:17:43+00:00
- **Updated**: 2018-09-01 15:17:43+00:00
- **Authors**: Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz
- **Comment**: Manuscript accepted for publication at IEEE BTAS 2015. arXiv admin
  note: text overlap with arXiv:1809.00168
- **Journal**: 2015 IEEE 7th International Conference on Biometrics Theory,
  Applications and Systems (BTAS), Arlington, VA, 2015, pp. 1-6
- **Summary**: This paper presents an analysis of how the iris recognition is impacted by eye diseases and an appropriate dataset comprising 2996 iris images of 230 distinct eyes (including 184 illness-affected eyes representing more than 20 different eye conditions). The images were collected in near infrared and visible light during a routine ophthalmological practice. The experimental study shows four valuable results. First, the enrollment process is highly sensitive to those eye conditions that make the iris obstructed or introduce geometrical distortions. Second, even those conditions that do not produce visible changes to the iris structure may increase the dissimilarity among samples of the same eyes. Third, eye conditions affecting iris geometry, its tissue structure or producing obstructions significantly decrease the iris recognition reliability. Fourth, for eyes afflicted by a disease, the most prominent effect of the disease on iris recognition is to cause segmentation errors. To our knowledge this is the first database of iris images for disease-affected eyes made publicly available to researchers, and the most comprehensive study of what we can expect when the iris recognition is deployed for non-healthy eyes.



### Post-mortem Human Iris Recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.00208v1
- **DOI**: 10.1109/ICB.2016.7550073
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00208v1)
- **Published**: 2018-09-01 15:24:57+00:00
- **Updated**: 2018-09-01 15:24:57+00:00
- **Authors**: Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz
- **Comment**: Accepted for publication version of the manuscript submitted for the
  IEEE ICB 2016
- **Journal**: 2016 International Conference on Biometrics (ICB), Halmstad, 2016,
  pp. 1-6
- **Summary**: This paper presents a unique analysis of post-mortem human iris recognition. Post-mortem human iris images were collected at the university mortuary in three sessions separated by approximately 11 hours, with the first session organized from 5 to 7 hours after demise. Analysis performed for four independent iris recognition methods shows that the common claim of the iris being useless for biometric identification soon after death is not entirely true. Since the pupil has a constant and neutral dilation after death (the so called "cadaveric position"), this makes the iris pattern perfectly visible from the standpoint of dilation. We found that more than 90% of irises are still correctly recognized when captured a few hours after death, and that serious iris deterioration begins approximately 22 hours later, since the recognition rate drops to a range of 13.3-73.3% (depending on the method used) when the cornea starts to be cloudy. There were only two failures to enroll (out of 104 images) observed for only a single method (out of four employed in this study). These findings show that the dynamics of post-mortem changes to the iris that are important for biometric identification are much more moderate than previously believed. To the best of our knowledge, this paper presents the first experimental study of how iris recognition works after death, and we hope that these preliminary findings will stimulate further research in this area.



### Cataract influence on iris recognition performance
- **Arxiv ID**: http://arxiv.org/abs/1809.00211v1
- **DOI**: 10.1117/12.2076040
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00211v1)
- **Published**: 2018-09-01 15:40:47+00:00
- **Updated**: 2018-09-01 15:40:47+00:00
- **Authors**: Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz
- **Comment**: None
- **Journal**: Photonics Applications in Astronomy, Communications, Industry, and
  High-Energy Physics Experiments 2014, 929020 (2014)
- **Summary**: This paper presents the experimental study revealing weaker performance of the automatic iris recognition methods for cataract-affected eyes when compared to healthy eyes. There is little research on the topic, mostly incorporating scarce databases that are often deficient in images representing more than one illness. We built our own database, acquiring 1288 eye images of 37 patients of the Medical University of Warsaw. Those images represent several common ocular diseases, such as cataract, along with less ordinary conditions, such as iris pattern alterations derived from illness or eye trauma. Images were captured in near-infrared light (used in biometrics) and for selected cases also in visible light (used in ophthalmological diagnosis). Since cataract is a disorder that is most populated by samples in the database, in this paper we focus solely on this illness. To assess the extent of the performance deterioration we use three iris recognition methodologies (commercial and academic solutions) to calculate genuine match scores for healthy eyes and those influenced by cataract. Results show a significant degradation in iris recognition reliability manifesting by worsening the genuine scores in all three matchers used in this study (12% of genuine score increase for an academic matcher, up to 175% of genuine score increase obtained for an example commercial matcher). This increase in genuine scores affected the final false non-match rate in two matchers. To our best knowledge this is the only study of such kind that employs more than one iris matcher, and analyzes the iris image segmentation as a potential source of decreased reliability.



### Database of iris images acquired in the presence of ocular pathologies and assessment of iris recognition reliability for disease-affected eyes
- **Arxiv ID**: http://arxiv.org/abs/1809.00212v1
- **DOI**: 10.1109/CYBConf.2015.7175984
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00212v1)
- **Published**: 2018-09-01 15:47:23+00:00
- **Updated**: 2018-09-01 15:47:23+00:00
- **Authors**: Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz
- **Comment**: None
- **Journal**: 2015 IEEE 2nd International Conference on Cybernetics (CYBCONF),
  Gdynia, 2015, pp. 495-500
- **Summary**: This paper presents a database of iris images collected from disease affected eyes and an analysis related to the influence of ocular diseases on iris recognition reliability. For that purpose we have collected a database of iris images acquired for 91 different eyes during routine ophthalmology visits. This collection gathers samples for healthy eyes as well as those with various eye pathologies, including cataract, acute glaucoma, posterior and anterior synechiae, retinal detachment, rubeosis iridis, corneal vascularization, corneal grafting, iris damage and atrophy and corneal ulcers, haze or opacities. To our best knowledge this is the first database of such kind that will be made publicly available. In the analysis the data were divided into five groups of samples presenting similar anticipated impact on iris recognition: 1) healthy (no impact), 2) unaffected, clear iris (although the illness was detected), 3) geometrically distorted irides, 4) distorted iris tissue and 5) obstructed iris tissue. Three different iris recognition methods (MIRLIN, VeriEye and OSIRIS) were then used to find differences in average genuine and impostor comparison scores calculated for healthy eyes and those impacted by a disease. Specifically, we obtained significantly worse genuine comparison scores for all iris matchers and all disease-affected eyes when compared to a group of healthy eyes, what have a high potential of impacting false non-match rate.



### Iris and periocular recognition in arabian race horses using deep convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1809.00213v1
- **DOI**: 10.1109/BTAS.2017.8272736
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00213v1)
- **Published**: 2018-09-01 15:54:29+00:00
- **Updated**: 2018-09-01 15:54:29+00:00
- **Authors**: Mateusz Trokielewicz, Mateusz Szadkowski
- **Comment**: None
- **Journal**: 2017 IEEE International Joint Conference on Biometrics (IJCB),
  Denver, CO, 2017, pp. 510-516
- **Summary**: This paper presents a study devoted to recognizing horses by means of their iris and periocular features using deep convolutional neural networks (DCNNs). Identification of race horses is crucial for animal identity confirmation prior to racing. As this is usually done shortly before a race, fast and reliable methods that are friendly and inflict no harm upon animals are important. Iris recognition has been shown to work with horse irides, provided that algorithms deployed for such task are fine-tuned for horse irides and input data is of very high quality. In our work, we examine a possibility of utilizing deep convolutional neural networks for a fusion of both iris and periocular region features. With such methodology, ocular biometrics in horses could perform well without employing complicated algorithms that require a lot of fine-tuning and prior knowledge of the input image, while at the same time being rotation, translation, and to some extent also image quality invariant. We were able to achieve promising results, with EER=9.5% using two network architectures with score-level fusion.



### Iris Recognition with a Database of Iris Images Obtained in Visible Light Using Smartphone Camera
- **Arxiv ID**: http://arxiv.org/abs/1809.00214v1
- **DOI**: 10.1109/ISBA.2016.7477233
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00214v1)
- **Published**: 2018-09-01 15:55:40+00:00
- **Updated**: 2018-09-01 15:55:40+00:00
- **Authors**: Mateusz Trokielewicz
- **Comment**: Accepted version of the IEEE ISBA 2016 conference
- **Journal**: 2016 IEEE International Conference on Identity, Security and
  Behavior Analysis (ISBA), Sendai, 2016, pp. 1-6
- **Summary**: This paper delivers a new database of iris images collected in visible light using a mobile phone's camera and presents results of experiments involving existing commercial and open-source iris recognition methods, namely: IriCore, VeriEye, MIRLIN and OSIRIS. Several important observations are made.   First, we manage to show that after simple preprocessing, such images offer good visibility of iris texture even in heavily-pigmented irides. Second, for all four methods, the enrollment stage is not much affected by the fact that different type of data is used as input. This translates to zero or close-to-zero Failure To Enroll, i.e., cases when templates could not be extracted from the samples. Third, we achieved good matching accuracy, with correct genuine match rate exceeding 94.5% for all four methods, while simultaneously being able to maintain zero false match rate in every case. Correct genuine match rate of over 99.5% was achieved using one of the commercial methods, showing that such images can be used with the existing biometric solutions with minimum additional effort required. Finally, the experiments revealed that incorrect image segmentation is the most prevalent cause of recognition accuracy decrease.   To our best knowledge, this is the first database of iris images captured using a mobile device, in which image quality exceeds this of a near-infrared illuminated iris images, as defined in ISO/IEC 19794-6 and 29794-6 documents. This database will be publicly available to all researchers.



### Evaluation of Neural Networks for Image Recognition Applications: Designing a 0-1 MILP Model of a CNN to create adversarials
- **Arxiv ID**: http://arxiv.org/abs/1809.00216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00216v1)
- **Published**: 2018-09-01 16:10:28+00:00
- **Updated**: 2018-09-01 16:10:28+00:00
- **Authors**: Lucas Schelkes
- **Comment**: Thesis Bergische Universit\"at Wuppertal
- **Journal**: None
- **Summary**: Image Recognition is a central task in computer vision with applications ranging across search, robotics, self-driving cars and many others. There are three purposes of this document: 1. We follow up on (Fischetti & Jo, December, 2017) and show how standard convolutional neural network can be optimized to a more sophisticated capsule architecture. 2. We introduce a MILP model based on CNN to create adversarials. 3. We compare and evaluate each network for image recognition tasks.



### ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.00219v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00219v2)
- **Published**: 2018-09-01 16:21:03+00:00
- **Updated**: 2018-09-17 07:00:39+00:00
- **Authors**: Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, Xiaoou Tang
- **Comment**: To appear in ECCV 2018 workshop. Won Region 3 in the PIRM2018-SR
  Challenge. Code and models are at https://github.com/xinntao/ESRGAN
- **Journal**: None
- **Summary**: The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN - network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge. The code is available at https://github.com/xinntao/ESRGAN .



### VoxSegNet: Volumetric CNNs for Semantic Part Segmentation of 3D Shapes
- **Arxiv ID**: http://arxiv.org/abs/1809.00226v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.00226v1)
- **Published**: 2018-09-01 17:34:08+00:00
- **Updated**: 2018-09-01 17:34:08+00:00
- **Authors**: Zongji Wang, Feng Lu
- **Comment**: 11 pages, 10 figures
- **Journal**: None
- **Summary**: Voxel is an important format to represent geometric data, which has been widely used for 3D deep learning in shape analysis due to its generalization ability and regular data format. However, fine-grained tasks like part segmentation require detailed structural information, which increases voxel resolution and thus causes other issues such as the exhaustion of computational resources. In this paper, we propose a novel volumetric convolutional neural network, which could extract discriminative features encoding detailed information from voxelized 3D data under a limited resolution. To this purpose, a spatial dense extraction (SDE) module is designed to preserve the spatial resolution during the feature extraction procedure, alleviating the loss of detail caused by sub-sampling operations such as max-pooling. An attention feature aggregation (AFA) module is also introduced to adaptively select informative features from different abstraction scales, leading to segmentation with both semantic consistency and high accuracy of details. Experiment results on the large-scale dataset demonstrate the effectiveness of our method in 3D shape part segmentation.



### Activity Recognition on a Large Scale in Short Videos - Moments in Time Dataset
- **Arxiv ID**: http://arxiv.org/abs/1809.00241v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1809.00241v2)
- **Published**: 2018-09-01 19:39:06+00:00
- **Updated**: 2018-09-13 15:37:34+00:00
- **Authors**: Ankit Shah, Harini Kesavamoorthy, Poorva Rane, Pramati Kalwad, Alexander Hauptmann, Florian Metze
- **Comment**: Action recognition submission for Moments in Time Dataset - Improved
  results over challenge submission
- **Journal**: None
- **Summary**: Moments capture a huge part of our lives. Accurate recognition of these moments is challenging due to the diverse and complex interpretation of the moments. Action recognition refers to the act of classifying the desired action/activity present in a given video. In this work, we perform experiments on Moments in Time dataset to recognize accurately activities occurring in 3 second clips. We use state of the art techniques for visual, auditory and spatio temporal localization and develop method to accurately classify the activity in the Moments in Time dataset. Our novel approach of using Visual Based Textual features and fusion techniques performs well providing an overall 89.23 % Top - 5 accuracy on the 20 classes - a significant improvement over the Baseline TRN model.



### Car Monitoring System in Apartment Garages by Small Autonomous Car using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.00251v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1809.00251v3)
- **Published**: 2018-09-01 21:00:58+00:00
- **Updated**: 2019-09-14 21:40:42+00:00
- **Authors**: Leonardo León, Felipe Moreno-Vera, Renato Castro, José Navío, Marco Capcha
- **Comment**: 13 pages, 12 figures, Version 1 accepted in SimBig 2018. Improving to
  get better results
- **Journal**: None
- **Summary**: Currently, there is an increase in the number of Peruvian families living in apartments instead of houses for the lots of advantage; However, in some cases there are troubles such as robberies of goods that are usually left at the parking lots or the entrance of strangers that use the tenants parking lots (this last trouble sometimes is related to kidnappings or robberies in building apartments). Due to these problems, the use of a self-driving mini-car is proposed to implement a monitoring system of license plates in an underground garage inside a building using a deep learning model with the aim of recording the vehicles and identifying their owners if they were tenants or not. In addition, the small robot has its own location system using beacons that allow us to identify the position of the parking lot corresponding to each tenant of the building while the mini-car is on its way. Finally, one of the objectives of this work is to build a low-cost mini-robot that would replace expensive cameras or work together in order to keep safe the goods of tenants.



### Stochastic Dynamics for Video Infilling
- **Arxiv ID**: http://arxiv.org/abs/1809.00263v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.00263v5)
- **Published**: 2018-09-01 22:58:49+00:00
- **Updated**: 2019-06-07 09:13:07+00:00
- **Authors**: Qiangeng Xu, Hanwang Zhang, Weiyue Wang, Peter N. Belhumeur, Ulrich Neumann
- **Comment**: Winter Conference on Applications of Computer Vision (WACV 2020)
- **Journal**: None
- **Summary**: In this paper, we introduce a stochastic dynamics video infilling (SDVI) framework to generate frames between long intervals in a video. Our task differs from video interpolation which aims to produce transitional frames for a short interval between every two frames and increase the temporal resolution. Our task, namely video infilling, however, aims to infill long intervals with plausible frame sequences. Our framework models the infilling as a constrained stochastic generation process and sequentially samples dynamics from the inferred distribution. SDVI consists of two parts: (1) a bi-directional constraint propagation module to guarantee the spatial-temporal coherence among frames, (2) a stochastic sampling process to generate dynamics from the inferred distributions. Experimental results show that SDVI can generate clear frame sequences with varying contents. Moreover, motions in the generated sequence are realistic and able to transfer smoothly from the given start frame to the terminal frame. Our project site is https://xharlie.github.io/projects/project_sites/SDVI/video_results.html



