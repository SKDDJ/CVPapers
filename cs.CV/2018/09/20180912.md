# Arxiv Papers in cs.CV on 2018-09-12
### Hyperspectral Image Classification in the Presence of Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1809.04212v2
- **DOI**: 10.1109/TGRS.2018.2861992
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04212v2)
- **Published**: 2018-09-12 01:20:46+00:00
- **Updated**: 2019-04-02 02:02:57+00:00
- **Authors**: Junjun Jiang, Jiayi Ma, Zheng Wang, Chen Chen, Xianming Liu
- **Comment**: Accepted by IEEE TGRS. In this version, the Table III is revised
- **Journal**: None
- **Summary**: Label information plays an important role in supervised hyperspectral image classification problem. However, current classification methods all ignore an important and inevitable problem---labels may be corrupted and collecting clean labels for training samples is difficult, and often impractical. Therefore, how to learn from the database with noisy labels is a problem of great practical importance. In this paper, we study the influence of label noise on hyperspectral image classification, and develop a random label propagation algorithm (RLPA) to cleanse the label noise. The key idea of RLPA is to exploit knowledge (e.g., the superpixel based spectral-spatial constraints) from the observed hyperspectral images and apply it to the process of label propagation. Specifically, RLPA first constructs a spectral-spatial probability transfer matrix (SSPTM) that simultaneously considers the spectral similarity and superpixel based spatial information. It then randomly chooses some training samples as "clean" samples and sets the rest as unlabeled samples, and propagates the label information from the "clean" samples to the rest unlabeled samples with the SSPTM. By repeating the random assignment (of "clean" labeled samples and unlabeled samples) and propagation, we can obtain multiple labels for each training sample. Therefore, the final propagated label can be calculated by a majority vote algorithm. Experimental studies show that RLPA can reduce the level of noisy label and demonstrates the advantages of our proposed method over four major classifiers with a significant margin---the gains in terms of the average OA, AA, Kappa are impressive, e.g., 9.18%, 9.58%, and 0.1043. The Matlab source code is available at https://github.com/junjun-jiang/RLPA



### Ensemble of Convolutional Neural Networks for Automatic Grading of Diabetic Retinopathy and Macular Edema
- **Arxiv ID**: http://arxiv.org/abs/1809.04228v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04228v1)
- **Published**: 2018-09-12 02:29:44+00:00
- **Updated**: 2018-09-12 02:29:44+00:00
- **Authors**: Avinash Kori, Sai Saketh Chennamsetty, Mohammed Safwan K. P., Varghese Alex
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: In this manuscript, we automate the procedure of grading of diabetic retinopathy and macular edema from fundus images using an ensemble of convolutional neural networks. The availability of limited amount of labeled data to perform supervised learning was circumvented by using transfer learning approach. The models in the ensemble were pre-trained on a large dataset comprising natural images and were later fine-tuned with the limited data for the task of choice. For an image, the ensemble of classifiers generate multiple predictions, and a max-voting based approach was utilized to attain the final grade of the anomaly in the image. For the task of grading DR, on the test data (n=56), the ensemble achieved an accuracy of 83.9\%, while for the task for grading macular edema the network achieved an accuracy of 95.45% (n=44).



### Joint Segmentation and Uncertainty Visualization of Retinal Layers in Optical Coherence Tomography Images using Bayesian Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1809.04282v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04282v1)
- **Published**: 2018-09-12 07:22:15+00:00
- **Updated**: 2018-09-12 07:22:15+00:00
- **Authors**: Suman Sedai, Bhavna Antony, Dwarikanath Mahapatra, Rahil Garnavi
- **Comment**: None
- **Journal**: None
- **Summary**: Optical coherence tomography (OCT) is commonly used to analyze retinal layers for assessment of ocular diseases. In this paper, we propose a method for retinal layer segmentation and quantification of uncertainty based on Bayesian deep learning. Our method not only performs end-to-end segmentation of retinal layers, but also gives the pixel wise uncertainty measure of the segmentation output. The generated uncertainty map can be used to identify erroneously segmented image regions which is useful in downstream analysis. We have validated our method on a dataset of 1487 images obtained from 15 subjects (OCT volumes) and compared it against the state-of-the-art segmentation algorithms that does not take uncertainty into account. The proposed uncertainty based segmentation method results in comparable or improved performance, and most importantly is more robust against noise.



### Learning regression and verification networks for long-term visual tracking
- **Arxiv ID**: http://arxiv.org/abs/1809.04320v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04320v2)
- **Published**: 2018-09-12 09:13:48+00:00
- **Updated**: 2018-11-19 04:15:33+00:00
- **Authors**: Yunhua Zhang, Dong Wang, Lijun Wang, Jinqing Qi, Huchuan Lu
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Compared with short-term tracking, the long-term tracking task requires determining the tracked object is present or absent, and then estimating the accurate bounding box if present or conducting image-wide re-detection if absent. Until now, few attempts have been done although this task is much closer to designing practical tracking systems. In this work, we propose a novel long-term tracking framework based on deep regression and verification networks. The offline-trained regression model is designed using the object-aware feature fusion and region proposal networks to generate a series of candidates and estimate their similarity scores effectively. The verification network evaluates these candidates to output the optimal one as the tracked object with its classification score, which is online updated to adapt to the appearance variations based on newly reliable observations. The similarity and classification scores are combined to obtain a final confidence value, based on which our tracker can determine the absence of the target accurately and conduct image-wide re-detection to capture the target successfully when it reappears. Extensive experiments show that our tracker achieves the best performance on the VOT2018 long-term challenge and state-of-the-art results on the OxUvA long-term dataset.



### The Wisdom of MaSSeS: Majority, Subjectivity, and Semantic Similarity in the Evaluation of VQA
- **Arxiv ID**: http://arxiv.org/abs/1809.04344v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1809.04344v1)
- **Published**: 2018-09-12 10:11:39+00:00
- **Updated**: 2018-09-12 10:11:39+00:00
- **Authors**: Shailza Jolly, Sandro Pezzelle, Tassilo Klein, Andreas Dengel, Moin Nabi
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: We introduce MASSES, a simple evaluation metric for the task of Visual Question Answering (VQA). In its standard form, the VQA task is operationalized as follows: Given an image and an open-ended question in natural language, systems are required to provide a suitable answer. Currently, model performance is evaluated by means of a somehow simplistic metric: If the predicted answer is chosen by at least 3 human annotators out of 10, then it is 100% correct. Though intuitively valuable, this metric has some important limitations. First, it ignores whether the predicted answer is the one selected by the Majority (MA) of annotators. Second, it does not account for the quantitative Subjectivity (S) of the answers in the sample (and dataset). Third, information about the Semantic Similarity (SES) of the responses is completely neglected. Based on such limitations, we propose a multi-component metric that accounts for all these issues. We show that our metric is effective in providing a more fine-grained evaluation both on the quantitative and qualitative level.



### Thermal Features for Presentation Attack Detection in Hand Biometrics
- **Arxiv ID**: http://arxiv.org/abs/1809.04364v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04364v1)
- **Published**: 2018-09-12 11:36:17+00:00
- **Updated**: 2018-09-12 11:36:17+00:00
- **Authors**: Ewelina Bartuzi, Mateusz Trokielewicz
- **Comment**: Accepted for the BTAS2018, Special Session on Image And Video
  Forensics In Biometrics, 22-25 Oct, 2018, Los Angeles, USA
- **Journal**: None
- **Summary**: This paper proposes a method for utilizing thermal features of the hand for the purpose of presentation attack detection (PAD) that can be employed in a hand biometrics system's pipeline. By envisaging two different operational modes of our system, and by employing a DCNN-based classifiers fine-tuned with a dataset of real and fake hand representations captured in both visible and ther- mal spectrum, we were able to bring two important deliverables. First, a PAD method operating in an open-set mode, capable of correctly discerning 100% of fake thermal samples, achieving Attack Presentation Classification Error Rate (APCER) and Bona-Fide Presentation Classification Error Rate (BPCER) equal to 0%, which can be easily implemented into any existing system as a separate component. Second, a hand biometrics system operating in a closed-set mode, that has PAD built right into the recognition pipeline, and operating simultaneously with the user-wise classification, achieving rank-1 recognition accuracy of up to 99.75%. We also show that thermal images of the human hand, in addition to liveness features they carry, can also improve classification accuracy of a biometric system, when coupled with visible light images. To follow the reproducibility guidelines and to stimulate further research in this area, we share the trained model weights, source codes, and a newly created dataset of fake hand representations with interested researchers.



### Label Denoising with Large Ensembles of Heterogeneous Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.04403v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.04403v2)
- **Published**: 2018-09-12 13:14:59+00:00
- **Updated**: 2019-01-15 20:30:28+00:00
- **Authors**: Pavel Ostyakov, Elizaveta Logacheva, Roman Suvorov, Vladimir Aliev, Gleb Sterkin, Oleg Khomenko, Sergey I. Nikolenko
- **Comment**: None
- **Journal**: None
- **Summary**: Despite recent advances in computer vision based on various convolutional architectures, video understanding remains an important challenge. In this work, we present and discuss a top solution for the large-scale video classification (labeling) problem introduced as a Kaggle competition based on the YouTube-8M dataset. We show and compare different approaches to preprocessing, data augmentation, model architectures, and model combination. Our final model is based on a large ensemble of video- and frame-level models but fits into rather limiting hardware constraints. We apply an approach based on knowledge distillation to deal with noisy labels in the original dataset and the recently developed mixup technique to improve the basic models.



### Real-time Multiple People Tracking with Deeply Learned Candidate Selection and Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1809.04427v1
- **DOI**: 10.1109/ICME.2018.8486597
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04427v1)
- **Published**: 2018-09-12 13:39:27+00:00
- **Updated**: 2018-09-12 13:39:27+00:00
- **Authors**: Long Chen, Haizhou Ai, Zijie Zhuang, Chong Shang
- **Comment**: ICME 2018
- **Journal**: None
- **Summary**: Online multi-object tracking is a fundamental problem in time-critical video analysis applications. A major challenge in the popular tracking-by-detection framework is how to associate unreliable detection results with existing tracks. In this paper, we propose to handle unreliable detection by collecting candidates from outputs of both detection and tracking. The intuition behind generating redundant candidates is that detection and tracks can complement each other in different scenarios. Detection results of high confidence prevent tracking drifts in the long term, and predictions of tracks can handle noisy detection caused by occlusion. In order to apply optimal selection from a considerable amount of candidates in real-time, we present a novel scoring function based on a fully convolutional neural network, that shares most computations on the entire image. Moreover, we adopt a deeply learned appearance representation, which is trained on large-scale person re-identification datasets, to improve the identification ability of our tracker. Extensive experiments show that our tracker achieves real-time and state-of-the-art performance on a widely used people tracking benchmark.



### Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy
- **Arxiv ID**: http://arxiv.org/abs/1809.04430v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, physics.med-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.04430v3)
- **Published**: 2018-09-12 13:42:38+00:00
- **Updated**: 2021-01-13 17:43:14+00:00
- **Authors**: Stanislav Nikolov, Sam Blackwell, Alexei Zverovitch, Ruheena Mendes, Michelle Livne, Jeffrey De Fauw, Yojan Patel, Clemens Meyer, Harry Askham, Bernardino Romera-Paredes, Christopher Kelly, Alan Karthikesalingam, Carlton Chu, Dawn Carnell, Cheng Boon, Derek D'Souza, Syed Ali Moinuddin, Bethany Garie, Yasmin McQuinlan, Sarah Ireland, Kiarna Hampton, Krystle Fuller, Hugh Montgomery, Geraint Rees, Mustafa Suleyman, Trevor Back, Cían Hughes, Joseph R. Ledsam, Olaf Ronneberger
- **Comment**: None
- **Journal**: None
- **Summary**: Over half a million individuals are diagnosed with head and neck cancer each year worldwide. Radiotherapy is an important curative treatment for this disease, but it requires manual time consuming delineation of radio-sensitive organs at risk (OARs). This planning process can delay treatment, while also introducing inter-operator variability with resulting downstream radiation dose differences. While auto-segmentation algorithms offer a potentially time-saving solution, the challenges in defining, quantifying and achieving expert performance remain. Adopting a deep learning approach, we demonstrate a 3D U-Net architecture that achieves expert-level performance in delineating 21 distinct head and neck OARs commonly segmented in clinical practice. The model was trained on a dataset of 663 deidentified computed tomography (CT) scans acquired in routine clinical practice and with both segmentations taken from clinical practice and segmentations created by experienced radiographers as part of this research, all in accordance with consensus OAR definitions. We demonstrate the model's clinical applicability by assessing its performance on a test set of 21 CT scans from clinical practice, each with the 21 OARs segmented by two independent experts. We also introduce surface Dice similarity coefficient (surface DSC), a new metric for the comparison of organ delineation, to quantify deviation between OAR surface contours rather than volumes, better reflecting the clinical task of correcting errors in the automated organ segmentations. The model's generalisability is then demonstrated on two distinct open source datasets, reflecting different centres and countries to model training. With appropriate validation studies and regulatory approvals, this system could improve the efficiency, consistency, and safety of radiotherapy pathways.



### End-to-end depth from motion with stabilized monocular videos
- **Arxiv ID**: http://arxiv.org/abs/1809.04453v1
- **DOI**: 10.5194/isprs-annals-IV-2-W3-67-2017
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04453v1)
- **Published**: 2018-09-12 13:54:48+00:00
- **Updated**: 2018-09-12 13:54:48+00:00
- **Authors**: Clément Pinard, Laure Chevalley, Antoine Manzanera, David Filliat
- **Comment**: None
- **Journal**: ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial
  Information Sciences, Volume IV-2/W3, 2017 International Conference on
  Unmanned Aerial Vehicles in Geomatics, 4-7 September 2017, Bonn, Germany
- **Summary**: We propose a depth map inference system from monocular videos based on a novel dataset for navigation that mimics aerial footage from gimbal stabilized monocular camera in rigid scenes. Unlike most navigation datasets, the lack of rotation implies an easier structure from motion problem which can be leveraged for different kinds of tasks such as depth inference and obstacle avoidance. We also propose an architecture for end-to-end depth inference with a fully convolutional network. Results show that although tied to camera inner parameters, the problem is locally solvable and leads to good quality depth prediction.



### Multi range Real-time depth inference from a monocular stabilized footage using a Fully Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1809.04467v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04467v1)
- **Published**: 2018-09-12 14:03:33+00:00
- **Updated**: 2018-09-12 14:03:33+00:00
- **Authors**: Clément Pinard, Laure Chevalley, Antoine Manzanera, David Filliat
- **Comment**: arXiv admin note: text overlap with arXiv:1809.04453
- **Journal**: European Conference on Mobile Robotics 2017
- **Summary**: Using a neural network architecture for depth map inference from monocular stabilized videos with application to UAV videos in rigid scenes, we propose a multi-range architecture for unconstrained UAV flight, leveraging flight data from sensors to make accurate depth maps for uncluttered outdoor environment. We try our algorithm on both synthetic scenes and real UAV flight data. Quantitative results are given for synthetic scenes with a slightly noisy orientation, and show that our multi-range architecture improves depth inference. Along with this article is a video that present our results more thoroughly.



### Learning structure-from-motion from motion
- **Arxiv ID**: http://arxiv.org/abs/1809.04471v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04471v2)
- **Published**: 2018-09-12 14:10:35+00:00
- **Updated**: 2018-10-19 16:09:37+00:00
- **Authors**: Clément Pinard, Laure Chevalley, Antoine Manzanera, David Filliat
- **Comment**: None
- **Journal**: None
- **Summary**: This work is based on a questioning of the quality metrics used by deep neural networks performing depth prediction from a single image, and then of the usability of recently published works on unsupervised learning of depth from videos. To overcome their limitations, we propose to learn in the same unsupervised manner a depth map inference system from monocular videos that takes a pair of images as input. This algorithm actually learns structure-from-motion from motion, and not only structure from context appearance. The scale factor issue is explicitly treated, and the absolute depth map can be estimated from camera displacement magnitude, which can be easily measured from cheap external sensors. Our solution is also much more robust with respect to domain variation and adaptation via fine tuning, because it does not rely entirely in depth from context. Two use cases are considered, unstabilized moving camera videos, and stabilized ones. This choice is motivated by the UAV (for Unmanned Aerial Vehicle) use case that generally provides reliable orientation measurement. We provide a set of experiments showing that, used in real conditions where only speed can be known, our network outperforms competitors for most depth quality measures. Results are given on the well known KITTI dataset, which provides robust stabilization for our second use case, but also contains moving scenes which are very typical of the in-car road context. We then present results on a synthetic dataset that we believe to be more representative of typical UAV scenes. Lastly, we present two domain adaptation use cases showing superior robustness of our method compared to single view depth algorithms, which indicates that it is better suited for highly variable visual contexts.



### Joint Sub-bands Learning with Clique Structures for Wavelet Domain Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1809.04508v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04508v3)
- **Published**: 2018-09-12 15:19:37+00:00
- **Updated**: 2018-10-25 12:29:12+00:00
- **Authors**: Zhisheng Zhong, Tiancheng Shen, Yibo Yang, Zhouchen Lin, Chao Zhang
- **Comment**: Accepted in NIPS 2018
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have recently achieved great success in single-image super-resolution (SISR). However, these methods tend to produce over-smoothed outputs and miss some textural details. To solve these problems, we propose the Super-Resolution CliqueNet (SRCliqueNet) to reconstruct the high resolution (HR) image with better textural details in the wavelet domain. The proposed SRCliqueNet firstly extracts a set of feature maps from the low resolution (LR) image by the clique blocks group. Then we send the set of feature maps to the clique up-sampling module to reconstruct the HR image. The clique up-sampling module consists of four sub-nets which predict the high resolution wavelet coefficients of four sub-bands. Since we consider the edge feature properties of four sub-bands, the four sub-nets are connected to the others so that they can learn the coefficients of four sub-bands jointly. Finally we apply inverse discrete wavelet transform (IDWT) to the output of four sub-nets at the end of the clique up-sampling module to increase the resolution and reconstruct the HR image. Extensive quantitative and qualitative experiments on benchmark datasets show that our method achieves superior performance over the state-of-the-art methods.



### Image contrast enhancement using fuzzy logic
- **Arxiv ID**: http://arxiv.org/abs/1809.04529v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04529v1)
- **Published**: 2018-09-12 15:46:48+00:00
- **Updated**: 2018-09-12 15:46:48+00:00
- **Authors**: Sandeep Joshi, Samrudh Kumar
- **Comment**: 4 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: Image enhancement is a method of improving the quality of an image and contrast is a major aspect. Traditional methods of contrast enhancement like histogram equalization results in over/under enhancement of the image especially a lower resolution one. This paper aims at developing a new Fuzzy Inference System to enhance the contrast of the low resolution images overcoming the shortcomings of the traditional methods. Results obtained using both the approaches are compared.



### Unpaired Brain MR-to-CT Synthesis using a Structure-Constrained CycleGAN
- **Arxiv ID**: http://arxiv.org/abs/1809.04536v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04536v1)
- **Published**: 2018-09-12 15:56:42+00:00
- **Updated**: 2018-09-12 15:56:42+00:00
- **Authors**: Heran Yang, Jian Sun, Aaron Carass, Can Zhao, Junghoon Lee, Zongben Xu, Jerry Prince
- **Comment**: 8 pages, 5 figures, accepted by MICCAI 2018 Workshop: Deep Learning
  in Medical Image Analysis (DLMIA)
- **Journal**: None
- **Summary**: The cycleGAN is becoming an influential method in medical image synthesis. However, due to a lack of direct constraints between input and synthetic images, the cycleGAN cannot guarantee structural consistency between these two images, and such consistency is of extreme importance in medical imaging. To overcome this, we propose a structure-constrained cycleGAN for brain MR-to-CT synthesis using unpaired data that defines an extra structure-consistency loss based on the modality independent neighborhood descriptor to constrain structural consistency. Additionally, we use a position-based selection strategy for selecting training images instead of a completely random selection scheme. Experimental results on synthesizing CT images from brain MR images demonstrate that our method is better than the conventional cycleGAN and approximates the cycleGAN trained with paired data.



### End-to-end Audiovisual Speech Activity Detection with Bimodal Recurrent Neural Models
- **Arxiv ID**: http://arxiv.org/abs/1809.04553v1
- **DOI**: 10.1016/j.specom.2019.07.003
- **Categories**: **cs.CL**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1809.04553v1)
- **Published**: 2018-09-12 16:44:46+00:00
- **Updated**: 2018-09-12 16:44:46+00:00
- **Authors**: Fei Tao, Carlos Busso
- **Comment**: Submitted to Speech Communication
- **Journal**: Speech Communication, vol. 113, pp. 25-35, October 2019
- **Summary**: Speech activity detection (SAD) plays an important role in current speech processing systems, including automatic speech recognition (ASR). SAD is particularly difficult in environments with acoustic noise. A practical solution is to incorporate visual information, increasing the robustness of the SAD approach. An audiovisual system has the advantage of being robust to different speech modes (e.g., whisper speech) or background noise. Recent advances in audiovisual speech processing using deep learning have opened opportunities to capture in a principled way the temporal relationships between acoustic and visual features. This study explores this idea proposing a \emph{bimodal recurrent neural network} (BRNN) framework for SAD. The approach models the temporal dynamic of the sequential audiovisual data, improving the accuracy and robustness of the proposed SAD system. Instead of estimating hand-crafted features, the study investigates an end-to-end training approach, where acoustic and visual features are directly learned from the raw data during training. The experimental evaluation considers a large audiovisual corpus with over 60.8 hours of recordings, collected from 105 speakers. The results demonstrate that the proposed framework leads to absolute improvements up to 1.2% under practical scenarios over a VAD baseline using only audio implemented with deep neural network (DNN). The proposed approach achieves 92.7% F1-score when it is evaluated using the sensors from a portable tablet under noisy acoustic environment, which is only 1.0% lower than the performance obtained under ideal conditions (e.g., clean speech obtained with a high definition camera and a close-talking microphone).



### Game-Based Video-Context Dialogue
- **Arxiv ID**: http://arxiv.org/abs/1809.04560v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.04560v2)
- **Published**: 2018-09-12 16:53:13+00:00
- **Updated**: 2018-10-17 15:26:48+00:00
- **Authors**: Ramakanth Pasunuru, Mohit Bansal
- **Comment**: EMNLP 2018 (14 pages) (fixed Table5 typo in v2)
- **Journal**: None
- **Summary**: Current dialogue systems focus more on textual and speech context knowledge and are usually based on two speakers. Some recent work has investigated static image-based dialogue. However, several real-world human interactions also involve dynamic visual context (similar to videos) as well as dialogue exchanges among multiple speakers. To move closer towards such multimodal conversational skills and visually-situated applications, we introduce a new video-context, many-speaker dialogue dataset based on live-broadcast soccer game videos and chats from Twitch.tv. This challenging testbed allows us to develop visually-grounded dialogue models that should generate relevant temporal and spatial event language from the live video, while also being relevant to the chat history. For strong baselines, we also present several discriminative and generative models, e.g., based on tridirectional attention flow (TriDAF). We evaluate these models via retrieval ranking-recall, automatic phrase-matching metrics, as well as human evaluation studies. We also present dataset analyses, model ablations, and visualizations to understand the contribution of different modalities and model components.



### A Two-Step Learning Method For Detecting Landmarks on Faces From Different Domains
- **Arxiv ID**: http://arxiv.org/abs/1809.04621v1
- **DOI**: 10.1109/ICIP.2018.8451026
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.04621v1)
- **Published**: 2018-09-12 18:14:12+00:00
- **Updated**: 2018-09-12 18:14:12+00:00
- **Authors**: Bruna Vieira Frade, Erickson R. Nascimento
- **Comment**: https://ieeexplore.ieee.org/document/8451026/
- **Journal**: None
- **Summary**: The detection of fiducial points on faces has significantly been favored by the rapid progress in the field of machine learning, in particular in the convolution networks. However, the accuracy of most of the detectors strongly depends on an enormous amount of annotated data. In this work, we present a domain adaptation approach based on a two-step learning to detect fiducial points on human and animal faces. We evaluate our method on three different datasets composed of different animal faces (cats, dogs, and horses). The experiments show that our method performs better than state of the art and can use few annotated data to leverage the detection of landmarks reducing the demand for large volume of annotated data.



### Visual-Quality-Driven Learning for Underwater Vision Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1809.04624v1
- **DOI**: 10.1109/ICIP.2018.8451356
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1809.04624v1)
- **Published**: 2018-09-12 18:22:38+00:00
- **Updated**: 2018-09-12 18:22:38+00:00
- **Authors**: Walysson Vital Barbosa, Henrique Grandinetti Barbosa Amaral, Thiago Lages Rocha, Erickson Rangel Nascimento
- **Comment**: Accepted for publication and presented in 2018 IEEE International
  Conference on Image Processing (ICIP)
- **Journal**: None
- **Summary**: The image processing community has witnessed remarkable advances in enhancing and restoring images. Nevertheless, restoring the visual quality of underwater images remains a great challenge. End-to-end frameworks might fail to enhance the visual quality of underwater images since in several scenarios it is not feasible to provide the ground truth of the scene radiance. In this work, we propose a CNN-based approach that does not require ground truth data since it uses a set of image quality metrics to guide the restoration learning process. The experiments showed that our method improved the visual quality of underwater images preserving their edges and also performed well considering the UCIQE metric.



### Deep Spectral Correspondence for Matching Disparate Image Pairs
- **Arxiv ID**: http://arxiv.org/abs/1809.04642v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04642v1)
- **Published**: 2018-09-12 19:33:26+00:00
- **Updated**: 2018-09-12 19:33:26+00:00
- **Authors**: Arun CS Kumar, Shefali Srivastava, Anirban Mukhopadhyay, Suchendra M. Bhandarkar
- **Comment**: 43 pages, under submission to Computer Vision and Image Understanding
  Journal
- **Journal**: None
- **Summary**: A novel, non-learning-based, saliency-aware, shape-cognizant correspondence determination technique is proposed for matching image pairs that are significantly disparate in nature. Images in the real world often exhibit high degrees of variation in scale, orientation, viewpoint, illumination and affine projection parameters, and are often accompanied by the presence of textureless regions and complete or partial occlusion of scene objects. The above conditions confound most correspondence determination techniques by rendering impractical the use of global contour-based descriptors or local pixel-level features for establishing correspondence. The proposed deep spectral correspondence (DSC) determination scheme harnesses the representational power of local feature descriptors to derive a complex high-level global shape representation for matching disparate images. The proposed scheme reasons about correspondence between disparate images using high-level global shape cues derived from low-level local feature descriptors. Consequently, the proposed scheme enjoys the best of both worlds, i.e., a high degree of invariance to affine parameters such as scale, orientation, viewpoint, illumination afforded by the global shape cues and robustness to occlusion provided by the low-level feature descriptors. While the shape-based component within the proposed scheme infers what to look for, an additional saliency-based component dictates where to look at thereby tackling the noisy correspondences arising from the presence of textureless regions and complex backgrounds. In the proposed scheme, a joint image graph is constructed using distances computed between interest points in the appearance (i.e., image) space. Eigenspectral decomposition of the joint image graph allows for reasoning about shape similarity to be performed jointly, in the appearance space and eigenspace.



### Are object detection assessment criteria ready for maritime computer vision?
- **Arxiv ID**: http://arxiv.org/abs/1809.04659v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04659v2)
- **Published**: 2018-09-12 20:18:04+00:00
- **Updated**: 2019-11-17 16:29:39+00:00
- **Authors**: Dilip K. Prasad, Huixu Dong, Deepu Rajan, Chai Quek
- **Comment**: None
- **Journal**: IEEE Transactions on Intelligent Transportation Systems (2020)
- **Summary**: Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime setting. Thus, a large body of related work in computer vision appears inapplicable to the maritime setting at the first sight. We discuss the problem of defining assessment metrics suitable for maritime computer vision. We consider new bottom edge proximity metrics as assessment metrics for maritime computer vision. These metrics indicate that existing computer vision approaches are indeed promising for maritime computer vision and can play a foundational role in the emerging field of maritime computer vision.



### An Online Plug-and-Play Algorithm for Regularized Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1809.04693v1
- **DOI**: 10.1109/TCI.2019.2893568
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1809.04693v1)
- **Published**: 2018-09-12 22:26:26+00:00
- **Updated**: 2018-09-12 22:26:26+00:00
- **Authors**: Yu Sun, Brendt Wohlberg, Ulugbek S. Kamilov
- **Comment**: None
- **Journal**: None
- **Summary**: Plug-and-play priors (PnP) is a powerful framework for regularizing imaging inverse problems by using advanced denoisers within an iterative algorithm. Recent experimental evidence suggests that PnP algorithms achieve state-of-the-art performance in a range of imaging applications. In this paper, we introduce a new online PnP algorithm based on the iterative shrinkage/thresholding algorithm (ISTA). The proposed algorithm uses only a subset of measurements at every iteration, which makes it scalable to very large datasets. We present a new theoretical convergence analysis, for both batch and online variants of PnP-ISTA, for denoisers that do not necessarily correspond to proximal operators. We also present simulations illustrating the applicability of the algorithm to image reconstruction in diffraction tomography. The results in this paper have the potential to expand the applicability of the PnP framework to very large and redundant datasets.



### Geometric Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1809.04696v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04696v2)
- **Published**: 2018-09-12 22:38:22+00:00
- **Updated**: 2018-12-01 21:32:01+00:00
- **Authors**: Hassan Abu Alhaija, Siva Karthik Mustikovela, Andreas Geiger, Carsten Rother
- **Comment**: None
- **Journal**: None
- **Summary**: The task of generating natural images from 3D scenes has been a long standing goal in computer graphics. On the other hand, recent developments in deep neural networks allow for trainable models that can produce natural-looking images with little or no knowledge about the scene structure. While the generated images often consist of realistic looking local patterns, the overall structure of the generated images is often inconsistent. In this work we propose a trainable, geometry-aware image generation method that leverages various types of scene information, including geometry and segmentation, to create realistic looking natural images that match the desired scene structure. Our geometrically-consistent image synthesis method is a deep neural network, called Geometry to Image Synthesis (GIS) framework, which retains the advantages of a trainable method, e.g., differentiability and adaptiveness, but, at the same time, makes a step towards the generalizability, control and quality output of modern graphics rendering engines. We utilize the GIS framework to insert vehicles in outdoor driving scenes, as well as to generate novel views of objects from the Linemod dataset. We qualitatively show that our network is able to generalize beyond the training set to novel scene geometries, object shapes and segmentations. Furthermore, we quantitatively show that the GIS framework can be used to synthesize large amounts of training data which proves beneficial for training instance segmentation models.



### Do-It-Yourself Single Camera 3D Pointer Input Device
- **Arxiv ID**: http://arxiv.org/abs/1809.04704v1
- **DOI**: 10.1109/CRV.2018.00038
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.04704v1)
- **Published**: 2018-09-12 23:07:43+00:00
- **Updated**: 2018-09-12 23:07:43+00:00
- **Authors**: Bernard Llanos, Yee-Hong Yang
- **Comment**: 8 pages, 6 figures, 2018 15th Conference on Computer and Robot Vision
- **Journal**: B. Llanos, and Y.-H. Yang, "Do-It-Yourself Single Camera 3D
  Pointer Input Device," in 2018 15th Conference on Computer and Robot Vision
  (CRV), 2018, pp. 214-221
- **Summary**: We present a new algorithm for single camera 3D reconstruction, or 3D input for human-computer interfaces, based on precise tracking of an elongated object, such as a pen, having a pattern of colored bands. To configure the system, the user provides no more than one labelled image of a handmade pointer, measurements of its colored bands, and the camera's pinhole projection matrix. Other systems are of much higher cost and complexity, requiring combinations of multiple cameras, stereocameras, and pointers with sensors and lights. Instead of relying on information from multiple devices, we examine our single view more closely, integrating geometric and appearance constraints to robustly track the pointer in the presence of occlusion and distractor objects. By probing objects of known geometry with the pointer, we demonstrate acceptable accuracy of 3D localization.



### Linear Algebra and Duality of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.04711v2
- **DOI**: None
- **Categories**: **cs.CV**, E.0; G.1.3; G.1.6; I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/1809.04711v2)
- **Published**: 2018-09-12 23:39:18+00:00
- **Updated**: 2018-09-17 22:23:26+00:00
- **Authors**: Galin Georgiev
- **Comment**: 48 pages with number of figures
- **Journal**: None
- **Summary**: Bases, mappings, projections and metrics, natural for Neural network training, are introduced. Graph-theoretical interpretation is offered. Non-Gaussianity naturally emerges, even in relatively simple datasets. Training statistics, hierarchies and energies are analyzed, from physics point of view. Duality between observables (for example, pixels) and observations is established. Relationship between exact and numerical solutions is studied. Physics and financial mathematics interpretations of a key problem are offered. Examples support all new concepts.



