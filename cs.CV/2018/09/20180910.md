# Arxiv Papers in cs.CV on 2018-09-10
### Memristive LSTM network hardware architecture for time-series predictive modeling problem
- **Arxiv ID**: http://arxiv.org/abs/1809.03119v1
- **DOI**: None
- **Categories**: **cs.ET**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.03119v1)
- **Published**: 2018-09-10 03:35:33+00:00
- **Updated**: 2018-09-10 03:35:33+00:00
- **Authors**: Kazybek Adam, Kamilya Smagulova, Alex Pappachen James
- **Comment**: IEEE Asia Pacific Conference on Circuits and Systems, 2018
- **Journal**: None
- **Summary**: Analysis of time-series data allows to identify long-term trends and make predictions that can help to improve our lives. With the rapid development of artificial neural networks, long short-term memory (LSTM) recurrent neural network (RNN) configuration is found to be capable in dealing with time-series forecasting problems where data points are time-dependent and possess seasonality trends. Gated structure of LSTM cell and flexibility in network topology (one-to-many, many-to-one, etc.) allows to model systems with multiple input variables and control several parameters such as the size of the look-back window to make a prediction and number of time steps to be predicted. These make LSTM attractive tool over conventional methods such as autoregression models, the simple average, moving average, naive approach, ARIMA, Holt's linear trend method, Holt's Winter seasonal method, and others. In this paper, we propose a hardware implementation of LSTM network architecture for time-series forecasting problem. All simulations were performed using TSMC 0.18um CMOS technology and HP memristor model.



### Tracking by Animation: Unsupervised Learning of Multi-Object Attentive Trackers
- **Arxiv ID**: http://arxiv.org/abs/1809.03137v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.03137v3)
- **Published**: 2018-09-10 04:59:25+00:00
- **Updated**: 2019-04-09 02:02:16+00:00
- **Authors**: Zhen He, Jian Li, Daxue Liu, Hangen He, David Barber
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Online Multi-Object Tracking (MOT) from videos is a challenging computer vision task which has been extensively studied for decades. Most of the existing MOT algorithms are based on the Tracking-by-Detection (TBD) paradigm combined with popular machine learning approaches which largely reduce the human effort to tune algorithm parameters. However, the commonly used supervised learning approaches require the labeled data (e.g., bounding boxes), which is expensive for videos. Also, the TBD framework is usually suboptimal since it is not end-to-end, i.e., it considers the task as detection and tracking, but not jointly. To achieve both label-free and end-to-end learning of MOT, we propose a Tracking-by-Animation framework, where a differentiable neural model first tracks objects from input frames and then animates these objects into reconstructed frames. Learning is then driven by the reconstruction error through backpropagation. We further propose a Reprioritized Attentive Tracking to improve the robustness of data association. Experiments conducted on both synthetic and real video datasets show the potential of the proposed model. Our project page is publicly available at: https://github.com/zhen-he/tracking-by-animation



### Deep MR Image Super-Resolution Using Structural Priors
- **Arxiv ID**: http://arxiv.org/abs/1809.03140v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.03140v1)
- **Published**: 2018-09-10 05:20:26+00:00
- **Updated**: 2018-09-10 05:20:26+00:00
- **Authors**: Venkateswararao Cherukuri, Tiantong Guo, Steven J. Schiff, Vishal Monga
- **Comment**: Accepted to IEEE ICIP 2018
- **Journal**: None
- **Summary**: High resolution magnetic resonance (MR) images are desired for accurate diagnostics. In practice, image resolution is restricted by factors like hardware, cost and processing constraints. Recently, deep learning methods have been shown to produce compelling state of the art results for image super-resolution. Paying particular attention to desired hi-resolution MR image structure, we propose a new regularized network that exploits image priors, namely a low-rank structure and a sharpness prior to enhance deep MR image superresolution. Our contributions are then incorporating these priors in an analytically tractable fashion in the learning of a convolutional neural network (CNN) that accomplishes the super-resolution task. This is particularly challenging for the low rank prior, since the rank is not a differentiable function of the image matrix (and hence the network parameters), an issue we address by pursuing differentiable approximations of the rank. Sharpness is emphasized by the variance of the Laplacian which we show can be implemented by a fixed {\em feedback} layer at the output of the network. Experiments performed on two publicly available MR brain image databases exhibit promising results particularly when training imagery is limited.



### The AAU Multimodal Annotation Toolboxes: Annotating Objects in Images and Videos
- **Arxiv ID**: http://arxiv.org/abs/1809.03171v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03171v1)
- **Published**: 2018-09-10 08:09:42+00:00
- **Updated**: 2018-09-10 08:09:42+00:00
- **Authors**: Chris H. Bahnsen, Andreas Møgelmose, Thomas B. Moeslund
- **Comment**: 6 pages, 10 figures
- **Journal**: None
- **Summary**: This tech report gives an introduction to two annotation toolboxes that enable the creation of pixel and polygon-based masks as well as bounding boxes around objects of interest. Both toolboxes support the annotation of sequential images in the RGB and thermal modalities. Each annotated object is assigned a classification tag, a unique ID, and one or more optional meta data tags. The toolboxes are written in C++ with the OpenCV and Qt libraries and are operated by using the visual interface and the extensive range of keyboard shortcuts. Pre-built binaries are available for Windows and MacOS and the tools can be built from source under Linux as well. So far, tens of thousands of frames have been annotated using the toolboxes.



### Geoseg: A Computer Vision Package for Automatic Building Segmentation and Outline Extraction
- **Arxiv ID**: http://arxiv.org/abs/1809.03175v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03175v1)
- **Published**: 2018-09-10 08:29:42+00:00
- **Updated**: 2018-09-10 08:29:42+00:00
- **Authors**: Guangming Wu, Zhiling Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep learning algorithms, especially fully convolutional network based methods, are becoming very popular in the field of remote sensing. However, these methods are implemented and evaluated through various datasets and deep learning frameworks. There has not been a package that covers these methods in a unifying manner. In this study, we introduce a computer vision package termed Geoseg that focus on building segmentation and outline extraction. Geoseg implements over nine state-of-the-art models as well as utility scripts needed to conduct model training, logging, evaluating and visualization. The implementation of Geoseg emphasizes unification, simplicity, and flexibility. The performance and computational efficiency of all implemented methods are evaluated by comparison experiment through a unified, high-quality aerial image dataset.



### Shallow vs deep learning architectures for white matter lesion segmentation in the early stages of multiple sclerosis
- **Arxiv ID**: http://arxiv.org/abs/1809.03185v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.03185v1)
- **Published**: 2018-09-10 08:50:34+00:00
- **Updated**: 2018-09-10 08:50:34+00:00
- **Authors**: Francesco La Rosa, Mário João Fartaria, Tobias Kober, Jonas Richiardi, Cristina Granziera, Jean-Philippe Thiran, Meritxell Bach Cuadra
- **Comment**: Accepted to the MICCAI 2018 Brain Lesion (BrainLes) workshop
- **Journal**: None
- **Summary**: In this work, we present a comparison of a shallow and a deep learning architecture for the automated segmentation of white matter lesions in MR images of multiple sclerosis patients. In particular, we train and test both methods on early stage disease patients, to verify their performance in challenging conditions, more similar to a clinical setting than what is typically provided in multiple sclerosis segmentation challenges. Furthermore, we evaluate a prototype naive combination of the two methods, which refines the final segmentation. All methods were trained on 32 patients, and the evaluation was performed on a pure test set of 73 cases. Results show low lesion-wise false positives (30%) for the deep learning architecture, whereas the shallow architecture yields the best Dice coefficient (63%) and volume difference (19%). Combining both shallow and deep architectures further improves the lesion-wise metrics (69% and 26% lesion-wise true and false positive rate, respectively).



### Recent Advances in Object Detection in the Age of Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.03193v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03193v2)
- **Published**: 2018-09-10 09:09:12+00:00
- **Updated**: 2019-08-20 11:58:17+00:00
- **Authors**: Shivang Agarwal, Jean Ogier Du Terrail, Frédéric Jurie
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection-the computer vision task dealing with detecting instances of objects of a certain class (e.g., 'car', 'plane', etc.) in images-attracted a lot of attention from the community during the last 5 years. This strong interest can be explained not only by the importance this task has for many applications but also by the phenomenal advances in this area since the arrival of deep convolutional neural networks (DCNN). This article reviews the recent literature on object detection with deep CNN, in a comprehensive way, and provides an in-depth view of these recent advances. The survey covers not only the typical architectures (SSD, YOLO, Faster-RCNN) but also discusses the challenges currently met by the community and goes on to show how the problem of object detection can be extended. This survey also reviews the public datasets and associated state-of-the-art algorithms.



### Hand-tremor frequency estimation in videos
- **Arxiv ID**: http://arxiv.org/abs/1809.03218v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03218v1)
- **Published**: 2018-09-10 09:43:19+00:00
- **Updated**: 2018-09-10 09:43:19+00:00
- **Authors**: Silvia L. Pintea, Jian Zheng, Xilin Li, Paulina J. M. Bank, Jacobus J. van Hilten, Jan C. van Gemert
- **Comment**: Best paper at ECCV-2018 Workshop on Observing and Understanding Hands
  in Action
- **Journal**: None
- **Summary**: We focus on the problem of estimating human hand-tremor frequency from input RGB video data. Estimating tremors from video is important for non-invasive monitoring, analyzing and diagnosing patients suffering from motor-disorders such as Parkinson's disease. We consider two approaches for hand-tremor frequency estimation: (a) a Lagrangian approach where we detect the hand at every frame in the video, and estimate the tremor frequency along the trajectory; and (b) an Eulerian approach where we first localize the hand, we subsequently remove the large motion along the movement trajectory of the hand, and we use the video information over time encoded as intensity values or phase information to estimate the tremor frequency.   We estimate hand tremors on a new human tremor dataset, TIM-Tremor, containing static tasks as well as a multitude of more dynamic tasks, involving larger motion of the hands. The dataset has 55 tremor patient recordings together with: associated ground truth accelerometer data from the most affected hand, RGB video data, and aligned depth data.



### Multi-Context Deep Network for Angle-Closure Glaucoma Screening in Anterior Segment OCT
- **Arxiv ID**: http://arxiv.org/abs/1809.03239v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03239v1)
- **Published**: 2018-09-10 11:26:33+00:00
- **Updated**: 2018-09-10 11:26:33+00:00
- **Authors**: Huazhu Fu, Yanwu Xu, Stephen Lin, Damon Wing Kee Wong, Baskaran Mani, Meenakshi Mahesh, Tin Aung, Jiang Liu
- **Comment**: Accepted by Medical Image Computing and Computer Assisted
  Intervention (MICCAI) 2018
- **Journal**: None
- **Summary**: A major cause of irreversible visual impairment is angle-closure glaucoma, which can be screened through imagery from Anterior Segment Optical Coherence Tomography (AS-OCT). Previous computational diagnostic techniques address this screening problem by extracting specific clinical measurements or handcrafted visual features from the images for classification. In this paper, we instead propose to learn from training data a discriminative representation that may capture subtle visual cues not modeled by predefined features. Based on clinical priors, we formulate this learning with a presented Multi-Context Deep Network (MCDN) architecture, in which parallel Convolutional Neural Networks are applied to particular image regions and at corresponding scales known to be informative for clinically diagnosing angle-closure glaucoma. The output feature maps of the parallel streams are merged into a classification layer to produce the deep screening result. Moreover, we incorporate estimated clinical parameters to further enhance performance. On a clinical AS-OCT dataset, our system is validated through comparisons to previous screening methods.



### Using phase instead of optical flow for action recognition
- **Arxiv ID**: http://arxiv.org/abs/1809.03258v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03258v2)
- **Published**: 2018-09-10 12:07:17+00:00
- **Updated**: 2018-09-14 12:35:09+00:00
- **Authors**: Omar Hommos, Silvia L. Pintea, Pascal S. M. Mettes, Jan C. van Gemert
- **Comment**: ECCV-2018 Workshop on "What is Optical Flow for?"
- **Journal**: None
- **Summary**: Currently, the most common motion representation for action recognition is optical flow. Optical flow is based on particle tracking which adheres to a Lagrangian perspective on dynamics. In contrast to the Lagrangian perspective, the Eulerian model of dynamics does not track, but describes local changes. For video, an Eulerian phase-based motion representation, using complex steerable filters, has been successfully employed recently for motion magnification and video frame interpolation. Inspired by these previous works, here, we proposes learning Eulerian motion representations in a deep architecture for action recognition. We learn filters in the complex domain in an end-to-end manner. We design these complex filters to resemble complex Gabor filters, typically employed for phase-information extraction. We propose a phase-information extraction module, based on these complex filters, that can be used in any network architecture for extracting Eulerian representations. We experimentally analyze the added value of Eulerian motion representations, as extracted by our proposed phase extraction module, and compare with existing motion representations based on optical flow, on the UCF101 dataset.



### Classification by Re-generation: Towards Classification Based on Variational Inference
- **Arxiv ID**: http://arxiv.org/abs/1809.03259v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03259v1)
- **Published**: 2018-09-10 12:08:52+00:00
- **Updated**: 2018-09-10 12:08:52+00:00
- **Authors**: Shideh Rezaeifar, Olga Taran, Slava Voloshynovskiy
- **Comment**: None
- **Journal**: None
- **Summary**: As Deep Neural Networks (DNNs) are considered the state-of-the-art in many classification tasks, the question of their semantic generalizations has been raised. To address semantic interpretability of learned features, we introduce a novel idea of classification by re-generation based on variational autoencoder (VAE) in which a separate encoder-decoder pair of VAE is trained for each class. Moreover, the proposed architecture overcomes the scalability issue in current DNN networks as there is no need to re-train the whole network with the addition of new classes and it can be done for each class separately. We also introduce a criterion based on Kullback-Leibler divergence to reject doubtful examples. This rejection criterion should improve the trust in the obtained results and can be further exploited to reject adversarial examples.



### A Brief Review of Real-World Color Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1809.03298v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03298v1)
- **Published**: 2018-09-10 13:30:15+00:00
- **Updated**: 2018-09-10 13:30:15+00:00
- **Authors**: Zhaoming Kong, Xiaowei Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Filtering real-world color images is challenging due to the complexity of noise that can not be formulated as a certain distribution. However, the rapid development of camera lens pos- es greater demands on image denoising in terms of both efficiency and effectiveness. Currently, the most widely accepted framework employs the combination of transform domain techniques and nonlocal similarity characteristics of natural images. Based on this framework, many competitive methods model the correlation of R, G, B channels with pre-defined or adaptively learned transforms. In this chapter, a brief review of related methods and publicly available datasets is presented, moreover, a new dataset that includes more natural outdoor scenes is introduced. Extensive experiments are performed and discussion on visual effect enhancement is included.



### Interactive Binary Image Segmentation with Edge Preservation
- **Arxiv ID**: http://arxiv.org/abs/1809.03334v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03334v1)
- **Published**: 2018-09-10 14:14:21+00:00
- **Updated**: 2018-09-10 14:14:21+00:00
- **Authors**: Jianfeng Zhang, Liezhuo Zhang, Yuankai Teng, Xiaoping Zhang, Song Wang, Lili Ju
- **Comment**: None
- **Journal**: None
- **Summary**: Binary image segmentation plays an important role in computer vision and has been widely used in many applications such as image and video editing, object extraction, and photo composition. In this paper, we propose a novel interactive binary image segmentation method based on the Markov Random Field (MRF) framework and the fast bilateral solver (FBS) technique. Specifically, we employ the geodesic distance component to build the unary term. To ensure both computation efficiency and effective responsiveness for interactive segmentation, superpixels are used in computing geodesic distances instead of pixels. Furthermore, we take a bilateral affinity approach for the pairwise term in order to preserve edge information and denoise. Through the alternating direction strategy, the MRF energy minimization problem is divided into two subproblems, which then can be easily solved by steepest gradient descent (SGD) and FBS respectively. Experimental results on the VGG interactive image segmentation dataset show that the proposed algorithm outperforms several state-of-the-art ones, and in particular, it can achieve satisfactory edge-smooth segmentation results even when the foreground and background color appearances are quite indistinctive.



### A Comparison of CNN-based Face and Head Detectors for Real-Time Video Surveillance Applications
- **Arxiv ID**: http://arxiv.org/abs/1809.03336v1
- **DOI**: 10.1109/IPTA.2017.8310113
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03336v1)
- **Published**: 2018-09-10 14:14:34+00:00
- **Updated**: 2018-09-10 14:14:34+00:00
- **Authors**: Le Thanh Nguyen-Meidine, Eric Granger, Madhu Kiran, Louis-Antoine Blais-Morin
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting faces and heads appearing in video feeds are challenging tasks in real-world video surveillance applications due to variations in appearance, occlusions and complex backgrounds. Recently, several CNN architectures have been proposed to increase the accuracy of detectors, although their computational complexity can be an issue, especially for real-time applications, where faces and heads must be detected live using high-resolution cameras. This paper compares the accuracy and complexity of state-of-the-art CNN architectures that are suitable for face and head detection. Single pass and region-based architectures are reviewed and compared empirically to baseline techniques according to accuracy and to time and memory complexity on images from several challenging datasets. The viability of these architectures is analyzed with real-time video surveillance applications in mind. Results suggest that, although CNN architectures can achieve a very high level of accuracy compared to traditional detectors, their computational cost can represent a limitation for many practical real-time applications.



### Learning to Zoom: a Saliency-Based Sampling Layer for Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1809.03355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03355v1)
- **Published**: 2018-09-10 14:36:15+00:00
- **Updated**: 2018-09-10 14:36:15+00:00
- **Authors**: Adrià Recasens, Petr Kellnhofer, Simon Stent, Wojciech Matusik, Antonio Torralba
- **Comment**: European Conference on Computer Vision, 2018, 14 pages, 6 figures
- **Journal**: None
- **Summary**: We introduce a saliency-based distortion layer for convolutional neural networks that helps to improve the spatial sampling of input data for a given task. Our differentiable layer can be added as a preprocessing block to existing task networks and trained altogether in an end-to-end fashion. The effect of the layer is to efficiently estimate how to sample from the original data in order to boost task performance. For example, for an image classification task in which the original data might range in size up to several megapixels, but where the desired input images to the task network are much smaller, our layer learns how best to sample from the underlying high resolution data in a manner which preserves task-relevant information better than uniform downsampling. This has the effect of creating distorted, caricature-like intermediate images, in which idiosyncratic elements of the image that improve task performance are zoomed and exaggerated. Unlike alternative approaches such as spatial transformer networks, our proposed layer is inspired by image saliency, computed efficiently from uniformly downsampled data, and degrades gracefully to a uniform sampling strategy under uncertainty. We apply our layer to improve existing networks for the tasks of human gaze estimation and fine-grained object classification. Code for our method is available in: http://github.com/recasens/Saliency-Sampler



### Torchbearer: A Model Fitting Library for PyTorch
- **Arxiv ID**: http://arxiv.org/abs/1809.03363v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.03363v1)
- **Published**: 2018-09-10 14:46:35+00:00
- **Updated**: 2018-09-10 14:46:35+00:00
- **Authors**: Ethan Harris, Matthew Painter, Jonathon Hare
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: We introduce torchbearer, a model fitting library for pytorch aimed at researchers working on deep learning or differentiable programming. The torchbearer library provides a high level metric and callback API that can be used for a wide range of applications. We also include a series of built in callbacks that can be used for: model persistence, learning rate decay, logging, data visualization and more. The extensive documentation includes an example library for deep learning and dynamic programming problems and can be found at http://torchbearer.readthedocs.io. The code is licensed under the MIT License and available at https://github.com/ecs-vlc/torchbearer.



### SPASS: Scientific Prominence Active Search System with Deep Image Captioning Network
- **Arxiv ID**: http://arxiv.org/abs/1809.03385v1
- **DOI**: 10.1016/j.pss.2020.104943
- **Categories**: **cs.LG**, cs.CV, cs.IR, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.03385v1)
- **Published**: 2018-09-10 15:18:37+00:00
- **Updated**: 2018-09-10 15:18:37+00:00
- **Authors**: Dicong Qiu
- **Comment**: 9 pages, 5 figures, 1 table. Preprint. Work in progress
- **Journal**: Planetary and Space Science, 2020, 118: 104943
- **Summary**: Planetary exploration missions with Mars rovers are complicated, which generally require elaborated task planning by human experts, from the path to take to the images to capture. NASA has been using this process to acquire over 22 million images from the planet Mars. In order to improve the degree of automation and thus efficiency in this process, we propose a system for planetary rovers to actively search for prominence of prespecified scientific features in captured images. Scientists can prespecify such search tasks in natural language and upload them to a rover, on which the deployed system constantly captions captured images with a deep image captioning network and compare the auto-generated captions to the prespecified search tasks by certain metrics so as to prioritize those images for transmission. As a beneficial side effect, the proposed system can also be deployed to ground-based planetary data systems as a content-based search engine.



### Beyond task success: A closer look at jointly learning to see, ask, and GuessWhat
- **Arxiv ID**: http://arxiv.org/abs/1809.03408v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.03408v2)
- **Published**: 2018-09-10 15:46:58+00:00
- **Updated**: 2019-03-15 08:01:14+00:00
- **Authors**: Ravi Shekhar, Aashish Venkatesh, Tim Baumgärtner, Elia Bruni, Barbara Plank, Raffaella Bernardi, Raquel Fernández
- **Comment**: Accepted to NAACL 2019
- **Journal**: None
- **Summary**: We propose a grounded dialogue state encoder which addresses a foundational issue on how to integrate visual grounding with dialogue system components. As a test-bed, we focus on the GuessWhat?! game, a two-player game where the goal is to identify an object in a complex visual scene by asking a sequence of yes/no questions. Our visually-grounded encoder leverages synergies between guessing and asking questions, as it is trained jointly using multi-task learning. We further enrich our model via a cooperative learning regime. We show that the introduction of both the joint architecture and cooperative learning lead to accuracy improvements over the baseline system. We compare our approach to an alternative system which extends the baseline with reinforcement learning. Our in-depth analysis shows that the linguistic skills of the two models differ dramatically, despite approaching comparable performance levels. This points at the importance of analyzing the linguistic output of competing systems beyond numeric comparison solely based on task success.



### Monocular Object and Plane SLAM in Structured Environments
- **Arxiv ID**: http://arxiv.org/abs/1809.03415v2
- **DOI**: 10.1109/LRA.2019.2924848
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1809.03415v2)
- **Published**: 2018-09-10 15:51:13+00:00
- **Updated**: 2019-06-28 04:52:08+00:00
- **Authors**: Shichao Yang, Sebastian Scherer
- **Comment**: IEEE Robotics and Automation Letters
- **Journal**: None
- **Summary**: In this paper, we present a monocular Simultaneous Localization and Mapping (SLAM) algorithm using high-level object and plane landmarks. The built map is denser, more compact and semantic meaningful compared to feature point based SLAM. We first propose a high order graphical model to jointly infer the 3D object and layout planes from single images considering occlusions and semantic constraints. The extracted objects and planes are further optimized with camera poses in a unified SLAM framework. Objects and planes can provide more semantic constraints such as Manhattan plane and object supporting relationships compared to points. Experiments on various public and collected datasets including ICL NUIM and TUM Mono show that our algorithm can improve camera localization accuracy compared to state-of-the-art SLAM especially when there is no loop closure, and also generate dense maps robustly in many structured environments.



### Inverse-Consistent Deep Networks for Unsupervised Deformable Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1809.03443v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03443v1)
- **Published**: 2018-09-10 16:30:28+00:00
- **Updated**: 2018-09-10 16:30:28+00:00
- **Authors**: Jun Zhang
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: Deformable image registration is a fundamental task in medical image analysis, aiming to establish a dense and non-linear correspondence between a pair of images. Previous deep-learning studies usually employ supervised neural networks to directly learn the spatial transformation from one image to another, requiring task-specific ground-truth registration for model training. Due to the difficulty in collecting precise ground-truth registration, implementation of these supervised methods is practically challenging. Although several unsupervised networks have been recently developed, these methods usually ignore the inherent inverse-consistent property (essential for diffeomorphic mapping) of transformations between a pair of images. Also, existing approaches usually encourage the to-be-estimated transformation to be locally smooth via a smoothness constraint only, which could not completely avoid folding in the resulting transformation. To this end, we propose an Inverse-Consistent deep Network (ICNet) for unsupervised deformable image registration. Specifically, we develop an inverse-consistent constraint to encourage that a pair of images are symmetrically deformed toward one another, until both warped images are matched. Besides using the conventional smoothness constraint, we also propose an anti-folding constraint to further avoid folding in the transformation. The proposed method does not require any supervision information, while encouraging the diffeomoprhic property of the transformation via the proposed inverse-consistent and anti-folding constraints. We evaluate our method on T1-weighted brain magnetic resonance imaging (MRI) scans for tissue segmentation and anatomical landmark detection, with results demonstrating the superior performance of our ICNet over several state-of-the-art approaches for deformable image registration. Our code will be made publicly available.



### Deep Single-View 3D Object Reconstruction with Visual Hull Embedding
- **Arxiv ID**: http://arxiv.org/abs/1809.03451v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03451v1)
- **Published**: 2018-09-10 16:49:36+00:00
- **Updated**: 2018-09-10 16:49:36+00:00
- **Authors**: Hanqing Wang, Jiaolong Yang, Wei Liang, Xin Tong
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: 3D object reconstruction is a fundamental task of many robotics and AI problems. With the aid of deep convolutional neural networks (CNNs), 3D object reconstruction has witnessed a significant progress in recent years. However, possibly due to the prohibitively high dimension of the 3D object space, the results from deep CNNs are often prone to missing some shape details. In this paper, we present an approach which aims to preserve more shape details and improve the reconstruction quality. The key idea of our method is to leverage object mask and pose estimation from CNNs to assist the 3D shape learning by constructing a probabilistic single-view visual hull inside of the network. Our method works by first predicting a coarse shape as well as the object pose and silhouette using CNNs, followed by a novel 3D refinement CNN which refines the coarse shapes using the constructed probabilistic visual hulls. Experiment on both synthetic data and real images show that embedding a single-view visual hull for shape refinement can significantly improve the reconstruction quality by recovering more shapes details and improving shape consistency with the input image.



### ViZDoom Competitions: Playing Doom from Pixels
- **Arxiv ID**: http://arxiv.org/abs/1809.03470v1
- **DOI**: 10.1109/TG.2018.2877047
- **Categories**: **cs.AI**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1809.03470v1)
- **Published**: 2018-09-10 17:41:39+00:00
- **Updated**: 2018-09-10 17:41:39+00:00
- **Authors**: Marek Wydmuch, Michał Kempka, Wojciech Jaśkowski
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents the first two editions of Visual Doom AI Competition, held in 2016 and 2017. The challenge was to create bots that compete in a multi-player deathmatch in a first-person shooter (FPS) game, Doom. The bots had to make their decisions based solely on visual information, i.e., a raw screen buffer. To play well, the bots needed to understand their surroundings, navigate, explore, and handle the opponents at the same time. These aspects, together with the competitive multi-agent aspect of the game, make the competition a unique platform for evaluating the state of the art reinforcement learning algorithms. The paper discusses the rules, solutions, results, and statistics that give insight into the agents' behaviors. Best-performing agents are described in more detail. The results of the competition lead to the conclusion that, although reinforcement learning can produce capable Doom bots, they still are not yet able to successfully compete against humans in this game. The paper also revisits the ViZDoom environment, which is a flexible, easy to use, and efficient 3D platform for research for vision-based reinforcement learning, based on a well-recognized first-person perspective game Doom.



### Annotating shadows, highlights and faces: the contribution of a 'human in the loop' for digital art history
- **Arxiv ID**: http://arxiv.org/abs/1809.03539v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1809.03539v1)
- **Published**: 2018-09-10 18:34:22+00:00
- **Updated**: 2018-09-10 18:34:22+00:00
- **Authors**: Maarten W. A. Wijntjes
- **Comment**: Presented at the "1st KDD Workshop on Data Science for Digital Art
  History: tackling big data Challenges, Algorithms, and Systems", see
  http://dsdah2018.blogs.dsv.su.se for more info. Manuscript should eventually
  be published in Journal of Digital Art History (www.dah-journal.org/)
- **Journal**: None
- **Summary**: While automatic computational techniques appear to reveal novel insights in digital art history, a complementary approach seems to get less attention: that of human annotation. We argue and exemplify that a 'human in the loop' can reveal insights that may be difficult to detect automatically. Specifically, we focussed on perceptual aspects within pictorial art. Using rather simple annotation tasks (e.g. delineate human lengths, indicate highlights and classify gaze direction) we could both replicate earlier findings and reveal novel insights into pictorial conventions. We found that Canaletto depicted human figures in rather accurate perspective, varied viewpoint elevation between approximately 3 and 9 meters and highly preferred light directions parallel to the projection plane. Furthermore, we found that taking the averaged images of leftward looking faces reveals a woman, and for rightward looking faces showed a male, confirming earlier accounts on lateral gender bias in pictorial art. Lastly, we confirmed and refined the well-known light-from-the-left bias. Together, the annotations, analyses and results exemplify how human annotation can contribute and complement to technical and digital art history.



### Pursuit of Low-Rank Models of Time-Varying Matrices Robust to Sparse and Measurement Noise
- **Arxiv ID**: http://arxiv.org/abs/1809.03550v3
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, math.ST, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1809.03550v3)
- **Published**: 2018-09-10 19:00:34+00:00
- **Updated**: 2020-02-04 16:22:33+00:00
- **Authors**: Albert Akhriev, Jakub Marecek, Andrea Simonetto
- **Comment**: 20 pages; camera-ready version + appendices
- **Journal**: Proceedings of the Thirty-Fourth AAAI Conference on Artificial
  Intelligence, 2020
- **Summary**: In tracking of time-varying low-rank models of time-varying matrices, we present a method robust to both uniformly-distributed measurement noise and arbitrarily-distributed ``sparse'' noise. In theory, we bound the tracking error. In practice, our use of randomised coordinate descent is scalable and allows for encouraging results on changedetection net, a benchmark.



### PedX: Benchmark Dataset for Metric 3D Pose Estimation of Pedestrians in Complex Urban Intersections
- **Arxiv ID**: http://arxiv.org/abs/1809.03605v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1809.03605v1)
- **Published**: 2018-09-10 21:31:42+00:00
- **Updated**: 2018-09-10 21:31:42+00:00
- **Authors**: Wonhui Kim, Manikandasriram Srinivasan Ramanagopal, Charles Barto, Ming-Yuan Yu, Karl Rosaen, Nick Goumas, Ram Vasudevan, Matthew Johnson-Roberson
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel dataset titled PedX, a large-scale multimodal collection of pedestrians at complex urban intersections. PedX consists of more than 5,000 pairs of high-resolution (12MP) stereo images and LiDAR data along with providing 2D and 3D labels of pedestrians. We also present a novel 3D model fitting algorithm for automatic 3D labeling harnessing constraints across different modalities and novel shape and temporal priors. All annotated 3D pedestrians are localized into the real-world metric space, and the generated 3D models are validated using a mocap system configured in a controlled outdoor environment to simulate pedestrians in urban intersections. We also show that the manual 2D labels can be replaced by state-of-the-art automated labeling approaches, thereby facilitating automatic generation of large scale datasets.



### URBAN-i: From urban scenes to mapping slums, transport modes, and pedestrians in cities using deep learning and computer vision
- **Arxiv ID**: http://arxiv.org/abs/1809.03609v1
- **DOI**: 10.1177/2399808319846517
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1809.03609v1)
- **Published**: 2018-09-10 21:49:38+00:00
- **Updated**: 2018-09-10 21:49:38+00:00
- **Authors**: Mohamed R. Ibrahim, James Haworth, Tao Cheng
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: Within the burgeoning expansion of deep learning and computer vision across the different fields of science, when it comes to urban development, deep learning and computer vision applications are still limited towards the notions of smart cities and autonomous vehicles. Indeed, a wide gap of knowledge appears when it comes to cities and urban regions in less developed countries where the chaos of informality is the dominant scheme. How can deep learning and Artificial Intelligence (AI) untangle the complexities of informality to advance urban modelling and our understanding of cities? Various questions and debates can be raised concerning the future of cities of the North and the South in the paradigm of AI and computer vision. In this paper, we introduce a new method for multipurpose realistic-dynamic urban modelling relying on deep learning and computer vision, using deep Convolutional Neural Networks (CNN), to sense and detect informality and slums in urban scenes from aerial and street view images in addition to detection of pedestrian and transport modes. The model has been trained on images of urban scenes in cities across the globe. The model shows a good validation of understanding a wide spectrum of nuances among the planned and the unplanned regions, including informal and slum areas. We attempt to advance urban modelling for better understanding the dynamics of city developments. We also aim to exemplify the significant impacts of AI in cities beyond how smart cities are discussed and perceived in the mainstream. The algorithms of the URBAN-i model are fully-coded in Python programming with the pre-trained deep learning models to be used as a tool for mapping and city modelling in the various corner of the globe, including informal settlements and slum regions.



### Improved Techniques for Adversarial Discriminative Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1809.03625v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03625v3)
- **Published**: 2018-09-10 22:56:55+00:00
- **Updated**: 2019-11-10 21:21:10+00:00
- **Authors**: Aaron Chadha, Yiannis Andreopoulos
- **Comment**: To appear in IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Adversarial discriminative domain adaptation (ADDA) is an efficient framework for unsupervised domain adaptation in image classification, where the source and target domains are assumed to have the same classes, but no labels are available for the target domain. We investigate whether we can improve performance of ADDA with a new framework and new loss formulations. Following the framework of semi-supervised GANs, we first extend the discriminator output over the source classes, in order to model the joint distribution over domain and task. We thus leverage on the distribution over the source encoder posteriors (which is fixed during adversarial training) and propose maximum mean discrepancy (MMD) and reconstruction-based loss functions for aligning the target encoder distribution to the source domain. We compare and provide a comprehensive analysis of how our framework and loss formulations extend over simple multi-class extensions of ADDA and other discriminative variants of semi-supervised GANs. In addition, we introduce various forms of regularization for stabilizing training, including treating the discriminator as a denoising autoencoder and regularizing the target encoder with source examples to reduce overfitting under a contraction mapping (i.e., when the target per-class distributions are contracting during alignment with the source). Finally, we validate our framework on standard domain adaptation datasets, such as SVHN and MNIST. We also examine how our framework benefits recognition problems based on modalities that lack training data, by introducing and evaluating on a neuromorphic vision sensing (NVS) sign language recognition dataset, where the source and target domains constitute emulated and real neuromorphic spike events respectively. Our results on all datasets show that our proposal competes or outperforms the state-of-the-art in unsupervised domain adaptation.



