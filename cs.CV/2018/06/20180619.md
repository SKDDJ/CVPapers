# Arxiv Papers in cs.CV on 2018-06-19
### VirtualHome: Simulating Household Activities via Programs
- **Arxiv ID**: http://arxiv.org/abs/1806.07011v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.07011v1)
- **Published**: 2018-06-19 02:16:44+00:00
- **Updated**: 2018-06-19 02:16:44+00:00
- **Authors**: Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba
- **Comment**: CVPR 2018 (Oral)
- **Journal**: None
- **Summary**: In this paper, we are interested in modeling complex activities that occur in a typical household. We propose to use programs, i.e., sequences of atomic actions and interactions, as a high level representation of complex tasks. Programs are interesting because they provide a non-ambiguous representation of a task, and allow agents to execute them. However, nowadays, there is no database providing this type of information. Towards this goal, we first crowd-source programs for a variety of activities that happen in people's homes, via a game-like interface used for teaching kids how to code. Using the collected dataset, we show how we can learn to extract programs directly from natural language descriptions or from videos. We then implement the most common atomic (inter)actions in the Unity3D game engine, and use our programs to "drive" an artificial agent to execute tasks in a simulated household environment. Our VirtualHome simulator allows us to create a large activity video dataset with rich ground-truth, enabling training and testing of video understanding models. We further showcase examples of our agent performing tasks in our VirtualHome based on language descriptions.



### Deep neural network based sparse measurement matrix for image compressed sensing
- **Arxiv ID**: http://arxiv.org/abs/1806.07026v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07026v1)
- **Published**: 2018-06-19 02:53:12+00:00
- **Updated**: 2018-06-19 02:53:12+00:00
- **Authors**: Wenxue Cui, Feng Jiang, Xinwei Gao, Wen Tao, Debin Zhao
- **Comment**: 5 pages, accepted by ICIP 2018
- **Journal**: None
- **Summary**: Gaussian random matrix (GRM) has been widely used to generate linear measurements in compressed sensing (CS) of natural images. However, there actually exist two disadvantages with GRM in practice. One is that GRM has large memory requirement and high computational complexity, which restrict the applications of CS. Another is that the CS measurements randomly obtained by GRM cannot provide sufficient reconstruction performances. In this paper, a Deep neural network based Sparse Measurement Matrix (DSMM) is learned by the proposed convolutional network to reduce the sampling computational complexity and improve the CS reconstruction performance. Two sub networks are included in the proposed network, which are the sampling sub-network and the reconstruction sub-network. In the sampling sub-network, the sparsity and the normalization are both considered by the limitation of the storage and the computational complexity. In order to improve the CS reconstruction performance, a reconstruction sub-network are introduced to help enhance the sampling sub-network. So by the offline iterative training of the proposed end-to-end network, the DSMM is generated for accurate measurement and excellent reconstruction. Experimental results demonstrate that the proposed DSMM outperforms GRM greatly on representative CS reconstruction methods



### MoE-SPNet: A Mixture-of-Experts Scene Parsing Network
- **Arxiv ID**: http://arxiv.org/abs/1806.07049v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07049v1)
- **Published**: 2018-06-19 05:32:49+00:00
- **Updated**: 2018-06-19 05:32:49+00:00
- **Authors**: Huan Fu, Mingming Gong, Chaohui Wang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Scene parsing is an indispensable component in understanding the semantics within a scene. Traditional methods rely on handcrafted local features and probabilistic graphical models to incorporate local and global cues. Recently, methods based on fully convolutional neural networks have achieved new records on scene parsing. An important strategy common to these methods is the aggregation of hierarchical features yielded by a deep convolutional neural network. However, typical algorithms usually aggregate hierarchical convolutional features via concatenation or linear combination, which cannot sufficiently exploit the diversities of contextual information in multi-scale features and the spatial inhomogeneity of a scene. In this paper, we propose a mixture-of-experts scene parsing network (MoE-SPNet) that incorporates a convolutional mixture-of-experts layer to assess the importance of features from different levels and at different spatial locations. In addition, we propose a variant of mixture-of-experts called the adaptive hierarchical feature aggregation (AHFA) mechanism which can be incorporated into existing scene parsing networks that use skip-connections to fuse features layer-wisely. In the proposed networks, different levels of features at each spatial location are adaptively re-weighted according to the local structure and surrounding contextual information before aggregation. We demonstrate the effectiveness of the proposed methods on two scene parsing datasets including PASCAL VOC 2012 and SceneParse150 based on two kinds of baseline models FCN-8s and DeepLab-ASPP.



### Cancer Metastasis Detection With Neural Conditional Random Field
- **Arxiv ID**: http://arxiv.org/abs/1806.07064v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07064v1)
- **Published**: 2018-06-19 06:44:34+00:00
- **Updated**: 2018-06-19 06:44:34+00:00
- **Authors**: Yi Li, Wei Ping
- **Comment**: 9 pages, 5 figures, MIDL 2018
- **Journal**: None
- **Summary**: Breast cancer diagnosis often requires accurate detection of metastasis in lymph nodes through Whole-slide Images (WSIs). Recent advances in deep convolutional neural networks (CNNs) have shown significant successes in medical image analysis and particularly in computational histopathology. Because of the outrageous large size of WSIs, most of the methods divide one slide into lots of small image patches and perform classification on each patch independently. However, neighboring patches often share spatial correlations, and ignoring these spatial correlations may result in inconsistent predictions. In this paper, we propose a neural conditional random field (NCRF) deep learning framework to detect cancer metastasis in WSIs. NCRF considers the spatial correlations between neighboring patches through a fully connected CRF which is directly incorporated on top of a CNN feature extractor. The whole deep network can be trained end-to-end with standard back-propagation algorithm with minor computational overhead from the CRF component. The CNN feature extractor can also benefit from considering spatial correlations via the CRF component. Compared to the baseline method without considering spatial correlations, we show that the proposed NCRF framework obtains probability maps of patch predictions with better visual quality. We also demonstrate that our method outperforms the baseline in cancer metastasis detection on the Camelyon16 dataset and achieves an average FROC score of 0.8096 on the test set. NCRF is open sourced at https://github.com/baidu-research/NCRF.



### A New COLD Feature based Handwriting Analysis for Ethnicity/Nationality Identification
- **Arxiv ID**: http://arxiv.org/abs/1806.07072v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CG, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1806.07072v1)
- **Published**: 2018-06-19 07:14:54+00:00
- **Updated**: 2018-06-19 07:14:54+00:00
- **Authors**: Sauradip Nag, Palaiahnakote Shivakumara, Wu Yirui, Umapada Pal, Tong Lu
- **Comment**: Accepted in ICFHR18
- **Journal**: None
- **Summary**: Identifying crime for forensic investigating teams when crimes involve people of different nationals is challenging. This paper proposes a new method for ethnicity (nationality) identification based on Cloud of Line Distribution (COLD) features of handwriting components. The proposed method, at first, explores tangent angle for the contour pixels in each row and the mean of intensity values of each row in an image for segmenting text lines. For segmented text lines, we use tangent angle and direction of base lines to remove rule lines in the image. We use polygonal approximation for finding dominant points for contours of edge components. Then the proposed method connects the nearest dominant points of every dominant point, which results in line segments of dominant point pairs. For each line segment, the proposed method estimates angle and length, which gives a point in polar domain. For all the line segments, the proposed method generates dense points in polar domain, which results in COLD distribution. As character component shapes change, according to nationals, the shape of the distribution changes. This observation is extracted based on distance from pixels of distribution to Principal Axis of the distribution. Then the features are subjected to an SVM classifier for identifying nationals. Experiments are conducted on a complex dataset, which show the proposed method is effective and outperforms the existing method



### Learning to Update for Object Tracking with Recurrent Meta-learner
- **Arxiv ID**: http://arxiv.org/abs/1806.07078v3
- **DOI**: 10.1109/TIP.2019.2900577
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07078v3)
- **Published**: 2018-06-19 07:31:37+00:00
- **Updated**: 2019-03-05 03:56:10+00:00
- **Authors**: Bi Li, Wenxuan Xie, Wenjun Zeng, Wenyu Liu
- **Comment**: accepted to TIP 2019
- **Journal**: IEEE Transactions on Image Processing 2019
- **Summary**: Model update lies at the heart of object tracking. Generally, model update is formulated as an online learning problem where a target model is learned over the online training set. Our key innovation is to \emph{formulate the model update problem in the meta-learning framework and learn the online learning algorithm itself using large numbers of offline videos}, i.e., \emph{learning to update}. The learned updater takes as input the online training set and outputs an updated target model. As a first attempt, we design the learned updater based on recurrent neural networks (RNNs) and demonstrate its application in a template-based tracker and a correlation filter-based tracker. Our learned updater consistently improves the base trackers and runs faster than realtime on GPU while requiring small memory footprint during testing. Experiments on standard benchmarks demonstrate that our learned updater outperforms commonly used update baselines including the efficient exponential moving average (EMA)-based update and the well-designed stochastic gradient descent (SGD)-based update. Equipped with our learned updater, the template-based tracker achieves state-of-the-art performance among realtime trackers on GPU.



### Diffeomorphic brain shape modelling using Gauss-Newton optimisation
- **Arxiv ID**: http://arxiv.org/abs/1806.07109v1
- **DOI**: 10.1007/978-3-030-00928-1_97
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07109v1)
- **Published**: 2018-06-19 08:52:09+00:00
- **Updated**: 2018-06-19 08:52:09+00:00
- **Authors**: Yaël Balbastre, Mikael Brudfors, Kevin Bronik, John Ashburner
- **Comment**: 8 pages, 4 figures, conference paper, accepted at MICCAI 2018
- **Journal**: MICCAI 2018. Lecture Notes in Computer Science, vol 11070
- **Summary**: Shape modelling describes methods aimed at capturing the natural variability of shapes and commonly relies on probabilistic interpretations of dimensionality reduction techniques such as principal component analysis. Due to their computational complexity when dealing with dense deformation models such as diffeomorphisms, previous attempts have focused on explicitly reducing their dimension, diminishing de facto their flexibility and ability to model complex shapes such as brains. In this paper, we present a generative model of shape that allows the covariance structure of deformations to be captured without squashing their domain, resulting in better normalisation. An efficient inference scheme based on Gauss-Newton optimisation is used, which enables processing of 3D neuroimaging data. We trained this algorithm on segmented brains from the OASIS database, generating physiologically meaningful deformation trajectories. To prove the model's robustness, we applied it to unseen data, which resulted in equivalent fitting scores.



### Modality Distillation with Multiple Stream Networks for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.07110v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07110v2)
- **Published**: 2018-06-19 08:56:13+00:00
- **Updated**: 2018-10-29 15:19:56+00:00
- **Authors**: Nuno Garcia, Pietro Morerio, Vittorio Murino
- **Comment**: Accepted at ECCV 2018; Supp. material at p.16; code available
- **Journal**: None
- **Summary**: Diverse input data modalities can provide complementary cues for several tasks, usually leading to more robust algorithms and better performance. However, while a (training) dataset could be accurately designed to include a variety of sensory inputs, it is often the case that not all modalities could be available in real life (testing) scenarios, where a model has to be deployed. This raises the challenge of how to learn robust representations leveraging multimodal data in the training stage, while considering limitations at test time, such as noisy or missing modalities.   This paper presents a new approach for multimodal video action recognition, developed within the unified frameworks of distillation and privileged information, named generalized distillation. Particularly, we consider the case of learning representations from depth and RGB videos, while relying on RGB data only at test time. We propose a new approach to train an hallucination network that learns to distill depth features through multiplicative connections of spatiotemporal representations, leveraging soft labels and hard labels, as well as distance between feature maps. We report state-of-the-art results on video action classification on the largest multimodal dataset available for this task, the NTU RGB+D. Code available at https://github.com/ncgarcia/modality-distillation .



### Infrared and Visible Image Fusion with ResNet and zero-phase component analysis
- **Arxiv ID**: http://arxiv.org/abs/1806.07119v7
- **DOI**: 10.1016/j.infrared.2019.103039
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07119v7)
- **Published**: 2018-06-19 09:15:54+00:00
- **Updated**: 2019-09-20 15:10:32+00:00
- **Authors**: Hui Li, Xiao-Jun Wu, Tariq S. Durrani
- **Comment**: 22pages, 9 figures, 7 tables
- **Journal**: Journal of Infrared Physics & Technology, available online 12
  September 2019
- **Summary**: Feature extraction and processing tasks play a key role in Image Fusion, and the fusion performance is directly affected by the different features and processing methods undertaken. By contrast, most of deep learning-based methods use deep features directly without feature extraction or processing. This leads to the fusion performance degradation in some cases. To solve these drawbacks, we propose a deep features and zero-phase component analysis (ZCA) based novel fusion framework is this paper. Firstly, the residual network (ResNet) is used to extract deep features from source images. Then ZCA is utilized to normalize the deep features and obtain initial weight maps. The final weight maps are obtained by employing a soft-max operation in association with the initial weight maps. Finally, the fused image is reconstructed using a weighted-averaging strategy. Compared with the existing fusion methods, experimental results demonstrate that the proposed framework achieves better performance in both objective assessment and visual quality. The code of our fusion algorithm is available at https://github.com/hli1221/imagefusion_resnet50



### FineTag: Multi-attribute Classification at Fine-grained Level in Images
- **Arxiv ID**: http://arxiv.org/abs/1806.07124v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07124v2)
- **Published**: 2018-06-19 09:27:58+00:00
- **Updated**: 2018-10-25 11:47:36+00:00
- **Authors**: Roshanak Zakizadeh, Michele Sasdelli, Yu Qian, Eduard Vazquez
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the extraction of the fine-grained attributes of an instance as a `multi-attribute classification' problem. To this end, we propose an end-to-end architecture by adopting the bi-linear Convolutional Neural Network with the pairwise ranking loss. This is the first time such architecture is applied for the fine-grained attributes classification problem. We compared the proposed method with a competitive deep Convolutional Neural Network baseline. Extensive experiments show that the proposed method attains/outperforms the performance of compared baseline with significantly less number of parameters ($40\times$ less). We demonstrated our approach on CUB200 birds dataset whose annotations are adapted in this work for multi-attribute classification at fine-grained level.



### Feature learning based on visual similarity triplets in medical image analysis: A case study of emphysema in chest CT scans
- **Arxiv ID**: http://arxiv.org/abs/1806.07131v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07131v1)
- **Published**: 2018-06-19 09:43:55+00:00
- **Updated**: 2018-06-19 09:43:55+00:00
- **Authors**: Silas Nyboe Ørting, Jens Petersen, Veronika Cheplygina, Laura H. Thomsen, Mathilde M W Wille, Marleen de Bruijne
- **Comment**: 10 pages. Submitted to LABELS2018 - MICCAI Workshop on Large-scale
  Annotation of Biomedical data and Expert Label Synthesis
- **Journal**: None
- **Summary**: Supervised feature learning using convolutional neural networks (CNNs) can provide concise and disease relevant representations of medical images. However, training CNNs requires annotated image data. Annotating medical images can be a time-consuming task and even expert annotations are subject to substantial inter- and intra-rater variability. Assessing visual similarity of images instead of indicating specific pathologies or estimating disease severity could allow non-experts to participate, help uncover new patterns, and possibly reduce rater variability. We consider the task of assessing emphysema extent in chest CT scans. We derive visual similarity triplets from visually assessed emphysema extent and learn a low dimensional embedding using CNNs. We evaluate the networks on 973 images, and show that the CNNs can learn disease relevant feature representations from derived similarity triplets. To our knowledge this is the first medical image application where similarity triplets has been used to learn a feature representation that can be used for embedding unseen test images



### Automatic segmentation of prostate zones
- **Arxiv ID**: http://arxiv.org/abs/1806.07146v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07146v1)
- **Published**: 2018-06-19 10:23:15+00:00
- **Updated**: 2018-06-19 10:23:15+00:00
- **Authors**: Germonda Mooij, Ines Bagulho, Henkjan Huisman
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional networks have become state-of-the-art techniques for automatic medical image analysis, with the U-net architecture being the most popular at this moment. In this article we report the application of a 3D version of U-net to the automatic segmentation of prostate peripheral and transition zones in 3D MRI images. Our results are slightly better than recent studies that used 2D U-net and handcrafted feature approaches.   In addition, we test ideas for improving the 3D U-net setup, by 1) letting the network segment surrounding tissues, making use of the fixed anatomy, and 2) adjusting the network architecture to reflect the anisotropy in the dimensions of the MRI image volumes. While the latter adjustment gave a marginal improvement, the former adjustment showed a significant deterioration of the network performance. We were able to explain this deterioration by inspecting feature map activations in all layers of the network. We show that to segment more tissues the network replaces feature maps that were dedicated to detecting prostate peripheral zones, by feature maps detecting the surrounding tissues.



### Semi-supervised Hashing for Semi-Paired Cross-View Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1806.07155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07155v1)
- **Published**: 2018-06-19 11:17:37+00:00
- **Updated**: 2018-06-19 11:17:37+00:00
- **Authors**: Jun Yu, Xiao-Jun Wu, Josef Kittler
- **Comment**: 6 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: Recently, hashing techniques have gained importance in large-scale retrieval tasks because of their retrieval speed. Most of the existing cross-view frameworks assume that data are well paired. However, the fully-paired multiview situation is not universal in real applications. The aim of the method proposed in this paper is to learn the hashing function for semi-paired cross-view retrieval tasks. To utilize the label information of partial data, we propose a semi-supervised hashing learning framework which jointly performs feature extraction and classifier learning. The experimental results on two datasets show that our method outperforms several state-of-the-art methods in terms of retrieval accuracy.



### Multimodal feature fusion for CNN-based gait recognition: an empirical comparison
- **Arxiv ID**: http://arxiv.org/abs/1806.07753v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07753v2)
- **Published**: 2018-06-19 11:36:22+00:00
- **Updated**: 2020-02-20 12:27:04+00:00
- **Authors**: Francisco Manuel Castro, Manuel Jesús Marín-Jiménez, Nicolás Guil, Nicolás Pérez de la Blanca
- **Comment**: arXiv admin note: text overlap with arXiv:1603.01006
- **Journal**: None
- **Summary**: People identification in video based on the way they walk (i.e. gait) is a relevant task in computer vision using a non-invasive approach. Standard and current approaches typically derive gait signatures from sequences of binary energy maps of subjects extracted from images, but this process introduces a large amount of non-stationary noise, thus, conditioning their efficacy. In contrast, in this paper we focus on the raw pixels, or simple functions derived from them, letting advanced learning techniques to extract relevant features. Therefore, we present a comparative study of different Convolutional Neural Network (CNN) architectures by using three different modalities (i.e. gray pixels, optical flow channels and depth maps) on two widely-adopted and challenging datasets: TUM-GAID and CASIA-B. In addition, we perform a comparative study between different early and late fusion methods used to combine the information obtained from each kind of modalities. Our experimental results suggest that (i) the raw pixel values represent a competitive input modality, compared to the traditional state-of-the-art silhouette-based features (e.g. GEI), since equivalent or better results are obtained; (ii) the fusion of the raw pixel information with information from optical flow and depth maps allows to obtain state-of-the-art results on the gait recognition task with an image resolution several times smaller than the previously reported results; and, (iii) the selection and the design of the CNN architecture are critical points that can make a difference between state-of-the-art results or poor ones.



### Non-deterministic Behavior of Ranking-based Metrics when Evaluating Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1806.07171v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07171v2)
- **Published**: 2018-06-19 11:59:10+00:00
- **Updated**: 2019-02-20 10:43:07+00:00
- **Authors**: Anguelos Nicolaou, Sounak Dey, Vincent Christlein, Andreas Maier, Dimosthenis Karatzas
- **Comment**: None
- **Journal**: None
- **Summary**: Embedding data into vector spaces is a very popular strategy of pattern recognition methods. When distances between embeddings are quantized, performance metrics become ambiguous. In this paper, we present an analysis of the ambiguity quantized distances introduce and provide bounds on the effect. We demonstrate that it can have a measurable effect in empirical data in state-of-the-art systems. We also approach the phenomenon from a computer security perspective and demonstrate how someone being evaluated by a third party can exploit this ambiguity and greatly outperform a random predictor without even access to the input data. We also suggest a simple solution making the performance metrics, which rely on ranking, totally deterministic and impervious to such exploits.



### Mixed batches and symmetric discriminators for GAN training
- **Arxiv ID**: http://arxiv.org/abs/1806.07185v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.07185v1)
- **Published**: 2018-06-19 12:39:11+00:00
- **Updated**: 2018-06-19 12:39:11+00:00
- **Authors**: Thomas Lucas, Corentin Tallec, Jakob Verbeek, Yann Ollivier
- **Comment**: Accepted at ICML 2018 (long oral)
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) are pow- erful generative models based on providing feed- back to a generative network via a discriminator network. However, the discriminator usually as- sesses individual samples. This prevents the dis- criminator from accessing global distributional statistics of generated samples, and often leads to mode dropping: the generator models only part of the target distribution. We propose to feed the discriminator with mixed batches of true and fake samples, and train it to predict the ratio of true samples in the batch. The latter score does not depend on the order of samples in a batch. Rather than learning this invariance, we introduce a generic permutation-invariant discriminator ar- chitecture. This architecture is provably a uni- versal approximator of all symmetric functions. Experimentally, our approach reduces mode col- lapse in GANs on two synthetic datasets, and obtains good results on the CIFAR10 and CelebA datasets, both qualitatively and quantitatively.



### Spatio-Temporal Channel Correlation Networks for Action Classification
- **Arxiv ID**: http://arxiv.org/abs/1806.07754v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07754v3)
- **Published**: 2018-06-19 12:43:40+00:00
- **Updated**: 2019-02-07 14:03:04+00:00
- **Authors**: Ali Diba, Mohsen Fayyaz, Vivek Sharma, M. Mahdi Arzani, Rahman Yousefzadeh, Juergen Gall, Luc Van Gool
- **Comment**: Accepted in ECCV 2018. arXiv admin note: substantial text overlap
  with arXiv:1711.08200
- **Journal**: None
- **Summary**: The work in this paper is driven by the question if spatio-temporal correlations are enough for 3D convolutional neural networks (CNN)? Most of the traditional 3D networks use local spatio-temporal features. We introduce a new block that models correlations between channels of a 3D CNN with respect to temporal and spatial features. This new block can be added as a residual unit to different parts of 3D CNNs. We name our novel block 'Spatio-Temporal Channel Correlation' (STC). By embedding this block to the current state-of-the-art architectures such as ResNext and ResNet, we improved the performance by 2-3\% on Kinetics dataset. Our experiments show that adding STC blocks to current state-of-the-art architectures outperforms the state-of-the-art methods on the HMDB51, UCF101 and Kinetics datasets. The other issue in training 3D CNNs is about training them from scratch with a huge labeled dataset to get a reasonable performance. So the knowledge learned in 2D CNNs is completely ignored. Another contribution in this work is a simple and effective technique to transfer knowledge from a pre-trained 2D CNN to a randomly initialized 3D CNN for a stable weight initialization. This allows us to significantly reduce the number of training samples for 3D CNNs. Thus, by fine-tuning this network, we beat the performance of generic and recent methods in 3D CNNs, which were trained on large video datasets, e.g. Sports-1M, and fine-tuned on the target datasets, e.g. HMDB51/UCF101.



### Magnetic Resonance Spectroscopy Quantification using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.07237v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.07237v1)
- **Published**: 2018-06-19 13:56:56+00:00
- **Updated**: 2018-06-19 13:56:56+00:00
- **Authors**: Nima Hatami, Michaël Sdika, Hélène Ratiney
- **Comment**: 21st International Conference on Medical Image Computing and Computer
  Assisted Intervention (MICCAI 2018)
- **Journal**: None
- **Summary**: Magnetic resonance spectroscopy (MRS) is an important technique in biomedical research and it has the unique capability to give a non-invasive access to the biochemical content (metabolites) of scanned organs. In the literature, the quantification (the extraction of the potential biomarkers from the MRS signals) involves the resolution of an inverse problem based on a parametric model of the metabolite signal. However, poor signal-to-noise ratio (SNR), presence of the macromolecule signal or high correlation between metabolite spectral patterns can cause high uncertainties for most of the metabolites, which is one of the main reasons that prevents use of MRS in clinical routine. In this paper, quantification of metabolites in MR Spectroscopic imaging using deep learning is proposed. A regression framework based on the Convolutional Neural Networks (CNN) is introduced for an accurate estimation of spectral parameters. The proposed model learns the spectral features from a large-scale simulated data set with different variations of human brain spectra and SNRs. Experimental results demonstrate the accuracy of the proposed method, compared to state of the art standard quantification method (QUEST), on concentration of 20 metabolites and the macromolecule.



### Learning Conditioned Graph Structures for Interpretable Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1806.07243v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07243v6)
- **Published**: 2018-06-19 13:59:05+00:00
- **Updated**: 2018-11-01 14:10:47+00:00
- **Authors**: Will Norcliffe-Brown, Efstathios Vafeias, Sarah Parisot
- **Comment**: NIPS 2018 (13 pages, 7 figures)
- **Journal**: None
- **Summary**: Visual Question answering is a challenging problem requiring a combination of concepts from Computer Vision and Natural Language Processing. Most existing approaches use a two streams strategy, computing image and question features that are consequently merged using a variety of techniques. Nonetheless, very few rely on higher level image representations, which can capture semantic and spatial relationships. In this paper, we propose a novel graph-based approach for Visual Question Answering. Our method combines a graph learner module, which learns a question specific graph representation of the input image, with the recent concept of graph convolutions, aiming to learn image representations that capture question specific interactions. We test our approach on the VQA v2 dataset using a simple baseline architecture enhanced by the proposed graph learner module. We obtain promising results with 66.18% accuracy and demonstrate the interpretability of the proposed method. Code can be found at github.com/aimbrain/vqa-project.



### An empirical study on evaluation metrics of generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/1806.07755v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.07755v2)
- **Published**: 2018-06-19 14:01:27+00:00
- **Updated**: 2018-08-17 00:20:11+00:00
- **Authors**: Qiantong Xu, Gao Huang, Yang Yuan, Chuan Guo, Yu Sun, Felix Wu, Kilian Weinberger
- **Comment**: arXiv admin note: text overlap with arXiv:1802.03446 by other authors
- **Journal**: None
- **Summary**: Evaluating generative adversarial networks (GANs) is inherently challenging. In this paper, we revisit several representative sample-based evaluation metrics for GANs, and address the problem of how to evaluate the evaluation metrics. We start with a few necessary conditions for metrics to produce meaningful scores, such as distinguishing real from generated samples, identifying mode dropping and mode collapsing, and detecting overfitting. With a series of carefully designed experiments, we comprehensively investigate existing sample-based metrics and identify their strengths and limitations in practical settings. Based on these results, we observe that kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbor (1-NN) two-sample test seem to satisfy most of the desirable properties, provided that the distances between samples are computed in a suitable feature space. Our experiments also unveil interesting properties about the behavior of several popular GAN models, such as whether they are memorizing training samples, and how far they are from learning the target distribution.



### Unsupervised Deep Multi-focus Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/1806.07272v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07272v1)
- **Published**: 2018-06-19 14:15:47+00:00
- **Updated**: 2018-06-19 14:15:47+00:00
- **Authors**: Xiang Yan, Syed Zulqarnain Gilani, Hanlin Qin, Ajmal Mian
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks have recently been used for multi-focus image fusion. However, due to the lack of labeled data for supervised training of such networks, existing methods have resorted to adding Gaussian blur in focused images to simulate defocus and generate synthetic training data with ground-truth for supervised learning. Moreover, they classify pixels as focused or defocused and leverage the results to construct the fusion weight maps which then necessitates a series of post-processing steps. In this paper, we present unsupervised end-to-end learning for directly predicting the fully focused output image from multi-focus input image pairs. The proposed approach uses a novel CNN architecture trained to perform fusion without the need for ground truth fused images and exploits the image structural similarity (SSIM) to calculate the loss; a metric that is widely accepted for fused image quality evaluation. Consequently, we are able to utilize {\em real} benchmark datasets, instead of simulated ones, to train our network. The model is a feed-forward, fully convolutional neural network that can process images of variable sizes during test time. Extensive evaluations on benchmark datasets show that our method outperforms existing state-of-the-art in terms of visual quality and objective evaluations.



### Built-in Vulnerabilities to Imperceptible Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/1806.07409v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.07409v2)
- **Published**: 2018-06-19 18:12:36+00:00
- **Updated**: 2019-05-07 22:20:58+00:00
- **Authors**: Thomas Tanay, Jerone T. A. Andrews, Lewis D. Griffin
- **Comment**: None
- **Journal**: None
- **Summary**: Designing models that are robust to small adversarial perturbations of their inputs has proven remarkably difficult. In this work we show that the reverse problem---making models more vulnerable---is surprisingly easy. After presenting some proofs of concept on MNIST, we introduce a generic tilting attack that injects vulnerabilities into the linear layers of pre-trained networks by increasing their sensitivity to components of low variance in the training data without affecting their performance on test data. We illustrate this attack on a multilayer perceptron trained on SVHN and use it to design a stand-alone adversarial module which we call a steganogram decoder. Finally, we show on CIFAR-10 that a poisoning attack with a poisoning rate as low as 0.1% can induce vulnerabilities to chosen imperceptible backdoor signals in state-of-the-art networks. Beyond their practical implications, these different results shed new light on the nature of the adversarial example phenomenon.



### Fast CapsNet for Lung Cancer Screening
- **Arxiv ID**: http://arxiv.org/abs/1806.07416v1
- **DOI**: 10.1007/978-3-030-00934-2_82
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07416v1)
- **Published**: 2018-06-19 18:26:11+00:00
- **Updated**: 2018-06-19 18:26:11+00:00
- **Authors**: Aryan Mobiny, Hien Van Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: Lung cancer is the leading cause of cancer-related deaths in the past several years. A major challenge in lung cancer screening is the detection of lung nodules from computed tomography (CT) scans. State-of-the-art approaches in automated lung nodule classification use deep convolutional neural networks (CNNs). However, these networks require a large number of training samples to generalize well. This paper investigates the use of capsule networks (CapsNets) as an alternative to CNNs. We show that CapsNets significantly outperforms CNNs when the number of training samples is small. To increase the computational efficiency, our paper proposes a consistent dynamic routing mechanism that results in $3\times$ speedup of CapsNet. Finally, we show that the original image reconstruction method of CapNets performs poorly on lung nodule data. We propose an efficient alternative, called convolutional decoder, that yields lower reconstruction error and higher classification accuracy.



### Improved Image Selection for Stack-Based HDR Imaging
- **Arxiv ID**: http://arxiv.org/abs/1806.07420v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07420v1)
- **Published**: 2018-06-19 18:37:33+00:00
- **Updated**: 2018-06-19 18:37:33+00:00
- **Authors**: Peter van Beek
- **Comment**: None
- **Journal**: None
- **Summary**: Stack-based high dynamic range (HDR) imaging is a technique for achieving a larger dynamic range in an image by combining several low dynamic range images acquired at different exposures. Minimizing the set of images to combine, while ensuring that the resulting HDR image fully captures the scene's irradiance, is important to avoid long image acquisition and post-processing times. The problem of selecting the set of images has received much attention. However, existing methods either are not fully automatic, can be slow, or can fail to fully capture more challenging scenes. In this paper, we propose a fully automatic method for selecting the set of exposures to acquire that is both fast and more accurate. We show on an extensive set of benchmark scenes that our proposed method leads to improved HDR images as measured against ground truth using the mean squared error, a pixel-based metric, and a visible difference predictor and a quality score, both perception-based metrics.



### RISE: Randomized Input Sampling for Explanation of Black-box Models
- **Arxiv ID**: http://arxiv.org/abs/1806.07421v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07421v3)
- **Published**: 2018-06-19 18:41:30+00:00
- **Updated**: 2018-09-25 18:16:18+00:00
- **Authors**: Vitali Petsiuk, Abir Das, Kate Saenko
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are being used increasingly to automate data analysis and decision making, yet their decision-making process is largely unclear and is difficult to explain to the end users. In this paper, we address the problem of Explainable AI for deep neural networks that take images as input and output a class probability. We propose an approach called RISE that generates an importance map indicating how salient each pixel is for the model's prediction. In contrast to white-box approaches that estimate pixel importance using gradients or other internal network state, RISE works on black-box models. It estimates importance empirically by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs. We compare our approach to state-of-the-art importance extraction methods using both an automatic deletion/insertion metric and a pointing metric based on human-annotated object segments. Extensive experiments on several benchmark datasets show that our approach matches or exceeds the performance of other methods, including white-box approaches.   Project page: http://cs-people.bu.edu/vpetsiuk/rise/



### Standard Plane Detection in 3D Fetal Ultrasound Using an Iterative Transformation Network
- **Arxiv ID**: http://arxiv.org/abs/1806.07486v2
- **DOI**: 10.1007/978-3-030-00928-1_45
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07486v2)
- **Published**: 2018-06-19 22:29:20+00:00
- **Updated**: 2018-10-07 00:49:04+00:00
- **Authors**: Yuanwei Li, Bishesh Khanal, Benjamin Hou, Amir Alansary, Juan J. Cerrolaza, Matthew Sinclair, Jacqueline Matthew, Chandni Gupta, Caroline Knight, Bernhard Kainz, Daniel Rueckert
- **Comment**: 8 pages, 2 figures, accepted for MICCAI 2018; Added link to source
  code
- **Journal**: LNCS 11070 (2018) 392-400
- **Summary**: Standard scan plane detection in fetal brain ultrasound (US) forms a crucial step in the assessment of fetal development. In clinical settings, this is done by manually manoeuvring a 2D probe to the desired scan plane. With the advent of 3D US, the entire fetal brain volume containing these standard planes can be easily acquired. However, manual standard plane identification in 3D volume is labour-intensive and requires expert knowledge of fetal anatomy. We propose a new Iterative Transformation Network (ITN) for the automatic detection of standard planes in 3D volumes. ITN uses a convolutional neural network to learn the relationship between a 2D plane image and the transformation parameters required to move that plane towards the location/orientation of the standard plane in the 3D volume. During inference, the current plane image is passed iteratively to the network until it converges to the standard plane location. We explore the effect of using different transformation representations as regression outputs of ITN. Under a multi-task learning framework, we introduce additional classification probability outputs to the network to act as confidence measures for the regressed transformation parameters in order to further improve the localisation accuracy. When evaluated on 72 US volumes of fetal brain, our method achieves an error of 3.83mm/12.7 degrees and 3.80mm/12.6 degrees for the transventricular and transcerebellar planes respectively and takes 0.46s per plane. Source code is publicly available at https://github.com/yuanwei1989/plane-detection.



### Towards the identification of Parkinson's Disease using only T1 MR Images
- **Arxiv ID**: http://arxiv.org/abs/1806.07489v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07489v1)
- **Published**: 2018-06-19 22:44:19+00:00
- **Updated**: 2018-06-19 22:44:19+00:00
- **Authors**: Sara Soltaninejad, Irene Cheng, Anup Basu
- **Comment**: ICSM 2018
- **Journal**: None
- **Summary**: Parkinson's Disease (PD) is one of the most common types of neurological diseases caused by progressive degeneration of dopamin- ergic neurons in the brain. Even though there is no fixed cure for this neurodegenerative disease, earlier diagnosis followed by earlier treatment can help patients have a better quality of life. Magnetic Resonance Imag- ing (MRI) has been one of the most popular diagnostic tool in recent years because it avoids harmful radiations. In this paper, we investi- gate the plausibility of using MRIs for automatically diagnosing PD. Our proposed method has three main steps : 1) Preprocessing, 2) Fea- ture Extraction, and 3) Classification. The FreeSurfer library is used for the first and the second steps. For classification, three main types of classifiers, including Logistic Regression (LR), Random Forest (RF) and Support Vector Machine (SVM), are applied and their classification abil- ity is compared. The Parkinsons Progression Markers Initiative (PPMI) data set is used to evaluate the proposed method. The proposed system prove to be promising in assisting the diagnosis of PD.



### Myocardial Segmentation of Contrast Echocardiograms Using Random Forests Guided by Shape Model
- **Arxiv ID**: http://arxiv.org/abs/1806.07490v1
- **DOI**: 10.1007/978-3-319-46726-9_19
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07490v1)
- **Published**: 2018-06-19 22:48:50+00:00
- **Updated**: 2018-06-19 22:48:50+00:00
- **Authors**: Yuanwei Li, Chin Pang Ho, Navtej Chahal, Roxy Senior, Meng-Xing Tang
- **Comment**: 8 pages, 2 figures, accepted for MICCAI 2016
- **Journal**: None
- **Summary**: Myocardial Contrast Echocardiography (MCE) with micro-bubble contrast agent enables myocardial perfusion quantification which is invaluable for the early detection of coronary artery diseases. In this paper, we proposed a new segmentation method called Shape Model guided Random Forests (SMRF) for the analysis of MCE data. The proposed method utilizes a statistical shape model of the myocardium to guide the Random Forest (RF) segmentation in two ways. First, we introduce a novel Shape Model (SM) feature which captures the global structure and shape of the myocardium to produce a more accurate RF probability map. Second, the shape model is fitted to the RF probability map to further refine and constrain the final segmentation to plausible myocardial shapes. Evaluated on clinical MCE images from 15 patients, our method obtained promising results (Dice=0.81, Jaccard=0.70, MAD=1.68 mm, HD=6.53 mm) and showed a notable improvement in segmentation accuracy over the classic RF and its variants.



### Deep Global-Connected Net With The Generalized Multi-Piecewise ReLU Activation in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.03116v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.03116v1)
- **Published**: 2018-06-19 22:53:48+00:00
- **Updated**: 2018-06-19 22:53:48+00:00
- **Authors**: Zhi Chen, Pin-han Ho
- **Comment**: 9 pages, 3 figures and 5 tables
- **Journal**: None
- **Summary**: Recent Progress has shown that exploitation of hidden layer neurons in convolution neural networks incorporating with a carefully designed activation function can yield better classification results in the field of computer vision. The paper firstly introduces a novel deep learning architecture aiming to mitigate the gradient-vanishing problem, in which the earlier hidden layer neurons could be directly connected with the last hidden layer and feed into the last layer for classification. We then design a generalized linear rectifier function as the activation function that can approximate arbitrary complex functions via training of the parameters. We will show that our design can achieve similar performance in a number of object recognition and video action benchmark tasks, under significantly less number of parameters and shallower network infrastructure, which is not only promising in training in terms of computation burden and memory usage, but is also applicable to low-computation, low-memory mobile scenarios.



### On the Learning of Deep Local Features for Robust Face Spoofing Detection
- **Arxiv ID**: http://arxiv.org/abs/1806.07492v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.07492v2)
- **Published**: 2018-06-19 22:56:17+00:00
- **Updated**: 2018-10-11 13:00:52+00:00
- **Authors**: Gustavo Botelho de Souza, João Paulo Papa, Aparecido Nilceu Marana
- **Comment**: None
- **Journal**: Proceedings of 31st Conference on Graphics, Patterns and Images
  (SIBGRAPI) 2018
- **Summary**: Biometrics emerged as a robust solution for security systems. However, given the dissemination of biometric applications, criminals are developing techniques to circumvent them by simulating physical or behavioral traits of legal users (spoofing attacks). Despite face being a promising characteristic due to its universality, acceptability and presence of cameras almost everywhere, face recognition systems are extremely vulnerable to such frauds since they can be easily fooled with common printed facial photographs. State-of-the-art approaches, based on Convolutional Neural Networks (CNNs), present good results in face spoofing detection. However, these methods do not consider the importance of learning deep local features from each facial region, even though it is known from face recognition that each facial region presents different visual aspects, which can also be exploited for face spoofing detection. In this work we propose a novel CNN architecture trained in two steps for such task. Initially, each part of the neural network learns features from a given facial region. Afterwards, the whole model is fine-tuned on the whole facial images. Results show that such pre-training step allows the CNN to learn different local spoofing cues, improving the performance and the convergence speed of the final model, outperforming the state-of-the-art approaches.



### Fully Automatic Myocardial Segmentation of Contrast Echocardiography Sequence Using Random Forests Guided by Shape Model
- **Arxiv ID**: http://arxiv.org/abs/1806.07497v1
- **DOI**: 10.1109/TMI.2017.2747081
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07497v1)
- **Published**: 2018-06-19 23:18:07+00:00
- **Updated**: 2018-06-19 23:18:07+00:00
- **Authors**: Yuanwei Li, Chin Pang Ho, Matthieu Toulemonde, Navtej Chahal, Roxy Senior, Meng-Xing Tang
- **Comment**: 11 pages, 9 figures, published in TMI
- **Journal**: None
- **Summary**: Myocardial contrast echocardiography (MCE) is an imaging technique that assesses left ventricle function and myocardial perfusion for the detection of coronary artery diseases. Automatic MCE perfusion quantification is challenging and requires accurate segmentation of the myocardium from noisy and time-varying images. Random forests (RF) have been successfully applied to many medical image segmentation tasks. However, the pixel-wise RF classifier ignores contextual relationships between label outputs of individual pixels. RF which only utilizes local appearance features is also susceptible to data suffering from large intensity variations. In this paper, we demonstrate how to overcome the above limitations of classic RF by presenting a fully automatic segmentation pipeline for myocardial segmentation in full-cycle 2D MCE data. Specifically, a statistical shape model is used to provide shape prior information that guide the RF segmentation in two ways. First, a novel shape model (SM) feature is incorporated into the RF framework to generate a more accurate RF probability map. Second, the shape model is fitted to the RF probability map to refine and constrain the final segmentation to plausible myocardial shapes. We further improve the performance by introducing a bounding box detection algorithm as a preprocessing step in the segmentation pipeline. Our approach on 2D image is further extended to 2D+t sequence which ensures temporal consistency in the resultant sequence segmentations. When evaluated on clinical MCE data, our proposed method achieves notable improvement in segmentation accuracy and outperforms other state-of-the-art methods including the classic RF and its variants, active shape model and image registration.



