# Arxiv Papers in cs.CV on 2018-06-08
### Learn from Your Neighbor: Learning Multi-modal Mappings from Sparse Annotations
- **Arxiv ID**: http://arxiv.org/abs/1806.02934v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.02934v1)
- **Published**: 2018-06-08 01:18:10+00:00
- **Updated**: 2018-06-08 01:18:10+00:00
- **Authors**: Ashwin Kalyan, Stefan Lee, Anitha Kannan, Dhruv Batra
- **Comment**: To be presented at ICML 2018; 10 pages 5 figures
- **Journal**: None
- **Summary**: Many structured prediction problems (particularly in vision and language domains) are ambiguous, with multiple outputs being correct for an input - e.g. there are many ways of describing an image, multiple ways of translating a sentence; however, exhaustively annotating the applicability of all possible outputs is intractable due to exponentially large output spaces (e.g. all English sentences). In practice, these problems are cast as multi-class prediction, with the likelihood of only a sparse set of annotations being maximized - unfortunately penalizing for placing beliefs on plausible but unannotated outputs. We make and test the following hypothesis - for a given input, the annotations of its neighbors may serve as an additional supervisory signal. Specifically, we propose an objective that transfers supervision from neighboring examples. We first study the properties of our developed method in a controlled toy setup before reporting results on multi-label classification and two image-grounded sequence modeling tasks - captioning and question generation. We evaluate using standard task-specific metrics and measures of output diversity, finding consistent improvements over standard maximum likelihood training and other baselines.



### RGCNN: Regularized Graph CNN for Point Cloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.02952v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02952v1)
- **Published**: 2018-06-08 02:50:19+00:00
- **Updated**: 2018-06-08 02:50:19+00:00
- **Authors**: Gusi Te, Wei Hu, Zongming Guo, Amin Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Point cloud, an efficient 3D object representation, has become popular with the development of depth sensing and 3D laser scanning techniques. It has attracted attention in various applications such as 3D tele-presence, navigation for unmanned vehicles and heritage reconstruction. The understanding of point clouds, such as point cloud segmentation, is crucial in exploiting the informative value of point clouds for such applications. Due to the irregularity of the data format, previous deep learning works often convert point clouds to regular 3D voxel grids or collections of images before feeding them into neural networks, which leads to voluminous data and quantization artifacts. In this paper, we instead propose a regularized graph convolutional neural network (RGCNN) that directly consumes point clouds. Leveraging on spectral graph theory, we treat features of points in a point cloud as signals on graph, and define the convolution over graph by Chebyshev polynomial approximation. In particular, we update the graph Laplacian matrix that describes the connectivity of features in each layer according to the corresponding learned features, which adaptively captures the structure of dynamic graphs. Further, we deploy a graph-signal smoothness prior in the loss function, thus regularizing the learning process. Experimental results on the ShapeNet part dataset show that the proposed approach significantly reduces the computational complexity while achieving competitive performance with the state of the art. Also, experiments show RGCNN is much more robust to both noise and point cloud density in comparison with other methods. We further apply RGCNN to point cloud classification and achieve competitive results on ModelNet40 dataset.



### Endoscopic navigation in the absence of CT imaging
- **Arxiv ID**: http://arxiv.org/abs/1806.03997v1
- **DOI**: 10.1007/978-3-030-00937-3_8
- **Categories**: **eess.IV**, cs.CV, G.3; I.4.m; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1806.03997v1)
- **Published**: 2018-06-08 03:21:13+00:00
- **Updated**: 2018-06-08 03:21:13+00:00
- **Authors**: Ayushi Sinha, Xingtong Liu, Austin Reiter, Masaru Ishii, Gregory D. Hager, Russell H. Taylor
- **Comment**: 8 pages, 3 figures, MICCAI 2018
- **Journal**: None
- **Summary**: Clinical examinations that involve endoscopic exploration of the nasal cavity and sinuses often do not have a reference image to provide structural context to the clinician. In this paper, we present a system for navigation during clinical endoscopic exploration in the absence of computed tomography (CT) scans by making use of shape statistics from past CT scans. Using a deformable registration algorithm along with dense reconstructions from video, we show that we are able to achieve submillimeter registrations in in-vivo clinical data and are able to assign confidence to these registrations using confidence criteria established using simulated data.



### BSN: Boundary Sensitive Network for Temporal Action Proposal Generation
- **Arxiv ID**: http://arxiv.org/abs/1806.02964v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02964v3)
- **Published**: 2018-06-08 04:22:54+00:00
- **Updated**: 2018-09-26 10:48:22+00:00
- **Authors**: Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, Ming Yang
- **Comment**: Accepted at ECCV 2018
- **Journal**: None
- **Summary**: Temporal action proposal generation is an important yet challenging problem, since temporal proposals with rich action content are indispensable for analysing real-world videos with long duration and high proportion irrelevant content. This problem requires methods not only generating proposals with precise temporal boundaries, but also retrieving proposals to cover truth action instances with high recall and high overlap using relatively fewer proposals. To address these difficulties, we introduce an effective proposal generation method, named Boundary-Sensitive Network (BSN), which adopts "local to global" fashion. Locally, BSN first locates temporal boundaries with high probabilities, then directly combines these boundaries as proposals. Globally, with Boundary-Sensitive Proposal feature, BSN retrieves proposals by evaluating the confidence of whether a proposal contains an action within its region. We conduct experiments on two challenging datasets: ActivityNet-1.3 and THUMOS14, where BSN outperforms other state-of-the-art temporal action proposal generation methods with high recall and high temporal precision. Finally, further experiments demonstrate that by combining existing action classifiers, our method significantly improves the state-of-the-art temporal action detection performance.



### Fingerprint liveness detection using local quality features
- **Arxiv ID**: http://arxiv.org/abs/1806.02974v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02974v1)
- **Published**: 2018-06-08 05:48:10+00:00
- **Updated**: 2018-06-08 05:48:10+00:00
- **Authors**: Ram Prakash Sharma, Somnath Dey
- **Comment**: 21 pages, 11 figures, 7 Tables
- **Journal**: None
- **Summary**: Fingerprint-based recognition has been widely deployed in various applications. However, current recognition systems are vulnerable to spoofing attacks which make use of an artificial replica of a fingerprint to deceive the sensors. In such scenarios, fingerprint liveness detection ensures the actual presence of a real legitimate fingerprint in contrast to a fake self-manufactured synthetic sample. In this paper, we propose a static software-based approach using quality features to detect the liveness in a fingerprint. We have extracted features from a single fingerprint image to overcome the issues faced in dynamic software-based approaches which require longer computational time and user cooperation. The proposed system extracts 8 sensor independent quality features on a local level containing minute details of the ridge-valley structure of real and fake fingerprints. These local quality features constitutes a 13-dimensional feature vector. The system is tested on a publically available dataset of LivDet 2009 competition. The experimental results exhibit supremacy of the proposed method over current state-of-the-art approaches providing least average classification error of 5.3% for LivDet 2009. Additionally, effectiveness of the best performing features over LivDet 2009 is evaluated on the latest LivDet 2015 dataset which contain fingerprints fabricated using unknown spoof materials. An average classification error rate of 4.22% is achieved in comparison with 4.49% obtained by the LivDet 2015 winner. Further, the proposed system utilizes a single fingerprint image, which results in faster implications and makes it more user-friendly.



### DeepFirearm: Learning Discriminative Feature Representation for Fine-grained Firearm Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1806.02984v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02984v2)
- **Published**: 2018-06-08 06:45:32+00:00
- **Updated**: 2018-06-11 08:37:16+00:00
- **Authors**: Jiedong Hao, Jing Dong, Wei Wang, Tieniu Tan
- **Comment**: 6 pages, 5 figures, accepted by ICPR 2018. Code are available at
  https://github.com/jdhao/deep_firearm. Dataset is available at
  http://forensics.idealtest.org/Firearm14k/
- **Journal**: None
- **Summary**: There are great demands for automatically regulating inappropriate appearance of shocking firearm images in social media or identifying firearm types in forensics. Image retrieval techniques have great potential to solve these problems. To facilitate research in this area, we introduce Firearm 14k, a large dataset consisting of over 14,000 images in 167 categories. It can be used for both fine-grained recognition and retrieval of firearm images. Recent advances in image retrieval are mainly driven by fine-tuning state-of-the-art convolutional neural networks for retrieval task. The conventional single margin contrastive loss, known for its simplicity and good performance, has been widely used. We find that it performs poorly on the Firearm 14k dataset due to: (1) Loss contributed by positive and negative image pairs is unbalanced during training process. (2) A huge domain gap exists between this dataset and ImageNet. We propose to deal with the unbalanced loss by employing a double margin contrastive loss. We tackle the domain gap issue with a two-stage training strategy, where we first fine-tune the network for classification, and then fine-tune it for retrieval. Experimental results show that our approach outperforms the conventional single margin approach by a large margin (up to 88.5% relative improvement) and even surpasses the strong triplet-loss-based approach.



### A Systematic Evaluation of Recent Deep Learning Architectures for Fine-Grained Vehicle Classification
- **Arxiv ID**: http://arxiv.org/abs/1806.02987v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02987v1)
- **Published**: 2018-06-08 06:55:16+00:00
- **Updated**: 2018-06-08 06:55:16+00:00
- **Authors**: Krassimir Valev, Arne Schumann, Lars Sommer, Jürgen Beyerer
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-grained vehicle classification is the task of classifying make, model, and year of a vehicle. This is a very challenging task, because vehicles of different types but similar color and viewpoint can often look much more similar than vehicles of same type but differing color and viewpoint. Vehicle make, model, and year in com- bination with vehicle color - are of importance in several applications such as vehicle search, re-identification, tracking, and traffic analysis. In this work we investigate the suitability of several recent landmark convolutional neural network (CNN) architectures, which have shown top results on large scale image classification tasks, for the task of fine-grained classification of vehicles. We compare the performance of the networks VGG16, several ResNets, Inception architectures, the recent DenseNets, and MobileNet. For classification we use the Stanford Cars-196 dataset which features 196 different types of vehicles. We investigate several aspects of CNN training, such as data augmentation and training from scratch vs. fine-tuning. Importantly, we introduce no aspects in the architectures or training process which are specific to vehicle classification. Our final model achieves a state-of-the-art classification accuracy of 94.6% outperforming all related works, even approaches which are specifically tailored for the task, e.g. by including vehicle part detections.



### q-Space Novelty Detection with Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1806.02997v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG, cs.NE, 62F15, 62G07, 62M45, 68T30, G.3; H.3.3; I.2.4; I.2.6; I.4.6; I.5; I.5.4; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1806.02997v2)
- **Published**: 2018-06-08 07:28:36+00:00
- **Updated**: 2018-10-25 17:34:51+00:00
- **Authors**: Aleksei Vasilev, Vladimir Golkov, Marc Meissner, Ilona Lipp, Eleonora Sgarlata, Valentina Tomassini, Derek K. Jones, Daniel Cremers
- **Comment**: 11 pages, 2 figures
- **Journal**: None
- **Summary**: In machine learning, novelty detection is the task of identifying novel unseen data. During training, only samples from the normal class are available. Test samples are classified as normal or abnormal by assignment of a novelty score. Here we propose novelty detection methods based on training variational autoencoders (VAEs) on normal data. Since abnormal samples are not used during training, we define novelty metrics based on the (partially complementary) assumptions that the VAE is less capable of reconstructing abnormal samples well; that abnormal samples more strongly violate the VAE regularizer; and that abnormal samples differ from normal samples not only in input-feature space, but also in the VAE latent space and VAE output. These approaches, combined with various possibilities of using (e.g. sampling) the probabilistic VAE to obtain scalar novelty scores, yield a large family of methods. We apply these methods to magnetic resonance imaging, namely to the detection of diffusion-space (q-space) abnormalities in diffusion MRI scans of multiple sclerosis patients, i.e. to detect multiple sclerosis lesions without using any lesion labels for training. Many of our methods outperform previously proposed q-space novelty detection methods. We also evaluate the proposed methods on the MNIST handwritten digits dataset and show that many of them are able to outperform the state of the art.



### Logarithmic mathematical morphology: a new framework adaptive to illumination changes
- **Arxiv ID**: http://arxiv.org/abs/1806.02998v3
- **DOI**: 10.1007/978-3-030-13469-3_53
- **Categories**: **cs.CV**, math.GN, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1806.02998v3)
- **Published**: 2018-06-08 07:28:42+00:00
- **Updated**: 2019-03-25 15:51:21+00:00
- **Authors**: Guillaume Noyel
- **Comment**: None
- **Journal**: 23rd Iberoamerican Congress on Pattern Recognition (CIARP 2018),
  Nov 2018, Madrid, Spain. Springer International Publishing, Lecture Notes in
  Computer Science, 11401, pp.453-461, 2019, Progress in Pattern Recognition,
  Image Analysis, Computer Vision, and Applications.
  https://atvs.ii.uam.es/ciarp2018/
- **Summary**: A new set of mathematical morphology (MM) operators adaptive to illumination changes caused by variation of exposure time or light intensity is defined thanks to the Logarithmic Image Processing (LIP) model. This model based on the physics of acquisition is consistent with human vision. The fundamental operators, the logarithmic-dilation and the logarithmic-erosion, are defined with the LIP-addition of a structuring function. The combination of these two adjunct operators gives morphological filters, namely the logarithmic-opening and closing, useful for pattern recognition. The mathematical relation existing between ``classical'' dilation and erosion and their logarithmic-versions is established facilitating their implementation. Results on simulated and real images show that logarithmic-MM is more efficient on low-contrasted information than ``classical'' MM.



### Domain Adaptive Generation of Aircraft on Satellite Imagery via Simulated and Unsupervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.03002v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03002v1)
- **Published**: 2018-06-08 07:46:34+00:00
- **Updated**: 2018-06-08 07:46:34+00:00
- **Authors**: Junghoon Seo, Seunghyun Jeon, Taegyun Jeon
- **Comment**: presented at the International Workshop on Machine Learning for
  Artificial Intelligence Platforms held in 2017 Asian Conference on Machine
  Learning (MLAIP@ACML)
- **Journal**: None
- **Summary**: Object detection and classification for aircraft are the most important tasks in the satellite image analysis. The success of modern detection and classification methods has been based on machine learning and deep learning. One of the key requirements for those learning processes is huge data to train. However, there is an insufficient portion of aircraft since the targets are on military action and oper- ation. Considering the characteristics of satellite imagery, this paper attempts to provide a framework of the simulated and unsupervised methodology without any additional su- pervision or physical assumptions. Finally, the qualitative and quantitative analysis revealed a potential to replenish insufficient data for machine learning platform for satellite image analysis.



### Machine learning-based colon deformation estimation method for colonoscope tracking
- **Arxiv ID**: http://arxiv.org/abs/1806.03014v1
- **DOI**: 10.1117/12.2293936
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03014v1)
- **Published**: 2018-06-08 08:15:29+00:00
- **Updated**: 2018-06-08 08:15:29+00:00
- **Authors**: Masahiro Oda, Takayuki Kitasaka, Kazuhiro Furukawa, Ryoji Miyahara, Yoshiki Hirooka, Hidemi Goto, Nassir Navab, Kensaku Mori
- **Comment**: Accepted paper for oral presentation at SPIE Medical Imaging 2018,
  Houston, TX, USA
- **Journal**: SPIE Medical Imaging 2018: Image-Guided Procedures, Robotic
  Interventions, and Modeling
- **Summary**: This paper presents a colon deformation estimation method, which can be used to estimate colon deformations during colonoscope insertions. Colonoscope tracking or navigation system that navigates a physician to polyp positions during a colonoscope insertion is required to reduce complications such as colon perforation. A previous colonoscope tracking method obtains a colonoscope position in the colon by registering a colonoscope shape and a colon shape. The colonoscope shape is obtained using an electromagnetic sensor, and the colon shape is obtained from a CT volume. However, large tracking errors were observed due to colon deformations occurred during colonoscope insertions. Such deformations make the registration difficult. Because the colon deformation is caused by a colonoscope, there is a strong relationship between the colon deformation and the colonoscope shape. An estimation method of colon deformations occur during colonoscope insertions is necessary to reduce tracking errors. We propose a colon deformation estimation method. This method is used to estimate a deformed colon shape from a colonoscope shape. We use the regression forests algorithm to estimate a deformed colon shape. The regression forests algorithm is trained using pairs of colon and colonoscope shapes, which contains deformations occur during colonoscope insertions. As a preliminary study, we utilized the method to estimate deformations of a colon phantom. In our experiments, the proposed method correctly estimated deformed colon phantom shapes.



### Large-scale Bisample Learning on ID Versus Spot Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.03018v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03018v3)
- **Published**: 2018-06-08 08:27:55+00:00
- **Updated**: 2019-02-14 02:43:50+00:00
- **Authors**: Xiangyu Zhu, Hao Liu, Zhen Lei, Hailin Shi, Fan Yang, Dong Yi, Guojun Qi, Stan Z. Li
- **Comment**: Accepted by special issue on Deep Learning for Face Analysis.
  International Journal of Computer Vision (IJCV), 2019
- **Journal**: None
- **Summary**: In real-world face recognition applications, there is a tremendous amount of data with two images for each person. One is an ID photo for face enrollment, and the other is a probe photo captured on spot. Most existing methods are designed for training data with limited breadth (a relatively small number of classes) and sufficient depth (many samples for each class). They would meet great challenges on ID versus Spot (IvS) data, including the under-represented intra-class variations and an excessive demand on computing devices. In this paper, we propose a deep learning based large-scale bisample learning (LBL) method for IvS face recognition. To tackle the bisample problem with only two samples for each class, a classification-verification-classification (CVC) training strategy is proposed to progressively enhance the IvS performance. Besides, a dominant prototype softmax (DP-softmax) is incorporated to make the deep learning scalable on large-scale classes. We conduct LBL on a IvS face dataset with more than two million identities. Experimental results show the proposed method achieves superior performance to previous ones, validating the effectiveness of LBL on IvS face recognition.



### 3D FCN Feature Driven Regression Forest-Based Pancreas Localization and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.03019v1
- **DOI**: 10.1007/978-3-319-67558-9_26
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03019v1)
- **Published**: 2018-06-08 08:34:30+00:00
- **Updated**: 2018-06-08 08:34:30+00:00
- **Authors**: Masahiro Oda, Natsuki Shimizu, Holger R. Roth, Ken'ichi Karasawa, Takayuki Kitasaka, Kazunari Misawa, Michitaka Fujiwara, Daniel Rueckert, Kensaku Mori
- **Comment**: Presented in MICCAI 2017 workshop, DLMIA 2017 (Deep Learning in
  Medical Image Analysis and Multimodal Learning for Clinical Decision Support)
- **Journal**: DLMIA 2017, ML-CDS 2017: Deep Learning in Medical Image Analysis
  and Multimodal Learning for Clinical Decision Support, pp.222-230
- **Summary**: This paper presents a fully automated atlas-based pancreas segmentation method from CT volumes utilizing 3D fully convolutional network (FCN) feature-based pancreas localization. Segmentation of the pancreas is difficult because it has larger inter-patient spatial variations than other organs. Previous pancreas segmentation methods failed to deal with such variations. We propose a fully automated pancreas segmentation method that contains novel localization and segmentation. Since the pancreas neighbors many other organs, its position and size are strongly related to the positions of the surrounding organs. We estimate the position and the size of the pancreas (localized) from global features by regression forests. As global features, we use intensity differences and 3D FCN deep learned features, which include automatically extracted essential features for segmentation. We chose 3D FCN features from a trained 3D U-Net, which is trained to perform multi-organ segmentation. The global features include both the pancreas and surrounding organ information. After localization, a patient-specific probabilistic atlas-based pancreas segmentation is performed. In evaluation results with 146 CT volumes, we achieved 60.6% of the Jaccard index and 73.9% of the Dice overlap.



### Generating Image Sequence from Description with LSTM Conditional GAN
- **Arxiv ID**: http://arxiv.org/abs/1806.03027v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03027v1)
- **Published**: 2018-06-08 08:47:18+00:00
- **Updated**: 2018-06-08 08:47:18+00:00
- **Authors**: Xu Ouyang, Xi Zhang, Di Ma, Gady Agam
- **Comment**: Accepted by ICPR 2018
- **Journal**: None
- **Summary**: Generating images from word descriptions is a challenging task. Generative adversarial networks(GANs) are shown to be able to generate realistic images of real-life objects. In this paper, we propose a new neural network architecture of LSTM Conditional Generative Adversarial Networks to generate images of real-life objects. Our proposed model is trained on the Oxford-102 Flowers and Caltech-UCSD Birds-200-2011 datasets. We demonstrate that our proposed model produces the better results surpassing other state-of-art approaches.



### Unsupervised Feature Learning Toward a Real-time Vehicle Make and Model Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.03028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03028v1)
- **Published**: 2018-06-08 08:53:50+00:00
- **Updated**: 2018-06-08 08:53:50+00:00
- **Authors**: Amir Nazemi, Mohammad Javad Shafiee, Zohreh Azimifar, Alexander Wong
- **Comment**: 15 pages include 14 figures and 5 tables
- **Journal**: None
- **Summary**: Vehicle Make and Model Recognition (MMR) systems provide a fully automatic framework to recognize and classify different vehicle models. Several approaches have been proposed to address this challenge, however they can perform in restricted conditions. Here, we formulate the vehicle make and model recognition as a fine-grained classification problem and propose a new configurable on-road vehicle make and model recognition framework. We benefit from the unsupervised feature learning methods and in more details we employ Locality constraint Linear Coding (LLC) method as a fast feature encoder for encoding the input SIFT features. The proposed method can perform in real environments of different conditions. This framework can recognize fifty models of vehicles and has an advantage to classify every other vehicle not belonging to one of the specified fifty classes as an unknown vehicle. The proposed MMR framework can be configured to become faster or more accurate based on the application domain. The proposed approach is examined on two datasets including Iranian on-road vehicle dataset and CompuCar dataset. The Iranian on-road vehicle dataset contains images of 50 models of vehicles captured in real situations by traffic cameras in different weather and lighting conditions. Experimental results show superiority of the proposed framework over the state-of-the-art methods on Iranian on-road vehicle datatset and comparable results on CompuCar dataset with 97.5% and 98.4% accuracies, respectively.



### Deep multi-scale architectures for monocular depth estimation
- **Arxiv ID**: http://arxiv.org/abs/1806.03051v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03051v1)
- **Published**: 2018-06-08 09:49:10+00:00
- **Updated**: 2018-06-08 09:49:10+00:00
- **Authors**: Michel Moukari, Sylvaine Picard, Loic Simon, Frédéric Jurie
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims at understanding the role of multi-scale information in the estimation of depth from monocular images. More precisely, the paper investigates four different deep CNN architectures, designed to explicitly make use of multi-scale features along the network, and compare them to a state-of-the-art single-scale approach. The paper also shows that involving multi-scale features in depth estimation not only improves the performance in terms of accuracy, but also gives qualitatively better depth maps. Experiments are done on the widely used NYU Depth dataset, on which the proposed method achieves state-of-the-art performance.



### Unifying Identification and Context Learning for Person Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.03084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03084v1)
- **Published**: 2018-06-08 11:05:05+00:00
- **Updated**: 2018-06-08 11:05:05+00:00
- **Authors**: Qingqiu Huang, Yu Xiong, Dahua Lin
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Despite the great success of face recognition techniques, recognizing persons under unconstrained settings remains challenging. Issues like profile views, unfavorable lighting, and occlusions can cause substantial difficulties. Previous works have attempted to tackle this problem by exploiting the context, e.g. clothes and social relations. While showing promising improvement, they are usually limited in two important aspects, relying on simple heuristics to combine different cues and separating the construction of context from people identities. In this work, we aim to move beyond such limitations and propose a new framework to leverage context for person recognition. In particular, we propose a Region Attention Network, which is learned to adaptively combine visual cues with instance-dependent weights. We also develop a unified formulation, where the social contexts are learned along with the reasoning of people identities. These models substantially improve the robustness when working with the complex contextual relations in unconstrained environments. On two large datasets, PIPA and Cast In Movies (CIM), a new dataset proposed in this work, our method consistently achieves state-of-the-art performance under multiple evaluation policies.



### Contextual Hourglass Networks for Segmentation and Density Estimation
- **Arxiv ID**: http://arxiv.org/abs/1806.04009v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04009v1)
- **Published**: 2018-06-08 11:55:56+00:00
- **Updated**: 2018-06-08 11:55:56+00:00
- **Authors**: Daniel Oñoro-Rubio, Mathias Niepert
- **Comment**: None
- **Journal**: None
- **Summary**: Hourglass networks such as the U-Net and V-Net are popular neural architectures for medical image segmentation and counting problems. Typical instances of hourglass networks contain shortcut connections between mirroring layers. These shortcut connections improve the performance and it is hypothesized that this is due to mitigating effects on the vanishing gradient problem and the ability of the model to combine feature maps from earlier and later layers. We propose a method for not only combining feature maps of mirroring layers but also feature maps of layers with different spatial dimensions. For instance, the method enables the integration of the bottleneck feature map with those of the reconstruction layers. The proposed approach is applicable to any hourglass architecture. We evaluated the contextual hourglass networks on image segmentation and object counting problems in the medical domain. We achieve competitive results outperforming popular hourglass networks by up to 17 percentage points.



### Uncertainty-driven Sanity Check: Application to Postoperative Brain Tumor Cavity Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.03106v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03106v1)
- **Published**: 2018-06-08 12:09:39+00:00
- **Updated**: 2018-06-08 12:09:39+00:00
- **Authors**: Alain Jungo, Raphael Meier, Ekin Ermis, Evelyn Herrmann, Mauricio Reyes
- **Comment**: Appears in Medical Imaging with Deep Learning (MIDL), 2018
- **Journal**: None
- **Summary**: Uncertainty estimates of modern neuronal networks provide additional information next to the computed predictions and are thus expected to improve the understanding of the underlying model. Reliable uncertainties are particularly interesting for safety-critical computer-assisted applications in medicine, e.g., neurosurgical interventions and radiotherapy planning. We propose an uncertainty-driven sanity check for the identification of segmentation results that need particular expert review. Our method uses a fully-convolutional neural network and computes uncertainty estimates by the principle of Monte Carlo dropout. We evaluate the performance of the proposed method on a clinical dataset with 30 postoperative brain tumor images. The method can segment the highly inhomogeneous resection cavities accurately (Dice coefficients 0.792 $\pm$ 0.154). Furthermore, the proposed sanity check is able to detect the worst segmentation and three out of the four outliers. The results highlight the potential of using the additional information from the model's parameter uncertainty to validate the segmentation performance of a deep learning model.



### Rotation Equivariant CNNs for Digital Pathology
- **Arxiv ID**: http://arxiv.org/abs/1806.03962v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.03962v1)
- **Published**: 2018-06-08 12:13:37+00:00
- **Updated**: 2018-06-08 12:13:37+00:00
- **Authors**: Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, Max Welling
- **Comment**: To be presented at MICCAI 2018. Implementations of equivariant layers
  available at https://github.com/basveeling/keras_gcnn . PCam details and data
  at https://github.com/basveeling/pcam
- **Journal**: None
- **Summary**: We propose a new model for digital pathology segmentation, based on the observation that histopathology images are inherently symmetric under rotation and reflection. Utilizing recent findings on rotation equivariant CNNs, the proposed model leverages these symmetries in a principled manner. We present a visual analysis showing improved stability on predictions, and demonstrate that exploiting rotation equivariance significantly improves tumor detection performance on a challenging lymph node metastases dataset. We further present a novel derived dataset to enable principled comparison of machine learning models, in combination with an initial benchmark. Through this dataset, the task of histopathology diagnosis becomes accessible as a challenging benchmark for fundamental machine learning research.



### VTrails: Inferring Vessels with Geodesic Connectivity Trees
- **Arxiv ID**: http://arxiv.org/abs/1806.03111v1
- **DOI**: 10.1007/978-3-319-59050-9_53
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03111v1)
- **Published**: 2018-06-08 12:16:30+00:00
- **Updated**: 2018-06-08 12:16:30+00:00
- **Authors**: Stefano Moriconi, Maria A. Zuluaga, H. Rolf Jäger, Parashkev Nachev, Sébastien Ourselin, M. Jorge Cardoso
- **Comment**: None
- **Journal**: IPMI 2017: Information Processing in Medical Imaging pp 672-684
- **Summary**: The analysis of vessel morphology and connectivity has an impact on a number of cardiovascular and neurovascular applications by providing patient-specific high-level quantitative features such as spatial location, direction and scale. In this paper we present an end-to-end approach to extract an acyclic vascular tree from angiographic data by solving a connectivity-enforcing anisotropic fast marching over a voxel-wise tensor field representing the orientation of the underlying vascular tree. The method is validated using synthetic and real vascular images. We compare VTrails against classical and state-of-the-art ridge detectors for tubular structures by assessing the connectedness of the vesselness map and inspecting the synthesized tensor field as proof of concept. VTrails performance is evaluated on images with different levels of degradation: we verify that the extracted vascular network is an acyclic graph (i.e. a tree), and we report the extraction accuracy, precision and recall.



### Fully automated primary particle size analysis of agglomerates on transmission electron microscopy images via artificial neural networks
- **Arxiv ID**: http://arxiv.org/abs/1806.04010v1
- **DOI**: 10.1016/j.powtec.2018.03.032
- **Categories**: **cs.CV**, cond-mat.mtrl-sci, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/1806.04010v1)
- **Published**: 2018-06-08 13:11:09+00:00
- **Updated**: 2018-06-08 13:11:09+00:00
- **Authors**: Max Frei, Frank Einar Kruis
- **Comment**: None
- **Journal**: Powder Technology, Volume 332, 2018, Pages 120-130
- **Summary**: There is a high demand for fully automated methods for the analysis of primary particle size distributions of agglomerates on transmission electron microscopy images. Therefore, a novel method, based on the utilization of artificial neural networks, was proposed, implemented and validated. The training of the artificial neural networks requires large quantities (up to several hundreds of thousands) of transmission electron microscopy images of agglomerates consisting of primary particles with known sizes. Since the manual evaluation of such large amounts of transmission electron microscopy images is not feasible, a synthesis of lifelike transmission electron microscopy images as training data was implemented. The proposed method can compete with state-of-the-art automated imaging particle size methods like the Hough transformation, ultimate erosion and watershed transformation and is in some cases even able to outperform these methods. It is however still outperformed by the manual analysis.



### Hierarchy of GANs for learning embodied self-awareness model
- **Arxiv ID**: http://arxiv.org/abs/1806.04012v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1806.04012v1)
- **Published**: 2018-06-08 13:24:57+00:00
- **Updated**: 2018-06-08 13:24:57+00:00
- **Authors**: Mahdyar Ravanbakhsh, Mohamad Baydoun, Damian Campo, Pablo Marin, David Martin, Lucio Marcenaro, Carlo S. Regazzoni
- **Comment**: 2018 IEEE International Conference on Image Processing - ICIP'18.
  arXiv admin note: text overlap with arXiv:1806.02609
- **Journal**: None
- **Summary**: In recent years several architectures have been proposed to learn embodied agents complex self-awareness models. In this paper, dynamic incremental self-awareness (SA) models are proposed that allow experiences done by an agent to be modeled in a hierarchical fashion, starting from more simple situations to more structured ones. Each situation is learned from subsets of private agent perception data as a model capable to predict normal behaviors and detect abnormalities. Hierarchical SA models have been already proposed using low dimensional sensorial inputs. In this work, a hierarchical model is introduced by means of a cross-modal Generative Adversarial Networks (GANs) processing high dimensional visual data. Different levels of the GANs are detected in a self-supervised manner using GANs discriminators decision boundaries. Real experiments on semi-autonomous ground vehicles are presented.



### Automatic View Planning with Multi-scale Deep Reinforcement Learning Agents
- **Arxiv ID**: http://arxiv.org/abs/1806.03228v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03228v1)
- **Published**: 2018-06-08 15:49:45+00:00
- **Updated**: 2018-06-08 15:49:45+00:00
- **Authors**: Amir Alansary, Loic Le Folgoc, Ghislain Vaillant, Ozan Oktay, Yuanwei Li, Wenjia Bai, Jonathan Passerat-Palmbach, Ricardo Guerrero, Konstantinos Kamnitsas, Benjamin Hou, Steven McDonagh, Ben Glocker, Bernhard Kainz, Daniel Rueckert
- **Comment**: Accepted for MICCAI2018
- **Journal**: None
- **Summary**: We propose a fully automatic method to find standardized view planes in 3D image acquisitions. Standard view images are important in clinical practice as they provide a means to perform biometric measurements from similar anatomical regions. These views are often constrained to the native orientation of a 3D image acquisition. Navigating through target anatomy to find the required view plane is tedious and operator-dependent. For this task, we employ a multi-scale reinforcement learning (RL) agent framework and extensively evaluate several Deep Q-Network (DQN) based strategies. RL enables a natural learning paradigm by interaction with the environment, which can be used to mimic experienced operators. We evaluate our results using the distance between the anatomical landmarks and detected planes, and the angles between their normal vector and target. The proposed algorithm is assessed on the mid-sagittal and anterior-posterior commissure planes of brain MRI, and the 4-chamber long-axis plane commonly used in cardiac MRI, achieving accuracy of 1.53mm, 1.98mm and 4.84mm, respectively.



### PatchFCN for Intracranial Hemorrhage Detection
- **Arxiv ID**: http://arxiv.org/abs/1806.03265v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03265v2)
- **Published**: 2018-06-08 16:28:05+00:00
- **Updated**: 2019-04-14 23:09:29+00:00
- **Authors**: Weicheng Kuo, Christian Häne, Esther Yuh, Pratik Mukherjee, Jitendra Malik
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies the problem of detecting and segmenting acute intracranial hemorrhage on head computed tomography (CT) scans. We propose to solve both tasks as a semantic segmentation problem using a patch-based fully convolutional network (PatchFCN). This formulation allows us to accurately localize hemorrhages while bypassing the complexity of object detection. Our system demonstrates competitive performance with a human expert and the state-of-the-art on classification tasks (0.976, 0.966 AUC of ROC on retrospective and prospective test sets) and on segmentation tasks (0.785 pixel AP, 0.766 Dice score), while using much less data and a simpler system. In addition, we conduct a series of controlled experiments to understand "why" PatchFCN outperforms standard FCN. Our studies show that PatchFCN finds a good trade-off between batch diversity and the amount of context during training. These findings may also apply to other medical segmentation tasks.



### Sheep identity recognition, age and weight estimation datasets
- **Arxiv ID**: http://arxiv.org/abs/1806.04017v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1806.04017v1)
- **Published**: 2018-06-08 16:31:19+00:00
- **Updated**: 2018-06-08 16:31:19+00:00
- **Authors**: Aya Salama Abdelhady, Aboul Ella Hassanenin, Aly Fahmy
- **Comment**: None
- **Journal**: None
- **Summary**: Increased interest of scientists, producers and consumers in sheep identification has been stimulated by the dramatic increase in population and the urge to increase productivity. The world population is expected to exceed 9.6 million in 2050. For this reason, awareness is raised towards the necessity of effective livestock production. Sheep is considered as one of the main of food resources. Most of the research now is directed towards developing real time applications that facilitate sheep identification for breed management and gathering related information like weight and age. Weight and age are key matrices in assessing the effectiveness of production. For this reason, visual analysis proved recently its significant success over other approaches. Visual analysis techniques need enough images for testing and study completion. For this reason, collecting sheep images database is a vital step to fulfill such objective. We provide here datasets for testing and comparing such algorithms which are under development. Our collected dataset consists of 416 color images for different features of sheep in different postures. Images were collected fifty two sheep at a range of year from three months to six years. For each sheep, two images were captured for both sides of the body, two images for both sides of the face, one image from the top view, one image for the hip and one image for the teeth. The collected images cover different illumination, quality levels and angle of rotation. The allocated data set can be used to test sheep identification, weigh estimation, and age detection algorithms. Such algorithms are crucial for disease management, animal assessment and ownership.



### DMCNN: Dual-Domain Multi-Scale Convolutional Neural Network for Compression Artifacts Removal
- **Arxiv ID**: http://arxiv.org/abs/1806.03275v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03275v2)
- **Published**: 2018-06-08 17:01:52+00:00
- **Updated**: 2018-06-11 07:22:49+00:00
- **Authors**: Xiaoshuai Zhang, Wenhan Yang, Yueyu Hu, Jiaying Liu
- **Comment**: To appear in IEEE ICIP 2018
- **Journal**: None
- **Summary**: JPEG is one of the most commonly used standards among lossy image compression methods. However, JPEG compression inevitably introduces various kinds of artifacts, especially at high compression rates, which could greatly affect the Quality of Experience (QoE). Recently, convolutional neural network (CNN) based methods have shown excellent performance for removing the JPEG artifacts. Lots of efforts have been made to deepen the CNNs and extract deeper features, while relatively few works pay attention to the receptive field of the network. In this paper, we illustrate that the quality of output images can be significantly improved by enlarging the receptive fields in many cases. One step further, we propose a Dual-domain Multi-scale CNN (DMCNN) to take full advantage of redundancies on both the pixel and DCT domains. Experiments show that DMCNN sets a new state-of-the-art for the task of JPEG artifact removal.



### Unsupervised Learning for Surgical Motion by Learning to Predict the Future
- **Arxiv ID**: http://arxiv.org/abs/1806.03318v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03318v1)
- **Published**: 2018-06-08 18:47:06+00:00
- **Updated**: 2018-06-08 18:47:06+00:00
- **Authors**: Robert DiPietro, Gregory D. Hager
- **Comment**: Accepted to MICCAI 2018
- **Journal**: None
- **Summary**: We show that it is possible to learn meaningful representations of surgical motion, without supervision, by learning to predict the future. An architecture that combines an RNN encoder-decoder and mixture density networks (MDNs) is developed to model the conditional distribution over future motion given past motion. We show that the learned encodings naturally cluster according to high-level activities, and we demonstrate the usefulness of these learned encodings in the context of information retrieval, where a database of surgical motion is searched for suturing activity using a motion-based query. Future prediction with MDNs is found to significantly outperform simpler baselines as well as the best previously-published result for this task, advancing state-of-the-art performance from an F1 score of 0.60 +- 0.14 to 0.77 +- 0.05.



### DSSLIC: Deep Semantic Segmentation-based Layered Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1806.03348v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03348v3)
- **Published**: 2018-06-08 20:38:34+00:00
- **Updated**: 2019-04-18 19:01:30+00:00
- **Authors**: Mohammad Akbari, Jie Liang, Jingning Han
- **Comment**: - More Experimental results added
- **Journal**: None
- **Summary**: Deep learning has revolutionized many computer vision fields in the last few years, including learning-based image compression. In this paper, we propose a deep semantic segmentation-based layered image compression (DSSLIC) framework in which the semantic segmentation map of the input image is obtained and encoded as the base layer of the bit-stream. A compact representation of the input image is also generated and encoded as the first enhancement layer. The segmentation map and the compact version of the image are then employed to obtain a coarse reconstruction of the image. The residual between the input and the coarse reconstruction is additionally encoded as another enhancement layer. Experimental results show that the proposed framework outperforms the H.265/HEVC-based BPG and other codecs in both PSNR and MS-SSIM metrics across a wide range of bit rates in RGB domain. Besides, since semantic segmentation map is included in the bit-stream, the proposed scheme can facilitate many other tasks such as image search and object-based adaptive image compression.



### A Content-Based Late Fusion Approach Applied to Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1806.03361v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1806.03361v1)
- **Published**: 2018-06-08 21:35:09+00:00
- **Updated**: 2018-06-08 21:35:09+00:00
- **Authors**: Jessica Sena, Artur Jordao, William Robson Schwartz
- **Comment**: None
- **Journal**: None
- **Summary**: The variety of pedestrians detectors proposed in recent years has encouraged some works to fuse pedestrian detectors to achieve a more accurate detection. The intuition behind is to combine the detectors based on its spatial consensus. We propose a novel method called Content-Based Spatial Consensus (CSBC), which, in addition to relying on spatial consensus, considers the content of the detection windows to learn a weighted-fusion of pedestrian detectors. The result is a reduction in false alarms and an enhancement in the detection. In this work, we also demonstrate that there is small influence of the feature used to learn the contents of the windows of each detector, which enables our method to be efficient even employing simple features. The CSBC overcomes state-of-the-art fusion methods in the ETH dataset and in the Caltech dataset. Particularly, our method is more efficient since fewer detectors are necessary to achieve expressive results.



### Self-supervisory Signals for Object Discovery and Detection
- **Arxiv ID**: http://arxiv.org/abs/1806.03370v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03370v1)
- **Published**: 2018-06-08 22:50:28+00:00
- **Updated**: 2018-06-08 22:50:28+00:00
- **Authors**: Etienne Pot, Alexander Toshev, Jana Kosecka
- **Comment**: None
- **Journal**: None
- **Summary**: In robotic applications, we often face the challenge of discovering new objects while having very little or no labelled training data. In this paper we explore the use of self-supervision provided by a robot traversing an environment to learn representations of encountered objects. Knowledge of ego-motion and depth perception enables the agent to effectively associate multiple object proposals, which serve as training data for learning object representations from unlabelled images. We demonstrate the utility of this representation in two ways. First, we can automatically discover objects by performing clustering in the learned embedding space. Each resulting cluster contains examples of one instance seen from various viewpoints and scales. Second, given a small number of labeled images, we can efficiently learn detectors for these labels. In the few-shot regime, these detectors have a substantially higher mAP of 0.22 compared to 0.12 of off-the-shelf standard detectors trained on this limited data. Thus, the proposed self-supervision results in effective environment specific object discovery and detection at no or very small human labeling cost.



### CS-VQA: Visual Question Answering with Compressively Sensed Images
- **Arxiv ID**: http://arxiv.org/abs/1806.03379v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68
- **Links**: [PDF](http://arxiv.org/pdf/1806.03379v1)
- **Published**: 2018-06-08 23:26:22+00:00
- **Updated**: 2018-06-08 23:26:22+00:00
- **Authors**: Li-Chi Huang, Kuldeep Kulkarni, Anik Jha, Suhas Lohit, Suren Jayasuriya, Pavan Turaga
- **Comment**: 5 pages, 2 figures, accepted to ICIP 2018
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) is a complex semantic task requiring both natural language processing and visual recognition. In this paper, we explore whether VQA is solvable when images are captured in a sub-Nyquist compressive paradigm. We develop a series of deep-network architectures that exploit available compressive data to increasing degrees of accuracy, and show that VQA is indeed solvable in the compressed domain. Our results show that there is nominal degradation in VQA performance when using compressive measurements, but that accuracy can be recovered when VQA pipelines are used in conjunction with state-of-the-art deep neural networks for CS reconstruction. The results presented yield important implications for resource-constrained VQA applications.



