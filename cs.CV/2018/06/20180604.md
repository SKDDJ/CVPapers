# Arxiv Papers in cs.CV on 2018-06-04
### Recent advances and opportunities in scene classification of aerial images with deep models
- **Arxiv ID**: http://arxiv.org/abs/1806.00899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00899v1)
- **Published**: 2018-06-04 00:02:17+00:00
- **Updated**: 2018-06-04 00:02:17+00:00
- **Authors**: Fan Hu, Gui-Song Xia, Wen Yang, Liangpei Zhang
- **Comment**: IGARSS'18 conference paper
- **Journal**: None
- **Summary**: Scene classification is a fundamental task in interpretation of remote sensing images, and has become an active research topic in remote sensing community due to its important role in a wide range of applications. Over the past years, tremendous efforts have been made for developing powerful approaches for scene classification of remote sensing images, evolving from the traditional bag-of-visual-words model to the new generation deep convolutional neural networks (CNNs). The deep CNN based methods have exhibited remarkable breakthrough on performance, dramatically outperforming previous methods which strongly rely on hand-crafted features. However, performance with deep CNNs has gradually plateaued on existing public scene datasets, due to the notable drawbacks of these datasets, such as the small scale and low-diversity of training samples. Therefore, to promote the development of new methods and move the scene classification task a step further, we deeply discuss the existing problems in scene classification task, and accordingly present three open directions. We believe these potential directions will be instructive for the researchers in this field.



### Large-scale Land Cover Classification in GaoFen-2 Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1806.00901v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00901v1)
- **Published**: 2018-06-04 00:12:00+00:00
- **Updated**: 2018-06-04 00:12:00+00:00
- **Authors**: Xin-Yi Tong, Qikai Lu, Gui-Song Xia, Liangpei Zhang
- **Comment**: IGARSS'18 conference paper
- **Journal**: None
- **Summary**: Many significant applications need land cover information of remote sensing images that are acquired from different areas and times, such as change detection and disaster monitoring. However, it is difficult to find a generic land cover classification scheme for different remote sensing images due to the spectral shift caused by diverse acquisition condition. In this paper, we develop a novel land cover classification method that can deal with large-scale data captured from widely distributed areas and different times. Additionally, we establish a large-scale land cover classification dataset consisting of 150 Gaofen-2 imageries as data support for model training and performance evaluation. Our experiments achieve outstanding classification accuracy compared with traditional methods.



### Accurate Building Detection in VHR Remote Sensing Images using Geometric Saliency
- **Arxiv ID**: http://arxiv.org/abs/1806.00908v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00908v2)
- **Published**: 2018-06-04 01:02:22+00:00
- **Updated**: 2018-06-10 01:38:45+00:00
- **Authors**: Jin Huang, Gui-Song Xia, Fan Hu, Liangpei Zhang
- **Comment**: IGRASS'18 conference paper
- **Journal**: None
- **Summary**: This paper aims to address the problem of detecting buildings from remote sensing images with very high resolution (VHR). Inspired by the observation that buildings are always more distinguishable in geometries than in texture or spectral, we propose a new geometric building index (GBI) for accurate building detection, which relies on the geometric saliency of building structures. The geometric saliency of buildings is derived from a mid-level geometric representations based on meaningful junctions that can locally describe anisotropic geometrical structures of images. The resulting GBI is measured by integrating the derived geometric saliency of buildings. Experiments on three public datasets demonstrate that the proposed GBI achieves very promising performance, and meanwhile shows impressive generalization capability.



### Bayesian Semantic Instance Segmentation in Open Set World
- **Arxiv ID**: http://arxiv.org/abs/1806.00911v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00911v2)
- **Published**: 2018-06-04 01:16:17+00:00
- **Updated**: 2018-07-30 02:38:12+00:00
- **Authors**: Trung Pham, Vijay Kumar B G, Thanh-Toan Do, Gustavo Carneiro, Ian Reid
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: This paper addresses the semantic instance segmentation task in the open-set conditions, where input images can contain known and unknown object classes. The training process of existing semantic instance segmentation methods requires annotation masks for all object instances, which is expensive to acquire or even infeasible in some realistic scenarios, where the number of categories may increase boundlessly. In this paper, we present a novel open-set semantic instance segmentation approach capable of segmenting all known and unknown object classes in images, based on the output of an object detector trained on known object classes. We formulate the problem using a Bayesian framework, where the posterior distribution is approximated with a simulated annealing optimization equipped with an efficient image partition sampler. We show empirically that our method is competitive with state-of-the-art supervised methods on known classes, but also performs well on unknown classes when compared with unsupervised methods.



### Automatic catheter detection in pediatric X-ray images using a scale-recurrent network and synthetic data
- **Arxiv ID**: http://arxiv.org/abs/1806.00921v1
- **DOI**: 10.1007/s10278-019-00201-7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00921v1)
- **Published**: 2018-06-04 01:54:38+00:00
- **Updated**: 2018-06-04 01:54:38+00:00
- **Authors**: Xin Yi, Scott Adams, Paul Babyn, Abdul Elnajmi
- **Comment**: accepted to the 1st Conference on Medical Imaging with Deep Learning
  (MIDL2018), Amsterdam, The Netherlands
- **Journal**: None
- **Summary**: Catheters are commonly inserted life supporting devices. X-ray images are used to assess the position of a catheter immediately after placement as serious complications can arise from malpositioned catheters. Previous computer vision approaches to detect catheters on X-ray images either relied on low-level cues that are not sufficiently robust or only capable of processing a limited number or type of catheters. With the resurgence of deep learning, supervised training approaches are begining to showing promising results. However, dense annotation maps are required, and the work of a human annotator is hard to scale. In this work, we proposed a simple way of synthesizing catheters on X-ray images and a scale recurrent network for catheter detection. By training on adult chest X-rays, the proposed network exhibits promising detection results on pediatric chest/abdomen X-rays in terms of both precision and recall.



### NRTR: A No-Recurrence Sequence-to-Sequence Model For Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.00926v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00926v2)
- **Published**: 2018-06-04 02:10:35+00:00
- **Updated**: 2019-10-10 11:30:21+00:00
- **Authors**: Fenfen Sheng, Zhineng Chen, Bo Xu
- **Comment**: 6 pages, 3 figures, 3 tables
- **Journal**: None
- **Summary**: Scene text recognition has attracted a great many researches due to its importance to various applications. Existing methods mainly adopt recurrence or convolution based networks. Though have obtained good performance, these methods still suffer from two limitations: slow training speed due to the internal recurrence of RNNs, and high complexity due to stacked convolutional layers for long-term feature extraction. This paper, for the first time, proposes a no-recurrence sequence-to-sequence text recognizer, named NRTR, that dispenses with recurrences and convolutions entirely. NRTR follows the encoder-decoder paradigm, where the encoder uses stacked self-attention to extract image features, and the decoder applies stacked self-attention to recognize texts based on encoder output. NRTR relies solely on self-attention mechanism thus could be trained with more parallelization and less complexity. Considering scene image has large variation in text and background, we further design a modality-transform block to effectively transform 2D input images to 1D sequences, combined with the encoder to extract more discriminative features. NRTR achieves state-of-the-art or highly competitive performance on both regular and irregular benchmarks, while requires only a small fraction of training time compared to the best model from the literature (at least 8 times faster).



### Training deep learning based image denoisers from undersampled measurements without ground truth and without image prior
- **Arxiv ID**: http://arxiv.org/abs/1806.00961v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00961v2)
- **Published**: 2018-06-04 05:41:46+00:00
- **Updated**: 2018-12-19 15:46:52+00:00
- **Authors**: Magauiya Zhussip, Shakarim Soltanayev, Se Young Chun
- **Comment**: 10 pages, 5 figures, 3 tables
- **Journal**: None
- **Summary**: Compressive sensing is a method to recover the original image from undersampled measurements. In order to overcome the ill-posedness of this inverse problem, image priors are used such as sparsity in the wavelet domain, minimum total-variation, or self-similarity. Recently, deep learning based compressive image recovery methods have been proposed and have yielded state-of-the-art performances. They used deep learning based data-driven approaches instead of hand-crafted image priors to solve the ill-posed inverse problem with undersampled data. Ironically, training deep neural networks for them requires "clean" ground truth images, but obtaining the best quality images from undersampled data requires well-trained deep neural networks. To resolve this dilemma, we propose novel methods based on two well-grounded theories: denoiser-approximate message passing and Stein's unbiased risk estimator. Our proposed methods were able to train deep learning based image denoisers from undersampled measurements without ground truth images and without image priors, and to recover images with state-of-the-art qualities from undersampled data. We evaluated our methods for various compressive sensing recovery problems with Gaussian random, coded diffraction pattern, and compressive sensing MRI measurement matrices. Our methods yielded state-of-the-art performances for all cases without ground truth images and without image priors. They also yielded comparable performances to the methods with ground truth data.



### ALMN: Deep Embedding Learning with Geometrical Virtual Point Generating
- **Arxiv ID**: http://arxiv.org/abs/1806.00974v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00974v2)
- **Published**: 2018-06-04 06:38:28+00:00
- **Updated**: 2018-06-05 10:47:05+00:00
- **Authors**: Binghui Chen, Weihong Deng
- **Comment**: None
- **Journal**: None
- **Summary**: Deep embedding learning becomes more attractive for discriminative feature learning, but many methods still require hard-class mining, which is computationally complex and performance-sensitive. To this end, we propose Adaptive Large Margin N-Pair loss (ALMN) to address the aforementioned issues. Instead of exploring hard example-mining strategy, we introduce the concept of large margin constraint. This constraint aims at encouraging local-adaptive large angular decision margin among dissimilar samples in multimodal feature space so as to significantly encourage intraclass compactness and interclass separability. And it is mainly achieved by a simple yet novel geometrical Virtual Point Generating (VPG) method, which converts artificially setting a fixed margin into automatically generating a boundary training sample in feature space and is an open question. We demonstrate the effectiveness of our method on several popular datasets for image retrieval and clustering tasks.



### Synthetic data generation for end-to-end thermal infrared tracking
- **Arxiv ID**: http://arxiv.org/abs/1806.01013v2
- **DOI**: 10.1109/TIP.2018.2879249
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01013v2)
- **Published**: 2018-06-04 08:52:28+00:00
- **Updated**: 2018-09-26 11:58:14+00:00
- **Authors**: Lichao Zhang, Abel Gonzalez-Garcia, Joost van de Weijer, Martin Danelljan, Fahad Shahbaz Khan
- **Comment**: None
- **Journal**: None
- **Summary**: The usage of both off-the-shelf and end-to-end trained deep networks have significantly improved performance of visual tracking on RGB videos. However, the lack of large labeled datasets hampers the usage of convolutional neural networks for tracking in thermal infrared (TIR) images. Therefore, most state of the art methods on tracking for TIR data are still based on handcrafted features. To address this problem, we propose to use image-to-image translation models. These models allow us to translate the abundantly available labeled RGB data to synthetic TIR data. We explore both the usage of paired and unpaired image translation models for this purpose. These methods provide us with a large labeled dataset of synthetic TIR sequences, on which we can train end-to-end optimal features for tracking. To the best of our knowledge we are the first to train end-to-end features for TIR tracking. We perform extensive experiments on VOT-TIR2017 dataset. We show that a network trained on a large dataset of synthetic TIR data obtains better performance than one trained on the available real TIR data. Combining both data sources leads to further improvement. In addition, when we combine the network with motion features we outperform the state of the art with a relative gain of over 10%, clearly showing the efficiency of using synthetic data to train end-to-end TIR trackers.



### A 2.5D Cascaded Convolutional Neural Network with Temporal Information for Automatic Mitotic Cell Detection in 4D Microscopic Images
- **Arxiv ID**: http://arxiv.org/abs/1806.01018v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01018v2)
- **Published**: 2018-06-04 09:05:24+00:00
- **Updated**: 2018-11-27 03:07:37+00:00
- **Authors**: Titinunt Kitrungrotsakul, Xian-Hau Han, Yutaro Iwamoto, Satoko Takemoto, Hideo Yokota, Sari Ipponjima, Tomomi Nemoto, Xiong Wei, Yen-Wei Chen
- **Comment**: 4 pages, 4 figures, conference paper (submitted to arxiv then update
  version and submitted to conference. Finally update in arxiv for newest
  version)
- **Journal**: None
- **Summary**: In recent years, intravital skin imaging has been increasingly used in mammalian skin research to investigate cell behaviors. A fundamental step of the investigation is mitotic cell (cell division) detection. Because of the complex backgrounds (normal cells), the majority of the existing methods cause several false positives. In this paper, we proposed a 2.5D cascaded end-to-end convolutional neural network (CasDetNet) with temporal information to accurately detect automatic mitotic cell in 4D microscopic images with few training data. The CasDetNet consists of two 2.5D networks. The first one is used for detecting candidate cells with only volume information and the second one, containing temporal information, for reducing false positive and adding mitotic cells that were missed in the first step. The experimental results show that our CasDetNet can achieve higher precision and recall compared to other state-of-the-art methods.



### Differential Diagnosis for Pancreatic Cysts in CT Scans Using Densely-Connected Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.01023v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01023v3)
- **Published**: 2018-06-04 09:25:59+00:00
- **Updated**: 2018-06-19 07:38:11+00:00
- **Authors**: Hongwei Li, Kanru Lin, Maximilian Reichert, Lina Xu, Rickmer Braren, Deliang Fu, Roland Schmid, Ji Li, Bjoern Menze, Kuangyu Shi
- **Comment**: submitted to miccai 2017, *corresponding author: liji@huashan.org.cn
- **Journal**: None
- **Summary**: The lethal nature of pancreatic ductal adenocarcinoma (PDAC) calls for early differential diagnosis of pancreatic cysts, which are identified in up to 16% of normal subjects, and some of which may develop into PDAC. Previous computer-aided developments have achieved certain accuracy for classification on segmented cystic lesions in CT. However, pancreatic cysts have a large variation in size and shape, and the precise segmentation of them remains rather challenging, which restricts the computer-aided interpretation of CT images acquired for differential diagnosis. We propose a computer-aided framework for early differential diagnosis of pancreatic cysts without pre-segmenting the lesions using densely-connected convolutional networks (Dense-Net). The Dense-Net learns high-level features from whole abnormal pancreas and builds mappings between medical imaging appearance to different pathological types of pancreatic cysts. To enhance the clinical applicability, we integrate saliency maps in the framework to assist the physicians to understand the decision of the deep learning method. The test on a cohort of 206 patients with 4 pathologically confirmed subtypes of pancreatic cysts has achieved an overall accuracy of 72.8%, which is significantly higher than the baseline accuracy of 48.1%, which strongly supports the clinical potential of our developed method.



### RedNet: Residual Encoder-Decoder Network for indoor RGB-D Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.01054v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01054v2)
- **Published**: 2018-06-04 11:33:57+00:00
- **Updated**: 2018-08-06 17:11:25+00:00
- **Authors**: Jindong Jiang, Lunan Zheng, Fei Luo, Zhijun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Indoor semantic segmentation has always been a difficult task in computer vision. In this paper, we propose an RGB-D residual encoder-decoder architecture, named RedNet, for indoor RGB-D semantic segmentation. In RedNet, the residual module is applied to both the encoder and decoder as the basic building block, and the skip-connection is used to bypass the spatial feature between the encoder and decoder. In order to incorporate the depth information of the scene, a fusion structure is constructed, which makes inference on RGB image and depth image separately, and fuses their features over several layers. In order to efficiently optimize the network's parameters, we propose a `pyramid supervision' training scheme, which applies supervised learning over different layers in the decoder, to cope with the problem of gradients vanishing. Experiment results show that the proposed RedNet(ResNet-50) achieves a state-of-the-art mIoU accuracy of 47.8% on the SUN RGB-D benchmark dataset.



### Deep Multi-Structural Shape Analysis: Application to Neuroanatomy
- **Arxiv ID**: http://arxiv.org/abs/1806.01069v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01069v1)
- **Published**: 2018-06-04 12:22:40+00:00
- **Updated**: 2018-06-04 12:22:40+00:00
- **Authors**: Benjamin Gutierrez-Becker, Christian Wachinger
- **Comment**: Accepted at MICCAI 2018
- **Journal**: None
- **Summary**: We propose a deep neural network for supervised learning on neuroanatomical shapes. The network directly operates on raw point clouds without the need for mesh processing or the identification of point correspondences, as spatial transformer networks map the data to a canonical space. Instead of relying on hand-crafted shape descriptors, an optimal representation is learned in the end-to-end training stage of the network. The proposed network consists of multiple branches, so that features for multiple structures are learned simultaneously. We demonstrate the performance of our method on two applications: (i) the prediction of Alzheimer's disease and mild cognitive impairment and (ii) the regression of the brain age. Finally, we visualize the important parts of the anatomy for the prediction by adapting the occlusion method to point clouds.



### Modeling Realistic Degradations in Non-blind Deconvolution
- **Arxiv ID**: http://arxiv.org/abs/1806.01097v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01097v1)
- **Published**: 2018-06-04 13:32:15+00:00
- **Updated**: 2018-06-04 13:32:15+00:00
- **Authors**: Jérémy Anger, Mauricio Delbracio, Gabriele Facciolo
- **Comment**: Accepted at the 2018 IEEE International Conference on Image
  Processing (ICIP 2018)
- **Journal**: None
- **Summary**: Most image deblurring methods assume an over-simplistic image formation model and as a result are sensitive to more realistic image degradations. We propose a novel variational framework, that explicitly handles pixel saturation, noise, quantization, as well as non-linear camera response function due to e.g., gamma correction. We show that accurately modeling a more realistic image acquisition pipeline leads to significant improvements, both in terms of image quality and PSNR. Furthermore, we show that incorporating the non-linear response in both the data and the regularization terms of the proposed energy leads to a more detailed restoration than a naive inversion of the non-linear curve. The minimization of the proposed energy is performed using stochastic optimization. A dataset consisting of realistically degraded images is created in order to evaluate the method.



### Deep Continuous Conditional Random Fields with Asymmetric Inter-object Constraints for Online Multi-object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1806.01183v1
- **DOI**: 10.1109/TCSVT.2018.2825679
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01183v1)
- **Published**: 2018-06-04 16:27:03+00:00
- **Updated**: 2018-06-04 16:27:03+00:00
- **Authors**: Hui Zhou, Wanli Ouyang, Jian Cheng, Xiaogang Wang, Hongsheng Li
- **Comment**: Accepted to TCSVT
- **Journal**: None
- **Summary**: Online Multi-Object Tracking (MOT) is a challenging problem and has many important applications including intelligence surveillance, robot navigation and autonomous driving. In existing MOT methods, individual object's movements and inter-object relations are mostly modeled separately and relations between them are still manually tuned. In addition, inter-object relations are mostly modeled in a symmetric way, which we argue is not an optimal setting. To tackle those difficulties, in this paper, we propose a Deep Continuous Conditional Random Field (DCCRF) for solving the online MOT problem in a track-by-detection framework. The DCCRF consists of unary and pairwise terms. The unary terms estimate tracked objects' displacements across time based on visual appearance information. They are modeled as deep Convolution Neural Networks, which are able to learn discriminative visual features for tracklet association. The asymmetric pairwise terms model inter-object relations in an asymmetric way, which encourages high-confidence tracklets to help correct errors of low-confidence tracklets and not to be affected by low-confidence ones much. The DCCRF is trained in an end-to-end manner for better adapting the influences of visual information as well as inter-object relations. Extensive experimental comparisons with state-of-the-arts as well as detailed component analysis of our proposed DCCRF on two public benchmarks demonstrate the effectiveness of our proposed MOT framework.



### Face Synthesis for Eyeglass-Robust Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.01196v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01196v2)
- **Published**: 2018-06-04 16:38:45+00:00
- **Updated**: 2021-02-10 13:09:00+00:00
- **Authors**: Jianzhu Guo, Xiangyu Zhu, Zhen Lei, Stan Z. Li
- **Comment**: Accepted by CCBR 2018, with MeGlass released at
  https://github.com/cleardusk/MeGlass
- **Journal**: None
- **Summary**: In the application of face recognition, eyeglasses could significantly degrade the recognition accuracy. A feasible method is to collect large-scale face images with eyeglasses for training deep learning methods. However, it is difficult to collect the images with and without glasses of the same identity, so that it is difficult to optimize the intra-variations caused by eyeglasses. In this paper, we propose to address this problem in a virtual synthesis manner. The high-fidelity face images with eyeglasses are synthesized based on 3D face model and 3D eyeglasses. Models based on deep learning methods are then trained on the synthesized eyeglass face dataset, achieving better performance than previous ones. Experiments on the real face database validate the effectiveness of our synthesized data for improving eyeglass face recognition performance.



### Image reconstruction through metamorphosis
- **Arxiv ID**: http://arxiv.org/abs/1806.01225v2
- **DOI**: None
- **Categories**: **cs.CV**, 65F22, 65R32, 65R30, 65D18, 94A12, 94A08, 92C55, 54C56, 57N25, 47A52
- **Links**: [PDF](http://arxiv.org/pdf/1806.01225v2)
- **Published**: 2018-06-04 17:09:54+00:00
- **Updated**: 2018-06-22 08:55:03+00:00
- **Authors**: Gris Barbara, Chen Chong, Öktem Ozan
- **Comment**: None
- **Journal**: None
- **Summary**: This article adapts the framework of metamorphosis to solve inverse problems in imaging that includes joint reconstruction and image registration. The deformations in question have two components, one that is a geometric deformation moving intensities and the other a deformation of intensity values itself, which, e.g., allows for appearance of a new structure. The idea developed here is to reconstruct an image from noisy and indirect observations by registering, via metamorphosis, a template to the observed data. Unlike a registration with only geometrical changes, this framework gives good results when intensities of the template are poorly chosen. We show that this method is a well-defined regularisation method (proving existence, stability and convergence) and present several numerical examples.



### Digging Into Self-Supervised Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1806.01260v4
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.01260v4)
- **Published**: 2018-06-04 17:58:05+00:00
- **Updated**: 2019-08-17 22:57:30+00:00
- **Authors**: Clément Godard, Oisin Mac Aodha, Michael Firman, Gabriel Brostow
- **Comment**: ICCV 19
- **Journal**: None
- **Summary**: Per-pixel ground-truth depth data is challenging to acquire at scale. To overcome this limitation, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation. In this paper, we propose a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing self-supervised methods.   Research on self-supervised monocular training usually explores increasingly complex architectures, loss functions, and image formation models, all of which have recently helped to close the gap with fully-supervised methods. We show that a surprisingly simple model, and associated design choices, lead to superior predictions. In particular, we propose (i) a minimum reprojection loss, designed to robustly handle occlusions, (ii) a full-resolution multi-scale sampling method that reduces visual artifacts, and (iii) an auto-masking loss to ignore training pixels that violate camera motion assumptions. We demonstrate the effectiveness of each component in isolation, and show high quality, state-of-the-art results on the KITTI benchmark.



### Infrared Safety of a Neural-Net Top Tagging Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1806.01263v2
- **DOI**: 10.1007/JHEP02(2019)132
- **Categories**: **hep-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.01263v2)
- **Published**: 2018-06-04 17:59:51+00:00
- **Updated**: 2019-02-16 16:43:37+00:00
- **Authors**: Suyong Choi, Seung J. Lee, Maxim Perelstein
- **Comment**: 7 pages, 8 figures, final version to be published in JHEP
- **Journal**: None
- **Summary**: Neural network-based algorithms provide a promising approach to jet classification problems, such as boosted top jet tagging. To date, NN-based top taggers demonstrated excellent performance in Monte Carlo studies. In this paper, we construct a top-jet tagger based on a Convolutional Neural Network (CNN), and apply it to parton-level boosted top samples, with and without an additional gluon in the final state. We show that the jet observable defined by the CNN obeys the canonical definition of infrared safety: it is unaffected by the presence of the extra gluon, as long as it is soft or collinear with one of the quarks. Our results indicate that the CNN tagger is robust with respect to possible mis-modeling of soft and collinear final-state radiation by Monte Carlo generators.



### End to End Brain Fiber Orientation Estimation using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.03969v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03969v1)
- **Published**: 2018-06-04 18:07:25+00:00
- **Updated**: 2018-06-04 18:07:25+00:00
- **Authors**: Nandakishore Puttashamachar, Ulas Bagci
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we explore the various Brain Neuron tracking techniques, which is one of the most significant applications of Diffusion Tensor Imaging. Tractography provides us with a non-invasive method to analyze underlying tissue micro-structure. Understanding the structure and organization of the tissues facilitates us with a diagnosis method to identify any aberrations and provide acute information on the occurrences of brain ischemia or stroke, the mutation of neurological diseases such as Alzheimer, multiple sclerosis and so on. Time if of essence and accurate localization of the aberrations can help save or change a diseased life. Following up with the limitations introduced by the current Tractography techniques such as computational complexity, reconstruction errors during tensor estimation and standardization, we aim to elucidate these limitations through our research findings. We introduce an end to end Deep Learning framework which can accurately estimate the most probable likelihood orientation at each voxel along a neuronal pathway. We use Probabilistic Tractography as our baseline model to obtain the training data and which also serve as a Tractography Gold Standard for our evaluations. Through experiments we show that our Deep Network can do a significant improvement over current Tractography implementations by reducing the run-time complexity to a significant new level. Our architecture also allows for variable sized input DWI signals eliminating the need to worry about memory issues as seen with the traditional techniques. The advantage of this architecture is that it is perfectly desirable to be processed on a cloud setup and utilize the existing multi GPU frameworks to perform whole brain Tractography in minutes rather than hours. We evaluate our network with Gold Standard and benchmark its performance across several parameters.



### Y-Net: Joint Segmentation and Classification for Diagnosis of Breast Biopsy Images
- **Arxiv ID**: http://arxiv.org/abs/1806.01313v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01313v1)
- **Published**: 2018-06-04 18:28:37+00:00
- **Updated**: 2018-06-04 18:28:37+00:00
- **Authors**: Sachin Mehta, Ezgi Mercan, Jamen Bartlett, Donald Weave, Joann G. Elmore, Linda Shapiro
- **Comment**: Accepted for publication at MICCAI'18
- **Journal**: None
- **Summary**: In this paper, we introduce a conceptually simple network for generating discriminative tissue-level segmentation masks for the purpose of breast cancer diagnosis. Our method efficiently segments different types of tissues in breast biopsy images while simultaneously predicting a discriminative map for identifying important areas in an image. Our network, Y-Net, extends and generalizes U-Net by adding a parallel branch for discriminative map generation and by supporting convolutional block modularity, which allows the user to adjust network efficiency without altering the network topology. Y-Net delivers state-of-the-art segmentation accuracy while learning 6.6x fewer parameters than its closest competitors. The addition of descriptive power from Y-Net's discriminative segmentation masks improve diagnostic classification accuracy by 7% over state-of-the-art methods for diagnostic classification. Source code is available at: https://sacmehta.github.io/YNet.



### Cube Padding for Weakly-Supervised Saliency Prediction in 360° Videos
- **Arxiv ID**: http://arxiv.org/abs/1806.01320v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01320v1)
- **Published**: 2018-06-04 18:42:37+00:00
- **Updated**: 2018-06-04 18:42:37+00:00
- **Authors**: Hsien-Tzu Cheng, Chun-Hung Chao, Jin-Dong Dong, Hao-Kai Wen, Tyng-Luh Liu, Min Sun
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Automatic saliency prediction in 360{\deg} videos is critical for viewpoint guidance applications (e.g., Facebook 360 Guide). We propose a spatial-temporal network which is (1) weakly-supervised trained and (2) tailor-made for 360{\deg} viewing sphere. Note that most existing methods are less scalable since they rely on annotated saliency map for training. Most importantly, they convert 360{\deg} sphere to 2D images (e.g., a single equirectangular image or multiple separate Normal Field-of-View (NFoV) images) which introduces distortion and image boundaries. In contrast, we propose a simple and effective Cube Padding (CP) technique as follows. Firstly, we render the 360{\deg} view on six faces of a cube using perspective projection. Thus, it introduces very little distortion. Then, we concatenate all six faces while utilizing the connectivity between faces on the cube for image padding (i.e., Cube Padding) in convolution, pooling, convolutional LSTM layers. In this way, CP introduces no image boundary while being applicable to almost all Convolutional Neural Network (CNN) structures. To evaluate our method, we propose Wild-360, a new 360{\deg} video saliency dataset, containing challenging videos with saliency heatmap annotations. In experiments, our method outperforms baseline methods in both speed and quality.



### Computing the Spatial Probability of Inclusion inside Partial Contours for Computer Vision Applications
- **Arxiv ID**: http://arxiv.org/abs/1806.01339v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1806.01339v2)
- **Published**: 2018-06-04 19:26:51+00:00
- **Updated**: 2019-08-18 14:49:02+00:00
- **Authors**: Dominique Beaini, Sofiane Achiche, Fabrice Nonez, Maxime Raison
- **Comment**: Keywords: Computer vision; Stroke analysis; Partial contour;
  Probability of inclusion; Edge interaction; Image convolution;
  Electromagnetic potential field
- **Journal**: None
- **Summary**: In Computer Vision, edge detection is one of the favored approaches for feature and object detection in images since it provides information about their objects boundaries. Other region-based approaches use probabilistic analysis such as clustering and Markov random fields, but those methods cannot be used to analyze edges and their interaction. In fact, only image segmentation can produce regions based on edges, but it requires thresholding by simply separating the regions into binary in-out information. Hence, there is currently a gap between edge-based and region-based algorithms, since edges cannot be used to study the properties of a region and vice versa. The objective of this paper is to present a novel spatial probability analysis that allows determining the probability of inclusion inside a set of partial contours (strokes). To answer this objective, we developed a new approach that uses electromagnetic convolutions and repulsion optimization to compute the required probabilities. Hence, it becomes possible to generate a continuous space of probability based only on the edge information, thus bridging the gap between the edge-based methods and the region-based methods. The developed method is consistent with the fundamental properties of inclusion probabilities and its results are validated by comparing an image with the probability-based estimation given by our algorithm. The method can also be generalized to take into consideration the intensity of the edges or to be used for 3D shapes. This is the first documented method that allows computing a space of probability based on interacting edges, which opens the path to broader applications such as image segmentation and contour completion.



### Design of optimal illumination patterns in single-pixel imaging using image dictionaries
- **Arxiv ID**: http://arxiv.org/abs/1806.01340v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.01340v2)
- **Published**: 2018-06-04 19:28:12+00:00
- **Updated**: 2020-01-17 10:32:22+00:00
- **Authors**: Jun Feng, Shuming Jiao, Yang Gao, Ting Lei, Xiaocong Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Single-pixel imaging (SPI) has a major drawback that many sequential illuminations are required for capturing one single image with long acquisition time. Basis illumination patterns such as Fourier patterns and Hadamard patterns can achieve much better imaging efficiency than random patterns. But the performance is still sub-optimal since the basis patterns are fixed and non-adaptive for varying object images. This Letter proposes a novel scheme for designing and optimizing the illumination patterns adaptively from an image dictionary by extracting the common image features using principal component analysis (PCA). Simulation and experimental results reveal that our proposed scheme outperforms conventional Fourier SPI in terms of imaging efficiency.



### gprHOG and the popularity of Histogram of Oriented Gradients (HOG) for Buried Threat Detection in Ground-Penetrating Radar
- **Arxiv ID**: http://arxiv.org/abs/1806.01349v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01349v2)
- **Published**: 2018-06-04 19:51:17+00:00
- **Updated**: 2018-10-02 15:02:08+00:00
- **Authors**: Daniel Reichman, Leslie M. Collins, Jordan M. Malof
- **Comment**: 5 pages, 6 figures, letter
- **Journal**: None
- **Summary**: Substantial research has been devoted to the development of algorithms that automate buried threat detection (BTD) with ground penetrating radar (GPR) data, resulting in a large number of proposed algorithms. One popular algorithm GPR-based BTD, originally applied by Torrione et al., 2012, is the Histogram of Oriented Gradients (HOG) feature. In a recent large-scale comparison among five veteran institutions, a modified version of HOG referred to here as "gprHOG", performed poorly compared to other modern algorithms. In this paper, we provide experimental evidence demonstrating that the modifications to HOG that comprise gprHOG result in a substantially better-performing algorithm. The results here, in conjunction with the large-scale algorithm comparison, suggest that HOG is not competitive with modern GPR-based BTD algorithms. Given HOG's popularity, these results raise some questions about many existing studies, and suggest gprHOG (and especially HOG) should be employed with caution in future studies.



### Adversarial Domain Adaptation for Classification of Prostate Histopathology Whole-Slide Images
- **Arxiv ID**: http://arxiv.org/abs/1806.01357v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01357v2)
- **Published**: 2018-06-04 20:01:09+00:00
- **Updated**: 2018-06-06 23:49:17+00:00
- **Authors**: Jian Ren, Ilker Hacihaliloglu, Eric A. Singer, David J. Foran, Xin Qi
- **Comment**: Accepted to MICCAI 2018
- **Journal**: None
- **Summary**: Automatic and accurate Gleason grading of histopathology tissue slides is crucial for prostate cancer diagnosis, treatment, and prognosis. Usually, histopathology tissue slides from different institutions show heterogeneous appearances because of different tissue preparation and staining procedures, thus the predictable model learned from one domain may not be applicable to a new domain directly. Here we propose to adopt unsupervised domain adaptation to transfer the discriminative knowledge obtained from the source domain to the target domain without requiring labeling of images at the target domain. The adaptation is achieved through adversarial training to find an invariant feature space along with the proposed Siamese architecture on the target domain to add a regularization that is appropriate for the whole-slide images. We validate the method on two prostate cancer datasets and obtain significant classification improvement of Gleason scores as compared with the baseline models.



### Factorized Adversarial Networks for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1806.01376v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.01376v1)
- **Published**: 2018-06-04 20:39:13+00:00
- **Updated**: 2018-06-04 20:39:13+00:00
- **Authors**: Jian Ren, Jianchao Yang, Ning Xu, David J. Foran
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose Factorized Adversarial Networks (FAN) to solve unsupervised domain adaptation problems for image classification tasks. Our networks map the data distribution into a latent feature space, which is factorized into a domain-specific subspace that contains domain-specific characteristics and a task-specific subspace that retains category information, for both source and target domains, respectively. Unsupervised domain adaptation is achieved by adversarial training to minimize the discrepancy between the distributions of two task-specific subspaces from source and target domains. We demonstrate that the proposed approach outperforms state-of-the-art methods on multiple benchmark datasets used in the literature for unsupervised domain adaptation. Furthermore, we collect two real-world tagging datasets that are much larger than existing benchmark datasets, and get significant improvement upon baselines, proving the practical value of our approach.



### FlowNet3D: Learning Scene Flow in 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1806.01411v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.01411v3)
- **Published**: 2018-06-04 22:07:36+00:00
- **Updated**: 2019-07-21 21:33:18+00:00
- **Authors**: Xingyu Liu, Charles R. Qi, Leonidas J. Guibas
- **Comment**: CVPR 2019. Source code available at
  http://github.com/xingyul/flownet3d
- **Journal**: None
- **Summary**: Many applications in robotics and human-computer interaction can benefit from understanding 3D motion of points in a dynamic environment, widely noted as scene flow. While most previous methods focus on stereo and RGB-D images as input, few try to estimate scene flow directly from point clouds. In this work, we propose a novel deep neural network named $FlowNet3D$ that learns scene flow from point clouds in an end-to-end fashion. Our network simultaneously learns deep hierarchical features of point clouds and flow embeddings that represent point motions, supported by two newly proposed learning layers for point sets. We evaluate the network on both challenging synthetic data from FlyingThings3D and real Lidar scans from KITTI. Trained on synthetic data only, our network successfully generalizes to real scans, outperforming various baselines and showing competitive results to the prior art. We also demonstrate two applications of our scene flow output (scan registration and motion segmentation) to show its potential wide use cases.



### CFCM: Segmentation via Coarse to Fine Context Memory
- **Arxiv ID**: http://arxiv.org/abs/1806.01413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01413v1)
- **Published**: 2018-06-04 22:12:41+00:00
- **Updated**: 2018-06-04 22:12:41+00:00
- **Authors**: Fausto Milletari, Nicola Rieke, Maximilian Baust, Marco Esposito, Nassir Navab
- **Comment**: Accepted for presentation at MICCAI 2018
- **Journal**: None
- **Summary**: Recent neural-network-based architectures for image segmentation make extensive usage of feature forwarding mechanisms to integrate information from multiple scales. Although yielding good results, even deeper architectures and alternative methods for feature fusion at different resolutions have been scarcely investigated for medical applications. In this work we propose to implement segmentation via an encoder-decoder architecture which differs from any other previously published method since (i) it employs a very deep architecture based on residual learning and (ii) combines features via a convolutional Long Short Term Memory (LSTM), instead of concatenation or summation. The intuition is that the memory mechanism implemented by LSTMs can better integrate features from different scales through a coarse-to-fine strategy; hence the name Coarse-to-Fine Context Memory (CFCM). We demonstrate the remarkable advantages of this approach on two datasets: the Montgomery county lung segmentation dataset, and the EndoVis 2015 challenge dataset for surgical instrument segmentation.



