# Arxiv Papers in cs.CV on 2018-06-15
### Deep Learning Approximation: Zero-Shot Neural Network Speedup
- **Arxiv ID**: http://arxiv.org/abs/1806.05779v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.05779v1)
- **Published**: 2018-06-15 01:25:47+00:00
- **Updated**: 2018-06-15 01:25:47+00:00
- **Authors**: Michele Pratusevich
- **Comment**: Submitted to NIPS 2018
- **Journal**: None
- **Summary**: Neural networks offer high-accuracy solutions to a range of problems, but are costly to run in production systems because of computational and memory requirements during a forward pass. Given a trained network, we propose a techique called Deep Learning Approximation to build a faster network in a tiny fraction of the time required for training by only manipulating the network structure and coefficients without requiring re-training or access to the training data. Speedup is achieved by by applying a sequential series of independent optimizations that reduce the floating-point operations (FLOPs) required to perform a forward pass. First, lossless optimizations are applied, followed by lossy approximations using singular value decomposition (SVD) and low-rank matrix decomposition. The optimal approximation is chosen by weighing the relative accuracy loss and FLOP reduction according to a single parameter specified by the user. On PASCAL VOC 2007 with the YOLO network, we show an end-to-end 2x speedup in a network forward pass with a 5% drop in mAP that can be re-gained by finetuning.



### A Survey of Automatic Facial Micro-expression Analysis: Databases, Methods and Challenges
- **Arxiv ID**: http://arxiv.org/abs/1806.05781v1
- **DOI**: 10.3389/fpsyg.2018.01128
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1806.05781v1)
- **Published**: 2018-06-15 01:42:33+00:00
- **Updated**: 2018-06-15 01:42:33+00:00
- **Authors**: Yee-Hui Oh, John See, Anh Cat Le Ngo, Raphael Chung-Wei Phan, Vishnu Monn Baskaran
- **Comment**: 45 pages, single column preprint version. Submitted: 2 December 2017,
  Accepted: 12 June 2018 to Frontiers in Psychology
- **Journal**: None
- **Summary**: Over the last few years, automatic facial micro-expression analysis has garnered increasing attention from experts across different disciplines because of its potential applications in various fields such as clinical diagnosis, forensic investigation and security systems. Advances in computer algorithms and video acquisition technology have rendered machine analysis of facial micro-expressions possible today, in contrast to decades ago when it was primarily the domain of psychiatrists where analysis was largely manual. Indeed, although the study of facial micro-expressions is a well-established field in psychology, it is still relatively new from the computational perspective with many interesting problems. In this survey, we present a comprehensive review of state-of-the-art databases and methods for micro-expressions spotting and recognition. Individual stages involved in the automation of these tasks are also described and reviewed at length. In addition, we also deliberate on the challenges and future directions in this growing field of automatic facial micro-expression analysis.



### Image classification and retrieval with random depthwise signed convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1806.05789v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.05789v3)
- **Published**: 2018-06-15 02:26:11+00:00
- **Updated**: 2019-03-15 21:20:48+00:00
- **Authors**: Yunzhe Xue, Usman Roshan
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a random convolutional neural network to generate a feature space in which we study image classification and retrieval performance. Put briefly we apply random convolutional blocks followed by global average pooling to generate a new feature, and we repeat this k times to produce a k-dimensional feature space. This can be interpreted as partitioning the space of image patches with random hyperplanes which we formalize as a random depthwise convolutional neural network. In the network's final layer we perform image classification and retrieval with the linear support vector machine and k-nearest neighbor classifiers and study other empirical properties. We show that the ratio of image pixel distribution similarity across classes to within classes is higher in our network's final layer compared to the input space. When we apply the linear support vector machine for image classification we see that the accuracy is higher than if we were to train just the final layer of VGG16, ResNet18, and DenseNet40 with random weights. In the same setting we compare it to an unsupervised feature learning method and find our accuracy to be comparable on CIFAR10 but higher on CIFAR100 and STL10. We see that the accuracy is not far behind that of trained networks, particularly in the top-k setting. For example the top-2 accuracy of our network is near 90% on both CIFAR10 and a 10-class mini ImageNet, and 85% on STL10. We find that k-nearest neighbor gives a comparable precision on the Corel Princeton Image Similarity Benchmark than if we were to use the final layer of trained networks. As with other networks we find that our network fails to a black box attack even though we lack a gradient and use the sign activation. We highlight sensitivity of our network to background as a potential pitfall and an advantage. Overall our work pushes the boundary of what can be achieved with random weights.



### Recurrent Multiresolution Convolutional Networks for VHR Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1806.05793v1
- **DOI**: 10.1109/TGRS.2018.2837357
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05793v1)
- **Published**: 2018-06-15 03:01:43+00:00
- **Updated**: 2018-06-15 03:01:43+00:00
- **Authors**: John Ray Bergado, Claudio Persello, Alfred Stein
- **Comment**: None
- **Journal**: None
- **Summary**: Classification of very high resolution (VHR) satellite images has three major challenges: 1) inherent low intra-class and high inter-class spectral similarities, 2) mismatching resolution of available bands, and 3) the need to regularize noisy classification maps. Conventional methods have addressed these challenges by adopting separate stages of image fusion, feature extraction, and post-classification map regularization. These processing stages, however, are not jointly optimizing the classification task at hand. In this study, we propose a single-stage framework embedding the processing stages in a recurrent multiresolution convolutional network trained in an end-to-end manner. The feedforward version of the network, called FuseNet, aims to match the resolution of the panchromatic and multispectral bands in a VHR image using convolutional layers with corresponding downsampling and upsampling operations. Contextual label information is incorporated into FuseNet by means of a recurrent version called ReuseNet. We compared FuseNet and ReuseNet against the use of separate processing steps for both image fusion, e.g. pansharpening and resampling through interpolation, and map regularization such as conditional random fields. We carried out our experiments on a land cover classification task using a Worldview-03 image of Quezon City, Philippines and the ISPRS 2D semantic labeling benchmark dataset of Vaihingen, Germany. FuseNet and ReuseNet surpass the baseline approaches in both quantitative and qualitative results.



### Deep Learning with Convolutional Neural Network for Objective Skill Evaluation in Robot-assisted Surgery
- **Arxiv ID**: http://arxiv.org/abs/1806.05796v2
- **DOI**: 10.1007/s11548-018-1860-1
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1806.05796v2)
- **Published**: 2018-06-15 03:22:06+00:00
- **Updated**: 2019-03-07 06:25:35+00:00
- **Authors**: Ziheng Wang, Ann Majewicz Fey
- **Comment**: Manuscript published. For reference, see
  https://link.springer.com/article/10.1007/s11548-018-1860-1
- **Journal**: 2018 International Journal of Computer Assisted Radiology and
  Surgery
- **Summary**: With the advent of robot-assisted surgery, the role of data-driven approaches to integrate statistics and machine learning is growing rapidly with prominent interests in objective surgical skill assessment. However, most existing work requires translating robot motion kinematics into intermediate features or gesture segments that are expensive to extract, lack efficiency, and require significant domain-specific knowledge. We propose an analytical deep learning framework for skill assessment in surgical training. A deep convolutional neural network is implemented to map multivariate time series data of the motion kinematics to individual skill levels. We perform experiments on the public minimally invasive surgical robotic dataset, JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS). Our proposed learning model achieved a competitive accuracy of 92.5%, 95.4%, and 91.3%, in the standard training tasks: Suturing, Needle-passing, and Knot-tying, respectively. Without the need of engineered features or carefully-tuned gesture segmentation, our model can successfully decode skill information from raw motion profiles via end-to-end learning. Meanwhile, the proposed model is able to reliably interpret skills within 1-3 second window, without needing an observation of entire training trial. This study highlights the potentials of deep architectures for an proficient online skill assessment in modern surgical training.



### SATR-DL: Improving Surgical Skill Assessment and Task Recognition in Robot-assisted Surgery with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.05798v1
- **DOI**: 10.1109/EMBC.2018.8512575
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1806.05798v1)
- **Published**: 2018-06-15 03:31:23+00:00
- **Updated**: 2018-06-15 03:31:23+00:00
- **Authors**: Ziheng Wang, Ann Majewicz Fey
- **Comment**: None
- **Journal**: 2018 40th Annual International Conference of the IEEE Engineering
  in Medicine and Biology Society (EMBC)
- **Summary**: Purpose: This paper focuses on an automated analysis of surgical motion profiles for objective skill assessment and task recognition in robot-assisted surgery. Existing techniques heavily rely on conventional statistic measures or shallow modelings based on hand-engineered features and gesture segmentation. Such developments require significant expert knowledge, are prone to errors, and are less efficient in online adaptive training systems. Methods: In this work, we present an efficient analytic framework with a parallel deep learning architecture, SATR-DL, to assess trainee expertise and recognize surgical training activity. Through an end-to-end learning technique, abstract information of spatial representations and temporal dynamics is jointly obtained directly from raw motion sequences. Results: By leveraging a shared high-level representation learning, the resulting model is successful in the recognition of trainee skills and surgical tasks, suturing, needle-passing, and knot-tying. Meanwhile, we explore the use of ensemble in classification at the trial level, where the SATR-DL outperforms state-of-the-art performance by achieving accuracies of 0.960 and 1.000 in skill assessment and task recognition, respectively. Conclusion: This study highlights the potential of SATR-DL to provide improvements for an efficient data-driven assessment in intelligent robotic surgery.



### Weakly Supervised Deep Image Hashing through Tag Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1806.05804v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05804v3)
- **Published**: 2018-06-15 05:24:30+00:00
- **Updated**: 2019-01-28 02:53:56+00:00
- **Authors**: Vijetha Gattupalli, Yaoxin Zhuo, Baoxin Li
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Many approaches to semantic image hashing have been formulated as supervised learning problems that utilize images and label information to learn the binary hash codes. However, large-scale labeled image data is expensive to obtain, thus imposing a restriction on the usage of such algorithms. On the other hand, unlabelled image data is abundant due to the existence of many Web image repositories. Such Web images may often come with images tags that contain useful information, although raw tags, in general, do not readily lead to semantic labels. Motivated by this scenario, we formulate the problem of semantic image hashing as a weakly-supervised learning problem. We utilize the information contained in the user-generated tags associated with the images to learn the hash codes. More specifically, we extract the word2vec semantic embeddings of the tags and use the information contained in them for constraining the learning. Accordingly, we name our model Weakly Supervised Deep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of semantic image retrieval and is compared against several state-of-art models. Results show that our approach sets a new state-of-art in the area of weekly supervised image hashing.



### Best sources forward: domain generalization through source-specific nets
- **Arxiv ID**: http://arxiv.org/abs/1806.05810v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.05810v1)
- **Published**: 2018-06-15 05:42:20+00:00
- **Updated**: 2018-06-15 05:42:20+00:00
- **Authors**: Massimiliano Mancini, Samuel Rota Bulò, Barbara Caputo, Elisa Ricci
- **Comment**: None
- **Journal**: None
- **Summary**: A long standing problem in visual object categorization is the ability of algorithms to generalize across different testing conditions. The problem has been formalized as a covariate shift among the probability distributions generating the training data (source) and the test data (target) and several domain adaptation methods have been proposed to address this issue. While these approaches have considered the single source-single target scenario, it is plausible to have multiple sources and require adaptation to any possible target domain. This last scenario, named Domain Generalization (DG), is the focus of our work. Differently from previous DG methods which learn domain invariant representations from source data, we design a deep network with multiple domain-specific classifiers, each associated to a source domain. At test time we estimate the probabilities that a target sample belongs to each source domain and exploit them to optimally fuse the classifiers predictions. To further improve the generalization ability of our model, we also introduced a domain agnostic component supporting the final classifier. Experiments on two public benchmarks demonstrate the power of our approach.



### Three dimensional Deep Learning approach for remote sensing image classification
- **Arxiv ID**: http://arxiv.org/abs/1806.05824v1
- **DOI**: 10.1109/TGRS.2018.2818945
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.05824v1)
- **Published**: 2018-06-15 06:35:47+00:00
- **Updated**: 2018-06-15 06:35:47+00:00
- **Authors**: Amina Ben Hamida, A Benoit, Patrick Lambert, Chokri Ben Amar
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, Institute of
  Electrical and Electronics Engineers, 2018, pp.1 - 15
- **Summary**: Recently, a variety of approaches has been enriching the field of Remote Sensing (RS) image processing and analysis. Unfortunately, existing methods remain limited faced to the rich spatio-spectral content of today's large datasets. It would seem intriguing to resort to Deep Learning (DL) based approaches at this stage with regards to their ability to offer accurate semantic interpretation of the data. However, the specificity introduced by the coexistence of spectral and spatial content in the RS datasets widens the scope of the challenges presented to adapt DL methods to these contexts. Therefore, the aim of this paper is firstly to explore the performance of DL architectures for the RS hyperspectral dataset classification and secondly to introduce a new three-dimensional DL approach that enables a joint spectral and spatial information process. A set of three-dimensional schemes is proposed and evaluated. Experimental results based on well knownhyperspectral datasets demonstrate that the proposed method is able to achieve a better classification rate than state of the art methods with lower computational costs.



### Real-time Monocular Visual Odometry for Turbid and Dynamic Underwater Environments
- **Arxiv ID**: http://arxiv.org/abs/1806.05842v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.05842v3)
- **Published**: 2018-06-15 07:47:00+00:00
- **Updated**: 2020-02-27 09:15:04+00:00
- **Authors**: Maxime Ferrera, Julien Moras, Pauline Trouvé-Peloux, Vincent Creuze
- **Comment**: None
- **Journal**: Sensors, MDPI, 2019
- **Summary**: In the context of robotic underwater operations, the visual degradations induced by the medium properties make difficult the exclusive use of cameras for localization purpose. Hence, most localization methods are based on expensive navigational sensors associated with acoustic positioning. On the other hand, visual odometry and visual SLAM have been exhaustively studied for aerial or terrestrial applications, but state-of-the-art algorithms fail underwater. In this paper we tackle the problem of using a simple low-cost camera for underwater localization and propose a new monocular visual odometry method dedicated to the underwater environment. We evaluate different tracking methods and show that optical flow based tracking is more suited to underwater images than classical approaches based on descriptors. We also propose a keyframe-based visual odometry approach highly relying on nonlinear optimization. The proposed algorithm has been assessed on both simulated and real underwater datasets and outperforms state-of-the-art visual SLAM methods under many of the most challenging conditions. The main application of this work is the localization of Remotely Operated Vehicles (ROVs) used for underwater archaeological missions but the developed system can be used in any other applications as long as visual information is available.



### A simple blind-denoising filter inspired by electrically coupled photoreceptors in the retina
- **Arxiv ID**: http://arxiv.org/abs/1806.05882v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1806.05882v4)
- **Published**: 2018-06-15 10:08:59+00:00
- **Updated**: 2018-08-27 10:04:13+00:00
- **Authors**: Yang Yue, Liuyuan He, Gan He, Jian. K. Liu, Kai Du, Yonghong Tian, Tiejun Huang
- **Comment**: 16 pages, 8 figures, 9 tables, Submitted to NIPS 2018
- **Journal**: None
- **Summary**: Photoreceptors in the retina are coupled by electrical synapses called "gap junctions". It has long been established that gap junctions increase the signal-to-noise ratio of photoreceptors. Inspired by electrically coupled photoreceptors, we introduced a simple filter, the PR-filter, with only one variable. On BSD68 dataset, PR-filter showed outstanding performance in SSIM during blind denoising tasks. It also significantly improved the performance of state-of-the-art convolutional neural network blind denosing on non-Gaussian noise. The performance of keeping more details might be attributed to small receptive field of the photoreceptors.



### Automated Image Data Preprocessing with Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.05886v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05886v2)
- **Published**: 2018-06-15 10:15:10+00:00
- **Updated**: 2021-04-29 17:42:02+00:00
- **Authors**: Tran Ngoc Minh, Mathieu Sinn, Hoang Thanh Lam, Martin Wistuba
- **Comment**: None
- **Journal**: None
- **Summary**: Data preparation, i.e. the process of transforming raw data into a format that can be used for training effective machine learning models, is a tedious and time-consuming task. For image data, preprocessing typically involves a sequence of basic transformations such as cropping, filtering, rotating or flipping images. Currently, data scientists decide manually based on their experience which transformations to apply in which particular order to a given image data set. Besides constituting a bottleneck in real-world data science projects, manual image data preprocessing may yield suboptimal results as data scientists need to rely on intuition or trial-and-error approaches when exploring the space of possible image transformations and thus might not be able to discover the most effective ones. To mitigate the inefficiency and potential ineffectiveness of manual data preprocessing, this paper proposes a deep reinforcement learning framework to automatically discover the optimal data preprocessing steps for training an image classifier. The framework takes as input sets of labeled images and predefined preprocessing transformations. It jointly learns the classifier and the optimal preprocessing transformations for individual images. Experimental results show that the proposed approach not only improves the accuracy of image classifiers, but also makes them substantially more robust to noisy inputs at test time.



### Learning Front-end Filter-bank Parameters using Convolutional Neural Networks for Abnormal Heart Sound Detection
- **Arxiv ID**: http://arxiv.org/abs/1806.05892v1
- **DOI**: 10.1109/EMBC.2018.8512578
- **Categories**: **cs.CV**, cs.LG, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.05892v1)
- **Published**: 2018-06-15 10:33:31+00:00
- **Updated**: 2018-06-15 10:33:31+00:00
- **Authors**: Ahmed Imtiaz Humayun, Shabnam Ghaffarzadegan, Zhe Feng, Taufiq Hasan
- **Comment**: 4 pages, 6 figures, IEEE International Engineering in Medicine and
  Biology Conference (EMBC)
- **Journal**: None
- **Summary**: Automatic heart sound abnormality detection can play a vital role in the early diagnosis of heart diseases, particularly in low-resource settings. The state-of-the-art algorithms for this task utilize a set of Finite Impulse Response (FIR) band-pass filters as a front-end followed by a Convolutional Neural Network (CNN) model. In this work, we propound a novel CNN architecture that integrates the front-end bandpass filters within the network using time-convolution (tConv) layers, which enables the FIR filter-bank parameters to become learnable. Different initialization strategies for the learnable filters, including random parameters and a set of predefined FIR filter-bank coefficients, are examined. Using the proposed tConv layers, we add constraints to the learnable FIR filters to ensure linear and zero phase responses. Experimental evaluations are performed on a balanced 4-fold cross-validation task prepared using the PhysioNet/CinC 2016 dataset. Results demonstrate that the proposed models yield superior performance compared to the state-of-the-art system, while the linear phase FIR filterbank method provides an absolute improvement of 9.54% over the baseline in terms of an overall accuracy metric.



### Efficient Nearest Neighbors Search for Large-Scale Landmark Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.05946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05946v1)
- **Published**: 2018-06-15 13:18:59+00:00
- **Updated**: 2018-06-15 13:18:59+00:00
- **Authors**: Federico Magliani, Tomaso Fontanini, Andrea Prati
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of landmark recognition has achieved excellent results in small-scale datasets. When dealing with large-scale retrieval, issues that were irrelevant with small amount of data, quickly become fundamental for an efficient retrieval phase. In particular, computational time needs to be kept as low as possible, whilst the retrieval accuracy has to be preserved as much as possible. In this paper we propose a novel multi-index hashing method called Bag of Indexes (BoI) for Approximate Nearest Neighbors (ANN) search. It allows to drastically reduce the query time and outperforms the accuracy results compared to the state-of-the-art methods for large-scale landmark recognition. It has been demonstrated that this family of algorithms can be applied on different embedding techniques like VLAD and R-MAC obtaining excellent results in very short times on different public datasets: Holidays+Flickr1M, Oxford105k and Paris106k.



### Uncertainty Estimations by Softplus normalization in Bayesian Convolutional Neural Networks with Variational Inference
- **Arxiv ID**: http://arxiv.org/abs/1806.05978v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.05978v6)
- **Published**: 2018-06-15 13:55:18+00:00
- **Updated**: 2019-05-14 09:04:11+00:00
- **Authors**: Kumar Shridhar, Felix Laumann, Marcus Liwicki
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel uncertainty estimation for classification tasks for Bayesian convolutional neural networks with variational inference. By normalizing the output of a Softplus function in the final layer, we estimate aleatoric and epistemic uncertainty in a coherent manner. The intractable posterior probability distributions over weights are inferred by Bayes by Backprop. Firstly, we demonstrate how this reliable variational inference method can serve as a fundamental construct for various network architectures. On multiple datasets in supervised learning settings (MNIST, CIFAR-10, CIFAR-100), this variational inference method achieves performances equivalent to frequentist inference in identical architectures, while the two desiderata, a measure for uncertainty and regularization are incorporated naturally. Secondly, we examine how our proposed measure for aleatoric and epistemic uncertainties is derived and validate it on the aforementioned datasets.



### Ego-Lane Analysis System (ELAS): Dataset and Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1806.05984v1
- **DOI**: 10.1016/J.IMAVIS.2017.07.005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05984v1)
- **Published**: 2018-06-15 14:02:44+00:00
- **Updated**: 2018-06-15 14:02:44+00:00
- **Authors**: Rodrigo F. Berriel, Edilson de Aguiar, Alberto F. de Souza, Thiago Oliveira-Santos
- **Comment**: 13 pages, 17 figures,
  github.com/rodrigoberriel/ego-lane-analysis-system, and published by Image
  and Vision Computing (IMAVIS)
- **Journal**: Image and Vision Computing 68 (2017) 64-75
- **Summary**: Decreasing costs of vision sensors and advances in embedded hardware boosted lane related research detection, estimation, and tracking in the past two decades. The interest in this topic has increased even more with the demand for advanced driver assistance systems (ADAS) and self-driving cars. Although extensively studied independently, there is still need for studies that propose a combined solution for the multiple problems related to the ego-lane, such as lane departure warning (LDW), lane change detection, lane marking type (LMT) classification, road markings detection and classification, and detection of adjacent lanes (i.e., immediate left and right lanes) presence. In this paper, we propose a real-time Ego-Lane Analysis System (ELAS) capable of estimating ego-lane position, classifying LMTs and road markings, performing LDW and detecting lane change events. The proposed vision-based system works on a temporal sequence of images. Lane marking features are extracted in perspective and Inverse Perspective Mapping (IPM) images that are combined to increase robustness. The final estimated lane is modeled as a spline using a combination of methods (Hough lines with Kalman filter and spline with particle filter). Based on the estimated lane, all other events are detected. To validate ELAS and cover the lack of lane datasets in the literature, a new dataset with more than 20 different scenes (in more than 15,000 frames) and considering a variety of scenarios (urban road, highways, traffic, shadows, etc.) was created. The dataset was manually annotated and made publicly available to enable evaluation of several events that are of interest for the research community (i.e., lane estimation, change, and centering; road markings; intersections; LMTs; crosswalks and adjacent lanes). ELAS achieved high detection rates in all real-world events and proved to be ready for real-time applications.



### Partially-Supervised Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1806.06004v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06004v2)
- **Published**: 2018-06-15 14:52:40+00:00
- **Updated**: 2018-11-28 15:29:42+00:00
- **Authors**: Peter Anderson, Stephen Gould, Mark Johnson
- **Comment**: NeurIPS 2018
- **Journal**: None
- **Summary**: Image captioning models are becoming increasingly successful at describing the content of images in restricted domains. However, if these models are to function in the wild - for example, as assistants for people with impaired vision - a much larger number and variety of visual concepts must be understood. To address this problem, we teach image captioning models new visual concepts from labeled images and object detection datasets. Since image labels and object classes can be interpreted as partial captions, we formulate this problem as learning from partially-specified sequence data. We then propose a novel algorithm for training sequence models, such as recurrent neural networks, on partially-specified sequences which we represent using finite state automata. In the context of image captioning, our method lifts the restriction that previously required image captioning models to be trained on paired image-sentence corpora only, or otherwise required specialized model architectures to take advantage of alternative data modalities. Applying our approach to an existing neural captioning model, we achieve state of the art results on the novel object captioning task using the COCO dataset. We further show that we can train a captioning model to describe new visual concepts from the Open Images dataset while maintaining competitive COCO evaluation scores.



### One-Shot Unsupervised Cross Domain Translation
- **Arxiv ID**: http://arxiv.org/abs/1806.06029v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06029v2)
- **Published**: 2018-06-15 16:03:36+00:00
- **Updated**: 2018-10-23 09:25:09+00:00
- **Authors**: Sagie Benaim, Lior Wolf
- **Comment**: Published at NIPS 2018
- **Journal**: None
- **Summary**: Given a single image x from domain A and a set of images from domain B, our task is to generate the analogous of x in B. We argue that this task could be a key AI capability that underlines the ability of cognitive agents to act in the world and present empirical evidence that the existing unsupervised domain translation methods fail on this task. Our method follows a two step process. First, a variational autoencoder for domain B is trained. Then, given the new sample x, we create a variational autoencoder for domain A by adapting the layers that are close to the image in order to directly fit x, and only indirectly adapt the other layers. Our experiments indicate that the new method does as well, when trained on one sample x, as the existing domain transfer methods, when these enjoy a multitude of training samples from domain A. Our code is made publicly available at https://github.com/sagiebenaim/OneShotTranslation



### The Toybox Dataset of Egocentric Visual Object Transformations
- **Arxiv ID**: http://arxiv.org/abs/1806.06034v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06034v3)
- **Published**: 2018-06-15 16:17:02+00:00
- **Updated**: 2018-11-26 21:37:42+00:00
- **Authors**: Xiaohan Wang, Tengyu Ma, James Ainooson, Seunghwan Cha, Xiaotian Wang, Azhar Molla, Maithilee Kunda
- **Comment**: None
- **Journal**: None
- **Summary**: In object recognition research, many commonly used datasets (e.g., ImageNet and similar) contain relatively sparse distributions of object instances and views, e.g., one might see a thousand different pictures of a thousand different giraffes, mostly taken from a few conventionally photographed angles. These distributional properties constrain the types of computational experiments that are able to be conducted with such datasets, and also do not reflect naturalistic patterns of embodied visual experience. As a contribution to the small (but growing) number of multi-view object datasets that have been created to bridge this gap, we introduce a new video dataset called Toybox that contains egocentric (i.e., first-person perspective) videos of common household objects and toys being manually manipulated to undergo structured transformations, such as rotation, translation, and zooming. To illustrate potential uses of Toybox, we also present initial neural network experiments that examine 1) how training on different distributions of object instances and views affects recognition performance, and 2) how viewpoint-dependent object concepts are represented within the hidden layers of a trained network.



### Deep Lip Reading: a comparison of models and an online application
- **Arxiv ID**: http://arxiv.org/abs/1806.06053v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06053v1)
- **Published**: 2018-06-15 17:37:01+00:00
- **Updated**: 2018-06-15 17:37:01+00:00
- **Authors**: Triantafyllos Afouras, Joon Son Chung, Andrew Zisserman
- **Comment**: To appear in Interspeech 2018
- **Journal**: None
- **Summary**: The goal of this paper is to develop state-of-the-art models for lip reading -- visual speech recognition. We develop three architectures and compare their accuracy and training times: (i) a recurrent model using LSTMs; (ii) a fully convolutional model; and (iii) the recently proposed transformer model. The recurrent and fully convolutional models are trained with a Connectionist Temporal Classification loss and use an explicit language model for decoding, the transformer is a sequence-to-sequence model. Our best performing model improves the state-of-the-art word error rate on the challenging BBC-Oxford Lip Reading Sentences 2 (LRS2) benchmark dataset by over 20 percent.   As a further contribution we investigate the fully convolutional model when used for online (real time) lip reading of continuous speech, and show that it achieves high performance with low latency.



### Unsupervised Training for 3D Morphable Model Regression
- **Arxiv ID**: http://arxiv.org/abs/1806.06098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.06098v1)
- **Published**: 2018-06-15 19:31:20+00:00
- **Updated**: 2018-06-15 19:31:20+00:00
- **Authors**: Kyle Genova, Forrester Cole, Aaron Maschinot, Aaron Sarna, Daniel Vlasic, William T. Freeman
- **Comment**: CVPR 2018 version with supplemental material
  (http://openaccess.thecvf.com/content_cvpr_2018/html/Genova_Unsupervised_Training_for_CVPR_2018_paper.html)
- **Journal**: Conference on Computer Vision and Pattern Recognition (CVPR),
  2018, pp. 8377-8386
- **Summary**: We present a method for training a regression network from image pixels to 3D morphable model coordinates using only unlabeled photographs. The training loss is based on features from a facial recognition network, computed on-the-fly by rendering the predicted faces with a differentiable renderer. To make training from features feasible and avoid network fooling effects, we introduce three objectives: a batch distribution loss that encourages the output distribution to match the distribution of the morphable model, a loopback loss that ensures the network can correctly reinterpret its own output, and a multi-view identity loss that compares the features of the predicted 3D face and the input photograph from multiple viewing angles. We train a regression network using these objectives, a set of unlabeled photographs, and the morphable model itself, and demonstrate state-of-the-art results.



