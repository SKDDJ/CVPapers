# Arxiv Papers in cs.CV on 2018-06-21
### Gradient Adversarial Training of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.08028v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.08028v1)
- **Published**: 2018-06-21 00:54:07+00:00
- **Updated**: 2018-06-21 00:54:07+00:00
- **Authors**: Ayan Sinha, Zhao Chen, Vijay Badrinarayanan, Andrew Rabinovich
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: We propose gradient adversarial training, an auxiliary deep learning framework applicable to different machine learning problems. In gradient adversarial training, we leverage a prior belief that in many contexts, simultaneous gradient updates should be statistically indistinguishable from each other. We enforce this consistency using an auxiliary network that classifies the origin of the gradient tensor, and the main network serves as an adversary to the auxiliary network in addition to performing standard task-based training. We demonstrate gradient adversarial training for three different scenarios: (1) as a defense to adversarial examples we classify gradient tensors and tune them to be agnostic to the class of their corresponding example, (2) for knowledge distillation, we do binary classification of gradient tensors derived from the student or teacher network and tune the student gradient tensor to mimic the teacher's gradient tensor; and (3) for multi-task learning we classify the gradient tensors derived from different task loss functions and tune them to be statistically indistinguishable. For each of the three scenarios we show the potential of gradient adversarial training procedure. Specifically, gradient adversarial training increases the robustness of a network to adversarial attacks, is able to better distill the knowledge from a teacher network to a student network compared to soft targets, and boosts multi-task learning by aligning the gradient tensors derived from the task specific loss functions. Overall, our experiments demonstrate that gradient tensors contain latent information about whatever tasks are being trained, and can support diverse machine learning problems when intelligently guided through adversarialization using a auxiliary network.



### Pixel-level Reconstruction and Classification for Noisy Handwritten Bangla Characters
- **Arxiv ID**: http://arxiv.org/abs/1806.08037v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.08037v1)
- **Published**: 2018-06-21 01:30:30+00:00
- **Updated**: 2018-06-21 01:30:30+00:00
- **Authors**: Manohar Karki, Qun Liu, Robert DiBiano, Saikat Basu, Supratik Mukhopadhyay
- **Comment**: Paper was accepted at the 16th International Conference on Frontiers
  in Handwriting Recognition (ICFHR 2018)
- **Journal**: None
- **Summary**: Classification techniques for images of handwritten characters are susceptible to noise. Quadtrees can be an efficient representation for learning from sparse features. In this paper, we improve the effectiveness of probabilistic quadtrees by using a pixel level classifier to extract the character pixels and remove noise from handwritten character images. The pixel level denoiser (a deep belief network) uses the map responses obtained from a pretrained CNN as features for reconstructing the characters eliminating noise. We experimentally demonstrate the effectiveness of our approach by reconstructing and classifying a noisy version of handwritten Bangla Numeral and Basic Character datasets.



### Flexible Neural Representation for Physics Prediction
- **Arxiv ID**: http://arxiv.org/abs/1806.08047v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1806.08047v2)
- **Published**: 2018-06-21 02:19:50+00:00
- **Updated**: 2018-10-27 05:28:48+00:00
- **Authors**: Damian Mrowca, Chengxu Zhuang, Elias Wang, Nick Haber, Li Fei-Fei, Joshua B. Tenenbaum, Daniel L. K. Yamins
- **Comment**: 23 pages, 20 figures
- **Journal**: None
- **Summary**: Humans have a remarkable capacity to understand the physical dynamics of objects in their environment, flexibly capturing complex structures and interactions at multiple levels of detail. Inspired by this ability, we propose a hierarchical particle-based object representation that covers a wide variety of types of three-dimensional objects, including both arbitrary rigid geometrical shapes and deformable materials. We then describe the Hierarchical Relation Network (HRN), an end-to-end differentiable neural network based on hierarchical graph convolution, that learns to predict physical dynamics in this representation. Compared to other neural network baselines, the HRN accurately handles complex collisions and nonrigid deformations, generating plausible dynamics predictions at long time scales in novel settings, and scaling to large scene configurations. These results demonstrate an architecture with the potential to form the basis of next-generation physics predictors for use in computer vision, robotics, and quantitative cognitive science.



### Error Compensated Quantized SGD and its Applications to Large-scale Distributed Optimization
- **Arxiv ID**: http://arxiv.org/abs/1806.08054v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1806.08054v1)
- **Published**: 2018-06-21 03:21:24+00:00
- **Updated**: 2018-06-21 03:21:24+00:00
- **Authors**: Jiaxiang Wu, Weidong Huang, Junzhou Huang, Tong Zhang
- **Comment**: Accepted by ICML 2018
- **Journal**: None
- **Summary**: Large-scale distributed optimization is of great importance in various applications. For data-parallel based distributed learning, the inter-node gradient communication often becomes the performance bottleneck. In this paper, we propose the error compensated quantized stochastic gradient descent algorithm to improve the training efficiency. Local gradients are quantized to reduce the communication overhead, and accumulated quantization error is utilized to speed up the convergence. Furthermore, we present theoretical analysis on the convergence behaviour, and demonstrate its advantage over competitors. Extensive experiments indicate that our algorithm can compress gradients by a factor of up to two magnitudes without performance degradation.



### Detection based Defense against Adversarial Examples from the Steganalysis Point of View
- **Arxiv ID**: http://arxiv.org/abs/1806.09186v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.09186v3)
- **Published**: 2018-06-21 04:57:20+00:00
- **Updated**: 2018-12-24 08:25:50+00:00
- **Authors**: Jiayang Liu, Weiming Zhang, Yiwei Zhang, Dongdong Hou, Yujia Liu, Hongyue Zha, Nenghai Yu
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have recently led to significant improvements in many fields. However, DNNs are vulnerable to adversarial examples which are samples with imperceptible perturbations while dramatically misleading the DNNs. Moreover, adversarial examples can be used to perform an attack on various kinds of DNN based systems, even if the adversary has no access to the underlying model. Many defense methods have been proposed, such as obfuscating gradients of the networks or detecting adversarial examples. However it is proved out that these defense methods are not effective or cannot resist secondary adversarial attacks. In this paper, we point out that steganalysis can be applied to adversarial examples detection, and propose a method to enhance steganalysis features by estimating the probability of modifications caused by adversarial attacks. Experimental results show that the proposed method can accurately detect adversarial examples. Moreover, secondary adversarial attacks cannot be directly performed to our method because our method is not based on a neural network but based on high-dimensional artificial features and FLD (Fisher Linear Discriminant) ensemble.



### Finding Original Image Of A Sub Image Using CNNs
- **Arxiv ID**: http://arxiv.org/abs/1806.08078v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08078v1)
- **Published**: 2018-06-21 06:24:08+00:00
- **Updated**: 2018-06-21 06:24:08+00:00
- **Authors**: Raja Asim
- **Comment**: 8 pages, 6 images
- **Journal**: None
- **Summary**: Convolututional Neural Networks have achieved state of the art in image classification, object detection and other image related tasks. In this paper I present another use of CNNs i.e. if given a set of images and then giving a single test image the network identifies that the test image is part of which image from the images given before. This is a task somehow similar to measuring image similarity and can be done using a simple CNN. Doing this task manually by looping can be quite a time consuming problem and won't be a generalizable solution. The task is quite similar to doing object detection but for that lots training data should be given or in the case of sliding window it takes lot of time and my algorithm can work with much fewer examples, is totally unsupervised and works much efficiently. Also, I explain that how unsupervised algorithm like K-Means or supervised algorithm like K-NN are not good enough to perform this task. The basic idea is that image encodings are collected for each image from a CNN, when a test image comes it is replaced by a part of original image, the encoding is generated using the same network, the frobenius norm is calculated and if it comes under a tolerance level then the test image is said to be the part of the original image.



### Deep Reinforcement Learning for Surgical Gesture Segmentation and Classification
- **Arxiv ID**: http://arxiv.org/abs/1806.08089v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08089v1)
- **Published**: 2018-06-21 07:09:43+00:00
- **Updated**: 2018-06-21 07:09:43+00:00
- **Authors**: Daochang Liu, Tingting Jiang
- **Comment**: 8 pages, 2 figures, accepted for MICCAI 2018
- **Journal**: None
- **Summary**: Recognition of surgical gesture is crucial for surgical skill assessment and efficient surgery training. Prior works on this task are based on either variant graphical models such as HMMs and CRFs, or deep learning models such as Recurrent Neural Networks and Temporal Convolutional Networks. Most of the current approaches usually suffer from over-segmentation and therefore low segment-level edit scores. In contrast, we present an essentially different methodology by modeling the task as a sequential decision-making process. An intelligent agent is trained using reinforcement learning with hierarchical features from a deep model. Temporal consistency is integrated into our action design and reward mechanism to reduce over-segmentation errors. Experiments on JIGSAWS dataset demonstrate that the proposed method performs better than state-of-the-art methods in terms of the edit score and on par in frame-wise accuracy. Our code will be released later.



### Hypergraph p-Laplacian Regularization for Remote Sensing Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.08104v1
- **DOI**: 10.1109/TGRS.2018.2867570
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08104v1)
- **Published**: 2018-06-21 08:28:31+00:00
- **Updated**: 2018-06-21 08:28:31+00:00
- **Authors**: Xueqi Ma, Weifeng Liu, Shuying Li, Yicong Zhou
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: It is of great importance to preserve locality and similarity information in semi-supervised learning (SSL) based applications. Graph based SSL and manifold regularization based SSL including Laplacian regularization (LapR) and Hypergraph Laplacian regularization (HLapR) are representative SSL methods and have achieved prominent performance by exploiting the relationship of sample distribution. However, it is still a great challenge to exactly explore and exploit the local structure of the data distribution. In this paper, we present an effect and effective approximation algorithm of Hypergraph p-Laplacian and then propose Hypergraph p-Laplacian regularization (HpLapR) to preserve the geometry of the probability distribution. In particular, p-Laplacian is a nonlinear generalization of the standard graph Laplacian and Hypergraph is a generalization of a standard graph. Therefore, the proposed HpLapR provides more potential to exploiting the local structure preserving. We apply HpLapR to logistic regression and conduct the implementations for remote sensing image recognition. We compare the proposed HpLapR to several popular manifold regularization based SSL methods including LapR, HLapR and HpLapR on UC-Merced dataset. The experimental results demonstrate the superiority of the proposed HpLapR.



### Ensemble p-Laplacian Regularization for Remote Sensing Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.08109v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08109v1)
- **Published**: 2018-06-21 08:37:48+00:00
- **Updated**: 2018-06-21 08:37:48+00:00
- **Authors**: Xueqi Ma, Weifeng Liu, Dapeng Tao, Yicong Zhou
- **Comment**: 13 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:1806.08104
- **Journal**: None
- **Summary**: Recently, manifold regularized semi-supervised learning (MRSSL) received considerable attention because it successfully exploits the geometry of the intrinsic data probability distribution including both labeled and unlabeled samples to leverage the performance of a learning model. As a natural nonlinear generalization of graph Laplacian, p-Laplacian has been proved having the rich theoretical foundations to better preserve the local structure. However, it is difficult to determine the fitting graph p-Lapalcian i.e. the parameter which is a critical factor for the performance of graph p-Laplacian. Therefore, we develop an ensemble p-Laplacian regularization (EpLapR) to fully approximate the intrinsic manifold of the data distribution. EpLapR incorporates multiple graphs into a regularization term in order to sufficiently explore the complementation of graph p-Laplacian. Specifically, we construct a fused graph by introducing an optimization approach to assign suitable weights on different p-value graphs. And then, we conduct semi-supervised learning framework on the fused graph. Extensive experiments on UC-Merced data set demonstrate the effectiveness and efficiency of the proposed method.



### Topological Data Analysis Made Easy with the Topology ToolKit
- **Arxiv ID**: http://arxiv.org/abs/1806.08126v1
- **DOI**: None
- **Categories**: **cs.DM**, cs.CG, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1806.08126v1)
- **Published**: 2018-06-21 09:18:58+00:00
- **Updated**: 2018-06-21 09:18:58+00:00
- **Authors**: Guillaume Favelier, Charles Gueunet, Attila Gyulassy, Julien Kitware, Joshua Levine, Jonas Lukasczyk, Daisuke Sakurai, Maxime Soler, Julien Tierny, Will Usher, Qi Wu
- **Comment**: None
- **Journal**: None
- **Summary**: This tutorial presents topological methods for the analysis and visualization of scientific data from a user's perspective, with the Topology ToolKit (TTK), a recently released open-source library for topological data analysis. Topological methods have gained considerably in popularity and maturity over the last twenty years and success stories of established methods have been documented in a wide range of applications (combustion, chemistry, astrophysics, material sciences, etc.) with both acquired and simulated data, in both post-hoc and in-situ contexts. While reference textbooks have been published on the topic, no tutorial at IEEE VIS has covered this area in recent years, and never at a software level and from a user's point-of-view. This tutorial fills this gap by providing a beginner's introduction to topological methods for practitioners, researchers, students, and lecturers. In particular, instead of focusing on theoretical aspects and algorithmic details, this tutorial focuses on how topological methods can be useful in practice for concrete data analysis tasks such as segmentation, feature extraction or tracking. The tutorial describes in detail how to achieve these tasks with TTK. First, after an introduction to topological methods and their application in data analysis, a brief overview of TTK's main entry point for end users, namely ParaView, will be presented. Second, an overview of TTK's main features will be given. A running example will be described in detail, showcasing how to access TTK's features via ParaView, Python, VTK/C++, and C++. Third, hands-on sessions will concretely show how to use TTK in ParaView for multiple, representative data analysis tasks. Fourth, the usage of TTK will be presented for developers, in particular by describing several examples of visualization and data analysis projects that were built on top of TTK. Finally, some feedback regarding the usage of TTK as a teaching platform for topological analysis will be given. Presenters of this tutorial include experts in topological methods, core authors of TTK as well as active users, coming from academia, labs, or industry. A large part of the tutorial will be dedicated to hands-on exercises and a rich material package (including TTK pre-installs in virtual machines, code, data, demos, video tutorials, etc.) will be provided to the participants. This tutorial mostly targets students, practitioners and researchers who are not experts in topological methods but who are interested in using them in their daily tasks. We also target researchers already familiar to topological methods and who are interested in using or contributing to TTK.



### Symmetry Aware Evaluation of 3D Object Detection and Pose Estimation in Scenes of Many Parts in Bulk
- **Arxiv ID**: http://arxiv.org/abs/1806.08129v1
- **DOI**: 10.1109/ICCVW.2017.258
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08129v1)
- **Published**: 2018-06-21 09:21:35+00:00
- **Updated**: 2018-06-21 09:21:35+00:00
- **Authors**: Romain Brégier, Frédéric Devernay, Laetitia Leyrit, James Crowley
- **Comment**: None
- **Journal**: 2017 IEEE International Conference on Computer Vision Workshop
  (ICCVW), Oct 2017, Venice, France. IEEE
- **Summary**: While 3D object detection and pose estimation has been studied for a long time, its evaluation is not yet completely satisfactory. Indeed, existing datasets typically consist in numerous acquisitions of only a few scenes because of the tediousness of pose annotation, and existing evaluation protocols cannot handle properly objects with symmetries. This work aims at addressing those two points. We first present automatic techniques to produce fully annotated RGBD data of many object instances in arbitrary poses, with which we produce a dataset of thousands of independent scenes of bulk parts composed of both real and synthetic images. We then propose a consistent evaluation methodology suitable for any rigid object, regardless of its symmetries. We illustrate it with two reference object detection and pose estimation methods on different objects, and show that incorporating symmetry considerations into pose estimation methods themselves can lead to significant performance gains. The proposed dataset is available at http://rbregier.github.io/dataset2017.



### CaloriNet: From silhouettes to calorie estimation in private environments
- **Arxiv ID**: http://arxiv.org/abs/1806.08152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08152v1)
- **Published**: 2018-06-21 10:09:28+00:00
- **Updated**: 2018-06-21 10:09:28+00:00
- **Authors**: Alessandro Masullo, Tilo Burghardt, Dima Damen, Sion Hannuna, Victor Ponce-López, Majid Mirmehdi
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: We propose a novel deep fusion architecture, CaloriNet, for the online estimation of energy expenditure for free living monitoring in private environments, where RGB data is discarded and replaced by silhouettes. Our fused convolutional neural network architecture is trainable end-to-end, to estimate calorie expenditure, using temporal foreground silhouettes alongside accelerometer data. The network is trained and cross-validated on a publicly available dataset, SPHERE_RGBD + Inertial_calorie. Results show state-of-the-art minimum error on the estimation of energy expenditure (calories per minute), outperforming alternative, standard and single-modal techniques.



### A convex method for classification of groups of examples
- **Arxiv ID**: http://arxiv.org/abs/1806.08169v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08169v1)
- **Published**: 2018-06-21 11:09:46+00:00
- **Updated**: 2018-06-21 11:09:46+00:00
- **Authors**: Dori Peleg
- **Comment**: None
- **Journal**: None
- **Summary**: There are many applications where it important to perform well on a set of examples as opposed to individual examples. For example in image or video classification the question is does an object appear somewhere in the image or video while there are several candidates of the object per image or video. In this context, it is not important what is the performance per candidate. Instead the performance per group is the ultimate objective.   For such problems one popular approach assumes weak supervision where labels exist for the entire group and then multiple instance learning is utilized. Another approach is to optimize per candidate, assuming each candidate is labeled, in the belief that this will achieve good performance per group.   We will show that better results can be achieved if we offer a new methodology which synthesizes the aforementioned approaches and directly optimizes for the final optimization objective while consisting of a convex optimization problem which solves the global optimization problem. The benefit of grouping examples is demonstrated on an image classification task for detecting polyps in images from capsule endoscopy of the colon. The algorithm was designed to efficiently handle hundreds of millions of examples. Furthermore, modifications to the penalty function of the standard SVM algorithm, have proven to significantly improve performance in our test case.



### Crowd disagreement about medical images is informative
- **Arxiv ID**: http://arxiv.org/abs/1806.08174v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08174v2)
- **Published**: 2018-06-21 11:27:38+00:00
- **Updated**: 2018-08-17 14:19:41+00:00
- **Authors**: Veronika Cheplygina, Josien P. W. Pluim
- **Comment**: Accepted for publication at MICCAI LABELS 2018
- **Journal**: None
- **Summary**: Classifiers for medical image analysis are often trained with a single consensus label, based on combining labels given by experts or crowds. However, disagreement between annotators may be informative, and thus removing it may not be the best strategy. As a proof of concept, we predict whether a skin lesion from the ISIC 2017 dataset is a melanoma or not, based on crowd annotations of visual characteristics of that lesion. We compare using the mean annotations, illustrating consensus, to standard deviations and other distribution moments, illustrating disagreement. We show that the mean annotations perform best, but that the disagreement measures are still informative. We also make the crowd annotations used in this paper available at \url{https://figshare.com/s/5cbbce14647b66286544}.



### Characterizing multiple instance datasets
- **Arxiv ID**: http://arxiv.org/abs/1806.08186v1
- **DOI**: 10.1007/978-3-319-24261-3_2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08186v1)
- **Published**: 2018-06-21 11:54:49+00:00
- **Updated**: 2018-06-21 11:54:49+00:00
- **Authors**: Veronika Cheplygina, David M. J. Tax
- **Comment**: Published at SIMBAD 2015 workshop
- **Journal**: None
- **Summary**: In many pattern recognition problems, a single feature vector is not sufficient to describe an object. In multiple instance learning (MIL), objects are represented by sets (\emph{bags}) of feature vectors (\emph{instances}). This requires an adaptation of standard supervised classifiers in order to train and evaluate on these bags of instances. Like for supervised classification, several benchmark datasets and numerous classifiers are available for MIL. When performing a comparison of different MIL classifiers, it is important to understand the differences of the datasets, used in the comparison. Seemingly different (based on factors such as dimensionality) datasets may elicit very similar behaviour in classifiers, and vice versa. This has implications for what kind of conclusions may be drawn from the comparison results. We aim to give an overview of the variability of available benchmark datasets and some popular MIL classifiers. We use a dataset dissimilarity measure, based on the differences between the ROC-curves obtained by different classifiers, and embed this dataset dissimilarity matrix into a low-dimensional space. Our results show that conceptually similar datasets can behave very differently. We therefore recommend examining such dataset characteristics when making comparisons between existing and new MIL classifiers.   The datasets are available via Figshare at \url{https://bit.ly/2K9iTja}.



### DPP-Net: Device-aware Progressive Search for Pareto-optimal Neural Architectures
- **Arxiv ID**: http://arxiv.org/abs/1806.08198v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.08198v2)
- **Published**: 2018-06-21 12:28:37+00:00
- **Updated**: 2018-07-25 07:56:33+00:00
- **Authors**: Jin-Dong Dong, An-Chieh Cheng, Da-Cheng Juan, Wei Wei, Min Sun
- **Comment**: 13 pages 9 figures, ECCV 2018 Camera Ready
- **Journal**: None
- **Summary**: Recent breakthroughs in Neural Architectural Search (NAS) have achieved state-of-the-art performances in applications such as image classification and language modeling. However, these techniques typically ignore device-related objectives such as inference time, memory usage, and power consumption. Optimizing neural architecture for device-related objectives is immensely crucial for deploying deep networks on portable devices with limited computing resources. We propose DPP-Net: Device-aware Progressive Search for Pareto-optimal Neural Architectures, optimizing for both device-related (e.g., inference time and memory usage) and device-agnostic (e.g., accuracy and model size) objectives. DPP-Net employs a compact search space inspired by current state-of-the-art mobile CNNs, and further improves search efficiency by adopting progressive search (Liu et al. 2017). Experimental results on CIFAR-10 are poised to demonstrate the effectiveness of Pareto-optimal networks found by DPP-Net, for three different devices: (1) a workstation with Titan X GPU, (2) NVIDIA Jetson TX1 embedded system, and (3) mobile phone with ARM Cortex-A53. Compared to CondenseNet and NASNet (Mobile), DPP-Net achieves better performances: higher accuracy and shorter inference time on various devices. Additional experimental results show that models found by DPP-Net also achieve considerably-good performance on ImageNet as well.



### Synaptic partner prediction from point annotations in insect brains
- **Arxiv ID**: http://arxiv.org/abs/1806.08205v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08205v2)
- **Published**: 2018-06-21 12:42:46+00:00
- **Updated**: 2018-07-16 09:43:59+00:00
- **Authors**: Julia Buhmann, Renate Krause, Rodrigo Ceballos Lentini, Nils Eckstein, Matthew Cook, Srinivas Turaga, Jan Funke
- **Comment**: None
- **Journal**: None
- **Summary**: High-throughput electron microscopy allows recording of lar- ge stacks of neural tissue with sufficient resolution to extract the wiring diagram of the underlying neural network. Current efforts to automate this process focus mainly on the segmentation of neurons. However, in order to recover a wiring diagram, synaptic partners need to be identi- fied as well. This is especially challenging in insect brains like Drosophila melanogaster, where one presynaptic site is associated with multiple post- synaptic elements. Here we propose a 3D U-Net architecture to directly identify pairs of voxels that are pre- and postsynaptic to each other. To that end, we formulate the problem of synaptic partner identification as a classification problem on long-range edges between voxels to encode both the presence of a synaptic pair and its direction. This formulation allows us to directly learn from synaptic point annotations instead of more ex- pensive voxel-based synaptic cleft or vesicle annotations. We evaluate our method on the MICCAI 2016 CREMI challenge and improve over the current state of the art, producing 3% fewer errors than the next best method.



### Reductive Clustering: An Efficient Linear-time Graph-based Divisive Cluster Analysis Approach
- **Arxiv ID**: http://arxiv.org/abs/1806.08245v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.DB, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.08245v3)
- **Published**: 2018-06-21 13:44:17+00:00
- **Updated**: 2020-09-25 12:20:22+00:00
- **Authors**: Ching Tarn, Yinan Zhang, Ye Feng
- **Comment**: http://res.ctarn.io/reductive-clustering
- **Journal**: None
- **Summary**: We propose an efficient linear-time graph-based divisive cluster analysis approach called Reductive Clustering. The approach tries to reveal the hierarchical structural information through reducing the graph into a more concise one repeatedly. With the reductions, the original graph can be divided into subgraphs recursively, and a lite informative dendrogram is constructed based on the divisions. The reduction consists of three steps: selection, connection, and partition. First a subset of vertices of the graph are selected as representatives to build a concise graph. The representatives are re-connected to maintain a consistent structure with the previous graph. If possible, the concise graph is divided into subgraphs, and each subgraph is further reduced recursively until the termination condition is met. We discuss the approach, along with several selection and connection methods, in detail both theoretically and experimentally in this paper. Our implementations run in linear time and achieve outstanding performance on various types of datasets. Experimental results show that they outperform state-of-the-art clustering algorithms with significantly less computing resource requirements.



### Finding Person Relations in Image Data of the Internet Archive
- **Arxiv ID**: http://arxiv.org/abs/1806.08246v2
- **DOI**: 10.1007/978-3-030-00066-0_20
- **Categories**: **cs.DL**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1806.08246v2)
- **Published**: 2018-06-21 13:48:21+00:00
- **Updated**: 2019-05-28 13:04:14+00:00
- **Authors**: Eric Müller-Budack, Kader Pustu-Iren, Sebastian Diering, Ralph Ewerth
- **Comment**: None
- **Journal**: In: M\'endez E., Crestani F., Ribeiro C., David G., Lopes J. (eds)
  Digital Libraries for Open Knowledge. TPDL 2018. Lecture Notes in Computer
  Science, vol 11057. Springer, Cham
- **Summary**: The multimedia content in the World Wide Web is rapidly growing and contains valuable information for many applications in different domains. For this reason, the Internet Archive initiative has been gathering billions of time-versioned web pages since the mid-nineties. However, the huge amount of data is rarely labeled with appropriate metadata and automatic approaches are required to enable semantic search. Normally, the textual content of the Internet Archive is used to extract entities and their possible relations across domains such as politics and entertainment, whereas image and video content is usually neglected. In this paper, we introduce a system for person recognition in image content of web news stored in the Internet Archive. Thus, the system complements entity recognition in text and allows researchers and analysts to track media coverage and relations of persons more precisely. Based on a deep learning face recognition approach, we suggest a system that automatically detects persons of interest and gathers sample material, which is subsequently used to identify them in the image data of the Internet Archive. We evaluate the performance of the face recognition system on an appropriate standard benchmark dataset and demonstrate the feasibility of the approach with two use cases.



### Learning Multimodal Representations for Unseen Activities
- **Arxiv ID**: http://arxiv.org/abs/1806.08251v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08251v4)
- **Published**: 2018-06-21 13:58:49+00:00
- **Updated**: 2020-07-07 17:36:54+00:00
- **Authors**: AJ Piergiovanni, Michael S. Ryoo
- **Comment**: None
- **Journal**: WACV 2020
- **Summary**: We present a method to learn a joint multimodal representation space that enables recognition of unseen activities in videos. We first compare the effect of placing various constraints on the embedding space using paired text and video data. We also propose a method to improve the joint embedding space using an adversarial formulation, allowing it to benefit from unpaired text and video data. By using unpaired text data, we show the ability to learn a representation that better captures unseen activities.   In addition to testing on publicly available datasets, we introduce a new, large-scale text/video dataset.   We experimentally confirm that using paired and unpaired data to learn a shared embedding space benefits three difficult tasks (i) zero-shot activity classification, (ii) unsupervised activity discovery, and (iii) unseen activity captioning, outperforming the state-of-the-arts.



### Don't only Feel Read: Using Scene text to understand advertisements
- **Arxiv ID**: http://arxiv.org/abs/1806.08279v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08279v3)
- **Published**: 2018-06-21 14:58:05+00:00
- **Updated**: 2019-11-13 10:04:54+00:00
- **Authors**: Arka Ujjal Dey, Suman K. Ghosh, Ernest Valveny
- **Comment**: Accepted in CVPR 2018 Workshop: Towards Automatic Understanding of
  Visual Advertisements (ADS)
- **Journal**: None
- **Summary**: We propose a framework for automated classification of Advertisement Images, using not just Visual features but also Textual cues extracted from embedded text. Our approach takes inspiration from the assumption that Ad images contain meaningful textual content, that can provide discriminative semantic interpretetion, and can thus aid in classifcation tasks. To this end, we develop a framework using off-the-shelf components, and demonstrate the effectiveness of Textual cues in semantic Classfication tasks.



### Layouts from Panoramic Images with Geometry and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.08294v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08294v1)
- **Published**: 2018-06-21 15:38:18+00:00
- **Updated**: 2018-06-21 15:38:18+00:00
- **Authors**: Clara Fernandez-Labrador, Alejandro Perez-Yus, Gonzalo Lopez-Nicolas, Jose J. Guerrero
- **Comment**: 8 pages, 12 figures, 2 tables
- **Journal**: None
- **Summary**: In this paper, we propose a novel procedure for 3D layout recovery of indoor scenes from single 360 degrees panoramic images. With such images, all scene is seen at once, allowing to recover closed geometries. Our method combines strategically the accuracy provided by geometric reasoning (lines and vanishing points) with the higher level of data abstraction and pattern recognition achieved by deep learning techniques (edge and normal maps). Thus, we extract structural corners from which we generate layout hypotheses of the room assuming Manhattan world. The best layout model is selected, achieving good performance on both simple rooms (box-type) and complex shaped rooms (with more than four walls). Experiments of the proposed approach are conducted within two public datasets, SUN360 and Stanford (2D-3D-S) demonstrating the advantages of estimating layouts by combining geometry and deep learning and the effectiveness of our proposal with respect to the state of the art.



### Are good local minima wide in sparse recovery?
- **Arxiv ID**: http://arxiv.org/abs/1806.08296v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1806.08296v1)
- **Published**: 2018-06-21 15:39:22+00:00
- **Updated**: 2018-06-21 15:39:22+00:00
- **Authors**: Michael Moeller, Otmar Loffeld, Juergen Gall, Felix Krahmer
- **Comment**: None
- **Journal**: None
- **Summary**: The idea of compressed sensing is to exploit representations in suitable (overcomplete) dictionaries that allow to recover signals far beyond the Nyquist rate provided that they admit a sparse representation in the respective dictionary. The latter gives rise to the sparse recovery problem of finding the best sparse linear approximation of given data in a given generating system. In this paper we analyze the iterative hard thresholding (IHT) algorithm as one of the most popular greedy methods for solving the sparse recovery problem, and demonstrate that systematically perturbing the IHT algorithm by adding noise to intermediate iterates yields improved results. Further improvements can be obtained by entirely rephrasing the problem as a parametric deep-learning-type of optimization problem. By introducing perturbations via dropout, we demonstrate to significantly outperform the classical IHT algorithm, obtaining $3$ to $6$ times lower average objective errors.



### Can Deep Learning Relax Endomicroscopy Hardware Miniaturization Requirements?
- **Arxiv ID**: http://arxiv.org/abs/1806.08338v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08338v1)
- **Published**: 2018-06-21 17:28:00+00:00
- **Updated**: 2018-06-21 17:28:00+00:00
- **Authors**: Saeed Izadi, Kathleen P. Moriarty, Ghassan Hamarneh
- **Comment**: Accepted to MICCAI 2018
- **Journal**: None
- **Summary**: Confocal laser endomicroscopy (CLE) is a novel imaging modality that provides in vivo histological cross-sections of examined tissue. Recently, attempts have been made to develop miniaturized in vivo imaging devices, specifically confocal laser microscopes, for both clinical and research applications. However, current implementations of miniature CLE components, such as confocal lenses, compromise image resolution, signal-to-noise ratio, or both, which negatively impacts the utility of in vivo imaging. In this work, we demonstrate that software-based techniques can be used to recover lost information due to endomicroscopy hardware miniaturization and reconstruct images of higher resolution. Particularly, a densely connected convolutional neural network is used to reconstruct a high-resolution CLE image from a low-resolution input. In the proposed network, each layer is directly connected to all subsequent layers, which results in an effective combination of low-level and high-level features and efficient information flow throughout the network. To train and evaluate our network, we use a dataset of 181 high-resolution CLE images. Both quantitative and qualitative results indicate superiority of the proposed network compared to traditional interpolation techniques and competing learning-based methods. This work demonstrates that software-based super-resolution is a viable approach to compensate for loss of resolution due to endoscopic hardware miniaturization.



### Quantizing deep convolutional networks for efficient inference: A whitepaper
- **Arxiv ID**: http://arxiv.org/abs/1806.08342v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.08342v1)
- **Published**: 2018-06-21 17:32:46+00:00
- **Updated**: 2018-06-21 17:32:46+00:00
- **Authors**: Raghuraman Krishnamoorthi
- **Comment**: 37 pages
- **Journal**: None
- **Summary**: We present an overview of techniques for quantizing convolutional neural networks for inference with integer weights and activations. Per-channel quantization of weights and per-layer quantization of activations to 8-bits of precision post-training produces classification accuracies within 2% of floating point networks for a wide variety of CNN architectures. Model sizes can be reduced by a factor of 4 by quantizing weights to 8-bits, even when 8-bit arithmetic is not supported. This can be achieved with simple, post training quantization of weights.We benchmark latencies of quantized networks on CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations compared to floating point on CPUs. Speedups of up to 10x are observed on specialized processors with fixed point SIMD capabilities, like the Qualcomm QDSPs with HVX.   Quantization-aware training can provide further improvements, reducing the gap to floating point to 1% at 8-bit precision. Quantization-aware training also allows for reducing the precision of weights to four bits with accuracy losses ranging from 2% to 10%, with higher accuracy drop for smaller networks.We introduce tools in TensorFlow and TensorFlowLite for quantizing convolutional networks and review best practices for quantization-aware training to obtain high accuracy with quantized weights and activations. We recommend that per-channel quantization of weights and per-layer quantization of activations be the preferred quantization scheme for hardware acceleration and kernel optimization. We also propose that future processors and hardware accelerators for optimized inference support precisions of 4, 8 and 16 bits.



### Learning Instance Segmentation by Interaction
- **Arxiv ID**: http://arxiv.org/abs/1806.08354v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.08354v1)
- **Published**: 2018-06-21 17:59:09+00:00
- **Updated**: 2018-06-21 17:59:09+00:00
- **Authors**: Deepak Pathak, Yide Shentu, Dian Chen, Pulkit Agrawal, Trevor Darrell, Sergey Levine, Jitendra Malik
- **Comment**: Website at https://pathak22.github.io/seg-by-interaction/
- **Journal**: None
- **Summary**: We present an approach for building an active agent that learns to segment its visual observations into individual objects by interacting with its environment in a completely self-supervised manner. The agent uses its current segmentation model to infer pixels that constitute objects and refines the segmentation model by interacting with these pixels. The model learned from over 50K interactions generalizes to novel objects and backgrounds. To deal with noisy training signal for segmenting objects obtained by self-supervised interactions, we propose robust set loss. A dataset of robot's interactions along-with a few human labeled examples is provided as a benchmark for future research. We test the utility of the learned segmentation model by providing results on a downstream vision-based control task of rearranging multiple objects into target configurations from visual inputs alone. Videos, code, and robotic interaction dataset are available at https://pathak22.github.io/seg-by-interaction/



### End-to-End Audio Visual Scene-Aware Dialog using Multimodal Attention-Based Video Features
- **Arxiv ID**: http://arxiv.org/abs/1806.08409v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1806.08409v2)
- **Published**: 2018-06-21 19:43:13+00:00
- **Updated**: 2018-06-30 00:35:25+00:00
- **Authors**: Chiori Hori, Huda Alamri, Jue Wang, Gordon Wichern, Takaaki Hori, Anoop Cherian, Tim K. Marks, Vincent Cartillier, Raphael Gontijo Lopes, Abhishek Das, Irfan Essa, Dhruv Batra, Devi Parikh
- **Comment**: A prototype system for the Audio Visual Scene-aware Dialog (AVSD) at
  DSTC7
- **Journal**: None
- **Summary**: Dialog systems need to understand dynamic visual scenes in order to have conversations with users about the objects and events around them. Scene-aware dialog systems for real-world applications could be developed by integrating state-of-the-art technologies from multiple research areas, including: end-to-end dialog technologies, which generate system responses using models trained from dialog data; visual question answering (VQA) technologies, which answer questions about images using learned image features; and video description technologies, in which descriptions/captions are generated from videos using multimodal information. We introduce a new dataset of dialogs about videos of human behaviors. Each dialog is a typed conversation that consists of a sequence of 10 question-and-answer(QA) pairs between two Amazon Mechanical Turk (AMT) workers. In total, we collected dialogs on roughly 9,000 videos. Using this new dataset for Audio Visual Scene-aware dialog (AVSD), we trained an end-to-end conversation model that generates responses in a dialog about a video. Our experiments demonstrate that using multimodal features that were developed for multimodal attention-based video description enhances the quality of generated dialog about dynamic scenes (videos). Our dataset, model code and pretrained models will be publicly available for a new Video Scene-Aware Dialog challenge.



### Star Shape Prior in Fully Convolutional Networks for Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.08437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.08437v1)
- **Published**: 2018-06-21 22:10:19+00:00
- **Updated**: 2018-06-21 22:10:19+00:00
- **Authors**: Zahra Mirikharaji, Ghassan Hamarneh
- **Comment**: Accepted in MICCAI 2018
- **Journal**: None
- **Summary**: Semantic segmentation is an important preliminary step towards automatic medical image interpretation. Recently deep convolutional neural networks have become the first choice for the task of pixel-wise class prediction. While incorporating prior knowledge about the structure of target objects has proven effective in traditional energy-based segmentation approaches, there has not been a clear way for encoding prior knowledge into deep learning frameworks. In this work, we propose a new loss term that encodes the star shape prior into the loss function of an end-to-end trainable fully convolutional network (FCN) framework. We penalize non-star shape segments in FCN prediction maps to guarantee a global structure in segmentation results. Our experiments demonstrate the advantage of regularizing FCN parameters by the star shape prior and our results on the ISBI 2017 skin segmentation challenge data set achieve the first rank in the segmentation task among $21$ participating teams.



