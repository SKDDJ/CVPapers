# Arxiv Papers in cs.CV on 2018-06-10
### Semantic Correspondence: A Hierarchical Approach
- **Arxiv ID**: http://arxiv.org/abs/1806.03560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03560v1)
- **Published**: 2018-06-10 00:30:37+00:00
- **Updated**: 2018-06-10 00:30:37+00:00
- **Authors**: Akila Pemasiri, Kien Nguyen, Sridha Sridhara, and Clinton Fookes
- **Comment**: None
- **Journal**: None
- **Summary**: Establishing semantic correspondence across images when the objects in the images have undergone complex deformations remains a challenging task in the field of computer vision. In this paper, we propose a hierarchical method to tackle this problem by first semantically targeting the foreground objects to localize the search space and then looking deeply into multiple levels of the feature representation to search for point-level correspondence. In contrast to existing approaches, which typically penalize large discrepancies, our approach allows for significant displacements, with the aim to accommodate large deformations of the objects in scene. Localizing the search space by semantically matching object-level correspondence, our method robustly handles large deformations of objects. Representing the target region by concatenated hypercolumn features which take into account the hierarchical levels of the surrounding context, helps to clear the ambiguity to further improve the accuracy. By conducting multiple experiments across scenes with non-rigid objects, we validate the proposed approach, and show that it outperforms the state of the art methods for semantic correspondence establishment.



### FMHash: Deep Hashing of In-Air-Handwriting for User Identification
- **Arxiv ID**: http://arxiv.org/abs/1806.03574v2
- **DOI**: None
- **Categories**: **cs.CV**, D.4.6; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1806.03574v2)
- **Published**: 2018-06-10 02:15:29+00:00
- **Updated**: 2019-07-16 20:49:27+00:00
- **Authors**: Duo Lu, Dijiang Huang, Anshul Rai
- **Comment**: 6 pages, 10 figures, deep hashing, in-air-handwriting, user
  identification, biometrics, accepted by ICC 2019 security track
- **Journal**: None
- **Summary**: Many mobile systems and wearable devices, such as Virtual Reality (VR) or Augmented Reality (AR) headsets, lack a keyboard or touchscreen to type an ID and password for signing into a virtual website. However, they are usually equipped with gesture capture interfaces to allow the user to interact with the system directly with hand gestures. Although gesture-based authentication has been well-studied, less attention is paid to the gesture-based user identification problem, which is essentially an input method of account ID and an efficient searching and indexing method of a database of gesture signals. In this paper, we propose FMHash (i.e., Finger Motion Hash), a user identification framework that can generate a compact binary hash code from a piece of in-air-handwriting of an ID string. This hash code enables indexing and fast search of a large account database using the in-air-handwriting by a hash table. To demonstrate the effectiveness of the framework, we implemented a prototype and achieved >99.5% precision and >92.6% recall with exact hash code match on a dataset of 200 accounts collected by us. The ability of hashing in-air-handwriting pattern to binary code can be used to achieve convenient sign-in and sign-up with in-air-handwriting gesture ID on future mobile and wearable systems connected to the Internet.



### Accurate Spectral Super-resolution from Single RGB Image Using Multi-scale CNN
- **Arxiv ID**: http://arxiv.org/abs/1806.03575v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03575v3)
- **Published**: 2018-06-10 02:32:02+00:00
- **Updated**: 2018-11-29 08:59:39+00:00
- **Authors**: Yiqi Yan, Lei Zhang, Jun Li, Wei Wei, Yanning Zhang
- **Comment**: PRCV 2018 camera ready
- **Journal**: None
- **Summary**: Different from traditional hyperspectral super-resolution approaches that focus on improving the spatial resolution, spectral super-resolution aims at producing a high-resolution hyperspectral image from the RGB observation with super-resolution in spectral domain. However, it is challenging to accurately reconstruct a high-dimensional continuous spectrum from three discrete intensity values at each pixel, since too much information is lost during the procedure where the latent hyperspectral image is downsampled (e.g., with x10 scaling factor) in spectral domain to produce an RGB observation. To address this problem, we present a multi-scale deep convolutional neural network (CNN) to explicitly map the input RGB image into a hyperspectral image. Through symmetrically downsampling and upsampling the intermediate feature maps in a cascading paradigm, the local and non-local image information can be jointly encoded for spectral representation, ultimately improving the spectral reconstruction accuracy. Extensive experiments on a large hyperspectral dataset demonstrate the effectiveness of the proposed method.



### Instance Search via Instance Level Segmentation and Feature Representation
- **Arxiv ID**: http://arxiv.org/abs/1806.03576v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1806.03576v2)
- **Published**: 2018-06-10 02:39:52+00:00
- **Updated**: 2019-05-09 03:14:51+00:00
- **Authors**: Yu Zhan, Wan-Lei Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Instance search is an interesting task as well as a challenging issue due to the lack of effective feature representation. In this paper, an instance level feature representation built upon fully convolutional instance-aware segmentation is proposed. The feature is ROI-pooled from the segmented instance region. So that instances in various sizes and layouts are represented by deep features in uniform length. This representation is further enhanced by the use of deformable ResNeXt blocks. Superior performance is observed in terms of its distinctiveness and scalability on a challenging evaluation dataset built by ourselves. In addition, the proposed enhancement on the network structure also shows superior performance on the instance segmentation task.



### EREL Selection using Morphological Relation
- **Arxiv ID**: http://arxiv.org/abs/1806.03580v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1806.03580v1)
- **Published**: 2018-06-10 04:03:46+00:00
- **Updated**: 2018-06-10 04:03:46+00:00
- **Authors**: Yuying Li, Mehdi Faraji
- **Comment**: 6 pages, 8 figures, accepted to be published in International
  Conference on SMART MULTIMEDIA, 2018. The final authenticated publication is
  available online at https://doi.org/
- **Journal**: None
- **Summary**: This work concentrates on Extremal Regions of Extremum Level (EREL) selection. EREL is a recently proposed feature detector aiming at detecting regions from a set of extremal regions. This is a branching problem derived from segmentation of arterial wall boundaries from Intravascular Ultrasound (IVUS) images. For each IVUS frame, a set of EREL regions is generated to describe the luminal area of human coronary. Each EREL is then fitted by an ellipse to represent the luminal border. The goal is to assign the most appropriate EREL as the lumen. In this work, EREL selection carries out in two rounds. In the first round, the pattern in a set of EREL regions is analyzed and used to generate an approximate luminal region. Then, the two-dimensional (2D) correlation coefficients are computed between this approximate region and each EREL to keep the ones with tightest relevance. In the second round, a compactness measure is calculated for each EREL and its fitted ellipse to guarantee that the resulting EREL has not affected by the common artifacts such as bifurcations, shadows, and side branches. We evaluated the selected ERELs in terms of Hausdorff Distance (HD) and Jaccard Measure (JM) on the train and test set of a publicly available dataset. The results show that our selection strategy outperforms the current state-of-the-art.



### A Simplified Active Calibration algorithm for Focal Length Estimation
- **Arxiv ID**: http://arxiv.org/abs/1806.03584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03584v1)
- **Published**: 2018-06-10 04:29:36+00:00
- **Updated**: 2018-06-10 04:29:36+00:00
- **Authors**: Mehdi Faraji, Anup Basu
- **Comment**: 5 page, 4 figures, accepted to be published in International
  Conference of Smart Multimedia, 2018. The final authenticated publication is
  available online at https://doi.org/
- **Journal**: None
- **Summary**: We introduce new linear mathematical formulations to calculate the focal length of a camera in an active platform. Through mathematical derivations, we show that the focal lengths in each direction can be estimated using only one point correspondence that relates images taken before and after a degenerate rotation of the camera. The new formulations will be beneficial in robotic and dynamic surveillance environments when the camera needs to be calibrated while it freely moves and zooms. By establishing a correspondence between only two images taken after slightly panning and tilting the camera and a reference image, our proposed Simplified Calibration Method is able to calculate the focal length of the camera. We extensively evaluate the derived formulations on a simulated camera, 3D scenes and real-world images. Our error analysis over simulated and real images indicates that the proposed Simplified Active Calibration formulation estimates the parameters of a camera with low error rates.



### Free-Form Image Inpainting with Gated Convolution
- **Arxiv ID**: http://arxiv.org/abs/1806.03589v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.03589v2)
- **Published**: 2018-06-10 05:51:32+00:00
- **Updated**: 2019-10-22 03:06:37+00:00
- **Authors**: Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas Huang
- **Comment**: Accepted in ICCV 2019 Oral; open sourced; interactive demo available:
  http://jiahuiyu.com/deepfill/
- **Journal**: None
- **Summary**: We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: https://github.com/JiahuiYu/generative_inpainting



### Weighted Tanimoto Coefficient for 3D Molecule Structure Similarity Measurement
- **Arxiv ID**: http://arxiv.org/abs/1806.05237v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05237v1)
- **Published**: 2018-06-10 06:55:06+00:00
- **Updated**: 2018-06-10 06:55:06+00:00
- **Authors**: Siti Asmah Bero, Azah Kamilah Muda, Yun-Huoy Choo, Noor Azilah Muda, Satrya Fajri Pratama
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Similarity searching of molecular structure has been an important application in the Chemoinformatics, especially in drug discovery. Similarity searching is a common method used for identification of molecular structure. It involve three main principal component of similarity searching: structure representation; weighting scheme; and similarity coefficient. In this paper, we introduces Weighted Tanimoto Coefficient based on weighted Euclidean distance in order to investigate the effect of weight function on the result for similarity searching. The Tanimoto coefficient is one of the popular similarity coefficients used to measure the similarity between pairs of the molecule. The most of research area found that the similarity searching is based on binary or fingerprint data. Meanwhile, we used non-binary data and was set amphetamine structure as a reference or targeted structure and the rest of the dataset becomes a database structure. Throughout this study, it showed that there is definitely gives a different result between a similarity searching with and without weight.



### VoxelAtlasGAN: 3D Left Ventricle Segmentation on Echocardiography with Atlas Guided Generation and Voxel-to-voxel Discrimination
- **Arxiv ID**: http://arxiv.org/abs/1806.03619v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03619v1)
- **Published**: 2018-06-10 09:25:17+00:00
- **Updated**: 2018-06-10 09:25:17+00:00
- **Authors**: Suyu Dong, Gongning Luo, Kuanquan Wang, Shaodong Cao, Ashley Mercado, Olga Shmuilovich, Henggui Zhang, Shuo Li
- **Comment**: Accepted by MICCAI 2018
- **Journal**: None
- **Summary**: 3D left ventricle (LV) segmentation on echocardiography is very important for diagnosis and treatment of cardiac disease. It is not only because of that echocardiography is a real-time imaging technology and widespread in clinical application, but also because of that LV segmentation on 3D echocardiography can provide more full volume information of heart than LV segmentation on 2D echocardiography. However, 3D LV segmentation on echocardiography is still an open and challenging task owing to the lower contrast, higher noise and data dimensionality, limited annotation of 3D echocardiography. In this paper, we proposed a novel real-time framework, i.e., VoxelAtlasGAN, for 3D LV segmentation on 3D echocardiography. This framework has three contributions: 1) It is based on voxel-to-voxel conditional generative adversarial nets (cGAN). For the first time, cGAN is used for 3D LV segmentation on echocardiography. And cGAN advantageously fuses substantial 3D spatial context information from 3D echocardiography by self-learning structured loss; 2) For the first time, it embeds the atlas into an end-to-end optimization framework, which uses 3D LV atlas as a powerful prior knowledge to improve the inference speed, address the lower contrast and the limited annotation problems of 3D echocardiography; 3) It combines traditional discrimination loss and the new proposed consistent constraint, which further improves the generalization of the proposed framework. VoxelAtlasGAN was validated on 60 subjects on 3D echocardiography and it achieved satisfactory segmentation results and high inference speed. The mean surface distance is 1.85 mm, the mean hausdorff surface distance is 7.26 mm, mean dice is 0.953, the correlation of EF is 0.918, and the mean inference speed is 0.1s. These results have demonstrated that our proposed method has great potential for clinical application



### Transformationally Identical and Invariant Convolutional Neural Networks through Symmetric Element Operators
- **Arxiv ID**: http://arxiv.org/abs/1806.03636v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03636v3)
- **Published**: 2018-06-10 11:16:36+00:00
- **Updated**: 2018-07-10 22:42:54+00:00
- **Authors**: Shih Chung B. Lo, Matthew T. Freedman, Seong K. Mun, Shuo Gu
- **Comment**: 20 pages, 2 figures
- **Journal**: None
- **Summary**: Mathematically speaking, a transformationally invariant operator, such as a transformationally identical (TI) matrix kernel (i.e., K= T{K}), commutes with the transformation (T{.}) itself when they operate on the first operand matrix. We found that by consistently applying the same type of TI kernels in a convolutional neural networks (CNN) system, the commutative property holds throughout all layers of convolution processes with and without involving an activation function and/or a 1D convolution across channels within a layer. We further found that any CNN possessing the same TI kernel property for all convolution layers followed by a flatten layer with weight sharing among their transformation corresponding elements would output the same result for all transformation versions of the original input vector. In short, CNN[ Vi ] = CNN[ T{Vi} ] providing every K = T{K} in CNN, where Vi denotes input vector and CNN[.] represents the whole CNN process as a function of input vector that produces an output vector. With such a transformationally identical CNN (TI-CNN) system, each transformation, that is not associated with a predefined TI used in data augmentation, would inherently include all of its corresponding transformation versions of the input vector for the training. Hence the use of same TI property for every kernel in the CNN would serve as an orientation or a translation independent training guide in conjunction with the error-backpropagation during the training. This TI kernel property is desirable for applications requiring a highly consistent output result from corresponding transformation versions of an input. Several C programming routines are provided to facilitate interested parties of using the TI-CNN technique which is expected to produce a better generalization performance than its ordinary CNN counterpart.



### Smart Novel Computer-based Analytical Tool for Image Forgery Authentication
- **Arxiv ID**: http://arxiv.org/abs/1806.04576v1
- **DOI**: 10.1109/ICCircuitsAndSystems.2012.6408276
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.04576v1)
- **Published**: 2018-06-10 14:07:58+00:00
- **Updated**: 2018-06-10 14:07:58+00:00
- **Authors**: Rozita Teymourzadeh, Amirrize Alpha, VH Mok
- **Comment**: Circuit and Systems (CAS) Conference. Pp.120-125. ISBN:
  978-1-4673-3117
- **Journal**: None
- **Summary**: This paper presents an integration of image forgery detection with image facial recognition using black propagation neural network (BPNN). We observed that facial image recognition by itself will always give a matching output or closest possible output image for every input image irrespective of the authenticity or otherwise not of the testing input image. Based on this, we are proposing the combination of the blind but powerful automation image forgery detection for entire input images for the BPNN recognition program. Hence, an input image must first be authenticated before being fed into the recognition program. Thus, an image security identification and authentication requirement, any image that fails the authentication/verification stage are not to be used as an input/test image. In addition, the universal smart GUI tool is proposed and designed to perform image forgery detection with the high accuracy of 2% error rate.



### Segmentation of Arterial Walls in Intravascular Ultrasound Cross-Sectional Images Using Extremal Region Selection
- **Arxiv ID**: http://arxiv.org/abs/1806.03695v1
- **DOI**: 10.1016/j.ultras.2017.11.020
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1806.03695v1)
- **Published**: 2018-06-10 17:37:11+00:00
- **Updated**: 2018-06-10 17:37:11+00:00
- **Authors**: Mehdi Faraji, Irene Cheng, Iris Naudin, Anup Basu
- **Comment**: 15 pages, 5 figures, published in Elsevier Ultrasonics
- **Journal**: None
- **Summary**: Intravascular Ultrasound (IVUS) is an intra-operative imaging modality that facilitates observing and appraising the vessel wall structure of the human coronary arteries. Segmentation of arterial wall boundaries from the IVUS images is not only crucial for quantitative analysis of the vessel walls and plaque characteristics, but is also necessary for generating 3D reconstructed models of the artery. The aim of this study is twofold. Firstly, we investigate the feasibility of using a recently proposed region detector, namely Extremal Region of Extremum Level (EREL) to delineate the luminal and media-adventitia borders in IVUS frames acquired by 20 MHz probes. Secondly, we propose a region selection strategy to label two ERELs as lumen and media based on the stability of their textural information. We extensively evaluated our selection strategy on the test set of a standard publicly available dataset containing 326 IVUS B-mode images. We showed that in the best case, the average Hausdorff Distances (HD) between the extracted ERELs and the actual lumen and media were $0.22$ mm and $0.45$ mm, respectively. The results of our experiments revealed that our selection strategy was able to segment the lumen with $\le 0.3$ mm HD to the gold standard even though the images contained major artifacts such as bifurcations, shadows, and side branches. Moreover, when there was no artifact, our proposed method was able to delineate media-adventitia boundaries with $0.31$ mm HD to the gold standard. Furthermore, our proposed segmentation method runs in time that is linear in the number of pixels in each frame. Based on the results of this work, by using a 20 MHz IVUS probe with controlled pullback, not only can we now analyze the internal structure of human arteries more accurately, but also segment each frame during the pullback procedure because of the low run time of our proposed segmentation method.



### Unsupervised Video-to-Video Translation
- **Arxiv ID**: http://arxiv.org/abs/1806.03698v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03698v1)
- **Published**: 2018-06-10 18:18:26+00:00
- **Updated**: 2018-06-10 18:18:26+00:00
- **Authors**: Dina Bashkirova, Ben Usman, Kate Saenko
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised image-to-image translation is a recently proposed task of translating an image to a different style or domain given only unpaired image examples at training time. In this paper, we formulate a new task of unsupervised video-to-video translation, which poses its own unique challenges. Translating video implies learning not only the appearance of objects and scenes but also realistic motion and transitions between consecutive frames.We investigate the performance of per-frame video-to-video translation using existing image-to-image translation networks, and propose a spatio-temporal 3D translator as an alternative solution to this problem. We evaluate our 3D method on multiple synthetic datasets, such as moving colorized digits, as well as the realistic segmentation-to-video GTA dataset and a new CT-to-MRI volumetric images translation dataset. Our results show that frame-wise translation produces realistic results on a single frame level but underperforms significantly on the scale of the whole video compared to our three-dimensional translation approach, which is better able to learn the complex structure of video and motion and continuity of object appearance.



### Stochastic seismic waveform inversion using generative adversarial networks as a geological prior
- **Arxiv ID**: http://arxiv.org/abs/1806.03720v1
- **DOI**: None
- **Categories**: **physics.geo-ph**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.03720v1)
- **Published**: 2018-06-10 20:38:29+00:00
- **Updated**: 2018-06-10 20:38:29+00:00
- **Authors**: Lukas Mosser, Olivier Dubrule, Martin J. Blunt
- **Comment**: 16 pages, 12 figures
- **Journal**: None
- **Summary**: We present an application of deep generative models in the context of partial-differential equation (PDE) constrained inverse problems. We combine a generative adversarial network (GAN) representing an a priori model that creates subsurface geological structures and their petrophysical properties, with the numerical solution of the PDE governing the propagation of acoustic waves within the earth's interior. We perform Bayesian inversion using an approximate Metropolis-adjusted Langevin algorithm (MALA) to sample from the posterior given seismic observations. Gradients with respect to the model parameters governing the forward problem are obtained by solving the adjoint of the acoustic wave equation. Gradients of the mismatch with respect to the latent variables are obtained by leveraging the differentiable nature of the deep neural network used to represent the generative model. We show that approximate MALA sampling allows efficient Bayesian inversion of model parameters obtained from a prior represented by a deep generative model, obtaining a diverse set of realizations that reflect the observed seismic response.



### Learning Answer Embeddings for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1806.03724v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03724v1)
- **Published**: 2018-06-10 21:01:24+00:00
- **Updated**: 2018-06-10 21:01:24+00:00
- **Authors**: Hexiang Hu, Wei-Lun Chao, Fei Sha
- **Comment**: Accepted at CVPR 2018
- **Journal**: None
- **Summary**: We propose a novel probabilistic model for visual question answering (Visual QA). The key idea is to infer two sets of embeddings: one for the image and the question jointly and the other for the answers. The learning objective is to learn the best parameterization of those embeddings such that the correct answer has higher likelihood among all possible answers. In contrast to several existing approaches of treating Visual QA as multi-way classification, the proposed approach takes the semantic relationships (as characterized by the embeddings) among answers into consideration, instead of viewing them as independent ordinal numbers. Thus, the learned embedded function can be used to embed unseen answers (in the training dataset). These properties make the approach particularly appealing for transfer learning for open-ended Visual QA, where the source dataset on which the model is learned has limited overlapping with the target dataset in the space of answers. We have also developed large-scale optimization techniques for applying the model to datasets with a large number of answers, where the challenge is to properly normalize the proposed probabilistic models. We validate our approach on several Visual QA datasets and investigate its utility for transferring models across datasets. The empirical results have shown that the approach performs well not only on in-domain learning but also on transfer learning.



### Cross-Dataset Adaptation for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1806.03726v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03726v1)
- **Published**: 2018-06-10 21:06:28+00:00
- **Updated**: 2018-06-10 21:06:28+00:00
- **Authors**: Wei-Lun Chao, Hexiang Hu, Fei Sha
- **Comment**: Accepted at CVPR 2018
- **Journal**: None
- **Summary**: We investigate the problem of cross-dataset adaptation for visual question answering (Visual QA). Our goal is to train a Visual QA model on a source dataset but apply it to another target one. Analogous to domain adaptation for visual recognition, this setting is appealing when the target dataset does not have a sufficient amount of labeled data to learn an "in-domain" model. The key challenge is that the two datasets are constructed differently, resulting in the cross-dataset mismatch on images, questions, or answers.   We overcome this difficulty by proposing a novel domain adaptation algorithm. Our method reduces the difference in statistical distributions by transforming the feature representation of the data in the target dataset. Moreover, it maximizes the likelihood of answering questions (in the target dataset) correctly using the Visual QA model trained on the source dataset. We empirically studied the effectiveness of the proposed approach on adapting among several popular Visual QA datasets. We show that the proposed method improves over baselines where there is no adaptation and several other adaptation methods. We both quantitatively and qualitatively analyze when the adaptation can be mostly effective.



