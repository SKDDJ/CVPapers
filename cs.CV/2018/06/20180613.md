# Arxiv Papers in cs.CV on 2018-06-13
### BA-Net: Dense Bundle Adjustment Network
- **Arxiv ID**: http://arxiv.org/abs/1806.04807v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04807v3)
- **Published**: 2018-06-13 00:51:48+00:00
- **Updated**: 2019-08-25 19:20:00+00:00
- **Authors**: Chengzhou Tang, Ping Tan
- **Comment**: Revised for rebuttal Code available at
  https://github.com/frobelbest/BANet
- **Journal**: None
- **Summary**: This paper introduces a network architecture to solve the structure-from-motion (SfM) problem via feature-metric bundle adjustment (BA), which explicitly enforces multi-view geometry constraints in the form of feature-metric error. The whole pipeline is differentiable so that the network can learn suitable features that make the BA problem more tractable. Furthermore, this work introduces a novel depth parameterization to recover dense per-pixel depth. The network first generates several basis depth maps according to the input image and optimizes the final depth as a linear combination of these basis depth maps via feature-metric BA. The basis depth maps generator is also learned via end-to-end training. The whole system nicely combines domain knowledge (i.e. hard-coded multi-view geometry constraints) and deep learning (i.e. feature learning and basis depth maps learning) to address the challenging dense SfM problem. Experiments on large scale real data prove the success of the proposed method.



### A Unified Framework for Generalizable Style Transfer: Style and Content Separation
- **Arxiv ID**: http://arxiv.org/abs/1806.05173v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05173v1)
- **Published**: 2018-06-13 01:39:02+00:00
- **Updated**: 2018-06-13 01:39:02+00:00
- **Authors**: Yexun Zhang, Ya Zhang, Wenbin Cai
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1711.06454
- **Journal**: None
- **Summary**: Image style transfer has drawn broad attention in recent years. However, most existing methods aim to explicitly model the transformation between different styles, and the learned model is thus not generalizable to new styles. We here propose a unified style transfer framework for both character typeface transfer and neural style transfer tasks leveraging style and content separation. A key merit of such framework is its generalizability to new styles and contents. The overall framework consists of style encoder, content encoder, mixer and decoder. The style encoder and content encoder are used to extract the style and content representations from the corresponding reference images. The mixer integrates the above two representations and feeds it into the decoder to generate images with the target style and content. During training, the encoder networks learn to extract styles and contents from limited size of style/content reference images. This learning framework allows simultaneous style transfer among multiple styles and can be deemed as a special `multi-task' learning scenario. The encoders are expected to capture the underlying features for different styles and contents which is generalizable to new styles and contents. Under this framework, we design two individual networks for character typeface transfer and neural style transfer, respectively. For character typeface transfer, to separate the style features and content features, we leverage the conditional dependence of styles and contents given an image. For neural style transfer, we leverage the statistical information of feature maps in certain layers to represent style. Extensive experimental results have demonstrated the effectiveness and robustness of the proposed methods.



### Position Detection and Direction Prediction for Arbitrary-Oriented Ships via Multitask Rotation Region Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1806.04828v2
- **DOI**: 10.1109/ACCESS.2018.2869884
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04828v2)
- **Published**: 2018-06-13 02:48:44+00:00
- **Updated**: 2018-11-17 10:34:21+00:00
- **Authors**: Xue Yang, Hao Sun, Xian Sun, Menglong Yan, Zhi Guo, Kun Fu
- **Comment**: None
- **Journal**: IEEE ACCESS. 2018, 6, 50839 - 50849
- **Summary**: Ship detection is of great importance and full of challenges in the field of remote sensing. The complexity of application scenarios, the redundancy of detection region, and the difficulty of dense ship detection are all the main obstacles that limit the successful operation of traditional methods in ship detection. In this paper, we propose a brand new detection model based on multitask rotational region convolutional neural network to solve the problems above. This model is mainly consist of five consecutive parts: Dense Feature Pyramid Network (DFPN), adaptive region of interest (ROI) Align, rotational bounding box regression, prow direction prediction and rotational nonmaximum suppression (R-NMS). First of all, the low-level location information and high-level semantic information are fully utilized through multiscale feature networks. Then, we design Adaptive ROI Align to obtain high quality proposals which remain complete spatial and semantic information. Unlike most previous approaches, the prediction obtained by our method is the minimum bounding rectangle of the object with less redundant regions. Therefore, rotational region detection framework is more suitable to detect the dense object than traditional detection model. Additionally, we can find the berthing and sailing direction of ship through prediction. A detailed evaluation based on SRSS for rotation detection shows that our detection method has a competitive performance.



### Interpretable Partitioned Embedding for Customized Fashion Outfit Composition
- **Arxiv ID**: http://arxiv.org/abs/1806.04845v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.04845v4)
- **Published**: 2018-06-13 04:57:06+00:00
- **Updated**: 2018-06-21 04:21:30+00:00
- **Authors**: Zunlei Feng, Zhenyun Yu, Yezhou Yang, Yongcheng Jing, Junxiao Jiang, Mingli Song
- **Comment**: None
- **Journal**: None
- **Summary**: Intelligent fashion outfit composition becomes more and more popular in these years. Some deep learning based approaches reveal competitive composition recently. However, the unexplainable characteristic makes such deep learning based approach cannot meet the the designer, businesses and consumers' urge to comprehend the importance of different attributes in an outfit composition. To realize interpretable and customized fashion outfit compositions, we propose a partitioned embedding network to learn interpretable representations from clothing items. The overall network architecture consists of three components: an auto-encoder module, a supervised attributes module and a multi-independent module. The auto-encoder module serves to encode all useful information into the embedding. In the supervised attributes module, multiple attributes labels are adopted to ensure that different parts of the overall embedding correspond to different attributes. In the multi-independent module, adversarial operation are adopted to fulfill the mutually independent constraint. With the interpretable and partitioned embedding, we then construct an outfit composition graph and an attribute matching map. Given specified attributes description, our model can recommend a ranked list of outfit composition with interpretable matching scores. Extensive experiments demonstrate that 1) the partitioned embedding have unmingled parts which corresponding to different attributes and 2) outfits recommended by our model are more desirable in comparison with the existing methods.



### Learning Visual Knowledge Memory Networks for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1806.04860v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1806.04860v1)
- **Published**: 2018-06-13 06:37:42+00:00
- **Updated**: 2018-06-13 06:37:42+00:00
- **Authors**: Zhou Su, Chen Zhu, Yinpeng Dong, Dongqi Cai, Yurong Chen, Jianguo Li
- **Comment**: Supplementary to CVPR 2018 version
- **Journal**: None
- **Summary**: Visual question answering (VQA) requires joint comprehension of images and natural language questions, where many questions can't be directly or clearly answered from visual content but require reasoning from structured human knowledge with confirmation from visual content. This paper proposes visual knowledge memory network (VKMN) to address this issue, which seamlessly incorporates structured human knowledge and deep visual features into memory networks in an end-to-end learning framework. Comparing to existing methods for leveraging external knowledge for supporting VQA, this paper stresses more on two missing mechanisms. First is the mechanism for integrating visual contents with knowledge facts. VKMN handles this issue by embedding knowledge triples (subject, relation, target) and deep visual features jointly into the visual knowledge features. Second is the mechanism for handling multiple knowledge facts expanding from question and answer pairs. VKMN stores joint embedding using key-value pair structure in the memory networks so that it is easy to handle multiple facts. Experiments show that the proposed method achieves promising results on both VQA v1.0 and v2.0 benchmarks, while outperforms state-of-the-art methods on the knowledge-reasoning related questions.



### Adversarial Learning with Local Coordinate Coding
- **Arxiv ID**: http://arxiv.org/abs/1806.04895v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04895v2)
- **Published**: 2018-06-13 08:49:30+00:00
- **Updated**: 2018-06-14 02:05:09+00:00
- **Authors**: Jiezhang Cao, Yong Guo, Qingyao Wu, Chunhua Shen, Junzhou Huang, Mingkui Tan
- **Comment**: 14 pages, 7 figures, 4 tables
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) aim to generate realistic data from some prior distribution (e.g., Gaussian noises). However, such prior distribution is often independent of real data and thus may lose semantic information (e.g., geometric structure or content in images) of data. In practice, the semantic information might be represented by some latent distribution learned from data, which, however, is hard to be used for sampling in GANs. In this paper, rather than sampling from the pre-defined prior distribution, we propose a Local Coordinate Coding (LCC) based sampling method to improve GANs. We derive a generalization bound for LCC based GANs and prove that a small dimensional input is sufficient to achieve good generalization. Extensive experiments on various real-world datasets demonstrate the effectiveness of the proposed method.



### Reservoir Computing Hardware with Cellular Automata
- **Arxiv ID**: http://arxiv.org/abs/1806.04932v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, nlin.CG
- **Links**: [PDF](http://arxiv.org/pdf/1806.04932v2)
- **Published**: 2018-06-13 10:28:44+00:00
- **Updated**: 2018-06-21 09:23:43+00:00
- **Authors**: Alejandro Morán, Christiam F. Frasser, Josep L. Rosselló
- **Comment**: 20 pages, 11 figures, draft of an article currently submitted to IEEE
  journal
- **Journal**: None
- **Summary**: Elementary cellular automata (ECA) is a widely studied one-dimensional processing methodology where the successive iteration of the automaton may lead to the recreation of a rich pattern dynamic. Recently, cellular automata have been proposed as a feasible way to implement Reservoir Computing (RC) systems in which the automata rule is fixed and the training is performed using a linear regression. In this work we perform an exhaustive study of the performance of the different ECA rules when applied to pattern recognition of time-independent input signals using a RC scheme. Once the different ECA rules have been tested, the most accurate one (rule 90) is selected to implement a digital circuit. Rule 90 is easily reproduced using a reduced set of XOR gates and shift-registers, thus representing a high-performance alternative for RC hardware implementation in terms of processing time, circuit area, power dissipation and system accuracy. The model (both in software and its hardware implementation) has been tested using a pattern recognition task of handwritten numbers (the MNIST database) for which we obtained competitive results in terms of accuracy, speed and power dissipation. The proposed model can be considered to be a low-cost method to implement fast pattern recognition digital circuits.



### Convolutional sparse coding for capturing high speed video content
- **Arxiv ID**: http://arxiv.org/abs/1806.04935v1
- **DOI**: 10.1111/cgf.13086
- **Categories**: **cs.GR**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1806.04935v1)
- **Published**: 2018-06-13 10:31:07+00:00
- **Updated**: 2018-06-13 10:31:07+00:00
- **Authors**: Ana Serrano, Elena Garces, Diego Gutierrez, Belen Masia
- **Comment**: None
- **Journal**: Computer Graphics Forum 36, 8, Pages 380-389 (February 2017)
- **Summary**: Video capture is limited by the trade-off between spatial and temporal resolution: when capturing videos of high temporal resolution, the spatial resolution decreases due to bandwidth limitations in the capture system. Achieving both high spatial and temporal resolution is only possible with highly specialized and very expensive hardware, and even then the same basic trade-off remains. The recent introduction of compressive sensing and sparse reconstruction techniques allows for the capture of single-shot high-speed video, by coding the temporal information in a single frame, and then reconstructing the full video sequence from this single coded image and a trained dictionary of image patches. In this paper, we first analyze this approach, and find insights that help improve the quality of the reconstructed videos. We then introduce a novel technique, based on convolutional sparse coding (CSC), and show how it outperforms the state-of-the-art, patch-based approach in terms of flexibility and efficiency, due to the convolutional nature of its filter banks. The key idea for CSC high-speed video acquisition is extending the basic formulation by imposing an additional constraint in the temporal dimension, which enforces sparsity of the first-order derivatives over time.



### Convolutional Sparse Coding for High Dynamic Range Imaging
- **Arxiv ID**: http://arxiv.org/abs/1806.04942v1
- **DOI**: 10.1111/cgf.12819
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1806.04942v1)
- **Published**: 2018-06-13 10:48:33+00:00
- **Updated**: 2018-06-13 10:48:33+00:00
- **Authors**: Ana Serrano, Felix Heide, Diego Gutierrez, Gordon Wetzstein, Belen Masia
- **Comment**: None
- **Journal**: Computer Graphics Forum 35, 2, Pages 153-163 (May 2016)
- **Summary**: Current HDR acquisition techniques are based on either (i) fusing multibracketed, low dynamic range (LDR) images, (ii) modifying existing hardware and capturing different exposures simultaneously with multiple sensors, or (iii) reconstructing a single image with spatially-varying pixel exposures. In this paper, we propose a novel algorithm to recover high-quality HDRI images from a single, coded exposure. The proposed reconstruction method builds on recently-introduced ideas of convolutional sparse coding (CSC); this paper demonstrates how to make CSC practical for HDR imaging. We demonstrate that the proposed algorithm achieves higher-quality reconstructions than alternative methods, we evaluate optical coding schemes, analyze algorithmic parameters, and build a prototype coded HDR camera that demonstrates the utility of convolutional sparse HDRI coding with a custom hardware platform.



### fMRI Semantic Category Decoding using Linguistic Encoding of Word Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1806.05177v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.05177v1)
- **Published**: 2018-06-13 10:59:33+00:00
- **Updated**: 2018-06-13 10:59:33+00:00
- **Authors**: Subba Reddy Oota, Naresh Manwani, Bapi Raju S
- **Comment**: 12 pages, 7 Figures
- **Journal**: None
- **Summary**: The dispute of how the human brain represents conceptual knowledge has been argued in many scientific fields. Brain imaging studies have shown that the spatial patterns of neural activation in the brain are correlated with thinking about different semantic categories of words (for example, tools, animals, and buildings) or when viewing the related pictures. In this paper, we present a computational model that learns to predict the neural activation captured in functional magnetic resonance imaging (fMRI) data of test words. Unlike the models with hand-crafted features that have been used in the literature, in this paper we propose a novel approach wherein decoding models are built with features extracted from popular linguistic encodings of Word2Vec, GloVe, Meta-Embeddings in conjunction with the empirical fMRI data associated with viewing several dozen concrete nouns. We compared these models with several other models that use word features extracted from FastText, Randomly-generated features, Mitchell's 25 features [1]. The experimental results show that the predicted fMRI images using Meta-Embeddings meet the state-of-the-art performance. Although models with features from GloVe and Word2Vec predict fMRI images similar to the state-of-the-art model, model with features from Meta-Embeddings predicts significantly better. The proposed scheme that uses popular linguistic encoding offers a simple and easy approach for semantic decoding from fMRI experiments.



### Higher Order of Motion Magnification for Vessel Localisation in Surgical Video
- **Arxiv ID**: http://arxiv.org/abs/1806.04955v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04955v1)
- **Published**: 2018-06-13 11:22:54+00:00
- **Updated**: 2018-06-13 11:22:54+00:00
- **Authors**: Mirek Janatka, Ashwin Sridhar, John Kelly, Danail Stoyanov
- **Comment**: Accepted to the International Conference On Medical Image Computing &
  Computer Assisted Intervention, 2018
- **Journal**: None
- **Summary**: Locating vessels during surgery is critical for avoiding inadvertent damage, yet vasculature can be difficult to identify. Video motion magnification can potentially highlight vessels by exaggerating subtle motion embedded within the video to become perceivable to the surgeon. In this paper, we explore a physiological model of artery distension to extend motion magnification to incorporate higher orders of motion, leveraging the difference in acceleration over time (jerk) in pulsatile motion to highlight the vascular pulse wave. Our method is compared to first and second order motion based Eulerian video magnification algorithms. Using data from a surgical video retrieved during a robotic prostatectomy, we show that our method can accentuate cardio-physiological features and produce a more succinct and clearer video for motion magnification, with more similarities in areas without motion to the source video at large magnifications. We validate the approach with a Structure Similarity (SSIM) and Peak Signal to Noise Ratio (PSNR) assessment of three videos at an increasing working distance, using three different levels of optical magnification. Spatio-temporal cross sections are presented to show the effectiveness of our proposal and video samples are provided to demonstrates qualitatively our results.



### Expression Empowered ResiDen Network for Facial Action Unit Detection
- **Arxiv ID**: http://arxiv.org/abs/1806.04957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04957v1)
- **Published**: 2018-06-13 11:28:26+00:00
- **Updated**: 2018-06-13 11:28:26+00:00
- **Authors**: Shreyank Jyoti, Abhinav Dhall
- **Comment**: None
- **Journal**: None
- **Summary**: The paper explores the topic of Facial Action Unit (FAU) detection in the wild. In particular, we are interested in answering the following questions: (1) how useful are residual connections across dense blocks for face analysis? (2) how useful is the information from a network trained for categorical Facial Expression Recognition (FER) for the task of FAU detection? The proposed network (ResiDen) exploits dense blocks along with residual connections and uses auxiliary information from a FER network. The experiments are performed on the EmotionNet and DISFA datasets. The experiments show the usefulness of facial expression information for AU detection. The proposed network achieves state-of-art results on the two databases. Analysis of the results for cross database protocol shows the effectiveness of the network.



### Unsupervised Detection of Lesions in Brain MRI using constrained adversarial auto-encoders
- **Arxiv ID**: http://arxiv.org/abs/1806.04972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04972v1)
- **Published**: 2018-06-13 12:15:54+00:00
- **Updated**: 2018-06-13 12:15:54+00:00
- **Authors**: Xiaoran Chen, Ender Konukoglu
- **Comment**: 9 pages, 5 figures, accepted at MIDL 2018
- **Journal**: None
- **Summary**: Lesion detection in brain Magnetic Resonance Images (MRI) remains a challenging task. State-of-the-art approaches are mostly based on supervised learning making use of large annotated datasets. Human beings, on the other hand, even non-experts, can detect most abnormal lesions after seeing a handful of healthy brain images. Replicating this capability of using prior information on the appearance of healthy brain structure to detect lesions can help computers achieve human level abnormality detection, specifically reducing the need for numerous labeled examples and bettering generalization of previously unseen lesions. To this end, we study detection of lesion regions in an unsupervised manner by learning data distribution of brain MRI of healthy subjects using auto-encoder based methods. We hypothesize that one of the main limitations of the current models is the lack of consistency in latent representation. We propose a simple yet effective constraint that helps mapping of an image bearing lesion close to its corresponding healthy image in the latent space. We use the Human Connectome Project dataset to learn distribution of healthy-appearing brain MRI and report improved detection, in terms of AUC, of the lesions in the BRATS challenge dataset.



### Self-Supervised Feature Learning by Learning to Spot Artifacts
- **Arxiv ID**: http://arxiv.org/abs/1806.05024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05024v1)
- **Published**: 2018-06-13 13:24:42+00:00
- **Updated**: 2018-06-13 13:24:42+00:00
- **Authors**: Simon Jenni, Paolo Favaro
- **Comment**: CVPR 2018 (spotlight)
- **Journal**: None
- **Summary**: We introduce a novel self-supervised learning method based on adversarial training. Our objective is to train a discriminator network to distinguish real images from images with synthetic artifacts, and then to extract features from its intermediate layers that can be transferred to other data domains and tasks. To generate images with artifacts, we pre-train a high-capacity autoencoder and then we use a damage and repair strategy: First, we freeze the autoencoder and damage the output of the encoder by randomly dropping its entries. Second, we augment the decoder with a repair network, and train it in an adversarial manner against the discriminator. The repair network helps generate more realistic images by inpainting the dropped feature entries. To make the discriminator focus on the artifacts, we also make it predict what entries in the feature were dropped. We demonstrate experimentally that features learned by creating and spotting artifacts achieve state of the art performance in several benchmarks.



### Visually grounded cross-lingual keyword spotting in speech
- **Arxiv ID**: http://arxiv.org/abs/1806.05030v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.05030v1)
- **Published**: 2018-06-13 13:37:34+00:00
- **Updated**: 2018-06-13 13:37:34+00:00
- **Authors**: Herman Kamper, Michael Roth
- **Comment**: 5 pages, 2 figures, 4 tables
- **Journal**: None
- **Summary**: Recent work considered how images paired with speech can be used as supervision for building speech systems when transcriptions are not available. We ask whether visual grounding can be used for cross-lingual keyword spotting: given a text keyword in one language, the task is to retrieve spoken utterances containing that keyword in another language. This could enable searching through speech in a low-resource language using text queries in a high-resource language. As a proof-of-concept, we use English speech with German queries: we use a German visual tagger to add keyword labels to each training image, and then train a neural network to map English speech to German keywords. Without seeing parallel speech-transcriptions or translations, the model achieves a precision at ten of 58%. We show that most erroneous retrievals contain equivalent or semantically relevant keywords; excluding these would improve P@10 to 91%.



### Deep Sequence Learning with Auxiliary Information for Traffic Prediction
- **Arxiv ID**: http://arxiv.org/abs/1806.07380v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1806.07380v1)
- **Published**: 2018-06-13 13:38:22+00:00
- **Updated**: 2018-06-13 13:38:22+00:00
- **Authors**: Binbing Liao, Jingqing Zhang, Chao Wu, Douglas McIlwraith, Tong Chen, Shengwen Yang, Yike Guo, Fei Wu
- **Comment**: KDD 2018. The first two authors share equal contributions
- **Journal**: None
- **Summary**: Predicting traffic conditions from online route queries is a challenging task as there are many complicated interactions over the roads and crowds involved. In this paper, we intend to improve traffic prediction by appropriate integration of three kinds of implicit but essential factors encoded in auxiliary information. We do this within an encoder-decoder sequence learning framework that integrates the following data: 1) offline geographical and social attributes. For example, the geographical structure of roads or public social events such as national celebrations; 2) road intersection information. In general, traffic congestion occurs at major junctions; 3) online crowd queries. For example, when many online queries issued for the same destination due to a public performance, the traffic around the destination will potentially become heavier at this location after a while. Qualitative and quantitative experiments on a real-world dataset from Baidu have demonstrated the effectiveness of our framework.



### A Probabilistic U-Net for Segmentation of Ambiguous Images
- **Arxiv ID**: http://arxiv.org/abs/1806.05034v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.05034v4)
- **Published**: 2018-06-13 13:47:04+00:00
- **Updated**: 2019-01-29 18:26:47+00:00
- **Authors**: Simon A. A. Kohl, Bernardino Romera-Paredes, Clemens Meyer, Jeffrey De Fauw, Joseph R. Ledsam, Klaus H. Maier-Hein, S. M. Ali Eslami, Danilo Jimenez Rezende, Olaf Ronneberger
- **Comment**: Last update: added further details about the LIDC experiment. 11
  pages for the main paper, 28 pages including appendix. 5 figures in the main
  paper, 18 figures in total, Advances in Neural Information Processing Systems
  (NeurIPS), 2018
- **Journal**: None
- **Summary**: Many real-world vision problems suffer from inherent ambiguities. In clinical applications for example, it might not be clear from a CT scan alone which particular region is cancer tissue. Therefore a group of graders typically produces a set of diverse but plausible segmentations. We consider the task of learning a distribution over segmentations given an input. To this end we propose a generative segmentation model based on a combination of a U-Net with a conditional variational autoencoder that is capable of efficiently producing an unlimited number of plausible hypotheses. We show on a lung abnormalities segmentation task and on a Cityscapes segmentation task that our model reproduces the possible segmentation variants as well as the frequencies with which they occur, doing so significantly better than published approaches. These models could have a high impact in real-world applications, such as being used as clinical decision-making algorithms accounting for multiple plausible semantic segmentation hypotheses to provide possible diagnoses and recommend further actions to resolve the present ambiguities.



### Multiple Instance Learning for Heterogeneous Images: Training a CNN for Histopathology
- **Arxiv ID**: http://arxiv.org/abs/1806.05083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05083v1)
- **Published**: 2018-06-13 14:24:44+00:00
- **Updated**: 2018-06-13 14:24:44+00:00
- **Authors**: Heather D. Couture, J. S. Marron, Charles M. Perou, Melissa A. Troester, Marc Niethammer
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple instance (MI) learning with a convolutional neural network enables end-to-end training in the presence of weak image-level labels. We propose a new method for aggregating predictions from smaller regions of the image into an image-level classification by using the quantile function. The quantile function provides a more complete description of the heterogeneity within each image, improving image-level classification. We also adapt image augmentation to the MI framework by randomly selecting cropped regions on which to apply MI aggregation during each epoch of training. This provides a mechanism to study the importance of MI learning. We validate our method on five different classification tasks for breast tumor histology and provide a visualization method for interpreting local image classifications that could lead to future insights into tumor heterogeneity.



### Group Equivariant Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.05086v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05086v2)
- **Published**: 2018-06-13 14:30:27+00:00
- **Updated**: 2018-10-24 17:21:35+00:00
- **Authors**: Jan Eric Lenssen, Matthias Fey, Pascal Libuschewski
- **Comment**: Presented at NIPS 2018
- **Journal**: None
- **Summary**: We present group equivariant capsule networks, a framework to introduce guaranteed equivariance and invariance properties to the capsule network idea. Our work can be divided into two contributions. First, we present a generic routing by agreement algorithm defined on elements of a group and prove that equivariance of output pose vectors, as well as invariance of output activations, hold under certain conditions. Second, we connect the resulting equivariant capsule networks with work from the field of group convolutional networks. Through this connection, we provide intuitions of how both methods relate and are able to combine the strengths of both approaches in one deep neural network architecture. The resulting framework allows sparse evaluation of the group convolution operator, provides control over specific equivariance and invariance properties, and can use routing by agreement instead of pooling operations. In addition, it is able to provide interpretable and equivariant representation vectors as output capsules, which disentangle evidence of object existence from its pose.



### Estimating Achilles tendon healing progress with convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1806.05091v2
- **DOI**: 10.1007/978-3-030-00934-2_105
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05091v2)
- **Published**: 2018-06-13 14:43:21+00:00
- **Updated**: 2018-06-19 02:21:55+00:00
- **Authors**: Norbert Kapinski, Jakub Zielinski, Bartosz A. Borucki, Tomasz Trzcinski, Beata Ciszkowska-Lyson, Krzysztof S. Nowinski
- **Comment**: Paper accepted to MICCAI'18
- **Journal**: None
- **Summary**: Quantitative assessment of a treatment progress in the Achilles tendon healing process - one of the most common musculoskeletal disorder in modern medical practice - is typically a long and complex process: multiple MRI protocols need to be acquired and analysed by radiology experts. In this paper, we propose to significantly reduce the complexity of this assessment using a novel method based on a pre-trained convolutional neural network. We first train our neural network on over 500,000 2D axial cross-sections from over 3000 3D MRI studies to classify MRI images as belonging to a healthy or injured class, depending on the patient's condition. We then take the outputs of modified pre-trained network and apply linear regression on the PCA-reduced space of the features to assess treatment progress. Our method allows to reduce up to 5-fold the amount of data needed to be registered during the MRI scan without any information loss. Furthermore, we are able to predict the healing process phase with equal accuracy to human experts in 3 out of 6 main criteria. Finally, contrary to the current approaches to regeneration assessment that rely on radiologist subjective opinion, our method allows to objectively compare different treatments methods which can lead to improved diagnostics and patient's recovery.



### Improving Cytoarchitectonic Segmentation of Human Brain Areas with Self-supervised Siamese Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.05104v1
- **DOI**: 10.1007/978-3-030-00931-1_76
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05104v1)
- **Published**: 2018-06-13 15:17:27+00:00
- **Updated**: 2018-06-13 15:17:27+00:00
- **Authors**: Hannah Spitzer, Kai Kiwitz, Katrin Amunts, Stefan Harmeling, Timo Dickscheid
- **Comment**: Accepted at MICCAI 2018
- **Journal**: None
- **Summary**: Cytoarchitectonic parcellations of the human brain serve as anatomical references in multimodal atlas frameworks. They are based on analysis of cell-body stained histological sections and the identification of borders between brain areas. The de-facto standard involves a semi-automatic, reproducible border detection, but does not scale with high-throughput imaging in large series of sections at microscopical resolution. Automatic parcellation, however, is extremely challenging due to high variation in the data, and the need for a large field of view at microscopic resolution. The performance of a recently proposed Convolutional Neural Network model that addresses this problem especially suffers from the naturally limited amount of expert annotations for training. To circumvent this limitation, we propose to pre-train neural networks on a self-supervised auxiliary task, predicting the 3D distance between two patches sampled from the same brain. Compared to a random initialization, fine-tuning from these networks results in significantly better segmentations. We show that the self-supervised model has implicitly learned to distinguish several cortical brain areas -- a strong indicator that the proposed auxiliary task is appropriate for cytoarchitectonic mapping.



### What Is It Like Down There? Generating Dense Ground-Level Views and Image Features From Overhead Imagery Using Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.05129v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05129v2)
- **Published**: 2018-06-13 16:17:30+00:00
- **Updated**: 2018-09-23 18:26:56+00:00
- **Authors**: Xueqing Deng, Yi Zhu, Shawn Newsam
- **Comment**: 10 pages, 5 figures, camera-ready version of ACM SIGSPATIAL 2018
  (ORAL)
- **Journal**: None
- **Summary**: This paper investigates conditional generative adversarial networks (cGANs) to overcome a fundamental limitation of using geotagged media for geographic discovery, namely its sparse and uneven spatial distribution. We train a cGAN to generate ground-level views of a location given overhead imagery. We show the "fake" ground-level images are natural looking and are structurally similar to the real images. More significantly, we show the generated images are representative of the locations and that the representations learned by the cGANs are informative. In particular, we show that dense feature maps generated using our framework are more effective for land-cover classification than approaches which spatially interpolate features extracted from sparse ground-level images. To our knowledge, ours is the first work to use cGANs to generate ground-level views given overhead imagery and to explore the benefits of the learned representations.



### Fully Convolutional Network for Automatic Road Extraction from Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1806.05182v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05182v2)
- **Published**: 2018-06-13 16:36:55+00:00
- **Updated**: 2018-06-19 19:20:13+00:00
- **Authors**: Alexander V. Buslaev, Selim S. Seferbekov, Vladimir I. Iglovikov, Alexey A. Shvets
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1806.03510,
  arXiv:1804.08024, arXiv:1801.05746, arXiv:1803.01207
- **Journal**: None
- **Summary**: Analysis of high-resolution satellite images has been an important research topic for traffic management, city planning, and road monitoring. One of the problems here is automatic and precise road extraction. From an original image, it is difficult and computationally expensive to extract roads due to presences of other road-like features with straight edges. In this paper, we propose an approach for automatic road extraction based on a fully convolutional neural network of U-net family. This network consists of ResNet-34 pre-trained on ImageNet and decoder adapted from vanilla U-Net. Based on validation results, leaderboard and our own experience this network shows superior results for the DEEPGLOBE - CVPR 2018 road extraction sub-challenge. Moreover, this network uses moderate memory that allows using just one GTX 1080 or 1080ti video cards to perform whole training and makes pretty fast predictions.



### Cross-modal Hallucination for Few-shot Fine-grained Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.05147v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1806.05147v2)
- **Published**: 2018-06-13 17:06:10+00:00
- **Updated**: 2018-06-14 09:22:20+00:00
- **Authors**: Frederik Pahde, Patrick Jähnichen, Tassilo Klein, Moin Nabi
- **Comment**: CVPR 2018 Workshop on Fine-Grained Visual Categorization
- **Journal**: None
- **Summary**: State-of-the-art deep learning algorithms generally require large amounts of data for model training. Lack thereof can severely deteriorate the performance, particularly in scenarios with fine-grained boundaries between categories. To this end, we propose a multimodal approach that facilitates bridging the information gap by means of meaningful joint embeddings. Specifically, we present a benchmark that is multimodal during training (i.e. images and texts) and single-modal in testing time (i.e. images), with the associated task to utilize multimodal data in base classes (with many samples), to learn explicit visual classifiers for novel classes (with few samples). Next, we propose a framework built upon the idea of cross-modal data hallucination. In this regard, we introduce a discriminative text-conditional GAN for sample generation with a simple self-paced strategy for sample selection. We show the results of our proposed discriminative hallucinated method for 1-, 2-, and 5- shot learning on the CUB dataset, where the accuracy is improved by employing multimodal data.



### Automated Performance Assessment in Transoesophageal Echocardiography with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.05154v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05154v1)
- **Published**: 2018-06-13 17:29:29+00:00
- **Updated**: 2018-06-13 17:29:29+00:00
- **Authors**: Evangelos B. Mazomenos, Kamakshi Bansal, Bruce Martin, Andrew Smith, Susan Wright, Danail Stoyanov
- **Comment**: to be presented in MICCAI 2018, Granada, Spain, 16-20 Sep 2018
- **Journal**: None
- **Summary**: Transoesophageal echocardiography (TEE) is a valuable diagnostic and monitoring imaging modality. Proper image acquisition is essential for diagnosis, yet current assessment techniques are solely based on manual expert review. This paper presents a supervised deep learn ing framework for automatically evaluating and grading the quality of TEE images. To obtain the necessary dataset, 38 participants of varied experience performed TEE exams with a high-fidelity virtual reality (VR) platform. Two Convolutional Neural Network (CNN) architectures, AlexNet and VGG, structured to perform regression, were finetuned and validated on manually graded images from three evaluators. Two different scoring strategies, a criteria-based percentage and an overall general impression, were used. The developed CNN models estimate the average score with a root mean square accuracy ranging between 84%-93%, indicating the ability to replicate expert valuation. Proposed strategies for automated TEE assessment can have a significant impact on the training process of new TEE operators, providing direct feedback and facilitating the development of the necessary dexterous skills.



### Skeletracks: automatic separation of overlapping fission tracks in apatite and muscovite using image processing
- **Arxiv ID**: http://arxiv.org/abs/1806.05199v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05199v2)
- **Published**: 2018-06-13 18:03:47+00:00
- **Updated**: 2018-12-18 20:59:40+00:00
- **Authors**: Alexandre Fioravante de Siqueira, Wagner Massayuki Nakasuga, Sandro Guedes
- **Comment**: 14 pages, 10 figures
- **Journal**: None
- **Summary**: One of the major difficulties of automatic track counting using photomicrographs is separating overlapped tracks. We address this issue combining image processing algorithms such as skeletonization, and we test our algorithm with several binarization techniques. The counting algorithm was successfully applied to determine the efficiency factor GQR, necessary for standardless fission-track dating, involving counting induced tracks in apatite and muscovite with superficial densities of about $6 \times 10^5$ tracks/$cm^2$.



### Impostor Networks for Fast Fine-Grained Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.05217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05217v1)
- **Published**: 2018-06-13 18:44:10+00:00
- **Updated**: 2018-06-13 18:44:10+00:00
- **Authors**: Vadim Lebedev, Artem Babenko, Victor Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we introduce impostor networks, an architecture that allows to perform fine-grained recognition with high accuracy and using a light-weight convolutional network, making it particularly suitable for fine-grained applications on low-power and non-GPU enabled platforms. Impostor networks compensate for the lightness of its `backend' network by combining it with a lightweight non-parametric classifier. The combination of a convolutional network and such non-parametric classifier is trained in an end-to-end fashion. Similarly to convolutional neural networks, impostor networks can fit large-scale training datasets very well, while also being able to generalize to new data points. At the same time, the bulk of computations within impostor networks happen through nearest neighbor search in high-dimensions. Such search can be performed efficiently on a variety of architectures including standard CPUs, where deep convolutional networks are inefficient. In a series of experiments with three fine-grained datasets, we show that impostor networks are able to boost the classification accuracy of a moderate-sized convolutional network considerably at a very small computational cost.



### Human Activity Recognition Based on Wearable Sensor Data: A Standardization of the State-of-the-Art
- **Arxiv ID**: http://arxiv.org/abs/1806.05226v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05226v3)
- **Published**: 2018-06-13 19:07:29+00:00
- **Updated**: 2019-02-01 17:59:54+00:00
- **Authors**: Artur Jordao, Antonio C. Nazare Jr., Jessica Sena, William Robson Schwartz
- **Comment**: None
- **Journal**: None
- **Summary**: Human activity recognition based on wearable sensor data has been an attractive research topic due to its application in areas such as healthcare and smart environments. In this context, many works have presented remarkable results using accelerometer, gyroscope and magnetometer data to represent the activities categories. However, current studies do not consider important issues that lead to skewed results, making it hard to assess the quality of sensor-based human activity recognition and preventing a direct comparison of previous works. These issues include the samples generation processes and the validation protocols used. We emphasize that in other research areas, such as image classification and object detection, these issues are already well-defined, which brings more efforts towards the application. Inspired by this, we conduct an extensive set of experiments that analyze different sample generation processes and validation protocols to indicate the vulnerable points in human activity recognition based on wearable sensor data. For this purpose, we implement and evaluate several top-performance methods, ranging from handcrafted-based approaches to convolutional neural networks. According to our study, most of the experimental evaluations that are currently employed are not adequate to perform the activity recognition in the context of wearable sensor data, in which the recognition accuracy drops considerably when compared to an appropriate evaluation approach. To the best of our knowledge, this is the first study that tackles essential issues that compromise the understanding of the performance in human activity recognition based on wearable sensor data.



### 3D-CODED : 3D Correspondences by Deep Deformation
- **Arxiv ID**: http://arxiv.org/abs/1806.05228v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05228v2)
- **Published**: 2018-06-13 19:07:37+00:00
- **Updated**: 2018-07-27 09:06:24+00:00
- **Authors**: Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new deep learning approach for matching deformable shapes by introducing {\it Shape Deformation Networks} which jointly encode 3D shapes and correspondences. This is achieved by factoring the surface representation into (i) a template, that parameterizes the surface, and (ii) a learnt global feature vector that parameterizes the transformation of the template into the input surface. By predicting this feature for a new shape, we implicitly predict correspondences between this shape and the template. We show that these correspondences can be improved by an additional step which improves the shape feature by minimizing the Chamfer distance between the input and transformed template. We demonstrate that our simple approach improves on state-of-the-art results on the difficult FAUST-inter challenge, with an average correspondence error of 2.88cm. We show, on the TOSCA dataset, that our method is robust to many types of perturbations, and generalizes to non-human shapes. This robustness allows it to perform well on real unclean, meshes from the the SCAPE dataset.



### Identifying Recurring Patterns with Deep Neural Networks for Natural Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1806.05229v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05229v3)
- **Published**: 2018-06-13 19:11:19+00:00
- **Updated**: 2019-12-10 17:36:26+00:00
- **Authors**: Zhihao Xia, Ayan Chakrabarti
- **Comment**: Project page at https://projects.ayanc.org/rpcnn/
- **Journal**: None
- **Summary**: Image denoising methods must effectively model, implicitly or explicitly, the vast diversity of patterns and textures that occur in natural images. This is challenging, even for modern methods that leverage deep neural networks trained to regress to clean images from noisy inputs. One recourse is to rely on "internal" image statistics, by searching for similar patterns within the input image itself. In this work, we propose a new method for natural image denoising that trains a deep neural network to determine whether patches in a noisy image input share common underlying patterns. Given a pair of noisy patches, our network predicts whether different sub-band coefficients of the original noise-free patches are similar. The denoising algorithm then aggregates matched coefficients to obtain an initial estimate of the clean image. Finally, this estimate is provided as input, along with the original noisy image, to a standard regression-based denoising network. Experiments show that our method achieves state-of-the-art color image denoising performance, including with a blind version that trains a common model for a range of noise levels, and does not require knowledge of level of noise in an input image. Our approach also has a distinct advantage when training with limited amounts of training data.



### End-to-End Parkinson Disease Diagnosis using Brain MR-Images by 3D-CNN
- **Arxiv ID**: http://arxiv.org/abs/1806.05233v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1806.05233v1)
- **Published**: 2018-06-13 19:23:51+00:00
- **Updated**: 2018-06-13 19:23:51+00:00
- **Authors**: Soheil Esmaeilzadeh, Yao Yang, Ehsan Adeli
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we use a deep learning framework for simultaneous classification and regression of Parkinson disease diagnosis based on MR-Images and personal information (i.e. age, gender). We intend to facilitate and increase the confidence in Parkinson disease diagnosis through our deep learning framework.



### Boosted Training of Convolutional Neural Networks for Multi-Class Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.05974v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05974v2)
- **Published**: 2018-06-13 19:42:20+00:00
- **Updated**: 2018-07-06 16:23:15+00:00
- **Authors**: Lorenz Berger, Eoin Hyde, Matt Gibb, Nevil Pavithran, Garin Kelly, Faiz Mumtaz, Sébastien Ourselin
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1709.02764
- **Journal**: None
- **Summary**: Training deep neural networks on large and sparse datasets is still challenging and can require large amounts of computation and memory. In this work, we address the task of performing semantic segmentation on large volumetric data sets, such as CT scans. Our contribution is threefold: 1) We propose a boosted sampling scheme that uses a-posterior error maps, generated throughout training, to focus sampling on difficult regions, resulting in a more informative loss. This results in a significant training speed up and improves learning performance for image segmentation. 2) We propose a novel algorithm for boosting the SGD learning rate schedule by adaptively increasing and lowering the learning rate, avoiding the need for extensive hyperparameter tuning. 3) We show that our method is able to attain new state-of-the-art results on the VISCERAL Anatomy benchmark.



### Finding your Lookalike: Measuring Face Similarity Rather than Face Identity
- **Arxiv ID**: http://arxiv.org/abs/1806.05252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05252v1)
- **Published**: 2018-06-13 20:17:53+00:00
- **Updated**: 2018-06-13 20:17:53+00:00
- **Authors**: Amir Sadovnik, Wassim Gharbi, Thanh Vu, Andrew Gallagher
- **Comment**: Accepted to the 1st CVPR Workshop on Visual Understanding of
  Subjective Attributes of Data 2018
- **Journal**: None
- **Summary**: Face images are one of the main areas of focus for computer vision, receiving on a wide variety of tasks. Although face recognition is probably the most widely researched, many other tasks such as kinship detection, facial expression classification and facial aging have been examined. In this work we propose the new, subjective task of quantifying perceived face similarity between a pair of faces. That is, we predict the perceived similarity between facial images, given that they are not of the same person. Although this task is clearly correlated with face recognition, it is different and therefore justifies a separate investigation. Humans often remark that two persons look alike, even in cases where the persons are not actually confused with one another. In addition, because face similarity is different than traditional image similarity, there are challenges in data collection and labeling, and dealing with diverging subjective opinions between human labelers. We present evidence that finding facial look-alikes and recognizing faces are two distinct tasks. We propose a new dataset for facial similarity and introduce the Lookalike network, directed towards similar face classification, which outperforms the ad hoc usage of a face recognition network directed at the same task.



### Online Self-supervised Scene Segmentation for Micro Aerial Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1806.05269v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.05269v1)
- **Published**: 2018-06-13 21:18:37+00:00
- **Updated**: 2018-06-13 21:18:37+00:00
- **Authors**: Shreyansh Daftry, Yashasvi Agrawal, Larry Matthies
- **Comment**: None
- **Journal**: IEEE International Conference on Robotics and Automation (ICRA)
  2018 Workshop on Representing a Complex World
- **Summary**: Recently, there have been numerous advances in the development of payload and power constrained lightweight Micro Aerial Vehicles (MAVs). As these robots aspire for high-speed autonomous flights in complex dynamic environments, robust scene understanding at long-range becomes critical. The problem is heavily characterized by either the limitations imposed by sensor capabilities for geometry-based methods, or the need for large-amounts of manually annotated training data required by data-driven methods. This motivates the need to build systems that have the capability to alleviate these problems by exploiting the complimentary strengths of both geometry and data-driven methods. In this paper, we take a step in this direction and propose a generic framework for adaptive scene segmentation using self-supervised online learning. We present this in the context of vision-based autonomous MAV flight, and demonstrate the efficacy of our proposed system through extensive experiments on benchmark datasets and real-world field tests.



### Benchmarks for Image Classification and Other High-dimensional Pattern Recognition Problems
- **Arxiv ID**: http://arxiv.org/abs/1806.05272v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.05272v1)
- **Published**: 2018-06-13 21:22:30+00:00
- **Updated**: 2018-06-13 21:22:30+00:00
- **Authors**: Tarun Yellamraju, Jonas Hepp, Mireille Boutin
- **Comment**: None
- **Journal**: None
- **Summary**: A good classification method should yield more accurate results than simple heuristics. But there are classification problems, especially high-dimensional ones like the ones based on image/video data, for which simple heuristics can work quite accurately; the structure of the data in such problems is easy to uncover without any sophisticated or computationally expensive method. On the other hand, some problems have a structure that can only be found with sophisticated pattern recognition methods. We are interested in quantifying the difficulty of a given high-dimensional pattern recognition problem. We consider the case where the patterns come from two pre-determined classes and where the objects are represented by points in a high-dimensional vector space. However, the framework we propose is extendable to an arbitrarily large number of classes. We propose classification benchmarks based on simple random projection heuristics. Our benchmarks are 2D curves parameterized by the classification error and computational cost of these simple heuristics. Each curve divides the plane into a "positive- gain" and a "negative-gain" region. The latter contains methods that are ill-suited for the given classification problem. The former is divided into two by the curve asymptote; methods that lie in the small region under the curve but right of the asymptote merely provide a computational gain but no structural advantage over the random heuristics. We prove that the curve asymptotes are optimal (i.e. at Bayes error) in some cases, and thus no sophisticated method can provide a structural advantage over the random heuristics. Such classification problems, an example of which we present in our numerical experiments, provide poor ground for testing new pattern classification methods.



### A Flexible Convolutional Solver with Application to Photorealistic Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1806.05285v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05285v1)
- **Published**: 2018-06-13 22:05:35+00:00
- **Updated**: 2018-06-13 22:05:35+00:00
- **Authors**: Gilles Puy, Patrick Pérez
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new flexible deep convolutional neural network (convnet) to perform fast visual style transfer. In contrast to existing convnets that address the same task, our architecture derives directly from the structure of the gradient descent originally used to solve the style transfer problem [Gatys et al., 2016]. Like existing convnets, ours approximately solves the original problem much faster than the gradient descent. However, our network is uniquely flexible by design: it can be manipulated at runtime to enforce new constraints on the final solution. In particular, we show how to modify it to obtain a photorealistic result with no retraining. We study the modifications made by [Luan et al., 2017] to the original cost function of [Gatys et al., 2016] to achieve photorealistic style transfer. These modifications affect directly the gradient descent and can be reported on-the-fly in our network. These modifications are possible as the proposed architecture stems from unrolling the gradient descent.



### Geometric Shape Features Extraction Using a Steady State Partial Differential Equation System
- **Arxiv ID**: http://arxiv.org/abs/1806.05299v3
- **DOI**: 10.1016/j.jcde.2019.03.006
- **Categories**: **cs.CV**, cs.AI, cs.GR, math.AP, 35Q68, 68T10, 65D18, 68U05, 51N05
- **Links**: [PDF](http://arxiv.org/pdf/1806.05299v3)
- **Published**: 2018-06-13 23:33:08+00:00
- **Updated**: 2019-04-13 22:57:30+00:00
- **Authors**: Takayuki Yamada
- **Comment**: 31 pages, 10 figures
- **Journal**: Journal of Computational Design and Engineering, 2019
- **Summary**: A unified method for extracting geometric shape features from binary image data using a steady state partial differential equation (PDE) system as a boundary value problem is presented in this paper. The PDE and functions are formulated to extract the thickness, orientation, and skeleton simultaneously. The main advantages of the proposed method is that the orientation is defined without derivatives and thickness computation is not imposed a topological constraint on the target shape. A one-dimensional analytical solution is provided to validate the proposed method. In addition, two-dimensional numerical examples are presented to confirm the usefulness of the proposed method.



