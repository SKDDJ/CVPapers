# Arxiv Papers in cs.CV on 2018-06-28
### Deep CNN Denoiser and Multi-layer Neighbor Component Embedding for Face Hallucination
- **Arxiv ID**: http://arxiv.org/abs/1806.10726v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10726v1)
- **Published**: 2018-06-28 01:00:09+00:00
- **Updated**: 2018-06-28 01:00:09+00:00
- **Authors**: Junjun Jiang, Yi Yu, Jinhui Hu, Suhua Tang, Jiayi Ma
- **Comment**: Accepted by IJCAI 2018
- **Journal**: None
- **Summary**: Most of the current face hallucination methods, whether they are shallow learning-based or deep learning-based, all try to learn a relationship model between Low-Resolution (LR) and High-Resolution (HR) spaces with the help of a training set. They mainly focus on modeling image prior through either model-based optimization or discriminative inference learning. However, when the input LR face is tiny, the learned prior knowledge is no longer effective and their performance will drop sharply. To solve this problem, in this paper we propose a general face hallucination method that can integrate model-based optimization and discriminative inference. In particular, to exploit the model based prior, the Deep Convolutional Neural Networks (CNN) denoiser prior is plugged into the super-resolution optimization model with the aid of image-adaptive Laplacian regularization. Additionally, we further develop a high-frequency details compensation method by dividing the face image to facial components and performing face hallucination in a multi-layer neighbor embedding manner. Experiments demonstrate that the proposed method can achieve promising super-resolution results for tiny input LR faces.



### Two-layer Lossless HDR Coding considering Histogram Sparseness with Backward Compatibility to JPEG
- **Arxiv ID**: http://arxiv.org/abs/1806.10746v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10746v1)
- **Published**: 2018-06-28 02:53:11+00:00
- **Updated**: 2018-06-28 02:53:11+00:00
- **Authors**: Osamu Watanabe, Hiroyuki Kobayashi, Hitoshi Kiya
- **Comment**: to appear in Proc. Picture Coding Symposium, San Francisco, USA, 25th
  June, 2018
- **Journal**: None
- **Summary**: An efficient two-layer coding method using the histogram packing technique with the backward compatibility to the legacy JPEG is proposed in this paper. The JPEG XT, which is the international standard to compress HDR images, adopts two-layer coding scheme for backward compatibility to the legacy JPEG. However, this two-layer coding structure does not give better lossless performance than the other existing single-layer coding methods for HDR images. Moreover, the JPEG XT has problems on determination of the lossless coding parameters; Finding appropriate combination of the parameter values is necessary to achieve good lossless performance. The histogram sparseness of HDR images is discussed and it is pointed out that the histogram packing technique considering the sparseness is able to improve the performance of lossless compression for HDR images and a novel two-layer coding with the histogram packing technique is proposed. The experimental results demonstrate that not only the proposed method has a better lossless compression performance than that of the JPEG XT, but also there is no need to determine image-dependent parameter values for good compression performance in spite of having the backward compatibility to the well known legacy JPEG standard.



### Towards automatic initialization of registration algorithms using simulated endoscopy images
- **Arxiv ID**: http://arxiv.org/abs/1806.10748v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, J.2; J.3; I.2.6; I.2.10; I.3.3; I.3.7
- **Links**: [PDF](http://arxiv.org/pdf/1806.10748v1)
- **Published**: 2018-06-28 02:58:25+00:00
- **Updated**: 2018-06-28 02:58:25+00:00
- **Authors**: Ayushi Sinha, Masaru Ishii, Russell H. Taylor, Gregory D. Hager, Austin Reiter
- **Comment**: 4 pages, 4 figures
- **Journal**: None
- **Summary**: Registering images from different modalities is an active area of research in computer aided medical interventions. Several registration algorithms have been developed, many of which achieve high accuracy. However, these results are dependent on many factors, including the quality of the extracted features or segmentations being registered as well as the initial alignment. Although several methods have been developed towards improving segmentation algorithms and automating the segmentation process, few automatic initialization algorithms have been explored. In many cases, the initial alignment from which a registration is initiated is performed manually, which interferes with the clinical workflow. Our aim is to use scene classification in endoscopic procedures to achieve coarse alignment of the endoscope and a preoperative image of the anatomy. In this paper, we show using simulated scenes that a neural network can predict the region of anatomy (with respect to a preoperative image) that the endoscope is located in by observing a single endoscopic video frame. With limited training and without any hyperparameter tuning, our method achieves an accuracy of 76.53 (+/-1.19)%. There are several avenues for improvement, making this a promising direction of research. Code is available at https://github.com/AyushiSinha/AutoInitialization.



### State-aware Anti-drift Robust Correlation Tracking
- **Arxiv ID**: http://arxiv.org/abs/1806.10759v1
- **DOI**: 10.1109/TIP.2019.2905984
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10759v1)
- **Published**: 2018-06-28 03:58:44+00:00
- **Updated**: 2018-06-28 03:58:44+00:00
- **Authors**: Yuqi Han, Chenwei Deng, Zengshuo Zhang, Jinghong Nan, Baojun Zhao
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: Correlation filter (CF) based trackers have aroused increasing attentions in visual tracking field due to the superior performance on several datasets while maintaining high running speed. For each frame, an ideal filter is trained in order to discriminate the target from its surrounding background. Considering that the target always undergoes external and internal interference during tracking procedure, the trained filter should take consideration of not only the external distractions but also the target appearance variation synchronously. To this end, we present a State-aware Anti-drift Tracker (SAT) in this paper, which jointly model the discrimination and reliability information in filter learning. Specifically, global context patches are incorporated into filter training stage to better distinguish the target from backgrounds. Meanwhile, a color-based reliable mask is learned to encourage the filter to focus on more reliable regions suitable for tracking. We show that the proposed optimization problem could be efficiently solved using Alternative Direction Method of Multipliers and fully carried out in Fourier domain. Extensive experiments are conducted on OTB-100 datasets to compare the SAT tracker (both hand-crafted feature and CNN feature) with other relevant state-of-the-art methods. Both quantitative and qualitative evaluations further demonstrate the effectiveness and robustness of the proposed work.



### Differentiable Learning-to-Normalize via Switchable Normalization
- **Arxiv ID**: http://arxiv.org/abs/1806.10779v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.10779v5)
- **Published**: 2018-06-28 05:55:57+00:00
- **Updated**: 2019-04-24 05:31:23+00:00
- **Authors**: Ping Luo, Jiamin Ren, Zhanglin Peng, Ruimao Zhang, Jingyu Li
- **Comment**: International Conference on Learning Representations (ICLR)
- **Journal**: None
- **Summary**: We address a learning-to-normalize problem by proposing Switchable Normalization (SN), which learns to select different normalizers for different normalization layers of a deep neural network. SN employs three distinct scopes to compute statistics (means and variances) including a channel, a layer, and a minibatch. SN switches between them by learning their importance weights in an end-to-end manner. It has several good properties. First, it adapts to various network architectures and tasks (see Fig.1). Second, it is robust to a wide range of batch sizes, maintaining high performance even when small minibatch is presented (e.g. 2 images/GPU). Third, SN does not have sensitive hyper-parameter, unlike group normalization that searches the number of groups as a hyper-parameter. Without bells and whistles, SN outperforms its counterparts on various challenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, and Kinetics. Analyses of SN are also presented. We hope SN will help ease the usage and understand the normalization techniques in deep learning. The code of SN has been made available in https://github.com/switchablenorms/.



### Accurate and efficient video de-fencing using convolutional neural networks and temporal information
- **Arxiv ID**: http://arxiv.org/abs/1806.10781v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1806.10781v1)
- **Published**: 2018-06-28 05:59:56+00:00
- **Updated**: 2018-06-28 05:59:56+00:00
- **Authors**: Chen Du, Byeongkeun Kang, Zheng Xu, Ji Dai, Truong Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: De-fencing is to eliminate the captured fence on an image or a video, providing a clear view of the scene. It has been applied for many purposes including assisting photographers and improving the performance of computer vision algorithms such as object detection and recognition. However, the state-of-the-art de-fencing methods have limited performance caused by the difficulty of fence segmentation and also suffer from the motion of the camera or objects. To overcome these problems, we propose a novel method consisting of segmentation using convolutional neural networks and a fast/robust recovery algorithm. The segmentation algorithm using convolutional neural network achieves significant improvement in the accuracy of fence segmentation. The recovery algorithm using optical flow produces plausible de-fenced images and videos. The proposed method is experimented on both our diverse and complex dataset and publicly available datasets. The experimental results demonstrate that the proposed method achieves the state-of-the-art performance for both segmentation and content recovery.



### How To Extract Fashion Trends From Social Media? A Robust Object Detector With Support For Unsupervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.10787v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.10787v1)
- **Published**: 2018-06-28 06:23:56+00:00
- **Updated**: 2018-06-28 06:23:56+00:00
- **Authors**: Vijay Gabale, Anand Prabhu Subramanian
- **Comment**: 6 pages, 3 figures, AI for Fashion, KDD 2018
- **Journal**: None
- **Summary**: With the proliferation of social media, fashion inspired from celebrities, reputed designers as well as fashion influencers has shortened the cycle of fashion design and manufacturing. However, with the explosion of fashion related content and large number of user generated fashion photos, it is an arduous task for fashion designers to wade through social media photos and create a digest of trending fashion. This necessitates deep parsing of fashion photos on social media to localize and classify multiple fashion items from a given fashion photo. While object detection competitions such as MSCOCO have thousands of samples for each of the object categories, it is quite difficult to get large labeled datasets for fast fashion items. Moreover, state-of-the-art object detectors do not have any functionality to ingest large amount of unlabeled data available on social media in order to fine tune object detectors with labeled datasets. In this work, we show application of a generic object detector, that can be pretrained in an unsupervised manner, on 24 categories from recently released Open Images V4 dataset. We first train the base architecture of the object detector using unsupervisd learning on 60K unlabeled photos from 24 categories gathered from social media, and then subsequently fine tune it on 8.2K labeled photos from Open Images V4 dataset. On 300 X 300 image inputs, we achieve 72.7% mAP on a test dataset of 2.4K photos while performing 11% to 17% better as compared to the state-of-the-art object detectors. We show that this improvement is due to our choice of architecture that lets us do unsupervised learning and that performs significantly better in identifying small objects.



### Beyond One-hot Encoding: lower dimensional target embedding
- **Arxiv ID**: http://arxiv.org/abs/1806.10805v1
- **DOI**: 10.1016/j.imavis.2018.04.004
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1806.10805v1)
- **Published**: 2018-06-28 07:34:14+00:00
- **Updated**: 2018-06-28 07:34:14+00:00
- **Authors**: Pau Rodríguez, Miguel A. Bautista, Jordi Gonzàlez, Sergio Escalera
- **Comment**: Published at Image and Vision Computing
- **Journal**: None
- **Summary**: Target encoding plays a central role when learning Convolutional Neural Networks. In this realm, One-hot encoding is the most prevalent strategy due to its simplicity. However, this so widespread encoding schema assumes a flat label space, thus ignoring rich relationships existing among labels that can be exploited during training. In large-scale datasets, data does not span the full label space, but instead lies in a low-dimensional output manifold. Following this observation, we embed the targets into a low-dimensional space, drastically improving convergence speed while preserving accuracy. Our contribution is two fold: (i) We show that random projections of the label space are a valid tool to find such lower dimensional embeddings, boosting dramatically convergence rates at zero computational cost; and (ii) we propose a normalized eigenrepresentation of the class manifold that encodes the targets with minimal information loss, improving the accuracy of random projections encoding while enjoying the same convergence rates. Experiments on CIFAR-100, CUB200-2011, Imagenet, and MIT Places demonstrate that the proposed approach drastically improves convergence speed while reaching very competitive accuracy rates.



### Automatic Rank Selection for High-Speed Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1806.10821v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10821v2)
- **Published**: 2018-06-28 08:25:40+00:00
- **Updated**: 2018-06-29 10:03:22+00:00
- **Authors**: Hyeji Kim, Chong-Min Kyung
- **Comment**: This idea was submitted to CVPR 2018 (Nov. 2017)
- **Journal**: None
- **Summary**: Low-rank decomposition plays a central role in accelerating convolutional neural network (CNN), and the rank of decomposed kernel-tensor is a key parameter that determines the complexity and accuracy of a neural network. In this paper, we define rank selection as a combinatorial optimization problem and propose a methodology to minimize network complexity while maintaining the desired accuracy. Combinatorial optimization is not feasible due to search space limitations. To restrict the search space and obtain the optimal rank, we define the space constraint parameters with a boundary condition. We also propose a linearly-approximated accuracy function to predict the fine-tuned accuracy of the optimized CNN model during the cost reduction. Experimental results on AlexNet and VGG-16 show that the proposed rank selection algorithm satisfies the accuracy constraint. Our method combined with truncated-SVD outperforms state-of-the-art methods in terms of inference and training time at almost the same accuracy.



### Grassmannian Discriminant Maps (GDM) for Manifold Dimensionality Reduction with Application to Image Set Classification
- **Arxiv ID**: http://arxiv.org/abs/1806.10830v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10830v2)
- **Published**: 2018-06-28 08:50:24+00:00
- **Updated**: 2022-01-22 07:31:34+00:00
- **Authors**: Rui Wang, Xiao-Jun Wu, Kai-Xuan Chen, Josef Kittler
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: In image set classification, a considerable progress has been made by representing original image sets on Grassmann manifolds. In order to extend the advantages of the Euclidean based dimensionality reduction methods to the Grassmann Manifold, several methods have been suggested recently which jointly perform dimensionality reduction and metric learning on Grassmann manifold to improve performance. Nevertheless, when applied to complex datasets, the learned features do not exhibit enough discriminatory power. To overcome this problem, we propose a new method named Grassmannian Discriminant Maps (GDM) for manifold dimensionality reduction problems. The core of the method is a new discriminant function for metric learning and dimensionality reduction. For comparison and better understanding, we also study a simple variations to GDM. The key difference between them is the discriminant function. We experiment on data sets corresponding to three tasks: face recognition, object categorization, and hand gesture recognition to evaluate the proposed method and its simple extensions. Compared with the state of the art, the results achieved show the effectiveness of the proposed algorithm.



### CT Image Registration in Acute Stroke Monitoring
- **Arxiv ID**: http://arxiv.org/abs/1806.10836v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1806.10836v1)
- **Published**: 2018-06-28 09:11:57+00:00
- **Updated**: 2018-06-28 09:11:57+00:00
- **Authors**: Lucio Amelio, Alessia Amelio
- **Comment**: 10 pages, 9 figures, Accepted at the 41th Jubilee International
  Convention on Information and Communication Technology, Electronics and
  Microelectronics (MIPRO), Opatija, Croatia
- **Journal**: None
- **Summary**: We present a new system based on tracking the temporal evolution of stroke lesions using an image registration technique on CT exams of the patient's brain. The system is able to compare past CT exams with the most recent one related to stroke event in order to evaluate past lesions which are not related to stroke. Then, it can compare recent CT exams related to the current stroke for assessing the evolution of the lesion over time. A new similarity measure is also introduced for the comparison of the source and target images during image registration. It will result in a cheaper, faster and more accessible evaluation of the acute phase of the stroke overcoming the current limitations of the proposed systems in the state-of-the-art.



### DeepSDCS: Dissecting cancer proliferation heterogeneity in Ki67 digital whole slide images
- **Arxiv ID**: http://arxiv.org/abs/1806.10850v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10850v1)
- **Published**: 2018-06-28 09:43:07+00:00
- **Updated**: 2018-06-28 09:43:07+00:00
- **Authors**: Priya Lakshmi Narayanan, Shan E Ahmed Raza, Andrew Dodson, Barry Gusterson, Mitchell Dowsett, Yinyin Yuan
- **Comment**: None
- **Journal**: MIDL 2018 Conference
- **Summary**: Ki67 is an important biomarker for breast cancer. Classification of positive and negative Ki67 cells in histology slides is a common approach to determine cancer proliferation status. However, there is a lack of generalizable and accurate methods to automate Ki67 scoring in large-scale patient cohorts. In this work, we have employed a novel deep learning technique based on hypercolumn descriptors for cell classification in Ki67 images. Specifically, we developed the Simultaneous Detection and Cell Segmentation (DeepSDCS) network to perform cell segmentation and detection. VGG16 network was used for the training and fine tuning to training data. We extracted the hypercolumn descriptors of each cell to form the vector of activation from specific layers to capture features at different granularity. Features from these layers that correspond to the same pixel were propagated using a stochastic gradient descent optimizer to yield the detection of the nuclei and the final cell segmentations. Subsequently, seeds generated from cell segmentation were propagated to a spatially constrained convolutional neural network for the classification of the cells into stromal, lymphocyte, Ki67-positive cancer cell, and Ki67-negative cancer cell. We validated its accuracy in the context of a large-scale clinical trial of oestrogen-receptor-positive breast cancer. We achieved 99.06% and 89.59% accuracy on two separate test sets of Ki67 stained breast cancer dataset comprising biopsy and whole-slide images.



### Expolring Architectures for CNN-Based Word Spotting
- **Arxiv ID**: http://arxiv.org/abs/1806.10866v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10866v1)
- **Published**: 2018-06-28 10:20:41+00:00
- **Updated**: 2018-06-28 10:20:41+00:00
- **Authors**: Eugen Rusakov, Sebastian Sudholt, Fabian Wolf, Gernot A. Fink
- **Comment**: None
- **Journal**: None
- **Summary**: The goal in word spotting is to retrieve parts of document images which are relevant with respect to a certain user-defined query. The recent past has seen attribute-based Convolutional Neural Networks take over this field of research. As is common for other fields of computer vision, the CNNs used for this task are already considerably deep. The question that arises, however, is: How complex does a CNN have to be for word spotting? Are increasingly deeper models giving increasingly bet- ter results or does performance behave asymptotically for these architectures? On the other hand, can similar results be obtained with a much smaller CNN? The goal of this paper is to give an answer to these questions. Therefore, the recently successful TPP- PHOCNet will be compared to a Residual Network, a Densely Connected Convolutional Network and a LeNet architecture empirically. As will be seen in the evaluation, a complex model can be beneficial for word spotting on harder tasks such as the IAM Offline Database but gives no advantage for easier benchmarks such as the George Washington Database.



### Efficient CNN Implementation for Eye-Gaze Estimation on Low-Power/Low-Quality Consumer Imaging Systems
- **Arxiv ID**: http://arxiv.org/abs/1806.10890v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10890v1)
- **Published**: 2018-06-28 11:33:03+00:00
- **Updated**: 2018-06-28 11:33:03+00:00
- **Authors**: Joseph Lemley, Anuradha Kar, Alexandru Drimbarean, Peter Corcoran
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate and efficient eye gaze estimation is important for emerging consumer electronic systems such as driver monitoring systems and novel user interfaces. Such systems are required to operate reliably in difficult, unconstrained environments with low power consumption and at minimal cost. In this paper a new hardware friendly, convolutional neural network model with minimal computational requirements is introduced and assessed for efficient appearance-based gaze estimation. The model is tested and compared against existing appearance based CNN approaches, achieving better eye gaze accuracy with significantly fewer computational requirements. A brief updated literature review is also provided.



### Deep learning for dehazing: Comparison and analysis
- **Arxiv ID**: http://arxiv.org/abs/1806.10923v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1806.10923v1)
- **Published**: 2018-06-28 12:37:54+00:00
- **Updated**: 2018-06-28 12:37:54+00:00
- **Authors**: A Benoit, Leonel Cuevas, Jean-Baptiste Thomas
- **Comment**: None
- **Journal**: Colour and Visual Computing Symposium (CVCS), Sep 2018,
  Gj{{\o}}vik, Norway
- **Summary**: We compare a recent dehazing method based on deep learning, Dehazenet, with traditional state-of-the-art approaches , on benchmark data with reference. Dehazenet estimates the depth map from transmission factor on a single color image, which is used to inverse the Koschmieder model of imaging in the presence of haze. In this sense, the solution is still attached to the Koschmieder model. We demonstrate that the transmission is very well estimated by the network, but also that this method exhibits the same limitation than others due to the use of the same imaging model.



### High Diversity Attribute Guided Face Generation with GANs
- **Arxiv ID**: http://arxiv.org/abs/1806.10982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10982v1)
- **Published**: 2018-06-28 14:02:21+00:00
- **Updated**: 2018-06-28 14:02:21+00:00
- **Authors**: Evgeny Izutov
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we focused on GAN-based solution for the attribute guided face synthesis. Previous works exploited GANs for generation of photo-realistic face images and did not pay attention to the question of diversity of the resulting images. The proposed solution in its turn introducing novel latent space of unit complex numbers is able to provide the diversity on the "birthday paradox" score 3 times higher than the size of the training dataset. It is important to emphasize that our result is shown on relatively small dataset (20k samples vs 200k) while preserving photo-realistic properties of generated faces on significantly higher resolution (128x128 in comparison to 32x32 of previous works).



### Modeling Spatio-Temporal Human Track Structure for Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1806.11008v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11008v1)
- **Published**: 2018-06-28 14:34:00+00:00
- **Updated**: 2018-06-28 14:34:00+00:00
- **Authors**: Guilhem Chéron, Anton Osokin, Ivan Laptev, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses spatio-temporal localization of human actions in video. In order to localize actions in time, we propose a recurrent localization network (RecLNet) designed to model the temporal structure of actions on the level of person tracks. Our model is trained to simultaneously recognize and localize action classes in time and is based on two layer gated recurrent units (GRU) applied separately to two streams, i.e. appearance and optical flow streams. When used together with state-of-the-art person detection and tracking, our model is shown to improve substantially spatio-temporal action localization in videos. The gain is shown to be mainly due to improved temporal localization. We evaluate our method on two recent datasets for spatio-temporal action localization, UCF101-24 and DALY, demonstrating a significant improvement of the state of the art.



### Robust pose tracking with a joint model of appearance and shape
- **Arxiv ID**: http://arxiv.org/abs/1806.11011v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11011v1)
- **Published**: 2018-06-28 14:37:32+00:00
- **Updated**: 2018-06-28 14:37:32+00:00
- **Authors**: Yuliang Guo, Lakshmi Narasimhan Govindarajan, Benjamin Kimia, Thomas Serre
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach for estimating the 2D pose of an articulated object with an application to automated video analysis of small laboratory animals. We have found that deformable part models developed for humans, exemplified by the flexible mixture of parts (FMP) model, typically fail on challenging animal poses. We argue that beyond encoding appearance and spatial relations, shape is needed to overcome the lack of distinctive landmarks on laboratory animal bodies. In our approach, a shape consistent FMP (scFMP) model computes promising pose candidates after a standard FMP model is used to rapidly discard false part detections. This "cascaded" approach combines the relative strengths of spatial-relations, appearance and shape representations and is shown to yield significant improvements over the original FMP model as well as a representative deep neural network baseline.



### Deep Semi Supervised Generative Learning for Automated PD-L1 Tumor Cell Scoring on NSCLC Tissue Needle Biopsies
- **Arxiv ID**: http://arxiv.org/abs/1806.11036v1
- **DOI**: 10.1038/s41598-018-35501-5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11036v1)
- **Published**: 2018-06-28 15:30:29+00:00
- **Updated**: 2018-06-28 15:30:29+00:00
- **Authors**: Ansh Kapil, Armin Meier, Aleksandra Zuraw, Keith Steele, Marlon Rebelatto, Günter Schmidt, Nicolas Brieu
- **Comment**: 10 pages, 7 figures, 3 tables, submited to Scientific Reports on 28
  June 2018
- **Journal**: Scientific Reports, volume 8, Article number: 17343 (2018)
- **Summary**: The level of PD-L1 expression in immunohistochemistry (IHC) assays is a key biomarker for the identification of Non-Small-Cell-Lung-Cancer (NSCLC) patients that may respond to anti PD-1/PD-L1 treatments. The quantification of PD-L1 expression currently includes the visual estimation of a Tumor Cell (TC) score by a pathologist and consists of evaluating the ratio of PD-L1 positive and PD-L1 negative tumor cells. Known challenges like differences in positivity estimation around clinically relevant cut-offs and sub-optimal quality of samples makes visual scoring tedious and subjective, yielding a scoring variability between pathologists. In this work, we propose a novel deep learning solution that enables the first automated and objective scoring of PD-L1 expression in late stage NSCLC needle biopsies. To account for the low amount of tissue available in biopsy images and to restrict the amount of manual annotations necessary for training, we explore the use of semi-supervised approaches against standard fully supervised methods. We consolidate the manual annotations used for training as well the visual TC scores used for quantitative evaluation with multiple pathologists. Concordance measures computed on a set of slides unseen during training provide evidence that our automatic scoring method matches visual scoring on the considered dataset while ensuring repeatability and objectivity.



### A probabilistic constrained clustering for transfer learning and image category discovery
- **Arxiv ID**: http://arxiv.org/abs/1806.11078v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.11078v1)
- **Published**: 2018-06-28 16:49:19+00:00
- **Updated**: 2018-06-28 16:49:19+00:00
- **Authors**: Yen-Chang Hsu, Zhaoyang Lv, Joel Schlosser, Phillip Odom, Zsolt Kira
- **Comment**: CVPR 2018 Deep-Vision Workshop
- **Journal**: None
- **Summary**: Neural network-based clustering has recently gained popularity, and in particular a constrained clustering formulation has been proposed to perform transfer learning and image category discovery using deep learning. The core idea is to formulate a clustering objective with pairwise constraints that can be used to train a deep clustering network; therefore the cluster assignments and their underlying feature representations are jointly optimized end-to-end. In this work, we provide a novel clustering formulation to address scalability issues of previous work in terms of optimizing deeper networks and larger amounts of categories. The proposed objective directly minimizes the negative log-likelihood of cluster assignment with respect to the pairwise constraints, has no hyper-parameters, and demonstrates improved scalability and performance on both supervised learning and unsupervised transfer learning.



### Unsupervised Natural Image Patch Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.03130v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03130v1)
- **Published**: 2018-06-28 18:21:43+00:00
- **Updated**: 2018-06-28 18:21:43+00:00
- **Authors**: Dov Danon, Hadar Averbuch-Elor, Ohad Fried, Daniel Cohen-Or
- **Comment**: None
- **Journal**: None
- **Summary**: Learning a metric of natural image patches is an important tool for analyzing images. An efficient means is to train a deep network to map an image patch to a vector space, in which the Euclidean distance reflects patch similarity. Previous attempts learned such an embedding in a supervised manner, requiring the availability of many annotated images. In this paper, we present an unsupervised embedding of natural image patches, avoiding the need for annotated images. The key idea is that the similarity of two patches can be learned from the prevalence of their spatial proximity in natural images. Clearly, relying on this simple principle, many spatially nearby pairs are outliers, however, as we show, the outliers do not harm the convergence of the metric learning. We show that our unsupervised embedding approach is more effective than a supervised one or one that uses deep patch representations. Moreover, we show that it naturally leads itself to an efficient self-supervised domain adaptation technique onto a target domain that contains a common foreground object.



### Deep Learning Based Instance Segmentation in 3D Biomedical Images Using Weak Annotation
- **Arxiv ID**: http://arxiv.org/abs/1806.11137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11137v1)
- **Published**: 2018-06-28 18:22:52+00:00
- **Updated**: 2018-06-28 18:22:52+00:00
- **Authors**: Zhuo Zhao, Lin Yang, Hao Zheng, Ian H. Guldner, Siyuan Zhang, Danny Z. Chen
- **Comment**: Accepted by MICCAI 2018
- **Journal**: None
- **Summary**: Instance segmentation in 3D images is a fundamental task in biomedical image analysis. While deep learning models often work well for 2D instance segmentation, 3D instance segmentation still faces critical challenges, such as insufficient training data due to various annotation difficulties in 3D biomedical images. Common 3D annotation methods (e.g., full voxel annotation) incur high workloads and costs for labeling enough instances for training deep learning 3D instance segmentation models. In this paper, we propose a new weak annotation approach for training a fast deep learning 3D instance segmentation model without using full voxel mask annotation. Our approach needs only 3D bounding boxes for all instances and full voxel annotation for a small fraction of the instances, and uses a novel two-stage 3D instance segmentation model utilizing these two kinds of annotation, respectively. We evaluate our approach on several biomedical image datasets, and the experimental results show that (1) with full annotated boxes and a small amount of masks, our approach can achieve similar performance as the best known methods using full annotation, and (2) with similar annotation time, our approach outperforms the best known methods that use full annotation.



### Adversarial Reprogramming of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.11146v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.11146v2)
- **Published**: 2018-06-28 19:06:26+00:00
- **Updated**: 2018-11-29 22:50:01+00:00
- **Authors**: Gamaleldin F. Elsayed, Ian Goodfellow, Jascha Sohl-Dickstein
- **Comment**: None
- **Journal**: International Conference on Learning Representations 2019
- **Summary**: Deep neural networks are susceptible to \emph{adversarial} attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduce attacks that instead {\em reprogram} the target model to perform a task chosen by the attacker---without the attacker needing to specify or compute the desired output for each test-time input. This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary---even if the model was not trained to do this task. These perturbations can thus be considered a program for the new task. We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model.



### Ballistocardiogram-based Authentication using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.03216v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, q-bio.QM, 68U35
- **Links**: [PDF](http://arxiv.org/pdf/1807.03216v1)
- **Published**: 2018-06-28 19:11:19+00:00
- **Updated**: 2018-06-28 19:11:19+00:00
- **Authors**: Joshua Hebert, Brittany Lewis, Hang Cai, Krishna K. Venkatasubramanian, Matthew Provost, Kelly Charlebois
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: The goal of this work is to demonstrate the use of the ballistocardiogram (BCG) signal, derived using head-mounted wearable devices, as a viable biometric for authentication. The BCG signal is the measure of an person's body acceleration as a result of the heart's ejection of blood. It is a characterization of the cardiac cycle and can be derived non-invasively from the measurement of subtle movements of a person's extremities. In this paper, we use several versions of the BCG signal, derived from accelerometer and gyroscope sensors on a Smart Eyewear (SEW) device, for authentication. The derived BCG signals are used to train a convolutional neural network (CNN) as an authentication model, which is personalized for each subject. We evaluate our authentication models using data from 12 subjects and show that our approach has an equal error rate (EER) of 3.5% immediately after training and 13\% after about 2 months, in the worst case. We also explore the use of our authentication approach for people with motor disabilities. Our analysis using a separate dataset of 6 subjects with non-spastic cerebral palsy shows an EER of 11.2% immediately after training and 21.6% after about 2 months, in the worst-case.



### 3D Normal Coordinate Systems for Cortical Areas
- **Arxiv ID**: http://arxiv.org/abs/1806.11169v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11169v3)
- **Published**: 2018-06-28 20:16:57+00:00
- **Updated**: 2019-07-13 16:04:29+00:00
- **Authors**: J. Tilak Ratnanather, Sylvain Arguillère, Kwame S. Kutten, Peter Hubka, Andrej Kral, Laurent Younes
- **Comment**: None
- **Journal**: None
- **Summary**: A surface-based diffeomorphic algorithm to generate 3D coordinate grids in the cortical ribbon is described. In the grid, normal coordinate lines are generated by the diffeomorphic evolution from the grey/white (inner) surface to the grey/csf (outer) surface. Specifically, the cortical ribbon is described by two triangulated surfaces with open boundaries. Conceptually, the inner surface sits on top of the white matter structure and the outer on top of the gray matter. It is assumed that the cortical ribbon consists of cortical columns which are orthogonal to the white matter surface. This might be viewed as a consequence of the development of the columns in the embryo. It is also assumed that the columns are orthogonal to the outer surface so that the resultant vector field is orthogonal to the evolving surface. Then the distance of the normal lines from the vector field such that the inner surface evolves diffeomorphically towards the outer one can be construed as a measure of thickness. Applications are described for the auditory cortices in human adults and cats with normal hearing or hearing loss. The approach offers great potential for cortical morphometry.



### A New Angle on L2 Regularization
- **Arxiv ID**: http://arxiv.org/abs/1806.11186v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.11186v1)
- **Published**: 2018-06-28 20:57:40+00:00
- **Updated**: 2018-06-28 20:57:40+00:00
- **Authors**: Thomas Tanay, Lewis D Griffin
- **Comment**: An explorable explanation on the phenomenon of adversarial examples
  in linear classification and its relation to L2 regularization. Interactive
  version available at: https://thomas-tanay.github.io/post--L2-regularization/
- **Journal**: None
- **Summary**: Imagine two high-dimensional clusters and a hyperplane separating them. Consider in particular the angle between: the direction joining the two clusters' centroids and the normal to the hyperplane. In linear classification, this angle depends on the level of L2 regularization used. Can you explain why?



### CR-GAN: Learning Complete Representations for Multi-view Generation
- **Arxiv ID**: http://arxiv.org/abs/1806.11191v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11191v1)
- **Published**: 2018-06-28 21:04:21+00:00
- **Updated**: 2018-06-28 21:04:21+00:00
- **Authors**: Yu Tian, Xi Peng, Long Zhao, Shaoting Zhang, Dimitris N. Metaxas
- **Comment**: 7 pages, 9 figures, accepted by IJCAI 2018
- **Journal**: None
- **Summary**: Generating multi-view images from a single-view input is an essential yet challenging problem. It has broad applications in vision, graphics, and robotics. Our study indicates that the widely-used generative adversarial network (GAN) may learn "incomplete" representations due to the single-pathway framework: an encoder-decoder network followed by a discriminator network. We propose CR-GAN to address this problem. In addition to the single reconstruction path, we introduce a generation sideway to maintain the completeness of the learned embedding space. The two learning pathways collaborate and compete in a parameter-sharing manner, yielding considerably improved generalization ability to "unseen" dataset. More importantly, the two-pathway framework makes it possible to combine both labeled and unlabeled data for self-supervised learning, which further enriches the embedding space for realistic generations. The experimental results prove that CR-GAN significantly outperforms state-of-the-art methods, especially when generating from "unseen" inputs in wild conditions.



### Adversarial and Perceptual Refinement for Compressed Sensing MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1806.11216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11216v1)
- **Published**: 2018-06-28 22:12:39+00:00
- **Updated**: 2018-06-28 22:12:39+00:00
- **Authors**: Maximilian Seitzer, Guang Yang, Jo Schlemper, Ozan Oktay, Tobias Würfl, Vincent Christlein, Tom Wong, Raad Mohiaddin, David Firmin, Jennifer Keegan, Daniel Rueckert, Andreas Maier
- **Comment**: To be published at MICCAI 2018
- **Journal**: None
- **Summary**: Deep learning approaches have shown promising performance for compressed sensing-based Magnetic Resonance Imaging. While deep neural networks trained with mean squared error (MSE) loss functions can achieve high peak signal to noise ratio, the reconstructed images are often blurry and lack sharp details, especially for higher undersampling rates. Recently, adversarial and perceptual loss functions have been shown to achieve more visually appealing results. However, it remains an open question how to (1) optimally combine these loss functions with the MSE loss function and (2) evaluate such a perceptual enhancement. In this work, we propose a hybrid method, in which a visual refinement component is learnt on top of an MSE loss-based reconstruction network. In addition, we introduce a semantic interpretability score, measuring the visibility of the region of interest in both ground truth and reconstructed images, which allows us to objectively quantify the usefulness of the image quality for image post-processing and analysis. Applied on a large cardiac MRI dataset simulated with 8-fold undersampling, we demonstrate significant improvements ($p<0.01$) over the state-of-the-art in both a human observer study and the semantic interpretability score.



### Subject2Vec: Generative-Discriminative Approach from a Set of Image Patches to a Vector
- **Arxiv ID**: http://arxiv.org/abs/1806.11217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11217v1)
- **Published**: 2018-06-28 22:12:56+00:00
- **Updated**: 2018-06-28 22:12:56+00:00
- **Authors**: Sumedha Singla, Mingming Gong, Siamak Ravanbakhsh, Frank Sciurba, Barnabas Poczos, Kayhan N. Batmanghelich
- **Comment**: MICCAI 2018
- **Journal**: None
- **Summary**: We propose an attention-based method that aggregates local image features to a subject-level representation for predicting disease severity. In contrast to classical deep learning that requires a fixed dimensional input, our method operates on a set of image patches; hence it can accommodate variable length input image without image resizing. The model learns a clinically interpretable subject-level representation that is reflective of the disease severity. Our model consists of three mutually dependent modules which regulate each other: (1) a discriminative network that learns a fixed-length representation from local features and maps them to disease severity; (2) an attention mechanism that provides interpretability by focusing on the areas of the anatomy that contribute the most to the prediction task; and (3) a generative network that encourages the diversity of the local latent features. The generative term ensures that the attention weights are non-degenerate while maintaining the relevance of the local regions to the disease severity. We train our model end-to-end in the context of a large-scale lung CT study of Chronic Obstructive Pulmonary Disease (COPD). Our model gives state-of-the art performance in predicting clinical measures of severity for COPD. The distribution of the attention provides the regional relevance of lung tissue to the clinical measurements.



### Active query-driven visual search using probabilistic bisection and convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1806.11223v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11223v3)
- **Published**: 2018-06-28 23:05:40+00:00
- **Updated**: 2020-02-27 16:16:39+00:00
- **Authors**: Athanasios Tsiligkaridis, Theodoros Tsiligkaridis
- **Comment**: 5 pages, 4 figures, 1 table
- **Journal**: None
- **Summary**: We present a novel efficient object detection and localization framework based on the probabilistic bisection algorithm. A Convolutional Neural Network (CNN) is trained and used as a noisy oracle that provides answers to input query images. The responses along with error probability estimates obtained from the CNN are used to update beliefs on the object location along each dimension. We show that querying along each dimension achieves the same lower bound on localization error as the joint query design. Finally, we compare our approach to the traditional sliding window technique on a real world face localization task and show speed improvements by at least an order of magnitude while maintaining accurate localization.



### A Multimodal Recommender System for Large-scale Assortment Generation in E-commerce
- **Arxiv ID**: http://arxiv.org/abs/1806.11226v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.11226v1)
- **Published**: 2018-06-28 23:11:54+00:00
- **Updated**: 2018-06-28 23:11:54+00:00
- **Authors**: Murium Iqbal, Adair Kovac, Kamelia Aryafar
- **Comment**: SIGIR eComm Accepted Paper
- **Journal**: None
- **Summary**: E-commerce platforms surface interesting products largely through product recommendations that capture users' styles and aesthetic preferences. Curating recommendations as a complete complementary set, or assortment, is critical for a successful e-commerce experience, especially for product categories such as furniture, where items are selected together with the overall theme, style or ambiance of a space in mind. In this paper, we propose two visually-aware recommender systems that can automatically curate an assortment of living room furniture around a couple of pre-selected seed pieces for the room. The first system aims to maximize the visual-based style compatibility of the entire selection by making use of transfer learning and topic modeling. The second system extends the first by incorporating text data and applying polylingual topic modeling to infer style over both modalities. We review the production pipeline for surfacing these visually-aware recommender systems and compare them through offline validations and large-scale online A/B tests on Overstock. Our experimental results show that complimentary style is best discovered over product sets when both visual and textual data are incorporated.



### Human Action Recognition and Prediction: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1806.11230v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.11230v3)
- **Published**: 2018-06-28 23:43:42+00:00
- **Updated**: 2022-02-13 04:11:52+00:00
- **Authors**: Yu Kong, Yun Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Derived from rapid advances in computer vision and machine learning, video analysis tasks have been moving from inferring the present state to predicting the future state. Vision-based action recognition and prediction from videos are such tasks, where action recognition is to infer human actions (present state) based upon complete action executions, and action prediction to predict human actions (future state) based upon incomplete action executions. These two tasks have become particularly prevalent topics recently because of their explosively emerging real-world applications, such as visual surveillance, autonomous driving vehicle, entertainment, and video retrieval, etc. Many attempts have been devoted in the last a few decades in order to build a robust and effective framework for action recognition and prediction. In this paper, we survey the complete state-of-the-art techniques in action recognition and prediction. Existing models, popular algorithms, technical difficulties, popular action databases, evaluation protocols, and promising future directions are also provided with systematic discussions.



