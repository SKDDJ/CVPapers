# Arxiv Papers in cs.CV on 2018-06-01
### k-Space Deep Learning for Reference-free EPI Ghost Correction
- **Arxiv ID**: http://arxiv.org/abs/1806.00153v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.00153v3)
- **Published**: 2018-06-01 01:01:27+00:00
- **Updated**: 2019-06-18 00:24:07+00:00
- **Authors**: Juyoung Lee, Yoseob Han, Jae-Kyun Ryu, Jang-Yeon Park, Jong Chul Ye
- **Comment**: To appear in Magnetic Resonance in Medicine
- **Journal**: None
- **Summary**: Nyquist ghost artifacts in EPI are originated from phase mismatch between the even and odd echoes. However, conventional correction methods using reference scans often produce erroneous results especially in high-field MRI due to the non-linear and time-varying local magnetic field changes. Recently, it was shown that the problem of ghost correction can be reformulated as k-space interpolation problem that can be solved using structured low-rank Hankel matrix approaches. Another recent work showed that data driven Hankel matrix decomposition can be reformulated to exhibit similar structures as deep convolutional neural network. By synergistically combining these findings, we propose a k-space deep learning approach that immediately corrects the phase mismatch without a reference scan in both accelerated and non-accelerated EPI acquisitions. To take advantage of the even and odd-phase directional redundancy, the k-space data is divided into two channels configured with even and odd phase encodings. The redundancies between coils are also exploited by stacking the multi-coil k-space data into additional input channels. Then, our k-space ghost correction network is trained to learn the interpolation kernel to estimate the missing virtual k-space data. For the accelerated EPI data, the same neural network is trained to directly estimate the interpolation kernels for missing k-space data from both ghost and subsampling. Reconstruction results using 3T and 7T in-vivo data showed that the proposed method outperformed the image quality compared to the existing methods, and the computing time is much faster.The proposed k-space deep learning for EPI ghost correction is highly robust and fast, and can be combined with acceleration, so that it can be used as a promising correction tool for high-field MRI without changing the current acquisition protocol.



### IGCV3: Interleaved Low-Rank Group Convolutions for Efficient Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.00178v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00178v2)
- **Published**: 2018-06-01 03:18:10+00:00
- **Updated**: 2018-07-20 08:45:37+00:00
- **Authors**: Ke Sun, Mingjie Li, Dong Liu, Jingdong Wang
- **Comment**: 10 pages, 2 figures, accepted by BMVC 2018
- **Journal**: None
- **Summary**: In this paper, we are interested in building lightweight and efficient convolutional neural networks. Inspired by the success of two design patterns, composition of structured sparse kernels, e.g., interleaved group convolutions (IGC), and composition of low-rank kernels, e.g., bottle-neck modules, we study the combination of such two design patterns, using the composition of structured sparse low-rank kernels, to form a convolutional kernel. Rather than introducing a complementary condition over channels, we introduce a loose complementary condition, which is formulated by imposing the complementary condition over super-channels, to guide the design for generating a dense convolutional kernel. The resulting network is called IGCV3. We empirically demonstrate that the combination of low-rank and sparse kernels boosts the performance and the superiority of our proposed approach to the state-of-the-arts, IGCV2 and MobileNetV2 over image classification on CIFAR and ImageNet and object detection on COCO.



### The Nonlinearity Coefficient - Predicting Generalization in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.00179v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.00179v2)
- **Published**: 2018-06-01 03:58:14+00:00
- **Updated**: 2019-01-31 02:21:20+00:00
- **Authors**: George Philipp, Jaime G. Carbonell
- **Comment**: Previous name: The Nonlinearity Coefficient - Predicting Overfitting
  in Deep Neural Networks
- **Journal**: None
- **Summary**: For a long time, designing neural architectures that exhibit high performance was considered a dark art that required expert hand-tuning. One of the few well-known guidelines for architecture design is the avoidance of exploding gradients, though even this guideline has remained relatively vague and circumstantial. We introduce the nonlinearity coefficient (NLC), a measurement of the complexity of the function computed by a neural network that is based on the magnitude of the gradient. Via an extensive empirical study, we show that the NLC is a powerful predictor of test error and that attaining a right-sized NLC is essential for optimal performance.   The NLC exhibits a range of intriguing and important properties. It is closely tied to the amount of information gained from computing a single network gradient. It is tied to the error incurred when replacing the nonlinearity operations in the network with linear operations. It is not susceptible to the confounders of multiplicative scaling, additive bias and layer width. It is stable from layer to layer. Hence, we argue that the NLC is the first robust predictor of overfitting in deep networks.



### Hyperspectral Image Denoising Employing a Spatial-Spectral Deep Residual Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1806.00183v3
- **DOI**: 10.1109/TGRS.2018.2865197
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00183v3)
- **Published**: 2018-06-01 04:24:09+00:00
- **Updated**: 2018-08-11 00:06:48+00:00
- **Authors**: Qiangqiang Yuan, Qiang Zhang, Jie Li, Huanfeng Shen, Liangpei Zhang
- **Comment**: Accepted by IEEE TGRS, available codes:
  https://github.com/WHUQZhang/HSID-CNN
- **Journal**: None
- **Summary**: Hyperspectral image (HSI) denoising is a crucial preprocessing procedure to improve the performance of the subsequent HSI interpretation and applications. In this paper, a novel deep learning-based method for this task is proposed, by learning a non-linear end-to-end mapping between the noisy and clean HSIs with a combined spatial-spectral deep convolutional neural network (HSID-CNN). Both the spatial and spectral information are simultaneously assigned to the proposed network. In addition, multi-scale feature extraction and multi-level feature representation are respectively employed to capture both the multi-scale spatial-spectral feature and fuse the feature representations with different levels for the final restoration. The simulated and real-data experiments demonstrate that the proposed HSID-CNN outperforms many of the mainstream methods in both the quantitative evaluation indexes, visual effects, and HSI classification accuracy.



### Video Description: A Survey of Methods, Datasets and Evaluation Metrics
- **Arxiv ID**: http://arxiv.org/abs/1806.00186v4
- **DOI**: 10.1145/3355390
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00186v4)
- **Published**: 2018-06-01 04:31:58+00:00
- **Updated**: 2020-03-03 02:30:57+00:00
- **Authors**: Nayyer Aafaq, Ajmal Mian, Wei Liu, Syed Zulqarnain Gilani, Mubarak Shah
- **Comment**: Accepted by ACM Computing Surveys
- **Journal**: ACM Computing Surveys (CSUR) 52(6), 115 (2019)
- **Summary**: Video description is the automatic generation of natural language sentences that describe the contents of a given video. It has applications in human-robot interaction, helping the visually impaired and video subtitling. The past few years have seen a surge of research in this area due to the unprecedented success of deep learning in computer vision and natural language processing. Numerous methods, datasets and evaluation metrics have been proposed in the literature, calling the need for a comprehensive survey to focus research efforts in this flourishing new direction. This paper fills the gap by surveying the state of the art approaches with a focus on deep learning models; comparing benchmark datasets in terms of their domains, number of classes, and repository size; and identifying the pros and cons of various evaluation metrics like SPICE, CIDEr, ROUGE, BLEU, METEOR, and WMD. Classical video description approaches combined subject, object and verb detection with template based language models to generate sentences. However, the release of large datasets revealed that these methods can not cope with the diversity in unconstrained open domain videos. Classical approaches were followed by a very short era of statistical methods which were soon replaced with deep learning, the current state of the art in video description. Our survey shows that despite the fast-paced developments, video description research is still in its infancy due to the following reasons. Analysis of video description models is challenging because it is difficult to ascertain the contributions, towards accuracy or errors, of the visual features and the adopted language model in the final description. Existing datasets neither contain adequate visual diversity nor complexity of linguistic structures. Finally, current evaluation metrics ...



### Deep Imbalanced Learning for Face Recognition and Attribute Prediction
- **Arxiv ID**: http://arxiv.org/abs/1806.00194v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00194v2)
- **Published**: 2018-06-01 04:55:47+00:00
- **Updated**: 2019-04-30 03:49:42+00:00
- **Authors**: Chen Huang, Yining Li, Chen Change Loy, Xiaoou Tang
- **Comment**: 14 pages, 10 figures, 8 tables. Accepted to TPAMI
- **Journal**: None
- **Summary**: Data for face analysis often exhibit highly-skewed class distribution, i.e., most data belong to a few majority classes, while the minority classes only contain a scarce amount of instances. To mitigate this issue, contemporary deep learning methods typically follow classic strategies such as class re-sampling or cost-sensitive training. In this paper, we conduct extensive and systematic experiments to validate the effectiveness of these classic schemes for representation learning on class-imbalanced data. We further demonstrate that more discriminative deep representation can be learned by enforcing a deep network to maintain inter-cluster margins both within and between classes. This tight constraint effectively reduces the class imbalance inherent in the local data neighborhood, thus carving much more balanced class boundaries locally. We show that it is easy to deploy angular margins between the cluster distributions on a hypersphere manifold. Such learned Cluster-based Large Margin Local Embedding (CLMLE), when combined with a simple k-nearest cluster algorithm, shows significant improvements in accuracy over existing methods on both face recognition and face attribute prediction tasks that exhibit imbalanced class distribution.



### Generative Adversarial Networks for Unsupervised Object Co-localization
- **Arxiv ID**: http://arxiv.org/abs/1806.00236v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00236v2)
- **Published**: 2018-06-01 08:33:30+00:00
- **Updated**: 2018-07-08 08:51:34+00:00
- **Authors**: Junsuk Choe, Joo Hyun Park, Hyunjung Shim
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel approach for unsupervised object co-localization using Generative Adversarial Networks (GANs). GAN is a powerful tool that can implicitly learn unknown data distributions in an unsupervised manner. From the observation that GAN discriminator is highly influenced by pixels where objects appear, we analyze the internal layers of discriminator and visualize the activated pixels. Our important finding is that high image diversity of GAN, which is a main goal in GAN research, is ironically disadvantageous for object localization, because such discriminators focus not only on the target object, but also on the various objects, such as background objects. Based on extensive evaluations and experimental studies, we show the image diversity and localization performance have a negative correlation. In addition, our approach achieves meaningful accuracy for unsupervised object co-localization using publicly available benchmark datasets, even comparable to state-of-the-art weakly-supervised approach.



### Combining Pyramid Pooling and Attention Mechanism for Pelvic MR Image Semantic Segmentaion
- **Arxiv ID**: http://arxiv.org/abs/1806.00264v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00264v2)
- **Published**: 2018-06-01 10:13:45+00:00
- **Updated**: 2018-06-28 16:57:39+00:00
- **Authors**: Ting-Ting Liang, Satoshi Tsutsui, Liangcai Gao, Jing-Jing Lu, Mengyan Sun
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: One of the time-consuming routine work for a radiologist is to discern anatomical structures from tomographic images. For assisting radiologists, this paper develops an automatic segmentation method for pelvic magnetic resonance (MR) images. The task has three major challenges 1) A pelvic organ can have various sizes and shapes depending on the axial image, which requires local contexts to segment correctly. 2) Different organs often have quite similar appearance in MR images, which requires global context to segment. 3) The number of available annotated images are very small to use the latest segmentation algorithms. To address the challenges, we propose a novel convolutional neural network called Attention-Pyramid network (APNet) that effectively exploits both local and global contexts, in addition to a data-augmentation technique that is particularly effective for MR images. In order to evaluate our method, we construct fine-grained (50 pelvic organs) MR image segmentation dataset, and experimentally confirm the superior performance of our techniques over the state-of-the-art image segmentation methods.



### Learn the new, keep the old: Extending pretrained models with new anatomy and images
- **Arxiv ID**: http://arxiv.org/abs/1806.00265v1
- **DOI**: 10.1007/978-3-030-00937-3_42
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.00265v1)
- **Published**: 2018-06-01 10:14:31+00:00
- **Updated**: 2018-06-01 10:14:31+00:00
- **Authors**: Firat Ozdemir, Philipp Fuernstahl, Orcun Goksel
- **Comment**: Accepted to MICCAI 2018
- **Journal**: None
- **Summary**: Deep learning has been widely accepted as a promising solution for medical image segmentation, given a sufficiently large representative dataset of images with corresponding annotations. With ever increasing amounts of annotated medical datasets, it is infeasible to train a learning method always with all data from scratch. This is also doomed to hit computational limits, e.g., memory or runtime feasible for training. Incremental learning can be a potential solution, where new information (images or anatomy) is introduced iteratively. Nevertheless, for the preservation of the collective information, it is essential to keep some "important" (i.e. representative) images and annotations from the past, while adding new information. In this paper, we introduce a framework for applying incremental learning for segmentation and propose novel methods for selecting representative data therein. We comparatively evaluate our methods in different scenarios using MR images and validate the increased learning capacity with using our methods.



### Automatic Detection of Neurons in NeuN-stained Histological Images of Human Brain
- **Arxiv ID**: http://arxiv.org/abs/1806.00292v1
- **DOI**: 10.1016/j.physa.2018.12.027
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00292v1)
- **Published**: 2018-06-01 11:38:37+00:00
- **Updated**: 2018-06-01 11:38:37+00:00
- **Authors**: Andrija Štajduhar, Domagoj Džaja, Miloš Judaš, Sven Lončarić
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel use of an anisotropic diffusion model for automatic detection of neurons in histological sections of the adult human brain cortex. We use a partial differential equation model to process high resolution images to acquire locations of neuronal bodies. We also present a novel approach in model training and evaluation that considers variability among the human experts, addressing the issue of existence and correctness of the golden standard for neuron and cell counting, used in most of relevant papers. Our method, trained on dataset manually labeled by three experts, has correctly distinguished over 95% of neuron bodies in test data, doing so in time much shorter than other comparable methods.



### Domain Adaptation for MRI Organ Segmentation using Reverse Classification Accuracy
- **Arxiv ID**: http://arxiv.org/abs/1806.00363v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00363v1)
- **Published**: 2018-06-01 14:09:33+00:00
- **Updated**: 2018-06-01 14:09:33+00:00
- **Authors**: Vanya V. Valindria, Ioannis Lavdas, Wenjia Bai, Konstantinos Kamnitsas, Eric O. Aboagye, Andrea G. Rockall, Daniel Rueckert, Ben Glocker
- **Comment**: Accepted at the International Conference on Medical Imaging with Deep
  Learning (MIDL) 2018
- **Journal**: None
- **Summary**: The variations in multi-center data in medical imaging studies have brought the necessity of domain adaptation. Despite the advancement of machine learning in automatic segmentation, performance often degrades when algorithms are applied on new data acquired from different scanners or sequences than the training data. Manual annotation is costly and time consuming if it has to be carried out for every new target domain. In this work, we investigate automatic selection of suitable subjects to be annotated for supervised domain adaptation using the concept of reverse classification accuracy (RCA). RCA predicts the performance of a trained model on data from the new domain and different strategies of selecting subjects to be included in the adaptation via transfer learning are evaluated. We perform experiments on a two-center MR database for the task of organ segmentation. We show that subject selection via RCA can reduce the burden of annotation of new data for the target domain.



### Accurate and Efficient Similarity Search for Large Scale Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.00365v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00365v1)
- **Published**: 2018-06-01 14:13:56+00:00
- **Updated**: 2018-06-01 14:13:56+00:00
- **Authors**: Ce Qi, Zhizhong Liu, Fei Su
- **Comment**: None
- **Journal**: None
- **Summary**: Face verification is a relatively easy task with the help of discriminative features from deep neural networks. However, it is still a challenge to recognize faces on millions of identities while keeping high performance and efficiency. The challenge 2 of MS-Celeb-1M is a classification task. However, the number of identities is too large and it is not that elegant to treat the task as an image classification task. We treat the classification task as similarity search and do experiments on different similarity search strategies. Similarity search strategy accelerates the speed of searching and boosts the accuracy of final results. The model used for extracting features is a single deep neural network pretrained on CASIA-Webface, which is not trained on the base set or novel set offered by official. Finally, we rank \textbf{3rd}, while the speed of searching is 1ms/image.



### Radio Galaxy Morphology Generation Using DNN Autoencoder and Gaussian Mixture Models
- **Arxiv ID**: http://arxiv.org/abs/1806.00398v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00398v1)
- **Published**: 2018-06-01 15:33:16+00:00
- **Updated**: 2018-06-01 15:33:16+00:00
- **Authors**: Zhixian Ma, Jie Zhu, Weitian Li, Haiguang Xu
- **Comment**: Accepted by the 14th International Conference on Signal Processing
  (ICSP2018)
- **Journal**: None
- **Summary**: The morphology of a radio galaxy is highly affected by its central active galactic nuclei (AGN), which is studied to reveal the evolution of the super massive black hole (SMBH). In this work, we propose a morphology generation framework for two typical radio galaxies namely Fanaroff-Riley type-I (FRI) and type-II (FRII) with deep neural network based autoencoder (DNNAE) and Gaussian mixture models (GMMs). The encoder and decoder subnets in the DNNAE are symmetric aside a fully-connected layer namely code layer hosting the extracted feature vectors. By randomly generating the feature vectors later with a three-component Gaussian Mixture models, new FRI or FRII radio galaxy morphologies are simulated. Experiments were demonstrated on real radio galaxy images, where we discussed the length of feature vectors, selection of lost functions, and made comparisons on batch normalization and dropout techniques for training the network. The results suggest a high efficiency and performance of our morphology generation framework. Code is available at: https://github.com/myinxd/dnnae-gmm.



### Adapted and Oversegmenting Graphs: Application to Geometric Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.00411v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00411v2)
- **Published**: 2018-06-01 15:56:50+00:00
- **Updated**: 2019-09-05 17:43:17+00:00
- **Authors**: Alberto Gomez, Veronika A. Zimmer, Bishesh Khanal, Nicolas Toussaint, Julia A. Schnabel
- **Comment**: Submited to CVIU
- **Journal**: None
- **Summary**: We propose a novel iterative method to adapt a a graph to d-dimensional image data. The method drives the nodes of the graph towards image features. The adaptation process naturally lends itself to a measure of feature saliency which can then be used to retain meaningful nodes and edges in the graph. From the adapted graph, we also propose the computation of a dual graph, which inherits the saliency measure from the adapted graph, and whose edges run along image features, hence producing an oversegmenting graph. The proposed method is computationally efficient and fully parallelisable. We propose two distance measures to find image saliency along graph edges, and evaluate the performance on synthetic images and on natural images from publicly available databases. In both cases, the most salient nodes of the graph achieve average boundary recall over 90%. We also apply our method to image classification on the MNIST hand-written digit dataset, using a recently proposed Deep Geometric Learning architecture, and achieving state-of-the-art classification accuracy, for a graph-based method, of 97.86%.



### A Classification approach towards Unsupervised Learning of Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/1806.00428v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.00428v1)
- **Published**: 2018-06-01 16:35:08+00:00
- **Updated**: 2018-06-01 16:35:08+00:00
- **Authors**: Aditya Vora
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a technique for unsupervised learning of visual representations. Specifically, we train a model for foreground and background classification task, in the process of which it learns visual representations. Foreground and background patches for training come af- ter mining for such patches from hundreds and thousands of unlabelled videos available on the web which we ex- tract using a proposed patch extraction algorithm. With- out using any supervision, with just using 150, 000 unla- belled videos and the PASCAL VOC 2007 dataset, we train a object recognition model that achieves 45.3 mAP which is close to the best performing unsupervised feature learn- ing technique whereas better than many other proposed al- gorithms. The code for patch extraction is implemented in Matlab and available open source at the following link .



### Surgical Activity Recognition in Robot-Assisted Radical Prostatectomy using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.00466v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00466v1)
- **Published**: 2018-06-01 17:55:38+00:00
- **Updated**: 2018-06-01 17:55:38+00:00
- **Authors**: Aneeq Zia, Andrew Hung, Irfan Essa, Anthony Jarc
- **Comment**: Accepted at MICCAI 2018
- **Journal**: None
- **Summary**: Adverse surgical outcomes are costly to patients and hospitals. Approaches to benchmark surgical care are often limited to gross measures across the entire procedure despite the performance of particular tasks being largely responsible for undesirable outcomes. In order to produce metrics from tasks as opposed to the whole procedure, methods to recognize automatically individual surgical tasks are needed. In this paper, we propose several approaches to recognize surgical activities in robot-assisted minimally invasive surgery using deep learning. We collected a clinical dataset of 100 robot-assisted radical prostatectomies (RARP) with 12 tasks each and propose `RP-Net', a modified version of InceptionV3 model, for image based surgical activity recognition. We achieve an average precision of 80.9% and average recall of 76.7% across all tasks using RP-Net which out-performs all other RNN and CNN based models explored in this paper. Our results suggest that automatic surgical activity recognition during RARP is feasible and can be the foundation for advanced analytics.



### AGIL: Learning Attention from Human for Visuomotor Tasks
- **Arxiv ID**: http://arxiv.org/abs/1806.03960v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.03960v1)
- **Published**: 2018-06-01 18:36:36+00:00
- **Updated**: 2018-06-01 18:36:36+00:00
- **Authors**: Ruohan Zhang, Zhuode Liu, Luxin Zhang, Jake A. Whritner, Karl S. Muller, Mary M. Hayhoe, Dana H. Ballard
- **Comment**: None
- **Journal**: None
- **Summary**: When intelligent agents learn visuomotor behaviors from human demonstrations, they may benefit from knowing where the human is allocating visual attention, which can be inferred from their gaze. A wealth of information regarding intelligent decision making is conveyed by human gaze allocation; hence, exploiting such information has the potential to improve the agents' performance. With this motivation, we propose the AGIL (Attention Guided Imitation Learning) framework. We collect high-quality human action and gaze data while playing Atari games in a carefully controlled experimental setting. Using these data, we first train a deep neural network that can predict human gaze positions and visual attention with high accuracy (the gaze network) and then train another network to predict human actions (the policy network). Incorporating the learned attention model from the gaze network into the policy network significantly improves the action prediction accuracy and task performance.



### Targeted Kernel Networks: Faster Convolutions with Attentive Regularization
- **Arxiv ID**: http://arxiv.org/abs/1806.00523v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00523v2)
- **Published**: 2018-06-01 19:46:16+00:00
- **Updated**: 2018-09-21 20:27:47+00:00
- **Authors**: Kashyap Chitta
- **Comment**: ECCV 2018 Workshops
- **Journal**: None
- **Summary**: We propose Attentive Regularization (AR), a method to constrain the activation maps of kernels in Convolutional Neural Networks (CNNs) to specific regions of interest (ROIs). Each kernel learns a location of specialization along with its weights through standard backpropagation. A differentiable attention mechanism requiring no additional supervision is used to optimize the ROIs. Traditional CNNs of different types and structures can be modified with this idea into equivalent Targeted Kernel Networks (TKNs), while keeping the network size nearly identical. By restricting kernel ROIs, we reduce the number of sliding convolutional operations performed throughout the network in its forward pass, speeding up both training and inference. We evaluate our proposed architecture on both synthetic and natural tasks across multiple domains. TKNs obtain significant improvements over baselines, requiring less computation (around an order of magnitude) while achieving superior performance.



### Audio Visual Scene-Aware Dialog (AVSD) Challenge at DSTC7
- **Arxiv ID**: http://arxiv.org/abs/1806.00525v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.00525v1)
- **Published**: 2018-06-01 19:51:58+00:00
- **Updated**: 2018-06-01 19:51:58+00:00
- **Authors**: Huda Alamri, Vincent Cartillier, Raphael Gontijo Lopes, Abhishek Das, Jue Wang, Irfan Essa, Dhruv Batra, Devi Parikh, Anoop Cherian, Tim K. Marks, Chiori Hori
- **Comment**: None
- **Journal**: None
- **Summary**: Scene-aware dialog systems will be able to have conversations with users about the objects and events around them. Progress on such systems can be made by integrating state-of-the-art technologies from multiple research areas including end-to-end dialog systems visual dialog, and video description. We introduce the Audio Visual Scene Aware Dialog (AVSD) challenge and dataset. In this challenge, which is one track of the 7th Dialog System Technology Challenges (DSTC7) workshop1, the task is to build a system that generates responses in a dialog about an input video



### Spatially Localized Atlas Network Tiles Enables 3D Whole Brain Segmentation from Limited Data
- **Arxiv ID**: http://arxiv.org/abs/1806.00546v2
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1806.00546v2)
- **Published**: 2018-06-01 21:39:47+00:00
- **Updated**: 2018-06-05 04:49:05+00:00
- **Authors**: Yuankai Huo, Zhoubing Xu, Katherine Aboud, Prasanna Parvathaneni, Shunxing Bao, Camilo Bermudez, Susan M. Resnick, Laurie E. Cutting, Bennett A. Landman
- **Comment**: To appear in MICCAI2018
- **Journal**: None
- **Summary**: Whole brain segmentation on a structural magnetic resonance imaging (MRI) is essential in non-invasive investigation for neuroanatomy. Historically, multi-atlas segmentation (MAS) has been regarded as the de facto standard method for whole brain segmentation. Recently, deep neural network approaches have been applied to whole brain segmentation by learning random patches or 2D slices. Yet, few previous efforts have been made on detailed whole brain segmentation using 3D networks due to the following challenges: (1) fitting entire whole brain volume into 3D networks is restricted by the current GPU memory, and (2) the large number of targeting labels (e.g., > 100 labels) with limited number of training 3D volumes (e.g., < 50 scans). In this paper, we propose the spatially localized atlas network tiles (SLANT) method to distribute multiple independent 3D fully convolutional networks to cover overlapped sub-spaces in a standard atlas space. This strategy simplifies the whole brain learning task to localized sub-tasks, which was enabled by combing canonical registration and label fusion techniques with deep learning. To address the second challenge, auxiliary labels on 5111 initially unlabeled scans were created by MAS for pre-training. From empirical validation, the state-of-the-art MAS method achieved mean Dice value of 0.76, 0.71, and 0.68, while the proposed method achieved 0.78, 0.73, and 0.71 on three validation cohorts. Moreover, the computational time reduced from > 30 hours using MAS to ~15 minutes using the proposed method. The source code is available online https://github.com/MASILab/SLANTbrainSeg



### Neural Proximal Gradient Descent for Compressive Imaging
- **Arxiv ID**: http://arxiv.org/abs/1806.03963v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.03963v1)
- **Published**: 2018-06-01 21:48:39+00:00
- **Updated**: 2018-06-01 21:48:39+00:00
- **Authors**: Morteza Mardani, Qingyun Sun, Shreyas Vasawanala, Vardan Papyan, Hatef Monajemi, John Pauly, David Donoho
- **Comment**: arXiv admin note: text overlap with arXiv:1711.10046
- **Journal**: None
- **Summary**: Recovering high-resolution images from limited sensory data typically leads to a serious ill-posed inverse problem, demanding inversion algorithms that effectively capture the prior information. Learning a good inverse mapping from training data faces severe challenges, including: (i) scarcity of training data; (ii) need for plausible reconstructions that are physically feasible; (iii) need for fast reconstruction, especially in real-time applications. We develop a successful system solving all these challenges, using as basic architecture the recurrent application of proximal gradient algorithm. We learn a proximal map that works well with real images based on residual networks. Contraction of the resulting map is analyzed, and incoherence conditions are investigated that drive the convergence of the iterates. Extensive experiments are carried out under different settings: (a) reconstructing abdominal MRI of pediatric patients from highly undersampled Fourier-space data and (b) superresolving natural face images. Our key findings include: 1. a recurrent ResNet with a single residual block unrolled from an iterative algorithm yields an effective proximal which accurately reveals MR image details. 2. Our architecture significantly outperforms conventional non-recurrent deep ResNets by 2dB SNR; it is also trained much more rapidly. 3. It outperforms state-of-the-art compressed-sensing Wavelet-based methods by 4dB SNR, with 100x speedups in reconstruction time.



### CubeSLAM: Monocular 3D Object SLAM
- **Arxiv ID**: http://arxiv.org/abs/1806.00557v2
- **DOI**: 10.1109/TRO.2019.2909168
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.00557v2)
- **Published**: 2018-06-01 22:44:20+00:00
- **Updated**: 2019-04-05 06:05:44+00:00
- **Authors**: Shichao Yang, Sebastian Scherer
- **Comment**: IEEE Transactions on Robotics
- **Journal**: None
- **Summary**: We present a method for single image 3D cuboid object detection and multi-view object SLAM in both static and dynamic environments, and demonstrate that the two parts can improve each other. Firstly for single image object detection, we generate high-quality cuboid proposals from 2D bounding boxes and vanishing points sampling. The proposals are further scored and selected based on the alignment with image edges. Secondly, multi-view bundle adjustment with new object measurements is proposed to jointly optimize poses of cameras, objects and points. Objects can provide long-range geometric and scale constraints to improve camera pose estimation and reduce monocular drift. Instead of treating dynamic regions as outliers, we utilize object representation and motion model constraints to improve the camera pose estimation. The 3D detection experiments on SUN RGBD and KITTI show better accuracy and robustness over existing approaches. On the public TUM, KITTI odometry and our own collected datasets, our SLAM method achieves the state-of-the-art monocular camera pose estimation and at the same time, improves the 3D object detection accuracy.



