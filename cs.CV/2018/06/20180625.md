# Arxiv Papers in cs.CV on 2018-06-25
### FBI-Pose: Towards Bridging the Gap between 2D Images and 3D Human Poses using Forward-or-Backward Information
- **Arxiv ID**: http://arxiv.org/abs/1806.09241v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09241v1)
- **Published**: 2018-06-25 00:35:10+00:00
- **Updated**: 2018-06-25 00:35:10+00:00
- **Authors**: Yulong Shi, Xiaoguang Han, Nianjuan Jiang, Kun Zhou, Kui Jia, Jiangbo Lu
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Although significant advances have been made in the area of human poses estimation from images using deep Convolutional Neural Network (ConvNet), it remains a big challenge to perform 3D pose inference in-the-wild. This is due to the difficulty to obtain 3D pose groundtruth for outdoor environments. In this paper, we propose a novel framework to tackle this problem by exploiting the information of each bone indicating if it is forward or backward with respect to the view of the camera(we term it Forwardor-Backward Information abbreviated as FBI). Our method firstly trains a ConvNet with two branches which maps an image of a human to both the 2D joint locations and the FBI of bones. These information is further fed into a deep regression network to predict the 3D positions of joints. To support the training, we also develop an annotation user interface and labeled such FBI for around 12K in-the-wild images which are randomly selected from MPII (a public dataset of 2D pose annotation). Our experimental results on the standard benchmarks demonstrate that our approach outperforms state-of-the-art methods both qualitatively and quantitatively.



### Color Constancy by Reweighting Image Feature Maps
- **Arxiv ID**: http://arxiv.org/abs/1806.09248v3
- **DOI**: 10.1109/TIP.2020.2985296
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09248v3)
- **Published**: 2018-06-25 01:50:26+00:00
- **Updated**: 2020-05-06 13:38:01+00:00
- **Authors**: Jueqin Qiu, Haisong Xu, Zhengnan Ye
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing 29 (2020) 5711-5721
- **Summary**: In this study, a novel illuminant color estimation framework is proposed for computational color constancy, which incorporates the high representational capacity of deep-learning-based models and the great interpretability of assumption-based models. The well-designed building block, feature map reweight unit (ReWU), helps to achieve comparative accuracy on benchmark datasets with respect to prior state-of-the-art deep learning based models while requiring more compact model size and cheaper computational cost. In addition to local color estimation, a confidence estimation branch is also included such that the model is able to simultaneously produce point estimate and its uncertainty estimate, which provides useful clues for local estimates aggregation and multiple illumination estimation. The source code and the dataset have been made available at https://github.com/QiuJueqin/Reweight-CC.



### Track Xplorer: A System for Visual Analysis of Sensor-based Motor Activity Predictions
- **Arxiv ID**: http://arxiv.org/abs/1806.09256v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV, cs.DB, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.09256v1)
- **Published**: 2018-06-25 02:19:24+00:00
- **Updated**: 2018-06-25 02:19:24+00:00
- **Authors**: Marco Cavallo, Çağatay Demiralp
- **Comment**: EuroVis'18
- **Journal**: None
- **Summary**: With the rapid commoditization of wearable sensors, detecting human movements from sensor datasets has become increasingly common over a wide range of applications. To detect activities, data scientists iteratively experiment with different classifiers before deciding which model to deploy. Effective reasoning about and comparison of alternative classifiers are crucial in successful model development. This is, however, inherently difficult in developing classifiers for sensor data, where the intricacy of long temporal sequences, high prediction frequency, and imprecise labeling make standard evaluation methods relatively ineffective and even misleading. We introduce Track Xplorer, an interactive visualization system to query, analyze, and compare the predictions of sensor-data classifiers. Track Xplorer enables users to interactively explore and compare the results of different classifiers, and assess their accuracy with respect to the ground-truth labels and video. Through integration with a version control system, Track Xplorer supports tracking of models and their parameters without additional workload on model developers. Track Xplorer also contributes an extensible algebra over track representations to filter, compose, and compare classification outputs, enabling users to reason effectively about classifier performance. We apply Track Xplorer in a collaborative project to develop classifiers to detect movements from multisensor data gathered from Parkinson's disease patients. We demonstrate how Track Xplorer helps identify early on possible systemic data errors, effectively track and compare the results of different classifiers, and reason about and pinpoint the causes of misclassifications.



### Learning Task-Oriented Grasping for Tool Manipulation from Simulated Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/1806.09266v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.09266v1)
- **Published**: 2018-06-25 03:08:28+00:00
- **Updated**: 2018-06-25 03:08:28+00:00
- **Authors**: Kuan Fang, Yuke Zhu, Animesh Garg, Andrey Kurenkov, Viraj Mehta, Li Fei-Fei, Silvio Savarese
- **Comment**: RSS 2018
- **Journal**: None
- **Summary**: Tool manipulation is vital for facilitating robots to complete challenging task goals. It requires reasoning about the desired effect of the task and thus properly grasping and manipulating the tool to achieve the task. Task-agnostic grasping optimizes for grasp robustness while ignoring crucial task-specific constraints. In this paper, we propose the Task-Oriented Grasping Network (TOG-Net) to jointly optimize both task-oriented grasping of a tool and the manipulation policy for that tool. The training process of the model is based on large-scale simulated self-supervision with procedurally generated tool objects. We perform both simulated and real-world experiments on two tool-based manipulation tasks: sweeping and hammering. Our model achieves overall 71.1% task success rate for sweeping and 80.0% task success rate for hammering. Supplementary material is available at: bit.ly/task-oriented-grasp



### Best Vision Technologies Submission to ActivityNet Challenge 2018-Task: Dense-Captioning Events in Videos
- **Arxiv ID**: http://arxiv.org/abs/1806.09278v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09278v1)
- **Published**: 2018-06-25 04:11:03+00:00
- **Updated**: 2018-06-25 04:11:03+00:00
- **Authors**: Yuan Liu, Moyini Yao
- **Comment**: Rank 2 in ActivityNet Captions Challenge 2018
- **Journal**: None
- **Summary**: This note describes the details of our solution to the dense-captioning events in videos task of ActivityNet Challenge 2018. Specifically, we solve this problem with a two-stage way, i.e., first temporal event proposal and then sentence generation. For temporal event proposal, we directly leverage the three-stage workflow in [13, 16]. For sentence generation, we capitalize on LSTM-based captioning framework with temporal attention mechanism (dubbed as LSTM-T). Moreover, the input visual sequence to the LSTM-based video captioning model is comprised of RGB and optical flow images. At inference, we adopt a late fusion scheme to fuse the two LSTM-based captioning models for sentence generation.



### RAM: A Region-Aware Deep Model for Vehicle Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1806.09283v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09283v1)
- **Published**: 2018-06-25 04:40:49+00:00
- **Updated**: 2018-06-25 04:40:49+00:00
- **Authors**: Xiaobin Liu, Shiliang Zhang, Qingming Huang, Wen Gao
- **Comment**: Accepted by ICME 2018
- **Journal**: None
- **Summary**: Previous works on vehicle Re-ID mainly focus on extracting global features and learning distance metrics. Because some vehicles commonly share same model and maker, it is hard to distinguish them based on their global appearances. Compared with the global appearance, local regions such as decorations and inspection stickers attached to the windshield, may be more distinctive for vehicle Re-ID. To embed the detailed visual cues in those local regions, we propose a Region-Aware deep Model (RAM). Specifically, in addition to extracting global features, RAM also extracts features from a series of local regions. As each local region conveys more distinctive visual cues, RAM encourages the deep model to learn discriminative features. We also introduce a novel learning algorithm to jointly use vehicle IDs, types/models, and colors to train the RAM. This strategy fuses more cues for training and results in more discriminative global and regional features. We evaluate our methods on two large-scale vehicle Re-ID datasets, i.e., VeRi and VehicleID. Experimental results show our methods achieve promising performance in comparison with recent works.



### Towards Radiologist-Level Accurate Deep Learning System for Pulmonary Screening
- **Arxiv ID**: http://arxiv.org/abs/1807.03120v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1807.03120v1)
- **Published**: 2018-06-25 07:07:25+00:00
- **Updated**: 2018-06-25 07:07:25+00:00
- **Authors**: Mrinal Haloi, K. Raja Rajalakshmi, Pradeep Walia
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose advanced pneumonia and Tuberculosis grading system for X-ray images. The proposed system is a very deep fully convolutional classification network with online augmentation that outputs confidence values for diseases prevalence. Its a fully automated system capable of disease feature understanding without any offline preprocessing step or manual feature extraction. We have achieved state- of-the- art performance on the public databases such as ChestXray-14, Mendeley, Shenzhen Hospital X-ray and Belarus X-ray set.



### Vision-based Pose Estimation for Augmented Reality : A Comparison Study
- **Arxiv ID**: http://arxiv.org/abs/1806.09316v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09316v1)
- **Published**: 2018-06-25 08:01:45+00:00
- **Updated**: 2018-06-25 08:01:45+00:00
- **Authors**: Hayet Belghit, Abdelkader Bellarbi, Nadia Zenati, Samir Otmane
- **Comment**: IEEE International Conference on Pattern Analysis and Intelligent
  Systems PAIS'2018
- **Journal**: None
- **Summary**: Augmented reality aims to enrich our real world by inserting 3D virtual objects. In order to accomplish this goal, it is important that virtual elements are rendered and aligned in the real scene in an accurate and visually acceptable way. The solution of this problem can be related to a pose estimation and 3D camera localization. This paper presents a survey on different approaches of 3D pose estimation in augmented reality and gives classification of key-points-based techniques. The study given in this paper may help both developers and researchers in the field of augmented reality.



### A temporal neural network model for object recognition using a biologically plausible decision making layer
- **Arxiv ID**: http://arxiv.org/abs/1806.09334v2
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.09334v2)
- **Published**: 2018-06-25 09:04:28+00:00
- **Updated**: 2018-11-22 18:24:50+00:00
- **Authors**: Hamed Heidari Gorji, Sajjad Zabbah, Reza Ebrahimpour
- **Comment**: Version 2 contains more details about model. Comparisons with some
  known deep neural networks have been included and are shown in figure 7. text
  was corrected and edited
- **Journal**: None
- **Summary**: Brain can recognize different objects as ones that it has experienced before. The recognition accuracy and its processing time depend on task properties such as viewing condition, level of noise and etc. Recognition accuracy can be well explained by different models. However, less attention has been paid to the processing time and the ones that do, are not biologically plausible. By extracting features temporally as well as utilizing an accumulation to bound decision making model, an object recognition model accounting for both recognition time and accuracy is proposed. To temporally extract informative features in support of possible classes of stimuli, a hierarchical spiking neural network, called spiking HMAX is modified. In the decision making part of the model the extracted information accumulates over time using accumulator units. The input category is determined as soon as any of the accumulators reaches a threshold, called decision bound. Results show that not only does the model follow human accuracy in a psychophysics task better than the classic spiking HMAX model, but also it predicts human response time in each choice. Results provide enough evidence that temporal representation of features are informative since they can improve the accuracy of a biological plausible decision maker over time. This is also in line with the well-known idea of speed accuracy trade-off in decision making studies.



### Sparse 3D Point-cloud Map Upsampling and Noise Removal as a vSLAM Post-processing Step: Experimental Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1806.09346v1
- **DOI**: 10.1007/978-3-319-99582-3_3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09346v1)
- **Published**: 2018-06-25 09:33:48+00:00
- **Updated**: 2018-06-25 09:33:48+00:00
- **Authors**: Andrey Bokovoy, Konstantin Yakovlev
- **Comment**: 10 pages, 4 figures, camera-ready version of paper for "The 3rd
  International Conference on Interactive Collaborative Robotics (ICR 2018)"
- **Journal**: None
- **Summary**: The monocular vision-based simultaneous localization and mapping (vSLAM) is one of the most challenging problem in mobile robotics and computer vision. In this work we study the post-processing techniques applied to sparse 3D point-cloud maps, obtained by feature-based vSLAM algorithms. Map post-processing is split into 2 major steps: 1) noise and outlier removal and 2) upsampling. We evaluate different combinations of known algorithms for outlier removing and upsampling on datasets of real indoor and outdoor environments and identify the most promising combination. We further use it to convert a point-cloud map, obtained by the real UAV performing indoor flight to 3D voxel grid (octo-map) potentially suitable for path planning.



### Exploring Adversarial Examples: Patterns of One-Pixel Attacks
- **Arxiv ID**: http://arxiv.org/abs/1806.09410v2
- **DOI**: 10.1007/978-3-030-02628-8_8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09410v2)
- **Published**: 2018-06-25 12:20:49+00:00
- **Updated**: 2018-11-13 16:34:57+00:00
- **Authors**: David Kügler, Alexander Distergoft, Arjan Kuijper, Anirban Mukhopadhyay
- **Comment**: Figure 4 corrected from published Version to correct y-axis labels
- **Journal**: Understanding and Interpreting Machine Learning in Medical Image
  Computing Applications Lecture Notes in Computer Science vol. 11038. Springer
  International Publishing, Cham (2018)
- **Summary**: Failure cases of black-box deep learning, e.g. adversarial examples, might have severe consequences in healthcare. Yet such failures are mostly studied in the context of real-world images with calibrated attacks. To demystify the adversarial examples, rigorous studies need to be designed. Unfortunately, complexity of the medical images hinders such study design directly from the medical images. We hypothesize that adversarial examples might result from the incorrect mapping of image space to the low dimensional generation manifold by deep networks. To test the hypothesis, we simplify a complex medical problem namely pose estimation of surgical tools into its barest form. An analytical decision boundary and exhaustive search of the one-pixel attack across multiple image dimensions let us localize the regions of frequent successful one-pixel attacks at the image space.



### A Unified Model with Structured Output for Fashion Images Classification
- **Arxiv ID**: http://arxiv.org/abs/1806.09445v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09445v1)
- **Published**: 2018-06-25 13:19:47+00:00
- **Updated**: 2018-06-25 13:19:47+00:00
- **Authors**: Beatriz Quintino Ferreira, Luís Baía, João Faria, Ricardo Gamelas Sousa
- **Comment**: Accepted in KDD 2018's AI for Fashion workshop
- **Journal**: None
- **Summary**: A picture is worth a thousand words. Albeit a clich\'e, for the fashion industry, an image of a clothing piece allows one to perceive its category (e.g., dress), sub-category (e.g., day dress) and properties (e.g., white colour with floral patterns). The seasonal nature of the fashion industry creates a highly dynamic and creative domain with evermore data, making it unpractical to manually describe a large set of images (of products). In this paper, we explore the concept of visual recognition for fashion images through an end-to-end architecture embedding the hierarchical nature of the annotations directly into the model. Towards that goal, and inspired by the work of [7], we have modified and adapted the original architecture proposal. Namely, we have removed the message passing layer symmetry to cope with Farfetch category tree, added extra layers for hierarchy level specificity, and moved the message passing layer into an enriched latent space. We compare the proposed unified architecture against state-of-the-art models and demonstrate the performance advantage of our model for structured multi-level categorization on a dataset of about 350k fashion product images.



### Context-Aware Pedestrian Motion Prediction In Urban Intersections
- **Arxiv ID**: http://arxiv.org/abs/1806.09453v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.09453v1)
- **Published**: 2018-06-25 13:45:57+00:00
- **Updated**: 2018-06-25 13:45:57+00:00
- **Authors**: Golnaz Habibi, Nikita Jaipuria, Jonathan P. How
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel context-based approach for pedestrian motion prediction in crowded, urban intersections, with the additional flexibility of prediction in similar, but new, environments. Previously, Chen et. al. combined Markovian-based and clustering-based approaches to learn motion primitives in a grid-based world and subsequently predict pedestrian trajectories by modeling the transition between learned primitives as a Gaussian Process (GP). This work extends that prior approach by incorporating semantic features from the environment (relative distance to curbside and status of pedestrian traffic lights) in the GP formulation for more accurate predictions of pedestrian trajectories over the same timescale. We evaluate the new approach on real-world data collected using one of the vehicles in the MIT Mobility On Demand fleet. The results show 12.5% improvement in prediction accuracy and a 2.65 times reduction in Area Under the Curve (AUC), which is used as a metric to quantify the span of predicted set of trajectories, such that a lower AUC corresponds to a higher level of confidence in the future direction of pedestrian motion.



### Semi-Automatic RECIST Labeling on CT Scans with Cascaded Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.09507v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09507v1)
- **Published**: 2018-06-25 14:52:07+00:00
- **Updated**: 2018-06-25 14:52:07+00:00
- **Authors**: Youbao Tang, Adam P. Harrison, Mohammadhadi Bagheri, Jing Xiao, Ronald M. Summers
- **Comment**: Accepted by MICCAI 2018
- **Journal**: None
- **Summary**: Response evaluation criteria in solid tumors (RECIST) is the standard measurement for tumor extent to evaluate treatment responses in cancer patients. As such, RECIST annotations must be accurate. However, RECIST annotations manually labeled by radiologists require professional knowledge and are time-consuming, subjective, and prone to inconsistency among different observers. To alleviate these problems, we propose a cascaded convolutional neural network based method to semi-automatically label RECIST annotations and drastically reduce annotation time. The proposed method consists of two stages: lesion region normalization and RECIST estimation. We employ the spatial transformer network (STN) for lesion region normalization, where a localization network is designed to predict the lesion region and the transformation parameters with a multi-task learning strategy. For RECIST estimation, we adapt the stacked hourglass network (SHN), introducing a relationship constraint loss to improve the estimation precision. STN and SHN can both be learned in an end-to-end fashion. We train our system on the DeepLesion dataset, obtaining a consensus model trained on RECIST annotations performed by multiple radiologists over a multi-year period. Importantly, when judged against the inter-reader variability of two additional radiologist raters, our system performs more stably and with less variability, suggesting that RECIST annotations can be reliably obtained with reduced labor and time.



### Self-supervised Learning for Dense Depth Estimation in Monocular Endoscopy
- **Arxiv ID**: http://arxiv.org/abs/1806.09521v2
- **DOI**: 10.1007/978-3-030-01201-4_15
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09521v2)
- **Published**: 2018-06-25 15:12:57+00:00
- **Updated**: 2018-07-26 14:42:23+00:00
- **Authors**: Xingtong Liu, Ayushi Sinha, Mathias Unberath, Masaru Ishii, Gregory Hager, Russell H. Taylor, Austin Reiter
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: We present a self-supervised approach to training convolutional neural networks for dense depth estimation from monocular endoscopy data without a priori modeling of anatomy or shading. Our method only requires sequential data from monocular endoscopic videos and a multi-view stereo reconstruction method, e.g. structure from motion, that supervises learning in a sparse but accurate manner. Consequently, our method requires neither manual interaction, such as scaling or labeling, nor patient CT in the training and application phases. We demonstrate the performance of our method on sinus endoscopy data from two patients and validate depth prediction quantitatively using corresponding patient CT scans where we found submillimeter residual errors.



### SkinNet: A Deep Learning Framework for Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.09522v1
- **DOI**: 10.1109/NSSMIC.2018.8824732
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09522v1)
- **Published**: 2018-06-25 15:14:31+00:00
- **Updated**: 2018-06-25 15:14:31+00:00
- **Authors**: Sulaiman Vesal, Nishant Ravikumar, Andreas Maier
- **Comment**: 2 pages, submitted to NSS/MIC 2018
- **Journal**: None
- **Summary**: There has been a steady increase in the incidence of skin cancer worldwide, with a high rate of mortality. Early detection and segmentation of skin lesions are crucial for timely diagnosis and treatment, necessary to improve the survival rate of patients. However, skin lesion segmentation is a challenging task due to the low contrast of lesions and their high similarity in terms of appearance, to healthy tissue. This underlines the need for an accurate and automatic approach for skin lesion segmentation. To tackle this issue, we propose a convolutional neural network (CNN) called SkinNet. The proposed CNN is a modified version of U-Net. We compared the performance of our approach with other state-of-the-art techniques, using the ISBI 2017 challenge dataset. Our approach outperformed the others in terms of the Dice coefficient, Jaccard index and sensitivity, evaluated on the held-out challenge test data set, across 5-fold cross validation experiments. SkinNet achieved an average value of 85.10, 76.67 and 93.0%, for the DC, JI, and SE, respectively.



### IR2VI: Enhanced Night Environmental Perception by Unsupervised Thermal Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1806.09565v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09565v1)
- **Published**: 2018-06-25 16:57:00+00:00
- **Updated**: 2018-06-25 16:57:00+00:00
- **Authors**: Shuo Liu, Vijay John, Erik Blasch, Zheng Liu, Ying Huang
- **Comment**: Present at CVPR Workshops 2018
- **Journal**: None
- **Summary**: Context enhancement is critical for night vision (NV) applications, especially for the dark night situation without any artificial lights. In this paper, we present the infrared-to-visual (IR2VI) algorithm, a novel unsupervised thermal-to-visible image translation framework based on generative adversarial networks (GANs). IR2VI is able to learn the intrinsic characteristics from VI images and integrate them into IR images. Since the existing unsupervised GAN-based image translation approaches face several challenges, such as incorrect mapping and lack of fine details, we propose a structure connection module and a region-of-interest (ROI) focal loss method to address the current limitations. Experimental results show the superiority of the IR2VI algorithm over baseline methods.



### Learning Single-Image Depth from Videos using Quality Assessment Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.09573v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09573v3)
- **Published**: 2018-06-25 17:17:55+00:00
- **Updated**: 2019-04-24 15:57:28+00:00
- **Authors**: Weifeng Chen, Shengyi Qian, Jia Deng
- **Comment**: None
- **Journal**: None
- **Summary**: Depth estimation from a single image in the wild remains a challenging problem. One main obstacle is the lack of high-quality training data for images in the wild. In this paper we propose a method to automatically generate such data through Structure-from-Motion (SfM) on Internet videos. The core of this method is a Quality Assessment Network that identifies high-quality reconstructions obtained from SfM. Using this method, we collect single-view depth training data from a large number of YouTube videos and construct a new dataset called YouTube3D. Experiments show that YouTube3D is useful in training depth estimation networks and advances the state of the art of single-view depth estimation in the wild.



### Tracking Emerges by Colorizing Videos
- **Arxiv ID**: http://arxiv.org/abs/1806.09594v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.MM, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1806.09594v2)
- **Published**: 2018-06-25 17:44:40+00:00
- **Updated**: 2018-07-27 22:38:39+00:00
- **Authors**: Carl Vondrick, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama, Kevin Murphy
- **Comment**: ECCV 2018. Blog post:
  https://ai.googleblog.com/2018/06/self-supervised-tracking-via-video.html
- **Journal**: None
- **Summary**: We use large amounts of unlabeled video to learn models for visual tracking without manual human supervision. We leverage the natural temporal coherency of color to create a model that learns to colorize gray-scale videos by copying colors from a reference frame. Quantitative and qualitative experiments suggest that this task causes the model to automatically learn to track visual regions. Although the model is trained without any ground-truth labels, our method learns to track well enough to outperform the latest methods based on optical flow. Moreover, our results suggest that failures to track are correlated with failures to colorize, indicating that advancing video colorization may further improve self-supervised visual tracking.



### A Machine-learning framework for automatic reference-free quality assessment in MRI
- **Arxiv ID**: http://arxiv.org/abs/1806.09602v2
- **DOI**: 10.1016/j.mri.2018.07.003
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.09602v2)
- **Published**: 2018-06-25 17:56:32+00:00
- **Updated**: 2018-07-18 09:53:40+00:00
- **Authors**: Thomas Küstner, Sergios Gatidis, Annika Liebgott, Martin Schwartz, Lukas Mauch, Petros Martirosian, Holger Schmidt, Nina F. Schwenzer, Konstantin Nikolaou, Fabian Bamberg, Bin Yang, Fritz Schick
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic resonance (MR) imaging offers a wide variety of imaging techniques. A large amount of data is created per examination which needs to be checked for sufficient quality in order to derive a meaningful diagnosis. This is a manual process and therefore time- and cost-intensive. Any imaging artifacts originating from scanner hardware, signal processing or induced by the patient may reduce the image quality and complicate the diagnosis or any image post-processing. Therefore, the assessment or the ensurance of sufficient image quality in an automated manner is of high interest. Usually no reference image is available or difficult to define. Therefore, classical reference-based approaches are not applicable. Model observers mimicking the human observers (HO) can assist in this task. Thus, we propose a new machine-learning-based reference-free MR image quality assessment framework which is trained on HO-derived labels to assess MR image quality immediately after each acquisition. We include the concept of active learning and present an efficient blinded reading platform to reduce the effort in the HO labeling procedure. Derived image features and the applied classifiers (support-vector-machine, deep neural network) are investigated for a cohort of 250 patients. The MR image quality assessment framework can achieve a high test accuracy of 93.7$\%$ for estimating quality classes on a 5-point Likert-scale. The proposed MR image quality assessment framework is able to provide an accurate and efficient quality estimation which can be used as a prospective quality assurance including automatic acquisition adaptation or guided MR scanner operation, and/or as a retrospective quality assessment including support of diagnostic decisions or quality control in cohort studies.



### 3D Context Enhanced Region-based Convolutional Neural Network for End-to-End Lesion Detection
- **Arxiv ID**: http://arxiv.org/abs/1806.09648v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09648v2)
- **Published**: 2018-06-25 18:10:16+00:00
- **Updated**: 2018-07-29 17:56:13+00:00
- **Authors**: Ke Yan, Mohammadhadi Bagheri, Ronald M. Summers
- **Comment**: MICCAI 2018
- **Journal**: None
- **Summary**: Detecting lesions from computed tomography (CT) scans is an important but difficult problem because non-lesions and true lesions can appear similar. 3D context is known to be helpful in this differentiation task. However, existing end-to-end detection frameworks of convolutional neural networks (CNNs) are mostly designed for 2D images. In this paper, we propose 3D context enhanced region-based CNN (3DCE) to incorporate 3D context information efficiently by aggregating feature maps of 2D images. 3DCE is easy to train and end-to-end in training and inference. A universal lesion detector is developed to detect all kinds of lesions in one algorithm using the DeepLesion dataset. Experimental results on this challenging task prove the effectiveness of 3DCE. We have released the code of 3DCE in https://github.com/rsummers11/CADLab/tree/master/lesion_detector_3DCE.



### Learning what you can do before doing anything
- **Arxiv ID**: http://arxiv.org/abs/1806.09655v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.09655v2)
- **Published**: 2018-06-25 18:33:34+00:00
- **Updated**: 2019-02-12 18:53:33+00:00
- **Authors**: Oleh Rybkin, Karl Pertsch, Konstantinos G. Derpanis, Kostas Daniilidis, Andrew Jaegle
- **Comment**: Published at ICLR 2019. 10 pages + 15 pages of references and
  appendices
- **Journal**: International Conference on Learning Representations, 2019
- **Summary**: Intelligent agents can learn to represent the action spaces of other agents simply by observing them act. Such representations help agents quickly learn to predict the effects of their own actions on the environment and to plan complex action sequences. In this work, we address the problem of learning an agent's action space purely from visual observation. We use stochastic video prediction to learn a latent variable that captures the scene's dynamics while being minimally sensitive to the scene's static content. We introduce a loss term that encourages the network to capture the composability of visual sequences and show that it leads to representations that disentangle the structure of actions. We call the full model with composable action representations Composable Learned Action Space Predictor (CLASP). We show the applicability of our method to synthetic settings and its potential to capture action spaces in complex, realistic visual settings. When used in a semi-supervised setting, our learned representations perform comparably to existing fully supervised methods on tasks such as action-conditioned video prediction and planning in the learned action space, while requiring orders of magnitude fewer action labels. Project website: https://daniilidis-group.github.io/learned_action_spaces



### Physics-based Scene-level Reasoning for Object Pose Estimation in Clutter
- **Arxiv ID**: http://arxiv.org/abs/1806.10457v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.10457v2)
- **Published**: 2018-06-25 20:39:11+00:00
- **Updated**: 2019-04-01 19:47:33+00:00
- **Authors**: Chaitanya Mitash, Abdeslam Boularias, Kostas Bekris
- **Comment**: 18 pages, 13 figures, International Journal of Robotics Research
  (IJRR) 2019. arXiv admin note: text overlap with arXiv:1710.08577
- **Journal**: None
- **Summary**: This paper focuses on vision-based pose estimation for multiple rigid objects placed in clutter, especially in cases involving occlusions and objects resting on each other. Progress has been achieved recently in object recognition given advancements in deep learning. Nevertheless, such tools typically require a large amount of training data and significant manual effort to label objects. This limits their applicability in robotics, where solutions must scale to a large number of objects and variety of conditions. Moreover, the combinatorial nature of the scenes that could arise from the placement of multiple objects is hard to capture in the training dataset. Thus, the learned models might not produce the desired level of precision required for tasks, such as robotic manipulation. This work proposes an autonomous process for pose estimation that spans from data generation to scene-level reasoning and self-learning. In particular, the proposed framework first generates a labeled dataset for training a Convolutional Neural Network (CNN) for object detection in clutter. These detections are used to guide a scene-level optimization process, which considers the interactions between the different objects present in the clutter to output pose estimates of high precision. Furthermore, confident estimates are used to label online real images from multiple views and re-train the process in a self-learning pipeline. Experimental results indicate that this process is quickly able to identify in cluttered scenes physically-consistent object poses that are more precise than the ones found by reasoning over individual instances of objects. Furthermore, the quality of pose estimates increases over time given the self-learning process.



### Person Re-Identification in Identity Regression Space
- **Arxiv ID**: http://arxiv.org/abs/1806.09695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09695v1)
- **Published**: 2018-06-25 20:40:25+00:00
- **Updated**: 2018-06-25 20:40:25+00:00
- **Authors**: Hanxiao Wang, Xiatian Zhu, Shaogang Gong, Tao Xiang
- **Comment**: accepted by International Journal of Computer Vision (IJCV)
- **Journal**: None
- **Summary**: Most existing person re-identification (re-id) methods are unsuitable for real-world deployment due to two reasons: Unscalability to large population size, and Inadaptability over time. In this work, we present a unified solution to address both problems. Specifically, we propose to construct an Identity Regression Space (IRS) based on embedding different training person identities (classes) and formulate re-id as a regression problem solved by identity regression in the IRS. The IRS approach is characterised by a closed-form solution with high learning efficiency and an inherent incremental learning capability with human-in-the-loop. Extensive experiments on four benchmarking datasets(VIPeR, CUHK01, CUHK03 and Market-1501) show that the IRS model not only outperforms state-of-the-art re-id methods, but also is more scalable to large re-id population size by rapidly updating model and actively selecting informative samples with reduced human labelling effort.



