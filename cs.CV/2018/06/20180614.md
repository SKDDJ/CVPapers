# Arxiv Papers in cs.CV on 2018-06-14
### SCSP: Spectral Clustering Filter Pruning with Soft Self-adaption Manners
- **Arxiv ID**: http://arxiv.org/abs/1806.05320v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05320v1)
- **Published**: 2018-06-14 01:24:17+00:00
- **Updated**: 2018-06-14 01:24:17+00:00
- **Authors**: Huiyuan Zhuo, Xuelin Qian, Yanwei Fu, Heng Yang, Xiangyang Xue
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNN) has achieved significant success in computer vision field. However, the high computational cost of the deep complex models prevents the deployment on edge devices with limited memory and computational resource. In this paper, we proposed a novel filter pruning for convolutional neural networks compression, namely spectral clustering filter pruning with soft self-adaption manners (SCSP). We first apply spectral clustering on filters layer by layer to explore their intrinsic connections and only count on efficient groups. By self-adaption manners, the pruning operations can be done in few epochs to let the network gradually choose meaningful groups. According to this strategy, we not only achieve model compression while keeping considerable performance, but also find a novel angle to interpret the model compression process.



### Hierarchical interpretations for neural network predictions
- **Arxiv ID**: http://arxiv.org/abs/1806.05337v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.05337v2)
- **Published**: 2018-06-14 02:41:03+00:00
- **Updated**: 2019-01-16 07:15:40+00:00
- **Authors**: Chandan Singh, W. James Murdoch, Bin Yu
- **Comment**: Published in ICLR 2019
- **Journal**: ICLR 2019
- **Summary**: Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables. However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications. To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method, agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. Using examples from Stanford Sentiment Treebank and ImageNet, we show that ACD is effective at diagnosing incorrect predictions and identifying dataset bias. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.



### From Trailers to Storylines: An Efficient Way to Learn from Movies
- **Arxiv ID**: http://arxiv.org/abs/1806.05341v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05341v1)
- **Published**: 2018-06-14 02:52:09+00:00
- **Updated**: 2018-06-14 02:52:09+00:00
- **Authors**: Qingqiu Huang, Yuanjun Xiong, Yu Xiong, Yuqi Zhang, Dahua Lin
- **Comment**: None
- **Journal**: None
- **Summary**: The millions of movies produced in the human history are valuable resources for computer vision research. However, learning a vision model from movie data would meet with serious difficulties. A major obstacle is the computational cost -- the length of a movie is often over one hour, which is substantially longer than the short video clips that previous study mostly focuses on. In this paper, we explore an alternative approach to learning vision models from movies. Specifically, we consider a framework comprised of a visual module and a temporal analysis module. Unlike conventional learning methods, the proposed approach learns these modules from different sets of data -- the former from trailers while the latter from movies. This allows distinctive visual features to be learned within a reasonable budget while still preserving long-term temporal structures across an entire movie. We construct a large-scale dataset for this study and define a series of tasks on top. Experiments on this dataset showed that the proposed method can substantially reduce the training time while obtaining highly effective features and coherent temporal structures.



### Convex Class Model on Symmetric Positive Definite Manifolds
- **Arxiv ID**: http://arxiv.org/abs/1806.05343v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05343v2)
- **Published**: 2018-06-14 02:57:35+00:00
- **Updated**: 2019-05-29 08:17:10+00:00
- **Authors**: Kun Zhao, Arnold Wiliem, Shaokang Chen, Brian C. Lovell
- **Comment**: None
- **Journal**: None
- **Summary**: The effectiveness of Symmetric Positive Definite (SPD) manifold features has been proven in various computer vision tasks. However, due to the non-Euclidean geometry of these features, existing Euclidean machineries cannot be directly used. In this paper, we tackle the classification tasks with limited training data on SPD manifolds. Our proposed framework, named Manifold Convex Class Model, represents each class on SPD manifolds using a convex model, and classification can be performed by computing distances to the convex models. We provide three methods based on different metrics to address the optimization problem of the smallest distance of a point to the convex model on SPD manifold. The efficacy of our proposed framework is demonstrated both on synthetic data and several computer vision tasks including object recognition, texture classification, person re-identification and traffic scene classification.



### View-volume Network for Semantic Scene Completion from a Single Depth Image
- **Arxiv ID**: http://arxiv.org/abs/1806.05361v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05361v1)
- **Published**: 2018-06-14 04:42:05+00:00
- **Updated**: 2018-06-14 04:42:05+00:00
- **Authors**: Yu-Xiao Guo, Xin Tong
- **Comment**: To appear in IJCAI 2018
- **Journal**: None
- **Summary**: We introduce a View-Volume convolutional neural network (VVNet) for inferring the occupancy and semantic labels of a volumetric 3D scene from a single depth image. The VVNet concatenates a 2D view CNN and a 3D volume CNN with a differentiable projection layer. Given a single RGBD image, our method extracts the detailed geometric features from the input depth image with a 2D view CNN and then projects the features into a 3D volume according to the input depth map via a projection layer. After that, we learn the 3D context information of the scene with a 3D volume CNN for computing the result volumetric occupancy and semantic labels. With combined 2D and 3D representations, the VVNet efficiently reduces the computational cost, enables feature extraction from multi-channel high resolution inputs, and thus significantly improves the result accuracy. We validate our method and demonstrate its efficiency and effectiveness on both synthetic SUNCG and real NYU dataset.



### Fire SSD: Wide Fire Modules based Single Shot Detector on Edge Device
- **Arxiv ID**: http://arxiv.org/abs/1806.05363v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05363v5)
- **Published**: 2018-06-14 04:56:41+00:00
- **Updated**: 2018-12-11 03:14:12+00:00
- **Authors**: Hengfui Liau, Nimmagadda Yamini, YengLiong Wong
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: With the emergence of edge computing, there is an increasing need for running convolutional neural network based object detection on small form factor edge computing devices with limited compute and thermal budget for applications such as video surveillance. To address this problem, efficient object detection frameworks such as YOLO and SSD were proposed. However, SSD based object detection that uses VGG16 as backend network is insufficient to achieve real time speed on edge devices. To further improve the detection speed, the backend network is replaced by more efficient networks such as SqueezeNet and MobileNet. Although the speed is greatly improved, it comes with a price of lower accuracy. In this paper, we propose an efficient SSD named Fire SSD. Fire SSD achieves 70.7mAP on Pascal VOC 2007 test set. Fire SSD achieves the speed of 30.6FPS on low power mainstream CPU and is about 6 times faster than SSD300 and has about 4 times smaller model size. Fire SSD also achieves 22.2FPS on integrated GPU.



### Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.05372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05372v1)
- **Published**: 2018-06-14 05:45:22+00:00
- **Updated**: 2018-06-14 05:45:22+00:00
- **Authors**: Ming Sun, Yuchen Yuan, Feng Zhou, Errui Ding
- **Comment**: None
- **Journal**: None
- **Summary**: Attention-based learning for fine-grained image recognition remains a challenging task, where most of the existing methods treat each object part in isolation, while neglecting the correlations among them. In addition, the multi-stage or multi-scale mechanisms involved make the existing methods less efficient and hard to be trained end-to-end. In this paper, we propose a novel attention-based convolutional neural network (CNN) which regulates multiple object parts among different input images. Our method first learns multiple attention region features of each input image through the one-squeeze multi-excitation (OSME) module, and then apply the multi-attention multi-class constraint (MAMC) in a metric learning framework. For each anchor feature, the MAMC functions by pulling same-attention same-class features closer, while pushing different-attention or different-class features away. Our method can be easily trained end-to-end, and is highly efficient which requires only one training stage. Moreover, we introduce Dogs-in-the-Wild, a comprehensive dog species dataset that surpasses similar existing datasets by category coverage, data volume and annotation quality. This dataset will be released upon acceptance to facilitate the research of fine-grained image recognition. Extensive experiments are conducted to show the substantial improvements of our method on four benchmark datasets.



### Single Image Reflection Separation with Perceptual Losses
- **Arxiv ID**: http://arxiv.org/abs/1806.05376v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05376v1)
- **Published**: 2018-06-14 06:03:03+00:00
- **Updated**: 2018-06-14 06:03:03+00:00
- **Authors**: Xuaner Zhang, Ren Ng, Qifeng Chen
- **Comment**: 9 pages, 8 figures, CVPR 2018
- **Journal**: None
- **Summary**: We present an approach to separating reflection from a single image. The approach uses a fully convolutional network trained end-to-end with losses that exploit low-level and high-level image information. Our loss function includes two perceptual losses: a feature loss from a visual perception network, and an adversarial loss that encodes characteristics of images in the transmission layers. We also propose a novel exclusion loss that enforces pixel-level layer separation. We create a dataset of real-world images with reflection and corresponding ground-truth transmission layers for quantitative evaluation and model training. We validate our method through comprehensive quantitative experiments and show that our approach outperforms state-of-the-art reflection removal methods in PSNR, SSIM, and perceptual user study. We also extend our method to two other image enhancement tasks to demonstrate the generality of our approach.



### PCAS: Pruning Channels with Attention Statistics for Deep Network Compression
- **Arxiv ID**: http://arxiv.org/abs/1806.05382v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.05382v3)
- **Published**: 2018-06-14 06:28:59+00:00
- **Updated**: 2019-08-20 06:58:29+00:00
- **Authors**: Kohei Yamamoto, Kurato Maeno
- **Comment**: Accepted at BMVC 2019 (Oral)
- **Journal**: 30th British Machine Vision Conference (BMVC), 2019, 138
- **Summary**: Compression techniques for deep neural networks are important for implementing them on small embedded devices. In particular, channel-pruning is a useful technique for realizing compact networks. However, many conventional methods require manual setting of compression ratios in each layer. It is difficult to analyze the relationships between all layers, especially for deeper models. To address these issues, we propose a simple channel-pruning technique based on attention statistics that enables to evaluate the importance of channels. We improved the method by means of a criterion for automatic channel selection, using a single compression ratio for the entire model in place of per-layer model analysis. The proposed approach achieved superior performance over conventional methods with respect to accuracy and the computational costs for various models and datasets. We provide analysis results for behavior of the proposed criterion on different datasets to demonstrate its favorable properties for channel pruning.



### Selfless Sequential Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.05421v5
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.05421v5)
- **Published**: 2018-06-14 09:06:10+00:00
- **Updated**: 2019-04-12 14:47:02+00:00
- **Authors**: Rahaf Aljundi, Marcus Rohrbach, Tinne Tuytelaars
- **Comment**: Published as a conference paper at ICLR 2019
- **Journal**: None
- **Summary**: Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task. In this paper we look at a scenario with fixed model capacity, and postulate that the learning process should not be selfish, i.e. it should account for future tasks to be added and thus leave enough capacity for them. To achieve Selfless Sequential Learning we study different regularization strategies and activation functions. We find that imposing sparsity at the level of the representation (i.e.~neuron activations) is more beneficial for sequential learning than encouraging parameter sparsity. In particular, we propose a novel regularizer, that encourages representation sparsity by means of neural inhibition. It results in few active neurons which in turn leaves more free neurons to be utilized by upcoming tasks. As neural inhibition over an entire layer can be too drastic, especially for complex tasks requiring strong representations, our regularizer only inhibits other neurons in a local neighbourhood, inspired by lateral inhibition processes in the brain. We combine our novel regularizer, with state-of-the-art lifelong learning methods that penalize changes to important previously learned parts of the network. We show that our new regularizer leads to increased sparsity which translates in consistent performance improvement %over alternative regularizers we studied on diverse datasets.



### Deep Generative Models in the Real-World: An Open Challenge from Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/1806.05452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05452v1)
- **Published**: 2018-06-14 10:23:54+00:00
- **Updated**: 2018-06-14 10:23:54+00:00
- **Authors**: Xiaoran Chen, Nick Pawlowski, Martin Rajchl, Ben Glocker, Ender Konukoglu
- **Comment**: 10 pages. 3 figures
- **Journal**: None
- **Summary**: Recent advances in deep learning led to novel generative modeling techniques that achieve unprecedented quality in generated samples and performance in learning complex distributions in imaging data. These new models in medical image computing have important applications that form clinically relevant and very challenging unsupervised learning problems. In this paper, we explore the feasibility of using state-of-the-art auto-encoder-based deep generative models, such as variational and adversarial auto-encoders, for one such task: abnormality detection in medical imaging. We utilize typical, publicly available datasets with brain scans from healthy subjects and patients with stroke lesions and brain tumors. We use the data from healthy subjects to train different auto-encoder based models to learn the distribution of healthy images and detect pathologies as outliers. Models that can better learn the data distribution should be able to detect outliers more accurately. We evaluate the detection performance of deep generative models and compare them with non-deep learning based approaches to provide a benchmark of the current state of research. We conclude that abnormality detection is a challenging task for deep generative models and large room exists for improvement. In order to facilitate further research, we aim to provide carefully pre-processed imaging data available to the research community.



### Analysis of the Effect of Unexpected Outliers in the Classification of Spectroscopy Data
- **Arxiv ID**: http://arxiv.org/abs/1806.05455v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05455v1)
- **Published**: 2018-06-14 10:44:03+00:00
- **Updated**: 2018-06-14 10:44:03+00:00
- **Authors**: Frank G. Glavin, Michael G. Madden
- **Comment**: Irish Conference on Artificial Intelligence and Cognitive Science
  (2009)
- **Journal**: Glavin, Frank G., and Michael G. Madden. "Analysis of the effect
  of unexpected outliers in the classification of spectroscopy data."
  Artificial Intelligence and Cognitive Science (AICS), pp. 124-133. Springer,
  Berlin, Heidelberg, 2009
- **Summary**: Multi-class classification algorithms are very widely used, but we argue that they are not always ideal from a theoretical perspective, because they assume all classes are characterized by the data, whereas in many applications, training data for some classes may be entirely absent, rare, or statistically unrepresentative. We evaluate one-sided classifiers as an alternative, since they assume that only one class (the target) is well characterized. We consider a task of identifying whether a substance contains a chlorinated solvent, based on its chemical spectrum. For this application, it is not really feasible to collect a statistically representative set of outliers, since that group may contain \emph{anything} apart from the target chlorinated solvents. Using a new one-sided classification toolkit, we compare a One-Sided k-NN algorithm with two well-known binary classification algorithms, and conclude that the one-sided classifier is more robust to unexpected outliers.



### Efficient Active Learning for Image Classification and Segmentation using a Sample Selection and Conditional Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1806.05473v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05473v4)
- **Published**: 2018-06-14 11:29:10+00:00
- **Updated**: 2019-10-22 15:17:27+00:00
- **Authors**: Dwarikanath Mahapatra, Behzad Bozorgtabar, Jean-Philippe Thiran, Mauricio Reyes
- **Comment**: None
- **Journal**: None
- **Summary**: Training robust deep learning (DL) systems for medical image classification or segmentation is challenging due to limited images covering different disease types and severity. We propose an active learning (AL) framework to select most informative samples and add to the training data. We use conditional generative adversarial networks (cGANs) to generate realistic chest xray images with different disease characteristics by conditioning its generation on a real image sample. Informative samples to add to the training set are identified using a Bayesian neural network. Experiments show our proposed AL framework is able to achieve state of the art performance by using about 35% of the full dataset, thus saving significant time and effort over conventional methods.



### Copycat CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/1806.05476v1
- **DOI**: 10.1109/IJCNN.2018.8489592
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.05476v1)
- **Published**: 2018-06-14 11:32:27+00:00
- **Updated**: 2018-06-14 11:32:27+00:00
- **Authors**: Jacson Rodrigues Correia-Silva, Rodrigo F. Berriel, Claudine Badue, Alberto F. de Souza, Thiago Oliveira-Santos
- **Comment**: 8 pages, 3 figures, accepted by IJCNN 2018
- **Journal**: None
- **Summary**: In the past few years, Convolutional Neural Networks (CNNs) have been achieving state-of-the-art performance on a variety of problems. Many companies employ resources and money to generate these models and provide them as an API, therefore it is in their best interest to protect them, i.e., to avoid that someone else copies them. Recent studies revealed that state-of-the-art CNNs are vulnerable to adversarial examples attacks, and this weakness indicates that CNNs do not need to operate in the problem domain (PD). Therefore, we hypothesize that they also do not need to be trained with examples of the PD in order to operate in it.   Given these facts, in this paper, we investigate if a target black-box CNN can be copied by persuading it to confess its knowledge through random non-labeled data. The copy is two-fold: i) the target network is queried with random data and its predictions are used to create a fake dataset with the knowledge of the network; and ii) a copycat network is trained with the fake dataset and should be able to achieve similar performance as the target network.   This hypothesis was evaluated locally in three problems (facial expression, object, and crosswalk classification) and against a cloud-based API. In the copy attacks, images from both non-problem domain and PD were used. All copycat networks achieved at least 93.7% of the performance of the original models with non-problem domain data, and at least 98.6% using additional data from the PD. Additionally, the copycat CNN successfully copied at least 97.3% of the performance of the Microsoft Azure Emotion API. Our results show that it is possible to create a copycat CNN by simply querying a target network as black-box with random non-labeled data.



### Scrutinizing and De-Biasing Intuitive Physics with Neural Stethoscopes
- **Arxiv ID**: http://arxiv.org/abs/1806.05502v5
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.05502v5)
- **Published**: 2018-06-14 12:35:50+00:00
- **Updated**: 2019-09-06 13:49:37+00:00
- **Authors**: Fabian B. Fuchs, Oliver Groth, Adam R. Kosiorek, Alex Bewley, Markus Wulfmeier, Andrea Vedaldi, Ingmar Posner
- **Comment**: None
- **Journal**: None
- **Summary**: Visually predicting the stability of block towers is a popular task in the domain of intuitive physics. While previous work focusses on prediction accuracy, a one-dimensional performance measure, we provide a broader analysis of the learned physical understanding of the final model and how the learning process can be guided. To this end, we introduce neural stethoscopes as a general purpose framework for quantifying the degree of importance of specific factors of influence in deep neural networks as well as for actively promoting and suppressing information as appropriate. In doing so, we unify concepts from multitask learning as well as training with auxiliary and adversarial losses. We apply neural stethoscopes to analyse the state-of-the-art neural network for stability prediction. We show that the baseline model is susceptible to being misled by incorrect visual cues. This leads to a performance breakdown to the level of random guessing when training on scenarios where visual cues are inversely correlated with stability. Using stethoscopes to promote meaningful feature extraction increases performance from 51% to 90% prediction accuracy. Conversely, training on an easy dataset where visual cues are positively correlated with stability, the baseline model learns a bias leading to poor performance on a harder dataset. Using an adversarial stethoscope, the network is successfully de-biased, leading to a performance increase from 66% to 88%.



### Dense Light Field Reconstruction From Sparse Sampling Using Residual Network
- **Arxiv ID**: http://arxiv.org/abs/1806.05506v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05506v2)
- **Published**: 2018-06-14 12:45:55+00:00
- **Updated**: 2018-08-11 11:34:57+00:00
- **Authors**: Mantang Guo, Hao Zhu, Guoqing Zhou, Qing Wang
- **Comment**: None
- **Journal**: None
- **Summary**: A light field records numerous light rays from a real-world scene. However, capturing a dense light field by existing devices is a time-consuming process. Besides, reconstructing a large amount of light rays equivalent to multiple light fields using sparse sampling arises a severe challenge for existing methods. In this paper, we present a learning based method to reconstruct multiple novel light fields between two mutually independent light fields. We indicate that light rays distributed in different light fields have the same consistent constraints under a certain condition. The most significant constraint is a depth related correlation between angular and spatial dimensions. Our method avoids working out the error-sensitive constraint by employing a deep neural network. We solve residual values of pixels on epipolar plane image (EPI) to reconstruct novel light fields. Our method is able to reconstruct 2 to 4 novel light fields between two mutually independent input light fields. We also compare our results with those yielded by a number of alternatives elsewhere in the literature, which shows our reconstructed light fields have better structure similarity and occlusion relationship.



### ReConvNet: Video Object Segmentation with Spatio-Temporal Features Modulation
- **Arxiv ID**: http://arxiv.org/abs/1806.05510v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05510v2)
- **Published**: 2018-06-14 12:52:28+00:00
- **Updated**: 2018-06-18 13:57:58+00:00
- **Authors**: Francesco Lattari, Marco Ciccone, Matteo Matteucci, Jonathan Masci, Francesco Visin
- **Comment**: CVPR Workshop - DAVIS Challenge 2018
- **Journal**: None
- **Summary**: We introduce ReConvNet, a recurrent convolutional architecture for semi-supervised video object segmentation that is able to fast adapt its features to focus on any specific object of interest at inference time. Generalization to new objects never observed during training is known to be a hard task for supervised approaches that would need to be retrained. To tackle this problem, we propose a more efficient solution that learns spatio-temporal features self-adapting to the object of interest via conditional affine transformations. This approach is simple, can be trained end-to-end and does not necessarily require extra training steps at inference time. Our method shows competitive results on DAVIS2016 with respect to state-of-the art approaches that use online fine-tuning, and outperforms them on DAVIS2017. ReConvNet shows also promising results on the DAVIS-Challenge 2018 winning the $10$-th position.



### NetScore: Towards Universal Metrics for Large-scale Performance Analysis of Deep Neural Networks for Practical On-Device Edge Usage
- **Arxiv ID**: http://arxiv.org/abs/1806.05512v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.05512v2)
- **Published**: 2018-06-14 12:55:35+00:00
- **Updated**: 2018-08-26 01:33:02+00:00
- **Authors**: Alexander Wong
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Much of the focus in the design of deep neural networks has been on improving accuracy, leading to more powerful yet highly complex network architectures that are difficult to deploy in practical scenarios, particularly on edge devices such as mobile and other consumer devices given their high computational and memory requirements. As a result, there has been a recent interest in the design of quantitative metrics for evaluating deep neural networks that accounts for more than just model accuracy as the sole indicator of network performance. In this study, we continue the conversation towards universal metrics for evaluating the performance of deep neural networks for practical on-device edge usage. In particular, we propose a new balanced metric called NetScore, which is designed specifically to provide a quantitative assessment of the balance between accuracy, computational complexity, and network architecture complexity of a deep neural network, which is important for on-device edge operation. In what is one of the largest comparative analysis between deep neural networks in literature, the NetScore metric, the top-1 accuracy metric, and the popular information density metric were compared across a diverse set of 60 different deep convolutional neural networks for image classification on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC 2012) dataset. The evaluation results across these three metrics for this diverse set of networks are presented in this study to act as a reference guide for practitioners in the field. The proposed NetScore metric, along with the other tested metrics, are by no means perfect, but the hope is to push the conversation towards better universal metrics for evaluating deep neural networks for use in practical on-device edge scenarios to help guide practitioners in model design for such scenarios.



### EL-GAN: Embedding Loss Driven Generative Adversarial Networks for Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/1806.05525v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05525v2)
- **Published**: 2018-06-14 13:20:20+00:00
- **Updated**: 2018-07-05 16:54:27+00:00
- **Authors**: Mohsen Ghafoorian, Cedric Nugteren, Nóra Baka, Olaf Booij, Michael Hofmann
- **Comment**: 14 pages, 7 figures
- **Journal**: None
- **Summary**: Convolutional neural networks have been successfully applied to semantic segmentation problems. However, there are many problems that are inherently not pixel-wise classification problems but are nevertheless frequently formulated as semantic segmentation. This ill-posed formulation consequently necessitates hand-crafted scenario-specific and computationally expensive post-processing methods to convert the per pixel probability maps to final desired outputs. Generative adversarial networks (GANs) can be used to make the semantic segmentation network output to be more realistic or better structure-preserving, decreasing the dependency on potentially complex post-processing. In this work, we propose EL-GAN: a GAN framework to mitigate the discussed problem using an embedding loss. With EL-GAN, we discriminate based on learned embeddings of both the labels and the prediction at the same time. This results in more stable training due to having better discriminative information, benefiting from seeing both `fake' and `real' predictions at the same time. This substantially stabilizes the adversarial training process. We use the TuSimple lane marking challenge to demonstrate that with our proposed framework it is viable to overcome the inherent anomalies of posing it as a semantic segmentation problem. Not only is the output considerably more similar to the labels when compared to conventional methods, the subsequent post-processing is also simpler and crosses the competitive 96% accuracy threshold.



### Correlation Tracking via Robust Region Proposals
- **Arxiv ID**: http://arxiv.org/abs/1806.05530v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05530v1)
- **Published**: 2018-06-14 13:30:30+00:00
- **Updated**: 2018-06-14 13:30:30+00:00
- **Authors**: Yuqi Han, Jinghong Nan, Zengshuo Zhang, Jingjing Wang, Baojun Zhao
- **Comment**: 4 pages, 3 figures, IET2018
- **Journal**: None
- **Summary**: Recently, correlation filter-based trackers have received extensive attention due to their simplicity and superior speed. However, such trackers perform poorly when the target undergoes occlusion, viewpoint change or other challenging attributes due to pre-defined sampling strategy. To tackle these issues, in this paper, we propose an adaptive region proposal scheme to facilitate visual tracking. To be more specific, a novel tracking monitoring indicator is advocated to forecast tracking failure. Afterwards, we incorporate detection and scale proposals respectively, to recover from model drift as well as handle aspect ratio variation. We test the proposed algorithm on several challenging sequences, which have demonstrated that the proposed tracker performs favourably against state-of-the-art trackers.



### Cardiac Motion Scoring with Segment- and Subject-level Non-Local Modeling
- **Arxiv ID**: http://arxiv.org/abs/1806.05569v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05569v1)
- **Published**: 2018-06-14 14:20:59+00:00
- **Updated**: 2018-06-14 14:20:59+00:00
- **Authors**: Wufeng Xue, Gary Brahm, Stephanie Leung, Ogla Shmuilovich, Shuo Li
- **Comment**: MICCAI 2018
- **Journal**: None
- **Summary**: Motion scoring of cardiac myocardium is of paramount importance for early detection and diagnosis of various cardiac disease. It aims at identifying regional wall motions into one of the four types including normal, hypokinetic, akinetic, and dyskinetic, and is extremely challenging due to the complex myocardium deformation and subtle inter-class difference of motion patterns. All existing work on automated motion analysis are focused on binary abnormality detection to avoid the much more demanding motion scoring, which is urgently required in real clinical practice yet has never been investigated before. In this work, we propose Cardiac-MOS, the first powerful method for cardiac motion scoring from cardiac MR sequences based on deep convolution neural network. Due to the locality of convolution, the relationship between distant non-local responses of the feature map cannot be explored, which is closely related to motion difference between segments. In Cardiac-MOS, such non-local relationship is modeled with non-local neural network within each segment and across all segments of one subject, i.e., segment- and subject-level non-local modeling, and lead to obvious performance improvement. Besides, Cardiac-MOS can effectively extract motion information from MR sequences of various lengths by interpolating the convolution kernel along the temporal dimension, therefore can be applied to MR sequences of multiple sources. Experiments on 1440 myocardium segments of 90 subjects from short axis MR sequences of multiple lengths prove that Cardiac-MOS achieves reliable performance, with correlation of 0.926 for motion score index estimation and accuracy of 77.4\% for motion scoring. Cardiac-MOS also outperforms all existing work for binary abnormality detection. As the first automatic motion scoring solution, Cardiac-MOS demonstrates great potential in future clinical application.



### Direct Automated Quantitative Measurement of Spine via Cascade Amplifier Regression Network
- **Arxiv ID**: http://arxiv.org/abs/1806.05570v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05570v1)
- **Published**: 2018-06-14 14:21:01+00:00
- **Updated**: 2018-06-14 14:21:01+00:00
- **Authors**: Shumao Pang, Stephanie Leung, Ilanit Ben Nachum, Qianjin Feng, Shuo Li
- **Comment**: Accepted by MICCAI 2018
- **Journal**: None
- **Summary**: Automated quantitative measurement of the spine (i.e., multiple indices estimation of heights, widths, areas, and so on for the vertebral body and disc) is of the utmost importance in clinical spinal disease diagnoses, such as osteoporosis, intervertebral disc degeneration, and lumbar disc herniation, yet still an unprecedented challenge due to the variety of spine structure and the high dimensionality of indices to be estimated. In this paper, we propose a novel cascade amplifier regression network (CARN), which includes the CARN architecture and local shape-constrained manifold regularization (LSCMR) loss function, to achieve accurate direct automated multiple indices estimation. The CARN architecture is composed of a cascade amplifier network (CAN) for expressive feature embedding and a linear regression model for multiple indices estimation. The CAN consists of cascade amplifier units (AUs), which are used for selective feature reuse by stimulating effective feature and suppressing redundant feature during propagating feature map between adjacent layers, thus an expressive feature embedding is obtained. During training, the LSCMR is utilized to alleviate overfitting and generate realistic estimation by learning the multiple indices distribution. Experiments on MR images of 195 subjects show that the proposed CARN achieves impressive performance with mean absolute errors of 1.2496 mm, 1.2887 mm, and 1.2692 mm for estimation of 15 heights of discs, 15 heights of vertebral bodies, and total indices respectively. The proposed method has great potential in clinical spinal disease diagnoses.



### Weakly-Supervised Learning for Tool Localization in Laparoscopic Videos
- **Arxiv ID**: http://arxiv.org/abs/1806.05573v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05573v2)
- **Published**: 2018-06-14 14:27:12+00:00
- **Updated**: 2018-07-18 08:21:13+00:00
- **Authors**: Armine Vardazaryan, Didier Mutter, Jacques Marescaux, Nicolas Padoy
- **Comment**: MICCAI LABELS 2018. Supplementary video: https://youtu.be/7VWVY04Z0MA
- **Journal**: None
- **Summary**: Surgical tool localization is an essential task for the automatic analysis of endoscopic videos. In the literature, existing methods for tool localization, tracking and segmentation require training data that is fully annotated, thereby limiting the size of the datasets that can be used and the generalization of the approaches. In this work, we propose to circumvent the lack of annotated data with weak supervision. We propose a deep architecture, trained solely on image level annotations, that can be used for both tool presence detection and localization in surgical videos. Our architecture relies on a fully convolutional neural network, trained end-to-end, enabling us to localize surgical tools without explicit spatial annotations. We demonstrate the benefits of our approach on a large public dataset, Cholec80, which is fully annotated with binary tool presence information and of which 5 videos have been fully annotated with bounding boxes and tool centers for the evaluation.



### Tract orientation mapping for bundle-specific tractography
- **Arxiv ID**: http://arxiv.org/abs/1806.05580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05580v1)
- **Published**: 2018-06-14 14:38:06+00:00
- **Updated**: 2018-06-14 14:38:06+00:00
- **Authors**: Jakob Wasserthal, Peter F. Neher, Klaus H. Maier-Hein
- **Comment**: Accepted at MICCAI 2018
- **Journal**: None
- **Summary**: While the major white matter tracts are of great interest to numerous studies in neuroscience and medicine, their manual dissection in larger cohorts from diffusion MRI tractograms is time-consuming, requires expert knowledge and is hard to reproduce. Tract orientation mapping (TOM) is a novel concept that facilitates bundle-specific tractography based on a learned mapping from the original fiber orientation distribution function (fODF) peaks to a list of tract orientation maps (also abbr. TOM). Each TOM represents one of the known tracts with each voxel containing no more than one orientation vector. TOMs can act as a prior or even as direct input for tractography. We use an encoder-decoder fully-convolutional neural network architecture to learn the required mapping. In comparison to previous concepts for the reconstruction of specific bundles, the presented one avoids various cumbersome processing steps like whole brain tractography, atlas registration or clustering. We compare it to four state of the art bundle recognition methods on 20 different bundles in a total of 105 subjects from the Human Connectome Project. Results are anatomically convincing even for difficult tracts, while reaching low angular errors, unprecedented runtimes and top accuracy values (Dice). Our code and our data are openly available.



### Semantic Image Retrieval by Uniting Deep Neural Networks and Cognitive Architectures
- **Arxiv ID**: http://arxiv.org/abs/1806.06946v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.06946v1)
- **Published**: 2018-06-14 14:53:53+00:00
- **Updated**: 2018-06-14 14:53:53+00:00
- **Authors**: Alexey Potapov, Innokentii Zhdanov, Oleg Scherbakov, Nikolai Skorobogatko, Hugo Latapie, Enzo Fenoglio
- **Comment**: None
- **Journal**: None
- **Summary**: Image and video retrieval by their semantic content has been an important and challenging task for years, because it ultimately requires bridging the symbolic/subsymbolic gap. Recent successes in deep learning enabled detection of objects belonging to many classes greatly outperforming traditional computer vision techniques. However, deep learning solutions capable of executing retrieval queries are still not available. We propose a hybrid solution consisting of a deep neural network for object detection and a cognitive architecture for query execution. Specifically, we use YOLOv2 and OpenCog. Queries allowing the retrieval of video frames containing objects of specified classes and specified spatial arrangement are implemented.



### There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average
- **Arxiv ID**: http://arxiv.org/abs/1806.05594v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.05594v3)
- **Published**: 2018-06-14 14:58:36+00:00
- **Updated**: 2019-02-21 15:26:31+00:00
- **Authors**: Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, Andrew Gordon Wilson
- **Comment**: Appears at ICLR 2019
- **Journal**: None
- **Summary**: Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. To understand consistency regularization, we conceptually explore how loss geometry interacts with training procedures. The consistency loss dramatically improves generalization performance over supervised-only training; however, we show that SGD struggles to converge on the consistency loss and continues to make large steps that lead to changes in predictions on the test data. Motivated by these observations, we propose to train consistency-based methods with Stochastic Weight Averaging (SWA), a recent approach which averages weights along the trajectory of SGD with a modified learning rate schedule. We also propose fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning rate schedule. With weight averaging, we achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100, over many different quantities of labeled training data. For example, we achieve 5.0% error on CIFAR-10 with only 4000 labels, compared to the previous best result in the literature of 6.3%.



### From Self-ception to Image Self-ception: A method to represent an image with its own approximations
- **Arxiv ID**: http://arxiv.org/abs/1806.05610v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05610v1)
- **Published**: 2018-06-14 15:28:28+00:00
- **Updated**: 2018-06-14 15:28:28+00:00
- **Authors**: Hamed Shah-Hosseini
- **Comment**: None
- **Journal**: None
- **Summary**: A concept of defining images based on its own approximate ones is proposed here, which is called 'Self-ception'. In this regard, an algorithm is proposed to implement the self-ception for images, which we call it 'Image Self-ception' since we use it for images. We can control the accuracy of this self-ception representation by deciding how many segments or regions we want to use for the representation. Some self-ception images are included in the paper. The video versions of the proposed image self-ception algorithm in action are shown in a YouTube channel (find it by Googling image self-ception).



### DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/1806.05620v2
- **DOI**: 10.1109/LRA.2018.2860039
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05620v2)
- **Published**: 2018-06-14 15:52:07+00:00
- **Updated**: 2018-08-15 08:09:22+00:00
- **Authors**: Berta Bescos, José M. Fácil, Javier Civera, José Neira
- **Comment**: This work has been accepted at IEEE Robotics and Automation Letters,
  and will be presented at the IEEE Conference on Intelligent Robots and
  Systems 2018
- **Journal**: IEEE Robotics and Automation Letters ( Volume: 3, Issue: 4, Oct.
  2018 )
- **Summary**: The assumption of scene rigidity is typical in SLAM algorithms. Such a strong assumption limits the use of most visual SLAM systems in populated real-world environments, which are the target of several relevant applications like service robotics or autonomous vehicles. In this paper we present DynaSLAM, a visual SLAM system that, building over ORB-SLAM2 [1], adds the capabilities of dynamic object detection and background inpainting. DynaSLAM is robust in dynamic scenarios for monocular, stereo and RGB-D configurations. We are capable of detecting the moving objects either by multi-view geometry, deep learning or both. Having a static map of the scene allows inpainting the frame background that has been occluded by such dynamic objects. We evaluate our system in public monocular, stereo and RGB-D datasets. We study the impact of several accuracy/speed trade-offs to assess the limits of the proposed methodology. DynaSLAM outperforms the accuracy of standard visual SLAM baselines in highly dynamic scenarios. And it also estimates a map of the static parts of the scene, which is a must for long-term applications in real-world environments.



### VoxCeleb2: Deep Speaker Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.05622v2
- **DOI**: 10.21437/Interspeech.2018-1929
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1806.05622v2)
- **Published**: 2018-06-14 15:59:12+00:00
- **Updated**: 2018-06-27 01:49:17+00:00
- **Authors**: Joon Son Chung, Arsha Nagrani, Andrew Zisserman
- **Comment**: To appear in Interspeech 2018. The audio-visual dataset can be
  downloaded from http://www.robots.ox.ac.uk/~vgg/data/voxceleb2 .
  1806.05622v2: minor fixes; 5 pages
- **Journal**: None
- **Summary**: The objective of this paper is speaker recognition under noisy and unconstrained conditions.   We make two key contributions. First, we introduce a very large-scale audio-visual speaker recognition dataset collected from open-source media. Using a fully automated pipeline, we curate VoxCeleb2 which contains over a million utterances from over 6,000 speakers. This is several times larger than any publicly available speaker recognition dataset.   Second, we develop and compare Convolutional Neural Network (CNN) models and training strategies that can effectively recognise identities from voice under various conditions. The models trained on the VoxCeleb2 dataset surpass the performance of previous works on a benchmark dataset by a significant margin.



### Grounded Textual Entailment
- **Arxiv ID**: http://arxiv.org/abs/1806.05645v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.05645v1)
- **Published**: 2018-06-14 16:56:44+00:00
- **Updated**: 2018-06-14 16:56:44+00:00
- **Authors**: Hoa Trong Vu, Claudio Greco, Aliia Erofeeva, Somayeh Jafaritazehjan, Guido Linders, Marc Tanti, Alberto Testoni, Raffaella Bernardi, Albert Gatt
- **Comment**: 15 pages, 2 figures, 14 tables, 2 appendices. Accepted in COLING 2018
- **Journal**: None
- **Summary**: Capturing semantic relations between sentences, such as entailment, is a long-standing challenge for computational semantics. Logic-based models analyse entailment in terms of possible worlds (interpretations, or situations) where a premise P entails a hypothesis H iff in all worlds where P is true, H is also true. Statistical models view this relationship probabilistically, addressing it in terms of whether a human would likely infer H from P. In this paper, we wish to bridge these two perspectives, by arguing for a visually-grounded version of the Textual Entailment task. Specifically, we ask whether models can perform better if, in addition to P and H, there is also an image (corresponding to the relevant "world" or "situation"). We use a multimodal version of the SNLI dataset (Bowman et al., 2015) and we compare "blind" and visually-augmented models of textual entailment. We show that visual information is beneficial, but we also conduct an in-depth error analysis that reveals that current multimodal models are not performing "grounding" in an optimal fashion.



### HGR-Net: A Fusion Network for Hand Gesture Segmentation and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.05653v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05653v3)
- **Published**: 2018-06-14 17:15:16+00:00
- **Updated**: 2019-12-28 17:43:46+00:00
- **Authors**: Amirhossein Dadashzadeh, Alireza Tavakoli Targhi, Maryam Tahmasbi, Majid Mirmehdi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a two-stage convolutional neural network (CNN) architecture for robust recognition of hand gestures, called HGR-Net, where the first stage performs accurate semantic segmentation to determine hand regions, and the second stage identifies the gesture. The segmentation stage architecture is based on the combination of fully convolutional residual network and atrous spatial pyramid pooling. Although the segmentation sub-network is trained without depth information, it is particularly robust against challenges such as illumination variations and complex backgrounds. The recognition stage deploys a two-stream CNN, which fuses the information from the red-green-blue and segmented images by combining their deep representations in a fully connected layer before classification. Extensive experiments on public datasets show that our architecture achieves almost as good as state-of-the-art performance in segmentation and recognition of static hand gestures, at a fraction of training time, run time, and model size. Our method can operate at an average of 23 ms per frame.



### Interactive Classification for Deep Learning Interpretation
- **Arxiv ID**: http://arxiv.org/abs/1806.05660v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1806.05660v2)
- **Published**: 2018-06-14 17:36:02+00:00
- **Updated**: 2019-04-10 20:57:55+00:00
- **Authors**: Ángel Alexander Cabrera, Fred Hohman, Jason Lin, Duen Horng Chau
- **Comment**: Presented as a demo at CVPR'18
- **Journal**: None
- **Summary**: We present an interactive system enabling users to manipulate images to explore the robustness and sensitivity of deep learning image classifiers. Using modern web technologies to run in-browser inference, users can remove image features using inpainting algorithms and obtain new classifications in real time, which allows them to ask a variety of "what if" questions by experimentally modifying images and seeing how the model reacts. Our system allows users to compare and contrast what image regions humans and machine learning models use for classification, revealing a wide range of surprising results ranging from spectacular failures (e.g., a "water bottle" image becomes a "concert" when removing a person) to impressive resilience (e.g., a "baseball player" image remains correctly classified even without a glove or base). We demonstrate our system at The 2018 Conference on Computer Vision and Pattern Recognition (CVPR) for the audience to try it live. Our system is open-sourced at https://github.com/poloclub/interactive-classification. A video demo is available at https://youtu.be/llub5GcOF6w.



### GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations
- **Arxiv ID**: http://arxiv.org/abs/1806.05662v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.05662v3)
- **Published**: 2018-06-14 17:41:19+00:00
- **Updated**: 2018-07-02 20:24:33+00:00
- **Authors**: Zhilin Yang, Jake Zhao, Bhuwan Dhingra, Kaiming He, William W. Cohen, Ruslan Salakhutdinov, Yann LeCun
- **Comment**: None
- **Journal**: None
- **Summary**: Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (e.g., words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained (including GloVe embeddings, ELMo embeddings, and task-specific RNN hidden unit), or embedding-free units such as image pixels.



### Learning Human Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1806.05666v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05666v2)
- **Published**: 2018-06-14 17:50:36+00:00
- **Updated**: 2018-07-22 12:21:40+00:00
- **Authors**: Anurag Ranjan, Javier Romero, Michael J. Black
- **Comment**: British Machine Vision Conference 2018 (Oral)
- **Journal**: None
- **Summary**: The optical flow of humans is well known to be useful for the analysis of human action. Given this, we devise an optical flow algorithm specifically for human motion and show that it is superior to generic flow methods. Designing a method by hand is impractical, so we develop a new training database of image sequences with ground truth optical flow. For this we use a 3D model of the human body and motion capture data to synthesize realistic flow fields. We then train a convolutional neural network to estimate human flow fields from pairs of images. Since many applications in human motion analysis depend on speed, and we anticipate mobile applications, we base our method on SpyNet with several modifications. We demonstrate that our trained network is more accurate than a wide range of top methods on held-out test data and that it generalizes well to real image sequences. When combined with a person detector/tracker, the approach provides a full solution to the problem of 2D human flow estimation. Both the code and the dataset are available for research.



### Action Learning for 3D Point Cloud Based Organ Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.05724v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05724v1)
- **Published**: 2018-06-14 20:14:25+00:00
- **Updated**: 2018-06-14 20:14:25+00:00
- **Authors**: Xia Zhong, Mario Amrehn, Nishant Ravikumar, Shuqing Chen, Norbert Strobel, Annette Birkhold, Markus Kowarschik, Rebecca Fahrig, Andreas Maier
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel point cloud based 3D organ segmentation pipeline utilizing deep Q-learning. In order to preserve shape properties, the learning process is guided using a statistical shape model. The trained agent directly predicts piece-wise linear transformations for all vertices in each iteration. This mapping between the ideal transformation for an object outline estimation is learned based on image features. To this end, we introduce aperture features that extract gray values by sampling the 3D volume within the cone centered around the associated vertex and its normal vector. Our approach is also capable of estimating a hierarchical pyramid of non rigid deformations for multi-resolution meshes. In the application phase, we use a marginal approach to gradually estimate affine as well as non-rigid transformations. We performed extensive evaluations to highlight the robust performance of our approach on a variety of challenge data as well as clinical data. Additionally, our method has a run time ranging from 0.3 to 2.7 seconds to segment each organ. In addition, we show that the proposed method can be applied to different organs, X-ray based modalities, and scanning protocols without the need of transfer learning. As we learn actions, even unseen reference meshes can be processed as demonstrated in an example with the Visible Human. From this we conclude that our method is robust, and we believe that our method can be successfully applied to many more applications, in particular, in the interventional imaging space.



### Age and Gender Classification From Ear Images
- **Arxiv ID**: http://arxiv.org/abs/1806.05742v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05742v1)
- **Published**: 2018-06-14 21:00:09+00:00
- **Updated**: 2018-06-14 21:00:09+00:00
- **Authors**: Dogucan Yaman, Fevziye Irem Eyiokur, Nurdan Sezgin, Hazım Kemal Ekenel
- **Comment**: 7 pages, 3 figures, accepted for IAPR/IEEE International Workshop on
  Biometrics and Forensics (IWBF) 2018
- **Journal**: None
- **Summary**: In this paper, we present a detailed analysis on extracting soft biometric traits, age and gender, from ear images. Although there have been a few previous work on gender classification using ear images, to the best of our knowledge, this study is the first work on age classification from ear images. In the study, we have utilized both geometric features and appearance-based features for ear representation. The utilized geometric features are based on eight anthropometric landmarks and consist of 14 distance measurements and two area calculations. The appearance-based methods employ deep convolutional neural networks for representation and classification. The well-known convolutional neural network models, namely, AlexNet, VGG-16, GoogLeNet, and SqueezeNet have been adopted for the study. They have been fine-tuned on a large-scale ear dataset that has been built from the profile and close-to-profile face images in the Multi-PIE face dataset. This way, we have performed a domain adaptation. The updated models have been fine-tuned once more time on the small-scale target ear dataset, which contains only around 270 ear images for training. According to the experimental results, appearance-based methods have been found to be superior to the methods based on geometric features. We have achieved 94\% accuracy for gender classification, whereas 52\% accuracy has been obtained for age classification. These results indicate that ear images provide useful cues for age and gender classification, however, further work is required for age estimation.



### Insights on representational similarity in neural networks with canonical correlation
- **Arxiv ID**: http://arxiv.org/abs/1806.05759v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1806.05759v3)
- **Published**: 2018-06-14 22:34:11+00:00
- **Updated**: 2018-10-23 18:59:02+00:00
- **Authors**: Ari S. Morcos, Maithra Raghu, Samy Bengio
- **Comment**: NIPS 2018
- **Journal**: None
- **Summary**: Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difficult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method (Raghu et al., 2017). We first improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations. We also investigate the representational dynamics of RNNs, across both training and sequential timesteps, finding that RNNs converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of CNNs and RNNs, and demonstrate the utility of using CCA to understand representations.



### Generative Adversarial Networks and Perceptual Losses for Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1806.05764v2
- **DOI**: 10.1109/TIP.2019.2895768
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.05764v2)
- **Published**: 2018-06-14 23:14:14+00:00
- **Updated**: 2018-12-19 15:11:45+00:00
- **Authors**: Alice Lucas, Santiago Lopez Tapia, Rafael Molina, Aggelos K. Katsaggelos
- **Comment**: In the the review process IEEE TIP
- **Journal**: None
- **Summary**: Video super-resolution (VSR) has become one of the most critical problems in video processing. In the deep learning literature, recent works have shown the benefits of using adversarial-based and perceptual losses to improve the performance on various image restoration tasks; however, these have yet to be applied for video super-resolution. In this work, we propose a Generative Adversarial Network(GAN)-based formulation for VSR. We introduce a new generator network optimized for the VSR problem, named VSRResNet, along with a new discriminator architecture to properly guide VSRResNet during the GAN training. We further enhance our VSR GAN formulation with two regularizers, a distance loss in feature-space and pixel-space, to obtain our final VSRResFeatGAN model. We show that pre-training our generator with the Mean-Squared-Error loss only quantitatively surpasses the current state-of-the-art VSR models. Finally, we employ the PercepDist metric (Zhang et al., 2018) to compare state-of-the-art VSR models. We show that this metric more accurately evaluates the perceptual quality of SR solutions obtained from neural networks, compared with the commonly used PSNR/SSIM metrics. Finally, we show that our proposed model, the VSRResFeatGAN model, outperforms current state-of-the-art SR models, both quantitatively and qualitatively.



