# Arxiv Papers in cs.CV on 2018-06-06
### A Variational Image Segmentation Model based on Normalized Cut with Adaptive Similarity and Spatial Regularization
- **Arxiv ID**: http://arxiv.org/abs/1806.01977v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01977v3)
- **Published**: 2018-06-06 02:10:08+00:00
- **Updated**: 2020-02-25 17:32:17+00:00
- **Authors**: Faqiang Wang, Cuicui Zhao, Jun Liu, Haiyang Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Image segmentation is a fundamental research topic in image processing and computer vision. In the last decades, researchers developed a large number of segmentation algorithms for various applications. Amongst these algorithms, the Normalized cut (Ncut) segmentation method is widely applied due to its good performance. The Ncut segmentation model is an optimization problem whose energy is defined on a specifically designed graph. Thus, the segmentation results of the existing Ncut method are largely dependent on a pre-constructed similarity measure on the graph since this measure is usually given empirically by users. This flaw will lead to some undesirable segmentation results. In this paper, we propose a Ncut-based segmentation algorithm by integrating an adaptive similarity measure and spatial regularization. The proposed model combines the Parzen-Rosenblatt window method, non-local weights entropy, Ncut energy, and regularizer of phase field in a variational framework. Our method can adaptively update the similarity measure function by estimating some parameters. This adaptive procedure enables the proposed algorithm finding a better similarity measure for classification than the Ncut method. We provide some mathematical interpretation of the proposed adaptive similarity from multi-viewpoints such as statistics and convex optimization. In addition, the regularizer of phase field can guarantee that the proposed algorithm has a robust performance in the presence of noise, and it can also rectify the similarity measure with a spatial priori. The well-posed theory such as the existence of the minimizer for the proposed model is given in the paper. Compared with some existing segmentation methods such as the traditional Ncut-based model and the classical Chan-Vese model, the numerical experiments show that our method can provide promising segmentation results.



### Robust Structured Multi-task Multi-view Sparse Tracking
- **Arxiv ID**: http://arxiv.org/abs/1806.01985v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01985v1)
- **Published**: 2018-06-06 02:31:47+00:00
- **Updated**: 2018-06-06 02:31:47+00:00
- **Authors**: Mohammadreza Javanmardi, Xiaojun Qi
- **Comment**: IEEE International Conference on Multimedia and Expo (ICME), 2018
- **Journal**: None
- **Summary**: Sparse representation is a viable solution to visual tracking. In this paper, we propose a structured multi-task multi-view tracking (SMTMVT) method, which exploits the sparse appearance model in the particle filter framework to track targets under different challenges. Specifically, we extract features of the target candidates from different views and sparsely represent them by a linear combination of templates of different views. Unlike the conventional sparse trackers, SMTMVT not only jointly considers the relationship between different tasks and different views but also retains the structures among different views in a robust multi-task multi-view formulation. We introduce a numerical algorithm based on the proximal gradient method to quickly and effectively find the sparsity by dividing the optimization problem into two subproblems with the closed-form solutions. Both qualitative and quantitative evaluations on the benchmark of challenging image sequences demonstrate the superior performance of the proposed tracker against various state-of-the-art trackers.



### Deep Algorithms: designs for networks
- **Arxiv ID**: http://arxiv.org/abs/1806.02003v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.02003v1)
- **Published**: 2018-06-06 04:39:37+00:00
- **Updated**: 2018-06-06 04:39:37+00:00
- **Authors**: Abhejit Rajagopal, Shivkumar Chandrasekaran, Hrushikesh N. Mhaskar
- **Comment**: submitted to Thirty-second Annual Conference on Neural Information
  Processing Systems (NIPS), May 2018
- **Journal**: None
- **Summary**: A new design methodology for neural networks that is guided by traditional algorithm design is presented. To prove our point, we present two heuristics and demonstrate an algorithmic technique for incorporating additional weights in their signal-flow graphs. We show that with training the performance of these networks can not only exceed the performance of the initial network, but can match the performance of more-traditional neural network architectures. A key feature of our approach is that these networks are initialized with parameters that provide a known performance threshold for the architecture on a given task.



### A Peek Into the Hidden Layers of a Convolutional Neural Network Through a Factorization Lens
- **Arxiv ID**: http://arxiv.org/abs/1806.02012v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.02012v1)
- **Published**: 2018-06-06 05:27:38+00:00
- **Updated**: 2018-06-06 05:27:38+00:00
- **Authors**: Uday Singh Saini, Evangelos E. Papalexakis
- **Comment**: None
- **Journal**: None
- **Summary**: Despite their increasing popularity and success in a variety of supervised learning problems, deep neural networks are extremely hard to interpret and debug: Given and already trained Deep Neural Net, and a set of test inputs, how can we gain insight into how those inputs interact with different layers of the neural network? Furthermore, can we characterize a given deep neural network based on it's observed behavior on different inputs? In this paper we propose a novel factorization based approach on understanding how different deep neural networks operate. In our preliminary results, we identify fascinating patterns that link the factorization rank (typically used as a measure of interestingness in unsupervised data analysis) with how well or poorly the deep network has been trained. Finally, our proposed approach can help provide visual insights on how high-level. interpretable patterns of the network's input behave inside the hidden layers of the deep network.



### Joint Estimation of Age and Gender from Unconstrained Face Images using Lightweight Multi-task CNN for Mobile Applications
- **Arxiv ID**: http://arxiv.org/abs/1806.02023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02023v1)
- **Published**: 2018-06-06 06:22:16+00:00
- **Updated**: 2018-06-06 06:22:16+00:00
- **Authors**: Jia-Hong Lee, Yi-Ming Chan, Ting-Yen Chen, Chu-Song Chen
- **Comment**: To publish in the IEEE first International Conference on Multimedia
  Information Processing and Retrieval, 2018. (IEEE MIPR 2018)
- **Journal**: None
- **Summary**: Automatic age and gender classification based on unconstrained images has become essential techniques on mobile devices. With limited computing power, how to develop a robust system becomes a challenging task. In this paper, we present an efficient convolutional neural network (CNN) called lightweight multi-task CNN for simultaneous age and gender classification. Lightweight multi-task CNN uses depthwise separable convolution to reduce the model size and save the inference time. On the public challenging Adience dataset, the accuracy of age and gender classification is better than baseline multi-task CNN methods.



### Real-time Surgical Tools Recognition in Total Knee Arthroplasty Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.02031v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02031v1)
- **Published**: 2018-06-06 06:52:48+00:00
- **Updated**: 2018-06-06 06:52:48+00:00
- **Authors**: Moazzem Hossain, Soichi Nishio, Takafumi Hiranaka, Syoji Kobashi
- **Comment**: Accepted in ICIEV 2018 conference
- **Journal**: None
- **Summary**: Total knee arthroplasty (TKA) is a commonly performed surgical procedure to mitigate knee pain and improve functions for people with knee arthritis. The procedure is complicated due to the different surgical tools used in the stages of surgery. The recognition of surgical tools in real-time can be a solution to simplify surgical procedures for the surgeon. Also, the presence and movement of tools in surgery are crucial information for the recognition of the operational phase and to identify the surgical workflow. Therefore, this research proposes the development of a real-time system for the recognition of surgical tools during surgery using a convolutional neural network (CNN). Surgeons wearing smart glasses can see essential information about tools during surgery that may reduce the complication of the procedures. To evaluate the performance of the proposed method, we calculated and compared the Mean Average Precision (MAP) with state-of-the-art methods which are fast R-CNN and deformable part models (DPM). We achieved 87.6% mAP which is better in comparison to the existing methods. With the additional improvements of our proposed method, it can be a future point of reference, also the baseline for operational phase recognition.



### Why rankings of biomedical image analysis competitions should be interpreted with care
- **Arxiv ID**: http://arxiv.org/abs/1806.02051v2
- **DOI**: 10.1038/s41467-018-07619-7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02051v2)
- **Published**: 2018-06-06 08:13:27+00:00
- **Updated**: 2019-09-18 11:32:07+00:00
- **Authors**: Lena Maier-Hein, Matthias Eisenmann, Annika Reinke, Sinan Onogur, Marko Stankovic, Patrick Scholz, Tal Arbel, Hrvoje Bogunovic, Andrew P. Bradley, Aaron Carass, Carolin Feldmann, Alejandro F. Frangi, Peter M. Full, Bram van Ginneken, Allan Hanbury, Katrin Honauer, Michal Kozubek, Bennett A. Landman, Keno März, Oskar Maier, Klaus Maier-Hein, Bjoern H. Menze, Henning Müller, Peter F. Neher, Wiro Niessen, Nasir Rajpoot, Gregory C. Sharp, Korsuk Sirinukunwattana, Stefanie Speidel, Christian Stock, Danail Stoyanov, Abdel Aziz Taha, Fons van der Sommen, Ching-Wei Wang, Marc-André Weber, Guoyan Zheng, Pierre Jannin, Annette Kopp-Schneider
- **Comment**: Article published in Nature Communications: https://rdcu.be/bRmNr
- **Journal**: Nature communications 9.1 (2018): 5217
- **Summary**: International challenges have become the standard for validation of biomedical image analysis methods. Given their scientific impact, it is surprising that a critical analysis of common practices related to the organization of challenges has not yet been performed. In this paper, we present a comprehensive analysis of biomedical image analysis challenges conducted up to now. We demonstrate the importance of challenges and show that the lack of quality control has critical consequences. First, reproducibility and interpretation of the results is often hampered as only a fraction of relevant information is typically provided. Second, the rank of an algorithm is generally not robust to a number of variables such as the test data used for validation, the ranking scheme applied and the observers that make the reference annotations. To overcome these problems, we recommend best practice guidelines and define open research questions to be addressed in the future.



### Spatial Frequency Loss for Learning Convolutional Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1806.02336v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.02336v1)
- **Published**: 2018-06-06 08:34:12+00:00
- **Updated**: 2018-06-06 08:34:12+00:00
- **Authors**: Naoyuki Ichimura
- **Comment**: 9 pages, 7 figures, 1 table
- **Journal**: None
- **Summary**: This paper presents a learning method for convolutional autoencoders (CAEs) for extracting features from images. CAEs can be obtained by utilizing convolutional neural networks to learn an approximation to the identity function in an unsupervised manner. The loss function based on the pixel loss (PL) that is the mean squared error between the pixel values of original and reconstructed images is the common choice for learning. However, using the loss function leads to blurred reconstructed images. A method for learning CAEs using a loss function computed from features reflecting spatial frequencies is proposed to mitigate the problem. The blurs in reconstructed images show lack of high spatial frequency components mainly constituting edges and detailed textures that are important features for tasks such as object detection and spatial matching. In order to evaluate the lack of components, a convolutional layer with a Laplacian filter bank as weights is added to CAEs and the mean squared error of features in a subband, called the spatial frequency loss (SFL), is computed from the outputs of each filter. The learning is performed using a loss function based on the SFL. Empirical evaluation demonstrates that using the SFL reduces the blurs in reconstructed images.



### PieAPP: Perceptual Image-Error Assessment through Pairwise Preference
- **Arxiv ID**: http://arxiv.org/abs/1806.02067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02067v1)
- **Published**: 2018-06-06 08:50:53+00:00
- **Updated**: 2018-06-06 08:50:53+00:00
- **Authors**: Ekta Prashnani, Hong Cai, Yasamin Mostofi, Pradeep Sen
- **Comment**: 8 pages; 5 figures; proceedings of CVPR 2018
- **Journal**: E. Prashnani, H. Cai, Y. Mostofi and P. Sen. PieAPP: Perceptual
  Image-Error Assessment through Pairwise Preference. In Proceedings of the
  IEEE Conference on Computer Vision and Pattern Recognition, 2018
- **Summary**: The ability to estimate the perceptual error between images is an important problem in computer vision with many applications. Although it has been studied extensively, however, no method currently exists that can robustly predict visual differences like humans. Some previous approaches used hand-coded models, but they fail to model the complexity of the human visual system. Others used machine learning to train models on human-labeled datasets, but creating large, high-quality datasets is difficult because people are unable to assign consistent error labels to distorted images. In this paper, we present a new learning-based method that is the first to predict perceptual image error like human observers. Since it is much easier for people to compare two given images and identify the one more similar to a reference than to assign quality scores to each, we propose a new, large-scale dataset labeled with the probability that humans will prefer one image over another. We then train a deep-learning model using a novel, pairwise-learning framework to predict the preference of one distorted image over the other. Our key observation is that our trained network can then be used separately with only one distorted image and a reference to predict its perceptual error, without ever being trained on explicit human perceptual-error labels. The perceptual error estimated by our new metric, PieAPP, is well-correlated with human opinion. Furthermore, it significantly outperforms existing algorithms, beating the state-of-the-art by almost 3x on our test set in terms of binary error rate, while also generalizing to new kinds of distortions, unlike previous learning-based methods.



### Instance Segmentation and Tracking with Cosine Embeddings and Recurrent Hourglass Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.02070v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02070v3)
- **Published**: 2018-06-06 08:57:15+00:00
- **Updated**: 2018-07-30 09:32:56+00:00
- **Authors**: Christian Payer, Darko Štern, Thomas Neff, Horst Bischof, Martin Urschler
- **Comment**: Accepted for ORAL presentation at MICCAI 2018
- **Journal**: None
- **Summary**: Different to semantic segmentation, instance segmentation assigns unique labels to each individual instance of the same class. In this work, we propose a novel recurrent fully convolutional network architecture for tracking such instance segmentations over time. The network architecture incorporates convolutional gated recurrent units (ConvGRU) into a stacked hourglass network to utilize temporal video information. Furthermore, we train the network with a novel embedding loss based on cosine similarities, such that the network predicts unique embeddings for every instance throughout videos. Afterwards, these embeddings are clustered among subsequent video frames to create the final tracked instance segmentations. We evaluate the recurrent hourglass network by segmenting left ventricles in MR videos of the heart, where it outperforms a network that does not incorporate video information. Furthermore, we show applicability of the cosine embedding loss for segmenting leaf instances on still images of plants. Finally, we evaluate the framework for instance segmentation and tracking on six datasets of the ISBI celltracking challenge, where it shows state-of-the-art performance.



### Attention Incorporate Network: A network can adapt various data size
- **Arxiv ID**: http://arxiv.org/abs/1806.03961v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03961v1)
- **Published**: 2018-06-06 11:09:35+00:00
- **Updated**: 2018-06-06 11:09:35+00:00
- **Authors**: Liangbo He, Hao Sun
- **Comment**: None
- **Journal**: None
- **Summary**: In traditional neural networks for image processing, the inputs of the neural networks should be the same size such as 224*224*3. But how can we train the neural net model with different input size? A common way to do is image deformation which accompany a problem of information loss (e.g. image crop or wrap). Sequence model(RNN, LSTM, etc.) can accept different size of input like text and audio. But one disadvantage for sequence model is that the previous information will become more fragmentary during the transfer in time step, it will make the network hard to train especially for long sequential data. In this paper we propose a new network structure called Attention Incorporate Network(AIN). It solve the problem of different size of inputs including: images, text, audio, and extract the key features of the inputs by attention mechanism, pay different attention depends on the importance of the features not rely on the data size. Experimentally, AIN achieve a higher accuracy, better convergence comparing to the same size of other network structure



### TextRay: Mining Clinical Reports to Gain a Broad Understanding of Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/1806.02121v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.02121v1)
- **Published**: 2018-06-06 11:17:59+00:00
- **Updated**: 2018-06-06 11:17:59+00:00
- **Authors**: Jonathan Laserson, Christine Dan Lantsman, Michal Cohen-Sfady, Itamar Tamir, Eli Goz, Chen Brestel, Shir Bar, Maya Atar, Eldad Elnekave
- **Comment**: Accepted to MICCAI 2018
- **Journal**: None
- **Summary**: The chest X-ray (CXR) is by far the most commonly performed radiological examination for screening and diagnosis of many cardiac and pulmonary diseases. There is an immense world-wide shortage of physicians capable of providing rapid and accurate interpretation of this study. A radiologist-driven analysis of over two million CXR reports generated an ontology including the 40 most prevalent pathologies on CXR. By manually tagging a relatively small set of sentences, we were able to construct a training set of 959k studies. A deep learning model was trained to predict the findings given the patient frontal and lateral scans. For 12 of the findings we compare the model performance against a team of radiologists and show that in most cases the radiologists agree on average more with the algorithm than with each other.



### Deep supervision with additional labels for retinal vessel segmentation task
- **Arxiv ID**: http://arxiv.org/abs/1806.02132v3
- **DOI**: 10.1007/978-3-030-00934-2_10
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02132v3)
- **Published**: 2018-06-06 11:41:01+00:00
- **Updated**: 2019-02-11 07:11:08+00:00
- **Authors**: Yishuo Zhang, Albert C. S. Chung
- **Comment**: Accepted be MICCAI 2018
- **Journal**: None
- **Summary**: Automatic analysis of retinal blood images is of vital importance in diagnosis tasks of retinopathy. Segmenting vessels accurately is a fundamental step in analysing retinal images. However, it is usually difficult due to various imaging conditions, low image contrast and the appearance of pathologies such as micro-aneurysms. In this paper, we propose a novel method with deep neural networks to solve this problem. We utilize U-net with residual connection to detect vessels. To achieve better accuracy, we introduce an edge-aware mechanism, in which we convert the original task into a multi-class task by adding additional labels on boundary areas. In this way, the network will pay more attention to the boundary areas of vessels and achieve a better performance, especially in tiny vessels detecting. Besides, side output layers are applied in order to give deep supervision and therefore help convergence. We train and evaluate our model on three databases: DRIVE, STARE, and CHASEDB1. Experimental results show that our method has a comparable performance with AUC of 97.99% on DRIVE and an efficient running time compared to the state-of-the-art methods.



### Multi-chart Generative Surface Modeling
- **Arxiv ID**: http://arxiv.org/abs/1806.02143v3
- **DOI**: 10.1145/3272127.3275052
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02143v3)
- **Published**: 2018-06-06 12:25:10+00:00
- **Updated**: 2019-03-03 14:12:52+00:00
- **Authors**: Heli Ben-Hamu, Haggai Maron, Itay Kezurer, Gal Avineri, Yaron Lipman
- **Comment**: None
- **Journal**: ACM Trans. Graph. 37, 6, Article 215 (December 2018)
- **Summary**: This paper introduces a 3D shape generative model based on deep neural networks. A new image-like (i.e., tensor) data representation for genus-zero 3D shapes is devised. It is based on the observation that complicated shapes can be well represented by multiple parameterizations (charts), each focusing on a different part of the shape. The new tensor data representation is used as input to Generative Adversarial Networks for the task of 3D shape generation. The 3D shape tensor representation is based on a multi-chart structure that enjoys a shape covering property and scale-translation rigidity. Scale-translation rigidity facilitates high quality 3D shape learning and guarantees unique reconstruction. The multi-chart structure uses as input a dataset of 3D shapes (with arbitrary connectivity) and a sparse correspondence between them. The output of our algorithm is a generative model that learns the shape distribution and is able to generate novel shapes, interpolate shapes, and explore the generated shape space. The effectiveness of the method is demonstrated for the task of anatomic shape generation including human body and bone (teeth) shape generation.



### PointFlowNet: Learning Representations for Rigid Motion Estimation from Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1806.02170v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02170v3)
- **Published**: 2018-06-06 13:24:30+00:00
- **Updated**: 2019-01-07 13:01:03+00:00
- **Authors**: Aseem Behl, Despoina Paschalidou, Simon Donné, Andreas Geiger
- **Comment**: None
- **Journal**: None
- **Summary**: Despite significant progress in image-based 3D scene flow estimation, the performance of such approaches has not yet reached the fidelity required by many applications. Simultaneously, these applications are often not restricted to image-based estimation: laser scanners provide a popular alternative to traditional cameras, for example in the context of self-driving cars, as they directly yield a 3D point cloud. In this paper, we propose to estimate 3D motion from such unstructured point clouds using a deep neural network. In a single forward pass, our model jointly predicts 3D scene flow as well as the 3D bounding box and rigid body motion of objects in the scene. While the prospect of estimating 3D scene flow from unstructured point clouds is promising, it is also a challenging task. We show that the traditional global representation of rigid body motion prohibits inference by CNNs, and propose a translation equivariant representation to circumvent this problem. For training our deep network, a large dataset is required. Because of this, we augment real scans from KITTI with virtual objects, realistically modeling occlusions and simulating sensor noise. A thorough comparison with classic and learning-based techniques highlights the robustness of the proposed approach.



### Drowsy Driver Detection by EEG Analysis Using Fast Fourier Transform
- **Arxiv ID**: http://arxiv.org/abs/1806.07286v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.07286v1)
- **Published**: 2018-06-06 14:17:51+00:00
- **Updated**: 2018-06-06 14:17:51+00:00
- **Authors**: Mejdi Ben Dkhil, Ali Wali, Adel M. Alimi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we try to analyze drowsiness which is a major factor in many traffic accidents due to the clear decline in the attention and recognition of danger drivers. The object of this work is to develop an automatic method to evaluate the drowsiness stage by analysis of EEG signals records. The absolute band power of the EEG signal was computed by taking the Fast Fourier Transform (FFT) of the time series signal. Finally, the algorithm developed in this work has been improved on eight samples from the Physionet sleep-EDF database.



### A multi-scale pyramid of 3D fully convolutional networks for abdominal multi-organ segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.02237v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02237v1)
- **Published**: 2018-06-06 15:11:00+00:00
- **Updated**: 2018-06-06 15:11:00+00:00
- **Authors**: Holger R. Roth, Chen Shen, Hirohisa Oda, Takaaki Sugino, Masahiro Oda, Yuichiro Hayashi, Kazunari Misawa, Kensaku Mori
- **Comment**: Accepted for presentation at the 21st International Conference on
  Medical Image Computing and Computer Assisted Intervention - MICCAI 2018,
  September 16-20, Granada, Spain
- **Journal**: None
- **Summary**: Recent advances in deep learning, like 3D fully convolutional networks (FCNs), have improved the state-of-the-art in dense semantic segmentation of medical images. However, most network architectures require severely downsampling or cropping the images to meet the memory limitations of today's GPU cards while still considering enough context in the images for accurate segmentation. In this work, we propose a novel approach that utilizes auto-context to perform semantic segmentation at higher resolutions in a multi-scale pyramid of stacked 3D FCNs. We train and validate our models on a dataset of manually annotated abdominal organs and vessels from 377 clinical CT images used in gastric surgery, and achieve promising results with close to 90% Dice score on average. For additional evaluation, we perform separate testing on datasets from different sources and achieve competitive results, illustrating the robustness of the model and approach.



### Deep Vessel Segmentation By Learning Graphical Connectivity
- **Arxiv ID**: http://arxiv.org/abs/1806.02279v1
- **DOI**: 10.1016/j.media.2019.101556
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02279v1)
- **Published**: 2018-06-06 16:16:52+00:00
- **Updated**: 2018-06-06 16:16:52+00:00
- **Authors**: Seung Yeon Shin, Soochahn Lee, Il Dong Yun, Kyoung Mu Lee
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel deep-learning-based system for vessel segmentation. Existing methods using CNNs have mostly relied on local appearances learned on the regular image grid, without considering the graphical structure of vessel shape. To address this, we incorporate a graph convolutional network into a unified CNN architecture, where the final segmentation is inferred by combining the different types of features. The proposed method can be applied to expand any type of CNN-based vessel segmentation method to enhance the performance. Experiments show that the proposed method outperforms the current state-of-the-art methods on two retinal image datasets as well as a coronary artery X-ray angiography dataset.



### Spatiotemporal Manifold Prediction Model for Anterior Vertebral Body Growth Modulation Surgery in Idiopathic Scoliosis
- **Arxiv ID**: http://arxiv.org/abs/1806.02285v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02285v1)
- **Published**: 2018-06-06 16:24:28+00:00
- **Updated**: 2018-06-06 16:24:28+00:00
- **Authors**: William Mandel, Olivier Turcot, Dejan Knez, Stefan Parent, Samuel Kadoury
- **Comment**: None
- **Journal**: None
- **Summary**: Anterior Vertebral Body Growth Modulation (AVBGM) is a minimally invasive surgical technique that gradually corrects spine deformities while preserving lumbar motion. However the selection of potential surgical patients is currently based on clinical judgment and would be facilitated by the identification of patients responding to AVBGM prior to surgery. We introduce a statistical framework for predicting the surgical outcomes following AVBGM in adolescents with idiopathic scoliosis. A discriminant manifold is first constructed to maximize the separation between responsive and non-responsive groups of patients treated with AVBGM for scoliosis. The model then uses subject-specific correction trajectories based on articulated transformations in order to map spine correction profiles to a group-average piecewise-geodesic path. Spine correction trajectories are described in a piecewise-geodesic fashion to account for varying times at follow-up exams, regressing the curve via a quadratic optimization process. To predict the evolution of correction, a baseline reconstruction is projected onto the manifold, from which a spatiotemporal regression model is built from parallel transport curves inferred from neighboring exemplars. The model was trained on 438 reconstructions and tested on 56 subjects using 3D spine reconstructions from follow-up exams, with the probabilistic framework yielding accurate results with differences of 2.1 +/- 0.6deg in main curve angulation, and generating models similar to biomechanical simulations.



### Regularization by Denoising: Clarifications and New Interpretations
- **Arxiv ID**: http://arxiv.org/abs/1806.02296v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02296v4)
- **Published**: 2018-06-06 16:49:59+00:00
- **Updated**: 2018-11-01 04:09:07+00:00
- **Authors**: Edward T. Reehorst, Philip Schniter
- **Comment**: None
- **Journal**: None
- **Summary**: Regularization by Denoising (RED), as recently proposed by Romano, Elad, and Milanfar, is powerful image-recovery framework that aims to minimize an explicit regularization objective constructed from a plug-in image-denoising function. Experimental evidence suggests that the RED algorithms are state-of-the-art. We claim, however, that explicit regularization does not explain the RED algorithms. In particular, we show that many of the expressions in the paper by Romano et al. hold only when the denoiser has a symmetric Jacobian, and we demonstrate that such symmetry does not occur with practical denoisers such as non-local means, BM3D, TNRD, and DnCNN. To explain the RED algorithms, we propose a new framework called Score-Matching by Denoising (SMD), which aims to match a "score" (i.e., the gradient of a log-prior). We then show tight connections between SMD, kernel density estimation, and constrained minimum mean-squared error denoising. Furthermore, we interpret the RED algorithms from Romano et al. and propose new algorithms with acceleration and convergence guarantees. Finally, we show that the RED algorithms seek a consensus equilibrium solution, which facilitates a comparison to plug-and-play ADMM.



### Dilatation of Lateral Ventricles with Brain Volumes in Infants with 3D Transfontanelle US
- **Arxiv ID**: http://arxiv.org/abs/1806.02305v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02305v1)
- **Published**: 2018-06-06 17:10:06+00:00
- **Updated**: 2018-06-06 17:10:06+00:00
- **Authors**: Marc-Antoine Boucher, Sarah Lippe, Amelie Damphousse, Ramy El-Jalbout, Samuel Kadoury
- **Comment**: None
- **Journal**: None
- **Summary**: Ultrasound (US) can be used to assess brain development in newborns, as MRI is challenging due to immobilization issues, and may require sedation. Dilatation of the lateral ventricles in the brain is a risk factor for poorer neurodevelopment outcomes in infants. Hence, 3D US has the ability to assess the volume of the lateral ventricles similar to clinically standard MRI, but manual segmentation is time consuming. The objective of this study is to develop an approach quantifying the ratio of lateral ventricular dilatation with respect to total brain volume using 3D US, which can assess the severity of macrocephaly. Automatic segmentation of the lateral ventricles is achieved with a multi-atlas deformable registration approach using locally linear correlation metrics for US-MRI fusion, followed by a refinement step using deformable mesh models. Total brain volume is estimated using a 3D ellipsoid modeling approach. Validation was performed on a cohort of 12 infants, ranging from 2 to 8.5 months old, where 3D US and MRI were used to compare brain volumes and segmented lateral ventricles. Automatically extracted volumes from 3D US show a high correlation and no statistically significant difference when compared to ground truth measurements. Differences in volume ratios was 6.0 +/- 4.8% compared to MRI, while lateral ventricular segmentation yielded a mean Dice coefficient of 70.8 +/- 3.6% and a mean absolute distance (MAD) of 0.88 +/- 0.2mm, demonstrating the clinical benefit of this tool in paediatric ultrasound.



### Unsupervised Attention-guided Image to Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1806.02311v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1806.02311v3)
- **Published**: 2018-06-06 17:21:38+00:00
- **Updated**: 2018-11-08 14:43:37+00:00
- **Authors**: Youssef A. Mejjati, Christian Richardt, James Tompkin, Darren Cosker, Kwang In Kim
- **Comment**: None
- **Journal**: NIPS 2018
- **Summary**: Current unsupervised image-to-image translation techniques struggle to focus their attention on individual objects without altering the background or the way multiple objects interact within a scene. Motivated by the important role of attention in human perception, we tackle this limitation by introducing unsupervised attention mechanisms that are jointly adversarialy trained with the generators and discriminators. We demonstrate qualitatively and quantitatively that our approach is able to attend to relevant regions in the image without requiring supervision, and that by doing so it achieves more realistic mappings compared to recent approaches.



### Adaptive feature recombination and recalibration for semantic segmentation: application to brain tumor segmentation in MRI
- **Arxiv ID**: http://arxiv.org/abs/1806.02318v1
- **DOI**: 10.1007/978-3-030-00931-1_81
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02318v1)
- **Published**: 2018-06-06 17:37:15+00:00
- **Updated**: 2018-06-06 17:37:15+00:00
- **Authors**: Sérgio Pereira, Victor Alves, Carlos A. Silva
- **Comment**: Accepted at MICCAI 2018
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have been successfully used for brain tumor segmentation, specifically, fully convolutional networks (FCNs). FCNs can segment a set of voxels at once, having a direct spatial correspondence between units in feature maps (FMs) at a given location and the corresponding classified voxels. In convolutional layers, FMs are merged to create new FMs, so, channel combination is crucial. However, not all FMs have the same relevance for a given class. Recently, in classification problems, Squeeze-and-Excitation (SE) blocks have been proposed to re-calibrate FMs as a whole, and suppress the less informative ones. However, this is not optimal in FCN due to the spatial correspondence between units and voxels. In this article, we propose feature recombination through linear expansion and compression to create more complex features for semantic segmentation. Additionally, we propose a segmentation SE (SegSE) block for feature recalibration that collects contextual information, while maintaining the spatial meaning. Finally, we evaluate the proposed methods in brain tumor segmentation, using publicly available data.



### Fast and Accurate Online Video Object Segmentation via Tracking Parts
- **Arxiv ID**: http://arxiv.org/abs/1806.02323v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02323v1)
- **Published**: 2018-06-06 17:43:13+00:00
- **Updated**: 2018-06-06 17:43:13+00:00
- **Authors**: Jingchun Cheng, Yi-Hsuan Tsai, Wei-Chih Hung, Shengjin Wang, Ming-Hsuan Yang
- **Comment**: Accepted in CVPR'18 as Spotlight. Code and model are available at
  https://github.com/JingchunCheng/FAVOS
- **Journal**: None
- **Summary**: Online video object segmentation is a challenging task as it entails to process the image sequence timely and accurately. To segment a target object through the video, numerous CNN-based methods have been developed by heavily finetuning on the object mask in the first frame, which is time-consuming for online applications. In this paper, we propose a fast and accurate video object segmentation algorithm that can immediately start the segmentation process once receiving the images. We first utilize a part-based tracking method to deal with challenging factors such as large deformation, occlusion, and cluttered background. Based on the tracked bounding boxes of parts, we construct a region-of-interest segmentation network to generate part masks. Finally, a similarity-based scoring function is adopted to refine these object parts by comparing them to the visual information in the first frame. Our method performs favorably against state-of-the-art algorithms in accuracy on the DAVIS benchmark dataset, while achieving much faster runtime performance.



### A Multi-task Deep Learning Architecture for Maritime Surveillance using AIS Data Streams
- **Arxiv ID**: http://arxiv.org/abs/1806.03972v3
- **DOI**: 10.1109/DSAA.2018.00044
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.03972v3)
- **Published**: 2018-06-06 19:21:09+00:00
- **Updated**: 2018-08-07 21:12:20+00:00
- **Authors**: Duong Nguyen, Rodolphe Vadaine, Guillaume Hajduch, René Garello, Ronan Fablet
- **Comment**: Accepted to IEEE DSAA 2018
- **Journal**: None
- **Summary**: In a world of global trading, maritime safety, security and efficiency are crucial issues. We propose a multi-task deep learning framework for vessel monitoring using Automatic Identification System (AIS) data streams. We combine recurrent neural networks with latent variable modeling and an embedding of AIS messages to a new representation space to jointly address key issues to be dealt with when considering AIS data streams: massive amount of streaming data, noisy data and irregular timesampling. We demonstrate the relevance of the proposed deep learning framework on real AIS datasets for a three-task setting, namely trajectory reconstruction, anomaly detection and vessel type identification.



### A Comparative Study on Unsupervised Domain Adaptation Approaches for Coffee Crop Mapping
- **Arxiv ID**: http://arxiv.org/abs/1806.02400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02400v1)
- **Published**: 2018-06-06 19:36:55+00:00
- **Updated**: 2018-06-06 19:36:55+00:00
- **Authors**: Edemir Ferreira, Mário S. Alvim, Jefersson A. dos Santos
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we investigate the application of existing unsupervised domain adaptation (UDA) approaches to the task of transferring knowledge between crop regions having different coffee patterns. Given a geographical region with fully mapped coffee plantations, we observe that this knowledge can be used to train a classifier and to map a new county with no need of samples indicated in the target region. Experimental results show that transferring knowledge via some UDA strategies performs better than just applying a classifier trained in a region to predict coffee crops in a new one. However, UDA methods may lead to negative transfer, which may indicate that domains are too different that transferring knowledge is not appropriate. We also verify that normalization affect significantly some UDA methods; we observe a meaningful complementary contribution between coffee crops data; and a visual behavior suggests an existent of a cluster of samples that are more likely to be drawn from a specific data.



### Action4D: Real-time Action Recognition in the Crowd and Clutter
- **Arxiv ID**: http://arxiv.org/abs/1806.02424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02424v1)
- **Published**: 2018-06-06 20:59:40+00:00
- **Updated**: 2018-06-06 20:59:40+00:00
- **Authors**: Quanzeng You, Hao Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing every person's action in a crowded and cluttered environment is a challenging task. In this paper, we propose a real-time action recognition method, Action4D, which gives reliable and accurate results in the real-world settings. We propose to tackle the action recognition problem using a holistic 4D "scan" of a cluttered scene to include every detail about the people and environment. Recognizing multiple people's actions in the cluttered 4D representation is a new problem. In this paper, we propose novel methods to solve this problem. We propose a new method to track people in 4D, which can reliably detect and follow each person in real time. We propose a new deep neural network, the Action4D-Net, to recognize the action of each tracked person. The Action4D-Net's novel structure uses both the global feature and the focused attention to achieve state-of-the-art result. Our real-time method is invariant to camera view angles, resistant to clutter and able to handle crowd. The experimental results show that the proposed method is fast, reliable and accurate. Our method paves the way to action recognition in the real-world applications and is ready to be deployed to enable smart homes, smart factories and smart stores.



### Rethinking Radiology: An Analysis of Different Approaches to BraTS
- **Arxiv ID**: http://arxiv.org/abs/1806.03981v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.03981v1)
- **Published**: 2018-06-06 21:58:54+00:00
- **Updated**: 2018-06-06 21:58:54+00:00
- **Authors**: William Bakst, Linus Meyer-Teruel, Jasdeep Singh
- **Comment**: None
- **Journal**: None
- **Summary**: This paper discusses the deep learning architectures currently used for pixel-wise segmentation of primary and secondary glioblastomas and low-grade gliomas. We implement various models such as the popular UNet architecture and compare the performance of these implementations on the BRATS dataset. This paper will explore the different approaches and combinations, offering an in depth discussion of how they perform and how we may improve upon them using more recent advancements in deep learning architectures.



### Deep Ordinal Regression Network for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1806.02446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02446v1)
- **Published**: 2018-06-06 22:36:23+00:00
- **Updated**: 2018-06-06 22:36:23+00:00
- **Authors**: Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, Dacheng Tao
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Monocular depth estimation, which plays a crucial role in understanding 3D scene geometry, is an ill-posed problem. Recent methods have gained significant improvement by exploring image-level information and hierarchical features from deep convolutional neural networks (DCNNs). These methods model depth estimation as a regression problem and train the regression networks by minimizing mean squared error, which suffers from slow convergence and unsatisfactory local solutions. Besides, existing depth estimation networks employ repeated spatial pooling operations, resulting in undesirable low-resolution feature maps. To obtain high-resolution depth maps, skip-connections or multi-layer deconvolution networks are required, which complicates network training and consumes much more computations. To eliminate or at least largely reduce these problems, we introduce a spacing-increasing discretization (SID) strategy to discretize depth and recast depth network learning as an ordinal regression problem. By training the network using an ordinary regression loss, our method achieves much higher accuracy and \dd{faster convergence in synch}. Furthermore, we adopt a multi-scale network structure which avoids unnecessary spatial pooling and captures multi-scale information in parallel.   The method described in this paper achieves state-of-the-art results on four challenging benchmarks, i.e., KITTI [17], ScanNet [9], Make3D [50], and NYU Depth v2 [42], and win the 1st prize in Robust Vision Challenge 2018. Code has been made available at: https://github.com/hufu6371/DORN.



### NumtaDB - Assembled Bengali Handwritten Digits
- **Arxiv ID**: http://arxiv.org/abs/1806.02452v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T10, I.5.1; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1806.02452v1)
- **Published**: 2018-06-06 23:02:06+00:00
- **Updated**: 2018-06-06 23:02:06+00:00
- **Authors**: Samiul Alam, Tahsin Reasat, Rashed Mohammad Doha, Ahmed Imtiaz Humayun
- **Comment**: 6 page, 12 figures
- **Journal**: None
- **Summary**: To benchmark Bengali digit recognition algorithms, a large publicly available dataset is required which is free from biases originating from geographical location, gender, and age. With this aim in mind, NumtaDB, a dataset consisting of more than 85,000 images of hand-written Bengali digits, has been assembled. This paper documents the collection and curation process of numerals along with the salient statistics of the dataset.



### Visual Reasoning by Progressive Module Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.02453v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02453v2)
- **Published**: 2018-06-06 23:02:35+00:00
- **Updated**: 2018-09-27 18:09:38+00:00
- **Authors**: Seung Wook Kim, Makarand Tapaswi, Sanja Fidler
- **Comment**: 17 pages, 5 figures
- **Journal**: None
- **Summary**: Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn - most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.



### Quantitative Phase Imaging and Artificial Intelligence: A Review
- **Arxiv ID**: http://arxiv.org/abs/1806.03982v2
- **DOI**: None
- **Categories**: **cs.CV**, physics.data-an, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1806.03982v2)
- **Published**: 2018-06-06 23:52:20+00:00
- **Updated**: 2018-07-13 08:37:40+00:00
- **Authors**: YoungJu Jo, Hyungjoo Cho, Sang Yun Lee, Gunho Choi, Geon Kim, Hyun-seok Min, YongKeun Park
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in quantitative phase imaging (QPI) and artificial intelligence (AI) have opened up the possibility of an exciting frontier. The fast and label-free nature of QPI enables the rapid generation of large-scale and uniform-quality imaging data in two, three, and four dimensions. Subsequently, the AI-assisted interrogation of QPI data using data-driven machine learning techniques results in a variety of biomedical applications. Also, machine learning enhances QPI itself. Herein, we review the synergy between QPI and machine learning with a particular focus on deep learning. Further, we provide practical guidelines and perspectives for further development.



