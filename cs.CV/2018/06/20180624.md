# Arxiv Papers in cs.CV on 2018-06-24
### DARTS: Differentiable Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1806.09055v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.09055v2)
- **Published**: 2018-06-24 00:06:13+00:00
- **Updated**: 2019-04-23 06:29:32+00:00
- **Authors**: Hanxiao Liu, Karen Simonyan, Yiming Yang
- **Comment**: Published at ICLR 2019; Code and pretrained models available at
  https://github.com/quark0/darts
- **Journal**: None
- **Summary**: This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.



### Online Signature Verification using Deep Representation: A new Descriptor
- **Arxiv ID**: http://arxiv.org/abs/1806.09986v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09986v1)
- **Published**: 2018-06-24 03:15:39+00:00
- **Updated**: 2018-06-24 03:15:39+00:00
- **Authors**: Mohammad Hajizadeh Saffar, Mohsen Fayyaz, Mohammad Sabokrou, Mahmood Fathy
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1505.08153
- **Journal**: None
- **Summary**: This paper presents an accurate method for verifying online signatures. The main difficulty of signature verification come from: (1) Lacking enough training samples (2) The methods must be spatial change invariant. To deal with these difficulties and modeling the signatures efficiently, we propose a method that a one-class classifier per each user is built on discriminative features. First, we pre-train a sparse auto-encoder using a large number of unlabeled signatures, then we applied the discriminative features, which are learned by auto-encoder to represent the training and testing signatures as a self-thought learning method (i.e. we have introduced a signature descriptor). Finally, user's signatures are modeled and classified using a one-class classifier. The proposed method is independent on signature datasets thanks to self-taught learning. The experimental results indicate significant error reduction and accuracy enhancement in comparison with state-of-the-art methods on SVC2004 and SUSIG datasets.



### CT-image Super Resolution Using 3D Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1806.09074v1
- **DOI**: 10.1016/j.cageo.2019.104314
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09074v1)
- **Published**: 2018-06-24 03:22:33+00:00
- **Updated**: 2018-06-24 03:22:33+00:00
- **Authors**: Yukai Wang, Qizhi Teng, Xiaohai He, Junxi Feng, Tingrong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Computed Tomography (CT) imaging technique is widely used in geological exploration, medical diagnosis and other fields. In practice, however, the resolution of CT image is usually limited by scanning devices and great expense. Super resolution (SR) methods based on deep learning have achieved surprising performance in two-dimensional (2D) images. Unfortunately, there are few effective SR algorithms for three-dimensional (3D) images. In this paper, we proposed a novel network named as three-dimensional super resolution convolutional neural network (3DSRCNN) to realize voxel super resolution for CT images. To solve the practical problems in training process such as slow convergence of network training, insufficient memory, etc., we utilized adjustable learning rate, residual-learning, gradient clipping, momentum stochastic gradient descent (SGD) strategies to optimize training procedure. In addition, we have explored the empirical guidelines to set appropriate number of layers of network and how to use residual learning strategy. Additionally, previous learning-based algorithms need to separately train for different scale factors for reconstruction, yet our single model can complete the multi-scale SR. At last, our method has better performance in terms of PSNR, SSIM and efficiency compared with conventional methods.



### CNN-based Action Recognition and Supervised Domain Adaptation on 3D Body Skeletons via Kernel Feature Maps
- **Arxiv ID**: http://arxiv.org/abs/1806.09078v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09078v1)
- **Published**: 2018-06-24 04:06:18+00:00
- **Updated**: 2018-06-24 04:06:18+00:00
- **Authors**: Yusuf Tas, Piotr Koniusz
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning is ubiquitous across many areas areas of computer vision. It often requires large scale datasets for training before being fine-tuned on small-to-medium scale problems. Activity, or, in other words, action recognition, is one of many application areas of deep learning. While there exist many Convolutional Neural Network architectures that work with the RGB and optical flow frames, training on the time sequences of 3D body skeleton joints is often performed via recurrent networks such as LSTM.   In this paper, we propose a new representation which encodes sequences of 3D body skeleton joints in texture-like representations derived from mathematically rigorous kernel methods. Such a representation becomes the first layer in a standard CNN network e.g., ResNet-50, which is then used in the supervised domain adaptation pipeline to transfer information from the source to target dataset. This lets us leverage the available Kinect-based data beyond training on a single dataset and outperform simple fine-tuning on any two datasets combined in a naive manner. More specifically, in this paper we utilize the overlapping classes between datasets. We associate datapoints of the same class via so-called commonality, known from the supervised domain adaptation. We demonstrate state-of-the-art results on three publicly available benchmarks.



### Artwork Identification from Wearable Camera Images for Enhancing Experience of Museum Audiences
- **Arxiv ID**: http://arxiv.org/abs/1806.09084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09084v1)
- **Published**: 2018-06-24 05:01:59+00:00
- **Updated**: 2018-06-24 05:01:59+00:00
- **Authors**: Rui Zhang, Yusuf Tas, Piotr Koniusz
- **Comment**: None
- **Journal**: Museums and the Web, 2017
- **Summary**: Recommendation systems based on image recognition could prove a vital tool in enhancing the experience of museum audiences. However, for practical systems utilizing wearable cameras, a number of challenges exist which affect the quality of image recognition. In this pilot study, we focus on recognition of museum collections by using a wearable camera in three different museum spaces. We discuss the application of wearable cameras, and the practical and technical challenges in devising a robust system that can recognize artworks viewed by the visitors to create a detailed record of their visit. Specifically, to illustrate the impact of different kinds of museum spaces on image recognition, we collect three training datasets of museum exhibits containing variety of paintings, clocks, and sculptures. Subsequently, we equip selected visitors with wearable cameras to capture artworks viewed by them as they stroll along exhibitions. We use Convolutional Neural Networks (CNN) which are pre-trained on the ImageNet dataset and fine-tuned on each of the training sets for the purpose of artwork identification. In the testing stage, we use CNNs to identify artworks captured by the visitors with a wearable camera. We analyze the accuracy of their recognition and provide an insight into the applicability of such a system to further engage audiences with museum exhibitions.



### Segmentation of Overlapped Steatosis in Whole-Slide Liver Histopathology Microscopy Images
- **Arxiv ID**: http://arxiv.org/abs/1806.09090v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09090v1)
- **Published**: 2018-06-24 06:14:06+00:00
- **Updated**: 2018-06-24 06:14:06+00:00
- **Authors**: Mousumi Roy, Fusheng Wang, George Teodoro, Miriam B Vos, Alton Brad Farris, Jun Kong
- **Comment**: None
- **Journal**: None
- **Summary**: An accurate steatosis quantification with pathology tissue samples is of high clinical importance. However, such pathology measurement is manually made in most clinical practices, subject to severe reader variability due to large sampling bias and poor reproducibility. Although some computerized automated methods are developed to quantify the steatosis regions, they present limited analysis capacity for high resolution whole-slide microscopy images and accurate overlapped steatosis division. In this paper, we propose a method that extracts an individual whole tissue piece at high resolution with minimum background area by estimating tissue bounding box and rotation angle. This is followed by the segmentation and segregation of steatosis regions with high curvature point detection and an ellipse fitting quality assessment method. We validate our method with isolated and overlapped steatosis regions in liver tissue images of 11 patients. The experimental results suggest that our method is promising for enhanced support of steatosis quantization during the pathology review for liver disease treatment.



### Analysis of Cellular Feature Differences of Astrocytomas with Distinct Mutational Profiles Using Digitized Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/1806.09093v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09093v1)
- **Published**: 2018-06-24 06:27:27+00:00
- **Updated**: 2018-06-24 06:27:27+00:00
- **Authors**: Mousumi Roy, Fusheng Wang, George Teodoro, Jose Velazqeuz Vega, Daniel Brat, Jun Kong
- **Comment**: None
- **Journal**: None
- **Summary**: Cellular phenotypic features derived from histopathology images are the basis of pathologic diagnosis and are thought to be related to underlying molecular profiles. Due to overwhelming cell numbers and population heterogeneity, it remains challenging to quantitatively compute and compare features of cells with distinct molecular signatures. In this study, we propose a self-reliant and efficient analysis framework that supports quantitative analysis of cellular phenotypic difference across distinct molecular groups. To demonstrate efficacy, we quantitatively analyze astrocytomas that are molecularly characterized as either Isocitrate Dehydrogenase (IDH) mutant (MUT) or wildtype (WT) using imaging data from The Cancer Genome Atlas database. Representative cell instances that are phenotypically different between these two groups are retrieved after segmentation, feature computation, data pruning, dimensionality reduction, and unsupervised clustering. Our analysis is generic and can be applied to a wide set of cell-based biomedical research.



### Real-time Local Noise Filter in 3D Visualization of CT Data
- **Arxiv ID**: http://arxiv.org/abs/1807.03119v1
- **DOI**: 10.1109/TNS.2019.2893444
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03119v1)
- **Published**: 2018-06-24 10:27:04+00:00
- **Updated**: 2018-06-24 10:27:04+00:00
- **Authors**: N. Tan Jerome, Z. Ateyev, S. Schmelzle, S. Chilingaryan, A. Kopmann
- **Comment**: 5 pages, 5 figures, 21st IEEE Real Time Conference
- **Journal**: None
- **Summary**: Removing noise in computer tomography (CT) data for real-time 3D visualization is vital to improving the quality of the final display. However, the CT noise cannot be removed by straight averaging because the noise has a broadband spatial frequency that is overlapping with the interesting signal frequencies. To improve the display of structures and features contained in the data, we present spatially variant filtering that performs averaging of sub-regions around a central region. We compare our filter with four other similar spatially variant filters regarding entropy and processing time. The results demonstrate significant improvement of the visual quality with processing time still within the millisecond range.



### SSIMLayer: Towards Robust Deep Representation Learning via Nonlinear Structural Similarity
- **Arxiv ID**: http://arxiv.org/abs/1806.09152v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09152v2)
- **Published**: 2018-06-24 14:23:49+00:00
- **Updated**: 2018-07-28 14:08:31+00:00
- **Authors**: Ahmed Abobakr, Mohammed Hossny, Saeid Nahavandi
- **Comment**: 11 pages, 10 figures
- **Journal**: None
- **Summary**: Deeper convolutional neural networks provide more capacity to approximate complex mapping functions. However, increasing network depth imposes difficulties on training and increases model complexity. This paper presents a new nonlinear computational layer of considerably high capacity to the deep convolutional neural network architectures. This layer performs a set of comprehensive convolution operations that mimics the overall function of the human visual system (HVS) via focusing on learning structural information in its input. The core of its computations is evaluating the components of the structural similarity metric (SSIM) in a setting that allows the kernels to learn to match structural information. The proposed SSIMLayer is inherently nonlinear and hence, it does not require subsequent nonlinear transformations. Experiments conducted on CIFAR-10 benchmark demonstrates that the SSIMLayer provides better convergence than the traditional convolutional layer, bypasses the need for nonlinear transformations and shows more robustness against noise perturbations and adversarial attacks.



### Inferring Routing Preferences of Bicyclists from Sparse Sets of Trajectories
- **Arxiv ID**: http://arxiv.org/abs/1806.09158v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09158v1)
- **Published**: 2018-06-24 14:42:18+00:00
- **Updated**: 2018-06-24 14:42:18+00:00
- **Authors**: J. Oehrlein, A. Förster, D. Schunck, Y. Dehbi, R. Roscher, J. -H. Haunert
- **Comment**: accepted to International Conference on Smart Data and Smart Cities
- **Journal**: ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial
  Information Sciences 2018
- **Summary**: Understanding the criteria that bicyclists apply when they choose their routes is crucial for planning new bicycle paths or recommending routes to bicyclists. This is becoming more and more important as city councils are becoming increasingly aware of limitations of the transport infrastructure and problems related to automobile traffic. Since different groups of cyclists have different preferences, however, searching for a single set of criteria is prone to failure. Therefore, in this paper, we present a new approach to classify trajectories recorded and shared by bicyclists into different groups and, for each group, to identify favored and unfavored road types. Based on these results we show how to assign weights to the edges of a graph representing the road network such that minimum-weight paths in the graph, which can be computed with standard shortest-path algorithms, correspond to adequate routes. Our method combines known algorithms for machine learning and the analysis of trajectories in an innovative way and, thereby, constitutes a new comprehensive solution for the problem of deriving routing preferences from initially unclassified trajectories. An important property of our method is that it yields reasonable results even if the given set of trajectories is sparse in the sense that it does not cover all segments of the cycle network.



### Fusion of complex networks and randomized neural networks for texture analysis
- **Arxiv ID**: http://arxiv.org/abs/1806.09170v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1806.09170v2)
- **Published**: 2018-06-24 15:57:19+00:00
- **Updated**: 2020-08-17 18:56:15+00:00
- **Authors**: Lucas C. Ribas, Jarbas J. M. Sa Junior, Leonardo F. S. Scabini, Odemir M. Bruno
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: This paper presents a high discriminative texture analysis method based on the fusion of complex networks and randomized neural networks. In this approach, the input image is modeled as a complex networks and its topological properties as well as the image pixels are used to train randomized neural networks in order to create a signature that represents the deep characteristics of the texture. The results obtained surpassed the accuracies of many methods available in the literature. This performance demonstrates that our proposed approach opens a promising source of research, which consists of exploring the synergy of neural networks and complex networks in the texture analysis field.



### Dilated Temporal Fully-Convolutional Network for Semantic Segmentation of Motion Capture Data
- **Arxiv ID**: http://arxiv.org/abs/1806.09174v1
- **DOI**: 10.2312/sca.20181185
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.NE, I.4.6; I.4.8; I.2.10; I.6.8; I.3.7
- **Links**: [PDF](http://arxiv.org/pdf/1806.09174v1)
- **Published**: 2018-06-24 16:40:07+00:00
- **Updated**: 2018-06-24 16:40:07+00:00
- **Authors**: Noshaba Cheema, Somayeh Hosseini, Janis Sprenger, Erik Herrmann, Han Du, Klaus Fischer, Philipp Slusallek
- **Comment**: Eurographics/ ACM SIGGRAPH Symposium on Computer Animation - Posters
  2018;
  $\href{http://people.mpi-inf.mpg.de/~ncheema/SCA2018_poster.pdf}{\textit{Poster
  can be found here.}}$
- **Journal**: None
- **Summary**: Semantic segmentation of motion capture sequences plays a key part in many data-driven motion synthesis frameworks. It is a preprocessing step in which long recordings of motion capture sequences are partitioned into smaller segments. Afterwards, additional methods like statistical modeling can be applied to each group of structurally-similar segments to learn an abstract motion manifold. The segmentation task however often remains a manual task, which increases the effort and cost of generating large-scale motion databases. We therefore propose an automatic framework for semantic segmentation of motion capture data using a dilated temporal fully-convolutional network. Our model outperforms a state-of-the-art model in action segmentation, as well as three networks for sequence modeling. We further show our model is robust against high noisy training labels.



### A Deeper Look at Power Normalizations
- **Arxiv ID**: http://arxiv.org/abs/1806.09183v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09183v1)
- **Published**: 2018-06-24 17:38:15+00:00
- **Updated**: 2018-06-24 17:38:15+00:00
- **Authors**: Piotr Koniusz, Hongguang Zhang, Fatih Porikli
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition, 2018
- **Summary**: Power Normalizations (PN) are very useful non-linear operators in the context of Bag-of-Words data representations as they tackle problems such as feature imbalance. In this paper, we reconsider these operators in the deep learning setup by introducing a novel layer that implements PN for non-linear pooling of feature maps. Specifically, by using a kernel formulation, our layer combines the feature vectors and their respective spatial locations in the feature maps produced by the last convolutional layer of CNN. Linearization of such a kernel results in a positive definite matrix capturing the second-order statistics of the feature vectors, to which PN operators are applied. We study two types of PN functions, namely (i) MaxExp and (ii) Gamma, addressing their role and meaning in the context of nonlinear pooling. We also provide a probabilistic interpretation of these operators and derive their surrogates with well-behaved gradients for end-to-end CNN learning. We apply our theory to practice by implementing the PN layer on a ResNet-50 model and showcase experiments on four benchmarks for fine-grained recognition, scene recognition, and material classification. Our results demonstrate state-of-the-art performance across all these tasks.



### Attention-based Few-Shot Person Re-identification Using Meta Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.09613v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09613v3)
- **Published**: 2018-06-24 22:47:39+00:00
- **Updated**: 2019-03-24 21:41:10+00:00
- **Authors**: Alireza Rahimpour, Hairong Qi
- **Comment**: This is an ongoing project and the method has been completely revised
  and more details will be available soon
- **Journal**: None
- **Summary**: In this paper, we investigate the challenging task of person re-identification from a new perspective and propose an end-to-end attention-based architecture for few-shot re-identification through meta-learning. The motivation for this task lies in the fact that humans, can usually identify another person after just seeing that given person a few times (or even once) by attending to their memory. On the other hand, the unique nature of the person re-identification problem, i.e., only few examples exist per identity and new identities always appearing during testing, calls for a few shot learning architecture with the capacity of handling new identities. Hence, we frame the problem within a meta-learning setting, where a neural network based meta-learner is trained to optimize a learner i.e., an attention-based matching function. Another challenge of the person re-identification problem is the small inter-class difference between different identities and large intra-class difference of the same identity. In order to increase the discriminative power of the model, we propose a new attention-based feature encoding scheme that takes into account the critical intra-view and cross-view relationship of images. We refer to the proposed Attention-based Re-identification Metalearning model as ARM. Extensive evaluations demonstrate the advantages of the ARM as compared to the state-of-the-art on the challenging PRID2011, CUHK01, CUHK03 and Market1501 datasets.



### Deep $k$-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1806.09228v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.09228v1)
- **Published**: 2018-06-24 22:49:24+00:00
- **Updated**: 2018-06-24 22:49:24+00:00
- **Authors**: Junru Wu, Yue Wang, Zhenyu Wu, Zhangyang Wang, Ashok Veeraraghavan, Yingyan Lin
- **Comment**: Accepted by ICML 2018
- **Journal**: None
- **Summary**: The current trend of pushing CNNs deeper with convolutions has created a pressing demand to achieve higher compression gains on CNNs where convolutions dominate the computation and parameter amount (e.g., GoogLeNet, ResNet and Wide ResNet). Further, the high energy consumption of convolutions limits its deployment on mobile devices. To this end, we proposed a simple yet effective scheme for compressing convolutions though applying k-means clustering on the weights, compression is achieved through weight-sharing, by only recording $K$ cluster centers and weight assignment indexes. We then introduced a novel spectrally relaxed $k$-means regularization, which tends to make hard assignments of convolutional layer weights to $K$ learned cluster centers during re-training. We additionally propose an improved set of metrics to estimate energy consumption of CNN hardware implementations, whose estimation results are verified to be consistent with previously proposed energy estimation tool extrapolated from actual hardware measurements. We finally evaluated Deep $k$-Means across several CNN models in terms of both compression ratio and energy consumption reduction, observing promising results without incurring accuracy loss. The code is available at https://github.com/Sandbox3aster/Deep-K-Means



### Scale Space Approximation in Convolutional Neural Networks for Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.09230v2
- **DOI**: 10.1016/j.cmpb.2019.06.030Get
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09230v2)
- **Published**: 2018-06-24 23:11:54+00:00
- **Updated**: 2018-10-18 08:09:52+00:00
- **Authors**: Kyoung Jin Noh, Sang Jun Park, Soochahn Lee
- **Comment**: 10 pages, 7 figures
- **Journal**: Computer Methods and Programs in Biomedicine, Volume 178,
  September 2019, Pages 237-246
- **Summary**: Retinal images have the highest resolution and clarity among medical images. Thus, vessel analysis in retinal images may facilitate early diagnosis and treatment of many chronic diseases. In this paper, we propose a novel multi-scale residual convolutional neural network structure based on a \emph{scale-space approximation (SSA)} block of layers, comprising subsampling and subsequent upsampling, for multi-scale representation. Through analysis in the frequency domain, we show that this block structure is a close approximation of Gaussian filtering, the operation to achieve scale variations in scale-space theory. Experimental evaluations demonstrate that the proposed network outperforms current state-of-the-art methods. Ablative analysis shows that the SSA is indeed an important factor in performance improvement.



