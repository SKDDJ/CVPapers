# Arxiv Papers in cs.CV on 2018-06-03
### Data-Free/Data-Sparse Softmax Parameter Estimation with Structured Class Geometries
- **Arxiv ID**: http://arxiv.org/abs/1806.00728v2
- **DOI**: 10.1109/LSP.2018.2860238
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.SY, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1806.00728v2)
- **Published**: 2018-06-03 02:03:32+00:00
- **Updated**: 2018-07-22 03:02:03+00:00
- **Authors**: Nisar Ahmed
- **Comment**: Final version accepted to IEEE Signal Processing Letters (double
  column), submitted July 21, 2018
- **Journal**: None
- **Summary**: This note considers softmax parameter estimation when little/no labeled training data is available, but a priori information about the relative geometry of class label log-odds boundaries is available. It is shown that `data-free' softmax model synthesis corresponds to solving a linear system of parameter equations, wherein desired dominant class log-odds boundaries are encoded via convex polytopes that decompose the input feature space. When solvable, the linear equations yield closed-form softmax parameter solution families using class boundary polytope specifications only. This allows softmax parameter learning to be implemented without expensive brute force data sampling and numerical optimization. The linear equations can also be adapted to constrained maximum likelihood estimation in data-sparse settings. Since solutions may also fail to exist for the linear parameter equations derived from certain polytope specifications, it is thus also shown that there exist probabilistic classification problems over m convexly separable classes for which the log-odds boundaries cannot be learned using an m-class softmax model.



### Content-based Video Relevance Prediction Challenge: Data, Protocol, and Baseline
- **Arxiv ID**: http://arxiv.org/abs/1806.00737v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00737v1)
- **Published**: 2018-06-03 04:51:53+00:00
- **Updated**: 2018-06-03 04:51:53+00:00
- **Authors**: Mengyi Liu, Xiaohui Xie, Hanning Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Video relevance prediction is one of the most important tasks for online streaming service. Given the relevance of videos and viewer feedbacks, the system can provide personalized recommendations, which will help the user discover more content of interest. In most online service, the computation of video relevance table is based on users' implicit feedback, e.g. watch and search history. However, this kind of method performs poorly for "cold-start" problems - when a new video is added to the library, the recommendation system needs to bootstrap the video relevance score with very little user behavior known. One promising approach to solve it is analyzing video content itself, i.e. predicting video relevance by video frame, audio, subtitle and metadata. In this paper, we describe a challenge on Content-based Video Relevance Prediction (CBVRP) that is hosted by Hulu in the ACM Multimedia Conference 2018. In this challenge, Hulu drives the study on an open problem of exploiting content characteristics directly from original video for video relevance prediction. We provide massive video assets and ground truth relevance derived from our really system, to build up a common platform for algorithm development and performance evaluation.



### Contextualize, Show and Tell: A Neural Visual Storyteller
- **Arxiv ID**: http://arxiv.org/abs/1806.00738v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.00738v1)
- **Published**: 2018-06-03 05:09:54+00:00
- **Updated**: 2018-06-03 05:09:54+00:00
- **Authors**: Diana Gonzalez-Rico, Gibran Fuentes-Pineda
- **Comment**: None
- **Journal**: None
- **Summary**: We present a neural model for generating short stories from image sequences, which extends the image description model by Vinyals et al. (Vinyals et al., 2015). This extension relies on an encoder LSTM to compute a context vector of each story from the image sequence. This context vector is used as the first state of multiple independent decoder LSTMs, each of which generates the portion of the story corresponding to each image in the sequence by taking the image embedding as the first input. Our model showed competitive results with the METEOR metric and human ratings in the internal track of the Visual Storytelling Challenge 2018.



### Eye in the Sky: Real-time Drone Surveillance System (DSS) for Violent Individuals Identification using ScatterNet Hybrid Deep Learning Network
- **Arxiv ID**: http://arxiv.org/abs/1806.00746v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00746v1)
- **Published**: 2018-06-03 07:44:11+00:00
- **Updated**: 2018-06-03 07:44:11+00:00
- **Authors**: Amarjot Singh, Devendra Patil, SN Omkar
- **Comment**: To Appear in the Efficient Deep Learning for Computer Vision (ECV)
  workshop at IEEE Computer Vision and Pattern Recognition (CVPR) 2018. Youtube
  demo at this: https://www.youtube.com/watch?v=zYypJPJipYc
- **Journal**: None
- **Summary**: Drone systems have been deployed by various law enforcement agencies to monitor hostiles, spy on foreign drug cartels, conduct border control operations, etc. This paper introduces a real-time drone surveillance system to identify violent individuals in public areas. The system first uses the Feature Pyramid Network to detect humans from aerial images. The image region with the human is used by the proposed ScatterNet Hybrid Deep Learning (SHDL) network for human pose estimation. The orientations between the limbs of the estimated pose are next used to identify the violent individuals. The proposed deep network can learn meaningful representations quickly using ScatterNet and structural priors with relatively fewer labeled examples. The system detects the violent individuals in real-time by processing the drone images in the cloud. This research also introduces the aerial violent individual dataset used for training the deep network which hopefully may encourage researchers interested in using deep learning for aerial surveillance. The pose estimation and violent individuals identification performance is compared with the state-of-the-art techniques.



### Low Cost Edge Sensing for High Quality Demosaicking
- **Arxiv ID**: http://arxiv.org/abs/1806.00771v2
- **DOI**: 10.1109/TIP.2018.2883815
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00771v2)
- **Published**: 2018-06-03 11:17:15+00:00
- **Updated**: 2018-06-05 10:45:56+00:00
- **Authors**: Yan Niu, Jihong Ouyang, Wanli Zuo, Fuxin Wang
- **Comment**: Corresponding E-mail: niuyan@jlu.edu.cn;ouyj@jlu.edu.cn
- **Journal**: None
- **Summary**: Digital cameras that use Color Filter Arrays (CFA) entail a demosaicking procedure to form full RGB images. As today's camera users generally require images to be viewed instantly, demosaicking algorithms for real applications must be fast. Moreover, the associated cost should be lower than the cost saved by using CFA. For this purpose, we revisit the classical Hamilton-Adams (HA) algorithm, which outperforms many sophisticated techniques in both speed and accuracy. Inspired by HA's strength and weakness, we design a very low cost edge sensing scheme. Briefly, it guides demosaicking by a logistic functional of the difference between directional variations. We extensively compare our algorithm with 28 demosaicking algorithms by running their open source codes on benchmark datasets. Compared to methods of similar computational cost, our method achieves substantially higher accuracy, Whereas compared to methods of similar accuracy, our method has significantly lower cost. Moreover, on test images of currently popular resolution, the quality of our algorithm is comparable to top performers, whereas its speed is tens of times faster.



### ProFlow: Learning to Predict Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1806.00800v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00800v1)
- **Published**: 2018-06-03 14:38:17+00:00
- **Updated**: 2018-06-03 14:38:17+00:00
- **Authors**: Daniel Maurer, Andr√©s Bruhn
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal coherence is a valuable source of information in the context of optical flow estimation. However, finding a suitable motion model to leverage this information is a non-trivial task. In this paper we propose an unsupervised online learning approach based on a convolutional neural network (CNN) that estimates such a motion model individually for each frame. By relating forward and backward motion these learned models not only allow to infer valuable motion information based on the backward flow, they also help to improve the performance at occlusions, where a reliable prediction is particularly useful. Moreover, our learned models are spatially variant and hence allow to estimate non-rigid motion per construction. This, in turns, allows to overcome the major limitation of recent rigidity-based approaches that seek to improve the estimation by incorporating additional stereo/SfM constraints. Experiments demonstrate the usefulness of our new approach. They not only show a consistent improvement of up to 27% for all major benchmarks (KITTI 2012, KITTI 2015, MPI Sintel) compared to a baseline without prediction, they also show top results for the MPI Sintel benchmark -- the one of the three benchmarks that contains the largest amount of non-rigid motion.



### AID++: An Updated Version of AID on Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/1806.00801v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00801v1)
- **Published**: 2018-06-03 14:40:20+00:00
- **Updated**: 2018-06-03 14:40:20+00:00
- **Authors**: Pu Jin, Gui-Song Xia, Fan Hu, Qikai Lu, Liangpei Zhang
- **Comment**: IGARSS'18 conference paper
- **Journal**: None
- **Summary**: Aerial image scene classification is a fundamental problem for understanding high-resolution remote sensing images and has become an active research task in the field of remote sensing due to its important role in a wide range of applications. However, the limitations of existing datasets for scene classification, such as the small scale and low-diversity, severely hamper the potential usage of the new generation deep convolutional neural networks (CNNs). Although huge efforts have been made in building large-scale datasets very recently, e.g., the Aerial Image Dataset (AID) which contains 10,000 image samples, they are still far from sufficient to fully train a high-capacity deep CNN model. To this end, we present a larger-scale dataset in this paper, named as AID++, for aerial scene classification based on the AID dataset. The proposed AID++ consists of more than 400,000 image samples that are semi-automatically annotated by using the existing the geographical data. We evaluate several prevalent CNN models on the proposed dataset, and the results show that our dataset can be used as a promising benchmark for scene classification.



### NAM: Non-Adversarial Unsupervised Domain Mapping
- **Arxiv ID**: http://arxiv.org/abs/1806.00804v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.00804v2)
- **Published**: 2018-06-03 14:53:20+00:00
- **Updated**: 2018-09-04 08:59:26+00:00
- **Authors**: Yedid Hoshen, Lior Wolf
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Several methods were recently proposed for the task of translating images between domains without prior knowledge in the form of correspondences. The existing methods apply adversarial learning to ensure that the distribution of the mapped source domain is indistinguishable from the target domain, which suffers from known stability issues. In addition, most methods rely heavily on `cycle' relationships between the domains, which enforce a one-to-one mapping. In this work, we introduce an alternative method: Non-Adversarial Mapping (NAM), which separates the task of target domain generative modeling from the cross-domain mapping task. NAM relies on a pre-trained generative model of the target domain, and aligns each source image with an image synthesized from the target domain, while jointly optimizing the domain mapping function. It has several key advantages: higher quality and resolution image translations, simpler and more stable training and reusable target models. Extensive experiments are presented validating the advantages of our method.



### k-Space Deep Learning for Parallel MRI: Application to Time-Resolved MR Angiography
- **Arxiv ID**: http://arxiv.org/abs/1806.00806v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.00806v2)
- **Published**: 2018-06-03 14:56:46+00:00
- **Updated**: 2018-06-10 06:51:12+00:00
- **Authors**: Eunju Cha, Eung Yeop Kim, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Time-resolved angiography with interleaved stochastic trajectories (TWIST) has been widely used for dynamic contrast enhanced MRI (DCE-MRI). To achieve highly accelerated acquisitions, TWIST combines the periphery of the k-space data from several adjacent frames to reconstruct one temporal frame. However, this view-sharing scheme limits the true temporal resolution of TWIST. Moreover, the k-space sampling patterns have been specially designed for a specific generalized autocalibrating partial parallel acquisition (GRAPPA) factor so that it is not possible to reduce the number of view-sharing once the k-data is acquired. To address these issues, this paper proposes a novel k-space deep learning approach for parallel MRI. In particular, we have designed our neural network so that accurate k-space interpolations are performed simultaneously for multiple coils by exploiting the redundancies along the coils and images. Reconstruction results using in vivo TWIST data set confirm that the proposed method can immediately generate high-quality reconstruction results with various choices of view- sharing, allowing us to exploit the trade-off between spatial and temporal resolution in time-resolved MR angiography.



### Study and development of a Computer-Aided Diagnosis system for classification of chest x-ray images using convolutional neural networks pre-trained for ImageNet and data augmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.00839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00839v1)
- **Published**: 2018-06-03 17:31:42+00:00
- **Updated**: 2018-06-03 17:31:42+00:00
- **Authors**: Vinicius Pavanelli Vianna
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (ConvNets) are the actual standard for image recognizement and classification. On the present work we develop a Computer Aided-Diagnosis (CAD) system using ConvNets to classify a x-rays chest images dataset in two groups: Normal and Pneumonia. The study uses ConvNets models available on the PyTorch platform: AlexNet, SqueezeNet, ResNet and Inception. We initially use three training styles: complete from scratch using random initialization, using a pre-trained ImageNet model training only the last layer adapted to our problem (transfer learning) and a pre-trained model modified training all the classifying layers of the model (fine tuning). The last strategy of training used is with data augmentation techniques that avoid over fitting problems on ConvNets yielding the better results on this study



### TernausNetV2: Fully Convolutional Network for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.00844v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00844v2)
- **Published**: 2018-06-03 17:55:13+00:00
- **Updated**: 2018-06-19 19:13:47+00:00
- **Authors**: Vladimir I. Iglovikov, Selim Seferbekov, Alexander V. Buslaev, Alexey Shvets
- **Comment**: None
- **Journal**: None
- **Summary**: The most common approaches to instance segmentation are complex and use two-stage networks with object proposals, conditional random-fields, template matching or recurrent neural networks. In this work we present TernausNetV2 - a simple fully convolutional network that allows extracting objects from a high-resolution satellite imagery on an instance level. The network has popular encoder-decoder type of architecture with skip connections but has a few essential modifications that allows using for semantic as well as for instance segmentation tasks. This approach is universal and allows to extend any network that has been successfully applied for semantic segmentation to perform instance segmentation task. In addition, we generalize network encoder that was pre-trained for RGB images to use additional input channels. It makes possible to use transfer learning from visual to a wider spectral range. For DeepGlobe-CVPR 2018 building detection sub-challenge, based on public leaderboard score, our approach shows superior performance in comparison to other methods. The source code corresponding pre-trained weights are publicly available at https://github.com/ternaus/TernausNetV2



### On the Flip Side: Identifying Counterexamples in Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1806.00857v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00857v3)
- **Published**: 2018-06-03 19:31:47+00:00
- **Updated**: 2018-07-24 05:05:31+00:00
- **Authors**: Gabriel Grand, Aron Szanto, Yoon Kim, Alexander Rush
- **Comment**: KDD 2018 conference version
- **Journal**: None
- **Summary**: Visual question answering (VQA) models respond to open-ended natural language questions about images. While VQA is an increasingly popular area of research, it is unclear to what extent current VQA architectures learn key semantic distinctions between visually-similar images. To investigate this question, we explore a reformulation of the VQA task that challenges models to identify counterexamples: images that result in a different answer to the original question. We introduce two methods for evaluating existing VQA models against a supervised counterexample prediction task, VQA-CX. While our models surpass existing benchmarks on VQA-CX, we find that the multimodal representations learned by an existing state-of-the-art VQA model do not meaningfully contribute to performance on this task. These results call into question the assumption that successful performance on the VQA benchmark is indicative of general visual-semantic reasoning abilities.



### A Comprehensive Comparison between Neural Style Transfer and Universal Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1806.00868v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00868v1)
- **Published**: 2018-06-03 20:28:04+00:00
- **Updated**: 2018-06-03 20:28:04+00:00
- **Authors**: Somshubra Majumdar, Amlaan Bhoi, Ganesh Jagadeesan
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: Style transfer aims to transfer arbitrary visual styles to content images. We explore algorithms adapted from two papers that try to solve the problem of style transfer while generalizing on unseen styles or compromised visual quality. Majority of the improvements made focus on optimizing the algorithm for real-time style transfer while adapting to new styles with considerably less resources and constraints. We compare these strategies and compare how they measure up to produce visually appealing images. We explore two approaches to style transfer: neural style transfer with improvements and universal style transfer. We also make a comparison between the different images produced and how they can be qualitatively measured.



### Patch-Based Image Hallucination for Super Resolution with Detail Reconstruction from Similar Sample Images
- **Arxiv ID**: http://arxiv.org/abs/1806.00874v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1806.00874v1)
- **Published**: 2018-06-03 20:59:43+00:00
- **Updated**: 2018-06-03 20:59:43+00:00
- **Authors**: Chieh-Chi Kao, Yuxiang Wang, Jonathan Waltman, Pradeep Sen
- **Comment**: 13 pages, 8 figures, submitted to IEEE Transactions on Multimedia,
  under revision
- **Journal**: None
- **Summary**: Image hallucination and super-resolution have been studied for decades, and many approaches have been proposed to upsample low-resolution images using information from the images themselves, multiple example images, or large image databases. However, most of this work has focused exclusively on small magnification levels because the algorithms simply sharpen the blurry edges in the upsampled images - no actual new detail is typically reconstructed in the final result. In this paper, we present a patch-based algorithm for image hallucination which, for the first time, properly synthesizes novel high frequency detail. To do this, we pose the synthesis problem as a patch-based optimization which inserts coherent, high-frequency detail from contextually-similar images of the same physical scene/subject provided from either a personal image collection or a large online database. The resulting image is visually plausible and contains coherent high frequency information. We demonstrate the robustness of our algorithm by testing it on a large number of images and show that its performance is considerably superior to all state-of-the-art approaches, a result that is verified to be statistically significant through a randomized user study.



### Disconnected Manifold Learning for Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.00880v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.00880v3)
- **Published**: 2018-06-03 21:19:48+00:00
- **Updated**: 2019-01-10 22:54:41+00:00
- **Authors**: Mahyar Khayatkhoei, Ahmed Elgammal, Maneesh Singh
- **Comment**: NeurIPS 2018
- **Journal**: None
- **Summary**: Natural images may lie on a union of disjoint manifolds rather than one globally connected manifold, and this can cause several difficulties for the training of common Generative Adversarial Networks (GANs). In this work, we first show that single generator GANs are unable to correctly model a distribution supported on a disconnected manifold, and investigate how sample quality, mode dropping and local convergence are affected by this. Next, we show how using a collection of generators can address this problem, providing new insights into the success of such multi-generator GANs. Finally, we explain the serious issues caused by considering a fixed prior over the collection of generators and propose a novel approach for learning the prior and inferring the necessary number of generators without any supervision. Our proposed modifications can be applied on top of any other GAN model to enable learning of distributions supported on disconnected manifolds. We conduct several experiments to illustrate the aforementioned shortcoming of GANs, its consequences in practice, and the effectiveness of our proposed modifications in alleviating these issues.



### Soccer on Your Tabletop
- **Arxiv ID**: http://arxiv.org/abs/1806.00890v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.00890v1)
- **Published**: 2018-06-03 22:51:35+00:00
- **Updated**: 2018-06-03 22:51:35+00:00
- **Authors**: Konstantinos Rematas, Ira Kemelmacher-Shlizerman, Brian Curless, Steve Seitz
- **Comment**: CVPR'18. Project: http://grail.cs.washington.edu/projects/soccer/
- **Journal**: None
- **Summary**: We present a system that transforms a monocular video of a soccer game into a moving 3D reconstruction, in which the players and field can be rendered interactively with a 3D viewer or through an Augmented Reality device. At the heart of our paper is an approach to estimate the depth map of each player, using a CNN that is trained on 3D player data extracted from soccer video games. We compare with state of the art body pose and depth estimation techniques, and show results on both synthetic ground truth benchmarks, and real YouTube soccer footage.



### Infrastructure Quality Assessment in Africa using Satellite Imagery and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.00894v1
- **DOI**: 10.1145/3219819.3219924
- **Categories**: **cs.CY**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.00894v1)
- **Published**: 2018-06-03 23:30:01+00:00
- **Updated**: 2018-06-03 23:30:01+00:00
- **Authors**: Barak Oshri, Annie Hu, Peter Adelson, Xiao Chen, Pascaline Dupas, Jeremy Weinstein, Marshall Burke, David Lobell, Stefano Ermon
- **Comment**: None
- **Journal**: KDD 2018 Proceedings of the 24th ACM SIGKDD International
  Conference on Knowledge Discovery & Data Mining
- **Summary**: The UN Sustainable Development Goals allude to the importance of infrastructure quality in three of its seventeen goals. However, monitoring infrastructure quality in developing regions remains prohibitively expensive and impedes efforts to measure progress toward these goals. To this end, we investigate the use of widely available remote sensing data for the prediction of infrastructure quality in Africa. We train a convolutional neural network to predict ground truth labels from the Afrobarometer Round 6 survey using Landsat 8 and Sentinel 1 satellite imagery.   Our best models predict infrastructure quality with AUROC scores of 0.881 on Electricity, 0.862 on Sewerage, 0.739 on Piped Water, and 0.786 on Roads using Landsat 8. These performances are significantly better than models that leverage OpenStreetMap or nighttime light intensity on the same tasks. We also demonstrate that our trained model can accurately make predictions in an unseen country after fine-tuning on a small sample of images. Furthermore, the model can be deployed in regions with limited samples to predict infrastructure outcomes with higher performance than nearest neighbor spatial interpolation.



