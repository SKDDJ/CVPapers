# Arxiv Papers in cs.CV on 2018-06-30
### Classification of lung nodules in CT images based on Wasserstein distance in differential geometry
- **Arxiv ID**: http://arxiv.org/abs/1807.00094v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.00094v1)
- **Published**: 2018-06-30 00:30:53+00:00
- **Updated**: 2018-06-30 00:30:53+00:00
- **Authors**: Min Zhang, Qianli Ma, Chengfeng Wen, Hai Chen, Deruo Liu, Xianfeng Gu, Jie He, Xiaoyin Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Lung nodules are commonly detected in screening for patients with a risk for lung cancer. Though the status of large nodules can be easily diagnosed by fine needle biopsy or bronchoscopy, small nodules are often difficult to classify on computed tomography (CT). Recent works have shown that shape analysis of lung nodules can be used to differentiate benign lesions from malignant ones, though existing methods are limited in their sensitivity and specificity. In this work we introduced a new 3D shape analysis within the framework of differential geometry to calculate the Wasserstein distance between benign and malignant lung nodules to derive an accurate classification scheme. The Wasserstein distance between the nodules is calculated based on our new spherical optimal mass transport, this new algorithm works directly on sphere by using spherical metric, which is much more accurate and efficient than previous methods. In the process of deformation, the area-distortion factor gives a probability measure on the unit sphere, which forms the Wasserstein space. From known cases of benign and malignant lung nodules, we can calculate a unique optimal mass transport map between their correspondingly deformed Wasserstein spaces. This transportation cost defines the Wasserstein distance between them and can be used to classify new lung nodules into either the benign or malignant class. To the best of our knowledge, this is the first work that utilizes Wasserstein distance for lung nodule classification. The advantages of Wasserstein distance are it is invariant under rigid motions and scalings, thus it intrinsically measures shape distance even when the underlying shapes are of high complexity, making it well suited to classify lung nodules as they have different sizes, orientations, and appearances.



### G2C: A Generator-to-Classifier Framework Integrating Multi-Stained Visual Cues for Pathological Glomerulus Classification
- **Arxiv ID**: http://arxiv.org/abs/1807.03136v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03136v3)
- **Published**: 2018-06-30 01:09:32+00:00
- **Updated**: 2019-03-07 07:06:53+00:00
- **Authors**: Bingzhe Wu, Xiaolu Zhang, Shiwan Zhao, Lingxi Xie, Caihong Zeng, Zhihong Liu, Guangyu Sun
- **Comment**: Accepted by AAAI 2019
- **Journal**: None
- **Summary**: Pathological glomerulus classification plays a key role in the diagnosis of nephropathy. As the difference between different subcategories is subtle, doctors often refer to slides from different staining methods to make decisions. However, creating correspondence across various stains is labor-intensive, bringing major difficulties in collecting data and training a vision-based algorithm to assist nephropathy diagnosis. This paper provides an alternative solution for integrating multi-stained visual cues for glomerulus classification. Our approach, named generator-to-classifier (G2C), is a two-stage framework. Given an input image from a specified stain, several generators are first applied to estimate its appearances in other staining methods, and a classifier follows to combine visual cues from different stains for prediction (whether it is pathological, or which type of pathology it has). We optimize these two stages in a joint manner. To provide a reasonable initialization, we pre-train the generators in an unlabeled reference set under an unpaired image-to-image translation task, and then fine-tune them together with the classifier. We conduct experiments on a glomerulus type classification dataset collected by ourselves (there are no publicly available datasets for this purpose). Although joint optimization slightly harms the authenticity of the generated patches, it boosts classification performance, suggesting more effective visual cues are extracted in an automatic way. We also transfer our model to a public dataset for breast cancer classification, and outperform the state-of-the-arts significantly.



### Mammography Assessment using Multi-Scale Deep Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1807.03095v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.03095v1)
- **Published**: 2018-06-30 01:26:51+00:00
- **Updated**: 2018-06-30 01:26:51+00:00
- **Authors**: Ulzee An, Khader Shameer, Lakshmi Subramanian
- **Comment**: Prepared for MLMH at KDD 2018
- **Journal**: None
- **Summary**: Applying deep learning methods to mammography assessment has remained a challenging topic. Dense noise with sparse expressions, mega-pixel raw data resolution, lack of diverse examples have all been factors affecting performance. The lack of pixel-level ground truths have especially limited segmentation methods in pushing beyond approximately bounding regions. We propose a classification approach grounded in high performance tissue assessment as an alternative to all-in-one localization and assessment models that is also capable of pinpointing the causal pixels. First, the objective of the mammography assessment task is formalized in the context of local tissue classifiers. Then, the accuracy of a convolutional neural net is evaluated on classifying patches of tissue with suspicious findings at varying scales, where highest obtained AUC is above $0.9$. The local evaluations of one such expert tissue classifier is used to augment the results of a heatmap regression model and additionally recover the exact causal regions at high resolution as a saliency image suitable for clinical settings.



### Structure Inference Net: Object Detection Using Scene-Level Context and Instance-Level Relationships
- **Arxiv ID**: http://arxiv.org/abs/1807.00119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00119v1)
- **Published**: 2018-06-30 03:33:55+00:00
- **Updated**: 2018-06-30 03:33:55+00:00
- **Authors**: Yong Liu, Ruiping Wang, Shiguang Shan, Xilin Chen
- **Comment**: published in CVPR 2018
- **Journal**: None
- **Summary**: Context is important for accurate visual recognition. In this work we propose an object detection algorithm that not only considers object visual appearance, but also makes use of two kinds of context including scene contextual information and object relationships within a single image. Therefore, object detection is regarded as both a cognition problem and a reasoning problem when leveraging these structured information. Specifically, this paper formulates object detection as a problem of graph structure inference, where given an image the objects are treated as nodes in a graph and relationships between the objects are modeled as edges in such graph. To this end, we present a so-called Structure Inference Network (SIN), a detector that incorporates into a typical detection framework (e.g. Faster R-CNN) with a graphical model which aims to infer object state. Comprehensive experiments on PASCAL VOC and MS COCO datasets indicate that scene context and object relationships truly improve the performance of object detection with more desirable and reasonable outputs.



### Quantity beats quality for semantic segmentation of corrosion in images
- **Arxiv ID**: http://arxiv.org/abs/1807.03138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03138v1)
- **Published**: 2018-06-30 04:53:57+00:00
- **Updated**: 2018-06-30 04:53:57+00:00
- **Authors**: Will Nash, Tom Drummond, Nick Birbilis
- **Comment**: None
- **Journal**: None
- **Summary**: Dataset creation is typically one of the first steps when applying Artificial Intelligence methods to a new task; and the real world performance of models hinges on the quality and quantity of data available. Producing an image dataset for semantic segmentation is resource intensive, particularly for specialist subjects where class segmentation is not able to be effectively farmed out. The benefit of producing a large, but poorly labelled, dataset versus a small, expertly segmented dataset for semantic segmentation is an open question. Here we show that a large, noisy dataset outperforms a small, expertly segmented dataset for training a Fully Convolutional Network model for semantic segmentation of corrosion in images. A large dataset of 250 images with segmentations labelled by undergraduates and a second dataset of just 10 images, with segmentations labelled by subject matter experts were produced. The mean Intersection over Union and micro F-score metrics were compared after training for 50,000 epochs. This work is illustrative for researchers setting out to develop deep learning models for detection and location of specialist features.



### Fractional Wavelet Scattering Network and Applications
- **Arxiv ID**: http://arxiv.org/abs/1807.00141v1
- **DOI**: 10.1109/TBME.2018.2850356
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00141v1)
- **Published**: 2018-06-30 08:38:22+00:00
- **Updated**: 2018-06-30 08:38:22+00:00
- **Authors**: Li Liu, Jiasong Wu, Dengwang Li, Lotfi Senhadji, Huazhong Shu
- **Comment**: 11 pages, 6 figures, 3 tables, IEEE Transactions on Biomedical
  Engineering, 2018
- **Journal**: None
- **Summary**: Objective: The present study introduces a fractional wavelet scattering network (FrScatNet), which is a generalized translation invariant version of the classical wavelet scattering network (ScatNet). Methods: In our approach, the FrScatNet is constructed based on the fractional wavelet transform (FRWT). The fractional scattering coefficients are iteratively computed using FRWTs and modulus operators. The feature vectors constructed by fractional scattering coefficients are usually used for signal classification. In this work, an application example of FrScatNet is provided in order to assess its performance on pathological images. Firstly, the FrScatNet extracts feature vectors from patches of the original histological images under different orders. Then we classify those patches into target (benign or malignant) and background groups. And the FrScatNet property is analyzed by comparing error rates computed from different fractional orders respectively. Based on the above pathological image classification, a gland segmentation algorithm is proposed by combining the boundary information and the gland location. Results: The error rates for different fractional orders of FrScatNet are examined and show that the classification accuracy is significantly improved in fractional scattering domain. We also compare the FrScatNet based gland segmentation method with those proposed in the 2015 MICCAI Gland Segmentation Challenge and our method achieves comparable results. Conclusion: The FrScatNet is shown to achieve accurate and robust results. More stable and discriminative fractional scattering coefficients are obtained by the FrScatNet in this work. Significance: The added fractional order parameter is able to analyze the image in the fractional scattering domain.



### Cost-effective Object Detection: Active Sample Mining with Switchable Selection Criteria
- **Arxiv ID**: http://arxiv.org/abs/1807.00147v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00147v3)
- **Published**: 2018-06-30 09:07:33+00:00
- **Updated**: 2019-01-12 03:32:19+00:00
- **Authors**: Keze Wang, Liang Lin, Xiaopeng Yan, Ziliang Chen, Dongyu Zhang, Lei Zhang
- **Comment**: Automatically determining whether an unlabeled sample should be
  manually annotated or pseudo-labeled via a novel self-learning process
  (Accepted by TNNLS 2018) The source code is available at
  http://kezewang.com/codes/ASM_ver1.zip
- **Journal**: None
- **Summary**: Though quite challenging, leveraging large-scale unlabeled or partially labeled data in learning systems (e.g., model/classifier training) has attracted increasing attentions due to its fundamental importance. To address this problem, many active learning (AL) methods have been proposed that employ up-to-date detectors to retrieve representative minority samples according to predefined confidence or uncertainty thresholds. However, these AL methods cause the detectors to ignore the remaining majority samples (i.e., those with low uncertainty or high prediction confidence). In this work, by developing a principled active sample mining (ASM) framework, we demonstrate that cost-effectively mining samples from these unlabeled majority data is key to training more powerful object detectors while minimizing user effort. Specifically, our ASM framework involves a switchable sample selection mechanism for determining whether an unlabeled sample should be manually annotated via AL or automatically pseudo-labeled via a novel self-learning process. The proposed process can be compatible with mini-batch based training (i.e., using a batch of unlabeled or partially labeled data as a one-time input) for object detection. In addition, a few samples with low-confidence predictions are selected and annotated via AL. Notably, our method is suitable for object categories that are not seen in the unlabeled data during the learning process. Extensive experiments clearly demonstrate that our ASM framework can achieve performance comparable to that of alternative methods but with significantly fewer annotations.



### Utility in Fashion with implicit feedback
- **Arxiv ID**: http://arxiv.org/abs/1807.03139v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.03139v1)
- **Published**: 2018-06-30 12:27:17+00:00
- **Updated**: 2018-06-30 12:27:17+00:00
- **Authors**: Vikram Garg, Girish Sathyanarayana, Sumit Borar, Aruna Rajan
- **Comment**: None
- **Journal**: None
- **Summary**: Fashion preference is a fuzzy concept that depends on customer taste, prevailing norms in fashion product/style, henceforth used interchangeably, and a customer's perception of utility or fashionability, yet fashion e-retail relies on algorithmically generated search and recommendation systems that process structured data and images to best match customer preference. Retailers study tastes solely as a function of what sold vs what did not, and take it to represent customer preference. Such explicit modeling, however, belies the underlying user preference, which is a complicated interplay of preference and commercials such as brand, price point, promotions, other sale events, and competitor push/marketing. It is hard to infer a notion of utility or even customer preference by looking at sales data.   In search and recommendation systems for fashion e-retail, customer preference is implicitly derived by user-user similarity or item-item similarity. In this work, we aim to derive a metric that separates the buying preferences of users from the commercials of the merchandise (price, promotions, etc). We extend our earlier work on explicit signals to gauge sellability or preference with implicit signals from user behaviour.



### Improved Techniques for Learning to Dehaze and Beyond: A Collective Study
- **Arxiv ID**: http://arxiv.org/abs/1807.00202v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.00202v2)
- **Published**: 2018-06-30 16:52:33+00:00
- **Updated**: 2018-07-30 03:58:30+00:00
- **Authors**: Yu Liu, Guanlong Zhao, Boyuan Gong, Yang Li, Ritu Raj, Niraj Goel, Satya Kesav, Sandeep Gottimukkala, Zhangyang Wang, Wenqi Ren, Dacheng Tao
- **Comment**: updated: typo fixed and some other improvements on writing
- **Journal**: None
- **Summary**: Here we explore two related but important tasks based on the recently released REalistic Single Image DEhazing (RESIDE) benchmark dataset: (i) single image dehazing as a low-level image restoration problem; and (ii) high-level visual understanding (e.g., object detection) of hazy images. For the first task, we investigated a variety of loss functions and show that perception-driven loss significantly improves dehazing performance. In the second task, we provide multiple solutions including using advanced modules in the dehazing-detection cascade and domain-adaptive object detectors. In both tasks, our proposed solutions significantly improve performance. GitHub repository URL is: https://github.com/guanlongzhao/dehaze



### Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization
- **Arxiv ID**: http://arxiv.org/abs/1807.00230v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00230v2)
- **Published**: 2018-06-30 21:50:21+00:00
- **Updated**: 2018-11-09 22:20:11+00:00
- **Authors**: Bruno Korbar, Du Tran, Lorenzo Torresani
- **Comment**: Note: Changed name - added experiments
- **Journal**: None
- **Summary**: There is a natural correlation between the visual and auditive elements of a video. In this work we leverage this connection to learn general and effective models for both audio and video analysis from self-supervised temporal synchronization. We demonstrate that a calibrated curriculum learning scheme, a careful choice of negative examples, and the use of a contrastive loss are critical ingredients to obtain powerful multi-sensory representations from models optimized to discern temporal synchronization of audio-video pairs. Without further finetuning, the resulting audio features achieve performance superior or comparable to the state-of-the-art on established audio classification benchmarks (DCASE2014 and ESC-50). At the same time, our visual subnet provides a very effective initialization to improve the accuracy of video-based action recognition models: compared to learning from scratch, our self-supervised pretraining yields a remarkable gain of +19.9% in action recognition accuracy on UCF101 and a boost of +17.7% on HMDB51.



