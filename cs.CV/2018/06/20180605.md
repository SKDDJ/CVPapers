# Arxiv Papers in cs.CV on 2018-06-05
### SoPhie: An Attentive GAN for Predicting Paths Compliant to Social and Physical Constraints
- **Arxiv ID**: http://arxiv.org/abs/1806.01482v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01482v2)
- **Published**: 2018-06-05 03:49:46+00:00
- **Updated**: 2018-09-20 17:42:42+00:00
- **Authors**: Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki Hirose, S. Hamid Rezatofighi, Silvio Savarese
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of path prediction for multiple interacting agents in a scene, which is a crucial step for many autonomous platforms such as self-driving cars and social robots. We present \textit{SoPhie}; an interpretable framework based on Generative Adversarial Network (GAN), which leverages two sources of information, the path history of all the agents in a scene, and the scene context information, using images of the scene. To predict a future path for an agent, both physical and social information must be leveraged. Previous work has not been successful to jointly model physical and social interactions. Our approach blends a social attention mechanism with a physical attention that helps the model to learn where to look in a large scene and extract the most salient parts of the image relevant to the path. Whereas, the social attention component aggregates information across the different agent interactions and extracts the most important trajectory information from the surrounding neighbors. SoPhie also takes advantage of GAN to generates more realistic samples and to capture the uncertain nature of the future paths by modeling its distribution. All these mechanisms enable our approach to predict socially and physically plausible paths for the agents and to achieve state-of-the-art performance on several different trajectory forecasting benchmarks.



### 3D Human Pose Estimation with 2D Marginal Heatmaps
- **Arxiv ID**: http://arxiv.org/abs/1806.01484v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01484v2)
- **Published**: 2018-06-05 03:51:14+00:00
- **Updated**: 2018-11-08 02:47:32+00:00
- **Authors**: Aiden Nibali, Zhen He, Stuart Morgan, Luke Prendergast
- **Comment**: Accepted in WACV 2019
- **Journal**: None
- **Summary**: Automatically determining three-dimensional human pose from monocular RGB image data is a challenging problem. The two-dimensional nature of the input results in intrinsic ambiguities which make inferring depth particularly difficult. Recently, researchers have demonstrated that the flexible statistical modelling capabilities of deep neural networks are sufficient to make such inferences with reasonable accuracy. However, many of these models use coordinate output techniques which are memory-intensive, not differentiable, and/or do not spatially generalise well. We propose improvements to 3D coordinate prediction which avoid the aforementioned undesirable traits by predicting 2D marginal heatmaps under an augmented soft-argmax scheme. Our resulting model, MargiPose, produces visually coherent heatmaps whilst maintaining differentiability. We are also able to achieve state-of-the-art accuracy on publicly available 3D human pose estimation data.



### Deep Image Compression via End-to-End Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.01496v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.01496v1)
- **Published**: 2018-06-05 05:07:51+00:00
- **Updated**: 2018-06-05 05:07:51+00:00
- **Authors**: Haojie Liu, Tong Chen, Qiu Shen, Tao Yue, Zhan Ma
- **Comment**: None
- **Journal**: None
- **Summary**: We present a lossy image compression method based on deep convolutional neural networks (CNNs), which outperforms the existing BPG, WebP, JPEG2000 and JPEG as measured via multi-scale structural similarity (MS-SSIM), at the same bit rate. Currently, most of the CNNs based approaches train the network using a L2 loss between the reconstructions and the ground-truths in the pixel domain, which leads to over-smoothing results and visual quality degradation especially at a very low bit rate. Therefore, we improve the subjective quality with the combination of a perception loss and an adversarial loss additionally. To achieve better rate-distortion optimization (RDO), we also introduce an easy-to-hard transfer learning when adding quantization error and rate constraint. Finally, we evaluate our method on public Kodak and the Test Dataset P/M released by the Computer Vision Lab of ETH Zurich, resulting in averaged 7.81% and 19.1% BD-rate reduction over BPG, respectively.



### Construction of all-in-focus images assisted by depth sensing
- **Arxiv ID**: http://arxiv.org/abs/1806.01524v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4.6; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1806.01524v1)
- **Published**: 2018-06-05 07:34:38+00:00
- **Updated**: 2018-06-05 07:34:38+00:00
- **Authors**: Hang Liu, Hengyu Li, Jun Luo, Shaorong Xie, Yu Sun
- **Comment**: 18 pages. This paper has been submitted to Computer Vision and Image
  Understanding
- **Journal**: None
- **Summary**: Multi-focus image fusion is a technique for obtaining an all-in-focus image in which all objects are in focus to extend the limited depth of field (DoF) of an imaging system. Different from traditional RGB-based methods, this paper presents a new multi-focus image fusion method assisted by depth sensing. In this work, a depth sensor is used together with a color camera to capture images of a scene. A graph-based segmentation algorithm is used to segment the depth map from the depth sensor, and the segmented regions are used to guide a focus algorithm to locate in-focus image blocks from among multi-focus source images to construct the reference all-in-focus image. Five test scenes and six evaluation metrics were used to compare the proposed method and representative state-of-the-art algorithms. Experimental results quantitatively demonstrate that this method outperforms existing methods in both speed and quality (in terms of comprehensive fusion metrics). The generated images can potentially be used as reference all-in-focus images.



### Deep Mixture of Experts via Shallow Embedding
- **Arxiv ID**: http://arxiv.org/abs/1806.01531v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01531v3)
- **Published**: 2018-06-05 07:41:04+00:00
- **Updated**: 2019-04-11 20:55:58+00:00
- **Authors**: Xin Wang, Fisher Yu, Lisa Dunlap, Yi-An Ma, Ruth Wang, Azalia Mirhoseini, Trevor Darrell, Joseph E. Gonzalez
- **Comment**: None
- **Journal**: None
- **Summary**: Larger networks generally have greater representational power at the cost of increased computational complexity. Sparsifying such networks has been an active area of research but has been generally limited to static regularization or dynamic approaches using reinforcement learning. We explore a mixture of experts (MoE) approach to deep dynamic routing, which activates certain experts in the network on a per-example basis. Our novel DeepMoE architecture increases the representational power of standard convolutional networks by adaptively sparsifying and recalibrating channel-wise features in each convolutional layer. We employ a multi-headed sparse gating network to determine the selection and scaling of channels for each input, leveraging exponential combinations of experts within a single convolutional network. Our proposed architecture is evaluated on four benchmark datasets and tasks, and we show that Deep-MoEs are able to achieve higher accuracy with lower computation than standard convolutional networks.



### Semi-Supervised Clustering with Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.01547v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.01547v2)
- **Published**: 2018-06-05 08:23:42+00:00
- **Updated**: 2018-07-10 09:10:35+00:00
- **Authors**: Ankita Shukla, Gullal Singh Cheema, Saket Anand
- **Comment**: 9 Pages
- **Journal**: None
- **Summary**: Clustering using neural networks has recently demonstrated promising performance in machine learning and computer vision applications. However, the performance of current approaches is limited either by unsupervised learning or their dependence on large set of labeled data samples. In this paper, we propose ClusterNet that uses pairwise semantic constraints from very few labeled data samples (<5% of total data) and exploits the abundant unlabeled data to drive the clustering approach. We define a new loss function that uses pairwise semantic similarity between objects combined with constrained k-means clustering to efficiently utilize both labeled and unlabeled data in the same framework. The proposed network uses convolution autoencoder to learn a latent representation that groups data into k specified clusters, while also learning the cluster centers simultaneously. We evaluate and compare the performance of ClusterNet on several datasets and state of the art deep clustering approaches.



### TS-Net: Combining modality specific and common features for multimodal patch matching
- **Arxiv ID**: http://arxiv.org/abs/1806.01550v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01550v1)
- **Published**: 2018-06-05 08:25:46+00:00
- **Updated**: 2018-06-05 08:25:46+00:00
- **Authors**: Sovann En, Alexis Lechervy, Frédéric Jurie
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal patch matching addresses the problem of finding the correspondences between image patches from two different modalities, e.g. RGB vs sketch or RGB vs near-infrared. The comparison of patches of different modalities can be done by discovering the information common to both modalities (Siamese like approaches) or the modality-specific information (Pseudo-Siamese like approaches). We observed that none of these two scenarios is optimal. This motivates us to propose a three-stream architecture, dubbed as TS-Net, combining the benefits of the two. In addition, we show that adding extra constraints in the intermediate layers of such networks further boosts the performance. Experimentations on three multimodal datasets show significant performance gains in comparison with Siamese and Pseudo-Siamese networks.



### Adaptive Importance Learning for Improving Lightweight Image Super-resolution Network
- **Arxiv ID**: http://arxiv.org/abs/1806.01576v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01576v1)
- **Published**: 2018-06-05 09:31:19+00:00
- **Updated**: 2018-06-05 09:31:19+00:00
- **Authors**: Lei Zhang, Peng Wang, Chunhua Shen, Lingqiao Liu, Wei Wei, Yanning Zhang, Anton van den Hengel
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Deep neural networks have achieved remarkable success in single image super-resolution (SISR). The computing and memory requirements of these methods have hindered their application to broad classes of real devices with limited computing power, however. One approach to this problem has been lightweight network architectures that bal- ance the super-resolution performance and the computation burden. In this study, we revisit this problem from an orthog- onal view, and propose a novel learning strategy to maxi- mize the pixel-wise fitting capacity of a given lightweight network architecture. Considering that the initial capacity of the lightweight network is very limited, we present an adaptive importance learning scheme for SISR that trains the network with an easy-to-complex paradigm by dynam- ically updating the importance of image pixels on the basis of the training loss. Specifically, we formulate the network training and the importance learning into a joint optimization problem. With a carefully designed importance penalty function, the importance of individual pixels can be gradu- ally increased through solving a convex optimization problem. The training process thus begins with pixels that are easy to reconstruct, and gradually proceeds to more complex pixels as fitting improves.



### Stochastic Gradient Descent with Hyperbolic-Tangent Decay on Classification
- **Arxiv ID**: http://arxiv.org/abs/1806.01593v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01593v2)
- **Published**: 2018-06-05 10:14:01+00:00
- **Updated**: 2018-11-12 22:50:09+00:00
- **Authors**: Bo Yang Hsueh, Wei Li, I-Chen Wu
- **Comment**: WACV2019
- **Journal**: None
- **Summary**: Learning rate scheduler has been a critical issue in the deep neural network training. Several schedulers and methods have been proposed, including step decay scheduler, adaptive method, cosine scheduler and cyclical scheduler. This paper proposes a new scheduling method, named hyperbolic-tangent decay (HTD). We run experiments on several benchmarks such as: ResNet, Wide ResNet and DenseNet for CIFAR-10 and CIFAR-100 datasets, LSTM for PAMAP2 dataset, ResNet on ImageNet and Fashion-MNIST datasets. In our experiments, HTD outperforms step decay and cosine scheduler in nearly all cases, while requiring less hyperparameters than step decay, and more flexible than cosine scheduler. Code is available at https://github.com/BIGBALLON/HTD.



### Layer rotation: a surprisingly powerful indicator of generalization in deep networks?
- **Arxiv ID**: http://arxiv.org/abs/1806.01603v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.01603v2)
- **Published**: 2018-06-05 10:39:21+00:00
- **Updated**: 2019-07-01 16:01:43+00:00
- **Authors**: Simon Carbonnelle, Christophe De Vleeschouwer
- **Comment**: Extended version of paper presented at ICML workshop "Identifying and
  Understanding Deep Learning Phenomena"
- **Journal**: None
- **Summary**: Our work presents extensive empirical evidence that layer rotation, i.e. the evolution across training of the cosine distance between each layer's weight vector and its initialization, constitutes an impressively consistent indicator of generalization performance. In particular, larger cosine distances between final and initial weights of each layer consistently translate into better generalization performance of the final model. Interestingly, this relation admits a network independent optimum: training procedures during which all layers' weights reach a cosine distance of 1 from their initialization consistently outperform other configurations -by up to 30% test accuracy. Moreover, we show that layer rotations are easily monitored and controlled (helpful for hyperparameter tuning) and potentially provide a unified framework to explain the impact of learning rate tuning, weight decay, learning rate warmups and adaptive gradient methods on generalization and training speed. In an attempt to explain the surprising properties of layer rotation, we show on a 1-layer MLP trained on MNIST that layer rotation correlates with the degree to which features of intermediate layers have been trained.



### Real-time Lane Marker Detection Using Template Matching with RGB-D Camera
- **Arxiv ID**: http://arxiv.org/abs/1806.01621v1
- **DOI**: 10.1109/SIGTELCOM.2018.8325781
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1806.01621v1)
- **Published**: 2018-06-05 12:03:18+00:00
- **Updated**: 2018-06-05 12:03:18+00:00
- **Authors**: Cong Hoang Quach, Van Lien Tran, Duy Hung Nguyen, Viet Thang Nguyen, Minh Trien Pham, Manh Duong Phung
- **Comment**: 2018 2nd International Conference on Recent Advances in Signal
  Processing, Telecommunications & Computing (SigTelCom)
- **Journal**: None
- **Summary**: This paper addresses the problem of lane detection which is fundamental for self-driving vehicles. Our approach exploits both colour and depth information recorded by a single RGB-D camera to better deal with negative factors such as lighting conditions and lane-like objects. In the approach, colour and depth images are first converted to a half-binary format and a 2D matrix of 3D points. They are then used as the inputs of template matching and geometric feature extraction processes to form a response map so that its values represent the probability of pixels being lane markers. To further improve the results, the template and lane surfaces are finally refined by principal component analysis and lane model fitting techniques. A number of experiments have been conducted on both synthetic and real datasets. The result shows that the proposed approach can effectively eliminate unwanted noise to accurately detect lane markers in various scenarios. Moreover, the processing speed of 20 frames per second under hardware configuration of a popular laptop computer allows the proposed algorithm to be implemented for real-time autonomous driving applications.



### Recurrent Convolutional Fusion for RGB-D Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.01673v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01673v3)
- **Published**: 2018-06-05 13:14:17+00:00
- **Updated**: 2019-02-24 14:32:35+00:00
- **Authors**: Mohammad Reza Loghmani, Mirco Planamente, Barbara Caputo, Markus Vincze
- **Comment**: Under review at RA-L
- **Journal**: None
- **Summary**: Providing machines with the ability to recognize objects like humans has always been one of the primary goals of machine vision. The introduction of RGB-D cameras has paved the way for a significant leap forward in this direction thanks to the rich information provided by these sensors. However, the machine vision community still lacks an effective method to synergically use the RGB and depth data to improve object recognition. In order to take a step in this direction, we introduce a novel end-to-end architecture for RGB-D object recognition called recurrent convolutional fusion (RCFusion). Our method generates compact and highly discriminative multi-modal features by combining complementary RGB and depth information representing different levels of abstraction. Extensive experiments on two popular datasets, RGB-D Object Dataset and JHUIT-50, show that RCFusion significantly outperforms state-of-the-art approaches in both the object categorization and instance recognition tasks.



### Practical Deep Stereo (PDS): Toward applications-friendly deep stereo matching
- **Arxiv ID**: http://arxiv.org/abs/1806.01677v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1806.01677v1)
- **Published**: 2018-06-05 13:24:40+00:00
- **Updated**: 2018-06-05 13:24:40+00:00
- **Authors**: Stepan Tulyakov, Anton Ivanov, Francois Fleuret
- **Comment**: None
- **Journal**: None
- **Summary**: End-to-end deep-learning networks recently demonstrated extremely good perfor- mance for stereo matching. However, existing networks are difficult to use for practical applications since (1) they are memory-hungry and unable to process even modest-size images, (2) they have to be trained for a given disparity range. The Practical Deep Stereo (PDS) network that we propose addresses both issues: First, its architecture relies on novel bottleneck modules that drastically reduce the memory footprint in inference, and additional design choices allow to handle greater image size during training. This results in a model that leverages large image context to resolve matching ambiguities. Second, a novel sub-pixel cross- entropy loss combined with a MAP estimator make this network less sensitive to ambiguous matches, and applicable to any disparity range without re-training. We compare PDS to state-of-the-art methods published over the recent months, and demonstrate its superior performance on FlyingThings3D and KITTI sets.



### EasyConvPooling: Random Pooling with Easy Convolution for Accelerating Training and Testing
- **Arxiv ID**: http://arxiv.org/abs/1806.01729v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01729v1)
- **Published**: 2018-06-05 14:56:10+00:00
- **Updated**: 2018-06-05 14:56:10+00:00
- **Authors**: Jianzhong Sheng, Chuanbo Chen, Chenchen Fu, Chun Jason Xue
- **Comment**: None
- **Journal**: None
- **Summary**: Convolution operations dominate the overall execution time of Convolutional Neural Networks (CNNs). This paper proposes an easy yet efficient technique for both Convolutional Neural Network training and testing. The conventional convolution and pooling operations are replaced by Easy Convolution and Random Pooling (ECP). In ECP, we randomly select one pixel out of four and only conduct convolution operations of the selected pixel. As a result, only a quarter of the conventional convolution computations are needed. Experiments demonstrate that the proposed EasyConvPooling can achieve 1.45x speedup on training time and 1.64x on testing time. What's more, a speedup of 5.09x on pure Easy Convolution operations is obtained compared to conventional convolution operations.



### Monte Carlo Convolution for Learning on Non-Uniformly Sampled Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1806.01759v2
- **DOI**: 10.1145/3272127.3275110
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01759v2)
- **Published**: 2018-06-05 15:56:24+00:00
- **Updated**: 2018-09-25 08:29:12+00:00
- **Authors**: Pedro Hermosilla, Tobias Ritschel, Pere-Pau Vázquez, Àlvar Vinacua, Timo Ropinski
- **Comment**: ACM Transactions on Graphics (Proocedings of SIGGRAPH Asia 2018)
- **Journal**: None
- **Summary**: Deep learning systems extensively use convolution operations to process input data. Though convolution is clearly defined for structured data such as 2D images or 3D volumes, this is not true for other data types such as sparse point clouds. Previous techniques have developed approximations to convolutions for restricted conditions. Unfortunately, their applicability is limited and cannot be used for general point clouds. We propose an efficient and effective method to learn convolutions for non-uniformly sampled point clouds, as they are obtained with modern acquisition techniques. Learning is enabled by four key novelties: first, representing the convolution kernel itself as a multilayer perceptron; second, phrasing convolution as a Monte Carlo integration problem, third, using this notion to combine information from multiple samplings at different levels; and fourth using Poisson disk sampling as a scalable means of hierarchical point cloud learning. The key idea across all these contributions is to guarantee adequate consideration of the underlying non-uniform sample distribution function from a Monte Carlo perspective. To make the proposed concepts applicable to real-world tasks, we furthermore propose an efficient implementation which significantly reduces the GPU memory required during the training process. By employing our method in hierarchical network architectures we can outperform most of the state-of-the-art networks on established point cloud segmentation, classification and normal estimation benchmarks. Furthermore, in contrast to most existing approaches, we also demonstrate the robustness of our method with respect to sampling variations, even when training with uniformly sampled data only. To support the direct application of these concepts, we provide a ready-to-use TensorFlow implementation of these layers at https://github.com/viscom-ulm/MCCNN



### Graph Saliency Maps through Spectral Convolutional Networks: Application to Sex Classification with Brain Connectivity
- **Arxiv ID**: http://arxiv.org/abs/1806.01764v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01764v1)
- **Published**: 2018-06-05 16:01:36+00:00
- **Updated**: 2018-06-05 16:01:36+00:00
- **Authors**: Salim Arslan, Sofia Ira Ktena, Ben Glocker, Daniel Rueckert
- **Comment**: None
- **Journal**: None
- **Summary**: Graph convolutional networks (GCNs) allow to apply traditional convolution operations in non-Euclidean domains, where data are commonly modelled as irregular graphs. Medical imaging and, in particular, neuroscience studies often rely on such graph representations, with brain connectivity networks being a characteristic example, while ultimately seeking the locus of phenotypic or disease-related differences in the brain. These regions of interest (ROIs) are, then, considered to be closely associated with function and/or behaviour. Driven by this, we explore GCNs for the task of ROI identification and propose a visual attribution method based on class activation mapping. By undertaking a sex classification task as proof of concept, we show that this method can be used to identify salient nodes (brain regions) without prior node labels. Based on experiments conducted on neuroimaging data of more than 5000 participants from UK Biobank, we demonstrate the robustness of the proposed method in highlighting reproducible regions across individuals. We further evaluate the neurobiological relevance of the identified regions based on evidence from large-scale UK Biobank studies.



### Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects
- **Arxiv ID**: http://arxiv.org/abs/1806.01794v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.01794v2)
- **Published**: 2018-06-05 16:29:44+00:00
- **Updated**: 2018-11-21 16:08:23+00:00
- **Authors**: Adam R. Kosiorek, Hyunjik Kim, Ingmar Posner, Yee Whye Teh
- **Comment**: 25 pages, 19 figures, NeurIPS 2018, code:
  https://github.com/akosiorek/sqair, video: https://youtu.be/-IUNQgSLE0c
- **Journal**: None
- **Summary**: We present Sequential Attend, Infer, Repeat (SQAIR), an interpretable deep generative model for videos of moving objects. It can reliably discover and track objects throughout the sequence of frames, and can also generate future frames conditioning on the current frame, thereby simulating expected motion of objects. This is achieved by explicitly encoding object presence, locations and appearances in the latent variables of the model. SQAIR retains all strengths of its predecessor, Attend, Infer, Repeat (AIR, Eslami et. al., 2016), including learning in an unsupervised manner, and addresses its shortcomings. We use a moving multi-MNIST dataset to show limitations of AIR in detecting overlapping or partially occluded objects, and show how SQAIR overcomes them by leveraging temporal consistency of objects. Finally, we also apply SQAIR to real-world pedestrian CCTV data, where it learns to reliably detect, track and generate walking pedestrians with no supervision.



### Videos as Space-Time Region Graphs
- **Arxiv ID**: http://arxiv.org/abs/1806.01810v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01810v2)
- **Published**: 2018-06-05 16:58:59+00:00
- **Updated**: 2018-12-21 23:56:25+00:00
- **Authors**: Xiaolong Wang, Abhinav Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: How do humans recognize the action "opening a book" ? We argue that there are two important cues: modeling temporal shape dynamics and modeling functional relationships between humans and objects. In this paper, we propose to represent videos as space-time region graphs which capture these two important cues. Our graph nodes are defined by the object region proposals from different frames in a long range video. These nodes are connected by two types of relations: (i) similarity relations capturing the long range dependencies between correlated objects and (ii) spatial-temporal relations capturing the interactions between nearby objects. We perform reasoning on this graph representation via Graph Convolutional Networks. We achieve state-of-the-art results on both Charades and Something-Something datasets. Especially for Charades, we obtain a huge 4.4% gain when our model is applied in complex environments.



### DPatch: An Adversarial Patch Attack on Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/1806.02299v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.02299v4)
- **Published**: 2018-06-05 17:04:37+00:00
- **Updated**: 2019-04-23 16:44:32+00:00
- **Authors**: Xin Liu, Huanrui Yang, Ziwei Liu, Linghao Song, Hai Li, Yiran Chen
- **Comment**: AAAI Workshop on Artificial Intelligence Safety (SafeAI 2019) Oral
  Presentation
- **Journal**: None
- **Summary**: Object detectors have emerged as an indispensable module in modern computer vision systems. In this work, we propose DPatch -- a black-box adversarial-patch-based attack towards mainstream object detectors (i.e. Faster R-CNN and YOLO). Unlike the original adversarial patch that only manipulates image-level classifier, our DPatch simultaneously attacks the bounding box regression and object classification so as to disable their predictions. Compared to prior works, DPatch has several appealing properties: (1) DPatch can perform both untargeted and targeted effective attacks, degrading the mAP of Faster R-CNN and YOLO from 75.10% and 65.7% down to below 1%, respectively. (2) DPatch is small in size and its attacking effect is location-independent, making it very practical to implement real-world attacks. (3) DPatch demonstrates great transferability among different detectors as well as training datasets. For example, DPatch that is trained on Faster R-CNN can effectively attack YOLO, and vice versa. Extensive evaluations imply that DPatch can perform effective attacks under black-box setup, i.e., even without the knowledge of the attacked network's architectures and parameters. Successful realization of DPatch also illustrates the intrinsic vulnerability of the modern detector architectures to such patch-based adversarial attacks.



### Perturbative Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.01817v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.01817v1)
- **Published**: 2018-06-05 17:15:08+00:00
- **Updated**: 2018-06-05 17:15:08+00:00
- **Authors**: Felix Juefei-Xu, Vishnu Naresh Boddeti, Marios Savvides
- **Comment**: To appear in CVPR 2018. http://xujuefei.com/pnn.html
- **Journal**: None
- **Summary**: Convolutional neural networks are witnessing wide adoption in computer vision systems with numerous applications across a range of visual recognition tasks. Much of this progress is fueled through advances in convolutional neural network architectures and learning algorithms even as the basic premise of a convolutional layer has remained unchanged. In this paper, we seek to revisit the convolutional layer that has been the workhorse of state-of-the-art visual recognition models. We introduce a very simple, yet effective, module called a perturbation layer as an alternative to a convolutional layer. The perturbation layer does away with convolution in the traditional sense and instead computes its response as a weighted linear combination of non-linearly activated additive noise perturbed inputs. We demonstrate both analytically and empirically that this perturbation layer can be an effective replacement for a standard convolutional layer. Empirically, deep neural networks with perturbation layers, called Perturbative Neural Networks (PNNs), in lieu of convolutional layers perform comparably with standard CNNs on a range of visual datasets (MNIST, CIFAR-10, PASCAL VOC, and ImageNet) with fewer parameters.



### Integrating Flexible Normalization into Mid-Level Representations of Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.01823v3
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.01823v3)
- **Published**: 2018-06-05 17:26:07+00:00
- **Updated**: 2018-12-24 05:29:25+00:00
- **Authors**: Luis Gonzalo Sanchez Giraldo, Odelia Schwartz
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) are becoming increasingly popular models to predict neural responses in visual cortex. However, contextual effects, which are prevalent in neural processing and in perception, are not explicitly handled by current CNNs, including those used for neural prediction. In primary visual cortex, neural responses are modulated by stimuli spatially surrounding the classical receptive field in rich ways. These effects have been modeled with divisive normalization approaches, including flexible models, where spatial normalization is recruited only to the degree responses from center and surround locations are deemed statistically dependent. We propose a flexible normalization model applied to mid-level representations of deep CNNs as a tractable way to study contextual normalization mechanisms in mid-level cortical areas. This approach captures non-trivial spatial dependencies among mid-level features in CNNs, such as those present in textures and other visual stimuli, that arise from tiling high order features, geometrically. We expect that the proposed approach can make predictions about when spatial normalization might be recruited in mid-level cortical areas. We also expect this approach to be useful as part of the CNN toolkit, therefore going beyond more restrictive fixed forms of normalization.



### Focal Visual-Text Attention for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1806.01873v2
- **DOI**: 10.1109/TPAMI.2018.2890628
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01873v2)
- **Published**: 2018-06-05 18:08:29+00:00
- **Updated**: 2019-05-31 22:55:43+00:00
- **Authors**: Junwei Liang, Lu Jiang, Liangliang Cao, Li-Jia Li, Alexander Hauptmann
- **Comment**: In CVPR 2018. Code, models and dataset are available here:
  https://memexqa.cs.cmu.edu/
- **Journal**: None
- **Summary**: Recent insights on language and vision with neural networks have been successfully applied to simple single-image visual question answering. However, to tackle real-life question answering problems on multimedia collections such as personal photos, we have to look at whole collections with sequences of photos or videos. When answering questions from a large collection, a natural problem is to identify snippets to support the answer. In this paper, we describe a novel neural network called Focal Visual-Text Attention network (FVTA) for collective reasoning in visual question answering, where both visual and text sequence information such as images and text metadata are presented. FVTA introduces an end-to-end approach that makes use of a hierarchical process to dynamically determine what media and what time to focus on in the sequential data to answer the question. FVTA can not only answer the questions well but also provides the justifications which the system results are based upon to get the answers. FVTA achieves state-of-the-art performance on the MemexQA dataset and competitive results on the MovieQA dataset.



### State Classification with CNN
- **Arxiv ID**: http://arxiv.org/abs/1806.03973v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.03973v2)
- **Published**: 2018-06-05 18:51:52+00:00
- **Updated**: 2018-06-25 19:48:17+00:00
- **Authors**: Astha Sharma
- **Comment**: None
- **Journal**: None
- **Summary**: There is a plenty of research going on in field of object recognition, but object state recognition has not been addressed as much. There are many important applications which can utilize object state recognition, such as, in robotics, to decide for how to grab an object. A convolution neural network was designed to classify an image to one of its states. The approach used for training is transfer learning with Inception v3 module of GoogLeNet used as the pre-trained model. The model was trained on images of 18 cooking objects and tested on another set of cooking objects. The model was able to classify those images with 76% accuracy.



### Performance Evaluation of Deep Learning Networks for Semantic Segmentation of Traffic Stereo-Pair Images
- **Arxiv ID**: http://arxiv.org/abs/1806.01896v1
- **DOI**: 10.1145/3274005.3274032
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.01896v1)
- **Published**: 2018-06-05 19:00:35+00:00
- **Updated**: 2018-06-05 19:00:35+00:00
- **Authors**: Vlad Taran, Nikita Gordienko, Yuriy Kochura, Yuri Gordienko, Alexandr Rokovyi, Oleg Alienin, Sergii Stirenko
- **Comment**: 8 pages, 10 figures; accepted for presentation at 19-th International
  Conference on Computer Systems and Technologies (CompSysTech'18) 13-14
  September 2018, University of Ruse, Bulgaria
- **Journal**: Proceedings of the 19th International Conference on Computer
  Systems and Technologies (CompSysTech'18), Boris Rachev and Angel Smrikarov
  (Eds.). ACM, New York, NY, USA, 73-80 (2018)
- **Summary**: Semantic image segmentation is one the most demanding task, especially for analysis of traffic conditions for self-driving cars. Here the results of application of several deep learning architectures (PSPNet and ICNet) for semantic image segmentation of traffic stereo-pair images are presented. The images from Cityscapes dataset and custom urban images were analyzed as to the segmentation accuracy and image inference time. For the models pre-trained on Cityscapes dataset, the inference time was equal in the limits of standard deviation, but the segmentation accuracy was different for various cities and stereo channels even. The distributions of accuracy (mean intersection over union - mIoU) values for each city and channel are asymmetric, long-tailed, and have many extreme outliers, especially for PSPNet network in comparison to ICNet network. Some statistical properties of these distributions (skewness, kurtosis) allow us to distinguish these two networks and open the question about relations between architecture of deep learning networks and statistical distribution of the predicted results (mIoU here). The results obtained demonstrated the different sensitivity of these networks to: (1) the local street view peculiarities in different cities that should be taken into account during the targeted fine tuning the models before their practical applications, (2) the right and left data channels in stereo-pairs. For both networks, the difference in the predicted results (mIoU here) for the right and left data channels in stereo-pairs is out of the limits of statistical error in relation to mIoU values. It means that the traffic stereo pairs can be effectively used not only for depth calculations (as it is usually used), but also as an additional data channel that can provide much more information about scene objects than simple duplication of the same street view images.



### Y-Net: A deep Convolutional Neural Network for Polyp Detection
- **Arxiv ID**: http://arxiv.org/abs/1806.01907v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01907v1)
- **Published**: 2018-06-05 19:33:18+00:00
- **Updated**: 2018-06-05 19:33:18+00:00
- **Authors**: Ahmed Mohammed, Sule Yildirim, Ivar Farup, Marius Pedersen, Øistein Hovde
- **Comment**: 11 Pages, 3 figures
- **Journal**: British Machine Vision Conference (BMVC), 2018
- **Summary**: Colorectal polyps are important precursors to colon cancer, the third most common cause of cancer mortality for both men and women. It is a disease where early detection is of crucial importance. Colonoscopy is commonly used for early detection of cancer and precancerous pathology. It is a demanding procedure requiring significant amount of time from specialized physicians and nurses, in addition to a significant miss-rates of polyps by specialists. Automated polyp detection in colonoscopy videos has been demonstrated to be a promising way to handle this problem. {However, polyps detection is a challenging problem due to the availability of limited amount of training data and large appearance variations of polyps. To handle this problem, we propose a novel deep learning method Y-Net that consists of two encoder networks with a decoder network. Our proposed Y-Net method} relies on efficient use of pre-trained and un-trained models with novel sum-skip-concatenation operations. Each of the encoders are trained with encoder specific learning rate along the decoder. Compared with the previous methods employing hand-crafted features or 2-D/3-D convolutional neural network, our approach outperforms state-of-the-art methods for polyp detection with 7.3% F1-score and 13% recall improvement.



### Adversarial Scene Editing: Automatic Object Removal from Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/1806.01911v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.01911v1)
- **Published**: 2018-06-05 19:45:20+00:00
- **Updated**: 2018-06-05 19:45:20+00:00
- **Authors**: Rakshith Shetty, Mario Fritz, Bernt Schiele
- **Comment**: None
- **Journal**: None
- **Summary**: While great progress has been made recently in automatic image manipulation, it has been limited to object centric images like faces or structured scene datasets. In this work, we take a step towards general scene-level image editing by developing an automatic interaction-free object removal model. Our model learns to find and remove objects from general scene images using image-level labels and unpaired data in a generative adversarial network (GAN) framework. We achieve this with two key contributions: a two-stage editor architecture consisting of a mask generator and image in-painter that co-operate to remove objects, and a novel GAN based prior for the mask generator that allows us to flexibly incorporate knowledge about object shapes. We experimentally show on two datasets that our method effectively removes a wide variety of objects using weak supervision only



### Exploring Feature Reuse in DenseNet Architectures
- **Arxiv ID**: http://arxiv.org/abs/1806.01935v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01935v1)
- **Published**: 2018-06-05 21:11:23+00:00
- **Updated**: 2018-06-05 21:11:23+00:00
- **Authors**: Andy Hess
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Densely Connected Convolutional Networks (DenseNets) have been shown to achieve state-of-the-art results on image classification tasks while using fewer parameters and computation than competing methods. Since each layer in this architecture has full access to the feature maps of all previous layers, the network is freed from the burden of having to relearn previously useful features, thus alleviating issues with vanishing gradients. In this work we explore the question: To what extent is it necessary to connect to all previous layers in order to reap the benefits of feature reuse? To this end, we introduce the notion of local dense connectivity and present evidence that less connectivity, allowing for increased growth rate at a fixed network capacity, can achieve a more efficient reuse of features and lead to higher accuracy in dense architectures.



### Mining for meaning: from vision to language through multiple networks consensus
- **Arxiv ID**: http://arxiv.org/abs/1806.01954v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01954v2)
- **Published**: 2018-06-05 22:50:09+00:00
- **Updated**: 2018-09-18 08:53:12+00:00
- **Authors**: Iulia Duta, Andrei Liviu Nicolicioiu, Simion-Vlad Bogolin, Marius Leordeanu
- **Comment**: Accepted at BMVC 2018
- **Journal**: British Machine Vision Conference 2018, {BMVC} 2018
- **Summary**: Describing visual data into natural language is a very challenging task, at the intersection of computer vision, natural language processing and machine learning. Language goes well beyond the description of physical objects and their interactions and can convey the same abstract idea in many ways. It is both about content at the highest semantic level as well as about fluent form. Here we propose an approach to describe videos in natural language by reaching a consensus among multiple encoder-decoder networks. Finding such a consensual linguistic description, which shares common properties with a larger group, has a better chance to convey the correct meaning. We propose and train several network architectures and use different types of image, audio and video features. Each model produces its own description of the input video and the best one is chosen through an efficient, two-phase consensus process. We demonstrate the strength of our approach by obtaining state of the art results on the challenging MSR-VTT dataset.



### MILD-Net: Minimal Information Loss Dilated Network for Gland Instance Segmentation in Colon Histology Images
- **Arxiv ID**: http://arxiv.org/abs/1806.01963v4
- **DOI**: 10.1016/j.media.2018.12.001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.01963v4)
- **Published**: 2018-06-05 23:38:01+00:00
- **Updated**: 2019-02-18 11:30:49+00:00
- **Authors**: Simon Graham, Hao Chen, Jevgenij Gamper, Qi Dou, Pheng-Ann Heng, David Snead, Yee Wah Tsang, Nasir Rajpoot
- **Comment**: Initial version published at Medical Imaging with Deep Learning
  (MIDL) 2018
- **Journal**: Medical Image Analysis vol. 52, pp. 199-211, Feb. 2019
- **Summary**: The analysis of glandular morphology within colon histopathology images is an important step in determining the grade of colon cancer. Despite the importance of this task, manual segmentation is laborious, time-consuming and can suffer from subjectivity among pathologists. The rise of computational pathology has led to the development of automated methods for gland segmentation that aim to overcome the challenges of manual segmentation. However, this task is non-trivial due to the large variability in glandular appearance and the difficulty in differentiating between certain glandular and non-glandular histological structures. Furthermore, a measure of uncertainty is essential for diagnostic decision making. To address these challenges, we propose a fully convolutional neural network that counters the loss of information caused by max-pooling by re-introducing the original image at multiple points within the network. We also use atrous spatial pyramid pooling with varying dilation rates for preserving the resolution and multi-level aggregation. To incorporate uncertainty, we introduce random transformations during test time for an enhanced segmentation result that simultaneously generates an uncertainty map, highlighting areas of ambiguity. We show that this map can be used to define a metric for disregarding predictions with high uncertainty. The proposed network achieves state-of-the-art performance on the GlaS challenge dataset and on a second independent colorectal adenocarcinoma dataset. In addition, we perform gland instance segmentation on whole-slide images from two further datasets to highlight the generalisability of our method. As an extension, we introduce MILD-Net+ for simultaneous gland and lumen segmentation, to increase the diagnostic power of the network.



