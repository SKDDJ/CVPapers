# Arxiv Papers in cs.CV on 2018-06-11
### Robust Object Tracking with Crow Search Optimized Multi-cue Particle Filter
- **Arxiv ID**: http://arxiv.org/abs/1806.03753v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03753v1)
- **Published**: 2018-06-11 00:37:55+00:00
- **Updated**: 2018-06-11 00:37:55+00:00
- **Authors**: Kapil Sharma, Gurjit Singh Walia, Ashish Kumar, Astitwa Saxena, Kuldeep Singh
- **Comment**: 25 pages, 8 figures, 3 tables
- **Journal**: None
- **Summary**: Particle Filter(PF) is used extensively for estimation of target Non-linear and Non-gaussian state. However, its performance suffers due to inherent problem of sample degeneracy and impoverishment. In order to address this, we propose a novel resampling method based upon Crow Search Optimization to overcome low performing particles detected as outlier. Proposed outlier detection mechanism with transductive reliability achieve faster convergence of proposed PF tracking framework. In addition, we present an adaptive fuzzy fusion model to integrate multi-cue extracted for each evaluated particle. Automatic boosting and suppression of particles using proposed fusion model not only enhances performance of resampling method but also achieve optimal state estimation. Performance of the proposed tracker is evaluated over 12 benchmark video sequences and compared with state-of-the-art solutions. Qualitative and quantitative results reveals that the proposed tracker not only outperforms existing solutions but also efficiently handle various tracking challenges. On average of outcome, we achieve CLE of 7.98 and F-measure of 0.734.



### DOOBNet: Deep Object Occlusion Boundary Detection from an Image
- **Arxiv ID**: http://arxiv.org/abs/1806.03772v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03772v3)
- **Published**: 2018-06-11 02:24:31+00:00
- **Updated**: 2018-09-13 14:18:34+00:00
- **Authors**: Guoxia Wang, Xiaohui Liang, Frederick W. B. Li
- **Comment**: None
- **Journal**: None
- **Summary**: Object occlusion boundary detection is a fundamental and crucial research problem in computer vision. This is challenging to solve as encountering the extreme boundary/non-boundary class imbalance during training an object occlusion boundary detector. In this paper, we propose to address this class imbalance by up-weighting the loss contribution of false negative and false positive examples with our novel Attention Loss function. We also propose a unified end-to-end multi-task deep object occlusion boundary detection network (DOOBNet) by sharing convolutional features to simultaneously predict object boundary and occlusion orientation. DOOBNet adopts an encoder-decoder structure with skip connection in order to automatically learn multi-scale and multi-level features. We significantly surpass the state-of-the-art on the PIOD dataset (ODS F-score of .702) and the BSDS ownership dataset (ODS F-score of .555), as well as improving the detecting speed to as 0.037s per image on the PIOD dataset.



### Cross-dataset Person Re-Identification Using Similarity Preserved Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.04533v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04533v2)
- **Published**: 2018-06-11 03:40:06+00:00
- **Updated**: 2018-06-21 02:20:59+00:00
- **Authors**: Jianming Lv, Xintong Wang
- **Comment**: Accepted by KSEM 2018. arXiv admin note: text overlap with
  arXiv:1803.07293
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) aims to match the image frames which contain the same person in the surveillance videos. Most of the Re-ID algorithms conduct supervised training in some small labeled datasets, so directly deploying these trained models to the real-world large camera networks may lead to a poor performance due to underfitting. The significant difference between the source training dataset and the target testing dataset makes it challenging to incrementally optimize the model. To address this challenge, we propose a novel solution by transforming the unlabeled images in the target domain to fit the original classifier by using our proposed similarity preserved generative adversarial networks model, SimPGAN. Specifically, SimPGAN adopts the generative adversarial networks with the cycle consistency constraint to transform the unlabeled images in the target domain to the style of the source domain. Meanwhile, SimPGAN uses the similarity consistency loss, which is measured by a siamese deep convolutional neural network, to preserve the similarity of the transformed images of the same person. Comprehensive experiments based on multiple real surveillance datasets are conducted, and the results show that our algorithm is better than the state-of-the-art cross-dataset unsupervised person Re-ID algorithms.



### Generative Adversarial Network Architectures For Image Synthesis Using Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.03796v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.03796v4)
- **Published**: 2018-06-11 03:54:24+00:00
- **Updated**: 2018-11-20 18:33:11+00:00
- **Authors**: Yash Upadhyay, Paul Schrater
- **Comment**: Pre-print
- **Journal**: None
- **Summary**: In this paper, we propose Generative Adversarial Network (GAN) architectures that use Capsule Networks for image-synthesis. Based on the principal of positional-equivariance of features, Capsule Network's ability to encode spatial relationships between the features of the image helps it become a more powerful critic in comparison to Convolutional Neural Networks (CNNs) used in current architectures for image synthesis. Our proposed GAN architectures learn the data manifold much faster and therefore, synthesize visually accurate images in significantly lesser number of training samples and training epochs in comparison to GANs and its variants that use CNNs. Apart from analyzing the quantitative results corresponding the images generated by different architectures, we also explore the reasons for the lower coverage and diversity explored by the GAN architectures that use CNN critics.



### Auto-Meta: Automated Gradient Based Meta Learner Search
- **Arxiv ID**: http://arxiv.org/abs/1806.06927v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.06927v2)
- **Published**: 2018-06-11 04:28:02+00:00
- **Updated**: 2018-12-10 19:02:53+00:00
- **Authors**: Jaehong Kim, Sangyeul Lee, Sungwan Kim, Moonsu Cha, Jung Kwon Lee, Youngduck Choi, Yongseok Choi, Dong-Yeon Cho, Jiwon Kim
- **Comment**: Presented at NIPS 2018 Workshop on Meta-Learning (MetaLearn 2018)
- **Journal**: None
- **Summary**: Fully automating machine learning pipelines is one of the key challenges of current artificial intelligence research, since practical machine learning often requires costly and time-consuming human-powered processes such as model design, algorithm development, and hyperparameter tuning. In this paper, we verify that automated architecture search synergizes with the effect of gradient-based meta learning. We adopt the progressive neural architecture search \cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimal architectures for meta-learners. The gradient based meta-learner whose architecture was automatically found achieved state-of-the-art results on the 5-shot 5-way Mini-ImageNet classification problem with $74.65\%$ accuracy, which is $11.54\%$ improvement over the result obtained by the first gradient-based meta-learner called MAML \cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work is the first successful neural architecture search implementation in the context of meta learning.



### DFNet: Semantic Segmentation on Panoramic Images with Dynamic Loss Weights and Residual Fusion Block
- **Arxiv ID**: http://arxiv.org/abs/1806.07226v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07226v1)
- **Published**: 2018-06-11 05:09:25+00:00
- **Updated**: 2018-06-11 05:09:25+00:00
- **Authors**: Wei Jiang, Yan Wu
- **Comment**: 6 pages,3 figures
- **Journal**: None
- **Summary**: For the self-driving and automatic parking, perception is the basic and critical technique, moreover, the detection of lane markings and parking slots is an important part of visual perception. In this paper, we use the semantic segmentation method to segment the area and classify the class of lane makings and parking slots on panoramic surround view (PSV) dataset. We propose the DFNet and make two main contributions, one is dynamic loss weights, and the other is residual fusion block (RFB). Dynamic loss weights are varying from classes, calculated according to the pixel number of each class in a batch. RFB is composed of two convolutional layers, one pooling layer, and a fusion layer to combine the feature maps by pixel multiplication. We evaluate our method on PSV dataset, and achieve an advanced result.



### Compression of phase-only holograms with JPEG standard and deep learning
- **Arxiv ID**: http://arxiv.org/abs/1806.03811v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.03811v1)
- **Published**: 2018-06-11 05:11:58+00:00
- **Updated**: 2018-06-11 05:11:58+00:00
- **Authors**: Shuming Jiao, Zhi Jin, Chenliang Chang, Changyuan Zhou, Wenbin Zou, Xia Li
- **Comment**: None
- **Journal**: None
- **Summary**: It is a critical issue to reduce the enormous amount of data in the processing, storage and transmission of a hologram in digital format. In photograph compression, the JPEG standard is commonly supported by almost every system and device. It will be favorable if JPEG standard is applicable to hologram compression, with advantages of universal compatibility. However, the reconstructed image from a JPEG compressed hologram suffers from severe quality degradation since some high frequency features in the hologram will be lost during the compression process. In this work, we employ a deep convolutional neural network to reduce the artifacts in a JPEG compressed hologram. Simulation and experimental results reveal that our proposed "JPEG + deep learning" hologram compression scheme can achieve satisfactory reconstruction results for a computer-generated phase-only hologram after compression.



### An optimized system to solve text-based CAPTCHA
- **Arxiv ID**: http://arxiv.org/abs/1806.07202v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07202v1)
- **Published**: 2018-06-11 06:52:19+00:00
- **Updated**: 2018-06-11 06:52:19+00:00
- **Authors**: Ye Wang, Mi Lu
- **Comment**: None
- **Journal**: None
- **Summary**: CAPTCHA(Completely Automated Public Turing test to Tell Computers and Humans Apart) can be used to protect data from auto bots. Countless kinds of CAPTCHAs are thus designed, while we most frequently utilize text-based scheme because of most convenience and user-friendly way \cite{bursztein2011text}. Currently, various types of CAPTCHAs need corresponding segmentation to identify single character due to the numerous different segmentation ways. Our goal is to defeat the CAPTCHA, thus firstly the CAPTCHAs need to be split into character by character. There isn't a regular segmentation algorithm to obtain the divided characters in all kinds of examples, which means that we have to treat the segmentation individually. In this paper, we build a whole system to defeat the CAPTCHAs as well as achieve state-of-the-art performance. In detail, we present our self-adaptive algorithm to segment different kinds of characters optimally, and then utilize both the existing methods and our own constructed convolutional neural network as an extra classifier. Results are provided showing how our system work well towards defeating these CAPTCHAs.



### Interactive Visual Grounding of Referring Expressions for Human-Robot Interaction
- **Arxiv ID**: http://arxiv.org/abs/1806.03831v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.03831v1)
- **Published**: 2018-06-11 06:58:19+00:00
- **Updated**: 2018-06-11 06:58:19+00:00
- **Authors**: Mohit Shridhar, David Hsu
- **Comment**: In Robotics: Science & Systems (RSS) 2018
- **Journal**: None
- **Summary**: This paper presents INGRESS, a robot system that follows human natural language instructions to pick and place everyday objects. The core issue here is the grounding of referring expressions: infer objects and their relationships from input images and language expressions. INGRESS allows for unconstrained object categories and unconstrained language expressions. Further, it asks questions to disambiguate referring expressions interactively. To achieve these, we take the approach of grounding by generation and propose a two-stage neural network model for grounding. The first stage uses a neural network to generate visual descriptions of objects, compares them with the input language expression, and identifies a set of candidate objects. The second stage uses another neural network to examine all pairwise relations between the candidates and infers the most likely referred object. The same neural networks are used for both grounding and question generation for disambiguation. Experiments show that INGRESS outperformed a state-of-the-art method on the RefCOCO dataset and in robot experiments with humans.



### Synthetic Perfusion Maps: Imaging Perfusion Deficits in DSC-MRI with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.03848v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03848v1)
- **Published**: 2018-06-11 07:52:36+00:00
- **Updated**: 2018-06-11 07:52:36+00:00
- **Authors**: Andreas Hess, Raphael Meier, Johannes Kaesmacher, Simon Jung, Fabien Scalzo, David Liebeskind, Roland Wiest, Richard McKinley
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a novel convolutional neural net- work based method for perfusion map generation in dynamic suscepti- bility contrast-enhanced perfusion imaging. The proposed architecture is trained end-to-end and solely relies on raw perfusion data for inference. We used a dataset of 151 acute ischemic stroke cases for evaluation. Our method generates perfusion maps that are comparable to the target maps used for clinical routine, while being model-free, fast, and less noisy.



### Data augmentation instead of explicit regularization
- **Arxiv ID**: http://arxiv.org/abs/1806.03852v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03852v5)
- **Published**: 2018-06-11 08:10:18+00:00
- **Updated**: 2020-11-12 10:35:59+00:00
- **Authors**: Alex Hernández-García, Peter König
- **Comment**: Major changes: 1. updated figures; 2. statistical significance
  analysis of results through bootstrap; 3. information about carbon emissions
  produced with the experiments; 4. extended discussion about the need for
  weight decay and dropout; 5. literature review updated; 6. slight update of
  the definitions of explicit and implicit regularization, with several
  examples for illustration
- **Journal**: None
- **Summary**: Contrary to most machine learning models, modern deep artificial neural networks typically include multiple components that contribute to regularization. Despite the fact that some (explicit) regularization techniques, such as weight decay and dropout, require costly fine-tuning of sensitive hyperparameters, the interplay between them and other elements that provide implicit regularization is not well understood yet. Shedding light upon these interactions is key to efficiently using computational resources and may contribute to solving the puzzle of generalization in deep learning. Here, we first provide formal definitions of explicit and implicit regularization that help understand essential differences between techniques. Second, we contrast data augmentation with weight decay and dropout. Our results show that visual object categorization models trained with data augmentation alone achieve the same performance or higher than models trained also with weight decay and dropout, as is common practice. We conclude that the contribution on generalization of weight decay and dropout is not only superfluous when sufficient implicit regularization is provided, but also such techniques can dramatically deteriorate the performance if the hyperparameters are not carefully tuned for the architecture and data set. In contrast, data augmentation systematically provides large generalization gains and does not require hyperparameter re-tuning. In view of our results, we suggest to optimize neural networks without weight decay and dropout to save computational resources, hence carbon emissions, and focus more on data augmentation and other inductive biases to improve performance and robustness.



### Object detection and tracking benchmark in industry based on improved correlation filter
- **Arxiv ID**: http://arxiv.org/abs/1806.03853v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03853v2)
- **Published**: 2018-06-11 08:15:30+00:00
- **Updated**: 2018-06-12 01:29:05+00:00
- **Authors**: Shangzhen Luan, Yan Li, Xiaodi Wang, Baochang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Real-time object detection and tracking have shown to be the basis of intelligent production for industrial 4.0 applications. It is a challenging task because of various distorted data in complex industrial setting. The correlation filter (CF) has been used to trade off the low-cost computation and high performance. However, traditional CF training strategy can not get satisfied performance for the various industrial data; because the simple sampling(bagging) during training process will not find the exact solutions in a data space with a large diversity. In this paper, we propose Dijkstra-distance based correlation filters (DBCF), which establishes a new learning framework that embeds distribution-related constraints into the multi-channel correlation filters (MCCF). DBCF is able to handle the huge variations existing in the industrial data by improving those constraints based on the shortest path among all solutions. To evaluate DBCF, we build a new dataset as the benchmark for industrial 4.0 application. Extensive experiments demonstrate that DBCF produces high performance and exceeds the state-of-the-art methods. The dataset and source code can be found at https://github.com/bczhangbczhang



### Massively Parallel Video Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.03863v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03863v2)
- **Published**: 2018-06-11 08:46:51+00:00
- **Updated**: 2018-09-05 17:52:49+00:00
- **Authors**: Joao Carreira, Viorica Patraucean, Laurent Mazare, Andrew Zisserman, Simon Osindero
- **Comment**: Fixed typos in densenet model definition in appendix
- **Journal**: None
- **Summary**: We introduce a class of causal video understanding models that aims to improve efficiency of video processing by maximising throughput, minimising latency, and reducing the number of clock cycles. Leveraging operation pipelining and multi-rate clocks, these models perform a minimal amount of computation (e.g. as few as four convolutional layers) for each frame per timestep to produce an output. The models are still very deep, with dozens of such operations being performed but in a pipelined fashion that enables depth-parallel computation. We illustrate the proposed principles by applying them to existing image architectures and analyse their behaviour on two video tasks: action recognition and human keypoint localisation. The results show that a significant degree of parallelism, and implicitly speedup, can be achieved with little loss in performance.



### Multi-Task Deep Networks for Depth-Based 6D Object Pose and Joint Registration in Crowd Scenarios
- **Arxiv ID**: http://arxiv.org/abs/1806.03891v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03891v1)
- **Published**: 2018-06-11 10:05:42+00:00
- **Updated**: 2018-06-11 10:05:42+00:00
- **Authors**: Juil Sock, Kwang In Kim, Caner Sahin, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: In bin-picking scenarios, multiple instances of an object of interest are stacked in a pile randomly, and hence, the instances are inherently subjected to the challenges: severe occlusion, clutter, and similar-looking distractors. Most existing methods are, however, for single isolated object instances, while some recent methods tackle crowd scenarios as post-refinement which accounts multiple object relations. In this paper, we address recovering 6D poses of multiple instances in bin-picking scenarios in depth modality by multi-task learning in deep neural networks. Our architecture jointly learns multiple sub-tasks: 2D detection, depth, and 3D pose estimation of individual objects; and joint registration of multiple objects. For training data generation, depth images of physically plausible object pose configurations are generated by a 3D object model in a physics simulation, which yields diverse occlusion patterns to learn. We adopt a state-of-the-art object detector, and 2D offsets are further estimated via a network to refine misaligned 2D detections. The depth and 3D pose estimator is designed to generate multiple hypotheses per detection. This allows the joint registration network to learn occlusion patterns and remove physically implausible pose hypotheses. We apply our architecture on both synthetic (our own and Sileane dataset) and real (a public Bin-Picking dataset) data, showing that it significantly outperforms state-of-the-art methods by 15-31% in average precision.



### Dual Pattern Learning Networks by Empirical Dual Prediction Risk Minimization
- **Arxiv ID**: http://arxiv.org/abs/1806.03902v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03902v1)
- **Published**: 2018-06-11 11:00:58+00:00
- **Updated**: 2018-06-11 11:00:58+00:00
- **Authors**: Haimin Zhang, Min Xu
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: Motivated by the observation that humans can learn patterns from two given images at one time, we propose a dual pattern learning network architecture in this paper. Unlike conventional networks, the proposed architecture has two input branches and two loss functions. Instead of minimizing the empirical risk of a given dataset, dual pattern learning networks is trained by minimizing the empirical dual prediction loss. We show that this can improve the performance for single image classification. This architecture forces the network to learn discriminative class-specific features by analyzing and comparing two input images. In addition, the dual input structure allows the network to have a considerably large number of image pairs, which can help address the overfitting issue due to limited training data. Moreover, we propose to associate each input branch with a random interest value for learning corresponding image during training. This method can be seen as a stochastic regularization technique, and can further lead to generalization performance improvement. State-of-the-art deep networks can be adapted to dual pattern learning networks without increasing the same number of parameters. Extensive experiments on CIFAR-10, CIFAR- 100, FI-8, Google commands dataset, and MNIST demonstrate that our DPLNets exhibit better performance than original networks. The experimental results on subsets of CIFAR- 10, CIFAR-100, and MNIST demonstrate that dual pattern learning networks have good generalization performance on small datasets.



### Retinal Optic Disc Segmentation using Conditional Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1806.03905v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03905v1)
- **Published**: 2018-06-11 11:04:05+00:00
- **Updated**: 2018-06-11 11:04:05+00:00
- **Authors**: Vivek Kumar Singh, Hatem Rashwan, Farhan Akram, Nidhi Pandey, Md. Mostaf Kamal Sarker, Adel Saleh, Saddam Abdulwahab, Najlaa Maaroof, Santiago Romani, Domenec Puig
- **Comment**: 8 pages, Submitted to 21st International Conference of the Catalan
  Association for Artificial Intelligence (CCIA 2018)
- **Journal**: None
- **Summary**: This paper proposed a retinal image segmentation method based on conditional Generative Adversarial Network (cGAN) to segment optic disc. The proposed model consists of two successive networks: generator and discriminator. The generator learns to map information from the observing input (i.e., retinal fundus color image), to the output (i.e., binary mask). Then, the discriminator learns as a loss function to train this mapping by comparing the ground-truth and the predicted output with observing the input image as a condition.Experiments were performed on two publicly available dataset; DRISHTI GS1 and RIM-ONE. The proposed model outperformed state-of-the-art-methods by achieving around 0.96% and 0.98% of Jaccard and Dice coefficients, respectively. Moreover, an image segmentation is performed in less than a second on recent GPU.



### Multilingual Scene Character Recognition System using Sparse Auto-Encoder for Efficient Local Features Representation in Bag of Features
- **Arxiv ID**: http://arxiv.org/abs/1806.07374v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.07374v4)
- **Published**: 2018-06-11 11:21:42+00:00
- **Updated**: 2018-07-19 17:24:39+00:00
- **Authors**: Maroua Tounsi, Ikram Moalla, Frank Lebourgeois, Adel M. Alimi
- **Comment**: This paper is under consideration at Pattern Recognition Letters
- **Journal**: None
- **Summary**: The recognition of texts existing in camera-captured images has become an important issue for a great deal of research during the past few decades. This give birth to Scene Character Recognition (SCR) which is an important step in scene text recognition pipeline. In this paper, we extended the Bag of Features (BoF)-based model using deep learning for representing features for accurate SCR of different languages. In the features coding step, a deep Sparse Auto-encoder (SAE)-based strategy was applied to enhance the representative and discriminative abilities of image features. This deep learning architecture provides more efficient features representation and therefore a better recognition accuracy. Our system was evaluated extensively on all the scene character datasets of five different languages. The experimental results proved the efficiency of our system for a multilingual SCR.



### Learning to Estimate Indoor Lighting from 3D Objects
- **Arxiv ID**: http://arxiv.org/abs/1806.03994v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03994v3)
- **Published**: 2018-06-11 14:01:31+00:00
- **Updated**: 2018-08-13 15:02:31+00:00
- **Authors**: Henrique Weber, Donald Prévost, Jean-François Lalonde
- **Comment**: 3DV 2018 - International Conference on 3D Vision
- **Journal**: None
- **Summary**: In this work, we propose a step towards a more accurate prediction of the environment light given a single picture of a known object. To achieve this, we developed a deep learning method that is able to encode the latent space of indoor lighting using few parameters and that is trained on a database of environment maps. This latent space is then used to generate predictions of the light that are both more realistic and accurate than previous methods. To achieve this, our first contribution is a deep autoencoder which is capable of learning the feature space that compactly models lighting. Our second contribution is a convolutional neural network that predicts the light from a single image of a known object. To train these networks, our third contribution is a novel dataset that contains 21,000 HDR indoor environment maps. The results indicate that the predictor can generate plausible lighting estimations even from diffuse objects.



### CT-Realistic Lung Nodule Simulation from 3D Conditional Generative Adversarial Networks for Robust Lung Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.04051v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04051v1)
- **Published**: 2018-06-11 15:19:36+00:00
- **Updated**: 2018-06-11 15:19:36+00:00
- **Authors**: Dakai Jin, Ziyue Xu, Youbao Tang, Adam P. Harrison, Daniel J. Mollura
- **Comment**: MICCAI 2018
- **Journal**: None
- **Summary**: Data availability plays a critical role for the performance of deep learning systems. This challenge is especially acute within the medical image domain, particularly when pathologies are involved, due to two factors: 1) limited number of cases, and 2) large variations in location, scale, and appearance. In this work, we investigate whether augmenting a dataset with artificially generated lung nodules can improve the robustness of the progressive holistically nested network (P-HNN) model for pathological lung segmentation of CT scans. To achieve this goal, we develop a 3D generative adversarial network (GAN) that effectively learns lung nodule property distributions in 3D space. In order to embed the nodules within their background context, we condition the GAN based on a volume of interest whose central part containing the nodule has been erased. To further improve realism and blending with the background, we propose a novel multi-mask reconstruction loss. We train our method on over 1000 nodules from the LIDC dataset. Qualitative results demonstrate the effectiveness of our method compared to the state-of-art. We then use our GAN to generate simulated training images where nodules lie on the lung border, which are cases where the published P-HNN model struggles. Qualitative and quantitative results demonstrate that armed with these simulated images, the P-HNN model learns to better segment lung regions under these challenging situations. As a result, our system provides a promising means to help overcome the data paucity that commonly afflicts medical imaging.



### Joint Learning of Motion Estimation and Segmentation for Cardiac MR Image Sequences
- **Arxiv ID**: http://arxiv.org/abs/1806.04066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04066v1)
- **Published**: 2018-06-11 15:45:47+00:00
- **Updated**: 2018-06-11 15:45:47+00:00
- **Authors**: Chen Qin, Wenjia Bai, Jo Schlemper, Steffen E. Petersen, Stefan K. Piechnik, Stefan Neubauer, Daniel Rueckert
- **Comment**: accepted by MICCAI 2018
- **Journal**: None
- **Summary**: Cardiac motion estimation and segmentation play important roles in quantitatively assessing cardiac function and diagnosing cardiovascular diseases. In this paper, we propose a novel deep learning method for joint estimation of motion and segmentation from cardiac MR image sequences. The proposed network consists of two branches: a cardiac motion estimation branch which is built on a novel unsupervised Siamese style recurrent spatial transformer network, and a cardiac segmentation branch that is based on a fully convolutional network. In particular, a joint multi-scale feature encoder is learned by optimizing the segmentation branch and the motion estimation branch simultaneously. This enables the weakly-supervised segmentation by taking advantage of features that are unsupervisedly learned in the motion estimation branch from a large amount of unannotated data. Experimental results using cardiac MRI images from 220 subjects show that the joint learning of both tasks is complementary and the proposed models outperform the competing methods significantly in terms of accuracy and speed.



### The Research of the Real-time Detection and Recognition of Targets in Streetscape Videos
- **Arxiv ID**: http://arxiv.org/abs/1806.04070v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.04070v1)
- **Published**: 2018-06-11 15:50:41+00:00
- **Updated**: 2018-06-11 15:50:41+00:00
- **Authors**: Liu Jian-min
- **Comment**: None
- **Journal**: None
- **Summary**: This study proposes a method for the real-time detection and recognition of targets in streetscape videos. The proposed method is based on separation confidence computation and scale synthesis optimization. We use the proposed method to detect and recognize targets in streetscape videos with high frame rates and high definition. Furthermore, we experimentally demonstrate that the accuracy and robustness of our proposed method are superior to those of conventional methods.



### Semantically Selective Augmentation for Deep Compact Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1806.04074v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04074v3)
- **Published**: 2018-06-11 15:58:10+00:00
- **Updated**: 2018-06-18 14:27:43+00:00
- **Authors**: Víctor Ponce-López, Tilo Burghardt, Sion Hannunna, Dima Damen, Alessandro Masullo, Majid Mirmehdi
- **Comment**: None
- **Journal**: None
- **Summary**: We present a deep person re-identification approach that combines semantically selective, deep data augmentation with clustering-based network compression to generate high performance, light and fast inference networks. In particular, we propose to augment limited training data via sampling from a deep convolutional generative adversarial network (DCGAN), whose discriminator is constrained by a semantic classifier to explicitly control the domain specificity of the generation process. Thereby, we encode information in the classifier network which can be utilized to steer adversarial synthesis, and which fuels our CondenseNet ID-network training. We provide a quantitative and qualitative analysis of the approach and its variants on a number of datasets, obtaining results that outperform the state-of-the-art on the LIMA dataset for long-term monitoring in indoor living spaces.



### Learning to Decompose and Disentangle Representations for Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/1806.04166v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.04166v2)
- **Published**: 2018-06-11 18:12:59+00:00
- **Updated**: 2018-10-17 18:44:19+00:00
- **Authors**: Jun-Ting Hsieh, Bingbin Liu, De-An Huang, Li Fei-Fei, Juan Carlos Niebles
- **Comment**: None
- **Journal**: None
- **Summary**: Our goal is to predict future video frames given a sequence of input frames. Despite large amounts of video data, this remains a challenging task because of the high-dimensionality of video frames. We address this challenge by proposing the Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a framework that combines structured probabilistic models and deep networks to automatically (i) decompose the high-dimensional video that we aim to predict into components, and (ii) disentangle each component to have low-dimensional temporal dynamics that are easier to predict. Crucially, with an appropriately specified generative model of video frames, our DDPAE is able to learn both the latent decomposition and disentanglement without explicit supervision. For the Moving MNIST dataset, we show that DDPAE is able to recover the underlying components (individual digits) and disentanglement (appearance and location) as we would intuitively do. We further demonstrate that DDPAE can be applied to the Bouncing Balls dataset involving complex interactions between multiple objects to predict the video frame directly from the pixels and recover physical states without explicit supervision.



### Synthetic Depth-of-Field with a Single-Camera Mobile Phone
- **Arxiv ID**: http://arxiv.org/abs/1806.04171v1
- **DOI**: 10.1145/3197517.3201329
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1806.04171v1)
- **Published**: 2018-06-11 18:29:12+00:00
- **Updated**: 2018-06-11 18:29:12+00:00
- **Authors**: Neal Wadhwa, Rahul Garg, David E. Jacobs, Bryan E. Feldman, Nori Kanazawa, Robert Carroll, Yair Movshovitz-Attias, Jonathan T. Barron, Yael Pritch, Marc Levoy
- **Comment**: Accepted to SIGGRAPH 2018. Basis for Portrait Mode on Google Pixel 2
  and Pixel 2 XL
- **Journal**: None
- **Summary**: Shallow depth-of-field is commonly used by photographers to isolate a subject from a distracting background. However, standard cell phone cameras cannot produce such images optically, as their short focal lengths and small apertures capture nearly all-in-focus images. We present a system to computationally synthesize shallow depth-of-field images with a single mobile camera and a single button press. If the image is of a person, we use a person segmentation network to separate the person and their accessories from the background. If available, we also use dense dual-pixel auto-focus hardware, effectively a 2-sample light field with an approximately 1 millimeter baseline, to compute a dense depth map. These two signals are combined and used to render a defocused image. Our system can process a 5.4 megapixel image in 4 seconds on a mobile phone, is fully automatic, and is robust enough to be used by non-experts. The modular nature of our system allows it to degrade naturally in the absence of a dual-pixel sensor or a human subject.



### 3D Convolutional Neural Networks for Classification of Functional Connectomes
- **Arxiv ID**: http://arxiv.org/abs/1806.04209v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.04209v2)
- **Published**: 2018-06-11 19:30:20+00:00
- **Updated**: 2018-06-13 17:15:27+00:00
- **Authors**: Meenakshi Khosla, Keith Jamison, Amy Kuceyeski, Mert Sabuncu
- **Comment**: 9 pages, 3 figures
- **Journal**: None
- **Summary**: Resting-state functional MRI (rs-fMRI) scans hold the potential to serve as a diagnostic or prognostic tool for a wide variety of conditions, such as autism, Alzheimer's disease, and stroke. While a growing number of studies have demonstrated the promise of machine learning algorithms for rs-fMRI based clinical or behavioral prediction, most prior models have been limited in their capacity to exploit the richness of the data. For example, classification techniques applied to rs-fMRI often rely on region-based summary statistics and/or linear models. In this work, we propose a novel volumetric Convolutional Neural Network (CNN) framework that takes advantage of the full-resolution 3D spatial structure of rs-fMRI data and fits non-linear predictive models. We showcase our approach on a challenging large-scale dataset (ABIDE, with N > 2,000) and report state-of-the-art accuracy results on rs-fMRI-based discrimination of autism patients and healthy controls.



### NeuroNet: Fast and Robust Reproduction of Multiple Brain Image Segmentation Pipelines
- **Arxiv ID**: http://arxiv.org/abs/1806.04224v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.04224v1)
- **Published**: 2018-06-11 20:21:23+00:00
- **Updated**: 2018-06-11 20:21:23+00:00
- **Authors**: Martin Rajchl, Nick Pawlowski, Daniel Rueckert, Paul M. Matthews, Ben Glocker
- **Comment**: International conference on Medical Imaging with Deep Learning (MIDL)
  2018
- **Journal**: None
- **Summary**: NeuroNet is a deep convolutional neural network mimicking multiple popular and state-of-the-art brain segmentation tools including FSL, SPM, and MALPEM. The network is trained on 5,000 T1-weighted brain MRI scans from the UK Biobank Imaging Study that have been automatically segmented into brain tissue and cortical and sub-cortical structures using the standard neuroimaging pipelines. Training a single model from these complementary and partially overlapping label maps yields a new powerful "all-in-one", multi-output segmentation tool. The processing time for a single subject is reduced by an order of magnitude compared to running each individual software package. We demonstrate very good reproducibility of the original outputs while increasing robustness to variations in the input data. We believe NeuroNet could be an important tool in large-scale population imaging studies and serve as a new standard in neuroscience by reducing the risk of introducing bias when choosing a specific software package.



### Physical Representation-based Predicate Optimization for a Visual Analytics Database
- **Arxiv ID**: http://arxiv.org/abs/1806.04226v3
- **DOI**: 10.1109/ICDE.2019.00132
- **Categories**: **cs.DB**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.04226v3)
- **Published**: 2018-06-11 20:28:12+00:00
- **Updated**: 2019-02-27 18:08:09+00:00
- **Authors**: Michael R. Anderson, Michael Cafarella, German Ros, Thomas F. Wenisch
- **Comment**: Camera-ready version of the paper submitted to ICDE 2019, In
  Proceedings of the 35th IEEE International Conference on Data Engineering
  (ICDE 2019)
- **Journal**: Proceedings of the 35th IEEE International Conference on Data
  Engineering (ICDE 2019), 1466-1477
- **Summary**: Querying the content of images, video, and other non-textual data sources requires expensive content extraction methods. Modern extraction techniques are based on deep convolutional neural networks (CNNs) and can classify objects within images with astounding accuracy. Unfortunately, these methods are slow: processing a single image can take about 10 milliseconds on modern GPU-based hardware. As massive video libraries become ubiquitous, running a content-based query over millions of video frames is prohibitive.   One promising approach to reduce the runtime cost of queries of visual content is to use a hierarchical model, such as a cascade, where simple cases are handled by an inexpensive classifier. Prior work has sought to design cascades that optimize the computational cost of inference by, for example, using smaller CNNs. However, we observe that there are critical factors besides the inference time that dramatically impact the overall query time. Notably, by treating the physical representation of the input image as part of our query optimization---that is, by including image transforms, such as resolution scaling or color-depth reduction, within the cascade---we can optimize data handling costs and enable drastically more efficient classifier cascades.   In this paper, we propose Tahoma, which generates and evaluates many potential classifier cascades that jointly optimize the CNN architecture and input data representation. Our experiments on a subset of ImageNet show that Tahoma's input transformations speed up cascades by up to 35 times. We also find up to a 98x speedup over the ResNet50 classifier with no loss in accuracy, and a 280x speedup if some accuracy is sacrificed.



### Improving Whole Slide Segmentation Through Visual Context - A Systematic Study
- **Arxiv ID**: http://arxiv.org/abs/1806.04259v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04259v1)
- **Published**: 2018-06-11 22:35:51+00:00
- **Updated**: 2018-06-11 22:35:51+00:00
- **Authors**: Korsuk Sirinukunwattana, Nasullah Khalid Alham, Clare Verrill, Jens Rittscher
- **Comment**: None
- **Journal**: None
- **Summary**: While challenging, the dense segmentation of histology images is a necessary first step to assess changes in tissue architecture and cellular morphology. Although specific convolutional neural network architectures have been applied with great success to the problem, few effectively incorporate visual context information from multiple scales. With this paper, we present a systematic comparison of different architectures to assess how including multi-scale information affects segmentation performance. A publicly available breast cancer and a locally collected prostate cancer datasets are being utilised for this study. The results support our hypothesis that visual context and scale play a crucial role in histology image classification problems.



### Task Driven Generative Modeling for Unsupervised Domain Adaptation: Application to X-ray Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.07201v1
- **DOI**: 10.1007/978-3-030-00934-2_67
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.07201v1)
- **Published**: 2018-06-11 22:39:35+00:00
- **Updated**: 2018-06-11 22:39:35+00:00
- **Authors**: Yue Zhang, Shun Miao, Tommaso Mansi, Rui Liao
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic parsing of anatomical objects in X-ray images is critical to many clinical applications in particular towards image-guided invention and workflow automation. Existing deep network models require a large amount of labeled data. However, obtaining accurate pixel-wise labeling in X-ray images relies heavily on skilled clinicians due to the large overlaps of anatomy and the complex texture patterns. On the other hand, organs in 3D CT scans preserve clearer structures as well as sharper boundaries and thus can be easily delineated. In this paper, we propose a novel model framework for learning automatic X-ray image parsing from labeled CT scans. Specifically, a Dense Image-to-Image network (DI2I) for multi-organ segmentation is first trained on X-ray like Digitally Reconstructed Radiographs (DRRs) rendered from 3D CT volumes. Then we introduce a Task Driven Generative Adversarial Network (TD-GAN) architecture to achieve simultaneous style transfer and parsing for unseen real X-ray images. TD-GAN consists of a modified cycle-GAN substructure for pixel-to-pixel translation between DRRs and X-ray images and an added module leveraging the pre-trained DI2I to enforce segmentation consistency. The TD-GAN framework is general and can be easily adapted to other learning tasks. In the numerical experiments, we validate the proposed model on 815 DRRs and 153 topograms. While the vanilla DI2I without any adaptation fails completely on segmenting the topograms, the proposed model does not require any topogram labels and is able to provide a promising average dice of 85% which achieves the same level accuracy of supervised training (88%).



### Accurate and Robust Neural Networks for Security Related Applications Exampled by Face Morphing Attacks
- **Arxiv ID**: http://arxiv.org/abs/1806.04265v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1806.04265v1)
- **Published**: 2018-06-11 23:24:11+00:00
- **Updated**: 2018-06-11 23:24:11+00:00
- **Authors**: Clemens Seibold, Wojciech Samek, Anna Hilsmann, Peter Eisert
- **Comment**: 16 pages, 7 figures
- **Journal**: None
- **Summary**: Artificial neural networks tend to learn only what they need for a task. A manipulation of the training data can counter this phenomenon. In this paper, we study the effect of different alterations of the training data, which limit the amount and position of information that is available for the decision making. We analyze the accuracy and robustness against semantic and black box attacks on the networks that were trained on different training data modifications for the particular example of morphing attacks. A morphing attack is an attack on a biometric facial recognition system where the system is fooled to match two different individuals with the same synthetic face image. Such a synthetic image can be created by aligning and blending images of the two individuals that should be matched with this image.



### Understanding Patch-Based Learning by Explaining Predictions
- **Arxiv ID**: http://arxiv.org/abs/1806.06926v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.06926v1)
- **Published**: 2018-06-11 23:44:45+00:00
- **Updated**: 2018-06-11 23:44:45+00:00
- **Authors**: Christopher Anders, Grégoire Montavon, Wojciech Samek, Klaus-Robert Müller
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: Deep networks are able to learn highly predictive models of video data. Due to video length, a common strategy is to train them on small video snippets. We apply the deep Taylor / LRP technique to understand the deep network's classification decisions, and identify a "border effect": a tendency of the classifier to look mainly at the bordering frames of the input. This effect relates to the step size used to build the video snippet, which we can then tune in order to improve the classifier's accuracy without retraining the model. To our knowledge, this is the the first work to apply the deep Taylor / LRP technique on any video analyzing neural network.



