# Arxiv Papers in cs.CV on 2018-06-26
### Cycle Consistent Adversarial Denoising Network for Multiphase Coronary CT Angiography
- **Arxiv ID**: http://arxiv.org/abs/1806.09748v3
- **DOI**: 10.1002/mp.13284
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.09748v3)
- **Published**: 2018-06-26 01:17:51+00:00
- **Updated**: 2018-11-07 14:21:14+00:00
- **Authors**: Eunhee Kang, Hyun Jung Koo, Dong Hyun Yang, Joon Bum Seo, Jong Chul Ye
- **Comment**: This work is accepted in Medical Physics
- **Journal**: None
- **Summary**: In coronary CT angiography, a series of CT images are taken at different levels of radiation dose during the examination. Although this reduces the total radiation dose, the image quality during the low-dose phases is significantly degraded. To address this problem, here we propose a novel semi-supervised learning technique that can remove the noises of the CT images obtained in the low-dose phases by learning from the CT images in the routine dose phases. Although a supervised learning approach is not possible due to the differences in the underlying heart structure in two phases, the images in the two phases are closely related so that we propose a cycle-consistent adversarial denoising network to learn the non-degenerate mapping between the low and high dose cardiac phases. Experimental results showed that the proposed method effectively reduces the noise in the low-dose CT image while the preserving detailed texture and edge information. Moreover, thanks to the cyclic consistency and identity loss, the proposed network does not create any artificial features that are not present in the input images. Visual grading and quality evaluation also confirm that the proposed method provides significant improvement in diagnostic quality.



### Syn2Real: A New Benchmark forSynthetic-to-Real Visual Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1806.09755v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09755v1)
- **Published**: 2018-06-26 01:53:13+00:00
- **Updated**: 2018-06-26 01:53:13+00:00
- **Authors**: Xingchao Peng, Ben Usman, Kuniaki Saito, Neela Kaushik, Judy Hoffman, Kate Saenko
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised transfer of object recognition models from synthetic to real data is an important problem with many potential applications. The challenge is how to "adapt" a model trained on simulated images so that it performs well on real-world data without any additional supervision. Unfortunately, current benchmarks for this problem are limited in size and task diversity. In this paper, we present a new large-scale benchmark called Syn2Real, which consists of a synthetic domain rendered from 3D object models and two real-image domains containing the same object categories. We define three related tasks on this benchmark: closed-set object classification, open-set object classification, and object detection. Our evaluation of multiple state-of-the-art methods reveals a large gap in adaptation performance between the easier closed-set classification task and the more difficult open-set and detection tasks. We conclude that developing adaptation methods that work well across all three tasks presents a significant future challenge for syn2real domain transfer.



### Deep Generative Models with Learnable Knowledge Constraints
- **Arxiv ID**: http://arxiv.org/abs/1806.09764v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.09764v2)
- **Published**: 2018-06-26 02:31:35+00:00
- **Updated**: 2018-11-20 02:10:48+00:00
- **Authors**: Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, Xiaodan Liang, Lianhui Qin, Haoye Dong, Eric Xing
- **Comment**: Neural Information Processing Systems (NeurIPS) 2018
- **Journal**: None
- **Summary**: The broad set of deep generative models (DGMs) has achieved remarkable advances. However, it is often difficult to incorporate rich structured domain knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a principled framework to impose structured constraints on probabilistic models, but has limited applicability to the diverse DGMs that can lack a Bayesian formulation or even explicit density evaluation. PR also requires constraints to be fully specified a priori, which is impractical or suboptimal for complex knowledge with learnable uncertain parts. In this paper, we establish mathematical correspondence between PR and reinforcement learning (RL), and, based on the connection, expand PR to learn constraints as the extrinsic reward in RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is flexible to adapt arbitrary constraints with the model jointly. Experiments on human image generation and templated sentence generation show models with learned knowledge constraints by our algorithm greatly improve over base generative models.



### Simultaneous Segmentation and Classification of Bone Surfaces from Ultrasound Using a Multi-feature Guided CNN
- **Arxiv ID**: http://arxiv.org/abs/1806.09766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09766v1)
- **Published**: 2018-06-26 02:35:50+00:00
- **Updated**: 2018-06-26 02:35:50+00:00
- **Authors**: Puyang Wang, Vishal M. Patel, Ilker Hacihaliloglu
- **Comment**: None
- **Journal**: None
- **Summary**: Various imaging artifacts, low signal-to-noise ratio, and bone surfaces appearing several millimeters in thickness have hindered the success of ultrasound (US) guided computer assisted orthopedic surgery procedures. In this work, a multi-feature guided convolutional neural network (CNN) architecture is proposed for simultaneous enhancement, segmentation, and classification of bone surfaces from US data. The proposed CNN consists of two main parts: a pre-enhancing net, that takes the concatenation of B-mode US scan and three filtered image features for the enhancement of bone surfaces, and a modified U-net with a classification layer. The proposed method was validated on 650 in vivo US scans collected using two US machines, by scanning knee, femur, distal radius and tibia bones. Validation, against expert annotation, achieved statistically significant improvements in segmentation of bone surfaces compared to state-of-the-art.



### Cross-position Activity Recognition with Stratified Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.09776v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09776v2)
- **Published**: 2018-06-26 03:01:59+00:00
- **Updated**: 2018-11-13 02:34:07+00:00
- **Authors**: Yiqiang Chen, Jindong Wang, Meiyu Huang, Han Yu
- **Comment**: Submit to Pervasive and Mobile Computing as an extension to PerCom 18
  paper; First revision. arXiv admin note: substantial text overlap with
  arXiv:1801.00820
- **Journal**: None
- **Summary**: Human activity recognition aims to recognize the activities of daily living by utilizing the sensors on different body parts. However, when the labeled data from a certain body position (i.e. target domain) is missing, how to leverage the data from other positions (i.e. source domain) to help learn the activity labels of this position? When there are several source domains available, it is often difficult to select the most similar source domain to the target domain. With the selected source domain, we need to perform accurate knowledge transfer between domains. Existing methods only learn the global distance between domains while ignoring the local property. In this paper, we propose a \textit{Stratified Transfer Learning} (STL) framework to perform both source domain selection and knowledge transfer. STL is based on our proposed \textit{Stratified} distance to capture the local property of domains. STL consists of two components: Stratified Domain Selection (STL-SDS) can select the most similar source domain to the target domain; Stratified Activity Transfer (STL-SAT) is able to perform accurate knowledge transfer. Extensive experiments on three public activity recognition datasets demonstrate the superiority of STL. Furthermore, we extensively investigate the performance of transfer learning across different degrees of similarities and activity levels between domains. We also discuss the potential applications of STL in other fields of pervasive computing for future research.



### CFENet: An Accurate and Efficient Single-Shot Object Detector for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1806.09790v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09790v2)
- **Published**: 2018-06-26 04:48:01+00:00
- **Updated**: 2018-10-10 07:28:52+00:00
- **Authors**: Qijie Zhao, Tao Sheng, Yongtao Wang, Feng Ni, Ling Cai
- **Comment**: 5 pages, 4 figures, CVPR2018, Workshop of Autonomous Driving (WAD)
- **Journal**: None
- **Summary**: The ability to detect small objects and the speed of the object detector are very important for the application of autonomous driving, and in this paper, we propose an effective yet efficient one-stage detector, which gained the second place in the Road Object Detection competition of CVPR2018 workshop - Workshop of Autonomous Driving(WAD). The proposed detector inherits the architecture of SSD and introduces a novel Comprehensive Feature Enhancement(CFE) module into it. Experimental results on this competition dataset as well as the MSCOCO dataset demonstrate that the proposed detector (named CFENet) performs much better than the original SSD and the state-of-the-art method RefineDet especially for small objects, while keeping high efficiency close to the original SSD. Specifically, the single scale version of the proposed detector can run at the speed of 21 fps, while the multi-scale version with larger input size achieves the mAP 29.69, ranking second on the leaderboard



### SuperPCA: A Superpixelwise PCA Approach for Unsupervised Feature Extraction of Hyperspectral Imagery
- **Arxiv ID**: http://arxiv.org/abs/1806.09807v2
- **DOI**: 10.1109/TGRS.2018.2828029
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09807v2)
- **Published**: 2018-06-26 06:38:07+00:00
- **Updated**: 2018-09-15 03:24:17+00:00
- **Authors**: Junjun Jiang, Jiayi Ma, Chen Chen, Zhongyuan Wang, Zhihua Cai, Lizhe Wang
- **Comment**: 13 pages, 10 figures, Accepted by IEEE TGRS
- **Journal**: None
- **Summary**: As an unsupervised dimensionality reduction method, principal component analysis (PCA) has been widely considered as an efficient and effective preprocessing step for hyperspectral image (HSI) processing and analysis tasks. It takes each band as a whole and globally extracts the most representative bands. However, different homogeneous regions correspond to different objects, whose spectral features are diverse. It is obviously inappropriate to carry out dimensionality reduction through a unified projection for an entire HSI. In this paper, a simple but very effective superpixelwise PCA approach, called SuperPCA, is proposed to learn the intrinsic low-dimensional features of HSIs. In contrast to classical PCA models, SuperPCA has four main properties. (1) Unlike the traditional PCA method based on a whole image, SuperPCA takes into account the diversity in different homogeneous regions, that is, different regions should have different projections. (2) Most of the conventional feature extraction models cannot directly use the spatial information of HSIs, while SuperPCA is able to incorporate the spatial context information into the unsupervised dimensionality reduction by superpixel segmentation. (3) Since the regions obtained by superpixel segmentation have homogeneity, SuperPCA can extract potential low-dimensional features even under noise. (4) Although SuperPCA is an unsupervised method, it can achieve competitive performance when compared with supervised approaches. The resulting features are discriminative, compact, and noise resistant, leading to improved HSI classification performance. Experiments on three public datasets demonstrate that the SuperPCA model significantly outperforms the conventional PCA based dimensionality reduction baselines for HSI classification. The Matlab source code is available at https://github.com/junjun-jiang/SuperPCA



### Generating Counterfactual Explanations with Natural Language
- **Arxiv ID**: http://arxiv.org/abs/1806.09809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09809v1)
- **Published**: 2018-06-26 06:43:36+00:00
- **Updated**: 2018-06-26 06:43:36+00:00
- **Authors**: Lisa Anne Hendricks, Ronghang Hu, Trevor Darrell, Zeynep Akata
- **Comment**: presented at 2018 ICML Workshop on Human Interpretability in Machine
  Learning (WHI 2018), Stockholm, Sweden
- **Journal**: None
- **Summary**: Natural language explanations of deep neural network decisions provide an intuitive way for a AI agent to articulate a reasoning process. Current textual explanations learn to discuss class discriminative features in an image. However, it is also helpful to understand which attributes might change a classification decision if present in an image (e.g., "This is not a Scarlet Tanager because it does not have black wings.") We call such textual explanations counterfactual explanations, and propose an intuitive method to generate counterfactual explanations by inspecting which evidence in an input is missing, but might contribute to a different classification decision if present in the image. To demonstrate our method we consider a fine-grained image classification task in which we take as input an image and a counterfactual class and output text which explains why the image does not belong to a counterfactual class. We then analyze our generated counterfactual explanations both qualitatively and quantitatively using proposed automatic metrics.



### Visually-Aware Personalized Recommendation using Interpretable Image Representations
- **Arxiv ID**: http://arxiv.org/abs/1806.09820v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09820v2)
- **Published**: 2018-06-26 07:24:06+00:00
- **Updated**: 2018-08-21 21:38:37+00:00
- **Authors**: Charles Packer, Julian McAuley, Arnau Ramisa
- **Comment**: AI for Fashion workshop, held in conjunction with KDD 2018, London. 4
  pages
- **Journal**: None
- **Summary**: Visually-aware recommender systems use visual signals present in the underlying data to model the visual characteristics of items and users' preferences towards them. In the domain of clothing recommendation, incorporating items' visual information (e.g., product images) is particularly important since clothing item appearance is often a critical factor in influencing the user's purchasing decisions. Current state-of-the-art visually-aware recommender systems utilize image features extracted from pre-trained deep convolutional neural networks, however these extremely high-dimensional representations are difficult to interpret, especially in relation to the relatively low number of visual properties that may guide users' decisions.   In this paper we propose a novel approach to personalized clothing recommendation that models the dynamics of individual users' visual preferences. By using interpretable image representations generated with a unique feature learning process, our model learns to explain users' prior feedback in terms of their affinity towards specific visual attributes and styles. Our approach achieves state-of-the-art performance on personalized ranking tasks, and the incorporation of interpretable visual features allows for powerful model introspection, which we demonstrate by using an interactive recommendation algorithm and visualizing the rise and fall of fashion trends over time.



### An Overview of Perception Methods for Horticultural Robots: From Pollination to Harvest
- **Arxiv ID**: http://arxiv.org/abs/1807.03124v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.03124v1)
- **Published**: 2018-06-26 07:44:48+00:00
- **Updated**: 2018-06-26 07:44:48+00:00
- **Authors**: Ho Seok Ahn, Feras Dayoub, Marija Popovic, Bruce MacDonald, Roland Siegwart, Inkyu Sa
- **Comment**: 6 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: Horticultural enterprises are becoming more sophisticated as the range of the crops they target expands. Requirements for enhanced efficiency and productivity have driven the demand for automating on-field operations. However, various problems remain yet to be solved for their reliable, safe deployment in real-world scenarios. This paper examines major research trends and current challenges in horticultural robotics. Specifically, our work focuses on sensing and perception in the three main horticultural procedures: pollination, yield estimation, and harvesting. For each task, we expose major issues arising from the unstructured, cluttered, and rugged nature of field environments, including variable lighting conditions and difficulties in fruit-specific detection, and highlight promising contemporary studies.



### Fully Convolutional Networks for Automated Segmentation of Abdominal Adipose Tissue Depots in Multicenter Water-Fat MRI
- **Arxiv ID**: http://arxiv.org/abs/1807.03122v5
- **DOI**: 10.1002/mrm.27550
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03122v5)
- **Published**: 2018-06-26 08:28:21+00:00
- **Updated**: 2018-11-01 10:09:09+00:00
- **Authors**: Taro Langner, Anders Hedström, Katharina Mörwald, Daniel Weghuber, Anders Forslund, Peter Bergsten, Håkan Ahlström, Joel Kullberg
- **Comment**: Key words: deep learning, fully convolutional networks, segmentation,
  water-fat MRI, adipose tissue, abdominal
- **Journal**: Magn Reson Med. 2018;00:1-10
- **Summary**: Purpose: An approach for the automated segmentation of visceral adipose tissue (VAT) and subcutaneous adipose tissue (SAT) in multicenter water-fat MRI scans of the abdomen was investigated, using two different neural network architectures.   Methods: The two fully convolutional network architectures U-Net and V-Net were trained, evaluated and compared on the water-fat MRI data. Data of the study Tellus with 90 scans from a single center was used for a 10-fold cross-validation in which the most successful configuration for both networks was determined. These configurations were then tested on 20 scans of the multicenter study beta-cell function in JUvenile Diabetes and Obesity (BetaJudo), which involved a different study population and scanning device.   Results: The U-Net outperformed the used implementation of the V-Net in both cross-validation and testing. In cross-validation, the U-Net reached average dice scores of 0.988 (VAT) and 0.992 (SAT). The average of the absolute quantification errors amount to 0.67% (VAT) and 0.39% (SAT). On the multi-center test data, the U-Net performs only slightly worse, with average dice scores of 0.970 (VAT) and 0.987 (SAT) and quantification errors of 2.80% (VAT) and 1.65% (SAT).   Conclusion: The segmentations generated by the U-Net allow for reliable quantification and could therefore be viable for high-quality automated measurements of VAT and SAT in large-scale studies with minimal need for human intervention. The high performance on the multicenter test data furthermore shows the robustness of this approach for data of different patient demographics and imaging centers, as long as a consistent imaging protocol is used.



### Multi-Task Deep Convolutional Neural Network for the Segmentation of Type B Aortic Dissection
- **Arxiv ID**: http://arxiv.org/abs/1806.09860v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09860v6)
- **Published**: 2018-06-26 09:11:25+00:00
- **Updated**: 2018-11-22 08:46:59+00:00
- **Authors**: Jianning Li, Long Cao, Yangyang Ge, W. Cheng, M. Bowen, G. Wei
- **Comment**: This article has been removed by arXiv administrators because the
  submitter did not have the rights to agree to the license at the time of
  submission
- **Journal**: None
- **Summary**: Segmentation of the entire aorta and true-false lumen is crucial to inform plan and follow-up for endovascular repair of the rare yet life threatening type B aortic dissection. Manual segmentation by slice is time-consuming and requires expertise, while current computer-aided methods focus on the segmentation of the entire aorta, are unable to concurrently segment true-false lumen, and some require human interaction. We here report a fully automated approach based on a 3-D multi-task deep convolutional neural network that segments the entire aorta and true-false lumen from CTA images in a unified framework. For training, we built a database containing 254 CTA images (210 preoperative and 44 postoperative) obtained using various systems from 254 unique patients with type B aortic dissection. Slice-wise manual segmentation of the entire aorta and the true-false lumen for each 3-D CTA image was provided. Upon evaluation of another 16 CTA images (11 preoperative and 5 postoperative) with ground truth segmentation provided by experienced vascular surgeons, our method achieves a mean dice similarity score(DSC) of 0.910,0.849 and 0.821 for the entire aorta,true lumen and false lumen respectively.



### Multi-modal Image Processing based on Coupled Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.09882v1
- **DOI**: 10.1109/SPAWC.2018.8446001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09882v1)
- **Published**: 2018-06-26 10:01:10+00:00
- **Updated**: 2018-06-26 10:01:10+00:00
- **Authors**: Pingfan Song, Miguel R. D. Rodrigues
- **Comment**: SPAWC 2018, 19th IEEE International Workshop On Signal Processing
  Advances In Wireless Communications
- **Journal**: None
- **Summary**: In real-world scenarios, many data processing problems often involve heterogeneous images associated with different imaging modalities. Since these multimodal images originate from the same phenomenon, it is realistic to assume that they share common attributes or characteristics. In this paper, we propose a multi-modal image processing framework based on coupled dictionary learning to capture similarities and disparities between different image modalities. In particular, our framework can capture favorable structure similarities across different image modalities such as edges, corners, and other elementary primitives in a learned sparse transform domain, instead of the original pixel domain, that can be used to improve a number of image processing tasks such as denoising, inpainting, or super-resolution. Practical experiments demonstrate that incorporating multimodal information using our framework brings notable benefits.



### Multimodal Image Denoising based on Coupled Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.10678v1
- **DOI**: 10.1109/ICIP.2018.8451697
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10678v1)
- **Published**: 2018-06-26 10:56:14+00:00
- **Updated**: 2018-06-26 10:56:14+00:00
- **Authors**: Pingfan Song, Miguel R. D. Rodrigues
- **Comment**: 2018 IEEE International Conference on Image Processing (ICIP). arXiv
  admin note: text overlap with arXiv:1806.09882
- **Journal**: None
- **Summary**: In this paper, we propose a new multimodal image denoising approach to attenuate white Gaussian additive noise in a given image modality under the aid of a guidance image modality. The proposed coupled image denoising approach consists of two stages: coupled sparse coding and reconstruction. The first stage performs joint sparse transform for multimodal images with respect to a group of learned coupled dictionaries, followed by a shrinkage operation on the sparse representations. Then, in the second stage, the shrunken representations, together with coupled dictionaries, contribute to the reconstruction of the denoised image via an inverse transform. The proposed denoising scheme demonstrates the capability to capture both the common and distinct features of different data modalities. This capability makes our approach more robust to inconsistencies between the guidance and the target images, thereby overcoming drawbacks such as the texture copying artifacts. Experiments on real multimodal images demonstrate that the proposed approach is able to better employ guidance information to bring notable benefits in the image denoising task with respect to the state-of-the-art.



### AirLab: Autograd Image Registration Laboratory
- **Arxiv ID**: http://arxiv.org/abs/1806.09907v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09907v2)
- **Published**: 2018-06-26 11:12:43+00:00
- **Updated**: 2020-03-02 16:03:27+00:00
- **Authors**: Robin Sandkühler, Christoph Jud, Simon Andermatt, Philippe C. Cattin
- **Comment**: Corresponding author: Christoph Jud, e-mail: christoph.jud@unibas.ch
- **Journal**: None
- **Summary**: Medical image registration is an active research topic and forms a basis for many medical image analysis tasks. Although image registration is a rather general concept specialized methods are usually required to target a specific registration problem. The development and implementation of such methods has been tough so far as the gradient of the objective has to be computed. Also, its evaluation has to be performed preferably on a GPU for larger images and for more complex transformation models and regularization terms. This hinders researchers from rapid prototyping and poses hurdles to reproduce research results. There is a clear need for an environment which hides this complexity to put the modeling and the experimental exploration of registration methods into the foreground. With the "Autograd Image Registration Laboratory" (AIRLab), we introduce an open laboratory for image registration tasks, where the analytic gradients of the objective function are computed automatically and the device where the computations are performed, on a CPU or a GPU, is transparent. It is meant as a laboratory for researchers and developers enabling them to rapidly try out new ideas for registering images and to reproduce registration results which have already been published. AIRLab is implemented in Python using PyTorch as tensor and optimization library and SimpleITK for basic image IO. Therefore, it profits from recent advances made by the machine learning community concerning optimization and deep neural network models. The presented draft of this paper outlines AIRLab with first code snippets and performance analyses. A more exhaustive introduction will follow as a final version soon.



### Coupled Dictionary Learning for Multi-contrast MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1806.09930v1
- **DOI**: 10.1109/TMI.2019.2932961
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.09930v1)
- **Published**: 2018-06-26 11:52:57+00:00
- **Updated**: 2018-06-26 11:52:57+00:00
- **Authors**: Pingfan Song, Lior Weizman, Joao F. C. Mota, Yonina C. Eldar, Miguel R. D. Rodrigues
- **Comment**: 2018 IEEE International Conference on Image Processing (ICIP)
- **Journal**: None
- **Summary**: Medical imaging tasks often involve multiple contrasts, such as T1- and T2-weighted magnetic resonance imaging (MRI) data. These contrasts capture information associated with the same underlying anatomy and thus exhibit similarities. In this paper, we propose a Coupled Dictionary Learning based multi-contrast MRI reconstruction (CDLMRI) approach to leverage an available guidance contrast to restore the target contrast. Our approach consists of three stages: coupled dictionary learning, coupled sparse denoising, and $k$-space consistency enforcing. The first stage learns a group of dictionaries that capture correlations among multiple contrasts. By capitalizing on the learned adaptive dictionaries, the second stage performs joint sparse coding to denoise the corrupted target image with the aid of a guidance contrast. The third stage enforces consistency between the denoised image and the measurements in the $k$-space domain. Numerical experiments on the retrospective under-sampling of clinical MR images demonstrate that incorporating additional guidance contrast via our design improves MRI reconstruction, compared to state-of-the-art approaches.



### Scaling Neural Network Performance through Customized Hardware Architectures on Reconfigurable Logic
- **Arxiv ID**: http://arxiv.org/abs/1807.03123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03123v1)
- **Published**: 2018-06-26 14:39:44+00:00
- **Updated**: 2018-06-26 14:39:44+00:00
- **Authors**: Michaela Blott, Thomas B. Preusser, Nicholas Fraser, Giulio Gambardella, Kenneth OBrien, Yaman Umuroglu, Miriam Leeser
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks have dramatically improved in recent years, surpassing human accuracy on certain problems and performance exceeding that of traditional computer vision algorithms. While the compute pattern in itself is relatively simple, significant compute and memory challenges remain as CNNs may contain millions of floating-point parameters and require billions of floating-point operations to process a single image. These computational requirements, combined with storage footprints that exceed typical cache sizes, pose a significant performance and power challenge for modern compute architectures. One of the promising opportunities to scale performance and power efficiency is leveraging reduced precision representations for all activations and weights as this allows to scale compute capabilities, reduce weight and feature map buffering requirements as well as energy consumption. While a small reduction in accuracy is encountered, these Quantized Neural Networks have been shown to achieve state-of-the-art accuracy on standard benchmark datasets, such as MNIST, CIFAR-10, SVHN and even ImageNet, and thus provide highly attractive design trade-offs. Current research has focused mainly on the implementation of extreme variants with full binarization of weights and or activations, as well typically smaller input images. Within this paper, we investigate the scalability of dataflow architectures with respect to supporting various precisions for both weights and activations, larger image dimensions, and increasing numbers of feature map channels. Key contributions are a formalized approach to understanding the scalability of the existing hardware architecture with cost models and a performance prediction as a function of the target device size. We provide validating experimental results for an ImageNet classification on a server-class platform, namely the AWS F1 node.



### Crowd Counting with Density Adaption Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.10040v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10040v1)
- **Published**: 2018-06-26 14:57:16+00:00
- **Updated**: 2018-06-26 14:57:16+00:00
- **Authors**: Li Wang, Weiyuan Shao, Yao Lu, Hao Ye, Jian Pu, Yingbin Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Crowd counting is one of the core tasks in various surveillance applications. A practical system involves estimating accurate head counts in dynamic scenarios under different lightning, camera perspective and occlusion states. Previous approaches estimate head counts despite that they can vary dramatically in different density settings; the crowd is often unevenly distributed and the results are therefore unsatisfactory. In this paper, we propose a lightweight deep learning framework that can automatically estimate the crowd density level and adaptively choose between different counter networks that are explicitly trained for different density domains. Experiments on two recent crowd counting datasets, UCF_CC_50 and ShanghaiTech, show that the proposed mechanism achieves promising improvements over state-of-the-art methods. Moreover, runtime speed is 20 FPS on a single GPU.



### Multi-Mapping Image-to-Image Translation with Central Biasing Normalization
- **Arxiv ID**: http://arxiv.org/abs/1806.10050v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10050v5)
- **Published**: 2018-06-26 15:07:42+00:00
- **Updated**: 2020-04-17 09:17:08+00:00
- **Authors**: Xiaoming Yu, Zhenqiang Ying, Thomas Li, Shan Liu, Ge Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in image-to-image translation have seen a rise in approaches generating diverse images through a single network. To indicate the target domain for a one-to-many mapping, the latent code is injected into the generator network. However, we found that the injection method leads to mode collapse because of normalization strategies. Existing normalization strategies might either cause the inconsistency of feature distribution or eliminate the effect of the latent code. To solve these problems, we propose the consistency within diversity criteria for designing the multi-mapping model. Based on the criteria, we propose central biasing normalization to inject the latent code information. Experiments show that our method can improve the quality and diversity of existing image-to-image translation models, such as StarGAN, BicycleGAN, and pix2pix.



### Leveraging Disease Progression Learning for Medical Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.10128v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10128v2)
- **Published**: 2018-06-26 17:58:56+00:00
- **Updated**: 2018-09-01 20:48:18+00:00
- **Authors**: Qicheng Lao, Thomas Fevens, Boyu Wang
- **Comment**: None
- **Journal**: IEEE International Conference on Bioinformatics and Biomedicine
  (BIBM) 2018
- **Summary**: Unlike natural images, medical images often have intrinsic characteristics that can be leveraged for neural network learning. For example, images that belong to different stages of a disease may continuously follow a certain progression pattern. In this paper, we propose a novel method that leverages disease progression learning for medical image recognition. In our method, sequences of images ordered by disease stages are learned by a neural network that consists of a shared vision model for feature extraction and a long short-term memory network for the learning of stage sequences. Auxiliary vision outputs are also included to capture stage features that tend to be discrete along the disease progression. Our proposed method is evaluated on a public diabetic retinopathy dataset, and achieves about 3.3% improvement in disease staging accuracy, compared to the baseline method that does not use disease progression learning.



### Detection of Alzheimers Disease from MRI using Convolutional Neural Network with Tensorflow
- **Arxiv ID**: http://arxiv.org/abs/1806.10170v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10170v2)
- **Published**: 2018-06-26 18:59:28+00:00
- **Updated**: 2019-10-14 07:47:22+00:00
- **Authors**: Gururaj Awate, Sunil Bangare, G Pradeepini, S Patil
- **Comment**: New version with more accurate information is available at:
  arXiv:1901.10231
- **Journal**: IEEE Xplore 2018
- **Summary**: Nowadays, due to tremendous improvements in high performance computing, it has become easier to train Neural Networks. We intend to take advantage of this situation and apply this technology in solving real world problems. There was a need for automatic diagnosis certain diseases from medical images that could help a doctor and radiologist for further action towards treating the illness. We chose Alzheimer disease for this purpose. Alzheimer disease is the leading cause of dementia and memory loss. Alzheimer disease, it is caused by atrophy of the certain brain regions and by brain cell death. MRI scans reveal this information but atrophy regions are different for different people which makes the diagnosis a little trickier and often gets miss-diagnosed by doctors and radiologists. The Dataset used for this project is provided by OASIS, which contains over 400 subjects 100 of which having mild to severe dementia and is supplemented by MMSE and CDR standards of diagnosis in the same context. Enter CNN, Convolutional Neural Networks are a hybrid of Kernel Convolutions and Neural Networks. Kernel Convolutions is a technique that uses filters to recognize and segment images based on features. Neural Networks consist of neurons which are loosely based on human brains neuron which represents a single classifier and interconnected by weights, have different biases and are activated by some activation functions. By using Convolutional Neural Networks, the problem can be solved with minimal error rate. The technologies we intend to use are libraries like CUDA CuDNN for making use of GPU and its multiple cores-parallel computing to train models while giving us high performance.



### MMSE Approximation For Sparse Coding Algorithms Using Stochastic Resonance
- **Arxiv ID**: http://arxiv.org/abs/1806.10171v5
- **DOI**: 10.1109/TSP.2019.2929464
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.10171v5)
- **Published**: 2018-06-26 19:03:39+00:00
- **Updated**: 2019-04-11 18:57:28+00:00
- **Authors**: Dror Simon, Jeremias Sulam, Yaniv Romano, Yue M. Lu, Michael Elad
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse coding refers to the pursuit of the sparsest representation of a signal in a typically overcomplete dictionary. From a Bayesian perspective, sparse coding provides a Maximum a Posteriori (MAP) estimate of the unknown vector under a sparse prior. In this work, we suggest enhancing the performance of sparse coding algorithms by a deliberate and controlled contamination of the input with random noise, a phenomenon known as stochastic resonance. The proposed method adds controlled noise to the input and estimates a sparse representation from the perturbed signal. A set of such solutions is then obtained by projecting the original input signal onto the recovered set of supports. We present two variants of the described method, which differ in their final step. The first is a provably convergent approximation to the Minimum Mean Square Error (MMSE) estimator, relying on the generative model and applying a weighted average over the recovered solutions. The second is a relaxed variant of the former that simply applies an empirical mean. We show that both methods provide a computationally efficient approximation to the MMSE estimator, which is typically intractable to compute. We demonstrate our findings empirically and provide a theoretical analysis of our method under several different cases.



### Unsupervised Learning by Competing Hidden Units
- **Arxiv ID**: http://arxiv.org/abs/1806.10181v2
- **DOI**: 10.1073/pnas.1820458116
- **Categories**: **cs.LG**, cs.CV, cs.NE, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.10181v2)
- **Published**: 2018-06-26 19:32:58+00:00
- **Updated**: 2019-08-29 02:36:17+00:00
- **Authors**: Dmitry Krotov, John Hopfield
- **Comment**: None
- **Journal**: Proceedings of the National Academy of Sciences of the USA, 116
  (16) 7723-7731 (2019)
- **Summary**: It is widely believed that the backpropagation algorithm is essential for learning good feature detectors in early layers of artificial neural networks, so that these detectors are useful for the task performed by the higher layers of that neural network. At the same time, the traditional form of backpropagation is biologically implausible. In the present paper we propose an unusual learning rule, which has a degree of biological plausibility, and which is motivated by Hebb's idea that change of the synapse strength should be local - i.e. should depend only on the activities of the pre and post synaptic neurons. We design a learning algorithm that utilizes global inhibition in the hidden layer, and is capable of learning early feature detectors in a completely unsupervised way. These learned lower layer feature detectors can be used to train higher layer weights in a usual supervised way so that the performance of the full network is comparable to the performance of standard feedforward networks trained end-to-end with a backpropagation algorithm.



### Deep Feature Factorization For Concept Discovery
- **Arxiv ID**: http://arxiv.org/abs/1806.10206v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.10206v5)
- **Published**: 2018-06-26 20:39:13+00:00
- **Updated**: 2018-10-08 10:49:15+00:00
- **Authors**: Edo Collins, Radhakrishna Achanta, Sabine Süsstrunk
- **Comment**: The European Conference on Computer Vision (ECCV), 2018
- **Journal**: None
- **Summary**: We propose Deep Feature Factorization (DFF), a method capable of localizing similar semantic concepts within an image or a set of images. We use DFF to gain insight into a deep convolutional neural network's learned features, where we detect hierarchical cluster structures in feature space. This is visualized as heat maps, which highlight semantically matching regions across a set of images, revealing what the network `perceives' as similar. DFF can also be used to perform co-segmentation and co-localization, and we report state-of-the-art results on these tasks.



