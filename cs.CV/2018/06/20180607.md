# Arxiv Papers in cs.CV on 2018-06-07
### Interlinked Convolutional Neural Networks for Face Parsing
- **Arxiv ID**: http://arxiv.org/abs/1806.02479v1
- **DOI**: 10.1007/978-3-319-25393-0_25
- **Categories**: **cs.CV**, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/1806.02479v1)
- **Published**: 2018-06-07 01:10:08+00:00
- **Updated**: 2018-06-07 01:10:08+00:00
- **Authors**: Yisu Zhou, Xiaolin Hu, Bo Zhang
- **Comment**: 11 pages, 4 figures, ISNN2015 Conference
- **Journal**: International Symposium on Neural Networks. Springer, Cham, 2015
- **Summary**: Face parsing is a basic task in face image analysis. It amounts to labeling each pixel with appropriate facial parts such as eyes and nose. In the paper, we present a interlinked convolutional neural network (iCNN) for solving this problem in an end-to-end fashion. It consists of multiple convolutional neural networks (CNNs) taking input in different scales. A special interlinking layer is designed to allow the CNNs to exchange information, enabling them to integrate local and contextual information efficiently. The hallmark of iCNN is the extensive use of downsampling and upsampling in the interlinking layers, while traditional CNNs usually uses downsampling only. A two-stage pipeline is proposed for face parsing and both stages use iCNN. The first stage localizes facial parts in the size-reduced image and the second stage labels the pixels in the identified facial parts in the original image. On a benchmark dataset we have obtained better results than the state-of-the-art methods.



### Information-Maximizing Sampling to Promote Tracking-by-Detection
- **Arxiv ID**: http://arxiv.org/abs/1806.02523v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02523v1)
- **Published**: 2018-06-07 05:59:02+00:00
- **Updated**: 2018-06-07 05:59:02+00:00
- **Authors**: Kourosh Meshgi, Maryam Sadat Mirzaei, Shigeyuki Oba
- **Comment**: visual tracking, information-maximizing sampling, active learning,
  structured sample learning
- **Journal**: None
- **Summary**: The performance of an adaptive tracking-by-detection algorithm not only depends on the classification and updating processes but also on the sampling. Typically, such trackers select their samples from the vicinity of the last predicted object location, or from its expected location using a pre-defined motion model, which does not exploit the contents of the samples nor the information provided by the classifier. We introduced the idea of most informative sampling, in which the sampler attempts to select samples that trouble the classifier of a discriminative tracker. We then proposed an active discriminative co-tracker that embed an adversarial sampler to increase its robustness against various tracking challenges. Experiments show that our proposed tracker outperforms state-of-the-art trackers on various benchmark videos.



### Shape Robust Text Detection with Progressive Scale Expansion Network
- **Arxiv ID**: http://arxiv.org/abs/1806.02559v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02559v1)
- **Published**: 2018-06-07 08:28:54+00:00
- **Updated**: 2018-06-07 08:28:54+00:00
- **Authors**: Xiang Li, Wenhai Wang, Wenbo Hou, Ruo-Ze Liu, Tong Lu, Jian Yang
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: The challenges of shape robust text detection lie in two aspects: 1) most existing quadrangular bounding box based detectors are difficult to locate texts with arbitrary shapes, which are hard to be enclosed perfectly in a rectangle; 2) most pixel-wise segmentation-based detectors may not separate the text instances that are very close to each other. To address these problems, we propose a novel Progressive Scale Expansion Network (PSENet), designed as a segmentation-based detector with multiple predictions for each text instance. These predictions correspond to different `kernels' produced by shrinking the original text instance into various scales. Consequently, the final detection can be conducted through our progressive scale expansion algorithm which gradually expands the kernels with minimal scales to the text instances with maximal and complete shapes. Due to the fact that there are large geometrical margins among these minimal kernels, our method is effective to distinguish the adjacent text instances and is robust to arbitrary shapes. The state-of-the-art results on ICDAR 2015 and ICDAR 2017 MLT benchmarks further confirm the great effectiveness of PSENet. Notably, PSENet outperforms the previous best record by absolute 6.37\% on the curve text dataset SCUT-CTW1500. Code will be available in https://github.com/whai362/PSENet.



### On the Effect of Inter-observer Variability for a Reliable Estimation of Uncertainty of Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.02562v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02562v1)
- **Published**: 2018-06-07 08:44:43+00:00
- **Updated**: 2018-06-07 08:44:43+00:00
- **Authors**: Alain Jungo, Raphael Meier, Ekin Ermis, Marcela Blatti-Moreno, Evelyn Herrmann, Roland Wiest, Mauricio Reyes
- **Comment**: Appears in Medical Image Computing and Computer Assisted
  Interventions (MICCAI), 2018
- **Journal**: None
- **Summary**: Uncertainty estimation methods are expected to improve the understanding and quality of computer-assisted methods used in medical applications (e.g., neurosurgical interventions, radiotherapy planning), where automated medical image segmentation is crucial. In supervised machine learning, a common practice to generate ground truth label data is to merge observer annotations. However, as many medical image tasks show a high inter-observer variability resulting from factors such as image quality, different levels of user expertise and domain knowledge, little is known as to how inter-observer variability and commonly used fusion methods affect the estimation of uncertainty of automated image segmentation. In this paper we analyze the effect of common image label fusion techniques on uncertainty estimation, and propose to learn the uncertainty among observers. The results highlight the negative effect of fusion methods applied in deep learning, to obtain reliable estimates of segmentation uncertainty. Additionally, we show that the learned observers' uncertainty can be combined with current standard Monte Carlo dropout Bayesian neural networks to characterize uncertainty of model's parameters.



### Generative Adversarial Networks for Realistic Synthesis of Hyperspectral Samples
- **Arxiv ID**: http://arxiv.org/abs/1806.02583v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.02583v1)
- **Published**: 2018-06-07 09:36:12+00:00
- **Updated**: 2018-06-07 09:36:12+00:00
- **Authors**: Nicolas Audebert, Bertrand Le Saux, Sébastien Lefèvre
- **Comment**: None
- **Journal**: International Geoscience and Remote Sensing Symposium (IGARSS
  2018), Jul 2018, Valencia, Spain
- **Summary**: This work addresses the scarcity of annotated hyperspectral data required to train deep neural networks. Especially, we investigate generative adversarial networks and their application to the synthesis of consistent labeled spectra. By training such networks on public datasets, we show that these models are not only able to capture the underlying distribution, but also to generate genuine-looking and physically plausible spectra. Moreover, we experimentally validate that the synthetic samples can be used as an effective data augmentation strategy. We validate our approach on several public hyper-spectral datasets using a variety of deep classifiers.



### Learning Multi-Modal Self-Awareness Models for Autonomous Vehicles from Human Driving
- **Arxiv ID**: http://arxiv.org/abs/1806.02609v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02609v1)
- **Published**: 2018-06-07 11:08:53+00:00
- **Updated**: 2018-06-07 11:08:53+00:00
- **Authors**: Mahdyar Ravanbakhsh, Mohamad Baydoun, Damian Campo, Pablo Marin, David Martin, Lucio Marcenaro, Carlo S. Regazzoni
- **Comment**: FUSION 2018 - 21st International Conference on Information Fusion,
  Cambridge, UK
- **Journal**: None
- **Summary**: This paper presents a novel approach for learning self-awareness models for autonomous vehicles. The proposed technique is based on the availability of synchronized multi-sensor dynamic data related to different maneuvering tasks performed by a human operator. It is shown that different machine learning approaches can be used to first learn single modality models using coupled Dynamic Bayesian Networks; such models are then correlated at event level to discover contextual multi-modal concepts. In the presented case, visual perception and localization are used as modalities. Cross-correlations among modalities in time is discovered from data and are described as probabilistic links connecting shared and private multi-modal DBNs at the event (discrete) level. Results are presented on experiments performed on an autonomous vehicle, highlighting potentiality of the proposed approach to allow anomaly detection and autonomous decision making based on learned self-awareness models.



### Dimensionality-Driven Learning with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1806.02612v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.02612v2)
- **Published**: 2018-06-07 11:11:13+00:00
- **Updated**: 2018-07-31 14:54:24+00:00
- **Authors**: Xingjun Ma, Yisen Wang, Michael E. Houle, Shuo Zhou, Sarah M. Erfani, Shu-Tao Xia, Sudanthi Wijewickrema, James Bailey
- **Comment**: In Proceedings of the International Conference on Machine Learning
  (ICML), 2018
- **Journal**: None
- **Summary**: Datasets with significant proportions of noisy (incorrect) class labels present challenges for training accurate Deep Neural Networks (DNNs). We propose a new perspective for understanding DNN generalization for such datasets, by investigating the dimensionality of the deep representation subspace of training samples. We show that from a dimensionality perspective, DNNs exhibit quite distinctive learning styles when trained with clean labels versus when trained with a proportion of noisy labels. Based on this finding, we develop a new dimensionality-driven learning strategy, which monitors the dimensionality of subspaces during training and adapts the loss function accordingly. We empirically demonstrate that our approach is highly tolerant to significant proportions of noisy labels, and can effectively learn low-dimensional local subspaces that capture the data distribution.



### Nonparametric Density Flows for MRI Intensity Normalisation
- **Arxiv ID**: http://arxiv.org/abs/1806.02613v1
- **DOI**: 10.1007/978-3-030-00928-1_24
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02613v1)
- **Published**: 2018-06-07 11:13:35+00:00
- **Updated**: 2018-06-07 11:13:35+00:00
- **Authors**: Daniel C. Castro, Ben Glocker
- **Comment**: Accepted for publication at MICCAI 2018
- **Journal**: None
- **Summary**: With the adoption of powerful machine learning methods in medical image analysis, it is becoming increasingly desirable to aggregate data that is acquired across multiple sites. However, the underlying assumption of many analysis techniques that corresponding tissues have consistent intensities in all images is often violated in multi-centre databases. We introduce a novel intensity normalisation scheme based on density matching, wherein the histograms are modelled as Dirichlet process Gaussian mixtures. The source mixture model is transformed to minimise its $L^2$ divergence towards a target model, then the voxel intensities are transported through a mass-conserving flow to maintain agreement with the moving density. In a multi-centre study with brain MRI data, we show that the proposed technique produces excellent correspondence between the matched densities and histograms. We further demonstrate that our method makes tissue intensity statistics substantially more compatible between images than a baseline affine transformation and is comparable to state-of-the-art while providing considerably smoother transformations. Finally, we validate that nonlinear intensity normalisation is a step toward effective imaging data harmonisation.



### Writing Style Invariant Deep Learning Model for Historical Manuscripts Alignment
- **Arxiv ID**: http://arxiv.org/abs/1806.03987v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.03987v1)
- **Published**: 2018-06-07 11:13:39+00:00
- **Updated**: 2018-06-07 11:13:39+00:00
- **Authors**: Majeed Kassis, Jumana Nassour, Jihad El-Sana
- **Comment**: None
- **Journal**: None
- **Summary**: Historical manuscript alignment is a widely known problem in document analysis. Finding the differences between manuscript editions is mostly done manually. In this paper, we present a writer independent deep learning model which is trained on several writing styles, and able to achieve high detection accuracy when tested on writing styles not present in training data. We test our model using cross validation, each time we train the model on five manuscripts, and test it on the other two manuscripts, never seen in the training data. We've applied cross validation on seven manuscripts, netting 21 different tests, achieving average accuracy of $\%92.17$. We also present a new alignment algorithm based on dynamic sized sliding window, which is able to successfully handle complex cases.



### Super-Resolution using Convolutional Neural Networks without Any Checkerboard Artifacts
- **Arxiv ID**: http://arxiv.org/abs/1806.02658v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02658v1)
- **Published**: 2018-06-07 13:15:50+00:00
- **Updated**: 2018-06-07 13:15:50+00:00
- **Authors**: Yusuke Sugawara, Sayaka Shiota, Hitoshi Kiya
- **Comment**: To appear in Proc. ICIP2018 October 07-10, 2018, Athens, Greece
- **Journal**: None
- **Summary**: It is well-known that a number of excellent super-resolution (SR) methods using convolutional neural networks (CNNs) generate checkerboard artifacts. A condition to avoid the checkerboard artifacts is proposed in this paper. So far, checkerboard artifacts have been mainly studied for linear multirate systems, but the condition to avoid checkerboard artifacts can not be applied to CNNs due to the non-linearity of CNNs. We extend the avoiding condition for CNNs, and apply the proposed structure to some typical SR methods to confirm the effectiveness of the new scheme. Experiment results demonstrate that the proposed structure can perfectly avoid to generate checkerboard artifacts under two loss conditions: mean square error and perceptual loss, while keeping excellent properties that the SR methods have.



### Probabilistic AND-OR Attribute Grouping for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.02664v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.02664v2)
- **Published**: 2018-06-07 13:25:01+00:00
- **Updated**: 2018-07-07 12:32:48+00:00
- **Authors**: Yuval Atzmon, Gal Chechik
- **Comment**: Accepted to the Conference on Uncertainty in Artificial Intelligence
  (UAI), 2018
- **Journal**: None
- **Summary**: In zero-shot learning (ZSL), a classifier is trained to recognize visual classes without any image samples. Instead, it is given semantic information about the class, like a textual description or a set of attributes. Learning from attributes could benefit from explicitly modeling structure of the attribute space. Unfortunately, learning of general structure from empirical samples is hard with typical dataset sizes.   Here we describe LAGO, a probabilistic model designed to capture natural soft and-or relations across groups of attributes. We show how this model can be learned end-to-end with a deep attribute-detection model. The soft group structure can be learned from data jointly as part of the model, and can also readily incorporate prior knowledge about groups if available. The soft and-or structure succeeds to capture meaningful and predictive structures, improving the accuracy of zero-shot learning on two of three benchmarks.   Finally, LAGO reveals a unified formulation over two ZSL approaches: DAP (Lampert et al., 2009) and ESZSL (Romera-Paredes & Torr, 2015). Interestingly, taking only one singleton group for each attribute, introduces a new soft-relaxation of DAP, that outperforms DAP by ~40.



### Semi-Supervised Learning via Compact Latent Space Clustering
- **Arxiv ID**: http://arxiv.org/abs/1806.02679v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.02679v2)
- **Published**: 2018-06-07 13:41:56+00:00
- **Updated**: 2018-07-29 19:20:19+00:00
- **Authors**: Konstantinos Kamnitsas, Daniel C. Castro, Loic Le Folgoc, Ian Walker, Ryutaro Tanno, Daniel Rueckert, Ben Glocker, Antonio Criminisi, Aditya Nori
- **Comment**: Presented as a long oral in ICML 2018. Post-conference camera ready
- **Journal**: None
- **Summary**: We present a novel cost function for semi-supervised learning of neural networks that encourages compact clustering of the latent space to facilitate separation. The key idea is to dynamically create a graph over embeddings of labeled and unlabeled samples of a training batch to capture underlying structure in feature space, and use label propagation to estimate its high and low density regions. We then devise a cost function based on Markov chains on the graph that regularizes the latent space to form a single compact cluster per class, while avoiding to disturb existing clusters during optimization. We evaluate our approach on three benchmarks and compare to state-of-the art with promising results. Our approach combines the benefits of graph-based regularization with efficient, inductive inference, does not require modifications to a network architecture, and can thus be easily applied to existing networks to enable an effective use of unlabeled data.



### Efficient semantic image segmentation with superpixel pooling
- **Arxiv ID**: http://arxiv.org/abs/1806.02705v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.02705v1)
- **Published**: 2018-06-07 14:38:08+00:00
- **Updated**: 2018-06-07 14:38:08+00:00
- **Authors**: Mathijs Schuurmans, Maxim Berman, Matthew B. Blaschko
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we evaluate the use of superpixel pooling layers in deep network architectures for semantic segmentation. Superpixel pooling is a flexible and efficient replacement for other pooling strategies that incorporates spatial prior information. We propose a simple and efficient GPU-implementation of the layer and explore several designs for the integration of the layer into existing network architectures. We provide experimental results on the IBSR and Cityscapes dataset, demonstrating that superpixel pooling can be leveraged to consistently increase network accuracy with minimal computational overhead. Source code is available at https://github.com/bermanmaxim/superpixPool



### Speaker-Follower Models for Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/1806.02724v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1806.02724v2)
- **Published**: 2018-06-07 15:15:35+00:00
- **Updated**: 2018-10-27 01:38:56+00:00
- **Authors**: Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell
- **Comment**: NIPS 2018
- **Journal**: None
- **Summary**: Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.



### Evaluating surgical skills from kinematic data using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1806.02750v1
- **DOI**: 10.1007/978-3-030-00937-3_25
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02750v1)
- **Published**: 2018-06-07 16:06:10+00:00
- **Updated**: 2018-06-07 16:06:10+00:00
- **Authors**: Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, Pierre-Alain Muller
- **Comment**: Accepted at MICCAI 2018
- **Journal**: None
- **Summary**: The need for automatic surgical skills assessment is increasing, especially because manual feedback from senior surgeons observing junior surgeons is prone to subjectivity and time consuming. Thus, automating surgical skills evaluation is a very important step towards improving surgical practice. In this paper, we designed a Convolutional Neural Network (CNN) to evaluate surgeon skills by extracting patterns in the surgeon motions performed in robotic surgery. The proposed method is validated on the JIGSAWS dataset and achieved very competitive results with 100% accuracy on the suturing and needle passing tasks. While we leveraged from the CNNs efficiency, we also managed to mitigate its black-box effect using class activation map. This feature allows our method to automatically highlight which parts of the surgical task influenced the skill prediction and can be used to explain the classification and to provide personalized feedback to the trainee.



### Model-based active learning to detect isometric deformable objects in the wild with deep architectures
- **Arxiv ID**: http://arxiv.org/abs/1806.02850v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02850v1)
- **Published**: 2018-06-07 18:18:14+00:00
- **Updated**: 2018-06-07 18:18:14+00:00
- **Authors**: Shrinivasan Sankar, Adrien Bartoli
- **Comment**: Accepted in Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: In the recent past, algorithms based on Convolutional Neural Networks (CNNs) have achieved significant milestones in object recognition. With large examples of each object class, standard datasets train well for inter-class variability. However, gathering sufficient data to train for a particular instance of an object within a class is impractical. Furthermore, quantitatively assessing the imaging conditions for each image in a given dataset is not feasible. By generating sufficient images with known imaging conditions, we study to what extent CNNs can cope with hard imaging conditions for instance-level recognition in an active learning regime.   Leveraging powerful rendering techniques to achieve instance-level detection, we present results of training three state-of-the-art object detection algorithms namely, Fast R-CNN, Faster R-CNN and YOLO9000, for hard imaging conditions imposed into the scene by rendering. Our extensive experiments produce a mean Average Precision score of 0.92 on synthetic images and 0.83 on real images using the best performing Faster R-CNN. We show for the first time how well detection algorithms based on deep architectures fare for each hard imaging condition studied.



### In Ictu Oculi: Exposing AI Generated Fake Face Videos by Detecting Eye Blinking
- **Arxiv ID**: http://arxiv.org/abs/1806.02877v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02877v2)
- **Published**: 2018-06-07 19:36:09+00:00
- **Updated**: 2018-06-11 19:28:49+00:00
- **Authors**: Yuezun Li, Ming-Ching Chang, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: The new developments in deep generative networks have significantly improve the quality and efficiency in generating realistically-looking fake face videos. In this work, we describe a new method to expose fake face videos generated with neural networks. Our method is based on detection of eye blinking in the videos, which is a physiological signal that is not well presented in the synthesized fake videos. Our method is tested over benchmarks of eye-blinking detection datasets and also show promising performance on detecting videos generated with DeepFake.



### Correspondence of Deep Neural Networks and the Brain for Visual Textures
- **Arxiv ID**: http://arxiv.org/abs/1806.02888v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.02888v1)
- **Published**: 2018-06-07 20:20:07+00:00
- **Updated**: 2018-06-07 20:20:07+00:00
- **Authors**: Md Nasir Uddin Laskar, Luis G Sanchez Giraldo, Odelia Schwartz
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) trained on objects and scenes have shown intriguing ability to predict some response properties of visual cortical neurons. However, the factors and computations that give rise to such ability, and the role of intermediate processing stages in explaining changes that develop across areas of the cortical hierarchy, are poorly understood. We focused on the sensitivity to textures as a paradigmatic example, since recent neurophysiology experiments provide rich data pointing to texture sensitivity in secondary but not primary visual cortex. We developed a quantitative approach for selecting a subset of the neural unit population from the CNN that best describes the brain neural recordings. We found that the first two layers of the CNN showed qualitative and quantitative correspondence to the cortical data across a number of metrics. This compatibility was reduced for the architecture alone rather than the learned weights, for some other related hierarchical models, and only mildly in the absence of a nonlinear computation akin to local divisive normalization. Our results show that the CNN class of model is effective for capturing changes that develop across early areas of cortex, and has the potential to facilitate understanding of the computations that give rise to hierarchical processing in the brain.



### Real-time coherent diffraction inversion using deep generative networks
- **Arxiv ID**: http://arxiv.org/abs/1806.03992v1
- **DOI**: 10.1038/s41598-018-34525-1
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1806.03992v1)
- **Published**: 2018-06-07 20:33:27+00:00
- **Updated**: 2018-06-07 20:33:27+00:00
- **Authors**: Mathew J. Cherukara, Youssef S. G. Nashed, Ross J. Harder
- **Comment**: None
- **Journal**: Scientific Reports (2018) 8:16520
- **Summary**: Phase retrieval, or the process of recovering phase information in reciprocal space to reconstruct images from measured intensity alone, is the underlying basis to a variety of imaging applications including coherent diffraction imaging (CDI). Typical phase retrieval algorithms are iterative in nature, and hence, are time-consuming and computationally expensive, precluding real-time imaging. Furthermore, iterative phase retrieval algorithms struggle to converge to the correct solution especially in the presence of strong phase structures. In this work, we demonstrate the training and testing of CDI NN, a pair of deep deconvolutional networks trained to predict structure and phase in real space of a 2D object from its corresponding far-field diffraction intensities alone. Once trained, CDI NN can invert a diffraction pattern to an image within a few milliseconds of compute time on a standard desktop machine, opening the door to real-time imaging.



### Revisiting the Importance of Individual Units in CNNs via Ablation
- **Arxiv ID**: http://arxiv.org/abs/1806.02891v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.02891v1)
- **Published**: 2018-06-07 20:40:56+00:00
- **Updated**: 2018-06-07 20:40:56+00:00
- **Authors**: Bolei Zhou, Yiyou Sun, David Bau, Antonio Torralba
- **Comment**: None
- **Journal**: None
- **Summary**: We revisit the importance of the individual units in Convolutional Neural Networks (CNNs) for visual recognition. By conducting unit ablation experiments on CNNs trained on large scale image datasets, we demonstrate that, though ablating any individual unit does not hurt overall classification accuracy, it does lead to significant damage on the accuracy of specific classes. This result shows that an individual unit is specialized to encode information relevant to a subset of classes. We compute the correlation between the accuracy drop under unit ablation and various attributes of an individual unit such as class selectivity and weight L1 norm. We confirm that unit attributes such as class selectivity are a poor predictor for impact on overall accuracy as found previously in recent work \cite{morcos2018importance}. However, our results show that class selectivity along with other attributes are good predictors of the importance of one unit to individual classes. We evaluate the impact of random rotation, batch normalization, and dropout to the importance of units to specific classes. Our results show that units with high selectivity play an important role in network classification power at the individual class level. Understanding and interpreting the behavior of these units is necessary and meaningful.



### Training Faster by Separating Modes of Variation in Batch-normalized Models
- **Arxiv ID**: http://arxiv.org/abs/1806.02892v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.02892v2)
- **Published**: 2018-06-07 20:41:09+00:00
- **Updated**: 2018-11-14 16:09:03+00:00
- **Authors**: Mahdi M. Kalayeh, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Batch Normalization (BN) is essential to effectively train state-of-the-art deep Convolutional Neural Networks (CNN). It normalizes inputs to the layers during training using the statistics of each mini-batch. In this work, we study BN from the viewpoint of Fisher kernels. We show that assuming samples within a mini-batch are from the same probability density function, then BN is identical to the Fisher vector of a Gaussian distribution. That means BN can be explained in terms of kernels that naturally emerge from the probability density function of the underlying data distribution. However, given the rectifying non-linearities employed in CNN architectures, distribution of inputs to the layers show heavy tail and asymmetric characteristics. Therefore, we propose approximating underlying data distribution not with one, but a mixture of Gaussian densities. Deriving Fisher vector for a Gaussian Mixture Model (GMM), reveals that BN can be improved by independently normalizing with respect to the statistics of disentangled sub-populations. We refer to our proposed soft piecewise version of BN as Mixture Normalization (MN). Through extensive set of experiments on CIFAR-10 and CIFAR-100, we show that MN not only effectively accelerates training image classification and Generative Adversarial networks, but also reaches higher quality models.



### Copy Move Forgery using Hus Invariant Moments and Log Polar Transformations
- **Arxiv ID**: http://arxiv.org/abs/1806.02907v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02907v1)
- **Published**: 2018-06-07 21:22:36+00:00
- **Updated**: 2018-06-07 21:22:36+00:00
- **Authors**: Tejas K, Swathi C, Rajesh Kumar M
- **Comment**: This paper was submitted, accepted and presented in the 3rd
  International Conference on RTEICT, IEEE Conference
- **Journal**: None
- **Summary**: With the increase in interchange of data, there is a growing necessity of security. Considering the volumes of digital data that is transmitted, they are in need to be secure. Among the many forms of tampering possible, one widespread technique is Copy Move Forgery CMF. This forgery occurs when parts of the image are copied and duplicated elsewhere in the same image. There exist a number of algorithms to detect such a forgery in which the primary step involved is feature extraction. The feature extraction techniques employed must have lesser time and space complexity involved for an efficient and faster processing of media. Also, majority of the existing state of art techniques often tend to falsely match similar genuine objects as copy move forged during the detection process. To tackle these problems, the paper proposes a novel algorithm that recognizes a unique approach of using Hus Invariant Moments and Log polar Transformations to reduce feature vector dimension to one feature per block simultaneously detecting CMF among genuine similar objects in an image. The qualitative and quantitative results obtained demonstrate the effectiveness of this algorithm.



### CapsGAN: Using Dynamic Routing for Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.03968v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.03968v1)
- **Published**: 2018-06-07 21:33:46+00:00
- **Updated**: 2018-06-07 21:33:46+00:00
- **Authors**: Raeid Saqur, Sal Vivona
- **Comment**: Draft Submission
- **Journal**: None
- **Summary**: In this paper, we propose a novel technique for generating images in the 3D domain from images with high degree of geometrical transformations. By coalescing two popular concurrent methods that have seen rapid ascension to the machine learning zeitgeist in recent years: GANs (Goodfellow et. al.) and Capsule networks (Sabour, Hinton et. al.) - we present: \textbf{CapsGAN}. We show that CapsGAN performs better than or equal to traditional CNN based GANs in generating images with high geometric transformations using rotated MNIST. In the process, we also show the efficacy of using capsules architecture in the GANs domain. Furthermore, we tackle the Gordian Knot in training GANs - the performance control and training stability by experimenting with using Wasserstein distance (gradient clipping, penalty) and Spectral Normalization. The experimental findings of this paper should propel the application of capsules and GANs in the still exciting and nascent domain of 3D image generation, and plausibly video (frame) generation.



### Color Sails: Discrete-Continuous Palettes for Deep Color Exploration
- **Arxiv ID**: http://arxiv.org/abs/1806.02918v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.02918v1)
- **Published**: 2018-06-07 22:42:00+00:00
- **Updated**: 2018-06-07 22:42:00+00:00
- **Authors**: Maria Shugrina, Amlan Kar, Karan Singh, Sanja Fidler
- **Comment**: 14 pages, 13 figures
- **Journal**: None
- **Summary**: We present color sails, a discrete-continuous color gamut representation that extends the color gradient analogy to three dimensions and allows interactive control of the color blending behavior. Our representation models a wide variety of color distributions in a compact manner, and lends itself to applications such as color exploration for graphic design, illustration and similar fields. We propose a Neural Network that can fit a color sail to any image. Then, the user can adjust color sail parameters to change the base colors, their blending behavior and the number of colors, exploring a wide range of options for the original design. In addition, we propose a Deep Learning model that learns to automatically segment an image into color-compatible alpha masks, each equipped with its own color sail. This allows targeted color exploration by either editing their corresponding color sails or using standard software packages. Our model is trained on a custom diverse dataset of art and design. We provide both quantitative evaluations, and a user study, demonstrating the effectiveness of color sail interaction. Interactive demos are available at www.colorsails.com.



### Non-Local Recurrent Network for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1806.02919v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.02919v2)
- **Published**: 2018-06-07 22:50:49+00:00
- **Updated**: 2018-12-11 05:44:37+00:00
- **Authors**: Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, Thomas S. Huang
- **Comment**: NIPS 2018
- **Journal**: None
- **Summary**: Many classic methods have shown non-local self-similarity in natural images to be an effective prior for image restoration. However, it remains unclear and challenging to make use of this intrinsic property via deep networks. In this paper, we propose a non-local recurrent network (NLRN) as the first attempt to incorporate non-local operations into a recurrent neural network (RNN) for image restoration. The main contributions of this work are: (1) Unlike existing methods that measure self-similarity in an isolated manner, the proposed non-local module can be flexibly integrated into existing deep networks for end-to-end training to capture deep feature correlation between each location and its neighborhood. (2) We fully employ the RNN structure for its parameter efficiency and allow deep feature correlation to be propagated along adjacent recurrent states. This new design boosts robustness against inaccurate correlation estimation due to severely degraded images. (3) We show that it is essential to maintain a confined neighborhood for computing deep feature correlation given degraded images. This is in contrast to existing practice that deploys the whole image. Extensive experiments on both image denoising and super-resolution tasks are conducted. Thanks to the recurrent non-local operations and correlation propagation, the proposed NLRN achieves superior results to state-of-the-art methods with much fewer parameters.



