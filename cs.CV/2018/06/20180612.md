# Arxiv Papers in cs.CV on 2018-06-12
### iParaphrasing: Extracting Visually Grounded Paraphrases via an Image
- **Arxiv ID**: http://arxiv.org/abs/1806.04284v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1806.04284v1)
- **Published**: 2018-06-12 00:58:59+00:00
- **Updated**: 2018-06-12 00:58:59+00:00
- **Authors**: Chenhui Chu, Mayu Otani, Yuta Nakashima
- **Comment**: COLING 2018
- **Journal**: None
- **Summary**: A paraphrase is a restatement of the meaning of a text in other words. Paraphrases have been studied to enhance the performance of many natural language processing tasks. In this paper, we propose a novel task iParaphrasing to extract visually grounded paraphrases (VGPs), which are different phrasal expressions describing the same visual concept in an image. These extracted VGPs have the potential to improve language and image multimodal tasks such as visual question answering and image captioning. How to model the similarity between VGPs is the key of iParaphrasing. We apply various existing methods as well as propose a novel neural network-based method with image attention, and report the results of the first attempt toward iParaphrasing.



### 3D Pose Estimation for Fine-Grained Object Categories
- **Arxiv ID**: http://arxiv.org/abs/1806.04314v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04314v3)
- **Published**: 2018-06-12 03:44:54+00:00
- **Updated**: 2018-11-07 06:27:34+00:00
- **Authors**: Yaming Wang, Xiao Tan, Yi Yang, Xiao Liu, Errui Ding, Feng Zhou, Larry S. Davis
- **Comment**: 4th International Workshop on Recovering 6D Object Pose (ECCVW 2018).
  arXiv admin note: text overlap with arXiv:1810.09263
- **Journal**: None
- **Summary**: Existing object pose estimation datasets are related to generic object types and there is so far no dataset for fine-grained object categories. In this work, we introduce a new large dataset to benchmark pose estimation for fine-grained objects, thanks to the availability of both 2D and 3D fine-grained data recently. Specifically, we augment two popular fine-grained recognition datasets (StanfordCars and CompCars) by finding a fine-grained 3D CAD model for each sub-category and manually annotating each object in images with 3D pose. We show that, with enough training data, a full perspective model with continuous parameters can be estimated using 2D appearance information alone. We achieve this via a framework based on Faster/Mask R-CNN. This goes beyond previous works on category-level pose estimation, which only estimate discrete/continuous viewpoint angles or recover rotation matrices often with the help of key points. Furthermore, with fine-grained 3D models available, we incorporate a dense 3D representation named as location field into the CNN-based pose estimation framework to further improve the performance. The new dataset is available at www.umiacs.umd.edu/~wym/3dpose.html



### Sparse, Collaborative, or Nonnegative Representation: Which Helps Pattern Classification?
- **Arxiv ID**: http://arxiv.org/abs/1806.04329v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04329v2)
- **Published**: 2018-06-12 04:38:40+00:00
- **Updated**: 2018-12-05 06:16:48+00:00
- **Authors**: Jun Xu, Wangpeng An, Lei Zhang, David Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The use of sparse representation (SR) and collaborative representation (CR) for pattern classification has been widely studied in tasks such as face recognition and object categorization. Despite the success of SR/CR based classifiers, it is still arguable whether it is the $\ell_{1}$-norm sparsity or the $\ell_{2}$-norm collaborative property that brings the success of SR/CR based classification. In this paper, we investigate the use of nonnegative representation (NR) for pattern classification, which is largely ignored by previous work. Our analyses reveal that NR can boost the representation power of homogeneous samples while limiting the representation power of heterogeneous samples, making the representation sparse and discriminative simultaneously and thus providing a more effective solution to representation based classification than SR/CR. Our experiments demonstrate that the proposed NR based classifier (NRC) outperforms previous representation based classifiers. With deep features as inputs, it also achieves state-of-the-art performance on various visual classification tasks.



### Automatic Ship Detection of Remote Sensing Images from Google Earth in Complex Scenes Based on Multi-Scale Rotation Dense Feature Pyramid Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.04331v1
- **DOI**: 10.3390/rs10010132
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04331v1)
- **Published**: 2018-06-12 04:51:36+00:00
- **Updated**: 2018-06-12 04:51:36+00:00
- **Authors**: Xue Yang, Hao Sun, Kun Fu, Jirui Yang, Xian Sun, Menglong Yan, Zhi Guo
- **Comment**: 14 pages, 11 figures
- **Journal**: Remote Sens. 2018, 10, 132
- **Summary**: Ship detection has been playing a significant role in the field of remote sensing for a long time but it is still full of challenges. The main limitations of traditional ship detection methods usually lie in the complexity of application scenarios, the difficulty of intensive object detection and the redundancy of detection region. In order to solve such problems above, we propose a framework called Rotation Dense Feature Pyramid Networks (R-DFPN) which can effectively detect ship in different scenes including ocean and port. Specifically, we put forward the Dense Feature Pyramid Network (DFPN), which is aimed at solving the problem resulted from the narrow width of the ship. Compared with previous multi-scale detectors such as Feature Pyramid Network (FPN), DFPN builds the high-level semantic feature-maps for all scales by means of dense connections, through which enhances the feature propagation and encourages the feature reuse. Additionally, in the case of ship rotation and dense arrangement, we design a rotation anchor strategy to predict the minimum circumscribed rectangle of the object so as to reduce the redundant detection region and improve the recall. Furthermore, we also propose multi-scale ROI Align for the purpose of maintaining the completeness of semantic and spatial information. Experiments based on remote sensing images from Google Earth for ship detection show that our detection method based on R-DFPN representation has a state-of-the-art performance.



### A Graph Transduction Game for Multi-target Tracking
- **Arxiv ID**: http://arxiv.org/abs/1806.07227v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GT
- **Links**: [PDF](http://arxiv.org/pdf/1806.07227v2)
- **Published**: 2018-06-12 05:47:35+00:00
- **Updated**: 2018-06-25 01:54:54+00:00
- **Authors**: Tewodros Mulugeta Dagnew, Dalia Coppi, Marcello Pelillo, Rita Cucchiara
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised learning is a popular class of techniques to learn from labeled and unlabeled data. The paper proposes an application of a recently proposed approach of graph transduction that exploits game theoretic notions to the problem of multiple people tracking. Within the proposed framework, targets are considered as players of a multi-player non-cooperative game. The equilibria of the game is considered as a consistent labeling solution and thus an estimation of the target association in the sequence of frames. Patches of persons are extracted from the video frames using a HOG based detector and their similarity is modeled using distances among their covariance matrices. The solution we propose achieves satisfactory results on video surveillance datasets. The experiments show the robustness of the method even with a heavy unbalance between the number of labeled and unlabeled input patches.



### MSplit LBI: Realizing Feature Selection and Dense Estimation Simultaneously in Few-shot and Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.04360v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04360v1)
- **Published**: 2018-06-12 07:07:44+00:00
- **Updated**: 2018-06-12 07:07:44+00:00
- **Authors**: Bo Zhao, Xinwei Sun, Yanwei Fu, Yuan Yao, Yizhou Wang
- **Comment**: Accepted by the 35th International Conference on Machine Learning
- **Journal**: International Conference on Machine Learning 2018
- **Summary**: It is one typical and general topic of learning a good embedding model to efficiently learn the representation coefficients between two spaces/subspaces. To solve this task, $L_{1}$ regularization is widely used for the pursuit of feature selection and avoiding overfitting, and yet the sparse estimation of features in $L_{1}$ regularization may cause the underfitting of training data. $L_{2}$ regularization is also frequently used, but it is a biased estimator. In this paper, we propose the idea that the features consist of three orthogonal parts, \emph{namely} sparse strong signals, dense weak signals and random noise, in which both strong and weak signals contribute to the fitting of data. To facilitate such novel decomposition, \emph{MSplit} LBI is for the first time proposed to realize feature selection and dense estimation simultaneously. We provide theoretical and simulational verification that our method exceeds $L_{1}$ and $L_{2}$ regularization, and extensive experimental results show that our method achieves state-of-the-art performance in the few-shot and zero-shot learning.



### Initialize globally before acting locally: Enabling Landmark-free 3D US to MRI Registration
- **Arxiv ID**: http://arxiv.org/abs/1806.04368v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.04368v1)
- **Published**: 2018-06-12 07:27:53+00:00
- **Updated**: 2018-06-12 07:27:53+00:00
- **Authors**: Julia Rackerseder, Maximilian Baust, Rüdiger Göbl, Nassir Navab, Christoph Hennersperger
- **Comment**: This is a pre-print of an article published in the Proceedings of the
  21st International Conference on Medical Image Computing and Computer
  Assisted Interventions (MICCAI), Granada, Spain, September 2018
- **Journal**: None
- **Summary**: Registration of partial-view 3D US volumes with MRI data is influenced by initialization. The standard of practice is using extrinsic or intrinsic landmarks, which can be very tedious to obtain. To overcome the limitations of registration initialization, we present a novel approach that is based on Euclidean distance maps derived from easily obtainable coarse segmentations. We evaluate our approach quantitatively on the publicly available RESECT dataset and show that it is robust regarding overlap of target area and initial position. Furthermore, our method provides initializations that are suitable for state-of-the-art nonlinear, deformable image registration algorithm's capture ranges.



### DeepTerramechanics: Terrain Classification and Slip Estimation for Ground Robots via Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.07379v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1806.07379v1)
- **Published**: 2018-06-12 07:29:25+00:00
- **Updated**: 2018-06-12 07:29:25+00:00
- **Authors**: Ramon Gonzalez, Karl Iagnemma
- **Comment**: 22 pages, 23 figures
- **Journal**: None
- **Summary**: Terramechanics plays a critical role in the areas of ground vehicles and ground mobile robots since understanding and estimating the variables influencing the vehicle-terrain interaction may mean the success or the failure of an entire mission. This research applies state-of-the-art algorithms in deep learning to two key problems: estimating wheel slip and classifying the terrain being traversed by a ground robot. Three data sets collected by ground robotic platforms (MIT single-wheel testbed, MSL Curiosity rover, and tracked robot Fitorobot) are employed in order to compare the performance of traditional machine learning methods (i.e. Support Vector Machine (SVM) and Multi-layer Perceptron (MLP)) against Deep Neural Networks (DNNs) and Convolutional Neural Networks (CNNs). This work also shows the impact that certain tuning parameters and the network architecture (MLP, DNN and CNN) play on the performance of those methods. This paper also contributes a deep discussion with the lessons learned in the implementation of DNNs and CNNs and how these methods can be extended to solve other problems.



### Fast Rotational Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/1806.04374v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.04374v2)
- **Published**: 2018-06-12 07:49:42+00:00
- **Updated**: 2020-01-29 19:00:46+00:00
- **Authors**: Michael T. McCann, Vincent Andrearczyk, Michael Unser, Adrien Depeursinge
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: We propose an algorithm for rotational sparse coding along with an efficient implementation using steerability. Sparse coding (also called dictionary learning) is an important technique in image processing, useful in inverse problems, compression, and analysis; however, the usual formulation fails to capture an important aspect of the structure of images: images are formed from building blocks, e.g., edges, lines, or points, that appear at different locations, orientations, and scales. The sparse coding problem can be reformulated to explicitly account for these transforms, at the cost of increased computation. In this work, we propose an algorithm for a rotational version of sparse coding that is based on K-SVD with additional rotation operations. We then propose a method to accelerate these rotations by learning the dictionary in a steerable basis. Our experiments on patch coding and texture classification demonstrate that the proposed algorithm is fast enough for practical use and compares favorably to standard sparse coding.



### Qiniu Submission to ActivityNet Challenge 2018
- **Arxiv ID**: http://arxiv.org/abs/1806.04391v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04391v1)
- **Published**: 2018-06-12 08:42:55+00:00
- **Updated**: 2018-06-12 08:42:55+00:00
- **Authors**: Xiaoteng Zhang, Yixin Bao, Feiyun Zhang, Kai Hu, Yicheng Wang, Liang Zhu, Qinzhu He, Yining Lin, Jie Shao, Yao Peng
- **Comment**: 4 pages, 3 figures, CVPR workshop
- **Journal**: None
- **Summary**: In this paper, we introduce our submissions for the tasks of trimmed activity recognition (Kinetics) and trimmed event recognition (Moments in Time) for Activitynet Challenge 2018. In the two tasks, non-local neural networks and temporal segment networks are implemented as our base models. Multi-modal cues such as RGB image, optical flow and acoustic signal have also been used in our method. We also propose new non-local-based models for further improvement on the recognition accuracy. The final submissions after ensembling the models achieve 83.5% top-1 accuracy and 96.8% top-5 accuracy on the Kinetics validation set, 35.81% top-1 accuracy and 62.59% top-5 accuracy on the MIT validation set.



### Enhancing clinical MRI Perfusion maps with data-driven maps of complementary nature for lesion outcome prediction
- **Arxiv ID**: http://arxiv.org/abs/1806.04413v1
- **DOI**: 10.1007/978-3-030-00931-1_13
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04413v1)
- **Published**: 2018-06-12 09:34:20+00:00
- **Updated**: 2018-06-12 09:34:20+00:00
- **Authors**: Adriano Pinto, Sergio Pereira, Raphael Meier, Victor Alves, Roland Wiest, Carlos A. Silva, Mauricio Reyes
- **Comment**: Accepted at MICCAI 2018
- **Journal**: None
- **Summary**: Stroke is the second most common cause of death in developed countries, where rapid clinical intervention can have a major impact on a patient's life. To perform the revascularization procedure, the decision making of physicians considers its risks and benefits based on multi-modal MRI and clinical experience. Therefore, automatic prediction of the ischemic stroke lesion outcome has the potential to assist the physician towards a better stroke assessment and information about tissue outcome. Typically, automatic methods consider the information of the standard kinetic models of diffusion and perfusion MRI (e.g. Tmax, TTP, MTT, rCBF, rCBV) to perform lesion outcome prediction. In this work, we propose a deep learning method to fuse this information with an automated data selection of the raw 4D PWI image information, followed by a data-driven deep-learning modeling of the underlying blood flow hemodynamics. We demonstrate the ability of the proposed approach to improve prediction of tissue at risk before therapy, as compared to only using the standard clinical perfusion maps, hence suggesting on the potential benefits of the proposed data-driven raw perfusion data modelling approach.



### Sample Dropout for Audio Scene Classification Using Multi-Scale Dense Connected Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1806.04422v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04422v1)
- **Published**: 2018-06-12 09:59:11+00:00
- **Updated**: 2018-06-12 09:59:11+00:00
- **Authors**: Dawei Feng, Kele Xu, Haibo Mi, Feifan Liao, Yan Zhou
- **Comment**: Accepted to 2018 Pacific Rim Knowledge Acquisition Workshop (PKAW)
- **Journal**: None
- **Summary**: Acoustic scene classification is an intricate problem for a machine. As an emerging field of research, deep Convolutional Neural Networks (CNN) achieve convincing results. In this paper, we explore the use of multi-scale Dense connected convolutional neural network (DenseNet) for the classification task, with the goal to improve the classification performance as multi-scale features can be extracted from the time-frequency representation of the audio signal. On the other hand, most of previous CNN-based audio scene classification approaches aim to improve the classification accuracy, by employing different regularization techniques, such as the dropout of hidden units and data augmentation, to reduce overfitting. It is widely known that outliers in the training set have a high negative influence on the trained model, and culling the outliers may improve the classification performance, while it is often under-explored in previous studies. In this paper, inspired by the silence removal in the speech signal processing, a novel sample dropout approach is proposed, which aims to remove outliers in the training dataset. Using the DCASE 2017 audio scene classification datasets, the experimental results demonstrates the proposed multi-scale DenseNet providing a superior performance than the traditional single-scale DenseNet, while the sample dropout method can further improve the classification robustness of multi-scale DenseNet.



### U-SegNet: Fully Convolutional Neural Network based Automated Brain tissue segmentation Tool
- **Arxiv ID**: http://arxiv.org/abs/1806.04429v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04429v1)
- **Published**: 2018-06-12 10:21:32+00:00
- **Updated**: 2018-06-12 10:21:32+00:00
- **Authors**: Pulkit Kumar, Pravin Nagar, Chetan Arora, Anubha Gupta
- **Comment**: Accepted in ICIP, 2018
- **Journal**: None
- **Summary**: Automated brain tissue segmentation into white matter (WM), gray matter (GM), and cerebro-spinal fluid (CSF) from magnetic resonance images (MRI) is helpful in the diagnosis of neuro-disorders such as epilepsy, Alzheimer's, multiple sclerosis, etc. However, thin GM structures at the periphery of cortex and smooth transitions on tissue boundaries such as between GM and WM, or WM and CSF pose difficulty in building a reliable segmentation tool. This paper proposes a Fully Convolutional Neural Network (FCN) tool, that is a hybrid of two widely used deep learning segmentation architectures SegNet and U-Net, for improved brain tissue segmentation. We propose a skip connection inspired from U-Net, in the SegNet architetcure, to incorporate fine multiscale information for better tissue boundary identification. We show that the proposed U-SegNet architecture, improves segmentation performance, as measured by average dice ratio, to 89.74% on the widely used IBSR dataset consisting of T-1 weighted MRI volumes of 18 subjects.



### The Unusual Effectiveness of Averaging in GAN Training
- **Arxiv ID**: http://arxiv.org/abs/1806.04498v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.04498v2)
- **Published**: 2018-06-12 13:27:23+00:00
- **Updated**: 2019-02-26 12:17:11+00:00
- **Authors**: Yasin Yazıcı, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, Georgios Piliouras, Vijay Chandrasekhar
- **Comment**: Published as a conference paper at ICLR 2019
- **Journal**: None
- **Summary**: We examine two different techniques for parameter averaging in GAN training. Moving Average (MA) computes the time-average of parameters, whereas Exponential Moving Average (EMA) computes an exponentially discounted sum. Whilst MA is known to lead to convergence in bilinear settings, we provide the -- to our knowledge -- first theoretical arguments in support of EMA. We show that EMA converges to limit cycles around the equilibrium with vanishing amplitude as the discount parameter approaches one for simple bilinear games and also enhances the stability of general GAN training. We establish experimentally that both techniques are strikingly effective in the non-convex-concave GAN setting as well. Both improve inception and FID scores on different architectures and for different GAN objectives. We provide comprehensive experimental results across a range of datasets -- mixture of Gaussians, CIFAR-10, STL-10, CelebA and ImageNet -- to demonstrate its effectiveness. We achieve state-of-the-art results on CIFAR-10 and produce clean CelebA face images.\footnote{~The code is available at \url{https://github.com/yasinyazici/EMA_GAN}}



### Learning Deep Similarity Metric for 3D MR-TRUS Registration
- **Arxiv ID**: http://arxiv.org/abs/1806.04548v2
- **DOI**: 10.1007/s11548-018-1875-7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04548v2)
- **Published**: 2018-06-12 14:13:00+00:00
- **Updated**: 2018-10-15 13:50:43+00:00
- **Authors**: Grant Haskins, Jochen Kruecker, Uwe Kruger, Sheng Xu, Peter A. Pinto, Brad J. Wood, Pingkun Yan
- **Comment**: To appear on IJCARS
- **Journal**: None
- **Summary**: Purpose: The fusion of transrectal ultrasound (TRUS) and magnetic resonance (MR) images for guiding targeted prostate biopsy has significantly improved the biopsy yield of aggressive cancers. A key component of MR-TRUS fusion is image registration. However, it is very challenging to obtain a robust automatic MR-TRUS registration due to the large appearance difference between the two imaging modalities. The work presented in this paper aims to tackle this problem by addressing two challenges: (i) the definition of a suitable similarity metric and (ii) the determination of a suitable optimization strategy.   Methods: This work proposes the use of a deep convolutional neural network to learn a similarity metric for MR-TRUS registration. We also use a composite optimization strategy that explores the solution space in order to search for a suitable initialization for the second-order optimization of the learned metric. Further, a multi-pass approach is used in order to smooth the metric for optimization.   Results: The learned similarity metric outperforms the classical mutual information and also the state-of-the-art MIND feature based methods. The results indicate that the overall registration framework has a large capture range. The proposed deep similarity metric based approach obtained a mean TRE of 3.86mm (with an initial TRE of 16mm) for this challenging problem.   Conclusion: A similarity metric that is learned using a deep neural network can be used to assess the quality of any given image registration and can be used in conjunction with the aforementioned optimization framework to perform automatic registration that is robust to poor initialization.



### Combining Model-Free Q-Ensembles and Model-Based Approaches for Informed Exploration
- **Arxiv ID**: http://arxiv.org/abs/1806.04552v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.04552v1)
- **Published**: 2018-06-12 14:24:02+00:00
- **Updated**: 2018-06-12 14:24:02+00:00
- **Authors**: Sreecharan Sankaranarayanan, Raghuram Mandyam Annasamy, Katia Sycara, Carolyn Penstein Rosé
- **Comment**: Submitted to the Thirty-Second Annual Conference on Neural
  Information Processing Systems (NIPS 2018)
- **Journal**: None
- **Summary**: Q-Ensembles are a model-free approach where input images are fed into different Q-networks and exploration is driven by the assumption that uncertainty is proportional to the variance of the output Q-values obtained. They have been shown to perform relatively well compared to other exploration strategies. Further, model-based approaches, such as encoder-decoder models have been used successfully for next frame prediction given previous frames. This paper proposes to integrate the model-free Q-ensembles and model-based approaches with the hope of compounding the benefits of both and achieving superior exploration as a result. Results show that a model-based trajectory memory approach when combined with Q-ensembles produces superior performance when compared to only using Q-ensembles.



### An Extension of Averaged-Operator-Based Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1806.04561v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.04561v1)
- **Published**: 2018-06-12 14:35:40+00:00
- **Updated**: 2018-06-12 14:35:40+00:00
- **Authors**: Miguel Simões, José Bioucas-Dias, Luis B. Almeida
- **Comment**: 26th Eur. Signal Process. Conf. (EUSIPCO 2018), accepted. 5 pages, 1
  figure
- **Journal**: None
- **Summary**: Many of the algorithms used to solve minimization problems with sparsity-inducing regularizers are generic in the sense that they do not take into account the sparsity of the solution in any particular way. However, algorithms known as semismooth Newton are able to take advantage of this sparsity to accelerate their convergence. We show how to extend these algorithms in different directions, and study the convergence of the resulting algorithms by showing that they are a particular case of an extension of the well-known Krasnosel'ski\u{\i}--Mann scheme.



### Detection of Premature Ventricular Contractions Using Densely Connected Deep Convolutional Neural Network with Spatial Pyramid Pooling Layer
- **Arxiv ID**: http://arxiv.org/abs/1806.04564v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04564v7)
- **Published**: 2018-06-12 14:42:35+00:00
- **Updated**: 2019-10-10 16:35:24+00:00
- **Authors**: Jianning Li
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Premature ventricular contraction(PVC) is a type of premature ectopic beat originating from the ventricles. Automatic method for accurate and robust detection of PVC is highly clinically desired.Currently, most of these methods are developed and tested using the same database divided into training and testing set and their generalization performance across databases has not been fully validated. In this paper, a method based on densely connected convolutional neural network and spatial pyramid pooling is proposed for PVC detection which can take arbitrarily-sized QRS complexes as input both in training and testing. With a much less complicated and more straightforward architecture,the proposed network achieves comparable results to current state-of-the-art deep learning based method with regard to accuracy,sensitivity and specificity by training and testing using the MIT-BIH arrhythmia database as benchmark.Besides the benchmark database,QRS complexes are extracted from four more open databases namely the St-Petersburg Institute of Cardiological Technics 12-lead Arrhythmia Database,The MIT-BIH Normal Sinus Rhythm Database,The MIT-BIH Long Term Database and European ST-T Database. The extracted QRS complexes are different in length and sampling rate among the five databases.Cross-database training and testing is also experimented.The performance of the network shows an improvement on the benchmark database according to the result demonstrating the advantage of using multiple databases for training over using only a single database.The network also achieves satisfactory scores on the other four databases showing good generalization capability.



### Multiview Two-Task Recursive Attention Model for Left Atrium and Atrial Scars Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.04597v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1806.04597v1)
- **Published**: 2018-06-12 15:16:32+00:00
- **Updated**: 2018-06-12 15:16:32+00:00
- **Authors**: Jun Chen, Guang Yang, Zhifan Gao, Hao Ni, Elsa Angelini, Raad Mohiaddin, Tom Wong, Yanping Zhang, Xiuquan Du, Heye Zhang, Jennifer Keegan, David Firmin
- **Comment**: 8 pages, 4 figures, accepted by MICCAI 2018
- **Journal**: None
- **Summary**: Late Gadolinium Enhanced Cardiac MRI (LGE-CMRI) for detecting atrial scars in atrial fibrillation (AF) patients has recently emerged as a promising technique to stratify patients, guide ablation therapy and predict treatment success. Visualisation and quantification of scar tissues require a segmentation of both the left atrium (LA) and the high intensity scar regions from LGE-CMRI images. These two segmentation tasks are challenging due to the cancelling of healthy tissue signal, low signal-to-noise ratio and often limited image quality in these patients. Most approaches require manual supervision and/or a second bright-blood MRI acquisition for anatomical segmentation. Segmenting both the LA anatomy and the scar tissues automatically from a single LGE-CMRI acquisition is highly in demand. In this study, we proposed a novel fully automated multiview two-task (MVTT) recursive attention model working directly on LGE-CMRI images that combines a sequential learning and a dilated residual learning to segment the LA (including attached pulmonary veins) and delineate the atrial scars simultaneously via an innovative attention model. Compared to other state-of-the-art methods, the proposed MVTT achieves compelling improvement, enabling to generate a patient-specific anatomical and atrial scar assessment model.



### Knowledge Distillation by On-the-Fly Native Ensemble
- **Arxiv ID**: http://arxiv.org/abs/1806.04606v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04606v2)
- **Published**: 2018-06-12 15:28:53+00:00
- **Updated**: 2018-09-08 22:27:55+00:00
- **Authors**: Xu Lan, Xiatian Zhu, Shaogang Gong
- **Comment**: To appear in NIPS2018
- **Journal**: None
- **Summary**: Knowledge distillation is effective to train small and generalisable network models for meeting the low-memory and fast running requirements. Existing offline distillation methods rely on a strong pre-trained teacher, which enables favourable knowledge discovery and transfer but requires a complex two-phase training procedure. Online counterparts address this limitation at the price of lacking a highcapacity teacher. In this work, we present an On-the-fly Native Ensemble (ONE) strategy for one-stage online distillation. Specifically, ONE trains only a single multi-branch network while simultaneously establishing a strong teacher on-the- fly to enhance the learning of target network. Extensive evaluations show that ONE improves the generalisation performance a variety of deep neural networks more significantly than alternative methods on four image classification dataset: CIFAR10, CIFAR100, SVHN, and ImageNet, whilst having the computational efficiency advantages.



### Imperfect Segmentation Labels: How Much Do They Matter?
- **Arxiv ID**: http://arxiv.org/abs/1806.04618v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04618v3)
- **Published**: 2018-06-12 15:54:42+00:00
- **Updated**: 2018-09-24 03:10:53+00:00
- **Authors**: Nicholas Heller, Joshua Dean, Nikolaos Papanikolopoulos
- **Comment**: 9 pages, 3 figures, Accepted at MICCAI LABELS 2018
- **Journal**: None
- **Summary**: Labeled datasets for semantic segmentation are imperfect, especially in medical imaging where borders are often subtle or ill-defined. Little work has been done to analyze the effect that label errors have on the performance of segmentation methodologies. Here we present a large-scale study of model performance in the presence of varying types and degrees of error in training data. We trained U-Net, SegNet, and FCN32 several times for liver segmentation with 10 different modes of ground-truth perturbation. Our results show that for each architecture, performance steadily declines with boundary-localized errors, however, U-Net was significantly more robust to jagged boundary errors than the other architectures. We also found that each architecture was very robust to non-boundary-localized errors, suggesting that boundary-localized errors are fundamentally different and more challenging problem than random label errors in a classification setting.



### Fast forwarding Egocentric Videos by Listening and Watching
- **Arxiv ID**: http://arxiv.org/abs/1806.04620v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04620v1)
- **Published**: 2018-06-12 15:58:53+00:00
- **Updated**: 2018-06-12 15:58:53+00:00
- **Authors**: Vinicius S. Furlan, Ruzena Bajcsy, Erickson R. Nascimento
- **Comment**: None
- **Journal**: None
- **Summary**: The remarkable technological advance in well-equipped wearable devices is pushing an increasing production of long first-person videos. However, since most of these videos have long and tedious parts, they are forgotten or never seen. Despite a large number of techniques proposed to fast-forward these videos by highlighting relevant moments, most of them are image based only. Most of these techniques disregard other relevant sensors present in the current devices such as high-definition microphones. In this work, we propose a new approach to fast-forward videos using psychoacoustic metrics extracted from the soundtrack. These metrics can be used to estimate the annoyance of a segment allowing our method to emphasize moments of sound pleasantness. The efficiency of our method is demonstrated through qualitative results and quantitative results as far as of speed-up and instability are concerned.



### Adversarial Attacks on Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1806.04646v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1806.04646v1)
- **Published**: 2018-06-12 16:59:14+00:00
- **Updated**: 2018-06-12 16:59:14+00:00
- **Authors**: George Gondim-Ribeiro, Pedro Tabacof, Eduardo Valle
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial attacks are malicious inputs that derail machine-learning models. We propose a scheme to attack autoencoders, as well as a quantitative evaluation framework that correlates well with the qualitative assessment of the attacks. We assess --- with statistically validated experiments --- the resistance to attacks of three variational autoencoders (simple, convolutional, and DRAW) in three datasets (MNIST, SVHN, CelebA), showing that both DRAW's recurrence and attention mechanism lead to better resistance. As autoencoders are proposed for compressing data --- a scenario in which their safety is paramount --- we expect more attention will be given to adversarial attacks on them.



### Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features
- **Arxiv ID**: http://arxiv.org/abs/1806.04659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04659v1)
- **Published**: 2018-06-12 17:42:10+00:00
- **Updated**: 2018-06-12 17:42:10+00:00
- **Authors**: Xiang Wang, Shaodi You, Xi Li, Huimin Ma
- **Comment**: accepted by CVPR 2018
- **Journal**: None
- **Summary**: Weakly-supervised semantic segmentation under image tags supervision is a challenging task as it directly associates high-level semantic to low-level appearance. To bridge this gap, in this paper, we propose an iterative bottom-up and top-down framework which alternatively expands object regions and optimizes segmentation network. We start from initial localization produced by classification networks. While classification networks are only responsive to small and coarse discriminative object regions, we argue that, these regions contain significant common features about objects. So in the bottom-up step, we mine common object features from the initial localization and expand object regions with the mined features. To supplement non-discriminative regions, saliency maps are then considered under Bayesian framework to refine the object regions. Then in the top-down step, the refined object regions are used as supervision to train the segmentation network and to predict object masks. These object masks provide more accurate localization and contain more regions of object. Further, we take these object masks as initial localization and mine common object features from them. These processes are conducted iteratively to progressively produce fine object masks and optimize segmentation networks. Experimental results on Pascal VOC 2012 dataset demonstrate that the proposed method outperforms previous state-of-the-art methods by a large margin.



### Accurate Detection of Inner Ears in Head CTs Using a Deep Volume-to-Volume Regression Network with False Positive Suppression and a Shape-Based Constraint
- **Arxiv ID**: http://arxiv.org/abs/1806.04725v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04725v1)
- **Published**: 2018-06-12 19:19:00+00:00
- **Updated**: 2018-06-12 19:19:00+00:00
- **Authors**: Dongqing Zhang, Jianing Wang, Jack H. Noble, Benoit M. Dawant
- **Comment**: Accepted to MICCAI 2018
- **Journal**: None
- **Summary**: Cochlear implants (CIs) are neural prosthetics which are used to treat patients with hearing loss. CIs use an array of electrodes which are surgically inserted into the cochlea to stimulate the auditory nerve endings. After surgery, CIs need to be programmed. Studies have shown that the spatial relationship between the intra-cochlear anatomy and electrodes derived from medical images can guide CI programming and lead to significant improvement in hearing outcomes. However, clinical head CT images are usually obtained from scanners of different brands with different protocols. The field of view thus varies greatly and visual inspection is needed to document their content prior to applying algorithms for electrode localization and intra-cochlear anatomy segmentation. In this work, to determine the presence/absence of inner ears and to accurately localize them in head CTs, we use a volume-to-volume convolutional neural network which can be trained end-to-end to map a raw CT volume to probability maps which indicate inner ear positions. We incorporate a false positive suppression strategy in training and apply a shape-based constraint. We achieve a labeling accuracy of 98.59% and a localization error of 2.45mm. The localization error is significantly smaller than a random forest-based approach that has been proposed recently to perform the same task.



### RepMet: Representative-based metric learning for classification and one-shot object detection
- **Arxiv ID**: http://arxiv.org/abs/1806.04728v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04728v3)
- **Published**: 2018-06-12 19:25:38+00:00
- **Updated**: 2018-11-18 13:33:52+00:00
- **Authors**: Leonid Karlinsky, Joseph Shtok, Sivan Harary, Eli Schwartz, Amit Aides, Rogerio Feris, Raja Giryes, Alex M. Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: Distance metric learning (DML) has been successfully applied to object classification, both in the standard regime of rich training data and in the few-shot scenario, where each category is represented by only a few examples. In this work, we propose a new method for DML that simultaneously learns the backbone network parameters, the embedding space, and the multi-modal distribution of each of the training categories in that space, in a single end-to-end training process. Our approach outperforms state-of-the-art methods for DML-based object classification on a variety of standard fine-grained datasets. Furthermore, we demonstrate the effectiveness of our approach on the problem of few-shot object detection, by incorporating the proposed DML architecture as a classification head into a standard object detection model. We achieve the best results on the ImageNet-LOC dataset compared to strong baselines, when only a few training examples are available. We also offer the community a new episodic benchmark based on the ImageNet dataset for the few-shot object detection task.



### Delta-encoder: an effective sample synthesis method for few-shot object recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.04734v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04734v3)
- **Published**: 2018-06-12 19:31:11+00:00
- **Updated**: 2018-11-29 14:38:20+00:00
- **Authors**: Eli Schwartz, Leonid Karlinsky, Joseph Shtok, Sivan Harary, Mattias Marder, Rogerio Feris, Abhishek Kumar, Raja Giryes, Alex M. Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: Learning to classify new categories based on just one or a few examples is a long-standing challenge in modern computer vision. In this work, we proposes a simple yet effective method for few-shot (and one-shot) object recognition. Our approach is based on a modified auto-encoder, denoted Delta-encoder, that learns to synthesize new samples for an unseen category just by seeing few examples from it. The synthesized samples are then used to train a classifier. The proposed approach learns to both extract transferable intra-class deformations, or "deltas", between same-class pairs of training examples, and to apply those deltas to the few provided examples of a novel class (unseen during training) in order to efficiently synthesize samples from that new class. The proposed method improves over the state-of-the-art in one-shot object-recognition and compares favorably in the few-shot case. Upon acceptance code will be made available.



### Fully Convolutional Network for Melanoma Diagnostics
- **Arxiv ID**: http://arxiv.org/abs/1806.04765v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04765v1)
- **Published**: 2018-06-12 20:53:55+00:00
- **Updated**: 2018-06-12 20:53:55+00:00
- **Authors**: Adon Phillips, Iris Teo, Jochen Lang
- **Comment**: None
- **Journal**: None
- **Summary**: This work seeks to determine how modern machine learning techniques may be applied to the previously unexplored topic of melanoma diagnostics using digital pathology. We curated a new dataset of 50 patient cases of cutaneous melanoma using digital pathology. We provide gold standard annotations for three tissue types (tumour, epidermis, and dermis) which are important for the prognostic measurements known as Breslow thickness and Clark level. Then, we devised a novel multi-stride fully convolutional network (FCN) architecture that outperformed other networks trained and evaluated using the same data according to standard metrics. Finally, we trained a model to detect and localize the target tissue types. When processing previously unseen cases, our model's output is qualitatively very similar to the gold standard. In addition to the standard metrics computed as a baseline for our approach, we asked three additional pathologists to measure the Breslow thickness on the network's output. Their responses were diagnostically equivalent to the ground truth measurements, and when removing cases where a measurement was not appropriate, inter-rater reliability (IRR) between the four pathologists was 75.0%. Given the qualitative and quantitative results, it is possible to overcome the discriminative challenges of the skin and tumour anatomy for segmentation using modern machine learning techniques, though more work is required to improve the network's performance on dermis segmentation. Further, we show that it is possible to achieve a level of accuracy required to manually perform the Breslow thickness measurement.



### Hierarchical Long-term Video Prediction without Supervision
- **Arxiv ID**: http://arxiv.org/abs/1806.04768v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04768v1)
- **Published**: 2018-06-12 21:11:07+00:00
- **Updated**: 2018-06-12 21:11:07+00:00
- **Authors**: Nevan Wichers, Ruben Villegas, Dumitru Erhan, Honglak Lee
- **Comment**: International Conference on Machine Learning (ICML) 2018
- **Journal**: None
- **Summary**: Much of recent research has been devoted to video prediction and generation, yet most of the previous works have demonstrated only limited success in generating videos on short-term horizons. The hierarchical video prediction method by Villegas et al. (2017) is an example of a state-of-the-art method for long-term video prediction, but their method is limited because it requires ground truth annotation of high-level structures (e.g., human joint landmarks) at training time. Our network encodes the input frame, predicts a high-level encoding into the future, and then a decoder with access to the first frame produces the predicted image from the predicted encoding. The decoder also produces a mask that outlines the predicted foreground object (e.g., person) as a by-product. Unlike Villegas et al. (2017), we develop a novel training method that jointly trains the encoder, the predictor, and the decoder together without highlevel supervision; we further improve upon this by using an adversarial loss in the feature space to train the predictor. Our method can predict about 20 seconds into the future and provides better results compared to Denton and Fergus (2018) and Finn et al. (2016) on the Human 3.6M dataset.



### Convolutional Neural Networks for Aircraft Noise Monitoring
- **Arxiv ID**: http://arxiv.org/abs/1806.04779v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.04779v1)
- **Published**: 2018-06-12 21:51:50+00:00
- **Updated**: 2018-06-12 21:51:50+00:00
- **Authors**: Nicholas Heller, Derek Anderson, Matt Baker, Brad Juffer, Nikolaos Papanikolopoulos
- **Comment**: Technical report
- **Journal**: None
- **Summary**: Air travel is one of the fastest growing modes of transportation, however, the effects of aircraft noise on populations surrounding airports is hindering its growth. In an effort to study and ultimately mitigate the impact that this noise has, many airports continuously monitor the aircraft noise in their surrounding communities. Noise monitoring and analysis is complicated by the fact that aircraft are not the only source of noise. In this work, we show that a Convolutional Neural Network is well-suited for the task of identifying noise events which are not caused by aircraft. Our system achieves an accuracy of 0.970 when trained on 900 manually labeled noise events. Our training data and a TensorFlow implementation of our model are available at https://github.com/neheller/aircraftnoise.



### A Connectome Based Hexagonal Lattice Convolutional Network Model of the Drosophila Visual System
- **Arxiv ID**: http://arxiv.org/abs/1806.04793v2
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.04793v2)
- **Published**: 2018-06-12 22:57:14+00:00
- **Updated**: 2018-06-24 10:36:40+00:00
- **Authors**: Fabian David Tschopp, Michael B. Reiser, Srinivas C. Turaga
- **Comment**: Work in progress. Final paper with results from an updated model with
  new connectome data will be coming soon
- **Journal**: None
- **Summary**: What can we learn from a connectome? We constructed a simplified model of the first two stages of the fly visual system, the lamina and medulla. The resulting hexagonal lattice convolutional network was trained using backpropagation through time to perform object tracking in natural scene videos. Networks initialized with weights from connectome reconstructions automatically discovered well-known orientation and direction selectivity properties in T4 neurons and their inputs, while networks initialized at random did not. Our work is the first demonstration, that knowledge of the connectome can enable in silico predictions of the functional properties of individual neurons in a circuit, leading to an understanding of circuit function from structure alone.



