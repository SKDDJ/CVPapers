# Arxiv Papers in cs.CV on 2018-06-27
### Learning a Saliency Evaluation Metric Using Crowdsourced Perceptual Judgments
- **Arxiv ID**: http://arxiv.org/abs/1806.10257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10257v1)
- **Published**: 2018-06-27 00:45:33+00:00
- **Updated**: 2018-06-27 00:45:33+00:00
- **Authors**: Changqun Xia, Jia Li, Jinming Su, Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: In the area of human fixation prediction, dozens of computational saliency models are proposed to reveal certain saliency characteristics under different assumptions and definitions. As a result, saliency model benchmarking often requires several evaluation metrics to simultaneously assess saliency models from multiple perspectives. However, most computational metrics are not designed to directly measure the perceptual similarity of saliency maps so that the evaluation results may be sometimes inconsistent with the subjective impression. To address this problem, this paper first conducts extensive subjective tests to find out how the visual similarities between saliency maps are perceived by humans. Based on the crowdsourced data collected in these tests, we conclude several key factors in assessing saliency maps and quantize the performance of existing metrics. Inspired by these factors, we propose to learn a saliency evaluation metric based on a two-stream convolutional neural network using crowdsourced perceptual judgements. Specifically, the relative saliency score of each pair from the crowdsourced data is utilized to regularize the network during the training process. By capturing the key factors shared by various subjects in comparing saliency maps, the learned metric better aligns with human perception of saliency maps, making it a good complement to the existing metrics. Experimental results validate that the learned metric can be generalized to the comparisons of saliency maps from new images, new datasets, new models and synthetic data. Due to the effectiveness of the learned metric, it also can be used to facilitate the development of new models for fixation prediction.



### Collaborative Annotation of Semantic Objects in Images with Multi-granularity Supervisions
- **Arxiv ID**: http://arxiv.org/abs/1806.10269v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10269v1)
- **Published**: 2018-06-27 02:05:25+00:00
- **Updated**: 2018-06-27 02:05:25+00:00
- **Authors**: Lishi Zhang, Chenghan Fu, Jia Li
- **Comment**: None
- **Journal**: None
- **Summary**: Per-pixel masks of semantic objects are very useful in many applications, which, however, are tedious to be annotated. In this paper, we propose a human-agent collaborative annotation approach that can efficiently generate per-pixel masks of semantic objects in tagged images with multi-granularity supervisions. Given a set of tagged image, a computer agent is first dynamically generated to roughly localize the semantic objects described by the tag. The agent first extracts massive object proposals from an image and then infer the tag-related ones under the weak and strong supervisions from linguistically and visually similar images and previously annotated object masks. By representing such supervisions by over-complete dictionaries, the tag-related object proposals can pop-out according to their sparse coding length, which are then converted to superpixels with binary labels. After that, human annotators participate in the annotation process by flipping labels and dividing superpixels with mouse clicks, which are used as click supervisions that teach the agent to recover false positives/negatives in processing images with the same tags. Experimental results show that our approach can facilitate the annotation process and generate object masks that are highly consistent with those generated by the LabelMe toolbox.



### Hierarchical Deep Co-segmentation of Primary Objects in Aerial Videos
- **Arxiv ID**: http://arxiv.org/abs/1806.10274v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10274v2)
- **Published**: 2018-06-27 02:34:51+00:00
- **Updated**: 2018-11-05 07:15:01+00:00
- **Authors**: Jia Li, Pengcheng Yuan, Daxin Gu, Yonghong Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Primary object segmentation plays an important role in understanding videos generated by unmanned aerial vehicles. In this paper, we propose a large-scale dataset with 500 aerial videos and manually annotated primary objects. To the best of our knowledge, it is the largest dataset to date for primary object segmentation in aerial videos. From this dataset, we find most aerial videos contain large-scale scenes, small primary objects as well as consistently varying scales and viewpoints. Inspired by that, we propose a hierarchical deep co-segmentation approach that repeatedly divides a video into two sub-videos formed by the odd and even frames, respectively. In this manner, the primary objects shared by sub-videos can be co-segmented by training two-stream CNNs and finally refined within the neighborhood reversible flows. Experimental results show that our approach remarkably outperforms 17 state-of-the-art methods in segmenting primary objects in various types of aerial videos.



### Feature-less Stitching of Cylindrical Tunnel
- **Arxiv ID**: http://arxiv.org/abs/1806.10278v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10278v1)
- **Published**: 2018-06-27 02:56:19+00:00
- **Updated**: 2018-06-27 02:56:19+00:00
- **Authors**: Ramanpreet Singh Pahwa, Wei Kiat Leong, Shaohui Foong, Karianto Leman, Minh N. Do
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Traditional image stitching algorithms use transforms such as homography to combine different views of a scene. They usually work well when the scene is planar or when the camera is only rotated, keeping its position static. This severely limits their use in real world scenarios where an unmanned aerial vehicle (UAV) potentially hovers around and flies in an enclosed area while rotating to capture a video sequence. We utilize known scene geometry along with recorded camera trajectory to create cylindrical images captured in a given environment such as a tunnel where the camera rotates around its center. The captured images of the inner surface of the given scene are combined to create a composite panoramic image that is textured onto a 3D geometrical object in Unity graphical engine to create an immersive environment for end users.



### Attention to Head Locations for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1806.10287v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10287v1)
- **Published**: 2018-06-27 03:52:19+00:00
- **Updated**: 2018-06-27 03:52:19+00:00
- **Authors**: Youmei Zhang, Chunluan Zhou, Faliang Chang, Alex C. Kot
- **Comment**: None
- **Journal**: None
- **Summary**: Occlusions, complex backgrounds, scale variations and non-uniform distributions present great challenges for crowd counting in practical applications. In this paper, we propose a novel method using an attention model to exploit head locations which are the most important cue for crowd counting. The attention model estimates a probability map in which high probabilities indicate locations where heads are likely to be present. The estimated probability map is used to suppress non-head regions in feature maps from several multi-scale feature extraction branches of a convolution neural network for crowd density estimation, which makes our method robust to complex backgrounds, scale variations and non-uniform distributions. In addition, we introduce a relative deviation loss to compensate a commonly used training loss, Euclidean distance, to improve the accuracy of sparse crowd density estimation. Experiments on Shanghai-Tech, UCF_CC_50 and World-Expo'10 data sets demonstrate the effectiveness of our method.



### QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1806.10293v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.10293v3)
- **Published**: 2018-06-27 04:34:30+00:00
- **Updated**: 2018-11-28 02:40:54+00:00
- **Authors**: Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, Sergey Levine
- **Comment**: CoRL 2018 camera ready. 23 pages, 14 figures
- **Journal**: None
- **Summary**: In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.



### Exploiting Spatial-Temporal Modelling and Multi-Modal Fusion for Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.10319v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10319v1)
- **Published**: 2018-06-27 06:44:02+00:00
- **Updated**: 2018-06-27 06:44:02+00:00
- **Authors**: Dongliang He, Fu Li, Qijie Zhao, Xiang Long, Yi Fu, Shilei Wen
- **Comment**: None
- **Journal**: None
- **Summary**: In this report, our approach to tackling the task of ActivityNet 2018 Kinetics-600 challenge is described in detail. Though spatial-temporal modelling methods, which adopt either such end-to-end framework as I3D \cite{i3d} or two-stage frameworks (i.e., CNN+RNN), have been proposed in existing state-of-the-arts for this task, video modelling is far from being well solved. In this challenge, we propose spatial-temporal network (StNet) for better joint spatial-temporal modelling and comprehensively video understanding. Besides, given that multi-modal information is contained in video source, we manage to integrate both early-fusion and later-fusion strategy of multi-modal information via our proposed improved temporal Xception network (iTXN) for video understanding. Our StNet RGB single model achieves 78.99\% top-1 precision in the Kinetics-600 validation set and that of our improved temporal Xception network which integrates RGB, flow and audio modalities is up to 82.35\%. After model ensemble, we achieve top-1 precision as high as 85.0\% on the validation set and rank No.1 among all submissions.



### 3D RoI-aware U-Net for Accurate and Efficient Colorectal Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1806.10342v5
- **DOI**: 10.1109/TCYB.2020.2980145
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10342v5)
- **Published**: 2018-06-27 08:42:58+00:00
- **Updated**: 2019-02-15 08:46:25+00:00
- **Authors**: Yi-Jie Huang, Qi Dou, Zi-Xian Wang, Li-Zhi Liu, Ying Jin, Chao-Feng Li, Lisheng Wang, Hao Chen, Rui-Hua Xu
- **Comment**: IEEE transactions on cybernetics (2020)
- **Journal**: None
- **Summary**: Segmentation of colorectal cancerous regions from 3D Magnetic Resonance (MR) images is a crucial procedure for radiotherapy which conventionally requires accurate delineation of tumour boundaries at an expense of labor, time and reproducibility. While deep learning based methods serve good baselines in 3D image segmentation tasks, small applicable patch size limits effective receptive field and degrades segmentation performance. In addition, Regions of interest (RoIs) localization from large whole volume 3D images serves as a preceding operation that brings about multiple benefits in terms of speed, target completeness, reduction of false positives. Distinct from sliding window or non-joint localization-segmentation based models, we propose a novel multitask framework referred to as 3D RoI-aware U-Net (3D RU-Net), for RoI localization and in-region segmentation where the two tasks share one backbone encoder network. With the region proposals from the encoder, we crop multi-level RoI in-region features from the encoder to form a GPU memory-efficient decoder for detailpreserving segmentation and therefore enlarged applicable volume size and effective receptive field. To effectively train the model, we designed a Dice formulated loss function for the global-to-local multi-task learning procedure. Based on the efficiency gains, we went on to ensemble models with different receptive fields to achieve even higher performance costing minor extra computational expensiveness. Extensive experiments were conducted on 64 cancerous cases with a four-fold cross-validation, and the results showed significant superiority in terms of accuracy and efficiency over conventional frameworks. In conclusion, the proposed method has a huge potential for extension to other 3D object segmentation tasks from medical images due to its inherent generalizability. The code for the proposed method is publicly available.



### A Multi-Task Learning Approach for Meal Assessment
- **Arxiv ID**: http://arxiv.org/abs/1806.10343v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10343v1)
- **Published**: 2018-06-27 08:44:25+00:00
- **Updated**: 2018-06-27 08:44:25+00:00
- **Authors**: Ya Lu, Dario Allegra, Marios Anthimopoulos, Filippo Stanco, Giovanni Maria Farinella, Stavroula Mougiakakou
- **Comment**: None
- **Journal**: None
- **Summary**: Key role in the prevention of diet-related chronic diseases plays the balanced nutrition together with a proper diet. The conventional dietary assessment methods are time-consuming, expensive and prone to errors. New technology-based methods that provide reliable and convenient dietary assessment, have emerged during the last decade. The advances in the field of computer vision permitted the use of meal image to assess the nutrient content usually through three steps: food segmentation, recognition and volume estimation. In this paper, we propose a use one RGB meal image as input to a multi-task learning based Convolutional Neural Network (CNN). The proposed approach achieved outstanding performance, while a comparison with state-of-the-art methods indicated that the proposed approach exhibits clear advantage in accuracy, along with a massive reduction of processing time.



### Learning Visually-Grounded Semantics from Contrastive Adversarial Samples
- **Arxiv ID**: http://arxiv.org/abs/1806.10348v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1806.10348v1)
- **Published**: 2018-06-27 08:58:57+00:00
- **Updated**: 2018-06-27 08:58:57+00:00
- **Authors**: Haoyue Shi, Jiayuan Mao, Tete Xiao, Yuning Jiang, Jian Sun
- **Comment**: To Appear at COLING 2018
- **Journal**: None
- **Summary**: We study the problem of grounding distributional representations of texts on the visual domain, namely visual-semantic embeddings (VSE for short). Begin with an insightful adversarial attack on VSE embeddings, we show the limitation of current frameworks and image-text datasets (e.g., MS-COCO) both quantitatively and qualitatively. The large gap between the number of possible constitutions of real-world semantics and the size of parallel data, to a large extent, restricts the model to establish the link between textual semantics and visual concepts. We alleviate this problem by augmenting the MS-COCO image captioning datasets with textual contrastive adversarial samples. These samples are synthesized using linguistic rules and the WordNet knowledge base. The construction procedure is both syntax- and semantics-aware. The samples enforce the model to ground learned embeddings to concrete concepts within the image. This simple but powerful technique brings a noticeable improvement over the baselines on a diverse set of downstream tasks, in addition to defending known-type adversarial attacks. We release the codes at https://github.com/ExplorerFreda/VSE-C.



### Disparity Image Segmentation For ADAS
- **Arxiv ID**: http://arxiv.org/abs/1806.10350v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, 68W99, I.4.6; I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/1806.10350v1)
- **Published**: 2018-06-27 09:01:38+00:00
- **Updated**: 2018-06-27 09:01:38+00:00
- **Authors**: Viktor Mukha, Inon Sharony
- **Comment**: 7 pages, 5 figures, 2 tables, 1 algorithm
- **Journal**: Elektronik Automotive issue 6 (2019)
- **Summary**: We present a simple solution for segmenting grayscale images using existing Connected Component Labeling (CCL) algorithms (which are generally applied to binary images), which was efficient enough to be implemented in a constrained (embedded automotive) architecture. Our solution customizes the region growing and merging approach, and is primarily targeted for stereoscopic disparity images where nearer objects carry more relevance. We provide results from a standard OpenCV implementation for some basic cases and an image from the Tsukuba stereo-pair dataset.



### Learn-to-Score: Efficient 3D Scene Exploration by Predicting View Utility
- **Arxiv ID**: http://arxiv.org/abs/1806.10354v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10354v2)
- **Published**: 2018-06-27 09:09:50+00:00
- **Updated**: 2018-07-31 11:50:20+00:00
- **Authors**: Benjamin Hepp, Debadeepta Dey, Sudipta N. Sinha, Ashish Kapoor, Neel Joshi, Otmar Hilliges
- **Comment**: 16 pages, 7 figures, 5 tables
- **Journal**: None
- **Summary**: Camera equipped drones are nowadays being used to explore large scenes and reconstruct detailed 3D maps. When free space in the scene is approximately known, an offline planner can generate optimal plans to efficiently explore the scene. However, for exploring unknown scenes, the planner must predict and maximize usefulness of where to go on the fly. Traditionally, this has been achieved using handcrafted utility functions. We propose to learn a better utility function that predicts the usefulness of future viewpoints. Our learned utility function is based on a 3D convolutional neural network. This network takes as input a novel volumetric scene representation that implicitly captures previously visited viewpoints and generalizes to new scenes. We evaluate our method on several large 3D models of urban scenes using simulated depth cameras. We show that our method outperforms existing utility measures in terms of reconstruction performance and is robust to sensor noise.



### Context Proposals for Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/1806.10359v1
- **DOI**: 10.1016/j.cviu.2018.06.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10359v1)
- **Published**: 2018-06-27 09:11:50+00:00
- **Updated**: 2018-06-27 09:11:50+00:00
- **Authors**: Aymen Azaza, Joost van de Weijer, Ali Douik, Marc Masana
- **Comment**: Accepted at Computer Vision and Image Understanding (CVIU)
- **Journal**: None
- **Summary**: One of the fundamental properties of a salient object region is its contrast with the immediate context. The problem is that numerous object regions exist which potentially can all be salient. One way to prevent an exhaustive search over all object regions is by using object proposal algorithms. These return a limited set of regions which are most likely to contain an object. Several saliency estimation methods have used object proposals. However, they focus on the saliency of the proposal only, and the importance of its immediate context has not been evaluated.   In this paper, we aim to improve salient object detection. Therefore, we extend object proposal methods with context proposals, which allow to incorporate the immediate context in the saliency computation. We propose several saliency features which are computed from the context proposals. In the experiments, we evaluate five object proposal methods for the task of saliency segmentation, and find that Multiscale Combinatorial Grouping outperforms the others. Furthermore, experiments show that the proposed context features improve performance, and that our method matches results on the FT datasets and obtains competitive results on three other datasets (PASCAL-S, MSRA-B and ECSSD).



### Divergence-Free Shape Interpolation and Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1806.10417v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1806.10417v2)
- **Published**: 2018-06-27 11:37:24+00:00
- **Updated**: 2018-10-16 11:52:34+00:00
- **Authors**: Marvin Eisenberger, Zorah Lähner, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel method to model and calculate deformation fields between shapes embedded in $\mathbb{R}^D$. Our framework combines naturally interpolating the two input shapes and calculating correspondences at the same time. The key idea is to compute a divergence-free deformation field represented in a coarse-to-fine basis using the Karhunen-Lo\`eve expansion. The advantages are that there is no need to discretize the embedding space and the deformation is volume-preserving. Furthermore, the optimization is done on downsampled versions of the shapes but the morphing can be applied to any resolution without a heavy increase in complexity. We show results for shape correspondence, registration, inter- and extrapolation on the TOSCA and FAUST data sets.



### MTBI Identification From Diffusion MR Images Using Bag of Adversarial Visual Features
- **Arxiv ID**: http://arxiv.org/abs/1806.10419v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10419v1)
- **Published**: 2018-06-27 11:41:34+00:00
- **Updated**: 2018-06-27 11:41:34+00:00
- **Authors**: Shervin Minaee, Yao Wang, Alp Aygar, Sohae Chung, Xiuyuan Wang, Yvonne W. Lui, Els Fieremans, Steven Flanagan, Joseph Rath
- **Comment**: IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: In this work, we propose bag of adversarial features (BAF) for identifying mild traumatic brain injury (MTBI) patients from their diffusion magnetic resonance images (MRI) (obtained within one month of injury) by incorporating unsupervised feature learning techniques. MTBI is a growing public health problem with an estimated incidence of over 1.7 million people annually in US. Diagnosis is based on clinical history and symptoms, and accurate, concrete measures of injury are lacking. Unlike most of previous works, which use hand-crafted features extracted from different parts of brain for MTBI classification, we employ feature learning algorithms to learn more discriminative representation for this task. A major challenge in this field thus far is the relatively small number of subjects available for training. This makes it difficult to use an end-to-end convolutional neural network to directly classify a subject from MR images. To overcome this challenge, we first apply an adversarial auto-encoder (with convolutional structure) to learn patch-level features, from overlapping image patches extracted from different brain regions. We then aggregate these features through a bag-of-word approach. We perform an extensive experimental study on a dataset of 227 subjects (including 109 MTBI patients, and 118 age and sex matched healthy controls), and compare the bag-of-deep-features with several previous approaches. Our experimental results show that the BAF significantly outperforms earlier works relying on the mean values of MR metrics in selected brain regions.



### Deep Steganalysis: End-to-End Learning with Supervisory Information beyond Class Labels
- **Arxiv ID**: http://arxiv.org/abs/1806.10443v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10443v1)
- **Published**: 2018-06-27 12:46:30+00:00
- **Updated**: 2018-06-27 12:46:30+00:00
- **Authors**: Wei Wang, Jing Dong, Yinlong Qian, Tieniu Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep learning has shown its power in steganalysis. However, the proposed deep models have been often learned from pre-calculated noise residuals with fixed high-pass filters rather than from raw images. In this paper, we propose a new end-to-end learning framework that can learn steganalytic features directly from pixels. In the meantime, the high-pass filters are also automatically learned. Besides class labels, we make use of additional pixel level supervision of cover-stego image pair to jointly and iteratively train the proposed network which consists of a residual calculation network and a steganalysis network. The experimental results prove the effectiveness of the proposed architecture.



### LPRNet: License Plate Recognition via Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1806.10447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10447v1)
- **Published**: 2018-06-27 12:57:17+00:00
- **Updated**: 2018-06-27 12:57:17+00:00
- **Authors**: Sergey Zherzdev, Alexey Gruzdev
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes LPRNet - end-to-end method for Automatic License Plate Recognition without preliminary character segmentation. Our approach is inspired by recent breakthroughs in Deep Neural Networks, and works in real-time with recognition accuracy up to 95% for Chinese license plates: 3 ms/plate on nVIDIA GeForce GTX 1080 and 1.3 ms/plate on Intel Core i7-6700K CPU. LPRNet consists of the lightweight Convolutional Neural Network, so it can be trained in end-to-end way. To the best of our knowledge, LPRNet is the first real-time License Plate Recognition system that does not use RNNs. As a result, the LPRNet algorithm may be used to create embedded solutions for LPR that feature high level accuracy even on challenging Chinese license plates.



### Homogeneity of a region in the logarithmic image processing framework: application to region growing algorithms
- **Arxiv ID**: http://arxiv.org/abs/1806.10472v1
- **DOI**: None
- **Categories**: **cs.CV**, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1806.10472v1)
- **Published**: 2018-06-27 13:28:46+00:00
- **Updated**: 2018-06-27 13:28:46+00:00
- **Authors**: Michel Jourlin, Guillaume Noyel
- **Comment**: None
- **Journal**: International Workshop on the Physics and Mechanics of Random
  Structures: from Morphology to Material Properties, Jun 2018, Island of
  Ol{\'e}ron, France
- **Summary**: The current paper deals with the role played by Logarithmic Image Processing (LIP) operators for evaluating the homogeneity of a region. Two new criteria of heterogeneity are introduced, one based on the LIP addition and the other based on the LIP scalar multiplication. Such tools are able to manage Region Growing algorithms following the Revol's technique: starting from an initial seed, they consist of applying specific dilations to the growing region while its inhomogeneity level does not exceed a certain level. The new approaches we introduce are significantly improving Revol's existing technique by making it robust to contrast variations in images. Such a property strongly reduces the chaining effect arising in region growing processes.



### Customizing an Adversarial Example Generator with Class-Conditional GANs
- **Arxiv ID**: http://arxiv.org/abs/1806.10496v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1806.10496v1)
- **Published**: 2018-06-27 14:24:27+00:00
- **Updated**: 2018-06-27 14:24:27+00:00
- **Authors**: Shih-hong Tsai
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial examples are intentionally crafted data with the purpose of deceiving neural networks into misclassification. When we talk about strategies to create such examples, we usually refer to perturbation-based methods that fabricate adversarial examples by applying invisible perturbations onto normal data. The resulting data reserve their visual appearance to human observers, yet can be totally unrecognizable to DNN models, which in turn leads to completely misleading predictions. In this paper, however, we consider crafting adversarial examples from existing data as a limitation to example diversity. We propose a non-perturbation-based framework that generates native adversarial examples from class-conditional generative adversarial networks.As such, the generated data will not resemble any existing data and thus expand example diversity, raising the difficulty in adversarial defense. We then extend this framework to pre-trained conditional GANs, in which we turn an existing generator into an "adversarial-example generator". We conduct experiments on our approach for MNIST and CIFAR10 datasets and have satisfactory results, showing that this approach can be a potential alternative to previous attack strategies.



### Watch to Edit: Video Retargeting using Gaze
- **Arxiv ID**: http://arxiv.org/abs/1807.03125v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1807.03125v1)
- **Published**: 2018-06-27 16:21:03+00:00
- **Updated**: 2018-06-27 16:21:03+00:00
- **Authors**: Kranthi Kumar, Moneish Kumar, Vineet Gandhi, Ramanathan Subramanian
- **Comment**: None
- **Journal**: Computer Graphics Forum, Volume37, Issue2(2018)205-215
- **Summary**: We present a novel approach to optimally retarget videos for varied displays with differing aspect ratios by preserving salient scene content discovered via eye tracking. Our algorithm performs editing with cut, pan and zoom operations by optimizing the path of a cropping window within the original video while seeking to (i) preserve salient regions, and (ii) adhere to the principles of cinematography. Our approach is (a) content agnostic as the same methodology is employed to re-edit a wide-angle video recording or a close-up movie sequence captured with a static or moving camera, and (b) independent of video length and can in principle re-edit an entire movie in one shot. Our algorithm consists of two steps. The first step employs gaze transition cues to detect time stamps where new cuts are to be introduced in the original video via dynamic programming. A subsequent step optimizes the cropping window path (to create pan and zoom effects), while accounting for the original and new cuts. The cropping window path is designed to include maximum gaze information, and is composed of piecewise constant, linear and parabolic segments. It is obtained via L(1) regularized convex optimization which ensures a smooth viewing experience. We test our approach on a wide variety of videos and demonstrate significant improvement over the state-of-the-art, both in terms of computational complexity and qualitative aspects. A study performed with 16 users confirms that our approach results in a superior viewing experience as compared to gaze driven re-editing and letterboxing methods, especially for wide-angle static camera recordings.



### Every Pixel Counts: Unsupervised Geometry Learning with Holistic 3D Motion Understanding
- **Arxiv ID**: http://arxiv.org/abs/1806.10556v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10556v2)
- **Published**: 2018-06-27 16:23:03+00:00
- **Updated**: 2018-08-15 18:34:48+00:00
- **Authors**: Zhenheng Yang, Peng Wang, Yang Wang, Wei Xu, Ram Nevatia
- **Comment**: ECCV18' submission
- **Journal**: None
- **Summary**: Learning to estimate 3D geometry in a single image by watching unlabeled videos via deep convolutional network has made significant process recently. Current state-of-the-art (SOTA) methods, are based on the learning framework of rigid structure-from-motion, where only 3D camera ego motion is modeled for geometry estimation.However, moving objects also exist in many videos, e.g. moving cars in a street scene. In this paper, we tackle such motion by additionally incorporating per-pixel 3D object motion into the learning framework, which provides holistic 3D scene flow understanding and helps single image geometry estimation. Specifically, given two consecutive frames from a video, we adopt a motion network to predict their relative 3D camera pose and a segmentation mask distinguishing moving objects and rigid background. An optical flow network is used to estimate dense 2D per-pixel correspondence. A single image depth network predicts depth maps for both images. The four types of information, i.e. 2D flow, camera pose, segment mask and depth maps, are integrated into a differentiable holistic 3D motion parser (HMP), where per-pixel 3D motion for rigid background and moving objects are recovered. We design various losses w.r.t. the two types of 3D motions for training the depth and motion networks, yielding further error reduction for estimated geometry. Finally, in order to solve the 3D motion confusion from monocular videos, we combine stereo images into joint training. Experiments on KITTI 2015 dataset show that our estimated geometry, 3D motion and moving object masks, not only are constrained to be consistent, but also significantly outperforms other SOTA algorithms, demonstrating the benefits of our approach.



### This Looks Like That: Deep Learning for Interpretable Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1806.10574v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1806.10574v5)
- **Published**: 2018-06-27 17:18:03+00:00
- **Updated**: 2019-12-28 20:12:11+00:00
- **Authors**: Chaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su, Cynthia Rudin
- **Comment**: Chaofan Chen and Oscar Li contributed equally to this work. This work
  has been accepted for spotlight presentation (top 3% of papers) at NeurIPS
  2019
- **Journal**: Advances in Neural Information Processing Systems 32 (NeurIPS
  2019)
- **Summary**: When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture -- prototypical part network (ProtoPNet), that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The model thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training without any annotations for parts of images. We demonstrate our method on the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show that ProtoPNet can achieve comparable accuracy with its analogous non-interpretable counterpart, and when several ProtoPNets are combined into a larger network, it can achieve an accuracy that is on par with some of the best-performing deep models. Moreover, ProtoPNet provides a level of interpretability that is absent in other interpretable deep models.



### Estimating Bicycle Route Attractivity from Image Data
- **Arxiv ID**: http://arxiv.org/abs/1807.03126v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.03126v1)
- **Published**: 2018-06-27 19:29:05+00:00
- **Updated**: 2018-06-27 19:29:05+00:00
- **Authors**: Vít Růžička
- **Comment**: 86 pages. (Master's thesis, 2017)
- **Journal**: None
- **Summary**: This master thesis focuses on practical application of Convolutional Neural Network models on the task of road labeling with bike attractivity score. We start with an abstraction of real world locations into nodes and scored edges in partially annotated dataset. We enhance information available about each edge with photographic data from Google Street View service and with additional neighborhood information from Open Street Map database. We teach a model on this enhanced dataset and experiment with ImageNet Large Scale Visual Recognition Competition. We try different dataset enhancing techniques as well as various model architectures to improve road scoring. We also make use of transfer learning to use features from a task with rich dataset of ImageNet into our task with smaller number of images, to prevent model overfitting.



### Dynamic texture analysis with diffusion in networks
- **Arxiv ID**: http://arxiv.org/abs/1806.10681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10681v1)
- **Published**: 2018-06-27 20:14:08+00:00
- **Updated**: 2018-06-27 20:14:08+00:00
- **Authors**: Lucas C. Ribas, Wesley N. Goncalves, Odemir M. Bruno
- **Comment**: 30 pages, 20 figures
- **Journal**: None
- **Summary**: Dynamic texture is a field of research that has gained considerable interest from computer vision community due to the explosive growth of multimedia databases. In addition, dynamic texture is present in a wide range of videos, which makes it very important in expert systems based on videos such as medical systems, traffic monitoring systems, forest fire detection system, among others. In this paper, a new method for dynamic texture characterization based on diffusion in directed networks is proposed. The dynamic texture is modeled as a directed network. The method consists in the analysis of the dynamic of this network after a series of graph cut transformations based on the edge weights. For each network transformation, the activity for each vertex is estimated. The activity is the relative frequency that one vertex is visited by random walks in balance. Then, texture descriptor is constructed by concatenating the activity histograms. The main contributions of this paper are the use of directed network modeling and diffusion in network to dynamic texture characterization. These tend to provide better performance in dynamic textures classification. Experiments with rotation and interference of the motion pattern were conducted in order to demonstrate the robustness of the method. The proposed approach is compared to other dynamic texture methods on two very well know dynamic texture database and on traffic condition classification, and outperform in most of the cases.



### A Hybrid Framework for Tumor Saliency Estimation
- **Arxiv ID**: http://arxiv.org/abs/1806.10696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1806.10696v1)
- **Published**: 2018-06-27 20:51:17+00:00
- **Updated**: 2018-06-27 20:51:17+00:00
- **Authors**: Fei Xu, Min Xian, Yingtao Zhang, Kuan Huang, H. D. Cheng, Boyu Zhang, Jianrui Ding, Chunping Ning, Ying Wang
- **Comment**: 6 pages and 8 figures, Conference paper and accepted by ICPR 2018
- **Journal**: None
- **Summary**: Automatic tumor segmentation of breast ultrasound (BUS) image is quite challenging due to the complicated anatomic structure of breast and poor image quality. Most tumor segmentation approaches achieve good performance on BUS images collected in controlled settings; however, the performance degrades greatly with BUS images from different sources. Tumor saliency estimation (TSE) has attracted increasing attention to solving the problem by modeling radiologists' attention mechanism. In this paper, we propose a novel hybrid framework for TSE, which integrates both high-level domain-knowledge and robust low-level saliency assumptions and can overcome drawbacks caused by direct mapping in traditional TSE approaches. The new framework integrated the Neutro-Connectedness (NC) map, the adaptive-center, the correlation and the layer structure-based weighted map. The experimental results demonstrate that the proposed approach outperforms state-of-the-art TSE methods.



### Gradient Similarity: An Explainable Approach to Detect Adversarial Attacks against Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1806.10707v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1806.10707v1)
- **Published**: 2018-06-27 22:47:37+00:00
- **Updated**: 2018-06-27 22:47:37+00:00
- **Authors**: Jasjeet Dhaliwal, Saurabh Shintre
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: Deep neural networks are susceptible to small-but-specific adversarial perturbations capable of deceiving the network. This vulnerability can lead to potentially harmful consequences in security-critical applications. To address this vulnerability, we propose a novel metric called \emph{Gradient Similarity} that allows us to capture the influence of training data on test inputs. We show that \emph{Gradient Similarity} behaves differently for normal and adversarial inputs, and enables us to detect a variety of adversarial attacks with a near perfect ROC-AUC of 95-100\%. Even white-box adversaries equipped with perfect knowledge of the system cannot bypass our detector easily. On the MNIST dataset, white-box attacks are either detected with a high ROC-AUC of 87-96\%, or require very high distortion to bypass our detector.



