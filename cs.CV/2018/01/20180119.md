# Arxiv Papers in cs.CV on 2018-01-19
### Deep Chain HDRI: Reconstructing a High Dynamic Range Image from a Single Low Dynamic Range Image
- **Arxiv ID**: http://arxiv.org/abs/1801.06277v1
- **DOI**: 10.1109/ACCESS.2018.2868246
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1801.06277v1)
- **Published**: 2018-01-19 03:03:13+00:00
- **Updated**: 2018-01-19 03:03:13+00:00
- **Authors**: Siyeong Lee, Gwon Hwan An, Suk-Ju Kang
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In this paper, we propose a novel deep neural network model that reconstructs a high dynamic range (HDR) image from a single low dynamic range (LDR) image. The proposed model is based on a convolutional neural network composed of dilated convolutional layers, and infers LDR images with various exposures and illumination from a single LDR image of the same scene. Then, the final HDR image can be formed by merging these inference results. It is relatively easy for the proposed method to find the mapping between the LDR and an HDR with a different bit depth because of the chaining structure inferring the relationship between the LDR images with brighter (or darker) exposures from a given LDR image. The method not only extends the range, but also has the advantage of restoring the light information of the actual physical world. For the HDR images obtained by the proposed method, the HDR-VDP2 Q score, which is the most popular evaluation metric for HDR images, was 56.36 for a display with a 1920$\times$1200 resolution, which is an improvement of 6 compared with the scores of conventional algorithms. In addition, when comparing the peak signal-to-noise ratio values for tone mapped HDR images generated by the proposed and conventional algorithms, the average value obtained by the proposed algorithm is 30.86 dB, which is 10 dB higher than those obtained by the conventional algorithms.



### An End-to-End Deep Learning Histochemical Scoring System for Breast Cancer Tissue Microarray
- **Arxiv ID**: http://arxiv.org/abs/1801.06288v1
- **DOI**: 10.1109/TMI.2018.2868333
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06288v1)
- **Published**: 2018-01-19 04:04:50+00:00
- **Updated**: 2018-01-19 04:04:50+00:00
- **Authors**: Jingxin Liu, Bolei Xu, Chi Zheng, Yuanhao Gong, Jon Garibaldi, Daniele Soria, Andew Green, Ian O. Ellis, Wenbin Zou, Guoping Qiu
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging 03 September 2018
- **Summary**: One of the methods for stratifying different molecular classes of breast cancer is the Nottingham Prognostic Index Plus (NPI+) which uses breast cancer relevant biomarkers to stain tumour tissues prepared on tissue microarray (TMA). To determine the molecular class of the tumour, pathologists will have to manually mark the nuclei activity biomarkers through a microscope and use a semi-quantitative assessment method to assign a histochemical score (H-Score) to each TMA core. Manually marking positively stained nuclei is a time consuming, imprecise and subjective process which will lead to inter-observer and intra-observer discrepancies. In this paper, we present an end-to-end deep learning system which directly predicts the H-Score automatically. Our system imitates the pathologists' decision process and uses one fully convolutional network (FCN) to extract all nuclei region (tumour and non-tumour), a second FCN to extract tumour nuclei region, and a multi-column convolutional neural network which takes the outputs of the first two FCNs and the stain intensity description image as input and acts as the high-level decision making mechanism to directly output the H-Score of the input TMA image. To the best of our knowledge, this is the first end-to-end system that takes a TMA image as input and directly outputs a clinical score. We will present experimental results which demonstrate that the H-Scores predicted by our model have very high and statistically significant correlation with experienced pathologists' scores and that the H-Score discrepancy between our algorithm and the pathologists is on par with the inter-subject discrepancy between the pathologists.



### Describing Semantic Representations of Brain Activity Evoked by Visual Stimuli
- **Arxiv ID**: http://arxiv.org/abs/1802.02210v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02210v1)
- **Published**: 2018-01-19 05:12:59+00:00
- **Updated**: 2018-01-19 05:12:59+00:00
- **Authors**: Eri Matsuo, Ichiro Kobayashi, Shinji Nishimoto, Satoshi Nishida, Hideki Asoh
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: Quantitative modeling of human brain activity based on language representations has been actively studied in systems neuroscience. However, previous studies examined word-level representation, and little is known about whether we could recover structured sentences from brain activity. This study attempts to generate natural language descriptions of semantic contents from human brain activity evoked by visual stimuli. To effectively use a small amount of available brain activity data, our proposed method employs a pre-trained image-captioning network model using a deep learning framework. To apply brain activity to the image-captioning network, we train regression models that learn the relationship between brain activity and deep-layer image features. The results demonstrate that the proposed model can decode brain activity and generate descriptions using natural language sentences. We also conducted several experiments with data from different subsets of brain regions known to process visual stimuli. The results suggest that semantic information for sentence generations is widespread across the entire cortex.



### Fully Point-wise Convolutional Neural Network for Modeling Statistical Regularities in Natural Images
- **Arxiv ID**: http://arxiv.org/abs/1801.06302v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06302v3)
- **Published**: 2018-01-19 05:32:33+00:00
- **Updated**: 2018-08-21 00:06:22+00:00
- **Authors**: Jing Zhang, Yang Cao, Yang Wang, Chenglin Wen, Chang Wen Chen
- **Comment**: 9 pages, 7 figures. To appear in ACM MM 2018
- **Journal**: None
- **Summary**: Modeling statistical regularity plays an essential role in ill-posed image processing problems. Recently, deep learning based methods have been presented to implicitly learn statistical representation of pixel distributions in natural images and leverage it as a constraint to facilitate subsequent tasks, such as color constancy and image dehazing. However, the existing CNN architecture is prone to variability and diversity of pixel intensity within and between local regions, which may result in inaccurate statistical representation. To address this problem, this paper presents a novel fully point-wise CNN architecture for modeling statistical regularities in natural images. Specifically, we propose to randomly shuffle the pixels in the origin images and leverage the shuffled image as input to make CNN more concerned with the statistical properties. Moreover, since the pixels in the shuffled image are independent identically distributed, we can replace all the large convolution kernels in CNN with point-wise ($1*1$) convolution kernels while maintaining the representation ability. Experimental results on two applications: color constancy and image dehazing, demonstrate the superiority of our proposed network over the existing architectures, i.e., using 1/10$\sim$1/100 network parameters and computational cost while achieving comparable performance.



### A predictor-corrector method for the training of deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1803.05779v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05779v1)
- **Published**: 2018-01-19 06:30:59+00:00
- **Updated**: 2018-01-19 06:30:59+00:00
- **Authors**: Yatin Saraiya
- **Comment**: 6 pages, 2 figures, 2 tables
- **Journal**: None
- **Summary**: The training of deep neural nets is expensive. We present a predictor- corrector method for the training of deep neural nets. It alternates a predictor pass with a corrector pass using stochastic gradient descent with backpropagation such that there is no loss in validation accuracy. No special modifications to SGD with backpropagation is required by this methodology. Our experiments showed a time improvement of 9% on the CIFAR-10 dataset.



### BinaryRelax: A Relaxation Approach For Training Deep Neural Networks With Quantized Weights
- **Arxiv ID**: http://arxiv.org/abs/1801.06313v3
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1801.06313v3)
- **Published**: 2018-01-19 06:35:23+00:00
- **Updated**: 2018-09-05 00:01:37+00:00
- **Authors**: Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, Jack Xin
- **Comment**: None
- **Journal**: None
- **Summary**: We propose BinaryRelax, a simple two-phase algorithm, for training deep neural networks with quantized weights. The set constraint that characterizes the quantization of weights is not imposed until the late stage of training, and a sequence of \emph{pseudo} quantized weights is maintained. Specifically, we relax the hard constraint into a continuous regularizer via Moreau envelope, which turns out to be the squared Euclidean distance to the set of quantized weights. The pseudo quantized weights are obtained by linearly interpolating between the float weights and their quantizations. A continuation strategy is adopted to push the weights towards the quantized state by gradually increasing the regularization parameter. In the second phase, exact quantization scheme with a small learning rate is invoked to guarantee fully quantized weights. We test BinaryRelax on the benchmark CIFAR and ImageNet color image datasets to demonstrate the superiority of the relaxed quantization approach and the improved accuracy over the state-of-the-art training methods. Finally, we prove the convergence of BinaryRelax under an approximate orthogonality condition.



### SCUT-FBP5500: A Diverse Benchmark Dataset for Multi-Paradigm Facial Beauty Prediction
- **Arxiv ID**: http://arxiv.org/abs/1801.06345v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06345v1)
- **Published**: 2018-01-19 09:53:19+00:00
- **Updated**: 2018-01-19 09:53:19+00:00
- **Authors**: Lingyu Liang, Luojun Lin, Lianwen Jin, Duorui Xie, Mengru Li
- **Comment**: 6 pages, 14 figures, conference paper
- **Journal**: None
- **Summary**: Facial beauty prediction (FBP) is a significant visual recognition problem to make assessment of facial attractiveness that is consistent to human perception. To tackle this problem, various data-driven models, especially state-of-the-art deep learning techniques, were introduced, and benchmark dataset become one of the essential elements to achieve FBP. Previous works have formulated the recognition of facial beauty as a specific supervised learning problem of classification, regression or ranking, which indicates that FBP is intrinsically a computation problem with multiple paradigms. However, most of FBP benchmark datasets were built under specific computation constrains, which limits the performance and flexibility of the computational model trained on the dataset. In this paper, we argue that FBP is a multi-paradigm computation problem, and propose a new diverse benchmark dataset, called SCUT-FBP5500, to achieve multi-paradigm facial beauty prediction. The SCUT-FBP5500 dataset has totally 5500 frontal faces with diverse properties (male/female, Asian/Caucasian, ages) and diverse labels (face landmarks, beauty scores within [1,~5], beauty score distribution), which allows different computational models with different FBP paradigms, such as appearance-based/shape-based facial beauty classification/regression model for male/female of Asian/Caucasian. We evaluated the SCUT-FBP5500 dataset for FBP using different combinations of feature and predictor, and various deep learning methods. The results indicates the improvement of FBP and the potential applications based on the SCUT-FBP5500.



### Proceedings of eNTERFACE 2015 Workshop on Intelligent Interfaces
- **Arxiv ID**: http://arxiv.org/abs/1801.06349v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1801.06349v1)
- **Published**: 2018-01-19 10:03:35+00:00
- **Updated**: 2018-01-19 10:03:35+00:00
- **Authors**: Matei Mancas, Christian Frisson, Joëlle Tilmanne, Nicolas d'Alessandro, Petr Barborka, Furkan Bayansar, Francisco Bernard, Rebecca Fiebrink, Alexis Heloir, Edgar Hemery, Sohaib Laraba, Alexis Moinet, Fabrizio Nunnari, Thierry Ravet, Loïc Reboursière, Alvaro Sarasua, Mickaël Tits, Noé Tits, François Zajéga, Paolo Alborno, Ksenia Kolykhalova, Emma Frid, Damiano Malafronte, Lisanne Huis in't Veld, Hüseyin Cakmak, Kevin El Haddad, Nicolas Riche, Julien Leroy, Pierre Marighetto, Bekir Berker Türker, Hossein Khaki, Roberto Pulisci, Emer Gilmartin, Fasih Haider, Kübra Cengiz, Martin Sulir, Ilaria Torre, Shabbir Marzban, Ramazan Yazıcı, Furkan Burak Bâgcı, Vedat Gazi Kılı, Hilal Sezer, Sena Büsra Yenge, Charles-Alexandre Delestage, Sylvie Leleu-Merviel, Muriel Meyer-Chemenska, Daniel Schmitt, Willy Yvart, Stéphane Dupont, Ozan Can Altiok, Aysegül Bumin, Ceren Dikmen, Ivan Giangreco, Silvan Heller, Emre Külah, Gueorgui Pironkov, Luca Rossetto, Yusuf Sahillioglu, Heiko Schuldt, Omar Seddati, Yusuf Setinkaya, Metin Sezgin, Claudiu Tanase, Emre Toyan, Sean Wood, Doguhan Yeke, Françcois Rocca, Pierre-Henri De Deken, Alessandra Bandrabur, Fabien Grisard, Axel Jean-Caurant, Vincent Courboulay, Radhwan Ben Madhkour, Ambroise Moreau
- **Comment**: 159 pages
- **Journal**: None
- **Summary**: The 11th Summer Workshop on Multimodal Interfaces eNTERFACE 2015 was hosted by the Numediart Institute of Creative Technologies of the University of Mons from August 10th to September 2015. During the four weeks, students and researchers from all over the world came together in the Numediart Institute of the University of Mons to work on eight selected projects structured around intelligent interfaces. Eight projects were selected and their reports are shown here.



### Transfer Learning for Improving Speech Emotion Classification Accuracy
- **Arxiv ID**: http://arxiv.org/abs/1801.06353v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1801.06353v4)
- **Published**: 2018-01-19 10:16:11+00:00
- **Updated**: 2020-07-28 01:36:53+00:00
- **Authors**: Siddique Latif, Rajib Rana, Shahzad Younis, Junaid Qadir, Julien Epps
- **Comment**: Proc. Interspeech 2018
- **Journal**: None
- **Summary**: The majority of existing speech emotion recognition research focuses on automatic emotion detection using training and testing data from same corpus collected under the same conditions. The performance of such systems has been shown to drop significantly in cross-corpus and cross-language scenarios. To address the problem, this paper exploits a transfer learning technique to improve the performance of speech emotion recognition systems that is novel in cross-language and cross-corpus scenarios. Evaluations on five different corpora in three different languages show that Deep Belief Networks (DBNs) offer better accuracy than previous approaches on cross-corpus emotion recognition, relative to a Sparse Autoencoder and SVM baseline system. Results also suggest that using a large number of languages for training and using a small fraction of the target data in training can significantly boost accuracy compared with baseline also for the corpus with limited training examples.



### What Makes Good Synthetic Training Data for Learning Disparity and Optical Flow Estimation?
- **Arxiv ID**: http://arxiv.org/abs/1801.06397v3
- **DOI**: 10.1007/s11263-018-1082-6
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.06397v3)
- **Published**: 2018-01-19 13:21:07+00:00
- **Updated**: 2018-03-22 10:26:58+00:00
- **Authors**: Nikolaus Mayer, Eddy Ilg, Philipp Fischer, Caner Hazirbas, Daniel Cremers, Alexey Dosovitskiy, Thomas Brox
- **Comment**: added references (UCL dataset); added IJCV copyright information
- **Journal**: None
- **Summary**: The finding that very large networks can be trained efficiently and reliably has led to a paradigm shift in computer vision from engineered solutions to learning formulations. As a result, the research challenge shifts from devising algorithms to creating suitable and abundant training data for supervised learning. How to efficiently create such training data? The dominant data acquisition method in visual recognition is based on web data and manual annotation. Yet, for many computer vision problems, such as stereo or optical flow estimation, this approach is not feasible because humans cannot manually enter a pixel-accurate flow field. In this paper, we promote the use of synthetically generated data for the purpose of training deep networks on such tasks.We suggest multiple ways to generate such data and evaluate the influence of dataset properties on the performance and generalization properties of the resulting networks. We also demonstrate the benefit of learning schedules that use different types of data at selected stages of the training process.



### EffNet: An Efficient Structure for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.06434v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1801.06434v6)
- **Published**: 2018-01-19 14:57:23+00:00
- **Updated**: 2018-06-05 12:10:12+00:00
- **Authors**: Ido Freeman, Lutz Roese-Koerner, Anton Kummert
- **Comment**: None
- **Journal**: None
- **Summary**: With the ever increasing application of Convolutional Neural Networks to customer products the need emerges for models to efficiently run on embedded, mobile hardware. Slimmer models have therefore become a hot research topic with various approaches which vary from binary networks to revised convolution layers. We offer our contribution to the latter and propose a novel convolution block which significantly reduces the computational burden while surpassing the current state-of-the-art. Our model, dubbed EffNet, is optimised for models which are slim to begin with and is created to tackle issues in existing models such as MobileNet and ShuffleNet.



### Quality Classified Image Analysis with Application to Face Detection and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.06445v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06445v1)
- **Published**: 2018-01-19 15:18:21+00:00
- **Updated**: 2018-01-19 15:18:21+00:00
- **Authors**: Fei Yang, Qian Zhang, Miaohui Wang, Guoping Qiu
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Motion blur, out of focus, insufficient spatial resolution, lossy compression and many other factors can all cause an image to have poor quality. However, image quality is a largely ignored issue in traditional pattern recognition literature. In this paper, we use face detection and recognition as case studies to show that image quality is an essential factor which will affect the performances of traditional algorithms. We demonstrated that it is not the image quality itself that is the most important, but rather the quality of the images in the training set should have similar quality as those in the testing set. To handle real-world application scenarios where images with different kinds and severities of degradation can be presented to the system, we have developed a quality classified image analysis framework to deal with images of mixed qualities adaptively. We use deep neural networks first to classify images based on their quality classes and then design a separate face detector and recognizer for images in each quality class. We will present experimental results to show that our quality classified framework can accurately classify images based on the type and severity of image degradations and can significantly boost the performances of state-of-the-art face detector and recognizer in dealing with image datasets containing mixed quality images.



### Quantitative analysis of patch-based fully convolutional neural networks for tissue segmentation on brain magnetic resonance imaging
- **Arxiv ID**: http://arxiv.org/abs/1801.06457v2
- **DOI**: 10.1109/ACCESS.2019.2926697
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06457v2)
- **Published**: 2018-01-19 15:24:15+00:00
- **Updated**: 2018-02-19 11:43:59+00:00
- **Authors**: Jose Bernal, Kaisar Kushibar, Mariano Cabezas, Sergi Valverde, Arnau Oliver, Xavier Lladó
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate brain tissue segmentation in Magnetic Resonance Imaging (MRI) has attracted the attention of medical doctors and researchers since variations in tissue volume help in diagnosing and monitoring neurological diseases. Several proposals have been designed throughout the years comprising conventional machine learning strategies as well as convolutional neural networks (CNN) approaches. In particular, in this paper, we analyse a sub-group of deep learning methods producing dense predictions. This branch, referred in the literature as Fully CNN (FCNN), is of interest as these architectures can process an input volume in less time than CNNs and local spatial dependencies may be encoded since several voxels are classified at once. Our study focuses on understanding architectural strengths and weaknesses of literature-like approaches. Hence, we implement eight FCNN architectures inspired by robust state-of-the-art methods on brain segmentation related tasks. We evaluate them using the IBSR18, MICCAI2012 and iSeg2017 datasets as they contain infant and adult data and exhibit varied voxel spacing, image quality, number of scans and available imaging modalities. The discussion is driven in three directions: comparison between 2D and 3D approaches, the importance of multiple modalities and overlapping as a sampling strategy for training and testing models. To encourage other researchers to explore the evaluation framework, a public version is accessible to download from our research website.



### Detecting and counting tiny faces
- **Arxiv ID**: http://arxiv.org/abs/1801.06504v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.06504v2)
- **Published**: 2018-01-19 17:41:12+00:00
- **Updated**: 2018-01-24 16:04:15+00:00
- **Authors**: Alexandre Attia, Sharone Dayan
- **Comment**: 4 pages, 10 figures, 2 appendix page
- **Journal**: None
- **Summary**: Finding Tiny Faces (by Hu and Ramanan) proposes a novel approach to find small objects in an image. Our contribution consists in deeply understanding the choices of the paper together with applying and extending a similar method to a real world subject which is the counting of people in a public demonstration.



### Image Provenance Analysis at Scale
- **Arxiv ID**: http://arxiv.org/abs/1801.06510v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1801.06510v2)
- **Published**: 2018-01-19 17:54:22+00:00
- **Updated**: 2018-01-23 14:41:34+00:00
- **Authors**: Daniel Moreira, Aparna Bharati, Joel Brogan, Allan Pinto, Michael Parowski, Kevin W. Bowyer, Patrick J. Flynn, Anderson Rocha, Walter J. Scheirer
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Prior art has shown it is possible to estimate, through image processing and computer vision techniques, the types and parameters of transformations that have been applied to the content of individual images to obtain new images. Given a large corpus of images and a query image, an interesting further step is to retrieve the set of original images whose content is present in the query image, as well as the detailed sequences of transformations that yield the query image given the original images. This is a problem that recently has received the name of image provenance analysis. In these times of public media manipulation ( e.g., fake news and meme sharing), obtaining the history of image transformations is relevant for fact checking and authorship verification, among many other applications. This article presents an end-to-end processing pipeline for image provenance analysis, which works at real-world scale. It employs a cutting-edge image filtering solution that is custom-tailored for the problem at hand, as well as novel techniques for obtaining the provenance graph that expresses how the images, as nodes, are ancestrally connected. A comprehensive set of experiments for each stage of the pipeline is provided, comparing the proposed solution with state-of-the-art results, employing previously published datasets. In addition, this work introduces a new dataset of real-world provenance cases from the social media site Reddit, along with baseline results.



### Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights
- **Arxiv ID**: http://arxiv.org/abs/1801.06519v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06519v2)
- **Published**: 2018-01-19 18:25:59+00:00
- **Updated**: 2018-03-16 21:29:28+00:00
- **Authors**: Arun Mallya, Dillon Davis, Svetlana Lazebnik
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents a method for adapting a single, fixed deep neural network to multiple tasks without affecting performance on already learned tasks. By building upon ideas from network quantization and pruning, we learn binary masks that piggyback on an existing network, or are applied to unmodified weights of that network to provide good performance on a new task. These masks are learned in an end-to-end differentiable fashion, and incur a low overhead of 1 bit per network parameter, per task. Even though the underlying network is fixed, the ability to mask individual weights allows for the learning of a large number of filters. We show performance comparable to dedicated fine-tuned networks for a variety of classification tasks, including those with large domain shifts from the initial task (ImageNet), and a variety of network architectures. Unlike prior work, we do not suffer from catastrophic forgetting or competition between tasks, and our performance is agnostic to task ordering. Code available at https://github.com/arunmallya/piggyback.



### How would surround vehicles move? A Unified Framework for Maneuver Classification and Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/1801.06523v1
- **DOI**: 10.1109/TIV.2018.2804159
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06523v1)
- **Published**: 2018-01-19 18:31:27+00:00
- **Updated**: 2018-01-19 18:31:27+00:00
- **Authors**: Nachiket Deo, Akshay Rangesh, Mohan M. Trivedi
- **Comment**: Accepted for publication in IEEE transactions on Intelligent Vehicles
- **Journal**: IEEE Transactions on Intelligent Vehicles ( Volume: 3, Issue: 2,
  June 2018 )
- **Summary**: Reliable prediction of surround vehicle motion is a critical requirement for path planning for autonomous vehicles. In this paper we propose a unified framework for surround vehicle maneuver classification and motion prediction that exploits multiple cues, namely, the estimated motion of vehicles, an understanding of typical motion patterns of freeway traffic and inter-vehicle interaction. We report our results in terms of maneuver classification accuracy and mean and median absolute error of predicted trajectories against the ground truth for real traffic data collected using vehicle mounted sensors on freeways. An ablative analysis is performed to analyze the relative importance of each cue for trajectory prediction. Additionally, an analysis of execution time for the components of the framework is presented. Finally, we present multiple case studies analyzing the outputs of our model for complex traffic scenarios



### A machine learning approach to reconstruction of heart surface potentials from body surface potentials
- **Arxiv ID**: http://arxiv.org/abs/1802.02240v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.02240v1)
- **Published**: 2018-01-19 22:36:46+00:00
- **Updated**: 2018-01-19 22:36:46+00:00
- **Authors**: Avinash Malik, Tommy Peng, Mark Trew
- **Comment**: 4 pages, 9 Figures, 1 Table
- **Journal**: None
- **Summary**: Invasive cardiac catheterisation is a common procedure that is carried out before surgical intervention. Yet, invasive cardiac diagnostics are full of risks, especially for young children. Decades of research has been conducted on the so called inverse problem of electrocardiography, which can be used to reconstruct Heart Surface Potentials (HSPs) from Body Surface Potentials (BSPs), for non-invasive diagnostics. State of the art solutions to the inverse problem are unsatisfactory, since the inverse problem is known to be ill-posed. In this paper we propose a novel approach to reconstructing HSPs from BSPs using a Time-Delay Artificial Neural Network (TDANN). We first design the TDANN architecture, and then develop an iterative search space algorithm to find the parameters of the TDANN, which results in the best overall HSP prediction. We use real-world recorded BSPs and HSPs from individuals suffering from serious cardiac conditions to validate our TDANN. The results are encouraging, in that coefficients obtained by correlating the predicted HSP with the recorded patient' HSP approach ideal values.



### A Foreground Inference Network for Video Surveillance Using Multi-View Receptive Field
- **Arxiv ID**: http://arxiv.org/abs/1801.06593v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06593v1)
- **Published**: 2018-01-19 23:01:16+00:00
- **Updated**: 2018-01-19 23:01:16+00:00
- **Authors**: Thangarajah Akilan
- **Comment**: None
- **Journal**: None
- **Summary**: Foreground (FG) pixel labelling plays a vital role in video surveillance. Recent engineering solutions have attempted to exploit the efficacy of deep learning (DL) models initially targeted for image classification to deal with FG pixel labelling. One major drawback of such strategy is the lacking delineation of visual objects when training samples are limited. To grapple with this issue, we introduce a multi-view receptive field fully convolutional neural network (MV-FCN) that harness recent seminal ideas, such as, fully convolutional structure, inception modules, and residual networking. Therefrom, we implement a system in an encoder-decoder fashion that subsumes a core and two complementary feature flow paths. The model exploits inception modules at early and late stages with three different sizes of receptive fields to capture invariance at various scales. The features learned in the encoding phase are fused with appropriate feature maps in the decoding phase through residual connections for achieving enhanced spatial representation. These multi-view receptive fields and residual feature connections are expected to yield highly generalized features for an accurate pixel-wise FG region identification. It is, then, trained with database specific exemplary segmentations to predict desired FG objects.   The comparative experimental results on eleven benchmark datasets validate that the proposed model achieves very competitive performance with the prior- and state-of-the-art algorithms. We also report that how well a transfer learning approach can be useful to enhance the performance of our proposed MV-FCN.



