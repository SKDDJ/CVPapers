# Arxiv Papers in cs.CV on 2018-01-29
### Document Image Classification with Intra-Domain Transfer Learning and Stacked Generalization of Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.09321v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.09321v3)
- **Published**: 2018-01-29 00:03:54+00:00
- **Updated**: 2018-08-30 21:54:44+00:00
- **Authors**: Arindam Das, Saikat Roy, Ujjwal Bhattacharya, Swapan Kumar Parui
- **Comment**: Accepted in 24th International Conference in Pattern Recognition
  (ICPR), Beijing, China, 2018
- **Journal**: None
- **Summary**: In this work, a region-based Deep Convolutional Neural Network framework is proposed for document structure learning. The contribution of this work involves efficient training of region based classifiers and effective ensembling for document image classification. A primary level of `inter-domain' transfer learning is used by exporting weights from a pre-trained VGG16 architecture on the ImageNet dataset to train a document classifier on whole document images. Exploiting the nature of region based influence modelling, a secondary level of `intra-domain' transfer learning is used for rapid training of deep learning models for image segments. Finally, stacked generalization based ensembling is utilized for combining the predictions of the base deep neural network models. The proposed method achieves state-of-the-art accuracy of 92.2% on the popular RVL-CDIP document image dataset, exceeding benchmarks set by existing algorithms.



### Stochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.09335v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1801.09335v1)
- **Published**: 2018-01-29 01:16:03+00:00
- **Updated**: 2018-01-29 01:16:03+00:00
- **Authors**: Jason Kuen, Xiangfei Kong, Zhe Lin, Gang Wang, Jianxiong Yin, Simon See, Yap-Peng Tan
- **Comment**: None
- **Journal**: None
- **Summary**: It is desirable to train convolutional networks (CNNs) to run more efficiently during inference. In many cases however, the computational budget that the system has for inference cannot be known beforehand during training, or the inference budget is dependent on the changing real-time resource availability. Thus, it is inadequate to train just inference-efficient CNNs, whose inference costs are not adjustable and cannot adapt to varied inference budgets. We propose a novel approach for cost-adjustable inference in CNNs - Stochastic Downsampling Point (SDPoint). During training, SDPoint applies feature map downsampling to a random point in the layer hierarchy, with a random downsampling ratio. The different stochastic downsampling configurations known as SDPoint instances (of the same model) have computational costs different from each other, while being trained to minimize the same prediction loss. Sharing network parameters across different instances provides significant regularization boost. During inference, one may handpick a SDPoint instance that best fits the inference budget. The effectiveness of SDPoint, as both a cost-adjustable inference approach and a regularizer, is validated through extensive experiments on image classification.



### Game of Sketches: Deep Recurrent Models of Pictionary-style Word Guessing
- **Arxiv ID**: http://arxiv.org/abs/1801.09356v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1801.09356v1)
- **Published**: 2018-01-29 03:54:03+00:00
- **Updated**: 2018-01-29 03:54:03+00:00
- **Authors**: Ravi Kiran Sarvadevabhatla, Shiv Surya, Trisha Mittal, Venkatesh Babu Radhakrishnan
- **Comment**: To be presented at AAAI-2018. Code, pre-trained models and dataset at
  github.com/val-iisc/sketchguess
- **Journal**: None
- **Summary**: The ability of intelligent agents to play games in human-like fashion is popularly considered a benchmark of progress in Artificial Intelligence. Similarly, performance on multi-disciplinary tasks such as Visual Question Answering (VQA) is considered a marker for gauging progress in Computer Vision. In our work, we bring games and VQA together. Specifically, we introduce the first computational model aimed at Pictionary, the popular word-guessing social game. We first introduce Sketch-QA, an elementary version of Visual Question Answering task. Styled after Pictionary, Sketch-QA uses incrementally accumulated sketch stroke sequences as visual data. Notably, Sketch-QA involves asking a fixed question ("What object is being drawn?") and gathering open-ended guess-words from human guessers. We analyze the resulting dataset and present many interesting findings therein. To mimic Pictionary-style guessing, we subsequently propose a deep neural model which generates guess-words in response to temporally evolving human-drawn sketches. Our model even makes human-like mistakes while guessing, thus amplifying the human mimicry factor. We evaluate our model on the large-scale guess-word dataset generated via Sketch-QA task and compare with various baselines. We also conduct a Visual Turing Test to obtain human impressions of the guess-words generated by humans and our model. Experimental results demonstrate the promise of our approach for Pictionary and similarly themed games.



### Comparative Study of ECO and CFNet Trackers in Noisy Environment
- **Arxiv ID**: http://arxiv.org/abs/1801.09360v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09360v1)
- **Published**: 2018-01-29 04:56:32+00:00
- **Updated**: 2018-01-29 04:56:32+00:00
- **Authors**: Mustansar Fiaz, Sajid Javed, Arif Mahmood, Soon Ki Jung
- **Comment**: 4 pages, 5 figures
- **Journal**: None
- **Summary**: Object tracking is one of the most challenging task and has secured significant attention of computer vision researchers in the past two decades. Recent deep learning based trackers have shown good performance on various tracking challenges. A tracking method should track objects in sequential frames accurately in challenges such as deformation, low resolution, occlusion, scale and light variations. Most trackers achieve good performance on specific challenges instead of all tracking problems, hence there is a lack of general purpose tracking algorithms that can perform well in all conditions. Moreover, performance of tracking techniques has not been evaluated in noisy environments. Visual object tracking has real world applications and there is good chance that noise may get added during image acquisition in surveillance cameras. We aim to study the robustness of two state of the art trackers in the presence of noise including Efficient Convolutional Operators (ECO) and Correlation Filter Network (CFNet). Our study demonstrates that the performance of these trackers degrades as the noise level increases, which demonstrate the need to design more robust tracking algorithms.



### Shift-Net: Image Inpainting via Deep Feature Rearrangement
- **Arxiv ID**: http://arxiv.org/abs/1801.09392v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09392v2)
- **Published**: 2018-01-29 08:13:49+00:00
- **Updated**: 2018-04-13 12:43:24+00:00
- **Authors**: Zhaoyi Yan, Xiaoming Li, Mu Li, Wangmeng Zuo, Shiguang Shan
- **Comment**: 25 pages, 17 figures, 1 table, main paper + supplementary material
- **Journal**: None
- **Summary**: Deep convolutional networks (CNNs) have exhibited their potential in image inpainting for producing plausible results. However, in most existing methods, e.g., context encoder, the missing parts are predicted by propagating the surrounding convolutional features through a fully connected layer, which intends to produce semantically plausible but blurry result. In this paper, we introduce a special shift-connection layer to the U-Net architecture, namely Shift-Net, for filling in missing regions of any shape with sharp structures and fine-detailed textures. To this end, the encoder feature of the known region is shifted to serve as an estimation of the missing parts. A guidance loss is introduced on decoder feature to minimize the distance between the decoder feature after fully connected layer and the ground-truth encoder feature of the missing parts. With such constraint, the decoder feature in missing region can be used to guide the shift of encoder feature in known region. An end-to-end learning algorithm is further developed to train the Shift-Net. Experiments on the Paris StreetView and Places datasets demonstrate the efficiency and effectiveness of our Shift-Net in producing sharper, fine-detailed, and visually plausible results. The codes and pre-trained models are available at https://github.com/Zhaoyi-Yan/Shift-Net.



### CosFace: Large Margin Cosine Loss for Deep Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.09414v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09414v2)
- **Published**: 2018-01-29 09:23:55+00:00
- **Updated**: 2018-04-03 11:15:57+00:00
- **Authors**: Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, Wei Liu
- **Comment**: Accepted by CVPR 2018
- **Journal**: None
- **Summary**: Face recognition has made extraordinary progress owing to the advancement of deep convolutional neural networks (CNNs). The central task of face recognition, including face verification and identification, involves face feature discrimination. However, the traditional softmax loss of deep CNNs usually lacks the power of discrimination. To address this problem, recently several loss functions such as center loss, large margin softmax loss, and angular softmax loss have been proposed. All these improved losses share the same idea: maximizing inter-class variance and minimizing intra-class variance. In this paper, we propose a novel loss function, namely large margin cosine loss (LMCL), to realize this idea from a different perspective. More specifically, we reformulate the softmax loss as a cosine loss by $L_2$ normalizing both features and weight vectors to remove radial variations, based on which a cosine margin term is introduced to further maximize the decision margin in the angular space. As a result, minimum intra-class variance and maximum inter-class variance are achieved by virtue of normalization and cosine decision margin maximization. We refer to our model trained with LMCL as CosFace. Extensive experimental evaluations are conducted on the most popular public-domain face recognition datasets such as MegaFace Challenge, Youtube Faces (YTF) and Labeled Face in the Wild (LFW). We achieve the state-of-the-art performance on these benchmarks, which confirms the effectiveness of our proposed approach.



### Local Visual Microphones: Improved Sound Extraction from Silent Video
- **Arxiv ID**: http://arxiv.org/abs/1801.09436v1
- **DOI**: 10.5244/C.31.102
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09436v1)
- **Published**: 2018-01-29 10:29:43+00:00
- **Updated**: 2018-01-29 10:29:43+00:00
- **Authors**: Mohammad Amin Shabani, Laleh Samadfam, Mohammad Amin Sadeghi
- **Comment**: Accepted to BMVC 2017
- **Journal**: None
- **Summary**: Sound waves cause small vibrations in nearby objects. A few techniques exist in the literature that can extract sound from video. In this paper we study local vibration patterns at different image locations. We show that different locations in the image vibrate differently. We carefully aggregate local vibrations and produce a sound quality that improves state-of-the-art. We show that local vibrations could have a time delay because sound waves take time to travel through the air. We use this phenomenon to estimate sound direction. We also present a novel algorithm that speeds up sound extraction by two to three orders of magnitude and reaches real-time performance in a 20KHz video.



### TernaryNet: Faster Deep Model Inference without GPUs for Medical 3D Segmentation using Sparse and Binary Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1801.09449v1
- **DOI**: 10.1007/s11548-018-1797-4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09449v1)
- **Published**: 2018-01-29 11:13:43+00:00
- **Updated**: 2018-01-29 11:13:43+00:00
- **Authors**: Mattias P. Heinrich, Max Blendowski, Ozan Oktay
- **Comment**: None
- **Journal**: International Journal of Computer Assisted Radiology and Surgery
  2018
- **Summary**: Deep convolutional neural networks (DCNN) are currently ubiquitous in medical imaging. While their versatility and high quality results for common image analysis tasks including segmentation, localisation and prediction is astonishing, the large representational power comes at the cost of highly demanding computational effort. This limits their practical applications for image guided interventions and diagnostic (point-of-care) support using mobile devices without graphics processing units (GPU). We propose a new scheme that approximates both trainable weights and neural activations in deep networks by ternary values and tackles the open question of backpropagation when dealing with non-differentiable functions. Our solution enables the removal of the expensive floating-point matrix multiplications throughout any convolutional neural network and replaces them by energy and time preserving binary operators and population counts. Our approach, which is demonstrated using a fully-convolutional network (FCN) for CT pancreas segmentation leads to more than 10-fold reduced memory requirements and we provide a concept for sub-second inference without GPUs. Our ternary approximation obtains high accuracies (without any post-processing) with a Dice overlap of 71.0% that are statistically equivalent to using networks with high-precision weights and activations. We further demonstrate the significant improvements reached in comparison to binary quantisation and without our proposed ternary hyperbolic tangent continuation. We present a key enabling technique for highly efficient DCNN inference without GPUs that will help to bring the advances of deep learning to practical clinical applications. It has also great promise for improving accuracies in large-scale medical data retrieval.



### Road Damage Detection Using Deep Neural Networks with Images Captured Through a Smartphone
- **Arxiv ID**: http://arxiv.org/abs/1801.09454v2
- **DOI**: 10.1111/mice.12387
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1801.09454v2)
- **Published**: 2018-01-29 11:25:20+00:00
- **Updated**: 2018-02-02 03:23:24+00:00
- **Authors**: Hiroya Maeda, Yoshihide Sekimoto, Toshikazu Seto, Takehiro Kashiyama, Hiroshi Omata
- **Comment**: 14 pages, 7 figures
- **Journal**: Computer Aided Civil and Infrastructure Engineering, 2018
- **Summary**: Research on damage detection of road surfaces using image processing techniques has been actively conducted, achieving considerably high detection accuracies. Many studies only focus on the detection of the presence or absence of damage. However, in a real-world scenario, when the road managers from a governing body need to repair such damage, they need to clearly understand the type of damage in order to take effective action. In addition, in many of these previous studies, the researchers acquire their own data using different methods. Hence, there is no uniform road damage dataset available openly, leading to the absence of a benchmark for road damage detection. This study makes three contributions to address these issues. First, to the best of our knowledge, for the first time, a large-scale road damage dataset is prepared. This dataset is composed of 9,053 road damage images captured with a smartphone installed on a car, with 15,435 instances of road surface damage included in these road images. In order to generate this dataset, we cooperated with 7 municipalities in Japan and acquired road images for more than 40 hours. These images were captured in a wide variety of weather and illuminance conditions. In each image, we annotated the bounding box representing the location and type of damage. Next, we used a state-of-the-art object detection method using convolutional neural networks to train the damage detection model with our dataset, and compared the accuracy and runtime speed on both, using a GPU server and a smartphone. Finally, we demonstrate that the type of damage can be classified into eight types with high accuracy by applying the proposed object detection method. The road damage dataset, our experimental results, and the developed smartphone application used in this study are publicly available (https://github.com/sekilab/RoadDamageDetector/).



### Hierarchical Spatial Transformer Network
- **Arxiv ID**: http://arxiv.org/abs/1801.09467v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09467v2)
- **Published**: 2018-01-29 12:09:58+00:00
- **Updated**: 2018-01-30 02:33:41+00:00
- **Authors**: Chang Shu, Xi Chen, Qiwei Xie, Hua Han
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision researchers have been expecting that neural networks have spatial transformation ability to eliminate the interference caused by geometric distortion for a long time. Emergence of spatial transformer network makes dream come true. Spatial transformer network and its variants can handle global displacement well, but lack the ability to deal with local spatial variance. Hence how to achieve a better manner of deformation in the neural network has become a pressing matter of the moment. To address this issue, we analyze the advantages and disadvantages of approximation theory and optical flow theory, then we combine them to propose a novel way to achieve image deformation and implement it with a hierarchical convolutional neural network. This new approach solves for a linear deformation along with an optical flow field to model image deformation. In the experiments of cluttered MNIST handwritten digits classification and image plane alignment, our method outperforms baseline methods by a large margin.



### DeepSIC: Deep Semantic Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1801.09468v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09468v1)
- **Published**: 2018-01-29 12:10:04+00:00
- **Updated**: 2018-01-29 12:10:04+00:00
- **Authors**: Sihui Luo, Yezhou Yang, Mingli Song
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: Incorporating semantic information into the codecs during image compression can significantly reduce the repetitive computation of fundamental semantic analysis (such as object recognition) in client-side applications. The same practice also enable the compressed code to carry the image semantic information during storage and transmission. In this paper, we propose a concept called Deep Semantic Image Compression (DeepSIC) and put forward two novel architectures that aim to reconstruct the compressed image and generate corresponding semantic representations at the same time. The first architecture performs semantic analysis in the encoding process by reserving a portion of the bits from the compressed code to store the semantic representations. The second performs semantic analysis in the decoding step with the feature maps that are embedded in the compressed code. In both architectures, the feature maps are shared by the compression and the semantic analytics modules. To validate our approaches, we conduct experiments on the publicly available benchmarking datasets and achieve promising results. We also provide a thorough analysis of the advantages and disadvantages of the proposed technique.



### Hyper-Hue and EMAP on Hyperspectral Images for Supervised Layer Decomposition of Old Master Drawings
- **Arxiv ID**: http://arxiv.org/abs/1801.09472v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DL
- **Links**: [PDF](http://arxiv.org/pdf/1801.09472v2)
- **Published**: 2018-01-29 12:29:44+00:00
- **Updated**: 2018-05-28 12:30:48+00:00
- **Authors**: AmirAbbas Davari, Nikolaos Sakaltras, Armin Haeberle, Sulaiman Vesal, Vincent Christlein, Andreas Maier, Christian Riess
- **Comment**: None
- **Journal**: None
- **Summary**: Old master drawings were mostly created step by step in several layers using different materials. To art historians and restorers, examination of these layers brings various insights into the artistic work process and helps to answer questions about the object, its attribution and its authenticity. However, these layers typically overlap and are oftentimes difficult to differentiate with the unaided eye. For example, a common layer combination is red chalk under ink.   In this work, we propose an image processing pipeline that operates on hyperspectral images to separate such layers. Using this pipeline, we show that hyperspectral images enable better layer separation than RGB images, and that spectral focus stacking aids the layer separation. In particular, we propose to use two descriptors in hyperspectral historical document analysis, namely hyper-hue and extended multi-attribute profile (EMAP). Our comparative results with other features underline the efficacy of the three proposed improvements.



### Histogram of Oriented Depth Gradients for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.09477v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09477v1)
- **Published**: 2018-01-29 12:38:31+00:00
- **Updated**: 2018-01-29 12:38:31+00:00
- **Authors**: Nachwa Abou Bakr, James Crowley
- **Comment**: ORASIS 2017, Jun 2017, Colleville-sur-Mer, France. 2017
- **Journal**: None
- **Summary**: In this paper, we report on experiments with the use of local measures for depth motion for visual action recognition from MPEG encoded RGBD video sequences. We show that such measures can be combined with local space-time video descriptors for appearance to provide a computationally efficient method for recognition of actions. Fisher vectors are used for encoding and concatenating a depth descriptor with existing RGB local descriptors. We then employ a linear SVM for recognizing manipulation actions using such vectors. We evaluate the effectiveness of such measures by comparison to the state-of-the-art using two recent datasets for action recognition in kitchen environments.



### Learning-based Image Reconstruction via Parallel Proximal Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1801.09518v1
- **DOI**: 10.1109/LSP.2018.2833812
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09518v1)
- **Published**: 2018-01-29 14:17:28+00:00
- **Updated**: 2018-01-29 14:17:28+00:00
- **Authors**: Emrah Bostan, Ulugbek S. Kamilov, Laura Waller
- **Comment**: None
- **Journal**: None
- **Summary**: In the past decade, sparsity-driven regularization has led to advancement of image reconstruction algorithms. Traditionally, such regularizers rely on analytical models of sparsity (e.g. total variation (TV)). However, more recent methods are increasingly centered around data-driven arguments inspired by deep learning. In this letter, we propose to generalize TV regularization by replacing the l1-penalty with an alternative prior that is trainable. Specifically, our method learns the prior via extending the recently proposed fast parallel proximal algorithm (FPPA) to incorporate data-adaptive proximal operators. The proposed framework does not require additional inner iterations for evaluating the proximal mappings of the corresponding learned prior. Moreover, our formalism ensures that the training and reconstruction processes share the same algorithmic structure, making the end-to-end implementation intuitive. As an example, we demonstrate our algorithm on the problem of deconvolution in a fluorescence microscope.



### End-to-End Fine-Grained Action Segmentation and Recognition Using Conditional Random Field Models and Discriminative Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/1801.09571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09571v1)
- **Published**: 2018-01-29 15:24:56+00:00
- **Updated**: 2018-01-29 15:24:56+00:00
- **Authors**: Effrosyni Mavroudi, Divya Bhaskara, Shahin Sefati, Haider Ali, René Vidal
- **Comment**: Camera ready version accepted at WACV 2018
- **Journal**: None
- **Summary**: Fine-grained action segmentation and recognition is an important yet challenging task. Given a long, untrimmed sequence of kinematic data, the task is to classify the action at each time frame and segment the time series into the correct sequence of actions. In this paper, we propose a novel framework that combines a temporal Conditional Random Field (CRF) model with a powerful frame-level representation based on discriminative sparse coding. We introduce an end-to-end algorithm for jointly learning the weights of the CRF model, which include action classification and action transition costs, as well as an overcomplete dictionary of mid-level action primitives. This results in a CRF model that is driven by sparse coding features obtained using a discriminative dictionary that is shared among different actions and adapted to the task of structured output learning. We evaluate our method on three surgical tasks using kinematic data from the JIGSAWS dataset, as well as on a food preparation task using accelerometer data from the 50 Salads dataset. Our results show that the proposed method performs on par or better than state-of-the-art methods.



### Improving Multiple Object Tracking with Optical Flow and Edge Preprocessing
- **Arxiv ID**: http://arxiv.org/abs/1801.09646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09646v1)
- **Published**: 2018-01-29 17:42:33+00:00
- **Updated**: 2018-01-29 17:42:33+00:00
- **Authors**: David-Alexandre Beaupré, Guillaume-Alexandre Bilodeau, Nicolas Saunier
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a new method for detecting road users in an urban environment which leads to an improvement in multiple object tracking. Our method takes as an input a foreground image and improves the object detection and segmentation. This new image can be used as an input to trackers that use foreground blobs from background subtraction. The first step is to create foreground images for all the frames in an urban video. Then, starting from the original blobs of the foreground image, we merge the blobs that are close to one another and that have similar optical flow. The next step is extracting the edges of the different objects to detect multiple objects that might be very close (and be merged in the same blob) and to adjust the size of the original blobs. At the same time, we use the optical flow to detect occlusion of objects that are moving in opposite directions. Finally, we make a decision on which information we keep in order to construct a new foreground image with blobs that can be used for tracking. The system is validated on four videos of an urban traffic dataset. Our method improves the recall and precision metrics for the object detection task compared to the vanilla background subtraction method and improves the CLEAR MOT metrics in the tracking tasks for most videos.



### Object-based reasoning in VQA
- **Arxiv ID**: http://arxiv.org/abs/1801.09718v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09718v1)
- **Published**: 2018-01-29 19:24:51+00:00
- **Updated**: 2018-01-29 19:24:51+00:00
- **Authors**: Mikyas T. Desta, Larry Chen, Tomasz Kornuta
- **Comment**: 10 pages, 15 figures, published as a conference paper at 2018 IEEE
  Winter Conf. on Applications of Computer Vision (WACV'2018)
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) is a novel problem domain where multi-modal inputs must be processed in order to solve the task given in the form of a natural language. As the solutions inherently require to combine visual and natural language processing with abstract reasoning, the problem is considered as AI-complete. Recent advances indicate that using high-level, abstract facts extracted from the inputs might facilitate reasoning. Following that direction we decided to develop a solution combining state-of-the-art object detection and reasoning modules. The results, achieved on the well-balanced CLEVR dataset, confirm the promises and show significant, few percent improvements of accuracy on the complex "counting" task.



### Deep Learning based Retinal OCT Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1801.09749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09749v1)
- **Published**: 2018-01-29 20:44:40+00:00
- **Updated**: 2018-01-29 20:44:40+00:00
- **Authors**: Mike Pekala, Neil Joshi, David E. Freund, Neil M. Bressler, Delia Cabrera DeBuc, Philippe M Burlina
- **Comment**: None
- **Journal**: None
- **Summary**: Our objective is to evaluate the efficacy of methods that use deep learning (DL) for the automatic fine-grained segmentation of optical coherence tomography (OCT) images of the retina. OCT images from 10 patients with mild non-proliferative diabetic retinopathy were used from a public (U. of Miami) dataset. For each patient, five images were available: one image of the fovea center, two images of the perifovea, and two images of the parafovea. For each image, two expert graders each manually annotated five retinal surfaces (i.e. boundaries between pairs of retinal layers). The first grader's annotations were used as ground truth and the second grader's annotations to compute inter-operator agreement. The proposed automated approach segments images using fully convolutional networks (FCNs) together with Gaussian process (GP)-based regression as a post-processing step to improve the quality of the estimates. Using 10-fold cross validation, the performance of the algorithms is determined by computing the per-pixel unsigned error (distance) between the automated estimates and the ground truth annotations generated by the first manual grader. We compare the proposed method against five state of the art automatic segmentation techniques. The results show that the proposed methods compare favorably with state of the art techniques, resulting in the smallest mean unsigned error values and associated standard deviations, and performance is comparable with human annotation of retinal layers from OCT when there is only mild retinopathy. The results suggest that semantic segmentation using FCNs, coupled with regression-based post-processing, can effectively solve the OCT segmentation problem on par with human capabilities with mild retinopathy.



### Denoising Arterial Spin Labeling Cerebral Blood Flow Images Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1801.09672v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09672v1)
- **Published**: 2018-01-29 20:54:29+00:00
- **Updated**: 2018-01-29 20:54:29+00:00
- **Authors**: Danfeng Xie, Li Bai, Ze Wang
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Arterial spin labeling perfusion MRI is a noninvasive technique for measuring quantitative cerebral blood flow (CBF), but the measurement is subject to a low signal-to-noise-ratio(SNR). Various post-processing methods have been proposed to denoise ASL MRI but only provide moderate improvement. Deep learning (DL) is an emerging technique that can learn the most representative signal from data without prior modeling which can be highly complex and analytically indescribable. The purpose of this study was to assess whether the record breaking performance of DL can be translated into ASL MRI denoising. We used convolutional neural network (CNN) to build the DL ASL denosing model (DL-ASL) to inherently consider the inter-voxel correlations. To better guide DL-ASL training, we incorporated prior knowledge about ASL MRI: the structural similarity between ASL CBF map and grey matter probability map. A relatively large sample data were used to train the model which was subsequently applied to a new set of data for testing. Experimental results showed that DL-ASL achieved state-of-the-art denoising performance for ASL MRI as compared to current routine methods in terms of higher SNR, keeping CBF quantification quality while shorten the acquisition time by 75%, and automatic partial volume correction.



