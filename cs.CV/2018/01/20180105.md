# Arxiv Papers in cs.CV on 2018-01-05
### Combination of Hyperband and Bayesian Optimization for Hyperparameter Optimization in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1801.01596v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.01596v1)
- **Published**: 2018-01-05 01:00:03+00:00
- **Updated**: 2018-01-05 01:00:03+00:00
- **Authors**: Jiazhuo Wang, Jason Xu, Xuejun Wang
- **Comment**: preprint
- **Journal**: None
- **Summary**: Deep learning has achieved impressive results on many problems. However, it requires high degree of expertise or a lot of experience to tune well the hyperparameters, and such manual tuning process is likely to be biased. Moreover, it is not practical to try out as many different hyperparameter configurations in deep learning as in other machine learning scenarios, because evaluating each single hyperparameter configuration in deep learning would mean training a deep neural network, which usually takes quite long time. Hyperband algorithm achieves state-of-the-art performance on various hyperparameter optimization problems in the field of deep learning. However, Hyperband algorithm does not utilize history information of previous explored hyperparameter configurations, thus the solution found is suboptimal. We propose to combine Hyperband algorithm with Bayesian optimization (which does not ignore history when sampling next trial configuration). Experimental results show that our combination approach is superior to other hyperparameter optimization approaches including Hyperband algorithm.



### Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies
- **Arxiv ID**: http://arxiv.org/abs/1801.01615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01615v1)
- **Published**: 2018-01-05 02:41:54+00:00
- **Updated**: 2018-01-05 02:41:54+00:00
- **Authors**: Hanbyul Joo, Tomas Simon, Yaser Sheikh
- **Comment**: None
- **Journal**: None
- **Summary**: We present a unified deformation model for the markerless capture of multiple scales of human movement, including facial expressions, body motion, and hand gestures. An initial model is generated by locally stitching together models of the individual parts of the human body, which we refer to as the "Frankenstein" model. This model enables the full expression of part movements, including face and hands by a single seamless model. Using a large-scale capture of people wearing everyday clothes, we optimize the Frankenstein model to create "Adam". Adam is a calibrated model that shares the same skeleton hierarchy as the initial model but can express hair and clothing geometry, making it directly usable for fitting people as they normally appear in everyday life. Finally, we demonstrate the use of these models for total motion tracking, simultaneously capturing the large-scale body movements and the subtle face and hand motion of a social group of people.



### Deep learning for word-level handwritten Indic script identification
- **Arxiv ID**: http://arxiv.org/abs/1801.01627v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01627v1)
- **Published**: 2018-01-05 04:52:55+00:00
- **Updated**: 2018-01-05 04:52:55+00:00
- **Authors**: Soumya Ukil, Swarnendu Ghosh, Sk Md Obaidullah, K. C. Santosh, Kaushik Roy, Nibaran Das
- **Comment**: 11 pages, 6 figures , 2 tables
- **Journal**: None
- **Summary**: We propose a novel method that uses convolutional neural networks (CNNs) for feature extraction. Not just limited to conventional spatial domain representation, we use multilevel 2D discrete Haar wavelet transform, where image representations are scaled to a variety of different sizes. These are then used to train different CNNs to select features. To be precise, we use 10 different CNNs that select a set of 10240 features, i.e. 1024/CNN. With this, 11 different handwritten scripts are identified, where 1K words per script are used. In our test, we have achieved the maximum script identification rate of 94.73% using multi-layer perceptron (MLP). Our results outperform the state-of-the-art techniques.



### VSE-ens: Visual-Semantic Embeddings with Efficient Negative Sampling
- **Arxiv ID**: http://arxiv.org/abs/1801.01632v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01632v2)
- **Published**: 2018-01-05 05:19:37+00:00
- **Updated**: 2018-08-11 08:58:05+00:00
- **Authors**: Guibing Guo, Songlin Zhai, Fajie Yuan, Yuan Liu, Xingwei Wang
- **Comment**: Published by The Thirty-Second AAAI Conference on Artificial
  Intelligence (AAAI-18)
- **Journal**: None
- **Summary**: Jointing visual-semantic embeddings (VSE) have become a research hotpot for the task of image annotation, which suffers from the issue of semantic gap, i.e., the gap between images' visual features (low-level) and labels' semantic features (high-level). This issue will be even more challenging if visual features cannot be retrieved from images, that is, when images are only denoted by numerical IDs as given in some real datasets. The typical way of existing VSE methods is to perform a uniform sampling method for negative examples that violate the ranking order against positive examples, which requires a time-consuming search in the whole label space. In this paper, we propose a fast adaptive negative sampler that can work well in the settings of no figure pixels available. Our sampling strategy is to choose the negative examples that are most likely to meet the requirements of violation according to the latent factors of images. In this way, our approach can linearly scale up to large datasets. The experiments demonstrate that our approach converges 5.02x faster than the state-of-the-art approaches on OpenImages, 2.5x on IAPR-TCI2 and 2.06x on NUS-WIDE datasets, as well as better ranking accuracy across datasets.



### FOTS: Fast Oriented Text Spotting with a Unified Network
- **Arxiv ID**: http://arxiv.org/abs/1801.01671v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01671v2)
- **Published**: 2018-01-05 08:41:57+00:00
- **Updated**: 2018-01-15 11:30:21+00:00
- **Authors**: Xuebo Liu, Ding Liang, Shi Yan, Dagui Chen, Yu Qiao, Junjie Yan
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Incidental scene text spotting is considered one of the most difficult and valuable challenges in the document analysis community. Most existing methods treat text detection and recognition as separate tasks. In this work, we propose a unified end-to-end trainable Fast Oriented Text Spotting (FOTS) network for simultaneous detection and recognition, sharing computation and visual information among the two complementary tasks. Specially, RoIRotate is introduced to share convolutional features between detection and recognition. Benefiting from convolution sharing strategy, our FOTS has little computation overhead compared to baseline text detection network, and the joint training method learns more generic features to make our method perform better than these two-stage methods. Experiments on ICDAR 2015, ICDAR 2017 MLT, and ICDAR 2013 datasets demonstrate that the proposed method outperforms state-of-the-art methods significantly, which further allows us to develop the first real-time oriented text spotting system which surpasses all previous state-of-the-art results by more than 5% on ICDAR 2015 text spotting task while keeping 22.6 fps.



### Multi-Scale Attention with Dense Encoder for Handwritten Mathematical Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.03530v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03530v2)
- **Published**: 2018-01-05 09:22:42+00:00
- **Updated**: 2018-01-31 01:52:21+00:00
- **Authors**: Jianshu Zhang, Jun Du, Lirong Dai
- **Comment**: None
- **Journal**: None
- **Summary**: Handwritten mathematical expression recognition is a challenging problem due to the complicated two-dimensional structures, ambiguous handwriting input and variant scales of handwritten math symbols. To settle this problem, we utilize the attention based encoder-decoder model that recognizes mathematical expression images from two-dimensional layouts to one-dimensional LaTeX strings. We improve the encoder by employing densely connected convolutional networks as they can strengthen feature extraction and facilitate gradient propagation especially on a small training set. We also present a novel multi-scale attention model which is employed to deal with the recognition of math symbols in different scales and save the fine-grained details that will be dropped by pooling operations. Validated on the CROHME competition task, the proposed method significantly outperforms the state-of-the-art methods with an expression recognition accuracy of 52.8% on CROHME 2014 and 50.1% on CROHME 2016, by only using the official training dataset.



### Accelerated Training for Massive Classification via Dynamic Class Selection
- **Arxiv ID**: http://arxiv.org/abs/1801.01687v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01687v1)
- **Published**: 2018-01-05 10:11:53+00:00
- **Updated**: 2018-01-05 10:11:53+00:00
- **Authors**: Xingcheng Zhang, Lei Yang, Junjie Yan, Dahua Lin
- **Comment**: 8 pages, 6 figures, AAAI 2018
- **Journal**: None
- **Summary**: Massive classification, a classification task defined over a vast number of classes (hundreds of thousands or even millions), has become an essential part of many real-world systems, such as face recognition. Existing methods, including the deep networks that achieved remarkable success in recent years, were mostly devised for problems with a moderate number of classes. They would meet with substantial difficulties, e.g. excessive memory demand and computational cost, when applied to massive problems. We present a new method to tackle this problem. This method can efficiently and accurately identify a small number of "active classes" for each mini-batch, based on a set of dynamic class hierarchies constructed on the fly. We also develop an adaptive allocation scheme thereon, which leads to a better tradeoff between performance and cost. On several large-scale benchmarks, our method significantly reduces the training cost and memory demand, while maintaining competitive performance.



### Efficient Image Evidence Analysis of CNN Classification Results
- **Arxiv ID**: http://arxiv.org/abs/1801.01693v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01693v1)
- **Published**: 2018-01-05 10:31:21+00:00
- **Updated**: 2018-01-05 10:31:21+00:00
- **Authors**: Keyang Zhou, Bernhard Kainz
- **Comment**: 14 pages, 19 figures
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) define the current state-of-the-art for image recognition. With their emerging popularity, especially for critical applications like medical image analysis or self-driving cars, confirmability is becoming an issue. The black-box nature of trained predictors make it difficult to trace failure cases or to understand the internal reasoning processes leading to results. In this paper we introduce a novel efficient method to visualise evidence that lead to decisions in CNNs. In contrast to network fixation or saliency map methods, our method is able to illustrate the evidence for or against a classifier's decision in input pixel space approximately 10 times faster than previous methods. We also show that our approach is less prone to noise and can focus on the most relevant input regions, thus making it more accurate and interpretable. Moreover, by making simplifications we link our method with other visualisation methods, providing a general explanation for gradient-based visualisation techniques. We believe that our work makes network introspection more feasible for debugging and understanding deep convolutional networks. This will increase trust between humans and deep learning models.



### Cross-Sensor Iris Recognition: LG4000-to-LG2200 Comparison
- **Arxiv ID**: http://arxiv.org/abs/1801.01695v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1801.01695v1)
- **Published**: 2018-01-05 10:33:08+00:00
- **Updated**: 2018-01-05 10:33:08+00:00
- **Authors**: Nicolaie Popescu-Bodorin, Lucian Stefanita Grigore, Valentina Emilia Balas, Cristina Madalina Noaica, Ionut Axenie, Justinian Popa, Cristian Munteanu, Victor Stroescu, Ionut Manu, Alexandru Herea, Kartal Horasanli, Iulia Maria Motoc
- **Comment**: Pages: 18; Figures: 21; Iris Codes Comparisons: O(1E9); Results
  obtained by `ACSTL Cross-Sensor Comparison Competition Team 2013` during the
  Cross-Sensor Comparison Competition 2013 organized within the IEEE-BTAS-2013
  Conference
- **Journal**: None
- **Summary**: Cross-sensor comparison experimental results reported here show that the procedure defined and simulated during the Cross-Sensor Comparison Competition 2013 by our team for migrating / upgrading LG2200 based to LG4000 based biometric systems leads to better LG4000-to-LG2200 cross-sensor iris recognition results than previously reported, both in terms of user comfort and in terms of system safety. On the other hand, LG2200-to-LG400 migration/upgrade procedure defined and implemented by us is applicable to solve interoperability issues between LG2200 based and LG4000 based systems, but also to other pairs of systems having the same shift in the quality of acquired images.



### Moving Vehicle Detection Using AdaBoost and Haar-Like Feature in Surveillance Videos
- **Arxiv ID**: http://arxiv.org/abs/1801.01698v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01698v1)
- **Published**: 2018-01-05 10:41:02+00:00
- **Updated**: 2018-01-05 10:41:02+00:00
- **Authors**: Mohammad Mahdi Moghimi, Maryam Nayeri, Majid Pourahmadi, Mohammad Kazem Moghimi
- **Comment**: 13 pages
- **Journal**: International Journal of Imaging and Robotics, vol. 18, no. 1, pp.
  94-106 (2018)
- **Summary**: Vehicle detection is a technology which its aim is to locate and show the vehicle size in digital images. In this technology, vehicles are detected in presence of other things like trees and buildings. It has an important role in many computer vision applications such as vehicle tracking, analyzing the traffic scene and efficient traffic management. In this paper, vehicles detected based on the boosting technique by Viola Jones. Our proposed system is tested in some real scenes of surveillance videos with different light conditions. The experimental results show that the accuracy,completeness, and quality of the proposed vehicle detection method are better than the previous techniques (about 94%, 92%, and 87%, respectively). Thus, our proposed approach is robust and efficient to detect vehicles in surveillance videos and their applications.



### Semantic-aware Grad-GAN for Virtual-to-Real Urban Scene Adaption
- **Arxiv ID**: http://arxiv.org/abs/1801.01726v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01726v2)
- **Published**: 2018-01-05 11:54:27+00:00
- **Updated**: 2018-07-14 06:03:56+00:00
- **Authors**: Peilun Li, Xiaodan Liang, Daoyuan Jia, Eric P. Xing
- **Comment**: In proceedings of BMVC 2018
- **Journal**: None
- **Summary**: Recent advances in vision tasks (e.g., segmentation) highly depend on the availability of large-scale real-world image annotations obtained by cumbersome human labors. Moreover, the perception performance often drops significantly for new scenarios, due to the poor generalization capability of models trained on limited and biased annotations. In this work, we resort to transfer knowledge from automatically rendered scene annotations in virtual-world to facilitate real-world visual tasks. Although virtual-world annotations can be ideally diverse and unlimited, the discrepant data distributions between virtual and real-world make it challenging for knowledge transferring. We thus propose a novel Semantic-aware Grad-GAN (SG-GAN) to perform virtual-to-real domain adaption with the ability of retaining vital semantic information. Beyond the simple holistic color/texture transformation achieved by prior works, SG-GAN successfully personalizes the appearance adaption for each semantic region in order to preserve their key characteristic for better recognition. It presents two main contributions to traditional GANs: 1) a soft gradient-sensitive objective for keeping semantic boundaries; 2) a semantic-aware discriminator for validating the fidelity of personalized adaptions with respect to each semantic region. Qualitative and quantitative experiments demonstrate the superiority of our SG-GAN in scene adaption over state-of-the-art GANs. Further evaluations on semantic segmentation on Cityscapes show using adapted virtual images by SG-GAN dramatically improves segmentation performance than original virtual data. We release our code at https://github.com/Peilun-Li/SG-GAN.



### 2D-Densely Connected Convolution Neural Networks for automatic Liver and Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1802.02182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02182v1)
- **Published**: 2018-01-05 12:30:53+00:00
- **Updated**: 2018-01-05 12:30:53+00:00
- **Authors**: Krishna Chaitanya Kaluva, Mahendra Khened, Avinash Kori, Ganapathy Krishnamurthi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a fully automatic 2-stage cascaded approach for segmentation of liver and its tumors in CT (Computed Tomography) images using densely connected fully convolutional neural network (DenseNet). We independently train liver and tumor segmentation models and cascade them for a combined segmentation of the liver and its tumor. The first stage involves segmentation of liver and the second stage uses the first stage's segmentation results for localization of liver and henceforth tumor segmentations inside liver region. The liver model was trained on the down-sampled axial slices $(256 \times 256)$, whereas for the tumor model no down-sampling of slices was done, but instead it was trained on the CT axial slices windowed at three different Hounsfield (HU) levels. On the test set our model achieved a global dice score of 0.923 and 0.625 on liver and tumor respectively. The computed tumor burden had an rmse of 0.044.



### Enhanced Image Classification With Data Augmentation Using Position Coordinates
- **Arxiv ID**: http://arxiv.org/abs/1802.02183v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02183v1)
- **Published**: 2018-01-05 12:32:43+00:00
- **Updated**: 2018-01-05 12:32:43+00:00
- **Authors**: Avinash Kori, Ganapathy Krishnamurthi, Balaji Srinivasan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose the use of image pixel position coordinate system to improve image classification accuracy in various applications. Specifically, we hypothesize that the use of pixel coordinates will lead to (a) Resolution invariant performance. Here, by resolution we mean the spacing between the pixels rather than the size of the image matrix. (b) Overall improvement in classification accuracy in comparison with network models trained without local pixel coordinates. This is due to position coordinates enabling the network to learn relationship between parts of objects, mimicking the human vision system. We demonstrate our hypothesis using empirical results and intuitive explanations of the feature maps learnt by deep neural networks. Specifically, our approach showed improvements in MNIST digit classification and beats state of the results on the SVHN database. We also show that the performance of our networks is unaffected despite training the same using blurred images of the MNIST database and predicting on the high resolution database.



### 3D-DETNet: a Single Stage Video-Based Vehicle Detector
- **Arxiv ID**: http://arxiv.org/abs/1801.01769v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01769v2)
- **Published**: 2018-01-05 14:38:14+00:00
- **Updated**: 2018-01-15 09:06:07+00:00
- **Authors**: Suichan Li
- **Comment**: None
- **Journal**: None
- **Summary**: Video-based vehicle detection has received considerable attention over the last ten years and there are many deep learning based detection methods which can be applied to it. However, these methods are devised for still images and applying them for video vehicle detection directly always obtains poor performance. In this work, we propose a new single-stage video-based vehicle detector integrated with 3DCovNet and focal loss, called 3D-DETNet. Draw support from 3D Convolution network and focal loss, our method has ability to capture motion information and is more suitable to detect vehicle in video than other single-stage methods devised for static images. The multiple video frames are initially fed to 3D-DETNet to generate multiple spatial feature maps, then sub-model 3DConvNet takes spatial feature maps as input to capture temporal information which is fed to final fully convolution model for predicting locations of vehicles in video frames. We evaluate our method on UA-DETAC vehicle detection dataset and our 3D-DETNet yields best performance and keeps a higher detection speed of 26 fps compared with other competing methods.



### Learning Implicit Brain MRI Manifolds with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1801.01847v1
- **DOI**: 10.1117/12.2293515
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01847v1)
- **Published**: 2018-01-05 17:24:37+00:00
- **Updated**: 2018-01-05 17:24:37+00:00
- **Authors**: Camilo Bermudez, Andrew J. Plassard, Larry T. Davis, Allen T. Newton, Susan M Resnick, Bennett A. Landman
- **Comment**: SPIE Medical Imaging 2018
- **Journal**: None
- **Summary**: An important task in image processing and neuroimaging is to extract quantitative information from the acquired images in order to make observations about the presence of disease or markers of development in populations. Having a lowdimensional manifold of an image allows for easier statistical comparisons between groups and the synthesis of group representatives. Previous studies have sought to identify the best mapping of brain MRI to a low-dimensional manifold, but have been limited by assumptions of explicit similarity measures. In this work, we use deep learning techniques to investigate implicit manifolds of normal brains and generate new, high-quality images. We explore implicit manifolds by addressing the problems of image synthesis and image denoising as important tools in manifold learning. First, we propose the unsupervised synthesis of T1-weighted brain MRI using a Generative Adversarial Network (GAN) by learning from 528 examples of 2D axial slices of brain MRI. Synthesized images were first shown to be unique by performing a crosscorrelation with the training set. Real and synthesized images were then assessed in a blinded manner by two imaging experts providing an image quality score of 1-5. The quality score of the synthetic image showed substantial overlap with that of the real images. Moreover, we use an autoencoder with skip connections for image denoising, showing that the proposed method results in higher PSNR than FSL SUSAN after denoising. This work shows the power of artificial networks to synthesize realistic imaging data, which can be used to improve image processing techniques and provide a quantitative framework to structural changes in the brain.



### Hi-Fi: Hierarchical Feature Integration for Skeleton Detection
- **Arxiv ID**: http://arxiv.org/abs/1801.01849v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01849v4)
- **Published**: 2018-01-05 17:34:18+00:00
- **Updated**: 2018-08-07 08:07:01+00:00
- **Authors**: Kai Zhao, Wei Shen, Shanghua Gao, Dandan Li, Ming-Ming Cheng
- **Comment**: IJCAI2018
- **Journal**: None
- **Summary**: In natural images, the scales (thickness) of object skeletons may dramatically vary among objects and object parts, making object skeleton detection a challenging problem. We present a new convolutional neural network (CNN) architecture by introducing a novel hierarchical feature integration mechanism, named Hi-Fi, to address the skeleton detection problem. The proposed CNN-based approach has a powerful multi-scale feature integration ability that intrinsically captures high-level semantics from deeper layers as well as low-level details from shallower layers. % By hierarchically integrating different CNN feature levels with bidirectional guidance, our approach (1) enables mutual refinement across features of different levels, and (2) possesses the strong ability to capture both rich object context and high-resolution details. Experimental results show that our method significantly outperforms the state-of-the-art methods in terms of effectively fusing features from very different scales, as evidenced by a considerable performance improvement on several benchmarks.



### Improved Style Transfer by Respecting Inter-layer Correlations
- **Arxiv ID**: http://arxiv.org/abs/1801.01933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01933v1)
- **Published**: 2018-01-05 22:41:11+00:00
- **Updated**: 2018-01-05 22:41:11+00:00
- **Authors**: Mao-Chuang Yeh, Shuai Tang
- **Comment**: None
- **Journal**: None
- **Summary**: A popular series of style transfer methods apply a style to a content image by controlling mean and covariance of values in early layers of a feature stack. This is insufficient for transferring styles that have strong structure across spatial scales like, e.g., textures where dots lie on long curves. This paper demonstrates that controlling inter-layer correlations yields visible improvements in style transfer methods. We achieve this control by computing cross-layer, rather than within-layer, gram matrices. We find that (a) cross-layer gram matrices are sufficient to control within-layer statistics. Inter-layer correlations improves style transfer and texture synthesis. The paper shows numerous examples on "hard" real style transfer problems (e.g. long scale and hierarchical patterns); (b) a fast approximate style transfer method can control cross-layer gram matrices; (c) we demonstrate that multiplicative, rather than additive style and content loss, results in very good style transfer. Multiplicative loss produces a visible emphasis on boundaries, and means that one hyper-parameter can be eliminated.



