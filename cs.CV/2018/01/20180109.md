# Arxiv Papers in cs.CV on 2018-01-09
### Data Augmentation for Brain-Computer Interfaces: Analysis on Event-Related Potentials Data
- **Arxiv ID**: http://arxiv.org/abs/1801.02730v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1801.02730v1)
- **Published**: 2018-01-09 00:34:34+00:00
- **Updated**: 2018-01-09 00:34:34+00:00
- **Authors**: Mario Michael Krell, Anett Seeland, Su Kyoung Kim
- **Comment**: None
- **Journal**: None
- **Summary**: On image data, data augmentation is becoming less relevant due to the large amount of available training data and regularization techniques. Common approaches are moving windows (cropping), scaling, affine distortions, random noise, and elastic deformations. For electroencephalographic data, the lack of sufficient training data is still a major issue. We suggest and evaluate different approaches to generate augmented data using temporal and spatial/rotational distortions. Our results on the perception of rare stimuli (P300 data) and movement prediction (MRCP data) show that these approaches are feasible and can significantly increase the performance of signal processing chains for brain-computer interfaces by 1% to 6%.



### A Systematic Analysis for State-of-the-Art 3D Lung Nodule Proposals Generation
- **Arxiv ID**: http://arxiv.org/abs/1802.02179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02179v1)
- **Published**: 2018-01-09 01:13:33+00:00
- **Updated**: 2018-01-09 01:13:33+00:00
- **Authors**: Hui Wu, Matrix Yao, Albert Hu, Gaofeng Sun, Xiaokun Yu, Jian Tang
- **Comment**: 7 pages, 9 figures, 3 tables
- **Journal**: None
- **Summary**: Lung nodule proposals generation is the primary step of lung nodule detection and has received much attention in recent years . In this paper, we first construct a model of 3-dimension Convolutional Neural Network (3D CNN) to generate lung nodule proposals, which can achieve the state-of-the-art performance. Then, we analyze a series of key problems concerning the training performance and efficiency. Firstly, we train the 3D CNN model with data in different resolutions and find out that models trained by high resolution input data achieve better lung nodule proposals generation performances especially for nodules in too small sizes, while consumes much more memory at the same time. Then, we analyze the memory consumptions on different platforms and the experimental results indicate that CPU architecture can provide us with larger memory and enables us to explore more possibilities of 3D applications. We implement the 3D CNN model on CPU platform and propose an Intel Extended-Caffe framework which supports many highly-efficient 3D computations, which is opened source at https://github.com/extendedcaffe/extended-caffe.



### SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1801.02753v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.02753v2)
- **Published**: 2018-01-09 02:07:26+00:00
- **Updated**: 2018-04-12 22:18:29+00:00
- **Authors**: Wengling Chen, James Hays
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: Synthesizing realistic images from human drawn sketches is a challenging problem in computer graphics and vision. Existing approaches either need exact edge maps, or rely on retrieval of existing photographs. In this work, we propose a novel Generative Adversarial Network (GAN) approach that synthesizes plausible images from 50 categories including motorcycles, horses and couches. We demonstrate a data augmentation technique for sketches which is fully automatic, and we show that the augmented data is helpful to our task. We introduce a new network building block suitable for both the generator and discriminator which improves the information flow by injecting the input image at multiple scales. Compared to state-of-the-art image translation methods, our approach generates more realistic images and achieves significantly higher Inception Scores.



### TextBoxes++: A Single-Shot Oriented Scene Text Detector
- **Arxiv ID**: http://arxiv.org/abs/1801.02765v3
- **DOI**: 10.1109/TIP.2018.2825107
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.02765v3)
- **Published**: 2018-01-09 02:40:49+00:00
- **Updated**: 2018-04-27 09:02:20+00:00
- **Authors**: Minghui Liao, Baoguang Shi, Xiang Bai
- **Comment**: 15 pages
- **Journal**: IEEE Transactions on Image Processing 27 (2018) 3676-3690
- **Summary**: Scene text detection is an important step of scene text recognition system and also a challenging problem. Different from general object detection, the main challenges of scene text detection lie on arbitrary orientations, small sizes, and significantly variant aspect ratios of text in natural images. In this paper, we present an end-to-end trainable fast scene text detector, named TextBoxes++, which detects arbitrary-oriented scene text with both high accuracy and efficiency in a single network forward pass. No post-processing other than an efficient non-maximum suppression is involved. We have evaluated the proposed TextBoxes++ on four public datasets. In all experiments, TextBoxes++ outperforms competing methods in terms of text localization accuracy and runtime. More specifically, TextBoxes++ achieves an f-measure of 0.817 at 11.6fps for 1024*1024 ICDAR 2015 Incidental text images, and an f-measure of 0.5591 at 19.8fps for 768*768 COCO-Text images. Furthermore, combined with a text recognizer, TextBoxes++ significantly outperforms the state-of-the-art approaches for word spotting and end-to-end text recognition tasks on popular benchmarks. Code is available at: https://github.com/MhLiao/TextBoxes_plusplus



### Adversarial Spheres
- **Arxiv ID**: http://arxiv.org/abs/1801.02774v3
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1801.02774v3)
- **Published**: 2018-01-09 03:24:53+00:00
- **Updated**: 2018-09-10 17:08:27+00:00
- **Authors**: Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S. Schoenholz, Maithra Raghu, Martin Wattenberg, Ian Goodfellow
- **Comment**: None
- **Journal**: None
- **Summary**: State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We hypothesize that this counter intuitive behavior is a naturally occurring result of the high dimensional geometry of the data manifold. As a first step towards exploring this hypothesis, we study a simple synthetic dataset of classifying between two concentric high dimensional spheres. For this dataset we show a fundamental tradeoff between the amount of test error and the average distance to nearest error. In particular, we prove that any model which misclassifies a small constant fraction of a sphere will be vulnerable to adversarial perturbations of size $O(1/\sqrt{d})$. Surprisingly, when we train several different architectures on this dataset, all of their error sets naturally approach this theoretical bound. As a result of the theory, the vulnerability of neural networks to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this very simple case will point the way forward to explore how the geometry of complex real-world data sets leads to adversarial examples.



### CANDY: Conditional Adversarial Networks based Fully End-to-End System for Single Image Haze Removal
- **Arxiv ID**: http://arxiv.org/abs/1801.02892v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.02892v2)
- **Published**: 2018-01-09 11:35:55+00:00
- **Updated**: 2018-05-02 11:36:49+00:00
- **Authors**: Kunal Swami, Saikat Kumar Das
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Single image haze removal is a very challenging and ill-posed problem. The existing haze removal methods in literature, including the recently introduced deep learning methods, model the problem of haze removal as that of estimating intermediate parameters, viz., scene transmission map and atmospheric light. These are used to compute the haze-free image from the hazy input image. Such an approach only focuses on accurate estimation of intermediate parameters, while the aesthetic quality of the haze-free image is unaccounted for in the optimization framework. Thus, errors in the estimation of intermediate parameters often lead to generation of inferior quality haze-free images. In this paper, we present CANDY (Conditional Adversarial Networks based Dehazing of hazY images), a fully end-to-end model which directly generates a clean haze-free image from a hazy input image. CANDY also incorporates the visual quality of haze-free image into the optimization function; thus, generating a superior quality haze-free image. To the best of our knowledge, this is the first work in literature to propose a fully end-to-end model for single image haze removal. Also, this is the first work to explore the newly introduced concept of generative adversarial networks for the problem of single image haze removal. The proposed model CANDY was trained on a synthetically created haze image dataset, while evaluation was performed on challenging synthetic as well as real haze image datasets. The extensive evaluation and comparison results of CANDY reveal that it significantly outperforms existing state-of-the-art haze removal methods in literature, both quantitatively as well as qualitatively.



### Data Augmentation by Pairing Samples for Images Classification
- **Arxiv ID**: http://arxiv.org/abs/1801.02929v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.02929v2)
- **Published**: 2018-01-09 13:37:11+00:00
- **Updated**: 2018-04-11 13:28:07+00:00
- **Authors**: Hiroshi Inoue
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation is a widely used technique in many machine learning tasks, such as image classification, to virtually enlarge the training dataset size and avoid overfitting. Traditional data augmentation techniques for image classification tasks create new samples from the original training data by, for example, flipping, distorting, adding a small amount of noise to, or cropping a patch from an original image. In this paper, we introduce a simple but surprisingly effective data augmentation technique for image classification tasks. With our technique, named SamplePairing, we synthesize a new sample from one image by overlaying another image randomly chosen from the training data (i.e., taking an average of two images for each pixel). By using two images randomly selected from the training set, we can generate $N^2$ new samples from $N$ training samples. This simple data augmentation technique significantly improved classification accuracy for all the tested datasets; for example, the top-1 error rate was reduced from 33.5% to 29.0% for the ILSVRC 2012 dataset with GoogLeNet and from 8.22% to 6.93% in the CIFAR-10 dataset. We also show that our SamplePairing technique largely improved accuracy when the number of samples in the training set was very small. Therefore, our technique is more valuable for tasks with a limited amount of training data, such as medical imaging tasks.



### EBIC: an evolutionary-based parallel biclustering algorithm for pattern discover
- **Arxiv ID**: http://arxiv.org/abs/1801.03039v2
- **DOI**: 10.1093/bioinformatics/bty401
- **Categories**: **cs.LG**, cs.CV, cs.IR, q-bio.GN, 68, 92, I.5.2; I.2.11; I.5.3; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1801.03039v2)
- **Published**: 2018-01-09 17:13:07+00:00
- **Updated**: 2018-07-26 14:08:11+00:00
- **Authors**: Patryk Orzechowski, Moshe Sipper, Xiuzhen Huang, Jason H. Moore
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: In this paper a novel biclustering algorithm based on artificial intelligence (AI) is introduced. The method called EBIC aims to detect biologically meaningful, order-preserving patterns in complex data. The proposed algorithm is probably the first one capable of discovering with accuracy exceeding 50% multiple complex patterns in real gene expression datasets. It is also one of the very few biclustering methods designed for parallel environments with multiple graphics processing units (GPUs). We demonstrate that EBIC outperforms state-of-the-art biclustering methods, in terms of recovery and relevance, on both synthetic and genetic datasets. EBIC also yields results over 12 times faster than the most accurate reference algorithms. The proposed algorithm is anticipated to be added to the repertoire of unsupervised machine learning algorithms for the analysis of datasets, including those from large-scale genomic studies.



### Meta-Tracker: Fast and Robust Online Adaptation for Visual Object Trackers
- **Arxiv ID**: http://arxiv.org/abs/1801.03049v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.03049v2)
- **Published**: 2018-01-09 17:38:10+00:00
- **Updated**: 2018-03-19 19:48:00+00:00
- **Authors**: Eunbyung Park, Alexander C. Berg
- **Comment**: Code: https://github.com/silverbottlep/meta_trackers
- **Journal**: None
- **Summary**: This paper improves state-of-the-art visual object trackers that use online adaptation. Our core contribution is an offline meta-learning-based method to adjust the initial deep networks used in online adaptation-based tracking. The meta learning is driven by the goal of deep networks that can quickly be adapted to robustly model a particular target in future frames. Ideally the resulting models focus on features that are useful for future frames, and avoid overfitting to background clutter, small parts of the target, or noise. By enforcing a small number of update iterations during meta-learning, the resulting networks train significantly faster. We demonstrate this approach on top of the high performance tracking approaches: tracking-by-detection based MDNet and the correlation based CREST. Experimental results on standard benchmarks, OTB2015 and VOT2016, show that our meta-learned versions of both trackers improve speed, accuracy, and robustness.



### Recognizing Material Properties from Images
- **Arxiv ID**: http://arxiv.org/abs/1801.03127v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03127v1)
- **Published**: 2018-01-09 20:22:41+00:00
- **Updated**: 2018-01-09 20:22:41+00:00
- **Authors**: Gabriel Schwartz, Ko Nishino
- **Comment**: None
- **Journal**: None
- **Summary**: Humans rely on properties of the materials that make up objects to guide our interactions with them. Grasping smooth materials, for example, requires care, and softness is an ideal property for fabric used in bedding. Even when these properties are not visual (e.g. softness is a physical property), we may still infer their presence visually. We refer to such material properties as visual material attributes. Recognizing these attributes in images can contribute valuable information for general scene understanding and material recognition. Unlike well-known object and scene attributes, visual material attributes are local properties with no fixed shape or spatial extent. We show that given a set of images annotated with known material attributes, we may accurately recognize the attributes from small local image patches. Obtaining such annotations in a consistent fashion at scale, however, is challenging. To address this, we introduce a method that allows us to probe the human visual perception of materials by asking simple yes/no questions comparing pairs of image patches. This provides sufficient weak supervision to build a set of attributes and associated classifiers that, while unnamed, serve the same function as the named attributes we use to describe materials. Doing so allows us to recognize visual material attributes without resorting to exhaustive manual annotation of a fixed set of named attributes. Furthermore, we show that this method may be integrated in the end-to-end learning of a material classification CNN to simultaneously recognize materials and discover their visual attributes. Our experimental results show that visual material attributes, whether named or automatically discovered, provide a useful intermediate representation for known material categories themselves as well as a basis for transfer learning when recognizing previously-unseen categories.



### Visual and Semantic Knowledge Transfer for Large Scale Semi-supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1801.03145v2
- **DOI**: 10.1109/TPAMI.2017.2771779
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03145v2)
- **Published**: 2018-01-09 21:29:12+00:00
- **Updated**: 2018-03-13 16:09:50+00:00
- **Authors**: Yuxing Tang, Josiah Wang, Xiaofang Wang, Boyang Gao, Emmanuel Dellandrea, Robert Gaizauskas, Liming Chen
- **Comment**: TPAMI. correct some typos
- **Journal**: Published in IEEE Transactions on Pattern Analysis and Machine
  Intelligence, November 2017
- **Summary**: Deep CNN-based object detection systems have achieved remarkable success on several large-scale object detection benchmarks. However, training such detectors requires a large number of labeled bounding boxes, which are more difficult to obtain than image-level annotations. Previous work addresses this issue by transforming image-level classifiers into object detectors. This is done by modeling the differences between the two on categories with both image-level and bounding box annotations, and transferring this information to convert classifiers to detectors for categories without bounding box annotations. We improve this previous work by incorporating knowledge about object similarities from visual and semantic domains during the transfer process. The intuition behind our proposed method is that visually and semantically similar categories should exhibit more common transferable properties than dissimilar categories, e.g. a better detector would result by transforming the differences between a dog classifier and a dog detector onto the cat class, than would by transforming from the violin class. Experimental results on the challenging ILSVRC2013 detection dataset demonstrate that each of our proposed object similarity based knowledge transfer methods outperforms the baseline methods. We found strong evidence that visual similarity and semantic relatedness are complementary for the task, and when combined notably improve detection, achieving state-of-the-art detection performance in a semi-supervised setting.



### An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos
- **Arxiv ID**: http://arxiv.org/abs/1801.03149v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03149v2)
- **Published**: 2018-01-09 21:44:26+00:00
- **Updated**: 2018-01-30 10:50:11+00:00
- **Authors**: B Ravi Kiran, Dilip Mathew Thomas, Ranjith Parakkal
- **Comment**: 15 pages, double column
- **Journal**: None
- **Summary**: Videos represent the primary source of information for surveillance applications and are available in large amounts but in most cases contain little or no annotation for supervised learning. This article reviews the state-of-the-art deep learning based methods for video anomaly detection and categorizes them based on the type of model and criteria of detection. We also perform simple studies to understand the different approaches and provide the criteria of evaluation for spatio-temporal anomaly detection.



### Moments in Time Dataset: one million videos for event understanding
- **Arxiv ID**: http://arxiv.org/abs/1801.03150v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1801.03150v3)
- **Published**: 2018-01-09 21:46:38+00:00
- **Updated**: 2019-02-16 13:20:03+00:00
- **Authors**: Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfruend, Carl Vondrick, Aude Oliva
- **Comment**: None
- **Journal**: None
- **Summary**: We present the Moments in Time Dataset, a large-scale human-annotated collection of one million short videos corresponding to dynamic events unfolding within three seconds. Modeling the spatial-audio-temporal dynamics even for actions occurring in 3 second videos poses many challenges: meaningful events do not include only people, but also objects, animals, and natural phenomena; visual and auditory events can be symmetrical in time ("opening" is "closing" in reverse), and either transient or sustained. We describe the annotation process of our dataset (each video is tagged with one action or activity label among 339 different classes), analyze its scale and diversity in comparison to other large-scale video datasets for action recognition, and report results of several baseline models addressing separately, and jointly, three modalities: spatial, temporal and auditory. The Moments in Time dataset, designed to have a large coverage and diversity of events in both visual and auditory modalities, can serve as a new challenge to develop models that scale to the level of complexity and abstract reasoning that a human processes on a daily basis.



### RGB image-based data analysis via discrete Morse theory and persistent homology
- **Arxiv ID**: http://arxiv.org/abs/1801.09530v1
- **DOI**: None
- **Categories**: **cs.CV**, math.AT, math.CO, 05E45, 57T99, 91C99, 52-04, 55U99
- **Links**: [PDF](http://arxiv.org/pdf/1801.09530v1)
- **Published**: 2018-01-09 21:57:04+00:00
- **Updated**: 2018-01-09 21:57:04+00:00
- **Authors**: Chuan Du, Christopher Szul, Adarsh Manawa, Nima Rasekh, Rosemary K. Guzman, Ruth Davidson
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding and comparing images for the purposes of data analysis is currently a very computationally demanding task. A group at Australian National University (ANU) recently developed open-source code that can detect fundamental topological features of a grayscale image in a computationally feasible manner. This is made possible by the fact that computers store grayscale images as cubical cellular complexes. These complexes can be studied using the techniques of discrete Morse theory. We expand the functionality of the ANU code by introducing methods and software for analyzing images encoded in red, green, and blue (RGB), because this image encoding is very popular for publicly available data. Our methods allow the extraction of key topological information from RGB images via informative persistence diagrams by introducing novel methods for transforming RGB-to-grayscale. This paradigm allows us to perform data analysis directly on RGB images representing water scarcity variability as well as crime variability. We introduce software enabling a a user to predict future image properties, towards the eventual aim of more rapid image-based data behavior prediction.



### BUSIS: A Benchmark for Breast Ultrasound Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1801.03182v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03182v2)
- **Published**: 2018-01-09 23:19:00+00:00
- **Updated**: 2021-10-19 06:52:11+00:00
- **Authors**: Min Xian, Yingtao Zhang, H. D. Cheng, Fei Xu, Kuan Huang, Boyu Zhang, Jianrui Ding, Chunping Ning, Ying Wang
- **Comment**: 27 pages, 4 figures, 3 tables
- **Journal**: None
- **Summary**: Breast ultrasound (BUS) image segmentation is challenging and critical for BUS Comput-er-Aided Diagnosis (CAD) systems. Many BUS segmentation approaches have been studied in the last two decades, but the performances of most approaches have been assessed using relatively small private datasets with different quantitative metrics, which results in a discrepancy in performance comparison. Therefore, there is a pressing need for building a benchmark to compare existing methods using a public dataset objectively, to determine the performance of the best breast tumor segmentation algorithm available today, and to investigate what segmentation strategies are valuable in clinical practice and theoretical study. In this work, a benchmark for B-mode breast ultrasound image segmentation is presented. In the benchmark, 1) we collected 562 breast ultrasound images, prepared a software tool, and involved four radiologists in obtaining accurate annotations through standardized procedures; 2) we extensively compared the performance of sixteen state-of-the-art segmentation methods and discussed their advantages and disadvantages; 3) we proposed a set of valuable quantitative metrics to evaluate both semi-automatic and fully automatic segmentation approaches; and 4) the successful segmentation strategies and possible future improvements are discussed in details.



