# Arxiv Papers in cs.CV on 2018-01-11
### Soft Locality Preserving Map (SLPM) for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.03754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03754v1)
- **Published**: 2018-01-11 13:39:24+00:00
- **Updated**: 2018-01-11 13:39:24+00:00
- **Authors**: Cigdem Turan, Kin-Man Lam, Xiangjian He
- **Comment**: None
- **Journal**: None
- **Summary**: For image recognition, an extensive number of methods have been proposed to overcome the high-dimensionality problem of feature vectors being used. These methods vary from unsupervised to supervised, and from statistics to graph-theory based. In this paper, the most popular and the state-of-the-art methods for dimensionality reduction are firstly reviewed, and then a new and more efficient manifold-learning method, named Soft Locality Preserving Map (SLPM), is presented. Furthermore, feature generation and sample selection are proposed to achieve better manifold learning. SLPM is a graph-based subspace-learning method, with the use of k-neighbourhood information and the class information. The key feature of SLPM is that it aims to control the level of spread of the different classes, because the spread of the classes in the underlying manifold is closely connected to the generalizability of the learned subspace. Our proposed manifold-learning method can be applied to various pattern recognition applications, and we evaluate its performances on facial expression recognition. Experiments on databases, such as the Bahcesehir University Multilingual Affective Face Database (BAUM-2), the Extended Cohn-Kanade (CK+) Database, the Japanese Female Facial Expression (JAFFE) Database, and the Taiwanese Facial Expression Image Database (TFEID), show that SLPM can effectively reduce the dimensionality of the feature vectors and enhance the discriminative power of the extracted features for expression recognition. Furthermore, the proposed feature-generation method can improve the generalizability of the underlying manifolds for facial expression recognition.



### Cortical-inspired image reconstruction via sub-Riemannian geometry and hypoelliptic diffusion
- **Arxiv ID**: http://arxiv.org/abs/1801.03800v1
- **DOI**: 10.1051/proc/201864037
- **Categories**: **cs.CV**, math.NA, Primary: 94A08. Secondary: 35H10, 53C17
- **Links**: [PDF](http://arxiv.org/pdf/1801.03800v1)
- **Published**: 2018-01-11 14:59:54+00:00
- **Updated**: 2018-01-11 14:59:54+00:00
- **Authors**: Ugo Boscain, Roman Chertovskih, Jean-Paul Gauthier, Dario Prandi, Alexey Remizov
- **Comment**: None
- **Journal**: ESAIM: ProcS. 64 (2018), pp. 37-53
- **Summary**: In this paper we review several algorithms for image inpainting based on the hypoelliptic diffusion naturally associated with a mathematical model of the primary visual cortex. In particular, we present one algorithm that does not exploit the information of where the image is corrupted, and others that do it. While the first algorithm is able to reconstruct only images that our visual system is still capable of recognize, we show that those of the second type completely transcend such limitation providing reconstructions at the state-of-the-art in image inpainting. This can be interpreted as a validation of the fact that our visual cortex actually encodes the first type of algorithm.



### Multi-view Consistency as Supervisory Signal for Learning Shape and Pose Prediction
- **Arxiv ID**: http://arxiv.org/abs/1801.03910v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03910v2)
- **Published**: 2018-01-11 18:15:47+00:00
- **Updated**: 2018-04-24 17:27:36+00:00
- **Authors**: Shubham Tulsiani, Alexei A. Efros, Jitendra Malik
- **Comment**: Project url with code: https://shubhtuls.github.io/mvcSnP/
- **Journal**: None
- **Summary**: We present a framework for learning single-view shape and pose prediction without using direct supervision for either. Our approach allows leveraging multi-view observations from unknown poses as supervisory signal during training. Our proposed training setup enforces geometric consistency between the independently predicted shape and pose from two views of the same instance. We consequently learn to predict shape in an emergent canonical (view-agnostic) frame along with a corresponding pose predictor. We show empirical and qualitative results using the ShapeNet dataset and observe encouragingly competitive performance to previous techniques which rely on stronger forms of supervision. We also demonstrate the applicability of our framework in a realistic setting which is beyond the scope of existing techniques: using a training dataset comprised of online product images where the underlying shape and pose are unknown.



### The Unreasonable Effectiveness of Deep Features as a Perceptual Metric
- **Arxiv ID**: http://arxiv.org/abs/1801.03924v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1801.03924v2)
- **Published**: 2018-01-11 18:54:17+00:00
- **Updated**: 2018-04-10 19:25:07+00:00
- **Authors**: Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang
- **Comment**: Accepted to CVPR 2018; Code and data available at
  https://www.github.com/richzhang/PerceptualSimilarity
- **Journal**: None
- **Summary**: While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.



### Fully-Coupled Two-Stream Spatiotemporal Networks for Extremely Low Resolution Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.03983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03983v1)
- **Published**: 2018-01-11 20:39:30+00:00
- **Updated**: 2018-01-11 20:39:30+00:00
- **Authors**: Mingze Xu, Aidean Sharghi, Xin Chen, David J Crandall
- **Comment**: 9 pagers, 5 figures, published in WACV 2018
- **Journal**: None
- **Summary**: A major emerging challenge is how to protect people's privacy as cameras and computer vision are increasingly integrated into our daily lives, including in smart devices inside homes. A potential solution is to capture and record just the minimum amount of information needed to perform a task of interest. In this paper, we propose a fully-coupled two-stream spatiotemporal architecture for reliable human action recognition on extremely low resolution (e.g., 12x16 pixel) videos. We provide an efficient method to extract spatial and temporal features and to aggregate them into a robust feature representation for an entire action video sequence. We also consider how to incorporate high resolution videos during training in order to build better low resolution action recognition models. We evaluate on two publicly-available datasets, showing significant improvements over the state-of-the-art.



### Multi-Task Spatiotemporal Neural Networks for Structured Surface Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1801.03986v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03986v2)
- **Published**: 2018-01-11 20:47:33+00:00
- **Updated**: 2018-07-20 21:30:25+00:00
- **Authors**: Mingze Xu, Chenyou Fan, John D Paden, Geoffrey C Fox, David J Crandall
- **Comment**: 10 pages, 7 figures, published in WACV 2018
- **Journal**: None
- **Summary**: Deep learning methods have surpassed the performance of traditional techniques on a wide range of problems in computer vision, but nearly all of this work has studied consumer photos, where precisely correct output is often not critical. It is less clear how well these techniques may apply on structured prediction problems where fine-grained output with high precision is required, such as in scientific imaging domains. Here we consider the problem of segmenting echogram radar data collected from the polar ice sheets, which is challenging because segmentation boundaries are often very weak and there is a high degree of noise. We propose a multi-task spatiotemporal neural network that combines 3D ConvNets and Recurrent Neural Networks (RNNs) to estimate ice surface boundaries from sequences of tomographic radar images. We show that our model outperforms the state-of-the-art on this problem by (1) avoiding the need for hand-tuned parameters, (2) extracting multiple surfaces (ice-air and ice-bed) simultaneously, (3) requiring less non-visual metadata, and (4) being about 6 times faster.



### Enhancing Underwater Imagery using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.04011v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1801.04011v1)
- **Published**: 2018-01-11 22:58:42+00:00
- **Updated**: 2018-01-11 22:58:42+00:00
- **Authors**: Cameron Fabbri, Md Jahidul Islam, Junaed Sattar
- **Comment**: Submitted to ICRA 2018
- **Journal**: None
- **Summary**: Autonomous underwater vehicles (AUVs) rely on a variety of sensors - acoustic, inertial and visual - for intelligent decision making. Due to its non-intrusive, passive nature, and high information content, vision is an attractive sensing modality, particularly at shallower depths. However, factors such as light refraction and absorption, suspended particles in the water, and color distortion affect the quality of visual data, resulting in noisy and distorted images. AUVs that rely on visual sensing thus face difficult challenges, and consequently exhibit poor performance on vision-driven tasks. This paper proposes a method to improve the quality of visual underwater scenes using Generative Adversarial Networks (GANs), with the goal of improving input to vision-driven behaviors further down the autonomy pipeline. Furthermore, we show how recently proposed methods are able to generate a dataset for the purpose of such underwater image restoration. For any visually-guided underwater robots, this improvement can result in increased safety and reliability through robust visual perception. To that effect, we present quantitative and qualitative data which demonstrates that images corrected through the proposed approach generate more visually appealing images, and also provide increased accuracy for a diver tracking algorithm.



### Non-Rigid Image Registration Using Self-Supervised Fully Convolutional Networks without Training Data
- **Arxiv ID**: http://arxiv.org/abs/1801.04012v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04012v1)
- **Published**: 2018-01-11 22:58:48+00:00
- **Updated**: 2018-01-11 22:58:48+00:00
- **Authors**: Hongming Li, Yong Fan
- **Comment**: Accepted by IEEE International Symposium on Biomedical Imaging 2018
  (ISBI'18)
- **Journal**: None
- **Summary**: A novel non-rigid image registration algorithm is built upon fully convolutional networks (FCNs) to optimize and learn spatial transformations between pairs of images to be registered in a self-supervised learning framework. Different from most existing deep learning based image registration methods that learn spatial transformations from training data with known corresponding spatial transformations, our method directly estimates spatial transformations between pairs of images by maximizing an image-wise similarity metric between fixed and deformed moving images, similar to conventional image registration algorithms. The image registration is implemented in a multi-resolution image registration framework to jointly optimize and learn spatial transformations and FCNs at different spatial resolutions with deep self-supervision through typical feedforward and backpropagation computation. The proposed method has been evaluated for registering 3D structural brain magnetic resonance (MR) images and obtained better performance than state-of-the-art image registration algorithms.



### Brain Age Prediction Based on Resting-State Functional Connectivity Patterns Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.04013v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04013v1)
- **Published**: 2018-01-11 23:09:38+00:00
- **Updated**: 2018-01-11 23:09:38+00:00
- **Authors**: Hongming Li, Theodore D. Satterthwaite, Yong Fan
- **Comment**: Accepted by IEEE International Symposium on Biomedical Imaging 2018
  (ISBI'18)
- **Journal**: None
- **Summary**: Brain age prediction based on neuroimaging data could help characterize both the typical brain development and neuropsychiatric disorders. Pattern recognition models built upon functional connectivity (FC) measures derived from resting state fMRI (rsfMRI) data have been successfully used to predict the brain age. However, most existing studies focus on coarse-grained FC measures between brain regions or intrinsic connectivity networks (ICNs), which may sacrifice fine-grained FC information of the rsfMRI data. Whole brain voxel-wise FC measures could provide fine-grained FC information of the brain and may improve the prediction performance. In this study, we develop a deep learning method to use convolutional neural networks (CNNs) to learn informative features from the fine-grained whole brain FC measures for the brain age prediction. Experimental results on a large dataset of resting-state fMRI demonstrate that the deep learning model with fine-grained FC measures could better predict the brain age.



### Application of a semantic segmentation convolutional neural network for accurate automatic detection and mapping of solar photovoltaic arrays in aerial imagery
- **Arxiv ID**: http://arxiv.org/abs/1801.04018v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04018v1)
- **Published**: 2018-01-11 23:40:41+00:00
- **Updated**: 2018-01-11 23:40:41+00:00
- **Authors**: Joseph Camilo, Rui Wang, Leslie M. Collins, Kyle Bradbury, Jordan M. Malof
- **Comment**: Accepted for publication at the 2017 IEEE Applied Imagery Pattern
  Recognition (AIPR) Workshop. Presented at the conference in Washington D.C.,
  October 10-12
- **Journal**: None
- **Summary**: We consider the problem of automatically detecting small-scale solar photovoltaic arrays for behind-the-meter energy resource assessment in high resolution aerial imagery. Such algorithms offer a faster and more cost-effective solution to collecting information on distributed solar photovoltaic (PV) arrays, such as their location, capacity, and generated energy. The surface area of PV arrays, a characteristic which can be estimated from aerial imagery, provides an important proxy for array capacity and energy generation. In this work, we employ a state-of-the-art convolutional neural network architecture, called SegNet (Badrinarayanan et. al., 2015), to semantically segment (or map) PV arrays in aerial imagery. This builds on previous work focused on identifying the locations of PV arrays, as opposed to their specific shapes and sizes. We measure the ability of our SegNet implementation to estimate the surface area of PV arrays on a large, publicly available, dataset that has been employed in several previous studies. The results indicate that the SegNet model yields substantial performance improvements with respect to estimating shape and size as compared to a recently proposed convolutional neural network PV detection algorithm.



