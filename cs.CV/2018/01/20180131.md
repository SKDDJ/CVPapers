# Arxiv Papers in cs.CV on 2018-01-31
### Learning Video-Story Composition via Recurrent Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1801.10281v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10281v1)
- **Published**: 2018-01-31 02:35:30+00:00
- **Updated**: 2018-01-31 02:35:30+00:00
- **Authors**: Guangyu Zhong, Yi-Hsuan Tsai, Sifei Liu, Zhixun Su, Ming-Hsuan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a learning-based method to compose a video-story from a group of video clips that describe an activity or experience. We learn the coherence between video clips from real videos via the Recurrent Neural Network (RNN) that jointly incorporates the spatial-temporal semantics and motion dynamics to generate smooth and relevant compositions. We further rearrange the results generated by the RNN to make the overall video-story compatible with the storyline structure via a submodular ranking optimization process. Experimental results on the video-story dataset show that the proposed algorithm outperforms the state-of-the-art approach.



### Netizen-Style Commenting on Fashion Photos: Dataset and Diversity Measures
- **Arxiv ID**: http://arxiv.org/abs/1801.10300v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10300v1)
- **Published**: 2018-01-31 05:08:58+00:00
- **Updated**: 2018-01-31 05:08:58+00:00
- **Authors**: Wen Hua Lin, Kuan-Ting Chen, Hung Yueh Chiang, Winston Hsu
- **Comment**: The Web Conference (WWW) 2018
- **Journal**: None
- **Summary**: Recently, deep neural network models have achieved promising results in image captioning task. Yet, "vanilla" sentences, only describing shallow appearances (e.g., types, colors), generated by current works are not satisfied netizen style resulting in lacking engagements, contexts, and user intentions. To tackle this problem, we propose Netizen Style Commenting (NSC), to automatically generate characteristic comments to a user-contributed fashion photo. We are devoted to modulating the comments in a vivid "netizen" style which reflects the culture in a designated social community and hopes to facilitate more engagement with users. In this work, we design a novel framework that consists of three major components: (1) We construct a large-scale clothing dataset named NetiLook, which contains 300K posts (photos) with 5M comments to discover netizen-style comments. (2) We propose three unique measures to estimate the diversity of comments. (3) We bring diversity by marrying topic models with neural networks to make up the insufficiency of conventional image captioning works. Experimenting over Flickr30k and our NetiLook datasets, we demonstrate our proposed approaches benefit fashion photo commenting and improve image captioning tasks both in accuracy and diversity.



### Action Recognition with Spatio-Temporal Visual Attention on Skeleton Image Sequences
- **Arxiv ID**: http://arxiv.org/abs/1801.10304v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10304v2)
- **Published**: 2018-01-31 05:23:05+00:00
- **Updated**: 2018-04-11 15:09:25+00:00
- **Authors**: Zhengyuan Yang, Yuncheng Li, Jianchao Yang, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Action recognition with 3D skeleton sequences is becoming popular due to its speed and robustness. The recently proposed Convolutional Neural Networks (CNN) based methods have shown good performance in learning spatio-temporal representations for skeleton sequences. Despite the good recognition accuracy achieved by previous CNN based methods, there exist two problems that potentially limit the performance. First, previous skeleton representations are generated by chaining joints with a fixed order. The corresponding semantic meaning is unclear and the structural information among the joints is lost. Second, previous models do not have an ability to focus on informative joints. The attention mechanism is important for skeleton based action recognition because there exist spatio-temporal key stages while the joint predictions can be inaccurate. To solve these two problems, we propose a novel CNN based method for skeleton based action recognition. We first redesign the skeleton representations with a depth-first tree traversal order, which enhances the semantic meaning of skeleton images and better preserves the associated structural information. We then propose the idea of a two-branch attention architecture that focuses on spatio-temporal key stages and filters out unreliable joint predictions. A base attention model with the simplest structure is first introduced. By improving the structures in both branches, we further propose a Global Long-sequence Attention Network (GLAN). Furthermore, in order to adjust the kernel's spatio-temporal aspect ratios and better capture long term dependencies, we propose a Sub-Sequence Attention Network (SSAN) that takes sub-image sequences as inputs. Our experiment results on NTU RGB+D and SBU Kinetic Interaction outperforms the state-of-the-art. The model is further validated on noisy estimated poses from UCF101 and Kinetics.



### A Deep Ranking Model for Spatio-Temporal Highlight Detection from a 360 Video
- **Arxiv ID**: http://arxiv.org/abs/1801.10312v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1801.10312v1)
- **Published**: 2018-01-31 06:29:11+00:00
- **Updated**: 2018-01-31 06:29:11+00:00
- **Authors**: Youngjae Yu, Sangho Lee, Joonil Na, Jaeyun Kang, Gunhee Kim
- **Comment**: In AAAI 2018, 9 pages
- **Journal**: None
- **Summary**: We address the problem of highlight detection from a 360 degree video by summarizing it both spatially and temporally. Given a long 360 degree video, we spatially select pleasantly-looking normal field-of-view (NFOV) segments from unlimited field of views (FOV) of the 360 degree video, and temporally summarize it into a concise and informative highlight as a selected subset of subshots. We propose a novel deep ranking model named as Composition View Score (CVS) model, which produces a spherical score map of composition per video segment, and determines which view is suitable for highlight via a sliding window kernel at inference. To evaluate the proposed framework, we perform experiments on the Pano2Vid benchmark dataset and our newly collected 360 degree video highlight dataset from YouTube and Vimeo. Through evaluation using both quantitative summarization metrics and user studies via Amazon Mechanical Turk, we demonstrate that our approach outperforms several state-of-the-art highlight detection methods. We also show that our model is 16 times faster at inference than AutoCam, which is one of the first summarization algorithms of 360 degree videos



### SESR: Single Image Super Resolution with Recursive Squeeze and Excitation Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.10319v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10319v1)
- **Published**: 2018-01-31 06:50:49+00:00
- **Updated**: 2018-01-31 06:50:49+00:00
- **Authors**: Xi Cheng, Xiang Li, Ying Tai, Jian Yang
- **Comment**: Preprint version with 6 pages for ICPR18
- **Journal**: None
- **Summary**: Single image super resolution is a very important computer vision task, with a wide range of applications. In recent years, the depth of the super-resolution model has been constantly increasing, but with a small increase in performance, it has brought a huge amount of computation and memory consumption. In this work, in order to make the super resolution models more effective, we proposed a novel single image super resolution method via recursive squeeze and excitation networks (SESR). By introducing the squeeze and excitation module, our SESR can model the interdependencies and relationships between channels and that makes our model more efficiency. In addition, the recursive structure and progressive reconstruction method in our model minimized the layers and parameters and enabled SESR to simultaneously train multi-scale super resolution in a single model. After evaluating on four benchmark test sets, our model is proved to be above the state-of-the-art methods in terms of speed and accuracy.



### From BoW to CNN: Two Decades of Texture Representation for Texture Classification
- **Arxiv ID**: http://arxiv.org/abs/1801.10324v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1801.10324v2)
- **Published**: 2018-01-31 07:06:12+00:00
- **Updated**: 2018-10-03 20:55:57+00:00
- **Authors**: Li Liu, Jie Chen, Paul Fieguth, Guoying Zhao, Rama Chellappa, Matti Pietikainen
- **Comment**: Accepted by IJCV
- **Journal**: None
- **Summary**: Texture is a fundamental characteristic of many types of images, and texture representation is one of the essential and challenging problems in computer vision and pattern recognition which has attracted extensive research attention. Since 2000, texture representations based on Bag of Words (BoW) and on Convolutional Neural Networks (CNNs) have been extensively studied with impressive performance. Given this period of remarkable evolution, this paper aims to present a comprehensive survey of advances in texture representation over the last two decades. More than 200 major publications are cited in this survey covering different aspects of the research, which includes (i) problem description; (ii) recent advances in the broad categories of BoW-based, CNN-based and attribute-based methods; and (iii) evaluation issues, specifically benchmark datasets and state of the art results. In retrospect of what has been achieved so far, the survey discusses open challenges and directions for future research.



### An Infinitesimal Probabilistic Model for Principal Component Analysis of Manifold Valued Data
- **Arxiv ID**: http://arxiv.org/abs/1801.10341v2
- **DOI**: None
- **Categories**: **math.ST**, cs.CV, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1801.10341v2)
- **Published**: 2018-01-31 08:16:16+00:00
- **Updated**: 2018-06-24 19:21:08+00:00
- **Authors**: Stefan Sommer
- **Comment**: None
- **Journal**: None
- **Summary**: We provide a probabilistic and infinitesimal view of how the principal component analysis procedure (PCA) can be generalized to analysis of nonlinear manifold valued data. Starting with the probabilistic PCA interpretation of the Euclidean PCA procedure, we show how PCA can be generalized to manifolds in an intrinsic way that does not resort to linearization of the data space. The underlying probability model is constructed by mapping a Euclidean stochastic process to the manifold using stochastic development of Euclidean semimartingales. The construction uses a connection and bundles of covariant tensors to allow global transport of principal eigenvectors, and the model is thereby an example of how principal fiber bundles can be used to handle the lack of global coordinate system and orientations that characterizes manifold valued statistics. We show how curvature implies non-integrability of the equivalent of Euclidean principal subspaces, and how the stochastic flows provide an alternative to explicit construction of such subspaces. We describe estimation procedures for inference of parameters and prediction of principal components, and we give examples of properties of the model on embedded surfaces.



### ConvCSNet: A Convolutional Compressive Sensing Framework Based on Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1801.10342v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10342v1)
- **Published**: 2018-01-31 08:22:53+00:00
- **Updated**: 2018-01-31 08:22:53+00:00
- **Authors**: Xiaotong Lu, Weisheng Dong, Peiyao Wang, Guangming Shi, Xuemei Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Compressive sensing (CS), aiming to reconstruct an image/signal from a small set of random measurements has attracted considerable attentions in recent years. Due to the high dimensionality of images, previous CS methods mainly work on image blocks to avoid the huge requirements of memory and computation, i.e., image blocks are measured with Gaussian random matrices, and the whole images are recovered from the reconstructed image blocks. Though efficient, such methods suffer from serious blocking artifacts. In this paper, we propose a convolutional CS framework that senses the whole image using a set of convolutional filters. Instead of reconstructing individual blocks, the whole image is reconstructed from the linear convolutional measurements. Specifically, the convolutional CS is implemented based on a convolutional neural network (CNN), which performs both the convolutional CS and nonlinear reconstruction. Through end-to-end training, the sensing filters and the reconstruction network can be jointly optimized. To facilitate the design of the CS reconstruction network, a novel two-branch CNN inspired from a sparsity-based CS reconstruction model is developed. Experimental results show that the proposed method substantially outperforms previous state-of-the-art CS methods in term of both PSNR and visual quality.



### Fast and Accurate Reconstruction of Compressed Color Light Field
- **Arxiv ID**: http://arxiv.org/abs/1801.10351v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10351v2)
- **Published**: 2018-01-31 08:44:02+00:00
- **Updated**: 2018-03-28 10:19:48+00:00
- **Authors**: Ofir Nabati, David Mendlovic, Raja Giryes
- **Comment**: None
- **Journal**: ICCP 2018
- **Summary**: Light field photography has been studied thoroughly in recent years. One of its drawbacks is the need for multi-lens in the imaging. To compensate that, compressed light field photography has been proposed to tackle the trade-offs between the spatial and angular resolutions. It obtains by only one lens, a compressed version of the regular multi-lens system. The acquisition system consists of a dedicated hardware followed by a decompression algorithm, which usually suffers from high computational time. In this work, we propose a computationally efficient neural network that recovers a high-quality color light field from a single coded image. Unlike previous works, we compress the color channels as well, removing the need for a CFA in the imaging system. Our approach outperforms existing solutions in terms of recovery quality and computational complexity. We propose also a neural network for depth map extraction based on the decompressed light field, which is trained in an unsupervised manner without the ground truth depth map.



### A CNN-based Spatial Feature Fusion Algorithm for Hyperspectral Imagery Classification
- **Arxiv ID**: http://arxiv.org/abs/1801.10355v2
- **DOI**: 10.1109/TGRS.2019.2911993
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10355v2)
- **Published**: 2018-01-31 08:57:10+00:00
- **Updated**: 2018-08-07 07:50:02+00:00
- **Authors**: Alan J. X. Guo, Fei Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: The shortage of training samples remains one of the main obstacles in applying the artificial neural networks (ANN) to the hyperspectral images classification. To fuse the spatial and spectral information, pixel patches are often utilized to train a model, which may further aggregate this problem. In the existing works, an ANN model supervised by center-loss (ANNC) was introduced. Training merely with spectral information, the ANNC yields discriminative spectral features suitable for the subsequent classification tasks. In this paper, a CNN-based spatial feature fusion (CSFF) algorithm is proposed, which allows a smart fusion of the spatial information to the spectral features extracted by ANNC. As a critical part of CSFF, a CNN-based discriminant model is introduced to estimate whether two paring pixels belong to the same class. At the testing stage, by applying the discriminant model to the pixel-pairs generated by the test pixel and its neighbors, the local structure is estimated and represented as a customized convolutional kernel. The spectral-spatial feature is obtained by a convolutional operation between the estimated kernel and the corresponding spectral features within a neighborhood. At last, the label of the test pixel is predicted by classifying the resulting spectral-spatial feature. Without increasing the number of training samples or involving pixel patches at the training stage, the CSFF framework achieves the state-of-the-art by declining $20\%-50\%$ classification failures in experiments on three well-known hyperspectral images.



### Synchronized Detection and Recovery of Steganographic Messages with Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1801.10365v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1801.10365v3)
- **Published**: 2018-01-31 09:11:18+00:00
- **Updated**: 2019-03-29 07:15:26+00:00
- **Authors**: Haichao Shi, Xiao-Yu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we mainly study the mechanism of learning the steganographic algorithm as well as combining the learning process with adversarial learning to learn a good steganographic algorithm. To handle the problem of embedding secret messages into the specific medium, we design a novel adversarial modules to learn the steganographic algorithm, and simultaneously train three modules called generator, discriminator and steganalyzer. Different from existing methods, the three modules are formalized as a game to communicate with each other. In the game, the generator and discriminator attempt to communicate with each other using secret messages hidden in an image. While the steganalyzer attempts to analyze whether there is a transmission of confidential information. We show that through unsupervised adversarial training, the adversarial model can produce robust steganographic solutions, which act like an encryption. Furthermore, we propose to utilize supervised adversarial training method to train a robust steganalyzer, which is utilized to discriminate whether an image contains secret information. Numerous experiments are conducted on publicly available dataset to demonstrate the effectiveness of the proposed method.



### Robust 3D Human Motion Reconstruction Via Dynamic Template Construction
- **Arxiv ID**: http://arxiv.org/abs/1801.10434v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1801.10434v1)
- **Published**: 2018-01-31 13:03:29+00:00
- **Updated**: 2018-01-31 13:03:29+00:00
- **Authors**: Zhong Li, Yu Ji, Wei Yang, Jinwei Ye, Jingyi Yu
- **Comment**: 3DV 2017 paper
- **Journal**: None
- **Summary**: In multi-view human body capture systems, the recovered 3D geometry or even the acquired imagery data can be heavily corrupted due to occlusions, noise, limited field of- view, etc. Direct estimation of 3D pose, body shape or motion on these low-quality data has been traditionally challenging.In this paper, we present a graph-based non-rigid shape registration framework that can simultaneously recover 3D human body geometry and estimate pose/motion at high fidelity.Our approach first generates a global full-body template by registering all poses in the acquired motion sequence.We then construct a deformable graph by utilizing the rigid components in the global template. We directly warp the global template graph back to each motion frame in order to fill in missing geometry. Specifically, we combine local rigidity and temporal coherence constraints to maintain geometry and motion consistencies. Comprehensive experiments on various scenes show that our method is accurate and robust even in the presence of drastic motions.



### Weighted Nonlocal Total Variation in Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1801.10441v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1801.10441v1)
- **Published**: 2018-01-31 13:24:34+00:00
- **Updated**: 2018-01-31 13:24:34+00:00
- **Authors**: Haohan Li, Zuoqiang Shi, Xiaoping Wang
- **Comment**: 15 pages, 49 figures
- **Journal**: None
- **Summary**: In this paper, a novel weighted nonlocal total variation (WNTV) method is proposed. Compared to the classical nonlocal total variation methods, our method modifies the energy functional to introduce a weight to balance between the labeled sets and unlabeled sets. With extensive numerical examples in semi-supervised clustering, image inpainting and image colorization, we demonstrate that WNTV provides an effective and efficient method in many image processing and machine learning problems.



### From Benedict Cumberbatch to Sherlock Holmes: Character Identification in TV series without a Script
- **Arxiv ID**: http://arxiv.org/abs/1801.10442v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10442v1)
- **Published**: 2018-01-31 13:25:29+00:00
- **Updated**: 2018-01-31 13:25:29+00:00
- **Authors**: Arsha Nagrani, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of this paper is the automatic identification of characters in TV and feature film material. In contrast to standard approaches to this task, which rely on the weak supervision afforded by transcripts and subtitles, we propose a new method requiring only a cast list. This list is used to obtain images of actors from freely available sources on the web, providing a form of partial supervision for this task. In using images of actors to recognize characters, we make the following three contributions: (i) We demonstrate that an automated semi-supervised learning approach is able to adapt from the actor's face to the character's face, including the face context of the hair; (ii) By building voice models for every character, we provide a bridge between frontal faces (for which there is plenty of actor-level supervision) and profile (for which there is very little or none); and (iii) by combining face context and speaker identification, we are able to identify characters with partially occluded faces and extreme facial poses. Results are presented on the TV series 'Sherlock' and the feature film 'Casablanca'. We achieve the state-of-the-art on the Casablanca benchmark, surpassing previous methods that have used the stronger supervision available from transcripts.



### Counting Cells in Time-Lapse Microscopy using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.10443v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10443v1)
- **Published**: 2018-01-31 13:27:45+00:00
- **Updated**: 2018-01-31 13:27:45+00:00
- **Authors**: Alexander Gomez Villa, Augusto Salazar, Igor Stefanini
- **Comment**: None
- **Journal**: None
- **Summary**: An automatic approach to counting any kind of cells could alleviate work of the experts and boost the research in fields such as regenerative medicine. In this paper, a method for microscopy cell counting using multiple frames (hence temporal information) is proposed. Unlike previous approaches where the cell counting is done independently in each frame (static cell counting), in this work the cell counting prediction is done using multiple frames (dynamic cell counting). A spatiotemporal model using ConvNets and long short term memory (LSTM) recurrent neural networks is proposed to overcome temporal variations. The model outperforms static cell counting in a publicly available dataset of stem cells. The advantages, working conditions and limitations of the ConvNet-LSTM method are discussed. Although our method is tested in cell counting, it can be extrapolated to quantify in video (or correlated image series) any kind of objects or volumes.



### Recovering from Random Pruning: On the Plasticity of Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.10447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10447v1)
- **Published**: 2018-01-31 13:40:47+00:00
- **Updated**: 2018-01-31 13:40:47+00:00
- **Authors**: Deepak Mittal, Shweta Bhardwaj, Mitesh M. Khapra, Balaraman Ravindran
- **Comment**: Accepted at WACV 2018
- **Journal**: None
- **Summary**: Recently there has been a lot of work on pruning filters from deep convolutional neural networks (CNNs) with the intention of reducing computations. The key idea is to rank the filters based on a certain criterion (say, $l_1$-norm, average percentage of zeros, etc) and retain only the top ranked filters. Once the low scoring filters are pruned away the remainder of the network is fine tuned and is shown to give performance comparable to the original unpruned network. In this work, we report experiments which suggest that the comparable performance of the pruned network is not due to the specific criterion chosen but due to the inherent plasticity of deep neural networks which allows them to recover from the loss of pruned filters once the rest of the filters are fine-tuned. Specifically, we show counter-intuitive results wherein by randomly pruning 25-50\% filters from deep CNNs we are able to obtain the same performance as obtained by using state of the art pruning methods. We empirically validate our claims by doing an exhaustive evaluation with VGG-16 and ResNet-50. Further, we also evaluate a real world scenario where a CNN trained on all 1000 ImageNet classes needs to be tested on only a small set of classes at test time (say, only animals). We create a new benchmark dataset from ImageNet to evaluate such class specific pruning and show that even here a random pruning strategy gives close to state of the art performance. Lastly, unlike existing approaches which mainly focus on the task of image classification, in this work we also report results on object detection. We show that using a simple random pruning strategy we can achieve significant speed up in object detection (74$\%$ improvement in fps) while retaining the same accuracy as that of the original Faster RCNN model.



### Densely Dilated Spatial Pooling Convolutional Network using benign loss functions for imbalanced volumetric prostate segmentation
- **Arxiv ID**: http://arxiv.org/abs/1801.10517v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10517v2)
- **Published**: 2018-01-31 16:01:36+00:00
- **Updated**: 2018-02-01 11:41:28+00:00
- **Authors**: Qiuhua Liu, Min Fu, Xinqi Gong, Hao Jiang
- **Comment**: 14pages, 5 figures, anonymous review in IJACAI2018
- **Journal**: None
- **Summary**: The high incidence rate of prostate disease poses a requirement in early detection for diagnosis. As one of the main imaging methods used for prostate cancer detection, Magnetic Resonance Imaging (MRI) has wide range of appearance and imbalance problems, making automated prostate segmentation fundamental but challenging. Here we propose a novel Densely Dilated Spatial Pooling Convolutional Network (DDSP ConNet) in encoder-decoder structure. It employs dense structure to combine dilated convolution and global pooling, thus supplies coarse segmentation results from encoder and decoder subnet and preserves more contextual information. To obtain richer hierarchical feature maps, residual long connection is furtherly adopted to fuse contexture features. Meanwhile, we adopt DSC loss and Jaccard loss functions to train our DDSP ConNet. We surprisingly found and proved that, in contrast to re-weighted cross entropy, DSC loss and Jaccard loss have a lot of benign properties in theory, including symmetry, continuity and differentiability about the parameters of network. Extensive experiments on the MICCAI PROMISE12 challenge dataset have been done to corroborate the effectiveness of our DDSP ConNet with DSC loss and Jaccard loss. Totally, our method achieves a score of 85.78 in the test dataset, outperforming most of other competitors.



### Feature Decomposition Based Saliency Detection in Electron Cryo-Tomograms
- **Arxiv ID**: http://arxiv.org/abs/1801.10562v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.10562v1)
- **Published**: 2018-01-31 17:25:14+00:00
- **Updated**: 2018-01-31 17:25:14+00:00
- **Authors**: Bo Zhou, Qiang Guo, Xiangrui Zeng, Min Xu
- **Comment**: 14 pages
- **Journal**: IEEE International Conference on Bioinformatics & Biomedicine,
  Workshop on Machine Learning in High Resolution Microscopy (BIBM-MLHRM 2018)
- **Summary**: Electron Cryo-Tomography (ECT) allows 3D visualization of subcellular structures at the submolecular resolution in close to the native state. However, due to the high degree of structural complexity and imaging limits, the automatic segmentation of cellular components from ECT images is very difficult. To complement and speed up existing segmentation methods, it is desirable to develop a generic cell component segmentation method that is 1) not specific to particular types of cellular components, 2) able to segment unknown cellular components, 3) fully unsupervised and does not rely on the availability of training data. As an important step towards this goal, in this paper, we propose a saliency detection method that computes the likelihood that a subregion in a tomogram stands out from the background. Our method consists of four steps: supervoxel over-segmentation, feature extraction, feature matrix decomposition, and computation of saliency. The method produces a distribution map that represents the regions' saliency in tomograms. Our experiments show that our method can successfully label most salient regions detected by a human observer, and able to filter out regions not containing cellular components. Therefore, our method can remove the majority of the background region, and significantly speed up the subsequent processing of segmentation and recognition of cellular components captured by ECT.



### Inference, Learning and Attention Mechanisms that Exploit and Preserve Sparsity in Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.10585v3
- **DOI**: 10.1007/s11263-020-01302-5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10585v3)
- **Published**: 2018-01-31 18:12:24+00:00
- **Updated**: 2020-03-12 13:44:04+00:00
- **Authors**: Timo Hackel, Mikhail Usvyatsov, Silvano Galliani, Jan D. Wegner, Konrad Schindler
- **Comment**: Updated to IJCV version
- **Journal**: None
- **Summary**: While CNNs naturally lend themselves to densely sampled data, and sophisticated implementations are available, they lack the ability to efficiently process sparse data. In this work we introduce a suite of tools that exploit sparsity in both the feature maps and the filter weights, and thereby allow for significantly lower memory footprints and computation times than the conventional dense framework when processing data with a high degree of sparsity. Our scheme provides (i) an efficient GPU implementation of a convolution layer based on direct, sparse convolution; (ii) a filter step within the convolution layer, which we call attention, that prevents fill-in, i.e., the tendency of convolution to rapidly decrease sparsity, and guarantees an upper bound on the computational resources; and (iii) an adaptation of the back-propagation algorithm, which makes it possible to combine our approach with standard learning frameworks, while still exploiting sparsity in the data and the model.



### Model compression for faster structural separation of macromolecules captured by Cellular Electron Cryo-Tomography
- **Arxiv ID**: http://arxiv.org/abs/1801.10597v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.10597v1)
- **Published**: 2018-01-31 18:39:41+00:00
- **Updated**: 2018-01-31 18:39:41+00:00
- **Authors**: Jialiang Guo, Bo Zhou, Xiangrui Zeng, Zachary Freyberg, Min Xu
- **Comment**: 8 pages
- **Journal**: International Conference on Image Analysis and Recognition (ICIAR)
  2018
- **Summary**: Electron Cryo-Tomography (ECT) enables 3D visualization of macromolecule structure inside single cells. Macromolecule classification approaches based on convolutional neural networks (CNN) were developed to separate millions of macromolecules captured from ECT systematically. However, given the fast accumulation of ECT data, it will soon become necessary to use CNN models to efficiently and accurately separate substantially more macromolecules at the prediction stage, which requires additional computational costs. To speed up the prediction, we compress classification models into compact neural networks with little in accuracy for deployment. Specifically, we propose to perform model compression through knowledge distillation. Firstly, a complex teacher network is trained to generate soft labels with better classification feasibility followed by training of customized student networks with simple architectures using the soft label to compress model complexity. Our tests demonstrate that our compressed models significantly reduce the number of parameters and time cost while maintaining similar classification accuracy.



### The Heart of an Image: Quantum Superposition and Entanglement in Visual Perception
- **Arxiv ID**: http://arxiv.org/abs/1802.02216v1
- **DOI**: None
- **Categories**: **cs.CV**, quant-ph
- **Links**: [PDF](http://arxiv.org/pdf/1802.02216v1)
- **Published**: 2018-01-31 19:41:57+00:00
- **Updated**: 2018-01-31 19:41:57+00:00
- **Authors**: Jonito Aerts Arguelles
- **Comment**: 20 pages, 7 figures
- **Journal**: None
- **Summary**: We analyse the way in which the principle that 'the whole is greater than the sum of its parts' manifests itself with phenomena of visual perception. For this investigation we use insights and techniques coming from quantum cognition, and more specifically we are inspired by the correspondence of this principle with the phenomenon of the conjunction effect in human cognition. We identify entities of meaning within artefacts of visual perception and rely on how such entities are modelled for corpuses of texts such as the webpages of the World-Wide Web for our study of how they appear in phenomena of visual perception. We identify concretely the conjunction effect in visual artefacts and analyse its structure in the example of a photograph. We also analyse quantum entanglement between different aspects of meaning in artefacts of visual perception. We confirm its presence by showing that well elected experiments on images retrieved accordingly by Google Images give rise to probabilities and expectation values violating the Clauser Horne Shimony Holt version of Bell's inequalities. We point out how this approach can lead to a mathematical description of the meaning content of a visual artefact such as a photograph.



### In Defense of Classical Image Processing: Fast Depth Completion on the CPU
- **Arxiv ID**: http://arxiv.org/abs/1802.00036v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00036v1)
- **Published**: 2018-01-31 19:46:11+00:00
- **Updated**: 2018-01-31 19:46:11+00:00
- **Authors**: Jason Ku, Ali Harakeh, Steven L. Waslander
- **Comment**: None
- **Journal**: None
- **Summary**: With the rise of data driven deep neural networks as a realization of universal function approximators, most research on computer vision problems has moved away from hand crafted classical image processing algorithms. This paper shows that with a well designed algorithm, we are capable of outperforming neural network based methods on the task of depth completion. The proposed algorithm is simple and fast, runs on the CPU, and relies only on basic image processing operations to perform depth completion of sparse LIDAR depth data. We evaluate our algorithm on the challenging KITTI depth completion benchmark, and at the time of submission, our method ranks first on the KITTI test server among all published methods. Furthermore, our algorithm is data independent, requiring no training data to perform the task at hand. The code written in Python will be made publicly available at https://github.com/kujason/ip_basic.



### Dynamics of Driver's Gaze: Explorations in Behavior Modeling & Maneuver Prediction
- **Arxiv ID**: http://arxiv.org/abs/1802.00066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00066v1)
- **Published**: 2018-01-31 21:09:01+00:00
- **Updated**: 2018-01-31 21:09:01+00:00
- **Authors**: Sujitha Martin, Sourabh Vora, Kevan Yuen, Mohan M. Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: The study and modeling of driver's gaze dynamics is important because, if and how the driver is monitoring the driving environment is vital for driver assistance in manual mode, for take-over requests in highly automated mode and for semantic perception of the surround in fully autonomous mode. We developed a machine vision based framework to classify driver's gaze into context rich zones of interest and model driver's gaze behavior by representing gaze dynamics over a time period using gaze accumulation, glance duration and glance frequencies. As a use case, we explore the driver's gaze dynamic patterns during maneuvers executed in freeway driving, namely, left lane change maneuver, right lane change maneuver and lane keeping. It is shown that condensing gaze dynamics into durations and frequencies leads to recurring patterns based on driver activities. Furthermore, modeling these patterns show predictive powers in maneuver detection up to a few hundred milliseconds a priori.



### Improved Image Segmentation via Cost Minimization of Multiple Hypotheses
- **Arxiv ID**: http://arxiv.org/abs/1802.00088v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00088v1)
- **Published**: 2018-01-31 22:37:46+00:00
- **Updated**: 2018-01-31 22:37:46+00:00
- **Authors**: Marc Bosch, Christopher M. Gifford, Austin G. Dress, Clare W. Lau, Jeffrey G. Skibo, Gordon A. Christie
- **Comment**: Accepted BMVC 17
- **Journal**: None
- **Summary**: Image segmentation is an important component of many image understanding systems. It aims to group pixels in a spatially and perceptually coherent manner. Typically, these algorithms have a collection of parameters that control the degree of over-segmentation produced. It still remains a challenge to properly select such parameters for human-like perceptual grouping. In this work, we exploit the diversity of segments produced by different choices of parameters. We scan the segmentation parameter space and generate a collection of image segmentation hypotheses (from highly over-segmented to under-segmented). These are fed into a cost minimization framework that produces the final segmentation by selecting segments that: (1) better describe the natural contours of the image, and (2) are more stable and persistent among all the segmentation hypotheses. We compare our algorithm's performance with state-of-the-art algorithms, showing that we can achieve improved results. We also show that our framework is robust to the choice of segmentation kernel that produces the initial set of hypotheses.



### Cross-domain CNN for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1802.00093v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00093v2)
- **Published**: 2018-01-31 23:02:08+00:00
- **Updated**: 2018-05-02 19:43:20+00:00
- **Authors**: Hyungtae Lee, Sungmin Eum, Heesung Kwon
- **Comment**: IGARSS 2018
- **Journal**: None
- **Summary**: In this paper, we address the dataset scarcity issue with the hyperspectral image classification. As only a few thousands of pixels are available for training, it is difficult to effectively learn high-capacity Convolutional Neural Networks (CNNs). To cope with this problem, we propose a novel cross-domain CNN containing the shared parameters which can co-learn across multiple hyperspectral datasets. The network also contains the non-shared portions designed to handle the dataset specific spectral characteristics and the associated classification tasks. Our approach is the first attempt to learn a CNN for multiple hyperspectral datasets, in an end-to-end fashion. Moreover, we have experimentally shown that the proposed network trained on three of the widely used datasets outperform all the baseline networks which are trained on single dataset.



### Single Image Reflection Removal Using Deep Encoder-Decoder Network
- **Arxiv ID**: http://arxiv.org/abs/1802.00094v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.00094v1)
- **Published**: 2018-01-31 23:05:44+00:00
- **Updated**: 2018-01-31 23:05:44+00:00
- **Authors**: Zhixiang Chi, Xiaolin Wu, Xiao Shu, Jinjin Gu
- **Comment**: None
- **Journal**: None
- **Summary**: Image of a scene captured through a piece of transparent and reflective material, such as glass, is often spoiled by a superimposed layer of reflection image. While separating the reflection from a familiar object in an image is mentally not difficult for humans, it is a challenging, ill-posed problem in computer vision. In this paper, we propose a novel deep convolutional encoder-decoder method to remove the objectionable reflection by learning a map between image pairs with and without reflection. For training the neural network, we model the physical formation of reflections in images and synthesize a large number of photo-realistic reflection-tainted images from reflection-free images collected online. Extensive experimental results show that, although the neural network learns only from synthetic data, the proposed method is effective on real-world images, and it significantly outperforms the other tested state-of-the-art techniques.



