# Arxiv Papers in cs.CV on 2018-01-20
### Multiple Description Convolutional Neural Networks for Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1801.06611v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1801.06611v2)
- **Published**: 2018-01-20 01:28:20+00:00
- **Updated**: 2019-02-28 02:47:01+00:00
- **Authors**: Lijun Zhao, Huihui Bai, Anhong Wang, Yao Zhao
- **Comment**: 13 pages, 3 tables, and 6 figures. Accepted by IEEE Trans. CSVT. Our
  another MDC paper based on deep learning is "Deep Multiple Description Coding
  by Learning Scalar Quantization", which was accepted by DCC-2019.
  (arXiv:1811.01504)
- **Journal**: None
- **Summary**: Multiple description coding (MDC) is able to stably transmit the signal in the un-reliable and non-prioritized networks, which has been broadly studied for several decades. However, the traditional MDC doesn't well leverage image's context features to generate multiple descriptions. In this paper, we propose a novel standard-compliant convolutional neural network-based MDC framework in term of image's context features. Firstly, multiple description generator network (MDGN) is designed to produce appearance-similar yet feature-different multiple descriptions automatically according to image's content, which are compressed by standard codec. Secondly, we present multiple description reconstruction network (MDRN) including side reconstruction network (SRN) and central reconstruction network (CRN). When any one of two lossy descriptions is received at the decoder, SRN network is used to improve the quality of this decoded lossy description by removing the compression artifact and up-sampling simultaneously. Meanwhile, we utilize CRN network with two decoded descriptions as inputs for better reconstruction, if both of lossy descriptions are available. Thirdly, multiple description virtual codec network (MDVCN) is proposed to bridge the gap between MDGN network and MDRN network in order to train an end-to-end MDC framework. Here, two learning algorithms are provided to train our whole framework. In addition to structural similarity loss function, the produced descriptions are used as opposing labels with multiple description distance loss function to regularize the training of MDGN network. These losses guarantee that the generated description images are structurally similar yet finely diverse. Experimental results show a great deal of objective and subjective quality measurements to validate the efficiency of the proposed method.



### Side Information for Face Completion: a Robust PCA Approach
- **Arxiv ID**: http://arxiv.org/abs/1801.07580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.07580v1)
- **Published**: 2018-01-20 02:00:06+00:00
- **Updated**: 2018-01-20 02:00:06+00:00
- **Authors**: Niannan Xue, Jiankang Deng, Shiyang Cheng, Yannis Panagakis, Stefanos Zafeiriou
- **Comment**: arXiv admin note: text overlap with arXiv:1702.00648
- **Journal**: None
- **Summary**: Robust principal component analysis (RPCA) is a powerful method for learning low-rank feature representation of various visual data. However, for certain types as well as significant amount of error corruption, it fails to yield satisfactory results; a drawback that can be alleviated by exploiting domain-dependent prior knowledge or information. In this paper, we propose two models for the RPCA that take into account such side information, even in the presence of missing values. We apply this framework to the task of UV completion which is widely used in pose-invariant face recognition. Moreover, we construct a generative adversarial network (GAN) to extract side information as well as subspaces. These subspaces not only assist in the recovery but also speed up the process in case of large-scale data. We quantitatively and qualitatively evaluate the proposed approaches through both synthetic data and five real-world datasets to verify their effectiveness.



### Visualization of Hyperspectral Images Using Moving Least Squares
- **Arxiv ID**: http://arxiv.org/abs/1801.06635v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1801.06635v1)
- **Published**: 2018-01-20 07:01:27+00:00
- **Updated**: 2018-01-20 07:01:27+00:00
- **Authors**: Danping Liao, Siyu Chen, Yuntao Qian
- **Comment**: arXiv admin note: text overlap with arXiv:1712.01657
- **Journal**: None
- **Summary**: Displaying the large number of bands in a hyper spectral image on a trichromatic monitor has been an active research topic. The visualized image shall convey as much information as possible form the original data and facilitate image interpretation. Most existing methods display HSIs in false colors which contradict with human's experience and expectation. In this paper, we propose a nonlinear approach to visualize an input HSI with natural colors by taking advantage of a corresponding RGB image. Our approach is based on Moving Least Squares, an interpolation scheme for reconstructing a surface from a set of control points, which in our case is a set of matching pixels between the HSI and the corresponding RGB image. Based on MLS, the proposed method solves for each spectral signature a unique transformation so that the non linear structure of the HSI can be preserved. The matching pixels between a pair of HSI and RGB image can be reused to display other HSIs captured b the same imaging sensor with natural colors. Experiments show that the output image of the proposed method no only have natural colors but also maintain the visual information necessary for human analysis.



### Structured Inhomogeneous Density Map Learning for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1801.06642v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06642v1)
- **Published**: 2018-01-20 09:21:52+00:00
- **Updated**: 2018-01-20 09:21:52+00:00
- **Authors**: Hanhui Li, Xiangjian He, Hefeng Wu, Saeed Amirgholipour Kasmani, Ruomei Wang, Xiaonan Luo, Liang Lin
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: In this paper, we aim at tackling the problem of crowd counting in extremely high-density scenes, which contain hundreds, or even thousands of people. We begin by a comprehensive analysis of the most widely used density map-based methods, and demonstrate how easily existing methods are affected by the inhomogeneous density distribution problem, e.g., causing them to be sensitive to outliers, or be hard to optimized. We then present an extremely simple solution to the inhomogeneous density distribution problem, which can be intuitively summarized as extending the density map from 2D to 3D, with the extra dimension implicitly indicating the density level. Such solution can be implemented by a single Density-Aware Network, which is not only easy to train, but also can achieve the state-of-art performance on various challenging datasets.



### Visual Data Augmentation through Learning
- **Arxiv ID**: http://arxiv.org/abs/1801.06665v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.06665v1)
- **Published**: 2018-01-20 12:08:59+00:00
- **Updated**: 2018-01-20 12:08:59+00:00
- **Authors**: Grigorios G. Chrysos, Yannis Panagakis, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: The rapid progress in machine learning methods has been empowered by i) huge datasets that have been collected and annotated, ii) improved engineering (e.g. data pre-processing/normalization). The existing datasets typically include several million samples, which constitutes their extension a colossal task. In addition, the state-of-the-art data-driven methods demand a vast amount of data, hence a standard engineering trick employed is artificial data augmentation for instance by adding into the data cropped and (affinely) transformed images. However, this approach does not correspond to any change in the natural 3D scene.   We propose instead to perform data augmentation through learning realistic local transformations. We learn a forward and an inverse transformation that maps an image from the high-dimensional space of pixel intensities to a latent space which varies (approximately) linearly with the latent space of a realistically transformed version of the image. Such transformed images can be considered two successive frames in a video. Next, we utilize these transformations to learn a linear model that modifies the latent spaces and then use the inverse transformation to synthesize a new image. We argue that the this procedure produces powerful invariant representations. We perform both qualitative and quantitative experiments that demonstrate our proposed method creates new realistic images.



### A Directionally Selective Small Target Motion Detecting Visual Neural Network in Cluttered Backgrounds
- **Arxiv ID**: http://arxiv.org/abs/1801.06687v5
- **DOI**: 10.1109/TCYB.2018.2869384
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1801.06687v5)
- **Published**: 2018-01-20 15:11:07+00:00
- **Updated**: 2018-09-29 10:37:55+00:00
- **Authors**: Hongxin Wang, Jigen Peng, Shigang Yue
- **Comment**: 14 pages, 21 figures
- **Journal**: None
- **Summary**: Discriminating targets moving against a cluttered background is a huge challenge, let alone detecting a target as small as one or a few pixels and tracking it in flight. In the fly's visual system, a class of specific neurons, called small target motion detectors (STMDs), have been identified as showing exquisite selectivity for small target motion. Some of the STMDs have also demonstrated directional selectivity which means these STMDs respond strongly only to their preferred motion direction. Directional selectivity is an important property of these STMD neurons which could contribute to tracking small targets such as mates in flight. However, little has been done on systematically modeling these directional selective STMD neurons. In this paper, we propose a directional selective STMD-based neural network (DSTMD) for small target detection in a cluttered background. In the proposed neural network, a new correlation mechanism is introduced for direction selectivity via correlating signals relayed from two pixels. Then, a lateral inhibition mechanism is implemented on the spatial field for size selectivity of STMD neurons. Extensive experiments showed that the proposed neural network not only is in accord with current biological findings, i.e. showing directional preferences, but also worked reliably in detecting small targets against cluttered backgrounds.



### Determination of Digital Straight Segments Using the Slope
- **Arxiv ID**: http://arxiv.org/abs/1801.06694v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1801.06694v1)
- **Published**: 2018-01-20 16:08:12+00:00
- **Updated**: 2018-01-20 16:08:12+00:00
- **Authors**: Alejandro Cartas, María Elena Algorri
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new method for the recognition of digital straight lines based on the slope. This method combines the Freeman's chain coding scheme and new discovered properties of the digital slope introduced in this paper. We also present the efficiency of our method from a testbed.



### Learning Light Field Reconstruction from a Single Coded Image
- **Arxiv ID**: http://arxiv.org/abs/1801.06710v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06710v2)
- **Published**: 2018-01-20 18:17:19+00:00
- **Updated**: 2018-04-26 18:06:58+00:00
- **Authors**: Anil Kumar Vadathya, Saikiran Cholleti, Gautham Ramajayam, Vijayalakshmi Kanchana, Kaushik Mitra
- **Comment**: accepted at ACPR 2017
- **Journal**: None
- **Summary**: Light field imaging is a rich way of representing the 3D world around us. However, due to limited sensor resolution capturing light field data inherently poses spatio-angular resolution trade-off. In this paper, we propose a deep learning based solution to tackle the resolution trade-off. Specifically, we reconstruct full sensor resolution light field from a single coded image. We propose to do this in three stages 1) reconstruction of center view from the coded image 2) estimating disparity map from the coded image and center view 3) warping center view using the disparity to generate light field. We propose three neural networks for these stages. Our disparity estimation network is trained in an unsupervised manner alleviating the need for ground truth disparity. Our results demonstrate better recovery of parallax from the coded image. Also, we get better results than dictionary learning based approaches both qualitatively and quatitatively.



### DeepISP: Towards Learning an End-to-End Image Processing Pipeline
- **Arxiv ID**: http://arxiv.org/abs/1801.06724v2
- **DOI**: 10.1109/TIP.2018.2872858
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1801.06724v2)
- **Published**: 2018-01-20 20:41:05+00:00
- **Updated**: 2019-02-03 12:32:36+00:00
- **Authors**: Eli Schwartz, Raja Giryes, Alex M. Bronstein
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing 28.2 (2019): 912-923
- **Summary**: We present DeepISP, a full end-to-end deep neural model of the camera image signal processing (ISP) pipeline. Our model learns a mapping from the raw low-light mosaiced image to the final visually compelling image and encompasses low-level tasks such as demosaicing and denoising as well as higher-level tasks such as color correction and image adjustment. The training and evaluation of the pipeline were performed on a dedicated dataset containing pairs of low-light and well-lit images captured by a Samsung S7 smartphone camera in both raw and processed JPEG formats. The proposed solution achieves state-of-the-art performance in objective evaluation of PSNR on the subtask of joint denoising and demosaicing. For the full end-to-end pipeline, it achieves better visual quality compared to the manufacturer ISP, in both a subjective human assessment and when rated by a deep model trained for assessing image quality.



### EnKCF: Ensemble of Kernelized Correlation Filters for High-Speed Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1801.06729v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06729v1)
- **Published**: 2018-01-20 21:22:46+00:00
- **Updated**: 2018-01-20 21:22:46+00:00
- **Authors**: Burak Uzkent, YoungWoo Seo
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision technologies are very attractive for practical applications running on embedded systems. For such an application, it is desirable for the deployed algorithms to run in high-speed and require no offline training. To develop a single-target tracking algorithm with these properties, we propose an ensemble of the kernelized correlation filters (KCF), we call it EnKCF. A committee of KCFs is specifically designed to address the variations in scale and translation of moving objects. To guarantee a high-speed run-time performance, we deploy each of KCFs in turn, instead of applying multiple KCFs to each frame. To minimize any potential drifts between individual KCFs transition, we developed a particle filter. Experimental results showed that the performance of ours is, on average, 70.10% for precision at 20 pixels, 53.00% for success rate for the OTB100 data, and 54.50% and 40.2% for the UAV123 data. Experimental results showed that our method is better than other high-speed trackers over 5% on precision on 20 pixels and 10-20% on AUC on average. Moreover, our implementation ran at 340 fps for the OTB100 and at 416 fps for the UAV123 dataset that is faster than DCF (292 fps) for the OTB100 and KCF (292 fps) for the UAV123. To increase flexibility of the proposed EnKCF running on various platforms, we also explored different levels of deep convolutional features.



### Boundary-based Image Forgery Detection by Fast Shallow CNN
- **Arxiv ID**: http://arxiv.org/abs/1801.06732v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06732v2)
- **Published**: 2018-01-20 21:33:12+00:00
- **Updated**: 2018-02-03 00:45:33+00:00
- **Authors**: Zhongping Zhang, Yixuan Zhang, Zheng Zhou, Jiebo Luo
- **Comment**: 6 pages, 9 figures
- **Journal**: None
- **Summary**: Image forgery detection is the task of detecting and localizing forged parts in tampered images. Previous works mostly focus on high resolution images using traces of resampling features, demosaicing features or sharpness of edges. However, a good detection method should also be applicable to low resolution images because compressed or resized images are common these days. To this end, we propose a Shallow Convolutional Neural Network(SCNN), capable of distinguishing the boundaries of forged regions from original edges in low resolution images. SCNN is designed to utilize the information of chroma and saturation. Based on SCNN, two approaches that are named Sliding Windows Detection (SWD) and Fast SCNN, respectively, are developed to detect and localize image forgery region. In this paper, we substantiate that Fast SCNN can detect drastic change of chroma and saturation. In image forgery detection experiments Our model is evaluated on the CASIA 2.0 dataset. The results show that Fast SCNN performs well on low resolution images and achieves significant improvements over the state-of-the-art.



### End-to-end Multi-Modal Multi-Task Vehicle Control for Self-Driving Cars with Visual Perception
- **Arxiv ID**: http://arxiv.org/abs/1801.06734v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06734v2)
- **Published**: 2018-01-20 21:59:08+00:00
- **Updated**: 2018-02-02 22:13:57+00:00
- **Authors**: Zhengyuan Yang, Yixuan Zhang, Jerry Yu, Junjie Cai, Jiebo Luo
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) have been successfully applied to autonomous driving tasks, many in an end-to-end manner. Previous end-to-end steering control methods take an image or an image sequence as the input and directly predict the steering angle with CNN. Although single task learning on steering angles has reported good performances, the steering angle alone is not sufficient for vehicle control. In this work, we propose a multi-task learning framework to predict the steering angle and speed control simultaneously in an end-to-end manner. Since it is nontrivial to predict accurate speed values with only visual inputs, we first propose a network to predict discrete speed commands and steering angles with image sequences. Moreover, we propose a multi-modal multi-task network to predict speed values and steering angles by taking previous feedback speeds and visual recordings as inputs. Experiments are conducted on the public Udacity dataset and a newly collected SAIC dataset. Results show that the proposed model predicts steering angles and speed values accurately. Furthermore, we improve the failure data synthesis methods to solve the problem of error accumulation in real road tests.



