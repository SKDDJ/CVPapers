# Arxiv Papers in cs.CV on 2018-01-15
### Deep Net Triage: Analyzing the Importance of Network Layers via Structural Compression
- **Arxiv ID**: http://arxiv.org/abs/1801.04651v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04651v2)
- **Published**: 2018-01-15 03:04:18+00:00
- **Updated**: 2018-03-22 03:31:00+00:00
- **Authors**: Theodore S. Nowak, Jason J. Corso
- **Comment**: None
- **Journal**: None
- **Summary**: Despite their prevalence, deep networks are poorly understood. This is due, at least in part, to their highly parameterized nature. As such, while certain structures have been found to work better than others, the significance of a model's unique structure, or the importance of a given layer, and how these translate to overall accuracy, remains unclear. In this paper, we analyze these properties of deep neural networks via a process we term deep net triage. Like medical triage---the assessment of the importance of various wounds---we assess the importance of layers in a neural network, or as we call it, their criticality. We do this by applying structural compression, whereby we reduce a block of layers to a single layer. After compressing a set of layers, we apply a combination of initialization and training schemes, and look at network accuracy, convergence, and the layer's learned filters to assess the criticality of the layer. We apply this analysis across four data sets of varying complexity. We find that the accuracy of the model does not depend on which layer was compressed; that accuracy can be recovered or exceeded after compression by fine-tuning across the entire model; and, lastly, that Knowledge Distillation can be used to hasten convergence of a compressed network, but constrains the accuracy attainable to that of the base model.



### Hyperspectral recovery from RGB images using Gaussian Processes
- **Arxiv ID**: http://arxiv.org/abs/1801.04654v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04654v2)
- **Published**: 2018-01-15 03:26:09+00:00
- **Updated**: 2018-07-20 02:05:11+00:00
- **Authors**: Naveed Akhtar, Ajmal Mian
- **Comment**: Revision submitted to IEEE TPAMI
- **Journal**: None
- **Summary**: We propose to recover spectral details from RGB images of known spectral quantization by modeling natural spectra under Gaussian Processes and combining them with the RGB images. Our technique exploits Process Kernels to model the relative smoothness of reflectance spectra, and encourages non-negativity in the resulting signals for better estimation of the reflectance values. The Gaussian Processes are inferred in sets using clusters of spatio-spectrally correlated hyperspectral training patches. Each set is transformed to match the spectral quantization of the test RGB image. We extract overlapping patches from the RGB image and match them to the hyperspectral training patches by spectrally transforming the latter. The RGB patches are encoded over the transformed Gaussian Processes related to those hyperspectral patches and the resulting image is constructed by combining the codes with the original Processes. Our approach infers the desired Gaussian Processes under a fully Bayesian model inspired by Beta-Bernoulli Process, for which we also present the inference procedure. A thorough evaluation using three hyperspectral datasets demonstrates the effective extraction of spectral details from RGB images by the proposed technique.



### Enlarging Context with Low Cost: Efficient Arithmetic Coding with Trimmed Convolution
- **Arxiv ID**: http://arxiv.org/abs/1801.04662v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04662v2)
- **Published**: 2018-01-15 04:28:32+00:00
- **Updated**: 2018-07-03 14:54:20+00:00
- **Authors**: Mu Li, Shuhang Gu, David Zhang, Wangmeng Zuo
- **Comment**: None
- **Journal**: None
- **Summary**: Arithmetic coding is an essential class of coding techniques. One key issue of arithmetic encoding method is to predict the probability of the current coding symbol from its context, i.e., the preceding encoded symbols, which usually can be executed by building a look-up table (LUT). However, the complexity of LUT increases exponentially with the length of context. Thus, such solutions are limited to modeling large context, which inevitably restricts the compression performance. Several recent deep neural network-based solutions have been developed to account for large context, but are still costly in computation. The inefficiency of the existing methods are mainly attributed to that probability prediction is performed independently for the neighboring symbols, which actually can be efficiently conducted by shared computation. To this end, we propose a trimmed convolutional network for arithmetic encoding (TCAE) to model large context while maintaining computational efficiency. As for trimmed convolution, the convolutional kernels are specially trimmed to respect the compression order and context dependency of the input symbols. Benefited from trimmed convolution, the probability prediction of all symbols can be efficiently performed in one single forward pass via a fully convolutional network. Furthermore, to speed up the decoding process, a slope TCAE model is presented to divide the codes from a 3D code map into several blocks and remove the dependency between the codes inner one block for parallel decoding, which can 60x speed up the decoding process. Experiments show that our TCAE and slope TCAE attain better compression ratio in lossless gray image compression, and can be adopted in CNN-based lossy image compression to achieve state-of-the-art rate-distortion performance with real-time encoding speed.



### Combining Stereo Disparity and Optical Flow for Basic Scene Flow
- **Arxiv ID**: http://arxiv.org/abs/1801.04720v1
- **DOI**: 10.1007/978-3-658-21300-8_8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04720v1)
- **Published**: 2018-01-15 10:21:08+00:00
- **Updated**: 2018-01-15 10:21:08+00:00
- **Authors**: René Schuster, Christian Bailer, Oliver Wasenmüller, Didier Stricker
- **Comment**: Commercial Vehicle Technology Symposium (CVTS), 2018
- **Journal**: None
- **Summary**: Scene flow is a description of real world motion in 3D that contains more information than optical flow. Because of its complexity there exists no applicable variant for real-time scene flow estimation in an automotive or commercial vehicle context that is sufficiently robust and accurate. Therefore, many applications estimate the 2D optical flow instead. In this paper, we examine the combination of top-performing state-of-the-art optical flow and stereo disparity algorithms in order to achieve a basic scene flow. On the public KITTI Scene Flow Benchmark we demonstrate the reasonable accuracy of the combination approach and show its speed in computation.



### SAR Image Despeckling Using Quadratic-Linear Approximated L1-Norm
- **Arxiv ID**: http://arxiv.org/abs/1801.04751v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04751v1)
- **Published**: 2018-01-15 11:47:44+00:00
- **Updated**: 2018-01-15 11:47:44+00:00
- **Authors**: Fatih Nar
- **Comment**: None
- **Journal**: None
- **Summary**: Speckle noise, inherent in synthetic aperture radar (SAR) images, degrades the performance of the various SAR image analysis tasks. Thus, speckle noise reduction is a critical preprocessing step for smoothing homogeneous regions while preserving details. This letter proposes a variational despeckling approach where L1-norm total variation regularization term is approximated in a quadratic and linear manner to increase accuracy while decreasing the computation time. Despeckling performance and computational efficiency of the proposed method are shown using synthetic and real-world SAR images.



### Deep Metric Learning with BIER: Boosting Independent Embeddings Robustly
- **Arxiv ID**: http://arxiv.org/abs/1801.04815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04815v1)
- **Published**: 2018-01-15 14:22:30+00:00
- **Updated**: 2018-01-15 14:22:30+00:00
- **Authors**: Michael Opitz, Georg Waltner, Horst Possegger, Horst Bischof
- **Comment**: Extension to our paper BIER: Boosting Independent Embeddings Robustly
  (ICCV 2017 oral) - submitted to PAMI
- **Journal**: None
- **Summary**: Learning similarity functions between image pairs with deep neural networks yields highly correlated activations of embeddings. In this work, we show how to improve the robustness of such embeddings by exploiting the independence within ensembles. To this end, we divide the last embedding layer of a deep network into an embedding ensemble and formulate training this ensemble as an online gradient boosting problem. Each learner receives a reweighted training sample from the previous learners. Further, we propose two loss functions which increase the diversity in our ensemble. These loss functions can be applied either for weight initialization or during training. Together, our contributions leverage large embedding sizes more effectively by significantly reducing correlation of the embedding and consequently increase retrieval accuracy of the embedding. Our method works with any differentiable loss function and does not introduce any additional parameters during test time. We evaluate our metric learning method on image retrieval tasks and show that it improves over state-of-the-art methods on the CUB 200-2011, Cars-196, Stanford Online Products, In-Shop Clothes Retrieval and VehicleID datasets.



### Generalizing, Decoding, and Optimizing Support Vector Machine Classification
- **Arxiv ID**: http://arxiv.org/abs/1801.04929v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.04929v1)
- **Published**: 2018-01-15 16:49:52+00:00
- **Updated**: 2018-01-15 16:49:52+00:00
- **Authors**: Mario Michael Krell
- **Comment**: None
- **Journal**: PhD Thesis, University of Bremen, Bremen, 1-236, 2015
- **Summary**: The classification of complex data usually requires the composition of processing steps. Here, a major challenge is the selection of optimal algorithms for preprocessing and classification (including parameterizations). Nowadays, parts of the optimization process are automized but expert knowledge and manual work are still required. We present three steps to face this process and ease the optimization. Namely, we take a theoretical view on classical classifiers, provide an approach to interpret the classifier together with the preprocessing, and integrate both into one framework which enables a semiautomatic optimization of the processing chain and which interfaces numerous algorithms.



### Classification of histopathological breast cancer images using iterative VMD aided Zernike moments & textural signatures
- **Arxiv ID**: http://arxiv.org/abs/1801.04880v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04880v1)
- **Published**: 2018-01-15 17:17:28+00:00
- **Updated**: 2018-01-15 17:17:28+00:00
- **Authors**: Subhankar Chattoraj, Karan Vishwakarma
- **Comment**: Working Paper
- **Journal**: None
- **Summary**: In this paper we present a novel method for an automated diagnosis of breast carcinoma through multilevel iterative variational mode decomposition (VMD) and textural features encompassing Zernaike moments, fractal dimension and entropy features namely, Kapoor entropy, Renyi entropy, Yager entropy features are extracted from VMD components. The proposed method considers the histopathological image as a set of multidimensional spatially-evolving signals. ReliefF algorithm is used to select the discriminatory features and statistically most significant features are fed to squares support vector machine (SVM) for classification. We evaluate the efficiency of the proposed methodology on publicly available Breakhis dataset containing 7,909 breast cancer histological images, collected from 82 patients, of both benign and malignant cases. Experimental results shows the efficacy of the proposed method in outperforming the state of the art while achieving an average classification rates of 89.61% and 88:23% using three-fold and ten-fold cross-validation strategies, respectively. This system can aid the pathologist in accurate and reliable diagnosis of biopsy samples. BreaKHis, a publicly dataset available at http://web.inf.ufpr.br/vri/breast-cancer-database.



### An octree cells occupancy geometric dimensionality descriptor for massive on-server point cloud visualisation and classification
- **Arxiv ID**: http://arxiv.org/abs/1801.05038v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.05038v1)
- **Published**: 2018-01-15 21:40:24+00:00
- **Updated**: 2018-01-15 21:40:24+00:00
- **Authors**: Remi Cura, Julien Perret, Nicolas Paparoditis
- **Comment**: extracted from article arXiv:1602.06920 ( arXiv:1602.06920 was split
  into 2 articles because it was to long and to hard to read)
- **Journal**: None
- **Summary**: Lidar datasets are becoming more and more common. They are appreciated for their precise 3D nature, and have a wide range of applications, such as surface reconstruction, object detection, visualisation, etc. For all this applications, having additional semantic information per point has potential of increasing the quality and the efficiency of the application. In the last decade the use of Machine Learning and more specifically classification methods have proved to be successful to create this semantic information. In this paradigm, the goal is to classify points into a set of given classes (for instance tree, building, ground, other). Some of these methods use descriptors (also called feature) of a point to learn and predict its class. Designing the descriptors is then the heart of these methods. Descriptors can be based on points geometry and attributes, use contextual information, etc. Furthermore, descriptors can be used by humans for easier visual understanding and sometimes filtering. In this work we propose a new simple geometric descriptor that gives information about the implicit local dimensionality of the point cloud at various scale. For instance a tree seen from afar is more volumetric in nature (3D), yet locally each leaves is rather planar (2D). To do so we build an octree centred on the point to consider, and compare the variation of the occupancy of the cells across the levels of the octree. We compare this descriptor with the state of the art dimensionality descriptor and show its interest. We further test the descriptor for classification within the Point Cloud Server, and demonstrate efficiency and correctness results.



### Student Beats the Teacher: Deep Neural Networks for Lateral Ventricles Segmentation in Brain MR
- **Arxiv ID**: http://arxiv.org/abs/1801.05040v2
- **DOI**: 10.1117/12.2293569
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.05040v2)
- **Published**: 2018-01-15 21:43:48+00:00
- **Updated**: 2018-03-03 11:54:32+00:00
- **Authors**: Mohsen Ghafoorian, Jonas Teuwen, Rashindra Manniesing, Frank-Erik de Leeuw, Bram van Ginneken, Nico Karssemeijer, Bram Platel
- **Comment**: 7 pages, 4 figures, SPIE Medical Imaging 2018 Conference paper
- **Journal**: Proc. SPIE 10574, 105742U (2 March 2018)
- **Summary**: Ventricular volume and its progression are known to be linked to several brain diseases such as dementia and schizophrenia. Therefore accurate measurement of ventricle volume is vital for longitudinal studies on these disorders, making automated ventricle segmentation algorithms desirable. In the past few years, deep neural networks have shown to outperform the classical models in many imaging domains. However, the success of deep networks is dependent on manually labeled data sets, which are expensive to acquire especially for higher dimensional data in the medical domain. In this work, we show that deep neural networks can be trained on much-cheaper-to-acquire pseudo-labels (e.g., generated by other automated less accurate methods) and still produce more accurate segmentations compared to the quality of the labels. To show this, we use noisy segmentation labels generated by a conventional region growing algorithm to train a deep network for lateral ventricle segmentation. Then on a large manually annotated test set, we show that the network significantly outperforms the conventional region growing algorithm which was used to produce the training labels for the network. Our experiments report a Dice Similarity Coefficient (DSC) of $0.874$ for the trained network compared to $0.754$ for the conventional region growing algorithm ($p < 0.001$).



### Circular Antenna Array Design for Breast Cancer Detection
- **Arxiv ID**: http://arxiv.org/abs/1801.05068v1
- **DOI**: 10.1109/SENSET.2017.8125016
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1801.05068v1)
- **Published**: 2018-01-15 23:42:27+00:00
- **Updated**: 2018-01-15 23:42:27+00:00
- **Authors**: Kalthoum Ouerghi, Najib Fadlallah, Amor Smida, Ridha Ghayoula, Jaouhar Fattahi, Noureddine Boulejfen
- **Comment**: None
- **Journal**: Sensors Networks Smart and Emerging Technologies (SENSET), 2017,
  pp. 1-4
- **Summary**: Microwave imaging for breast cancer detection is based on the contrast in the electrical properties of healthy fatty breast tissues. This paper presents an industrial, scientific and medical (ISM) bands comparative study of five microstrip patch antennas for microwave imaging at a frequency of 2.45 GHz. The choice of one antenna is made for an antenna array composed of 8 antennas for a microwave breast imaging system. Each antenna element is arranged in a circular configuration so that it can be directly faced to the breast phantom for better tumor detection. This choice is made by putting each antenna alone on the Breast skin to study the electric field, magnetic fields and current density in the healthy tissue of the breast phantom designed and simulated in Ansoft High Frequency Simulation Software (HFSS).



