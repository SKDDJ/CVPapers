# Arxiv Papers in cs.CV on 2018-01-12
### Deep Stereo Matching with Explicit Cost Aggregation Sub-Architecture
- **Arxiv ID**: http://arxiv.org/abs/1801.04065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04065v1)
- **Published**: 2018-01-12 05:58:27+00:00
- **Updated**: 2018-01-12 05:58:27+00:00
- **Authors**: Lidong Yu, Yucheng Wang, Yuwei Wu, Yunde Jia
- **Comment**: Accepted by AAAI18 as a spotlight poster
- **Journal**: None
- **Summary**: Deep neural networks have shown excellent performance for stereo matching. Many efforts focus on the feature extraction and similarity measurement of the matching cost computation step while less attention is paid on cost aggregation which is crucial for stereo matching. In this paper, we present a learning-based cost aggregation method for stereo matching by a novel sub-architecture in the end-to-end trainable pipeline. We reformulate the cost aggregation as a learning process of the generation and selection of cost aggregation proposals which indicate the possible cost aggregation results. The cost aggregation sub-architecture is realized by a two-stream network: one for the generation of cost aggregation proposals, the other for the selection of the proposals. The criterion for the selection is determined by the low-level structure information obtained from a light convolutional network. The two-stream network offers a global view guidance for the cost aggregation to rectify the mismatching value stemming from the limited view of the matching cost computation. The comprehensive experiments on challenge datasets such as KITTI and Scene Flow show that our method outperforms the state-of-the-art methods.



### How to augment a small learning set for improving the performances of a CNN-based steganalyzer?
- **Arxiv ID**: http://arxiv.org/abs/1801.04076v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1801.04076v2)
- **Published**: 2018-01-12 07:30:22+00:00
- **Updated**: 2018-02-03 14:48:30+00:00
- **Authors**: Mehdi Yedroudj, Marc Chaumont, Frédéric Comby
- **Comment**: EI'2018, in Proceedings of Media Watermarking, Security, and
  Forensics, Part of IS&T International Symposium on Electronic Imaging, San
  Francisco, California, USA, 28 Jan. -2 Feb. 2018, 7 pages
- **Journal**: None
- **Summary**: Deep learning and convolutional neural networks (CNN) have been intensively used in many image processing topics during last years. As far as steganalysis is concerned, the use of CNN allows reaching the state-of-the-art results. The performances of such networks often rely on the size of their learning database. An obvious preliminary assumption could be considering that "the bigger a database is, the better the results are". However, it appears that cautions have to be taken when increasing the database size if one desire to improve the classification accuracy i.e. enhance the steganalysis efficiency. To our knowledge, no study has been performed on the enrichment impact of a learning database on the steganalysis performance. What kind of images can be added to the initial learning set? What are the sensitive criteria: the camera models used for acquiring the images, the treatments applied to the images, the cameras proportions in the database, etc? This article continues the work carried out in a previous paper, and explores the ways to improve the performances of CNN. It aims at studying the effects of "base augmentation" on the performance of steganalysis using a CNN. We present the results of this study using various experimental protocols and various databases to define the good practices in base augmentation for steganalysis.



### How should a fixed budget of dwell time be spent in scanning electron microscopy to optimize image quality?
- **Arxiv ID**: http://arxiv.org/abs/1801.04085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04085v1)
- **Published**: 2018-01-12 08:28:49+00:00
- **Updated**: 2018-01-12 08:28:49+00:00
- **Authors**: Patrick Trampert, Faysal Bourghorbel, Pavel Potocek, Maurice Peemen, Christian Schlinkmann, Tim Dahmen, Philipp Slusallek
- **Comment**: submitted to Ultramicroscopy as a Full Length Article
- **Journal**: None
- **Summary**: In scanning electron microscopy, the achievable image quality is often limited by a maximum feasible acquisition time per dataset. Particularly with regard to three-dimensional or large field-of-view imaging, a compromise must be found between a high amount of shot noise, which leads to a low signal-to-noise ratio, and excessive acquisition times. Assuming a fixed acquisition time per frame, we compared three different strategies for algorithm-assisted image acquisition in scanning electron microscopy. We evaluated (1) raster scanning with a reduced dwell time per pixel followed by a state-of-the-art Denoising algorithm, (2) raster scanning with a decreased resolution in conjunction with a state-of-the-art Super Resolution algorithm, and (3) a sparse scanning approach where a fixed percentage of pixels is visited by the beam in combination with state-of-the-art inpainting algorithms. Additionally, we considered increased beam currents for each of the strategies. The experiments showed that sparse scanning using an appropriate reconstruction technique was superior to the other strategies.



### Hierarchical Motion Consistency Constraint for Efficient Geometrical Verification in UAV Image Matching
- **Arxiv ID**: http://arxiv.org/abs/1801.04096v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04096v1)
- **Published**: 2018-01-12 09:15:29+00:00
- **Updated**: 2018-01-12 09:15:29+00:00
- **Authors**: San Jiang, Wanshou Jiang
- **Comment**: 31 pages; 11104 words
- **Journal**: None
- **Summary**: This paper proposes a strategy for efficient geometrical verification in unmanned aerial vehicle (UAV) image matching. First, considering the complex transformation model between correspondence set in the image-space, feature points of initial candidate matches are projected onto an elevation plane in the object-space, with assistant of UAV flight control data and camera mounting angles. Spatial relationships are simplified as a 2D-translation in which a motion establishes the relation of two correspondence points. Second, a hierarchical motion consistency constraint, termed HMCC, is designed to eliminate outliers from initial candidate matches, which includes three major steps, namely the global direction consistency constraint, the local direction-change consistency constraint and the global length consistency constraint. To cope with scenarios with high outlier ratios, the HMCC is achieved by using a voting scheme. Finally, an efficient geometrical verification strategy is proposed by using the HMCC as a pre-processing step to increase inlier ratios before the consequent application of the basic RANSAC algorithm. The performance of the proposed strategy is verified through comprehensive comparison and analysis by using real UAV datasets captured with different photogrammetric systems. Experimental results demonstrate that the generated motions have noticeable separation ability, and the HMCC-RANSAC algorithm can efficiently eliminate outliers based on the motion consistency constraint, with a speedup ratio reaching to 6 for oblique UAV images. Even though the completeness sacrifice of approximately 7 percent of points is observed from image orientation tests, competitive orientation accuracy is achieved from all used datasets. For geometrical verification of both nadir and oblique UAV images, the proposed method can be a more efficient solution.



### Generative Single Image Reflection Separation
- **Arxiv ID**: http://arxiv.org/abs/1801.04102v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04102v1)
- **Published**: 2018-01-12 09:36:17+00:00
- **Updated**: 2018-01-12 09:36:17+00:00
- **Authors**: Donghoon Lee, Ming-Hsuan Yang, Songhwai Oh
- **Comment**: None
- **Journal**: None
- **Summary**: Single image reflection separation is an ill-posed problem since two scenes, a transmitted scene and a reflected scene, need to be inferred from a single observation. To make the problem tractable, in this work we assume that categories of two scenes are known. It allows us to address the problem by generating both scenes that belong to the categories while their contents are constrained to match with the observed image. A novel network architecture is proposed to render realistic images of both scenes based on adversarial learning. The network can be trained in a weakly supervised manner, i.e., it learns to separate an observed image without corresponding ground truth images of transmission and reflection scenes which are difficult to collect in practice. Experimental results on real and synthetic datasets demonstrate that the proposed algorithm performs favorably against existing methods.



### Detecting abnormal events in video using Narrowed Normality Clusters
- **Arxiv ID**: http://arxiv.org/abs/1801.05030v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.05030v4)
- **Published**: 2018-01-12 10:57:08+00:00
- **Updated**: 2018-11-16 08:59:19+00:00
- **Authors**: Radu Tudor Ionescu, Sorina Smeureanu, Marius Popescu, Bogdan Alexe
- **Comment**: Accepted at WACV 2019. arXiv admin note: text overlap with
  arXiv:1705.08182
- **Journal**: None
- **Summary**: We formulate the abnormal event detection problem as an outlier detection task and we propose a two-stage algorithm based on k-means clustering and one-class Support Vector Machines (SVM) to eliminate outliers. In the feature extraction stage, we propose to augment spatio-temporal cubes with deep appearance features extracted from the last convolutional layer of a pre-trained neural network. After extracting motion and appearance features from the training video containing only normal events, we apply k-means clustering to find clusters representing different types of normal motion and appearance features. In the first stage, we consider that clusters with fewer samples (with respect to a given threshold) contain mostly outliers, and we eliminate these clusters altogether. In the second stage, we shrink the borders of the remaining clusters by training a one-class SVM model on each cluster. To detected abnormal events in the test video, we analyze each test sample and consider its maximum normality score provided by the trained one-class SVM models, based on the intuition that a test sample can belong to only one cluster of normality. If the test sample does not fit well in any narrowed normality cluster, then it is labeled as abnormal. We compare our method with several state-of-the-art methods on three benchmark data sets. The empirical results indicate that our abnormal event detection framework can achieve better results in most cases, while processing the test video in real-time at 24 frames per second on a single CPU.



### Deep Episodic Memory: Encoding, Recalling, and Predicting Episodic Experiences for Robot Action Execution
- **Arxiv ID**: http://arxiv.org/abs/1801.04134v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1801.04134v3)
- **Published**: 2018-01-12 11:22:55+00:00
- **Updated**: 2018-07-14 21:20:44+00:00
- **Authors**: Jonas Rothfuss, Fabio Ferreira, Eren Erdal Aksoy, You Zhou, Tamim Asfour
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel deep neural network architecture for representing robot experiences in an episodic-like memory which facilitates encoding, recalling, and predicting action experiences. Our proposed unsupervised deep episodic memory model 1) encodes observed actions in a latent vector space and, based on this latent encoding, 2) infers most similar episodes previously experienced, 3) reconstructs original episodes, and 4) predicts future frames in an end-to-end fashion. Results show that conceptually similar actions are mapped into the same region of the latent vector space. Based on these results, we introduce an action matching and retrieval mechanism, benchmark its performance on two large-scale action datasets, 20BN-something-something and ActivityNet and evaluate its generalization capability in a real-world scenario on a humanoid robot.



### QuickNAT: A Fully Convolutional Network for Quick and Accurate Segmentation of Neuroanatomy
- **Arxiv ID**: http://arxiv.org/abs/1801.04161v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04161v2)
- **Published**: 2018-01-12 13:37:34+00:00
- **Updated**: 2018-11-24 09:12:31+00:00
- **Authors**: Abhijit Guha Roy, Sailesh Conjeti, Nassir Navab, Christian Wachinger
- **Comment**: Accepted for Publication at NeuroImage
- **Journal**: None
- **Summary**: Whole brain segmentation from structural magnetic resonance imaging (MRI) is a prerequisite for most morphological analyses, but is computationally intense and can therefore delay the availability of image markers after scan acquisition. We introduce QuickNAT, a fully convolutional, densely connected neural network that segments a \revision{MRI brain scan} in 20 seconds. To enable training of the complex network with millions of learnable parameters using limited annotated data, we propose to first pre-train on auxiliary labels created from existing segmentation software. Subsequently, the pre-trained model is fine-tuned on manual labels to rectify errors in auxiliary labels. With this learning strategy, we are able to use large neuroimaging repositories without manual annotations for training. In an extensive set of evaluations on eight datasets that cover a wide age range, pathology, and different scanners, we demonstrate that QuickNAT achieves superior segmentation accuracy and reliability in comparison to state-of-the-art methods, while being orders of magnitude faster. The speed up facilitates processing of large data repositories and supports translation of imaging biomarkers by making them available within seconds for fast clinical decision making.



### MSDNN: Multi-Scale Deep Neural Network for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1801.04187v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04187v1)
- **Published**: 2018-01-12 14:54:36+00:00
- **Updated**: 2018-01-12 14:54:36+00:00
- **Authors**: Fen Xiao, Wenzheng Deng, Liangchan Peng, Chunhong Cao, Kai Hu, Xieping Gao
- **Comment**: 10 pages, 12 figures
- **Journal**: None
- **Summary**: Salient object detection is a fundamental problem and has been received a great deal of attentions in computer vision. Recently deep learning model became a powerful tool for image feature extraction. In this paper, we propose a multi-scale deep neural network (MSDNN) for salient object detection. The proposed model first extracts global high-level features and context information over the whole source image with recurrent convolutional neural network (RCNN). Then several stacked deconvolutional layers are adopted to get the multi-scale feature representation and obtain a series of saliency maps. Finally, we investigate a fusion convolution module (FCM) to build a final pixel level saliency map. The proposed model is extensively evaluated on four salient object detection benchmark datasets. Results show that our deep model significantly outperforms other 12 state-of-the-art approaches.



### A High-Performance HOG Extractor on FPGA
- **Arxiv ID**: http://arxiv.org/abs/1802.02187v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02187v1)
- **Published**: 2018-01-12 18:12:43+00:00
- **Updated**: 2018-01-12 18:12:43+00:00
- **Authors**: Vinh Ngo, Arnau Casadevall, Marc Codina, David Castells-Rufas, Jordi Carrabina
- **Comment**: Presented at HIP3ES, 2018
- **Journal**: None
- **Summary**: Pedestrian detection is one of the key problems in emerging self-driving car industry. And HOG algorithm has proven to provide good accuracy for pedestrian detection. There are plenty of research works have been done in accelerating HOG algorithm on FPGA because of its low-power and high-throughput characteristics. In this paper, we present a high-performance HOG architecture for pedestrian detection on a low-cost FPGA platform. It achieves a maximum throughput of 526 FPS with 640x480 input images, which is 3.25 times faster than the state of the art design. The accelerator is integrated with SVM-based prediction in realizing a pedestrian detection system. And the power consumption of the whole system is comparable with the best existing implementations.



### Conditional Probability Models for Deep Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1801.04260v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.04260v4)
- **Published**: 2018-01-12 18:29:05+00:00
- **Updated**: 2019-06-04 14:38:14+00:00
- **Authors**: Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, Luc Van Gool
- **Comment**: CVPR 2018. Code available at https://github.com/fab-jul/imgcomp-cvpr
  . The first two authors contributed equally. Minor revision: fixed Fig. 2,
  added page numbers
- **Journal**: None
- **Summary**: Deep Neural Networks trained as image auto-encoders have recently emerged as a promising direction for advancing the state-of-the-art in image compression. The key challenge in learning such networks is twofold: To deal with quantization, and to control the trade-off between reconstruction error (distortion) and entropy (rate) of the latent image representation. In this paper, we focus on the latter challenge and propose a new technique to navigate the rate-distortion trade-off for an image compression auto-encoder. The main idea is to directly model the entropy of the latent representation by using a context model: A 3D-CNN which learns a conditional probability model of the latent distribution of the auto-encoder. During training, the auto-encoder makes use of the context model to estimate the entropy of its representation, and the context model is concurrently updated to learn the dependencies between the symbols in the latent representation. Our experiments show that this approach, when measured in MS-SSIM, yields a state-of-the-art image compression system based on a simple convolutional auto-encoder.



### Deep saliency: What is learnt by a deep network about saliency?
- **Arxiv ID**: http://arxiv.org/abs/1801.04261v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04261v2)
- **Published**: 2018-01-12 18:32:15+00:00
- **Updated**: 2018-03-22 16:48:49+00:00
- **Authors**: Sen He, Nicolas Pugeault
- **Comment**: Accepted paper in 2nd Workshop on Visualisation for Deep Learning in
  the 34th International Conference On Machine Learning
- **Journal**: 2nd Workshop on Visualisation for Deep Learning, ICML 2017
- **Summary**: Deep convolutional neural networks have achieved impressive performance on a broad range of problems, beating prior art on established benchmarks, but it often remains unclear what are the representations learnt by those systems and how they achieve such performance. This article examines the specific problem of saliency detection, where benchmarks are currently dominated by CNN-based approaches, and investigates the properties of the learnt representation by visualizing the artificial neurons' receptive fields.   We demonstrate that fine tuning a pre-trained network on the saliency detection task lead to a profound transformation of the network's deeper layers. Moreover we argue that this transformation leads to the emergence of receptive fields conceptually similar to the centre-surround filters hypothesized by early research on visual saliency.



### Real-world Anomaly Detection in Surveillance Videos
- **Arxiv ID**: http://arxiv.org/abs/1801.04264v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04264v3)
- **Published**: 2018-01-12 18:46:23+00:00
- **Updated**: 2019-02-14 16:00:30+00:00
- **Authors**: Waqas Sultani, Chen Chen, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Surveillance videos are able to capture a variety of realistic anomalies. In this paper, we propose to learn anomalies by exploiting both normal and anomalous videos. To avoid annotating the anomalous segments or clips in training videos, which is very time consuming, we propose to learn anomaly through the deep multiple instance ranking framework by leveraging weakly labeled training videos, i.e. the training labels (anomalous or normal) are at video-level instead of clip-level. In our approach, we consider normal and anomalous videos as bags and video segments as instances in multiple instance learning (MIL), and automatically learn a deep anomaly ranking model that predicts high anomaly scores for anomalous video segments. Furthermore, we introduce sparsity and temporal smoothness constraints in the ranking loss function to better localize anomaly during training. We also introduce a new large-scale first of its kind dataset of 128 hours of videos. It consists of 1900 long and untrimmed real-world surveillance videos, with 13 realistic anomalies such as fighting, road accident, burglary, robbery, etc. as well as normal activities. This dataset can be used for two tasks. First, general anomaly detection considering all anomalies in one group and all normal activities in another group. Second, for recognizing each of 13 anomalous activities. Our experimental results show that our MIL method for anomaly detection achieves significant improvement on anomaly detection performance as compared to the state-of-the-art approaches. We provide the results of several recent deep learning baselines on anomalous activity recognition. The low recognition performance of these baselines reveals that our dataset is very challenging and opens more opportunities for future work. The dataset is available at: https://webpages.uncc.edu/cchen62/dataset.html



### Light Field Super-Resolution using a Low-Rank Prior and Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.04314v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04314v1)
- **Published**: 2018-01-12 21:02:24+00:00
- **Updated**: 2018-01-12 21:02:24+00:00
- **Authors**: Reuben A. Farrugia, Christine Guillemot
- **Comment**: None
- **Journal**: None
- **Summary**: Light field imaging has recently known a regain of interest due to the availability of practical light field capturing systems that offer a wide range of applications in the field of computer vision. However, capturing high-resolution light fields remains technologically challenging since the increase in angular resolution is often accompanied by a significant reduction in spatial resolution. This paper describes a learning-based spatial light field super-resolution method that allows the restoration of the entire light field with consistency across all sub-aperture images. The algorithm first uses optical flow to align the light field and then reduces its angular dimension using low-rank approximation. We then consider the linearly independent columns of the resulting low-rank model as an embedding, which is restored using a deep convolutional neural network (DCNN). The super-resolved embedding is then used to reconstruct the remaining sub-aperture images. The original disparities are restored using inverse warping where missing pixels are approximated using a novel light field inpainting algorithm. Experimental results show that the proposed method outperforms existing light field super-resolution algorithms, achieving PSNR gains of 0.23 dB over the second best performing method. This performance can be further improved using iterative back-projection as a post-processing step.



### Prototypicality effects in global semantic description of objects
- **Arxiv ID**: http://arxiv.org/abs/1801.04331v3
- **DOI**: 10.1109/WACV.2019.00136
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04331v3)
- **Published**: 2018-01-12 21:58:40+00:00
- **Updated**: 2018-12-17 20:27:35+00:00
- **Authors**: Omar Vidal Pino, Erickson Rangel Nascimento, Mario Fernando Montenegro Campos
- **Comment**: Paper accepted in IEEE Winter Conference on Applications of Computer
  Vision 2019 (WACV2019). Content: 10 pages (8 + 2 reference) with 7 figures
- **Journal**: None
- **Summary**: In this paper, we introduce a novel approach for semantic description of object features based on the prototypicality effects of the Prototype Theory. Our prototype-based description model encodes and stores the semantic meaning of an object, while describing its features using the semantic prototype computed by CNN-classifications models. Our method uses semantic prototypes to create discriminative descriptor signatures that describe an object highlighting its most distinctive features within the category. Our experiments show that: i) our descriptor preserves the semantic information used by the CNN-models in classification tasks; ii) our distance metric can be used as the object's typicality score; iii) our descriptor signatures are semantically interpretable and enables the simulation of the prototypical organization of objects within a category.



### TieNet: Text-Image Embedding Network for Common Thorax Disease Classification and Reporting in Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/1801.04334v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04334v1)
- **Published**: 2018-01-12 22:04:30+00:00
- **Updated**: 2018-01-12 22:04:30+00:00
- **Authors**: Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Ronald M. Summers
- **Comment**: v1: Main paper + supplementary material
- **Journal**: None
- **Summary**: Chest X-rays are one of the most common radiological examinations in daily clinical routines. Reporting thorax diseases using chest X-rays is often an entry-level task for radiologist trainees. Yet, reading a chest X-ray image remains a challenging job for learning-oriented machine intelligence, due to (1) shortage of large-scale machine-learnable medical image datasets, and (2) lack of techniques that can mimic the high-level reasoning of human radiologists that requires years of knowledge accumulation and professional training. In this paper, we show the clinical free-text radiological reports can be utilized as a priori knowledge for tackling these two key problems. We propose a novel Text-Image Embedding network (TieNet) for extracting the distinctive image and text representations. Multi-level attention models are integrated into an end-to-end trainable CNN-RNN architecture for highlighting the meaningful text words and image regions. We first apply TieNet to classify the chest X-rays by using both image features and text embeddings extracted from associated reports. The proposed auto-annotation framework achieves high accuracy (over 0.9 on average in AUCs) in assigning disease labels for our hand-label evaluation dataset. Furthermore, we transform the TieNet into a chest X-ray reporting system. It simulates the reporting process and can output disease classification and a preliminary report together. The classification results are significantly improved (6% increase on average in AUCs) compared to the state-of-the-art baseline on an unseen and hand-labeled dataset (OpenI).



