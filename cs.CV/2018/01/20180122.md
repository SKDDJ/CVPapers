# Arxiv Papers in cs.CV on 2018-01-22
### Trajectory-based Radical Analysis Network for Online Handwritten Chinese Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.10109v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10109v1)
- **Published**: 2018-01-22 02:42:32+00:00
- **Updated**: 2018-01-22 02:42:32+00:00
- **Authors**: Jianshu Zhang, Yixing Zhu, Jun Du, Lirong Dai
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, great progress has been made for online handwritten Chinese character recognition due to the emergence of deep learning techniques. However, previous research mostly treated each Chinese character as one class without explicitly considering its inherent structure, namely the radical components with complicated geometry. In this study, we propose a novel trajectory-based radical analysis network (TRAN) to firstly identify radicals and analyze two-dimensional structures among radicals simultaneously, then recognize Chinese characters by generating captions of them based on the analysis of their internal radicals. The proposed TRAN employs recurrent neural networks (RNNs) as both an encoder and a decoder. The RNN encoder makes full use of online information by directly transforming handwriting trajectory into high-level features. The RNN decoder aims at generating the caption by detecting radicals and spatial structures through an attention model. The manner of treating a Chinese character as a two-dimensional composition of radicals can reduce the size of vocabulary and enable TRAN to possess the capability of recognizing unseen Chinese character classes, only if the corresponding radicals have been seen. Evaluated on CASIA-OLHWDB database, the proposed approach significantly outperforms the state-of-the-art whole-character modeling approach with a relative character error rate (CER) reduction of 10%. Meanwhile, for the case of recognition of 500 unseen Chinese characters, TRAN can achieve a character accuracy of about 60% while the traditional whole-character method has no capability to handle them.



### MRI Cross-Modality NeuroImage-to-NeuroImage Translation
- **Arxiv ID**: http://arxiv.org/abs/1801.06940v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06940v2)
- **Published**: 2018-01-22 02:53:28+00:00
- **Updated**: 2018-09-11 07:17:13+00:00
- **Authors**: Qianye Yang, Nannan Li, Zixu Zhao, Xingyu Fan, Eric I-Chao Chang, Yan Xu
- **Comment**: 46 pages, 16 figures
- **Journal**: None
- **Summary**: We present a cross-modality generation framework that learns to generate translated modalities from given modalities in MR images without real acquisition. Our proposed method performs NeuroImage-to-NeuroImage translation (abbreviated as N2N) by means of a deep learning model that leverages conditional generative adversarial networks (cGANs). Our framework jointly exploits the low-level features (pixel-wise information) and high-level representations (e.g. brain tumors, brain structure like gray matter, etc.) between cross modalities which are important for resolving the challenging complexity in brain structures. Our framework can serve as an auxiliary method in clinical diagnosis and has great application potential. Based on our proposed framework, we first propose a method for cross-modality registration by fusing the deformation fields to adopt the cross-modality information from translated modalities. Second, we propose an approach for MRI segmentation, translated multichannel segmentation (TMS), where given modalities, along with translated modalities, are segmented by fully convolutional networks (FCN) in a multichannel manner. Both of these two methods successfully adopt the cross-modality information to improve the performance without adding any extra data. Experiments demonstrate that our proposed framework advances the state-of-the-art on five brain MRI datasets. We also observe encouraging results in cross-modality registration and segmentation on some widely adopted brain datasets. Overall, our work can serve as an auxiliary method in clinical diagnosis and be applied to various tasks in medical fields.   Keywords: image-to-image, cross-modality, registration, segmentation, brain MRI



### An Improved LPTC Neural Model for Background Motion Direction Estimation
- **Arxiv ID**: http://arxiv.org/abs/1801.06976v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1801.06976v1)
- **Published**: 2018-01-22 06:57:05+00:00
- **Updated**: 2018-01-22 06:57:05+00:00
- **Authors**: Hongxin Wang, Jigen Peng, Shigang Yue
- **Comment**: 6 pages, 11 figures
- **Journal**: None
- **Summary**: A class of specialized neurons, called lobula plate tangential cells (LPTCs) has been shown to respond strongly to wide-field motion. The classic model, elementary motion detector (EMD) and its improved model, two-quadrant detector (TQD) have been proposed to simulate LPTCs. Although EMD and TQD can percept background motion, their outputs are so cluttered that it is difficult to discriminate actual motion direction of the background. In this paper, we propose a max operation mechanism to model a newly-found transmedullary neuron Tm9 whose physiological properties do not map onto EMD and TQD. This proposed max operation mechanism is able to improve the detection performance of TQD in cluttered background by filtering out irrelevant motion signals. We will demonstrate the functionality of this proposed mechanism in wide-field motion perception.



### Towards Automated Tuberculosis detection using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1801.07080v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.07080v1)
- **Published**: 2018-01-22 13:21:06+00:00
- **Updated**: 2018-01-22 13:21:06+00:00
- **Authors**: Sonaal Kant, Muktabh Mayank Srivastava
- **Comment**: None
- **Journal**: None
- **Summary**: Tuberculosis(TB) in India is the world's largest TB epidemic. TB leads to 480,000 deaths every year. Between the years 2006 and 2014, Indian economy lost US$340 Billion due to TB. This combined with the emergence of drug resistant bacteria in India makes the problem worse. The government of India has hence come up with a new strategy which requires a high-sensitivity microscopy based TB diagnosis mechanism. We propose a new Deep Neural Network based drug sensitive TB detection methodology with recall and precision of 83.78% and 67.55% respectively for bacillus detection. This method takes a microscopy image with proper zoom level as input and returns location of suspected TB germs as output. The high accuracy of our method gives it the potential to evolve into a high sensitivity system to diagnose TB when trained at scale.



### Staff line Removal using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.07141v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.07141v3)
- **Published**: 2018-01-22 15:35:51+00:00
- **Updated**: 2018-06-05 18:29:31+00:00
- **Authors**: Aishik Konwer, Ayan Kumar Bhunia, Abir Bhowmick, Ankan Kumar Bhunia, Prithaj Banerjee, Partha Pratim Roy, Umapada Pal
- **Comment**: To be appeared in ICPR 2018, 2018 International Conference on Pattern
  Recognition(Oral)
- **Journal**: None
- **Summary**: Staff line removal is a crucial pre-processing step in Optical Music Recognition. It is a challenging task to simultaneously reduce the noise and also retain the quality of music symbol context in ancient degraded music score images. In this paper we propose a novel approach for staff line removal, based on Generative Adversarial Networks. We convert staff line images into patches and feed them into a U-Net, used as Generator. The Generator intends to produce staff-less images at the output. Then the Discriminator does binary classification and differentiates between the generated fake staff-less image and real ground truth staff less image. For training, we use a Loss function which is a weighted combination of L2 loss and Adversarial loss. L2 loss minimizes the difference between real and fake staff-less image. Adversarial loss helps to retrieve more high quality textures in generated images. Thus our architecture supports solutions which are closer to ground truth and it reflects in our results. For evaluation we consider the ICDAR/GREC 2013 staff removal database. Our method achieves superior performance in comparison to other conventional approaches.



### E-swish: Adjusting Activations to Different Network Depths
- **Arxiv ID**: http://arxiv.org/abs/1801.07145v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.07145v1)
- **Published**: 2018-01-22 15:40:29+00:00
- **Updated**: 2018-01-22 15:40:29+00:00
- **Authors**: Eric Alcaide
- **Comment**: None
- **Journal**: None
- **Summary**: Activation functions have a notorious impact on neural networks on both training and testing the models against the desired problem. Currently, the most used activation function is the Rectified Linear Unit (ReLU). This paper introduces a new and novel activation function, closely related with the new activation $Swish = x * sigmoid(x)$ (Ramachandran et al., 2017) which generalizes it. We call the new activation $E-swish = \beta x * sigmoid(x)$. We show that E-swish outperforms many other well-known activations including both ReLU and Swish. For example, using E-swish provided 1.5% and 4.6% accuracy improvements on Cifar10 and Cifar100 respectively for the WRN 10-2 when compared to ReLU and 0.35% and 0.6% respectively when compared to Swish. The code to reproduce all our experiments can be found at https://github.com/EricAlcaide/E-swish



### Word Level Font-to-Font Image Translation using Convolutional Recurrent Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.07156v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.07156v3)
- **Published**: 2018-01-22 15:58:20+00:00
- **Updated**: 2018-05-24 01:45:56+00:00
- **Authors**: Ankan Kumar Bhunia, Ayan Kumar Bhunia, Prithaj Banerjee, Aishik Konwer, Abir Bhowmick, Partha Pratim Roy, Umapada Pal
- **Comment**: To be appeared in ICPR 2018, 2018 International Conference on Pattern
  Recognition
- **Journal**: None
- **Summary**: Conversion of one font to another font is very useful in real life applications. In this paper, we propose a Convolutional Recurrent Generative model to solve the word level font transfer problem. Our network is able to convert the font style of any printed text images from its current font to the required font. The network is trained end-to-end for the complete word images. Thus it eliminates the necessary pre-processing steps, like character segmentations. We extend our model to conditional setting that helps to learn one-to-many mapping function. We employ a novel convolutional recurrent model architecture in the Generator that efficiently deals with the word images of arbitrary width. It also helps to maintain the consistency of the final images after concatenating the generated image patches of target font. Besides, the Generator and the Discriminator network, we employ a Classification network to classify the generated word images of converted font style to their subsequent font categories. Most of the earlier works related to image translation are performed on square images. Our proposed architecture is the first work which can handle images of varying widths. Word images generally have varying width depending on the number of characters present. Hence, we test our model on a synthetically generated font dataset. We compare our method with some of the state-of-the-art methods for image translation. The superior performance of our network on the same dataset proves the ability of our model to learn the font distributions.



### Three Dimensional Fluorescence Microscopy Image Synthesis and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1801.07198v2
- **DOI**: 10.1109/CVPRW.2018.00298
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1801.07198v2)
- **Published**: 2018-01-22 17:08:13+00:00
- **Updated**: 2018-04-21 03:46:50+00:00
- **Authors**: Chichen Fu, Soonam Lee, David Joon Ho, Shuo Han, Paul Salama, Kenneth W. Dunn, Edward J. Delp
- **Comment**: Accepted by CVPR Workshop on Computer Vision for Microscopy Image
  Analysis (CVMI)
- **Journal**: None
- **Summary**: Advances in fluorescence microscopy enable acquisition of 3D image volumes with better image quality and deeper penetration into tissue. Segmentation is a required step to characterize and analyze biological structures in the images and recent 3D segmentation using deep learning has achieved promising results. One issue is that deep learning techniques require a large set of groundtruth data which is impractical to annotate manually for large 3D microscopy volumes. This paper describes a 3D deep learning nuclei segmentation method using synthetic 3D volumes for training. A set of synthetic volumes and the corresponding groundtruth are generated using spatially constrained cycle-consistent adversarial networks. Segmentation results demonstrate that our proposed method is capable of segmenting nuclei successfully for various data sets.



### Handwriting Trajectory Recovery using End-to-End Deep Encoder-Decoder Network
- **Arxiv ID**: http://arxiv.org/abs/1801.07211v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.07211v4)
- **Published**: 2018-01-22 17:25:05+00:00
- **Updated**: 2018-06-03 14:00:37+00:00
- **Authors**: Ayan Kumar Bhunia, Abir Bhowmick, Ankan Kumar Bhunia, Aishik Konwer, Prithaj Banerjee, Partha Pratim Roy, Umapada Pal
- **Comment**: To be appeared in ICPR 2018, 2018 International Conference on Pattern
  Recognition, Code Link:
  https://drive.google.com/file/d/1clT-UuXgPp6uFn1tmIXx481qvPUcY0fV/view
- **Journal**: None
- **Summary**: In this paper, we introduce a novel technique to recover the pen trajectory of offline characters which is a crucial step for handwritten character recognition. Generally, online acquisition approach has more advantage than its offline counterpart as the online technique keeps track of the pen movement. Hence, pen tip trajectory retrieval from offline text can bridge the gap between online and offline methods. Our proposed framework employs sequence to sequence model which consists of an encoder-decoder LSTM module. Our encoder module consists of Convolutional LSTM network, which takes an offline character image as the input and encodes the feature sequence to a hidden representation. The output of the encoder is fed to a decoder LSTM and we get the successive coordinate points from every time step of the decoder LSTM. Although the sequence to sequence model is a popular paradigm in various computer vision and language translation tasks, the main contribution of our work lies in designing an end-to-end network for a decade old popular problem in Document Image Analysis community. Tamil, Telugu and Devanagari characters of LIPI Toolkit dataset are used for our experiments. Our proposed method has achieved superior performance compared to the other conventional approaches.



### DiscrimNet: Semi-Supervised Action Recognition from Videos using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.07230v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.07230v1)
- **Published**: 2018-01-22 18:26:14+00:00
- **Updated**: 2018-01-22 18:26:14+00:00
- **Authors**: Unaiza Ahsan, Chen Sun, Irfan Essa
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an action recognition framework using Gen- erative Adversarial Networks. Our model involves train- ing a deep convolutional generative adversarial network (DCGAN) using a large video activity dataset without la- bel information. Then we use the trained discriminator from the GAN model as an unsupervised pre-training step and fine-tune the trained discriminator model on a labeled dataset to recognize human activities. We determine good network architectural and hyperparameter settings for us- ing the discriminator from DCGAN as a trained model to learn useful representations for action recognition. Our semi-supervised framework using only appearance infor- mation achieves superior or comparable performance to the current state-of-the-art semi-supervised action recog- nition methods on two challenging video activity datasets: UCF101 and HMDB51.



### Food recognition and recipe analysis: integrating visual content, context and external knowledge
- **Arxiv ID**: http://arxiv.org/abs/1801.07239v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1801.07239v1)
- **Published**: 2018-01-22 18:45:23+00:00
- **Updated**: 2018-01-22 18:45:23+00:00
- **Authors**: Luis Herranz, Weiqing Min, Shuqiang Jiang
- **Comment**: Survey about contextual food recognition and multimodal recipe
  analysis
- **Journal**: None
- **Summary**: The central role of food in our individual and social life, combined with recent technological advances, has motivated a growing interest in applications that help to better monitor dietary habits as well as the exploration and retrieval of food-related information. We review how visual content, context and external knowledge can be integrated effectively into food-oriented applications, with special focus on recipe analysis and retrieval, food recommendation, and the restaurant context as emerging directions.



### Automated dataset generation for image recognition using the example of taxonomy
- **Arxiv ID**: http://arxiv.org/abs/1802.02207v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.02207v1)
- **Published**: 2018-01-22 19:30:04+00:00
- **Updated**: 2018-01-22 19:30:04+00:00
- **Authors**: Jaro Milan Zink
- **Comment**: None
- **Journal**: None
- **Summary**: This master thesis addresses the subject of automatically generating a dataset for image recognition, which takes a lot of time when being done manually. As the thesis was written with motivation from the context of the biodiversity workgroup at the City University of Applied Sciences Bremen, the classification of taxonomic entries was chosen as an exemplary use case. In order to automate the dataset creation, a prototype was conceptualized and implemented after working out knowledge basics and analyzing requirements for it. It makes use of an pre-trained abstract artificial intelligence which is able to sort out images that do not contain the desired content. Subsequent to the implementation and the automated dataset creation resulting from it, an evaluation was performed. Other, manually collected datasets were compared to the one the prototype produced in means of specifications and accuracy. The results were more than satisfactory and showed that automatically generating a dataset for image recognition is not only possible, but also might be a decent alternative to spending time and money in doing this task manually. At the very end of this work, an idea of how to use the principle of employing abstract artificial intelligences for step-by-step classification of deeper taxonomic layers in a productive system is presented and discussed.



### Vehicle Detection in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/1801.07339v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.07339v2)
- **Published**: 2018-01-22 22:08:28+00:00
- **Updated**: 2018-02-09 13:07:43+00:00
- **Authors**: Michael Ying Yang, Wentong Liao, Xinbo Li, Bodo Rosenhahn
- **Comment**: None
- **Journal**: None
- **Summary**: The detection of vehicles in aerial images is widely applied in many applications. Comparing with object detection in the ground view images, vehicle detection in aerial images remains a challenging problem because of small vehicle size, monotone appearance and the complex background. In this paper, we propose a novel double focal loss convolutional neural network framework (DFL-CNN). In the proposed framework, the skip connection is used in the CNN structure to enhance the feature learning. Also, the focal loss function is used to substitute for conventional cross entropy loss function in both of the region proposed network and the final classifier. We further introduce the first large-scale vehicle detection dataset ITCVD with ground truth annotations for all the vehicles in the scene. We demonstrate the performance of our model on the existing benchmark DLR 3K dataset as well as the ITCVD dataset. The experimental results show that our DFL-CNN outperforms the baselines on vehicle detection.



