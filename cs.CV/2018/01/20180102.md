# Arxiv Papers in cs.CV on 2018-01-02
### Unsupervised Object-Level Video Summarization with Online Motion Auto-Encoder
- **Arxiv ID**: http://arxiv.org/abs/1801.00543v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00543v2)
- **Published**: 2018-01-02 03:55:36+00:00
- **Updated**: 2018-08-11 09:36:32+00:00
- **Authors**: Yujia Zhang, Xiaodan Liang, Dingwen Zhang, Min Tan, Eric P. Xing
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised video summarization plays an important role on digesting, browsing, and searching the ever-growing videos every day, and the underlying fine-grained semantic and motion information (i.e., objects of interest and their key motions) in online videos has been barely touched. In this paper, we investigate a pioneer research direction towards the fine-grained unsupervised object-level video summarization. It can be distinguished from existing pipelines in two aspects: extracting key motions of participated objects, and learning to summarize in an unsupervised and online manner. To achieve this goal, we propose a novel online motion Auto-Encoder (online motion-AE) framework that functions on the super-segmented object motion clips. Comprehensive experiments on a newly-collected surveillance dataset and public datasets have demonstrated the effectiveness of our proposed method.



### Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1801.00553v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00553v3)
- **Published**: 2018-01-02 05:22:06+00:00
- **Updated**: 2018-02-26 06:18:58+00:00
- **Authors**: Naveed Akhtar, Ajmal Mian
- **Comment**: Incorporates feedback provided by multiple researchers
- **Journal**: None
- **Summary**: Deep learning is at the heart of the current rise of machine learning and artificial intelligence. In the field of Computer Vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has lead to a large influx of contributions in this direction. This article presents the first comprehensive survey on adversarial attacks on deep learning in Computer Vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, we draw on the literature to provide a broader outlook of the research direction.



### Accurate reconstruction of image stimuli from human fMRI based on the decoding model with capsule network architecture
- **Arxiv ID**: http://arxiv.org/abs/1801.00602v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1801.00602v1)
- **Published**: 2018-01-02 10:39:05+00:00
- **Updated**: 2018-01-02 10:39:05+00:00
- **Authors**: Kai Qiao, Chi Zhang, Linyuan Wang, Bin Yan, Jian Chen, Lei Zeng, Li Tong
- **Comment**: None
- **Journal**: None
- **Summary**: In neuroscience, all kinds of computation models were designed to answer the open question of how sensory stimuli are encoded by neurons and conversely, how sensory stimuli can be decoded from neuronal activities. Especially, functional Magnetic Resonance Imaging (fMRI) studies have made many great achievements with the rapid development of the deep network computation. However, comparing with the goal of decoding orientation, position and object category from activities in visual cortex, accurate reconstruction of image stimuli from human fMRI is a still challenging work. In this paper, the capsule network (CapsNet) architecture based visual reconstruction (CNAVR) method is developed to reconstruct image stimuli. The capsule means containing a group of neurons to perform the better organization of feature structure and representation, inspired by the structure of cortical mini column including several hundred neurons in primates. The high-level capsule features in the CapsNet includes diverse features of image stimuli such as semantic class, orientation, location and so on. We used these features to bridge between human fMRI and image stimuli. We firstly employed the CapsNet to train the nonlinear mapping from image stimuli to high-level capsule features, and from high-level capsule features to image stimuli again in an end-to-end manner. After estimating the serviceability of each voxel by encoding performance to accomplish the selecting of voxels, we secondly trained the nonlinear mapping from dimension-decreasing fMRI data to high-level capsule features. Finally, we can predict the high-level capsule features with fMRI data, and reconstruct image stimuli with the CapsNet. We evaluated the proposed CNAVR method on the dataset of handwritten digital images, and exceeded about 10% than the accuracy of all existing state-of-the-art methods on the structural similarity index (SSIM).



### Scene-Adapted Plug-and-Play Algorithm with Guaranteed Convergence: Applications to Data Fusion in Imaging
- **Arxiv ID**: http://arxiv.org/abs/1801.00605v1
- **DOI**: None
- **Categories**: **cs.CV**, 94A08, 68U10, 47N10, I.4.5; I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/1801.00605v1)
- **Published**: 2018-01-02 10:59:10+00:00
- **Updated**: 2018-01-02 10:59:10+00:00
- **Authors**: Afonso M. Teodoro, José M. Bioucas-Dias, Mário A. T. Figueiredo
- **Comment**: Submitted
- **Journal**: None
- **Summary**: The recently proposed plug-and-play (PnP) framework allows leveraging recent developments in image denoising to tackle other, more involved, imaging inverse problems. In a PnP method, a black-box denoiser is plugged into an iterative algorithm, taking the place of a formal denoising step that corresponds to the proximity operator of some convex regularizer. While this approach offers flexibility and excellent performance, convergence of the resulting algorithm may be hard to analyze, as most state-of-the-art denoisers lack an explicit underlying objective function. In this paper, we propose a PnP approach where a scene-adapted prior (i.e., where the denoiser is targeted to the specific scene being imaged) is plugged into ADMM (alternating direction method of multipliers), and prove convergence of the resulting algorithm. Finally, we apply the proposed framework in two different imaging inverse problems: hyperspectral sharpening/fusion and image deblurring from blurred/noisy image pairs.



### High Dimensional Spaces, Deep Learning and Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1801.00634v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.00634v5)
- **Published**: 2018-01-02 12:54:22+00:00
- **Updated**: 2018-04-15 19:39:11+00:00
- **Authors**: Simant Dube
- **Comment**: 29 pages, 15 figures
- **Journal**: None
- **Summary**: In this paper, we analyze deep learning from a mathematical point of view and derive several novel results. The results are based on intriguing mathematical properties of high dimensional spaces. We first look at perturbation based adversarial examples and show how they can be understood using topological and geometrical arguments in high dimensions. We point out mistake in an argument presented in prior published literature, and we present a more rigorous, general and correct mathematical result to explain adversarial examples in terms of topology of image manifolds. Second, we look at optimization landscapes of deep neural networks and examine the number of saddle points relative to that of local minima. Third, we show how multiresolution nature of images explains perturbation based adversarial examples in form of a stronger result. Our results state that expectation of $L_2$-norm of adversarial perturbations is $O\left(\frac{1}{\sqrt{n}}\right)$ and therefore shrinks to 0 as image resolution $n$ becomes arbitrarily large. Finally, by incorporating the parts-whole manifold learning hypothesis for natural images, we investigate the working of deep neural networks and root causes of adversarial examples and discuss how future improvements can be made and how adversarial examples can be eliminated.



### Image denoising through bivariate shrinkage function in framelet domain
- **Arxiv ID**: http://arxiv.org/abs/1801.00635v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1801.00635v1)
- **Published**: 2018-01-02 12:56:52+00:00
- **Updated**: 2018-01-02 12:56:52+00:00
- **Authors**: Hamid Reza Shahdoosti
- **Comment**: 8 pages, 2 figures, conference
- **Journal**: None
- **Summary**: Denoising of coefficients in a sparse domain (e.g. wavelet) has been researched extensively because of its simplicity and effectiveness. Literature mainly has focused on designing the best global threshold. However, this paper proposes a new denoising method using bivariate shrinkage function in framelet domain. In the proposed method, maximum aposteriori probability is used for estimate of the denoised coefficient and non-Gaussian bivariate function is applied to model the statistics of framelet coefficients. For every framelet coefficient, there is a corresponding threshold depending on the local statistics of framelet coefficients. Experimental results show that using bivariate shrinkage function in framelet domain yields significantly superior image quality and higher PSNR than some well-known denoising methods.



### Learning audio and image representations with bio-inspired trainable feature extractors
- **Arxiv ID**: http://arxiv.org/abs/1801.00688v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1801.00688v1)
- **Published**: 2018-01-02 15:34:03+00:00
- **Updated**: 2018-01-02 15:34:03+00:00
- **Authors**: Nicola Strisciuglio
- **Comment**: Accepted for publication in the journal "Eleectronic Letters on
  Computer Vision and Image Understanding"
- **Journal**: None
- **Summary**: Recent advancements in pattern recognition and signal processing concern the automatic learning of data representations from labeled training samples. Typical approaches are based on deep learning and convolutional neural networks, which require large amount of labeled training samples. In this work, we propose novel feature extractors that can be used to learn the representation of single prototype samples in an automatic configuration process. We employ the proposed feature extractors in applications of audio and image processing, and show their effectiveness on benchmark data sets.



### Denoising Adversarial Autoencoders: Classifying Skin Lesions Using Limited Labelled Training Data
- **Arxiv ID**: http://arxiv.org/abs/1801.00693v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00693v1)
- **Published**: 2018-01-02 15:56:53+00:00
- **Updated**: 2018-01-02 15:56:53+00:00
- **Authors**: Antonia Creswell, Alison Pouplin, Anil A Bharath
- **Comment**: Under consideration for the IET Computer Vision Journal special issue
  on "Computer Vision in Cancer Data Analysis"
- **Journal**: None
- **Summary**: We propose a novel deep learning model for classifying medical images in the setting where there is a large amount of unlabelled medical data available, but labelled data is in limited supply. We consider the specific case of classifying skin lesions as either malignant or benign. In this setting, the proposed approach -- the semi-supervised, denoising adversarial autoencoder -- is able to utilise vast amounts of unlabelled data to learn a representation for skin lesions, and small amounts of labelled data to assign class labels based on the learned representation. We analyse the contributions of both the adversarial and denoising components of the model and find that the combination yields superior classification performance in the setting of limited labelled training data.



### Restricted Deformable Convolution based Road Scene Semantic Segmentation Using Surround View Cameras
- **Arxiv ID**: http://arxiv.org/abs/1801.00708v3
- **DOI**: 10.1109/TITS.2019.2939832
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00708v3)
- **Published**: 2018-01-02 16:23:09+00:00
- **Updated**: 2019-09-16 14:14:09+00:00
- **Authors**: Liuyuan Deng, Ming Yang, Hao Li, Tianyi Li, Bing Hu, Chunxiang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding the surrounding environment of the vehicle is still one of the challenges for autonomous driving. This paper addresses 360-degree road scene semantic segmentation using surround view cameras, which are widely equipped in existing production cars. First, in order to address large distortion problem in the fisheye images, Restricted Deformable Convolution (RDC) is proposed for semantic segmentation, which can effectively model geometric transformations by learning the shapes of convolutional filters conditioned on the input feature map. Second, in order to obtain a large-scale training set of surround view images, a novel method called zoom augmentation is proposed to transform conventional images to fisheye images. Finally, an RDC based semantic segmentation model is built; the model is trained for real-world surround view images through a multi-task learning architecture by combining real-world images with transformed images. Experiments demonstrate the effectiveness of the RDC to handle images with large distortions, and that the proposed approach shows a good performance using surround view cameras with the help of the transformed images.



### A Novel Approach to Skew-Detection and Correction of English Alphabets for OCR
- **Arxiv ID**: http://arxiv.org/abs/1801.00824v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00824v1)
- **Published**: 2018-01-02 20:25:59+00:00
- **Updated**: 2018-01-02 20:25:59+00:00
- **Authors**: Chinmay Chinara, Nishant Nath, Subhajeet Mishra, Sangram Keshari Sahoo, Farida Ashraf Ali
- **Comment**: 4 pages, 11 figures, 8 references, 2012 IEEE Student Conference on
  Research and Development
- **Journal**: IEEE SCOReD, 2012, Page 241-244
- **Summary**: Optical Character Recognition has been a challenging field in the advent of digital computers. It is needed where information is to be readable both to humans and machines. The process of OCR is composed of a set of pre and post processing steps that decide the level of accuracy of recognition. This paper deals with one of the pre-processing steps involved in the OCR process i.e. Skew (Slant) Detection and Correction. The proposed algorithm implemented for skew-detection is termed as the COG (Centre of Gravity) method and for that of skew-correction is Sub-Pixel Shifting method. The algorithm has been kept simple and optimized for efficient skew-detection and correction. The performance analysis of the algorithm after testing has been aptly demonstrated.



### Optimal Bayesian Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1801.00857v2
- **DOI**: 10.1109/TSP.2018.2839583
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.00857v2)
- **Published**: 2018-01-02 23:15:56+00:00
- **Updated**: 2018-05-25 17:30:44+00:00
- **Authors**: Alireza Karbalayghareh, Xiaoning Qian, Edward R. Dougherty
- **Comment**: IEEE Transactions on Signal Processing
- **Journal**: IEEE Transactions On Signal Processing, Vol. 66, No. 14, July 15,
  2018
- **Summary**: Transfer learning has recently attracted significant research attention, as it simultaneously learns from different source domains, which have plenty of labeled data, and transfers the relevant knowledge to the target domain with limited labeled data to improve the prediction performance. We propose a Bayesian transfer learning framework where the source and target domains are related through the joint prior density of the model parameters. The modeling of joint prior densities enables better understanding of the "transferability" between domains. We define a joint Wishart density for the precision matrices of the Gaussian feature-label distributions in the source and target domains to act like a bridge that transfers the useful information of the source domain to help classification in the target domain by improving the target posteriors. Using several theorems in multivariate statistics, the posteriors and posterior predictive densities are derived in closed forms with hypergeometric functions of matrix argument, leading to our novel closed-form and fast Optimal Bayesian Transfer Learning (OBTL) classifier. Experimental results on both synthetic and real-world benchmark data confirm the superb performance of the OBTL compared to the other state-of-the-art transfer learning and domain adaptation methods.



### Utilizing Semantic Visual Landmarks for Precise Vehicle Navigation
- **Arxiv ID**: http://arxiv.org/abs/1801.00858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00858v1)
- **Published**: 2018-01-02 23:18:48+00:00
- **Updated**: 2018-01-02 23:18:48+00:00
- **Authors**: Varun Murali, Han-Pang Chiu, Supun Samarasekera, Rakesh, Kumar
- **Comment**: Published at IEEE ITSC 2017
- **Journal**: None
- **Summary**: This paper presents a new approach for integrating semantic information for vision-based vehicle navigation. Although vision-based vehicle navigation systems using pre-mapped visual landmarks are capable of achieving submeter level accuracy in large-scale urban environment, a typical error source in this type of systems comes from the presence of visual landmarks or features from temporal objects in the environment, such as cars and pedestrians. We propose a gated factor graph framework to use semantic information associated with visual features to make decisions on outlier/ inlier computation from three perspectives: the feature tracking process, the geo-referenced map building process, and the navigation system using pre-mapped landmarks. The class category that the visual feature belongs to is extracted from a pre-trained deep learning network trained for semantic segmentation. The feasibility and generality of our approach is demonstrated by our implementations on top of two vision-based navigation systems. Experimental evaluations validate that the injection of semantic information associated with visual landmarks using our approach achieves substantial improvements in accuracy on GPS-denied navigation solutions for large-scale urban scenarios



