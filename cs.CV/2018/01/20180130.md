# Arxiv Papers in cs.CV on 2018-01-30
### Predicting Rapid Fire Growth (Flashover) Using Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.09804v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1801.09804v1)
- **Published**: 2018-01-30 00:09:48+00:00
- **Updated**: 2018-01-30 00:09:48+00:00
- **Authors**: Kyongsik Yun, Jessi Bustos, Thomas Lu
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: A flashover occurs when a fire spreads very rapidly through crevices due to intense heat. Flashovers present one of the most frightening and challenging fire phenomena to those who regularly encounter them: firefighters. Firefighters' safety and lives often depend on their ability to predict flashovers before they occur. Typical pre-flashover fire characteristics include dark smoke, high heat, and rollover ("angel fingers") and can be quantified by color, size, and shape. Using a color video stream from a firefighter's body camera, we applied generative adversarial neural networks for image enhancement. The neural networks were trained to enhance very dark fire and smoke patterns in videos and monitor dynamic changes in smoke and fire areas. Preliminary tests with limited flashover training videos showed that we predicted a flashover as early as 55 seconds before it occurred.



### Parallel Tracking and Verifying
- **Arxiv ID**: http://arxiv.org/abs/1801.10496v1
- **DOI**: 10.1109/TIP.2019.2904789
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10496v1)
- **Published**: 2018-01-30 00:18:22+00:00
- **Updated**: 2018-01-30 00:18:22+00:00
- **Authors**: Heng Fan, Haibin Ling
- **Comment**: Project is available at
  http://www.dabi.temple.edu/~hbling/code/PTAV/ptav.htm. arXiv admin note: text
  overlap with arXiv:1708.00153
- **Journal**: None
- **Summary**: Being intensively studied, visual object tracking has witnessed great advances in either speed (e.g., with correlation filters) or accuracy (e.g., with deep features). Real-time and high accuracy tracking algorithms, however, remain scarce. In this paper we study the problem from a new perspective and present a novel parallel tracking and verifying (PTAV) framework, by taking advantage of the ubiquity of multi-thread techniques and borrowing ideas from the success of parallel tracking and mapping in visual SLAM. The proposed PTAV framework is typically composed of two components, a (base) tracker T and a verifier V, working in parallel on two separate threads. The tracker T aims to provide a super real-time tracking inference and is expected to perform well most of the time; by contrast, the verifier V validates the tracking results and corrects T when needed. The key innovation is that, V does not work on every frame but only upon the requests from T; on the other end, T may adjust the tracking according to the feedback from V. With such collaboration, PTAV enjoys both the high efficiency provided by T and the strong discriminative power by V. Meanwhile, to adapt V to object appearance changes over time, we maintain a dynamic target template pool for adaptive verification, resulting in further performance improvements. In our extensive experiments on popular benchmarks including OTB2015, TC128, UAV20L and VOT2016, PTAV achieves the best tracking accuracy among all real-time trackers, and in fact even outperforms many deep learning based algorithms. Moreover, as a general framework, PTAV is very flexible with great potentials for future improvement and generalization.



### Object Detection in Videos by High Quality Object Linking
- **Arxiv ID**: http://arxiv.org/abs/1801.09823v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09823v3)
- **Published**: 2018-01-30 01:59:50+00:00
- **Updated**: 2019-04-08 07:56:01+00:00
- **Authors**: Peng Tang, Chunyu Wang, Xinggang Wang, Wenyu Liu, Wenjun Zeng, Jingdong Wang
- **Comment**: accepted by TPAMI
- **Journal**: None
- **Summary**: Compared with object detection in static images, object detection in videos is more challenging due to degraded image qualities. An effective way to address this problem is to exploit temporal contexts by linking the same object across video to form tubelets and aggregating classification scores in the tubelets. In this paper, we focus on obtaining high quality object linking results for better classification. Unlike previous methods that link objects by checking boxes between neighboring frames, we propose to link in the same frame. To achieve this goal, we extend prior methods in following aspects: (1) a cuboid proposal network that extracts spatio-temporal candidate cuboids which bound the movement of objects; (2) a short tubelet detection network that detects short tubelets in short video segments; (3) a short tubelet linking algorithm that links temporally-overlapping short tubelets to form long tubelets. Experiments on the ImageNet VID dataset show that our method outperforms both the static image detector and the previous state of the art. In particular, our method improves results by 8.8% over the static image detector for fast moving objects.



### Open3D: A Modern Library for 3D Data Processing
- **Arxiv ID**: http://arxiv.org/abs/1801.09847v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1801.09847v1)
- **Published**: 2018-01-30 04:33:20+00:00
- **Updated**: 2018-01-30 04:33:20+00:00
- **Authors**: Qian-Yi Zhou, Jaesik Park, Vladlen Koltun
- **Comment**: http://www.open3d.org
- **Journal**: None
- **Summary**: Open3D is an open-source library that supports rapid development of software that deals with 3D data. The Open3D frontend exposes a set of carefully selected data structures and algorithms in both C++ and Python. The backend is highly optimized and is set up for parallelization. Open3D was developed from a clean slate with a small and carefully considered set of dependencies. It can be set up on different platforms and compiled from source with minimal effort. The code is clean, consistently styled, and maintained via a clear code review mechanism. Open3D has been used in a number of published research projects and is actively deployed in the cloud. We welcome contributions from the open-source community.



### Structured Memory based Deep Model to Detect as well as Characterize Novel Inputs
- **Arxiv ID**: http://arxiv.org/abs/1801.09859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09859v1)
- **Published**: 2018-01-30 06:04:11+00:00
- **Updated**: 2018-01-30 06:04:11+00:00
- **Authors**: Pratik Prabhanjan Brahma, Qiuyuan Huang, Dapeng Wu
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: While deep learning has pushed the boundaries in various machine learning tasks, the current models are still far away from replicating many functions that a normal human brain can do. Explicit memorization based deep architecture have been recently proposed with the objective to understand and predict better. In this work, we design a system that involves a primary learner and an adjacent representational memory bank which is organized using a comparative learner. This spatially forked deep architecture with a structured memory can simultaneously predict and reason about the nature of an input, which may even belong to a category never seen in the training data, by relating it with the memorized past representations at the higher layers. Characterizing images of unseen object classes in both synthetic and real world datasets is used as an example to showcase the operational success of the proposed framework.



### Object Detection on Dynamic Occupancy Grid Maps Using Deep Learning and Automatic Label Generation
- **Arxiv ID**: http://arxiv.org/abs/1802.02202v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1802.02202v1)
- **Published**: 2018-01-30 08:18:31+00:00
- **Updated**: 2018-01-30 08:18:31+00:00
- **Authors**: Stefan Hoermann, Philipp Henzler, Martin Bach, Klaus Dietmayer
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the problem of object detection and pose estimation in a shared space downtown environment. For perception multiple laser scanners with 360{\deg} coverage were fused in a dynamic occupancy grid map (DOGMa). A single-stage deep convolutional neural network is trained to provide object hypotheses comprising of shape, position, orientation and an existence score from a single input DOGMa. Furthermore, an algorithm for offline object extraction was developed to automatically label several hours of training data. The algorithm is based on a two-pass trajectory extraction, forward and backward in time. Typical for engineered algorithms, the automatic label generation suffers from misdetections, which makes hard negative mining impractical. Therefore, we propose a loss function counteracting the high imbalance between mostly static background and extremely rare dynamic grid cells. Experiments indicate, that the trained network has good generalization capabilities since it detects objects occasionally lost by the label algorithm. Evaluation reaches an average precision (AP) of 75.9%



### E2E-MLT - an Unconstrained End-to-End Method for Multi-Language Scene Text
- **Arxiv ID**: http://arxiv.org/abs/1801.09919v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09919v2)
- **Published**: 2018-01-30 10:09:15+00:00
- **Updated**: 2018-12-05 21:49:16+00:00
- **Authors**: Michal Bu≈°ta, Yash Patel, Jiri Matas
- **Comment**: None
- **Journal**: None
- **Summary**: An end-to-end trainable (fully differentiable) method for multi-language scene text localization and recognition is proposed. The approach is based on a single fully convolutional network (FCN) with shared layers for both tasks.   E2E-MLT is the first published multi-language OCR for scene text. While trained in multi-language setup, E2E-MLT demonstrates competitive performance when compared to other methods trained for English scene text alone. The experiments show that obtaining accurate multi-language multi-script annotations is a challenging problem.



### Diagnose like a Radiologist: Attention Guided Convolutional Neural Network for Thorax Disease Classification
- **Arxiv ID**: http://arxiv.org/abs/1801.09927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09927v1)
- **Published**: 2018-01-30 10:55:23+00:00
- **Updated**: 2018-01-30 10:55:23+00:00
- **Authors**: Qingji Guan, Yaping Huang, Zhun Zhong, Zhedong Zheng, Liang Zheng, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper considers the task of thorax disease classification on chest X-ray images. Existing methods generally use the global image as input for network learning. Such a strategy is limited in two aspects. 1) A thorax disease usually happens in (small) localized areas which are disease specific. Training CNNs using global image may be affected by the (excessive) irrelevant noisy areas. 2) Due to the poor alignment of some CXR images, the existence of irregular borders hinders the network performance. In this paper, we address the above problems by proposing a three-branch attention guided convolution neural network (AG-CNN). AG-CNN 1) learns from disease-specific regions to avoid noise and improve alignment, 2) also integrates a global branch to compensate the lost discriminative cues by local branch. Specifically, we first learn a global CNN branch using global images. Then, guided by the attention heat map generated from the global branch, we inference a mask to crop a discriminative region from the global image. The local region is used for training a local CNN branch. Lastly, we concatenate the last pooling layers of both the global and local branches for fine-tuning the fusion branch. The Comprehensive experiment is conducted on the ChestX-ray14 dataset. We first report a strong global baseline producing an average AUC of 0.841 with ResNet-50 as backbone. After combining the local cues with the global information, AG-CNN improves the average AUC to 0.868. While DenseNet-121 is used, the average AUC achieves 0.871, which is a new state of the art in the community.



### Sliding Line Point Regression for Shape Robust Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/1801.09969v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09969v1)
- **Published**: 2018-01-30 12:58:10+00:00
- **Updated**: 2018-01-30 12:58:10+00:00
- **Authors**: Yixing Zhu, Jun Du
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional text detection methods mostly focus on quadrangle text. In this study we propose a novel method named sliding line point regression (SLPR) in order to detect arbitrary-shape text in natural scene. SLPR regresses multiple points on the edge of text line and then utilizes these points to sketch the outlines of the text. The proposed SLPR can be adapted to many object detection architectures such as Faster R-CNN and R-FCN. Specifically, we first generate the smallest rectangular box including the text with region proposal network (RPN), then isometrically regress the points on the edge of text by using the vertically and horizontally sliding lines. To make full use of information and reduce redundancy, we calculate x-coordinate or y-coordinate of target point by the rectangular box position, and just regress the remaining y-coordinate or x-coordinate. Accordingly we can not only reduce the parameters of system, but also restrain the points which will generate more regular polygon. Our approach achieved competitive results on traditional ICDAR2015 Incidental Scene Text benchmark and curve text detection dataset CTW1500.



### An Iterative Spanning Forest Framework for Superpixel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1801.10041v1
- **DOI**: 10.1109/TIP.2019.2897941
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10041v1)
- **Published**: 2018-01-30 15:05:38+00:00
- **Updated**: 2018-01-30 15:05:38+00:00
- **Authors**: John E. Vargas-Mu√±oz, Ananda S. Chowdhury, Eduardo B. Alexandre, Felipe L. Galv√£o, Paulo A. Vechiatto Miranda, Alexandre X. Falc√£o
- **Comment**: None
- **Journal**: None
- **Summary**: Superpixel segmentation has become an important research problem in image processing. In this paper, we propose an Iterative Spanning Forest (ISF) framework, based on sequences of Image Foresting Transforms, where one can choose i) a seed sampling strategy, ii) a connectivity function, iii) an adjacency relation, and iv) a seed pixel recomputation procedure to generate improved sets of connected superpixels (supervoxels in 3D) per iteration. The superpixels in ISF structurally correspond to spanning trees rooted at those seeds. We present five ISF methods to illustrate different choices of its components. These methods are compared with approaches from the state-of-the-art in effectiveness and efficiency. The experiments involve 2D and 3D datasets with distinct characteristics, and a high level application, named sky image segmentation. The theoretical properties of ISF are demonstrated in the supplementary material and the results show that some of its methods are competitive with or superior to the best baselines in effectiveness and efficiency.



### Deep Adversarial Attention Alignment for Unsupervised Domain Adaptation: the Benefit of Target Expectation Maximization
- **Arxiv ID**: http://arxiv.org/abs/1801.10068v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10068v4)
- **Published**: 2018-01-30 15:55:57+00:00
- **Updated**: 2018-08-11 16:44:04+00:00
- **Authors**: Guoliang Kang, Liang Zheng, Yan Yan, Yi Yang
- **Comment**: Accepted by ECCV 2018
- **Journal**: None
- **Summary**: In this paper, we make two contributions to unsupervised domain adaptation (UDA) using the convolutional neural network (CNN). First, our approach transfers knowledge in all the convolutional layers through attention alignment. Most previous methods align high-level representations, e.g., activations of the fully connected (FC) layers. In these methods, however, the convolutional layers which underpin critical low-level domain knowledge cannot be updated directly towards reducing domain discrepancy. Specifically, we assume that the discriminative regions in an image are relatively invariant to image style changes. Based on this assumption, we propose an attention alignment scheme on all the target convolutional layers to uncover the knowledge shared by the source domain. Second, we estimate the posterior label distribution of the unlabeled data for target network training. Previous methods, which iteratively update the pseudo labels by the target network and refine the target network by the updated pseudo labels, are vulnerable to label estimation errors. Instead, our approach uses category distribution to calculate the cross-entropy loss for training, thereby ameliorating the error accumulation of the estimated labels. The two contributions allow our approach to outperform the state-of-the-art methods by +2.6% on the Office-31 dataset.



### SegDenseNet: Iris Segmentation for Pre and Post Cataract Surgery
- **Arxiv ID**: http://arxiv.org/abs/1801.10100v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10100v2)
- **Published**: 2018-01-30 17:09:23+00:00
- **Updated**: 2018-04-19 09:27:38+00:00
- **Authors**: Aditya Lakra, Pavani Tripathi, Rohit Keshari, Mayank Vatsa, Richa Singh
- **Comment**: Corrected diagrams. Results remain the same!
- **Journal**: None
- **Summary**: Cataract is caused due to various factors such as age, trauma, genetics, smoking and substance consumption, and radiation. It is one of the major common ophthalmic diseases worldwide which can potentially affect iris-based biometric systems. India, which hosts the largest biometrics project in the world, has about 8 million people undergoing cataract surgery annually. While existing research shows that cataract does not have a major impact on iris recognition, our observations suggest that the iris segmentation approaches are not well equipped to handle cataract or post cataract surgery cases. Therefore, failure in iris segmentation affects the overall recognition performance. This paper presents an efficient iris segmentation algorithm with variations due to cataract and post cataract surgery. The proposed algorithm, termed as SegDenseNet, is a deep learning algorithm based on DenseNets. The experiments on the IIITD Cataract database show that improving the segmentation enhances the identification by up to 25% across different sensors and matchers.



### Video-based Sign Language Recognition without Temporal Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1801.10111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10111v1)
- **Published**: 2018-01-30 17:37:42+00:00
- **Updated**: 2018-01-30 17:37:42+00:00
- **Authors**: Jie Huang, Wengang Zhou, Qilin Zhang, Houqiang Li, Weiping Li
- **Comment**: 32nd AAAI Conference on Artificial Intelligence (AAAI-18), Feb. 2-7,
  2018, New Orleans, Louisiana, USA
- **Journal**: None
- **Summary**: Millions of hearing impaired people around the world routinely use some variants of sign languages to communicate, thus the automatic translation of a sign language is meaningful and important. Currently, there are two sub-problems in Sign Language Recognition (SLR), i.e., isolated SLR that recognizes word by word and continuous SLR that translates entire sentences. Existing continuous SLR methods typically utilize isolated SLRs as building blocks, with an extra layer of preprocessing (temporal segmentation) and another layer of post-processing (sentence synthesis). Unfortunately, temporal segmentation itself is non-trivial and inevitably propagates errors into subsequent steps. Worse still, isolated SLR methods typically require strenuous labeling of each word separately in a sentence, severely limiting the amount of attainable training data. To address these challenges, we propose a novel continuous sign recognition framework, the Hierarchical Attention Network with Latent Space (LS-HAN), which eliminates the preprocessing of temporal segmentation. The proposed LS-HAN consists of three components: a two-stream Convolutional Neural Network (CNN) for video feature representation generation, a Latent Space (LS) for semantic gap bridging, and a Hierarchical Attention Network (HAN) for latent space based recognition. Experiments are carried out on two large scale datasets. Experimental results demonstrate the effectiveness of the proposed framework.



### Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence
- **Arxiv ID**: http://arxiv.org/abs/1801.10112v3
- **DOI**: 10.1007/978-3-030-01252-6_33
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10112v3)
- **Published**: 2018-01-30 17:47:35+00:00
- **Updated**: 2018-08-14 16:10:46+00:00
- **Authors**: Arslan Chaudhry, Puneet K. Dokania, Thalaiyasingam Ajanthan, Philip H. S. Torr
- **Comment**: None
- **Journal**: None
- **Summary**: Incremental learning (IL) has received a lot of attention recently, however, the literature lacks a precise problem definition, proper evaluation settings, and metrics tailored specifically for the IL problem. One of the main objectives of this work is to fill these gaps so as to provide a common ground for better understanding of IL. The main challenge for an IL algorithm is to update the classifier whilst preserving existing knowledge. We observe that, in addition to forgetting, a known issue while preserving knowledge, IL also suffers from a problem we call intransigence, inability of a model to update its knowledge. We introduce two metrics to quantify forgetting and intransigence that allow us to understand, analyse, and gain better insights into the behaviour of IL algorithms. We present RWalk, a generalization of EWC++ (our efficient version of EWC [Kirkpatrick2016EWC]) and Path Integral [Zenke2017Continual] with a theoretically grounded KL-divergence based perspective. We provide a thorough analysis of various IL algorithms on MNIST and CIFAR-100 datasets. In these experiments, RWalk obtains superior results in terms of accuracy, and also provides a better trade-off between forgetting and intransigence.



### Image Captioning at Will: A Versatile Scheme for Effectively Injecting Sentiments into Image Descriptions
- **Arxiv ID**: http://arxiv.org/abs/1801.10121v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.10121v1)
- **Published**: 2018-01-30 18:13:14+00:00
- **Updated**: 2018-01-30 18:13:14+00:00
- **Authors**: Quanzeng You, Hailin Jin, Jiebo Luo
- **Comment**: 8 pages, 5 figures and 4 tables
- **Journal**: None
- **Summary**: Automatic image captioning has recently approached human-level performance due to the latest advances in computer vision and natural language understanding. However, most of the current models can only generate plain factual descriptions about the content of a given image. However, for human beings, image caption writing is quite flexible and diverse, where additional language dimensions, such as emotion, humor and language styles, are often incorporated to produce diverse, emotional, or appealing captions. In particular, we are interested in generating sentiment-conveying image descriptions, which has received little attention. The main challenge is how to effectively inject sentiments into the generated captions without altering the semantic matching between the visual content and the generated descriptions. In this work, we propose two different models, which employ different schemes for injecting sentiments into image captions. Compared with the few existing approaches, the proposed models are much simpler and yet more effective. The experimental results show that our model outperform the state-of-the-art models in generating sentimental (i.e., sentiment-bearing) image captions. In addition, we can also easily manipulate the model by assigning different sentiments to the testing image to generate captions with the corresponding sentiments.



### IONet: Learning to Cure the Curse of Drift in Inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/1802.02209v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1802.02209v1)
- **Published**: 2018-01-30 18:29:02+00:00
- **Updated**: 2018-01-30 18:29:02+00:00
- **Authors**: Changhao Chen, Xiaoxuan Lu, Andrew Markham, Niki Trigoni
- **Comment**: To appear in AAAI18 (Oral)
- **Journal**: None
- **Summary**: Inertial sensors play a pivotal role in indoor localization, which in turn lays the foundation for pervasive personal applications. However, low-cost inertial sensors, as commonly found in smartphones, are plagued by bias and noise, which leads to unbounded growth in error when accelerations are double integrated to obtain displacement. Small errors in state estimation propagate to make odometry virtually unusable in a matter of seconds. We propose to break the cycle of continuous integration, and instead segment inertial data into independent windows. The challenge becomes estimating the latent states of each window, such as velocity and orientation, as these are not directly observable from sensor data. We demonstrate how to formulate this as an optimization problem, and show how deep recurrent neural networks can yield highly accurate trajectories, outperforming state-of-the-art shallow techniques, on a wide range of tests and attachments. In particular, we demonstrate that IONet can generalize to estimate odometry for non-periodic motion, such as a shopping trolley or baby-stroller, an extremely challenging task for existing techniques.



