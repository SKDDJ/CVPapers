# Arxiv Papers in cs.CV on 2018-01-14
### Non-Parametric Transformation Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.04520v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.04520v6)
- **Published**: 2018-01-14 06:48:45+00:00
- **Updated**: 2018-09-08 22:45:23+00:00
- **Authors**: Dipan K. Pal, Marios Savvides
- **Comment**: Preprint only
- **Journal**: None
- **Summary**: ConvNets, through their architecture, only enforce invariance to translation. In this paper, we introduce a new class of deep convolutional architectures called Non-Parametric Transformation Networks (NPTNs) which can learn \textit{general} invariances and symmetries directly from data. NPTNs are a natural generalization of ConvNets and can be optimized directly using gradient descent. Unlike almost all previous works in deep architectures, they make no assumption regarding the structure of the invariances present in the data and in that aspect are flexible and powerful. We also model ConvNets and NPTNs under a unified framework called Transformation Networks (TN), which yields a better understanding of the connection between the two. We demonstrate the efficacy of NPTNs on data such as MNIST with extreme transformations and CIFAR10 where they outperform baselines, and further outperform several recent algorithms on ETH-80. They do so while having the same number of parameters. We also show that they are more effective than ConvNets in modelling symmetries and invariances from data, without the explicit knowledge of the added arbitrary nuisance transformations. Finally, we replace ConvNets with NPTNs within Capsule Networks and show that this enables Capsule Nets to perform even better.



### Using accumulation to optimize deep residual neural nets
- **Arxiv ID**: http://arxiv.org/abs/1803.05778v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05778v1)
- **Published**: 2018-01-14 07:17:00+00:00
- **Updated**: 2018-01-14 07:17:00+00:00
- **Authors**: Yatin Saraiya
- **Comment**: 7 pages, 6 figures, 1 table
- **Journal**: None
- **Summary**: Residual Neural Networks [1] won first place in all five main tracks of the ImageNet and COCO 2015 competitions. This kind of network involves the creation of pluggable modules such that the output contains a residual from the input. The residual in that paper is the identity function. We propose to include residuals from all lower layers, suitably normalized, to create the residual. This way, all previous layers contribute equally to the output of a layer. We show that our approach is an improvement on [1] for the CIFAR-10 dataset.



### Fix your classifier: the marginal value of training the last weight layer
- **Arxiv ID**: http://arxiv.org/abs/1801.04540v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.04540v2)
- **Published**: 2018-01-14 12:00:43+00:00
- **Updated**: 2018-03-20 08:56:25+00:00
- **Authors**: Elad Hoffer, Itay Hubara, Daniel Soudry
- **Comment**: https://openreview.net/forum?id=S1Dh8Tg0-
- **Journal**: International Conference on Learning Representations 2018
- **Summary**: Neural networks are commonly used as models for classification for a wide variety of tasks. Typically, a learned affine transformation is placed at the end of such models, yielding a per-class value used for classification. This classifier can have a vast number of parameters, which grows linearly with the number of possible classes, thus requiring increasingly more resources. In this work we argue that this classifier can be fixed, up to a global scale constant, with little or no loss of accuracy for most tasks, allowing memory and computational benefits. Moreover, we show that by initializing the classifier with a Hadamard matrix we can speed up inference as well. We discuss the implications for current understanding of neural network models.



### Frame-Recurrent Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1801.04590v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.04590v4)
- **Published**: 2018-01-14 17:53:53+00:00
- **Updated**: 2018-03-25 17:24:01+00:00
- **Authors**: Mehdi S. M. Sajjadi, Raviteja Vemulapalli, Matthew Brown
- **Comment**: Accepted at CVPR 2018
- **Journal**: None
- **Summary**: Recent advances in video super-resolution have shown that convolutional neural networks combined with motion compensation are able to merge information from multiple low-resolution (LR) frames to generate high-quality images. Current state-of-the-art methods process a batch of LR frames to generate a single high-resolution (HR) frame and run this scheme in a sliding window fashion over the entire video, effectively treating the problem as a large number of separate multi-frame super-resolution tasks. This approach has two main weaknesses: 1) Each input frame is processed and warped multiple times, increasing the computational cost, and 2) each output frame is estimated independently conditioned on the input frames, limiting the system's ability to produce temporally consistent results.   In this work, we propose an end-to-end trainable frame-recurrent video super-resolution framework that uses the previously inferred HR estimate to super-resolve the subsequent frame. This naturally encourages temporally consistent results and reduces the computational cost by warping only one image in each step. Furthermore, due to its recurrent nature, the proposed method has the ability to assimilate a large number of previous frames without increased computational demands. Extensive evaluations and comparisons with previous methods validate the strengths of our approach and demonstrate that the proposed framework is able to significantly outperform the current state of the art.



