# Arxiv Papers in cs.CV on 2018-01-23
### Learning to Prune Filters in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.07365v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.07365v1)
- **Published**: 2018-01-23 01:30:34+00:00
- **Updated**: 2018-01-23 01:30:34+00:00
- **Authors**: Qiangui Huang, Kevin Zhou, Suya You, Ulrich Neumann
- **Comment**: None
- **Journal**: None
- **Summary**: Many state-of-the-art computer vision algorithms use large scale convolutional neural networks (CNNs) as basic building blocks. These CNNs are known for their huge number of parameters, high redundancy in weights, and tremendous computing resource consumptions. This paper presents a learning algorithm to simplify and speed up these CNNs. Specifically, we introduce a "try-and-learn" algorithm to train pruning agents that remove unnecessary CNN filters in a data-driven way. With the help of a novel reward function, our agents removes a significant number of filters in CNNs while maintaining performance at a desired level. Moreover, this method provides an easy control of the tradeoff between network performance and its scale. Per- formance of our algorithm is validated with comprehensive pruning experiments on several popular CNNs for visual recognition and semantic segmentation tasks.



### Numerical Coordinate Regression with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.07372v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.07372v2)
- **Published**: 2018-01-23 02:18:04+00:00
- **Updated**: 2018-05-03 22:19:39+00:00
- **Authors**: Aiden Nibali, Zhen He, Stuart Morgan, Luke Prendergast
- **Comment**: None
- **Journal**: None
- **Summary**: We study deep learning approaches to inferring numerical coordinates for points of interest in an input image. Existing convolutional neural network-based solutions to this problem either take a heatmap matching approach or regress to coordinates with a fully connected output layer. Neither of these approaches is ideal, since the former is not entirely differentiable, and the latter lacks inherent spatial generalization. We propose our differentiable spatial to numerical transform (DSNT) to fill this gap. The DSNT layer adds no trainable parameters, is fully differentiable, and exhibits good spatial generalization. Unlike heatmap matching, DSNT works well with low heatmap resolutions, so it can be dropped in as an output layer for a wide range of existing fully convolutional architectures. Consequently, DSNT offers a better trade-off between inference speed and prediction accuracy compared to existing techniques. When used to replace the popular heatmap matching approach used in almost all state-of-the-art methods for pose estimation, DSNT gives better prediction accuracy for all model architectures tested.



### Let's Dance: Learning From Online Dance Videos
- **Arxiv ID**: http://arxiv.org/abs/1801.07388v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4; I.5; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1801.07388v1)
- **Published**: 2018-01-23 03:48:17+00:00
- **Updated**: 2018-01-23 03:48:17+00:00
- **Authors**: Daniel Castro, Steven Hickson, Patsorn Sangkloy, Bhavishya Mittal, Sean Dai, James Hays, Irfan Essa
- **Comment**: first submitted November 2016
- **Journal**: None
- **Summary**: In recent years, deep neural network approaches have naturally extended to the video domain, in their simplest case by aggregating per-frame classifications as a baseline for action recognition. A majority of the work in this area extends from the imaging domain, leading to visual-feature heavy approaches on temporal data. To address this issue we introduce "Let's Dance", a 1000 video dataset (and growing) comprised of 10 visually overlapping dance categories that require motion for their classification. We stress the important of human motion as a key distinguisher in our work given that, as we show in this work, visual information is not sufficient to classify motion-heavy categories. We compare our datasets' performance using imaging techniques with UCF-101 and demonstrate this inherent difficulty. We present a comparison of numerous state-of-the-art techniques on our dataset using three different representations (video, optical flow and multi-person pose data) in order to analyze these approaches. We discuss the motion parameterization of each of them and their value in learning to categorize online dance videos. Lastly, we release this dataset (and its three representations) for the research community to use.



### Revisiting Video Saliency: A Large-scale Benchmark and a New Model
- **Arxiv ID**: http://arxiv.org/abs/1801.07424v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.07424v3)
- **Published**: 2018-01-23 08:01:50+00:00
- **Updated**: 2018-05-26 05:07:41+00:00
- **Authors**: Wenguan Wang, Jianbing Shen, Fang Guo, Ming-Ming Cheng, Ali Borji
- **Comment**: CVPR2018 paper. Website: https://github.com/wenguanwang/DHF1K We have
  corrected some statistics of our results (baseline training setting (iii)) on
  UCF sports dataset
- **Journal**: None
- **Summary**: In this work, we contribute to video saliency research in two ways. First, we introduce a new benchmark for predicting human eye movements during dynamic scene free-viewing, which is long-time urged in this field. Our dataset, named DHF1K (Dynamic Human Fixation), consists of 1K high-quality, elaborately selected video sequences spanning a large range of scenes, motions, object types and background complexity. Existing video saliency datasets lack variety and generality of common dynamic scenes and fall short in covering challenging situations in unconstrained environments. In contrast, DHF1K makes a significant leap in terms of scalability, diversity and difficulty, and is expected to boost video saliency modeling. Second, we propose a novel video saliency model that augments the CNN-LSTM network architecture with an attention mechanism to enable fast, end-to-end saliency learning. The attention mechanism explicitly encodes static saliency information, thus allowing LSTM to focus on learning more flexible temporal saliency representation across successive frames. Such a design fully leverages existing large-scale static fixation datasets, avoids overfitting, and significantly improves training efficiency and testing performance. We thoroughly examine the performance of our model, with respect to state-of-the-art saliency models, on three large-scale datasets (i.e., DHF1K, Hollywood2, UCF sports). Experimental results over more than 1.2K testing videos containing 400K frames demonstrate that our model outperforms other competitors.



### Automatic construction of Chinese herbal prescription from tongue image via CNNs and auxiliary latent therapy topics
- **Arxiv ID**: http://arxiv.org/abs/1802.02203v4
- **DOI**: 10.1109/TCYB.2019.2909925
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1802.02203v4)
- **Published**: 2018-01-23 08:21:04+00:00
- **Updated**: 2019-05-06 09:37:56+00:00
- **Authors**: Yang Hu, Guihua Wen, Huiqiang Liao, Changjun Wang, Dan Dai, Zhiwen Yu
- **Comment**: 17 pages, 10 figures
- **Journal**: IEEE Transactions on Cybernetics ( Volume: 51, Issue: 2, Feb.
  2021)
- **Summary**: The tongue image provides important physical information of humans. It is of great importance for diagnoses and treatments in clinical medicine. Herbal prescriptions are simple, noninvasive and have low side effects. Thus, they are widely applied in China. Studies on the automatic construction technology of herbal prescriptions based on tongue images have great significance for deep learning to explore the relevance of tongue images for herbal prescriptions, it can be applied to healthcare services in mobile medical systems. In order to adapt to the tongue image in a variety of photographic environments and construct herbal prescriptions, a neural network framework for prescription construction is designed. It includes single/double convolution channels and fully connected layers. Furthermore, it proposes the auxiliary therapy topic loss mechanism to model the therapy of Chinese doctors and alleviate the interference of sparse output labels on the diversity of results. The experiment use the real world tongue images and the corresponding prescriptions and the results can generate prescriptions that are close to the real samples, which verifies the feasibility of the proposed method for the automatic construction of herbal prescriptions from tongue images. Also, it provides a reference for automatic herbal prescription construction from more physical information.



### Novel digital tissue phenotypic signatures of distant metastasis in colorectal cancer
- **Arxiv ID**: http://arxiv.org/abs/1801.07451v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/1801.07451v1)
- **Published**: 2018-01-23 09:30:23+00:00
- **Updated**: 2018-01-23 09:30:23+00:00
- **Authors**: Korsuk Sirinukunwattana, David Snead, David Epstein, Zia Aftab, Imaad Mujeeb, Yee Wah Tsang, Ian Cree, Nasir Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: Distant metastasis is the major cause of death in colorectal cancer (CRC). Patients at high risk of developing distant metastasis could benefit from appropriate adjuvant and follow-up treatments if stratified accurately at an early stage of the disease. Studies have increasingly recognized the role of diverse cellular components within the tumor microenvironment in the development and progression of CRC tumors. In this paper, we show that a new method of automated analysis of digitized images from colorectal cancer tissue slides can provide important estimates of distant metastasis-free survival (DMFS, the time before metastasis is first observed) on the basis of details of the microenvironment. Specifically, we determine what cell types are found in the vicinity of other cell types, and in what numbers, rather than concentrating exclusively on the cancerous cells. We then extract novel tissue phenotypic signatures using statistical measurements about tissue composition. Such signatures can underpin clinical decisions about the advisability of various types of adjuvant therapy.



### Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.07455v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.07455v2)
- **Published**: 2018-01-23 09:48:47+00:00
- **Updated**: 2018-01-25 07:17:02+00:00
- **Authors**: Sijie Yan, Yuanjun Xiong, Dahua Lin
- **Comment**: Accepted by AAAI 2018
- **Journal**: None
- **Summary**: Dynamics of human body skeletons convey significant information for human action recognition. Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we propose a novel model of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks (ST-GCN), which moves beyond the limitations of previous methods by automatically learning both the spatial and temporal patterns from data. This formulation not only leads to greater expressive power but also stronger generalization capability. On two large datasets, Kinetics and NTU-RGBD, it achieves substantial improvements over mainstream methods.



### Stacked Filters Stationary Flow For Hardware-Oriented Acceleration Of Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.07459v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.07459v3)
- **Published**: 2018-01-23 09:57:10+00:00
- **Updated**: 2018-02-06 04:08:45+00:00
- **Authors**: Yuechao Gao, Nianhong Liu, Sheng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: To address memory and computation resource limitations for hardware-oriented acceleration of deep convolutional neural networks (CNNs), we present a computation flow, stacked filters stationary flow (SFS), and a corresponding data encoding format, relative indexed compressed sparse filter format (CSF), to make the best of data sparsity, and simplify data handling at execution time. And we also propose a three dimensional Single Instruction Multiple Data (3D-SIMD) processor architecture to illustrate how to accelerate deep CNNs by taking advantage of SFS flow and CSF format. Comparing with the state-of-the-art result (Han et al., 2016b), our methods achieve 1.11x improvement in reducing the storage required by AlexNet, and 1.09x improvement in reducing the storage required by SqueezeNet, without loss of accuracy on the ImageNet dataset. Moreover, using these approaches, chip area for logics handling irregular sparse data access can be saved. Comparing with the 2D-SIMD processor structures in DVAS, ENVISION, etc., our methods achieve about 3.65x processing element (PE) array utilization rate improvement (from 26.4\% to 96.5\%) on the data from Deep Compression on AlexNet.



### Survey on Emotional Body Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.07481v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.07481v1)
- **Published**: 2018-01-23 11:03:37+00:00
- **Updated**: 2018-01-23 11:03:37+00:00
- **Authors**: Fatemeh Noroozi, Ciprian Adrian Corneanu, Dorota Kamińska, Tomasz Sapiński, Sergio Escalera, Gholamreza Anbarjafari
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic emotion recognition has become a trending research topic in the past decade. While works based on facial expressions or speech abound, recognizing affect from body gestures remains a less explored topic. We present a new comprehensive survey hoping to boost research in the field. We first introduce emotional body gestures as a component of what is commonly known as "body language" and comment general aspects as gender differences and culture dependence. We then define a complete framework for automatic emotional body gesture recognition. We introduce person detection and comment static and dynamic body pose estimation methods both in RGB and 3D. We then comment the recent literature related to representation learning and emotion recognition from images of emotionally expressive gestures. We also discuss multi-modal approaches that combine speech or face with body gestures for improved emotion recognition. While pre-processing methodologies (e.g. human detection and pose estimation) are nowadays mature technologies fully developed for robust large scale analysis, we show that for emotion recognition the quantity of labelled data is scarce, there is no agreement on clearly defined output spaces and the representations are shallow and largely based on naive geometrical representations.



### Statistically Motivated Second Order Pooling
- **Arxiv ID**: http://arxiv.org/abs/1801.07492v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.07492v3)
- **Published**: 2018-01-23 11:39:19+00:00
- **Updated**: 2018-07-16 11:11:25+00:00
- **Authors**: Kaicheng Yu, Mathieu Salzmann
- **Comment**: Accepted to ECCV 2018. Camera ready version. 14 page, 5 figures, 3
  tables
- **Journal**: None
- **Summary**: Second-order pooling, a.k.a.~bilinear pooling, has proven effective for deep learning based visual recognition. However, the resulting second-order networks yield a final representation that is orders of magnitude larger than that of standard, first-order ones, making them memory-intensive and cumbersome to deploy. Here, we introduce a general, parametric compression strategy that can produce more compact representations than existing compression techniques, yet outperform both compressed and uncompressed second-order models. Our approach is motivated by a statistical analysis of the network's activations, relying on operations that lead to a Gaussian-distributed final representation, as inherently used by first-order deep networks. As evidenced by our experiments, this lets us outperform the state-of-the-art first-order and second-order models on several benchmark recognition datasets.



### High Resolution Face Completion with Multiple Controllable Attributes via Fully End-to-End Progressive Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.07632v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1801.07632v1)
- **Published**: 2018-01-23 16:12:26+00:00
- **Updated**: 2018-01-23 16:12:26+00:00
- **Authors**: Zeyuan Chen, Shaoliang Nie, Tianfu Wu, Christopher G. Healey
- **Comment**: None
- **Journal**: None
- **Summary**: We present a deep learning approach for high resolution face completion with multiple controllable attributes (e.g., male and smiling) under arbitrary masks. Face completion entails understanding both structural meaningfulness and appearance consistency locally and globally to fill in "holes" whose content do not appear elsewhere in an input image. It is a challenging task with the difficulty level increasing significantly with respect to high resolution, the complexity of "holes" and the controllable attributes of filled-in fragments. Our system addresses the challenges by learning a fully end-to-end framework that trains generative adversarial networks (GANs) progressively from low resolution to high resolution with conditional vectors encoding controllable attributes.   We design novel network architectures to exploit information across multiple scales effectively and efficiently. We introduce new loss functions encouraging sharp completion. We show that our system can complete faces with large structural and appearance variations using a single feed-forward pass of computation with mean inference time of 0.007 seconds for images at 1024 x 1024 resolution. We also perform a pilot human study that shows our approach outperforms state-of-the-art face completion methods in terms of rank analysis. The code will be released upon publication.



### Human Activity Recognition for Mobile Robot
- **Arxiv ID**: http://arxiv.org/abs/1801.07633v1
- **DOI**: 10.1088/1742-6596/1069/1/012148
- **Categories**: **cs.HC**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1801.07633v1)
- **Published**: 2018-01-23 16:14:43+00:00
- **Updated**: 2018-01-23 16:14:43+00:00
- **Authors**: Iyiola E. Olatunji
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the increasing number of mobile robots including domestic robots for cleaning and maintenance in developed countries, human activity recognition is inevitable for congruent human-robot interaction. Needless to say that this is indeed a challenging task for robots, it is expedient to learn human activities for autonomous mobile robots (AMR) for navigating in an uncontrolled environment without any guidance. Building a correct classifier for complex human action is non-trivial since simple actions can be combined to recognize a complex human activity. In this paper, we trained a model for human activity recognition using convolutional neural network. We trained and validated the model using the Vicon physical action dataset and also tested the model on our generated dataset (VMCUHK). Our experiment shows that our method performs with high accuracy, human activity recognition task both on the Vicon physical action dataset and VMCUHK dataset.



### DeepGestalt - Identifying Rare Genetic Syndromes Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1801.07637v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.07637v1)
- **Published**: 2018-01-23 16:18:24+00:00
- **Updated**: 2018-01-23 16:18:24+00:00
- **Authors**: Yaron Gurovich, Yair Hanani, Omri Bar, Nicole Fleischer, Dekel Gelbman, Lina Basel-Salmon, Peter Krawitz, Susanne B Kamphausen, Martin Zenker, Lynne M. Bird, Karen W. Gripp
- **Comment**: None
- **Journal**: None
- **Summary**: Facial analysis technologies have recently measured up to the capabilities of expert clinicians in syndrome identification. To date, these technologies could only identify phenotypes of a few diseases, limiting their role in clinical settings where hundreds of diagnoses must be considered.   We developed a facial analysis framework, DeepGestalt, using computer vision and deep learning algorithms, that quantifies similarities to hundreds of genetic syndromes based on unconstrained 2D images. DeepGestalt is currently trained with over 26,000 patient cases from a rapidly growing phenotype-genotype database, consisting of tens of thousands of validated clinical cases, curated through a community-driven platform. DeepGestalt currently achieves 91% top-10-accuracy in identifying over 215 different genetic syndromes and has outperformed clinical experts in three separate experiments.   We suggest that this form of artificial intelligence is ready to support medical genetics in clinical and laboratory practices and will play a key role in the future of precision medicine.



### Clustering with Deep Learning: Taxonomy and New Methods
- **Arxiv ID**: http://arxiv.org/abs/1801.07648v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML, 62H30, 62M45, 91C20, H.3.3; I.2.6; I.5; I.5.3; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1801.07648v2)
- **Published**: 2018-01-23 16:41:03+00:00
- **Updated**: 2018-09-13 19:41:22+00:00
- **Authors**: Elie Aljalbout, Vladimir Golkov, Yawar Siddiqui, Maximilian Strobel, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: Clustering methods based on deep neural networks have proven promising for clustering real-world data because of their high representational power. In this paper, we propose a systematic taxonomy of clustering methods that utilize deep neural networks. We base our taxonomy on a comprehensive review of recent work and validate the taxonomy in a case study. In this case study, we show that the taxonomy enables researchers and practitioners to systematically create new clustering methods by selectively recombining and replacing distinct aspects of previous methods with the goal of overcoming their individual limitations. The experimental evaluation confirms this and shows that the method created for the case study achieves state-of-the-art clustering quality and surpasses it in some cases.



### A Classification Refinement Strategy for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1801.07674v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.07674v1)
- **Published**: 2018-01-23 17:45:54+00:00
- **Updated**: 2018-01-23 17:45:54+00:00
- **Authors**: James W. Davis, Christopher Menart, Muhammad Akbar, Roman Ilin
- **Comment**: None
- **Journal**: None
- **Summary**: Based on the observation that semantic segmentation errors are partially predictable, we propose a compact formulation using confusion statistics of the trained classifier to refine (re-estimate) the initial pixel label hypotheses. The proposed strategy is contingent upon computing the classifier confusion probabilities for a given dataset and estimating a relevant prior on the object classes present in the image to be classified. We provide a procedure to robustly estimate the confusion probabilities and explore multiple prior definitions. Experiments are shown comparing performances on multiple challenging datasets using different priors to improve a state-of-the-art semantic segmentation classifier. This study demonstrates the potential to significantly improve semantic labeling and motivates future work for reliable label prior estimation from images.



### ArcFace: Additive Angular Margin Loss for Deep Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.07698v4
- **DOI**: 10.1109/TPAMI.2021.3087709
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.07698v4)
- **Published**: 2018-01-23 18:39:19+00:00
- **Updated**: 2022-09-04 16:26:45+00:00
- **Authors**: Jiankang Deng, Jia Guo, Jing Yang, Niannan Xue, Irene Kotsia, Stefanos Zafeiriou
- **Comment**: ArcFace TPAMI version
- **Journal**: None
- **Summary**: Recently, a popular line of research in face recognition is adopting margins in the well-established softmax loss function to maximize class separability. In this paper, we first introduce an Additive Angular Margin Loss (ArcFace), which not only has a clear geometric interpretation but also significantly enhances the discriminative power. Since ArcFace is susceptible to the massive label noise, we further propose sub-center ArcFace, in which each class contains $K$ sub-centers and training samples only need to be close to any of the $K$ positive sub-centers. Sub-center ArcFace encourages one dominant sub-class that contains the majority of clean faces and non-dominant sub-classes that include hard or noisy faces. Based on this self-propelled isolation, we boost the performance through automatically purifying raw web faces under massive real-world noise. Besides discriminative feature embedding, we also explore the inverse problem, mapping feature vectors to face images. Without training any additional generator or discriminator, the pre-trained ArcFace model can generate identity-preserved face images for both subjects inside and outside the training data only by using the network gradient and Batch Normalization (BN) priors. Extensive experiments demonstrate that ArcFace can enhance the discriminative feature embedding as well as strengthen the generative face synthesis.



### The Shape of Art History in the Eyes of the Machine
- **Arxiv ID**: http://arxiv.org/abs/1801.07729v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1801.07729v2)
- **Published**: 2018-01-23 19:05:21+00:00
- **Updated**: 2018-02-12 10:00:28+00:00
- **Authors**: Ahmed Elgammal, Marian Mazzone, Bingchen Liu, Diana Kim, Mohamed Elhoseiny
- **Comment**: None
- **Journal**: None
- **Summary**: How does the machine classify styles in art? And how does it relate to art historians' methods for analyzing style? Several studies have shown the ability of the machine to learn and predict style categories, such as Renaissance, Baroque, Impressionism, etc., from images of paintings. This implies that the machine can learn an internal representation encoding discriminative features through its visual analysis. However, such a representation is not necessarily interpretable. We conducted a comprehensive study of several of the state-of-the-art convolutional neural networks applied to the task of style classification on 77K images of paintings, and analyzed the learned representation through correlation analysis with concepts derived from art history. Surprisingly, the networks could place the works of art in a smooth temporal arrangement mainly based on learning style labels, without any a priori knowledge of time of creation, the historical time and context of styles, or relations between styles. The learned representations showed that there are few underlying factors that explain the visual variations of style in art. Some of these factors were found to correlate with style patterns suggested by Heinrich W\"olfflin (1846-1945). The learned representations also consistently highlighted certain artists as the extreme distinctive representative of their styles, which quantitatively confirms art historian observations.



### Estimation of Variance and Spatial Correlation Width for Fine-scale Measurement Error in Digital Elevation Model
- **Arxiv ID**: http://arxiv.org/abs/1801.07740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.07740v1)
- **Published**: 2018-01-23 19:29:53+00:00
- **Updated**: 2018-01-23 19:29:53+00:00
- **Authors**: Mykhail Uss, Benoit Vozel, Vladimir Lukin, Kacem Chehdi
- **Comment**: 15 pages, 7 figures, 3 tables
- **Journal**: None
- **Summary**: In this paper, we borrow from blind noise parameter estimation (BNPE) methodology early developed in the image processing field an original and innovative no-reference approach to estimate Digital Elevation Model (DEM) vertical error parameters without resorting to a reference DEM. The challenges associated with the proposed approach related to the physical nature of the error and its multifactor structure in DEM are discussed in detail. A suitable multivariate method is then developed for estimating the error in gridded DEM. It is built on a recently proposed vectorial BNPE method for estimating spatially correlated noise using Noise Informative areas and Fractal Brownian Motion. The newly multivariate method is derived to estimate the effect of the stacking procedure and that of the epipolar line error on local (fine-scale) standard deviation and autocorrelation function width of photogrammetric DEM measurement error. Applying the new estimator to ASTER GDEM2 and ALOS World 3D DEMs, good agreement of derived estimates with results available in the literature is evidenced. In future works, the proposed no-reference method for analyzing DEM error can be extended to a larger number of predictors for accounting for other factors influencing remote sensing (RS) DEM accuracy.



### The WiLI benchmark dataset for written language identification
- **Arxiv ID**: http://arxiv.org/abs/1801.07779v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1801.07779v1)
- **Published**: 2018-01-23 21:40:53+00:00
- **Updated**: 2018-01-23 21:40:53+00:00
- **Authors**: Martin Thoma
- **Comment**: {"pages": 12, "figures": 4, "language": "English", "author-ORCiD":
  ["https://orcid.org/0000-0002-6517-1690"]}
- **Journal**: None
- **Summary**: This paper describes the WiLI-2018 benchmark dataset for monolingual written natural language identification. WiLI-2018 is a publicly available, free of charge dataset of short text extracts from Wikipedia. It contains 1000 paragraphs of 235 languages, totaling in 23500 paragraphs. WiLI is a classification dataset: Given an unknown paragraph written in one dominant language, it has to be decided which language it is.



### PointCNN: Convolution On $\mathcal{X}$-Transformed Points
- **Arxiv ID**: http://arxiv.org/abs/1801.07791v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, I.2.10; I.4.7; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1801.07791v5)
- **Published**: 2018-01-23 22:07:21+00:00
- **Updated**: 2018-11-05 09:31:45+00:00
- **Authors**: Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, Baoquan Chen
- **Comment**: To be published in NIPS 2018, code available at
  https://github.com/yangyanli/PointCNN
- **Journal**: None
- **Summary**: We present a simple and general framework for feature learning from point clouds. The key to the success of CNNs is the convolution operator that is capable of leveraging spatially-local correlation in data represented densely in grids (e.g. images). However, point clouds are irregular and unordered, thus directly convolving kernels against features associated with the points, will result in desertion of shape information and variance to point ordering. To address these problems, we propose to learn an $\mathcal{X}$-transformation from the input points, to simultaneously promote two causes. The first is the weighting of the input features associated with the points, and the second is the permutation of the points into a latent and potentially canonical order. Element-wise product and sum operations of the typical convolution operator are subsequently applied on the $\mathcal{X}$-transformed features. The proposed method is a generalization of typical CNNs to feature learning from point clouds, thus we call it PointCNN. Experiments show that PointCNN achieves on par or better performance than state-of-the-art methods on multiple challenging benchmark datasets and tasks.



