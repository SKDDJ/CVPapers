# Arxiv Papers in cs.CV on 2018-01-10
### FWLBP: A Scale Invariant Descriptor for Texture Classification
- **Arxiv ID**: http://arxiv.org/abs/1801.03228v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03228v2)
- **Published**: 2018-01-10 03:32:31+00:00
- **Updated**: 2018-08-02 10:36:24+00:00
- **Authors**: Swalpa Kumar Roy, Nilavra Bhattacharya, Bhabatosh Chanda, Bidyut B. Chaudhuri, Dipak Kumar Ghosh
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a novel texture descriptor called Fractal Weighted Local Binary Pattern (FWLBP). The fractal dimension (FD) measure is relatively invariant to scale-changes, and presents a good correlation with human viewpoint of surface roughness. We have utilized this property to construct a scale-invariant descriptor. Here, the input image is sampled using an augmented form of the local binary pattern (LBP) over three different radii, and then used an indexing operation to assign FD weights to the collected samples. The final histogram of the descriptor has its features calculated using LBP, and its weights computed from the FD image. The proposed descriptor is scale invariant, and is also robust in rotation or reflection, and partially tolerant to noise and illumination changes. In addition, the local fractal dimension is relatively insensitive to the bi-Lipschitz transformations, whereas its extension is adequate to precisely discriminate the fundamental of texture primitives. Experiment results carried out on standard texture databases show that the proposed descriptor achieved better classification rates compared to the state-of-the-art descriptors.



### Lung and Pancreatic Tumor Characterization in the Deep Learning Era: Novel Supervised and Unsupervised Learning Approaches
- **Arxiv ID**: http://arxiv.org/abs/1801.03230v3
- **DOI**: 10.1109/TMI.2019.2894349
- **Categories**: **cs.CV**, cs.AI, cs.LG, q-bio.QM, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/1801.03230v3)
- **Published**: 2018-01-10 03:47:07+00:00
- **Updated**: 2019-01-18 13:25:51+00:00
- **Authors**: Sarfaraz Hussein, Pujan Kandel, Candice W. Bolan, Michael B. Wallace, Ulas Bagci
- **Comment**: Accepted for publication in IEEE Transactions on Medical Imaging 2019
- **Journal**: None
- **Summary**: Risk stratification (characterization) of tumors from radiology images can be more accurate and faster with computer-aided diagnosis (CAD) tools. Tumor characterization through such tools can also enable non-invasive cancer staging, prognosis, and foster personalized treatment planning as a part of precision medicine. In this study, we propose both supervised and unsupervised machine learning strategies to improve tumor characterization. Our first approach is based on supervised learning for which we demonstrate significant gains with deep learning algorithms, particularly by utilizing a 3D Convolutional Neural Network and Transfer Learning. Motivated by the radiologists' interpretations of the scans, we then show how to incorporate task dependent feature representations into a CAD system via a graph-regularized sparse Multi-Task Learning (MTL) framework. In the second approach, we explore an unsupervised learning algorithm to address the limited availability of labeled training data, a common problem in medical imaging applications. Inspired by learning from label proportion (LLP) approaches in computer vision, we propose to use proportion-SVM for characterizing tumors. We also seek the answer to the fundamental question about the goodness of "deep features" for unsupervised tumor classification. We evaluate our proposed supervised and unsupervised learning algorithms on two different tumor diagnosis challenges: lung and pancreas with 1018 CT and 171 MRI scans, respectively, and obtain the state-of-the-art sensitivity and specificity results in both problems.



### Instance Map based Image Synthesis with a Denoising Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1801.03252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03252v1)
- **Published**: 2018-01-10 07:16:46+00:00
- **Updated**: 2018-01-10 07:16:46+00:00
- **Authors**: Ziqiang Zheng, Chao Wang, Zhibin Yu, Haiyong Zheng, Bing Zheng
- **Comment**: 10 pages, 16figures
- **Journal**: None
- **Summary**: Semantic layouts based Image synthesizing, which has benefited from the success of Generative Adversarial Network (GAN), has drawn much attention in these days. How to enhance the synthesis image equality while keeping the stochasticity of the GAN is still a challenge. We propose a novel denoising framework to handle this problem. The overlapped objects generation is another challenging task when synthesizing images from a semantic layout to a realistic RGB photo. To overcome this deficiency, we include a one-hot semantic label map to force the generator paying more attention on the overlapped objects generation. Furthermore, we improve the loss function of the discriminator by considering perturb loss and cascade layer loss to guide the generation process. We applied our methods on the Cityscapes, Facades and NYU datasets and demonstrate the image generation ability of our model.



### Simultaneous Tensor Completion and Denoising by Noise Inequality Constrained Convex Optimization
- **Arxiv ID**: http://arxiv.org/abs/1801.03299v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03299v1)
- **Published**: 2018-01-10 10:45:26+00:00
- **Updated**: 2018-01-10 10:45:26+00:00
- **Authors**: Tatsuya Yokota, Hidekata Hontani
- **Comment**: None
- **Journal**: None
- **Summary**: Tensor completion is a technique of filling missing elements of the incomplete data tensors. It being actively studied based on the convex optimization scheme such as nuclear-norm minimization. When given data tensors include some noises, the nuclear-norm minimization problem is usually converted to the nuclear-norm `regularization' problem which simultaneously minimize penalty and error terms with some trade-off parameter. However, the good value of trade-off is not easily determined because of the difference of two units and the data dependence. In the sense of trade-off tuning, the noisy tensor completion problem with the `noise inequality constraint' is better choice than the `regularization' because the good noise threshold can be easily bounded with noise standard deviation. In this study, we tackle to solve the convex tensor completion problems with two types of noise inequality constraints: Gaussian and Laplace distributions. The contributions of this study are follows: (1) New tensor completion and denoising models using tensor total variation and nuclear-norm are proposed which can be characterized as a generalization/extension of many past matrix and tensor completion models, (2) proximal mappings for noise inequalities are derived which are analytically computable with low computational complexity, (3) convex optimization algorithm is proposed based on primal-dual splitting framework, (4) new step-size adaptation method is proposed to accelerate the optimization, and (5) extensive experiments demonstrated the advantages of the proposed method for visual data retrieval such as for color images, movies, and 3D-volumetric data.



### Unsupervised Despeckling
- **Arxiv ID**: http://arxiv.org/abs/1801.03318v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03318v1)
- **Published**: 2018-01-10 11:44:48+00:00
- **Updated**: 2018-01-10 11:44:48+00:00
- **Authors**: Deepak Mishra, Santanu Chaudhury, Mukul Sarkar, Arvinder Singh Soin
- **Comment**: None
- **Journal**: None
- **Summary**: Contrast and quality of ultrasound images are adversely affected by the excessive presence of speckle. However, being an inherent imaging property, speckle helps in tissue characterization and tracking. Thus, despeckling of the ultrasound images requires the reduction of speckle extent without any oversmoothing. In this letter, we aim to address the despeckling problem using an unsupervised deep adversarial approach. A despeckling residual neural network (DRNN) is trained with an adversarial loss imposed by a discriminator. The discriminator tries to differentiate between the despeckled images generated by the DRNN and the set of high-quality images. Further to prevent the developed DRNN from oversmoothing, a structural loss term is used along with the adversarial loss. Experimental evaluations show that the proposed DRNN is able to outperform the state-of-the-art despeckling approaches.



### Worst-case Optimal Submodular Extensions for Marginal Estimation
- **Arxiv ID**: http://arxiv.org/abs/1801.06490v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.06490v1)
- **Published**: 2018-01-10 14:36:57+00:00
- **Updated**: 2018-01-10 14:36:57+00:00
- **Authors**: Pankaj Pansari, Chris Russell, M. Pawan Kumar
- **Comment**: Accepted to AISTATS 2018
- **Journal**: None
- **Summary**: Submodular extensions of an energy function can be used to efficiently compute approximate marginals via variational inference. The accuracy of the marginals depends crucially on the quality of the submodular extension. To identify the best possible extension, we show an equivalence between the submodular extensions of the energy and the objective functions of linear programming (LP) relaxations for the corresponding MAP estimation problem. This allows us to (i) establish the worst-case optimality of the submodular extension for Potts model used in the literature; (ii) identify the worst-case optimal submodular extension for the more general class of metric labeling; and (iii) efficiently compute the marginals for the widely used dense CRF model with the help of a recently proposed Gaussian filtering method. Using synthetic and real data, we show that our approach provides comparable upper bounds on the log-partition function to those obtained using tree-reweighted message passing (TRW) in cases where the latter is computationally feasible. Importantly, unlike TRW, our approach provides the first practical algorithm to compute an upper bound on the dense CRF model.



### Inferring a Third Spatial Dimension from 2D Histological Images
- **Arxiv ID**: http://arxiv.org/abs/1801.03431v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03431v1)
- **Published**: 2018-01-10 15:59:12+00:00
- **Updated**: 2018-01-10 15:59:12+00:00
- **Authors**: Maxime W. Lafarge, Josien P. W. Pluim, Koen A. J. Eppenhof, Pim Moeskops, Mitko Veta
- **Comment**: IEEE International Symposium on Biomedical Imaging (ISBI), 2018
- **Journal**: None
- **Summary**: Histological images are obtained by transmitting light through a tissue specimen that has been stained in order to produce contrast. This process results in 2D images of the specimen that has a three-dimensional structure. In this paper, we propose a method to infer how the stains are distributed in the direction perpendicular to the surface of the slide for a given 2D image in order to obtain a 3D representation of the tissue. This inference is achieved by decomposition of the staining concentration maps under constraints that ensure realistic decomposition and reconstruction of the original 2D images. Our study shows that it is possible to generate realistic 3D images making this method a potential tool for data augmentation when training deep learning models.



### Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.03454v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.03454v2)
- **Published**: 2018-01-10 17:01:36+00:00
- **Updated**: 2018-03-29 11:00:02+00:00
- **Authors**: Ruth Fong, Andrea Vedaldi
- **Comment**: Camera-Ready for CVPR18; supplementary materials:
  http://ruthcfong.github.io/files/net2vec_supps.pdf
- **Journal**: None
- **Summary**: In an effort to understand the meaning of the intermediate representations captured by deep networks, recent papers have tried to associate specific semantic concepts to individual neural network filter responses, where interesting correlations are often found, largely by focusing on extremal filter responses. In this paper, we show that this approach can favor easy-to-interpret cases that are not necessarily representative of the average behavior of a representation.   A more realistic but harder-to-study hypothesis is that semantic representations are distributed, and thus filters must be studied in conjunction. In order to investigate this idea while enabling systematic visualization and quantification of multiple filter responses, we introduce the Net2Vec framework, in which semantic concepts are mapped to vectorial embeddings based on corresponding filter responses. By studying such embeddings, we are able to show that 1., in most cases, multiple filters are required to code for a concept, that 2., often filters are not concept specific and help encode multiple concepts, and that 3., compared to single filter activations, filter embeddings are able to better characterize the meaning of a representation and its relationship to other concepts.



### Real-to-Virtual Domain Unification for End-to-End Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1801.03458v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03458v2)
- **Published**: 2018-01-10 17:08:54+00:00
- **Updated**: 2018-09-06 05:11:13+00:00
- **Authors**: Luona Yang, Xiaodan Liang, Tairui Wang, Eric Xing
- **Comment**: None
- **Journal**: None
- **Summary**: In the spectrum of vision-based autonomous driving, vanilla end-to-end models are not interpretable and suboptimal in performance, while mediated perception models require additional intermediate representations such as segmentation masks or detection bounding boxes, whose annotation can be prohibitively expensive as we move to a larger scale. More critically, all prior works fail to deal with the notorious domain shift if we were to merge data collected from different sources, which greatly hinders the model generalization ability. In this work, we address the above limitations by taking advantage of virtual data collected from driving simulators, and present DU-drive, an unsupervised real-to-virtual domain unification framework for end-to-end autonomous driving. It first transforms real driving data to its less complex counterpart in the virtual domain and then predicts vehicle control commands from the generated virtual image. Our framework has three unique advantages: 1) it maps driving data collected from a variety of source distributions into a unified domain, effectively eliminating domain shift; 2) the learned virtual representation is simpler than the input real image and closer in form to the "minimum sufficient statistic" for the prediction task, which relieves the burden of the compression phase while optimizing the information bottleneck tradeoff and leads to superior prediction performance; 3) it takes advantage of annotated virtual data which is unlimited and free to obtain. Extensive experiments on two public driving datasets and two driving simulators demonstrate the performance superiority and interpretive capability of DU-drive.



### Focus: Querying Large Video Datasets with Low Latency and Low Cost
- **Arxiv ID**: http://arxiv.org/abs/1801.03493v1
- **DOI**: None
- **Categories**: **cs.DB**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1801.03493v1)
- **Published**: 2018-01-10 18:52:25+00:00
- **Updated**: 2018-01-10 18:52:25+00:00
- **Authors**: Kevin Hsieh, Ganesh Ananthanarayanan, Peter Bodik, Paramvir Bahl, Matthai Philipose, Phillip B. Gibbons, Onur Mutlu
- **Comment**: None
- **Journal**: None
- **Summary**: Large volumes of videos are continuously recorded from cameras deployed for traffic control and surveillance with the goal of answering "after the fact" queries: identify video frames with objects of certain classes (cars, bags) from many days of recorded video. While advancements in convolutional neural networks (CNNs) have enabled answering such queries with high accuracy, they are too expensive and slow. We build Focus, a system for low-latency and low-cost querying on large video datasets. Focus uses cheap ingestion techniques to index the videos by the objects occurring in them. At ingest-time, it uses compression and video-specific specialization of CNNs. Focus handles the lower accuracy of the cheap CNNs by judiciously leveraging expensive CNNs at query-time. To reduce query time latency, we cluster similar objects and hence avoid redundant processing. Using experiments on video streams from traffic, surveillance and news channels, we see that Focus uses 58X fewer GPU cycles than running expensive ingest processors and is 37X faster than processing all the video at query time.



### Segment-based Methods for Facial Attribute Detection from Partial Faces
- **Arxiv ID**: http://arxiv.org/abs/1801.03546v1
- **DOI**: 10.1109/TAFFC.2018.2820048
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03546v1)
- **Published**: 2018-01-10 20:32:35+00:00
- **Updated**: 2018-01-10 20:32:35+00:00
- **Authors**: Upal Mahbub, Sayantan Sarkar, Rama Chellappa
- **Comment**: None
- **Journal**: IEEE Transactions on Affective Computing, 2018
- **Summary**: State-of-the-art methods of attribute detection from faces almost always assume the presence of a full, unoccluded face. Hence, their performance degrades for partially visible and occluded faces. In this paper, we introduce SPLITFACE, a deep convolutional neural network-based method that is explicitly designed to perform attribute detection in partially occluded faces. Taking several facial segments and the full face as input, the proposed method takes a data driven approach to determine which attributes are localized in which facial segments. The unique architecture of the network allows each attribute to be predicted by multiple segments, which permits the implementation of committee machine techniques for combining local and global decisions to boost performance. With access to segment-based predictions, SPLITFACE can predict well those attributes which are localized in the visible parts of the face, without having to rely on the presence of the whole face. We use the CelebA and LFWA facial attribute datasets for standard evaluations. We also modify both datasets, to occlude the faces, so that we can evaluate the performance of attribute detection algorithms on partial faces. Our evaluation shows that SPLITFACE significantly outperforms other recent methods especially for partial faces.



### From Superpixel to Human Shape Modelling for Carried Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1801.03551v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.03551v1)
- **Published**: 2018-01-10 21:07:13+00:00
- **Updated**: 2018-01-10 21:07:13+00:00
- **Authors**: Farnoosh Ghadiri, Robert Bergevin, Guillaume-Alexandre Bilodeau
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting carried objects is one of the requirements for developing systems to reason about activities involving people and objects. We present an approach to detect carried objects from a single video frame with a novel method that incorporates features from multiple scales. Initially, a foreground mask in a video frame is segmented into multi-scale superpixels. Then the human-like regions in the segmented area are identified by matching a set of extracted features from superpixels against learned features in a codebook. A carried object probability map is generated using the complement of the matching probabilities of superpixels to human-like regions and background information. A group of superpixels with high carried object probability and strong edge support is then merged to obtain the shape of the carried object. We applied our method to two challenging datasets, and results show that our method is competitive with or better than the state-of-the-art.



