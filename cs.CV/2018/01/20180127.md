# Arxiv Papers in cs.CV on 2018-01-27
### Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions
- **Arxiv ID**: http://arxiv.org/abs/1801.09041v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09041v1)
- **Published**: 2018-01-27 05:34:37+00:00
- **Updated**: 2018-01-27 05:34:37+00:00
- **Authors**: Qing Li, Jianlong Fu, Dongfei Yu, Tao Mei, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) has attracted attention from both computer vision and natural language processing communities. Most existing approaches adopt the pipeline of representing an image via pre-trained CNNs, and then using the uninterpretable CNN features in conjunction with the question to predict the answer. Although such end-to-end models might report promising performance, they rarely provide any insight, apart from the answer, into the VQA process. In this work, we propose to break up the end-to-end VQA into two steps: explaining and reasoning, in an attempt towards a more explainable VQA by shedding light on the intermediate results between these two steps. To that end, we first extract attributes and generate descriptions as explanations for an image using pre-trained attribute detectors and image captioning models, respectively. Next, a reasoning module utilizes these explanations in place of the image to infer an answer to the question. The advantages of such a breakdown include: (1) the attributes and captions can reflect what the system extracts from the image, thus can provide some explanations for the predicted answer; (2) these intermediate results can help us identify the inabilities of both the image understanding part and the answer inference part when the predicted answer is wrong. We conduct extensive experiments on a popular VQA dataset and dissect all results according to several measurements of the explanation quality. Our system achieves comparable performance with the state-of-the-art, yet with added benefits of explainability and the inherent ability to further improve with higher quality explanations.



### Image2GIF: Generating Cinemagraphs using Recurrent Deep Q-Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.09042v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09042v1)
- **Published**: 2018-01-27 05:48:20+00:00
- **Updated**: 2018-01-27 05:48:20+00:00
- **Authors**: Yipin Zhou, Yale Song, Tamara L. Berg
- **Comment**: WACV2018
- **Journal**: None
- **Summary**: Given a still photograph, one can imagine how dynamic objects might move against a static background. This idea has been actualized in the form of cinemagraphs, where the motion of particular objects within a still image is repeated, giving the viewer a sense of animation. In this paper, we learn computational models that can generate cinemagraph sequences automatically given a single image. To generate cinemagraphs, we explore combining generative models with a recurrent neural network and deep Q-networks to enhance the power of sequence generation. To enable and evaluate these models we make use of two datasets, one synthetically generated and the other containing real video generated cinemagraphs. Both qualitative and quantitative evaluations demonstrate the effectiveness of our models on the synthetic and real datasets.



### Ear Recognition With Score-Level Fusion Based On CMC In Long-Wave Infrared Spectrum
- **Arxiv ID**: http://arxiv.org/abs/1801.09054v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09054v1)
- **Published**: 2018-01-27 08:41:54+00:00
- **Updated**: 2018-01-27 08:41:54+00:00
- **Authors**: Umit Kacar, Murvet Kirci
- **Comment**: None
- **Journal**: None
- **Summary**: Only a few studies have been reported regarding human ear recognition in long wave infrared band. Thus, we have created ear database based on long wave infrared band. We have called that the database is long wave infrared band MIDAS consisting of 2430 records of 81 subjects. Thermal band provides seamless operation both night and day, robust against spoofing with understanding live ear and invariant to illumination conditions for human ear recognition. We have proposed to use different algorithms to reveal the distinctive features. Then, we have reduced the number of dimensions using subspace methods. Finally, the dimension of data is reduced in accordance with the classifier methods. After this, the decision is determined by the best sores or combining some of the best scores with matching fusion. The results have showed that the fusion technique was successful. We have reached 97.71% for rank-1 with 567 test probes. Furthermore, we have defined the perfect rank which is rank number when recognition rate reaches 100% in cumulative matching curve. This evaluation is important for especially forensics, for example corpse identification, criminal investigation etc.



### A Multi-Biometrics for Twins Identification Based Speech and Ear
- **Arxiv ID**: http://arxiv.org/abs/1801.09056v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09056v1)
- **Published**: 2018-01-27 08:47:07+00:00
- **Updated**: 2018-01-27 08:47:07+00:00
- **Authors**: Cihan Akin, Umit Kacar, Murvet Kirci
- **Comment**: None
- **Journal**: None
- **Summary**: The development of technology biometrics becomes crucial more. To define human characteristic biometric systems are used but because of inability of traditional biometric systems to recognize twins, multimodal biometric systems are developed. In this study a multimodal biometric recognition system is proposed to recognize twins from each other and from the other people by using image and speech data. The speech or image data can be enough to recognize people from each other but twins cannot be distinguished with one of these data. Therefore a robust recognition system with the combine of speech and ear images is needed. As database, the photos and speech data of 39 twins are used. For speech recognition MFCC and DTW algorithms are used. Also, Gabor filter and DCVA algorithms are used for ear identification. Multi-biometrics success rate is increased by making matching score level fusion. Especially, rank-5 is reached 100%. We think that speech and ear can be complementary. Therefore, it is result that multi-biometrics based speech and ear is effective for human identifications.



### Aligned to the Object, not to the Image: A Unified Pose-aligned Representation for Fine-grained Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.09057v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09057v4)
- **Published**: 2018-01-27 08:53:29+00:00
- **Updated**: 2018-09-11 03:06:32+00:00
- **Authors**: Pei Guo, Ryan Farrell
- **Comment**: Accepted to WACV 2019
- **Journal**: None
- **Summary**: Dramatic appearance variation due to pose constitutes a great challenge in fine-grained recognition, one which recent methods using attention mechanisms or second-order statistics fail to adequately address. Modern CNNs typically lack an explicit understanding of object pose and are instead confused by entangled pose and appearance. In this paper, we propose a unified object representation built from a hierarchy of pose-aligned regions. Rather than representing an object by regions aligned to image axes, the proposed representation characterizes appearance relative to the object's pose using pose-aligned patches whose features are robust to variations in pose, scale and rotation. We propose an algorithm that performs pose estimation and forms the unified object representation as the concatenation of hierarchical pose-aligned regions features, which is then fed into a classification network. The proposed algorithm surpasses the performance of other approaches, increasing the state-of-the-art by nearly 2% on the widely-used CUB-200 dataset and by more than 8% on the much larger NABirds dataset. The effectiveness of this paradigm relative to competing methods suggests the critical importance of disentangling pose and appearance for continued progress in fine-grained recognition.



### Interactive Deep Colorization With Simultaneous Global and Local Inputs
- **Arxiv ID**: http://arxiv.org/abs/1801.09083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09083v1)
- **Published**: 2018-01-27 12:36:31+00:00
- **Updated**: 2018-01-27 12:36:31+00:00
- **Authors**: Yi Xiao, Peiyao Zhou, Yan Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Colorization methods using deep neural networks have become a recent trend. However, most of them do not allow user inputs, or only allow limited user inputs (only global inputs or only local inputs), to control the output colorful images. The possible reason is that it's difficult to differentiate the influence of different kind of user inputs in network training. To solve this problem, we present a novel deep colorization method, which allows simultaneous global and local inputs to better control the output colorized images. The key step is to design an appropriate loss function that can differentiate the influence of input data, global inputs and local inputs. With this design, our method accepts no inputs, or global inputs, or local inputs, or both global and local inputs, which is not supported in previous deep colorization methods. In addition, we propose a global color theme recommendation system to help users determine global inputs. Experimental results shows that our methods can better control the colorized images and generate state-of-art results.



### A Generative Approach to Zero-Shot and Few-Shot Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.09086v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09086v1)
- **Published**: 2018-01-27 13:13:29+00:00
- **Updated**: 2018-01-27 13:13:29+00:00
- **Authors**: Ashish Mishra, Vinay Kumar Verma, M Shiva Krishna Reddy, Arulkumar S, Piyush Rai, Anurag Mittal
- **Comment**: Accepted in WACV 2018
- **Journal**: None
- **Summary**: We present a generative framework for zero-shot action recognition where some of the possible action classes do not occur in the training data. Our approach is based on modeling each action class using a probability distribution whose parameters are functions of the attribute vector representing that action class. In particular, we assume that the distribution parameters for any action class in the visual space can be expressed as a linear combination of a set of basis vectors where the combination weights are given by the attributes of the action class. These basis vectors can be learned solely using labeled data from the known (i.e., previously seen) action classes, and can then be used to predict the parameters of the probability distributions of unseen action classes. We consider two settings: (1) Inductive setting, where we use only the labeled examples of the seen action classes to predict the unseen action class parameters; and (2) Transductive setting which further leverages unlabeled data from the unseen action classes. Our framework also naturally extends to few-shot action recognition where a few labeled examples from unseen classes are available. Our experiments on benchmark datasets (UCF101, HMDB51 and Olympic) show significant performance improvements as compared to various baselines, in both standard zero-shot (disjoint seen and unseen classes) and generalized zero-shot learning settings.



### Interactive Generative Adversarial Networks for Facial Expression Generation in Dyadic Interactions
- **Arxiv ID**: http://arxiv.org/abs/1801.09092v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09092v2)
- **Published**: 2018-01-27 14:01:17+00:00
- **Updated**: 2018-01-30 19:02:43+00:00
- **Authors**: Behnaz Nojavanasghari, Yuchi Huang, Saad Khan
- **Comment**: None
- **Journal**: None
- **Summary**: A social interaction is a social exchange between two or more individuals,where individuals modify and adjust their behaviors in response to their interaction partners. Our social interactions are one of most fundamental aspects of our lives and can profoundly affect our mood, both positively and negatively. With growing interest in virtual reality and avatar-mediated interactions,it is desirable to make these interactions natural and human like to promote positive effect in the interactions and applications such as intelligent tutoring systems, automated interview systems and e-learning. In this paper, we propose a method to generate facial behaviors for an agent. These behaviors include facial expressions and head pose and they are generated considering the users affective state. Our models learn semantically meaningful representations of the face and generate appropriate and temporally smooth facial behaviors in dyadic interactions.



### Towards an Understanding of Neural Networks in Natural-Image Spaces
- **Arxiv ID**: http://arxiv.org/abs/1801.09097v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09097v2)
- **Published**: 2018-01-27 14:41:59+00:00
- **Updated**: 2019-02-11 05:18:00+00:00
- **Authors**: Yifei Fan, Anthony Yezzi
- **Comment**: None
- **Journal**: None
- **Summary**: Two major uncertainties, dataset bias and adversarial examples, prevail in state-of-the-art AI algorithms with deep neural networks. In this paper, we present an intuitive explanation for these issues as well as an interpretation of the performance of deep networks in a natural-image space. The explanation consists of two parts: the philosophy of neural networks and a hypothetical model of natural-image spaces. Following the explanation, we 1) demonstrate that the values of training samples differ, 2) provide incremental boost to the accuracy of a CIFAR-10 classifier by introducing an additional "random-noise" category during training, 3) alleviate over-fitting thereby enhancing the robustness against adversarial examples by detecting and excluding illusive training samples that are consistently misclassified. Our overall contribution is therefore twofold. First, while most existing algorithms treat data equally and have a strong appetite for more data, we demonstrate in contrast that an individual datum can sometimes have disproportionate and counterproductive influence and that it is not always better to train neural networks with more data. Next, we consider more thoughtful strategies by taking into account the geometric and topological properties of natural-image spaces to which deep networks are applied.



### Understanding Deep Architectures by Visual Summaries
- **Arxiv ID**: http://arxiv.org/abs/1801.09103v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09103v3)
- **Published**: 2018-01-27 15:26:07+00:00
- **Updated**: 2019-08-29 11:32:25+00:00
- **Authors**: Marco Carletti, Marco Godi, Maedeh Aghaei, Francesco Giuliari, Marco Cristani
- **Comment**: Project page and code available at
  http://marcocarletti.altervista.org/publications/understanding-visual-summaries/
- **Journal**: None
- **Summary**: In deep learning, visualization techniques extract the salient patterns exploited by deep networks for image classification, focusing on single images; no effort has been spent in investigating whether these patterns are systematically related to precise semantic entities over multiple images belonging to a same class, thus failing to capture the very understanding of the image class the network has realized. This paper goes in this direction, presenting a visualization framework which produces a group of clusters or summaries, each one formed by crisp salient image regions focusing on a particular part that the network has exploited with high regularity to decide for a given class. The approach is based on a sparse optimization step providing sharp image saliency masks that are clustered together by means of a semantic flow similarity measure. The summaries communicate clearly what a network has exploited of a particular image class, and this is proved through automatic image tagging and with a user study. Beyond the deep network understanding, summaries are also useful for many quantitative reasons: their number is correlated with ability of a network to classify (more summaries, better performances), and they can be used to improve the classification accuracy of a network through summary-driven specializations.



### Deep Neural Networks In Fully Connected CRF For Image Labeling With Social Network Metadata
- **Arxiv ID**: http://arxiv.org/abs/1801.09108v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09108v1)
- **Published**: 2018-01-27 16:13:22+00:00
- **Updated**: 2018-01-27 16:13:22+00:00
- **Authors**: Chengjiang Long, Roddy Collins, Eran Swears, Anthony Hoogs
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel method for predicting image labels by fusing image content descriptors with the social media context of each image. An image uploaded to a social media site such as Flickr often has meaningful, associated information, such as comments and other images the user has uploaded, that is complementary to pixel content and helpful in predicting labels. Prediction challenges such as ImageNet~\cite{imagenet_cvpr09} and MSCOCO~\cite{LinMBHPRDZ:ECCV14} use only pixels, while other methods make predictions purely from social media context \cite{McAuleyECCV12}. Our method is based on a novel fully connected Conditional Random Field (CRF) framework, where each node is an image, and consists of two deep Convolutional Neural Networks (CNN) and one Recurrent Neural Network (RNN) that model both textual and visual node/image information. The edge weights of the CRF graph represent textual similarity and link-based metadata such as user sets and image groups. We model the CRF as an RNN for both learning and inference, and incorporate the weighted ranking loss and cross entropy loss into the CRF parameter optimization to handle the training data imbalance issue. Our proposed approach is evaluated on the MIR-9K dataset and experimentally outperforms current state-of-the-art approaches.



### Feature Based Framework to Detect Diseases, Tumor, and Bleeding in Wireless Capsule Endoscopy
- **Arxiv ID**: http://arxiv.org/abs/1802.02232v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02232v1)
- **Published**: 2018-01-27 16:17:04+00:00
- **Updated**: 2018-01-27 16:17:04+00:00
- **Authors**: Omid Haji Maghsoudi, Mahdi Alizadeh
- **Comment**: None
- **Journal**: None
- **Summary**: Studying animal locomotion improves our understanding of motor control and aids in the treatment of motor impairment. Mice are a premier model of human disease and are the model system of choice for much of basic neuroscience. High frame rates (250 Hz) are needed to quantify the kinematics of these running rodents. Manual tracking, especially for multiple markers, becomes time-consuming and impossible. Therefore, an automated method is necessary. We propose a method to track the paws of the animal in the following manner: first, segmenting all the possible paws based on color; second, classifying the segmented objects using a support vector machine (SVM) and neural network (NN); third, classifying the objects using the kinematic features of the running animal, coupled with texture features from earlier frames; and finally, detecting and handling collisions to assure the correctness of labelled paws. The proposed method is validated in sixty 1,000 frame video sequences (4 seconds) captured by four cameras from five mice. The total sensitivity for tracking of the front and hind paw is 99.70% using the SVM classifier and 99.76% using the NN classifier. In addition, we show the feasibility of 3D reconstruction using the four camera system.



### Robust Multi-subspace Analysis Using Novel Column L0-norm Constrained Matrix Factorization
- **Arxiv ID**: http://arxiv.org/abs/1801.09111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09111v1)
- **Published**: 2018-01-27 17:00:02+00:00
- **Updated**: 2018-01-27 17:00:02+00:00
- **Authors**: Binghui Wang, Chuang Lin
- **Comment**: 13 pages, 8 figures, 8 tables
- **Journal**: None
- **Summary**: We study the underlying structure of data (approximately) generated from a union of independent subspaces. Traditional methods learn only one subspace, failing to discover the multi-subspace structure, while state-of-the-art methods analyze the multi-subspace structure using data themselves as the dictionary, which cannot offer the explicit basis to span each subspace and are sensitive to errors via an indirect representation. Additionally, they also suffer from a high computational complexity, being quadratic or cubic to the sample size. To tackle all these problems, we propose a method, called Matrix Factorization with Column L0-norm constraint (MFC0), that can simultaneously learn the basis for each subspace, generate a direct sparse representation for each data sample, as well as removing errors in the data in an efficient way. Furthermore, we develop a first-order alternating direction algorithm, whose computational complexity is linear to the sample size, to stably and effectively solve the nonconvex objective function and non- smooth l0-norm constraint of MFC0. Experimental results on both synthetic and real-world datasets demonstrate that besides the superiority over traditional and state-of-the-art methods for subspace clustering, data reconstruction, error correction, MFC0 also shows its uniqueness for multi-subspace basis learning and direct sparse representation.



### Meshed Up: Learnt Error Correction in 3D Reconstructions
- **Arxiv ID**: http://arxiv.org/abs/1801.09128v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1801.09128v1)
- **Published**: 2018-01-27 19:38:21+00:00
- **Updated**: 2018-01-27 19:38:21+00:00
- **Authors**: Michael Tanner, Stefan Saftescu, Alex Bewley, Paul Newman
- **Comment**: Accepted for the International Conference on Robotics and Automation
  (ICRA) 2018
- **Journal**: None
- **Summary**: Dense reconstructions often contain errors that prior work has so far minimised using high quality sensors and regularising the output. Nevertheless, errors still persist. This paper proposes a machine learning technique to identify errors in three dimensional (3D) meshes. Beyond simply identifying errors, our method quantifies both the magnitude and the direction of depth estimate errors when viewing the scene. This enables us to improve the reconstruction accuracy.   We train a suitably deep network architecture with two 3D meshes: a high-quality laser reconstruction, and a lower quality stereo image reconstruction. The network predicts the amount of error in the lower quality reconstruction with respect to the high-quality one, having only view the former through its input. We evaluate our approach by correcting two-dimensional (2D) inverse-depth images extracted from the 3D model, and show that our method improves the quality of these depth reconstructions by up to a relative 10% RMSE.



