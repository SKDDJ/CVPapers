# Arxiv Papers in cs.CV on 2018-01-17
### Identification of Seed Cells in Multispectral Images for GrowCut Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1801.05525v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1801.05525v1)
- **Published**: 2018-01-17 01:57:27+00:00
- **Updated**: 2018-01-17 01:57:27+00:00
- **Authors**: Wuilan Torres, Antonio Rueda-Toicen
- **Comment**: 10 pages, in Spanish, originally presented at CIMENICS 2016, accepted
  to the Journal of the Faculty of Engineering UCV
- **Journal**: None
- **Summary**: The segmentation of satellite images is a necessary step to perform object-oriented image classification, which has become relevant due to its applicability on images with a high spatial resolution. To perform object-oriented image classification, the studied image must first be segmented in uniform regions. This segmentation requires manual work by an expert user, who must exhaustively explore the image to establish thresholds that generate useful and representative segments without oversegmenting and without discarding representative segments. We propose a technique that automatically segments the multispectral image while facing these issues. We identify in the image homogenous zones according to their spectral signatures through the use of morphological filters. These homogenous zones are representatives of different types of land coverings in the image and are used as seeds for the GrowCut multispectral segmentation algorithm. GrowCut is a cellular automaton with competitive region growth, its cells are linked to every pixel in the image through three parameters: the pixel's spectral signature, a label, and a strength factor that represents the strength with which a cell defends its label. The seed cells possess maximum strength and maintain their state throughout the automaton's evolution. Starting from seed cells, each cell in the image is iteratively attacked by its neighboring cells. When the automaton stops updating its states, we obtain a segmented image where each pixel has taken the label of one of its cells. In this paper the algorithm was applied in an image acquired by Landsat8 on agricultural land of Calabozo, Guarico, Venezuela where there are different types of land coverings: agriculture, urban regions, water bodies, and savannas with different degrees of human intervention. The segmentation obtained is presented as irregular polygons enclosing geographical objects.



### Cahn--Hilliard inpainting with the double obstacle potential
- **Arxiv ID**: http://arxiv.org/abs/1801.05527v2
- **DOI**: None
- **Categories**: **math.AP**, cs.CV, 49J40, 94A08, 68U10, 35K55
- **Links**: [PDF](http://arxiv.org/pdf/1801.05527v2)
- **Published**: 2018-01-17 02:20:18+00:00
- **Updated**: 2018-08-31 03:30:04+00:00
- **Authors**: Harald Garcke, Kei Fong Lam, Vanessa Styles
- **Comment**: 26 pages, 8 figures, accepted version
- **Journal**: None
- **Summary**: The inpainting of damaged images has a wide range of applications, and many different mathematical methods have been proposed to solve this problem. Inpainting with the help of Cahn--Hilliard models has been particularly successful, and it turns out that Cahn--Hilliard inpainting with the double obstacle potential can lead to better results compared to inpainting with a smooth double well potential. However, a mathematical analysis of this approach is missing so far. In this paper we give first analytical results for a Cahn--Hilliard double obstacle inpainting model regarding existence of global solutions to the time-dependent problem and stationary solutions to the time-independent problem without constraints on the parameters involved. With the help of numerical results we show the effectiveness of the approach for binary and grayscale images.



### RED-Net: A Recurrent Encoder-Decoder Network for Video-based Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/1801.06066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06066v1)
- **Published**: 2018-01-17 04:29:44+00:00
- **Updated**: 2018-01-17 04:29:44+00:00
- **Authors**: Xi Peng, Rogerio S. Feris, Xiaoyu Wang, Dimitris N. Metaxas
- **Comment**: International Journal of Computer Vision. arXiv admin note: text
  overlap with arXiv:1608.05477
- **Journal**: None
- **Summary**: We propose a novel method for real-time face alignment in videos based on a recurrent encoder-decoder network model. Our proposed model predicts 2D facial point heat maps regularized by both detection and regression loss, while uniquely exploiting recurrent learning at both spatial and temporal dimensions. At the spatial level, we add a feedback loop connection between the combined output response map and the input, in order to enable iterative coarse-to-fine face alignment using a single network model, instead of relying on traditional cascaded model ensembles. At the temporal level, we first decouple the features in the bottleneck of the network into temporal-variant factors, such as pose and expression, and temporal-invariant factors, such as identity information. Temporal recurrent learning is then applied to the decoupled temporal-variant features. We show that such feature disentangling yields better generalization and significantly more accurate results at test time. We perform a comprehensive experimental analysis, showing the importance of each component of our proposed model, as well as superior results over the state of the art and several variations of our method in standard datasets.



### Semi-supervised FusedGAN for Conditional Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1801.05551v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.05551v1)
- **Published**: 2018-01-17 05:07:49+00:00
- **Updated**: 2018-01-17 05:07:49+00:00
- **Authors**: Navaneeth Bodla, Gang Hua, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: We present FusedGAN, a deep network for conditional image synthesis with controllable sampling of diverse images. Fidelity, diversity and controllable sampling are the main quality measures of a good image generation model. Most existing models are insufficient in all three aspects. The FusedGAN can perform controllable sampling of diverse images with very high fidelity. We argue that controllability can be achieved by disentangling the generation process into various stages. In contrast to stacked GANs, where multiple stages of GANs are trained separately with full supervision of labeled intermediate images, the FusedGAN has a single stage pipeline with a built-in stacking of GANs. Unlike existing methods, which requires full supervision with paired conditions and images, the FusedGAN can effectively leverage more abundant images without corresponding conditions in training, to produce more diverse samples with high fidelity. We achieve this by fusing two generators: one for unconditional image generation, and the other for conditional image generation, where the two partly share a common latent space thereby disentangling the generation. We demonstrate the efficacy of the FusedGAN in fine grained image generation tasks such as text-to-image, and attribute-to-face generation.



### Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace
- **Arxiv ID**: http://arxiv.org/abs/1801.05558v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.05558v3)
- **Published**: 2018-01-17 05:34:08+00:00
- **Updated**: 2018-06-14 12:33:23+00:00
- **Authors**: Yoonho Lee, Seungjin Choi
- **Comment**: Accepted to ICML 2018
- **Journal**: None
- **Summary**: Gradient-based meta-learning methods leverage gradient descent to learn the commonalities among various tasks. While previous such methods have been successful in meta-learning tasks, they resort to simple gradient descent during meta-testing. Our primary contribution is the {\em MT-net}, which enables the meta-learner to learn on each layer's activation space a subspace that the task-specific learner performs gradient descent on. Additionally, a task-specific learner of an {\em MT-net} performs gradient descent with respect to a meta-learned distance metric, which warps the activation space to be more sensitive to task identity. We demonstrate that the dimension of this learned subspace reflects the complexity of the task-specific learner's adaptation task, and also that our model is less sensitive to the choice of initial learning rates than previous gradient-based meta-learning methods. Our method achieves state-of-the-art or comparable performance on few-shot classification and regression tasks.



### Fruit Quantity and Quality Estimation using a Robotic Vision System
- **Arxiv ID**: http://arxiv.org/abs/1801.05560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.05560v1)
- **Published**: 2018-01-17 05:53:26+00:00
- **Updated**: 2018-01-17 05:53:26+00:00
- **Authors**: M. Halstead, C. McCool, S. Denman, T. Perez, C. Fookes
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate localisation of crop remains highly challenging in unstructured environments such as farms. Many of the developed systems still rely on the use of hand selected features for crop identification and often neglect the estimation of crop quantity and quality, which is key to assigning labor during farming processes. To alleviate these limitations we present a robotic vision system that can accurately estimate the quantity and quality of sweet pepper (Capsicum annuum L), a key horticultural crop. This system consists of three parts: detection, quality estimation, and tracking. Efficient detection is achieved using the FasterRCNN framework. Quality is then estimated in the same framework by learning a parallel layer which we show experimentally results in superior performance than treating quality as extra classes in the traditional Faster-RCNN framework. Evaluation of these two techniques outlines the improved performance of the parallel layer, where we achieve an F1 score of 77.3 for the parallel technique yet only 72.5 for the best scoring (red) of the multi-class implementation. To track the crop we present a tracking via detection approach, which uses the FasterRCNN with parallel layers, that is also a vision-only solution. This approach is cheap to implement as it only requires a camera and in experiments across 2 days we show that our proposed system can accurately estimate the number of sweet pepper present, within 4.1% of the ground truth.



### Image Captioning using Deep Neural Architectures
- **Arxiv ID**: http://arxiv.org/abs/1801.05568v1
- **DOI**: 10.1109/ICIIECS.2017.8276124
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.05568v1)
- **Published**: 2018-01-17 06:24:44+00:00
- **Updated**: 2018-01-17 06:24:44+00:00
- **Authors**: Parth Shah, Vishvajit Bakarola, Supriya Pati
- **Comment**: Pre-print version of paper accepted at 2017 International Conference
  on Innovations in information Embedded and Communication Systems (ICIIECS)
- **Journal**: None
- **Summary**: Automatically creating the description of an image using any natural languages sentence like English is a very challenging task. It requires expertise of both image processing as well as natural language processing. This paper discuss about different available models for image captioning task. We have also discussed about how the advancement in the task of object recognition and machine translation has greatly improved the performance of image captioning model in recent years. In addition to that we have discussed how this model can be implemented. In the end, we have also evaluated the performance of model using standard evaluation matrices.



### Brenier approach for optimal transportation between a quasi-discrete measure and a discrete measure
- **Arxiv ID**: http://arxiv.org/abs/1801.05574v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.05574v1)
- **Published**: 2018-01-17 07:06:21+00:00
- **Updated**: 2018-01-17 07:06:21+00:00
- **Authors**: Ying Lu, Liming Chen, Alexandre Saidi, Xianfeng Gu
- **Comment**: None
- **Journal**: None
- **Summary**: Correctly estimating the discrepancy between two data distributions has always been an important task in Machine Learning. Recently, Cuturi proposed the Sinkhorn distance which makes use of an approximate Optimal Transport cost between two distributions as a distance to describe distribution discrepancy. Although it has been successfully adopted in various machine learning applications (e.g. in Natural Language Processing and Computer Vision) since then, the Sinkhorn distance also suffers from two unnegligible limitations. The first one is that the Sinkhorn distance only gives an approximation of the real Wasserstein distance, the second one is the `divide by zero' problem which often occurs during matrix scaling when setting the entropy regularization coefficient to a small value. In this paper, we introduce a new Brenier approach for calculating a more accurate Wasserstein distance between two discrete distributions, this approach successfully avoids the two limitations shown above for Sinkhorn distance and gives an alternative way for estimating distribution discrepancy.



### Light-weight pixel context encoders for image inpainting
- **Arxiv ID**: http://arxiv.org/abs/1801.05585v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.05585v1)
- **Published**: 2018-01-17 08:19:41+00:00
- **Updated**: 2018-01-17 08:19:41+00:00
- **Authors**: Nanne van Noord, Eric Postma
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we propose Pixel Content Encoders (PCE), a light-weight image inpainting model, capable of generating novel con-tent for large missing regions in images. Unlike previously presented convolutional neural network based models, our PCE model has an order of magnitude fewer trainable parameters. Moreover, by incorporating dilated convolutions we are able to preserve fine grained spatial information, achieving state-of-the-art performance on benchmark datasets of natural images and paintings. Besides image inpainting, we show that without changing the architecture, PCE can be used for image extrapolation, generating novel content beyond existing image boundaries.



### Additive Margin Softmax for Face Verification
- **Arxiv ID**: http://arxiv.org/abs/1801.05599v4
- **DOI**: 10.1109/LSP.2018.2822810
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.05599v4)
- **Published**: 2018-01-17 09:13:05+00:00
- **Updated**: 2018-05-30 12:35:03+00:00
- **Authors**: Feng Wang, Weiyang Liu, Haijun Liu, Jian Cheng
- **Comment**: Published in Signal Processing Letters, Volume: 25 Issue: 7 Pages:
  926-930
- **Journal**: None
- **Summary**: In this paper, we propose a conceptually simple and geometrically interpretable objective function, i.e. additive margin Softmax (AM-Softmax), for deep face verification. In general, the face verification task can be viewed as a metric learning problem, so learning large-margin face features whose intra-class variation is small and inter-class difference is large is of great importance in order to achieve good performance. Recently, Large-margin Softmax and Angular Softmax have been proposed to incorporate the angular margin in a multiplicative manner. In this work, we introduce a novel additive angular margin for the Softmax loss, which is intuitively appealing and more interpretable than the existing works. We also emphasize and discuss the importance of feature normalization in the paper. Most importantly, our experiments on LFW BLUFR and MegaFace show that our additive margin softmax loss consistently performs better than the current state-of-the-art methods using the same network architecture and training dataset. Our code has also been made available at https://github.com/happynear/AMSoftmax



### Multi-View Stereo 3D Edge Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1801.05606v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.05606v1)
- **Published**: 2018-01-17 09:46:01+00:00
- **Updated**: 2018-01-17 09:46:01+00:00
- **Authors**: Andrea Bignoli, Andrea Romanoni, Matteo Matteucci
- **Comment**: Accepted for WACV 2018
- **Journal**: None
- **Summary**: This paper presents a novel method for the reconstruction of 3D edges in multi-view stereo scenarios. Previous research in the field typically relied on video sequences and limited the reconstruction process to either straight line-segments, or edge-points, i.e., 3D points that correspond to image edges. We instead propose a system, denoted as EdgeGraph3D, able to recover both straight and curved 3D edges from an unordered image sequence. A second contribution of this work is a graph-based representation for 2D edges that allows the identification of the most structurally significant edges detected in an image. We integrate EdgeGraph3D in a multi-view stereo reconstruction pipeline and analyze the benefits provided by 3D edges to the accuracy of the recovered surfaces. We evaluate the effectiveness of our approach on multiple datasets from two different collections in the multi-view stereo literature. Experimental results demonstrate the ability of EdgeGraph3D to work in presence of strong illumination changes and reflections, which are usually detrimental to the effectiveness of classical photometric reconstruction systems.



### FastNet
- **Arxiv ID**: http://arxiv.org/abs/1802.02186v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.02186v1)
- **Published**: 2018-01-17 10:37:58+00:00
- **Updated**: 2018-01-17 10:37:58+00:00
- **Authors**: John Olafenwa, Moses Olafenwa
- **Comment**: None
- **Journal**: None
- **Summary**: Inception and the Resnet family of Convolutional Neural Network archi-tectures have broken records in the past few years, but recent state of the art models have also incurred very high computational cost in terms of training, inference and model size. Making the deployment of these models on Edge devices, impractical. In light of this, we present a new novel architecture that is designed for high computational efficiency on both GPUs and CPUs, and is highly suited for deployment on Mobile Applications, Smart Cameras, Iot devices and controllers as well as low cost drones. Our architecture boasts competitive accuracies on standard Datasets even out-performing the original Resnet. We present below the motivation for this research, the architecture of the network, single test accuracies on CIFAR 10 and CIFAR 100 , a detailed comparison with other well-known architectures and link to an implementation in Keras.



### Face Recognition via Centralized Coordinate Learning
- **Arxiv ID**: http://arxiv.org/abs/1801.05678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.05678v1)
- **Published**: 2018-01-17 14:32:40+00:00
- **Updated**: 2018-01-17 14:32:40+00:00
- **Authors**: Xianbiao Qi, Lei Zhang
- **Comment**: 14 pages, 9 figures
- **Journal**: None
- **Summary**: Owe to the rapid development of deep neural network (DNN) techniques and the emergence of large scale face databases, face recognition has achieved a great success in recent years. During the training process of DNN, the face features and classification vectors to be learned will interact with each other, while the distribution of face features will largely affect the convergence status of network and the face similarity computing in test stage. In this work, we formulate jointly the learning of face features and classification vectors, and propose a simple yet effective centralized coordinate learning (CCL) method, which enforces the features to be dispersedly spanned in the coordinate space while ensuring the classification vectors to lie on a hypersphere. An adaptive angular margin is further proposed to enhance the discrimination capability of face features. Extensive experiments are conducted on six face benchmarks, including those have large age gap and hard negative samples. Trained only on the small-scale CASIA Webface dataset with 460K face images from about 10K subjects, our CCL model demonstrates high effectiveness and generality, showing consistently competitive performance across all the six benchmark databases.



### A modified fuzzy C means algorithm for shading correction in craniofacial CBCT images
- **Arxiv ID**: http://arxiv.org/abs/1801.05694v1
- **DOI**: 10.1007/978-981-10-4166-2_81
- **Categories**: **cs.CV**, eess.IV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1801.05694v1)
- **Published**: 2018-01-17 14:54:39+00:00
- **Updated**: 2018-01-17 14:54:39+00:00
- **Authors**: Awais Ashfaq, Jonas Adler
- **Comment**: 15 pages, published in CMBEBIH 2017
- **Journal**: Proceedings of the International Conference on Medical and
  Biological Engineering 2017
- **Summary**: CBCT images suffer from acute shading artifacts primarily due to scatter. Numerous image-domain correction algorithms have been proposed in the literature that use patient-specific planning CT images to estimate shading contributions in CBCT images. However, in the context of radiosurgery applications such as gamma knife, planning images are often acquired through MRI which impedes the use of polynomial fitting approaches for shading correction. We present a new shading correction approach that is independent of planning CT images. Our algorithm is based on the assumption that true CBCT images follow a uniform volumetric intensity distribution per material, and scatter perturbs this uniform texture by contributing cupping and shading artifacts in the image domain. The framework is a combination of fuzzy C-means coupled with a neighborhood regularization term and Otsu's method. Experimental results on artificially simulated craniofacial CBCT images are provided to demonstrate the effectiveness of our algorithm. Spatial non-uniformity is reduced from 16% to 7% in soft tissue and from 44% to 8% in bone regions. With shading-correction, thresholding based segmentation accuracy for bone pixels is improved from 85% to 91% when compared to thresholding without shading-correction. The proposed algorithm is thus practical and qualifies as a plug and play extension into any CBCT reconstruction software for shading correction.



### TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1801.05746v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.05746v1)
- **Published**: 2018-01-17 16:49:10+00:00
- **Updated**: 2018-01-17 16:49:10+00:00
- **Authors**: Vladimir Iglovikov, Alexey Shvets
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Pixel-wise image segmentation is demanding task in computer vision. Classical U-Net architectures composed of encoders and decoders are very popular for segmentation of medical images, satellite images etc. Typically, neural network initialized with weights from a network pre-trained on a large data set like ImageNet shows better performance than those trained from scratch on a small dataset. In some practical applications, particularly in medicine and traffic safety, the accuracy of the models is of utmost importance. In this paper, we demonstrate how the U-Net type architecture can be improved by the use of the pre-trained encoder. Our code and corresponding pre-trained weights are publicly available at https://github.com/ternaus/TernausNet. We compare three weight initialization schemes: LeCun uniform, the encoder with weights from VGG11 and full network trained on the Carvana dataset. This network architecture was a part of the winning solution (1st out of 735) in the Kaggle: Carvana Image Masking Challenge.



### Seismic-Net: A Deep Densely Connected Neural Network to Detect Seismic Events
- **Arxiv ID**: http://arxiv.org/abs/1802.02241v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1802.02241v1)
- **Published**: 2018-01-17 17:06:04+00:00
- **Updated**: 2018-01-17 17:06:04+00:00
- **Authors**: Yue Wu, Youzuo Lin, Zheng Zhou, Andrew Delorey
- **Comment**: None
- **Journal**: None
- **Summary**: One of the risks of large-scale geologic carbon sequestration is the potential migration of fluids out of the storage formations. Accurate and fast detection of this fluids migration is not only important but also challenging, due to the large subsurface uncertainty and complex governing physics. Traditional leakage detection and monitoring techniques rely on geophysical observations including seismic. However, the resulting accuracy of these methods is limited because of indirect information they provide requiring expert interpretation, therefore yielding in-accurate estimates of leakage rates and locations. In this work, we develop a novel machine-learning detection package, named "Seismic-Net", which is based on the deep densely connected neural network. To validate the performance of our proposed leakage detection method, we employ our method to a natural analog site at Chimay\'o, New Mexico. The seismic events in the data sets are generated because of the eruptions of geysers, which is due to the leakage of $\mathrm{CO}_\mathrm{2}$. In particular, we demonstrate the efficacy of our Seismic-Net by formulating our detection problem as an event detection problem with time series data. A fixed-length window is slid throughout the time series data and we build a deep densely connected network to classify each window to determine if a geyser event is included. Through our numerical tests, we show that our model achieves precision/recall as high as 0.889/0.923. Therefore, our Seismic-Net has a great potential for detection of $\mathrm{CO}_\mathrm{2}$ leakage.



### Faster gaze prediction with dense networks and Fisher pruning
- **Arxiv ID**: http://arxiv.org/abs/1801.05787v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.05787v2)
- **Published**: 2018-01-17 18:34:33+00:00
- **Updated**: 2018-07-09 10:38:35+00:00
- **Authors**: Lucas Theis, Iryna Korshunova, Alykhan Tejani, Ferenc Huszár
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting human fixations from images has recently seen large improvements by leveraging deep representations which were pretrained for object recognition. However, as we show in this paper, these networks are highly overparameterized for the task of fixation prediction. We first present a simple yet principled greedy pruning method which we call Fisher pruning. Through a combination of knowledge distillation and Fisher pruning, we obtain much more runtime-efficient architectures for saliency prediction, achieving a 10x speedup for the same AUC performance as a state of the art network on the CAT2000 dataset. Speeding up single-image gaze prediction is important for many real-world applications, but it is also a crucial step in the development of video saliency models, where the amount of data to be processed is substantially larger.



### Interactive in-base street model edit: how common GIS software and a database can serve as a custom Graphical User Interface
- **Arxiv ID**: http://arxiv.org/abs/1801.05800v1
- **DOI**: None
- **Categories**: **cs.DB**, cs.CG, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1801.05800v1)
- **Published**: 2018-01-17 18:55:43+00:00
- **Updated**: 2018-01-17 18:55:43+00:00
- **Authors**: Remi Cura, Julien Perret, Nicolas Paparoditis
- **Comment**: this article is an extract from PhD thesis
- **Journal**: None
- **Summary**: Our modern world produces an increasing quantity of data, and especially geospatial data, with advance of sensing technologies, and growing complexity and organisation of vector data. Tools are needed to efficiently create and edit those vector geospatial data. Procedural generation has been a tool of choice to generate strongly organised data, yet it may be hard to control. Because those data may be involved to take consequence-full real life decisions, user interactions are required to check data and edit it. The classical process to do so would be to build an adhoc Graphical User Interface (GUI) tool adapted for the model and method being used. This task is difficult, takes a large amount of resources, and is very specific to one model, making it hard to share and re-use.   Besides, many common generic GUI already exists to edit vector data, each having its specialities. We propose a change of paradigm; instead of building a specific tool for one task, we use common GIS software as GUIs, and deport the specific interactions from the software to within the database. In this paradigm, GIS software simply modify geometry and attributes of database layers, and those changes are used by the database to perform automated task.   This new paradigm has many advantages. The first one is genericity. With in-base interaction, any GIS software can be used to perform edition, whatever the software is a Desktop sofware or a web application. The second is concurrency and coherency. Because interaction is in-base, use of database features allows seamless multi-user work, and can guarantee that the data is in a coherent state. Last we propose tools to facilitate multi-user edits, both during the edit phase (each user knows what areas are edited by other users), and before and after edit (planning of edit, analyse of edited areas).



### Smile detection in the wild based on transfer learning
- **Arxiv ID**: http://arxiv.org/abs/1802.02185v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02185v1)
- **Published**: 2018-01-17 19:58:26+00:00
- **Updated**: 2018-01-17 19:58:26+00:00
- **Authors**: Xin Guo, Luisa F. Polanía, Kenneth E. Barner
- **Comment**: None
- **Journal**: None
- **Summary**: Smile detection from unconstrained facial images is a specialized and challenging problem. As one of the most informative expressions, smiles convey basic underlying emotions, such as happiness and satisfaction, which lead to multiple applications, e.g., human behavior analysis and interactive controlling. Compared to the size of databases for face recognition, far less labeled data is available for training smile detection systems. To leverage the large amount of labeled data from face recognition datasets and to alleviate overfitting on smile detection, an efficient transfer learning-based smile detection approach is proposed in this paper. Unlike previous works which use either hand-engineered features or train deep convolutional networks from scratch, a well-trained deep face recognition model is explored and fine-tuned for smile detection in the wild. Three different models are built as a result of fine-tuning the face recognition model with different inputs, including aligned, unaligned and grayscale images generated from the GENKI-4K dataset. Experiments show that the proposed approach achieves improved state-of-the-art performance. Robustness of the model to noise and blur artifacts is also evaluated in this paper.



