# Arxiv Papers in cs.CV on 2018-01-13
### Feature Space Transfer for Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1801.04356v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04356v3)
- **Published**: 2018-01-13 01:02:28+00:00
- **Updated**: 2019-04-16 18:23:13+00:00
- **Authors**: Bo Liu, Xudong Wang, Mandar Dixit, Roland Kwitt, Nuno Vasconcelos
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of data augmentation in feature space is considered. A new architecture, denoted the FeATure TransfEr Network (FATTEN), is proposed for the modeling of feature trajectories induced by variations of object pose. This architecture exploits a parametrization of the pose manifold in terms of pose and appearance. This leads to a deep encoder/decoder network architecture, where the encoder factors into an appearance and a pose predictor. Unlike previous attempts at trajectory transfer, FATTEN can be efficiently trained end-to-end, with no need to train separate feature transfer functions. This is realized by supplying the decoder with information about a target pose and the use of a multi-task loss that penalizes category- and pose-mismatches. In result, FATTEN discourages discontinuous or non-smooth trajectories that fail to capture the structure of the pose manifold, and generalizes well on object recognition tasks involving large pose variation. Experimental results on the artificial ModelNet database show that it can successfully learn to map source features to target features of a desired pose, while preserving class identity. Most notably, by using feature space transfer for data augmentation (w.r.t. pose and depth) on SUN-RGBD objects, we demonstrate considerable performance improvements on one/few-shot object recognition in a transfer learning setup, compared to current state-of-the-art methods.



### MobileNetV2: Inverted Residuals and Linear Bottlenecks
- **Arxiv ID**: http://arxiv.org/abs/1801.04381v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04381v4)
- **Published**: 2018-01-13 04:46:26+00:00
- **Updated**: 2019-03-21 19:44:34+00:00
- **Authors**: Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen
- **Comment**: None
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2018, pp. 4510-4520
- **Summary**: In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.   The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters



### Autonomous Driving in Reality with Reinforcement Learning and Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1801.05299v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.05299v2)
- **Published**: 2018-01-13 06:55:23+00:00
- **Updated**: 2019-04-25 17:28:52+00:00
- **Authors**: Nayun Xu, Bowen Tan, Bingyu Kong
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised learning is widely used in training autonomous driving vehicle. However, it is trained with large amount of supervised labeled data. Reinforcement learning can be trained without abundant labeled data, but we cannot train it in reality because it would involve many unpredictable accidents. Nevertheless, training an agent with good performance in virtual environment is relatively much easier. Because of the huge difference between virtual and real, how to fill the gap between virtual and real is challenging. In this paper, we proposed a novel framework of reinforcement learning with image semantic segmentation network to make the whole model adaptable to reality. The agent is trained in TORCS, a car racing simulator.



### Benchmark Visual Question Answer Models by using Focus Map
- **Arxiv ID**: http://arxiv.org/abs/1801.05302v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.05302v1)
- **Published**: 2018-01-13 09:09:33+00:00
- **Updated**: 2018-01-13 09:09:33+00:00
- **Authors**: Wenda Qiu, Yueyang Xianzang, Zhekai Zhang
- **Comment**: A group project paper for course CS348. arXiv admin note: text
  overlap with arXiv:1705.03633 by other authors
- **Journal**: None
- **Summary**: Inferring and Executing Programs for Visual Reasoning proposes a model for visual reasoning that consists of a program generator and an execution engine to avoid end-to-end models. To show that the model actually learns which objects to focus on to answer the questions, the authors give a visualization of the norm of the gradient of the sum of the predicted answer scores with respect to the final feature map. However, the authors do not evaluate the efficiency of focus map. This paper purposed a method for evaluating it. We generate several kinds of questions to test different keywords. We infer focus maps from the model by asking these questions and evaluate them by comparing with the segmentation graph. Furthermore, this method can be applied to any model if focus maps can be inferred from it. By evaluating focus map of different models on the CLEVR dataset, we will show that CLEVR-iep model has learned where to focus more than end-to-end models.



### Semi-supervised Fisher vector network
- **Arxiv ID**: http://arxiv.org/abs/1801.04438v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04438v1)
- **Published**: 2018-01-13 13:42:02+00:00
- **Updated**: 2018-01-13 13:42:02+00:00
- **Authors**: Petar Palasek, Ioannis Patras
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we explore how the architecture proposed in [8], which expresses the processing steps of the classical Fisher vector pipeline approaches, i.e. dimensionality reduction by principal component analysis (PCA) projection, Gaussian mixture model (GMM) and Fisher vector descriptor extraction as network layers, can be modified into a hybrid network that combines the benefits of both unsupervised and supervised training methods, resulting in a model that learns a semi-supervised Fisher vector descriptor of the input data. We evaluate the proposed model at image classification and action recognition problems and show how the model's classification performance improves as the amount of unlabeled data increases during training.



### Size-to-depth: A New Perspective for Single Image Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1801.04461v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.04461v1)
- **Published**: 2018-01-13 16:14:42+00:00
- **Updated**: 2018-01-13 16:14:42+00:00
- **Authors**: Yiran Wu, Sihao Ying, Lianmin Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we consider the problem of single monocular image depth estimation. It is a challenging problem due to its ill-posedness nature and has found wide application in industry. Previous efforts belongs roughly to two families: learning-based method and interactive method. Learning-based method, in which deep convolutional neural network (CNN) is widely used, can achieve good result. But they suffer low generalization ability and typically perform poorly for unfamiliar scenes. Besides, data-hungry nature for such method makes data aquisition expensive and time-consuming. Interactive method requires human annotation of depth which, however, is errorneous and of large variance.   To overcome these problems, we propose a new perspective for single monocular image depth estimation problem: size to depth. Our method require sparse label for real-world size of object rather than raw depth. A Coarse depth map is then inferred following geometric relationships according to size labels. Then we refine the depth map by doing energy function optimization on conditional random field(CRF). We experimentally demonstrate that our method outperforms traditional depth-labeling methods and can produce satisfactory depth maps.



### Can Computers Create Art?
- **Arxiv ID**: http://arxiv.org/abs/1801.04486v6
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.04486v6)
- **Published**: 2018-01-13 21:04:13+00:00
- **Updated**: 2018-05-08 03:45:56+00:00
- **Authors**: Aaron Hertzmann
- **Comment**: to appear in Arts, special issue on Machine as Artist (21st Century)
- **Journal**: None
- **Summary**: This essay discusses whether computers, using Artificial Intelligence (AI), could create art. First, the history of technologies that automated aspects of art is surveyed, including photography and animation. In each case, there were initial fears and denial of the technology, followed by a blossoming of new creative and professional opportunities for artists. The current hype and reality of Artificial Intelligence (AI) tools for art making is then discussed, together with predictions about how AI tools will be used. It is then speculated about whether it could ever happen that AI systems could be credited with authorship of artwork. It is theorized that art is something created by social agents, and so computers cannot be credited with authorship of art in our current understanding. A few ways that this could change are also hypothesized.



