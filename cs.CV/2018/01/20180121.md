# Arxiv Papers in cs.CV on 2018-01-21
### Multi-pseudo Regularized Label for Generated Data in Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1801.06742v3
- **DOI**: 10.1109/TIP.2018.2874715
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06742v3)
- **Published**: 2018-01-21 00:20:30+00:00
- **Updated**: 2018-10-01 05:52:12+00:00
- **Authors**: Yan Huang, Jinsong Xu, Qiang Wu, Zhedong Zheng, Zhaoxiang Zhang, Jian Zhang
- **Comment**: To appear on IEEE Transaction on Image Processing
- **Journal**: None
- **Summary**: Sufficient training data normally is required to train deeply learned models. However, due to the expensive manual process for labelling large number of images, the amount of available training data is always limited. To produce more data for training a deep network, Generative Adversarial Network (GAN) can be used to generate artificial sample data. However, the generated data usually does not have annotation labels. To solve this problem, in this paper, we propose a virtual label called Multi-pseudo Regularized Label (MpRL) and assign it to the generated data. With MpRL, the generated data will be used as the supplementary of real training data to train a deep neural network in a semi-supervised learning fashion. To build the corresponding relationship between the real data and generated data, MpRL assigns each generated data a proper virtual label which reflects the likelihood of the affiliation of the generated data to pre-defined training classes in the real data domain. Unlike the traditional label which usually is a single integral number, the virtual label proposed in this work is a set of weight-based values each individual of which is a number in (0,1] called multi-pseudo label and reflects the degree of relation between each generated data to every pre-defined class of real data. A comprehensive evaluation is carried out by adopting two state-of-the-art convolutional neural networks (CNNs) in our experiments to verify the effectiveness of MpRL. Experiments demonstrate that by assigning MpRL to generated data, we can further improve the person re-ID performance on five re-ID datasets, i.e., Market-1501, DukeMTMC-reID, CUHK03, VIPeR, and CUHK01. The proposed method obtains +6.29%, +6.30%, +5.58%, +5.84%, and +3.48% improvements in rank-1 accuracy over a strong CNN baseline on the five datasets respectively, and outperforms state-of-the-art methods.



### Denoising Prior Driven Deep Neural Network for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1801.06756v2
- **DOI**: 10.1109/TPAMI.2018.2873610
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06756v2)
- **Published**: 2018-01-21 03:08:11+00:00
- **Updated**: 2020-10-27 07:35:32+00:00
- **Authors**: Weisheng Dong, Peiyao Wang, Wotao Yin, Guangming Shi, Fangfang Wu, Xiaotong Lu
- **Comment**: 14page, 11 figures
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have shown very promising results for various image restoration (IR) tasks. However, the design of network architectures remains a major challenging for achieving further improvements. While most existing DNN-based methods solve the IR problems by directly mapping low quality images to desirable high-quality images, the observation models characterizing the image degradation processes have been largely ignored. In this paper, we first propose a denoising-based IR algorithm, whose iterative steps can be computed efficiently. Then, the iterative process is unfolded into a deep neural network, which is composed of multiple denoisers modules interleaved with back-projection (BP) modules that ensure the observation consistencies. A convolutional neural network (CNN) based denoiser that can exploit the multi-scale redundancies of natural images is proposed. As such, the proposed network not only exploits the powerful denoising ability of DNNs, but also leverages the prior of the observation model. Through end-to-end training, both the denoisers and the BP modules can be jointly optimized. Experimental results on several IR tasks, e.g., image denoisig, super-resolution and deblurring show that the proposed method can lead to very competitive and often state-of-the-art results on several IR tasks, including image denoising, deblurring and super-resolution.



### PU-Net: Point Cloud Upsampling Network
- **Arxiv ID**: http://arxiv.org/abs/1801.06761v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1801.06761v2)
- **Published**: 2018-01-21 04:10:52+00:00
- **Updated**: 2018-03-26 06:20:14+00:00
- **Authors**: Lequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, Pheng-Ann Heng
- **Comment**: accepted by CVPR2018
- **Journal**: None
- **Summary**: Learning and analyzing 3D point clouds with deep networks is challenging due to the sparseness and irregularity of the data. In this paper, we present a data-driven point cloud upsampling technique. The key idea is to learn multi-level features per point and expand the point set via a multi-branch convolution unit implicitly in feature space. The expanded feature is then split to a multitude of features, which are then reconstructed to an upsampled point set. Our network is applied at a patch-level, with a joint loss function that encourages the upsampled points to remain on the underlying surface with a uniform distribution. We conduct various experiments using synthesis and scan data to evaluate our method and demonstrate its superiority over some baseline methods and an optimization-based method. Results show that our upsampled points have better uniformity and are located closer to the underlying surfaces.



### Deep joint rain and haze removal from single images
- **Arxiv ID**: http://arxiv.org/abs/1801.06769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06769v1)
- **Published**: 2018-01-21 05:09:58+00:00
- **Updated**: 2018-01-21 05:09:58+00:00
- **Authors**: Liang Shen, Zihan Yue, Quan Chen, Fan Feng, Jie Ma
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Rain removal from a single image is a challenge which has been studied for a long time. In this paper, a novel convolutional neural network based on wavelet and dark channel is proposed. On one hand, we think that rain streaks correspond to high frequency component of the image. Therefore, haar wavelet transform is a good choice to separate the rain streaks and background to some extent. More specifically, the LL subband of a rain image is more inclined to express the background information, while LH, HL, HH subband tend to represent the rain streaks and the edges. On the other hand, the accumulation of rain streaks from long distance makes the rain image look like haze veil. We extract dark channel of rain image as a feature map in network. By increasing this mapping between the dark channel of input and output images, we achieve haze removal in an indirect way. All of the parameters are optimized by back-propagation. Experiments on both synthetic and real- world datasets reveal that our method outperforms other state-of- the-art methods from a qualitative and quantitative perspective.



### Decoupled Learning for Conditional Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.06790v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06790v1)
- **Published**: 2018-01-21 08:48:23+00:00
- **Updated**: 2018-01-21 08:48:23+00:00
- **Authors**: Zhifei Zhang, Yang Song, Hairong Qi
- **Comment**: None
- **Journal**: None
- **Summary**: Incorporating encoding-decoding nets with adversarial nets has been widely adopted in image generation tasks. We observe that the state-of-the-art achievements were obtained by carefully balancing the reconstruction loss and adversarial loss, and such balance shifts with different network structures, datasets, and training strategies. Empirical studies have demonstrated that an inappropriate weight between the two losses may cause instability, and it is tricky to search for the optimal setting, especially when lacking prior knowledge on the data and network.   This paper gives the first attempt to relax the need of manual balancing by proposing the concept of \textit{decoupled learning}, where a novel network structure is designed that explicitly disentangles the backpropagation paths of the two losses.   Experimental results demonstrate the effectiveness, robustness, and generality of the proposed method. The other contribution of the paper is the design of a new evaluation metric to measure the image quality of generative models. We propose the so-called \textit{normalized relative discriminative score} (NRDS), which introduces the idea of relative comparison, rather than providing absolute estimates like existing metrics.



### Depth CNNs for RGB-D scene recognition: learning from scratch better than transferring from RGB-CNNs
- **Arxiv ID**: http://arxiv.org/abs/1801.06797v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.06797v1)
- **Published**: 2018-01-21 09:38:50+00:00
- **Updated**: 2018-01-21 09:38:50+00:00
- **Authors**: Xinhang Song, Luis Herranz, Shuqiang Jiang
- **Comment**: AAAI Conference on Artificial Intelligence 2017
- **Journal**: AAAI Conference on Artificial Intelligence 2017, 4271-4277
- **Summary**: Scene recognition with RGB images has been extensively studied and has reached very remarkable recognition levels, thanks to convolutional neural networks (CNN) and large scene datasets. In contrast, current RGB-D scene data is much more limited, so often leverages RGB large datasets, by transferring pretrained RGB CNN models and fine-tuning with the target RGB-D dataset. However, we show that this approach has the limitation of hardly reaching bottom layers, which is key to learn modality-specific features. In contrast, we focus on the bottom layers, and propose an alternative strategy to learn depth features combining local weakly supervised training from patches followed by global fine tuning with images. This strategy is capable of learning very discriminative depth-specific features with limited depth images, without resorting to Places-CNN. In addition we propose a modified CNN architecture to further match the complexity of the model and the amount of data available. For RGB-D scene recognition, depth and RGB features are combined by projecting them in a common space and further leaning a multilayer classifier, which is jointly optimized in an end-to-end network. Our framework achieves state-of-the-art accuracy on NYU2 and SUN RGB-D in both depth only and combined RGB-D data.



### Curvature-based Comparison of Two Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.06801v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1801.06801v1)
- **Published**: 2018-01-21 10:17:33+00:00
- **Updated**: 2018-01-21 10:17:33+00:00
- **Authors**: Tao Yu, Huan Long, John E. Hopcroft
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we show the similarities and differences of two deep neural networks by comparing the manifolds composed of activation vectors in each fully connected layer of them. The main contribution of this paper includes 1) a new data generating algorithm which is crucial for determining the dimension of manifolds; 2) a systematic strategy to compare manifolds. Especially, we take Riemann curvature and sectional curvature as part of criterion, which can reflect the intrinsic geometric properties of manifolds. Some interesting results and phenomenon are given, which help in specifying the similarities and differences between the features extracted by two networks and demystifying the intrinsic mechanism of deep neural networks.



### Dense Recurrent Neural Networks for Scene Labeling
- **Arxiv ID**: http://arxiv.org/abs/1801.06831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06831v1)
- **Published**: 2018-01-21 14:43:22+00:00
- **Updated**: 2018-01-21 14:43:22+00:00
- **Authors**: Heng Fan, Haibin Ling
- **Comment**: Tech. Report
- **Journal**: None
- **Summary**: Recently recurrent neural networks (RNNs) have demonstrated the ability to improve scene labeling through capturing long-range dependencies among image units. In this paper, we propose dense RNNs for scene labeling by exploring various long-range semantic dependencies among image units. In comparison with existing RNN based approaches, our dense RNNs are able to capture richer contextual dependencies for each image unit via dense connections between each pair of image units, which significantly enhances their discriminative power. Besides, to select relevant and meanwhile restrain irrelevant dependencies for each unit from dense connections, we introduce an attention model into dense RNNs. The attention model enables automatically assigning more importance to helpful dependencies while less weight to unconcerned dependencies. Integrating with convolutional neural networks (CNNs), our method achieves state-of-the-art performances on the PASCAL Context, MIT ADE20K and SiftFlow benchmarks.



### Detecting Volcano Deformation in InSAR using Deep learning
- **Arxiv ID**: http://arxiv.org/abs/1803.00380v1
- **DOI**: None
- **Categories**: **cs.CV**, 68
- **Links**: [PDF](http://arxiv.org/pdf/1803.00380v1)
- **Published**: 2018-01-21 17:28:58+00:00
- **Updated**: 2018-01-21 17:28:58+00:00
- **Authors**: N. Anantrasirichai, F. Albino, P. Hill, D. Bull, J. Biggs
- **Comment**: Janet Watson Meeting 2018: A Data Explosion - The Geological Society
- **Journal**: None
- **Summary**: Globally 800 million people live within 100 km of a volcano and currently 1500 volcanoes are considered active, but half of these have no ground-based monitoring. Alternatively, satellite radar (InSAR) can be employed to observe volcanic ground deformation, which has shown a significant statistical link to eruptions. Modern satellites provide large coverage with high resolution signals, leading to huge amounts of data. The explosion in data has brought major challenges associated with timely dissemination of information and distinguishing volcano deformation patterns from noise, which currently relies on manual inspection. Moreover, volcano observatories still lack expertise to exploit satellite datasets, particularly in developing countries. This paper presents a novel approach to detect volcanic ground deformation automatically from wrapped-phase InSAR images. Convolutional neural networks (CNN) are employed to detect unusual patterns within the radar data.



### Scene recognition with CNNs: objects, scales and dataset bias
- **Arxiv ID**: http://arxiv.org/abs/1801.06867v1
- **DOI**: 10.1109/CVPR.2016.68
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.06867v1)
- **Published**: 2018-01-21 18:18:47+00:00
- **Updated**: 2018-01-21 18:18:47+00:00
- **Authors**: Luis Herranz, Shuqiang Jiang, Xiangyang Li
- **Comment**: CVPR 2016
- **Journal**: L. Herranz, S. Jiang, X. Li, "Scene recognition with CNNs:
  objects, scales and dataset bias", Proc. International Conference on Computer
  Vision and Pattern Recognition (CVPR16), Las Vegas, Nevada, USA, June 2016
- **Summary**: Since scenes are composed in part of objects, accurate recognition of scenes requires knowledge about both scenes and objects. In this paper we address two related problems: 1) scale induced dataset bias in multi-scale convolutional neural network (CNN) architectures, and 2) how to combine effectively scene-centric and object-centric knowledge (i.e. Places and ImageNet) in CNNs. An earlier attempt, Hybrid-CNN, showed that incorporating ImageNet did not help much. Here we propose an alternative method taking the scale into account, resulting in significant recognition gains. By analyzing the response of ImageNet-CNNs and Places-CNNs at different scales we find that both operate in different scale ranges, so using the same network for all the scales induces dataset bias resulting in limited performance. Thus, adapting the feature extractor to each particular scale (i.e. scale-specific CNNs) is crucial to improve recognition, since the objects in the scenes have their specific range of scales. Experimental results show that the recognition accuracy highly depends on the scale, and that simple yet carefully chosen multi-scale combinations of ImageNet-CNNs and Places-CNNs, can push the state-of-the-art recognition accuracy in SUN397 up to 66.26% (and even 70.17% with deeper architectures, comparable to human performance).



### Bayesian Deep Convolutional Encoder-Decoder Networks for Surrogate Modeling and Uncertainty Quantification
- **Arxiv ID**: http://arxiv.org/abs/1801.06879v1
- **DOI**: 10.1016/j.jcp.2018.04.018
- **Categories**: **physics.comp-ph**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.06879v1)
- **Published**: 2018-01-21 19:18:13+00:00
- **Updated**: 2018-01-21 19:18:13+00:00
- **Authors**: Yinhao Zhu, Nicholas Zabaras
- **Comment**: 52 pages, 28 figures, submitted to Journal of Computational Physics
- **Journal**: None
- **Summary**: We are interested in the development of surrogate models for uncertainty quantification and propagation in problems governed by stochastic PDEs using a deep convolutional encoder-decoder network in a similar fashion to approaches considered in deep learning for image-to-image regression tasks. Since normal neural networks are data intensive and cannot provide predictive uncertainty, we propose a Bayesian approach to convolutional neural nets. A recently introduced variational gradient descent algorithm based on Stein's method is scaled to deep convolutional networks to perform approximate Bayesian inference on millions of uncertain network parameters. This approach achieves state of the art performance in terms of predictive accuracy and uncertainty quantification in comparison to other approaches in Bayesian neural networks as well as techniques that include Gaussian processes and ensemble methods even when the training data size is relatively small. To evaluate the performance of this approach, we consider standard uncertainty quantification benchmark problems including flow in heterogeneous media defined in terms of limited data-driven permeability realizations. The performance of the surrogate model developed is very good even though there is no underlying structure shared between the input (permeability) and output (flow/pressure) fields as is often the case in the image-to-image regression models used in computer vision problems. Studies are performed with an underlying stochastic input dimensionality up to $4,225$ where most other uncertainty quantification methods fail. Uncertainty propagation tasks are considered and the predictive output Bayesian statistics are compared to those obtained with Monte Carlo estimates.



