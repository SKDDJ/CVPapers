# Arxiv Papers in cs.CV on 2018-01-07
### SBNet: Sparse Blocks Network for Fast Inference
- **Arxiv ID**: http://arxiv.org/abs/1801.02108v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.02108v2)
- **Published**: 2018-01-07 01:03:25+00:00
- **Updated**: 2018-06-07 14:44:00+00:00
- **Authors**: Mengye Ren, Andrei Pokrovsky, Bin Yang, Raquel Urtasun
- **Comment**: 10 pages, CVPR 2018
- **Journal**: None
- **Summary**: Conventional deep convolutional neural networks (CNNs) apply convolution operators uniformly in space across all feature maps for hundreds of layers - this incurs a high computational cost for real-time applications. For many problems such as object detection and semantic segmentation, we are able to obtain a low-cost computation mask, either from a priori problem knowledge, or from a low-resolution segmentation network. We show that such computation masks can be used to reduce computation in the high-resolution main network. Variants of sparse activation CNNs have previously been explored on small-scale tasks and showed no degradation in terms of object classification accuracy, but often measured gains in terms of theoretical FLOPs without realizing a practical speed-up when compared to highly optimized dense convolution implementations. In this work, we leverage the sparsity structure of computation masks and propose a novel tiling-based sparse convolution algorithm. We verified the effectiveness of our sparse CNN on LiDAR-based 3D object detection, and we report significant wall-clock speed-ups compared to dense convolution without noticeable loss of accuracy.



### Convex Relaxations for Pose Graph Optimization with Outliers
- **Arxiv ID**: http://arxiv.org/abs/1801.02112v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, 68W01, 68W40, 68W25, 49K30, I.2.9, G.1.6
- **Links**: [PDF](http://arxiv.org/pdf/1801.02112v1)
- **Published**: 2018-01-07 02:18:28+00:00
- **Updated**: 2018-01-07 02:18:28+00:00
- **Authors**: Luca Carlone, Giuseppe C. Calafiore
- **Comment**: 10 pages, 5 figures, accepted for publication in the IEEE Robotics
  and Automation Letters, 2018
- **Journal**: None
- **Summary**: Pose Graph Optimization involves the estimation of a set of poses from pairwise measurements and provides a formalization for many problems arising in mobile robotics and geometric computer vision. In this paper, we consider the case in which a subset of the measurements fed to pose graph optimization is spurious. Our first contribution is to develop robust estimators that can cope with heavy-tailed measurement noise, hence increasing robustness to the presence of outliers. Since the resulting estimators require solving nonconvex optimization problems, we further develop convex relaxations that approximately solve those problems via semidefinite programming. We then provide conditions under which the proposed relaxations are exact. Contrarily to existing approaches, our convex relaxations do not rely on the availability of an initial guess for the unknown poses, hence they are more suitable for setups in which such guess is not available (e.g., multi-robot localization, recovery after localization failure). We tested the proposed techniques in extensive simulations, and we show that some of the proposed relaxations are indeed tight (i.e., they solve the original nonconvex problem 10 exactly) and ensure accurate estimation in the face of a large number of outliers.



### Architecture Based Classification of Leaf Images
- **Arxiv ID**: http://arxiv.org/abs/1801.02121v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.02121v1)
- **Published**: 2018-01-07 03:26:30+00:00
- **Updated**: 2018-01-07 03:26:30+00:00
- **Authors**: Mahmoud Sadeghi, Ali Zakerolhosseini, Ali Sonboli
- **Comment**: 28 pages, 17 figures
- **Journal**: None
- **Summary**: Plant classification and identification has so far been an important and difficult task. In this paper, an efficient and systematic approach for extracting the leaf architecture characters from captured digital images is proposed. The input image is first pre-processed in five steps to be prepared for feature extraction. In the second stage, methods for extracting different architectural features are studied using various mathematical and computational methods. Also, the classification rules for mapping the calculated values of each feature to semantic botanical terms in proposed. Compared with previous studies, the proposed method combines extracted features of an image with specific knowledge of leaf architecture in the domain of botany to provide a comprehensive framework for both computer engineers and botanist. Finally, Based on the proposed method, experiments on the classification of the ImagerCLEF 2012 dataset has been performed with promising results.



### Applications of a Graph Theoretic Based Clustering Framework in Computer Vision and Pattern Recognition
- **Arxiv ID**: http://arxiv.org/abs/1802.02181v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02181v1)
- **Published**: 2018-01-07 09:35:12+00:00
- **Updated**: 2018-01-07 09:35:12+00:00
- **Authors**: Yonatan Tariku Tesfaye
- **Comment**: doctoral dissertation
- **Journal**: None
- **Summary**: Recently, several clustering algorithms have been used to solve variety of problems from different discipline. This dissertation aims to address different challenging tasks in computer vision and pattern recognition by casting the problems as a clustering problem. We proposed novel approaches to solve multi-target tracking, visual geo-localization and outlier detection problems using a unified underlining clustering framework, i.e., dominant set clustering and its extensions, and presented a superior result over several state-of-the-art approaches.



### Detection and segmentation of the Left Ventricle in Cardiac MRI using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1801.02171v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.02171v1)
- **Published**: 2018-01-07 11:22:12+00:00
- **Updated**: 2018-01-07 11:22:12+00:00
- **Authors**: Alexandre Attia, Sharone Dayan
- **Comment**: None
- **Journal**: None
- **Summary**: Manual segmentation of the Left Ventricle (LV) is a tedious and meticulous task that can vary depending on the patient, the Magnetic Resonance Images (MRI) cuts and the experts. Still today, we consider manual delineation done by experts as being the ground truth for cardiac diagnosticians. Thus, we are reviewing the paper - written by Avendi and al. - who presents a combined approach with Convolutional Neural Networks, Stacked Auto-Encoders and Deformable Models, to try and automate the segmentation while performing more accurately. Furthermore, we have implemented parts of the paper (around three quarts) and experimented both the original method and slightly modified versions when changing the architecture and the parameters.



### Approximate FPGA-based LSTMs under Computation Time Constraints
- **Arxiv ID**: http://arxiv.org/abs/1801.02190v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.02190v1)
- **Published**: 2018-01-07 13:46:03+00:00
- **Updated**: 2018-01-07 13:46:03+00:00
- **Authors**: Michalis Rizakis, Stylianos I. Venieris, Alexandros Kouris, Christos-Savvas Bouganis
- **Comment**: Accepted at the 14th International Symposium in Applied
  Reconfigurable Computing (ARC) 2018
- **Journal**: None
- **Summary**: Recurrent Neural Networks and in particular Long Short-Term Memory (LSTM) networks have demonstrated state-of-the-art accuracy in several emerging Artificial Intelligence tasks. However, the models are becoming increasingly demanding in terms of computational and memory load. Emerging latency-sensitive applications including mobile robots and autonomous vehicles often operate under stringent computation time constraints. In this paper, we address the challenge of deploying computationally demanding LSTMs at a constrained time budget by introducing an approximate computing scheme that combines iterative low-rank compression and pruning, along with a novel FPGA-based LSTM architecture. Combined in an end-to-end framework, the approximation method's parameters are optimised and the architecture is configured to address the problem of high-performance LSTM execution in time-constrained applications. Quantitative evaluation on a real-life image captioning application indicates that the proposed methods required up to 6.5x less time to achieve the same application-level accuracy compared to a baseline method, while achieving an average of 25x higher accuracy under the same computation time constraints.



### Cross-modal Embeddings for Video and Audio Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1801.02200v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1801.02200v1)
- **Published**: 2018-01-07 15:43:22+00:00
- **Updated**: 2018-01-07 15:43:22+00:00
- **Authors**: Didac Surís, Amanda Duarte, Amaia Salvador, Jordi Torres, Xavier Giró-i-Nieto
- **Comment**: 6 pages, 3 figures
- **Journal**: None
- **Summary**: The increasing amount of online videos brings several opportunities for training self-supervised neural networks. The creation of large scale datasets of videos such as the YouTube-8M allows us to deal with this large amount of data in manageable way. In this work, we find new ways of exploiting this dataset by taking advantage of the multi-modal information it provides. By means of a neural network, we are able to create links between audio and visual documents, by projecting them into a common region of the feature space, obtaining joint audio-visual embeddings. These links are used to retrieve audio samples that fit well to a given silent video, and also to retrieve images that match a given a query audio. The results in terms of Recall@K obtained over a subset of YouTube-8M videos show the potential of this unsupervised approach for cross-modal feature learning. We train embeddings for both scales and assess their quality in a retrieval problem, formulated as using the feature extracted from one modality to retrieve the most similar videos based on the features computed in the other modality.



### Foreground Segmentation Using a Triplet Convolutional Neural Network for Multiscale Feature Encoding
- **Arxiv ID**: http://arxiv.org/abs/1801.02225v1
- **DOI**: 10.1016/j.patrec.2018.08.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.02225v1)
- **Published**: 2018-01-07 18:33:43+00:00
- **Updated**: 2018-01-07 18:33:43+00:00
- **Authors**: Long Ang Lim, Hacer Yalim Keles
- **Comment**: This paper is under consideration at Pattern Recognition Letters
- **Journal**: None
- **Summary**: A common approach for moving objects segmentation in a scene is to perform a background subtraction. Several methods have been proposed in this domain. However, they lack the ability of handling various difficult scenarios such as illumination changes, background or camera motion, camouflage effect, shadow etc. To address these issues, we propose a robust and flexible encoder-decoder type neural network based approach. We adapt a pre-trained convolutional network, i.e. VGG-16 Net, under a triplet framework in the encoder part to embed an image in multiple scales into the feature space and use a transposed convolutional network in the decoder part to learn a mapping from feature space to image space. We train this network end-to-end by using only a few training samples. Our network takes an RGB image in three different scales and produces a foreground segmentation probability mask for the corresponding image. In order to evaluate our model, we entered the Change Detection 2014 Challenge (changedetection.net) and our method outperformed all the existing state-of-the-art methods by an average F-Measure of 0.9770. Our source code will be made publicly available at https://github.com/lim-anggun/FgSegNet.



### Graph Autoencoder-Based Unsupervised Feature Selection with Broad and Local Data Structure Preservation
- **Arxiv ID**: http://arxiv.org/abs/1801.02251v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.02251v2)
- **Published**: 2018-01-07 21:14:01+00:00
- **Updated**: 2018-04-21 03:14:38+00:00
- **Authors**: Siwei Feng, Marco F. Duarte
- **Comment**: None
- **Journal**: None
- **Summary**: Feature selection is a dimensionality reduction technique that selects a subset of representative features from high dimensional data by eliminating irrelevant and redundant features. Recently, feature selection combined with sparse learning has attracted significant attention due to its outstanding performance compared with traditional feature selection methods that ignores correlation between features. These works first map data onto a low-dimensional subspace and then select features by posing a sparsity constraint on the transformation matrix. However, they are restricted by design to linear data transformation, a potential drawback given that the underlying correlation structures of data are often non-linear. To leverage a more sophisticated embedding, we propose an autoencoder-based unsupervised feature selection approach that leverages a single-layer autoencoder for a joint framework of feature selection and manifold learning. More specifically, we enforce column sparsity on the weight matrix connecting the input layer and the hidden layer, as in previous work. Additionally, we include spectral graph analysis on the projected data into the learning process to achieve local data geometry preservation from the original data space to the low-dimensional feature space. Extensive experiments are conducted on image, audio, text, and biological data. The promising experimental results validate the superiority of the proposed method.



### Anatomical Data Augmentation For CNN based Pixel-wise Classification
- **Arxiv ID**: http://arxiv.org/abs/1801.02261v1
- **DOI**: 10.1109/ISBI.2018.8363762
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.02261v1)
- **Published**: 2018-01-07 23:00:02+00:00
- **Updated**: 2018-01-07 23:00:02+00:00
- **Authors**: Avi Ben-Cohen, Eyal Klang, Michal Marianne Amitai, Jacob Goldberger, Hayit Greenspan
- **Comment**: To be presented at IEEE ISBI 2018
- **Journal**: None
- **Summary**: In this work we propose a method for anatomical data augmentation that is based on using slices of computed tomography (CT) examinations that are adjacent to labeled slices as another resource of labeled data for training the network. The extended labeled data is used to train a U-net network for a pixel-wise classification into different hepatic lesions and normal liver tissues. Our dataset contains CT examinations from 140 patients with 333 CT images annotated by an expert radiologist. We tested our approach and compared it to the conventional training process. Results indicate superiority of our method. Using the anatomical data augmentation we achieved an improvement of 3% in the success rate, 5% in the classification accuracy, and 4% in Dice.



