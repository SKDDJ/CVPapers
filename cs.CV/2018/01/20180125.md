# Arxiv Papers in cs.CV on 2018-01-25
### Personalized Human Activity Recognition Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.08252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08252v1)
- **Published**: 2018-01-25 01:35:25+00:00
- **Updated**: 2018-01-25 01:35:25+00:00
- **Authors**: Seyed Ali Rokni, Marjan Nourollahi, Hassan Ghasemzadeh
- **Comment**: None
- **Journal**: None
- **Summary**: A major barrier to the personalized Human Activity Recognition using wearable sensors is that the performance of the recognition model drops significantly upon adoption of the system by new users or changes in physical/ behavioral status of users. Therefore, the model needs to be retrained by collecting new labeled data in the new context. In this study, we develop a transfer learning framework using convolutional neural networks to build a personalized activity recognition model with minimal user supervision.



### Visual Weather Temperature Prediction
- **Arxiv ID**: http://arxiv.org/abs/1801.08267v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08267v1)
- **Published**: 2018-01-25 03:37:02+00:00
- **Updated**: 2018-01-25 03:37:02+00:00
- **Authors**: Wei-Ta Chu, Kai-Chia Ho, Ali Borji
- **Comment**: 8 pages, accepted to WACV 2018
- **Journal**: None
- **Summary**: In this paper, we attempt to employ convolutional recurrent neural networks for weather temperature estimation using only image data. We study ambient temperature estimation based on deep neural networks in two scenarios a) estimating temperature of a single outdoor image, and b) predicting temperature of the last image in an image sequence. In the first scenario, visual features are extracted by a convolutional neural network trained on a large-scale image dataset. We demonstrate that promising performance can be obtained, and analyze how volume of training data influences performance. In the second scenario, we consider the temporal evolution of visual appearance, and construct a recurrent neural network to predict the temperature of the last image in a given image sequence. We obtain better prediction accuracy compared to the state-of-the-art models. Further, we investigate how performance varies when information is extracted from different scene regions, and when images are captured in different daytime hours. Our approach further reinforces the idea of using only visual information for cost efficient weather prediction in the future.



### A Tutorial on Modeling and Inference in Undirected Graphical Models for Hyperspectral Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1801.08268v1
- **DOI**: 10.1080/01431161.2018.1465614
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1801.08268v1)
- **Published**: 2018-01-25 03:44:43+00:00
- **Updated**: 2018-01-25 03:44:43+00:00
- **Authors**: Utsav B. Gewali, Sildomar T. Monteiro
- **Comment**: None
- **Journal**: None
- **Summary**: Undirected graphical models have been successfully used to jointly model the spatial and the spectral dependencies in earth observing hyperspectral images. They produce less noisy, smooth, and spatially coherent land cover maps and give top accuracies on many datasets. Moreover, they can easily be combined with other state-of-the-art approaches, such as deep learning. This has made them an essential tool for remote sensing researchers and practitioners. However, graphical models have not been easily accessible to the larger remote sensing community as they are not discussed in standard remote sensing textbooks and not included in the popular remote sensing software and toolboxes. In this tutorial, we provide a theoretical introduction to Markov random fields and conditional random fields based spatial-spectral classification for land cover mapping along with a detailed step-by-step practical guide on applying these methods using freely available software. Furthermore, the discussed methods are benchmarked on four public hyperspectral datasets for a fair comparison among themselves and easy comparison with the vast number of methods in literature which use the same datasets. The source code necessary to reproduce all the results in the paper is published on-line to make it easier for the readers to apply these techniques to different remote sensing problems.



### NDDR-CNN: Layerwise Feature Fusing in Multi-Task CNNs by Neural Discriminative Dimensionality Reduction
- **Arxiv ID**: http://arxiv.org/abs/1801.08297v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.08297v4)
- **Published**: 2018-01-25 07:38:52+00:00
- **Updated**: 2019-04-04 23:24:48+00:00
- **Authors**: Yuan Gao, Jiayi Ma, Mingbo Zhao, Wei Liu, Alan L. Yuille
- **Comment**: 11 pages, 3 figures, 9 tables
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition, 2019
- **Summary**: In this paper, we propose a novel Convolutional Neural Network (CNN) structure for general-purpose multi-task learning (MTL), which enables automatic feature fusing at every layer from different tasks. This is in contrast with the most widely used MTL CNN structures which empirically or heuristically share features on some specific layers (e.g., share all the features except the last convolutional layer). The proposed layerwise feature fusing scheme is formulated by combining existing CNN components in a novel way, with clear mathematical interpretability as discriminative dimensionality reduction, which is referred to as Neural Discriminative Dimensionality Reduction (NDDR). Specifically, we first concatenate features with the same spatial resolution from different tasks according to their channel dimension. Then, we show that the discriminative dimensionality reduction can be fulfilled by 1x1 Convolution, Batch Normalization, and Weight Decay in one CNN. The use of existing CNN components ensures the end-to-end training and the extensibility of the proposed NDDR layer to various state-of-the-art CNN architectures in a "plug-and-play" manner. The detailed ablation analysis shows that the proposed NDDR layer is easy to train and also robust to different hyperparameters. Experiments on different task sets with various base network architectures demonstrate the promising performance and desirable generalizability of our proposed method. The code of our paper is available at https://github.com/ethanygao/NDDR-CNN.



### Class label autoencoder for zero-shot learning
- **Arxiv ID**: http://arxiv.org/abs/1801.08301v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08301v1)
- **Published**: 2018-01-25 08:00:36+00:00
- **Updated**: 2018-01-25 08:00:36+00:00
- **Authors**: Guangfeng Lin, Caixia Fan, Wanjun Chen, Yajun Chen, Fan Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Existing zero-shot learning (ZSL) methods usually learn a projection function between a feature space and a semantic embedding space(text or attribute space) in the training seen classes or testing unseen classes. However, the projection function cannot be used between the feature space and multi-semantic embedding spaces, which have the diversity characteristic for describing the different semantic information of the same class. To deal with this issue, we present a novel method to ZSL based on learning class label autoencoder (CLA). CLA can not only build a uniform framework for adapting to multi-semantic embedding spaces, but also construct the encoder-decoder mechanism for constraining the bidirectional projection between the feature space and the class label space. Moreover, CLA can jointly consider the relationship of feature classes and the relevance of the semantic classes for improving zero-shot classification. The CLA solution can provide both unseen class labels and the relation of the different classes representation(feature or semantic information) that can encode the intrinsic structure of classes. Extensive experiments demonstrate the CLA outperforms state-of-art methods on four benchmark datasets, which are AwA, CUB, Dogs and ImNet-2.



### SocialML: machine learning for social media video creators
- **Arxiv ID**: http://arxiv.org/abs/1802.02204v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1802.02204v1)
- **Published**: 2018-01-25 08:15:54+00:00
- **Updated**: 2018-01-25 08:15:54+00:00
- **Authors**: Tomasz Trzcinski, Adam Bielski, Pawe≈Ç Cyrta, Matthew Zak
- **Comment**: 2pages, 6 figures
- **Journal**: None
- **Summary**: In the recent years, social media have become one of the main places where creative content is being published and consumed by billions of users. Contrary to traditional media, social media allow the publishers to receive almost instantaneous feedback regarding their creative work at an unprecedented scale. This is a perfect use case for machine learning methods that can use these massive amounts of data to provide content creators with inspirational ideas and constructive criticism of their work. In this work, we present a comprehensive overview of machine learning-empowered tools we developed for video creators at Group Nine Media - one of the major social media companies that creates short-form videos with over three billion views per month. Our main contribution is a set of tools that allow the creators to leverage massive amounts of data to improve their creation process, evaluate their videos before the publication and improve content quality. These applications include an interactive conversational bot that allows access to material archives, a Web-based application for automatic selection of optimal video thumbnail, as well as deep learning methods for optimizing headline and predicting video popularity. Our A/B tests show that deployment of our tools leads to significant increase of average video view count by 12.9%. Our additional contribution is a set of considerations collected during the deployment of those tools that can hel



### Phonocardiographic Sensing using Deep Learning for Abnormal Heartbeat Detection
- **Arxiv ID**: http://arxiv.org/abs/1801.08322v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08322v4)
- **Published**: 2018-01-25 09:25:41+00:00
- **Updated**: 2020-07-28 01:48:35+00:00
- **Authors**: Siddique Latif, Muhammad Usman, Rajib Rana, Junaid Qadir
- **Comment**: None
- **Journal**: IEEE Sensors Journal 2018
- **Summary**: Cardiac auscultation involves expert interpretation of abnormalities in heart sounds using stethoscope. Deep learning based cardiac auscultation is of significant interest to the healthcare community as it can help reducing the burden of manual auscultation with automated detection of abnormal heartbeats. However, the problem of automatic cardiac auscultation is complicated due to the requirement of reliability and high accuracy, and due to the presence of background noise in the heartbeat sound. In this work, we propose a Recurrent Neural Networks (RNNs) based automated cardiac auscultation solution. Our choice of RNNs is motivated by the great success of deep learning in medical applications and by the observation that RNNs represent the deep learning configuration most suitable for dealing with sequential or temporal data even in the presence of noise. We explore the use of various RNN models, and demonstrate that these models deliver the abnormal heartbeat classification score with significant improvement. Our proposed approach using RNNs can be potentially be used for real-time abnormal heartbeat detection in the Internet of Medical Things for remote monitoring applications.



### Using Deep Autoencoders for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1801.08329v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08329v1)
- **Published**: 2018-01-25 10:06:01+00:00
- **Updated**: 2018-01-25 10:06:01+00:00
- **Authors**: Muhammad Usman, Siddique Latif, Junaid Qadir
- **Comment**: None
- **Journal**: None
- **Summary**: Feature descriptors involved in image processing are generally manually chosen and high dimensional in nature. Selecting the most important features is a very crucial task for systems like facial expression recognition. This paper investigates the performance of deep autoencoders for feature selection and dimension reduction for facial expression recognition on multiple levels of hidden layers. The features extracted from the stacked autoencoder outperformed when compared to other state-of-the-art feature selection and dimension reduction techniques.



### Dual Asymmetric Deep Hashing Learning
- **Arxiv ID**: http://arxiv.org/abs/1801.08360v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08360v1)
- **Published**: 2018-01-25 11:31:29+00:00
- **Updated**: 2018-01-25 11:31:29+00:00
- **Authors**: Jinxing Li, Bob Zhang, Guangming Lu, David Zhang
- **Comment**: 12 pages, 6 figures, 7 tables, 37 conferences
- **Journal**: None
- **Summary**: Due to the impressive learning power, deep learning has achieved a remarkable performance in supervised hash function learning. In this paper, we propose a novel asymmetric supervised deep hashing method to preserve the semantic structure among different categories and generate the binary codes simultaneously. Specifically, two asymmetric deep networks are constructed to reveal the similarity between each pair of images according to their semantic labels. The deep hash functions are then learned through two networks by minimizing the gap between the learned features and discrete codes. Furthermore, since the binary codes in the Hamming space also should keep the semantic affinity existing in the original space, another asymmetric pairwise loss is introduced to capture the similarity between the binary codes and real-value features. This asymmetric loss not only improves the retrieval performance, but also contributes to a quick convergence at the training phase. By taking advantage of the two-stream deep structures and two types of asymmetric pairwise functions, an alternating algorithm is designed to optimize the deep features and high-quality binary codes efficiently. Experimental results on three real-world datasets substantiate the effectiveness and superiority of our approach as compared with state-of-the-art.



### Collaborative Large-Scale Dense 3D Reconstruction with Online Inter-Agent Pose Optimisation
- **Arxiv ID**: http://arxiv.org/abs/1801.08361v2
- **DOI**: 10.1109/TVCG.2018.2868533
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1801.08361v2)
- **Published**: 2018-01-25 11:31:44+00:00
- **Updated**: 2019-07-02 15:00:40+00:00
- **Authors**: Stuart Golodetz, Tommaso Cavallari, Nicholas A Lord, Victor A Prisacariu, David W Murray, Philip H S Torr
- **Comment**: Stuart Golodetz, Tommaso Cavallari and Nicholas Lord assert joint
  first authorship
- **Journal**: IEEE Transactions on Visualization and Computer Graphics
  24(11):2895-2905, 2018
- **Summary**: Reconstructing dense, volumetric models of real-world 3D scenes is important for many tasks, but capturing large scenes can take significant time, and the risk of transient changes to the scene goes up as the capture time increases. These are good reasons to want instead to capture several smaller sub-scenes that can be joined to make the whole scene. Achieving this has traditionally been difficult: joining sub-scenes that may never have been viewed from the same angle requires a high-quality camera relocaliser that can cope with novel poses, and tracking drift in each sub-scene can prevent them from being joined to make a consistent overall scene. Recent advances, however, have significantly improved our ability to capture medium-sized sub-scenes with little to no tracking drift: real-time globally consistent reconstruction systems can close loops and re-integrate the scene surface on the fly, whilst new visual-inertial odometry approaches can significantly reduce tracking drift during live reconstruction. Moreover, high-quality regression forest-based relocalisers have recently been made more practical by the introduction of a method to allow them to be trained and used online. In this paper, we leverage these advances to present what to our knowledge is the first system to allow multiple users to collaborate interactively to reconstruct dense, voxel-based models of whole buildings using only consumer-grade hardware, a task that has traditionally been both time-consuming and dependent on the availability of specialised hardware. Using our system, an entire house or lab can be reconstructed in under half an hour and at a far lower cost than was previously possible.



### A Benchmark and Evaluation of Non-Rigid Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/1801.08388v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08388v3)
- **Published**: 2018-01-25 12:59:09+00:00
- **Updated**: 2020-11-11 15:03:42+00:00
- **Authors**: Sebastian Hoppe Nesgaard Jensen, Mads Emil Brix Doest, Henrik Aanaes, Alessio Del Bue
- **Comment**: Accepted, International Journal of Computer Vision (IJCV)
- **Journal**: International Journal of Computer Vision (IJCV), 2020
- **Summary**: Non-Rigid structure from motion (NRSfM), is a long standing and central problem in computer vision and its solution is necessary for obtaining 3D information from multiple images when the scene is dynamic. A main issue regarding the further development of this important computer vision topic, is the lack of high quality data sets. We here address this issue by presenting a data set created for this purpose, which is made publicly available, and considerably larger than the previous state of the art. To validate the applicability of this data set, and provide an investigation into the state of the art of NRSfM, including potential directions forward, we here present a benchmark and a scrupulous evaluation using this data set. This benchmark evaluates 18 different methods with available code that reasonably spans the state of the art in sparse NRSfM. This new public data set and evaluation protocol will provide benchmark tools for further development in this challenging field.



### Global and Local Consistent Age Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.08390v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08390v1)
- **Published**: 2018-01-25 13:04:13+00:00
- **Updated**: 2018-01-25 13:04:13+00:00
- **Authors**: Peipei Li, Yibo Hu, Qi Li, Ran He, Zhenan Sun
- **Comment**: 6 pages, 8 figures, submitted to ICPR 2018
- **Journal**: None
- **Summary**: Age progression/regression is a challenging task due to the complicated and non-linear transformation in human aging process. Many researches have shown that both global and local facial features are essential for face representation, but previous GAN based methods mainly focused on the global feature in age synthesis. To utilize both global and local facial information, we propose a Global and Local Consistent Age Generative Adversarial Network (GLCA-GAN). In our generator, a global network learns the whole facial structure and simulates the aging trend of the whole face, while three crucial facial patches are progressed or regressed by three local networks aiming at imitating subtle changes of crucial facial subregions. To preserve most of the details in age-attribute-irrelevant areas, our generator learns the residual face. Moreover, we employ an identity preserving loss to better preserve the identity information, as well as age preserving loss to enhance the accuracy of age synthesis. A pixel loss is also adopted to preserve detailed facial information of the input face. Our proposed method is evaluated on three face aging datasets, i.e., CACD dataset, Morph dataset and FG-NET dataset. Experimental results show appealing performance of the proposed method by comparing with the state-of-the-art.



### Understanding Human Behaviors in Crowds by Imitating the Decision-Making Process
- **Arxiv ID**: http://arxiv.org/abs/1801.08391v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08391v1)
- **Published**: 2018-01-25 13:08:43+00:00
- **Updated**: 2018-01-25 13:08:43+00:00
- **Authors**: Haosheng Zou, Hang Su, Shihong Song, Jun Zhu
- **Comment**: accepted to AAAI-18
- **Journal**: None
- **Summary**: Crowd behavior understanding is crucial yet challenging across a wide range of applications, since crowd behavior is inherently determined by a sequential decision-making process based on various factors, such as the pedestrians' own destinations, interaction with nearby pedestrians and anticipation of upcoming events. In this paper, we propose a novel framework of Social-Aware Generative Adversarial Imitation Learning (SA-GAIL) to mimic the underlying decision-making process of pedestrians in crowds. Specifically, we infer the latent factors of human decision-making process in an unsupervised manner by extending the Generative Adversarial Imitation Learning framework to anticipate future paths of pedestrians. Different factors of human decision making are disentangled with mutual information maximization, with the process modeled by collision avoidance regularization and Social-Aware LSTMs. Experimental results demonstrate the potential of our framework in disentangling the latent decision-making factors of pedestrians and stronger abilities in predicting future trajectories.



### C2MSNet: A Novel approach for single image haze removal
- **Arxiv ID**: http://arxiv.org/abs/1801.08406v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08406v1)
- **Published**: 2018-01-25 14:16:14+00:00
- **Updated**: 2018-01-25 14:16:14+00:00
- **Authors**: Akshay Dudhane, Subrahmanyam Murala
- **Comment**: Accepted in Winter Conference on Applications of Computer Vision
  (WACV-2018)
- **Journal**: None
- **Summary**: Degradation of image quality due to the presence of haze is a very common phenomenon. Existing DehazeNet [3], MSCNN [11] tackled the drawbacks of hand crafted haze relevant features. However, these methods have the problem of color distortion in gloomy (poor illumination) environment. In this paper, a cardinal (red, green and blue) color fusion network for single image haze removal is proposed. In first stage, network fusses color information present in hazy images and generates multi-channel depth maps. The second stage estimates the scene transmission map from generated dark channels using multi channel multi scale convolutional neural network (McMs-CNN) to recover the original scene. To train the proposed network, we have used two standard datasets namely: ImageNet [5] and D-HAZY [1]. Performance evaluation of the proposed approach has been carried out using structural similarity index (SSIM), mean square error (MSE) and peak signal to noise ratio (PSNR). Performance analysis shows that the proposed approach outperforms the existing state-of-the-art methods for single image dehazing.



### Identifying Corresponding Patches in SAR and Optical Images with a Pseudo-Siamese CNN
- **Arxiv ID**: http://arxiv.org/abs/1801.08467v1
- **DOI**: 10.1109/LGRS.2018.2799232
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1801.08467v1)
- **Published**: 2018-01-25 16:12:38+00:00
- **Updated**: 2018-01-25 16:12:38+00:00
- **Authors**: Lloyd H. Hughes, Michael Schmitt, Lichao Mou, Yuanyuan Wang, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: In this letter, we propose a pseudo-siamese convolutional neural network (CNN) architecture that enables to solve the task of identifying corresponding patches in very-high-resolution (VHR) optical and synthetic aperture radar (SAR) remote sensing imagery. Using eight convolutional layers each in two parallel network streams, a fully connected layer for the fusion of the features learned in each stream, and a loss function based on binary cross-entropy, we achieve a one-hot indication if two patches correspond or not. The network is trained and tested on an automatically generated dataset that is based on a deterministic alignment of SAR and optical imagery via previously reconstructed and subsequently co-registered 3D point clouds. The satellite images, from which the patches comprising our dataset are extracted, show a complex urban scene containing many elevated objects (i.e. buildings), thus providing one of the most difficult experimental environments. The achieved results show that the network is able to predict corresponding patches with high accuracy, thus indicating great potential for further development towards a generalized multi-sensor key-point matching procedure. Index Terms-synthetic aperture radar (SAR), optical imagery, data fusion, deep learning, convolutional neural networks (CNN), image matching, deep matching



### Convolutional Invasion and Expansion Networks for Tumor Growth Prediction
- **Arxiv ID**: http://arxiv.org/abs/1801.08468v1
- **DOI**: 10.1109/TMI.2017.2774044
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08468v1)
- **Published**: 2018-01-25 16:13:29+00:00
- **Updated**: 2018-01-25 16:13:29+00:00
- **Authors**: Ling Zhang, Le Lu, Ronald M. Summers, Electron Kebebew, Jianhua Yao
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging, 15 November 2017, Volume:PP
  Issue: 99
- **Summary**: Tumor growth is associated with cell invasion and mass-effect, which are traditionally formulated by mathematical models, namely reaction-diffusion equations and biomechanics. Such models can be personalized based on clinical measurements to build the predictive models for tumor growth. In this paper, we investigate the possibility of using deep convolutional neural networks (ConvNets) to directly represent and learn the cell invasion and mass-effect, and to predict the subsequent involvement regions of a tumor. The invasion network learns the cell invasion from information related to metabolic rate, cell density and tumor boundary derived from multimodal imaging data. The expansion network models the mass-effect from the growing motion of tumor mass. We also study different architectures that fuse the invasion and expansion networks, in order to exploit the inherent correlations among them. Our network can easily be trained on population data and personalized to a target patient, unlike most previous mathematical modeling methods that fail to incorporate population data. Quantitative experiments on a pancreatic tumor data set show that the proposed method substantially outperforms a state-of-the-art mathematical model-based approach in both accuracy and efficiency, and that the information captured by each of the two subnetworks are complementary.



### An Integrated Soft Computing Approach to a Multi-biometric Security Model
- **Arxiv ID**: http://arxiv.org/abs/1801.08480v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1801.08480v1)
- **Published**: 2018-01-25 16:54:16+00:00
- **Updated**: 2018-01-25 16:54:16+00:00
- **Authors**: Prem Sewak Sudhish
- **Comment**: Ph.D. thesis of Prem Sewak Sudhish, Dayalbagh Educational Institute.
  The thesis has been formatted for duplex printing
- **Journal**: None
- **Summary**: The abstract of the thesis consists of three sections, videlicet,   Motivation   Chapter Organization   Salient Contributions.   The complete abstract is included with the thesis. The final section on Salient Contributions is reproduced below.   Salient Contributions   The research presents the following salient contributions:   i. A novel technique has been developed for comparing biographical information, by combining the average impact of Levenshtein, Damerau-Levenshtein, and editor distances. The impact is calculated as the ratio of the edit distance to the maximum possible edit distance between two strings of the same lengths as the given pair of strings. This impact lies in the range [0, 1] and can easily be converted to a similarity (matching) score by subtracting the impact from unity.   ii. A universal soft computing framework is proposed for adaptively fusing biometric and biographical information by making real-time decisions to determine after consideration of each individual identifier whether computation of matching scores and subsequent fusion of additional identifiers, including biographical information is required. This proposed framework not only improves the accuracy of the system by fusing less reliable information (e.g. biographical information) only for instances where such a fusion is required, but also improves the efficiency of the system by computing matching scores for various available identifiers only when this computation is considered necessary.   iii. A scientific method for comparing efficiency of fusion strategies through a predicted effort to error trade-off curve.



### Self-Learning to Detect and Segment Cysts in Lung CT Images without Manual Annotation
- **Arxiv ID**: http://arxiv.org/abs/1801.08486v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08486v1)
- **Published**: 2018-01-25 17:02:29+00:00
- **Updated**: 2018-01-25 17:02:29+00:00
- **Authors**: Ling Zhang, Vissagan Gopalakrishnan, Le Lu, Ronald M. Summers, Joel Moss, Jianhua Yao
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: Image segmentation is a fundamental problem in medical image analysis. In recent years, deep neural networks achieve impressive performances on many medical image segmentation tasks by supervised learning on large manually annotated data. However, expert annotations on big medical datasets are tedious, expensive or sometimes unavailable. Weakly supervised learning could reduce the effort for annotation but still required certain amounts of expertise. Recently, deep learning shows a potential to produce more accurate predictions than the original erroneous labels. Inspired by this, we introduce a very weakly supervised learning method, for cystic lesion detection and segmentation in lung CT images, without any manual annotation. Our method works in a self-learning manner, where segmentation generated in previous steps (first by unsupervised segmentation then by neural networks) is used as ground truth for the next level of network learning. Experiments on a cystic lung lesion dataset show that the deep learning could perform better than the initial unsupervised annotation, and progressively improve itself after self-learning.



### Unmixing urban hyperspectral imagery with a Gaussian mixture model on endmember variability
- **Arxiv ID**: http://arxiv.org/abs/1801.08513v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08513v1)
- **Published**: 2018-01-25 18:22:22+00:00
- **Updated**: 2018-01-25 18:22:22+00:00
- **Authors**: Yuan Zhou, Erin B. Wetherley, Paul D. Gader
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we model a pixel as a linear combination of endmembers sampled from probability distributions of Gaussian mixture models (GMM). The parameters of the GMM distributions are estimated using spectral libraries. Abundances are estimated based on the distribution parameters. The advantage of this algorithm is that the model size grows very slowly as a function of the library size. To validate this method, we used data collected by the AVIRIS sensor over the Santa Barbara region: two 16 m spatial resolution and two 4 m spatial resolution images. 64 validated regions of interest (ROI) (180 m by 180 m) were used to assess estimate accuracy. Ground truth was obtained using 1 m images leading to the following 6 classes: turfgrass, non-photosynthetic vegetation (NPV), paved, roof, soil, and tree. Spectral libraries were built by manually identifying and extracting pure spectra from both resolution images, resulting in 3,287 spectra at 16 m and 15,426 spectra at 4 m. We then unmixed ROIs of each resolution using the following unmixing algorithms: the set-based algorithms MESMA and AAM, and the distribution-based algorithms GMM, NCM, and BCM. The original libraries were used for the distribution-based algorithms whereas set-based methods required a sophisticated reduction method, resulting in reduced libraries of 61 spectra at 16 m and 95 spectra at 4 m. The results show that GMM performs best among the distribution-based methods, producing comparable accuracy to MESMA, and may be more robust across datasets.



### Deep Learning for End-to-End Automatic Target Recognition from Synthetic Aperture Radar Imagery
- **Arxiv ID**: http://arxiv.org/abs/1801.08558v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08558v1)
- **Published**: 2018-01-25 19:05:38+00:00
- **Updated**: 2018-01-25 19:05:38+00:00
- **Authors**: Hidetoshi Furukawa
- **Comment**: Technical Report, 6 pages, 7 figures, 7 tables, Copyright(C)2018
  IEICE
- **Journal**: IEICE Technical Report, vol.117, no.403, SANE2017-92, pp.35-40,
  Jan. 2018
- **Summary**: The standard architecture of synthetic aperture radar (SAR) automatic target recognition (ATR) consists of three stages: detection, discrimination, and classification. In recent years, convolutional neural networks (CNNs) for SAR ATR have been proposed, but most of them classify target classes from a target chip extracted from SAR imagery, as a classification for the third stage of SAR ATR. In this report, we propose a novel CNN for end-to-end ATR from SAR imagery. The CNN named verification support network (VersNet) performs all three stages of SAR ATR end-to-end. VersNet inputs a SAR image of arbitrary sizes with multiple classes and multiple targets, and outputs a SAR ATR image representing the position, class, and pose of each detected target. This report describes the evaluation results of VersNet which trained to output scores of all 12 classes: 10 target classes, a target front class, and a background class, for each pixel using the moving and stationary target acquisition and recognition (MSTAR) public dataset.



### Effective Building Block Design for Deep Convolutional Neural Networks using Search
- **Arxiv ID**: http://arxiv.org/abs/1801.08577v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.08577v1)
- **Published**: 2018-01-25 19:40:44+00:00
- **Updated**: 2018-01-25 19:40:44+00:00
- **Authors**: Jayanta K Dutta, Jiayi Liu, Unmesh Kurup, Mohak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has shown promising results on many machine learning tasks but DL models are often complex networks with large number of neurons and layers, and recently, complex layer structures known as building blocks. Finding the best deep model requires a combination of finding both the right architecture and the correct set of parameters appropriate for that architecture. In addition, this complexity (in terms of layer types, number of neurons, and number of layers) also present problems with generalization since larger networks are easier to overfit to the data. In this paper, we propose a search framework for finding effective architectural building blocks for convolutional neural networks (CNN). Our approach is much faster at finding models that are close to state-of-the-art in performance. In addition, the models discovered by our approach are also smaller than models discovered by similar techniques. We achieve these twin advantages by designing our search space in such a way that it searches over a reduced set of state-of-the-art building blocks for CNNs including residual block, inception block, inception-residual block, ResNeXt block and many others. We apply this technique to generate models for multiple image datasets and show that these models achieve performance comparable to state-of-the-art (and even surpassing the state-of-the-art in one case). We also show that learned models are transferable between datasets.



### Deep LOGISMOS: Deep Learning Graph-based 3D Segmentation of Pancreatic Tumors on CT scans
- **Arxiv ID**: http://arxiv.org/abs/1801.08599v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08599v1)
- **Published**: 2018-01-25 21:34:44+00:00
- **Updated**: 2018-01-25 21:34:44+00:00
- **Authors**: Zhihui Guo, Ling Zhang, Le Lu, Mohammadhadi Bagheri, Ronald M. Summers, Milan Sonka, Jianhua Yao
- **Comment**: 4 pages,3 figures
- **Journal**: None
- **Summary**: This paper reports Deep LOGISMOS approach to 3D tumor segmentation by incorporating boundary information derived from deep contextual learning to LOGISMOS - layered optimal graph image segmentation of multiple objects and surfaces. Accurate and reliable tumor segmentation is essential to tumor growth analysis and treatment selection. A fully convolutional network (FCN), UNet, is first trained using three adjacent 2D patches centered at the tumor, providing contextual UNet segmentation and probability map for each 2D patch. The UNet segmentation is then refined by Gaussian Mixture Model (GMM) and morphological operations. The refined UNet segmentation is used to provide the initial shape boundary to build a segmentation graph. The cost for each node of the graph is determined by the UNet probability maps. Finally, a max-flow algorithm is employed to find the globally optimal solution thus obtaining the final segmentation. For evaluation, we applied the method to pancreatic tumor segmentation on a dataset of 51 CT scans, among which 30 scans were used for training and 21 for testing. With Deep LOGISMOS, DICE Similarity Coefficient (DSC) and Relative Volume Difference (RVD) reached 83.2+-7.8% and 18.6+-17.4% respectively, both are significantly improved (p<0.05) compared with contextual UNet and/or LOGISMOS alone.



### A Rapidly Deployable Classification System using Visual Data for the Application of Precision Weed Management
- **Arxiv ID**: http://arxiv.org/abs/1801.08613v2
- **DOI**: 10.1016/j.compag.2018.02.023
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08613v2)
- **Published**: 2018-01-25 22:00:00+00:00
- **Updated**: 2018-04-26 07:36:23+00:00
- **Authors**: David Hall, Feras Dayoub, Tristan Perez, Chris McCool
- **Comment**: 36 pages, 14 figures, published Computers and Electronics in
  Agriculture Vol. 148
- **Journal**: D. Hall, F. Dayoub, T. Perez, and C. McCool, "A Rapidly Deployable
  Classification System using Visual Data for the Application of Precision Weed
  Management," Computers and Electronics in Agriculture, Vol. 148, pp. 107-120,
  May 2018
- **Summary**: In this work we demonstrate a rapidly deployable weed classification system that uses visual data to enable autonomous precision weeding without making prior assumptions about which weed species are present in a given field. Previous work in this area relies on having prior knowledge of the weed species present in the field. This assumption cannot always hold true for every field, and thus limits the use of weed classification systems based on this assumption. In this work, we obviate this assumption and introduce a rapidly deployable approach able to operate on any field without any weed species assumptions prior to deployment. We present a three stage pipeline for the implementation of our weed classification system consisting of initial field surveillance, offline processing and selective labelling, and automated precision weeding. The key characteristic of our approach is the combination of plant clustering and selective labelling which is what enables our system to operate without prior weed species knowledge. Testing using field data we are able to label 12.3 times fewer images than traditional full labelling whilst reducing classification accuracy by only 14%.



### Accurate Weakly Supervised Deep Lesion Segmentation on CT Scans: Self-Paced 3D Mask Generation from RECIST
- **Arxiv ID**: http://arxiv.org/abs/1801.08614v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08614v1)
- **Published**: 2018-01-25 22:05:19+00:00
- **Updated**: 2018-01-25 22:05:19+00:00
- **Authors**: Jinzheng Cai, Youbao Tang, Le Lu, Adam P. Harrison, Ke Yan, Jing Xiao, Lin Yang, Ronald M. Summers
- **Comment**: v1: Main paper + supplementary material
- **Journal**: None
- **Summary**: Volumetric lesion segmentation via medical imaging is a powerful means to precisely assess multiple time-point lesion/tumor changes. Because manual 3D segmentation is prohibitively time consuming and requires radiological experience, current practices rely on an imprecise surrogate called response evaluation criteria in solid tumors (RECIST). Despite their coarseness, RECIST marks are commonly found in current hospital picture and archiving systems (PACS), meaning they can provide a potentially powerful, yet extraordinarily challenging, source of weak supervision for full 3D segmentation. Toward this end, we introduce a convolutional neural network based weakly supervised self-paced segmentation (WSSS) method to 1) generate the initial lesion segmentation on the axial RECIST-slice; 2) learn the data distribution on RECIST-slices; 3) adapt to segment the whole volume slice by slice to finally obtain a volumetric segmentation. In addition, we explore how super-resolution images (2~5 times beyond the physical CT imaging), generated from a proposed stacked generative adversarial network, can aid the WSSS performance. We employ the DeepLesion dataset, a comprehensive CT-image lesion dataset of 32,735 PACS-bookmarked findings, which include lesions, tumors, and lymph nodes of varying sizes, categories, body regions and surrounding contexts. These are drawn from 10,594 studies of 4,459 patients. We also validate on a lymph-node dataset, where 3D ground truth masks are available for all images. For the DeepLesion dataset, we report mean Dice coefficients of 93% on RECIST-slices and 76% in 3D lesion volumes. We further validate using a subjective user study, where an experienced radiologist accepted our WSSS-generated lesion segmentation results with a high probability of 92.4%.



### DeepPap: Deep Convolutional Networks for Cervical Cell Classification
- **Arxiv ID**: http://arxiv.org/abs/1801.08616v1
- **DOI**: 10.1109/JBHI.2017.2705583
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08616v1)
- **Published**: 2018-01-25 22:12:29+00:00
- **Updated**: 2018-01-25 22:12:29+00:00
- **Authors**: Ling Zhang, Le Lu, Isabella Nogues, Ronald M. Summers, Shaoxiong Liu, Jianhua Yao
- **Comment**: None
- **Journal**: IEEE Journal of Biomedical and Health Informatics, 19 May 2017,
  Volume: 21 Issue: 6
- **Summary**: Automation-assisted cervical screening via Pap smear or liquid-based cytology (LBC) is a highly effective cell imaging based cancer detection tool, where cells are partitioned into "abnormal" and "normal" categories. However, the success of most traditional classification methods relies on the presence of accurate cell segmentations. Despite sixty years of research in this field, accurate segmentation remains a challenge in the presence of cell clusters and pathologies. Moreover, previous classification methods are only built upon the extraction of hand-crafted features, such as morphology and texture. This paper addresses these limitations by proposing a method to directly classify cervical cells - without prior segmentation - based on deep features, using convolutional neural networks (ConvNets). First, the ConvNet is pre-trained on a natural image dataset. It is subsequently fine-tuned on a cervical cell dataset consisting of adaptively re-sampled image patches coarsely centered on the nuclei. In the testing phase, aggregation is used to average the prediction scores of a similar set of image patches. The proposed method is evaluated on both Pap smear and LBC datasets. Results show that our method outperforms previous algorithms in classification accuracy (98.3%), area under the curve (AUC) (0.99) values, and especially specificity (98.3%), when applied to the Herlev benchmark Pap smear dataset and evaluated using five-fold cross-validation. Similar superior performances are also achieved on the HEMLBC (H&E stained manual LBC) dataset. Our method is promising for the development of automation-assisted reading systems in primary cervical screening.



### Generating Handwritten Chinese Characters using CycleGAN
- **Arxiv ID**: http://arxiv.org/abs/1801.08624v1
- **DOI**: 10.1109/WACV.2018.00028
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08624v1)
- **Published**: 2018-01-25 22:36:05+00:00
- **Updated**: 2018-01-25 22:36:05+00:00
- **Authors**: Bo Chang, Qiong Zhang, Shenyi Pan, Lili Meng
- **Comment**: Accepted at WACV 2018
- **Journal**: None
- **Summary**: Handwriting of Chinese has long been an important skill in East Asia. However, automatic generation of handwritten Chinese characters poses a great challenge due to the large number of characters. Various machine learning techniques have been used to recognize Chinese characters, but few works have studied the handwritten Chinese character generation problem, especially with unpaired training data. In this work, we formulate the Chinese handwritten character generation as a problem that learns a mapping from an existing printed font to a personalized handwritten style. We further propose DenseNet CycleGAN to generate Chinese handwritten characters. Our method is applied not only to commonly used Chinese characters but also to calligraphy work with aesthetic values. Furthermore, we propose content accuracy and style discrepancy as the evaluation metrics to assess the quality of the handwritten characters generated. We then use our proposed metrics to evaluate the generated characters from CASIA dataset as well as our newly introduced Lanting calligraphy dataset.



### DeepLung: Deep 3D Dual Path Nets for Automated Pulmonary Nodule Detection and Classification
- **Arxiv ID**: http://arxiv.org/abs/1801.09555v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1801.09555v1)
- **Published**: 2018-01-25 23:22:00+00:00
- **Updated**: 2018-01-25 23:22:00+00:00
- **Authors**: Wentao Zhu, Chaochun Liu, Wei Fan, Xiaohui Xie
- **Comment**: 9 pages, 8 figures, IEEE WACV conference. arXiv admin note:
  substantial text overlap with arXiv:1709.05538
- **Journal**: None
- **Summary**: In this work, we present a fully automated lung computed tomography (CT) cancer diagnosis system, DeepLung. DeepLung consists of two components, nodule detection (identifying the locations of candidate nodules) and classification (classifying candidate nodules into benign or malignant). Considering the 3D nature of lung CT data and the compactness of dual path networks (DPN), two deep 3D DPN are designed for nodule detection and classification respectively. Specifically, a 3D Faster Regions with Convolutional Neural Net (R-CNN) is designed for nodule detection with 3D dual path blocks and a U-net-like encoder-decoder structure to effectively learn nodule features. For nodule classification, gradient boosting machine (GBM) with 3D dual path network features is proposed. The nodule classification subnetwork was validated on a public dataset from LIDC-IDRI, on which it achieved better performance than state-of-the-art approaches and surpassed the performance of experienced doctors based on image modality. Within the DeepLung system, candidate nodules are detected first by the nodule detection subnetwork, and nodule diagnosis is conducted by the classification subnetwork. Extensive experimental results demonstrate that DeepLung has performance comparable to experienced doctors both for the nodule-level and patient-level diagnosis on the LIDC-IDRI dataset.\footnote{https://github.com/uci-cbcl/DeepLung.git}



### Generative Adversarial Networks using Adaptive Convolution
- **Arxiv ID**: http://arxiv.org/abs/1802.02226v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1802.02226v1)
- **Published**: 2018-01-25 23:49:19+00:00
- **Updated**: 2018-01-25 23:49:19+00:00
- **Authors**: Nhat M. Nguyen, Nilanjan Ray
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing GANs architectures that generate images use transposed convolution or resize-convolution as their upsampling algorithm from lower to higher resolution feature maps in the generator. We argue that this kind of fixed operation is problematic for GANs to model objects that have very different visual appearances. We propose a novel adaptive convolution method that learns the upsampling algorithm based on the local context at each location to address this problem. We modify a baseline GANs architecture by replacing normal convolutions with adaptive convolutions in the generator. Experiments on CIFAR-10 dataset show that our modified models improve the baseline model by a large margin. Furthermore, our models achieve state-of-the-art performance on CIFAR-10 and STL-10 datasets in the unsupervised setting.



