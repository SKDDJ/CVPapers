# Arxiv Papers in cs.CV on 2018-01-03
### Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1801.00868v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00868v3)
- **Published**: 2018-01-03 00:21:31+00:00
- **Updated**: 2019-04-10 18:17:40+00:00
- **Authors**: Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr DollÃ¡r
- **Comment**: accepted to CVPR 2019
- **Journal**: None
- **Summary**: We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation.



### Optimizing Channel Selection for Seizure Detection
- **Arxiv ID**: http://arxiv.org/abs/1801.02472v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.02472v1)
- **Published**: 2018-01-03 00:59:32+00:00
- **Updated**: 2018-01-03 00:59:32+00:00
- **Authors**: Vinit Shah, Meysam Golmohammadi, Saeedeh Ziyabari, Eva Von Weltin, Iyad Obeid, Joseph Picone
- **Comment**: Published in Dec 2017 publication IEEE Signal Processing in Medicine
  and Biology Symposium. Philadelphia, Pennsylvania, USA
- **Journal**: None
- **Summary**: Interpretation of electroencephalogram (EEG) signals can be complicated by obfuscating artifacts. Artifact detection plays an important role in the observation and analysis of EEG signals. Spatial information contained in the placement of the electrodes can be exploited to accurately detect artifacts. However, when fewer electrodes are used, less spatial information is available, making it harder to detect artifacts. In this study, we investigate the performance of a deep learning algorithm, CNN-LSTM, on several channel configurations. Each configuration was designed to minimize the amount of spatial information lost compared to a standard 22-channel EEG. Systems using a reduced number of channels ranging from 8 to 20 achieved sensitivities between 33% and 37% with false alarms in the range of [38, 50] per 24 hours. False alarms increased dramatically (e.g., over 300 per 24 hours) when the number of channels was further reduced. Baseline performance of a system that used all 22 channels was 39% sensitivity with 23 false alarms. Since the 22-channel system was the only system that included referential channels, the rapid increase in the false alarm rate as the number of channels was reduced underscores the importance of retaining referential channels for artifact reduction. This cautionary result is important because one of the biggest differences between various types of EEGs administered is the type of referential channel used.



### A Novel Feature Descriptor for Image Retrieval by Combining Modified Color Histogram and Diagonally Symmetric Co-occurrence Texture Pattern
- **Arxiv ID**: http://arxiv.org/abs/1801.00879v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00879v1)
- **Published**: 2018-01-03 01:39:05+00:00
- **Updated**: 2018-01-03 01:39:05+00:00
- **Authors**: Ayan Kumar Bhunia, Avirup Bhattacharyya, Prithaj Banerjee, Partha Pratim Roy, Subrahmanyam Murala
- **Comment**: Preprint Submitted
- **Journal**: None
- **Summary**: In this paper, we have proposed a novel feature descriptors combining color and texture information collectively. In our proposed color descriptor component, the inter-channel relationship between Hue (H) and Saturation (S) channels in the HSV color space has been explored which was not done earlier. We have quantized the H channel into a number of bins and performed the voting with saturation values and vice versa by following a principle similar to that of the HOG descriptor, where orientation of the gradient is quantized into a certain number of bins and voting is done with gradient magnitude. This helps us to study the nature of variation of saturation with variation in Hue and nature of variation of Hue with the variation in saturation. The texture component of our descriptor considers the co-occurrence relationship between the pixels symmetric about both the diagonals of a 3x3 window. Our work is inspired from the work done by Dubey et al.[1]. These two components, viz. color and texture information individually perform better than existing texture and color descriptors. Moreover, when concatenated the proposed descriptors provide significant improvement over existing descriptors for content base color image retrieval. The proposed descriptor has been tested for image retrieval on five databases, including texture image databases - MIT VisTex database and Salzburg texture database and natural scene databases Corel 1K, Corel 5K and Corel 10K. The precision and recall values experimented on these databases are compared with some state-of-art local patterns. The proposed method provided satisfactory results from the experiments.



### Deep convolutional neural networks for segmenting 3D in vivo multiphoton images of vasculature in Alzheimer disease mouse models
- **Arxiv ID**: http://arxiv.org/abs/1801.00880v4
- **DOI**: 10.1371/journal.pone.0213539
- **Categories**: **cs.CV**, I.2.1; I.2.6; I.4.3; I.4.6; I.5.1; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1801.00880v4)
- **Published**: 2018-01-03 01:51:02+00:00
- **Updated**: 2019-02-25 17:05:08+00:00
- **Authors**: Mohammad Haft-Javaherian, Linjing Fang, Victorine Muse, Chris B. Schaffer, Nozomi Nishimura, Mert R. Sabuncu
- **Comment**: 34 pages, 9 figures
- **Journal**: None
- **Summary**: The health and function of tissue rely on its vasculature network to provide reliable blood perfusion. Volumetric imaging approaches, such as multiphoton microscopy, are able to generate detailed 3D images of blood vessels that could contribute to our understanding of the role of vascular structure in normal physiology and in disease mechanisms. The segmentation of vessels, a core image analysis problem, is a bottleneck that has prevented the systematic comparison of 3D vascular architecture across experimental populations. We explored the use of convolutional neural networks to segment 3D vessels within volumetric in vivo images acquired by multiphoton microscopy. We evaluated different network architectures and machine learning techniques in the context of this segmentation problem. We show that our optimized convolutional neural network architecture, which we call DeepVess, yielded a segmentation accuracy that was better than both the current state-of-the-art and a trained human annotator, while also being orders of magnitude faster. To explore the effects of aging and Alzheimer's disease on capillaries, we applied DeepVess to 3D images of cortical blood vessels in young and old mouse models of Alzheimer's disease and wild type littermates. We found little difference in the distribution of capillary diameter or tortuosity between these groups, but did note a decrease in the number of longer capillary segments ($>75\mu m$) in aged animals as compared to young, in both wild type and Alzheimer's disease mouse models.



### Deep Spatial Feature Reconstruction for Partial Person Re-identification: Alignment-Free Approach
- **Arxiv ID**: http://arxiv.org/abs/1801.00881v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00881v3)
- **Published**: 2018-01-03 01:59:48+00:00
- **Updated**: 2018-09-04 02:00:13+00:00
- **Authors**: Lingxiao He, Jian Liang, Haiqing Li, Zhenan Sun
- **Comment**: 8 pages, 11 figures, accepted by CVPR 2018
- **Journal**: None
- **Summary**: Partial person re-identification (re-id) is a challenging problem, where only several partial observations (images) of people are available for matching. However, few studies have provided flexible solutions to identifying a person in an image containing arbitrary part of the body. In this paper, we propose a fast and accurate matching method to address this problem. The proposed method leverages Fully Convolutional Network (FCN) to generate fix-sized spatial feature maps such that pixel-level features are consistent. To match a pair of person images of different sizes, a novel method called Deep Spatial feature Reconstruction (DSR) is further developed to avoid explicit alignment. Specifically, DSR exploits the reconstructing error from popular dictionary learning models to calculate the similarity between different spatial feature maps. In that way, we expect that the proposed FCN can decrease the similarity of coupled images from different persons and increase that from the same person. Experimental results on two partial person datasets demonstrate the efficiency and effectiveness of the proposed method in comparison with several state-of-the-art partial person re-id approaches. Additionally, DSR achieves competitive results on a benchmark person dataset Market1501 with 83.58\% Rank-1 accuracy.



### Seeded Ising Model and Statistical Natures of Human Iris Templates
- **Arxiv ID**: http://arxiv.org/abs/1802.02223v1
- **DOI**: 10.1103/PhysRevE.98.032115
- **Categories**: **stat.AP**, cs.CV, cs.HC, physics.data-an, q-bio.OT
- **Links**: [PDF](http://arxiv.org/pdf/1802.02223v1)
- **Published**: 2018-01-03 02:32:18+00:00
- **Updated**: 2018-01-03 02:32:18+00:00
- **Authors**: Song-Hwa Kwon, Hyeong In Choi, Sung Jin Lee, Nam-Sook Wee
- **Comment**: 7 pages
- **Journal**: Phys. Rev. E 98, 032115 (2018)
- **Summary**: We propose a variant of Ising model, called the Seeded Ising Model, to model probabilistic nature of human iris templates. This model is an Ising model in which the values at certain lattice points are held fixed throughout Ising model evolution. Using this we show how to reconstruct the full iris template from partial information, and we show that about 1/6 of the given template is needed to recover almost all information content of the original one in the sense that the resulting Hamming distance is well within the range to assert correctly the identity of the subject. This leads us to propose the concept of effective statistical degree of freedom of iris templates and show it is about 1/6 of the total number of bits. In particular, for a template of $2048$ bits, its effective statistical degree of freedom is about $342$ bits, which coincides very well with the degree of freedom computed by the completely different method proposed by Daugman.



### Recovery of Point Clouds on Surfaces: Application to Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1801.00886v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00886v1)
- **Published**: 2018-01-03 02:49:56+00:00
- **Updated**: 2018-01-03 02:49:56+00:00
- **Authors**: Sunrita Poddar, Mathews Jacob
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a framework for the recovery of points on a smooth surface in high-dimensional space, with application to dynamic imaging. We assume the surface to be the zero-level set of a bandlimited function. We show that the exponential maps of the points on the surface satisfy annihilation relations, implying that they lie in a finite dimensional subspace. We rely on nuclear norm minimization of the maps to recover the points from noisy and undersampled measurements. Since this direct approach suffers from the curse of dimensionality, we introduce an iterative reweighted algorithm that uses the "kernel trick". The resulting algorithm has similarities to iterative algorithms used in graph signal processing (GSP); this framework can be seen as a continuous domain alternative to discrete GSP theory. The use of the algorithm in recovering free breathing and ungated cardiac data shows the potential of this framework in practical applications.



### Recovery of Noisy Points on Band-limited Surfaces: Kernel Methods Re-explained
- **Arxiv ID**: http://arxiv.org/abs/1801.00890v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00890v2)
- **Published**: 2018-01-03 03:00:37+00:00
- **Updated**: 2018-03-19 16:40:22+00:00
- **Authors**: Sunrita Poddar, Mathews Jacob
- **Comment**: arXiv admin note: text overlap with arXiv:1801.00886
- **Journal**: None
- **Summary**: We introduce a continuous domain framework for the recovery of points on a surface in high dimensional space, represented as the zero-level set of a bandlimited function. We show that the exponential maps of the points on the surface satisfy annihilation relations, implying that they lie in a finite dimensional subspace. The subspace properties are used to derive sampling conditions, which will guarantee the perfect recovery of the surface from finite number of points. We rely on nuclear norm minimization to exploit the low-rank structure of the maps to recover the points from noisy measurements. Since the direct estimation of the surface is computationally prohibitive in very high dimensions, we propose an iterative reweighted algorithm using the "kernel trick". The iterative algorithm reveals deep links to Laplacian based algorithms widely used in graph signal processing; the theory and the sampling conditions can serve as a basis for discrete-continuous domain processing of signals on a graph.



### Improved EEG Event Classification Using Differential Energy
- **Arxiv ID**: http://arxiv.org/abs/1801.02477v1
- **DOI**: 10.1109/SPMB.2015.7405421
- **Categories**: **eess.SP**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.02477v1)
- **Published**: 2018-01-03 03:55:55+00:00
- **Updated**: 2018-01-03 03:55:55+00:00
- **Authors**: Amir Harati, Meysam Golmohammadi, Silvia Lopez, Iyad Obeid, Joseph Picone
- **Comment**: Published in IEEE Signal Processing in Medicine and Biology
  Symposium. Philadelphia, Pennsylvania, USA
- **Journal**: A. Harati, M. Golmohammadi, S. Lopez, I. Obeid and J. Picone,
  "Improved EEG event classification using differential energy," 2015 IEEE
  Signal Processing in Medicine and Biology Symposium (SPMB), Philadelphia, PA,
  2015, pp. 1-4
- **Summary**: Feature extraction for automatic classification of EEG signals typically relies on time frequency representations of the signal. Techniques such as cepstral-based filter banks or wavelets are popular analysis techniques in many signal processing applications including EEG classification. In this paper, we present a comparison of a variety of approaches to estimating and postprocessing features. To further aid in discrimination of periodic signals from aperiodic signals, we add a differential energy term. We evaluate our approaches on the TUH EEG Corpus, which is the largest publicly available EEG corpus and an exceedingly challenging task due to the clinical nature of the data. We demonstrate that a variant of a standard filter bank-based approach, coupled with first and second derivatives, provides a substantial reduction in the overall error rate. The combination of differential energy and derivatives produces a 24% absolute reduction in the error rate and improves our ability to discriminate between signal events and background noise. This relatively simple approach proves to be comparable to other popular feature extraction approaches such as wavelets, but is much more computationally efficient.



### ScreenerNet: Learning Self-Paced Curriculum for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.00904v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00904v4)
- **Published**: 2018-01-03 05:49:37+00:00
- **Updated**: 2018-06-06 06:46:02+00:00
- **Authors**: Tae-Hoon Kim, Jonghyun Choi
- **Comment**: curricular learning, deep learning, deep q-learning
- **Journal**: None
- **Summary**: We propose to learn a curriculum or a syllabus for supervised learning and deep reinforcement learning with deep neural networks by an attachable deep neural network, called ScreenerNet. Specifically, we learn a weight for each sample by jointly training the ScreenerNet and the main network in an end-to-end self-paced fashion. The ScreenerNet neither has sampling bias nor requires to remember the past learning history. We show the networks augmented with the ScreenerNet achieve early convergence with better accuracy than the state-of-the-art curricular learning methods in extensive experiments using three popular vision datasets such as MNIST, CIFAR10 and Pascal VOC2012, and a Cart-pole task using Deep Q-learning. Moreover, the ScreenerNet can extend other curriculum learning methods such as Prioritized Experience Replay (PER) for further accuracy improvement.



### Neural Networks in Adversarial Setting and Ill-Conditioned Weight Space
- **Arxiv ID**: http://arxiv.org/abs/1801.00905v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1801.00905v1)
- **Published**: 2018-01-03 05:52:52+00:00
- **Updated**: 2018-01-03 05:52:52+00:00
- **Authors**: Mayank Singh, Abhishek Sinha, Balaji Krishnamurthy
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Neural networks have seen a huge surge in its adoption due to their ability to provide high accuracy on various tasks. On the other hand, the existence of adversarial examples have raised suspicions regarding the generalization capabilities of neural networks. In this work, we focus on the weight matrix learnt by the neural networks and hypothesize that ill conditioned weight matrix is one of the contributing factors in neural network's susceptibility towards adversarial examples. For ensuring that the learnt weight matrix's condition number remains sufficiently low, we suggest using orthogonal regularizer. We show that this indeed helps in increasing the adversarial accuracy on MNIST and F-MNIST datasets.



### Instance Embedding Transfer to Unsupervised Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1801.00908v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00908v2)
- **Published**: 2018-01-03 05:55:23+00:00
- **Updated**: 2018-02-27 02:06:25+00:00
- **Authors**: Siyang Li, Bryan Seybold, Alexey Vorobyov, Alireza Fathi, Qin Huang, C. -C. Jay Kuo
- **Comment**: To appear in CVPR 2018
- **Journal**: None
- **Summary**: We propose a method for unsupervised video object segmentation by transferring the knowledge encapsulated in image-based instance embedding networks. The instance embedding network produces an embedding vector for each pixel that enables identifying all pixels belonging to the same object. Though trained on static images, the instance embeddings are stable over consecutive video frames, which allows us to link objects together over time. Thus, we adapt the instance networks trained on static images to video object segmentation and incorporate the embeddings with objectness and optical flow features, without model retraining or online fine-tuning. The proposed method outperforms state-of-the-art unsupervised segmentation methods in the DAVIS dataset and the FBMS dataset.



### Joint Optic Disc and Cup Segmentation Based on Multi-label Deep Network and Polar Transformation
- **Arxiv ID**: http://arxiv.org/abs/1801.00926v3
- **DOI**: 10.1109/TMI.2018.2791488
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00926v3)
- **Published**: 2018-01-03 08:56:45+00:00
- **Updated**: 2018-01-11 03:15:16+00:00
- **Authors**: Huazhu Fu, Jun Cheng, Yanwu Xu, Damon Wing Kee Wong, Jiang Liu, Xiaochun Cao
- **Comment**: Project homepage: http://hzfu.github.io/proj_glaucoma_fundus.html ,
  and Accepted by IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Glaucoma is a chronic eye disease that leads to irreversible vision loss. The cup to disc ratio (CDR) plays an important role in the screening and diagnosis of glaucoma. Thus, the accurate and automatic segmentation of optic disc (OD) and optic cup (OC) from fundus images is a fundamental task. Most existing methods segment them separately, and rely on hand-crafted visual feature from fundus images. In this paper, we propose a deep learning architecture, named M-Net, which solves the OD and OC segmentation jointly in a one-stage multi-label system. The proposed M-Net mainly consists of multi-scale input layer, U-shape convolutional network, side-output layer, and multi-label loss function. The multi-scale input layer constructs an image pyramid to achieve multiple level receptive field sizes. The U-shape convolutional network is employed as the main body network structure to learn the rich hierarchical representation, while the side-output layer acts as an early classifier that produces a companion local prediction map for different scale layers. Finally, a multi-label loss function is proposed to generate the final segmentation map. For improving the segmentation performance further, we also introduce the polar transformation, which provides the representation of the original image in the polar coordinate system. The experiments show that our M-Net system achieves state-of-the-art OD and OC segmentation result on ORIGA dataset. Simultaneously, the proposed method also obtains the satisfactory glaucoma screening performances with calculated CDR value on both ORIGA and SCES datasets.



### Topological Tracking of Connected Components in Image Sequences
- **Arxiv ID**: http://arxiv.org/abs/1801.00939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.00939v1)
- **Published**: 2018-01-03 09:52:40+00:00
- **Updated**: 2018-01-03 09:52:40+00:00
- **Authors**: Rocio Gonzalez-Diaz, Maria-Jose Jimenez, Belen Medrano
- **Comment**: None
- **Journal**: None
- **Summary**: Persistent homology provides information about the lifetime of homology classes along a filtration of cell complexes. Persistence barcode is a graphical representation of such information. A filtration might be determined by time in a set of spatiotemporal data, but classical methods for computing persistent homology do not respect the fact that we can not move backwards in time. In this paper, taking as input a time-varying sequence of two-dimensional (2D) binary digital images, we develop an algorithm for encoding, in the so-called {\it spatiotemporal barcode}, lifetime of connected components (of either the foreground or background) that are moving in the image sequence over time (this information may not coincide with the one provided by the persistence barcode). This way, given a connected component at a specific time in the sequence, we can track the component backwards in time until the moment it was born, by what we call a {\it spatiotemporal path}. The main contribution of this paper with respect to our previous works lies in a new algorithm that computes spatiotemporal paths directly, valid for both foreground and background and developed in a general context, setting the ground for a future extension for tracking higher dimensional topological features in $nD$ binary digital image sequences.



### Joint convolutional neural pyramid for depth map super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1801.00968v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1801.00968v1)
- **Published**: 2018-01-03 11:53:34+00:00
- **Updated**: 2018-01-03 11:53:34+00:00
- **Authors**: Yi Xiao, Xiang Cao, Xianyi Zhu, Renzhi Yang, Yan Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: High-resolution depth map can be inferred from a low-resolution one with the guidance of an additional high-resolution texture map of the same scene. Recently, deep neural networks with large receptive fields are shown to benefit applications such as image completion. Our insight is that super resolution is similar to image completion, where only parts of the depth values are precisely known. In this paper, we present a joint convolutional neural pyramid model with large receptive fields for joint depth map super-resolution. Our model consists of three sub-networks, two convolutional neural pyramids concatenated by a normal convolutional neural network. The convolutional neural pyramids extract information from large receptive fields of the depth map and guidance map, while the convolutional neural network effectively transfers useful structures of the guidance image to the depth image. Experimental results show that our model outperforms existing state-of-the-art algorithms not only on data pairs of RGB/depth images, but also on other data pairs like color/saliency and color-scribbles/colorized images.



### Implementation of Deep Convolutional Neural Network in Multi-class Categorical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1801.01397v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01397v1)
- **Published**: 2018-01-03 15:29:44+00:00
- **Updated**: 2018-01-03 15:29:44+00:00
- **Authors**: Pushparaja Murugan
- **Comment**: 22 Pages. arXiv admin note: substantial text overlap with
  arXiv:1712.04711
- **Journal**: None
- **Summary**: Convolutional Neural Networks has been implemented in many complex machine learning takes such as image classification, object identification, autonomous vehicle and robotic vision tasks. However, ConvNet architecture efficiency and accuracy depend on a large number of fac- tors. Also, the complex architecture requires a significant amount of data to train and involves with a large number of hyperparameters that increases the computational expenses and difficul- ties. Hence, it is necessary to address the limitations and techniques to overcome the barriers to ensure that the architecture performs well in complex visual tasks. This article is intended to develop an efficient ConvNet architecture for multi-class image categorical classification applica- tion. In the development of the architecture, large pool of grey scale images are taken as input information images and split into training and test datasets. The numerously available technique is implemented to reduce the overfitting and poor generalization of the network. The hyperpa- rameters of determined by Bayesian Optimization with Gaussian Process prior algorithm. ReLu non-linear activation function is implemented after the convolutional layers. Max pooling op- eration is carried out to downsampling the data points in pooling layers. Cross-entropy loss function is used to measure the performance of the architecture where the softmax is used in the classification layer. Mini-batch gradient descent with Adam optimizer algorithm is used for backpropagation. Developed architecture is validated with confusion matrix and classification report.



### Spot the Difference by Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1801.01051v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01051v1)
- **Published**: 2018-01-03 15:31:25+00:00
- **Updated**: 2018-01-03 15:31:25+00:00
- **Authors**: Junhui Wu, Yun Ye, Yu Chen, Zhi Weng
- **Comment**: Tech Report, 10 pages
- **Journal**: None
- **Summary**: In this paper, we propose a simple yet effective solution to a change detection task that detects the difference between two images, which we call "spot the difference". Our approach uses CNN-based object detection by stacking two aligned images as input and considering the differences between the two images as objects to detect. An early-merging architecture is used as the backbone network. Our method is accurate, fast and robust while using very cheap annotation. We verify the proposed method on the task of change detection between the digital design and its photographic image of a book. Compared to verification based methods, our object detection based method outperforms other methods by a large margin and gives extra information of location. We compress the network and achieve 24 times acceleration while keeping the accuracy. Besides, as we synthesize the training data for detection using weakly labeled images, our method does not need expensive bounding box annotation.



### Computational complexity lower bounds of certain discrete Radon transform approximations
- **Arxiv ID**: http://arxiv.org/abs/1801.01054v1
- **DOI**: None
- **Categories**: **cs.CC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1801.01054v1)
- **Published**: 2018-01-03 15:41:08+00:00
- **Updated**: 2018-01-03 15:41:08+00:00
- **Authors**: Timur M. Khanipov
- **Comment**: Created in ShareLaTeX, 11 pages, 2 PDF and 1 TikZ figures
- **Journal**: None
- **Summary**: For the computational model where only additions are allowed, the $\Omega(n^2\log n)$ lower bound on operations count with respect to image size $n\times n$ is obtained for two types of the discrete Radon transform implementations: the fast Hough transform and a generic strip pattern class which includes the classical Hough transform, implying the fast Hough transform algorithm asymptotic optimality. The proofs are based on a specific result from the boolean circuits complexity theory and are generalized for the case of boolean $\vee$ binary operation.



### LIME: Live Intrinsic Material Estimation
- **Arxiv ID**: http://arxiv.org/abs/1801.01075v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01075v2)
- **Published**: 2018-01-03 16:55:31+00:00
- **Updated**: 2018-05-04 15:43:19+00:00
- **Authors**: Abhimitra Meka, Maxim Maximov, Michael Zollhoefer, Avishek Chatterjee, Hans-Peter Seidel, Christian Richardt, Christian Theobalt
- **Comment**: 17 pages, Spotlight paper in CVPR 2018
- **Journal**: None
- **Summary**: We present the first end to end approach for real time material estimation for general object shapes with uniform material that only requires a single color image as input. In addition to Lambertian surface properties, our approach fully automatically computes the specular albedo, material shininess, and a foreground segmentation. We tackle this challenging and ill posed inverse rendering problem using recent advances in image to image translation techniques based on deep convolutional encoder decoder architectures. The underlying core representations of our approach are specular shading, diffuse shading and mirror images, which allow to learn the effective and accurate separation of diffuse and specular albedo. In addition, we propose a novel highly efficient perceptual rendering loss that mimics real world image formation and obtains intermediate results even during run time. The estimation of material parameters at real time frame rates enables exciting mixed reality applications, such as seamless illumination consistent integration of virtual objects into real world scenes, and virtual material cloning. We demonstrate our approach in a live setup, compare it to the state of the art, and demonstrate its effectiveness through quantitative and qualitative evaluation.



### Fingerprint Distortion Rectification using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.01198v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.01198v1)
- **Published**: 2018-01-03 22:43:35+00:00
- **Updated**: 2018-01-03 22:43:35+00:00
- **Authors**: Ali Dabouei, Hadi Kazemi, Seyed Mehdi Iranmanesh, Jeremi Dawson, Nasser M. Nasrabadi
- **Comment**: Accepted at ICB 2018
- **Journal**: None
- **Summary**: Elastic distortion of fingerprints has a negative effect on the performance of fingerprint recognition systems. This negative effect brings inconvenience to users in authentication applications. However, in the negative recognition scenario where users may intentionally distort their fingerprints, this can be a serious problem since distortion will prevent recognition system from identifying malicious users. Current methods aimed at addressing this problem still have limitations. They are often not accurate because they estimate distortion parameters based on the ridge frequency map and orientation map of input samples, which are not reliable due to distortion. Secondly, they are not efficient and requiring significant computation time to rectify samples. In this paper, we develop a rectification model based on a Deep Convolutional Neural Network (DCNN) to accurately estimate distortion parameters from the input image. Using a comprehensive database of synthetic distorted samples, the DCNN learns to accurately estimate distortion bases ten times faster than the dictionary search methods used in the previous approaches. Evaluating the proposed method on public databases of distorted samples shows that it can significantly improve the matching performance of distorted samples.



