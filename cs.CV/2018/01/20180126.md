# Arxiv Papers in cs.CV on 2018-01-26
### Neural Algebra of Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1801.08676v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.08676v1)
- **Published**: 2018-01-26 05:13:10+00:00
- **Updated**: 2018-01-26 05:13:10+00:00
- **Authors**: Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian, Stephen Gould
- **Comment**: 2018 IEEE Winter Conference on Applications of Computer Vision (WACV)
- **Journal**: None
- **Summary**: The world is fundamentally compositional, so it is natural to think of visual recognition as the recognition of basic visually primitives that are composed according to well-defined rules. This strategy allows us to recognize unseen complex concepts from simple visual primitives. However, the current trend in visual recognition follows a data greedy approach where huge amounts of data are required to learn models for any desired visual concept. In this paper, we build on the compositionality principle and develop an "algebra" to compose classifiers for complex visual concepts. To this end, we learn neural network modules to perform boolean algebra operations on simple visual classifiers. Since these modules form a complete functional set, a classifier for any complex visual concept defined as a boolean expression of primitives can be obtained by recursively applying the learned modules, even if we do not have a single training sample. As our experiments show, using such a framework, we can compose classifiers for complex visual concepts outperforming standard baselines on two well-known visual recognition benchmarks. Finally, we present a qualitative analysis of our method and its properties.



### Cloud Detection From RGB Color Remote Sensing Images With Deep Pyramid Networks
- **Arxiv ID**: http://arxiv.org/abs/1801.08706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08706v1)
- **Published**: 2018-01-26 08:15:46+00:00
- **Updated**: 2018-01-26 08:15:46+00:00
- **Authors**: Savas Ozkan, Mehmet Efendioglu, Caner Demirpolat
- **Comment**: Submitted to IGARSS 2018
- **Journal**: None
- **Summary**: Cloud detection from remotely observed data is a critical pre-processing step for various remote sensing applications. In particular, this problem becomes even harder for RGB color images, since there is no distinct spectral pattern for clouds, which is directly separable from the Earth surface. In this paper, we adapt a deep pyramid network (DPN) to tackle this problem. For this purpose, the network is enhanced with a pre-trained parameter model at the encoder layer. Moreover, the method is able to obtain accurate pixel-level segmentation and classification results from a set of noisy labeled RGB color images. In order to demonstrate the superiority of the method, we collect and label data with the corresponding cloud/non-cloudy masks acquired from low-orbit Gokturk-2 and RASAT satellites. The experimental results validates that the proposed method outperforms several baselines even for hard cases (e.g. snowy mountains) that are perceptually difficult to distinguish by human eyes.



### Weakly Supervised Object Detection with Pointwise Mutual Information
- **Arxiv ID**: http://arxiv.org/abs/1801.08747v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08747v1)
- **Published**: 2018-01-26 10:46:43+00:00
- **Updated**: 2018-01-26 10:46:43+00:00
- **Authors**: Rene Grzeszick, Sebastian Sudholt, Gernot A. Fink
- **Comment**: None
- **Journal**: None
- **Summary**: In this work a novel approach for weakly supervised object detection that incorporates pointwise mutual information is presented. A fully convolutional neural network architecture is applied in which the network learns one filter per object class. The resulting feature map indicates the location of objects in an image, yielding an intuitive representation of a class activation map. While traditionally such networks are learned by a softmax or binary logistic regression (sigmoid cross-entropy loss), a learning approach based on a cosine loss is introduced. A pointwise mutual information layer is incorporated in the network in order to project predictions and ground truth presence labels in a non-categorical embedding space. Thus, the cosine loss can be employed in this non-categorical representation. Besides integrating image level annotations, it is shown how to integrate point-wise annotations using a Spatial Pyramid Pooling layer. The approach is evaluated on the VOC2012 dataset for classification, point localization and weakly supervised bounding box localization. It is shown that the combination of pointwise mutual information and a cosine loss eases the learning process and thus improves the accuracy. The integration of coarse point-wise localizations further improves the results at minimal annotation costs.



### KRISM --- Krylov Subspace-based Optical Computing of Hyperspectral Images
- **Arxiv ID**: http://arxiv.org/abs/1801.09343v4
- **DOI**: 10.1145/3345553
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1801.09343v4)
- **Published**: 2018-01-26 12:10:38+00:00
- **Updated**: 2019-10-21 18:17:40+00:00
- **Authors**: Vishwanath Saragadam, Aswin C. Sankaranarayanan
- **Comment**: 14 pages of main paper and 15 pages of supplementary material
- **Journal**: Vishwanath Saragadam and Aswin C. Sankaranarayanan, "KRISM ---
  Krylov Subspace-based Optical Computing of Hyperspectral Images", ACM Trans.
  Graphics 38, 5 (2019), 148:1-14
- **Summary**: We present an adaptive imaging technique that optically computes a low-rank approximation of a scene's hyperspectral image, conceptualized as a matrix. Central to the proposed technique is the optical implementation of two measurement operators: a spectrally-coded imager and a spatially-coded spectrometer. By iterating between the two operators, we show that the top singular vectors and singular values of a hyperspectral image can be adaptively and optically computed with only a few iterations. We present an optical design that uses pupil plane coding for implementing the two operations and show several compelling results using a lab prototype to demonstrate the effectiveness of the proposed hyperspectral imager.



### SRDA: Generating Instance Segmentation Annotation Via Scanning, Reasoning And Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1801.08839v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08839v3)
- **Published**: 2018-01-26 15:10:15+00:00
- **Updated**: 2018-07-13 05:48:23+00:00
- **Authors**: Wenqiang Xu, Yonglu Li, Cewu Lu
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Instance segmentation is a problem of significance in computer vision. However, preparing annotated data for this task is extremely time-consuming and costly. By combining the advantages of 3D scanning, reasoning, and GAN-based domain adaptation techniques, we introduce a novel pipeline named SRDA to obtain large quantities of training samples with very minor effort. Our pipeline is well-suited to scenes that can be scanned, i.e. most indoor and some outdoor scenarios. To evaluate our performance, we build three representative scenes and a new dataset, with 3D models of various common objects categories and annotated real-world scene images. Extensive experiments show that our pipeline can achieve decent instance segmentation performance given very low human labor cost.



### Supersaliency: A Novel Pipeline for Predicting Smooth Pursuit-Based Attention Improves Generalizability of Video Saliency
- **Arxiv ID**: http://arxiv.org/abs/1801.08925v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1801.08925v3)
- **Published**: 2018-01-26 18:24:45+00:00
- **Updated**: 2019-04-12 10:40:07+00:00
- **Authors**: Mikhail Startsev, Michael Dorr
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting attention is a popular topic at the intersection of human and computer vision. However, even though most of the available video saliency data sets and models claim to target human observers' fixations, they fail to differentiate them from smooth pursuit (SP), a major eye movement type that is unique to perception of dynamic scenes. In this work, we highlight the importance of SP and its prediction (which we call supersaliency, due to greater selectivity compared to fixations), and aim to make its distinction from fixations explicit for computational models. To this end, we (i) use algorithmic and manual annotations of SP and fixations for two well-established video saliency data sets, (ii) train Slicing Convolutional Neural Networks for saliency prediction on either fixation- or SP-salient locations, and (iii) evaluate our and 26 publicly available dynamic saliency models on three data sets against traditional saliency and supersaliency ground truth. Overall, our models outperform the state of the art in both the new supersaliency and the traditional saliency problem settings, for which literature models are optimized. Importantly, on two independent data sets, our supersaliency model shows greater generalization ability and outperforms all other models, even for fixation prediction.



### Deflecting Adversarial Attacks with Pixel Deflection
- **Arxiv ID**: http://arxiv.org/abs/1801.08926v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1801.08926v3)
- **Published**: 2018-01-26 18:24:59+00:00
- **Updated**: 2018-03-30 20:36:53+00:00
- **Authors**: Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, James Storer
- **Comment**: Accepted to IEEE CVPR 2018 as Spotlight
- **Journal**: None
- **Summary**: CNNs are poised to become integral parts of many critical systems. Despite their robustness to natural variations, image pixel values can be manipulated, via small, carefully crafted, imperceptible perturbations, to cause a model to misclassify images. We present an algorithm to process an image so that classification accuracy is significantly preserved in the presence of such adversarial manipulations. Image classifiers tend to be robust to natural noise, and adversarial attacks tend to be agnostic to object location. These observations motivate our strategy, which leverages model robustness to defend against adversarial perturbations by forcing the image to match natural image statistics. Our algorithm locally corrupts the image by redistributing pixel values via a process we term pixel deflection. A subsequent wavelet-based denoising operation softens this corruption, as well as some of the adversarial changes. We demonstrate experimentally that the combination of these techniques enables the effective recovery of the true class, against a variety of robust attacks. Our results compare favorably with current state-of-the-art defenses, without requiring retraining or modifying the CNN.



### Efficient Hierarchical Graph-Based Segmentation of RGBD Videos
- **Arxiv ID**: http://arxiv.org/abs/1801.08981v1
- **DOI**: 10.1109/CVPR.2014.51
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.08981v1)
- **Published**: 2018-01-26 21:21:25+00:00
- **Updated**: 2018-01-26 21:21:25+00:00
- **Authors**: Steven Hickson, Stan Birchfield, Irfan Essa, Henrik Christensen
- **Comment**: CVPR 2014
- **Journal**: None
- **Summary**: We present an efficient and scalable algorithm for segmenting 3D RGBD point clouds by combining depth, color, and temporal information using a multistage, hierarchical graph-based approach. Our algorithm processes a moving window over several point clouds to group similar regions over a graph, resulting in an initial over-segmentation. These regions are then merged to yield a dendrogram using agglomerative clustering via a minimum spanning tree algorithm. Bipartite graph matching at a given level of the hierarchical tree yields the final segmentation of the point clouds by maintaining region identities over arbitrarily long periods of time. We show that a multistage segmentation with depth then color yields better results than a linear combination of depth and color. Due to its incremental processing, our algorithm can process videos of any length and in a streaming pipeline. The algorithm's ability to produce robust, efficient segmentation is demonstrated with numerous experimental results on challenging sequences from our own as well as public RGBD data sets.



### Object category learning and retrieval with weak supervision
- **Arxiv ID**: http://arxiv.org/abs/1801.08985v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1801.08985v2)
- **Published**: 2018-01-26 21:47:59+00:00
- **Updated**: 2018-07-23 20:22:15+00:00
- **Authors**: Steven Hickson, Anelia Angelova, Irfan Essa, Rahul Sukthankar
- **Comment**: Camera-ready version for NIPS 2017 workshop Learning with Limited
  Labeled Data
- **Journal**: None
- **Summary**: We consider the problem of retrieving objects from image data and learning to classify them into meaningful semantic categories with minimal supervision. To that end, we propose a fully differentiable unsupervised deep clustering approach to learn semantic classes in an end-to-end fashion without individual class labeling using only unlabeled object proposals. The key contributions of our work are 1) a kmeans clustering objective where the clusters are learned as parameters of the network and are represented as memory units, and 2) simultaneously building a feature representation, or embedding, while learning to cluster it. This approach shows promising results on two popular computer vision datasets: on CIFAR10 for clustering objects, and on the more complex and challenging Cityscapes dataset for semantically discovering classes which visually correspond to cars, people, and bicycles. Currently, the only supervision provided is segmentation objectness masks, but this method can be extended to use an unsupervised objectness-based object generation mechanism which will make the approach completely unsupervised.



### A Two-point Method for PTZ Camera Calibration in Sports
- **Arxiv ID**: http://arxiv.org/abs/1801.09005v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1801.09005v1)
- **Published**: 2018-01-26 23:37:03+00:00
- **Updated**: 2018-01-26 23:37:03+00:00
- **Authors**: Jianhui Chen, Fangrui Zhu, James J. Little
- **Comment**: WACV 2018 accepted
- **Journal**: None
- **Summary**: Calibrating narrow field of view soccer cameras is challenging because there are very few field markings in the image. Unlike previous solutions, we propose a two-point method, which requires only two point correspondences given the prior knowledge of base location and orientation of a pan-tilt-zoom (PTZ) camera. We deploy this new calibration method to annotate pan-tilt-zoom data from soccer videos. The collected data are used as references for new images. We also propose a fast random forest method to predict pan-tilt angles without image-to-image feature matching, leading to an efficient calibration method for new images. We demonstrate our system on synthetic data and two real soccer datasets. Our two-point approach achieves superior performance over the state-of-the-art method.



