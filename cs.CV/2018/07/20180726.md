# Arxiv Papers in cs.CV on 2018-07-26
### Hierarchical Bilinear Pooling for Fine-Grained Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.09915v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09915v1)
- **Published**: 2018-07-26 01:46:15+00:00
- **Updated**: 2018-07-26 01:46:15+00:00
- **Authors**: Chaojian Yu, Xinyi Zhao, Qi Zheng, Peng Zhang, Xinge You
- **Comment**: 16 pages, 3 figures
- **Journal**: None
- **Summary**: Fine-grained visual recognition is challenging because it highly relies on the modeling of various semantic parts and fine-grained feature learning. Bilinear pooling based models have been shown to be effective at fine-grained recognition, while most previous approaches neglect the fact that inter-layer part feature interaction and fine-grained feature learning are mutually correlated and can reinforce each other. In this paper, we present a novel model to address these issues. First, a cross-layer bilinear pooling approach is proposed to capture the inter-layer part feature relations, which results in superior performance compared with other bilinear pooling based approaches. Second, we propose a novel hierarchical bilinear pooling framework to integrate multiple cross-layer bilinear features to enhance their representation capability. Our formulation is intuitive, efficient and achieves state-of-the-art results on the widely used fine-grained recognition datasets.



### Scaled Simplex Representation for Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1807.09930v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09930v3)
- **Published**: 2018-07-26 02:51:27+00:00
- **Updated**: 2019-09-23 12:23:37+00:00
- **Authors**: Jun Xu, Mengyang Yu, Ling Shao, Wangmeng Zuo, Deyu Meng, Lei Zhang, David Zhang
- **Comment**: Accepted by IEEE Transactions on Cybernetics. 13 pages, 9 figures, 10
  tables. Code can be found at https://github.com/csjunxu/SSRSC
- **Journal**: None
- **Summary**: The self-expressive property of data points, i.e., each data point can be linearly represented by the other data points in the same subspace, has proven effective in leading subspace clustering methods. Most self-expressive methods usually construct a feasible affinity matrix from a coefficient matrix, obtained by solving an optimization problem. However, the negative entries in the coefficient matrix are forced to be positive when constructing the affinity matrix via exponentiation, absolute symmetrization, or squaring operations. This consequently damages the inherent correlations among the data. Besides, the affine constraint used in these methods is not flexible enough for practical applications. To overcome these problems, in this paper, we introduce a scaled simplex representation (SSR) for subspace clustering problem. Specifically, the non-negative constraint is used to make the coefficient matrix physically meaningful, and the coefficient vector is constrained to be summed up to a scalar s<1 to make it more discriminative. The proposed SSR based subspace clustering (SSRSC) model is reformulated as a linear equality-constrained problem, which is solved efficiently under the alternating direction method of multipliers framework. Experiments on benchmark datasets demonstrate that the proposed SSRSC algorithm is very efficient and outperforms state-of-the-art subspace clustering methods on accuracy. The code can be found at https://github.com/csjunxu/SSRSC.



### Structured Point Cloud Data Analysis via Regularized Tensor Regression for Process Modeling and Optimization
- **Arxiv ID**: http://arxiv.org/abs/1807.10278v3
- **DOI**: 10.1080/00401706.2018.1529628
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.10278v3)
- **Published**: 2018-07-26 02:57:49+00:00
- **Updated**: 2018-12-01 08:33:17+00:00
- **Authors**: Hao Yan, Kamran Paynabar, Massimo Pacella
- **Comment**: Technometrics, accepted
- **Journal**: Technometrics 61.3 (2019): 385-395
- **Summary**: Advanced 3D metrology technologies such as Coordinate Measuring Machine (CMM) and laser 3D scanners have facilitated the collection of massive point cloud data, beneficial for process monitoring, control and optimization. However, due to their high dimensionality and structure complexity, modeling and analysis of point clouds are still a challenge. In this paper, we utilize multilinear algebra techniques and propose a set of tensor regression approaches to model the variational patterns of point clouds and to link them to process variables. The performance of the proposed methods is evaluated through simulations and a real case study of turning process optimization.



### HiDDeN: Hiding Data With Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.09937v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.09937v1)
- **Published**: 2018-07-26 03:25:15+00:00
- **Updated**: 2018-07-26 03:25:15+00:00
- **Authors**: Jiren Zhu, Russell Kaplan, Justin Johnson, Li Fei-Fei
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work has shown that deep neural networks are highly sensitive to tiny perturbations of input images, giving rise to adversarial examples. Though this property is usually considered a weakness of learned models, we explore whether it can be beneficial. We find that neural networks can learn to use invisible perturbations to encode a rich amount of useful information. In fact, one can exploit this capability for the task of data hiding. We jointly train encoder and decoder networks, where given an input message and cover image, the encoder produces a visually indistinguishable encoded image, from which the decoder can recover the original message. We show that these encodings are competitive with existing data hiding algorithms, and further that they can be made robust to noise: our models learn to reconstruct hidden information in an encoded image despite the presence of Gaussian blurring, pixel-wise dropout, cropping, and JPEG compression. Even though JPEG is non-differentiable, we show that a robust model can be trained using differentiable approximations. Finally, we demonstrate that adversarial training improves the visual quality of encoded images.



### Reverse Attention for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.09940v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09940v2)
- **Published**: 2018-07-26 03:30:57+00:00
- **Updated**: 2019-04-15 14:46:46+00:00
- **Authors**: Shuhan Chen, Xiuli Tan, Ben Wang, Xuelong Hu
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Benefit from the quick development of deep learning techniques, salient object detection has achieved remarkable progresses recently. However, there still exists following two major challenges that hinder its application in embedded devices, low resolution output and heavy model weight. To this end, this paper presents an accurate yet compact deep network for efficient salient object detection. More specifically, given a coarse saliency prediction in the deepest layer, we first employ residual learning to learn side-output residual features for saliency refinement, which can be achieved with very limited convolutional parameters while keep accuracy. Secondly, we further propose reverse attention to guide such side-output residual learning in a top-down manner. By erasing the current predicted salient regions from side-output features, the network can eventually explore the missing object parts and details which results in high resolution and accuracy. Experiments on six benchmark datasets demonstrate that the proposed approach compares favorably against state-of-the-art methods, and with advantages in terms of simplicity, efficiency (45 FPS) and model size (81 MB).



### Computationally Efficient Measures of Internal Neuron Importance
- **Arxiv ID**: http://arxiv.org/abs/1807.09946v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.09946v1)
- **Published**: 2018-07-26 03:47:45+00:00
- **Updated**: 2018-07-26 03:47:45+00:00
- **Authors**: Avanti Shrikumar, Jocelin Su, Anshul Kundaje
- **Comment**: 7 pages, 2 figures
- **Journal**: None
- **Summary**: The challenge of assigning importance to individual neurons in a network is of interest when interpreting deep learning models. In recent work, Dhamdhere et al. proposed Total Conductance, a "natural refinement of Integrated Gradients" for attributing importance to internal neurons. Unfortunately, the authors found that calculating conductance in tensorflow required the addition of several custom gradient operators and did not scale well. In this work, we show that the formula for Total Conductance is mathematically equivalent to Path Integrated Gradients computed on a hidden layer in the network. We provide a scalable implementation of Total Conductance using standard tensorflow gradient operators that we call Neuron Integrated Gradients. We compare Neuron Integrated Gradients to DeepLIFT, a pre-existing computationally efficient approach that is applicable to calculating internal neuron importance. We find that DeepLIFT produces strong empirical results and is faster to compute, but because it lacks the theoretical properties of Neuron Integrated Gradients, it may not always be preferred in practice. Colab notebook reproducing results: http://bit.ly/neuronintegratedgradients



### Learning to Forecast and Refine Residual Motion for Image-to-Video Generation
- **Arxiv ID**: http://arxiv.org/abs/1807.09951v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09951v1)
- **Published**: 2018-07-26 04:42:58+00:00
- **Updated**: 2018-07-26 04:42:58+00:00
- **Authors**: Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, Dimitris Metaxas
- **Comment**: 17 pages, 8 figures, 4 tables, accepted by ECCV 2018
- **Journal**: None
- **Summary**: We consider the problem of image-to-video translation, where an input image is translated into an output video containing motions of a single object. Recent methods for such problems typically train transformation networks to generate future frames conditioned on the structure sequence. Parallel work has shown that short high-quality motions can be generated by spatiotemporal generative networks that leverage temporal knowledge from the training data. We combine the benefits of both approaches and propose a two-stage generation framework where videos are generated from structures and then refined by temporal signals. To model motions more efficiently, we train networks to learn residual motion between the current and future frames, which avoids learning motion-irrelevant details. We conduct extensive experiments on two image-to-video translation tasks: facial expression retargeting and human pose forecasting. Superior results over the state-of-the-art methods on both tasks demonstrate the effectiveness of our approach.



### Multi-temporal Sentinel-1 and -2 Data Fusion for Optical Image Simulation
- **Arxiv ID**: http://arxiv.org/abs/1807.09954v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09954v1)
- **Published**: 2018-07-26 04:51:02+00:00
- **Updated**: 2018-07-26 04:51:02+00:00
- **Authors**: Wei He, Naoto Yokoya
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present the optical image simulation from a synthetic aperture radar (SAR) data using deep learning based methods. Two models, i.e., optical image simulation directly from the SAR data and from multi-temporal SARoptical data, are proposed to testify the possibilities. The deep learning based methods that we chose to achieve the models are a convolutional neural network (CNN) with a residual architecture and a conditional generative adversarial network (cGAN). We validate our models using the Sentinel-1 and -2 datasets. The experiments demonstrate that the model with multi-temporal SAR-optical data can successfully simulate the optical image, meanwhile, the model with simple SAR data as input failed. The optical image simulation results indicate the possibility of SARoptical information blending for the subsequent applications such as large-scale cloud removal, and optical data temporal superresolution. We also investigate the sensitivity of the proposed models against the training samples, and reveal possible future directions.



### Pythia v0.1: the Winning Entry to the VQA Challenge 2018
- **Arxiv ID**: http://arxiv.org/abs/1807.09956v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09956v2)
- **Published**: 2018-07-26 04:57:43+00:00
- **Updated**: 2018-07-27 17:31:54+00:00
- **Authors**: Yu Jiang, Vivek Natarajan, Xinlei Chen, Marcus Rohrbach, Dhruv Batra, Devi Parikh
- **Comment**: None
- **Journal**: None
- **Summary**: This document describes Pythia v0.1, the winning entry from Facebook AI Research (FAIR)'s A-STAR team to the VQA Challenge 2018.   Our starting point is a modular re-implementation of the bottom-up top-down (up-down) model. We demonstrate that by making subtle but important changes to the model architecture and the learning rate schedule, fine-tuning image features, and adding data augmentation, we can significantly improve the performance of the up-down model on VQA v2.0 dataset -- from 65.67% to 70.22%.   Furthermore, by using a diverse ensemble of models trained with different features and on different datasets, we are able to significantly improve over the 'standard' way of ensembling (i.e. same model with different random seeds) by 1.31%. Overall, we achieve 72.27% on the test-std split of the VQA v2.0 dataset. Our code in its entirety (training, evaluation, data-augmentation, ensembling) and pre-trained models are publicly available at: https://github.com/facebookresearch/pythia



### Rethinking the Form of Latent States in Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1807.09958v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.09958v1)
- **Published**: 2018-07-26 05:26:15+00:00
- **Updated**: 2018-07-26 05:26:15+00:00
- **Authors**: Bo Dai, Deming Ye, Dahua Lin
- **Comment**: ECCV 2018, first two authors contribute equally
- **Journal**: None
- **Summary**: RNNs and their variants have been widely adopted for image captioning. In RNNs, the production of a caption is driven by a sequence of latent states. Existing captioning models usually represent latent states as vectors, taking this practice for granted. We rethink this choice and study an alternative formulation, namely using two-dimensional maps to encode latent states. This is motivated by the curiosity about a question: how the spatial structures in the latent states affect the resultant captions? Our study on MSCOCO and Flickr30k leads to two significant observations. First, the formulation with 2D states is generally more effective in captioning, consistently achieving higher performance with comparable parameter sizes. Second, 2D states preserve spatial locality. Taking advantage of this, we visually reveal the internal dynamics in the process of caption generation, as well as the connections between input visual domain and output linguistic domain.



### Iterative Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1807.09959v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09959v1)
- **Published**: 2018-07-26 05:26:46+00:00
- **Updated**: 2018-07-26 05:26:46+00:00
- **Authors**: Viresh Ranjan, Hieu Le, Minh Hoai
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: In this work, we tackle the problem of crowd counting in images. We present a Convolutional Neural Network (CNN) based density estimation approach to solve this problem. Predicting a high resolution density map in one go is a challenging task. Hence, we present a two branch CNN architecture for generating high resolution density maps, where the first branch generates a low resolution density map, and the second branch incorporates the low resolution prediction and feature maps from the first branch to generate a high resolution density map. We also propose a multi-stage extension of our approach where each stage in the pipeline utilizes the predictions from all the previous stages. Empirical comparison with the previous state-of-the-art crowd counting methods shows that our method achieves the lowest mean absolute error on three challenging crowd counting benchmarks: Shanghaitech, WorldExpo'10, and UCF datasets.



### Face De-Spoofing: Anti-Spoofing via Noise Modeling
- **Arxiv ID**: http://arxiv.org/abs/1807.09968v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09968v1)
- **Published**: 2018-07-26 06:21:12+00:00
- **Updated**: 2018-07-26 06:21:12+00:00
- **Authors**: Amin Jourabloo, Yaojie Liu, Xiaoming Liu
- **Comment**: To appear in ECCV 2018. The first two authors contributed equally to
  this work
- **Journal**: None
- **Summary**: Many prior face anti-spoofing works develop discriminative models for recognizing the subtle differences between live and spoof faces. Those approaches often regard the image as an indivisible unit, and process it holistically, without explicit modeling of the spoofing process. In this work, motivated by the noise modeling and denoising algorithms, we identify a new problem of face de-spoofing, for the purpose of anti-spoofing: inversely decomposing a spoof face into a spoof noise and a live face, and then utilizing the spoof noise for classification. A CNN architecture with proper constraints and supervisions is proposed to overcome the problem of having no ground truth for the decomposition. We evaluate the proposed method on multiple face anti-spoofing databases. The results show promising improvements due to our spoof noise modeling. Moreover, the estimated spoof noise provides a visualization which helps to understand the added spoof noise by each spoof medium.



### A Minimal Closed-Form Solution for Multi-Perspective Pose Estimation using Points and Lines
- **Arxiv ID**: http://arxiv.org/abs/1807.09970v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.09970v1)
- **Published**: 2018-07-26 06:30:40+00:00
- **Updated**: 2018-07-26 06:30:40+00:00
- **Authors**: Pedro Miraldo, Tiago Dias, Srikumar Ramalingam
- **Comment**: 22 pages, 6 figures
- **Journal**: European Conference on Computer Vision (ECCV), 2018
- **Summary**: We propose a minimal solution for pose estimation using both points and lines for a multi-perspective camera. In this paper, we treat the multi-perspective camera as a collection of rigidly attached perspective cameras. These type of imaging devices are useful for several computer vision applications that require a large coverage such as surveillance, self-driving cars, and motion-capture studios. While prior methods have considered the cases using solely points or lines, the hybrid case involving both points and lines has not been solved for multi-perspective cameras. We present the solutions for two cases. In the first case, we are given 2D to 3D correspondences for two points and one line. In the later case, we are given 2D to 3D correspondences for one point and two lines. We show that the solution for the case of two points and one line can be formulated as a fourth degree equation. This is interesting because we can get a closed-form solution and thereby achieve high computational efficiency. The later case involving two lines and one point can be mapped to an eighth degree equation. We show simulations and real experiments to demonstrate the advantages and benefits over existing methods.



### Bottom-up Pose Estimation of Multiple Person with Bounding Box Constraint
- **Arxiv ID**: http://arxiv.org/abs/1807.09972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09972v1)
- **Published**: 2018-07-26 06:33:32+00:00
- **Updated**: 2018-07-26 06:33:32+00:00
- **Authors**: Miaopeng Li, Zimeng Zhou, Jie Li, Xinguo Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a new method for multi-person pose estimation which combines the traditional bottom-up and the top-down methods. Specifically, we perform the network feed-forwarding in a bottom-up manner, and then parse the poses with bounding box constraints in a top-down manner. In contrast to the previous top-down methods, our method is robust to bounding box shift and tightness. We extract features from an original image by a residual network and train the network to learn both the confidence maps of joints and the connection relationships between joints. During testing, the predicted confidence maps, the connection relationships and the bounding boxes are used to parse the poses of all persons. The experimental results showed that our method learns more accurate human poses especially in challenging situations and gains better time performance, compared with the bottom-up and the top-down methods.



### Person Re-identification with Deep Similarity-Guided Graph Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1807.09975v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09975v1)
- **Published**: 2018-07-26 06:56:51+00:00
- **Updated**: 2018-07-26 06:56:51+00:00
- **Authors**: Yantao Shen, Hongsheng Li, Shuai Yi, Dapeng Chen, Xiaogang Wang
- **Comment**: accepted to ECCV 2018
- **Journal**: None
- **Summary**: The person re-identification task requires to robustly estimate visual similarities between person images. However, existing person re-identification models mostly estimate the similarities of different image pairs of probe and gallery images independently while ignores the relationship information between different probe-gallery pairs. As a result, the similarity estimation of some hard samples might not be accurate. In this paper, we propose a novel deep learning framework, named Similarity-Guided Graph Neural Network (SGGNN) to overcome such limitations. Given a probe image and several gallery images, SGGNN creates a graph to represent the pairwise relationships between probe-gallery pairs (nodes) and utilizes such relationships to update the probe-gallery relation features in an end-to-end manner. Accurate similarity estimation can be achieved by using such updated probe-gallery relation features for prediction. The input features for nodes on the graph are the relation features of different probe-gallery image pairs. The probe-gallery relation feature updating is then performed by the messages passing in SGGNN, which takes other nodes' information into account for similarity estimation. Different from conventional GNN approaches, SGGNN learns the edge weights with rich labels of gallery instance pairs directly, which provides relation fusion more precise information. The effectiveness of our proposed method is validated on three public person re-identification datasets.



### Recurrent Fusion Network for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1807.09986v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09986v3)
- **Published**: 2018-07-26 07:25:06+00:00
- **Updated**: 2018-07-31 03:42:15+00:00
- **Authors**: Wenhao Jiang, Lin Ma, Yu-Gang Jiang, Wei Liu, Tong Zhang
- **Comment**: ECCV-18
- **Journal**: None
- **Summary**: Recently, much advance has been made in image captioning, and an encoder-decoder framework has been adopted by all the state-of-the-art models. Under this framework, an input image is encoded by a convolutional neural network (CNN) and then translated into natural language with a recurrent neural network (RNN). The existing models counting on this framework merely employ one kind of CNNs, e.g., ResNet or Inception-X, which describe image contents from only one specific view point. Thus, the semantic meaning of an input image cannot be comprehensively understood, which restricts the performance of captioning. In this paper, in order to exploit the complementary information from multiple encoders, we propose a novel Recurrent Fusion Network (RFNet) for tackling image captioning. The fusion process in our model can exploit the interactions among the outputs of the image encoders and then generate new compact yet informative representations for the decoder. Experiments on the MSCOCO dataset demonstrate the effectiveness of our proposed RFNet, which sets a new state-of-the-art for image captioning.



### Divide and Grow: Capturing Huge Diversity in Crowd Images with Incrementally Growing CNN
- **Arxiv ID**: http://arxiv.org/abs/1807.09993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09993v1)
- **Published**: 2018-07-26 07:52:17+00:00
- **Updated**: 2018-07-26 07:52:17+00:00
- **Authors**: Deepak Babu Sam, Neeraj N Sajjan, R. Venkatesh Babu
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Automated counting of people in crowd images is a challenging task. The major difficulty stems from the large diversity in the way people appear in crowds. In fact, features available for crowd discrimination largely depend on the crowd density to the extent that people are only seen as blobs in a highly dense scene. We tackle this problem with a growing CNN which can progressively increase its capacity to account for the wide variability seen in crowd scenes. Our model starts from a base CNN density regressor, which is trained in equivalence on all types of crowd images. In order to adapt with the huge diversity, we create two child regressors which are exact copies of the base CNN. A differential training procedure divides the dataset into two clusters and fine-tunes the child networks on their respective specialties. Consequently, without any hand-crafted criteria for forming specialties, the child regressors become experts on certain types of crowds. The child networks are again split recursively, creating two experts at every division. This hierarchical training leads to a CNN tree, where the child regressors are more fine experts than any of their parents. The leaf nodes are taken as the final experts and a classifier network is then trained to predict the correct specialty for a given test image patch. The proposed model achieves higher count accuracy on major crowd datasets. Further, we analyse the characteristics of specialties mined automatically by our method.



### Naturalistic Driver Intention and Path Prediction using Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.09995v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09995v1)
- **Published**: 2018-07-26 07:57:13+00:00
- **Updated**: 2018-07-26 07:57:13+00:00
- **Authors**: Alex Zyner, Stewart Worrall, Eduardo Nebot
- **Comment**: Submitted to IEEE Transactions on Intelligent Transportation Systems
- **Journal**: None
- **Summary**: Understanding the intentions of drivers at intersections is a critical component for autonomous vehicles. Urban intersections that do not have traffic signals are a common epicentre of highly variable vehicle movement and interactions. We present a method for predicting driver intent at urban intersections through multi-modal trajectory prediction with uncertainty. Our method is based on recurrent neural networks combined with a mixture density network output layer. To consolidate the multi-modal nature of the output probability distribution, we introduce a clustering algorithm that extracts the set of possible paths that exist in the prediction output, and ranks them according to likelihood. To verify the method's performance and generalizability, we present a real-world dataset that consists of over 23,000 vehicles traversing five different intersections, collected using a vehicle mounted Lidar based tracking system. An array of metrics is used to demonstrate the performance of the model against several baselines.



### A Data-driven Prior on Facet Orientation for Semantic Mesh Labeling
- **Arxiv ID**: http://arxiv.org/abs/1807.09999v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09999v1)
- **Published**: 2018-07-26 08:09:15+00:00
- **Updated**: 2018-07-26 08:09:15+00:00
- **Authors**: Andrea Romanoni, Matteo Matteucci
- **Comment**: Accepted at 3DV2018
- **Journal**: None
- **Summary**: Mesh labeling is the key problem of classifying the facets of a 3D mesh with a label among a set of possible ones. State-of-the-art methods model mesh labeling as a Markov Random Field over the facets. These algorithms map image segmentations to the mesh by minimizing an energy function that comprises a data term, a smoothness terms, and class-specific priors. The latter favor a labeling with respect to another depending on the orientation of the facet normals. In this paper we propose a novel energy term that acts as a prior, but does not require any prior knowledge about the scene nor scene-specific relationship among classes. It bootstraps from a coarse mapping of the 2D segmentations on the mesh, and it favors the facets to be labeled according to the statistics of the mesh normals in their neighborhood. We tested our approach against five different datasets and, even if we do not inject prior knowledge, our method adapts to the data and overcomes the state-of-the-art.



### Deep Pictorial Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/1807.10002v1
- **DOI**: 10.1007/978-3-030-01261-8_44
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10002v1)
- **Published**: 2018-07-26 08:14:46+00:00
- **Updated**: 2018-07-26 08:14:46+00:00
- **Authors**: Seonwook Park, Adrian Spurr, Otmar Hilliges
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating human gaze from natural eye images only is a challenging task. Gaze direction can be defined by the pupil- and the eyeball center where the latter is unobservable in 2D images. Hence, achieving highly accurate gaze estimates is an ill-posed problem. In this paper, we introduce a novel deep neural network architecture specifically designed for the task of gaze estimation from single eye input. Instead of directly regressing two angles for the pitch and yaw of the eyeball, we regress to an intermediate pictorial representation which in turn simplifies the task of 3D gaze direction estimation. Our quantitative and qualitative results show that our approach achieves higher accuracies than the state-of-the-art and is robust to variation in gaze, head pose and image quality.



### Instance Segmentation by Deep Coloring
- **Arxiv ID**: http://arxiv.org/abs/1807.10007v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10007v1)
- **Published**: 2018-07-26 08:20:15+00:00
- **Updated**: 2018-07-26 08:20:15+00:00
- **Authors**: Victor Kulikov, Victor Yurchenko, Victor Lempitsky
- **Comment**: 10 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: We propose a new and, arguably, a very simple reduction of instance segmentation to semantic segmentation. This reduction allows to train feed-forward non-recurrent deep instance segmentation systems in an end-to-end fashion using architectures that have been proposed for semantic segmentation. Our approach proceeds by introducing a fixed number of labels (colors) and then dynamically assigning object instances to those labels during training (coloring). A standard semantic segmentation objective is then used to train a network that can color previously unseen images. At test time, individual object instances can be recovered from the output of the trained convolutional network using simple connected component analysis. In the experimental validation, the coloring approach is shown to be capable of solving diverse instance segmentation tasks arising in autonomous driving (the Cityscapes benchmark), plant phenotyping (the CVPPP leaf segmentation challenge), and high-throughput microscopy image analysis.   The source code is publicly available: https://github.com/kulikovv/DeepColoring.



### Move Forward and Tell: A Progressive Generator of Video Descriptions
- **Arxiv ID**: http://arxiv.org/abs/1807.10018v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10018v1)
- **Published**: 2018-07-26 08:57:24+00:00
- **Updated**: 2018-07-26 08:57:24+00:00
- **Authors**: Yilei Xiong, Bo Dai, Dahua Lin
- **Comment**: Accepted by ECCV 2018
- **Journal**: European Conference on Computer Vision (ECCV), 2018
- **Summary**: We present an efficient framework that can generate a coherent paragraph to describe a given video. Previous works on video captioning usually focus on video clips. They typically treat an entire video as a whole and generate the caption conditioned on a single embedding. On the contrary, we consider videos with rich temporal structures and aim to generate paragraph descriptions that can preserve the story flow while being coherent and concise. Towards this goal, we propose a new approach, which produces a descriptive paragraph by assembling temporally localized descriptions. Given a video, it selects a sequence of distinctive clips and generates sentences thereon in a coherent manner. Particularly, the selection of clips and the production of sentences are done jointly and progressively driven by a recurrent network -- what to describe next depends on what have been said before. Here, the recurrent network is learned via self-critical sequence training with both sentence-level and paragraph-level rewards. On the ActivityNet Captions dataset, our method demonstrated the capability of generating high-quality paragraph descriptions for videos. Compared to those by other methods, the descriptions produced by our method are often more relevant, more coherent, and more concise.



### A Tensor Factorization Method for 3D Super-Resolution with Application to Dental CT
- **Arxiv ID**: http://arxiv.org/abs/1807.10027v1
- **DOI**: 10.1109/TMI.2018.2883517
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10027v1)
- **Published**: 2018-07-26 09:24:23+00:00
- **Updated**: 2018-07-26 09:24:23+00:00
- **Authors**: Janka Hatvani, Adrian Basarab, Jean-Yves Tourneret, Miklós Gyöngy, Denis Kouamé
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Available super-resolution techniques for 3D images are either computationally inefficient prior-knowledge-based iterative techniques or deep learning methods which require a large database of known low- and high-resolution image pairs. A recently introduced tensor-factorization-based approach offers a fast solution without the use of known image pairs or strict prior assumptions. In this article this factorization framework is investigated for single image resolution enhancement with an off-line estimate of the system point spread function. The technique is applied to 3D cone beam computed tomography for dental image resolution enhancement. To demonstrate the efficiency of our method, it is compared to a recent state-of-the-art iterative technique using low-rank and total variation regularizations. In contrast to this comparative technique, the proposed reconstruction technique gives a 2-order-of-magnitude improvement in running time -- 2 minutes compared to 2 hours for a dental volume of 282$\times$266$\times$392 voxels. Furthermore, it also offers slightly improved quantitative results (peak signal-to-noise ratio, segmentation quality). Another advantage of the presented technique is the low number of hyperparameters. As demonstrated in this paper, the framework is not sensitive to small changes of its parameters, proposing an ease of use.



### LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.10029v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1807.10029v1)
- **Published**: 2018-07-26 09:26:39+00:00
- **Updated**: 2018-07-26 09:26:39+00:00
- **Authors**: Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, Gang Hua
- **Comment**: ECCV'18 (European Conference on Computer Vision); Main paper + suppl.
  material
- **Journal**: None
- **Summary**: Although weight and activation quantization is an effective approach for Deep Neural Network (DNN) compression and has a lot of potentials to increase inference speed leveraging bit-operations, there is still a noticeable gap in terms of prediction accuracy between the quantized model and the full-precision model. To address this gap, we propose to jointly train a quantized, bit-operation-compatible DNN and its associated quantizers, as opposed to using fixed, handcrafted quantization schemes such as uniform or logarithmic quantization. Our method for learning the quantizers applies to both network weights and activations with arbitrary-bit precision, and our quantizers are easy to train. The comprehensive experiments on CIFAR-10 and ImageNet datasets show that our method works consistently well for various network structures such as AlexNet, VGG-Net, GoogLeNet, ResNet, and DenseNet, surpassing previous quantization methods in terms of accuracy by an appreciable margin. Code available at https://github.com/Microsoft/LQ-Nets



### Motion Feature Network: Fixed Motion Filter for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.10037v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10037v2)
- **Published**: 2018-07-26 09:45:36+00:00
- **Updated**: 2018-08-01 15:19:29+00:00
- **Authors**: Myunggi Lee, Seungeui Lee, Sungjoon Son, Gyutae Park, Nojun Kwak
- **Comment**: ECCV 2018, 14 pages, 6 figures, 4 tables
- **Journal**: None
- **Summary**: Spatio-temporal representations in frame sequences play an important role in the task of action recognition. Previously, a method of using optical flow as a temporal information in combination with a set of RGB images that contain spatial information has shown great performance enhancement in the action recognition tasks. However, it has an expensive computational cost and requires two-stream (RGB and optical flow) framework. In this paper, we propose MFNet (Motion Feature Network) containing motion blocks which make it possible to encode spatio-temporal information between adjacent frames in a unified network that can be trained end-to-end. The motion block can be attached to any existing CNN-based action recognition frameworks with only a small additional cost. We evaluated our network on two of the action recognition datasets (Jester and Something-Something) and achieved competitive performances for both datasets by training the networks from scratch.



### Automatic Processing and Solar Cell Detection in Photovoltaic Electroluminescence Images
- **Arxiv ID**: http://arxiv.org/abs/1807.10820v1
- **DOI**: 10.3233/ICA-180588
- **Categories**: **cs.CV**, stat.AP, 62P30, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1807.10820v1)
- **Published**: 2018-07-26 09:58:34+00:00
- **Updated**: 2018-07-26 09:58:34+00:00
- **Authors**: Evgenii Sovetkin, Ansgar Steland
- **Comment**: None
- **Journal**: Integrated Computer-Aided Engineering, vol. 26, no. 2, pp.
  123-137, 2019
- **Summary**: Electroluminescence (EL) imaging is a powerful and established technique for assessing the quality of photovoltaic (PV) modules, which consist of many electrically connected solar cells arranged in a grid. The analysis of imperfect real-world images requires reliable methods for preprocessing, detection and extraction of the cells. We propose several methods for those tasks, which, however, can be modified to related imaging problems where similar geometric objects need to be detected accurately. Allowing for images taken under difficult outdoor conditions, we present methods to correct for rotation and perspective distortions. The next important step is the extraction of the solar cells of a PV module, for instance to pass them to a procedure to detect and analyze defects on their surface. We propose a method based on specialized Hough transforms, which allows to extract the cells even when the module is surrounded by disturbing background and a fast method based on cumulated sums (CUSUM) change detection to extract the cell area of single-cell mini-module, where the correction of perspective distortion is implicitly done. The methods are highly automatized to allow for big data analyses. Their application to a large database of EL images substantiates that the methods work reliably on a large scale for real-world images. Simulations show that the approach achieves high accuracy, reliability and robustness. This even holds for low contrast images as evaluated by comparing the simulated accuracy for a low and a high contrast image.



### A Better Baseline for AVA
- **Arxiv ID**: http://arxiv.org/abs/1807.10066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10066v1)
- **Published**: 2018-07-26 11:11:25+00:00
- **Updated**: 2018-07-26 11:11:25+00:00
- **Authors**: Rohit Girdhar, João Carreira, Carl Doersch, Andrew Zisserman
- **Comment**: ActivityNet Workshop (AVA Challenge), CVPR 2018
- **Journal**: None
- **Summary**: We introduce a simple baseline for action localization on the AVA dataset. The model builds upon the Faster R-CNN bounding box detection framework, adapted to operate on pure spatiotemporal features - in our case produced exclusively by an I3D model pretrained on Kinetics. This model obtains 21.9% average AP on the validation set of AVA v2.1, up from 14.5% for the best RGB spatiotemporal model used in the original AVA paper (which was pretrained on Kinetics and ImageNet), and up from 11.3 of the publicly available baseline using a ResNet101 image feature extractor, that was pretrained on ImageNet. Our final model obtains 22.8%/21.9% mAP on the val/test sets and outperforms all submissions to the AVA challenge at CVPR 2018.



### Loosely-Coupled Semi-Direct Monocular SLAM
- **Arxiv ID**: http://arxiv.org/abs/1807.10073v3
- **DOI**: 10.1109/LRA.2018.2889156
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.10073v3)
- **Published**: 2018-07-26 11:35:34+00:00
- **Updated**: 2019-01-06 18:50:31+00:00
- **Authors**: Seong Hun Lee, Javier Civera
- **Comment**: Accepted for publication in IEEE Robotics and Automation Letters.
  Watch video demo at: https://youtu.be/j7WnU7ZpZ8c
- **Journal**: None
- **Summary**: We propose a novel semi-direct approach for monocular simultaneous localization and mapping (SLAM) that combines the complementary strengths of direct and feature-based methods. The proposed pipeline loosely couples direct odometry and feature-based SLAM to perform three levels of parallel optimizations: (1) photometric bundle adjustment (BA) that jointly optimizes the local structure and motion, (2) geometric BA that refines keyframe poses and associated feature map points, and (3) pose graph optimization to achieve global map consistency in the presence of loop closures. This is achieved in real-time by limiting the feature-based operations to marginalized keyframes from the direct odometry module. Exhaustive evaluation on two benchmark datasets demonstrates that our system outperforms the state-of-the-art monocular odometry and SLAM systems in terms of overall accuracy and robustness.



### AlphaGAN: Generative adversarial networks for natural image matting
- **Arxiv ID**: http://arxiv.org/abs/1807.10088v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10088v1)
- **Published**: 2018-07-26 12:17:22+00:00
- **Updated**: 2018-07-26 12:17:22+00:00
- **Authors**: Sebastian Lutz, Konstantinos Amplianitis, Aljosa Smolic
- **Comment**: Accepted at BMVC 2018
- **Journal**: None
- **Summary**: We present the first generative adversarial network (GAN) for natural image matting. Our novel generator network is trained to predict visually appealing alphas with the addition of the adversarial loss from the discriminator that is trained to classify well-composited images. Further, we improve existing encoder-decoder architectures to better deal with the spatial localization issues inherited in convolutional neural networks (CNN) by using dilated convolutions to capture global context information without downscaling feature maps and losing spatial information. We present state-of-the-art results on the alphamatting online benchmark for the gradient error and give comparable results in others. Our method is particularly well suited for fine structures like hair, which is of great importance in practical matting applications, e.g. in film/TV production.



### Learning to predict crisp boundaries
- **Arxiv ID**: http://arxiv.org/abs/1807.10097v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10097v1)
- **Published**: 2018-07-26 12:40:36+00:00
- **Updated**: 2018-07-26 12:40:36+00:00
- **Authors**: Ruoxi Deng, Chunhua Shen, Shengjun Liu, Huibing Wang, Xinru Liu
- **Comment**: Accepted to European Conf. Computer Vision (ECCV) 2018
- **Journal**: None
- **Summary**: Recent methods for boundary or edge detection built on Deep Convolutional Neural Networks (CNNs) typically suffer from the issue of predicted edges being thick and need post-processing to obtain crisp boundaries. Highly imbalanced categories of boundary versus background in training data is one of main reasons for the above problem. In this work, the aim is to make CNNs produce sharp boundaries without post-processing. We introduce a novel loss for boundary detection, which is very effective for classifying imbalanced data and allows CNNs to produce crisp boundaries. Moreover, we propose an end-to-end network which adopts the bottom-up/top-down architecture to tackle the task. The proposed network effectively leverages hierarchical features and produces pixel-accurate boundary mask, which is critical to reconstruct the edge map. Our experiments illustrate that directly making crisp prediction not only promotes the visual results of CNNs, but also achieves better results against the state-of-the-art on the BSDS500 dataset (ODS F-score of .815) and the NYU Depth dataset (ODS F-score of .762).



### Effects of Degradations on Deep Neural Network Architectures
- **Arxiv ID**: http://arxiv.org/abs/1807.10108v5
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1807.10108v5)
- **Published**: 2018-07-26 13:20:57+00:00
- **Updated**: 2023-03-29 16:48:45+00:00
- **Authors**: Prasun Roy, Subhankar Ghosh, Saumik Bhattacharya, Umapada Pal
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNN) have massively influenced recent advances in large-scale image classification. More recently, a dynamic routing algorithm with capsules (groups of neurons) has shown state-of-the-art recognition performance. However, the behavior of such networks in the presence of a degrading signal (noise) is mostly unexplored. An analytical study on different network architectures toward noise robustness is essential for selecting the appropriate model in a specific application scenario. This paper presents an extensive performance analysis of six deep architectures for image classification on six most common image degradation models. In this study, we have compared VGG-16, VGG-19, ResNet-50, Inception-v3, MobileNet and CapsuleNet architectures on Gaussian white, Gaussian color, salt-and-pepper, Gaussian blur, motion blur and JPEG compression noise models.



### MRI to FDG-PET: Cross-Modal Synthesis Using 3D U-Net For Multi-Modal Alzheimer's Classification
- **Arxiv ID**: http://arxiv.org/abs/1807.10111v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10111v2)
- **Published**: 2018-07-26 13:27:48+00:00
- **Updated**: 2018-07-30 15:36:41+00:00
- **Authors**: Apoorva Sikka, Skand Vishwanath Peri, Deepti. R. Bathula
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies suggest that combined analysis of Magnetic resonance imaging~(MRI) that measures brain atrophy and positron emission tomography~(PET) that quantifies hypo-metabolism provides improved accuracy in diagnosing Alzheimer's disease. However, such techniques are limited by the availability of corresponding scans of each modality. Current work focuses on a cross-modal approach to estimate FDG-PET scans for the given MR scans using a 3D U-Net architecture. The use of the complete MR image instead of a local patch based approach helps in capturing non-local and non-linear correlations between MRI and PET modalities. The quality of the estimated PET scans is measured using quantitative metrics such as MAE, PSNR and SSIM. The efficacy of the proposed method is evaluated in the context of Alzheimer's disease classification. The accuracy using only MRI is 70.18% while joint classification using synthesized PET and MRI is 74.43% with a p-value of $0.06$. The significant improvement in diagnosis demonstrates the utility of the synthesized PET scans for multi-modal analysis.



### A Benchmark of Selected Algorithmic Differentiation Tools on Some Problems in Computer Vision and Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.10129v1
- **DOI**: None
- **Categories**: **cs.MS**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.10129v1)
- **Published**: 2018-07-26 13:42:30+00:00
- **Updated**: 2018-07-26 13:42:30+00:00
- **Authors**: Filip Šrajer, Zuzana Kukelova, Andrew Fitzgibbon
- **Comment**: Previous versions of this article appeared at AD2016---7th
  International Conference on Algorithmic Differentiation, and in Optimization
  Methods and Software, Taylor and Francis, Feb 2018 (online)
- **Journal**: None
- **Summary**: Algorithmic differentiation (AD) allows exact computation of derivatives given only an implementation of an objective function. Although many AD tools are available, a proper and efficient implementation of AD methods is not straightforward. The existing tools are often too different to allow for a general test suite. In this paper, we compare fifteen ways of computing derivatives including eleven automatic differentiation tools implementing various methods and written in various languages (C++, F#, MATLAB, Julia and Python), two symbolic differentiation tools, finite differences, and hand-derived computation.   We look at three objective functions from computer vision and machine learning. These objectives are for the most part simple, in the sense that no iterative loops are involved, and conditional statements are encapsulated in functions such as {\tt abs} or {\tt logsumexp}. However, it is important for the success of algorithmic differentiation that such `simple' objective functions are handled efficiently, as so many problems in computer vision and machine learning are of this form.   Of course, our results depend on programmer skill, and familiarity with the tools. However, we contend that this paper presents an important datapoint: a skilled programmer devoting roughly a week to each tool produced the timings we present. We have made our implementations available as open source to allow the community to replicate and update these benchmarks.



### Adaptively Transforming Graph Matching
- **Arxiv ID**: http://arxiv.org/abs/1807.10160v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10160v1)
- **Published**: 2018-07-26 14:12:40+00:00
- **Updated**: 2018-07-26 14:12:40+00:00
- **Authors**: Fudong Wang, Nan Xue, Yipeng Zhang, Xiang Bai, Gui-Song Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, many graph matching methods that incorporate pairwise constraint and that can be formulated as a quadratic assignment problem (QAP) have been proposed. Although these methods demonstrate promising results for the graph matching problem, they have high complexity in space or time. In this paper, we introduce an adaptively transforming graph matching (ATGM) method from the perspective of functional representation. More precisely, under a transformation formulation, we aim to match two graphs by minimizing the discrepancy between the original graph and the transformed graph. With a linear representation map of the transformation, the pairwise edge attributes of graphs are explicitly represented by unary node attributes, which enables us to reduce the space and time complexity significantly. Due to an efficient Frank-Wolfe method-based optimization strategy, we can handle graphs with hundreds and thousands of nodes within an acceptable amount of time. Meanwhile, because transformation map can preserve graph structures, a domain adaptation-based strategy is proposed to remove the outliers. The experimental results demonstrate that our proposed method outperforms the state-of-the-art graph matching algorithms.



### Fast and Accurate Intrinsic Symmetry Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.10162v4
- **DOI**: 10.1007/978-3-030-01246-5_26
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10162v4)
- **Published**: 2018-07-26 14:18:36+00:00
- **Updated**: 2018-12-05 04:09:38+00:00
- **Authors**: Rajendra Nagar, Shanmuganathan Raman
- **Comment**: Accepted by ECCV2018
- **Journal**: None
- **Summary**: In computer vision and graphics, various types of symmetries are extensively studied since symmetry present in objects is a fundamental cue for understanding the shape and the structure of objects. In this work, we detect the intrinsic reflective symmetry in triangle meshes where we have to find the intrinsically symmetric point for each point of the shape. We establish correspondences between functions defined on the shapes by extending the functional map framework and then recover the point-to-point correspondences. Previous approaches using the functional map for this task find the functional correspondences matrix by solving a non-linear optimization problem which makes them slow. In this work, we propose a closed form solution for this matrix which makes our approach faster. We find the closed-form solution based on our following results. If the given shape is intrinsically symmetric, then the shortest length geodesic between two intrinsically symmetric points is also intrinsically symmetric. If an eigenfunction of the Laplace-Beltrami operator for the given shape is an even (odd) function, then its restriction on the shortest length geodesic between two intrinsically symmetric points is also an even (odd) function. The sign of a low-frequency eigenfunction is the same on the neighboring points. Our method is invariant to the ordering of the eigenfunctions and has the least time complexity. We achieve the best performance on the SCAPE dataset and comparable performance with the state-of-the-art methods on the TOSCA dataset.



### Superpixel Sampling Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.10174v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10174v1)
- **Published**: 2018-07-26 14:48:22+00:00
- **Updated**: 2018-07-26 14:48:22+00:00
- **Authors**: Varun Jampani, Deqing Sun, Ming-Yu Liu, Ming-Hsuan Yang, Jan Kautz
- **Comment**: ECCV2018. Project URL: https://varunjampani.github.io/ssn/
- **Journal**: None
- **Summary**: Superpixels provide an efficient low/mid-level representation of image data, which greatly reduces the number of image primitives for subsequent vision tasks. Existing superpixel algorithms are not differentiable, making them difficult to integrate into otherwise end-to-end trainable deep neural networks. We develop a new differentiable model for superpixel sampling that leverages deep networks for learning superpixel segmentation. The resulting "Superpixel Sampling Network" (SSN) is end-to-end trainable, which allows learning task-specific superpixels with flexible loss functions and has fast runtime. Extensive experimental analysis indicates that SSNs not only outperform existing superpixel algorithms on traditional segmentation benchmarks, but can also learn superpixels for other tasks. In addition, SSNs can be easily integrated into downstream deep networks resulting in performance improvements.



### Linkage between piecewise constant Mumford-Shah model and ROF model and its virtue in image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.10194v2
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/1807.10194v2)
- **Published**: 2018-07-26 15:28:12+00:00
- **Updated**: 2019-10-15 00:34:31+00:00
- **Authors**: Xiaohao Cai, Raymond Chan, Carola-Bibiane Schonlieb, Gabriele Steidl, Tieyong Zeng
- **Comment**: 31 pages
- **Journal**: SIAM Journal on Scientific Computing, 41(6):B1310-B1340, 2019
- **Summary**: The piecewise constant Mumford-Shah (PCMS) model and the Rudin-Osher-Fatemi (ROF) model are two important variational models in image segmentation and image restoration, respectively. In this paper, we explore a linkage between these models. We prove that for the two-phase segmentation problem a partial minimizer of the PCMS model can be obtained by thresholding the minimizer of the ROF model. A similar linkage is still valid for multiphase segmentation under specific assumptions. Thus it opens a new segmentation paradigm: image segmentation can be done via image restoration plus thresholding. This new paradigm, which circumvents the innate non-convex property of the PCMS model, therefore improves the segmentation performance in both efficiency (much faster than state-of-the-art methods based on PCMS model, particularly when the phase number is high) and effectiveness (producing segmentation results with better quality) due to the flexibility of the ROF model in tackling degraded images, such as noisy images, blurry images or images with information loss. As a by-product of the new paradigm, we derive a novel segmentation method, called thresholded-ROF (T-ROF) method, to illustrate the virtue of managing image segmentation through image restoration techniques. The convergence of the T-ROF method is proved, and elaborate experimental results and comparisons are presented.



### A Style-Aware Content Loss for Real-time HD Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1807.10201v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10201v2)
- **Published**: 2018-07-26 15:39:59+00:00
- **Updated**: 2018-07-28 07:31:00+00:00
- **Authors**: Artsiom Sanakoyeu, Dmytro Kotovenko, Sabine Lang, Björn Ommer
- **Comment**: Accepted at ECCV18 (Oral)
- **Journal**: None
- **Summary**: Recently, style transfer has received a lot of attention. While much of this research has aimed at speeding up processing, the approaches are still lacking from a principled, art historical standpoint: a style is more than just a single image or an artist, but previous work is limited to only a single instance of a style or shows no benefit from more images. Moreover, previous work has relied on a direct comparison of art in the domain of RGB images or on CNNs pre-trained on ImageNet, which requires millions of labeled object bounding boxes and can introduce an extra bias, since it has been assembled without artistic consideration. To circumvent these issues, we propose a style-aware content loss, which is trained jointly with a deep encoder-decoder network for real-time, high-resolution stylization of images and videos. We propose a quantitative measure for evaluating the quality of a stylized image and also have art historians rank patches from our approach against those from previous work. These and our qualitative results ranging from small image patches to megapixel stylistic images and videos show that our approach better captures the subtle nature in which a style affects content.



### DeepSPINE: Automated Lumbar Vertebral Segmentation, Disc-level Designation, and Spinal Stenosis Grading Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.10215v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.10215v1)
- **Published**: 2018-07-26 15:59:49+00:00
- **Updated**: 2018-07-26 15:59:49+00:00
- **Authors**: Jen-Tang Lu, Stefano Pedemonte, Bernardo Bizzo, Sean Doyle, Katherine P. Andriole, Mark H. Michalski, R. Gilberto Gonzalez, Stuart R. Pomerantz
- **Comment**: Accepted as spotlight talk at Machine Learning for Healthcare (MLHC)
  2018. Supplementary Video: https://bit.ly/DeepSPINE
- **Journal**: None
- **Summary**: The high prevalence of spinal stenosis results in a large volume of MRI imaging, yet interpretation can be time-consuming with high inter-reader variability even among the most specialized radiologists. In this paper, we develop an efficient methodology to leverage the subject-matter-expertise stored in large-scale archival reporting and image data for a deep-learning approach to fully-automated lumbar spinal stenosis grading. Specifically, we introduce three major contributions: (1) a natural-language-processing scheme to extract level-by-level ground-truth labels from free-text radiology reports for the various types and grades of spinal stenosis (2) accurate vertebral segmentation and disc-level localization using a U-Net architecture combined with a spine-curve fitting method, and (3) a multi-input, multi-task, and multi-class convolutional neural network to perform central canal and foraminal stenosis grading on both axial and sagittal imaging series inputs with the extracted report-derived labels applied to corresponding imaging level segments. This study uses a large dataset of 22796 disc-levels extracted from 4075 patients. We achieve state-of-the-art performance on lumbar spinal stenosis classification and expect the technique will increase both radiology workflow efficiency and the perceived value of radiology reports for referring clinicians and patients.



### Unified Perceptual Parsing for Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1807.10221v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10221v1)
- **Published**: 2018-07-26 16:13:49+00:00
- **Updated**: 2018-07-26 16:13:49+00:00
- **Authors**: Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun
- **Comment**: Accepted to European Conference on Computer Vision (ECCV) 2018
- **Journal**: None
- **Summary**: Humans recognize the visual world at multiple levels: we effortlessly categorize scenes and detect objects inside, while also identifying the textures and surfaces of the objects along with their different compositional parts. In this paper, we study a new task called Unified Perceptual Parsing, which requires the machine vision systems to recognize as many visual concepts as possible from a given image. A multi-task framework called UPerNet and a training strategy are developed to learn from heterogeneous image annotations. We benchmark our framework on Unified Perceptual Parsing and show that it is able to effectively segment a wide range of concepts from images. The trained networks are further applied to discover visual knowledge in natural scenes. Models are available at \url{https://github.com/CSAILVision/unifiedparsing}.



### Medical Image Synthesis for Data Augmentation and Anonymization using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.10225v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.10225v2)
- **Published**: 2018-07-26 16:25:18+00:00
- **Updated**: 2018-09-13 19:11:23+00:00
- **Authors**: Hoo-Chang Shin, Neil A Tenenholtz, Jameson K Rogers, Christopher G Schwarz, Matthew L Senjem, Jeffrey L Gunter, Katherine Andriole, Mark Michalski
- **Comment**: Accepted for 2018 Workshop on Simulation and Synthesis in Medical
  Imaging - SASHIMI2018
- **Journal**: None
- **Summary**: Data diversity is critical to success when training deep learning models. Medical imaging data sets are often imbalanced as pathologic findings are generally rare, which introduces significant challenges when training deep learning models. In this work, we propose a method to generate synthetic abnormal MRI images with brain tumors by training a generative adversarial network using two publicly available data sets of brain MRI. We demonstrate two unique benefits that the synthetic images provide. First, we illustrate improved performance on tumor segmentation by leveraging the synthetic images as a form of data augmentation. Second, we demonstrate the value of generative models as an anonymization tool, achieving comparable tumor segmentation results when trained on the synthetic data versus when trained on real subject data. Together, these results offer a potential solution to two of the largest challenges facing machine learning in medical imaging, namely the small incidence of pathological findings, and the restrictions around sharing of patient data.



### From handcrafted to deep local features
- **Arxiv ID**: http://arxiv.org/abs/1807.10254v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10254v3)
- **Published**: 2018-07-26 17:28:28+00:00
- **Updated**: 2019-06-14 13:52:49+00:00
- **Authors**: Gabriela Csurka, Christopher R. Dance, Martin Humenberger
- **Comment**: Preprint
- **Journal**: None
- **Summary**: This paper presents an overview of the evolution of local features from handcrafted to deep-learning-based methods, followed by a discussion of several benchmarks and papers evaluating such local features. Our investigations are motivated by 3D reconstruction problems, where the precise location of the features is important. As we describe these methods, we highlight and explain the challenges of feature extraction and potential ways to overcome them. We first present handcrafted methods, followed by methods based on classical machine learning and finally we discuss methods based on deep-learning. This largely chronologically-ordered presentation will help the reader to fully understand the topic of image and region description in order to make best use of it in modern computer vision applications. In particular, understanding handcrafted methods and their motivation can help to understand modern approaches and how machine learning is used to improve the results. We also provide references to most of the relevant literature and code.



### Layer-structured 3D Scene Inference via View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1807.10264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10264v1)
- **Published**: 2018-07-26 17:44:09+00:00
- **Updated**: 2018-07-26 17:44:09+00:00
- **Authors**: Shubham Tulsiani, Richard Tucker, Noah Snavely
- **Comment**: Project url: http://shubhtuls.github.io/lsi
- **Journal**: None
- **Summary**: We present an approach to infer a layer-structured 3D representation of a scene from a single input image. This allows us to infer not only the depth of the visible pixels, but also to capture the texture and depth for content in the scene that is not directly visible. We overcome the challenge posed by the lack of direct supervision by instead leveraging a more naturally available multi-view supervisory signal. Our insight is to use view synthesis as a proxy task: we enforce that our representation (inferred from a single image), when rendered from a novel perspective, matches the true observed image. We present a learning framework that operationalizes this insight using a new, differentiable novel view renderer. We provide qualitative and quantitative validation of our approach in two different settings, and demonstrate that we can learn to capture the hidden aspects of a scene.



### Generating 3D faces using Convolutional Mesh Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1807.10267v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10267v3)
- **Published**: 2018-07-26 17:53:50+00:00
- **Updated**: 2018-07-31 20:13:00+00:00
- **Authors**: Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, Michael J. Black
- **Comment**: None
- **Journal**: European Conference on Computer Vision 2018
- **Summary**: Learned 3D representations of human faces are useful for computer vision problems such as 3D face tracking and reconstruction from images, as well as graphics applications such as character generation and animation. Traditional models learn a latent representation of a face using linear subspaces or higher-order tensor generalizations. Due to this linearity, they can not capture extreme deformations and non-linear expressions. To address this, we introduce a versatile model that learns a non-linear representation of a face using spectral convolutions on a mesh surface. We introduce mesh sampling operations that enable a hierarchical mesh representation that captures non-linear variations in shape and expression at multiple scales within the model. In a variational setting, our model samples diverse realistic 3D faces from a multivariate Gaussian distribution. Our training data consists of 20,466 meshes of extreme expressions captured over 12 different subjects. Despite limited training data, our trained model outperforms state-of-the-art face models with 50% lower reconstruction error, while using 75% fewer parameters. We also show that, replacing the expression space of an existing state-of-the-art face model with our autoencoder, achieves a lower reconstruction error. Our data, model and code are available at http://github.com/anuragranj/coma



### Evaluating and Understanding the Robustness of Adversarial Logit Pairing
- **Arxiv ID**: http://arxiv.org/abs/1807.10272v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.10272v2)
- **Published**: 2018-07-26 17:58:26+00:00
- **Updated**: 2018-11-23 19:07:57+00:00
- **Authors**: Logan Engstrom, Andrew Ilyas, Anish Athalye
- **Comment**: NeurIPS SECML 2018. Source code at
  https://github.com/labsix/adversarial-logit-pairing-analysis
- **Journal**: None
- **Summary**: We evaluate the robustness of Adversarial Logit Pairing, a recently proposed defense against adversarial examples. We find that a network trained with Adversarial Logit Pairing achieves 0.6% accuracy in the threat model in which the defense is considered. We provide a brief overview of the defense and the threat models/claims considered, as well as a discussion of the methodology and results of our attack, which may offer insights into the reasons underlying the vulnerability of ALP to adversarial attack.



### Semantically Meaningful View Selection
- **Arxiv ID**: http://arxiv.org/abs/1807.10303v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.10303v1)
- **Published**: 2018-07-26 18:17:19+00:00
- **Updated**: 2018-07-26 18:17:19+00:00
- **Authors**: Joris Guérin, Olivier Gibaru, Eric Nyiri, Stéphane Thiery, Byron Boots
- **Comment**: 6 pages double columns, 5 figures, 3 tables, Accepted for
  presentation at IROS 2018, Madrid, Spain (46% acceptance)
- **Journal**: None
- **Summary**: An understanding of the nature of objects could help robots to solve both high-level abstract tasks and improve performance at lower-level concrete tasks. Although deep learning has facilitated progress in image understanding, a robot's performance in problems like object recognition often depends on the angle from which the object is observed. Traditionally, robot sorting tasks rely on a fixed top-down view of an object. By changing its viewing angle, a robot can select a more semantically informative view leading to better performance for object recognition. In this paper, we introduce the problem of semantic view selection, which seeks to find good camera poses to gain semantic knowledge about an observed object. We propose a conceptual formulation of the problem, together with a solvable relaxation based on clustering. We then present a new image dataset consisting of around 10k images representing various views of 144 objects under different poses. Finally we use this dataset to propose a first solution to the problem by training a neural network to predict a "semantic score" from a top view image and camera pose. The views predicted to have higher scores are then shown to provide better clustering results than fixed top-down views.



### Discriminative multi-view Privileged Information learning for image re-ranking
- **Arxiv ID**: http://arxiv.org/abs/1808.04437v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1808.04437v1)
- **Published**: 2018-07-26 18:57:14+00:00
- **Updated**: 2018-07-26 18:57:14+00:00
- **Authors**: Jun Li, Chang Xu, Wankou Yang, Changyin Sun, Dacheng Tao, Hong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional multi-view re-ranking methods usually perform asymmetrical matching between the region of interest (ROI) in the query image and the whole target image for similarity computation. Due to the inconsistency in the visual appearance, this practice tends to degrade the retrieval accuracy particularly when the image ROI, which is usually interpreted as the image objectness, accounts for a smaller region in the image. Since Privileged Information (PI), which can be viewed as the image prior, enables well characterizing the image objectness, we are aiming at leveraging PI for further improving the performance of the multi-view re-ranking accuracy in this paper. Towards this end, we propose a discriminative multi-view re-ranking approach in which both the original global image visual contents and the local auxiliary PI features are simultaneously integrated into a unified training framework for generating the latent subspaces with sufficient discriminating power. For the on-the-fly re-ranking, since the multi-view PI features are unavailable, we only project the original multi-view image representations onto the latent subspace, and thus the re-ranking can be achieved by computing and sorting the distances from the multi-view embeddings to the separating hyperplane. Extensive experimental evaluations on the two public benchmarks Oxford5k and Paris6k reveal our approach provides further performance boost for accurate image re-ranking, whilst the comparative study demonstrates the advantage of our method against other multi-view re-ranking methods.



### A general metric for identifying adversarial images
- **Arxiv ID**: http://arxiv.org/abs/1807.10335v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.10335v1)
- **Published**: 2018-07-26 19:29:37+00:00
- **Updated**: 2018-07-26 19:29:37+00:00
- **Authors**: Siddharth Krishna Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: It is well known that a determined adversary can fool a neural network by making imperceptible adversarial perturbations to an image. Recent studies have shown that these perturbations can be detected even without information about the neural network if the strategy taken by the adversary is known beforehand. Unfortunately, these studies suffer from the generalization limitation -- the detection method has to be recalibrated every time the adversary changes his strategy. In this study, we attempt to overcome the generalization limitation by deriving a metric which reliably identifies adversarial images even when the approach taken by the adversary is unknown. Our metric leverages key differences between the spectra of clean and adversarial images when an image is treated as a matrix. Our metric is able to detect adversarial images across different datasets and attack strategies without any additional re-calibration. In addition, our approach provides geometric insights into several unanswered questions about adversarial perturbations.



### A Reinforcement Learning Approach to Target Tracking in a Camera Network
- **Arxiv ID**: http://arxiv.org/abs/1807.10336v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10336v2)
- **Published**: 2018-07-26 19:37:57+00:00
- **Updated**: 2018-12-03 14:03:06+00:00
- **Authors**: Anil Sharma, Prabhat Kumar, Saket Anand, Sanjit K. Kaul
- **Comment**: The current version has a fault in the experiments section so we
  would like to withdraw the paper
- **Journal**: None
- **Summary**: Target tracking in a camera network is an important task for surveillance and scene understanding. The task is challenging due to disjoint views and illumination variation in different cameras. In this direction, many graph-based methods were proposed using appearance-based features. However, the appearance information fades with high illumination variation in the different camera FOVs. We, in this paper, use spatial and temporal information as the state of the target to learn a policy that predicts the next camera given the current state. The policy is trained using Q-learning and it does not assume any information about the topology of the camera network. We will show that the policy learns the camera network topology. We demonstrate the performance of the proposed method on the NLPR MCT dataset.



### Tackling 3D ToF Artifacts Through Learning and the FLAT Dataset
- **Arxiv ID**: http://arxiv.org/abs/1807.10376v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10376v1)
- **Published**: 2018-07-26 21:43:32+00:00
- **Updated**: 2018-07-26 21:43:32+00:00
- **Authors**: Qi Guo, Iuri Frosio, Orazio Gallo, Todd Zickler, Jan Kautz
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Scene motion, multiple reflections, and sensor noise introduce artifacts in the depth reconstruction performed by time-of-flight cameras. We propose a two-stage, deep-learning approach to address all of these sources of artifacts simultaneously. We also introduce FLAT, a synthetic dataset of 2000 ToF measurements that capture all of these nonidealities, and allows to simulate different camera hardware. Using the Kinect 2 camera as a baseline, we show improved reconstruction errors over state-of-the-art methods, on both simulated and real data.



### Conditional Prior Networks for Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1807.10378v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10378v1)
- **Published**: 2018-07-26 21:49:36+00:00
- **Updated**: 2018-07-26 21:49:36+00:00
- **Authors**: Yanchao Yang, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: Classical computation of optical flow involves generic priors (regularizers) that capture rudimentary statistics of images, but not long-range correlations or semantics. On the other hand, fully supervised methods learn the regularity in the annotated data, without explicit regularization and with the risk of overfitting. We seek to learn richer priors on the set of possible flows that are statistically compatible with an image. Once the prior is learned in a supervised fashion, one can easily learn the full map to infer optical flow directly from two or more images, without any need for (additional) supervision. We introduce a novel architecture, called Conditional Prior Network (CPN), and show how to train it to yield a conditional prior. When used in conjunction with a simple optical flow architecture, the CPN beats all variational methods and all unsupervised learning-based ones using the same data term. It performs comparably to fully supervised ones, that however are fine-tuned to a particular dataset. Our method, on the other hand, performs well even when transferred between datasets.



### A writer-independent approach for offline signature verification using deep convolutional neural networks features
- **Arxiv ID**: http://arxiv.org/abs/1807.10755v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.10755v1)
- **Published**: 2018-07-26 22:09:45+00:00
- **Updated**: 2018-07-26 22:09:45+00:00
- **Authors**: Victor L. F. Souza, Adriano L. I. Oliveira, Robert Sabourin
- **Comment**: None
- **Journal**: None
- **Summary**: The use of features extracted using a deep convolutional neural network (CNN) combined with a writer-dependent (WD) SVM classifier resulted in significant improvement in performance of handwritten signature verification (HSV) when compared to the previous state-of-the-art methods. In this work it is investigated whether the use of these CNN features provide good results in a writer-independent (WI) HSV context, based on the dichotomy transformation combined with the use of an SVM writer-independent classifier. The experiments performed in the Brazilian and GPDS datasets show that (i) the proposed approach outperformed other WI-HSV methods from the literature, (ii) in the global threshold scenario, the proposed approach was able to outperform the writer-dependent method with CNN features in the Brazilian dataset, (iii) in an user threshold scenario, the results are similar to those obtained by the writer-dependent method with CNN features.



### False Positive Reduction by Actively Mining Negative Samples for Pulmonary Nodule Detection in Chest Radiographs
- **Arxiv ID**: http://arxiv.org/abs/1807.10756v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.10756v1)
- **Published**: 2018-07-26 23:29:39+00:00
- **Updated**: 2018-07-26 23:29:39+00:00
- **Authors**: Sejin Park, Woochan Hwang, Kyu Hwan Jung, Joon Beom Seo, Namkug Kim
- **Comment**: Presented at the 2nd SIIM C-MIMI(SIIM Conference on Machine
  Intelligence in Medical Imaging)
- **Journal**: None
- **Summary**: Generating large quantities of quality labeled data in medical imaging is very time consuming and expensive. The performance of supervised algorithms for various tasks on imaging has improved drastically over the years, however the availability of data to train these algorithms have become one of the main bottlenecks for implementation. To address this, we propose a semi-supervised learning method where pseudo-negative labels from unlabeled data are used to further refine the performance of a pulmonary nodule detection network in chest radiographs. After training with the proposed network, the false positive rate was reduced to 0.1266 from 0.4864 while maintaining sensitivity at 0.89.



### Perturbation Robust Representations of Topological Persistence Diagrams
- **Arxiv ID**: http://arxiv.org/abs/1807.10400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10400v1)
- **Published**: 2018-07-26 23:39:52+00:00
- **Updated**: 2018-07-26 23:39:52+00:00
- **Authors**: Anirudh Som, Kowshik Thopalli, Karthikeyan Natesan Ramamurthy, Vinay Venkataraman, Ankita Shukla, Pavan Turaga
- **Comment**: 19 pages, 4 figures, 6 tables
- **Journal**: None
- **Summary**: Topological methods for data analysis present opportunities for enforcing certain invariances of broad interest in computer vision, including view-point in activity analysis, articulation in shape analysis, and measurement invariance in non-linear dynamical modeling. The increasing success of these methods is attributed to the complementary information that topology provides, as well as availability of tools for computing topological summaries such as persistence diagrams. However, persistence diagrams are multi-sets of points and hence it is not straightforward to fuse them with features used for contemporary machine learning tools like deep-nets. In this paper we present theoretically well-grounded approaches to develop novel perturbation robust topological representations, with the long-term view of making them amenable to fusion with contemporary learning architectures. We term the proposed representation as Perturbed Topological Signatures, which live on a Grassmann manifold and hence can be efficiently used in machine learning pipelines. We explore the use of the proposed descriptor on three applications: 3D shape analysis, view-invariant activity analysis, and non-linear dynamical modeling. We show favorable results in both high-level recognition performance and time-complexity when compared to other baseline methods.



