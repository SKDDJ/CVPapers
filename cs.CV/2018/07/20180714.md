# Arxiv Papers in cs.CV on 2018-07-14
### Real-Time Shape Tracking of Facial Landmarks
- **Arxiv ID**: http://arxiv.org/abs/1807.05333v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV - Image and Video Processing
- **Links**: [PDF](http://arxiv.org/pdf/1807.05333v1)
- **Published**: 2018-07-14 04:57:16+00:00
- **Updated**: 2018-07-14 04:57:16+00:00
- **Authors**: Hyungjoon Kim, Hyeonwoo Kim, Eenjun Hwang
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Detection of facial landmarks and accurate tracking of their shape are essential in real-time virtual makeup applications, where users can see the makeups effect by moving their face in different directions. Typical face tracking techniques detect diverse facial landmarks and track them using a point tracker such as the Kanade-Lucas-Tomasi (KLT) point tracker. Typically, 5 or 64 points are used for tracking a face. Even though these points are sufficient to track the approximate locations of facial landmarks, they are not sufficient to track the exact shape of facial landmarks. In this paper, we propose a method that can track the exact shape of facial landmarks in real-time by combining a deep learning technique and a point tracker. We detect facial landmarks accurately using SegNet, which performs semantic segmentation based on deep learning. Edge points of detected landmarks are tracked using the KLT point tracker. In spite of its popularity, the KLT point tracker suffers from the point loss problem. We solve this problem by executing SegNet periodically to calculate the shape of facial landmarks. That is, by combining the two techniques, we can avoid the computational overhead of SegNet for real-time shape tracking and the point loss problem of the KLT point tracker. We performed several experiments to evaluate the performance of our method and report some of the results herein.



### Motion Invariance in Visual Environments
- **Arxiv ID**: http://arxiv.org/abs/1807.06450v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06450v1)
- **Published**: 2018-07-14 07:16:42+00:00
- **Updated**: 2018-07-14 07:16:42+00:00
- **Authors**: Alessandro Betti, Marco Gori, Stefano Melacci
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1801.07110
- **Journal**: None
- **Summary**: The puzzle of computer vision might find new challenging solutions when we realize that most successful methods are working at image level, which is remarkably more difficult than processing directly visual streams, just as happens in nature. In this paper, we claim that their processing naturally leads to formulate the motion invariance principle, which enables the construction of a new theory of visual learning based on convolutional features. The theory addresses a number of intriguing questions that arise in natural vision, and offers a well-posed computational scheme for the discovery of convolutional filters over the retina. They are driven by the Euler-Lagrange differential equations derived from the principle of least cognitive action, that parallels laws of mechanics. Unlike traditional convolutional networks, which need massive supervision, the proposed theory offers a truly new scenario in which feature learning takes place by unsupervised processing of video signals. An experimental report of the theory is presented where we show that features extracted under motion invariance yield an improvement that can be assessed by measuring information-based indexes.



### Non-local RoIs for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.05361v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05361v1)
- **Published**: 2018-07-14 08:58:01+00:00
- **Updated**: 2018-07-14 08:58:01+00:00
- **Authors**: Shou-Yao Roy Tseng, Hwann-Tzong Chen, Shao-Heng Tai, Tyng-Luh Liu
- **Comment**: Robust Vision Challenge 2018
- **Journal**: None
- **Summary**: We introduce the concept of Non-Local RoI (NL-RoI) Block as a generic and flexible module that can be seamlessly adapted into different Mask R-CNN heads for various tasks. Mask R-CNN treats RoIs (Regions of Interest) independently and performs the prediction based on individual object bounding boxes. However, the correlation between objects may provide useful information for detection and segmentation. The proposed NL-RoI Block enables each RoI to refer to all other RoIs' information, and results in a simple, low-cost but effective module. Our experimental results show that generalizations with NL-RoI Blocks can improve the performance of Mask R-CNN for instance segmentation on the Robust Vision Challenge benchmarks.



### 3D Hand Pose Estimation using Simulation and Partial-Supervision with a Shared Latent Space
- **Arxiv ID**: http://arxiv.org/abs/1807.05380v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05380v1)
- **Published**: 2018-07-14 11:26:19+00:00
- **Updated**: 2018-07-14 11:26:19+00:00
- **Authors**: Masoud Abdi, Ehsan Abbasnejad, Chee Peng Lim, Saeid Nahavandi
- **Comment**: Oral presentation at British Machine Vision Conference (BMVC) 2018
- **Journal**: None
- **Summary**: Tremendous amounts of expensive annotated data are a vital ingredient for state-of-the-art 3d hand pose estimation. Therefore, synthetic data has been popularized as annotations are automatically available. However, models trained only with synthetic samples do not generalize to real data, mainly due to the gap between the distribution of synthetic and real data. In this paper, we propose a novel method that seeks to predict the 3d position of the hand using both synthetic and partially-labeled real data. Accordingly, we form a shared latent space between three modalities: synthetic depth image, real depth image, and pose. We demonstrate that by carefully learning the shared latent space, we can find a regression model that is able to generalize to real data. As such, we show that our method produces accurate predictions in both semi-supervised and unsupervised settings. Additionally, the proposed model is capable of generating novel, meaningful, and consistent samples from all of the three domains. We evaluate our method qualitatively and quantitively on two highly competitive benchmarks (i.e., NYU and ICVL) and demonstrate its superiority over the state-of-the-art methods. The source code will be made available at https://github.com/masabdi/LSPS.



### 3D human pose estimation from depth maps using a deep combination of poses
- **Arxiv ID**: http://arxiv.org/abs/1807.05389v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1807.05389v1)
- **Published**: 2018-07-14 12:31:38+00:00
- **Updated**: 2018-07-14 12:31:38+00:00
- **Authors**: Manuel J. Marin-Jimenez, Francisco J. Romero-Ramirez, Rafael Mu√±oz-Salinas, Rafael Medina-Carnicer
- **Comment**: Accepted for publication at "Journal of Visual Communication and
  Image Representation"
- **Journal**: None
- **Summary**: Many real-world applications require the estimation of human body joints for higher-level tasks as, for example, human behaviour understanding. In recent years, depth sensors have become a popular approach to obtain three-dimensional information. The depth maps generated by these sensors provide information that can be employed to disambiguate the poses observed in two-dimensional images. This work addresses the problem of 3D human pose estimation from depth maps employing a Deep Learning approach. We propose a model, named Deep Depth Pose (DDP), which receives a depth map containing a person and a set of predefined 3D prototype poses and returns the 3D position of the body joints of the person. In particular, DDP is defined as a ConvNet that computes the specific weights needed to linearly combine the prototypes for the given input. We have thoroughly evaluated DDP on the challenging 'ITOP' and 'UBC3V' datasets, which respectively depict realistic and synthetic samples, defining a new state-of-the-art on them.



### Specular-to-Diffuse Translation for Multi-View Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1807.05439v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05439v3)
- **Published**: 2018-07-14 20:51:30+00:00
- **Updated**: 2018-07-30 16:13:07+00:00
- **Authors**: Shihao Wu, Hui Huang, Tiziano Portenier, Matan Sela, Danny Cohen-Or, Ron Kimmel, Matthias Zwicker
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: Most multi-view 3D reconstruction algorithms, especially when shape-from-shading cues are used, assume that object appearance is predominantly diffuse. To alleviate this restriction, we introduce S2Dnet, a generative adversarial network for transferring multiple views of objects with specular reflection into diffuse ones, so that multi-view reconstruction methods can be applied more effectively. Our network extends unsupervised image-to-image translation to multi-view "specular to diffuse" translation. To preserve object appearance across multiple views, we introduce a Multi-View Coherence loss (MVC) that evaluates the similarity and faithfulness of local patches after the view-transformation. Our MVC loss ensures that the similarity of local correspondences among multi-view images is preserved under the image-to-image translation. As a result, our network yields significantly better results than several single-view baseline techniques. In addition, we carefully design and generate a large synthetic training data set using physically-based rendering. During testing, our network takes only the raw glossy images as input, without extra information such as segmentation masks or lighting estimation. Results demonstrate that multi-view reconstruction can be significantly improved using the images filtered by our network. We also show promising performance on real world training and testing data.



