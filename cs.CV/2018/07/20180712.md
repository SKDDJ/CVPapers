# Arxiv Papers in cs.CV on 2018-07-12
### Multi-Region Ensemble Convolutional Neural Network for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.10575v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1807.10575v1)
- **Published**: 2018-07-12 02:50:35+00:00
- **Updated**: 2018-07-12 02:50:35+00:00
- **Authors**: Yingruo Fan, Jacqueline C. K. Lam, Victor O. K. Li
- **Comment**: 10pages, 5 figures, Accepted by ICANN 2018
- **Journal**: None
- **Summary**: Facial expressions play an important role in conveying the emotional states of human beings. Recently, deep learning approaches have been applied to image recognition field due to the discriminative power of Convolutional Neural Network (CNN). In this paper, we first propose a novel Multi-Region Ensemble CNN (MRE-CNN) framework for facial expression recognition, which aims to enhance the learning power of CNN models by capturing both the global and the local features from multiple human face sub-regions. Second, the weighted prediction scores from each sub-network are aggregated to produce the final prediction of high accuracy. Third, we investigate the effects of different sub-regions of the whole face on facial expression recognition. Our proposed method is evaluated based on two well-known publicly available facial expression databases: AFEW 7.0 and RAF-DB, and has been shown to achieve the state-of-the-art recognition accuracy.



### Sem-GAN: Semantically-Consistent Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1807.04409v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04409v1)
- **Published**: 2018-07-12 02:55:19+00:00
- **Updated**: 2018-07-12 02:55:19+00:00
- **Authors**: Anoop Cherian, Alan Sullivan
- **Comment**: None
- **Journal**: None
- **Summary**: Unpaired image-to-image translation is the problem of mapping an image in the source domain to one in the target domain, without requiring corresponding image pairs. To ensure the translated images are realistically plausible, recent works, such as Cycle-GAN, demands this mapping to be invertible. While, this requirement demonstrates promising results when the domains are unimodal, its performance is unpredictable in a multi-modal scenario such as in an image segmentation task. This is because, invertibility does not necessarily enforce semantic correctness. To this end, we present a semantically-consistent GAN framework, dubbed Sem-GAN, in which the semantics are defined by the class identities of image segments in the source domain as produced by a semantic segmentation algorithm. Our proposed framework includes consistency constraints on the translation task that, together with the GAN loss and the cycle-constraints, enforces that the images when translated will inherit the appearances of the target domain, while (approximately) maintaining their identities from the source domain. We present experiments on several image-to-image translation tasks and demonstrate that Sem-GAN improves the quality of the translated images significantly, sometimes by more than 20% on the FCN score. Further, we show that semantic segmentation models, trained with synthetic images translated via Sem-GAN, leads to significantly better segmentation results than other variants.



### Subsampled Turbulence Removal Network
- **Arxiv ID**: http://arxiv.org/abs/1807.04418v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04418v2)
- **Published**: 2018-07-12 04:21:06+00:00
- **Updated**: 2018-08-13 05:56:51+00:00
- **Authors**: Wai Ho Chak, Chun Pong Lau, Lok Ming Lui
- **Comment**: None
- **Journal**: None
- **Summary**: We present a deep-learning approach to restore a sequence of turbulence-distorted video frames from turbulent deformations and space-time varying blurs. Instead of requiring a massive training sample size in deep networks, we purpose a training strategy that is based on a new data augmentation method to model turbulence from a relatively small dataset. Then we introduce a subsampled method to enhance the restoration performance of the presented GAN model. The contributions of the paper is threefold: first, we introduce a simple but effective data augmentation algorithm to model the turbulence in real life for training in the deep network; Second, we firstly purpose the Wasserstein GAN combined with $\ell_1$ cost for successful restoration of turbulence-corrupted video sequence; Third, we combine the subsampling algorithm to filter out strongly corrupted frames to generate a video sequence with better quality.



### Video-based Person Re-identification via 3D Convolutional Networks and Non-local Attention
- **Arxiv ID**: http://arxiv.org/abs/1807.05073v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05073v3)
- **Published**: 2018-07-12 05:30:26+00:00
- **Updated**: 2019-04-28 07:02:16+00:00
- **Authors**: Xingyu Liao, Lingxiao He, Zhouwang Yang, Chi Zhang
- **Comment**: arXiv admin note: text overlap with arXiv:1805.02104,
  arXiv:1711.07971, arXiv:1803.09882 by other authors
- **Journal**: None
- **Summary**: Video-based person re-identification (ReID) is a challenging problem, where some video tracks of people across non-overlapping cameras are available for matching. Feature aggregation from a video track is a key step for video-based person ReID. Many existing methods tackle this problem by average/maximum temporal pooling or RNNs with attention. However, these methods cannot deal with temporal dependency and spatial misalignment problems at the same time. We are inspired by video action recognition that involves the identification of different actions from video tracks. Firstly, we use 3D convolutions on video volume, instead of using 2D convolutions across frames, to extract spatial and temporal features simultaneously. Secondly, we use a non-local block to tackle the misalignment problem and capture spatial-temporal long-range dependencies. As a result, the network can learn useful spatial-temporal information as a weighted sum of the features in all space and temporal positions in the input feature map. Experimental results on three datasets show that our framework outperforms state-of-the-art approaches by a large margin on multiple metrics.



### Adding Attentiveness to the Neurons in Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.04445v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04445v1)
- **Published**: 2018-07-12 06:59:36+00:00
- **Updated**: 2018-07-12 06:59:36+00:00
- **Authors**: Pengfei Zhang, Jianru Xue, Cuiling Lan, Wenjun Zeng, Zhanning Gao, Nanning Zheng
- **Comment**: ECCV2018
- **Journal**: None
- **Summary**: Recurrent neural networks (RNNs) are capable of modeling the temporal dynamics of complex sequential information. However, the structures of existing RNN neurons mainly focus on controlling the contributions of current and historical information but do not explore the different importance levels of different elements in an input vector of a time slot. We propose adding a simple yet effective Element-wiseAttention Gate (EleAttG) to an RNN block (e.g., all RNN neurons in a network layer) that empowers the RNN neurons to have the attentiveness capability. For an RNN block, an EleAttG is added to adaptively modulate the input by assigning different levels of importance, i.e., attention, to each element/dimension of the input. We refer to an RNN block equipped with an EleAttG as an EleAtt-RNN block. Specifically, the modulation of the input is content adaptive and is performed at fine granularity, being element-wise rather than input-wise. The proposed EleAttG, as an additional fundamental unit, is general and can be applied to any RNN structures, e.g., standard RNN, Long Short-Term Memory (LSTM), or Gated Recurrent Unit (GRU). We demonstrate the effectiveness of the proposed EleAtt-RNN by applying it to the action recognition tasks on both 3D human skeleton data and RGB videos. Experiments show that adding attentiveness through EleAttGs to RNN blocks significantly boosts the power of RNNs.



### Prostate Segmentation using 2D Bridged U-net
- **Arxiv ID**: http://arxiv.org/abs/1807.04459v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04459v2)
- **Published**: 2018-07-12 08:08:31+00:00
- **Updated**: 2018-10-16 11:50:42+00:00
- **Authors**: Wanli Chen, Yue Zhang, Junjun He, Yu Qiao, Yifan Chen, Hongjian Shi, Xiaoying Tang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we focus on three problems in deep learning based medical image segmentation. Firstly, U-net, as a popular model for medical image segmentation, is difficult to train when convolutional layers increase even though a deeper network usually has a better generalization ability because of more learnable parameters. Secondly, the exponential ReLU (ELU), as an alternative of ReLU, is not much different from ReLU when the network of interest gets deep. Thirdly, the Dice loss, as one of the pervasive loss functions for medical image segmentation, is not effective when the prediction is close to ground truth and will cause oscillation during training. To address the aforementioned three problems, we propose and validate a deeper network that can fit medical image datasets that are usually small in the sample size. Meanwhile, we propose a new loss function to accelerate the learning process and a combination of different activation functions to improve the network performance. Our experimental results suggest that our network is comparable or superior to state-of-the-art methods.



### Competitive Analysis System for Theatrical Movie Releases Based on Movie Trailer Deep Video Representation
- **Arxiv ID**: http://arxiv.org/abs/1807.04465v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1807.04465v1)
- **Published**: 2018-07-12 08:24:56+00:00
- **Updated**: 2018-07-12 08:24:56+00:00
- **Authors**: Miguel Campo, Cheng-Kang Hsieh, Matt Nickens, JJ Espinoza, Abhinav Taliyan, Julie Rieger, Jean Ho, Bettina Sherick
- **Comment**: None
- **Journal**: None
- **Summary**: Audience discovery is an important activity at major movie studios. Deep models that use convolutional networks to extract frame-by-frame features of a movie trailer and represent it in a form that is suitable for prediction are now possible thanks to the availability of pre-built feature extractors trained on large image datasets. Using these pre-built feature extractors, we are able to process hundreds of publicly available movie trailers, extract frame-by-frame low level features (e.g., a face, an object, etc) and create video-level representations. We use the video-level representations to train a hybrid Collaborative Filtering model that combines video features with historical movie attendance records. The trained model not only makes accurate attendance and audience prediction for existing movies, but also successfully profiles new movies six to eight months prior to their release.



### Video Saliency Detection by 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.04514v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04514v1)
- **Published**: 2018-07-12 10:18:12+00:00
- **Updated**: 2018-07-12 10:18:12+00:00
- **Authors**: Guanqun Ding, Yuming Fang
- **Comment**: None
- **Journal**: International Forum on Digital TV and Wireless Multimedia
  Communications (IFTC 2017)
- **Summary**: Different from salient object detection methods for still images, a key challenging for video saliency detection is how to extract and combine spatial and temporal features. In this paper, we present a novel and effective approach for salient object detection for video sequences based on 3D convolutional neural networks. First, we design a 3D convolutional network (Conv3DNet) with the input as three video frame to learn the spatiotemporal features for video sequences. Then, we design a 3D deconvolutional network (Deconv3DNet) to combine the spatiotemporal features to predict the final saliency map for video sequences. Experimental results show that the proposed saliency detection model performs better in video saliency prediction compared with the state-of-the-art video saliency detection methods.



### Robustness Analysis of Pedestrian Detectors for Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1807.04562v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04562v2)
- **Published**: 2018-07-12 12:14:38+00:00
- **Updated**: 2018-07-17 13:54:02+00:00
- **Authors**: Yuming Fang, Guanqun Ding, Yuan Yuan, Weisi Lin, Haiwen Liu
- **Comment**: None
- **Journal**: IEEE Access 2018
- **Summary**: To obtain effective pedestrian detection results in surveillance video, there have been many methods proposed to handle the problems from severe occlusion, pose variation, clutter background, \emph{etc}. Besides detection accuracy, a robust surveillance video system should be stable to video quality degradation by network transmission, environment variation, etc. In this study, we conduct the research on the robustness of pedestrian detection algorithms to video quality degradation. The main contribution of this work includes the following three aspects. First, a large-scale Distorted Surveillance Video Data Set (DSurVD) is constructed from high-quality video sequences and their corresponding distorted versions. Second, we design a method to evaluate detection stability and a robustness measure called Robustness Quadrangle, which can be adopted to visualize detection accuracy of pedestrian detection algorithms on high-quality video sequences and stability with video quality degradation. Third, the robustness of seven existing pedestrian detection algorithms is evaluated by the built DSurVD. Experimental results show that the robustness can be further improved for existing pedestrian detection algorithms. Additionally, we provide much in-depth discussion on how different distortion types influence the performance of pedestrian detection algorithms, which is important to design effective pedestrian detection algorithms for surveillance. The DSurVD data set can be download from BaiduYunDisk, https://pan.baidu.com/s/1I9Kqj8rmubOYu7bkBfkUpA, Password: lqmc



### Deep Learning for Imbalance Data Classification using Class Expert Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1807.04585v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.04585v2)
- **Published**: 2018-07-12 12:51:24+00:00
- **Updated**: 2018-07-13 03:11:44+00:00
- **Authors**: Fanny, Tjeng Wawan Cenggoro
- **Comment**: Accepted in 3rd International Conference on Computer Science and
  Computational Intelligence, 7-8 September 2018
- **Journal**: None
- **Summary**: Without any specific way for imbalance data classification, artificial intelligence algorithm cannot recognize data from minority classes easily. In general, modifying the existing algorithm by assuming that the training data is imbalanced, is the only way to handle imbalance data. However, for a normal data handling, this way mostly produces a deficient result. In this research, we propose a class expert generative adversarial network (CE-GAN) as the solution for imbalance data classification. CE-GAN is a modification in deep learning algorithm architecture that does not have an assumption that the training data is imbalance data. Moreover, CE-GAN is designed to identify more detail about the character of each class before classification step. CE-GAN has been proved in this research to give a good performance for imbalance data classification.



### Visual Attention driven by Convolutional Features
- **Arxiv ID**: http://arxiv.org/abs/1807.10576v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10576v1)
- **Published**: 2018-07-12 13:02:54+00:00
- **Updated**: 2018-07-12 13:02:54+00:00
- **Authors**: Dario Zanca, Marco Gori
- **Comment**: None
- **Journal**: None
- **Summary**: The understanding of where humans look in a scene is a problem of great interest in visual perception and computer vision. When eye-tracking devices are not a viable option, models of human attention can be used to predict fixations. In this paper we give two contribution. First, we show a model of visual attention that is simply based on deep convolutional neural networks trained for object classification tasks. A method for visualizing saliency maps is defined which is evaluated in a saliency prediction task. Second, we integrate the information of these maps with a bottom-up differential model of eye-movements to simulate visual attention scanpaths. Results on saliency prediction and scores of similarity with human scanpaths demonstrate the effectiveness of this model.



### Deep semi-supervised segmentation with weight-averaged consistency targets
- **Arxiv ID**: http://arxiv.org/abs/1807.04657v2
- **DOI**: 10.1007/978-3-030-00889-5_2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04657v2)
- **Published**: 2018-07-12 14:55:26+00:00
- **Updated**: 2018-07-16 15:06:15+00:00
- **Authors**: Christian S. Perone, Julien Cohen-Adad
- **Comment**: 8 pages, 1 figure, accepted for DLMIA/MICCAI
- **Journal**: None
- **Summary**: Recently proposed techniques for semi-supervised learning such as Temporal Ensembling and Mean Teacher have achieved state-of-the-art results in many important classification benchmarks. In this work, we expand the Mean Teacher approach to segmentation tasks and show that it can bring important improvements in a realistic small data regime using a publicly available multi-center dataset from the Magnetic Resonance Imaging (MRI) domain. We also devise a method to solve the problems that arise when using traditional data augmentation strategies for segmentation tasks on our new training scheme.



### Learning Product Codebooks using Vector Quantized Autoencoders for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1807.04629v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.04629v4)
- **Published**: 2018-07-12 15:08:31+00:00
- **Updated**: 2019-03-04 11:50:21+00:00
- **Authors**: Hanwei Wu, Markus Flierl
- **Comment**: None
- **Journal**: None
- **Summary**: Vector-Quantized Variational Autoencoders (VQ-VAE)[1] provide an unsupervised model for learning discrete representations by combining vector quantization and autoencoders. In this paper, we study the use of VQ-VAE for representation learning for downstream tasks, such as image retrieval. We first describe the VQ-VAE in the context of an information-theoretic framework. We show that the regularization term on the learned representation is determined by the size of the embedded codebook before the training and it affects the generalization ability of the model. As a result, we introduce a hyperparameter to balance the strength of the vector quantizer and the reconstruction error. By tuning the hyperparameter, the embedded bottleneck quantizer is used as a regularizer that forces the output of the encoder to share a constrained coding space such that learned latent features preserve the similarity relations of the data space. In addition, we provide a search range for finding the best hyperparameter. Finally, we incorporate the product quantization into the bottleneck stage of VQ-VAE and propose an end-to-end unsupervised learning model for the image retrieval task. The product quantizer has the advantage of generating large-size codebooks. Fast retrieval can be achieved by using the lookup tables that store the distance between any pair of sub-codewords. State-of-the-art retrieval results are achieved by the learned codebooks.



### Learning to Segment Medical Images with Scribble-Supervision Alone
- **Arxiv ID**: http://arxiv.org/abs/1807.04668v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04668v1)
- **Published**: 2018-07-12 15:24:48+00:00
- **Updated**: 2018-07-12 15:24:48+00:00
- **Authors**: Yigit B. Can, Krishna Chaitanya, Basil Mustafa, Lisa M. Koch, Ender Konukoglu, Christian F. Baumgartner
- **Comment**: Accepted for presentation at DLMIA 2018
- **Journal**: None
- **Summary**: Semantic segmentation of medical images is a crucial step for the quantification of healthy anatomy and diseases alike. The majority of the current state-of-the-art segmentation algorithms are based on deep neural networks and rely on large datasets with full pixel-wise annotations. Producing such annotations can often only be done by medical professionals and requires large amounts of valuable time. Training a medical image segmentation network with weak annotations remains a relatively unexplored topic. In this work we investigate training strategies to learn the parameters of a pixel-wise segmentation network from scribble annotations alone. We evaluate the techniques on public cardiac (ACDC) and prostate (NCI-ISBI) segmentation datasets. We find that the networks trained on scribbles suffer from a remarkably small degradation in Dice of only 2.9% (cardiac) and 4.5% (prostate) with respect to a network trained on full annotations.



### Toward Convolutional Blind Denoising of Real Photographs
- **Arxiv ID**: http://arxiv.org/abs/1807.04686v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04686v2)
- **Published**: 2018-07-12 15:52:17+00:00
- **Updated**: 2019-04-19 07:05:40+00:00
- **Authors**: Shi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo, Lei Zhang
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  2019
- **Summary**: While deep convolutional neural networks (CNNs) have achieved impressive success in image denoising with additive white Gaussian noise (AWGN), their performance remains limited on real-world noisy photographs. The main reason is that their learned models are easy to overfit on the simplified AWGN model which deviates severely from the complicated real-world noise model. In order to improve the generalization ability of deep CNN denoisers, we suggest training a convolutional blind denoising network (CBDNet) with more realistic noise model and real-world noisy-clean image pairs. On the one hand, both signal-dependent noise and in-camera signal processing pipeline is considered to synthesize realistic noisy images. On the other hand, real-world noisy photographs and their nearly noise-free counterparts are also included to train our CBDNet. To further provide an interactive strategy to rectify denoising result conveniently, a noise estimation subnetwork with asymmetric learning to suppress under-estimation of noise level is embedded into CBDNet. Extensive experimental results on three datasets of real-world noisy photographs clearly demonstrate the superior performance of CBDNet over state-of-the-arts in terms of quantitative metrics and visual quality. The code has been made available at https://github.com/GuoShi28/CBDNet.



### LandmarkBoost: Efficient Visual Context Classifiers for Robust Localization
- **Arxiv ID**: http://arxiv.org/abs/1807.04702v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.04702v2)
- **Published**: 2018-07-12 16:14:37+00:00
- **Updated**: 2018-07-13 11:19:50+00:00
- **Authors**: Marcin Dymczyk, Igor Gilitschenski, Juan Nieto, Simon Lynen, Bernhard Zeisl, Roland Siegwart
- **Comment**: None
- **Journal**: None
- **Summary**: The growing popularity of autonomous systems creates a need for reliable and efficient metric pose retrieval algorithms. Currently used approaches tend to rely on nearest neighbor search of binary descriptors to perform the 2D-3D matching and guarantee realtime capabilities on mobile platforms. These methods struggle, however, with the growing size of the map, changes in viewpoint or appearance, and visual aliasing present in the environment. The rigidly defined descriptor patterns only capture a limited neighborhood of the keypoint and completely ignore the overall visual context.   We propose LandmarkBoost - an approach that, in contrast to the conventional 2D-3D matching methods, casts the search problem as a landmark classification task. We use a boosted classifier to classify landmark observations and directly obtain correspondences as classifier scores. We also introduce a formulation of visual context that is flexible, efficient to compute, and can capture relationships in the entire image plane. The original binary descriptors are augmented with contextual information and informative features are selected by the boosting framework. Through detailed experiments, we evaluate the retrieval quality and performance of LandmarkBoost, demonstrating that it outperforms common state-of-the-art descriptor matching methods.



### Visual Reinforcement Learning with Imagined Goals
- **Arxiv ID**: http://arxiv.org/abs/1807.04742v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.04742v2)
- **Published**: 2018-07-12 17:51:16+00:00
- **Updated**: 2018-12-04 08:44:08+00:00
- **Authors**: Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, Sergey Levine
- **Comment**: 15 pages, NeurIPS 2018
- **Journal**: None
- **Summary**: For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires. Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images. In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised "practice" phase where it imagines goals and attempts to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching. We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.



### Hydranet: Data Augmentation for Regression Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.04798v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04798v3)
- **Published**: 2018-07-12 19:30:21+00:00
- **Updated**: 2019-07-29 12:57:53+00:00
- **Authors**: Florian Dubost, Gerda Bortsova, Hieab Adams, M. Arfan Ikram, Wiro Niessen, Meike Vernooij, Marleen de Bruijne
- **Comment**: accepted in MICCAI 2019
- **Journal**: None
- **Summary**: Deep learning techniques are often criticized to heavily depend on a large quantity of labeled data. This problem is even more challenging in medical image analysis where the annotator expertise is often scarce. We propose a novel data-augmentation method to regularize neural network regressors that learn from a single global label per image. The principle of the method is to create new samples by recombining existing ones. We demonstrate the performance of our algorithm on two tasks: estimation of the number of enlarged perivascular spaces in the basal ganglia, and estimation of white matter hyperintensities volume. We show that the proposed method improves the performance over more basic data augmentation. The proposed method reached an intraclass correlation coefficient between ground truth and network predictions of 0.73 on the first task and 0.84 on the second task, only using between 25 and 30 scans with a single global label per scan for training. With the same number of training scans, more conventional data augmentation methods could only reach intraclass correlation coefficients of 0.68 on the first task, and 0.79 on the second task.



### Learning-based Regularization for Cardiac Strain Analysis with Ability for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1807.04807v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04807v1)
- **Published**: 2018-07-12 20:19:48+00:00
- **Updated**: 2018-07-12 20:19:48+00:00
- **Authors**: Allen Lu, Nripesh Parajuli, Maria Zontak, John Stendahl, Kevinminh Ta, Zhao Liu, Nabil Boutagy, Geng-Shi Jeng, Imran Alkhalil, Lawrence H. Staib, Matthew O'Donnell, Albert J. Sinusas, James S. Duncan
- **Comment**: None
- **Journal**: None
- **Summary**: Reliable motion estimation and strain analysis using 3D+time echocardiography (4DE) for localization and characterization of myocardial injury is valuable for early detection and targeted interventions. However, motion estimation is difficult due to the low-SNR that stems from the inherent image properties of 4DE, and intelligent regularization is critical for producing reliable motion estimates. In this work, we incorporated the notion of domain adaptation into a supervised neural network regularization framework. We first propose an unsupervised autoencoder network with biomechanical constraints for learning a latent representation that is shown to have more physiologically plausible displacements. We extended this framework to include a supervised loss term on synthetic data and showed the effects of biomechanical constraints on the network's ability for domain adaptation. We validated both the autoencoder and semi-supervised regularization method on in vivo data with implanted sonomicrometers. Finally, we showed the ability of our semi-supervised learning regularization approach to identify infarcted regions using estimated regional strain maps with good agreement to manually traced infarct regions from postmortem excised hearts.



### Latent Transformations for Object View Points Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1807.04812v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04812v4)
- **Published**: 2018-07-12 20:46:43+00:00
- **Updated**: 2018-11-28 17:52:39+00:00
- **Authors**: Sangpil Kim, Nick Winovich, Guang Lin, Karthik Ramani
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a fully-convolutional conditional generative model, the latent transformation neural network (LTNN), capable of view synthesis using a light-weight neural network suited for real-time applications. In contrast to existing conditional generative models which incorporate conditioning information via concatenation, we introduce a dedicated network component, the conditional transformation unit (CTU), designed to learn the latent space transformations corresponding to specified target views. In addition, a consistency loss term is defined to guide the network toward learning the desired latent space mappings, a task-divided decoder is constructed to refine the quality of generated views, and an adaptive discriminator is introduced to improve the adversarial training process. The generality of the proposed methodology is demonstrated on a collection of three diverse tasks: multi-view reconstruction on real hand depth images, view synthesis of real and synthetic faces, and the rotation of rigid objects. The proposed model is shown to exceed state-of-the-art results in each category while simultaneously achieving a reduction in the computational demand required for inference by 30% on average.



### CTAP: Complementary Temporal Action Proposal Generation
- **Arxiv ID**: http://arxiv.org/abs/1807.04821v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04821v2)
- **Published**: 2018-07-12 21:07:01+00:00
- **Updated**: 2018-07-18 20:25:26+00:00
- **Authors**: Jiyang Gao, Kan Chen, Ram Nevatia
- **Comment**: ECCV 2018 main conference paper (camera ready version). Code is
  available in http://www.github.com/jiyanggao/CTAP
- **Journal**: None
- **Summary**: Temporal action proposal generation is an important task, akin to object proposals, temporal action proposals are intended to capture "clips" or temporal intervals in videos that are likely to contain an action. Previous methods can be divided to two groups: sliding window ranking and actionness score grouping. Sliding windows uniformly cover all segments in videos, but the temporal boundaries are imprecise; grouping based method may have more precise boundaries but it may omit some proposals when the quality of actionness score is low. Based on the complementary characteristics of these two methods, we propose a novel Complementary Temporal Action Proposal (CTAP) generator. Specifically, we apply a Proposal-level Actionness Trustworthiness Estimator (PATE) on the sliding windows proposals to generate the probabilities indicating whether the actions can be correctly detected by actionness scores, the windows with high scores are collected. The collected sliding windows and actionness proposals are then processed by a temporal convolutional neural network for proposal ranking and boundary adjustment. CTAP outperforms state-of-the-art methods on average recall (AR) by a large margin on THUMOS-14 and ActivityNet 1.3 datasets. We further apply CTAP as a proposal generation method in an existing action detector, and show consistent significant improvements.



### Optimal Strategies for Matching and Retrieval Problems by Comparing Covariates
- **Arxiv ID**: http://arxiv.org/abs/1807.04834v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04834v2)
- **Published**: 2018-07-12 21:36:07+00:00
- **Updated**: 2018-07-16 00:53:37+00:00
- **Authors**: Yandong Wen, Mahmoud Al Ismail, Bhiksha Raj, Rita Singh
- **Comment**: support material for "Disjoint Mapping Network for Cross-modal
  Matching of Voices and Faces"
- **Journal**: None
- **Summary**: In many retrieval problems, where we must retrieve one or more entries from a gallery in response to a probe, it is common practice to learn to do by directly comparing the probe and gallery entries to one another. In many situations the gallery and probe have common covariates -- external variables that are common to both. In principle it is possible to perform the retrieval based merely on these covariates. The process, however, becomes gated by our ability to recognize the covariates for the probe and gallery entries correctly.   In this paper we analyze optimal strategies for retrieval based only on matching covariates, when the recognition of the covariates is itself inaccurate. We investigate multiple problems: recovering one item from a gallery of $N$ entries, matching pairs of instances, and retrieval from large collections. We verify our analytical formulae through experiments to verify their correctness in practical settings.



### Disjoint Mapping Network for Cross-modal Matching of Voices and Faces
- **Arxiv ID**: http://arxiv.org/abs/1807.04836v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04836v2)
- **Published**: 2018-07-12 21:37:34+00:00
- **Updated**: 2018-07-16 01:00:13+00:00
- **Authors**: Yandong Wen, Mahmoud Al Ismail, Weiyang Liu, Bhiksha Raj, Rita Singh
- **Comment**: Tech report
- **Journal**: None
- **Summary**: We propose a novel framework, called Disjoint Mapping Network (DIMNet), for cross-modal biometric matching, in particular of voices and faces. Different from the existing methods, DIMNet does not explicitly learn the joint relationship between the modalities. Instead, DIMNet learns a shared representation for different modalities by mapping them individually to their common covariates. These shared representations can then be used to find the correspondences between the modalities. We show empirically that DIMNet is able to achieve better performance than other current methods, with the additional benefits of being conceptually simpler and less data-intensive.



### A feature agnostic approach for glaucoma detection in OCT volumes
- **Arxiv ID**: http://arxiv.org/abs/1807.04855v4
- **DOI**: 10.1371/journal.pone.0219126
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.04855v4)
- **Published**: 2018-07-12 22:57:19+00:00
- **Updated**: 2019-10-24 03:14:22+00:00
- **Authors**: Stefan Maetschke, Bhavna Antony, Hiroshi Ishikawa, Gadi Wollstein, Joel S. Schuman, Rahil Garnavi
- **Comment**: 13 pages,3 figures
- **Journal**: None
- **Summary**: Optical coherence tomography (OCT) based measurements of retinal layer thickness, such as the retinal nerve fibre layer (RNFL) and the ganglion cell with inner plexiform layer (GCIPL) are commonly used for the diagnosis and monitoring of glaucoma. Previously, machine learning techniques have utilized segmentation-based imaging features such as the peripapillary RNFL thickness and the cup-to-disc ratio. Here, we propose a deep learning technique that classifies eyes as healthy or glaucomatous directly from raw, unsegmented OCT volumes of the optic nerve head (ONH) using a 3D Convolutional Neural Network (CNN). We compared the accuracy of this technique with various feature-based machine learning algorithms and demonstrated the superiority of the proposed deep learning based method.   Logistic regression was found to be the best performing classical machine learning technique with an AUC of 0.89. In direct comparison, the deep learning approach achieved a substantially higher AUC of 0.94 with the additional advantage of providing insight into which regions of an OCT volume are important for glaucoma detection.   Computing Class Activation Maps (CAM), we found that the CNN identified neuroretinal rim and optic disc cupping as well as the lamina cribrosa (LC) and its surrounding areas as the regions significantly associated with the glaucoma classification. These regions anatomically correspond to the well established and commonly used clinical markers for glaucoma diagnosis such as increased cup volume, cup diameter, and neuroretinal rim thinning at the superior and inferior segments.



### CADDY Underwater Stereo-Vision Dataset for Human-Robot Interaction (HRI) in the Context of Diver Activities
- **Arxiv ID**: http://arxiv.org/abs/1807.04856v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.04856v1)
- **Published**: 2018-07-12 23:00:06+00:00
- **Updated**: 2018-07-12 23:00:06+00:00
- **Authors**: Arturo Gomez Chavez, Andrea Ranieri, Davide Chiarella, Enrica Zereik, Anja Babić, Andreas Birk
- **Comment**: submitted to IJRR
- **Journal**: None
- **Summary**: In this article we present a novel underwater dataset collected from several field trials within the EU FP7 project "Cognitive autonomous diving buddy (CADDY)", where an Autonomous Underwater Vehicle (AUV) was used to interact with divers and monitor their activities. To our knowledge, this is one of the first efforts to collect a large dataset in underwater environments targeting object classification, segmentation and human pose estimation tasks. The first part of the dataset contains stereo camera recordings (~10K) of divers performing hand gestures to communicate and interact with an AUV in different environmental conditions. These gestures samples serve to test the robustness of object detection and classification algorithms against underwater image distortions i.e., color attenuation and light backscatter. The second part includes stereo footage (~12.7K) of divers free-swimming in front of the AUV, along with synchronized IMUs measurements located throughout the diver's suit (DiverNet) which serve as ground-truth for human pose and tracking methods. In both cases, these rectified images allow investigation of 3D representation and reasoning pipelines from low-texture targets commonly present in underwater scenarios. In this paper we describe our recording platform, sensor calibration procedure plus the data format and the utilities provided to use the dataset.



