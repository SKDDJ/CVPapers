# Arxiv Papers in cs.CV on 2018-07-03
### Resembled Generative Adversarial Networks: Two Domains with Similar Attributes
- **Arxiv ID**: http://arxiv.org/abs/1807.00947v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00947v1)
- **Published**: 2018-07-03 01:42:20+00:00
- **Updated**: 2018-07-03 01:42:20+00:00
- **Authors**: Duhyeon Bang, Hyunjung Shim
- **Comment**: Accepted at BMVC 2018
- **Journal**: None
- **Summary**: We propose a novel algorithm, namely Resembled Generative Adversarial Networks (GAN), that generates two different domain data simultaneously where they resemble each other. Although recent GAN algorithms achieve the great success in learning the cross-domain relationship, their application is limited to domain transfers, which requires the input image. The first attempt to tackle the data generation of two domains was proposed by CoGAN. However, their solution is inherently vulnerable for various levels of domain similarities. Unlike CoGAN, our Resembled GAN implicitly induces two generators to match feature covariance from both domains, thus leading to share semantic attributes. Hence, we effectively handle a wide range of structural and semantic similarities between various two domains. Based on experimental analysis on various datasets, we verify that the proposed algorithm is effective for generating two domains with similar attributes.



### Ballistocardiogram Signal Processing: A Literature Review
- **Arxiv ID**: http://arxiv.org/abs/1807.00951v1
- **DOI**: 10.1007/s13755-019-0071-7
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.00951v1)
- **Published**: 2018-07-03 01:57:36+00:00
- **Updated**: 2018-07-03 01:57:36+00:00
- **Authors**: Ibrahim Sadek
- **Comment**: Review Paper
- **Journal**: None
- **Summary**: Time-domain algorithms are focused on detecting local maxima or local minima using a moving window, and therefore finding the interval between the dominant J-peaks of ballistocardiogram (BCG) signal. However, this approach has many limitations due to the nonlinear and nonstationary behavior of the BCG signal. This is because the BCG signal does not display consistent J-peaks, which can usually be the case for overnight, in-home monitoring, particularly with frail elderly. Additionally, its accuracy will be undoubtedly affected by motion artifacts. Second, frequency-domain algorithms do not provide information about interbeat intervals. Nevertheless, they can provide information about heart rate variability. This is usually done by taking the fast Fourier transform or the inverse Fourier transform of the logarithm of the estimated spectrum, i.e., cepstrum of the signal using a sliding window. Thereafter, the dominant frequency is obtained in a particular frequency range. The limit of these algorithms is that the peak in the spectrum may get wider and multiple peaks may appear, which might cause a problem in measuring the vital signs. At last, the objective of wavelet-domain algorithms is to decompose the signal into different components, hence the component which shows an agreement with the vital signs can be selected i.e., the selected component contains only information about the heart cycles or respiratory cycles, respectively. An empirical mode decomposition is an alternative approach to wavelet decomposition, and it is also a very suitable approach to cope with nonlinear and nonstationary signals such as cardiorespiratory signals. Apart from the above-mentioned algorithms, machine learning approaches have been implemented for measuring heartbeats. However, manual labeling of training data is a restricting property.



### Iterative Attention Mining for Weakly Supervised Thoracic Disease Pattern Localization in Chest X-Rays
- **Arxiv ID**: http://arxiv.org/abs/1807.00958v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00958v1)
- **Published**: 2018-07-03 02:56:38+00:00
- **Updated**: 2018-07-03 02:56:38+00:00
- **Authors**: Jinzheng Cai, Le Lu, Adam P. Harrison, Xiaoshuang Shi, Pingjun Chen, Lin Yang
- **Comment**: 9 pages, 2 figures, 3 tables (with supplementary), accepted to MICCAI
  2018
- **Journal**: None
- **Summary**: Given image labels as the only supervisory signal, we focus on harvesting, or mining, thoracic disease localizations from chest X-ray images. Harvesting such localizations from existing datasets allows for the creation of improved data sources for computer-aided diagnosis and retrospective analyses. We train a convolutional neural network (CNN) for image classification and propose an attention mining (AM) strategy to improve the model's sensitivity or saliency to disease patterns. The intuition of AM is that once the most salient disease area is blocked or hidden from the CNN model, it will pay attention to alternative image regions, while still attempting to make correct predictions. However, the model requires to be properly constrained during AM, otherwise, it may overfit to uncorrelated image parts and forget the valuable knowledge that it has learned from the original image classification task. To alleviate such side effects, we then design a knowledge preservation (KP) loss, which minimizes the discrepancy between responses for X-ray images from the original and the updated networks. Furthermore, we modify the CNN model to include multi-scale aggregation (MSA), improving its localization ability on small-scale disease findings, e.g., lung nodules. We experimentally validate our method on the publicly-available ChestX-ray14 dataset, outperforming a class activation map (CAM)-based approach, and demonstrating the value of our novel framework for mining disease locations.



### SymmNet: A Symmetric Convolutional Neural Network for Occlusion Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.00959v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00959v2)
- **Published**: 2018-07-03 03:11:17+00:00
- **Updated**: 2018-09-23 07:58:56+00:00
- **Authors**: Ang Li, Zejian Yuan
- **Comment**: BMVC 2018 Camera-ready
- **Journal**: None
- **Summary**: Detecting the occlusion from stereo images or video frames is important to many computer vision applications. Previous efforts focus on bundling it with the computation of disparity or optical flow, leading to a chicken-and-egg problem. In this paper, we leverage convolutional neural network to liberate the occlusion detection task from the interleaved, traditional calculation framework. We propose a Symmetric Network (SymmNet) to directly exploit information from an image pair, without estimating disparity or motion in advance. The proposed network is structurally left-right symmetric to learn the binocular occlusion simultaneously, aimed at jointly improving both results. The comprehensive experiments show that our model achieves state-of-the-art results on detecting the stereo and motion occlusion.



### Supervision-by-Registration: An Unsupervised Approach to Improve the Precision of Facial Landmark Detectors
- **Arxiv ID**: http://arxiv.org/abs/1807.00966v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00966v2)
- **Published**: 2018-07-03 03:52:45+00:00
- **Updated**: 2018-07-04 04:05:52+00:00
- **Authors**: Xuanyi Dong, Shoou-I Yu, Xinshuo Weng, Shih-En Wei, Yi Yang, Yaser Sheikh
- **Comment**: Minor modifications to the CVPR 2018 version (add missing references)
- **Journal**: None
- **Summary**: In this paper, we present supervision-by-registration, an unsupervised approach to improve the precision of facial landmark detectors on both images and video. Our key observation is that the detections of the same landmark in adjacent frames should be coherent with registration, i.e., optical flow. Interestingly, the coherency of optical flow is a source of supervision that does not require manual labeling, and can be leveraged during detector training. For example, we can enforce in the training loss function that a detected landmark at frame$_{t-1}$ followed by optical flow tracking from frame$_{t-1}$ to frame$_t$ should coincide with the location of the detection at frame$_t$. Essentially, supervision-by-registration augments the training loss function with a registration loss, thus training the detector to have output that is not only close to the annotations in labeled images, but also consistent with registration on large amounts of unlabeled videos. End-to-end training with the registration loss is made possible by a differentiable Lucas-Kanade operation, which computes optical flow registration in the forward pass, and back-propagates gradients that encourage temporal coherency in the detector. The output of our method is a more precise image-based facial landmark detector, which can be applied to single images or video. With supervision-by-registration, we demonstrate (1) improvements in facial landmark detection on both images (300W, ALFW) and video (300VW, Youtube-Celebrities), and (2) significant reduction of jittering in video detections.



### A Spatial and Temporal Features Mixture Model with Body Parts for Video-based Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1807.00975v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1807.00975v1)
- **Published**: 2018-07-03 04:33:22+00:00
- **Updated**: 2018-07-03 04:33:22+00:00
- **Authors**: Jie Liu, Cheng Sun, Xiang Xu, Baomin Xu, Shuangyuan Yu
- **Comment**: None
- **Journal**: None
- **Summary**: The video-based person re-identification is to recognize a person under different cameras, which is a crucial task applied in visual surveillance system. Most previous methods mainly focused on the feature of full body in the frame. In this paper we propose a novel Spatial and Temporal Features Mixture Model (STFMM) based on convolutional neural network (CNN) and recurrent neural network (RNN), in which the human body is split into $N$ parts in horizontal direction so that we can obtain more specific features. The proposed method skillfully integrates features of each part to achieve more expressive representation of each person. We first split the video sequence into $N$ part sequences which include the information of head, waist, legs and so on. Then the features are extracted by STFMM whose $2N$ inputs are obtained from the developed Siamese network, and these features are combined into a discriminative representation for one person. Experiments are conducted on the iLIDS-VID and PRID-2011 datasets. The results demonstrate that our approach outperforms existing methods for video-based person re-identification. It achieves a rank-1 CMC accuracy of 74\% on the iLIDS-VID dataset, exceeding the the most recently developed method ASTPN by 12\%. For the cross-data testing, our method achieves a rank-1 CMC accuracy of 48\% exceeding the ASTPN method by 18\%, which shows that our model has significant stability.



### MetaAnchor: Learning to Detect Objects with Customized Anchors
- **Arxiv ID**: http://arxiv.org/abs/1807.00980v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00980v2)
- **Published**: 2018-07-03 05:20:20+00:00
- **Updated**: 2018-11-06 13:52:55+00:00
- **Authors**: Tong Yang, Xiangyu Zhang, Zeming Li, Wenqiang Zhang, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel and flexible anchor mechanism named MetaAnchor for object detection frameworks. Unlike many previous detectors model anchors via a predefined manner, in MetaAnchor anchor functions could be dynamically generated from the arbitrary customized prior boxes. Taking advantage of weight prediction, MetaAnchor is able to work with most of the anchor-based object detection systems such as RetinaNet. Compared with the predefined anchor scheme, we empirically find that MetaAnchor is more robust to anchor settings and bounding box distributions; in addition, it also shows the potential on transfer tasks. Our experiment on COCO detection task shows that MetaAnchor consistently outperforms the counterparts in various scenarios.



### Long Activity Video Understanding using Functional Object-Oriented Network
- **Arxiv ID**: http://arxiv.org/abs/1807.00983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.00983v1)
- **Published**: 2018-07-03 05:33:44+00:00
- **Updated**: 2018-07-03 05:33:44+00:00
- **Authors**: Ahmad Babaeian Jelodar, David Paulius, Yu Sun
- **Comment**: 12 pages, 12 figures
- **Journal**: None
- **Summary**: Video understanding is one of the most challenging topics in computer vision. In this paper, a four-stage video understanding pipeline is presented to simultaneously recognize all atomic actions and the single on-going activity in a video. This pipeline uses objects and motions from the video and a graph-based knowledge representation network as prior reference. Two deep networks are trained to identify objects and motions in each video sequence associated with an action. Low Level image features are then used to identify objects of interest in that video sequence. Confidence scores are assigned to objects of interest based on their involvement in the action and to motion classes based on results from a deep neural network that classifies the on-going action in video into motion classes. Confidence scores are computed for each candidate functional unit associated with an action using a knowledge representation network, object confidences, and motion confidences. Each action is therefore associated with a functional unit and the sequence of actions is further evaluated to identify the single on-going activity in the video. The knowledge representation used in the pipeline is called the functional object-oriented network which is a graph-based network useful for encoding knowledge about manipulation tasks. Experiments are performed on a dataset of cooking videos to test the proposed algorithm with action inference and activity classification. Experiments show that using functional object oriented network improves video understanding significantly.



### Modular Vehicle Control for Transferring Semantic Information Between Weather Conditions Using GANs
- **Arxiv ID**: http://arxiv.org/abs/1807.01001v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.01001v2)
- **Published**: 2018-07-03 07:29:19+00:00
- **Updated**: 2018-10-01 14:01:46+00:00
- **Authors**: Patrick Wenzel, Qadeer Khan, Daniel Cremers, Laura Leal-Taixé
- **Comment**: 2nd Conference on Robot Learning (CoRL 2018), Z\"urich, Switzerland
- **Journal**: None
- **Summary**: Even though end-to-end supervised learning has shown promising results for sensorimotor control of self-driving cars, its performance is greatly affected by the weather conditions under which it was trained, showing poor generalization to unseen conditions. In this paper, we show how knowledge can be transferred using semantic maps to new weather conditions without the need to obtain new ground truth data. To this end, we propose to divide the task of vehicle control into two independent modules: a control module which is only trained on one weather condition for which labeled steering data is available, and a perception module which is used as an interface between new weather conditions and the fixed control module. To generate the semantic data needed to train the perception module, we propose to use a generative adversarial network (GAN)-based model to retrieve the semantic information for the new conditions in an unsupervised manner. We introduce a master-servant architecture, where the master model (semantic labels available) trains the servant model (semantic labels not available). We show that our proposed method trained with ground truth data for a single weather condition is capable of achieving similar results on the task of steering angle prediction as an end-to-end model trained with ground truth data of 15 different weather conditions.



### Faster Bounding Box Annotation for Object Detection in Indoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/1807.03142v1
- **DOI**: 10.1109/EUVIP.2018.8611732
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.03142v1)
- **Published**: 2018-07-03 08:46:57+00:00
- **Updated**: 2018-07-03 08:46:57+00:00
- **Authors**: Bishwo Adhikari, Jukka Peltomäki, Jussi Puura, Heikki Huttunen
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: 7th European Workshop on Visual Information Processing (EUVIP),
  2018
- **Summary**: This paper proposes an approach for rapid bounding box annotation for object detection datasets. The procedure consists of two stages: The first step is to annotate a part of the dataset manually, and the second step proposes annotations for the remaining samples using a model trained with the first stage annotations. We experimentally study which first/second stage split minimizes to total workload. In addition, we introduce a new fully labeled object detection dataset collected from indoor scenes. Compared to other indoor datasets, our collection has more class categories, different backgrounds, lighting conditions, occlusion and high intra-class differences. We train deep learning based object detectors with a number of state-of-the-art models and compare them in terms of speed and accuracy. The fully annotated dataset is released freely available for the research community.



### Deep Architectures and Ensembles for Semantic Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1807.01026v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01026v3)
- **Published**: 2018-07-03 08:49:47+00:00
- **Updated**: 2018-10-07 14:51:29+00:00
- **Authors**: Eng-Jon Ong, Sameed Husain, Mikel Bober-Irizar, Miroslaw Bober
- **Comment**: None
- **Journal**: None
- **Summary**: This work addresses the problem of accurate semantic labelling of short videos. To this end, a multitude of different deep nets, ranging from traditional recurrent neural networks (LSTM, GRU), temporal agnostic networks (FV,VLAD,BoW), fully connected neural networks mid-stage AV fusion and others. Additionally, we also propose a residual architecture-based DNN for video classification, with state-of-the art classification performance at significantly reduced complexity. Furthermore, we propose four new approaches to diversity-driven multi-net ensembling, one based on fast correlation measure and three incorporating a DNN-based combiner. We show that significant performance gains can be achieved by ensembling diverse nets and we investigate factors contributing to high diversity. Based on the extensive YouTube8M dataset, we provide an in-depth evaluation and analysis of their behaviour. We show that the performance of the ensemble is state-of-the-art achieving the highest accuracy on the YouTube-8M Kaggle test data. The performance of the ensemble of classifiers was also evaluated on the HMDB51 and UCF101 datasets, and show that the resulting method achieves comparable accuracy with state-of-the-art methods using similar input features.



### Kitting in the Wild through Online Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1807.01028v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.01028v1)
- **Published**: 2018-07-03 08:53:27+00:00
- **Updated**: 2018-07-03 08:53:27+00:00
- **Authors**: Massimiliano Mancini, Hakan Karaoguz, Elisa Ricci, Patric Jensfelt, Barbara Caputo
- **Comment**: Accepted to IROS 2018
- **Journal**: None
- **Summary**: Technological developments call for increasing perception and action capabilities of robots. Among other skills, vision systems that can adapt to any possible change in the working conditions are needed. Since these conditions are unpredictable, we need benchmarks which allow to assess the generalization and robustness capabilities of our visual recognition algorithms. In this work we focus on robotic kitting in unconstrained scenarios. As a first contribution, we present a new visual dataset for the kitting task. Differently from standard object recognition datasets, we provide images of the same objects acquired under various conditions where camera, illumination and background are changed. This novel dataset allows for testing the robustness of robot visual recognition algorithms to a series of different domain shifts both in isolation and unified. Our second contribution is a novel online adaptation algorithm for deep models, based on batch-normalization layers, which allows to continuously adapt a model to the current working conditions. Differently from standard domain adaptation algorithms, it does not require any image from the target domain at training time. We benchmark the performance of the algorithm on the proposed dataset, showing its capability to fill the gap between the performances of a standard architecture and its counterpart adapted offline to the given target domain.



### MediaEval 2018: Predicting Media Memorability Task
- **Arxiv ID**: http://arxiv.org/abs/1807.01052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01052v1)
- **Published**: 2018-07-03 09:47:38+00:00
- **Updated**: 2018-07-03 09:47:38+00:00
- **Authors**: Romain Cohendet, Claire-Hélène Demarty, Ngoc Duong, Mats Sjöberg, Bogdan Ionescu, Thanh-Toan Do, France Rennes
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present the Predicting Media Memorability task, which is proposed as part of the MediaEval 2018 Benchmarking Initiative for Multimedia Evaluation. Participants are expected to design systems that automatically predict memorability scores for videos, which reflect the probability of a video being remembered. In contrast to previous work in image memorability prediction, where memorability was measured a few minutes after memorization, the proposed dataset comes with short-term and long-term memorability annotations. All task characteristics are described, namely: the task's challenges and breakthrough, the released data set and ground truth, the required participant runs and the evaluation metrics.



### HAMLET: Hierarchical Harmonic Filters for Learning Tracts from Diffusion MRI
- **Arxiv ID**: http://arxiv.org/abs/1807.01068v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1807.01068v1)
- **Published**: 2018-07-03 10:25:05+00:00
- **Updated**: 2018-07-03 10:25:05+00:00
- **Authors**: Marco Reisert, Volker A. Coenen, Christoph Kaller, Karl Egger, Henrik Skibbe
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we propose HAMLET, a novel tract learning algorithm, which, after training, maps raw diffusion weighted MRI directly onto an image which simultaneously indicates tract direction and tract presence. The automatic learning of fiber tracts based on diffusion MRI data is a rather new idea, which tries to overcome limitations of atlas-based techniques. HAMLET takes a such an approach. Unlike the current trend in machine learning, HAMLET has only a small number of free parameters HAMLET is based on spherical tensor algebra which allows a translation and rotation covariant treatment of the problem. HAMLET is based on a repeated application of convolutions and non-linearities, which all respect the rotation covariance. The intrinsic treatment of such basic image transformations in HAMLET allows the training and generalization of the algorithm without any additional data augmentation. We demonstrate the performance of our approach for twelve prominent bundles, and show that the obtained tract estimates are robust and reliable. It is also shown that the learned models are portable from one sequence to another.



### Deep Spatio-Temporal Random Fields for Efficient Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.03148v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.03148v1)
- **Published**: 2018-07-03 10:46:07+00:00
- **Updated**: 2018-07-03 10:46:07+00:00
- **Authors**: Siddhartha Chandra, Camille Couprie, Iasonas Kokkinos
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: In this work we introduce a time- and memory-efficient method for structured prediction that couples neuron decisions across both space at time. We show that we are able to perform exact and efficient inference on a densely connected spatio-temporal graph by capitalizing on recent advances on deep Gaussian Conditional Random Fields (GCRFs). Our method, called VideoGCRF is (a) efficient, (b) has a unique global minimum, and (c) can be trained end-to-end alongside contemporary deep networks for video understanding. We experiment with multiple connectivity patterns in the temporal domain, and present empirical improvements over strong baselines on the tasks of both semantic and instance segmentation of videos.



### Stochastic Channel Decorrelation Network and Its Application to Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1807.01103v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01103v2)
- **Published**: 2018-07-03 12:03:21+00:00
- **Updated**: 2018-08-20 12:40:50+00:00
- **Authors**: Jie Guo, Tingfa Xu, Shenwang Jiang, Ziyi Shen
- **Comment**: 8 pages, 3 figures, 4 tables
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have dominated many computer vision domains because of their great power to extract good features automatically. However, many deep CNNs-based computer vison tasks suffer from lack of training data while there are millions of parameters in the deep models. Obviously, these two biphase violation facts will result in parameter redundancy of many poorly designed deep CNNs. Therefore, we look deep into the existing CNNs and find that the redundancy of network parameters comes from the correlation between features in different channels within a convolutional layer. To solve this problem, we propose the stochastic channel decorrelation (SCD) block which, in every iteration, randomly selects multiple pairs of channels within a convolutional layer and calculates their normalized cross correlation (NCC). Then a squared max-margin loss is proposed as the objective of SCD to suppress correlation and keep diversity between channels explicitly. The proposed SCD is very flexible and can be applied to any current existing CNN models simply. Based on the SCD and the Fully-Convolutional Siamese Networks, we proposed a visual tracking algorithm to verify the effectiveness of SCD.



### Getting the subtext without the text: Scalable multimodal sentiment classification from visual and acoustic modalities
- **Arxiv ID**: http://arxiv.org/abs/1807.01122v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1807.01122v1)
- **Published**: 2018-07-03 12:38:11+00:00
- **Updated**: 2018-07-03 12:38:11+00:00
- **Authors**: Nathaniel Blanchard, Daniel Moreira, Aparna Bharati, Walter J. Scheirer
- **Comment**: Published in the First Workshop on Computational Modeling of Human
  Multimodal Language - ACL 2018
- **Journal**: None
- **Summary**: In the last decade, video blogs (vlogs) have become an extremely popular method through which people express sentiment. The ubiquitousness of these videos has increased the importance of multimodal fusion models, which incorporate video and audio features with traditional text features for automatic sentiment detection. Multimodal fusion offers a unique opportunity to build models that learn from the full depth of expression available to human viewers. In the detection of sentiment in these videos, acoustic and video features provide clarity to otherwise ambiguous transcripts. In this paper, we present a multimodal fusion model that exclusively uses high-level video and audio features to analyze spoken sentences for sentiment. We discard traditional transcription features in order to minimize human intervention and to maximize the deployability of our model on at-scale real-world data. We select high-level features for our model that have been successful in nonaffect domains in order to test their generalizability in the sentiment detection domain. We train and test our model on the newly released CMU Multimodal Opinion Sentiment and Emotion Intensity (CMUMOSEI) dataset, obtaining an F1 score of 0.8049 on the validation set and an F1 score of 0.6325 on the held-out challenge test set.



### Anomaly Detection Using GANs for Visual Inspection in Noisy Training Data
- **Arxiv ID**: http://arxiv.org/abs/1807.01136v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01136v2)
- **Published**: 2018-07-03 13:03:11+00:00
- **Updated**: 2018-11-07 15:44:07+00:00
- **Authors**: Masanari Kimura, Takashi Yanagihara
- **Comment**: None
- **Journal**: None
- **Summary**: The detection and the quantification of anomalies in image data are critical tasks in industrial scenes such as detecting micro scratches on product. In recent years, due to the difficulty of defining anomalies and the limit of correcting their labels, research on unsupervised anomaly detection using generative models has attracted attention. Generally, in those studies, only normal images are used for training to model the distribution of normal images. The model measures the anomalies in the target images by reproducing the most similar images and scoring image patches indicating their fit to the learned distribution. This approach is based on a strong presumption; the trained model should not be able to generate abnormal images. However, in reality, the model can generate abnormal images mainly due to noisy normal data which include small abnormal pixels, and such noise severely affects the accuracy of the model. Therefore, we propose a novel anomaly detection method to distort the distribution of the model with existing abnormal images. The proposed method detects pixel-level micro anomalies with a high accuracy from 1024x1024 high resolution images which are actually used in an industrial scene. In this paper, we share experimental results on open datasets, due to the confidentiality of the data.



### ReCoNet: Real-time Coherent Video Style Transfer Network
- **Arxiv ID**: http://arxiv.org/abs/1807.01197v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01197v2)
- **Published**: 2018-07-03 14:11:16+00:00
- **Updated**: 2018-11-01 05:48:14+00:00
- **Authors**: Chang Gao, Derun Gu, Fangjun Zhang, Yizhou Yu
- **Comment**: 16 pages, 7 figures. For supplementary material, see
  https://www.dropbox.com/s/go6f7uopjjsala7/ReCoNet%20Supplementary%20Material.pdf?dl=0
- **Journal**: None
- **Summary**: Image style transfer models based on convolutional neural networks usually suffer from high temporal inconsistency when applied to videos. Some video style transfer models have been proposed to improve temporal consistency, yet they fail to guarantee fast processing speed, nice perceptual style quality and high temporal consistency at the same time. In this paper, we propose a novel real-time video style transfer model, ReCoNet, which can generate temporally coherent style transfer videos while maintaining favorable perceptual styles. A novel luminance warping constraint is added to the temporal loss at the output level to capture luminance changes between consecutive frames and increase stylization stability under illumination effects. We also propose a novel feature-map-level temporal loss to further enhance temporal consistency on traceable objects. Experimental results indicate that our model exhibits outstanding performance both qualitatively and quantitatively.



### Local Gradients Smoothing: Defense against localized adversarial attacks
- **Arxiv ID**: http://arxiv.org/abs/1807.01216v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01216v2)
- **Published**: 2018-07-03 14:48:05+00:00
- **Updated**: 2018-11-19 05:45:03+00:00
- **Authors**: Muzammal Naseer, Salman H. Khan, Fatih Porikli
- **Comment**: Accepted At WACV-2019
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have shown vulnerability to adversarial attacks, i.e., carefully perturbed inputs designed to mislead the network at inference time. Recently introduced localized attacks, Localized and Visible Adversarial Noise (LaVAN) and Adversarial patch, pose a new challenge to deep learning security by adding adversarial noise only within a specific region without affecting the salient objects in an image. Driven by the observation that such attacks introduce concentrated high-frequency changes at a particular image location, we have developed an effective method to estimate noise location in gradient domain and transform those high activation regions caused by adversarial noise in image domain while having minimal effect on the salient object that is important for correct classification. Our proposed Local Gradients Smoothing (LGS) scheme achieves this by regularizing gradients in the estimated noisy region before feeding the image to DNN for inference. We have shown the effectiveness of our method in comparison to other defense methods including Digital Watermarking, JPEG compression, Total Variance Minimization (TVM) and Feature squeezing on ImageNet dataset. In addition, we systematically study the robustness of the proposed defense mechanism against Back Pass Differentiable Approximation (BPDA), a state of the art attack recently developed to break defenses that transform an input sample to minimize the adversarial effect. Compared to other defense mechanisms, LGS is by far the most resistant to BPDA in localized adversarial attack setting.



### SpaceNet: A Remote Sensing Dataset and Challenge Series
- **Arxiv ID**: http://arxiv.org/abs/1807.01232v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01232v3)
- **Published**: 2018-07-03 15:12:59+00:00
- **Updated**: 2019-07-15 02:40:55+00:00
- **Authors**: Adam Van Etten, Dave Lindenbaum, Todd M. Bacastow
- **Comment**: 10 pages, 5 figures, 2 tables, 5 appendices
- **Journal**: None
- **Summary**: Foundational mapping remains a challenge in many parts of the world, particularly in dynamic scenarios such as natural disasters when timely updates are critical. Updating maps is currently a highly manual process requiring a large number of human labelers to either create features or rigorously validate automated outputs. We propose that the frequent revisits of earth imaging satellite constellations may accelerate existing efforts to quickly update foundational maps when combined with advanced machine learning techniques. Accordingly, the SpaceNet partners (CosmiQ Works, Radiant Solutions, and NVIDIA), released a large corpus of labeled satellite imagery on Amazon Web Services (AWS) called SpaceNet. The SpaceNet partners also launched a series of public prize competitions to encourage improvement of remote sensing machine learning algorithms. The first two of these competitions focused on automated building footprint extraction, and the most recent challenge focused on road network extraction. In this paper we discuss the SpaceNet imagery, labels, evaluation metrics, prize challenge results to date, and future plans for the SpaceNet challenge series.



### Who did What at Where and When: Simultaneous Multi-Person Tracking and Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.01253v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01253v1)
- **Published**: 2018-07-03 15:51:25+00:00
- **Updated**: 2018-07-03 15:51:25+00:00
- **Authors**: Wenbo Li, Ming-Ching Chang, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a bootstrapping framework to simultaneously improve multi-person tracking and activity recognition at individual, interaction and social group activity levels. The inference consists of identifying trajectories of all pedestrian actors, individual activities, pairwise interactions, and collective activities, given the observed pedestrian detections. Our method uses a graphical model to represent and solve the joint tracking and recognition problems via multi-stages: (1) activity-aware tracking, (2) joint interaction recognition and occlusion recovery, and (3) collective activity recognition. We solve the where and when problem with visual tracking, as well as the who and what problem with recognition. High-order correlations among the visible and occluded individuals, pairwise interactions, groups, and activities are then solved using a hypergraph formulation within the Bayesian framework. Experiments on several benchmarks show the advantages of our approach over state-of-art methods.



### A Weakly Supervised Adaptive DenseNet for Classifying Thoracic Diseases and Identifying Abnormalities
- **Arxiv ID**: http://arxiv.org/abs/1807.01257v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01257v2)
- **Published**: 2018-07-03 16:03:10+00:00
- **Updated**: 2018-11-05 20:09:24+00:00
- **Authors**: Bo Zhou, Yuemeng Li, Jiangcong Wang
- **Comment**: 10 pages, 6 figures; accepted by IEEE Winter Conference on
  Applications of Computer Vision (2019 WACV)
- **Journal**: None
- **Summary**: We present a weakly supervised deep learning model for classifying thoracic diseases and identifying abnormalities in chest radiography. In this work, instead of learning from medical imaging data with region-level annotations, our model was merely trained on imaging data with image-level labels to classify diseases, and is able to identify abnormal image regions simultaneously. Our model consists of a customized pooling structure and an adaptive DenseNet front-end, which can effectively recognize possible disease features for classification and localization tasks. Our method has been validated on the publicly available ChestX-ray14 dataset. Experimental results have demonstrated that our classification and localization prediction performance achieved significant improvement over the previous models on the ChestX-ray14 dataset. In summary, our network can produce accurate disease classification and localization, which can potentially support clinical decisions.



### Viewpoint Estimation-Insights & Model
- **Arxiv ID**: http://arxiv.org/abs/1807.01312v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01312v1)
- **Published**: 2018-07-03 17:57:58+00:00
- **Updated**: 2018-07-03 17:57:58+00:00
- **Authors**: Gilad Divon, Ayellet Tal
- **Comment**: 17 pages, ECCV2018 submission
- **Journal**: None
- **Summary**: This paper addresses the problem of viewpoint estimation of an object in a given image. It presents five key insights that should be taken into consideration when designing a CNN that solves the problem. Based on these insights, the paper proposes a network in which (i) The architecture jointly solves detection, classification, and viewpoint estimation. (ii) New types of data are added and trained on. (iii) A novel loss function, which takes into account both the geometry of the problem and the new types of data, is propose. Our network improves the state-of-the-art results for this problem by 9.8%.



### A Dataset for Lane Instance Segmentation in Urban Environments
- **Arxiv ID**: http://arxiv.org/abs/1807.01347v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01347v2)
- **Published**: 2018-07-03 19:04:35+00:00
- **Updated**: 2018-08-02 11:03:01+00:00
- **Authors**: Brook Roberts, Sebastian Kaltwang, Sina Samangooei, Mark Pender-Bare, Konstantinos Tertikas, John Redford
- **Comment**: ECCV camera ready
- **Journal**: None
- **Summary**: Autonomous vehicles require knowledge of the surrounding road layout, which can be predicted by state-of-the-art CNNs. This work addresses the current lack of data for determining lane instances, which are needed for various driving manoeuvres. The main issue is the time-consuming manual labelling process, typically applied per image. We notice that driving the car is itself a form of annotation. Therefore, we propose a semi-automated method that allows for efficient labelling of image sequences by utilising an estimated road plane in 3D based on where the car has driven and projecting labels from this plane into all images of the sequence. The average labelling time per image is reduced to 5 seconds and only an inexpensive dash-cam is required for data capture. We are releasing a dataset of 24,000 images and additionally show experimental semantic segmentation and instance segmentation results.



### ModaNet: A Large-Scale Street Fashion Dataset with Polygon Annotations
- **Arxiv ID**: http://arxiv.org/abs/1807.01394v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01394v4)
- **Published**: 2018-07-03 23:20:36+00:00
- **Updated**: 2019-04-10 16:57:55+00:00
- **Authors**: Shuai Zheng, Fan Yang, M. Hadi Kiapour, Robinson Piramuthu
- **Comment**: Accepted as a full paper for an oral presentation at ACM Multimedia
  2018, Seoul, South Korea. ModaNet is only for non-commercial research
- **Journal**: None
- **Summary**: Understanding clothes from a single image has strong commercial and cultural impacts on modern societies. However, this task remains a challenging computer vision problem due to wide variations in the appearance, style, brand and layering of clothing items. We present a new database called ModaNet, a large-scale collection of images based on Paperdoll dataset. Our dataset provides 55,176 street images, fully annotated with polygons on top of the 1 million weakly annotated street images in Paperdoll. ModaNet aims to provide a technical benchmark to fairly evaluate the progress of applying the latest computer vision techniques that rely on large data for fashion understanding. The rich annotation of the dataset allows to measure the performance of state-of-the-art algorithms for object detection, semantic segmentation and polygon prediction on street fashion images in detail. The polygon-based annotation dataset has been released https://github.com/eBay/modanet, we also host the leaderboard at EvalAI: https://evalai.cloudcv.org/featured-challenges/136/overview.



### Endmember Extraction on the Grassmannian
- **Arxiv ID**: http://arxiv.org/abs/1807.01401v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1807.01401v1)
- **Published**: 2018-07-03 23:35:47+00:00
- **Updated**: 2018-07-03 23:35:47+00:00
- **Authors**: Elin Farnell, Henry Kvinge, Michael Kirby, Chris Peterson
- **Comment**: To appear in Proceedings of the 2018 IEEE Data Science Workshop,
  Lausanne, Switzerland
- **Journal**: None
- **Summary**: Endmember extraction plays a prominent role in a variety of data analysis problems as endmembers often correspond to data representing the purest or best representative of some feature. Identifying endmembers then can be useful for further identification and classification tasks. In settings with high-dimensional data, such as hyperspectral imagery, it can be useful to consider endmembers that are subspaces as they are capable of capturing a wider range of variations of a signature. The endmember extraction problem in this setting thus translates to finding the vertices of the convex hull of a set of points on a Grassmannian. In the presence of noise, it can be less clear whether a point should be considered a vertex. In this paper, we propose an algorithm to extract endmembers on a Grassmannian, identify subspaces of interest that lie near the boundary of a convex hull, and demonstrate the use of the algorithm on a synthetic example and on the 220 spectral band AVIRIS Indian Pines hyperspectral image.



