# Arxiv Papers in cs.CV on 2018-07-09
### Vulnerability Analysis of Chest X-Ray Image Classification Against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1807.02905v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02905v2)
- **Published**: 2018-07-09 00:58:04+00:00
- **Updated**: 2018-07-28 22:01:39+00:00
- **Authors**: Saeid Asgari Taghanaki, Arkadeep Das, Ghassan Hamarneh
- **Comment**: Accepted in MICCAI, DLF, 2018
- **Journal**: None
- **Summary**: Recently, there have been several successful deep learning approaches for automatically classifying chest X-ray images into different disease categories. However, there is not yet a comprehensive vulnerability analysis of these models against the so-called adversarial perturbations/attacks, which makes deep models more trustful in clinical practices. In this paper, we extensively analyzed the performance of two state-of-the-art classification deep networks on chest X-ray images. These two networks were attacked by three different categories (ten methods in total) of adversarial methods (both white- and black-box), namely gradient-based, score-based, and decision-based attacks. Furthermore, we modified the pooling operations in the two classification networks to measure their sensitivities against different attacks, on the specific task of chest X-ray classification.



### Partial Policy-based Reinforcement Learning for Anatomical Landmark Localization in 3D Medical Images
- **Arxiv ID**: http://arxiv.org/abs/1807.02908v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.02908v2)
- **Published**: 2018-07-09 01:34:14+00:00
- **Updated**: 2018-12-31 06:22:17+00:00
- **Authors**: Walid Abdullah Al, Il Dong Yun
- **Comment**: None
- **Journal**: None
- **Summary**: Deploying the idea of long-term cumulative return, reinforcement learning has shown remarkable performance in various fields. We propose a formulation of the landmark localization in 3D medical images as a reinforcement learning problem. Whereas value-based methods have been widely used to solve similar problems, we adopt an actor-critic based direct policy search method framed in a temporal difference learning approach. Successful behavior learning is challenging in large state and/or action spaces, requiring many trials. We introduce a partial policy-based reinforcement learning to enable solving the large problem of localization by learning the optimal policy on smaller partial domains. Independent actors efficiently learn the corresponding partial policies, each utilizing their own independent critic. The proposed policy reconstruction from the partial policies ensures a robust and efficient localization utilizing the sub-agents solving simple binary decision problems in their corresponding partial action spaces. The proposed reinforcement learning requires a small number of trials to learn the optimal behavior compared with the original behavior learning scheme.



### Attention to Refine through Multi-Scales for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.02917v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02917v1)
- **Published**: 2018-07-09 02:31:44+00:00
- **Updated**: 2018-07-09 02:31:44+00:00
- **Authors**: Shiqi Yang, Gang Peng
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel attention model for semantic segmentation, which aggregates multi-scale and context features to refine prediction. Specifically, the skeleton convolutional neural network framework takes in multiple different scales inputs, by which means the CNN can get representations in different scales. The proposed attention model will handle the features from different scale streams respectively and integrate them. Then location attention branch of the model learns to softly weight the multi-scale features at each pixel location. Moreover, we add an recalibrating branch, parallel to where location attention comes out, to recalibrate the score map per class. We achieve quite competitive results on PASCAL VOC 2012 and ADE20K datasets, which surpass baseline and related works.



### Vehicle Image Generation Going Well with The Surroundings
- **Arxiv ID**: http://arxiv.org/abs/1807.02925v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02925v4)
- **Published**: 2018-07-09 03:26:10+00:00
- **Updated**: 2021-10-07 11:32:46+00:00
- **Authors**: Jeesoo Kim, Jangho Kim, Jaeyoung Yoo, Daesik Kim, Nojun Kwak
- **Comment**: None
- **Journal**: None
- **Summary**: Since the generative neural networks have made a breakthrough in the image generation problem, lots of researches on their applications have been studied such as image restoration, style transfer and image completion. However, there has been few research generating objects in uncontrolled real-world environments. In this paper, we propose a novel approach for vehicle image generation in real-world scenes. Using a subnetwork based on a precedent work of image completion, our model makes the shape of an object. Details of objects are trained by an additional colorization and refinement subnetwork, resulting in a better quality of generated objects. Unlike many other works, our method does not require any segmentation layout but still makes a plausible vehicle in the image. We evaluate our method by using images from Berkeley Deep Drive (BDD) and Cityscape datasets, which are widely used for object detection and image segmentation problems. The adequacy of the generated images by the proposed method has also been evaluated using a widely utilized object detection algorithm and the FID score.



### Step-by-step Erasion, One-by-one Collection: A Weakly Supervised Temporal Action Detector
- **Arxiv ID**: http://arxiv.org/abs/1807.02929v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02929v2)
- **Published**: 2018-07-09 03:33:54+00:00
- **Updated**: 2018-07-18 08:12:36+00:00
- **Authors**: Jia-Xing Zhong, Nannan Li, Weijie Kong, Tao Zhang, Thomas H. Li, Ge Li
- **Comment**: To Appear in ACM Multimedia 2018
- **Journal**: None
- **Summary**: Weakly supervised temporal action detection is a Herculean task in understanding untrimmed videos, since no supervisory signal except the video-level category label is available on training data. Under the supervision of category labels, weakly supervised detectors are usually built upon classifiers. However, there is an inherent contradiction between classifier and detector; i.e., a classifier in pursuit of high classification performance prefers top-level discriminative video clips that are extremely fragmentary, whereas a detector is obliged to discover the whole action instance without missing any relevant snippet. To reconcile this contradiction, we train a detector by driving a series of classifiers to find new actionness clips progressively, via step-by-step erasion from a complete video. During the test phase, all we need to do is to collect detection results from the one-by-one trained classifiers at various erasing steps. To assist in the collection process, a fully connected conditional random field is established to refine the temporal localization outputs. We evaluate our approach on two prevailing datasets, THUMOS'14 and ActivityNet. The experiments show that our detector advances state-of-the-art weakly supervised temporal action detection results, and even compares with quite a few strongly supervised methods.



### PARN: Pyramidal Affine Regression Networks for Dense Semantic Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1807.02939v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02939v2)
- **Published**: 2018-07-09 04:55:09+00:00
- **Updated**: 2018-08-01 06:10:43+00:00
- **Authors**: Sangryul Jeon, Seungryong Kim, Dongbo Min, Kwanghoon Sohn
- **Comment**: To appear in ECCV 2018
- **Journal**: None
- **Summary**: This paper presents a deep architecture for dense semantic correspondence, called pyramidal affine regression networks (PARN), that estimates locally-varying affine transformation fields across images. To deal with intra-class appearance and shape variations that commonly exist among different instances within the same object category, we leverage a pyramidal model where affine transformation fields are progressively estimated in a coarse-to-fine manner so that the smoothness constraint is naturally imposed within deep networks. PARN estimates residual affine transformations at each level and composes them to estimate final affine transformations. Furthermore, to overcome the limitations of insufficient training data for semantic correspondence, we propose a novel weakly-supervised training scheme that generates progressive supervisions by leveraging a correspondence consistency across image pairs. Our method is fully learnable in an end-to-end manner and does not require quantizing infinite continuous affine transformation fields. To the best of our knowledge, it is the first work that attempts to estimate dense affine transformation fields in a coarse-to-fine manner within deep networks. Experimental results demonstrate that PARN outperforms the state-of-the-art methods for dense semantic correspondence on various benchmarks.



### Multi-Scale Coarse-to-Fine Segmentation for Screening Pancreatic Ductal Adenocarcinoma
- **Arxiv ID**: http://arxiv.org/abs/1807.02941v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02941v2)
- **Published**: 2018-07-09 05:01:19+00:00
- **Updated**: 2019-08-09 02:28:41+00:00
- **Authors**: Zhuotun Zhu, Yingda Xia, Lingxi Xie, Elliot K. Fishman, Alan L. Yuille
- **Comment**: Accepted by MICCAI 2019, 4 figures, 2 tables, 9 pages
- **Journal**: None
- **Summary**: We propose an intuitive approach of detecting pancreatic ductal adenocarcinoma (PDAC), the most common type of pancreatic cancer, by checking abdominal CT scans. Our idea is named multi-scale segmentation-for-classification, which classifies volumes by checking if at least a sufficient number of voxels is segmented as tumors, by which we can provide radiologists with tumor locations. In order to deal with tumors with different scales, we train and test our volumetric segmentation networks with multi-scale inputs in a coarse-to-fine flowchart. A post-processing module is used to filter out outliers and reduce false alarms. We collect a new dataset containing 439 CT scans, in which 136 cases were diagnosed with PDAC and 303 cases are normal, which is the largest set for PDAC tumors to the best of our knowledge. To offer the best trade-off between sensitivity and specificity, our proposed framework reports a sensitivity of 94.1% at a specificity of 98.5%, which demonstrates the potential to make a clinical impact.



### Human Activity Recognition in RGB-D Videos by Dynamic Images
- **Arxiv ID**: http://arxiv.org/abs/1807.02947v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02947v1)
- **Published**: 2018-07-09 05:28:19+00:00
- **Updated**: 2018-07-09 05:28:19+00:00
- **Authors**: Snehasis Mukherjee, Leburu Anvitha, T. Mohana Lahari
- **Comment**: Submitted in ICARCV 2018
- **Journal**: None
- **Summary**: Human Activity Recognition in RGB-D videos has been an active research topic during the last decade. However, no efforts have been found in the literature, for recognizing human activity in RGB-D videos where several performers are performing simultaneously. In this paper we introduce such a challenging dataset with several performers performing the activities. We present a novel method for recognizing human activities in such videos. The proposed method aims in capturing the motion information of the whole video by producing a dynamic image corresponding to the input video. We use two parallel ResNext-101 to produce the dynamic images for the RGB video and depth video separately. The dynamic images contain only the motion information and hence, the unnecessary background information are eliminated. We send the two dynamic images extracted from the RGB and Depth videos respectively, through a fully connected layer of neural networks. The proposed dynamic image reduces the complexity of the recognition process by extracting a sparse matrix from a video. However, the proposed system maintains the required motion information for recognizing the activity. The proposed method has been tested on the MSR Action 3D dataset and has shown comparable performances with respect to the state-of-the-art. We also apply the proposed method on our own dataset, where the proposed method outperforms the state-of-the-art approaches.



### Flow Network Tracking for Spatiotemporal and Periodic Point Matching: Applied to Cardiac Motion Analysis
- **Arxiv ID**: http://arxiv.org/abs/1807.02951v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02951v1)
- **Published**: 2018-07-09 05:54:05+00:00
- **Updated**: 2018-07-09 05:54:05+00:00
- **Authors**: Nripesh Parajuli, Allen Lu, Kevinminh Ta, John C. Stendahl, Nabil Boutagy, Imran Alkhalil, Melissa Eberle, Geng-Shi Jeng, Maria Zontak, Matthew ODonnell, Albert J. Sinusas, James S. Duncan
- **Comment**: Submitted manuscript to Medical Image Analysis Journal
- **Journal**: None
- **Summary**: The accurate quantification of left ventricular (LV) deformation/strain shows significant promise for quantitatively assessing cardiac function for use in diagnosis and therapy planning (Jasaityte et al., 2013). However, accurate estimation of the displacement of myocardial tissue and hence LV strain has been challenging due to a variety of issues, including those related to deriving tracking tokens from images and following tissue locations over the entire cardiac cycle. In this work, we propose a point matching scheme where correspondences are modeled as flow through a graphical network. Myocardial surface points are set up as nodes in the network and edges define neighborhood relationships temporally. The novelty lies in the constraints that are imposed on the matching scheme, which render the correspondences one-to-one through the entire cardiac cycle, and not just two consecutive frames. The constraints also encourage motion to be cyclic, which is an important characteristic of LV motion. We validate our method by applying it to the estimation of quantitative LV displacement and strain estimation using 8 synthetic and 8 open-chested canine 4D echocardiographic image sequences, the latter with sonomicrometric crystals implanted on the LV wall. We were able to achieve excellent tracking accuracy on the synthetic dataset and observed a good correlation with crystal-based strains on the in-vivo data.



### Learning to Index for Nearest Neighbor Search
- **Arxiv ID**: http://arxiv.org/abs/1807.02962v3
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.02962v3)
- **Published**: 2018-07-09 06:55:07+00:00
- **Updated**: 2019-03-26 12:21:18+00:00
- **Authors**: Chih-Yi Chiu, Amorntip Prayoonwong, Yin-Chih Liao
- **Comment**: This paper was accepted by IEEE Transcations on Pattern Analysis and
  Machine Intelligence in March 2019
- **Journal**: None
- **Summary**: In this study, we present a novel ranking model based on learning neighborhood relationships embedded in the index space. Given a query point, conventional approximate nearest neighbor search calculates the distances to the cluster centroids, before ranking the clusters from near to far based on the distances. The data indexed in the top-ranked clusters are retrieved and treated as the nearest neighbor candidates for the query. However, the loss of quantization between the data and cluster centroids will inevitably harm the search accuracy. To address this problem, the proposed model ranks clusters based on their nearest neighbor probabilities rather than the query-centroid distances. The nearest neighbor probabilities are estimated by employing neural networks to characterize the neighborhood relationships, i.e., the density function of nearest neighbors with respect to the query. The proposed probability-based ranking can replace the conventional distance-based ranking for finding candidate clusters, and the predicted probability can be used to determine the data quantity to be retrieved from the candidate cluster. Our experimental results demonstrated that the proposed ranking model could boost the search performance effectively in billion-scale datasets.



### Polarimetric Convolutional Network for PolSAR Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1807.02975v2
- **DOI**: 10.1109/TGRS.2018.2879984
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02975v2)
- **Published**: 2018-07-09 07:52:13+00:00
- **Updated**: 2019-04-03 02:23:41+00:00
- **Authors**: Xu Liu, Licheng Jiao, Xu Tang, Qigong Sun, Dan Zhang
- **Comment**: 15 pages
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, vol. 57, no.
  5, pp. 3040-3054, May 2019
- **Summary**: The approaches for analyzing the polarimetric scattering matrix of polarimetric synthetic aperture radar (PolSAR) data have always been the focus of PolSAR image classification. Generally, the polarization coherent matrix and the covariance matrix obtained by the polarimetric scattering matrix only show a limited number of polarimetric information. In order to solve this problem, we propose a sparse scattering coding way to deal with polarimetric scattering matrix and obtain a close complete feature. This encoding mode can also maintain polarimetric information of scattering matrix completely. At the same time, in view of this encoding way, we design a corresponding classification algorithm based on convolution network to combine this feature. Based on sparse scattering coding and convolution neural network, the polarimetric convolutional network is proposed to classify PolSAR images by making full use of polarimetric information. We perform the experiments on the PolSAR images acquired by AIRSAR and RADARSAT-2 to verify the proposed method. The experimental results demonstrate that the proposed method get better results and has huge potential for PolSAR data classification. Source code for sparse scattering coding is available at https://github.com/liuxuvip/Polarimetric-Scattering-Coding.



### Dynamic Objects Segmentation for Visual Localization in Urban Environments
- **Arxiv ID**: http://arxiv.org/abs/1807.02996v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.02996v1)
- **Published**: 2018-07-09 09:17:59+00:00
- **Updated**: 2018-07-09 09:17:59+00:00
- **Authors**: Guoxiang Zhou, Berta Bescos, Marcin Dymczyk, Mark Pfeiffer, José Neira, Roland Siegwart
- **Comment**: 4 pages, submitted to the IROS 2018 Workshop "From Freezing to
  Jostling Robots: Current Challenges and New Paradigms for Safe Robot
  Navigation in Dense Crowds"
- **Journal**: None
- **Summary**: Visual localization and mapping is a crucial capability to address many challenges in mobile robotics. It constitutes a robust, accurate and cost-effective approach for local and global pose estimation within prior maps. Yet, in highly dynamic environments, like crowded city streets, problems arise as major parts of the image can be covered by dynamic objects. Consequently, visual odometry pipelines often diverge and the localization systems malfunction as detected features are not consistent with the precomputed 3D model. In this work, we present an approach to automatically detect dynamic object instances to improve the robustness of vision-based localization and mapping in crowded environments. By training a convolutional neural network model with a combination of synthetic and real-world data, dynamic object instance masks are learned in a semi-supervised way. The real-world data can be collected with a standard camera and requires minimal further post-processing. Our experiments show that a wide range of dynamic objects can be reliably detected using the presented method. Promising performance is demonstrated on our own and also publicly available datasets, which also shows the generalization capabilities of this approach.



### External Patch-Based Image Restoration Using Importance Sampling
- **Arxiv ID**: http://arxiv.org/abs/1807.03018v1
- **DOI**: 10.1109/TIP.2019.2912122
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03018v1)
- **Published**: 2018-07-09 09:52:56+00:00
- **Updated**: 2018-07-09 09:52:56+00:00
- **Authors**: Milad Niknejad, Jose M. Bioucas-Dias, Mario A. T. Figueiredo
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a new approach to patch-based image restoration based on external datasets and importance sampling. The Minimum Mean Squared Error (MMSE) estimate of the image patches, the computation of which requires solving a multidimensional (typically intractable) integral, is approximated using samples from an external dataset. The new method, which can be interpreted as a generalization of the external non-local means (NLM), uses self-normalized importance sampling to efficiently approximate the MMSE estimates. The use of self-normalized importance sampling endows the proposed method with great flexibility, namely regarding the statistical properties of the measurement noise. The effectiveness of the proposed method is shown in a series of experiments using both generic large-scale and class-specific external datasets.



### Verisimilar Image Synthesis for Accurate Detection and Recognition of Texts in Scenes
- **Arxiv ID**: http://arxiv.org/abs/1807.03021v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T01
- **Links**: [PDF](http://arxiv.org/pdf/1807.03021v2)
- **Published**: 2018-07-09 09:58:06+00:00
- **Updated**: 2018-09-26 07:55:02+00:00
- **Authors**: Fangneng Zhan, Shijian Lu, Chuhui Xue
- **Comment**: 14 pages, ECCV2018, datasets:
  https://github.com/fnzhan/Verisimilar-Image-Synthesis-for-Accurate-Detection-and-Recognition-of-Texts-in-Scenes
- **Journal**: None
- **Summary**: The requirement of large amounts of annotated images has become one grand challenge while training deep neural network models for various visual detection and recognition tasks. This paper presents a novel image synthesis technique that aims to generate a large amount of annotated scene text images for training accurate and robust scene text detection and recognition models. The proposed technique consists of three innovative designs. First, it realizes "semantic coherent" synthesis by embedding texts at semantically sensible regions within the background image, where the semantic coherence is achieved by leveraging the semantic annotations of objects and image regions that have been created in the prior semantic segmentation research. Second, it exploits visual saliency to determine the embedding locations within each semantic sensible region, which coincides with the fact that texts are often placed around homogeneous regions for better visibility in scenes. Third, it designs an adaptive text appearance model that determines the color and brightness of embedded texts by learning from the feature of real scene text images adaptively. The proposed technique has been evaluated over five public datasets and the experiments show its superior performance in training accurate and robust scene text detection and recognition models.



### Pioneer Networks: Progressively Growing Generative Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1807.03026v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.03026v2)
- **Published**: 2018-07-09 10:19:51+00:00
- **Updated**: 2018-10-09 15:26:41+00:00
- **Authors**: Ari Heljakka, Arno Solin, Juho Kannala
- **Comment**: To appear in ACCV 2018
- **Journal**: None
- **Summary**: We introduce a novel generative autoencoder network model that learns to encode and reconstruct images with high quality and resolution, and supports smooth random sampling from the latent space of the encoder. Generative adversarial networks (GANs) are known for their ability to simulate random high-quality images, but they cannot reconstruct existing images. Previous works have attempted to extend GANs to support such inference but, so far, have not delivered satisfactory high-quality results. Instead, we propose the Progressively Growing Generative Autoencoder (PIONEER) network which achieves high-quality reconstruction with $128{\times}128$ images without requiring a GAN discriminator. We merge recent techniques for progressively building up the parts of the network with the recently introduced adversarial encoder-generator network. The ability to reconstruct input images is crucial in many real-world applications, and allows for precise intelligent manipulation of existing images. We show promising results in image synthesis and inference, with state-of-the-art results in CelebA inference tasks.



### Image Restoration Using Conditional Random Fields and Scale Mixtures of Gaussians
- **Arxiv ID**: http://arxiv.org/abs/1807.03027v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03027v1)
- **Published**: 2018-07-09 10:24:54+00:00
- **Updated**: 2018-07-09 10:24:54+00:00
- **Authors**: Milad Niknejad, Jose M. Bioucas-Dias, Mario A. T. Figueiredo
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a general framework for internal patch-based image restoration based on Conditional Random Fields (CRF). Unlike related models based on Markov Random Fields (MRF), our approach explicitly formulates the posterior distribution for the entire image. The potential functions are taken as proportional to the product of a likelihood and prior for each patch. By assuming identical parameters for similar patches, our approach can be classified as a model-based non-local method. For the prior term in the potential function of the CRF model, multivariate Gaussians and multivariate scale-mixture of Gaussians are considered, with the latter being a novel prior for image patches. Our results show that the proposed approach outperforms methods based on Gaussian mixture models for image denoising and state-of-the-art methods for image interpolation/inpainting.



### Convolutional Recurrent Neural Networks for Glucose Prediction
- **Arxiv ID**: http://arxiv.org/abs/1807.03043v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03043v5)
- **Published**: 2018-07-09 11:12:16+00:00
- **Updated**: 2019-02-25 21:06:08+00:00
- **Authors**: Kezhi Li, John Daniels, Chengyuan Liu, Pau Herrero, Pantelis Georgiou
- **Comment**: 10 pages, 7 figures
- **Journal**: IEEE journal of biomedical and health informatics 2019
- **Summary**: Control of blood glucose is essential for diabetes management. Current digital therapeutic approaches for subjects with Type 1 diabetes mellitus (T1DM) such as the artificial pancreas and insulin bolus calculators leverage machine learning techniques for predicting subcutaneous glucose for improved control. Deep learning has recently been applied in healthcare and medical research to achieve state-of-the-art results in a range of tasks including disease diagnosis, and patient state prediction among others. In this work, we present a deep learning model that is capable of forecasting glucose levels with leading accuracy for simulated patient cases (RMSE = 9.38$\pm$0.71 [mg/dL] over a 30-minute horizon, RMSE = 18.87$\pm$2.25 [mg/dL] over a 60-minute horizon) and real patient cases (RMSE = 21.07$\pm$2.35 [mg/dL] for 30-minute, RMSE = 33.27$\pm$4.79\% for 60-minute). In addition, the model provides competitive performance in providing effective prediction horizon ($PH_{eff}$) with minimal time lag both in a simulated patient dataset ($PH_{eff}$ = 29.0$\pm$0.7 for 30-min and $PH_{eff}$ = 49.8$\pm$2.9 for 60-min) and in a real patient dataset ($PH_{eff}$ = 19.3$\pm$3.1 for 30-min and $PH_{eff}$ = 29.3$\pm$9.4 for 60-min). This approach is evaluated on a dataset of 10 simulated cases generated from the UVa/Padova simulator and a clinical dataset of 10 real cases each containing glucose readings, insulin bolus, and meal (carbohydrate) data. Performance of the recurrent convolutional neural network is benchmarked against four algorithms. The proposed algorithm is implemented on an Android mobile phone, with an execution time of $6$ms on a phone compared to an execution time of $780$ms on a laptop.



### Deriving Neural Network Architectures using Precision Learning: Parallel-to-fan beam Conversion
- **Arxiv ID**: http://arxiv.org/abs/1807.03057v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03057v2)
- **Published**: 2018-07-09 11:43:07+00:00
- **Updated**: 2018-10-23 12:37:46+00:00
- **Authors**: Christopher Syben, Bernhard Stimpel, Jonathan Lommen, Tobias Würfl, Arnd Dörfler, Andreas Maier
- **Comment**: Inproceedings GCPR 2018
- **Journal**: None
- **Summary**: In this paper, we derive a neural network architecture based on an analytical formulation of the parallel-to-fan beam conversion problem following the concept of precision learning. The network allows to learn the unknown operators in this conversion in a data-driven manner avoiding interpolation and potential loss of resolution. Integration of known operators results in a small number of trainable parameters that can be estimated from synthetic data only. The concept is evaluated in the context of Hybrid MRI/X-ray imaging where transformation of the parallel-beam MRI projections to fan-beam X-ray projections is required. The proposed method is compared to a traditional rebinning method. The results demonstrate that the proposed method is superior to ray-by-ray interpolation and is able to deliver sharper images using the same amount of parallel-beam input projections which is crucial for interventional applications. We believe that this approach forms a basis for further work uniting deep learning, signal processing, physics, and traditional pattern recognition.



### ChestNet: A Deep Neural Network for Classification of Thoracic Diseases on Chest Radiography
- **Arxiv ID**: http://arxiv.org/abs/1807.03058v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03058v1)
- **Published**: 2018-07-09 11:48:42+00:00
- **Updated**: 2018-07-09 11:48:42+00:00
- **Authors**: Hongyu Wang, Yong Xia
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Computer-aided techniques may lead to more accurate and more acces-sible diagnosis of thorax diseases on chest radiography. Despite the success of deep learning-based solutions, this task remains a major challenge in smart healthcare, since it is intrinsically a weakly supervised learning problem. In this paper, we incorporate the attention mechanism into a deep convolutional neural network, and thus propose the ChestNet model to address effective diagnosis of thorax diseases on chest radiography. This model consists of two branches: a classification branch serves as a uniform feature extraction-classification network to free users from troublesome handcrafted feature extraction, and an attention branch exploits the correlation between class labels and the locations of patholog-ical abnormalities and allows the model to concentrate adaptively on the patholog-ically abnormal regions. We evaluated our model against three state-of-the-art deep learning models on the Chest X-ray 14 dataset using the official patient-wise split. The results indicate that our model outperforms other methods, which use no extra training data, in diagnosing 14 thorax diseases on chest radiography.



### Video Summarisation by Classification with Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.03089v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03089v3)
- **Published**: 2018-07-09 13:05:36+00:00
- **Updated**: 2018-09-03 23:56:18+00:00
- **Authors**: Kaiyang Zhou, Tao Xiang, Andrea Cavallaro
- **Comment**: In Proc. of BMVC 2018
- **Journal**: None
- **Summary**: Most existing video summarisation methods are based on either supervised or unsupervised learning. In this paper, we propose a reinforcement learning-based weakly supervised method that exploits easy-to-obtain, video-level category labels and encourages summaries to contain category-related information and maintain category recognisability. Specifically, We formulate video summarisation as a sequential decision-making process and train a summarisation network with deep Q-learning (DQSN). A companion classification network is also trained to provide rewards for training the DQSN. With the classification network, we develop a global recognisability reward based on the classification result. Critically, a novel dense ranking-based reward is also proposed in order to cope with the temporally delayed and sparse reward problems for long sequence reinforcement learning. Extensive experiments on two benchmark datasets show that the proposed approach achieves state-of-the-art performance.



### Deep Multimodal Clustering for Unsupervised Audiovisual Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.03094v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1807.03094v3)
- **Published**: 2018-07-09 13:13:10+00:00
- **Updated**: 2019-04-19 07:36:17+00:00
- **Authors**: Di Hu, Feiping Nie, Xuelong Li
- **Comment**: Accepted by CVPR2019
- **Journal**: None
- **Summary**: The seen birds twitter, the running cars accompany with noise, etc. These naturally audiovisual correspondences provide the possibilities to explore and understand the outside world. However, the mixed multiple objects and sounds make it intractable to perform efficient matching in the unconstrained environment. To settle this problem, we propose to adequately excavate audio and visual components and perform elaborate correspondence learning among them. Concretely, a novel unsupervised audiovisual learning model is proposed, named as \Deep Multimodal Clustering (DMC), that synchronously performs sets of clustering with multimodal vectors of convolutional maps in different shared spaces for capturing multiple audiovisual correspondences. And such integrated multimodal clustering network can be effectively trained with max-margin loss in the end-to-end fashion. Amounts of experiments in feature evaluation and audiovisual tasks are performed. The results demonstrate that DMC can learn effective unimodal representation, with which the classifier can even outperform human performance. Further, DMC shows noticeable performance in sound localization, multisource detection, and audiovisual understanding.



### Approximate k-space models and Deep Learning for fast photoacoustic reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1807.03191v1
- **DOI**: 10.1007/978-3-030-00129-2_12
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS, math.OC, 49N45, 65T50
- **Links**: [PDF](http://arxiv.org/pdf/1807.03191v1)
- **Published**: 2018-07-09 14:32:18+00:00
- **Updated**: 2018-07-09 14:32:18+00:00
- **Authors**: Andreas Hauptmann, Ben Cox, Felix Lucka, Nam Huynh, Marta Betcke, Paul Beard, Simon Arridge
- **Comment**: None
- **Journal**: None
- **Summary**: We present a framework for accelerated iterative reconstructions using a fast and approximate forward model that is based on k-space methods for photoacoustic tomography. The approximate model introduces aliasing artefacts in the gradient information for the iterative reconstruction, but these artefacts are highly structured and we can train a CNN that can use the approximate information to perform an iterative reconstruction. We show feasibility of the method for human in-vivo measurements in a limited-view geometry. The proposed method is able to produce superior results to total variation reconstructions with a speed-up of 32 times.



### Fashion is Taking Shape: Understanding Clothing Preference Based on Body Shape From Online Sources
- **Arxiv ID**: http://arxiv.org/abs/1807.03235v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1807.03235v2)
- **Published**: 2018-07-09 15:35:13+00:00
- **Updated**: 2018-12-15 11:27:46+00:00
- **Authors**: Hosnieh Sattar, Gerard Pons-Moll, Mario Fritz
- **Comment**: None
- **Journal**: None
- **Summary**: To study the correlation between clothing garments and body shape, we collected a new dataset (Fashion Takes Shape), which includes images of users with clothing category annotations. We employ our multi-photo approach to estimate body shapes of each user and build a conditional model of clothing categories given body-shape. We demonstrate that in real-world data, clothing categories and body-shapes are correlated and show that our multi-photo approach leads to a better predictive model for clothing categories compared to models based on single-view shape estimates or manually annotated body types. We see our method as the first step towards the large-scale understanding of clothing preferences from body shape.



### Automatic multi-objective based feature selection for classification
- **Arxiv ID**: http://arxiv.org/abs/1807.03236v4
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1807.03236v4)
- **Published**: 2018-07-09 15:37:10+00:00
- **Updated**: 2019-02-13 01:31:02+00:00
- **Authors**: Zhiguo Zhou, Shulong Li, Genggeng Qin, Michael Folkert, Steve Jiang, Jing Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Objective: Accurately classifying the malignancy of lesions detected in a screening scan is critical for reducing false positives. Radiomics holds great potential to differentiate malignant from benign tumors by extracting and analyzing a large number of quantitative image features. Since not all radiomic features contribute to an effective classifying model, selecting an optimal feature subset is critical. Methods: This work proposes a new multi-objective based feature selection (MO-FS) algorithm that considers sensitivity and specificity simultaneously as the objective functions during feature selection. For MO-FS, we developed a modified entropy based termination criterion (METC) that stops the algorithm automatically rather than relying on a preset number of generations. We also designed a solution selection methodology for multi-objective learning that uses the evidential reasoning approach (SMOLER) to automatically select the optimal solution from the Pareto-optimal set. Furthermore, we developed an adaptive mutation operation to generate the mutation probability in MO-FS automatically. Results: We evaluated the MO-FS for classifying lung nodule malignancy in low-dose CT and breast lesion malignancy in digital breast tomosynthesis. Conclusion: The experimental results demonstrated that the feature set selected by MO-FS achieved better classification performance than features selected by other commonly used methods. Significance: The proposed method is general and more effective radiomic feature selection strategy.



### Exploring Brain-wide Development of Inhibition through Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.03238v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1807.03238v1)
- **Published**: 2018-07-09 15:39:09+00:00
- **Updated**: 2018-07-09 15:39:09+00:00
- **Authors**: Asim Iqbal, Asfandyar Sheikh, Theofanis Karayannis
- **Comment**: 33 pages, 21 Figures
- **Journal**: None
- **Summary**: We introduce here a fully automated convolutional neural network-based method for brain image processing to Detect Neurons in different brain Regions during Development (DeNeRD). Our method takes a developing mouse brain as input and i) registers the brain sections against a developing mouse reference atlas, ii) detects various types of neurons, and iii) quantifies the neural density in many unique brain regions at different postnatal (P) time points. Our method is invariant to the shape, size and expression of neurons and by using DeNeRD, we compare the brain-wide neural density of all GABAergic neurons in developing brains of ages P4, P14 and P56. We discover and report 6 different clusters of regions in the mouse brain in which GABAergic neurons develop in a differential manner from early age (P4) to adulthood (P56). These clusters reveal key steps of GABAergic cell development that seem to track with the functional development of diverse brain regions as the mouse transitions from a passive receiver of sensory information (<P14) to an active seeker (>P14).



### An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution
- **Arxiv ID**: http://arxiv.org/abs/1807.03247v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.03247v2)
- **Published**: 2018-07-09 15:48:08+00:00
- **Updated**: 2018-12-03 16:31:58+00:00
- **Authors**: Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric Frank, Alex Sergeev, Jason Yosinski
- **Comment**: Published in NeurIPS 2018
- **Journal**: None
- **Summary**: Few ideas have enjoyed as large an impact on deep learning as convolution. For any problem involving pixels or spatial representations, common intuition holds that convolutional neural networks may be appropriate. In this paper we show a striking counterexample to this intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in (x,y) Cartesian space and one-hot pixel space. Although convolutional networks would seem appropriate for this task, we show that they fail spectacularly. We demonstrate and carefully analyze the failure first on a toy problem, at which point a simple fix becomes obvious. We call this solution CoordConv, which works by giving convolution access to its own input coordinates through the use of extra coordinate channels. Without sacrificing the computational and parametric efficiency of ordinary convolution, CoordConv allows networks to learn either complete translation invariance or varying degrees of translation dependence, as required by the end task. CoordConv solves the coordinate transform problem with perfect generalization and 150 times faster with 10--100 times fewer parameters than convolution. This stark contrast raises the question: to what extent has this inability of convolution persisted insidiously inside other tasks, subtly hampering performance from within? A complete answer to this question will require further investigation, but we show preliminary evidence that swapping convolution for CoordConv can improve models on a diverse set of tasks. Using CoordConv in a GAN produced less mode collapse as the transform between high-level spatial latents and pixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST showed 24% better IOU when using CoordConv, and in the RL domain agents playing Atari games benefit significantly from the use of CoordConv layers.



### Barqi Breed Sheep Weight Estimation based on Neural Network with Regression
- **Arxiv ID**: http://arxiv.org/abs/1807.10568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10568v1)
- **Published**: 2018-07-09 16:20:44+00:00
- **Updated**: 2018-07-09 16:20:44+00:00
- **Authors**: Chintan Bhatt, Aboul-ella Hassanien, Nirav Alpesh Shah, Jaydeep Thik
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision is a very powerful method for understanding the contents from the images. We tried to utilize this powerful technology to make the difficult task of estimating sheep weights quick and accurate. It has enabled us to minimize the human involvement in measuring weight of the sheep. We are using a novel approach for segmentation and neural network based regression model for achieving better results for the task of estimating sheep weight.



### Pooling Pyramid Network for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.03284v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03284v1)
- **Published**: 2018-07-09 17:40:09+00:00
- **Updated**: 2018-07-09 17:40:09+00:00
- **Authors**: Pengchong Jin, Vivek Rathod, Xiangxin Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: We'd like to share a simple tweak of Single Shot Multibox Detector (SSD) family of detectors, which is effective in reducing model size while maintaining the same quality. We share box predictors across all scales, and replace convolution between scales with max pooling. This has two advantages over vanilla SSD: (1) it avoids score miscalibration across scales; (2) the shared predictor sees the training data over all scales. Since we reduce the number of predictors to one, and trim all convolutions between them, model size is significantly smaller. We empirically show that these changes do not hurt model quality compared to vanilla SSD.



### Adaptive Adversarial Attack on Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.03326v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.03326v3)
- **Published**: 2018-07-09 18:12:27+00:00
- **Updated**: 2020-04-01 15:40:36+00:00
- **Authors**: Xiaoyong Yuan, Pan He, Xiaolin Andy Li, Dapeng Oliver Wu
- **Comment**: To be appear in INFOCOM 2020, The Eighth International Workshop on
  Security and Privacy in Big Data
- **Journal**: None
- **Summary**: Recent studies have shown that state-of-the-art deep learning models are vulnerable to the inputs with small perturbations (adversarial examples). We observe two critical obstacles in adversarial examples: (i) Strong adversarial attacks (e.g., C&W attack) require manually tuning hyper-parameters and take a long time to construct an adversarial example, making it impractical to attack real-time systems; (ii) Most of the studies focus on non-sequential tasks, such as image classification, yet only a few consider sequential tasks. In this work, we speed up adversarial attacks, especially on sequential learning tasks. By leveraging the uncertainty of each task, we directly learn the adaptive multi-task weightings, without manually searching hyper-parameters. A unified architecture is developed and evaluated for both non-sequential tasks and sequential ones. To validate the effectiveness, we take the scene text recognition task as a case study. To our best knowledge, our proposed method is the first attempt to adversarial attack for scene text recognition. Adaptive Attack achieves over 99.9\% success rate with 3-6X speedup compared to state-of-the-art adversarial attacks.



### PCL: Proposal Cluster Learning for Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.03342v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03342v2)
- **Published**: 2018-07-09 18:59:29+00:00
- **Updated**: 2018-10-13 18:28:09+00:00
- **Authors**: Peng Tang, Xinggang Wang, Song Bai, Wei Shen, Xiang Bai, Wenyu Liu, Alan Yuille
- **Comment**: Accepted by TPAMI. Codes are available at
  https://github.com/ppengtang/oicr/tree/pcl
- **Journal**: None
- **Summary**: Weakly Supervised Object Detection (WSOD), using only image-level annotations to train object detectors, is of growing importance in object recognition. In this paper, we propose a novel deep network for WSOD. Unlike previous networks that transfer the object detection problem to an image classification problem using Multiple Instance Learning (MIL), our strategy generates proposal clusters to learn refined instance classifiers by an iterative process. The proposals in the same cluster are spatially adjacent and associated with the same object. This prevents the network from concentrating too much on parts of objects instead of whole objects. We first show that instances can be assigned object or background labels directly based on proposal clusters for instance classifier refinement, and then show that treating each cluster as a small new bag yields fewer ambiguities than the directly assigning label method. The iterative instance classifier refinement is implemented online using multiple streams in convolutional neural networks, where the first is an MIL network and the others are for instance classifier refinement supervised by the preceding one. Experiments are conducted on the PASCAL VOC, ImageNet detection, and MS-COCO benchmarks for WSOD. Results show that our method outperforms the previous state of the art significantly.



### Complex Fully Convolutional Neural Networks for MR Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1807.03343v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1807.03343v1)
- **Published**: 2018-07-09 19:03:25+00:00
- **Updated**: 2018-07-09 19:03:25+00:00
- **Authors**: Muneer Ahmad Dedmari, Sailesh Conjeti, Santiago Estrada, Phillip Ehses, Tony Stöcker, Martin Reuter
- **Comment**: 9 pages, accepted in MICCAI-MLMIR 2018 Worshop
- **Journal**: None
- **Summary**: Undersampling the k-space data is widely adopted for acceleration of Magnetic Resonance Imaging (MRI). Current deep learning based approaches for supervised learning of MRI image reconstruction employ real-valued operations and representations by treating complex valued k-space/spatial-space as real values. In this paper, we propose complex dense fully convolutional neural network ($\mathbb{C}$DFNet) for learning to de-alias the reconstruction artifacts within undersampled MRI images. We fashioned a densely-connected fully convolutional block tailored for complex-valued inputs by introducing dedicated layers such as complex convolution, batch normalization, non-linearities etc. $\mathbb{C}$DFNet leverages the inherently complex-valued nature of input k-space and learns richer representations. We demonstrate improved perceptual quality and recovery of anatomical structures through $\mathbb{C}$DFNet in contrast to its real-valued counterparts.



### HDFD --- A High Deformation Facial Dynamics Benchmark for Evaluation of Non-Rigid Surface Registration and Classification
- **Arxiv ID**: http://arxiv.org/abs/1807.03354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03354v1)
- **Published**: 2018-07-09 19:36:42+00:00
- **Updated**: 2018-07-09 19:36:42+00:00
- **Authors**: Gareth Andrews, Sam Endean, Roberto Dyke, Yukun Lai, Gwenno Ffrancon, Gary KL Tam
- **Comment**: None
- **Journal**: None
- **Summary**: Objects that undergo non-rigid deformation are common in the real world. A typical and challenging example is the human faces. While various techniques have been developed for deformable shape registration and classification, benchmarks with detailed labels and landmarks suitable for evaluating such techniques are still limited. In this paper, we present a novel facial dynamic dataset HDFD which addresses the gap of existing datasets, including 4D funny faces with substantial non-isometric deformation, and 4D visual-audio faces of spoken phrases in a minority language (Welsh). Both datasets are captured from 21 participants. The sequences are manually landmarked, with the spoken phrases further rated by a Welsh expert for level of fluency. These are useful for quantitative evaluation of both registration and classification tasks. We further develop a methodology to evaluate several recent non-rigid surface registration techniques, using our dynamic sequences as test cases. The study demonstrates the significance and usefulness of our new dataset --- a challenging benchmark dataset for future techniques.



### Weakly-Supervised Convolutional Neural Networks for Multimodal Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1807.03361v1
- **DOI**: 10.1016/j.media.2018.07.002
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1807.03361v1)
- **Published**: 2018-07-09 19:53:16+00:00
- **Updated**: 2018-07-09 19:53:16+00:00
- **Authors**: Yipeng Hu, Marc Modat, Eli Gibson, Wenqi Li, Nooshin Ghavami, Ester Bonmati, Guotai Wang, Steven Bandula, Caroline M. Moore, Mark Emberton, Sébastien Ourselin, J. Alison Noble, Dean C. Barratt, Tom Vercauteren
- **Comment**: Accepted manuscript in Medical Image Analysis
- **Journal**: None
- **Summary**: One of the fundamental challenges in supervised learning for multimodal image registration is the lack of ground-truth for voxel-level spatial correspondence. This work describes a method to infer voxel-level transformation from higher-level correspondence information contained in anatomical labels. We argue that such labels are more reliable and practical to obtain for reference sets of image pairs than voxel-level correspondence. Typical anatomical labels of interest may include solid organs, vessels, ducts, structure boundaries and other subject-specific ad hoc landmarks. The proposed end-to-end convolutional neural network approach aims to predict displacement fields to align multiple labelled corresponding structures for individual image pairs during the training, while only unlabelled image pairs are used as the network input for inference. We highlight the versatility of the proposed strategy, for training, utilising diverse types of anatomical labels, which need not to be identifiable over all training image pairs. At inference, the resulting 3D deformable image registration algorithm runs in real-time and is fully-automated without requiring any anatomical labels or initialisation. Several network architecture variants are compared for registering T2-weighted magnetic resonance images and 3D transrectal ultrasound images from prostate cancer patients. A median target registration error of 3.6 mm on landmark centroids and a median Dice of 0.87 on prostate glands are achieved from cross-validation experiments, in which 108 pairs of multimodal images from 76 patients were tested with high-quality anatomical labels.



### Talk the Walk: Navigating New York City through Grounded Dialogue
- **Arxiv ID**: http://arxiv.org/abs/1807.03367v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.03367v3)
- **Published**: 2018-07-09 20:05:24+00:00
- **Updated**: 2018-12-23 22:42:59+00:00
- **Authors**: Harm de Vries, Kurt Shuster, Dhruv Batra, Devi Parikh, Jason Weston, Douwe Kiela
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce "Talk The Walk", the first large-scale dialogue dataset grounded in action and perception. The task involves two agents (a "guide" and a "tourist") that communicate via natural language in order to achieve a common goal: having the tourist navigate to a given target location. The task and dataset, which are described in detail, are challenging and their full solution is an open problem that we pose to the community. We (i) focus on the task of tourist localization and develop the novel Masked Attention for Spatial Convolutions (MASC) mechanism that allows for grounding tourist utterances into the guide's map, (ii) show it yields significant improvements for both emergent and natural language communication, and (iii) using this method, we establish non-trivial baselines on the full task.



### Beyond Pixels: Image Provenance Analysis Leveraging Metadata
- **Arxiv ID**: http://arxiv.org/abs/1807.03376v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03376v3)
- **Published**: 2018-07-09 20:34:30+00:00
- **Updated**: 2019-03-06 19:03:26+00:00
- **Authors**: Aparna Bharati, Daniel Moreira, Joel Brogan, Patricia Hale, Kevin W. Bowyer, Patrick J. Flynn, Anderson Rocha, Walter J. Scheirer
- **Comment**: Supplemental material for this paper can be found at
  https://drive.google.com/file/d/1Tbs2CQg_VQAc2PdztW5twVaiXD0G12-H/view?usp=sharing
- **Journal**: None
- **Summary**: Creative works, whether paintings or memes, follow unique journeys that result in their final form. Understanding these journeys, a process known as "provenance analysis", provides rich insights into the use, motivation, and authenticity underlying any given work. The application of this type of study to the expanse of unregulated content on the Internet is what we consider in this paper. Provenance analysis provides a snapshot of the chronology and validity of content as it is uploaded, re-uploaded, and modified over time. Although still in its infancy, automated provenance analysis for online multimedia is already being applied to different types of content. Most current works seek to build provenance graphs based on the shared content between images or videos. This can be a computationally expensive task, especially when considering the vast influx of content that the Internet sees every day. Utilizing non-content-based information, such as timestamps, geotags, and camera IDs can help provide important insights into the path a particular image or video has traveled during its time on the Internet without large computational overhead. This paper tests the scope and applicability of metadata-based inferences for provenance graph construction in two different scenarios: digital image forensics and cultural analytics.



### An Attention Model for group-level emotion recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.03380v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03380v1)
- **Published**: 2018-07-09 20:40:50+00:00
- **Updated**: 2018-07-09 20:40:50+00:00
- **Authors**: Aarush Gupta, Dakshit Agrawal, Hardik Chauhan, Jose Dolz, Marco Pedersoli
- **Comment**: 5 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: In this paper we propose a new approach for classifying the global emotion of images containing groups of people. To achieve this task, we consider two different and complementary sources of information: i) a global representation of the entire image (ii) a local representation where only faces are considered. While the global representation of the image is learned with a convolutional neural network (CNN), the local representation is obtained by merging face features through an attention mechanism. The two representations are first learned independently with two separate CNN branches and then fused through concatenation in order to obtain the final group-emotion classifier. For our submission to the EmotiW 2018 group-level emotion recognition challenge, we combine several variations of the proposed model into an ensemble, obtaining a final accuracy of 64.83% on the test set and ranking 4th among all challenge participants.



### High-Resolution Mammogram Synthesis using Progressive Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.03401v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03401v2)
- **Published**: 2018-07-09 21:53:54+00:00
- **Updated**: 2019-09-03 13:27:04+00:00
- **Authors**: Dimitrios Korkinof, Tobias Rijken, Michael O'Neill, Joseph Yearsley, Hugh Harvey, Ben Glocker
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to generate synthetic medical images is useful for data augmentation, domain transfer, and out-of-distribution detection. However, generating realistic, high-resolution medical images is challenging, particularly for Full Field Digital Mammograms (FFDM), due to the textural heterogeneity, fine structural details and specific tissue properties. In this paper, we explore the use of progressively trained generative adversarial networks (GANs) to synthesize mammograms, overcoming the underlying instabilities when training such adversarial models. This work is the first to show that generation of realistic synthetic medical images is feasible at up to 1280x1024 pixels, the highest resolution achieved for medical image synthesis, enabling visualizations within standard mammographic hanging protocols. We hope this work can serve as a useful guide and facilitate further research on GANs in the medical imaging domain.



### High Fidelity Semantic Shape Completion for Point Clouds using Latent Optimization
- **Arxiv ID**: http://arxiv.org/abs/1807.03407v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03407v2)
- **Published**: 2018-07-09 22:24:17+00:00
- **Updated**: 2018-09-30 01:06:28+00:00
- **Authors**: Swaminathan Gurumurthy, Shubham Agrawal
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic shape completion is a challenging problem in 3D computer vision where the task is to generate a complete 3D shape using a partial 3D shape as input. We propose a learning-based approach to complete incomplete 3D shapes through generative modeling and latent manifold optimization. Our algorithm works directly on point clouds. We use an autoencoder and a GAN to learn a distribution of embeddings for point clouds of object classes. An input point cloud with missing regions is first encoded to a feature vector. The representations learnt by the GAN are then used to find the best latent vector on the manifold using a combined optimization that finds a vector in the manifold of plausible vectors that is close to the original input (both in the feature space and the output space of the decoder). Experiments show that our algorithm is capable of successfully reconstructing point clouds with large missing regions with very high fidelity without having to rely on exemplar based database retrieval.



