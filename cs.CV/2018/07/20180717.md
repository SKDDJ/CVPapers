# Arxiv Papers in cs.CV on 2018-07-17
### Layer-wise Relevance Propagation for Explainable Recommendations
- **Arxiv ID**: http://arxiv.org/abs/1807.06160v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IR, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.06160v1)
- **Published**: 2018-07-17 00:38:31+00:00
- **Updated**: 2018-07-17 00:38:31+00:00
- **Authors**: Homanga Bharadhwaj
- **Comment**: Accepted in Proceedings of the EARS Workshop at SIGIR 2018
- **Journal**: Homanga Bharadhwaj. 2018. Layer-wise Relevance Propagation for
  Explainable Recommendations. In Proceedings of SIGIR 2018 Workshop on
  ExplainAble Recommendation and Search (EARS'18). ACM, New York, NY, USA
- **Summary**: In this paper, we tackle the problem of explanations in a deep-learning based model for recommendations by leveraging the technique of layer-wise relevance propagation. We use a Deep Convolutional Neural Network to extract relevant features from the input images before identifying similarity between the images in feature space. Relationships between the images are identified by the model and layer-wise relevance propagation is used to infer pixel-level details of the images that may have significantly informed the model's choice. We evaluate our method on an Amazon products dataset and demonstrate the efficacy of our approach.



### Photo-unrealistic Image Enhancement for Subject Placement in Outdoor Photography
- **Arxiv ID**: http://arxiv.org/abs/1807.06196v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1807.06196v1)
- **Published**: 2018-07-17 03:24:14+00:00
- **Updated**: 2018-07-17 03:24:14+00:00
- **Authors**: Christian Tendyck, Andrew Haddad, Mireille Boutin
- **Comment**: None
- **Journal**: None
- **Summary**: Camera display reflections are an issue in bright light situations, as they may prevent users from correctly positioning the subject in the picture. We propose a software solution to this problem, which consists in modifying the image in the viewer, in real time. In our solution, the user is seeing a posterized image which roughly represents the contour of the objects. Five enhancement methods are compared in a user study. Our results indicate that the problem considered is a valid one, as users had problems locating landmarks nearly 37% of the time under sunny conditions, and that our proposed enhancement method using contrasting colors is a practical solution to that problem.



### Learning Generic Diffusion Processes for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1807.06216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06216v1)
- **Published**: 2018-07-17 04:21:29+00:00
- **Updated**: 2018-07-17 04:21:29+00:00
- **Authors**: Peng Qiao, Yong Dou, Yunjin Chen, Wensen Feng
- **Comment**: 12 pages, 3 figures, 3 tables
- **Journal**: British Machine Vision Conference 2018
- **Summary**: Image restoration problems are typical ill-posed problems where the regularization term plays an important role. The regularization term learned via generative approaches is easy to transfer to various image restoration, but offers inferior restoration quality compared with that learned via discriminative approaches. On the contrary, the regularization term learned via discriminative approaches are usually trained for a specific image restoration problem, and fail in the problem for which it is not trained. To address this issue, we propose a generic diffusion process (genericDP) to handle multiple Gaussian denoising problems based on the Trainable Non-linear Reaction Diffusion (TNRD) models. Instead of one model, which consists of a diffusion and a reaction term, for one Gaussian denoising problem in TNRD, we enforce multiple TNRD models to share one diffusion term. The trained genericDP model can provide both promising denoising performance and high training efficiency compared with the original TNRD models. We also transfer the trained diffusion term to non-blind deconvolution which is unseen in the training phase. Experiment results show that the trained diffusion term for multiple Gaussian denoising can be transferred to image non-blind deconvolution as an image prior and provide competitive performance.



### Robust Deep Multi-modal Learning Based on Gated Information Fusion Network
- **Arxiv ID**: http://arxiv.org/abs/1807.06233v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06233v2)
- **Published**: 2018-07-17 05:42:32+00:00
- **Updated**: 2018-11-02 11:02:01+00:00
- **Authors**: Jaekyum Kim, Junho Koh, Yecheol Kim, Jaehyung Choi, Youngbae Hwang, Jun Won Choi
- **Comment**: 2018 Asian Conference on Computer Vision (ACCV)
- **Journal**: None
- **Summary**: The goal of multi-modal learning is to use complimentary information on the relevant task provided by the multiple modalities to achieve reliable and robust performance. Recently, deep learning has led significant improvement in multi-modal learning by allowing for the information fusion in the intermediate feature levels. This paper addresses a problem of designing robust deep multi-modal learning architecture in the presence of imperfect modalities. We introduce deep fusion architecture for object detection which processes each modality using the separate convolutional neural network (CNN) and constructs the joint feature map by combining the intermediate features from the CNNs. In order to facilitate the robustness to the degraded modalities, we employ the gated information fusion (GIF) network which weights the contribution from each modality according to the input feature maps to be fused. The weights are determined through the convolutional layers followed by a sigmoid function and trained along with the information fusion network in an end-to-end fashion. Our experiments show that the proposed GIF network offers the additional architectural flexibility to achieve robust performance in handling some degraded modalities, and show a significant performance improvement based on Single Shot Detector (SSD) for KITTI dataset using the proposed fusion network and data augmentation schemes.



### Bench-Marking Information Extraction in Semi-Structured Historical Handwritten Records
- **Arxiv ID**: http://arxiv.org/abs/1807.06270v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1807.06270v1)
- **Published**: 2018-07-17 08:13:19+00:00
- **Updated**: 2018-07-17 08:13:19+00:00
- **Authors**: Animesh Prasad, Hervé Déjean, Jean-Luc Meunier, Max Weidemann, Johannes Michael, Gundram Leifert
- **Comment**: None
- **Journal**: None
- **Summary**: In this report, we present our findings from benchmarking experiments for information extraction on historical handwritten marriage records Esposalles from IEHHR - ICDAR 2017 robust reading competition. The information extraction is modeled as semantic labeling of the sequence across 2 set of labels. This can be achieved by sequentially or jointly applying handwritten text recognition (HTR) and named entity recognition (NER). We deploy a pipeline approach where first we use state-of-the-art HTR and use its output as input for NER. We show that given low resource setup and simple structure of the records, high performance of HTR ensures overall high performance. We explore the various configurations of conditional random fields and neural networks to benchmark NER on given certain noisy input. The best model on 10-fold cross-validation as well as blind test data uses n-gram features with bidirectional long short-term memory.



### Real-time on-board obstacle avoidance for UAVs based on embedded stereo vision
- **Arxiv ID**: http://arxiv.org/abs/1807.06271v2
- **DOI**: 10.5194/isprs-archives-XLII-1-363-2018
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06271v2)
- **Published**: 2018-07-17 08:17:10+00:00
- **Updated**: 2019-09-21 20:22:38+00:00
- **Authors**: Boitumelo Ruf, Sebastian Monka, Matthias Kollmann, Michael Grinberg
- **Comment**: Accepted in the International Archives of the Photogrammetry, Remote
  Sensing and Spatial Information Science
- **Journal**: Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLII-1,
  363-370, 2018
- **Summary**: In order to improve usability and safety, modern unmanned aerial vehicles (UAVs) are equipped with sensors to monitor the environment, such as laser-scanners and cameras. One important aspect in this monitoring process is to detect obstacles in the flight path in order to avoid collisions. Since a large number of consumer UAVs suffer from tight weight and power constraints, our work focuses on obstacle avoidance based on a lightweight stereo camera setup. We use disparity maps, which are computed from the camera images, to locate obstacles and to automatically steer the UAV around them. For disparity map computation we optimize the well-known semi-global matching (SGM) approach for the deployment on an embedded FPGA. The disparity maps are then converted into simpler representations, the so called U-/V-Maps, which are used for obstacle detection. Obstacle avoidance is based on a reactive approach which finds the shortest path around the obstacles as soon as they have a critical distance to the UAV. One of the fundamental goals of our work was the reduction of development costs by closing the gap between application development and hardware optimization. Hence, we aimed at using high-level synthesis (HLS) for porting our algorithms, which are written in C/C++, to the embedded FPGA. We evaluated our implementation of the disparity estimation on the KITTI Stereo 2015 benchmark. The integrity of the overall realtime reactive obstacle avoidance algorithm has been evaluated by using Hardware-in-the-Loop testing in conjunction with two flight simulators.



### Domain Adaptation for Deviating Acquisition Protocols in CNN-based Lesion Classification on Diffusion-Weighted MR Images
- **Arxiv ID**: http://arxiv.org/abs/1807.06277v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06277v1)
- **Published**: 2018-07-17 08:33:43+00:00
- **Updated**: 2018-07-17 08:33:43+00:00
- **Authors**: Jennifer Kamphenkel, Paul F. Jaeger, Sebastian Bickelhaupt, Frederik Bernd Laun, Wolfgang Lederer, Heidi Daniel, Tristan Anselm Kuder, Stefan Delorme, Heinz-Peter Schlemmer, Franziska Koenig, Klaus H. Maier-Hein
- **Comment**: accepted at the MICCAI workshop on Breast Image Analysis
- **Journal**: None
- **Summary**: End-to-end deep learning improves breast cancer classification on diffusion-weighted MR images (DWI) using a convolutional neural network (CNN) architecture. A limitation of CNN as opposed to previous model-based approaches is the dependence on specific DWI input channels used during training. However, in the context of large-scale application, methods agnostic towards heterogeneous inputs are desirable, due to the high deviation of scanning protocols between clinical sites. We propose model-based domain adaptation to overcome input dependencies and avoid re-training of networks at clinical sites by restoring training inputs from altered input channels given during deployment. We demonstrate the method's significant increase in classification performance and superiority over implicit domain adaptation provided by training-schemes operating on model-parameters instead of raw DWI images.



### Accuracy to Throughput Trade-offs for Reduced Precision Neural Networks on Reconfigurable Logic
- **Arxiv ID**: http://arxiv.org/abs/1807.10577v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10577v1)
- **Published**: 2018-07-17 08:44:00+00:00
- **Updated**: 2018-07-17 08:44:00+00:00
- **Authors**: Jiang Su, Nicholas J. Fraser, Giulio Gambardella, Michaela Blott, Gianluca Durelli, David B. Thomas, Philip Leong, Peter Y. K. Cheung
- **Comment**: Accepted by ARC 2018
- **Journal**: None
- **Summary**: Modern CNN are typically based on floating point linear algebra based implementations. Recently, reduced precision NN have been gaining popularity as they require significantly less memory and computational resources compared to floating point. This is particularly important in power constrained compute environments. However, in many cases a reduction in precision comes at a small cost to the accuracy of the resultant network. In this work, we investigate the accuracy-throughput trade-off for various parameter precision applied to different types of NN models. We firstly propose a quantization training strategy that allows reduced precision NN inference with a lower memory footprint and competitive model accuracy. Then, we quantitatively formulate the relationship between data representation and hardware efficiency. Our experiments finally provide insightful observation. For example, one of our tests show 32-bit floating point is more hardware efficient than 1-bit parameters to achieve 99% MNIST accuracy. In general, 2-bit and 4-bit fixed point parameters show better hardware trade-off on small-scale datasets like MNIST and CIFAR-10 while 4-bit provide the best trade-off in large-scale tasks like AlexNet on ImageNet dataset within our tested problem domain.



### DeepPhase: Surgical Phase Recognition in CATARACTS Videos
- **Arxiv ID**: http://arxiv.org/abs/1807.10565v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.10565v1)
- **Published**: 2018-07-17 08:50:03+00:00
- **Updated**: 2018-07-17 08:50:03+00:00
- **Authors**: Odysseas Zisimopoulos, Evangello Flouty, Imanol Luengo, Petros Giataganas, Jean Nehme, Andre Chow, Danail Stoyanov
- **Comment**: 8 pages, 3 figures, 1 table, MICCAI 2018
- **Journal**: None
- **Summary**: Automated surgical workflow analysis and understanding can assist surgeons to standardize procedures and enhance post-surgical assessment and indexing, as well as, interventional monitoring. Computer-assisted interventional (CAI) systems based on video can perform workflow estimation through surgical instruments' recognition while linking them to an ontology of procedural phases. In this work, we adopt a deep learning paradigm to detect surgical instruments in cataract surgery videos which in turn feed a surgical phase inference recurrent network that encodes temporal aspects of phase steps within the phase classification. Our models present comparable to state-of-the-art results for surgical tool detection and phase recognition with accuracies of 99 and 78% respectively.



### PointSeg: Real-Time Semantic Segmentation Based on 3D LiDAR Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1807.06288v8
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06288v8)
- **Published**: 2018-07-17 09:06:30+00:00
- **Updated**: 2018-09-25 07:41:47+00:00
- **Authors**: Yuan Wang, Tianyue Shi, Peng Yun, Lei Tai, Ming Liu
- **Comment**: Video link: https://youtu.be/b1BAbcjJ10s code link:
  https://github.com/ywangeq/PointSeg
- **Journal**: None
- **Summary**: In this paper, we propose PointSeg, a real-time end-to-end semantic segmentation method for road-objects based on spherical images. We take the spherical image, which is transformed from the 3D LiDAR point clouds, as input of the convolutional neural networks (CNNs) to predict the point-wise semantic map. To make PointSeg applicable on a mobile system, we build the model based on the light-weight network, SqueezeNet, with several improvements. It maintains a good balance between memory cost and prediction performance. Our model is trained on spherical images and label masks projected from the KITTI 3D object detection dataset. Experiments show that PointSeg can achieve competitive accuracy with 90fps on a single GPU 1080ti. which makes it quite compatible for autonomous driving applications.



### GeoDesc: Learning Local Descriptors by Integrating Geometry Constraints
- **Arxiv ID**: http://arxiv.org/abs/1807.06294v2
- **DOI**: 10.1007/978-3-030-01240-3_11
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06294v2)
- **Published**: 2018-07-17 09:31:34+00:00
- **Updated**: 2018-08-16 12:46:10+00:00
- **Authors**: Zixin Luo, Tianwei Shen, Lei Zhou, Siyu Zhu, Runze Zhang, Yao Yao, Tian Fang, Long Quan
- **Comment**: Accepted to ECCV'18
- **Journal**: None
- **Summary**: Learned local descriptors based on Convolutional Neural Networks (CNNs) have achieved significant improvements on patch-based benchmarks, whereas not having demonstrated strong generalization ability on recent benchmarks of image-based 3D reconstruction. In this paper, we mitigate this limitation by proposing a novel local descriptor learning approach that integrates geometry constraints from multi-view reconstructions, which benefits the learning process in terms of data generation, data sampling and loss computation. We refer to the proposed descriptor as GeoDesc, and demonstrate its superior performance on various large-scale benchmarks, and in particular show its great success on challenging reconstruction tasks. Moreover, we provide guidelines towards practical integration of learned descriptors in Structure-from-Motion (SfM) pipelines, showing the good trade-off that GeoDesc delivers to 3D reconstruction tasks between accuracy and efficiency.



### Saliency Map Estimation for Omni-Directional Image Considering Prior Distributions
- **Arxiv ID**: http://arxiv.org/abs/1807.06329v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06329v1)
- **Published**: 2018-07-17 10:43:35+00:00
- **Updated**: 2018-07-17 10:43:35+00:00
- **Authors**: Tatsuya Suzuki, Takao Yamanaka
- **Comment**: SMC2018
- **Journal**: None
- **Summary**: In recent years, the deep learning techniques have been applied to the estimation of saliency maps, which represent probability density functions of fixations when people look at the images. Although the methods of saliency-map estimation have been actively studied for 2-dimensional planer images, the methods for omni-directional images to be utilized in virtual environments had not been studied, until a competition of saliency-map estimation for the omni-directional images was held in ICME2017. In this paper, novel methods for estimating saliency maps for the omni-directional images are proposed considering the properties of prior distributions for fixations in the planar images and the omni-directional images.



### Magnetic Resonance Fingerprinting Reconstruction via Spatiotemporal Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.06356v2
- **DOI**: 10.1007/978-3-030-00129-2_5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06356v2)
- **Published**: 2018-07-17 11:33:51+00:00
- **Updated**: 2018-07-24 10:28:45+00:00
- **Authors**: Fabian Balsiger, Amaresha Shridhar Konar, Shivaprasad Chikop, Vimal Chandran, Olivier Scheidegger, Sairam Geethanath, Mauricio Reyes
- **Comment**: Accepted for Machine Learning for Medical Image Reconstruction
  (MLMIR) workshop at MICCAI 2018. The revision corrects Amaresha's last name
  and Section 2.1 (scanner type and flip angles)
- **Journal**: None
- **Summary**: Magnetic resonance fingerprinting (MRF) quantifies multiple nuclear magnetic resonance parameters in a single and fast acquisition. Standard MRF reconstructs parametric maps using dictionary matching, which lacks scalability due to computational inefficiency. We propose to perform MRF map reconstruction using a spatiotemporal convolutional neural network, which exploits the relationship between neighboring MRF signal evolutions to replace the dictionary matching. We evaluate our method on multiparametric brain scans and compare it to three recent MRF reconstruction approaches. Our method achieves state-of-the-art reconstruction accuracy and yields qualitatively more appealing maps compared to other reconstruction methods. In addition, the reconstruction time is significantly reduced compared to a dictionary-based approach.



### IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1807.06358v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.06358v2)
- **Published**: 2018-07-17 11:37:31+00:00
- **Updated**: 2018-10-27 13:46:18+00:00
- **Authors**: Huaibo Huang, Zhihang Li, Ran He, Zhenan Sun, Tieniu Tan
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel introspective variational autoencoder (IntroVAE) model for synthesizing high-resolution photographic images. IntroVAE is capable of self-evaluating the quality of its generated samples and improving itself accordingly. Its inference and generator models are jointly trained in an introspective way. On one hand, the generator is required to reconstruct the input images from the noisy outputs of the inference model as normal VAEs. On the other hand, the inference model is encouraged to classify between the generated and real samples while the generator tries to fool it as GANs. These two famous generative frameworks are integrated in a simple yet efficient single-stream architecture that can be trained in a single stage. IntroVAE preserves the advantages of VAEs, such as stable training and nice latent manifold. Unlike most other hybrid models of VAEs and GANs, IntroVAE requires no extra discriminators, because the inference model itself serves as a discriminator to distinguish between the generated and real samples. Experiments demonstrate that our method produces high-resolution photo-realistic images (e.g., CELEBA images at \(1024^{2}\)), which are comparable to or better than the state-of-the-art GANs.



### A Dense CNN approach for skin lesion classification
- **Arxiv ID**: http://arxiv.org/abs/1807.06416v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06416v2)
- **Published**: 2018-07-17 13:33:41+00:00
- **Updated**: 2018-07-26 10:42:50+00:00
- **Authors**: Pierluigi Carcagnì, Andrea Cuna, Cosimo Distante
- **Comment**: ISIC 2018
- **Journal**: None
- **Summary**: This article presents a Deep CNN, based on the DenseNet architecture jointly with a highly discriminating learning methodology, in order to classify seven kinds of skin lesions: Melanoma, Melanocytic nevus, Basal cell carcinoma, Actinic keratosis / Bowen's disease, Benign keratosis, Dermatofibroma, Vascular lesion. In particular a 61 layers DenseNet, pre-trained on IMAGENET dataset, has been fine-tuned on ISIC 2018 Task 3 Challenge Dataset exploiting a Center Loss function.



### Bridging the Accuracy Gap for 2-bit Quantized Neural Networks (QNN)
- **Arxiv ID**: http://arxiv.org/abs/1807.06964v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06964v1)
- **Published**: 2018-07-17 13:36:57+00:00
- **Updated**: 2018-07-17 13:36:57+00:00
- **Authors**: Jungwook Choi, Pierce I-Jen Chuang, Zhuo Wang, Swagath Venkataramani, Vijayalakshmi Srinivasan, Kailash Gopalakrishnan
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1805.06085
- **Journal**: None
- **Summary**: Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. In order to reduce this cost, several quantization schemes have gained attention recently with some focusing on weight quantization, and others focusing on quantizing activations. This paper proposes novel techniques that target weight and activation quantizations separately resulting in an overall quantized neural network (QNN). The activation quantization technique, PArameterized Clipping acTivation (PACT), uses an activation clipping parameter $\alpha$ that is optimized during training to find the right quantization scale. The weight quantization scheme, statistics-aware weight binning (SAWB), finds the optimal scaling factor that minimizes the quantization error based on the statistical characteristics of the distribution of weights without the need for an exhaustive search. The combination of PACT and SAWB results in a 2-bit QNN that achieves state-of-the-art classification accuracy (comparable to full precision networks) across a range of popular models and datasets.



### A Robust Color Edge Detection Algorithm Based on Quaternion Hardy Filter
- **Arxiv ID**: http://arxiv.org/abs/1807.10586v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1807.10586v3)
- **Published**: 2018-07-17 14:02:35+00:00
- **Updated**: 2021-01-02 12:45:39+00:00
- **Authors**: Wenshan Bi Dong Cheng Wankai Liu, Kit Ian Kou
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a robust filter called quaternion Hardy filter (QHF) for color image edge detection. The QHF can be capable of color edge feature enhancement and noise resistance. It is flexible to use QHF by selecting suitable parameters to handle different levels of noise. In particular, the quaternion analytic signal, which is an effective tool in color image processing, can also be produced by quaternion Hardy filtering with specific parameters. Based on the QHF and the improved Di Zenzo gradient operator, a novel color edge detection algorithm is proposed. Importantly, it can be efficiently implemented by using the fast discrete quaternion Fourier transform technique. From the experimental results, we conclude that the minimum PSNR improvement rate is 2.3% and minimum SSIM improvement rate is 30.2% on the Dataset 3. The experiments demonstrate that the proposed algorithm outperforms several widely used algorithms.



### Automatic Skin Lesion Segmentation Using Deep Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.06466v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06466v1)
- **Published**: 2018-07-17 14:24:37+00:00
- **Updated**: 2018-07-17 14:24:37+00:00
- **Authors**: Hongming Xu, Tae Hyun Hwang
- **Comment**: 4 pages, ISIC Challenge 2018
- **Journal**: None
- **Summary**: This paper summarizes our method and validation results for the ISIC Challenge 2018 - Skin Lesion Analysis Towards Melanoma Detection - Task 1: Lesion Segmentation



### BAM: Bottleneck Attention Module
- **Arxiv ID**: http://arxiv.org/abs/1807.06514v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06514v2)
- **Published**: 2018-07-17 15:55:31+00:00
- **Updated**: 2018-07-18 11:17:12+00:00
- **Authors**: Jongchan Park, Sanghyun Woo, Joon-Young Lee, In So Kweon
- **Comment**: Accepted to BMVC 2018 (oral)
- **Journal**: None
- **Summary**: Recent advances in deep neural networks have been developed via architecture search for stronger representational power. In this work, we focus on the effect of attention in general deep neural networks. We propose a simple and effective attention module, named Bottleneck Attention Module (BAM), that can be integrated with any feed-forward convolutional neural networks. Our module infers an attention map along two separate pathways, channel and spatial. We place our module at each bottleneck of models where the downsampling of feature maps occurs. Our module constructs a hierarchical attention at bottlenecks with a number of parameters and it is trainable in an end-to-end manner jointly with any feed-forward models. We validate our BAM through extensive experiments on CIFAR-100, ImageNet-1K, VOC 2007 and MS COCO benchmarks. Our experiments show consistent improvement in classification and detection performances with various models, demonstrating the wide applicability of BAM. The code and models will be publicly available.



### CBAM: Convolutional Block Attention Module
- **Arxiv ID**: http://arxiv.org/abs/1807.06521v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06521v2)
- **Published**: 2018-07-17 16:05:59+00:00
- **Updated**: 2018-07-18 11:20:08+00:00
- **Authors**: Sanghyun Woo, Jongchan Park, Joon-Young Lee, In So Kweon
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.



### An Acceleration Scheme for Memory Limited, Streaming PCA
- **Arxiv ID**: http://arxiv.org/abs/1807.06530v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.06530v1)
- **Published**: 2018-07-17 16:25:01+00:00
- **Updated**: 2018-07-17 16:25:01+00:00
- **Authors**: Salaheddin Alakkari, John Dingliana
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: In this paper, we propose an acceleration scheme for online memory-limited PCA methods. Our scheme converges to the first $k>1$ eigenvectors in a single data pass. We provide empirical convergence results of our scheme based on the spiked covariance model. Our scheme does not require any predefined parameters such as the eigengap and hence is well facilitated for streaming data scenarios. Furthermore, we apply our scheme to challenging time-varying systems where online PCA methods fail to converge. Specifically, we discuss a family of time-varying systems that are based on Molecular Dynamics simulations where batch PCA converges to the actual analytic solution of such systems.



### A framework for remote sensing images processing using deep learning technique
- **Arxiv ID**: http://arxiv.org/abs/1807.06535v2
- **DOI**: 10.1109/LGRS.2018.2867949
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06535v2)
- **Published**: 2018-07-17 16:32:08+00:00
- **Updated**: 2018-09-05 11:13:47+00:00
- **Authors**: Rémi Cresson
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning techniques are becoming increasingly important to solve a number of image processing tasks. Among common algorithms, Convolutional Neural Networks and Recurrent Neural Networks based systems achieve state of the art results on satellite and aerial imagery in many applications. While these approaches are subject to scientific interest, there is currently no operational and generic implementation available at user-level for the remote sensing community. In this paper, we presents a framework enabling the use of deep learning techniques with remote sensing images and geospatial data. Our solution takes roots in two extensively used open-source libraries, the remote sensing image processing library Orfeo ToolBox, and the high performance numerical computation library TensorFlow. It can apply deep nets without restriction on images size and is computationally efficient, regardless hardware configuration.



### PIMMS: Permutation Invariant Multi-Modal Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.06537v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06537v1)
- **Published**: 2018-07-17 16:32:38+00:00
- **Updated**: 2018-07-17 16:32:38+00:00
- **Authors**: Thomas Varsavsky, Zach Eaton-Rosen, Carole H. Sudre, Parashkev Nachev, M. Jorge Cardoso
- **Comment**: Accepted at the 4th Workshop on Deep Learning in Medical Image
  Analysis held at MICCAI2018
- **Journal**: None
- **Summary**: In a research context, image acquisition will often involve a pre-defined static protocol and the data will be of high quality. If we are to build applications that work in hospitals without significant operational changes in care delivery, algorithms should be designed to cope with the available data in the best possible way. In a clinical environment, imaging protocols are highly flexible, with MRI sequences commonly missing appropriate sequence labeling (e.g. T1, T2, FLAIR). To this end we introduce PIMMS, a Permutation Invariant Multi-Modal Segmentation technique that is able to perform inference over sets of MRI scans without using modality labels. We present results which show that our convolutional neural network can, in some settings, outperform a baseline model which utilizes modality labels, and achieve comparable performance otherwise.



### Cavity Filling: Pseudo-Feature Generation for Multi-Class Imbalanced Data Problems in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.06538v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.06538v6)
- **Published**: 2018-07-17 16:34:47+00:00
- **Updated**: 2019-10-13 14:47:42+00:00
- **Authors**: Tomohiko Konno, Michiaki Iwazume
- **Comment**: The slides are available at https://goo.gl/SPsSDh in English and at
  https://goo.gl/RFHYAa in Japanese. 9 pages
- **Journal**: None
- **Summary**: Herein, we generate pseudo-features based on the multivariate probability distributions obtained from the feature maps in layers of trained deep neural networks. Further, we augment the minor-class data based on these generated pseudo-features to overcome the imbalanced data problems. The proposed method, i.e., cavity filling, improves the deep learning capabilities in several problems because all the real-world data are observed to be imbalanced.



### Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try After Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.06540v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.06540v1)
- **Published**: 2018-07-17 16:35:51+00:00
- **Updated**: 2018-07-17 16:35:51+00:00
- **Authors**: Tomohiko Konno, Michiaki Iwazume
- **Comment**: 3 pages
- **Journal**: None
- **Summary**: We found an easy and quick post-learning method named "Icing on the Cake" to enhance a classification performance in deep learning. The method is that we train only the final classifier again after an ordinary training is done.



### A Deep Learning Driven Active Framework for Segmentation of Large 3D Shape Collections
- **Arxiv ID**: http://arxiv.org/abs/1807.06551v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1807.06551v1)
- **Published**: 2018-07-17 16:58:06+00:00
- **Updated**: 2018-07-17 16:58:06+00:00
- **Authors**: David George, Xianguha Xie, Yu-Kun Lai, Gary KL Tam
- **Comment**: 16 pages, 17 figures, 5 tables
- **Journal**: None
- **Summary**: High-level shape understanding and technique evaluation on large repositories of 3D shapes often benefit from additional information known about the shapes. One example of such information is the semantic segmentation of a shape into functional or meaningful parts. Generating accurate segmentations with meaningful segment boundaries is, however, a costly process, typically requiring large amounts of user time to achieve high quality results. In this paper we present an active learning framework for large dataset segmentation, which iteratively provides the user with new predictions by training new models based on already segmented shapes. Our proposed pipeline consists of three novel components. First, we a propose a fast and relatively accurate feature-based deep learning model to provide dataset-wide segmentation predictions. Second, we propose an information theory measure to estimate the prediction quality and for ordering subsequent fast and meaningful shape selection. Our experiments show that such suggestive ordering helps reduce users time and effort, produce high quality predictions, and construct a model that generalizes well. Finally, we provide effective segmentation refinement features to help the user quickly correct any incorrect predictions. We show that our framework is more accurate and in general more efficient than state-of-the-art, for massive dataset segmentation with while also providing consistent segment boundaries.



### Interpretable Latent Spaces for Learning from Demonstration
- **Arxiv ID**: http://arxiv.org/abs/1807.06583v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.06583v2)
- **Published**: 2018-07-17 17:56:09+00:00
- **Updated**: 2018-10-02 15:27:23+00:00
- **Authors**: Yordan Hristov, Alex Lascarides, Subramanian Ramamoorthy
- **Comment**: 12 pages, 6 figures, accepted at the Conference on Robot Learning
  (CoRL) 2018, Zurich, Switzerland
- **Journal**: None
- **Summary**: Effective human-robot interaction, such as in robot learning from human demonstration, requires the learning agent to be able to ground abstract concepts (such as those contained within instructions) in a corresponding high-dimensional sensory input stream from the world. Models such as deep neural networks, with high capacity through their large parameter spaces, can be used to compress the high-dimensional sensory data to lower dimensional representations. These low-dimensional representations facilitate symbol grounding, but may not guarantee that the representation would be human-interpretable. We propose a method which utilises the grouping of user-defined symbols and their corresponding sensory observations in order to align the learnt compressed latent representation with the semantic notions contained in the abstract labels. We demonstrate this through experiments with both simulated and real-world object data, showing that such alignment can be achieved in a process of physical symbol grounding.



### Deep Exemplar-based Colorization
- **Arxiv ID**: http://arxiv.org/abs/1807.06587v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06587v2)
- **Published**: 2018-07-17 17:59:01+00:00
- **Updated**: 2018-07-21 15:39:03+00:00
- **Authors**: Mingming He, Dongdong Chen, Jing Liao, Pedro V. Sander, Lu Yuan
- **Comment**: To Appear in Siggraph 2018
- **Journal**: None
- **Summary**: We propose the first deep learning approach for exemplar-based local colorization. Given a reference color image, our convolutional neural network directly maps a grayscale image to an output colorized image. Rather than using hand-crafted rules as in traditional exemplar-based methods, our end-to-end colorization network learns how to select, propagate, and predict colors from the large-scale data. The approach performs robustly and generalizes well even when using reference images that are unrelated to the input grayscale image. More importantly, as opposed to other learning-based colorization methods, our network allows the user to achieve customizable results by simply feeding different references. In order to further reduce manual effort in selecting the references, the system automatically recommends references with our proposed image retrieval algorithm, which considers both semantic and luminance information. The colorization can be performed fully automatically by simply picking the top reference suggestion. Our approach is validated through a user study and favorable quantitative comparisons to the-state-of-the-art methods. Furthermore, our approach can be naturally extended to video colorization. Our code and models will be freely available for public use.



### A Fast Segmentation-free Fully Automated Approach to White Matter Injury Detection in Preterm Infants
- **Arxiv ID**: http://arxiv.org/abs/1807.06604v1
- **DOI**: 10.1007/s11517-018-1829-9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06604v1)
- **Published**: 2018-07-17 18:00:26+00:00
- **Updated**: 2018-07-17 18:00:26+00:00
- **Authors**: Subhayan Mukherjee, Irene Cheng, Steven Miller, Jessie Guo, Vann Chau, Anup Basu
- **Comment**: None
- **Journal**: Medical and Biological Engineering and Computing (Springer), 2018
- **Summary**: White Matter Injury (WMI) is the most prevalent brain injury in the preterm neonate leading to developmental deficits. However, detecting WMI in Magnetic Resonance (MR) images of preterm neonate brains using traditional WM segmentation-based methods is difficult mainly due to lack of reliable preterm neonate brain atlases to guide segmentation. Hence, we propose a segmentation-free, fast, unsupervised, atlas-free WMI detection method. We detect the ventricles as blobs using a fast linear Maximally Stable Extremal Regions algorithm. A reference contour equidistant from the blobs and the brain-background boundary is used to identify tissue adjacent to the blobs. Assuming normal distribution of the gray-value intensity of this tissue, the outlier intensities in the entire brain region are identified as potential WMI candidates. Thereafter, false positives are discriminated using appropriate heuristics. Experiments using an expert-annotated dataset show that the proposed method runs 20 times faster than our earlier work which relied on time-consuming segmentation of the WM region, without compromising WMI detection accuracy.



### 3D Inception-based CNN with sMRI and MD-DTI data fusion for Alzheimer's Disease diagnostics
- **Arxiv ID**: http://arxiv.org/abs/1809.03972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1809.03972v1)
- **Published**: 2018-07-17 18:50:01+00:00
- **Updated**: 2018-07-17 18:50:01+00:00
- **Authors**: Alexander Khvostikov, Karim Aderghal, Andrey Krylov, Gwenaelle Catheline, Jenny Benois-Pineau
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1801.05968
- **Journal**: None
- **Summary**: In the last decade, computer-aided early diagnostics of Alzheimer's Disease (AD) and its prodromal form, Mild Cognitive Impairment (MCI), has been the subject of extensive research. Some recent studies have shown promising results in the AD and MCI determination using structural and functional Magnetic Resonance Imaging (sMRI, fMRI), Positron Emission Tomography (PET) and Diffusion Tensor Imaging (DTI) modalities. Furthermore, fusion of imaging modalities in a supervised machine learning framework has shown promising direction of research.   In this paper we first review major trends in automatic classification methods such as feature extraction based methods as well as deep learning approaches in medical image analysis applied to the field of Alzheimer's Disease diagnostics. Then we propose our own design of a 3D Inception-based Convolutional Neural Network (CNN) for Alzheimer's Disease diagnostics. The network is designed with an emphasis on the interior resource utilization and uses sMRI and DTI modalities fusion on hippocampal ROI. The comparison with the conventional AlexNet-based network using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset (http://adni.loni.usc.edu) demonstrates significantly better performance of the proposed 3D Inception-based CNN.



### SRN: Side-output Residual Network for Object Reflection Symmetry Detection and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1807.06621v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06621v2)
- **Published**: 2018-07-17 18:51:19+00:00
- **Updated**: 2019-03-15 15:47:16+00:00
- **Authors**: Wei Ke, Jie Chen, Jianbin Jiao, Guoying Zhao, Qixiang Ye
- **Comment**: reject by PAMI
- **Journal**: None
- **Summary**: In this paper, we establish a baseline for object reflection symmetry detection in complex backgrounds by presenting a new benchmark and an end-to-end deep learning approach, opening up a promising direction for symmetry detection in the wild. The new benchmark, Sym-PASCAL, spans challenges including object diversity, multi-objects, part-invisibility, and various complex backgrounds that are far beyond those in existing datasets. The end-to-end deep learning approach, referred to as a side-output residual network (SRN), leverages the output residual units (RUs) to fit the errors between the object ground-truth symmetry and the side-outputs of multiple stages. By cascading RUs in a deep-to-shallow manner, SRN exploits the 'flow' of errors among multiple stages to address the challenges of fitting complex output with limited convolutional layers, suppressing the complex backgrounds, and effectively matching object symmetry at different scales. SRN is further upgraded to a multi-task side-output residual network (MT-SRN) for joint symmetry and edge detection, demonstrating its generality to image-to-mask learning tasks. Experimental results validate both the challenging aspects of Sym-PASCAL benchmark related to real-world images and the state-of-the-art performance of the proposed SRN approach.



### A Framework for Moment Invariants
- **Arxiv ID**: http://arxiv.org/abs/1807.06644v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06644v1)
- **Published**: 2018-07-17 20:03:19+00:00
- **Updated**: 2018-07-17 20:03:19+00:00
- **Authors**: Omar Tahri
- **Comment**: None
- **Journal**: None
- **Summary**: For more than half a century, moments have attracted lot ot interest in the pattern recognition community.The moments of a distribution (an object) provide several of its characteristics as center of gravity, orientation, disparity, volume. Moments can be used to define invariant characteristics to some transformations that an object can undergo, commonly called moment invariants. This work provides a simple and systematic formalism to compute geometric moment invariants in n-dimensional space.



### Invariant Information Clustering for Unsupervised Image Classification and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.06653v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.06653v4)
- **Published**: 2018-07-17 20:17:29+00:00
- **Updated**: 2019-08-22 14:32:16+00:00
- **Authors**: Xu Ji, João F. Henriques, Andrea Vedaldi
- **Comment**: International Conference on Computer Vision 2019
- **Journal**: None
- **Summary**: We present a novel clustering objective that learns a neural network classifier from scratch, given only unlabelled data samples. The model discovers clusters that accurately match semantic classes, achieving state-of-the-art results in eight unsupervised clustering benchmarks spanning image classification and segmentation. These include STL10, an unsupervised variant of ImageNet, and CIFAR10, where we significantly beat the accuracy of our closest competitors by 6.6 and 9.5 absolute percentage points respectively. The method is not specialised to computer vision and operates on any paired dataset samples; in our experiments we use random transforms to obtain a pair from each image. The trained network directly outputs semantic labels, rather than high dimensional representations that need external processing to be usable for semantic clustering. The objective is simply to maximise mutual information between the class assignments of each pair. It is easy to implement and rigorously grounded in information theory, meaning we effortlessly avoid degenerate solutions that other clustering methods are susceptible to. In addition to the fully unsupervised mode, we also test two semi-supervised settings. The first achieves 88.8% accuracy on STL10 classification, setting a new global state-of-the-art over all existing methods (whether supervised, semi-supervised or unsupervised). The second shows robustness to 90% reductions in label coverage, of relevance to applications that wish to make use of small amounts of labels. github.com/xu-ji/IIC



### Accel: A Corrective Fusion Network for Efficient Semantic Segmentation on Video
- **Arxiv ID**: http://arxiv.org/abs/1807.06667v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.06667v4)
- **Published**: 2018-07-17 20:45:23+00:00
- **Updated**: 2019-07-05 20:36:08+00:00
- **Authors**: Samvit Jain, Xin Wang, Joseph Gonzalez
- **Comment**: CVPR 2019 (oral)
- **Journal**: None
- **Summary**: We present Accel, a novel semantic video segmentation system that achieves high accuracy at low inference cost by combining the predictions of two network branches: (1) a reference branch that extracts high-detail features on a reference keyframe, and warps these features forward using frame-to-frame optical flow estimates, and (2) an update branch that computes features of adjustable quality on the current frame, performing a temporal update at each video frame. The modularity of the update branch, where feature subnetworks of varying layer depth can be inserted (e.g. ResNet-18 to ResNet-101), enables operation over a new, state-of-the-art accuracy-throughput trade-off spectrum. Over this curve, Accel models achieve both higher accuracy and faster inference times than the closest comparable single-frame segmentation networks. In general, Accel significantly outperforms previous work on efficient semantic video segmentation, correcting warping-related error that compounds on datasets with complex dynamics. Accel is end-to-end trainable and highly modular: the reference network, the optical flow network, and the update network can each be selected independently, depending on application requirements, and then jointly fine-tuned. The result is a robust, general system for fast, high-accuracy semantic segmentation on video.



### Query-Conditioned Three-Player Adversarial Network for Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1807.06677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06677v1)
- **Published**: 2018-07-17 21:21:32+00:00
- **Updated**: 2018-07-17 21:21:32+00:00
- **Authors**: Yujia Zhang, Michael Kampffmeyer, Xiaodan Liang, Min Tan, Eric P. Xing
- **Comment**: 13 pages, 3 figures, BMVC 2018
- **Journal**: None
- **Summary**: Video summarization plays an important role in video understanding by selecting key frames/shots. Traditionally, it aims to find the most representative and diverse contents in a video as short summaries. Recently, a more generalized task, query-conditioned video summarization, has been introduced, which takes user queries into consideration to learn more user-oriented summaries. In this paper, we propose a query-conditioned three-player generative adversarial network to tackle this challenge. The generator learns the joint representation of the user query and the video content, and the discriminator takes three pairs of query-conditioned summaries as the input to discriminate the real summary from a generated and a random one. A three-player loss is introduced for joint training of the generator and the discriminator, which forces the generator to learn better summary results, and avoids the generation of random trivial summaries. Experiments on a recently proposed query-conditioned video summarization benchmark dataset show the efficiency and efficacy of our proposed method.



### Adaptive Neural Trees
- **Arxiv ID**: http://arxiv.org/abs/1807.06699v5
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.06699v5)
- **Published**: 2018-07-17 23:01:35+00:00
- **Updated**: 2019-06-09 19:32:34+00:00
- **Authors**: Ryutaro Tanno, Kai Arulkumaran, Daniel C. Alexander, Antonio Criminisi, Aditya Nori
- **Comment**: International Conference on Machine Learning 2019
- **Journal**: None
- **Summary**: Deep neural networks and decision trees operate on largely separate paradigms; typically, the former performs representation learning with pre-specified architectures, while the latter is characterised by learning hierarchies over pre-specified features with data-driven architectures. We unite the two via adaptive neural trees (ANTs) that incorporates representation learning into edges, routing functions and leaf nodes of a decision tree, along with a backpropagation-based training algorithm that adaptively grows the architecture from primitive modules (e.g., convolutional layers). We demonstrate that, whilst achieving competitive performance on classification and regression datasets, ANTs benefit from (i) lightweight inference via conditional computation, (ii) hierarchical separation of features useful to the task e.g. learning meaningful class associations, such as separating natural vs. man-made objects, and (iii) a mechanism to adapt the architecture to the size and complexity of the training dataset.



### A Modulation Module for Multi-task Learning with Applications in Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1807.06708v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06708v2)
- **Published**: 2018-07-17 23:33:24+00:00
- **Updated**: 2018-09-05 21:13:03+00:00
- **Authors**: Xiangyun Zhao, Haoxiang Li, Xiaohui Shen, Xiaodan Liang, Ying Wu
- **Comment**: To appear in ECCV 2018
- **Journal**: None
- **Summary**: Multi-task learning has been widely adopted in many computer vision tasks to improve overall computation efficiency or boost the performance of individual tasks, under the assumption that those tasks are correlated and complementary to each other. However, the relationships between the tasks are complicated in practice, especially when the number of involved tasks scales up. When two tasks are of weak relevance, they may compete or even distract each other during joint training of shared parameters, and as a consequence undermine the learning of all the tasks. This will raise destructive interference which decreases learning efficiency of shared parameters and lead to low quality loss local optimum w.r.t. shared parameters. To address the this problem, we propose a general modulation module, which can be inserted into any convolutional neural network architecture, to encourage the coupling and feature sharing of relevant tasks while disentangling the learning of irrelevant tasks with minor parameters addition. Equipped with this module, gradient directions from different tasks can be enforced to be consistent for those shared parameters, which benefits multi-task joint training. The module is end-to-end learnable without ad-hoc design for specific tasks, and can naturally handle many tasks at the same time. We apply our approach on two retrieval tasks, face retrieval on the CelebA dataset [1] and product retrieval on the UT-Zappos50K dataset [2, 3], and demonstrate its advantage over other multi-task learning methods in both accuracy and storage efficiency.



