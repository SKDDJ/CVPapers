# Arxiv Papers in cs.CV on 2018-07-04
### Selective Deep Convolutional Neural Network for Low Cost Distorted Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1807.01418v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1807.01418v2)
- **Published**: 2018-07-04 01:06:45+00:00
- **Updated**: 2019-02-14 00:06:21+00:00
- **Authors**: Minho Ha, Younghoon Byeon, Youngjoo Lee, Sunggu Lee
- **Comment**: The authors think that the results of this paper are insufficient.
  Therefore, we will improve the experiments and analyses, and then rewrite the
  paper
- **Journal**: None
- **Summary**: Deep convolutional neural networks have proven to be well suited for image classification applications. However, if there is distortion in the image, the classification accuracy can be significantly degraded, even with state-of-the-art neural networks. The accuracy cannot be significantly improved by simply training with distorted images. Instead, this paper proposes a multiple neural network topology referred to as a selective deep convolutional neural network. By modifying existing state-of-the-art neural networks in the proposed manner, it is shown that a similar level of classification accuracy can be achieved, but at a significantly lower cost. The cost reduction is obtained primarily through the use of fewer weight parameters. Using fewer weights reduces the number of multiply-accumulate operations and also reduces the energy required for data accesses. Finally, it is shown that the effectiveness of the proposed selective deep convolutional neural network can be further improved by combining it with previously proposed network cost reduction methods.



### Unbiased Image Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1807.01424v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01424v2)
- **Published**: 2018-07-04 01:47:03+00:00
- **Updated**: 2018-07-05 09:09:06+00:00
- **Authors**: Hyun-Chul Choi, Minseong Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Recent fast image style transferring methods use feed-forward neural networks to generate an output image of desired style strength from the input pair of a content and a target style image. In the existing methods, the image of intermediate style between the content and the target style is obtained by decoding a linearly interpolated feature in encoded feature space. However, there has been no work on analyzing the effectiveness of this kind of style strength interpolation so far. In this paper, we tackle the missing work on the in-depth analysis of style interpolation and propose a method that is more effective in controlling style strength. We interpret the training task of a style transfer network as a regression learning between the control parameter and output style strength. In this understanding, the existing methods are biased due to the fact that training is performed with one-sided data of full style strength (alpha = 1.0). Thus, this biased learning does not guarantee the generation of a desired intermediate style corresponding to the style control parameter between 0.0 and 1.0. To solve this problem of the biased network, we propose an unbiased learning technique which uses unbiased training data and corresponding unbiased loss for alpha = 0.0 to make the feed-forward networks to generate a zero-style image, i.e., content image when alpha = 0.0. Our experimental results verified that our unbiased learning method achieved the reconstruction of a content image with zero style strength, better regression specification between style control parameter and output style, and more stable style transfer that is insensitive to the weight of style loss without additive complexity in image generating process.



### Restructuring Batch Normalization to Accelerate CNN Training
- **Arxiv ID**: http://arxiv.org/abs/1807.01702v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1807.01702v2)
- **Published**: 2018-07-04 02:00:19+00:00
- **Updated**: 2019-03-01 08:27:20+00:00
- **Authors**: Wonkyung Jung, Daejin Jung, and Byeongho Kim, Sunjung Lee, Wonjong Rhee, Jung Ho Ahn
- **Comment**: 13 pages, 8 figures, to appear in SysML 2019, added ResNet-50 results
- **Journal**: None
- **Summary**: Batch Normalization (BN) has become a core design block of modern Convolutional Neural Networks (CNNs). A typical modern CNN has a large number of BN layers in its lean and deep architecture. BN requires mean and variance calculations over each mini-batch during training. Therefore, the existing memory access reduction techniques, such as fusing multiple CONV layers, are not effective for accelerating BN due to their inability to optimize mini-batch related calculations during training. To address this increasingly important problem, we propose to restructure BN layers by first splitting a BN layer into two sub-layers (fission) and then combining the first sub-layer with its preceding CONV layer and the second sub-layer with the following activation and CONV layers (fusion). The proposed solution can significantly reduce main-memory accesses while training the latest CNN models, and the experiments on a chip multiprocessor show that the proposed BN restructuring can improve the performance of DenseNet-121 by 25.7%.



### SGAD: Soft-Guided Adaptively-Dropped Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1807.01430v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.01430v1)
- **Published**: 2018-07-04 02:23:10+00:00
- **Updated**: 2018-07-04 02:23:10+00:00
- **Authors**: Zhisheng Wang, Fangxuan Sun, Jun Lin, Zhongfeng Wang, Bo Yuan
- **Comment**: 9 pages, 4 figures; the first two authors contributed equally
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have been proven to have many redundancies. Hence, many efforts have been made to compress DNNs. However, the existing model compression methods treat all the input samples equally while ignoring the fact that the difficulties of various input samples being correctly classified are different. To address this problem, DNNs with adaptive dropping mechanism are well explored in this work. To inform the DNNs how difficult the input samples can be classified, a guideline that contains the information of input samples is introduced to improve the performance. Based on the developed guideline and adaptive dropping mechanism, an innovative soft-guided adaptively-dropped (SGAD) neural network is proposed in this paper. Compared with the 32 layers residual neural networks, the presented SGAD can reduce the FLOPs by 77% with less than 1% drop in accuracy on CIFAR-10.



### Small-scale Pedestrian Detection Based on Somatic Topology Localization and Temporal Feature Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1807.01438v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01438v1)
- **Published**: 2018-07-04 03:33:29+00:00
- **Updated**: 2018-07-04 03:33:29+00:00
- **Authors**: Tao Song, Leiyu Sun, Di Xie, Haiming Sun, Shiliang Pu
- **Comment**: Accepted by ECCV18
- **Journal**: None
- **Summary**: A critical issue in pedestrian detection is to detect small-scale objects that will introduce feeble contrast and motion blur in images and videos, which in our opinion should partially resort to deep-rooted annotation bias. Motivated by this, we propose a novel method integrated with somatic topological line localization (TLL) and temporal feature aggregation for detecting multi-scale pedestrians, which works particularly well with small-scale pedestrians that are relatively far from the camera. Moreover, a post-processing scheme based on Markov Random Field (MRF) is introduced to eliminate ambiguities in occlusion cases. Applying with these methodologies comprehensively, we achieve best detection performance on Caltech benchmark and improve performance of small-scale objects significantly (miss rate decreases from 74.53% to 60.79%). Beyond this, we also achieve competitive performance on CityPersons dataset and show the existence of annotation bias in KITTI dataset.



### Multi-task Mid-level Feature Alignment Network for Unsupervised Cross-Dataset Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1807.01440v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01440v2)
- **Published**: 2018-07-04 03:43:20+00:00
- **Updated**: 2018-07-11 08:19:14+00:00
- **Authors**: Shan Lin, Haoliang Li, Chang-Tsun Li, Alex Chichung Kot
- **Comment**: Accepted by BMVC 2018 as Spotlight
- **Journal**: None
- **Summary**: Most existing person re-identification (Re-ID) approaches follow a supervised learning framework, in which a large number of labelled matching pairs are required for training. Such a setting severely limits their scalability in real-world applications where no labelled samples are available during the training phase. To overcome this limitation, we develop a novel unsupervised Multi-task Mid-level Feature Alignment (MMFA) network for the unsupervised cross-dataset person re-identification task. Under the assumption that the source and target datasets share the same set of mid-level semantic attributes, our proposed model can be jointly optimised under the person's identity classification and the attribute learning task with a cross-dataset mid-level feature alignment regularisation term. In this way, the learned feature representation can be better generalised from one dataset to another which further improve the person re-identification accuracy. Experimental results on four benchmark datasets demonstrate that our proposed method outperforms the state-of-the-art baselines.



### Understanding Visual Ads by Aligning Symbols and Objects using Co-Attention
- **Arxiv ID**: http://arxiv.org/abs/1807.01448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01448v1)
- **Published**: 2018-07-04 04:50:09+00:00
- **Updated**: 2018-07-04 04:50:09+00:00
- **Authors**: Karuna Ahuja, Karan Sikka, Anirban Roy, Ajay Divakaran
- **Comment**: Accepted at CVPR 2018 workshop- Towards Automatic Understanding of
  Visual Advertisements
- **Journal**: None
- **Summary**: We tackle the problem of understanding visual ads where given an ad image, our goal is to rank appropriate human generated statements describing the purpose of the ad. This problem is generally addressed by jointly embedding images and candidate statements to establish correspondence. Decoding a visual ad requires inference of both semantic and symbolic nuances referenced in an image and prior methods may fail to capture such associations especially with weakly annotated symbols. In order to create better embeddings, we leverage an attention mechanism to associate image proposals with symbols and thus effectively aggregate information from aligned multimodal representations. We propose a multihop co-attention mechanism that iteratively refines the attention map to ensure accurate attention estimation. Our attention based embedding model is learned end-to-end guided by a max-margin loss function. We show that our model outperforms other baselines on the benchmark Ad dataset and also show qualitative results to highlight the advantages of using multihop co-attention.



### Semantic Instance Meets Salient Object: Study on Video Semantic Salient Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.01452v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01452v3)
- **Published**: 2018-07-04 05:30:52+00:00
- **Updated**: 2018-11-22 06:11:28+00:00
- **Authors**: Trung-Nghia Le, Akihiro Sugimoto
- **Comment**: accepted in WACV 2019
- **Journal**: None
- **Summary**: Focusing on only semantic instances that only salient in a scene gains more benefits for robot navigation and self-driving cars than looking at all objects in the whole scene. This paper pushes the envelope on salient regions in a video to decompose them into semantically meaningful components, namely, semantic salient instances. We provide the baseline for the new task of video semantic salient instance segmentation (VSSIS), that is, Semantic Instance - Salient Object (SISO) framework. The SISO framework is simple yet efficient, leveraging advantages of two different segmentation tasks, i.e. semantic instance segmentation and salient object segmentation to eventually fuse them for the final result. In SISO, we introduce a sequential fusion by looking at overlapping pixels between semantic instances and salient regions to have non-overlapping instances one by one. We also introduce a recurrent instance propagation to refine the shapes and semantic meanings of instances, and an identity tracking to maintain both the identity and the semantic meaning of instances over the entire video. Experimental results demonstrated the effectiveness of our SISO baseline, which can handle occlusions in videos. In addition, to tackle the task of VSSIS, we augment the DAVIS-2017 benchmark dataset by assigning semantic ground-truth for salient instance labels, obtaining SEmantic Salient Instance Video (SESIV) dataset. Our SESIV dataset consists of 84 high-quality video sequences with pixel-wisely per-frame ground-truth labels.



### Transfer Learning From Synthetic To Real Images Using Variational Autoencoders For Precise Position Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.01990v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01990v1)
- **Published**: 2018-07-04 05:45:48+00:00
- **Updated**: 2018-07-04 05:45:48+00:00
- **Authors**: Tadanobu Inoue, Subhajit Chaudhury, Giovanni De Magistris, Sakyasingha Dasgupta
- **Comment**: Copyright 2018 IEEE - Accepted at ICIP 2018, Athens, Greece, October
  7-10, 2018. Video: https://youtu.be/30vji7nJibA. arXiv admin note: text
  overlap with arXiv:1709.06762
- **Journal**: None
- **Summary**: Capturing and labeling camera images in the real world is an expensive task, whereas synthesizing labeled images in a simulation environment is easy for collecting large-scale image data. However, learning from only synthetic images may not achieve the desired performance in the real world due to a gap between synthetic and real images. We propose a method that transfers learned detection of an object position from a simulation environment to the real world. This method uses only a significantly limited dataset of real images while leveraging a large dataset of synthetic images using variational autoencoders. Additionally, the proposed method consistently performed well in different lighting conditions, in the presence of other distractor objects, and on different backgrounds. Experimental results showed that it achieved accuracy of 1.5mm to 3.5mm on average. Furthermore, we showed how the method can be used in a real-world scenario like a "pick-and-place" robotic task.



### Discriminative Feature Learning with Foreground Attention for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1807.01455v2
- **DOI**: 10.1109/TIP.2019.2908065
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01455v2)
- **Published**: 2018-07-04 06:18:17+00:00
- **Updated**: 2019-03-30 21:34:46+00:00
- **Authors**: Sanping Zhou, Jinjun Wang, Deyu Meng, Yudong Liang, Yihong Gong, Nanning Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of person re-identification (Re-ID) has been seriously effected by the large cross-view appearance variations caused by mutual occlusions and background clutters. Hence learning a feature representation that can adaptively emphasize the foreground persons becomes very critical to solve the person Re-ID problem. In this paper, we propose a simple yet effective foreground attentive neural network (FANN) to learn a discriminative feature representation for person Re-ID, which can adaptively enhance the positive side of foreground and weaken the negative side of background. Specifically, a novel foreground attentive subnetwork is designed to drive the network's attention, in which a decoder network is used to reconstruct the binary mask by using a novel local regression loss function, and an encoder network is regularized by the decoder network to focus its attention on the foreground persons. The resulting feature maps of encoder network are further fed into the body part subnetwork and feature fusion subnetwork to learn discriminative features. Besides, a novel symmetric triplet loss function is introduced to supervise feature learning, in which the intra-class distance is minimized and the inter-class distance is maximized in each triplet unit, simultaneously. Training our FANN in a multi-task learning framework, a discriminative feature representation can be learned to find out the matched reference to each probe among various candidates in the gallery. Extensive experimental results on several public benchmark datasets are evaluated, which have shown clear improvements of our method over the state-of-the-art approaches.



### Deep Saliency Hashing
- **Arxiv ID**: http://arxiv.org/abs/1807.01459v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01459v2)
- **Published**: 2018-07-04 06:31:13+00:00
- **Updated**: 2019-02-01 05:30:49+00:00
- **Authors**: Sheng Jin, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, Lei Zhang, Xiansheng Hua
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, hashing methods have been proved to be effective and efficient for the large-scale Web media search. However, the existing general hashing methods have limited discriminative power for describing fine-grained objects that share similar overall appearance but have subtle difference. To solve this problem, we for the first time introduce the attention mechanism to the learning of fine-grained hashing codes. Specifically, we propose a novel deep hashing model, named deep saliency hashing (DSaH), which automatically mines salient regions and learns semantic-preserving hashing codes simultaneously. DSaH is a two-step end-to-end model consisting of an attention network and a hashing network. Our loss function contains three basic components, including the semantic loss, the saliency loss, and the quantization loss. As the core of DSaH, the saliency loss guides the attention network to mine discriminative regions from pairs of images. We conduct extensive experiments on both fine-grained and general retrieval datasets for performance evaluation. Experimental results on fine-grained datasets, including Oxford Flowers-17, Stanford Dogs-120, and CUB Bird demonstrate that our DSaH performs the best for fine-grained retrieval task and beats the strongest competitor (DTQ) by approximately 10% on both Stanford Dogs-120 and CUB Bird. DSaH is also comparable to several state-of-the-art hashing methods on general datasets, including CIFAR-10 and NUS-WIDE.



### Video Frame Interpolation by Plug-and-Play Deep Locally Linear Embedding
- **Arxiv ID**: http://arxiv.org/abs/1807.01462v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1807.01462v1)
- **Published**: 2018-07-04 06:49:03+00:00
- **Updated**: 2018-07-04 06:49:03+00:00
- **Authors**: Anh-Duc Nguyen, Woojae Kim, Jongyoo Kim, Sanghoon Lee
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: We propose a generative framework which takes on the video frame interpolation problem. Our framework, which we call Deep Locally Linear Embedding (DeepLLE), is powered by a deep convolutional neural network (CNN) while it can be used instantly like conventional models. DeepLLE fits an auto-encoding CNN to a set of several consecutive frames and embeds a linearity constraint on the latent codes so that new frames can be generated by interpolating new latent codes. Different from the current deep learning paradigm which requires training on large datasets, DeepLLE works in a plug-and-play and unsupervised manner, and is able to generate an arbitrary number of frames. Thorough experiments demonstrate that without bells and whistles, our method is highly competitive among current state-of-the-art models.



### Diversity in Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.01477v2
- **DOI**: 10.1109/ACCESS.2019.2917620
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01477v2)
- **Published**: 2018-07-04 08:25:17+00:00
- **Updated**: 2019-05-15 13:41:11+00:00
- **Authors**: Zhiqiang Gong, Ping Zhong, Weidong Hu
- **Comment**: Accepted by IEEE Access
- **Journal**: IEEE Access,2019
- **Summary**: Machine learning methods have achieved good performance and been widely applied in various real-world applications. They can learn the model adaptively and be better fit for special requirements of different tasks. Generally, a good machine learning system is composed of plentiful training data, a good model training process, and an accurate inference. Many factors can affect the performance of the machine learning process, among which the diversity of the machine learning process is an important one. The diversity can help each procedure to guarantee a total good machine learning: diversity of the training data ensures that the training data can provide more discriminative information for the model, diversity of the learned model (diversity in parameters of each model or diversity among different base models) makes each parameter/model capture unique or complement information and the diversity in inference can provide multiple choices each of which corresponds to a specific plausible local optimal result. Even though the diversity plays an important role in machine learning process, there is no systematical analysis of the diversification in machine learning system. In this paper, we systematically summarize the methods to make data diversification, model diversification, and inference diversification in the machine learning process, respectively. In addition, the typical applications where the diversity technology improved the machine learning performance have been surveyed, including the remote sensing imaging tasks, machine translation, camera relocalization, image segmentation, object detection, topic modeling, and others. Finally, we discuss some challenges of the diversity technology in machine learning and point out some directions in future work.



### Uncorrelated Feature Encoding for Faster Image Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1807.01493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01493v1)
- **Published**: 2018-07-04 09:21:19+00:00
- **Updated**: 2018-07-04 09:21:19+00:00
- **Authors**: Minseong Kim, Jongju Shin, Myung-Cheol Roh, Hyun-Chul Choi
- **Comment**: None
- **Journal**: None
- **Summary**: Recent fast style transfer methods use a pre-trained convolutional neural network as a feature encoder and a perceptual loss network. Although the pre-trained network is used to generate responses of receptive fields effective for representing style and content of image, it is not optimized for image style transfer but rather for image classification. Furthermore, it also requires a time-consuming and correlation-considering feature alignment process for image style transfer because of its inter-channel correlation. In this paper, we propose an end-to-end learning method which optimizes an encoder/decoder network for the purpose of style transfer as well as relieves the feature alignment complexity from considering inter-channel correlation. We used uncorrelation loss, i.e., the total correlation coefficient between the responses of different encoder channels, with style and content losses for training style transfer network. This makes the encoder network to be trained to generate inter-channel uncorrelated features and to be optimized for the task of image style transfer which maintained the quality of image style only with a light-weighted and correlation-unaware feature alignment process. Moreover, our method drastically reduced redundant channels of the encoded feature and this resulted in the efficient size of structure of network and faster forward processing speed. Our method can also be applied to cascade network scheme for multiple scaled style transferring and allows user-control of style strength by using a content-style trade-off parameter.



### Deep Autoencoder for Combined Human Pose Estimation and body Model Upscaling
- **Arxiv ID**: http://arxiv.org/abs/1807.01511v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01511v1)
- **Published**: 2018-07-04 10:41:24+00:00
- **Updated**: 2018-07-04 10:41:24+00:00
- **Authors**: Matthew Trumble, Andrew Gilbert, Adrian Hilton, John Collomosse
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for simultaneously estimating 3D human pose and body shape from a sparse set of wide-baseline camera views. We train a symmetric convolutional autoencoder with a dual loss that enforces learning of a latent representation that encodes skeletal joint positions, and at the same time learns a deep representation of volumetric body shape. We harness the latter to up-scale input volumetric data by a factor of $4 \times$, whilst recovering a 3D estimate of joint positions with equal or greater accuracy than the state of the art. Inference runs in real-time (25 fps) and has the potential for passive human behaviour monitoring where there is a requirement for high fidelity estimation of human body shape and pose.



### An Integration of Bottom-up and Top-Down Salient Cues on RGB-D Data: Saliency from Objectness vs. Non-Objectness
- **Arxiv ID**: http://arxiv.org/abs/1807.01532v1
- **DOI**: 10.1007/s11760-017-1159-7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01532v1)
- **Published**: 2018-07-04 12:10:02+00:00
- **Updated**: 2018-07-04 12:10:02+00:00
- **Authors**: Nevrez Imamoglu, Wataru Shimoda, Chi Zhang, Yuming Fang, Asako Kanezaki, Keiji Yanai, Yoshifumi Nishida
- **Comment**: 9 pages, 3 figures, 3 tables, This work includes the accepted version
  content of the paper published in journal of Signal Image and Video
  Processing (SIViP, Springer), Vol. 12, Issue 2, pp 307-314, Feb 2018 (DOI:
  https://doi.org/10.1007/s11760-017-1159-7)
- **Journal**: Nevrez Imamoglu and Wataru Shimoda and Chi Zhang and Yuming Fang
  and Asako Kanezaki and Keiji Yanai and Yoshifumi Nishida, Signal Image and
  Video Processing (SIViP), Springer, Vol. 12, Issue 2, pp 307-314, Feb 2018
- **Summary**: Bottom-up and top-down visual cues are two types of information that helps the visual saliency models. These salient cues can be from spatial distributions of the features (space-based saliency) or contextual / task-dependent features (object based saliency). Saliency models generally incorporate salient cues either in bottom-up or top-down norm separately. In this work, we combine bottom-up and top-down cues from both space and object based salient features on RGB-D data. In addition, we also investigated the ability of various pre-trained convolutional neural networks for extracting top-down saliency on color images based on the object dependent feature activation. We demonstrate that combining salient features from color and dept through bottom-up and top-down methods gives significant improvement on the salient object detection with space based and object based salient cues. RGB-D saliency integration framework yields promising results compared with the several state-of-the-art-models.



### TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes
- **Arxiv ID**: http://arxiv.org/abs/1807.01544v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01544v2)
- **Published**: 2018-07-04 12:37:07+00:00
- **Updated**: 2020-08-18 00:54:35+00:00
- **Authors**: Shangbang Long, Jiaqiang Ruan, Wenjie Zhang, Xin He, Wenhao Wu, Cong Yao
- **Comment**: 17 pages, accepted to ECCV2018
- **Journal**: None
- **Summary**: Driven by deep neural networks and large scale datasets, scene text detection methods have progressed substantially over the past years, continuously refreshing the performance records on various standard benchmarks. However, limited by the representations (axis-aligned rectangles, rotated rectangles or quadrangles) adopted to describe text, existing methods may fall short when dealing with much more free-form text instances, such as curved text, which are actually very common in real-world scenarios. To tackle this problem, we propose a more flexible representation for scene text, termed as TextSnake, which is able to effectively represent text instances in horizontal, oriented and curved forms. In TextSnake, a text instance is described as a sequence of ordered, overlapping disks centered at symmetric axes, each of which is associated with potentially variable radius and orientation. Such geometry attributes are estimated via a Fully Convolutional Network (FCN) model. In experiments, the text detector based on TextSnake achieves state-of-the-art or comparable performance on Total-Text and SCUT-CTW1500, the two newly published benchmarks with special emphasis on curved text in natural images, as well as the widely-used datasets ICDAR 2015 and MSRA-TD500. Specifically, TextSnake outperforms the baseline on Total-Text by more than 40% in F-measure.



### Fuzzy Logic Interpretation of Quadratic Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.03215v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.03215v3)
- **Published**: 2018-07-04 12:45:25+00:00
- **Updated**: 2019-06-11 00:21:40+00:00
- **Authors**: Fenglei Fan, Ge Wang
- **Comment**: 10 pages and 9 figures
- **Journal**: None
- **Summary**: Over past several years, deep learning has achieved huge successes in various applications. However, such a data-driven approach is often criticized for lack of interpretability. Recently, we proposed artificial quadratic neural networks consisting of second-order neurons in potentially many layers. In each second-order neuron, a quadratic function is used in the place of the inner product in a traditional neuron, and then undergoes a nonlinear activation. With a single second-order neuron, any fuzzy logic operation, such as XOR, can be implemented. In this sense, any deep network constructed with quadratic neurons can be interpreted as a deep fuzzy logic system. Since traditional neural networks and second-order counterparts can represent each other and fuzzy logic operations are naturally implemented in second-order neural networks, it is plausible to explain how a deep neural network works with a second-order network as the system model. In this paper, we generalize and categorize fuzzy logic operations implementable with individual second-order neurons, and then perform statistical/information theoretic analyses of exemplary quadratic neural networks.



### The SEN1-2 Dataset for Deep Learning in SAR-Optical Data Fusion
- **Arxiv ID**: http://arxiv.org/abs/1807.01569v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01569v1)
- **Published**: 2018-07-04 13:29:14+00:00
- **Updated**: 2018-07-04 13:29:14+00:00
- **Authors**: Michael Schmitt, Lloyd Haydn Hughes, Xiao Xiang Zhu
- **Comment**: accepted for publication in the ISPRS Annals of the Photogrammetry,
  Remote Sensing and Spatial Information Sciences (online from October 2018)
- **Journal**: None
- **Summary**: While deep learning techniques have an increasing impact on many technical fields, gathering sufficient amounts of training data is a challenging problem in remote sensing. In particular, this holds for applications involving data from multiple sensors with heterogeneous characteristics. One example for that is the fusion of synthetic aperture radar (SAR) data and optical imagery. With this paper, we publish the SEN1-2 dataset to foster deep learning research in SAR-optical data fusion. SEN1-2 comprises 282,384 pairs of corresponding image patches, collected from across the globe and throughout all meteorological seasons. Besides a detailed description of the dataset, we show exemplary results for several possible applications, such as SAR image colorization, SAR-optical image matching, and creation of artificial optical images from SAR input data. Since SEN1-2 is the first large open dataset of this kind, we believe it will support further developments in the field of deep learning for remote sensing as well as multi-sensor data fusion.



### VideoKifu, or the automatic transcription of a Go game
- **Arxiv ID**: http://arxiv.org/abs/1807.01577v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4.8; I.5.5
- **Links**: [PDF](http://arxiv.org/pdf/1807.01577v1)
- **Published**: 2018-07-04 13:52:10+00:00
- **Updated**: 2018-07-04 13:52:10+00:00
- **Authors**: Mario Corsolini, Andrea Carta
- **Comment**: 14 pages, 6 figures. Accepted for the "International Conference on
  Research in Mind Games" (August 7-8, 2018) at the EGC in Pisa, Italy.
  Datasets available from http://www.oipaz.net/VideoKifu.html
- **Journal**: None
- **Summary**: In two previous papers [arXiv:1508.03269, arXiv:1701.05419] we described the techniques we employed for reconstructing the whole move sequence of a Go game. That task was at first accomplished by means of a series of photographs, manually shot, as explained during the scientific conference held within the LIX European Go Congress (Liberec, CZ). The photographs were subsequently replaced by a possibly unattended video live stream (provided by webcams, videocameras, smartphones and so on) or, were the live stream not available, by means of a pre-recorded video of the game itself, on condition that the goban and the stones were clearly visible more often than not. As we hinted in the latter paper, in the last two years we have improved both the algorithms employed for reconstructing the grid and detecting the stones, making extensive usage of the multicore capabilities offered by modern CPUs. Those capabilities prompted us to develop some asynchronous routines, capable of double-checking the position of the grid and the number and colour of any stone previously detected, in order to get rid of minor errors possibly occurred during the main analysis, and that may pass undetected especially in the course of an unattended live streaming. Those routines will be described in details, as they address some problems that are of general interest when reconstructing the move sequence, for example what to do when large movements of the whole goban occur (deliberate or not) and how to deal with captures of dead stones $-$ that could be wrongly detected and recorded as "fresh" moves if not promptly removed.



### Sensors, SLAM and Long-term Autonomy: A Review
- **Arxiv ID**: http://arxiv.org/abs/1807.01605v1
- **DOI**: 10.1109/AHS.2018.8541483
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.01605v1)
- **Published**: 2018-07-04 14:16:23+00:00
- **Updated**: 2018-07-04 14:16:23+00:00
- **Authors**: Mubariz Zaffar, Shoaib Ehsan, Rustam Stolkin, Klaus McDonald Maier
- **Comment**: 6 pages, 7 figures
- **Journal**: None
- **Summary**: Simultaneous Localization and Mapping, commonly known as SLAM, has been an active research area in the field of Robotics over the past three decades. For solving the SLAM problem, every robot is equipped with either a single sensor or a combination of similar/different sensors. This paper attempts to review, discuss, evaluate and compare these sensors. Keeping an eye on future, this paper also assesses the characteristics of these sensors against factors critical to the long-term autonomy challenge.



### Neonatal Pain Expression Recognition Using Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.01631v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01631v1)
- **Published**: 2018-07-04 15:15:05+00:00
- **Updated**: 2018-07-04 15:15:05+00:00
- **Authors**: Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, Yu Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Transfer learning using pre-trained Convolutional Neural Networks (CNNs) has been successfully applied to images for different classification tasks. In this paper, we propose a new pipeline for pain expression recognition in neonates using transfer learning. Specifically, we propose to exploit a pre-trained CNN that was originally trained on a relatively similar dataset for face recognition (VGG Face) as well as CNNs that were pre-trained on a relatively different dataset for image classification (iVGG F,M, and S) to extract deep features from neonates' faces. In the final stage, several supervised machine learning classifiers are trained to classify neonates' facial expression into pain or no pain expression. The proposed pipeline achieved, on a testing dataset, 0.841 AUC and 90.34 accuracy, which is approx. 7 higher than the accuracy of handcrafted traditional features. We also propose to combine deep features with traditional features and hypothesize that the mixed features would improve pain classification performance. Combining deep features with traditional features achieved 92.71 accuracy and 0.948 AUC. These results show that transfer learning, which is a faster and more practical option than training CNN from the scratch, can be used to extract useful features for pain expression recognition in neonates. It also shows that combining deep features with traditional handcrafted features is a good practice to improve the performance of pain expression recognition and possibly the performance of similar applications.



### Learning models for visual 3D localization with implicit mapping
- **Arxiv ID**: http://arxiv.org/abs/1807.03149v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.03149v2)
- **Published**: 2018-07-04 15:50:58+00:00
- **Updated**: 2018-12-12 11:26:23+00:00
- **Authors**: Dan Rosenbaum, Frederic Besse, Fabio Viola, Danilo J. Rezende, S. M. Ali Eslami
- **Comment**: None
- **Journal**: None
- **Summary**: We consider learning based methods for visual localization that do not require the construction of explicit maps in the form of point clouds or voxels. The goal is to learn an implicit representation of the environment at a higher, more abstract level. We propose to use a generative approach based on Generative Query Networks (GQNs, Eslami et al. 2018), asking the following questions: 1) Can GQN capture more complex scenes than those it was originally demonstrated on? 2) Can GQN be used for localization in those scenes? To study this approach we consider procedurally generated Minecraft worlds, for which we can generate images of complex 3D scenes along with camera pose coordinates. We first show that GQNs, enhanced with a novel attention mechanism can capture the structure of 3D scenes in Minecraft, as evidenced by their samples. We then apply the models to the localization problem, comparing the results to a discriminative baseline, and comparing the ways each approach captures the task uncertainty.



### Latent Space Autoregression for Novelty Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.01653v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01653v2)
- **Published**: 2018-07-04 16:06:39+00:00
- **Updated**: 2019-03-06 09:53:59+00:00
- **Authors**: Davide Abati, Angelo Porrello, Simone Calderara, Rita Cucchiara
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Novelty detection is commonly referred to as the discrimination of observations that do not conform to a learned model of regularity. Despite its importance in different application settings, designing a novelty detector is utterly complex due to the unpredictable nature of novelties and its inaccessibility during the training procedure, factors which expose the unsupervised nature of the problem. In our proposal, we design a general framework where we equip a deep autoencoder with a parametric density estimator that learns the probability distribution underlying its latent representations through an autoregressive procedure. We show that a maximum likelihood objective, optimized in conjunction with the reconstruction of normal samples, effectively acts as a regularizer for the task at hand, by minimizing the differential entropy of the distribution spanned by latent vectors. In addition to providing a very general formulation, extensive experiments of our model on publicly available datasets deliver on-par or superior performances if compared to state-of-the-art methods in one-class and video anomaly detection settings. Differently from prior works, our proposal does not make any assumption about the nature of the novelties, making our work readily applicable to diverse contexts.



### Encoding Spatial Relations from Natural Language
- **Arxiv ID**: http://arxiv.org/abs/1807.01670v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.01670v2)
- **Published**: 2018-07-04 16:38:49+00:00
- **Updated**: 2018-07-05 10:03:23+00:00
- **Authors**: Tiago Ramalho, Tomáš Kočiský, Frederic Besse, S. M. Ali Eslami, Gábor Melis, Fabio Viola, Phil Blunsom, Karl Moritz Hermann
- **Comment**: None
- **Journal**: None
- **Summary**: Natural language processing has made significant inroads into learning the semantics of words through distributional approaches, however representations learnt via these methods fail to capture certain kinds of information implicit in the real world. In particular, spatial relations are encoded in a way that is inconsistent with human spatial reasoning and lacking invariance to viewpoint changes. We present a system capable of capturing the semantics of spatial relations such as behind, left of, etc from natural language. Our key contributions are a novel multi-modal objective based on generating images of scenes from their textual descriptions, and a new dataset on which to train it. We demonstrate that internal representations are robust to meaning preserving transformations of descriptions (paraphrase invariance), while viewpoint invariance is an emergent property of the system.



### Building Damage Annotation on Post-Hurricane Satellite Imagery Based on Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.01688v4
- **DOI**: 10.1007/s11069-020-04133-2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01688v4)
- **Published**: 2018-07-04 17:11:16+00:00
- **Updated**: 2020-07-07 20:37:55+00:00
- **Authors**: Quoc Dung Cao, Youngjun Choe
- **Comment**: None
- **Journal**: None
- **Summary**: After a hurricane, damage assessment is critical to emergency managers for efficient response and resource allocation. One way to gauge the damage extent is to quantify the number of flooded/damaged buildings, which is traditionally done by ground survey. This process can be labor-intensive and time-consuming. In this paper, we propose to improve the efficiency of building damage assessment by applying image classification algorithms to post-hurricane satellite imagery. At the known building coordinates (available from public data), we extract square-sized images from the satellite imagery to create training, validation, and test datasets. Each square-sized image contains a building to be classified as either 'Flooded/Damaged' (labeled by volunteers in a crowd-sourcing project) or 'Undamaged'. We design and train a convolutional neural network from scratch and compare it with an existing neural network used widely for common object classification. We demonstrate the promise of our damage annotation model (over 97% accuracy) in the case study of building damage assessment in the Greater Houston area affected by 2017 Hurricane Harvey.



### Localization Recall Precision (LRP): A New Performance Metric for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.01696v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01696v2)
- **Published**: 2018-07-04 17:47:53+00:00
- **Updated**: 2018-07-05 14:32:06+00:00
- **Authors**: Kemal Oksuz, Baris Can Cam, Emre Akbas, Sinan Kalkan
- **Comment**: to appear in ECCV 2018
- **Journal**: None
- **Summary**: Average precision (AP), the area under the recall-precision (RP) curve, is the standard performance measure for object detection. Despite its wide acceptance, it has a number of shortcomings, the most important of which are (i) the inability to distinguish very different RP curves, and (ii) the lack of directly measuring bounding box localization accuracy. In this paper, we propose 'Localization Recall Precision (LRP) Error', a new metric which we specifically designed for object detection. LRP Error is composed of three components related to localization, false negative (FN) rate and false positive (FP) rate. Based on LRP, we introduce the 'Optimal LRP', the minimum achievable LRP error representing the best achievable configuration of the detector in terms of recall-precision and the tightness of the boxes. In contrast to AP, which considers precisions over the entire recall domain, Optimal LRP determines the 'best' confidence score threshold for a class, which balances the trade-off between localization and recall-precision. In our experiments, we show that, for state-of-the-art object (SOTA) detectors, Optimal LRP provides richer and more discriminative information than AP. We also demonstrate that the best confidence score thresholds vary significantly among classes and detectors. Moreover, we present LRP results of a simple online video object detector which uses a SOTA still image object detector and show that the class-specific optimized thresholds increase the accuracy against the common approach of using a general threshold for all classes. At https://github.com/cancam/LRP we provide the source code that can compute LRP for the PASCAL VOC and MSCOCO datasets. Our source code can easily be adapted to other datasets as well.



### Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations
- **Arxiv ID**: http://arxiv.org/abs/1807.01697v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.01697v5)
- **Published**: 2018-07-04 17:57:11+00:00
- **Updated**: 2019-04-27 18:19:39+00:00
- **Authors**: Dan Hendrycks, Thomas G. Dietterich
- **Comment**: Superseded by _Benchmarking Neural Network Robustness to Common
  Corruptions and Perturbations_ arXiv:1903.12261
- **Journal**: None
- **Summary**: In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Unlike recent robustness research, this benchmark evaluates performance on commonplace corruptions not worst-case adversarial corruptions. We find that there are negligible changes in relative corruption robustness from AlexNet to ResNet classifiers, and we discover ways to enhance corruption robustness. Then we propose a new dataset called Icons-50 which opens research on a new kind of robustness, surface variation robustness. With this dataset we evaluate the frailty of classifiers on new styles of known objects and unexpected instances of known classes. We also demonstrate two methods that improve surface variation robustness. Together our benchmarks may aid future work toward networks that learn fundamental class structure and also robustly generalize.



### LaneNet: Real-Time Lane Detection Networks for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1807.01726v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01726v1)
- **Published**: 2018-07-04 18:05:04+00:00
- **Updated**: 2018-07-04 18:05:04+00:00
- **Authors**: Ze Wang, Weiqiang Ren, Qiang Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: Lane detection is to detect lanes on the road and provide the accurate location and shape of each lane. It severs as one of the key techniques to enable modern assisted and autonomous driving systems. However, several unique properties of lanes challenge the detection methods. The lack of distinctive features makes lane detection algorithms tend to be confused by other objects with similar local appearance. Moreover, the inconsistent number of lanes on a road as well as diverse lane line patterns, e.g. solid, broken, single, double, merging, and splitting lines further hamper the performance. In this paper, we propose a deep neural network based method, named LaneNet, to break down the lane detection into two stages: lane edge proposal and lane line localization. Stage one uses a lane edge proposal network for pixel-wise lane edge classification, and the lane line localization network in stage two then detects lane lines based on lane edge proposals. Please note that the goal of our LaneNet is built to detect lane line only, which introduces more difficulties on suppressing the false detections on the similar lane marks on the road like arrows and characters. Despite all the difficulties, our lane detection is shown to be robust to both highway and urban road scenarios method without relying on any assumptions on the lane number or the lane line patterns. The high running speed and low computational cost endow our LaneNet the capability of being deployed on vehicle-based systems. Experiments validate that our LaneNet consistently delivers outstanding performances on real world traffic scenarios.



### Learning Personalized Representation for Inverse Problems in Medical Imaging Using Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1807.01759v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1807.01759v1)
- **Published**: 2018-07-04 20:00:00+00:00
- **Updated**: 2018-07-04 20:00:00+00:00
- **Authors**: Kuang Gong, Kyungsang Kim, Jianan Cui, Ning Guo, Ciprian Catana, Jinyi Qi, Quanzheng Li
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Recently deep neural networks have been widely and successfully applied in computer vision tasks and attracted growing interests in medical imaging. One barrier for the application of deep neural networks to medical imaging is the need of large amounts of prior training pairs, which is not always feasible in clinical practice. In this work we propose a personalized representation learning framework where no prior training pairs are needed, but only the patient's own prior images. The representation is expressed using a deep neural network with the patient's prior images as network input. We then applied this novel image representation to inverse problems in medical imaging in which the original inverse problem was formulated as a constraint optimization problem and solved using the alternating direction method of multipliers (ADMM) algorithm. Anatomically guided brain positron emission tomography (PET) image reconstruction and image denoising were employed as examples to demonstrate the effectiveness of the proposed framework. Quantification results based on simulation and real datasets show that the proposed personalized representation framework outperform other widely adopted methods.



### MITOS-RCNN: A Novel Approach to Mitotic Figure Detection in Breast Cancer Histopathology Images using Region Based Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.01788v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01788v1)
- **Published**: 2018-07-04 21:29:53+00:00
- **Updated**: 2018-07-04 21:29:53+00:00
- **Authors**: Siddhant Rao
- **Comment**: Submitted to Elsevier Medical Image Analysis journal. 17 pages. 3
  tables. 4 figures
- **Journal**: None
- **Summary**: Studies estimate that there will be 266,120 new cases of invasive breast cancer and 40,920 breast cancer induced deaths in the year of 2018 alone. Despite the pervasiveness of this affliction, the current process to obtain an accurate breast cancer prognosis is tedious and time consuming, requiring a trained pathologist to manually examine histopathological images in order to identify the features that characterize various cancer severity levels. We propose MITOS-RCNN: a novel region based convolutional neural network (RCNN) geared for small object detection to accurately grade one of the three factors that characterize tumor belligerence described by the Nottingham Grading System: mitotic count. Other computational approaches to mitotic figure counting and detection do not demonstrate ample recall or precision to be clinically viable. Our models outperformed all previous participants in the ICPR 2012 challenge, the AMIDA 2013 challenge and the MITOS-ATYPIA-14 challenge along with recently published works. Our model achieved an F-measure score of 0.955, a 6.11% improvement in accuracy from the most accurate of the previously proposed models.



### TextTopicNet - Self-Supervised Learning of Visual Features Through Embedding Images on Semantic Text Spaces
- **Arxiv ID**: http://arxiv.org/abs/1807.02110v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02110v1)
- **Published**: 2018-07-04 21:44:09+00:00
- **Updated**: 2018-07-04 21:44:09+00:00
- **Authors**: Yash Patel, Lluis Gomez, Raul Gomez, Marçal Rusiñol, Dimosthenis Karatzas, C. V. Jawahar
- **Comment**: arXiv admin note: text overlap with arXiv:1705.08631
- **Journal**: None
- **Summary**: The immense success of deep learning based methods in computer vision heavily relies on large scale training datasets. These richly annotated datasets help the network learn discriminative visual features. Collecting and annotating such datasets requires a tremendous amount of human effort and annotations are limited to popular set of classes. As an alternative, learning visual features by designing auxiliary tasks which make use of freely available self-supervision has become increasingly popular in the computer vision community.   In this paper, we put forward an idea to take advantage of multi-modal context to provide self-supervision for the training of computer vision algorithms. We show that adequate visual features can be learned efficiently by training a CNN to predict the semantic textual context in which a particular image is more probable to appear as an illustration. More specifically we use popular text embedding techniques to provide the self-supervision for the training of deep CNN.   Our experiments demonstrate state-of-the-art performance in image classification, object detection, and multi-modal retrieval compared to recent self-supervised or naturally-supervised approaches.



### Deep Cross-modality Adaptation via Semantics Preserving Adversarial Learning for Sketch-based 3D Shape Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1807.01806v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.01806v1)
- **Published**: 2018-07-04 22:51:44+00:00
- **Updated**: 2018-07-04 22:51:44+00:00
- **Authors**: Jiaxin Chen, Yi Fang
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the large cross-modality discrepancy between 2D sketches and 3D shapes, retrieving 3D shapes by sketches is a significantly challenging task. To address this problem, we propose a novel framework to learn a discriminative deep cross-modality adaptation model in this paper. Specifically, we first separately adopt two metric networks, following two deep convolutional neural networks (CNNs), to learn modality-specific discriminative features based on an importance-aware metric learning method. Subsequently, we explicitly introduce a cross-modality transformation network to compensate for the divergence between two modalities, which can transfer features of 2D sketches to the feature space of 3D shapes. We develop an adversarial learning based method to train the transformation model, by simultaneously enhancing the holistic correlations between data distributions of two modalities, and mitigating the local semantic divergences through minimizing a cross-modality mean discrepancy term. Experimental results on the SHREC 2013 and SHREC 2014 datasets clearly show the superior retrieval performance of our proposed model, compared to the state-of-the-art approaches.



