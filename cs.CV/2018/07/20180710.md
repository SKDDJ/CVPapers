# Arxiv Papers in cs.CV on 2018-07-10
### A GPU-Oriented Algorithm Design for Secant-Based Dimensionality Reduction
- **Arxiv ID**: http://arxiv.org/abs/1807.03425v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1807.03425v1)
- **Published**: 2018-07-10 00:02:16+00:00
- **Updated**: 2018-07-10 00:02:16+00:00
- **Authors**: Henry Kvinge, Elin Farnell, Michael Kirby, Chris Peterson
- **Comment**: To appear in the 17th IEEE International Symposium on Parallel and
  Distributed Computing, Geneva, Switzerland 2018
- **Journal**: None
- **Summary**: Dimensionality-reduction techniques are a fundamental tool for extracting useful information from high-dimensional data sets. Because secant sets encode manifold geometry, they are a useful tool for designing meaningful data-reduction algorithms. In one such approach, the goal is to construct a projection that maximally avoids secant directions and hence ensures that distinct data points are not mapped too close together in the reduced space. This type of algorithm is based on a mathematical framework inspired by the constructive proof of Whitney's embedding theorem from differential topology. Computing all (unit) secants for a set of points is by nature computationally expensive, thus opening the door for exploitation of GPU architecture for achieving fast versions of these algorithms. We present a polynomial-time data-reduction algorithm that produces a meaningful low-dimensional representation of a data set by iteratively constructing improved projections within the framework described above. Key to our algorithm design and implementation is the use of GPUs which, among other things, minimizes the computational time required for the calculation of all secant lines. One goal of this report is to share ideas with GPU experts and to discuss a class of mathematical algorithms that may be of interest to the broader GPU community.



### Unsupervised Domain Adaptation for Automatic Estimation of Cardiothoracic Ratio
- **Arxiv ID**: http://arxiv.org/abs/1807.03434v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03434v1)
- **Published**: 2018-07-10 01:18:40+00:00
- **Updated**: 2018-07-10 01:18:40+00:00
- **Authors**: Nanqing Dong, Michael Kampffmeyer, Xiaodan Liang, Zeya Wang, Wei Dai, Eric P. Xing
- **Comment**: Accepted by MICCAI 2018
- **Journal**: None
- **Summary**: The cardiothoracic ratio (CTR), a clinical metric of heart size in chest X-rays (CXRs), is a key indicator of cardiomegaly. Manual measurement of CTR is time-consuming and can be affected by human subjectivity, making it desirable to design computer-aided systems that assist clinicians in the diagnosis process. Automatic CTR estimation through chest organ segmentation, however, requires large amounts of pixel-level annotated data, which is often unavailable. To alleviate this problem, we propose an unsupervised domain adaptation framework based on adversarial networks. The framework learns domain invariant feature representations from openly available data sources to produce accurate chest organ segmentation for unlabeled datasets. Specifically, we propose a model that enforces our intuition that prediction masks should be domain independent. Hence, we introduce a discriminator that distinguishes segmentation predictions from ground truth masks. We evaluate our system's prediction based on the assessment of radiologists and demonstrate the clinical practicability for the diagnosis of cardiomegaly. We finally illustrate on the JSRT dataset that the semi-supervised performance of our model is also very promising.



### Developing Brain Atlas through Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.03440v2
- **DOI**: 10.1038/s42256-019-0058-8
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1807.03440v2)
- **Published**: 2018-07-10 01:28:44+00:00
- **Updated**: 2019-06-07 15:01:56+00:00
- **Authors**: Asim Iqbal, Romesa Khan, Theofanis Karayannis
- **Comment**: 31 pages, 17 figures, 1 Table
- **Journal**: None
- **Summary**: Neuroscientists have devoted significant effort into the creation of standard brain reference atlases for high-throughput registration of anatomical regions of interest. However, variability in brain size and form across individuals poses a significant challenge for such reference atlases. To overcome these limitations, we introduce a fully automated deep neural network-based method (SeBRe) for registration through Segmenting Brain Regions of interest with minimal human supervision. We demonstrate the validity of our method on brain images from different mouse developmental time points, across a range of neuronal markers and imaging modalities. We further assess the performance of our method on images from MR-scanned human brains. Our registration method can accelerate brain-wide exploration of region-specific changes in brain development and, by simply segmenting brain regions of interest for high-throughput brain-wide analysis, provides an alternative to existing complex brain registration techniques.



### The Helmholtz Method: Using Perceptual Compression to Reduce Machine Learning Complexity
- **Arxiv ID**: http://arxiv.org/abs/1807.10569v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/1807.10569v1)
- **Published**: 2018-07-10 01:49:50+00:00
- **Updated**: 2018-07-10 01:49:50+00:00
- **Authors**: Gerald Friedland, Jingkang Wang, Ruoxi Jia, Bo Li
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a fundamental answer to a frequently asked question in multimedia computing and machine learning: Do artifacts from perceptual compression contribute to error in the machine learning process and if so, how much? Our approach to the problem is a reinterpretation of the Helmholtz Free Energy formula from physics to explain the relationship between content and noise when using sensors (such as cameras or microphones) to capture multimedia data. The reinterpretation allows a bit-measurement of the noise contained in images, audio, and video by combining a classifier with perceptual compression, such as JPEG or MP3. Our experiments on CIFAR-10 as well as Fraunhofer's IDMT-SMT-Audio-Effects dataset indicate that, at the right quality level, perceptual compression is actually not harmful but contributes to a significant reduction of complexity of the machine learning process. That is, our noise quantification method can be used to speed up the training of deep learning classifiers significantly while maintaining, or sometimes even improving, overall classification accuracy. Moreover, our results provide insights into the reasons for the success of deep learning.



### SceneEDNet: A Deep Learning Approach for Scene Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1807.03464v1
- **DOI**: 10.1109/ICARCV.2018.8581172
- **Categories**: **cs.CV**, cs.CG, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.03464v1)
- **Published**: 2018-07-10 03:26:55+00:00
- **Updated**: 2018-07-10 03:26:55+00:00
- **Authors**: Ravi Kumar Thakur, Snehasis Mukherjee
- **Comment**: None
- **Journal**: ICARCV (2018) 394-399
- **Summary**: Estimating scene flow in RGB-D videos is attracting much interest of the computer vision researchers, due to its potential applications in robotics. The state-of-the-art techniques for scene flow estimation, typically rely on the knowledge of scene structure of the frame and the correspondence between frames. However, with the increasing amount of RGB-D data captured from sophisticated sensors like Microsoft Kinect, and the recent advances in the area of sophisticated deep learning techniques, introduction of an efficient deep learning technique for scene flow estimation, is becoming important. This paper introduces a first effort to apply a deep learning method for direct estimation of scene flow by presenting a fully convolutional neural network with an encoder-decoder (ED) architecture. The proposed network SceneEDNet involves estimation of three dimensional motion vectors of all the scene points from sequence of stereo images. The training for direct estimation of scene flow is done using consecutive pairs of stereo images and corresponding scene flow ground truth. The proposed architecture is applied on a huge dataset and provides meaningful results.



### Learning a Single Tucker Decomposition Network for Lossy Image Compression with Multiple Bits-Per-Pixel Rates
- **Arxiv ID**: http://arxiv.org/abs/1807.03470v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03470v1)
- **Published**: 2018-07-10 03:40:36+00:00
- **Updated**: 2018-07-10 03:40:36+00:00
- **Authors**: Jianrui Cai, Zisheng Cao, Lei Zhang
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: Lossy image compression (LIC), which aims to utilize inexact approximations to represent an image more compactly, is a classical problem in image processing. Recently, deep convolutional neural networks (CNNs) have achieved interesting results in LIC by learning an encoder-quantizer-decoder network from a large amount of data. However, existing CNN-based LIC methods usually can only train a network for a specific bits-per-pixel (bpp). Such a "one network per bpp" problem limits the generality and flexibility of CNNs to practical LIC applications. In this paper, we propose to learn a single CNN which can perform LIC at multiple bpp rates. A simple yet effective Tucker Decomposition Network (TDNet) is developed, where there is a novel tucker decomposition layer (TDL) to decompose a latent image representation into a set of projection matrices and a core tensor. By changing the rank of the core tensor and its quantization, we can easily adjust the bpp rate of latent image representation within a single CNN. Furthermore, an iterative non-uniform quantization scheme is presented to optimize the quantizer, and a coarse-to-fine training strategy is introduced to reconstruct the decompressed images. Extensive experiments demonstrate the state-of-the-art compression performance of TDNet in terms of both PSNR and MS-SSIM indices.



### Shape analysis of framed space curves
- **Arxiv ID**: http://arxiv.org/abs/1807.03477v1
- **DOI**: None
- **Categories**: **math.DG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.03477v1)
- **Published**: 2018-07-10 04:33:42+00:00
- **Updated**: 2018-07-10 04:33:42+00:00
- **Authors**: Tom Needham
- **Comment**: None
- **Journal**: None
- **Summary**: In the elastic shape analysis approach to shape matching and object classification, plane curves are represented as points in an infinite-dimensional Riemannian manifold, wherein shape dissimilarity is measured by geodesic distance. A remarkable result of Younes, Michor, Shah and Mumford says that the space of closed planar shapes, endowed with a natural metric, is isometric to an infinite-dimensional Grassmann manifold via the so-called square root transform. This result facilitates efficient shape comparison by virtue of explicit descriptions of Grassmannian geodesics. In this paper, we extend this shape analysis framework to treat shapes of framed space curves. By considering framed curves, we are able to generalize the square root transform by using quaternionic arithmetic and properties of the Hopf fibration. Under our coordinate transformation, the space of closed framed curves corresponds to an infinite-dimensional complex Grassmannian. This allows us to describe geodesics in framed curve space explicitly. We are also able to produce explicit geodesics between closed, unframed space curves by studying the action of the loop group of the circle on the Grassmann manifold. Averages of collections of plane and space curves are computed via a novel algorithm utilizing flag means.



### An Adaptive Learning Method of Restricted Boltzmann Machine by Neuron Generation and Annihilation Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1807.03478v2
- **DOI**: 10.1109/SMC.2016.7844417
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.03478v2)
- **Published**: 2018-07-10 04:39:18+00:00
- **Updated**: 2018-07-11 08:06:52+00:00
- **Authors**: Shin Kamada, Takumi Ichimura
- **Comment**: 6 pages, 6 figures
- **Journal**: Proc. of 2016 IEEE International Conference on Systems, Man, and
  Cybernetics (IEEE SMC 2016)
- **Summary**: Restricted Boltzmann Machine (RBM) is a generative stochastic energy-based model of artificial neural network for unsupervised learning. Recently, RBM is well known to be a pre-training method of Deep Learning. In addition to visible and hidden neurons, the structure of RBM has a number of parameters such as the weights between neurons and the coefficients for them. Therefore, we may meet some difficulties to determine an optimal network structure to analyze big data. In order to evade the problem, we investigated the variance of parameters to find an optimal structure during learning. For the reason, we should check the variance of parameters to cause the fluctuation for energy function in RBM model. In this paper, we propose the adaptive learning method of RBM that can discover an optimal number of hidden neurons according to the training situation by applying the neuron generation and annihilation algorithm. In this method, a new hidden neuron is generated if the energy function is not still converged and the variance of the parameters is large. Moreover, the inactivated hidden neuron will be annihilated if the neuron does not affect the learning situation. The experimental results for some benchmark data sets were discussed in this paper.



### Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video Demonstration
- **Arxiv ID**: http://arxiv.org/abs/1807.03480v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.03480v2)
- **Published**: 2018-07-10 04:55:45+00:00
- **Updated**: 2019-03-06 21:56:52+00:00
- **Authors**: De-An Huang, Suraj Nair, Danfei Xu, Yuke Zhu, Animesh Garg, Li Fei-Fei, Silvio Savarese, Juan Carlos Niebles
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Our goal is to generate a policy to complete an unseen task given just a single video demonstration of the task in a given domain. We hypothesize that to successfully generalize to unseen complex tasks from a single video demonstration, it is necessary to explicitly incorporate the compositional structure of the tasks into the model. To this end, we propose Neural Task Graph (NTG) Networks, which use conjugate task graph as the intermediate representation to modularize both the video demonstration and the derived policy. We empirically show NTG achieves inter-task generalization on two complex tasks: Block Stacking in BulletPhysics and Object Collection in AI2-THOR. NTG improves data efficiency with visual input as well as achieve strong generalization without the need for dense hierarchical supervision. We further show that similar performance trends hold when applied to real-world data. We show that NTG can effectively predict task structure on the JIGSAWS surgical dataset and generalize to unseen tasks.



### An Adaptive Learning Method of Deep Belief Network by Layer Generation Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1807.03486v2
- **DOI**: 10.1109/TENCON.2016.7848589
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.03486v2)
- **Published**: 2018-07-10 05:55:26+00:00
- **Updated**: 2018-07-11 08:04:14+00:00
- **Authors**: Shin Kamada, Takumi Ichimura
- **Comment**: 4 pages, 2 figures, Proc. of 2016 IEEE Region 10 Conference
  (TENCON2016)
- **Journal**: None
- **Summary**: Deep Belief Network (DBN) has a deep architecture that represents multiple features of input patterns hierarchically with the pre-trained Restricted Boltzmann Machines (RBM). A traditional RBM or DBN model cannot change its network structure during the learning phase. Our proposed adaptive learning method can discover the optimal number of hidden neurons and weights and/or layers according to the input space. The model is an important method to take account of the computational cost and the model stability. The regularities to hold the sparse structure of network is considerable problem, since the extraction of explicit knowledge from the trained network should be required. In our previous research, we have developed the hybrid method of adaptive structural learning method of RBM and Learning Forgetting method to the trained RBM. In this paper, we propose the adaptive learning method of DBN that can determine the optimal number of layers during the learning. We evaluated our proposed model on some benchmark data sets.



### Recovering affine features from orientation- and scale-invariant ones
- **Arxiv ID**: http://arxiv.org/abs/1807.03503v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03503v1)
- **Published**: 2018-07-10 07:34:05+00:00
- **Updated**: 2018-07-10 07:34:05+00:00
- **Authors**: Daniel Barath
- **Comment**: None
- **Journal**: None
- **Summary**: An approach is proposed for recovering affine correspondences (ACs) from orientation- and scale-invariant, e.g. SIFT, features. The method calculates the affine parameters consistent with a pre-estimated epipolar geometry from the point coordinates and the scales and rotations which the feature detector obtains. The closed-form solution is given as the roots of a quadratic polynomial equation, thus having two possible real candidates and fast procedure, i.e. <1 millisecond. It is shown, as a possible application, that using the proposed algorithm allows us to estimate a homography for every single correspondence independently. It is validated both in our synthetic environment and on publicly available real world datasets, that the proposed technique leads to accurate ACs. Also, the estimated homographies have similar accuracy to what the state-of-the-art methods obtain, but due to requiring only a single correspondence, the robust estimation, e.g. by locally optimized RANSAC, is an order of magnitude faster.



### Embedded Implementation of a Deep Learning Smile Detector
- **Arxiv ID**: http://arxiv.org/abs/1807.10570v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.10570v1)
- **Published**: 2018-07-10 07:37:37+00:00
- **Updated**: 2018-07-10 07:37:37+00:00
- **Authors**: Pedram Ghazi, Antti P. Happonen, Jani Boutellier, Heikki Huttunen
- **Comment**: This work has been submitted to the IEEE for possible publication
- **Journal**: None
- **Summary**: In this paper we study the real time deployment of deep learning algorithms in low resource computational environments. As the use case, we compare the accuracy and speed of neural networks for smile detection using different neural network architectures and their system level implementation on NVidia Jetson embedded platform. We also propose an asynchronous multithreading scheme for parallelizing the pipeline. Within this framework, we experimentally compare thirteen widely used network topologies. The experiments show that low complexity architectures can achieve almost equal performance as larger ones, with a fraction of computation required.



### Topic-Guided Attention for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1807.03514v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03514v1)
- **Published**: 2018-07-10 07:59:42+00:00
- **Updated**: 2018-07-10 07:59:42+00:00
- **Authors**: Zhihao Zhu, Zhan Xue, Zejian Yuan
- **Comment**: Accepted by ICIP 2018
- **Journal**: None
- **Summary**: Attention mechanisms have attracted considerable interest in image captioning because of its powerful performance. Existing attention-based models use feedback information from the caption generator as guidance to determine which of the image features should be attended to. A common defect of these attention generation methods is that they lack a higher-level guiding information from the image itself, which sets a limit on selecting the most informative image features. Therefore, in this paper, we propose a novel attention mechanism, called topic-guided attention, which integrates image topics in the attention model as a guiding information to help select the most important image features. Moreover, we extract image features and image topics with separate networks, which can be fine-tuned jointly in an end-to-end manner during training. The experimental results on the benchmark Microsoft COCO dataset show that our method yields state-of-art performance on various quantitative metrics.



### Multiresolution Tree Networks for 3D Point Cloud Processing
- **Arxiv ID**: http://arxiv.org/abs/1807.03520v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.03520v2)
- **Published**: 2018-07-10 08:28:01+00:00
- **Updated**: 2018-07-11 20:19:30+00:00
- **Authors**: Matheus Gadelha, Rui Wang, Subhransu Maji
- **Comment**: Accepted to ECCV 2018. 23 pages, including supplemental material
- **Journal**: None
- **Summary**: We present multiresolution tree-structured networks to process point clouds for 3D shape understanding and generation tasks. Our network represents a 3D shape as a set of locality-preserving 1D ordered list of points at multiple resolutions. This allows efficient feed-forward processing through 1D convolutions, coarse-to-fine analysis through a multi-grid architecture, and it leads to faster convergence and small memory footprint during training. The proposed tree-structured encoders can be used to classify shapes and outperform existing point-based architectures on shape classification benchmarks, while tree-structured decoders can be used for generating point clouds directly and they outperform existing approaches for image-to-shape inference tasks learned using the ShapeNet dataset. Our model also allows unsupervised learning of point-cloud based shapes by using a variational autoencoder, leading to higher-quality generated shapes.



### Deep Underwater Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1807.03528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03528v1)
- **Published**: 2018-07-10 08:44:04+00:00
- **Updated**: 2018-07-10 08:44:04+00:00
- **Authors**: Saeed Anwar, Chongyi Li, Fatih Porikli
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: In an underwater scene, wavelength-dependent light absorption and scattering degrade the visibility of images, causing low contrast and distorted color casts. To address this problem, we propose a convolutional neural network based image enhancement model, i.e., UWCNN, which is trained efficiently using a synthetic underwater image database. Unlike the existing works that require the parameters of underwater imaging model estimation or impose inflexible frameworks applicable only for specific scenes, our model directly reconstructs the clear latent underwater image by leveraging on an automatic end-to-end and data-driven training mechanism. Compliant with underwater imaging models and optical properties of underwater scenes, we first synthesize ten different marine image databases. Then, we separately train multiple UWCNN models for each underwater image formation type. Experimental results on real-world and synthetic underwater images demonstrate that the presented method generalizes well on different underwater scenes and outperforms the existing methods both qualitatively and quantitatively. Besides, we conduct an ablation study to demonstrate the effect of each component in our network.



### Accurate Scene Text Detection through Border Semantics Awareness and Bootstrapping
- **Arxiv ID**: http://arxiv.org/abs/1807.03547v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03547v3)
- **Published**: 2018-07-10 09:26:07+00:00
- **Updated**: 2018-07-31 05:39:57+00:00
- **Authors**: Chuhui Xue, Shijian Lu, Fangneng Zhan
- **Comment**: 14 pages, 8 figures, accepted by ECCV 2018
- **Journal**: None
- **Summary**: This paper presents a scene text detection technique that exploits bootstrapping and text border semantics for accurate localization of texts in scenes. A novel bootstrapping technique is designed which samples multiple 'subsections' of a word or text line and accordingly relieves the constraint of limited training data effectively. At the same time, the repeated sampling of text 'subsections' improves the consistency of the predicted text feature maps which is critical in predicting a single complete instead of multiple broken boxes for long words or text lines. In addition, a semantics-aware text border detection technique is designed which produces four types of text border segments for each scene text. With semantics-aware text borders, scene texts can be localized more accurately by regressing text pixels around the ends of words or text lines instead of all text pixels which often leads to inaccurate localization while dealing with long words or text lines. Extensive experiments demonstrate the effectiveness of the proposed techniques, and superior performance is obtained over several public datasets, e. g. 80.1 f-score for the MSRA-TD500, 67.1 f-score for the ICDAR2017-RCTW, etc.



### Two-stage iterative Procrustes match algorithm and its application for VQ-based speaker verification
- **Arxiv ID**: http://arxiv.org/abs/1807.03587v1
- **DOI**: None
- **Categories**: **cs.CV**, 94A12
- **Links**: [PDF](http://arxiv.org/pdf/1807.03587v1)
- **Published**: 2018-07-10 12:15:54+00:00
- **Updated**: 2018-07-10 12:15:54+00:00
- **Authors**: Richeng Tan, Jing Li
- **Comment**: Submitted in ICMV 2018, 7 pages
- **Journal**: None
- **Summary**: In the past decades, Vector Quantization (VQ) model has been very popular across different pattern recognition areas, especially for feature-based tasks. However, the classification or regression performance of VQ-based systems always confronts the feature mismatch problem, which will heavily affect the performance of them. In this paper, we propose a two-stage iterative Procrustes algorithm (TIPM) to address the feature mismatch problem for VQ-based applications. At the first stage, the algorithm will remove mismatched feature vector pairs for a pair of input feature sets. Then, the second stage will collect those correct matched feature pairs that were discarded during the first stage. To evaluate the effectiveness of the proposed TIPM algorithm, speaker verification is used as the case study in this paper. The experiments were conducted on the TIMIT database and the results show that TIPM can improve VQ-based speaker verification performance clean condition and all noisy conditions.



### Efficient Evaluation of the Number of False Alarm Criterion
- **Arxiv ID**: http://arxiv.org/abs/1807.03594v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03594v1)
- **Published**: 2018-07-10 12:37:14+00:00
- **Updated**: 2018-07-10 12:37:14+00:00
- **Authors**: Sylvie Le Hégarat-Mascle, Emanuel Aldea, Jennifer Vandoni
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a method for computing efficiently the significance of a parametric pattern inside a binary image. On the one hand, a-contrario strategies avoid the user involvement for tuning detection thresholds, and allow one to account fairly for different pattern sizes. On the other hand, a-contrario criteria become intractable when the pattern complexity in terms of parametrization increases. In this work, we introduce a strategy which relies on the use of a cumulative space of reduced dimensionality, derived from the coupling of a classic (Hough) cumulative space with an integral histogram trick. This space allows us to store partial computations which are required by the a-contrario criterion, and to evaluate the significance with a lower computational cost than by following a straightforward approach. The method is illustrated on synthetic examples on patterns with various parametrizations up to five dimensions. In order to demonstrate how to apply this generic concept in a real scenario, we consider a difficult crack detection task in still images, which has been addressed in the literature with various local and global detection strategies. We model cracks as bounded segments, detected by the proposed a-contrario criterion, which allow us to introduce additional spatial constraints based on their relative alignment. On this application, the proposed strategy yields state-of the-art results, and underlines its potential for handling complex pattern detection tasks.



### Essential Tensor Learning for Multi-view Spectral Clustering
- **Arxiv ID**: http://arxiv.org/abs/1807.03602v2
- **DOI**: 10.1109/TIP.2019.2916740
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03602v2)
- **Published**: 2018-07-10 13:01:48+00:00
- **Updated**: 2019-05-06 14:10:05+00:00
- **Authors**: Jianlong Wu, Zhouchen Lin, Hongbin Zha
- **Comment**: Accepted by IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Multi-view clustering attracts much attention recently, which aims to take advantage of multi-view information to improve the performance of clustering. However, most recent work mainly focus on self-representation based subspace clustering, which is of high computation complexity. In this paper, we focus on the Markov chain based spectral clustering method and propose a novel essential tensor learning method to explore the high order correlations for multi-view representation. We first construct a tensor based on multi-view transition probability matrices of the Markov chain. By incorporating the idea from robust principle component analysis, tensor singular value decomposition (t-SVD) based tensor nuclear norm is imposed to preserve the low-rank property of the essential tensor, which can well capture the principle information from multiple views. We also employ the tensor rotation operator for this task to better investigate the relationship among views as well as reduce the computation complexity. The proposed method can be efficiently optimized by the alternating direction method of multipliers~(ADMM). Extensive experiments on six real world datasets corresponding to five different applications show that our method achieves superior performance over other state-of-the-art methods.



### Convolutional neural network based automatic plaque characterization from intracoronary optical coherence tomography images
- **Arxiv ID**: http://arxiv.org/abs/1807.03613v1
- **DOI**: 10.1117/12.2293957
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03613v1)
- **Published**: 2018-07-10 13:18:21+00:00
- **Updated**: 2018-07-10 13:18:21+00:00
- **Authors**: Shenghua He, Jie Zheng, Akiko Maehara, Gary Mintz, Dalin Tang, Mark Anastasio, Hua Li
- **Comment**: SPIE 2018
- **Journal**: None
- **Summary**: Optical coherence tomography (OCT) can provide high-resolution cross-sectional images for analyzing superficial plaques in coronary arteries. Commonly, plaque characterization using intra-coronary OCT images is performed manually by expert observers. This manual analysis is time consuming and its accuracy heavily relies on the experience of human observers. Traditional machine learning based methods, such as the least squares support vector machine and random forest methods, have been recently employed to automatically characterize plaque regions in OCT images. Several processing steps, including feature extraction, informative feature selection, and final pixel classification, are commonly used in these traditional methods. Therefore, the final classification accuracy can be jeopardized by error or inaccuracy within each of these steps. In this study, we proposed a convolutional neural network (CNN) based method to automatically characterize plaques in OCT images. Unlike traditional methods, our method uses the image as a direct input and performs classification as a single-step process. The experiments on 269 OCT images showed that the average prediction accuracy of CNN-based method was 0.866, which indicated a great promise for clinical translation.



### Towards Head Motion Compensation Using Multi-Scale Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.03651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03651v1)
- **Published**: 2018-07-10 13:57:58+00:00
- **Updated**: 2018-07-10 13:57:58+00:00
- **Authors**: Omer Rajput, Nils Gessert, Martin Gromniak, Lars Matthäus, Alexander Schlaefer
- **Comment**: Presented at CURAC 2018 conference
- **Journal**: None
- **Summary**: Head pose estimation and tracking is useful in variety of medical applications. With the advent of RGBD cameras like Kinect, it has become feasible to do markerless tracking by estimating the head pose directly from the point clouds. One specific medical application is robot assisted transcranial magnetic stimulation (TMS) where any patient motion is compensated with the help of a robot. For increased patient comfort, it is important to track the head without markers. In this regard, we address the head pose estimation problem using two different approaches. In the first approach, we build upon the more traditional approach of model based head tracking, where a head model is morphed according to the particular head to be tracked and the morphed model is used to track the head in the point cloud streams. In the second approach, we propose a new multi-scale convolutional neural network architecture for more accurate pose regression. Additionally, we outline a systematic data set acquisition strategy using a head phantom mounted on the robot and ground-truth labels generated using a highly accurate tracking system.



### Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1807.07928v2
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.07928v2)
- **Published**: 2018-07-10 16:12:07+00:00
- **Updated**: 2019-05-20 19:56:26+00:00
- **Authors**: Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, Vivienne Sze
- **Comment**: accepted for publication in IEEE Journal on Emerging and Selected
  Topics in Circuits and Systems. This extended version on arXiv also includes
  Eyexam in the appendix
- **Journal**: None
- **Summary**: A recent trend in DNN development is to extend the reach of deep learning applications to platforms that are more resource and energy constrained, e.g., mobile devices. These endeavors aim to reduce the DNN model size and improve the hardware processing efficiency, and have resulted in DNNs that are much more compact in their structures and/or have high data sparsity. These compact or sparse models are different from the traditional large ones in that there is much more variation in their layer shapes and sizes, and often require specialized hardware to exploit sparsity for performance improvement. Thus, many DNN accelerators designed for large DNNs do not perform well on these models. In this work, we present Eyeriss v2, a DNN accelerator architecture designed for running compact and sparse DNNs. To deal with the widely varying layer shapes and sizes, it introduces a highly flexible on-chip network, called hierarchical mesh, that can adapt to the different amounts of data reuse and bandwidth requirements of different data types, which improves the utilization of the computation resources. Furthermore, Eyeriss v2 can process sparse data directly in the compressed domain for both weights and activations, and therefore is able to improve both processing speed and energy efficiency with sparse models. Overall, with sparse MobileNet, Eyeriss v2 in a 65nm CMOS process achieves a throughput of 1470.6 inferences/sec and 2560.3 inferences/J at a batch size of 1, which is 12.6x faster and 2.5x more energy efficient than the original Eyeriss running MobileNet. We also present an analysis methodology called Eyexam that provides a systematic way of understanding the performance limits for DNN processors as a function of specific characteristics of the DNN model and accelerator design; it applies these characteristics as sequential steps to increasingly tighten the bound on the performance limits.



### Efficient identification, localization and quantification of grapevine inflorescences in unprepared field images using Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.03770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03770v1)
- **Published**: 2018-07-10 17:46:40+00:00
- **Updated**: 2018-07-10 17:46:40+00:00
- **Authors**: Robert Rudolph, Katja Herzog, Reinhard Töpfer, Volker Steinhage
- **Comment**: None
- **Journal**: None
- **Summary**: Yield and its prediction is one of the most important tasks in grapevine breeding purposes and vineyard management. Commonly, this trait is estimated manually right before harvest by extrapolation, which mostly is labor-intensive, destructive and inaccurate. In the present study an automated image-based workflow was developed quantifying inflorescences and single flowers in unprepared field images of grapevines, i.e. no artificial background or light was applied. It is a novel approach for non-invasive, inexpensive and objective phenotyping with high-throughput.   First, image regions depicting inflorescences were identified and localized. This was done by segmenting the images into the classes "inflorescence" and "non-inflorescence" using a Fully Convolutional Network (FCN). Efficient image segmentation hereby is the most challenging step regarding the small geometry and dense distribution of flowers (several hundred flowers per inflorescence), similar color of all plant organs in the fore- and background as well as the circumstance that only approximately 5% of an image show inflorescences. The trained FCN achieved a mean Intersection Over Union (IOU) of 87.6% on the test data set. Finally, individual flowers were extracted from the "inflorescence"-areas using Circular Hough Transform. The flower extraction achieved a recall of 80.3% and a precision of 70.7% using the segmentation derived by the trained FCN model.   Summarized, the presented approach is a promising strategy in order to predict yield potential automatically in the earliest stage of grapevine development which is applicable for objective monitoring and evaluations of breeding material, genetic repositories or commercial vineyards.



### CIRL: Controllable Imitative Reinforcement Learning for Vision-based Self-driving
- **Arxiv ID**: http://arxiv.org/abs/1807.03776v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.03776v1)
- **Published**: 2018-07-10 17:56:22+00:00
- **Updated**: 2018-07-10 17:56:22+00:00
- **Authors**: Xiaodan Liang, Tairui Wang, Luona Yang, Eric Xing
- **Comment**: To appear in ECCV 2018
- **Journal**: None
- **Summary**: Autonomous urban driving navigation with complex multi-agent dynamics is under-explored due to the difficulty of learning an optimal driving policy. The traditional modular pipeline heavily relies on hand-designed rules and the pre-processing perception system while the supervised learning-based models are limited by the accessibility of extensive human experience. We present a general and principled Controllable Imitative Reinforcement Learning (CIRL) approach which successfully makes the driving agent achieve higher success rates based on only vision inputs in a high-fidelity car simulator. To alleviate the low exploration efficiency for large continuous action space that often prohibits the use of classical RL on challenging real tasks, our CIRL explores over a reasonably constrained action space guided by encoded experiences that imitate human demonstrations, building upon Deep Deterministic Policy Gradient (DDPG). Moreover, we propose to specialize adaptive policies and steering-angle reward designs for different control signals (i.e. follow, straight, turn right, turn left) based on the shared representations to improve the model capability in tackling with diverse cases. Extensive experiments on CARLA driving benchmark demonstrate that CIRL substantially outperforms all previous methods in terms of the percentage of successfully completed episodes on a variety of goal-directed driving tasks. We also show its superior generalization capability in unseen environments. To our knowledge, this is the first successful case of the learned driving policy through reinforcement learning in the high-fidelity simulator, which performs better-than supervised imitation learning.



### Model-based free-breathing cardiac MRI reconstruction using deep learned \& STORM priors: MoDL-STORM
- **Arxiv ID**: http://arxiv.org/abs/1807.03845v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.03845v1)
- **Published**: 2018-07-10 20:04:14+00:00
- **Updated**: 2018-07-10 20:04:14+00:00
- **Authors**: Sampurna Biswas, Hemant K. Aggarwal, Sunrita Poddar, Mathews Jacob
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a model-based reconstruction framework with deep learned (DL) and smoothness regularization on manifolds (STORM) priors to recover free breathing and ungated (FBU) cardiac MRI from highly undersampled measurements. The DL priors enable us to exploit the local correlations, while the STORM prior enables us to make use of the extensive non-local similarities that are subject dependent. We introduce a novel model-based formulation that allows the seamless integration of deep learning methods with available prior information, which current deep learning algorithms are not capable of. The experimental results demonstrate the preliminary potential of this work in accelerating FBU cardiac MRI.



### Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.03848v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03848v3)
- **Published**: 2018-07-10 20:19:27+00:00
- **Updated**: 2019-07-31 02:06:37+00:00
- **Authors**: Chun-Fu Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, Rogerio Feris
- **Comment**: git repo: https://github.com/IBM/BigLittleNet
- **Journal**: None
- **Summary**: In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks,using popular architectures including ResNet and ResNeXt. For object recognition, our approach reduces computation by 33% on object recognition while improving accuracy with 0.9%. Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains. The codes are available at https://github.com/IBM/BigLittleNet



### "Factual" or "Emotional": Stylized Image Captioning with Adaptive Learning and Attention
- **Arxiv ID**: http://arxiv.org/abs/1807.03871v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03871v3)
- **Published**: 2018-07-10 21:33:22+00:00
- **Updated**: 2018-07-29 22:01:26+00:00
- **Authors**: Tianlang Chen, Zhongping Zhang, Quanzeng You, Chen Fang, Zhaowen Wang, Hailin Jin, Jiebo Luo
- **Comment**: 17 pages, 7 figures, ECCV 2018
- **Journal**: None
- **Summary**: Generating stylized captions for an image is an emerging topic in image captioning. Given an image as input, it requires the system to generate a caption that has a specific style (e.g., humorous, romantic, positive, and negative) while describing the image content semantically accurately. In this paper, we propose a novel stylized image captioning model that effectively takes both requirements into consideration. To this end, we first devise a new variant of LSTM, named style-factual LSTM, as the building block of our model. It uses two groups of matrices to capture the factual and stylized knowledge, respectively, and automatically learns the word-level weights of the two groups based on previous context. In addition, when we train the model to capture stylized elements, we propose an adaptive learning approach based on a reference factual model, it provides factual knowledge to the model as the model learns from stylized caption labels, and can adaptively compute how much information to supply at each time step. We evaluate our model on two stylized image captioning datasets, which contain humorous/romantic captions and positive/negative captions, respectively. Experiments shows that our proposed model outperforms the state-of-the-art approaches, without using extra ground truth supervision.



### Vision System for AGI: Problems and Directions
- **Arxiv ID**: http://arxiv.org/abs/1807.03887v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03887v1)
- **Published**: 2018-07-10 22:12:52+00:00
- **Updated**: 2018-07-10 22:12:52+00:00
- **Authors**: Alexey Potapov, Sergey Rodionov, Maxim Peterson, Oleg Shcherbakov, Innokentii Zhdanov, Nikolai Skorobogatko
- **Comment**: None
- **Journal**: None
- **Summary**: What frameworks and architectures are necessary to create a vision system for AGI? In this paper, we propose a formal model that states the task of perception within AGI. We show the role of discriminative and generative models in achieving efficient and general solution of this task, thus specifying the task in more detail. We discuss some existing generative and discriminative models and demonstrate their insufficiency for our purposes. Finally, we discuss some architectural dilemmas and open questions.



### Deep Imbalanced Attribute Classification using Visual Attention Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1807.03903v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03903v2)
- **Published**: 2018-07-10 23:49:03+00:00
- **Updated**: 2018-07-26 02:12:58+00:00
- **Authors**: Nikolaos Sarafianos, Xiang Xu, Ioannis A. Kakadiaris
- **Comment**: To appear in ECCV 2018
- **Journal**: None
- **Summary**: For many computer vision applications, such as image description and human identification, recognizing the visual attributes of humans is an essential yet challenging problem. Its challenges originate from its multi-label nature, the large underlying class imbalance and the lack of spatial annotations. Existing methods follow either a computer vision approach while failing to account for class imbalance, or explore machine learning solutions, which disregard the spatial and semantic relations that exist in the images. With that in mind, we propose an effective method that extracts and aggregates visual attention masks at different scales. We introduce a loss function to handle class imbalance both at class and at an instance level and further demonstrate that penalizing attention masks with high prediction variance accounts for the weak supervision of the attention mechanism. By identifying and addressing these challenges, we achieve state-of-the-art results with a simple attention mechanism in both PETA and WIDER-Attribute datasets without additional context or side information.



