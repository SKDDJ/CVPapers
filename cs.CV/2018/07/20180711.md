# Arxiv Papers in cs.CV on 2018-07-11
### Generative Adversarial Networks with Decoder-Encoder Output Noise
- **Arxiv ID**: http://arxiv.org/abs/1807.03923v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03923v1)
- **Published**: 2018-07-11 01:50:17+00:00
- **Updated**: 2018-07-11 01:50:17+00:00
- **Authors**: Guoqiang Zhong, Wei Gao, Yongbin Liu, Youzhao Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, research on image generation methods has been developing fast. The auto-encoding variational Bayes method (VAEs) was proposed in 2013, which uses variational inference to learn a latent space from the image database and then generates images using the decoder. The generative adversarial networks (GANs) came out as a promising framework, which uses adversarial training to improve the generative ability of the generator. However, the generated pictures by GANs are generally blurry. The deep convolutional generative adversarial networks (DCGANs) were then proposed to leverage the quality of generated images. Since the input noise vectors are randomly sampled from a Gaussian distribution, the generator has to map from a whole normal distribution to the images. This makes DCGANs unable to reflect the inherent structure of the training data. In this paper, we propose a novel deep model, called generative adversarial networks with decoder-encoder output noise (DE-GANs), which takes advantage of both the adversarial training and the variational Bayesain inference to improve the performance of image generation. DE-GANs use a pre-trained decoder-encoder architecture to map the random Gaussian noise vectors to informative ones and pass them to the generator of the adversarial networks. Since the decoder-encoder architecture is trained by the same images as the generators, the output vectors could carry the intrinsic distribution information of the original images. Moreover, the loss function of DE-GANs is different from GANs and DCGANs. A hidden-space loss function is added to the adversarial loss function to enhance the robustness of the model. Extensive empirical results show that DE-GANs can accelerate the convergence of the adversarial training process and improve the quality of the generated images.



### Deep attention-based classification network for robust depth prediction
- **Arxiv ID**: http://arxiv.org/abs/1807.03959v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.03959v1)
- **Published**: 2018-07-11 06:19:22+00:00
- **Updated**: 2018-07-11 06:19:22+00:00
- **Authors**: Ruibo Li, Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Lingxiao Hang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present our deep attention-based classification (DABC) network for robust single image depth prediction, in the context of the Robust Vision Challenge 2018 (ROB 2018). Unlike conventional depth prediction, our goal is to design a model that can perform well in both indoor and outdoor scenes with a single parameter set. However, robust depth prediction suffers from two challenging problems: a) How to extract more discriminative features for different scenes (compared to a single scene)? b) How to handle the large differences of depth ranges between indoor and outdoor datasets? To address these two problems, we first formulate depth prediction as a multi-class classification task and apply a softmax classifier to classify the depth label of each pixel. We then introduce a global pooling layer and a channel-wise attention mechanism to adaptively select the discriminative channels of features and to update the original features by assigning important channels with higher weights. Further, to reduce the influence of quantization errors, we employ a soft-weighted sum inference strategy for the final prediction. Experimental results on both indoor and outdoor datasets demonstrate the effectiveness of our method. It is worth mentioning that we won the 2-nd place in single image depth prediction entry of ROB 2018, in conjunction with IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2018.



### Sparse Range-constrained Learning and Its Application for Medical Image Grading
- **Arxiv ID**: http://arxiv.org/abs/1807.10571v1
- **DOI**: 10.1109/TMI.2018.2851607
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.10571v1)
- **Published**: 2018-07-11 06:59:45+00:00
- **Updated**: 2018-07-11 06:59:45+00:00
- **Authors**: Jun Cheng
- **Comment**: Accepted for publication in IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Sparse learning has been shown to be effective in solving many real-world problems. Finding sparse representations is a fundamentally important topic in many fields of science including signal processing, computer vision, genome study and medical imaging. One important issue in applying sparse representation is to find the basis to represent the data,especially in computer vision and medical imaging where the data is not necessary incoherent. In medical imaging, clinicians often grade the severity or measure the risk score of a disease based on images. This process is referred to as medical image grading. Manual grading of the disease severity or risk score is often used. However, it is tedious, subjective and expensive. Sparse learning has been used for automatic grading of medical images for different diseases. In the grading, we usually begin with one step to find a sparse representation of the testing image using a set of reference images or atoms from the dictionary. Then in the second step, the selected atoms are used as references to compute the grades of the testing images. Since the two steps are conducted sequentially, the objective function in the first step is not necessarily optimized for the second step. In this paper, we propose a novel sparse range-constrained learning(SRCL)algorithm for medical image grading.Different from most of existing sparse learning algorithms, SRCL integrates the objective of finding a sparse representation and that of grading the image into one function. It aims to find a sparse representation of the testing image based on atoms that are most similar in both the data or feature representation and the medical grading scores. We apply the new proposed SRCL to CDR computation and cataract grading. Experimental results show that the proposed method is able to improve the accuracy in cup-to-disc ratio computation and cataract grading.



### Learning Neural Models for End-to-End Clustering
- **Arxiv ID**: http://arxiv.org/abs/1807.04001v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.04001v1)
- **Published**: 2018-07-11 08:45:45+00:00
- **Updated**: 2018-07-11 08:45:45+00:00
- **Authors**: Benjamin Bruno Meier, Ismail Elezi, Mohammadreza Amirian, Oliver Durr, Thilo Stadelmann
- **Comment**: Accepted for publication on ANNPR 2018
- **Journal**: None
- **Summary**: We propose a novel end-to-end neural network architecture that, once trained, directly outputs a probabilistic clustering of a batch of input examples in one pass. It estimates a distribution over the number of clusters $k$, and for each $1 \leq k \leq k_\mathrm{max}$, a distribution over the individual cluster assignment for each data point. The network is trained in advance in a supervised fashion on separate data to learn grouping by any perceptual similarity criterion based on pairwise labels (same/different group). It can then be applied to different data containing different groups. We demonstrate promising performance on high-dimensional data like images (COIL-100) and speech (TIMIT). We call this ``learning to cluster'' and show its conceptual difference to deep metric learning, semi-supervise clustering and other related approaches while having the advantage of performing learnable clustering fully end-to-end.



### CG-DIQA: No-reference Document Image Quality Assessment Based on Character Gradient
- **Arxiv ID**: http://arxiv.org/abs/1807.04047v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04047v1)
- **Published**: 2018-07-11 10:00:03+00:00
- **Updated**: 2018-07-11 10:00:03+00:00
- **Authors**: Hongyu Li, Fan Zhu, Junhua Qiu
- **Comment**: To be published in Proc. of ICPR 2018
- **Journal**: None
- **Summary**: Document image quality assessment (DIQA) is an important and challenging problem in real applications. In order to predict the quality scores of document images, this paper proposes a novel no-reference DIQA method based on character gradient, where the OCR accuracy is used as a ground-truth quality metric. Character gradient is computed on character patches detected with the maximally stable extremal regions (MSER) based method. Character patches are essentially significant to character recognition and therefore suitable for use in estimating document image quality. Experiments on a benchmark dataset show that the proposed method outperforms the state-of-the-art methods in estimating the quality score of document images.



### Perception of Image Features in Post-Mortem Iris Recognition: Humans vs Machines
- **Arxiv ID**: http://arxiv.org/abs/1807.04049v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04049v3)
- **Published**: 2018-07-11 10:01:07+00:00
- **Updated**: 2019-10-09 09:06:54+00:00
- **Authors**: Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz
- **Comment**: Accepted for the 10th IEEE International COnference on Biometrics:
  Theory, Applications and Systems (BTAS2019), 23-26 Sept 2019, Tampa, USA
- **Journal**: None
- **Summary**: Post-mortem iris recognition can offer an additional forensic method of personal identification. However, in contrary to already well-established human examination of fingerprints, making iris recognition human-interpretable is harder, and therefore it has never been applied in forensic proceedings. There is no strong consensus among biometric experts which iris features, especially those in iris images acquired post-mortem, are the most important for human experts solving an iris recognition task. This paper explores two ways of broadening this knowledge: (a) with an eye tracker, the salient features used by humans comparing iris images on a screen are extracted, and (b) class-activation maps produced by the convolutional neural network solving the iris recognition task are analyzed. Both humans and deep learning-based solutions were examined with the same set of iris image pairs. This made it possible to compare the attention maps and conclude that (a) deep learning-based method can offer human-interpretable decisions backed by visual explanations pointing a human examiner to salient regions, and (b) in many cases humans and a machine used different features, what means that a deep learning-based method can offer a complementary support to human experts. This paper offers the first known to us human-interpretable comparison of machine-based and human-based post-mortem iris recognition, and the trained models annotating salient iris image regions.



### DeSTNet: Densely Fused Spatial Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.04050v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04050v2)
- **Published**: 2018-07-11 10:06:32+00:00
- **Updated**: 2018-07-16 10:27:31+00:00
- **Authors**: Roberto Annunziata, Christos Sagonas, Jacques Calì
- **Comment**: Accepted for publication at the 29th British Machine Vision
  Conference (BMVC 2018)
- **Journal**: None
- **Summary**: Modern Convolutional Neural Networks (CNN) are extremely powerful on a range of computer vision tasks. However, their performance may degrade when the data is characterised by large intra-class variability caused by spatial transformations. The Spatial Transformer Network (STN) is currently the method of choice for providing CNNs the ability to remove those transformations and improve performance in an end-to-end learning framework. In this paper, we propose Densely Fused Spatial Transformer Network (DeSTNet), which, to our best knowledge, is the first dense fusion pattern for combining multiple STNs. Specifically, we show how changing the connectivity pattern of multiple STNs from sequential to dense leads to more powerful alignment modules. Extensive experiments on three benchmarks namely, MNIST, GTSRB, and IDocDB show that the proposed technique outperforms related state-of-the-art methods (i.e., STNs and CSTNs) both in terms of accuracy and robustness.



### SVD-based Visualisation and Approximation for Time Series Data in Smart Energy Systems
- **Arxiv ID**: http://arxiv.org/abs/1807.10120v1
- **DOI**: 10.1109/ISGTEurope.2017.8260303
- **Categories**: **physics.soc-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.10120v1)
- **Published**: 2018-07-11 10:06:48+00:00
- **Updated**: 2018-07-11 10:06:48+00:00
- **Authors**: Abdolrahman Khoshrou, Andre B. Dorsman, Eric. J. Pauwels
- **Comment**: None
- **Journal**: None
- **Summary**: Many time series in smart energy systems exhibit two different timescales. On the one hand there are patterns linked to daily human activities. On the other hand, there are relatively slow trends linked to seasonal variations. In this paper we interpret these time series as matrices, to be visualized as images. This approach has two advantages: First of all, interpreting such time series as images enables one to visually integrate across the image and makes it therefore easier to spot subtle or faint features. Second, the matrix interpretation also grants elucidation of the underlying structure using well-established matrix decomposition methods. We will illustrate both these aspects for data obtained from the German day-ahead market.



### Two-Layer Mixture Network Ensemble for Apparel Attributes Classification
- **Arxiv ID**: http://arxiv.org/abs/1807.10572v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.10572v1)
- **Published**: 2018-07-11 10:16:27+00:00
- **Updated**: 2018-07-11 10:16:27+00:00
- **Authors**: Tianqi Han, Zhihui Fu, Hongyu Li
- **Comment**: To be published in Proc. of AIFT 2018
- **Journal**: None
- **Summary**: Recognizing apparel attributes has recently drawn great interest in the computer vision community. Methods based on various deep neural networks have been proposed for image classification, which could be applied to apparel attributes recognition. An interesting problem raised is how to ensemble these methods to further improve the accuracy. In this paper, we propose a two-layer mixture framework for ensemble different networks. In the first layer of this framework, two types of ensemble learning methods, bagging and boosting, are separately applied. Different from traditional methods, our bagging process makes use of the whole training set, not random subsets, to train each model in the ensemble, where several differentiated deep networks are used to promote model variance. To avoid the bias of small-scale samples, the second layer only adopts bagging to mix the results obtained with bagging and boosting in the first layer. Experimental results demonstrate that the proposed mixture framework outperforms any individual network model or either independent ensemble method in apparel attributes classification.



### Temporal Convolution Networks for Real-Time Abdominal Fetal Aorta Analysis with Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/1807.04056v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.04056v1)
- **Published**: 2018-07-11 10:22:38+00:00
- **Updated**: 2018-07-11 10:22:38+00:00
- **Authors**: Nicolo' Savioli, Silvia Visentin, Erich Cosmi, Enrico Grisan, Pablo Lamata, Giovanni Montana
- **Comment**: 10 pages, 2 figures
- **Journal**: None
- **Summary**: The automatic analysis of ultrasound sequences can substantially improve the efficiency of clinical diagnosis. In this work we present our attempt to automate the challenging task of measuring the vascular diameter of the fetal abdominal aorta from ultrasound images. We propose a neural network architecture consisting of three blocks: a convolutional layer for the extraction of imaging features, a Convolution Gated Recurrent Unit (C-GRU) for enforcing the temporal coherence across video frames and exploiting the temporal redundancy of a signal, and a regularized loss function, called \textit{CyclicLoss}, to impose our prior knowledge about the periodicity of the observed signal. We present experimental evidence suggesting that the proposed architecture can reach an accuracy substantially superior to previously proposed methods, providing an average reduction of the mean squared error from $0.31 mm^2$ (state-of-art) to $0.09 mm^2$, and a relative error reduction from $8.1\%$ to $5.3\%$. The mean execution speed of the proposed approach of 289 frames per second makes it suitable for real time clinical use.



### Presentation Attack Detection for Cadaver Iris
- **Arxiv ID**: http://arxiv.org/abs/1807.04058v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04058v2)
- **Published**: 2018-07-11 10:35:22+00:00
- **Updated**: 2018-07-27 07:46:59+00:00
- **Authors**: Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz
- **Comment**: Accepted for publication at the 9th IEEE International Conference on
  Biometrics: Theory, Applications, and Systems (BTAS 2018), Los Angeles, USA,
  October 22-25, 2018
- **Journal**: None
- **Summary**: This paper presents a deep-learning-based method for iris presentation attack detection (PAD) when iris images are obtained from deceased people. Our approach is based on the VGG-16 architecture fine-tuned with a database of 574 post-mortem, near-infrared iris images from the Warsaw-BioBase-PostMortem-Iris-v1 database, complemented by a dataset of 256 images of live irises, collected within the scope of this study. Experiments described in this paper show that our approach is able to correctly classify iris images as either representing a live or a dead eye in almost 99% of the trials, averaged over 20 subject-disjoint, train/test splits. We also show that the post-mortem iris detection accuracy increases as time since death elapses, and that we are able to construct a classification system with APCER=0%@BPCER=1% (Attack Presentation and Bona Fide Presentation Classification Error Rates, respectively) when only post-mortem samples collected at least 16 hours post-mortem are considered. Since acquisitions of ante- and post-mortem samples differ significantly, we applied countermeasures to minimize bias in our classification methodology caused by image properties that are not related to the PAD. This included using the same iris sensor in collection of ante- and post-mortem samples, and analysis of class activation maps to ensure that discriminant iris regions utilized by our classifier are related to properties of the eye, and not to those of the acquisition protocol. This paper offers the first known to us PAD method in a post-mortem setting, together with an explanation of the decisions made by the convolutional neural network. Along with the paper we offer source codes, weights of the trained network, and a dataset of live iris images to facilitate reproducibility and further research.



### Cross-spectral Iris Recognition for Mobile Applications using High-quality Color Images
- **Arxiv ID**: http://arxiv.org/abs/1807.04061v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04061v1)
- **Published**: 2018-07-11 10:43:06+00:00
- **Updated**: 2018-07-11 10:43:06+00:00
- **Authors**: Mateusz Trokielewicz, Ewelina Bartuzi
- **Comment**: None
- **Journal**: Journal of Telecommunications and Information Technology, vol.
  3/2016, pages 91-97
- **Summary**: With the recent shift towards mobile computing, new challenges for biometric authentication appear on the horizon. This paper provides a comprehensive study of cross-spectral iris recognition in a scenario, in which high quality color images obtained with a mobile phone are used against enrollment images collected in typical, near-infrared setups. Grayscale conversion of the color images that employs selective RGB channel choice depending on the iris coloration is shown to improve the recognition accuracy for some combinations of eye colors and matching software, when compared to using the red channel only, with equal error rates driven down to as low as 2%. The authors are not aware of any other paper focusing on cross-spectral iris recognition is a scenario with near-infrared enrollment using a professional iris recognition setup and then a mobile-based verification employing color images.



### MultiPoseNet: Fast Multi-Person Pose Estimation using Pose Residual Network
- **Arxiv ID**: http://arxiv.org/abs/1807.04067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04067v1)
- **Published**: 2018-07-11 10:56:49+00:00
- **Updated**: 2018-07-11 10:56:49+00:00
- **Authors**: Muhammed Kocabas, Salih Karagoz, Emre Akbas
- **Comment**: to appear in ECCV 2018
- **Journal**: None
- **Summary**: In this paper, we present MultiPoseNet, a novel bottom-up multi-person pose estimation architecture that combines a multi-task model with a novel assignment method. MultiPoseNet can jointly handle person detection, keypoint detection, person segmentation and pose estimation problems. The novel assignment method is implemented by the Pose Residual Network (PRN) which receives keypoint and person detections, and produces accurate poses by assigning keypoints to person instances. On the COCO keypoints dataset, our pose estimation method outperforms all previous bottom-up methods both in accuracy (+4-point mAP over previous best result) and speed; it also performs on par with the best top-down methods while being at least 4x faster. Our method is the fastest real time system with 23 frames/sec. Source code is available at: https://github.com/mkocabas/pose-residual-network



### FINN-L: Library Extensions and Design Trade-off Analysis for Variable Precision LSTM Networks on FPGAs
- **Arxiv ID**: http://arxiv.org/abs/1807.04093v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.04093v1)
- **Published**: 2018-07-11 11:52:59+00:00
- **Updated**: 2018-07-11 11:52:59+00:00
- **Authors**: Vladimir Rybalkin, Alessandro Pappalardo, Muhammad Mohsin Ghaffar, Giulio Gambardella, Norbert Wehn, Michaela Blott
- **Comment**: Accepted for publication, 28th International Conference on Field
  Programmable Logic and Applications (FPL), August, 2018, Dublin, Ireland
- **Journal**: None
- **Summary**: It is well known that many types of artificial neural networks, including recurrent networks, can achieve a high classification accuracy even with low-precision weights and activations. The reduction in precision generally yields much more efficient hardware implementations in regards to hardware cost, memory requirements, energy, and achievable throughput. In this paper, we present the first systematic exploration of this design space as a function of precision for Bidirectional Long Short-Term Memory (BiLSTM) neural network. Specifically, we include an in-depth investigation of precision vs. accuracy using a fully hardware-aware training flow, where during training quantization of all aspects of the network including weights, input, output and in-memory cell activations are taken into consideration. In addition, hardware resource cost, power consumption and throughput scalability are explored as a function of precision for FPGA-based implementations of BiLSTM, and multiple approaches of parallelizing the hardware. We provide the first open source HLS library extension of FINN for parameterizable hardware architectures of LSTM layers on FPGAs which offers full precision flexibility and allows for parameterizable performance scaling offering different levels of parallelism within the architecture. Based on this library, we present an FPGA-based accelerator for BiLSTM neural network designed for optical character recognition, along with numerous other experimental proof points for a Zynq UltraScale+ XCZU7EV MPSoC within the given design space.



### Variational Capsules for Image Analysis and Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1807.04099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04099v1)
- **Published**: 2018-07-11 12:13:58+00:00
- **Updated**: 2018-07-11 12:13:58+00:00
- **Authors**: Huaibo Huang, Lingxiao Song, Ran He, Zhenan Sun, Tieniu Tan
- **Comment**: None
- **Journal**: None
- **Summary**: A capsule is a group of neurons whose activity vector models different properties of the same entity. This paper extends the capsule to a generative version, named variational capsules (VCs). Each VC produces a latent variable for a specific entity, making it possible to integrate image analysis and image synthesis into a unified framework. Variational capsules model an image as a composition of entities in a probabilistic model. Different capsules' divergence with a specific prior distribution represents the presence of different entities, which can be applied in image analysis tasks such as classification. In addition, variational capsules encode multiple entities in a semantically-disentangling way. Diverse instantiations of capsules are related to various properties of the same entity, making it easy to generate diverse samples with fine-grained semantic attributes. Extensive experiments demonstrate that deep networks designed with variational capsules can not only achieve promising performance on image analysis tasks (including image classification and attribute prediction) but can also improve the diversity and controllability of image synthesis.



### PDE-constrained LDDMM via geodesic shooting and inexact Gauss-Newton-Krylov optimization using the incremental adjoint Jacobi equations
- **Arxiv ID**: http://arxiv.org/abs/1807.04638v1
- **DOI**: 10.1088/1361-6560/aaf598
- **Categories**: **cs.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.04638v1)
- **Published**: 2018-07-11 14:08:16+00:00
- **Updated**: 2018-07-11 14:08:16+00:00
- **Authors**: Monica Hernandez
- **Comment**: None
- **Journal**: None
- **Summary**: The class of non-rigid registration methods proposed in the framework of PDE-constrained Large Deformation Diffeomorphic Metric Mapping is a particularly interesting family of physically meaningful diffeomorphic registration methods. Inexact Newton-Krylov optimization has shown an excellent numerical accuracy and an extraordinarily fast convergence rate in this framework. However, the Galerkin representation of the non-stationary velocity fields does not provide proper geodesic paths. In this work, we propose a method for PDE-constrained LDDMM parameterized in the space of initial velocity fields under the EPDiff equation. The derivation of the gradient and the Hessian-vector products are performed on the final velocity field and transported backward using the adjoint and the incremental adjoint Jacobi equations. This way, we avoid the complex dependence on the initial velocity field in the derivations and the computation of the adjoint equation and its incremental counterpart. The proposed method provides geodesics in the framework of PDE-constrained LDDMM, and it shows performance competitive to benchmark PDE-constrained LDDMM and EPDiff-LDDMM methods.



### Data-Driven Segmentation of Post-mortem Iris Images
- **Arxiv ID**: http://arxiv.org/abs/1807.04154v1
- **DOI**: 10.1109/IWBF.2018.8401558
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04154v1)
- **Published**: 2018-07-11 14:21:59+00:00
- **Updated**: 2018-07-11 14:21:59+00:00
- **Authors**: Mateusz Trokielewicz, Adam Czajka
- **Comment**: None
- **Journal**: 2018 International Workshop on Biometrics and Forensics (IWBF),
  IEEE Xplore, 2018
- **Summary**: This paper presents a method for segmenting iris images obtained from the deceased subjects, by training a deep convolutional neural network (DCNN) designed for the purpose of semantic segmentation. Post-mortem iris recognition has recently emerged as an alternative, or additional, method useful in forensic analysis. At the same time it poses many new challenges from the technological standpoint, one of them being the image segmentation stage, which has proven difficult to be reliably executed by conventional iris recognition methods. Our approach is based on the SegNet architecture, fine-tuned with 1,300 manually segmented post-mortem iris images taken from the Warsaw-BioBase-Post-Mortem-Iris v1.0 database. The experiments presented in this paper show that this data-driven solution is able to learn specific deformations present in post-mortem samples, which are missing from alive irises, and offers a considerable improvement over the state-of-the-art, conventional segmentation algorithm (OSIRIS): the Intersection over Union (IoU) metric was improved from 73.6% (for OSIRIS) to 83% (for DCNN-based presented in this paper) averaged over subject-disjoint, multiple splits of the data into train and test subsets. This paper offers the first known to us method of automatic processing of post-mortem iris images. We offer source codes with the trained DCNN that perform end-to-end segmentation of post-mortem iris images, as described in this paper. Also, we offer binary masks corresponding to manual segmentation of samples from Warsaw-BioBase-Post-Mortem-Iris v1.0 database to facilitate development of alternative methods for post-mortem iris segmentation.



### Underwater Image Haze Removal and Color Correction with an Underwater-ready Dark Channel Prior
- **Arxiv ID**: http://arxiv.org/abs/1807.04169v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04169v1)
- **Published**: 2018-07-11 14:43:57+00:00
- **Updated**: 2018-07-11 14:43:57+00:00
- **Authors**: Tomasz Łuczyński, Andreas Birk
- **Comment**: None
- **Journal**: None
- **Summary**: Underwater images suffer from extremely unfavourable conditions. Light is heavily attenuated and scattered. Attenuation creates change in hue, scattering causes so called veiling light. General state of the art methods for enhancing image quality are either unreliable or cannot be easily used in underwater operations. On the other hand there is a well known method for haze removal in air, called Dark Channel Prior. Even though there are known adaptations of this method to underwater applications, they do not always work correctly. This work elaborates and improves upon the initial concept presented in [1]. A modification to the Dark Channel Prior is proposed that allows for an easy application to underwater images. It is also shown that our method outperforms competing solutions based on the Dark Channel Prior. Experiments on real-life data collected within the DexROV project are also presented, showing the robustness and high performance of the proposed algorithm.



### Decision method choice in a human posture recognition context
- **Arxiv ID**: http://arxiv.org/abs/1807.04170v1
- **DOI**: 10.1007/978-3-319-62120-3_11
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.04170v1)
- **Published**: 2018-07-11 14:45:07+00:00
- **Updated**: 2018-07-11 14:45:07+00:00
- **Authors**: Stéphane Perrin, Eric Benoit, Didier Coquin
- **Comment**: None
- **Journal**: Human-Computer Systems Interaction. Backgrounds and Applications
  4, 4, 2018
- **Summary**: Human posture recognition provides a dynamic field that has produced many methods. Using fuzzy subsets based data fusion methods to aggregate the results given by different types of recognition processes is a convenient way to improve recognition methods. Nevertheless, choosing a defuzzification method to imple-ment the decision is a crucial point of this approach. The goal of this paper is to present an approach where the choice of the defuzzification method is driven by the constraints of the final data user, which are expressed as limitations on indica-tors like confidence or accuracy. A practical experimentation illustrating this ap-proach is presented: from a depth camera sensor, human posture is interpreted and the defuzzification method is selected in accordance with the constraints of the final information consumer. The paper illustrates the interest of the approach in a context of postures based human robot communication.



### A Computational Method for Evaluating UI Patterns
- **Arxiv ID**: http://arxiv.org/abs/1807.04191v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.04191v1)
- **Published**: 2018-07-11 15:21:59+00:00
- **Updated**: 2018-07-11 15:21:59+00:00
- **Authors**: Bardia Doosti, Tao Dong, Biplab Deka, Jeffrey Nichols
- **Comment**: None
- **Journal**: None
- **Summary**: UI design languages, such as Google's Material Design, make applications both easier to develop and easier to learn by providing a set of standard UI components. Nonetheless, it is hard to assess the impact of design languages in the wild. Moreover, designers often get stranded by strong-opinionated debates around the merit of certain UI components, such as the Floating Action Button and the Navigation Drawer. To address these challenges, this short paper introduces a method for measuring the impact of design languages and informing design debates through analyzing a dataset consisting of view hierarchies, screenshots, and app metadata for more than 9,000 mobile apps. Our data analysis shows that use of Material Design is positively correlated to app ratings, and to some extent, also the number of installs. Furthermore, we show that use of UI components vary by app category, suggesting a more nuanced view needed in design debates.



### With Friends Like These, Who Needs Adversaries?
- **Arxiv ID**: http://arxiv.org/abs/1807.04200v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04200v4)
- **Published**: 2018-07-11 15:38:33+00:00
- **Updated**: 2019-01-08 19:24:23+00:00
- **Authors**: Saumya Jetley, Nicholas A. Lord, Philip H. S. Torr
- **Comment**: Published in this form at NeurIPS 2018
- **Journal**: None
- **Summary**: The vulnerability of deep image classification networks to adversarial attack is now well known, but less well understood. Via a novel experimental analysis, we illustrate some facts about deep convolutional networks for image classification that shed new light on their behaviour and how it connects to the problem of adversaries. In short, the celebrated performance of these networks and their vulnerability to adversarial attack are simply two sides of the same coin: the input image-space directions along which the networks are most vulnerable to attack are the same directions which they use to achieve their classification performance in the first place. We develop this result in two main steps. The first uncovers the fact that classes tend to be associated with specific image-space directions. This is shown by an examination of the class-score outputs of nets as functions of 1D movements along these directions. This provides a novel perspective on the existence of universal adversarial perturbations. The second is a clear demonstration of the tight coupling between classification performance and vulnerability to adversarial attack within the spaces spanned by these directions. Thus, our analysis resolves the apparent contradiction between accuracy and vulnerability. It provides a new perspective on much of the prior art and reveals profound implications for efforts to construct neural nets that are both accurate and robust to adversarial attack.



### How Local is the Local Diversity? Reinforcing Sequential Determinantal Point Processes with Dynamic Ground Sets for Supervised Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1807.04219v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04219v4)
- **Published**: 2018-07-11 15:59:49+00:00
- **Updated**: 2018-08-24 02:06:38+00:00
- **Authors**: Yandong Li, Liqiang Wang, Tianbao Yang, Boqing Gong
- **Comment**: None
- **Journal**: European Conference on Computer Vision (ECCV 2018)
- **Summary**: The large volume of video content and high viewing frequency demand automatic video summarization algorithms, of which a key property is the capability of modeling diversity. If videos are lengthy like hours-long egocentric videos, it is necessary to track the temporal structures of the videos and enforce local diversity. The local diversity refers to that the shots selected from a short time duration are diverse but visually similar shots are allowed to co-exist in the summary if they appear far apart in the video. In this paper, we propose a novel probabilistic model, built upon SeqDPP, to dynamically control the time span of a video segment upon which the local diversity is imposed. In particular, we enable SeqDPP to learn to automatically infer how local the local diversity is supposed to be from the input video. The resulting model is extremely involved to train by the hallmark maximum likelihood estimation (MLE), which further suffers from the exposure bias and non-differentiable evaluation metrics. To tackle these problems, we instead devise a reinforcement learning algorithm for training the proposed model. Extensive experiments verify the advantages of our model and the new learning algorithm over MLE-based methods.



### LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System
- **Arxiv ID**: http://arxiv.org/abs/1807.10573v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.10573v1)
- **Published**: 2018-07-11 16:55:09+00:00
- **Updated**: 2018-07-11 16:55:09+00:00
- **Authors**: Pan Wei, Lucas Cagle, Tasmia Reza, John Ball, James Gafford
- **Comment**: 34 pages
- **Journal**: MDPI journal Electronics, 7(6), 84, May, 2018
- **Summary**: Collision avoidance is a critical task in many applications, such as ADAS (advanced driver-assistance systems), industrial automation and robotics. In an industrial automation setting, certain areas should be off limits to an automated vehicle for protection of people and high-valued assets. These areas can be quarantined by mapping (e.g., GPS) or via beacons that delineate a no-entry area. We propose a delineation method where the industrial vehicle utilizes a LiDAR {(Light Detection and Ranging)} and a single color camera to detect passive beacons and model-predictive control to stop the vehicle from entering a restricted space. The beacons are standard orange traffic cones with a highly reflective vertical pole attached. The LiDAR can readily detect these beacons, but suffers from false positives due to other reflective surfaces such as worker safety vests. Herein, we put forth a method for reducing false positive detection from the LiDAR by projecting the beacons in the camera imagery via a deep learning method and validating the detection using a neural network-learned projection from the camera to the LiDAR space. Experimental data collected at Mississippi State University's Center for Advanced Vehicular Systems (CAVS) shows the effectiveness of the proposed system in keeping the true detection while mitigating false positives.



### A Generic Approach to Lung Field Segmentation from Chest Radiographs using Deep Space and Shape Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.04339v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04339v1)
- **Published**: 2018-07-11 20:17:25+00:00
- **Updated**: 2018-07-11 20:17:25+00:00
- **Authors**: Awais Mansoor, Juan J. Cerrolaza, Geovanny Perez, Elijah Biggs, Kazunori Okada, Gustavo Nino, Marius George Linguraru
- **Comment**: None
- **Journal**: None
- **Summary**: Computer-aided diagnosis (CAD) techniques for lung field segmentation from chest radiographs (CXR) have been proposed for adult cohorts, but rarely for pediatric subjects. Statistical shape models (SSMs), the workhorse of most state-of-the-art CXR-based lung field segmentation methods, do not efficiently accommodate shape variation of the lung field during the pediatric developmental stages. The main contributions of our work are: (1) a generic lung field segmentation framework from CXR accommodating large shape variation for adult and pediatric cohorts; (2) a deep representation learning detection mechanism, \emph{ensemble space learning}, for robust object localization; and (3) \emph{marginal shape deep learning} for the shape deformation parameter estimation. Unlike the iterative approach of conventional SSMs, the proposed shape learning mechanism transforms the parameter space into marginal subspaces that are solvable efficiently using the recursive representation learning mechanism. Furthermore, our method is the first to include the challenging retro-cardiac region in the CXR-based lung segmentation for accurate lung capacity estimation. The framework is evaluated on 668 CXRs of patients between 3 month to 89 year of age. We obtain a mean Dice similarity coefficient of $0.96\pm0.03$ (including the retro-cardiac region). For a given accuracy, the proposed approach is also found to be faster than conventional SSM-based iterative segmentation methods. The computational simplicity of the proposed generic framework could be similarly applied to the fast segmentation of other deformable objects.



### A Reflectance Based Method For Shadow Detection and Removal
- **Arxiv ID**: http://arxiv.org/abs/1807.04352v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04352v1)
- **Published**: 2018-07-11 21:11:25+00:00
- **Updated**: 2018-07-11 21:11:25+00:00
- **Authors**: Sri Kalyan Yarlagadda, Fengqing Zhu
- **Comment**: Presented at the 2018 IEEE Southwest Symposium on Image Analysis and
  Interpretation
- **Journal**: None
- **Summary**: Shadows are common aspect of images and when left undetected can hinder scene understanding and visual processing. We propose a simple yet effective approach based on reflectance to detect shadows from single image. An image is first segmented and based on the reflectance, illumination and texture characteristics, segments pairs are identified as shadow and non-shadow pairs. The proposed method is tested on two publicly available and widely used datasets. Our method achieves higher accuracy in detecting shadows compared to previous reported methods despite requiring fewer parameters. We also show results of shadow-free images by relighting the pixels in the detected shadow regions.



### Deepwound: Automated Postoperative Wound Assessment and Surgical Site Surveillance through Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.04355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04355v1)
- **Published**: 2018-07-11 21:17:49+00:00
- **Updated**: 2018-07-11 21:17:49+00:00
- **Authors**: Varun Shenoy, Elizabeth Foster, Lauren Aalami, Bakar Majeed, Oliver Aalami
- **Comment**: 7 pages, 11 figures, 2 tables
- **Journal**: None
- **Summary**: Postoperative wound complications are a significant cause of expense for hospitals, doctors, and patients. Hence, an effective method to diagnose the onset of wound complications is strongly desired. Algorithmically classifying wound images is a difficult task due to the variability in the appearance of wound sites. Convolutional neural networks (CNNs), a subgroup of artificial neural networks that have shown great promise in analyzing visual imagery, can be leveraged to categorize surgical wounds. We present a multi-label CNN ensemble, Deepwound, trained to classify wound images using only image pixels and corresponding labels as inputs. Our final computational model can accurately identify the presence of nine labels: drainage, fibrinous exudate, granulation tissue, surgical site infection, open wound, staples, steri strips, and sutures. Our model achieves receiver operating curve (ROC) area under curve (AUC) scores, sensitivity, specificity, and F1 scores superior to prior work in this area. Smartphones provide a means to deliver accessible wound care due to their increasing ubiquity. Paired with deep neural networks, they offer the capability to provide clinical insight to assist surgeons during postoperative care. We also present a mobile application frontend to Deepwound that assists patients in tracking their wound and surgical recovery from the comfort of their home.



### A Trilateral Weighted Sparse Coding Scheme for Real-World Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1807.04364v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04364v1)
- **Published**: 2018-07-11 22:00:51+00:00
- **Updated**: 2018-07-11 22:00:51+00:00
- **Authors**: Jun Xu, Lei Zhang, David Zhang
- **Comment**: Accepted to ECCV 2018. 17 pages, not including supplemental material.
  Code will be published on https://github.com/csjunxu/TWSC-ECCV2018
- **Journal**: None
- **Summary**: Most of existing image denoising methods assume the corrupted noise to be additive white Gaussian noise (AWGN). However, the realistic noise in real-world noisy images is much more complex than AWGN, and is hard to be modelled by simple analytical distributions. As a result, many state-of-the-art denoising methods in literature become much less effective when applied to real-world noisy images captured by CCD or CMOS cameras. In this paper, we develop a trilateral weighted sparse coding (TWSC) scheme for robust real-world image denoising. Specifically, we introduce three weight matrices into the data and regularisation terms of the sparse coding framework to characterise the statistics of realistic noise and image priors. TWSC can be reformulated as a linear equality-constrained problem and can be solved by the alternating direction method of multipliers. The existence and uniqueness of the solution and convergence of the proposed algorithm are analysed. Extensive experiments demonstrate that the proposed TWSC scheme outperforms state-of-the-art denoising methods on removing realistic noise.



### State-of-the-art and gaps for deep learning on limited training data in remote sensing
- **Arxiv ID**: http://arxiv.org/abs/1807.11573v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.11573v1)
- **Published**: 2018-07-11 23:44:50+00:00
- **Updated**: 2018-07-11 23:44:50+00:00
- **Authors**: John E. Ball, Derek T. Anderson, Pan Wei
- **Comment**: arXiv admin note: text overlap with arXiv:1709.00308
- **Journal**: IGARSS June 2018
- **Summary**: Deep learning usually requires big data, with respect to both volume and variety. However, most remote sensing applications only have limited training data, of which a small subset is labeled. Herein, we review three state-of-the-art approaches in deep learning to combat this challenge. The first topic is transfer learning, in which some aspects of one domain, e.g., features, are transferred to another domain. The next is unsupervised learning, e.g., autoencoders, which operate on unlabeled data. The last is generative adversarial networks, which can generate realistic looking data that can fool the likes of both a deep learning network and human. The aim of this article is to raise awareness of this dilemma, to direct the reader to existing work and to highlight current gaps that need solving.



### Deep Learning Hyperspectral Image Classification Using Multiple Class-based Denoising Autoencoders, Mixed Pixel Training Augmentation, and Morphological Operations
- **Arxiv ID**: http://arxiv.org/abs/1807.10574v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.10574v1)
- **Published**: 2018-07-11 23:49:30+00:00
- **Updated**: 2018-07-11 23:49:30+00:00
- **Authors**: John E. Ball, Pan Wei
- **Comment**: None
- **Journal**: IGARSS, June 2018
- **Summary**: Herein, we present a system for hyperspectral image segmentation that utilizes multiple class--based denoising autoencoders which are efficiently trained. Moreover, we present a novel hyperspectral data augmentation method for labelled HSI data using linear mixtures of pixels from each class, which helps the system with edge pixels which are almost always mixed pixels. Finally, we utilize a deep neural network and morphological hole-filling to provide robust image classification. Results run on the Salinas dataset verify the high performance of the proposed algorithm.



