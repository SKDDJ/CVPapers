# Arxiv Papers in cs.CV on 2018-07-22
### NAVREN-RL: Learning to fly in real environment via end-to-end deep reinforcement learning using monocular images
- **Arxiv ID**: http://arxiv.org/abs/1807.08241v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.08241v1)
- **Published**: 2018-07-22 06:10:04+00:00
- **Updated**: 2018-07-22 06:10:04+00:00
- **Authors**: Malik Aqeel Anwar, Arijit Raychowdhury
- **Comment**: None
- **Journal**: None
- **Summary**: We present NAVREN-RL, an approach to NAVigate an unmanned aerial vehicle in an indoor Real ENvironment via end-to-end reinforcement learning RL. A suitable reward function is designed keeping in mind the cost and weight constraints for micro drone with minimum number of sensing modalities. Collection of small number of expert data and knowledge based data aggregation is integrated into the RL process to aid convergence. Experimentation is carried out on a Parrot AR drone in different indoor arenas and the results are compared with other baseline technologies. We demonstrate how the drone successfully avoids obstacles and navigates across different arenas.



### A Trace Lasso Regularized L1-norm Graph Cut for Highly Correlated Noisy Hyperspectral Image
- **Arxiv ID**: http://arxiv.org/abs/1807.10602v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.10602v1)
- **Published**: 2018-07-22 07:44:56+00:00
- **Updated**: 2018-07-22 07:44:56+00:00
- **Authors**: Ramanarayan Mohanty, S L Happy, Nilesh Suthar, Aurobinda Routray
- **Comment**: None
- **Journal**: None
- **Summary**: This work proposes an adaptive trace lasso regularized L1-norm based graph cut method for dimensionality reduction of Hyperspectral images, called as `Trace Lasso-L1 Graph Cut' (TL-L1GC). The underlying idea of this method is to generate the optimal projection matrix by considering both the sparsity as well as the correlation of the data samples. The conventional L2-norm used in the objective function is sensitive to noise and outliers. Therefore, in this work L1-norm is utilized as a robust alternative to L2-norm. Besides, for further improvement of the results, we use a penalty function of trace lasso with the L1GC method. It adaptively balances the L2-norm and L1-norm simultaneously by considering the data correlation along with the sparsity. We obtain the optimal projection matrix by maximizing the ratio of between-class dispersion to within-class dispersion using L1-norm with trace lasso as the penalty. Furthermore, an iterative procedure for this TL-L1GC method is proposed to solve the optimization function. The effectiveness of this proposed method is evaluated on two benchmark HSI datasets.



### Understanding hand-object manipulation by modeling the contextual relationship between actions, grasp types and object attributes
- **Arxiv ID**: http://arxiv.org/abs/1807.08254v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08254v1)
- **Published**: 2018-07-22 07:45:01+00:00
- **Updated**: 2018-07-22 07:45:01+00:00
- **Authors**: Minjie Cai, Kris Kitani, Yoichi Sato
- **Comment**: 14 pages, 13 figures
- **Journal**: None
- **Summary**: This paper proposes a novel method for understanding daily hand-object manipulation by developing computer vision-based techniques. Specifically, we focus on recognizing hand grasp types, object attributes and manipulation actions within an unified framework by exploring their contextual relationships. Our hypothesis is that it is necessary to jointly model hands, objects and actions in order to accurately recognize multiple tasks that are correlated to each other in hand-object manipulation. In the proposed model, we explore various semantic relationships between actions, grasp types and object attributes, and show how the context can be used to boost the recognition of each component. We also explore the spatial relationship between the hand and object in order to detect the manipulated object from hand in cluttered environment. Experiment results on all three recognition tasks show that our proposed method outperforms traditional appearance-based methods which are not designed to take into account contextual relationships involved in hand-object manipulation. The visualization and generalizability study of the learned context further supports our hypothesis.



### Deep Discriminative Model for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1807.08259v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08259v1)
- **Published**: 2018-07-22 08:46:02+00:00
- **Updated**: 2018-07-22 08:46:02+00:00
- **Authors**: Mohammad Tavakolian, Abdenour Hadid
- **Comment**: Accepted in ECCV 2018
- **Journal**: None
- **Summary**: This paper presents a new deep learning approach for video-based scene classification. We design a Heterogeneous Deep Discriminative Model (HDDM) whose parameters are initialized by performing an unsupervised pre-training in a layer-wise fashion using Gaussian Restricted Boltzmann Machines (GRBM). In order to avoid the redundancy of adjacent frames, we extract spatiotemporal variation patterns within frames and represent them sparsely using Sparse Cubic Symmetrical Pattern (SCSP). Then, a pre-initialized HDDM is separately trained using the videos of each class to learn class-specific models. According to the minimum reconstruction error from the learnt class-specific models, a weighted voting strategy is employed for the classification. The performance of the proposed method is extensively evaluated on two action recognition datasets; UCF101 and Hollywood II, and three dynamic texture and dynamic scene datasets; DynTex, YUPENN, and Maryland. The experimental results and comparisons against state-of-the-art methods demonstrate that the proposed method consistently achieves superior performance on all datasets.



### Macro-Micro Adversarial Network for Human Parsing
- **Arxiv ID**: http://arxiv.org/abs/1807.08260v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08260v2)
- **Published**: 2018-07-22 08:49:49+00:00
- **Updated**: 2018-07-25 07:47:46+00:00
- **Authors**: Yawei Luo, Zhedong Zheng, Liang Zheng, Tao Guan, Junqing Yu, Yi Yang
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: In human parsing, the pixel-wise classification loss has drawbacks in its low-level local inconsistency and high-level semantic inconsistency. The introduction of the adversarial network tackles the two problems using a single discriminator. However, the two types of parsing inconsistency are generated by distinct mechanisms, so it is difficult for a single discriminator to solve them both. To address the two kinds of inconsistencies, this paper proposes the Macro-Micro Adversarial Net (MMAN). It has two discriminators. One discriminator, Macro D, acts on the low-resolution label map and penalizes semantic inconsistency, e.g., misplaced body parts. The other discriminator, Micro D, focuses on multiple patches of the high-resolution label map to address the local inconsistency, e.g., blur and hole. Compared with traditional adversarial networks, MMAN not only enforces local and semantic consistency explicitly, but also avoids the poor convergence problem of adversarial networks when handling high resolution images. In our experiment, we validate that the two discriminators are complementary to each other in improving the human parsing accuracy. The proposed framework is capable of producing competitive parsing performance compared with the state-of-the-art methods, i.e., mIoU=46.81% and 59.91% on LIP and PASCAL-Person-Part, respectively. On a relatively small dataset PPSS, our pre-trained model demonstrates impressive generalization ability. The code is publicly available at https://github.com/RoyalVane/MMAN.



### RGBiD-SLAM for Accurate Real-time Localisation and 3D Mapping
- **Arxiv ID**: http://arxiv.org/abs/1807.08271v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08271v1)
- **Published**: 2018-07-22 11:05:24+00:00
- **Updated**: 2018-07-22 11:05:24+00:00
- **Authors**: Daniel Gutierrez-Gomez, Jose J. Guerrero
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a complete SLAM system for RGB-D cameras, namely RGB-iD SLAM. The presented approach is a dense direct SLAM method with the main characteristic of working with the depth maps in inverse depth parametrisation for the routines of dense alignment or keyframe fusion. The system consists in 2 CPU threads working in parallel, which share the use of the GPU for dense alignment and keyframe fusion routines. The first thread is a front-end operating at frame rate, which processes every incoming frame from the RGB-D sensor to compute the incremental odometry and integrate it in a keyframe which is changed periodically following a covisibility-based strategy. The second thread is a back-end which receives keyframes from the front-end. This thread is in charge of segmenting the keyframes based on their structure, describing them using Bags of Words, trying to find potential loop closures with previous keyframes, and in such case perform pose-graph optimisation for trajectory correction. In addition, our system allows is able to compute the odometry both with unregistered and registered depth maps, allowing to use customised calibrations of the RGB-D sensor. As a consequence in the paper we also propose a detailed calibration pipeline to compute customised calibrations for particular RGB-D cameras. The experiments with our approach in the TUM RGB-D benchmark datasets show results superior in accuracy to the state-of-the-art in many of the sequences. The code has been made available on-line for research purposes https://github.com/dangut/RGBiD-SLAM.



### FastOrient: Lightweight Computer Vision for Wrist Control in Assistive Robotic Grasping
- **Arxiv ID**: http://arxiv.org/abs/1807.08275v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.08275v1)
- **Published**: 2018-07-22 11:48:30+00:00
- **Updated**: 2018-07-22 11:48:30+00:00
- **Authors**: Mireia Ruiz Maymo, Ali Shafti, A. Aldo Faisal
- **Comment**: 6 pages. Accepted for publication at IEEE BioRob 2018
- **Journal**: None
- **Summary**: Wearable and Assistive robotics for human grasp support are broadly either tele-operated robotic arms or act through orthotic control of a paralyzed user's hand. Such devices require correct orientation for successful and efficient grasping. In many human-robot assistive settings, the end-user is required to explicitly control the many degrees of freedom making effective or efficient control problematic. Here we are demonstrating the off-loading of low-level control of assistive robotics and active orthotics, through automatic end-effector orientation control for grasping. This paper describes a compact algorithm implementing fast computer vision techniques to obtain the orientation of the target object to be grasped, by segmenting the images acquired with a camera positioned on top of the end-effector of the robotic device. The rotation needed that optimises grasping is directly computed from the object's orientation. The algorithm has been evaluated in 6 different scene backgrounds and end-effector approaches to 26 different objects. 94.8% of the objects were detected in all backgrounds. Grasping of the object was achieved in 91.1% of the cases and has been evaluated with a robot simulator confirming the performance of the algorithm.



### Predicting breast tumor proliferation from whole-slide images: the TUPAC16 challenge
- **Arxiv ID**: http://arxiv.org/abs/1807.08284v2
- **DOI**: 10.1016/j.media.2019.02.012
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08284v2)
- **Published**: 2018-07-22 13:46:03+00:00
- **Updated**: 2019-03-29 15:39:03+00:00
- **Authors**: Mitko Veta, Yujing J. Heng, Nikolas Stathonikos, Babak Ehteshami Bejnordi, Francisco Beca, Thomas Wollmann, Karl Rohr, Manan A. Shah, Dayong Wang, Mikael Rousson, Martin Hedlund, David Tellez, Francesco Ciompi, Erwan Zerhouni, David Lanyi, Matheus Viana, Vassili Kovalev, Vitali Liauchuk, Hady Ahmady Phoulady, Talha Qaiser, Simon Graham, Nasir Rajpoot, Erik Sjöblom, Jesper Molin, Kyunghyun Paeng, Sangheum Hwang, Sunggyun Park, Zhipeng Jia, Eric I-Chao Chang, Yan Xu, Andrew H. Beck, Paul J. van Diest, Josien P. W. Pluim
- **Comment**: Overview paper of the TUPAC16 challenge: http://tupac.tue-image.nl/
- **Journal**: None
- **Summary**: Tumor proliferation is an important biomarker indicative of the prognosis of breast cancer patients. Assessment of tumor proliferation in a clinical setting is highly subjective and labor-intensive task. Previous efforts to automate tumor proliferation assessment by image analysis only focused on mitosis detection in predefined tumor regions. However, in a real-world scenario, automatic mitosis detection should be performed in whole-slide images (WSIs) and an automatic method should be able to produce a tumor proliferation score given a WSI as input. To address this, we organized the TUmor Proliferation Assessment Challenge 2016 (TUPAC16) on prediction of tumor proliferation scores from WSIs. The challenge dataset consisted of 500 training and 321 testing breast cancer histopathology WSIs. In order to ensure fair and independent evaluation, only the ground truth for the training dataset was provided to the challenge participants. The first task of the challenge was to predict mitotic scores, i.e., to reproduce the manual method of assessing tumor proliferation by a pathologist. The second task was to predict the gene expression based PAM50 proliferation scores from the WSI. The best performing automatic method for the first task achieved a quadratic-weighted Cohen's kappa score of $\kappa$ = 0.567, 95% CI [0.464, 0.671] between the predicted scores and the ground truth. For the second task, the predictions of the top method had a Spearman's correlation coefficient of r = 0.617, 95% CI [0.581 0.651] with the ground truth. This was the first study that investigated tumor proliferation assessment from WSIs. The achieved results are promising given the difficulty of the tasks and weakly-labelled nature of the ground truth. However, further research is needed to improve the practical utility of image analysis methods for this task.



### Correlation Net: Spatiotemporal multimodal deep learning for action recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.08291v6
- **DOI**: 10.1016/j.image.2019.115731
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08291v6)
- **Published**: 2018-07-22 14:48:32+00:00
- **Updated**: 2019-12-16 06:57:10+00:00
- **Authors**: Novanto Yudistira, Takio Kurita
- **Comment**: None
- **Journal**: Signal Processing: Image Communication, Volume 82, March 2020,
  115731
- **Summary**: This paper describes a network that captures multimodal correlations over arbitrary timestamps. The proposed scheme operates as a complementary, extended network over a multimodal convolutional neural network (CNN). Spatial and temporal streams are required for action recognition by a deep CNN, but overfitting reduction and fusing these two streams remain open problems. The existing fusion approach averages the two streams. Here we propose a correlation network with a Shannon fusion for learning a pre-trained CNN. A Long-range video may consist of spatiotemporal correlations over arbitrary times, which can be captured by forming the correlation network from simple fully connected layers. This approach was found to complement the existing network fusion methods. The importance of multimodal correlation is validated in comparison experiments on the UCF-101 and HMDB-51 datasets. The multimodal correlation enhanced the accuracy of the video recognition results.



### Skin Lesion Analysis Towards Melanoma Detection via End-to-end Deep Learning of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.08332v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08332v1)
- **Published**: 2018-07-22 18:07:50+00:00
- **Updated**: 2018-07-22 18:07:50+00:00
- **Authors**: Katherine M. Li, Evelyn C. Li
- **Comment**: None
- **Journal**: None
- **Summary**: This article presents the design, experiments and results of our solution submitted to the 2018 ISIC challenge: Skin Lesion Analysis Towards Melanoma Detection. We design a pipeline using state-of-the-art Convolutional Neural Network (CNN) models for a Lesion Boundary Segmentation task and a Lesion Diagnosis task.



### AutoLoc: Weakly-supervised Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1807.08333v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08333v2)
- **Published**: 2018-07-22 18:14:45+00:00
- **Updated**: 2018-12-16 19:37:02+00:00
- **Authors**: Zheng Shou, Hang Gao, Lei Zhang, Kazuyuki Miyazawa, Shih-Fu Chang
- **Comment**: Accepted by ECCV'18
- **Journal**: None
- **Summary**: Temporal Action Localization (TAL) in untrimmed video is important for many applications. But it is very expensive to annotate the segment-level ground truth (action class and temporal boundary). This raises the interest of addressing TAL with weak supervision, namely only video-level annotations are available during training). However, the state-of-the-art weakly-supervised TAL methods only focus on generating good Class Activation Sequence (CAS) over time but conduct simple thresholding on CAS to localize actions. In this paper, we first develop a novel weakly-supervised TAL framework called AutoLoc to directly predict the temporal boundary of each action instance. We propose a novel Outer-Inner-Contrastive (OIC) loss to automatically discover the needed segment-level supervision for training such a boundary predictor. Our method achieves dramatically improved performance: under the IoU threshold 0.5, our method improves mAP on THUMOS'14 from 13.7% to 21.2% and mAP on ActivityNet from 7.4% to 27.3%. It is also very encouraging to see that our weakly-supervised method achieves comparable results with some fully-supervised methods.



### A Statistical Method for Object Counting
- **Arxiv ID**: http://arxiv.org/abs/1807.08335v1
- **DOI**: 10.1145/3121360.3121364
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08335v1)
- **Published**: 2018-07-22 18:25:23+00:00
- **Updated**: 2018-07-22 18:25:23+00:00
- **Authors**: Jans Glagolevs, Karlis Freivalds
- **Comment**: None
- **Journal**: In Proceedings of the International Conference on Graphics and
  Signal Processing (ICGSP '17). ACM, New York, NY, USA, 61-64 (2017)
- **Summary**: In this paper we present a new object counting method that is intended for counting similarly sized and mostly round objects. Unlike many other algorithms of the same purpose, the proposed method does not rely on identifying every object, it uses statistical data obtained from the image instead. The method is evaluated on images with human bone cells, oranges and pills achieving good accuracy. Its strengths are ability to deal with touching and partly overlapping objects, ability to work with different kinds of objects without prior configuration and good performance.



### Modeling Brain Networks with Artificial Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.08368v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08368v1)
- **Published**: 2018-07-22 21:04:46+00:00
- **Updated**: 2018-07-22 21:04:46+00:00
- **Authors**: Baran Baris Kivilcim, Itir Onal Ertugrul, Fatos T. Yarman Vural
- **Comment**: Accepted to 2nd Workshop on GRaphs in biomedicAl Image anaLysis,
  MICCAI 2018
- **Journal**: None
- **Summary**: In this study, we propose a neural network approach to capture the functional connectivities among anatomic brain regions. The suggested approach estimates a set of brain networks, each of which represents the connectivity patterns of a cognitive process. We employ two different architectures of neural networks to extract directed and undirected brain networks from functional Magnetic Resonance Imaging (fMRI) data. Then, we use the edge weights of the estimated brain networks to train a classifier, namely, Support Vector Machines(SVM) to label the underlying cognitive process. We compare our brain network models with popular models, which generate similar functional brain networks. We observe that both undirected and directed brain networks surpass the performances of the network models used in the fMRI literature. We also observe that directed brain networks offer more discriminative features compared to the undirected ones for recognizing the cognitive processes. The representation power of the suggested brain networks are tested in a task-fMRI dataset of Human Connectome Project and a Complex Problem Solving dataset.



### SiGAN: Siamese Generative Adversarial Network for Identity-Preserving Face Hallucination
- **Arxiv ID**: http://arxiv.org/abs/1807.08370v1
- **DOI**: 10.1109/TIP.2019.2924554
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08370v1)
- **Published**: 2018-07-22 21:18:38+00:00
- **Updated**: 2018-07-22 21:18:38+00:00
- **Authors**: Chih-Chung Hsu, Chia-Wen Lin, Weng-Tai Su, Gene Cheung
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Despite generative adversarial networks (GANs) can hallucinate photo-realistic high-resolution (HR) faces from low-resolution (LR) faces, they cannot guarantee preserving the identities of hallucinated HR faces, making the HR faces poorly recognizable. To address this problem, we propose a Siamese GAN (SiGAN) to reconstruct HR faces that visually resemble their corresponding identities. On top of a Siamese network, the proposed SiGAN consists of a pair of two identical generators and one discriminator. We incorporate reconstruction error and identity label information in the loss function of SiGAN in a pairwise manner. By iteratively optimizing the loss functions of the generator pair and discriminator of SiGAN, we cannot only achieve photo-realistic face reconstruction, but also ensures the reconstructed information is useful for identity recognition. Experimental results demonstrate that SiGAN significantly outperforms existing face hallucination GANs in objective face verification performance, while achieving photo-realistic reconstruction. Moreover, for input LR faces from unknown identities who are not included in training, SiGAN can still do a good job.



### Towards Privacy-Preserving Visual Recognition via Adversarial Training: A Pilot Study
- **Arxiv ID**: http://arxiv.org/abs/1807.08379v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08379v2)
- **Published**: 2018-07-22 22:30:56+00:00
- **Updated**: 2020-10-22 23:05:53+00:00
- **Authors**: Zhenyu Wu, Zhangyang Wang, Zhaowen Wang, Hailin Jin
- **Comment**: A significant extension of this paper is accepted by TPAMI-20. A
  conference version of this paper is accepted by ECCV-18. A shorter version of
  this paper is accepted by ICML-18 PiMLAI workshop
- **Journal**: None
- **Summary**: This paper aims to improve privacy-preserving visual recognition, an increasingly demanded feature in smart camera applications, by formulating a unique adversarial training framework. The proposed framework explicitly learns a degradation transform for the original video inputs, in order to optimize the trade-off between target task performance and the associated privacy budgets on the degraded video. A notable challenge is that the privacy budget, often defined and measured in task-driven contexts, cannot be reliably indicated using any single model performance, because a strong protection of privacy has to sustain against any possible model that tries to hack privacy information. Such an uncommon situation has motivated us to propose two strategies, i.e., budget model restarting and ensemble, to enhance the generalization of the learned degradation on protecting privacy against unseen hacker models. Novel training strategies, evaluation protocols, and result visualization methods have been designed accordingly. Two experiments on privacy-preserving action recognition, with privacy budgets defined in various ways, manifest the compelling effectiveness of the proposed framework in simultaneously maintaining high target task (action recognition) performance while suppressing the privacy breach risk.



### Pedestrian Trajectory Prediction with Structured Memory Hierarchies
- **Arxiv ID**: http://arxiv.org/abs/1807.08381v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08381v1)
- **Published**: 2018-07-22 23:17:12+00:00
- **Updated**: 2018-07-22 23:17:12+00:00
- **Authors**: Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: To appear in ECML-PKDD 2018
- **Journal**: None
- **Summary**: This paper presents a novel framework for human trajectory prediction based on multimodal data (video and radar). Motivated by recent neuroscience discoveries, we propose incorporating a structured memory component in the human trajectory prediction pipeline to capture historical information to improve performance. We introduce structured LSTM cells for modelling the memory content hierarchically, preserving the spatiotemporal structure of the information and enabling us to capture both short-term and long-term context. We demonstrate how this architecture can be extended to integrate salient information from multiple modalities to automatically store and retrieve important information for decision making without any supervision. We evaluate the effectiveness of the proposed models on a novel multimodal dataset that we introduce, consisting of 40,000 pedestrian trajectories, acquired jointly from a radar system and a CCTV camera system installed in a public place. The performance is also evaluated on the publicly available New York Grand Central pedestrian database. In both settings, the proposed models demonstrate their capability to better anticipate future pedestrian motion compared to existing state of the art.



### Real-Time 2D-3D Deformable Registration with Deep Learning and Application to Lung Radiotherapy Targeting
- **Arxiv ID**: http://arxiv.org/abs/1807.08388v2
- **DOI**: 10.1007/978-3-030-20351-1_20
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.08388v2)
- **Published**: 2018-07-22 23:45:34+00:00
- **Updated**: 2019-09-26 01:49:30+00:00
- **Authors**: Markus D. Foote, Blake E. Zimmerman, Amit Sawant, Sarang Joshi
- **Comment**: None
- **Journal**: IPMI 2019. Lecture Notes in Computer Science, vol 11492. Springer,
  Cham (2019)
- **Summary**: Radiation therapy presents a need for dynamic tracking of a target tumor volume. Fiducial markers such as implanted gold seeds have been used to gate radiation delivery but the markers are invasive and gating significantly increases treatment time. Pretreatment acquisition of a respiratory correlated 4DCT allows for determination of accurate motion tracking which is useful in treatment planning. We design a patient-specific motion subspace and a deep convolutional neural network to recover anatomical positions from a single fluoroscopic projection in real-time. We use this deep network to approximate the nonlinear inverse of a diffeomorphic deformation composed with radiographic projection. This network recovers subspace coordinates to define the patient-specific deformation of the lungs from a baseline anatomic position. The geometric accuracy of the subspace deformations on real patient data is similar to accuracy attained by original image registration between individual respiratory-phase image volumes.



