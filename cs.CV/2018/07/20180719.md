# Arxiv Papers in cs.CV on 2018-07-19
### Few-Shot Adaptation for Multimedia Semantic Indexing
- **Arxiv ID**: http://arxiv.org/abs/1807.07203v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.07203v1)
- **Published**: 2018-07-19 00:58:33+00:00
- **Updated**: 2018-07-19 00:58:33+00:00
- **Authors**: Nakamasa Inoue, Koichi Shinoda
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a few-shot adaptation framework, which bridges zero-shot learning and supervised many-shot learning, for semantic indexing of image and video data. Few-shot adaptation provides robust parameter estimation with few training examples, by optimizing the parameters of zero-shot learning and supervised many-shot learning simultaneously. In this method, first we build a zero-shot detector, and then update it by using the few examples. Our experiments show the effectiveness of the proposed framework on three datasets: TRECVID Semantic Indexing 2010, 2014, and ImageNET. On the ImageNET dataset, we show that our method outperforms recent few-shot learning methods. On the TRECVID 2014 dataset, we achieve 15.19% and 35.98% in Mean Average Precision under the zero-shot condition and the supervised condition, respectively. To the best of our knowledge, these are the best results on this dataset.



### Monocular Object Orientation Estimation using Riemannian Regression and Classification Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.07226v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07226v1)
- **Published**: 2018-07-19 03:28:30+00:00
- **Updated**: 2018-07-19 03:28:30+00:00
- **Authors**: Siddharth Mahendran, Ming Yang Lu, Haider Ali, René Vidal
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the task of estimating the 3D orientation of an object of known category given an image of the object and a bounding box around it. Recently, CNN-based regression and classification methods have shown significant performance improvements for this task. This paper proposes a new CNN-based approach to monocular orientation estimation that advances the state of the art in four different directions. First, we take into account the Riemannian structure of the orientation space when designing regression losses and nonlinear activation functions. Second, we propose a mixed Riemannian regression and classification framework that better handles the challenging case of nearly symmetric objects. Third, we propose a data augmentation strategy that is specifically designed to capture changes in 3D orientation. Fourth, our approach leads to state-of-the-art results on the PASCAL3D+ dataset.



### ArticulatedFusion: Real-time Reconstruction of Motion, Geometry and Segmentation Using a Single Depth Camera
- **Arxiv ID**: http://arxiv.org/abs/1807.07243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07243v1)
- **Published**: 2018-07-19 05:38:40+00:00
- **Updated**: 2018-07-19 05:38:40+00:00
- **Authors**: Chao Li, Zheheng Zhao, Xiaohu Guo
- **Comment**: European Conference on Computer Vision 2018
- **Journal**: None
- **Summary**: This paper proposes a real-time dynamic scene reconstruction method capable of reproducing the motion, geometry, and segmentation simultaneously given live depth stream from a single RGB-D camera. Our approach fuses geometry frame by frame and uses a segmentation-enhanced node graph structure to drive the deformation of geometry in registration step. A two-level node motion optimization is proposed. The optimization space of node motions and the range of physically-plausible deformations are largely reduced by taking advantage of the articulated motion prior, which is solved by an efficient node graph segmentation method. Compared to previous fusion-based dynamic scene reconstruction methods, our experiments show robust and improved reconstruction results for tangential and occluded motions.



### Chest X-rays Classification: A Multi-Label and Fine-Grained Problem
- **Arxiv ID**: http://arxiv.org/abs/1807.07247v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07247v3)
- **Published**: 2018-07-19 06:02:54+00:00
- **Updated**: 2018-07-24 22:15:49+00:00
- **Authors**: Zongyuan Ge, Dwarikanath Mahapatra, Suman Sedai, Rahil Garnavi, Rajib Chakravorty
- **Comment**: None
- **Journal**: None
- **Summary**: The widely used ChestX-ray14 dataset addresses an important medical image classification problem and has the following caveats: 1) many lung pathologies are visually similar, 2) a variant of diseases including lung cancer, tuberculosis, and pneumonia are present in a single scan, i.e. multiple labels and 3) The incidence of healthy images is much larger than diseased samples, creating imbalanced data. These properties are common in medical domain. Existing literature uses stateof- the-art DensetNet/Resnet models being transfer learned where output neurons of the networks are trained for individual diseases to cater for multiple diseases labels in each image. However, most of them don't consider relationship between multiple classes. In this work we have proposed a novel error function, Multi-label Softmax Loss (MSML), to specifically address the properties of multiple labels and imbalanced data. Moreover, we have designed deep network architecture based on fine-grained classification concept that incorporates MSML. We have evaluated our proposed method on various network backbones and showed consistent performance improvements of AUC-ROC scores on the ChestX-ray14 dataset. The proposed error function provides a new method to gain improved performance across wider medical datasets.



### Visual Domain Adaptation with Manifold Embedded Distribution Alignment
- **Arxiv ID**: http://arxiv.org/abs/1807.07258v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.07258v2)
- **Published**: 2018-07-19 06:45:42+00:00
- **Updated**: 2018-07-28 08:15:38+00:00
- **Authors**: Jindong Wang, Wenjie Feng, Yiqiang Chen, Han Yu, Meiyu Huang, Philip S. Yu
- **Comment**: ACM Multimedia conference 2018 (ACM MM) ORAL paper; top 10 papers; 9
  pages; code available at http://transferlearning.xyz
- **Journal**: None
- **Summary**: Visual domain adaptation aims to learn robust classifiers for the target domain by leveraging knowledge from a source domain. Existing methods either attempt to align the cross-domain distributions, or perform manifold subspace learning. However, there are two significant challenges: (1) degenerated feature transformation, which means that distribution alignment is often performed in the original feature space, where feature distortions are hard to overcome. On the other hand, subspace learning is not sufficient to reduce the distribution divergence. (2) unevaluated distribution alignment, which means that existing distribution alignment methods only align the marginal and conditional distributions with equal importance, while they fail to evaluate the different importance of these two distributions in real applications. In this paper, we propose a Manifold Embedded Distribution Alignment (MEDA) approach to address these challenges. MEDA learns a domain-invariant classifier in Grassmann manifold with structural risk minimization, while performing dynamic distribution alignment to quantitatively account for the relative importance of marginal and conditional distributions. To the best of our knowledge, MEDA is the first attempt to perform dynamic distribution alignment for manifold domain adaptation. Extensive experiments demonstrate that MEDA shows significant improvements in classification accuracy compared to state-of-the-art traditional and deep methods.



### In pixels we trust: From Pixel Labeling to Object Localization and Scene Categorization
- **Arxiv ID**: http://arxiv.org/abs/1807.07284v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.07284v1)
- **Published**: 2018-07-19 08:28:32+00:00
- **Updated**: 2018-07-19 08:28:32+00:00
- **Authors**: Carlos Herranz-Perdiguero, Carolina Redondo-Cabrera, Roberto J. López-Sastre
- **Comment**: IROS 2018
- **Journal**: None
- **Summary**: While there has been significant progress in solving the problems of image pixel labeling, object detection and scene classification, existing approaches normally address them separately. In this paper, we propose to tackle these problems from a bottom-up perspective, where we simply need a semantic segmentation of the scene as input. We employ the DeepLab architecture, based on the ResNet deep network, which leverages multi-scale inputs to later fuse their responses to perform a precise pixel labeling of the scene. This semantic segmentation mask is used to localize the objects and to recognize the scene, following two simple yet effective strategies. We evaluate the benefits of our solutions, performing a thorough experimental evaluation on the NYU Depth V2 dataset. Our approach achieves a performance that beats the leading results by a significant margin, defining the new state of the art in this benchmark for the three tasks comprising the scene understanding: semantic segmentation, object detection and scene categorization.



### Operator-in-the-Loop Deep Sequential Multi-camera Feature Fusion for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1807.07295v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07295v4)
- **Published**: 2018-07-19 08:52:19+00:00
- **Updated**: 2019-12-05 16:35:00+00:00
- **Authors**: K L Navaneet, Ravi Kiran Sarvadevabhatla, Shashank Shekhar, R. Venkatesh Babu, Anirban Chakraborty
- **Comment**: Accepted at IEEE Transactions on Information Forensics & Security
- **Journal**: None
- **Summary**: Given a target image as query, person re-identification systems retrieve a ranked list of candidate matches on a per-camera basis. In deployed systems, a human operator scans these lists and labels sighted targets by touch or mouse-based selection. However, classical re-id approaches generate per-camera lists independently. Therefore, target identifications by operator in a subset of cameras cannot be utilized to improve ranking of the target in remaining set of network cameras. To address this shortcoming, we propose a novel sequential multi-camera re-id approach. The proposed approach can accommodate human operator inputs and provides early gains via a monotonic improvement in target ranking. At the heart of our approach is a fusion function which operates on deep feature representations of query and candidate matches. We formulate an optimization procedure custom-designed to incrementally improve query representation. Since existing evaluation methods cannot be directly adopted to our setting, we also propose two novel evaluation protocols. The results on two large-scale re-id datasets (Market-1501, DukeMTMC-reID) demonstrate that our multi-camera method significantly outperforms baselines and other popular feature fusion schemes. Additionally, we conduct a comparative subject-based study of human operator performance. The superior operator performance enabled by our approach makes a compelling case for its integration into deployable video-surveillance systems.



### Attend and Rectify: a Gated Attention Mechanism for Fine-Grained Recovery
- **Arxiv ID**: http://arxiv.org/abs/1807.07320v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07320v2)
- **Published**: 2018-07-19 09:52:36+00:00
- **Updated**: 2018-07-24 09:23:39+00:00
- **Authors**: Pau Rodríguez, Josep M. Gonfaus, Guillem Cucurull, F. Xavier Roca, Jordi Gonzàlez
- **Comment**: Published at ECCV2018
- **Journal**: None
- **Summary**: We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. It learns to attend to lower-level feature activations without requiring part annotations and uses these activations to update and rectify the output likelihood distribution. In contrast to other approaches, the proposed mechanism is modular, architecture-independent and efficient both in terms of parameters and computation required. Experiments show that networks augmented with our approach systematically improve their classification accuracy and become more robust to clutter. As a result, Wide Residual Networks augmented with our proposal surpasses the state of the art classification accuracies in CIFAR-10, the Adience gender recognition task, Stanford dogs, and UEC Food-100.



### Deep Adaptive Proposal Network for Object Detection in Optical Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/1807.07327v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07327v1)
- **Published**: 2018-07-19 10:10:30+00:00
- **Updated**: 2018-07-19 10:10:30+00:00
- **Authors**: Lin Cheng, Xu Liu, Lingling Li, Licheng Jiao, Xu Tang
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Object detection is a fundamental and challenging problem in aerial and satellite image analysis. More recently, a two-stage detector Faster R-CNN is proposed and demonstrated to be a promising tool for object detection in optical remote sensing images, while the sparse and dense characteristic of objects in remote sensing images is complexity. It is unreasonable to treat all images with the same region proposal strategy, and this treatment limits the performance of two-stage detectors. In this paper, we propose a novel and effective approach, named deep adaptive proposal network (DAPNet), address this complexity characteristic of object by learning a new category prior network (CPN) on the basis of the existing Faster R-CNN architecture. Moreover, the candidate regions produced by DAPNet model are different from the traditional region proposal network (RPN), DAPNet predicts the detail category of each candidate region. And these candidate regions combine the object number, which generated by the category prior network to achieve a suitable number of candidate boxes for each image. These candidate boxes can satisfy detection tasks in sparse and dense scenes. The performance of the proposed framework has been evaluated on the challenging NWPU VHR-10 data set. Experimental results demonstrate the superiority of the proposed framework to the state-of-the-art.



### Automated Phenotyping of Epicuticular Waxes of Grapevine Berries Using Light Separation and Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.07343v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07343v3)
- **Published**: 2018-07-19 11:17:08+00:00
- **Updated**: 2018-09-06 12:10:40+00:00
- **Authors**: Pierre Barré, Katja Herzog, Rebecca Höfle, Matthias B. Hullin, Reinhard Töpfer, Volker Steinhage
- **Comment**: None
- **Journal**: None
- **Summary**: In viticulture the epicuticular wax as the outer layer of the berry skin is known as trait which is correlated to resilience towards Botrytis bunch rot. Traditionally this trait is classified using the OIV descriptor 227 (berry bloom) in a time consuming way resulting in subjective and error-prone phenotypic data. In the present study an objective, fast and sensor-based approach was developed to monitor berry bloom. From the technical point-of-view, it is known that the measurement of different illumination components conveys important information about observed object surfaces. A Mobile Light-Separation-Lab is proposed in order to capture illumination-separated images of grapevine berries for phenotyping the distribution of epicuticular waxes (berry bloom). For image analysis, an efficient convolutional neural network approach is used to derive the uniformity and intactness of waxes on berries. Method validation over six grapevine cultivars shows accuracies up to $97.3$%. In addition, electrical impedance of the cuticle and its epicuticular waxes (described as an indicator for the thickness of berry skin and its permeability) was correlated to the detected proportion of waxes with $r=0.76$. This novel, fast and non-invasive phenotyping approach facilitates enlarged screenings within grapevine breeding material and genetic repositories regarding berry bloom characteristics and its impact on resilience towards Botrytis bunch rot.



### Generative Adversarial Networks for MR-CT Deformable Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1807.07349v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07349v1)
- **Published**: 2018-07-19 11:41:07+00:00
- **Updated**: 2018-07-19 11:41:07+00:00
- **Authors**: Christine Tanner, Firat Ozdemir, Romy Profanter, Valeriy Vishnevsky, Ender Konukoglu, Orcun Goksel
- **Comment**: None
- **Journal**: None
- **Summary**: Deformable Image Registration (DIR) of MR and CT images is one of the most challenging registration task, due to the inherent structural differences of the modalities and the missing dense ground truth. Recently cycle Generative Adversarial Networks (cycle-GANs) have been used to learn the intensity relationship between these 2 modalities for unpaired brain data. Yet its usefulness for DIR was not assessed.   In this study we evaluate the DIR performance for thoracic and abdominal organs after synthesis by cycle-GAN. We show that geometric changes, which differentiate the two populations (e.g. inhale vs. exhale), are readily synthesized as well. This causes substantial problems for any application which relies on spatial correspondences being preserved between the real and the synthesized image (e.g. plan, segmentation, landmark propagation). To alleviate this problem, we investigated reducing the spatial information provided to the discriminator by decreasing the size of its receptive fields.   Image synthesis was learned from 17 unpaired subjects per modality. Registration performance was evaluated with respect to manual segmentations of 11 structures for 3 subjects from the VISERAL challenge. State-of-the-art DIR methods based on Normalized Mutual Information (NMI), Modality Independent Neighborhood Descriptor (MIND) and their novel combination achieved a mean segmentation overlap ratio of 76.7, 67.7, 76.9%, respectively. This dropped to 69.1% or less when registering images synthesized by cycle-GAN based on local correlation, due to the poor performance on the thoracic region, where large lung volume changes were synthesized. Performance for the abdominal region was similar to that of CT-MRI NMI registration (77.4 vs. 78.8%) when using 3D synthesizing MRIs (12 slices) and medium sized receptive fields for the discriminator.



### Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1807.07356v3
- **DOI**: 10.1016/j.neucom.2019.01.103
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07356v3)
- **Published**: 2018-07-19 11:58:14+00:00
- **Updated**: 2019-01-31 06:07:11+00:00
- **Authors**: Guotai Wang, Wenqi Li, Michael Aertsen, Jan Deprest, Sebastien Ourselin, Tom Vercauteren
- **Comment**: 13 pages, 8 figures, accepted by NeuroComputing
- **Journal**: None
- **Summary**: Despite the state-of-the-art performance for medical image segmentation, deep convolutional neural networks (CNNs) have rarely provided uncertainty estimations regarding their segmentation outputs, e.g., model (epistemic) and image-based (aleatoric) uncertainties. In this work, we analyze these different types of uncertainties for CNN-based 2D and 3D medical image segmentation tasks. We additionally propose a test-time augmentation-based aleatoric uncertainty to analyze the effect of different transformations of the input image on the segmentation output. Test-time augmentation has been previously used to improve segmentation accuracy, yet not been formulated in a consistent mathematical framework. Hence, we also propose a theoretical formulation of test-time augmentation, where a distribution of the prediction is estimated by Monte Carlo simulation with prior distributions of parameters in an image acquisition model that involves image transformations and noise. We compare and combine our proposed aleatoric uncertainty with model uncertainty. Experiments with segmentation of fetal brains and brain tumors from 2D and 3D Magnetic Resonance Images (MRI) showed that 1) the test-time augmentation-based aleatoric uncertainty provides a better uncertainty estimation than calculating the test-time dropout-based model uncertainty alone and helps to reduce overconfident incorrect predictions, and 2) our test-time augmentation outperforms a single-prediction baseline and dropout-based multiple predictions.



### EchoFusion: Tracking and Reconstruction of Objects in 4D Freehand Ultrasound Imaging without External Trackers
- **Arxiv ID**: http://arxiv.org/abs/1807.10583v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.10583v1)
- **Published**: 2018-07-19 12:07:50+00:00
- **Updated**: 2018-07-19 12:07:50+00:00
- **Authors**: Bishesh Khanal, Alberto Gomez, Nicolas Toussaint, Steven McDonagh, Veronika Zimmer, Emily Skelton, Jacqueline Matthew, Daniel Grzech, Robert Wright, Chandni Gupta, Benjamin Hou, Daniel Rueckert, Julia A. Schnabel, Bernhard Kainz
- **Comment**: MICCAI Workshop on Perinatal, Preterm and Paediatric Image analysis
  (PIPPI), 2018
- **Journal**: None
- **Summary**: Ultrasound (US) is the most widely used fetal imaging technique. However, US images have limited capture range, and suffer from view dependent artefacts such as acoustic shadows. Compounding of overlapping 3D US acquisitions into a high-resolution volume can extend the field of view and remove image artefacts, which is useful for retrospective analysis including population based studies. However, such volume reconstructions require information about relative transformations between probe positions from which the individual volumes were acquired. In prenatal US scans, the fetus can move independently from the mother, making external trackers such as electromagnetic or optical tracking unable to track the motion between probe position and the moving fetus. We provide a novel methodology for image-based tracking and volume reconstruction by combining recent advances in deep learning and simultaneous localisation and mapping (SLAM). Tracking semantics are established through the use of a Residual 3D U-Net and the output is fed to the SLAM algorithm. As a proof of concept, experiments are conducted on US volumes taken from a whole body fetal phantom, and from the heads of real fetuses. For the fetal head segmentation, we also introduce a novel weak annotation approach to minimise the required manual effort for ground truth annotation. We evaluate our method qualitatively, and quantitatively with respect to tissue discrimination accuracy and tracking robustness.



### Revisiting Cross Modal Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1807.07364v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07364v1)
- **Published**: 2018-07-19 12:35:24+00:00
- **Updated**: 2018-07-19 12:35:24+00:00
- **Authors**: Shah Nawaz, Muhammad Kamran Janjua, Alessandro Calefati, Ignazio Gallo
- **Comment**: 14 pages. Under review at ECCVW (MULA 2018)
- **Journal**: None
- **Summary**: This paper proposes a cross-modal retrieval system that leverages on image and text encoding. Most multimodal architectures employ separate networks for each modality to capture the semantic relationship between them. However, in our work image-text encoding can achieve comparable results in terms of cross-modal retrieval without having to use a separate network for each modality. We show that text encodings can capture semantic relationships between multiple modalities. In our knowledge, this work is the first of its kind in terms of employing a single network and fused image-text embedding for cross-modal retrieval. We evaluate our approach on two famous multimodal datasets: MS-COCO and Flickr30K.



### ISIC 2018-A Method for Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.07391v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07391v2)
- **Published**: 2018-07-19 13:23:00+00:00
- **Updated**: 2018-07-21 00:56:32+00:00
- **Authors**: Hongdiao Wen, Rongjian Xu, Tie Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Our team participate in the challenge of Task 1: Lesion Boundary Segmentation , and use a combined network, one of which is designed by ourselves named updcnn net and another is an improved VGG 16-layer net. Updcnn net uses reduced size images for training, and VGG 16-layer net utilizes large size images. Image enhancement is used to get a richer data set. We use boxes in the VGG 16-layer net network for local attention regularization to fine-tune the loss function, which can increase the number of training data, and also make the model more robust. In the test, the model is used for joint testing and achieves good results.



### Image Reconstruction via Variational Network for Real-Time Hand-Held Sound-Speed Imaging
- **Arxiv ID**: http://arxiv.org/abs/1807.07416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07416v1)
- **Published**: 2018-07-19 13:39:45+00:00
- **Updated**: 2018-07-19 13:39:45+00:00
- **Authors**: Valery Vishnevskiy, Sergio J Sanabria, Orcun Goksel
- **Comment**: None
- **Journal**: None
- **Summary**: Speed-of-sound is a biomechanical property for quantitative tissue differentiation, with great potential as a new ultrasound-based image modality. A conventional ultrasound array transducer can be used together with an acoustic mirror, or so-called reflector, to reconstruct sound-speed images from time-of-flight measurements to the reflector collected between transducer element pairs, which constitutes a challenging problem of limited-angle computed tomography. For this problem, we herein present a variational network based image reconstruction architecture that is based on optimization loop unrolling, and provide an efficient training protocol of this network architecture on fully synthetic inclusion data. Our results indicate that the learned model presents good generalization ability, being able to reconstruct images with significantly different statistics compared to the training set. Complex inclusion geometries were shown to be successfully reconstructed, also improving over the prior-art by 23% in reconstruction error and by 10% in contrast on synthetic data. In a phantom study, we demonstrated the detection of multiple inclusions that were not distinguishable by prior-art reconstruction, meanwhile improving the contrast by 27% for a stiff inclusion and by 219% for a soft inclusion. Our reconstruction algorithm takes approximately 10ms, enabling its use as a real-time imaging method on an ultrasound machine, for which we are demonstrating an example preliminary setup herein.



### Modeling Visual Context is Key to Augmenting Object Detection Datasets
- **Arxiv ID**: http://arxiv.org/abs/1807.07428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07428v1)
- **Published**: 2018-07-19 13:50:42+00:00
- **Updated**: 2018-07-19 13:50:42+00:00
- **Authors**: Nikita Dvornik, Julien Mairal, Cordelia Schmid
- **Comment**: None
- **Journal**: ECCV2018, Sep 2018, Munich, Germany. 2018
- **Summary**: Performing data augmentation for learning deep neural networks is well known to be important for training visual recognition systems. By artificially increasing the number of training examples, it helps reducing overfitting and improves generalization. For object detection, classical approaches for data augmentation consist of generating images obtained by basic geometrical transformations and color changes of original training images. In this work, we go one step further and leverage segmentation annotations to increase the number of object instances present on training data. For this approach to be successful, we show that modeling appropriately the visual context surrounding objects is crucial to place them in the right environment. Otherwise, we show that the previous strategy actually hurts. With our context model, we achieve significant mean average precision improvements when few labeled examples are available on the VOC'12 benchmark.



### Semi-Dense 3D Reconstruction with a Stereo Event Camera
- **Arxiv ID**: http://arxiv.org/abs/1807.07429v1
- **DOI**: 10.1007/978-3-030-01246-5_15
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.07429v1)
- **Published**: 2018-07-19 13:54:08+00:00
- **Updated**: 2018-07-19 13:54:08+00:00
- **Authors**: Yi Zhou, Guillermo Gallego, Henri Rebecq, Laurent Kneip, Hongdong Li, Davide Scaramuzza
- **Comment**: 19 pages, 8 figures, Video: https://youtu.be/Qrnpj2FD1e4
- **Journal**: European Conference on Computer Vision (ECCV), Munich, 2018
- **Summary**: Event cameras are bio-inspired sensors that offer several advantages, such as low latency, high-speed and high dynamic range, to tackle challenging scenarios in computer vision. This paper presents a solution to the problem of 3D reconstruction from data captured by a stereo event-camera rig moving in a static scene, such as in the context of stereo Simultaneous Localization and Mapping. The proposed method consists of the optimization of an energy function designed to exploit small-baseline spatio-temporal consistency of events triggered across both stereo image planes. To improve the density of the reconstruction and to reduce the uncertainty of the estimation, a probabilistic depth-fusion strategy is also developed. The resulting method has no special requirements on either the motion of the stereo event-camera rig or on prior knowledge about the scene. Experiments demonstrate our method can deal with both texture-rich scenes as well as sparse scenes, outperforming state-of-the-art stereo methods based on event data image representations.



### Selective Zero-Shot Classification with Augmented Attributes
- **Arxiv ID**: http://arxiv.org/abs/1807.07437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07437v1)
- **Published**: 2018-07-19 13:58:36+00:00
- **Updated**: 2018-07-19 13:58:36+00:00
- **Authors**: Jie Song, Chengchao Shen, Jie Lei, An-Xiang Zeng, Kairi Ou, Dacheng Tao, Mingli Song
- **Comment**: To appear in ECCV 2018
- **Journal**: None
- **Summary**: In this paper, we introduce a selective zero-shot classification problem: how can the classifier avoid making dubious predictions? Existing attribute-based zero-shot classification methods are shown to work poorly in the selective classification scenario. We argue the under-complete human defined attribute vocabulary accounts for the poor performance. We propose a selective zero-shot classifier based on both the human defined and the automatically discovered residual attributes. The proposed classifier is constructed by firstly learning the defined and the residual attributes jointly. Then the predictions are conducted within the subspace of the defined attributes. Finally, the prediction confidence is measured by both the defined and the residual attributes. Experiments conducted on several benchmarks demonstrate that our classifier produces a superior performance to other methods under the risk-coverage trade-off metric.



### Can Artificial Intelligence Reliably Report Chest X-Rays?: Radiologist Validation of an Algorithm trained on 2.3 Million X-Rays
- **Arxiv ID**: http://arxiv.org/abs/1807.07455v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07455v2)
- **Published**: 2018-07-19 14:13:30+00:00
- **Updated**: 2019-06-04 08:46:13+00:00
- **Authors**: Preetham Putha, Manoj Tadepalli, Bhargava Reddy, Tarun Raj, Justy Antony Chiramal, Shalini Govil, Namita Sinha, Manjunath KS, Sundeep Reddivari, Ammar Jagirdar, Pooja Rao, Prashant Warier
- **Comment**: v2
- **Journal**: None
- **Summary**: Background: Chest X-rays are the most commonly performed, cost-effective diagnostic imaging tests ordered by physicians. A clinically validated AI system that can reliably separate normals from abnormals can be invaluble particularly in low-resource settings. The aim of this study was to develop and validate a deep learning system to detect various abnormalities seen on a chest X-ray. Methods: A deep learning system was trained on 2.3 million chest X-rays and their corresponding radiology reports to identify various abnormalities seen on a Chest X-ray. The system was tested against - 1. A three-radiologist majority on an independent, retrospectively collected set of 2000 X-rays(CQ2000) 2. Radiologist reports on a separate validation set of 100,000 scans(CQ100k). The primary accuracy measure was area under the ROC curve (AUC), estimated separately for each abnormality and for normal versus abnormal scans. Results: On the CQ2000 dataset, the deep learning system demonstrated an AUC of 0.92(CI 0.91-0.94) for detection of abnormal scans, and AUC(CI) of 0.96(0.94-0.98), 0.96(0.94-0.98), 0.95(0.87-1), 0.95(0.92-0.98), 0.93(0.90-0.96), 0.89(0.83-0.94), 0.91(0.87-0.96), 0.94(0.93-0.96), 0.98(0.97-1) for the detection of blunted costophrenic angle, cardiomegaly, cavity, consolidation, fibrosis, hilar enlargement, nodule, opacity and pleural effusion. The AUCs were similar on the larger CQ100k dataset except for detecting normals where the AUC was 0.86(0.85-0.86). Interpretation: Our study demonstrates that a deep learning algorithm trained on a large, well-labelled dataset can accurately detect multiple abnormalities on chest X-rays. As these systems improve in accuracy, applying deep learning to widen the reach of chest X-ray interpretation and improve reporting efficiency will add tremendous value in radiology workflows and public health screenings globally.



### Fully Convolutional Pixel Adaptive Image Denoiser
- **Arxiv ID**: http://arxiv.org/abs/1807.07569v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.07569v4)
- **Published**: 2018-07-19 14:34:11+00:00
- **Updated**: 2019-10-27 20:24:23+00:00
- **Authors**: Sungmin Cha, Taesup Moon
- **Comment**: 17 pages (including Supplementary Materials), ICCV 2019 camera ready
  version
- **Journal**: None
- **Summary**: We propose a new image denoising algorithm, dubbed as Fully Convolutional Adaptive Image DEnoiser (FC-AIDE), that can learn from an offline supervised training set with a fully convolutional neural network as well as adaptively fine-tune the supervised model for each given noisy image. We significantly extend the framework of the recently proposed Neural AIDE, which formulates the denoiser to be context-based pixelwise mappings and utilizes the unbiased estimator of MSE for such denoisers. The two main contributions we make are; 1) implementing a novel fully convolutional architecture that boosts the base supervised model, and 2) introducing regularization methods for the adaptive fine-tuning such that a stronger and more robust adaptivity can be attained. As a result, FC-AIDE is shown to possess many desirable features; it outperforms the recent CNN-based state-of-the-art denoisers on all of the benchmark datasets we tested, and gets particularly strong for various challenging scenarios, e.g., with mismatched image/noise characteristics or with scarce supervised training data. The source code of our algorithm is available at https://github.com/csm9493/FC-AIDE-Keras.



### Conditional Random Fields as Recurrent Neural Networks for 3D Medical Imaging Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.07464v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07464v1)
- **Published**: 2018-07-19 14:37:43+00:00
- **Updated**: 2018-07-19 14:37:43+00:00
- **Authors**: Miguel Monteiro, Mário A. T. Figueiredo, Arlindo L. Oliveira
- **Comment**: 11 pages, 3 figures (with 11 subfigures)
- **Journal**: None
- **Summary**: The Conditional Random Field as a Recurrent Neural Network layer is a recently proposed algorithm meant to be placed on top of an existing Fully-Convolutional Neural Network to improve the quality of semantic segmentation. In this paper, we test whether this algorithm, which was shown to improve semantic segmentation for 2D RGB images, is able to improve segmentation quality for 3D multi-modal medical images. We developed an implementation of the algorithm which works for any number of spatial dimensions, input/output image channels, and reference image channels. As far as we know this is the first publicly available implementation of this sort. We tested the algorithm with two distinct 3D medical imaging datasets, we concluded that the performance differences observed were not statistically significant. Finally, in the discussion section of the paper, we go into the reasons as to why this technique transfers poorly from natural images to medical images.



### Guided Upsampling Network for Real-Time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.07466v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07466v1)
- **Published**: 2018-07-19 14:40:14+00:00
- **Updated**: 2018-07-19 14:40:14+00:00
- **Authors**: Davide Mazzini
- **Comment**: Accepted at BMVC 2018
- **Journal**: None
- **Summary**: Semantic segmentation architectures are mainly built upon an encoder-decoder structure. These models perform subsequent downsampling operations in the encoder. Since operations on high-resolution activation maps are computationally expensive, usually the decoder produces output segmentation maps by upsampling with parameters-free operators like bilinear or nearest-neighbor. We propose a Neural Network named Guided Upsampling Network which consists of a multiresolution architecture that jointly exploits high-resolution and large context information. Then we introduce a new module named Guided Upsampling Module (GUM) that enriches upsampling operators by introducing a learnable transformation for semantic maps. It can be plugged into any existing encoder-decoder architecture with little modifications and low additional computation cost. We show with quantitative and qualitative experiments how our network benefits from the use of GUM module. A comprehensive set of experiments on the publicly available Cityscapes dataset demonstrates that Guided Upsampling Network can efficiently process high-resolution images in real-time while attaining state-of-the art performances.



### Three for one and one for three: Flow, Segmentation, and Surface Normals
- **Arxiv ID**: http://arxiv.org/abs/1807.07473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07473v1)
- **Published**: 2018-07-19 14:54:21+00:00
- **Updated**: 2018-07-19 14:54:21+00:00
- **Authors**: Hoang-An Le, Anil S. Baslamisli, Thomas Mensink, Theo Gevers
- **Comment**: BMVC 2018
- **Journal**: None
- **Summary**: Optical flow, semantic segmentation, and surface normals represent different information modalities, yet together they bring better cues for scene understanding problems. In this paper, we study the influence between the three modalities: how one impacts on the others and their efficiency in combination. We employ a modular approach using a convolutional refinement network which is trained supervised but isolated from RGB images to enforce joint modality features. To assist the training process, we create a large-scale synthetic outdoor dataset that supports dense annotation of semantic segmentation, optical flow, and surface normals. The experimental results show positive influence among the three modalities, especially for objects' boundaries, region consistency, and scene structures.



### A Strategy of MR Brain Tissue Images' Suggestive Annotation Based on Modified U-Net
- **Arxiv ID**: http://arxiv.org/abs/1807.07510v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07510v4)
- **Published**: 2018-07-19 16:02:41+00:00
- **Updated**: 2018-07-28 03:44:09+00:00
- **Authors**: Yang Deng, Yao Sun, Yongpei Zhu, Mingwang Zhu, Wei Han, Kehong Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate segmentation of MR brain tissue is a crucial step for diagnosis,surgical planning, and treatment of brain abnormalities. However,it is a time-consuming task to be performed by medical experts. So, automatic and reliable segmentation methods are required. How to choose appropriate training dataset from limited labeled dataset rather than the whole also has great significance in saving training time. In addition, medical data labeled is too rare and expensive to obtain extensively, so choosing appropriate unlabeled dataset instead of all the datasets to annotate, which can attain at least same performance, is also very meaningful. To solve the problem above, we design an automatic segmentation method based on U-shaped deep convolutional network and obtain excellent result with average DSC metric of 0.8610, 0.9131, 0.9003 for Cerebrospinal Fluid (CSF), Gray Matter (GM) and White Matter (WM) respectively on the well-known IBSR18 dataset. We use bootstrapping algorithm for selecting the most effective training data and get more state-of-the-art segmentation performance by using only 50% of training data. Moreover, we propose a strategy of MR brain tissue images' suggestive annotation for unlabeled medical data based on the modified U-net. The proposed method performs fast and can be used in clinical.



### Hybrid Scene Compression for Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/1807.07512v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07512v2)
- **Published**: 2018-07-19 16:04:58+00:00
- **Updated**: 2019-04-22 09:20:05+00:00
- **Authors**: Federico Camposeco, Andrea Cohen, Marc Pollefeys, Torsten Sattler
- **Comment**: Published at CVPR 2019
- **Journal**: None
- **Summary**: Localizing an image wrt. a 3D scene model represents a core task for many computer vision applications. An increasing number of real-world applications of visual localization on mobile devices, e.g., Augmented Reality or autonomous robots such as drones or self-driving cars, demand localization approaches to minimize storage and bandwidth requirements. Compressing the 3D models used for localization thus becomes a practical necessity. In this work, we introduce a new hybrid compression algorithm that uses a given memory limit in a more effective way. Rather than treating all 3D points equally, it represents a small set of points with full appearance information and an additional, larger set of points with compressed information. This enables our approach to obtain a more complete scene representation without increasing the memory requirements, leading to a superior performance compared to previous compression schemes. As part of our contribution, we show how to handle ambiguous matches arising from point compression during RANSAC. Besides outperforming previous compression techniques in terms of pose accuracy under the same memory constraints, our compression scheme itself is also more efficient. Furthermore, the localization rates and accuracy obtained with our approach are comparable to state-of-the-art feature-based methods, while using a small fraction of the memory.



### Bio-Measurements Estimation and Support in Knee Recovery through Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.07521v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1807.07521v1)
- **Published**: 2018-07-19 16:24:22+00:00
- **Updated**: 2018-07-19 16:24:22+00:00
- **Authors**: João Bernardino, Luís Filipe Teixeira, Hugo Sereno Ferreira
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: Knee injuries are frequent, varied and often require the patient to undergo intensive rehabilitation for several months. Treatment protocols usually contemplate some recurrent measurements in order to assess progress, such as goniometry. The need for specific equipment or the complexity and duration of these tasks cause them to often be neglected. A novel deep learning based solution is presented, supported by the generation of a synthetic image dataset. A 3D human-body model was used for this purpose, simulating a recovering patient. For each image, the coordinates of three key points were registered: the centers of the thigh, the knee and the lower leg. These values are sufficient to estimate the flexion angle. Convolutional neural networks were then trained for predicting these six coordinates. Transfer learning was used with the VGG16 and InceptionV3 models pre-trained on the ImageNet dataset, being an additional custom model trained from scratch. All models were tested with different combinations of data augmentation techniques applied on the training sets. InceptionV3 achieved the best overall results, producing considerably good predictions even on real unedited pictures.



### Attention-Guided Curriculum Learning for Weakly Supervised Classification and Localization of Thoracic Diseases on Chest Radiographs
- **Arxiv ID**: http://arxiv.org/abs/1807.07532v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07532v1)
- **Published**: 2018-07-19 16:54:16+00:00
- **Updated**: 2018-07-19 16:54:16+00:00
- **Authors**: Yuxing Tang, Xiaosong Wang, Adam P. Harrison, Le Lu, Jing Xiao, Ronald M. Summers
- **Comment**: 9th International Conference on Machine Learning in Medical Imaging
  (MLMI 2018) In conjunction with MICCAI 2018
- **Journal**: None
- **Summary**: In this work, we exploit the task of joint classification and weakly supervised localization of thoracic diseases from chest radiographs, with only image-level disease labels coupled with disease severity-level (DSL) information of a subset. A convolutional neural network (CNN) based attention-guided curriculum learning (AGCL) framework is presented, which leverages the severity-level attributes mined from radiology reports. Images in order of difficulty (grouped by different severity-levels) are fed to CNN to boost the learning gradually. In addition, highly confident samples (measured by classification probabilities) and their corresponding class-conditional heatmaps (generated by the CNN) are extracted and further fed into the AGCL framework to guide the learning of more distinctive convolutional features in the next iteration. A two-path network architecture is designed to regress the heatmaps from selected seed samples in addition to the original classification task. The joint learning scheme can improve the classification and localization performance along with more seed samples for the next iteration. We demonstrate the effectiveness of this iterative refinement framework via extensive experimental evaluations on the publicly available ChestXray14 dataset. AGCL achieves over 5.7\% (averaged over 14 diseases) increase in classification AUC and 7%/11% increases in Recall/Precision for the localization task compared to the state of the art.



### Transfer Learning for Action Unit Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.07556v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07556v1)
- **Published**: 2018-07-19 17:54:08+00:00
- **Updated**: 2018-07-19 17:54:08+00:00
- **Authors**: Yen Khye Lim, Zukang Liao, Stavros Petridis, Maja Pantic
- **Comment**: 6 pages, Humanoids 2017 IEEE RAS International Conference workshop
  Cooperative Autonomous Robot Experience (Presentation)
- **Journal**: None
- **Summary**: This paper presents a classifier ensemble for Facial Expression Recognition (FER) based on models derived from transfer learning. The main experimentation work is conducted for facial action unit detection using feature extraction and fine-tuning convolutional neural networks (CNNs). Several classifiers for extracted CNN codes such as Linear Discriminant Analysis (LDA), Support Vector Machines (SVMs) and Long Short-Term Memory (LSTM) are compared and evaluated. Multi-model ensembles are also used to further improve the performance. We have found that VGG-Face and ResNet are the relatively optimal pre-trained models for action unit recognition using feature extraction and the ensemble of VGG-Net variants and ResNet achieves the best result.



### Capsule Networks against Medical Imaging Data Challenges
- **Arxiv ID**: http://arxiv.org/abs/1807.07559v1
- **DOI**: 10.1007/978-3-030-01364-6_17
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07559v1)
- **Published**: 2018-07-19 17:56:37+00:00
- **Updated**: 2018-07-19 17:56:37+00:00
- **Authors**: Amelia Jiménez-Sánchez, Shadi Albarqouni, Diana Mateus
- **Comment**: 10 pages, 3 figures, accepted at MICCAI-LABELS 2018 Workshop
- **Journal**: LABELS 2018, CVII 2018, STENT 2018: Intravascular Imaging and
  Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and
  Expert Label Synthesis pp 150-160
- **Summary**: A key component to the success of deep learning is the availability of massive amounts of training data. Building and annotating large datasets for solving medical image classification problems is today a bottleneck for many applications. Recently, capsule networks were proposed to deal with shortcomings of Convolutional Neural Networks (ConvNets). In this work, we compare the behavior of capsule networks against ConvNets under typical datasets constraints of medical image analysis, namely, small amounts of annotated data and class-imbalance. We evaluate our experiments on MNIST, Fashion-MNIST and medical (histological and retina images) publicly available datasets. Our results suggest that capsule networks can be trained with less amount of data for the same or better performance and are more robust to an imbalanced class distribution, which makes our approach very promising for the medical imaging community.



### Automated Characterization of Stenosis in Invasive Coronary Angiography Images with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.10597v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10597v1)
- **Published**: 2018-07-19 17:57:06+00:00
- **Updated**: 2018-07-19 17:57:06+00:00
- **Authors**: Benjamin Au, Uri Shaham, Sanket Dhruva, Georgios Bouras, Ecaterina Cristea, Alexandra Lansky MD, Andreas Coppi, Fred Warner, Shu-Xia Li, Harlan Krumholz
- **Comment**: None
- **Journal**: None
- **Summary**: The determination of a coronary stenosis and its severity in current clinical workflow is typically accomplished manually via physician visual assessment (PVA) during invasive coronary angiography. While PVA has shown large inter-rater variability, the more reliable and accurate alternative of Quantitative Coronary Angiography (QCA) is challenging to perform in real-time due to the busy workflow in cardiac catheterization laboratories. We propose a deep learning approach based on Convolutional Neural Networks (CNN) that automatically characterizes and analyzes coronary stenoses in real-time by automating clinical tasks performed during QCA. Our deep learning methods for localization, segmentation and classification of stenosis in still-frame invasive coronary angiography (ICA) images of the right coronary artery (RCA) achieve performance of 72.7% localization accuracy, 0.704 dice coefficient and 0.825 C-statistic in each respective task. Integrated in an end-to-end approach, our model's performance shows statistically significant improvement in false discovery rate over the current standard in real-time clinical stenosis assessment, PVA. To the best of the authors' knowledge, this is the first time an automated machine learning system has been developed that can implement tasks performed in QCA, and the first time an automated machine learning system has demonstrated significant improvement over the current clinical standard for rapid RCA stenosis analysis.



### Compositional GAN: Learning Image-Conditional Binary Composition
- **Arxiv ID**: http://arxiv.org/abs/1807.07560v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.07560v3)
- **Published**: 2018-07-19 17:57:16+00:00
- **Updated**: 2019-03-28 17:04:47+00:00
- **Authors**: Samaneh Azadi, Deepak Pathak, Sayna Ebrahimi, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) can produce images of remarkable complexity and realism but are generally structured to sample from a single latent source ignoring the explicit spatial interaction between multiple entities that could be present in a scene. Capturing such complex interactions between different objects in the world, including their relative scaling, spatial layout, occlusion, or viewpoint transformation is a challenging problem. In this work, we propose a novel self-consistent Composition-by-Decomposition (CoDe) network to compose a pair of objects. Given object images from two distinct distributions, our model can generate a realistic composite image from their joint distribution following the texture and shape of the input objects. We evaluate our approach through qualitative experiments and user evaluations. Our results indicate that the learned model captures potential interactions between the two object domains, and generates realistic composed scenes at test time.



### Deriving star cluster parameters with convolutional neural networks. I. Age, mass, and size
- **Arxiv ID**: http://arxiv.org/abs/1807.07658v2
- **DOI**: 10.1051/0004-6361/201833833
- **Categories**: **astro-ph.GA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.07658v2)
- **Published**: 2018-07-19 22:50:41+00:00
- **Updated**: 2019-01-14 08:06:49+00:00
- **Authors**: J. Bialopetravičius, D. Narbutis, V. Vansevičius
- **Comment**: 11 pages, 12 figures
- **Journal**: A&A 621, A103 (2019)
- **Summary**: Context. Convolutional neural networks (CNNs) have been proven to perform fast classification and detection on natural images and have potential to infer astrophysical parameters on the exponentially increasing amount of sky survey imaging data. The inference pipeline can be trained either from real human-annotated data or simulated mock observations. Until now star cluster analysis was based on integral or individual resolved stellar photometry. This limits the amount of information that can be extracted from cluster images.   Aims. Develop a CNN-based algorithm aimed to simultaneously derive ages, masses, and sizes of star clusters directly from multi-band images. Demonstrate CNN capabilities on low mass semi-resolved star clusters in a low signal-to-noise ratio regime.   Methods. A CNN was constructed based on the deep residual network (ResNet) architecture and trained on simulated images of star clusters with various ages, masses, and sizes. To provide realistic backgrounds, M31 star fields taken from the PHAT survey were added to the mock cluster images.   Results. The proposed CNN was verified on mock images of artificial clusters and has demonstrated high precision and no significant bias for clusters of ages $\lesssim$3Gyr and masses between 250 and 4,000 ${\rm M_\odot}$. The pipeline is end-to-end, starting from input images all the way to the inferred parameters; no hand-coded steps have to be performed: estimates of parameters are provided by the neural network in one inferential step from raw images.



### Automatically Designing CNN Architectures for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.07663v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.07663v1)
- **Published**: 2018-07-19 23:47:12+00:00
- **Updated**: 2018-07-19 23:47:12+00:00
- **Authors**: Aliasghar Mortazi, Ulas Bagci
- **Comment**: Accepted to Machine Learning in Medical Imaging (MLMI 2018)
- **Journal**: None
- **Summary**: Deep neural network architectures have traditionally been designed and explored with human expertise in a long-lasting trial-and-error process. This process requires huge amount of time, expertise, and resources. To address this tedious problem, we propose a novel algorithm to optimally find hyperparameters of a deep network architecture automatically. We specifically focus on designing neural architectures for medical image segmentation task. Our proposed method is based on a policy gradient reinforcement learning for which the reward function is assigned a segmentation evaluation utility (i.e., dice index). We show the efficacy of the proposed method with its low computational cost in comparison with the state-of-the-art medical image segmentation networks. We also present a new architecture design, a densely connected encoder-decoder CNN, as a strong baseline architecture to apply the proposed hyperparameter search algorithm. We apply the proposed algorithm to each layer of the baseline architectures. As an application, we train the proposed system on cine cardiac MR images from Automated Cardiac Diagnosis Challenge (ACDC) MICCAI 2017. Starting from a baseline segmentation architecture, the resulting network architecture obtains the state-of-the-art results in accuracy without performing any trial-and-error based architecture design approaches or close supervision of the hyperparameters changes.



