# Arxiv Papers in cs.CV on 2018-07-16
### Convolutional Sparse Kernel Network for Unsupervised Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1807.05648v4
- **DOI**: 10.1016/j.media.2019.06.005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05648v4)
- **Published**: 2018-07-16 01:33:00+00:00
- **Updated**: 2020-06-20 05:02:28+00:00
- **Authors**: Euijoon Ahn, Jinman Kim, Ashnil Kumar, Michael Fulham, Dagan Feng
- **Comment**: Accepted by Medical Image Analysis (with a new title 'Convolutional
  Sparse Kernel Network for Unsupervised Medical Image Analysis'). The
  manuscript is available from following link
  (https://doi.org/10.1016/j.media.2019.06.005)
- **Journal**: None
- **Summary**: The availability of large-scale annotated image datasets and recent advances in supervised deep learning methods enable the end-to-end derivation of representative image features that can impact a variety of image analysis problems. Such supervised approaches, however, are difficult to implement in the medical domain where large volumes of labelled data are difficult to obtain due to the complexity of manual annotation and inter- and intra-observer variability in label assignment. We propose a new convolutional sparse kernel network (CSKN), which is a hierarchical unsupervised feature learning framework that addresses the challenge of learning representative visual features in medical image analysis domains where there is a lack of annotated training data. Our framework has three contributions: (i) We extend kernel learning to identify and represent invariant features across image sub-patches in an unsupervised manner. (ii) We initialise our kernel learning with a layer-wise pre-training scheme that leverages the sparsity inherent in medical images to extract initial discriminative features. (iii) We adapt a multi-scale spatial pyramid pooling (SPP) framework to capture subtle geometric differences between learned visual features. We evaluated our framework in medical image retrieval and classification on three public datasets. Our results show that our CSKN had better accuracy when compared to other conventional unsupervised methods and comparable accuracy to methods that used state-of-the-art supervised convolutional neural networks (CNNs). Our findings indicate that our unsupervised CSKN provides an opportunity to leverage unannotated big data in medical imaging repositories.



### Learning and Matching Multi-View Descriptors for Registration of Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1807.05653v2
- **DOI**: 10.1007/978-3-030-01267-0_31
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05653v2)
- **Published**: 2018-07-16 01:58:27+00:00
- **Updated**: 2023-02-27 05:06:01+00:00
- **Authors**: Lei Zhou, Siyu Zhu, Zixin Luo, Tianwei Shen, Runze Zhang, Mingmin Zhen, Tian Fang, Long Quan
- **Comment**: None
- **Journal**: None
- **Summary**: Critical to the registration of point clouds is the establishment of a set of accurate correspondences between points in 3D space. The correspondence problem is generally addressed by the design of discriminative 3D local descriptors on the one hand, and the development of robust matching strategies on the other hand. In this work, we first propose a multi-view local descriptor, which is learned from the images of multiple views, for the description of 3D keypoints. Then, we develop a robust matching approach, aiming at rejecting outlier matches based on the efficient inference via belief propagation on the defined graphical model. We have demonstrated the boost of our approaches to registration on the public scanning and multi-view stereo datasets. The superior performance has been verified by the intensive comparisons against a variety of descriptors and matching methods.



### SCAN: Self-and-Collaborative Attention Network for Video Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1807.05688v4
- **DOI**: 10.1109/TIP.2019.2911488
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05688v4)
- **Published**: 2018-07-16 06:09:24+00:00
- **Updated**: 2019-08-06 13:51:34+00:00
- **Authors**: Ruimao Zhang, Hongbin Sun, Jingyu Li, Yuying Ge, Liang Lin, Ping Luo, Xiaogang Wang
- **Comment**: 10 pages, 5 figures
- **Journal**: IEEE Transactions on Image Processing 2019
- **Summary**: Video person re-identification attracts much attention in recent years. It aims to match image sequences of pedestrians from different camera views. Previous approaches usually improve this task from three aspects, including a) selecting more discriminative frames, b) generating more informative temporal representations, and c) developing more effective distance metrics. To address the above issues, we present a novel and practical deep architecture for video person re-identification termed Self-and-Collaborative Attention Network (SCAN). It has several appealing properties. First, SCAN adopts non-parametric attention mechanism to refine the intra-sequence and inter-sequence feature representation of videos, and outputs self-and-collaborative feature representation for each video, making the discriminative frames aligned between the probe and gallery sequences.Second, beyond existing models, a generalized pairwise similarity measurement is proposed to calculate the similarity feature representations of video pairs, enabling computing the matching scores by the binary classifier. Third, a dense clip segmentation strategy is also introduced to generate rich probe-gallery pairs to optimize the model. Extensive experiments demonstrate the effectiveness of SCAN, which outperforms the best-performing baselines on iLIDS-VID, PRID2011 and MARS dataset, respectively.



### LineNet: a Zoomable CNN for Crowdsourced High Definition Maps Modeling in Urban Environments
- **Arxiv ID**: http://arxiv.org/abs/1807.05696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05696v1)
- **Published**: 2018-07-16 06:35:48+00:00
- **Updated**: 2018-07-16 06:35:48+00:00
- **Authors**: Dun Liang, Yuanchen Guo, Shaokui Zhang, Song-Hai Zhang, Peter Hall, Min Zhang, Shimin Hu
- **Comment**: None
- **Journal**: None
- **Summary**: High Definition (HD) maps play an important role in modern traffic scenes. However, the development of HD maps coverage grows slowly because of the cost limitation. To efficiently model HD maps, we proposed a convolutional neural network with a novel prediction layer and a zoom module, called LineNet. It is designed for state-of-the-art lane detection in an unordered crowdsourced image dataset. And we introduced TTLane, a dataset for efficient lane detection in urban road modeling applications. Combining LineNet and TTLane, we proposed a pipeline to model HD maps with crowdsourced data for the first time. And the maps can be constructed precisely even with inaccurate crowdsourced data.



### Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining
- **Arxiv ID**: http://arxiv.org/abs/1807.05698v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05698v2)
- **Published**: 2018-07-16 06:49:22+00:00
- **Updated**: 2018-07-28 14:31:53+00:00
- **Authors**: Xia Li, Jianlong Wu, Zhouchen Lin, Hong Liu, Hongbin Zha
- **Comment**: Accepted by ECCV
- **Journal**: None
- **Summary**: Rain streaks can severely degrade the visibility, which causes many current computer vision algorithms fail to work. So it is necessary to remove the rain from images. We propose a novel deep network architecture based on deep convolutional and recurrent neural networks for single image deraining. As contextual information is very important for rain removal, we first adopt the dilated convolutional neural network to acquire large receptive field. To better fit the rain removal task, we also modify the network. In heavy rain, rain streaks have various directions and shapes, which can be regarded as the accumulation of multiple rain streak layers. We assign different alpha-values to various rain streak layers according to the intensity and transparency by incorporating the squeeze-and-excitation block. Since rain streak layers overlap with each other, it is not easy to remove the rain in one stage. So we further decompose the rain removal into multiple stages. Recurrent neural network is incorporated to preserve the useful information in previous stages and benefit the rain removal in later stages. We conduct extensive experiments on both synthetic and real-world datasets. Our proposed method outperforms the state-of-the-art approaches under all evaluation metrics. Codes and supplementary material are available at our project webpage: https://xialipku.github.io/RESCAN .



### ENG: End-to-end Neural Geometry for Robust Depth and Pose Estimation using CNNs
- **Arxiv ID**: http://arxiv.org/abs/1807.05705v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05705v2)
- **Published**: 2018-07-16 07:23:56+00:00
- **Updated**: 2018-11-06 06:02:51+00:00
- **Authors**: Thanuja Dharmasiri, Andrew Spek, Tom Drummond
- **Comment**: None
- **Journal**: None
- **Summary**: Recovering structure and motion parameters given a image pair or a sequence of images is a well studied problem in computer vision. This is often achieved by employing Structure from Motion (SfM) or Simultaneous Localization and Mapping (SLAM) algorithms based on the real-time requirements. Recently, with the advent of Convolutional Neural Networks (CNNs) researchers have explored the possibility of using machine learning techniques to reconstruct the 3D structure of a scene and jointly predict the camera pose. In this work, we present a framework that achieves state-of-the-art performance on single image depth prediction for both indoor and outdoor scenes. The depth prediction system is then extended to predict optical flow and ultimately the camera pose and trained end-to-end. Our motion estimation framework outperforms the previous motion prediction systems and we also demonstrate that the state-of-the-art metric depths can be further improved using the knowledge of pose.



### Disease Classification within Dermascopic Images Using features extracted by ResNet50 and classification through Deep Forest
- **Arxiv ID**: http://arxiv.org/abs/1807.05711v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05711v3)
- **Published**: 2018-07-16 07:57:31+00:00
- **Updated**: 2018-07-25 13:30:50+00:00
- **Authors**: Suhita Ray
- **Comment**: I have decided to not participate in the competition due to poor
  results in the used methodology and hence would like to withdraw this paper
- **Journal**: None
- **Summary**: In this report we propose a classification technique for skin lesion images as a part of our submission for ISIC 2018 Challenge in Skin Lesion Analysis Towards Melanoma Detection. Our data was extracted from the ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection grand challenge datasets. The features are extracted through a Convolutional Neural Network, in our case ResNet50 and then using these features we train a DeepForest, having cascading layers, to classify our skin lesion images. We know that Convolutional Neural Networks are a state-of-the-art technique in representation learning for images, with the convolutional filters learning to detect features from images through backpropagation. These features are then usually fed to a classifier like a softmax layer or other such classifiers for classification tasks. In our case we do not use the traditional backpropagation method and train a softmax layer for classification. Instead, we use Deep Forest, a novel decision tree ensemble approach with performance highly competitive to deep neural networks in a broad range of tasks. Thus we use a ResNet50 to extract the features from skin lesion images and then use the Deep Forest to classify these images. This method has been used because Deep Forest has been found to be hugely efficient in areas where there are only small-scale training data available. Also as the Deep Forest network decides its complexity by itself, it also caters to the problem of dataset imbalance we faced in this problem.



### Land-Cover Classification with High-Resolution Remote Sensing Images Using Transferable Deep Models
- **Arxiv ID**: http://arxiv.org/abs/1807.05713v3
- **DOI**: 10.1016/j.rse.2019.111322
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05713v3)
- **Published**: 2018-07-16 08:02:10+00:00
- **Updated**: 2022-04-22 16:30:06+00:00
- **Authors**: Xin-Yi Tong, Gui-Song Xia, Qikai Lu, Huanfeng Shen, Shengyang Li, Shucheng You, Liangpei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, large amount of high spatial-resolution remote sensing (HRRS) images are available for land-cover mapping. However, due to the complex information brought by the increased spatial resolution and the data disturbances caused by different conditions of image acquisition, it is often difficult to find an efficient method for achieving accurate land-cover classification with high-resolution and heterogeneous remote sensing images. In this paper, we propose a scheme to apply deep model obtained from labeled land-cover dataset to classify unlabeled HRRS images. The main idea is to rely on deep neural networks for presenting the contextual information contained in different types of land-covers and propose a pseudo-labeling and sample selection scheme for improving the transferability of deep models. More precisely, a deep Convolutional Neural Networks is first pre-trained with a well-annotated land-cover dataset, referred to as the source data. Then, given a target image with no labels, the pre-trained CNN model is utilized to classify the image in a patch-wise manner. The patches with high confidence are assigned with pseudo-labels and employed as the queries to retrieve related samples from the source data. The pseudo-labels confirmed with the retrieved results are regarded as supervised information for fine-tuning the pre-trained deep model. To obtain a pixel-wise land-cover classification with the target image, we rely on the fine-tuned CNN and develop a hybrid classification by combining patch-wise classification and hierarchical segmentation. In addition, we create a large-scale land-cover dataset containing 150 Gaofen-2 satellite images for CNN pre-training. Experiments on multi-source HRRS images show encouraging results and demonstrate the applicability of the proposed scheme to land-cover classification.



### Iterative Joint Image Demosaicking and Denoising using a Residual Denoising Network
- **Arxiv ID**: http://arxiv.org/abs/1807.06403v3
- **DOI**: 10.1109/TIP.2019.2905991
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06403v3)
- **Published**: 2018-07-16 08:17:46+00:00
- **Updated**: 2019-03-29 12:34:51+00:00
- **Authors**: Filippos Kokkinos, Stamatios Lefkimmiatis
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1803.05215
- **Journal**: None
- **Summary**: Modern digital cameras rely on the sequential execution of separate image processing steps to produce realistic images. The first two steps are usually related to denoising and demosaicking where the former aims to reduce noise from the sensor and the latter converts a series of light intensity readings to color images. Modern approaches try to jointly solve these problems, i.e. joint denoising-demosaicking which is an inherently ill-posed problem given that two-thirds of the intensity information is missing and the rest are perturbed by noise. While there are several machine learning systems that have been recently introduced to solve this problem, the majority of them relies on generic network architectures which do not explicitly take into account the physical image model. In this work we propose a novel algorithm which is inspired by powerful classical image regularization methods, large-scale optimization, and deep learning techniques. Consequently, our derived iterative optimization algorithm, which involves a trainable denoising network, has a transparent and clear interpretation compared to other black-box data driven approaches. Our extensive experimentation line demonstrates that our proposed method outperforms any previous approaches for both noisy and noise-free data across many different datasets. This improvement in reconstruction quality is attributed to the rigorous derivation of an iterative solution and the principled way we design our denoising network architecture, which as a result requires fewer trainable parameters than the current state-of-the-art solution and furthermore can be efficiently trained by using a significantly smaller number of training data than existing deep demosaicking networks. Code and results can be found at https://github.com/cig-skoltech/deep_demosaick



### BRIEF: Backward Reduction of CNNs with Information Flow Analysis
- **Arxiv ID**: http://arxiv.org/abs/1807.05726v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.05726v3)
- **Published**: 2018-07-16 08:32:54+00:00
- **Updated**: 2018-11-01 02:35:47+00:00
- **Authors**: Yu-Hsun Lin, Chun-Nan Chou, Edward Y. Chang
- **Comment**: IEEE Artificial Intelligence and Virtual Reality (IEEE AIVR) 2018
- **Journal**: None
- **Summary**: This paper proposes BRIEF, a backward reduction algorithm that explores compact CNN-model designs from the information flow perspective. This algorithm can remove substantial non-zero weighting parameters (redundant neural channels) of a network by considering its dynamic behavior, which traditional model-compaction techniques cannot achieve. With the aid of our proposed algorithm, we achieve significant model reduction on ResNet-34 in the ImageNet scale (32.3% reduction), which is 3X better than the previous result (10.8%). Even for highly optimized models such as SqueezeNet and MobileNet, we can achieve additional 10.81% and 37.56% reduction, respectively, with negligible performance degradation.



### An Extensive Review on Spectral Imaging in Biometric Systems: Challenges and Advancements
- **Arxiv ID**: http://arxiv.org/abs/1807.05771v2
- **DOI**: 10.1016/j.jvcir.2019.102660
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05771v2)
- **Published**: 2018-07-16 10:24:28+00:00
- **Updated**: 2019-05-29 07:16:04+00:00
- **Authors**: Rumaisah Munir, Rizwan Ahmed Khan
- **Comment**: None
- **Journal**: Journal of Visual Communication and Image Representation, 2019
- **Summary**: Spectral imaging has recently gained traction for face recognition in biometric systems. We investigate the merits of spectral imaging for face recognition and the current challenges that hamper the widespread deployment of spectral sensors for face recognition. The reliability of conventional face recognition systems operating in the visible range is compromised by illumination changes, pose variations and spoof attacks. Recent works have reaped the benefits of spectral imaging to counter these limitations in surveillance activities (defence, airport security checks, etc.). However, the implementation of this technology for biometrics, is still in its infancy due to multiple reasons. We present an overview of the existing work in the domain of spectral imaging for face recognition, different types of modalities and their assessment, availability of public databases for sake of reproducible research as well as evaluation of algorithms, and recent advancements in the field, such as, the use of deep learning-based methods for recognizing faces from spectral images.



### MIDV-500: A Dataset for Identity Documents Analysis and Recognition on Mobile Devices in Video Stream
- **Arxiv ID**: http://arxiv.org/abs/1807.05786v4
- **DOI**: 10.18287/2412-6179-2019-43-5-818-824
- **Categories**: **cs.CV**, cs.DL, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1807.05786v4)
- **Published**: 2018-07-16 10:51:10+00:00
- **Updated**: 2020-02-11 07:28:48+00:00
- **Authors**: Vladimir V. Arlazarov, Konstantin Bulatov, Timofey Chernov, Vladimir L. Arlazarov
- **Comment**: 7 pages, 6 figures, 5 tables
- **Journal**: Computer optics 43 N5 (2019) 818-824
- **Summary**: A lot of research has been devoted to identity documents analysis and recognition on mobile devices. However, no publicly available datasets designed for this particular problem currently exist. There are a few datasets which are useful for associated subtasks but in order to facilitate a more comprehensive scientific and technical approach to identity document recognition more specialized datasets are required. In this paper we present a Mobile Identity Document Video dataset (MIDV-500) consisting of 500 video clips for 50 different identity document types with ground truth which allows to perform research in a wide scope of document analysis problems. The paper presents characteristics of the dataset and evaluation results for existing methods of face detection, text line recognition, and document fields data extraction. Since an important feature of identity documents is their sensitiveness as they contain personal data, all source document images used in MIDV-500 are either in public domain or distributed under public copyright licenses.   The main goal of this paper is to present a dataset. However, in addition and as a baseline, we present evaluation results for existing methods for face detection, text line recognition, and document data extraction, using the presented dataset.   (The dataset is available for download at ftp://smartengines.com/midv-500/.)



### Spatial-Temporal Synergic Residual Learning for Video Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1807.05799v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05799v1)
- **Published**: 2018-07-16 11:39:59+00:00
- **Updated**: 2018-07-16 11:39:59+00:00
- **Authors**: Xinxing Su, Yingtian Zou, Yu Cheng, Shuangjie Xu, Mo Yu, Pan Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the problem of person re-identification in video setting in this paper, which has been viewed as a crucial task in many applications. Meanwhile, it is very challenging since the task requires learning effective representations from video sequences with heterogeneous spatial-temporal information. We present a novel method - Spatial-Temporal Synergic Residual Network (STSRN) for this problem. STSRN contains a spatial residual extractor, a temporal residual processor and a spatial-temporal smooth module. The smoother can alleviate sample noises along the spatial-temporal dimensions thus enable STSRN extracts more robust spatial-temporal features of consecutive frames. Extensive experiments are conducted on several challenging datasets including iLIDS-VID, PRID2011 and MARS. The results demonstrate that the proposed method achieves consistently superior performance over most of state-of-the-art methods.



### Deep Generative Model using Unregularized Score for Anomaly Detection with Heterogeneous Complexity
- **Arxiv ID**: http://arxiv.org/abs/1807.05800v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.05800v2)
- **Published**: 2018-07-16 11:41:32+00:00
- **Updated**: 2018-09-04 13:14:38+00:00
- **Authors**: Takashi Matsubara, Kenta Hama, Ryosuke Tachibana, Kuniaki Uehara
- **Comment**: An extended version of a manuscript in Proc. of The 2018
  International Joint Conference on Neural Networks (IJCNN2018)
- **Journal**: None
- **Summary**: Accurate and automated detection of anomalous samples in a natural image dataset can be accomplished with a probabilistic model for end-to-end modeling of images. Such images have heterogeneous complexity, however, and a probabilistic model overlooks simply shaped objects with small anomalies. This is because the probabilistic model assigns undesirably lower likelihoods to complexly shaped objects that are nevertheless consistent with set standards. To overcome this difficulty, we propose an unregularized score for deep generative models (DGMs), which are generative models leveraging deep neural networks. We found that the regularization terms of the DGMs considerably influence the anomaly score depending on the complexity of the samples. By removing these terms, we obtain an unregularized score, which we evaluated on a toy dataset and real-world manufacturing datasets. Empirical results demonstrate that the unregularized score is robust to the inherent complexity of samples and can be used to better detect anomalies.



### Assessing fish abundance from underwater video using deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1807.05838v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.05838v1)
- **Published**: 2018-07-16 13:13:37+00:00
- **Updated**: 2018-07-16 13:13:37+00:00
- **Authors**: Ranju Mandal, Rod M. Connolly, Thomas A. Schlacherz, Bela Stantic
- **Comment**: IJCNN 2018
- **Journal**: None
- **Summary**: Uses of underwater videos to assess diversity and abundance of fish are being rapidly adopted by marine biologists. Manual processing of videos for quantification by human analysts is time and labour intensive. Automatic processing of videos can be employed to achieve the objectives in a cost and time-efficient way. The aim is to build an accurate and reliable fish detection and recognition system, which is important for an autonomous robotic platform. However, there are many challenges involved in this task (e.g. complex background, deformation, low resolution and light propagation). Recent advancement in the deep neural network has led to the development of object detection and recognition in real time scenarios. An end-to-end deep learning-based architecture is introduced which outperformed the state of the art methods and first of its kind on fish assessment task. A Region Proposal Network (RPN) introduced by an object detector termed as Faster R-CNN was combined with three classification networks for detection and recognition of fish species obtained from Remote Underwater Video Stations (RUVS). An accuracy of 82.4% (mAP) obtained from the experiments are much higher than previously proposed methods.



### Assessment of electrical and infrastructure recovery in Puerto Rico following hurricane Maria using a multisource time series of satellite imagery
- **Arxiv ID**: http://arxiv.org/abs/1807.05854v1
- **DOI**: 10.1117/12.2325585
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1807.05854v1)
- **Published**: 2018-07-16 13:38:57+00:00
- **Updated**: 2018-07-16 13:38:57+00:00
- **Authors**: Jacob Shermeyer
- **Comment**: 15 pages, 5 figures, 5 videos
- **Journal**: None
- **Summary**: Puerto Rico suffered severe damage from the category 5 hurricane (Maria) in September 2017. Total monetary damages are estimated to be ~92 billion USD, the third most costly tropical cyclone in US history. The response to this damage has been tempered and slow moving, with recent estimates placing 45% of the population without power three months after the storm. Consequently, we developed a unique data-fusion mapping approach called the Urban Development Index (UDI) and new open source tool, Comet Time Series (CometTS), to analyze the recovery of electricity and infrastructure in Puerto Rico. Our approach incorporates a combination of time series visualizations and change detection mapping to create depictions of power or infrastructure loss. It also provides a unique independent assessment of areas that are still struggling to recover. For this workflow, our time series approach combines nighttime imagery from the Suomi National Polar-orbiting Partnership Visible Infrared Imaging Radiometer Suite (NPP VIIRS), multispectral imagery from two Landsat satellites, US Census data, and crowd-sourced building footprint labels. Based upon our approach we can identify and evaluate: 1) the recovery of electrical power compared to pre-storm levels, 2) the location of potentially damaged infrastructure that has yet to recover from the storm, and 3) the number of persons without power over time. As of May 31, 2018, declined levels of observed brightness across the island indicate that 13.9% +/- ~5.6% of persons still lack power and/or that 13.2% +/- ~5.3% of infrastructure has been lost. In comparison, the Puerto Rico Electric Power Authority states that less than 1% of their customers still are without power.



### Object Relation Detection Based on One-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.05857v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05857v1)
- **Published**: 2018-07-16 13:42:28+00:00
- **Updated**: 2018-07-16 13:42:28+00:00
- **Authors**: Li Zhou, Jian Zhao, Jianshu Li, Li Yuan, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting the relations among objects, such as "cat on sofa" and "person ride horse", is a crucial task in image understanding, and beneficial to bridging the semantic gap between images and natural language. Despite the remarkable progress of deep learning in detection and recognition of individual objects, it is still a challenging task to localize and recognize the relations between objects due to the complex combinatorial nature of various kinds of object relations. Inspired by the recent advances in one-shot learning, we propose a simple yet effective Semantics Induced Learner (SIL) model for solving this challenging task. Learning in one-shot manner can enable a detection model to adapt to a huge number of object relations with diverse appearance effectively and robustly. In addition, the SIL combines bottom-up and top-down attention mech- anisms, therefore enabling attention at the level of vision and semantics favorably. Within our proposed model, the bottom-up mechanism, which is based on Faster R-CNN, proposes objects regions, and the top-down mechanism selects and integrates visual features according to semantic information. Experiments demonstrate the effectiveness of our framework over other state-of-the-art methods on two large-scale data sets for object relation detection.



### Uncertainty and Interpretability in Convolutional Neural Networks for Semantic Segmentation of Colorectal Polyps
- **Arxiv ID**: http://arxiv.org/abs/1807.10584v1
- **DOI**: 10.1016/j.media.2019.101619
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.10584v1)
- **Published**: 2018-07-16 15:01:01+00:00
- **Updated**: 2018-07-16 15:01:01+00:00
- **Authors**: Kristoffer Wickstrøm, Michael Kampffmeyer, Robert Jenssen
- **Comment**: To appear in IEEE MLSP 2018
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are propelling advances in a range of different computer vision tasks such as object detection and object segmentation. Their success has motivated research in applications of such models for medical image analysis. If CNN-based models are to be helpful in a medical context, they need to be precise, interpretable, and uncertainty in predictions must be well understood. In this paper, we develop and evaluate recent advances in uncertainty estimation and model interpretability in the context of semantic segmentation of polyps from colonoscopy images. We evaluate and enhance several architectures of Fully Convolutional Networks (FCNs) for semantic segmentation of colorectal polyps and provide a comparison between these models. Our highest performing model achieves a 76.06\% mean IOU accuracy on the EndoScene dataset, a considerable improvement over the previous state-of-the-art.



### Applying Domain Randomization to Synthetic Data for Object Category Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.09834v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.09834v1)
- **Published**: 2018-07-16 15:08:57+00:00
- **Updated**: 2018-07-16 15:08:57+00:00
- **Authors**: João Borrego, Atabak Dehban, Rui Figueiredo, Plinio Moreno, Alexandre Bernardino, José Santos-Victor
- **Comment**: 17 pages, 9 figures. Under review for ACCV 2018
- **Journal**: None
- **Summary**: Recent advances in deep learning-based object detection techniques have revolutionized their applicability in several fields. However, since these methods rely on unwieldy and large amounts of data, a common practice is to download models pre-trained on standard datasets and fine-tune them for specific application domains with a small set of domain relevant images. In this work, we show that using synthetic datasets that are not necessarily photo-realistic can be a better alternative to simply fine-tune pre-trained networks. Specifically, our results show an impressive 25% improvement in the mAP metric over a fine-tuning baseline when only about 200 labelled images are available to train. Finally, an ablation study of our results is presented to delineate the individual contribution of different components in the randomization pipeline.



### Computationally Efficient Approaches for Image Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1807.05927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05927v1)
- **Published**: 2018-07-16 15:43:12+00:00
- **Updated**: 2018-07-16 15:43:12+00:00
- **Authors**: Ram Krishna Pandey, Samarjit Karmakar, A G Ramakrishnan
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we have investigated various style transfer approaches and (i) examined how the stylized reconstruction changes with the change of loss function and (ii) provided a computationally efficient solution for the same. We have used elegant techniques like depth-wise separable convolution in place of convolution and nearest neighbor interpolation in place of transposed convolution. Further, we have also added multiple interpolations in place of transposed convolution. The results obtained are perceptually similar in quality, while being computationally very efficient. The decrease in the computational complexity of our architecture is validated by the decrease in the testing time by 26.1%, 39.1%, and 57.1%, respectively.



### Visual Graphs from Motion (VGfM): Scene understanding with object geometry reasoning
- **Arxiv ID**: http://arxiv.org/abs/1807.05933v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05933v2)
- **Published**: 2018-07-16 15:49:43+00:00
- **Updated**: 2018-11-07 17:32:06+00:00
- **Authors**: Paul Gay, Stuart James, Alessio Del Bue
- **Comment**: Accepted to ACCV 2018
- **Journal**: None
- **Summary**: Recent approaches on visual scene understanding attempt to build a scene graph -- a computational representation of objects and their pairwise relationships. Such rich semantic representation is very appealing, yet difficult to obtain from a single image, especially when considering complex spatial arrangements in the scene. Differently, an image sequence conveys useful information using the multi-view geometric relations arising from camera motion. Indeed, in such cases, object relationships are naturally related to the 3D scene structure. To this end, this paper proposes a system that first computes the geometrical location of objects in a generic scene and then efficiently constructs scene graphs from video by embedding such geometrical reasoning. Such compelling representation is obtained using a new model where geometric and visual features are merged using an RNN framework. We report results on a dataset we created for the task of 3D scene graph generation in multiple views.



### A Multimodal Approach to Predict Social Media Popularity
- **Arxiv ID**: http://arxiv.org/abs/1807.05959v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05959v1)
- **Published**: 2018-07-16 16:35:23+00:00
- **Updated**: 2018-07-16 16:35:23+00:00
- **Authors**: Mayank Meghawat, Satyendra Yadav, Debanjan Mahata, Yifang Yin, Rajiv Ratn Shah, Roger Zimmermann
- **Comment**: Preprint version for paper accepted in Proceedings of 1st IEEE
  International Conference on Multimedia Information Processing and Retrieval
- **Journal**: None
- **Summary**: Multiple modalities represent different aspects by which information is conveyed by a data source. Modern day social media platforms are one of the primary sources of multimodal data, where users use different modes of expression by posting textual as well as multimedia content such as images and videos for sharing information. Multimodal information embedded in such posts could be useful in predicting their popularity. To the best of our knowledge, no such multimodal dataset exists for the prediction of social media photos. In this work, we propose a multimodal dataset consisiting of content, context, and social information for popularity prediction. Specifically, we augment the SMPT1 dataset for social media prediction in ACM Multimedia grand challenge 2017 with image content, titles, descriptions, and tags. Next, in this paper, we propose a multimodal approach which exploits visual features (i.e., content information), textual features (i.e., contextual information), and social features (e.g., average views and group counts) to predict popularity of social media photos in terms of view counts. Experimental results confirm that despite our multimodal approach uses the half of the training dataset from SMP-T1, it achieves comparable performance with that of state-of-the-art.



### Meta-Learning with Latent Embedding Optimization
- **Arxiv ID**: http://arxiv.org/abs/1807.05960v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.05960v3)
- **Published**: 2018-07-16 16:35:29+00:00
- **Updated**: 2019-03-26 13:36:45+00:00
- **Authors**: Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, Raia Hadsell
- **Comment**: None
- **Journal**: None
- **Summary**: Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this low-dimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space.



### ActiveStereoNet: End-to-End Self-Supervised Learning for Active Stereo Systems
- **Arxiv ID**: http://arxiv.org/abs/1807.06009v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06009v1)
- **Published**: 2018-07-16 16:55:19+00:00
- **Updated**: 2018-07-16 16:55:19+00:00
- **Authors**: Yinda Zhang, Sameh Khamis, Christoph Rhemann, Julien Valentin, Adarsh Kowdle, Vladimir Tankovich, Michael Schoenberg, Shahram Izadi, Thomas Funkhouser, Sean Fanello
- **Comment**: Accepted by ECCV2018, Oral Presentation, Main paper + Supplementary
  Materials
- **Journal**: None
- **Summary**: In this paper we present ActiveStereoNet, the first deep learning solution for active stereo systems. Due to the lack of ground truth, our method is fully self-supervised, yet it produces precise depth with a subpixel precision of $1/30th$ of a pixel; it does not suffer from the common over-smoothing issues; it preserves the edges; and it explicitly handles occlusions. We introduce a novel reconstruction loss that is more robust to noise and texture-less patches, and is invariant to illumination changes. The proposed loss is optimized using a window-based cost aggregation with an adaptive support weight scheme. This cost aggregation is edge-preserving and smooths the loss function, which is key to allow the network to reach compelling results. Finally we show how the task of predicting invalid regions, such as occlusions, can be trained end-to-end without ground-truth. This component is crucial to reduce blur and particularly improves predictions along depth discontinuities. Extensive quantitatively and qualitatively evaluations on real and synthetic data demonstrate state of the art results in many challenging scenes.



### Towards Single-phase Single-stage Detection of Pulmonary Nodules in Chest CT Imaging
- **Arxiv ID**: http://arxiv.org/abs/1807.05972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05972v1)
- **Published**: 2018-07-16 17:10:11+00:00
- **Updated**: 2018-07-16 17:10:11+00:00
- **Authors**: Zhongliu Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Detection of pulmonary nodules in chest CT imaging plays a crucial role in early diagnosis of lung cancer. Manual examination is highly time-consuming and error prone, calling for computer-aided detection, both to improve efficiency and reduce misdiagnosis. Over the years, a range of systems have been proposed, mostly following a two-phase paradigm with: 1) candidate detection, 2) false positive reduction. Recently, deep learning has become a dominant force in algorithm development. As for candidate detection, prior art was mainly based on the two-stage Faster R-CNN framework, which starts with an initial sub-net to generate a set of class-agnostic region proposals, followed by a second sub-net to perform classification and bounding-box regression. In contrast, we abandon the conventional two-phase paradigm and two-stage framework altogether and propose to train a single network for end-to-end nodule detection instead, without transfer learning or further post-processing. Our feature learning model is a modification of the ResNet and feature pyramid network combined, powered by RReLU activation. The major challenge is the condition of extreme inter-class and intra-class sample imbalance, where the positives are overwhelmed by a large negative pool, which is mostly composed of easy and a handful of hard negatives. Direct training on all samples can seriously undermine training efficacy. We propose a patch-based sampling strategy over a set of regularly updating anchors, which narrows sampling scope to all positives and only hard negatives, effectively addressing this issue. As a result, our approach substantially outperforms prior art in terms of both accuracy and speed. Finally, the prevailing FROC evaluation over [1/8, 1/4, 1/2, 1, 2, 4, 8] false positives per scan, is far from ideal in real clinical environments. We suggest FROC over [1, 2, 4] false positives as a better metric.



### Convolutional Neural Networks for Aerial Multi-Label Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.05983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05983v1)
- **Published**: 2018-07-16 17:25:54+00:00
- **Updated**: 2018-07-16 17:25:54+00:00
- **Authors**: Amir Soleimani, Nasser M. Nasrabadi
- **Comment**: This paper has been accepted in the 21st International Conference on
  Information Fusion and would be indexed in IEEE
- **Journal**: None
- **Summary**: The low resolution of objects of interest in aerial images makes pedestrian detection and action detection extremely challenging tasks. Furthermore, using deep convolutional neural networks to process large images can be demanding in terms of computational requirements. In order to alleviate these challenges, we propose a two-step, yes and no question answering framework to find specific individuals doing one or multiple specific actions in aerial images. First, a deep object detector, Single Shot Multibox Detector (SSD), is used to generate object proposals from small aerial images. Second, another deep network, is used to learn a latent common sub-space which associates the high resolution aerial imagery and the pedestrian action labels that are provided by the human-based sources



### EC-Net: an Edge-aware Point set Consolidation Network
- **Arxiv ID**: http://arxiv.org/abs/1807.06010v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06010v1)
- **Published**: 2018-07-16 17:44:18+00:00
- **Updated**: 2018-07-16 17:44:18+00:00
- **Authors**: Lequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, Pheng-Ann Heng
- **Comment**: accepted by ECCV2018; project in https://yulequan.github.io/ec-net/
- **Journal**: None
- **Summary**: Point clouds obtained from 3D scans are typically sparse, irregular, and noisy, and required to be consolidated. In this paper, we present the first deep learning based edge-aware technique to facilitate the consolidation of point clouds. We design our network to process points grouped in local patches, and train it to learn and help consolidate points, deliberately for edges. To achieve this, we formulate a regression component to simultaneously recover 3D point coordinates and point-to-edge distances from upsampled features, and an edge-aware joint loss function to directly minimize distances from output points to 3D meshes and to edges. Compared with previous neural network based works, our consolidation is edge-aware. During the synthesis, our network can attend to the detected sharp edges and enable more accurate 3D reconstructions. Also, we trained our network on virtual scanned point clouds, demonstrated the performance of our method on both synthetic and real point clouds, presented various surface reconstruction results, and showed how our method outperforms the state-of-the-arts.



### Unlimited Road-scene Synthetic Annotation (URSA) Dataset
- **Arxiv ID**: http://arxiv.org/abs/1807.06056v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06056v1)
- **Published**: 2018-07-16 18:45:49+00:00
- **Updated**: 2018-07-16 18:45:49+00:00
- **Authors**: Matt Angus, Mohamed ElBalkini, Samin Khan, Ali Harakeh, Oles Andrienko, Cody Reading, Steven Waslander, Krzysztof Czarnecki
- **Comment**: Accepted in The 21st IEEE International Conference on Intelligent
  Transportation Systems
- **Journal**: None
- **Summary**: In training deep neural networks for semantic segmentation, the main limiting factor is the low amount of ground truth annotation data that is available in currently existing datasets. The limited availability of such data is due to the time cost and human effort required to accurately and consistently label real images on a pixel level. Modern sandbox video game engines provide open world environments where traffic and pedestrians behave in a pseudo-realistic manner. This caters well to the collection of a believable road-scene dataset. Utilizing open-source tools and resources found in single-player modding communities, we provide a method for persistent, ground truth, asset annotation of a game world. By collecting a synthetic dataset containing upwards of $1,000,000$ images, we demonstrate real-time, on-demand, ground truth data annotation capability of our method. Supplementing this synthetic data to Cityscapes dataset, we show that our data generation method provides qualitative as well as quantitative improvements---for training networks---over previous methods that use video games as surrogate.



### Weakly Supervised Deep Learning for Thoracic Disease Classification and Localization on Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/1807.06067v1
- **DOI**: 10.1145/3233547.3233573
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06067v1)
- **Published**: 2018-07-16 19:19:38+00:00
- **Updated**: 2018-07-16 19:19:38+00:00
- **Authors**: Chaochao Yan, Jiawen Yao, Ruoyu Li, Zheng Xu, Junzhou Huang
- **Comment**: 10 pages. Accepted by the ACM BCB 2018
- **Journal**: None
- **Summary**: Chest X-rays is one of the most commonly available and affordable radiological examinations in clinical practice. While detecting thoracic diseases on chest X-rays is still a challenging task for machine intelligence, due to 1) the highly varied appearance of lesion areas on X-rays from patients of different thoracic disease and 2) the shortage of accurate pixel-level annotations by radiologists for model training. Existing machine learning methods are unable to deal with the challenge that thoracic diseases usually happen in localized disease-specific areas. In this article, we propose a weakly supervised deep learning framework equipped with squeeze-and-excitation blocks, multi-map transfer, and max-min pooling for classifying thoracic diseases as well as localizing suspicious lesion regions. The comprehensive experiments and discussions are performed on the ChestX-ray14 dataset. Both numerical and visual results have demonstrated the effectiveness of the proposed model and its better performance against the state-of-the-art pipelines.



### A Dataset of Laryngeal Endoscopic Images with Comparative Study on Convolution Neural Network Based Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.06081v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.06081v4)
- **Published**: 2018-07-16 19:56:13+00:00
- **Updated**: 2020-09-21 13:42:59+00:00
- **Authors**: Max-Heinrich Laves, Jens Bicker, Lüder A. Kahrs, Tobias Ortmaier
- **Comment**: Accepted for publication in International Journal of Computer
  Assisted Radiology and Surgery
- **Journal**: None
- **Summary**: Purpose Automated segmentation of anatomical structures in medical image analysis is a prerequisite for autonomous diagnosis as well as various computer and robot aided interventions. Recent methods based on deep convolutional neural networks (CNN) have outperformed former heuristic methods. However, those methods were primarily evaluated on rigid, real-world environments. In this study, existing segmentation methods were evaluated for their use on a new dataset of transoral endoscopic exploration. Methods Four machine learning based methods SegNet, UNet, ENet and ErfNet were trained with supervision on a novel 7-class dataset of the human larynx. The dataset contains 536 manually segmented images from two patients during laser incisions. The Intersection-over-Union (IoU) evaluation metric was used to measure the accuracy of each method. Data augmentation and network ensembling were employed to increase segmentation accuracy. Stochastic inference was used to show uncertainties of the individual models. Patient-to-patient transfer was investigated using patient-specific fine-tuning. Results In this study, a weighted average ensemble network of UNet and ErfNet was best suited for the segmentation of laryngeal soft tissue with a mean IoU of 84.7 %. The highest efficiency was achieved by ENet with a mean inference time of 9.22 ms per image. It is shown that 10 additional images from a new patient are sufficient for patient-specific fine-tuning. Conclusion CNN-based methods for semantic segmentation are applicable to endoscopic images of laryngeal soft tissue. The segmentation can be used for active constraints or to monitor morphological changes and autonomously detect pathologies. Further improvements could be achieved by using a larger dataset or training the models in a self-supervised manner on additional unlabeled data.



### Repeatability of Multiparametric Prostate MRI Radiomics Features
- **Arxiv ID**: http://arxiv.org/abs/1807.06089v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1807.06089v2)
- **Published**: 2018-07-16 20:17:18+00:00
- **Updated**: 2018-11-15 05:31:01+00:00
- **Authors**: Michael Schwier, Joost van Griethuysen, Mark G Vangel, Steve Pieper, Sharon Peled, Clare M Tempany, Hugo JWL Aerts, Ron Kikinis, Fiona M Fennessy, Andrey Fedorov
- **Comment**: None
- **Journal**: None
- **Summary**: In this study we assessed the repeatability of the values of radiomics features for small prostate tumors using test-retest Multiparametric Magnetic Resonance Imaging (mpMRI) images. The premise of radiomics is that quantitative image features can serve as biomarkers characterizing disease. For such biomarkers to be useful, repeatability is a basic requirement, meaning its value must remain stable between two scans, if the conditions remain stable. We investigated repeatability of radiomics features under various preprocessing and extraction configurations including various image normalization schemes, different image pre-filtering, 2D vs 3D texture computation, and different bin widths for image discretization. Image registration as means to re-identify regions of interest across time points was evaluated against human-expert segmented regions in both time points. Even though we found many radiomics features and preprocessing combinations with a high repeatability (Intraclass Correlation Coefficient (ICC) > 0.85), our results indicate that overall the repeatability is highly sensitive to the processing parameters (under certain configurations, it can be below 0.0). Image normalization, using a variety of approaches considered, did not result in consistent improvements in repeatability. There was also no consistent improvement of repeatability through the use of pre-filtering options, or by using image registration between timepoints to improve consistency of the region of interest localization. Based on these results we urge caution when interpreting radiomics features and advise paying close attention to the processing configuration details of reported results. Furthermore, we advocate reporting all processing details in radiomics studies and strongly recommend making the implementation available.



### Rectification from Radially-Distorted Scales
- **Arxiv ID**: http://arxiv.org/abs/1807.06110v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06110v4)
- **Published**: 2018-07-16 21:03:36+00:00
- **Updated**: 2018-11-30 12:02:51+00:00
- **Authors**: James Pritts, Zuzana Kukelova, Viktor Larsson, Ondrej Chum
- **Comment**: pre-print
- **Journal**: None
- **Summary**: This paper introduces the first minimal solvers that jointly estimate lens distortion and affine rectification from repetitions of rigidly transformed coplanar local features. The proposed solvers incorporate lens distortion into the camera model and extend accurate rectification to wide-angle images that contain nearly any type of coplanar repeated content. We demonstrate a principled approach to generating stable minimal solvers by the Grobner basis method, which is accomplished by sampling feasible monomial bases to maximize numerical stability. Synthetic and real-image experiments confirm that the solvers give accurate rectifications from noisy measurements when used in a RANSAC-based estimator. The proposed solvers demonstrate superior robustness to noise compared to the state-of-the-art. The solvers work on scenes without straight lines and, in general, relax the strong assumptions on scene content made by the state-of-the-art. Accurate rectifications on imagery that was taken with narrow focal length to near fish-eye lenses demonstrate the wide applicability of the proposed method. The method is fully automated, and the code is publicly available at https://github.com/prittjam/repeats.



### Effective Use of Synthetic Data for Urban Scene Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.06132v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.06132v1)
- **Published**: 2018-07-16 22:10:09+00:00
- **Updated**: 2018-07-16 22:10:09+00:00
- **Authors**: Fatemeh Sadat Saleh, Mohammad Sadegh Aliakbarian, Mathieu Salzmann, Lars Petersson, Jose M. Alvarez
- **Comment**: Accepted in European Conference on Computer Vision (ECCV), 2018
- **Journal**: None
- **Summary**: Training a deep network to perform semantic segmentation requires large amounts of labeled data. To alleviate the manual effort of annotating real images, researchers have investigated the use of synthetic data, which can be labeled automatically. Unfortunately, a network trained on synthetic data performs relatively poorly on real images. While this can be addressed by domain adaptation, existing methods all require having access to real images during training. In this paper, we introduce a drastically different way to handle synthetic images that does not require seeing any real images at training time. Our approach builds on the observation that foreground and background classes are not affected in the same manner by the domain shift, and thus should be treated differently. In particular, the former should be handled in a detection-based manner to better account for the fact that, while their texture in synthetic images is not photo-realistic, their shape looks natural. Our experiments evidence the effectiveness of our approach on Cityscapes and CamVid with models trained on synthetic data only.



### Longitudinal detection of radiological abnormalities with time-modulated LSTM
- **Arxiv ID**: http://arxiv.org/abs/1807.06144v1
- **DOI**: 10.1007/978-3-030-00889-5_37
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.06144v1)
- **Published**: 2018-07-16 22:53:46+00:00
- **Updated**: 2018-07-16 22:53:46+00:00
- **Authors**: Ruggiero Santeramo, Samuel Withey, Giovanni Montana
- **Comment**: Submitted to 4th MICCAI Workshop on Deep Learning in Medical Imaging
  Analysis
- **Journal**: DLMIA/ML-CDS@MICCAI 2018
- **Summary**: Convolutional neural networks (CNNs) have been successfully employed in recent years for the detection of radiological abnormalities in medical images such as plain x-rays. To date, most studies use CNNs on individual examinations in isolation and discard previously available clinical information. In this study we set out to explore whether Long-Short-Term-Memory networks (LSTMs) can be used to improve classification performance when modelling the entire sequence of radiographs that may be available for a given patient, including their reports. A limitation of traditional LSTMs, though, is that they implicitly assume equally-spaced observations, whereas the radiological exams are event-based, and therefore irregularly sampled. Using both a simulated dataset and a large-scale chest x-ray dataset, we demonstrate that a simple modification of the LSTM architecture, which explicitly takes into account the time lag between consecutive observations, can boost classification performance. Our empirical results demonstrate improved detection of commonly reported abnormalities on chest x-rays such as cardiomegaly, consolidation, pleural effusion and hiatus hernia.



### Constraint-Based Visual Generation
- **Arxiv ID**: http://arxiv.org/abs/1807.09202v3
- **DOI**: 10.1007/978-3-030-30508-6_45
- **Categories**: **cs.LG**, cs.CV, cs.GR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.09202v3)
- **Published**: 2018-07-16 22:56:15+00:00
- **Updated**: 2019-09-24 14:37:06+00:00
- **Authors**: Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti, Marco Gori
- **Comment**: None
- **Journal**: None
- **Summary**: In the last few years the systematic adoption of deep learning to visual generation has produced impressive results that, amongst others, definitely benefit from the massive exploration of convolutional architectures. In this paper, we propose a general approach to visual generation that combines learning capabilities with logic descriptions of the target to be generated. The process of generation is regarded as a constrained satisfaction problem, where the constraints describe a set of properties that characterize the target. Interestingly, the constraints can also involve logic variables, while all of them are converted into real-valued functions by means of the t-norm theory. We use deep architectures to model the involved variables, and propose a computational scheme where the learning process carries out a satisfaction of the constraints. We propose some examples in which the theory can naturally be used, including the modeling of GAN and auto-encoders, and report promising results in problems with the generation of handwritten characters and face transformations.



