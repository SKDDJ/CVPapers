# Arxiv Papers in cs.CV on 2018-07-06
### Sparse Deep Neural Network Exact Solutions
- **Arxiv ID**: http://arxiv.org/abs/1807.03165v1
- **DOI**: 10.1109/HPEC.2018.8547742
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.03165v1)
- **Published**: 2018-07-06 00:47:12+00:00
- **Updated**: 2018-07-06 00:47:12+00:00
- **Authors**: Jeremy Kepner, Vijay Gadepally, Hayden Jananthan, Lauren Milechin, Sid Samsi
- **Comment**: 8 pages, 10 figures, accepted to IEEE HPEC 2018. arXiv admin note:
  text overlap with arXiv:1708.02937
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have emerged as key enablers of machine learning. Applying larger DNNs to more diverse applications is an important challenge. The computations performed during DNN training and inference are dominated by operations on the weight matrices describing the DNN. As DNNs incorporate more layers and more neurons per layers, these weight matrices may be required to be sparse because of memory limitations. Sparse DNNs are one possible approach, but the underlying theory is in the early stages of development and presents a number of challenges, including determining the accuracy of inference and selecting nonzero weights for training. Associative array algebra has been developed by the big data community to combine and extend database, matrix, and graph/network concepts for use in large, sparse data problems. Applying this mathematics to DNNs simplifies the formulation of DNN mathematics and reveals that DNNs are linear over oscillating semirings. This work uses associative array DNNs to construct exact solutions and corresponding perturbation models to the rectified linear unit (ReLU) DNN equations that can be used to construct test vectors for sparse DNN implementations over various precisions. These solutions can be used for DNN verification, theoretical explorations of DNN properties, and a starting point for the challenge of sparse training.



### Data Augmentation for Detection of Architectural Distortion in Digital Mammography using Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1807.03167v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.03167v1)
- **Published**: 2018-07-06 02:12:49+00:00
- **Updated**: 2018-07-06 02:12:49+00:00
- **Authors**: Arthur C. Costa, Helder C. R. Oliveira, Juliana H. Catani, Nestor de Barros, Carlos F. E. Melo, Marcelo A. C. Vieira
- **Comment**: None
- **Journal**: None
- **Summary**: Early detection of breast cancer can increase treatment efficiency. Architectural Distortion (AD) is a very subtle contraction of the breast tissue and may represent the earliest sign of cancer. Since it is very likely to be unnoticed by radiologists, several approaches have been proposed over the years but none using deep learning techniques. To train a Convolutional Neural Network (CNN), which is a deep neural architecture, is necessary a huge amount of data. To overcome this problem, this paper proposes a data augmentation approach applied to clinical image dataset to properly train a CNN. Results using receiver operating characteristic analysis showed that with a very limited dataset we could train a CNN to detect AD in digital mammography with area under the curve (AUC = 0.74).



### Progressive Spatial Recurrent Neural Network for Intra Prediction
- **Arxiv ID**: http://arxiv.org/abs/1807.02232v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02232v2)
- **Published**: 2018-07-06 03:13:55+00:00
- **Updated**: 2019-05-25 14:59:34+00:00
- **Authors**: Yueyu Hu, Wenhan Yang, Mading Li, Jiaying Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Intra prediction is an important component of modern video codecs, which is able to efficiently squeeze out the spatial redundancy in video frames. With preceding pixels as the context, traditional intra prediction schemes generate linear predictions based on several predefined directions (i.e. modes) for blocks to be encoded. However, these modes are relatively simple and their predictions may fail when facing blocks with complex textures, which leads to additional bits encoding the residue. In this paper, we design a Progressive Spatial Recurrent Neural Network (PS-RNN) that learns to conduct intra prediction. Specifically, our PS-RNN consists of three spatial recurrent units and progressively generates predictions by passing information along from preceding contents to blocks to be encoded. To make our network generate predictions considering both distortion and bit-rate, we propose to use Sum of Absolute Transformed Difference (SATD) as the loss function to train PS-RNN since SATD is able to measure rate-distortion cost of encoding a residue block. Moreover, our method supports variable-block-size for intra prediction, which is more practical in real coding conditions. The proposed intra prediction scheme achieves on average 2.5% bit-rate reduction on variable-block-size settings under the same reconstruction quality compared with HEVC.



### Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes
- **Arxiv ID**: http://arxiv.org/abs/1807.02242v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02242v2)
- **Published**: 2018-07-06 03:40:11+00:00
- **Updated**: 2018-08-01 06:49:14+00:00
- **Authors**: Pengyuan Lyu, Minghui Liao, Cong Yao, Wenhao Wu, Xiang Bai
- **Comment**: To appear in ECCV 2018
- **Journal**: None
- **Summary**: Recently, models based on deep neural networks have dominated the fields of scene text detection and recognition. In this paper, we investigate the problem of scene text spotting, which aims at simultaneous text detection and recognition in natural images. An end-to-end trainable neural network model for scene text spotting is proposed. The proposed model, named as Mask TextSpotter, is inspired by the newly published work Mask R-CNN. Different from previous methods that also accomplish text spotting with end-to-end trainable deep neural networks, Mask TextSpotter takes advantage of simple and smooth end-to-end learning procedure, in which precise text detection and recognition are acquired via semantic segmentation. Moreover, it is superior to previous methods in handling text instances of irregular shapes, for example, curved text. Experiments on ICDAR2013, ICDAR2015 and Total-Text demonstrate that the proposed method achieves state-of-the-art results in both scene text detection and end-to-end text recognition tasks.



### Adversarial Learning for Fine-grained Image Search
- **Arxiv ID**: http://arxiv.org/abs/1807.02247v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02247v1)
- **Published**: 2018-07-06 04:03:11+00:00
- **Updated**: 2018-07-06 04:03:11+00:00
- **Authors**: Kevin Lin, Fan Yang, Qiaosong Wang, Robinson Piramuthu
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-grained image search is still a challenging problem due to the difficulty in capturing subtle differences regardless of pose variations of objects from fine-grained categories. In practice, a dynamic inventory with new fine-grained categories adds another dimension to this challenge. In this work, we propose an end-to-end network, called FGGAN, that learns discriminative representations by implicitly learning a geometric transformation from multi-view images for fine-grained image search. We integrate a generative adversarial network (GAN) that can automatically handle complex view and pose variations by converting them to a canonical view without any predefined transformations. Moreover, in an open-set scenario, our network is able to better match images from unseen and unknown fine-grained categories. Extensive experiments on two public datasets and a newly collected dataset have demonstrated the outstanding robust performance of the proposed FGGAN in both closed-set and open-set scenarios, providing as much as 10% relative improvement compared to baselines.



### Face-Cap: Image Captioning using Facial Expression Analysis
- **Arxiv ID**: http://arxiv.org/abs/1807.02250v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02250v2)
- **Published**: 2018-07-06 04:12:20+00:00
- **Updated**: 2019-01-25 13:30:42+00:00
- **Authors**: Omid Mohamad Nezami, Mark Dras, Peter Anderson, Len Hamey
- **Comment**: None
- **Journal**: None
- **Summary**: Image captioning is the process of generating a natural language description of an image. Most current image captioning models, however, do not take into account the emotional aspect of an image, which is very relevant to activities and interpersonal relationships represented therein. Towards developing a model that can produce human-like captions incorporating these, we use facial expression features extracted from images including human faces, with the aim of improving the descriptive ability of the model. In this work, we present two variants of our Face-Cap model, which embed facial expression features in different ways, to generate image captions. Using all standard evaluation metrics, our Face-Cap models outperform a state-of-the-art baseline model for generating image captions when applied to an image caption dataset extracted from the standard Flickr 30K dataset, consisting of around 11K images containing faces. An analysis of the captions finds that, perhaps surprisingly, the improvement in caption quality appears to come not from the addition of adjectives linked to emotional aspects of the images, but from more variety in the actions described in the captions.



### Minutia Texture Cylinder Codes for fingerprint matching
- **Arxiv ID**: http://arxiv.org/abs/1807.02251v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02251v1)
- **Published**: 2018-07-06 04:25:18+00:00
- **Updated**: 2018-07-06 04:25:18+00:00
- **Authors**: Wajih Ullah Baig, Umar Munir, Waqas Ellahi, Adeel Ejaz, Kashif Sardar
- **Comment**: None
- **Journal**: None
- **Summary**: Minutia Cylinder Codes (MCC) are minutiae based fingerprint descriptors that take into account minutiae information in a fingerprint image for fingerprint matching. In this paper, we present a modification to the underlying information of the MCC descriptor and show that using different features, the accuracy of matching is highly affected by such changes. MCC originally being a minutia only descriptor is transformed into a texture descriptor. The transformation is from minutiae angular information to orientation, frequency and energy information using Short Time Fourier Transform (STFT) analysis. The minutia cylinder codes are converted to minutiae texture cylinder codes (MTCC). Based on a fixed set of parameters, the proposed changes to MCC show improved performance on FVC 2002 and 2004 data sets and surpass the traditional MCC performance.



### Dynamic Multimodal Instance Segmentation guided by natural language queries
- **Arxiv ID**: http://arxiv.org/abs/1807.02257v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02257v2)
- **Published**: 2018-07-06 05:21:06+00:00
- **Updated**: 2018-07-22 22:31:18+00:00
- **Authors**: Edgar Margffoy-Tuay, Juan C. Pérez, Emilio Botero, Pablo Arbeláez
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of segmenting an object given a natural language expression that describes it. Current techniques tackle this task by either (\textit{i}) directly or recursively merging linguistic and visual information in the channel dimension and then performing convolutions; or by (\textit{ii}) mapping the expression to a space in which it can be thought of as a filter, whose response is directly related to the presence of the object at a given spatial coordinate in the image, so that a convolution can be applied to look for the object. We propose a novel method that integrates these two insights in order to fully exploit the recursive nature of language. Additionally, during the upsampling process, we take advantage of the intermediate information generated when downsampling the image, so that detailed segmentations can be obtained. We compare our method against the state-of-the-art approaches in four standard datasets, in which it surpasses all previous methods in six of eight of the splits for this task.



### Parallel Convolutional Networks for Image Recognition via a Discriminator
- **Arxiv ID**: http://arxiv.org/abs/1807.02265v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02265v3)
- **Published**: 2018-07-06 06:08:22+00:00
- **Updated**: 2018-09-25 05:33:08+00:00
- **Authors**: Shiqi Yang, Gang Peng
- **Comment**: Accepted by ACCV 2018
- **Journal**: None
- **Summary**: In this paper, we introduce a simple but quite effective recognition framework dubbed D-PCN, aiming at enhancing feature extracting ability of CNN. The framework consists of two parallel CNNs, a discriminator and an extra classifier which takes integrated features from parallel networks and gives final prediction. The discriminator is core which drives parallel networks to focus on different regions and learn different representations. The corresponding training strategy is introduced to ensures utilization of discriminator. We validate D-PCN with several CNN models on benchmark datasets: CIFAR-100, and ImageNet, D-PCN enhances all models. In particular it yields state of the art performance on CIFAR-100 compared with related works. We also conduct visualization experiment on fine-grained Stanford Dogs dataset to verify our motivation. Additionally, we apply D-PCN for segmentation on PASCAL VOC 2012 and also find promotion.



### Combining SLAM with muti-spectral photometric stereo for real-time dense 3D reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1807.02294v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02294v1)
- **Published**: 2018-07-06 07:40:17+00:00
- **Updated**: 2018-07-06 07:40:17+00:00
- **Authors**: Yuanhong Xu, Pei Dong, Junyu Dong, Lin Qi
- **Comment**: None
- **Journal**: None
- **Summary**: Obtaining dense 3D reconstrution with low computational cost is one of the important goals in the field of SLAM. In this paper we propose a dense 3D reconstruction framework from monocular multispectral video sequences using jointly semi-dense SLAM and Multispectral Photometric Stereo approaches. Starting from multispectral video, SALM (a) reconstructs a semi-dense 3D shape that will be densified;(b) recovers relative sparse depth map that is then fed as prioris into optimization-based multispectral photometric stereo for a more accurate dense surface normal recovery;(c)obtains camera pose that is subsequently used for conversion of view in the process of fusion where we combine the relative sparse point cloud with the dense surface normal using the automated cross-scale fusion method proposed in this paper to get a dense point cloud with subtle texture information. Experiments show that our method can effectively obtain denser 3D reconstructions.



### Graph of brain structures grading for early detection of Alzheimer's disease
- **Arxiv ID**: http://arxiv.org/abs/1807.03173v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.03173v1)
- **Published**: 2018-07-06 08:43:31+00:00
- **Updated**: 2018-07-06 08:43:31+00:00
- **Authors**: Kilian Hett, Vinh-Thong Ta, Jose Vicente Manjon, Pierrick Coupé
- **Comment**: None
- **Journal**: Medical Image Computing and Computer-Assisted Intervention, Sep
  2018, GRANADA, Spain
- **Summary**: Alzheimer's disease is the most common dementia leading to an irreversible neurodegenerative process. To date, subject revealed advanced brain structural alterations when the diagnosis is established. Therefore, an earlier diagnosis of this dementia is crucial although it is a challenging task. Recently, many studies have proposed biomarkers to perform early detection of Alzheimer's disease. Some of them have proposed methods based on inter-subject similarity while other approaches have investigated framework using intra-subject variability. In this work, we propose a novel framework combining both approaches within an efficient graph of brain structures grading. Subsequently, we demonstrate the competitive performance of the proposed method compared to state-of-the-art methods.



### Optimal Sensor Data Fusion Architecture for Object Detection in Adverse Weather Conditions
- **Arxiv ID**: http://arxiv.org/abs/1807.02323v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02323v1)
- **Published**: 2018-07-06 09:21:58+00:00
- **Updated**: 2018-07-06 09:21:58+00:00
- **Authors**: Andreas Pfeuffer, Klaus Dietmayer
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: 21st International Conference on Information Fusion (FUSION)
  (2008) 2592-2599
- **Summary**: A good and robust sensor data fusion in diverse weather conditions is a quite challenging task. There are several fusion architectures in the literature, e.g. the sensor data can be fused right at the beginning (Early Fusion), or they can be first processed separately and then concatenated later (Late Fusion). In this work, different fusion architectures are compared and evaluated by means of object detection tasks, in which the goal is to recognize and localize predefined objects in a stream of data. Usually, state-of-the-art object detectors based on neural networks are highly optimized for good weather conditions, since the well-known benchmarks only consist of sensor data recorded in optimal weather conditions. Therefore, the performance of these approaches decreases enormously or even fails in adverse weather conditions. In this work, different sensor fusion architectures are compared for good and adverse weather conditions for finding the optimal fusion architecture for diverse weather situations. A new training strategy is also introduced such that the performance of the object detector is greatly enhanced in adverse weather scenarios or if a sensor fails. Furthermore, the paper responds to the question if the detection accuracy can be increased further by providing the neural network with a-priori knowledge such as the spatial calibration of the sensors.



### Deep Back Projection for Sparse-View CT Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1807.02370v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.02370v1)
- **Published**: 2018-07-06 12:06:19+00:00
- **Updated**: 2018-07-06 12:06:19+00:00
- **Authors**: Dong Hye Ye, Gregery T. Buzzard, Max Ruby, Charles A. Bouman
- **Comment**: GlobalSIP 2018
- **Journal**: None
- **Summary**: Filtered back projection (FBP) is a classical method for image reconstruction from sinogram CT data. FBP is computationally efficient but produces lower quality reconstructions than more sophisticated iterative methods, particularly when the number of views is lower than the number required by the Nyquist rate. In this paper, we use a deep convolutional neural network (CNN) to produce high-quality reconstructions directly from sinogram data. A primary novelty of our approach is that we first back project each view separately to form a stack of back projections and then feed this stack as input into the convolutional neural network. These single-view back projections convert the encoding of sinogram data into the appropriate spatial location, which can then be leveraged by the spatial invariance of the CNN to learn the reconstruction effectively. We demonstrate the benefit of our CNN based back projection on simulated sparse-view CT data over classical FBP.



### End-to-End Race Driving with Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.02371v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.02371v2)
- **Published**: 2018-07-06 12:08:53+00:00
- **Updated**: 2018-08-31 13:28:49+00:00
- **Authors**: Maximilian Jaritz, Raoul de Charette, Marin Toromanoff, Etienne Perot, Fawzi Nashashibi
- **Comment**: ICRA 2018
- **Journal**: None
- **Summary**: We present research using the latest reinforcement learning algorithm for end-to-end driving without any mediated perception (object recognition, scene understanding). The newly proposed reward and learning strategies lead together to faster convergence and more robust driving using only RGB image from a forward facing camera. An Asynchronous Actor Critic (A3C) framework is used to learn the car control in a physically and graphically realistic rally game, with the agents evolving simultaneously on tracks with a variety of road structures (turns, hills), graphics (seasons, location) and physics (road adherence). A thorough evaluation is conducted and generalization is proven on unseen tracks and using legal speed limits. Open loop tests on real sequences of images show some domain adaption capability of our method.



### Reversed Active Learning based Atrous DenseNet for Pathological Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1807.02420v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02420v1)
- **Published**: 2018-07-06 13:57:48+00:00
- **Updated**: 2018-07-06 13:57:48+00:00
- **Authors**: Yuexiang Li, Xinpeng Xie, Linlin Shen, Shaoxiong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Witnessed the development of deep learning in recent years, increasing number of researches try to adopt deep learning model for medical image analysis. However, the usage of deep learning networks for the pathological image analysis encounters several challenges, e.g. high resolution (gigapixel) of pathological images and lack of annotations of cancer areas. To address the challenges, we proposed a complete framework for the pathological image classification, which consists of a novel training strategy, namely reversed active learning (RAL), and an advanced network, namely atrous DenseNet (ADN). The proposed RAL can remove the mislabel patches in the training set. The refined training set can then be used to train widely used deep learning networks, e.g. VGG-16, ResNets, etc. A novel deep learning network, i.e. atrous DenseNet (ADN), is also proposed for the classification of pathological images. The proposed ADN achieves multi-scale feature extraction by integrating the atrous convolutions to the Dense Block. The proposed RAL and ADN have been evaluated on two pathological datasets, i.e. BACH and CCG. The experimental results demonstrate the excellent performance of the proposed ADN + RAL framework, i.e. the average patch-level ACAs of 94.10% and 92.05% on BACH and CCG validation sets were achieved.



### Deep Sequential Segmentation of Organs in Volumetric Medical Scans
- **Arxiv ID**: http://arxiv.org/abs/1807.02437v2
- **DOI**: 10.1109/TMI.2018.2881678
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02437v2)
- **Published**: 2018-07-06 14:48:04+00:00
- **Updated**: 2019-03-11 10:13:15+00:00
- **Authors**: Alexey Novikov, David Major, Maria Wimmer, Dimitrios Lenis, Katja Bühler
- **Comment**: None
- **Journal**: Published in IEEE Transactions on Medical Imaging on 16 November
  2018, URL:
  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8537944&isnumber=4359023
- **Summary**: Segmentation in 3D scans is playing an increasingly important role in current clinical practice supporting diagnosis, tissue quantification, or treatment planning. The current 3D approaches based on convolutional neural networks usually suffer from at least three main issues caused predominantly by implementation constraints - first, they require resizing the volume to the lower-resolutional reference dimensions, second, the capacity of such approaches is very limited due to memory restrictions, and third, all slices of volumes have to be available at any given training or testing time. We address these problems by a U-Net-like architecture consisting of bidirectional convolutional LSTM and convolutional, pooling, upsampling and concatenation layers enclosed into time-distributed wrappers. Our network can either process the full volumes in a sequential manner, or segment slabs of slices on demand. We demonstrate performance of our architecture on vertebrae and liver segmentation tasks in 3D CT scans.



### Tangent Convolutions for Dense Prediction in 3D
- **Arxiv ID**: http://arxiv.org/abs/1807.02443v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02443v1)
- **Published**: 2018-07-06 15:14:51+00:00
- **Updated**: 2018-07-06 15:14:51+00:00
- **Authors**: Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, Qian-Yi Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach to semantic scene analysis using deep convolutional networks. Our approach is based on tangent convolutions - a new construction for convolutional networks on 3D data. In contrast to volumetric approaches, our method operates directly on surface geometry. Crucially, the construction is applicable to unstructured point clouds and other noisy real-world data. We show that tangent convolutions can be evaluated efficiently on large-scale point clouds with millions of points. Using tangent convolutions, we design a deep fully-convolutional network for semantic segmentation of 3D point clouds, and apply it to challenging real-world datasets of indoor and outdoor 3D environments. Experimental results show that the presented approach outperforms other recent deep network constructions in detailed analysis of large 3D scenes.



### Multi-modal Non-line-of-sight Passive Imaging
- **Arxiv ID**: http://arxiv.org/abs/1807.02444v2
- **DOI**: 10.1109/TIP.2019.2896517
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1807.02444v2)
- **Published**: 2018-07-06 15:15:03+00:00
- **Updated**: 2019-03-03 04:23:29+00:00
- **Authors**: Andre Beckus, Alexandru Tamasan, George K. Atia
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, vol. 28, no. 7, pp.
  3372-3382, July 2019
- **Summary**: We consider the non-line-of-sight (NLOS) imaging of an object using the light reflected off a diffusive wall. The wall scatters incident light such that a lens is no longer useful to form an image. Instead, we exploit the 4D spatial coherence function to reconstruct a 2D projection of the obscured object. The approach is completely passive in the sense that no control over the light illuminating the object is assumed and is compatible with the partially coherent fields ubiquitous in both the indoor and outdoor environments. We formulate a multi-criteria convex optimization problem for reconstruction, which fuses the reflected field's intensity and spatial coherence information at different scales. Our formulation leverages established optics models of light propagation and scattering and exploits the sparsity common to many images in different bases. We also develop an algorithm based on the alternating direction method of multipliers to efficiently solve the convex program proposed. A means for analyzing the null space of the measurement matrices is provided as well as a means for weighting the contribution of individual measurements to the reconstruction. This paper holds promise to advance passive imaging in the challenging NLOS regimes in which the intensity does not necessarily retain distinguishable features and provides a framework for multi-modal information fusion for efficient scene reconstruction.



### A Fully Convolutional Two-Stream Fusion Network for Interactive Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.02480v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02480v2)
- **Published**: 2018-07-06 16:48:31+00:00
- **Updated**: 2018-10-02 19:43:11+00:00
- **Authors**: Yang Hu, Andrea Soltoggio, Russell Lock, Steve Carter
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel fully convolutional two-stream fusion network (FCTSFN) for interactive image segmentation. The proposed network includes two sub-networks: a two-stream late fusion network (TSLFN) that predicts the foreground at a reduced resolution, and a multi-scale refining network (MSRN) that refines the foreground at full resolution. The TSLFN includes two distinct deep streams followed by a fusion network. The intuition is that, since user interactions are more direct information on foreground/background than the image itself, the two-stream structure of the TSLFN reduces the number of layers between the pure user interaction features and the network output, allowing the user interactions to have a more direct impact on the segmentation result. The MSRN fuses the features from different layers of TSLFN with different scales, in order to seek the local to global information on the foreground to refine the segmentation result at full resolution. We conduct comprehensive experiments on four benchmark datasets. The results show that the proposed network achieves competitive performance compared to current state-of-the-art interactive image segmentation methods



### YouTube for Patient Education: A Deep Learning Approach for Understanding Medical Knowledge from User-Generated Videos
- **Arxiv ID**: http://arxiv.org/abs/1807.03179v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.03179v1)
- **Published**: 2018-07-06 17:19:26+00:00
- **Updated**: 2018-07-06 17:19:26+00:00
- **Authors**: Xiao Liu, Bin Zhang, Anjana Susarla, Rema Padman
- **Comment**: None
- **Journal**: None
- **Summary**: YouTube presents an unprecedented opportunity to explore how machine learning methods can improve healthcare information dissemination. We propose an interdisciplinary lens that synthesizes machine learning methods with healthcare informatics themes to address the critical issue of developing a scalable algorithmic solution to evaluate videos from a health literacy and patient education perspective. We develop a deep learning method to understand the level of medical knowledge encoded in YouTube videos. Preliminary results suggest that we can extract medical knowledge from YouTube videos and classify videos according to the embedded knowledge with satisfying performance. Deep learning methods show great promise in knowledge extraction, natural language understanding, and image classification, especially in an era of patient-centric care and precision medicine.



### From Rank Estimation to Rank Approximation: Rank Residual Constraint for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1807.02504v11
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02504v11)
- **Published**: 2018-07-06 17:43:20+00:00
- **Updated**: 2020-02-04 02:49:01+00:00
- **Authors**: Zhiyuan Zha, Xin Yuan, Bihan Wen, Jiantao Zhou, Jiachao Zhang, Ce Zhu
- **Comment**: None
- **Journal**: IEEE Transaction on Image Processing 2020
- **Summary**: In this paper, we propose a novel approach to the rank minimization problem, termed rank residual constraint (RRC) model. Different from existing low-rank based approaches, such as the well-known nuclear norm minimization (NNM) and the weighted nuclear norm minimization (WNNM), which estimate the underlying low-rank matrix directly from the corrupted observations, we progressively approximate the underlying low-rank matrix via minimizing the rank residual. Through integrating the image nonlocal self-similarity (NSS) prior with the proposed RRC model, we apply it to image restoration tasks, including image denoising and image compression artifacts reduction. Towards this end, we first obtain a good reference of the original image groups by using the image NSS prior, and then the rank residual of the image groups between this reference and the degraded image is minimized to achieve a better estimate to the desired image. In this manner, both the reference and the estimated image are updated gradually and jointly in each iteration. Based on the group-based sparse representation model, we further provide a theoretical analysis on the feasibility of the proposed RRC model. Experimental results demonstrate that the proposed RRC model outperforms many state-of-the-art schemes in both the objective and perceptual quality.



### VLASE: Vehicle Localization by Aggregating Semantic Edges
- **Arxiv ID**: http://arxiv.org/abs/1807.02536v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.02536v1)
- **Published**: 2018-07-06 18:15:06+00:00
- **Updated**: 2018-07-06 18:15:06+00:00
- **Authors**: Xin Yu, Sagar Chaturvedi, Chen Feng, Yuichi Taguchi, Teng-Yok Lee, Clinton Fernandes, Srikumar Ramalingam
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose VLASE, a framework to use semantic edge features from images to achieve on-road localization. Semantic edge features denote edge contours that separate pairs of distinct objects such as building-sky, road- sidewalk, and building-ground. While prior work has shown promising results by utilizing the boundary between prominent classes such as sky and building using skylines, we generalize this approach to consider semantic edge features that arise from 19 different classes. Our localization algorithm is simple, yet very powerful. We extract semantic edge features using a recently introduced CASENet architecture and utilize VLAD framework to perform image retrieval. Our experiments show that we achieve improvement over some of the state-of-the-art localization algorithms such as SIFT-VLAD and its deep variant NetVLAD. We use ablation study to study the importance of different semantic classes and show that our unified approach achieves better performance compared to individual prominent features such as skylines.



### Automated and Interpretable Patient ECG Profiles for Disease Detection, Tracking, and Discovery
- **Arxiv ID**: http://arxiv.org/abs/1807.02569v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02569v1)
- **Published**: 2018-07-06 21:12:12+00:00
- **Updated**: 2018-07-06 21:12:12+00:00
- **Authors**: Geoffrey H. Tison, Jeffrey Zhang, Francesca N. Delling, Rahul C. Deo
- **Comment**: 13 pages, 6 figures, 1 Table + Supplement
- **Journal**: None
- **Summary**: The electrocardiogram or ECG has been in use for over 100 years and remains the most widely performed diagnostic test to characterize cardiac structure and electrical activity. We hypothesized that parallel advances in computing power, innovations in machine learning algorithms, and availability of large-scale digitized ECG data would enable extending the utility of the ECG beyond its current limitations, while at the same time preserving interpretability, which is fundamental to medical decision-making. We identified 36,186 ECGs from the UCSF database that were 1) in normal sinus rhythm and 2) would enable training of specific models for estimation of cardiac structure or function or detection of disease. We derived a novel model for ECG segmentation using convolutional neural networks (CNN) and Hidden Markov Models (HMM) and evaluated its output by comparing electrical interval estimates to 141,864 measurements from the clinical workflow. We built a 725-element patient-level ECG profile using downsampled segmentation data and trained machine learning models to estimate left ventricular mass, left atrial volume, mitral annulus e' and to detect and track four diseases: pulmonary arterial hypertension (PAH), hypertrophic cardiomyopathy (HCM), cardiac amyloid (CA), and mitral valve prolapse (MVP). CNN-HMM derived ECG segmentation agreed with clinical estimates, with median absolute deviations (MAD) as a fraction of observed value of 0.6% for heart rate and 4% for QT interval. Patient-level ECG profiles enabled quantitative estimates of left ventricular and mitral annulus e' velocity with good discrimination in binary classification models of left ventricular hypertrophy and diastolic function. Models for disease detection ranged from AUROC of 0.94 to 0.77 for MVP. Top-ranked variables for all models included known ECG characteristics along with novel predictors of these traits/diseases.



### Deep Virtual Stereo Odometry: Leveraging Deep Depth Prediction for Monocular Direct Sparse Odometry
- **Arxiv ID**: http://arxiv.org/abs/1807.02570v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02570v2)
- **Published**: 2018-07-06 21:14:31+00:00
- **Updated**: 2018-07-25 16:24:02+00:00
- **Authors**: Nan Yang, Rui Wang, Jörg Stückler, Daniel Cremers
- **Comment**: To appear in ECCV 2018, Munich. 17 pages including references, 7
  figures, 4 tables. Supplementary material:
  https://vision.in.tum.de/members/yangn
- **Journal**: None
- **Summary**: Monocular visual odometry approaches that purely rely on geometric cues are prone to scale drift and require sufficient motion parallax in successive frames for motion estimation and 3D reconstruction. In this paper, we propose to leverage deep monocular depth prediction to overcome limitations of geometry-based monocular visual odometry. To this end, we incorporate deep depth predictions into Direct Sparse Odometry (DSO) as direct virtual stereo measurements. For depth prediction, we design a novel deep network that refines predicted depth from a single image in a two-stage process. We train our network in a semi-supervised way on photoconsistency in stereo images and on consistency with accurate sparse depth reconstructions from Stereo DSO. Our deep predictions excel state-of-the-art approaches for monocular depth on the KITTI benchmark. Moreover, our Deep Virtual Stereo Odometry clearly exceeds previous monocular and deep learning based methods in accuracy. It even achieves comparable performance to the state-of-the-art stereo methods, while only relying on a single camera.



### Guided Proceduralization: Optimizing Geometry Processing and Grammar Extraction for Architectural Models
- **Arxiv ID**: http://arxiv.org/abs/1807.02578v1
- **DOI**: 10.1016/j.cag.2018.05.013
- **Categories**: **cs.GR**, cs.CV, 68U05, 65D18
- **Links**: [PDF](http://arxiv.org/pdf/1807.02578v1)
- **Published**: 2018-07-06 22:22:53+00:00
- **Updated**: 2018-07-06 22:22:53+00:00
- **Authors**: Ilke Demir, Daniel G. Aliaga
- **Comment**: None
- **Journal**: Computers & Graphics, Volume 74, 2018, Pages 257-267, ISSN
  0097-8493
- **Summary**: We describe a guided proceduralization framework that optimizes geometry processing on architectural input models to extract target grammars. We aim to provide efficient artistic workflows by creating procedural representations from existing 3D models, where the procedural expressiveness is controlled by the user. Architectural reconstruction and modeling tasks have been handled as either time consuming manual processes or procedural generation with difficult control and artistic influence. We bridge the gap between creation and generation by converting existing manually modeled architecture to procedurally editable parametrized models, and carrying the guidance to procedural domain by letting the user define the target procedural representation. Additionally, we propose various applications of such procedural representations, including guided completion of point cloud models, controllable 3D city modeling, and other benefits of procedural modeling.



### Fast and Accurate Point Cloud Registration using Trees of Gaussian Mixtures
- **Arxiv ID**: http://arxiv.org/abs/1807.02587v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02587v1)
- **Published**: 2018-07-06 23:44:51+00:00
- **Updated**: 2018-07-06 23:44:51+00:00
- **Authors**: Ben Eckart, Kihwan Kim, Jan Kautz
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Point cloud registration sits at the core of many important and challenging 3D perception problems including autonomous navigation, SLAM, object/scene recognition, and augmented reality. In this paper, we present a new registration algorithm that is able to achieve state-of-the-art speed and accuracy through its use of a hierarchical Gaussian Mixture Model (GMM) representation. Our method constructs a top-down multi-scale representation of point cloud data by recursively running many small-scale data likelihood segmentations in parallel on a GPU. We leverage the resulting representation using a novel PCA-based optimization criterion that adaptively finds the best scale to perform data association between spatial subsets of point cloud data. Compared to previous Iterative Closest Point and GMM-based techniques, our tree-based point association algorithm performs data association in logarithmic-time while dynamically adjusting the level of detail to best match the complexity and spatial distribution characteristics of local scene geometry. In addition, unlike other GMM methods that restrict covariances to be isotropic, our new PCA-based optimization criterion well-approximates the true MLE solution even when fully anisotropic Gaussian covariances are used. Efficient data association, multi-scale adaptability, and a robust MLE approximation produce an algorithm that is up to an order of magnitude both faster and more accurate than current state-of-the-art on a wide variety of 3D datasets captured from LiDAR to structured light.



### Generative Probabilistic Novelty Detection with Adversarial Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1807.02588v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.02588v2)
- **Published**: 2018-07-06 23:46:30+00:00
- **Updated**: 2018-11-10 01:39:29+00:00
- **Authors**: Stanislav Pidhorskyi, Ranya Almohsen, Donald A Adjeroh, Gianfranco Doretto
- **Comment**: None
- **Journal**: None
- **Summary**: Novelty detection is the problem of identifying whether a new data point is considered to be an inlier or an outlier. We assume that training data is available to describe only the inlier distribution. Recent approaches primarily leverage deep encoder-decoder network architectures to compute a reconstruction error that is used to either compute a novelty score or to train a one-class classifier. While we too leverage a novel network of that kind, we take a probabilistic approach and effectively compute how likely is that a sample was generated by the inlier distribution. We achieve this with two main contributions. First, we make the computation of the novelty probability feasible because we linearize the parameterized manifold capturing the underlying structure of the inlier distribution, and show how the probability factorizes and can be computed with respect to local coordinates of the manifold tangent space. Second, we improved the training of the autoencoder network. An extensive set of results show that the approach achieves state-of-the-art results on several benchmark datasets.



