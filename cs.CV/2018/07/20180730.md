# Arxiv Papers in cs.CV on 2018-07-30
### Geo-Supervised Visual Depth Prediction
- **Arxiv ID**: http://arxiv.org/abs/1807.11130v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.11130v4)
- **Published**: 2018-07-30 00:31:42+00:00
- **Updated**: 2019-06-11 20:49:52+00:00
- **Authors**: Xiaohan Fei, Alex Wong, Stefano Soatto
- **Comment**: ICRA 2019, RA-L 2019
- **Journal**: None
- **Summary**: We propose using global orientation from inertial measurements, and the bias it induces on the shape of objects populating the scene, to inform visual 3D reconstruction. We test the effect of using the resulting prior in depth prediction from a single image, where the normal vectors to surfaces of objects of certain classes tend to align with gravity or be orthogonal to it. Adding such a prior to baseline methods for monocular depth prediction yields improvements beyond the state-of-the-art and illustrates the power of gravity as a supervisory signal.



### Occluded Joints Recovery in 3D Human Pose Estimation based on Distance Matrix
- **Arxiv ID**: http://arxiv.org/abs/1807.11147v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11147v1)
- **Published**: 2018-07-30 02:32:38+00:00
- **Updated**: 2018-07-30 02:32:38+00:00
- **Authors**: Xiang Guo, Yuchao Dai
- **Comment**: Accepted by ICPR 2018
- **Journal**: None
- **Summary**: Albeit the recent progress in single image 3D human pose estimation due to the convolutional neural network, it is still challenging to handle real scenarios such as highly occluded scenes. In this paper, we propose to address the problem of single image 3D human pose estimation with occluded measurements by exploiting the Euclidean distance matrix (EDM). Specifically, we present two approaches based on EDM, which could effectively handle occluded joints in 2D images. The first approach is based on 2D-to-2D distance matrix regression achieved by a simple CNN architecture. The second approach is based on sparse coding along with a learned over-complete dictionary. Experiments on the Human3.6M dataset show the excellent performance of these two approaches in recovering occluded observations and demonstrate the improvements in accuracy for 3D human pose estimation with occluded joints.



### Pose Guided Human Video Generation
- **Arxiv ID**: http://arxiv.org/abs/1807.11152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11152v1)
- **Published**: 2018-07-30 02:46:43+00:00
- **Updated**: 2018-07-30 02:46:43+00:00
- **Authors**: Ceyuan Yang, Zhe Wang, Xinge Zhu, Chen Huang, Jianping Shi, Dahua Lin
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: Due to the emergence of Generative Adversarial Networks, video synthesis has witnessed exceptional breakthroughs. However, existing methods lack a proper representation to explicitly control the dynamics in videos. Human pose, on the other hand, can represent motion patterns intrinsically and interpretably, and impose the geometric constraints regardless of appearance. In this paper, we propose a pose guided method to synthesize human videos in a disentangled way: plausible motion prediction and coherent appearance generation. In the first stage, a Pose Sequence Generative Adversarial Network (PSGAN) learns in an adversarial manner to yield pose sequences conditioned on the class label. In the second stage, a Semantic Consistent Generative Adversarial Network (SCGAN) generates video frames from the poses while preserving coherent appearances in the input image. By enforcing semantic consistency between the generated and ground-truth poses at a high feature level, our SCGAN is robust to noisy or abnormal poses. Extensive experiments on both human action and human face datasets manifest the superiority of the proposed method over other state-of-the-arts.



### Transformationally Identical and Invariant Convolutional Neural Networks by Combining Symmetric Operations or Input Vectors
- **Arxiv ID**: http://arxiv.org/abs/1807.11156v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.11156v3)
- **Published**: 2018-07-30 03:14:50+00:00
- **Updated**: 2018-08-20 10:32:55+00:00
- **Authors**: ShihChung B. Lo, Matthew T. Freedman, Seong K. Mun
- **Comment**: 9 pages, 3 figures
- **Journal**: None
- **Summary**: Transformationally invariant processors constructed by transformed input vectors or operators have been suggested and applied to many applications. In this study, transformationally identical processing based on combining results of all sub-processes with corresponding transformations at one of the processing steps or at the beginning step were found to be equivalent for a given condition. This property can be applied to most convolutional neural network (CNN) systems. Specifically, a transformationally identical CNN can be constructed by arranging internally symmetric operations in parallel with the same transformation family that includes a flatten layer with weights sharing among their corresponding transformation elements. Other transformationally identical CNNs can be constructed by averaging transformed input vectors of the family at the input layer followed by an ordinary CNN process or by a set of symmetric operations. Interestingly, we found that both types of transformationally identical CNN systems are mathematically equivalent by either applying an averaging operation to corresponding elements of all sub-channels before the activation function or without using a non-linear activation function.



### Robust Student Network Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.11158v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.11158v2)
- **Published**: 2018-07-30 03:27:04+00:00
- **Updated**: 2018-07-31 01:01:55+00:00
- **Authors**: Tianyu Guo, Chang Xu, Shiyi He, Boxin Shi, Chao Xu, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks bring in impressive accuracy in various applications, but the success often relies on the heavy network architecture. Taking well-trained heavy networks as teachers, classical teacher-student learning paradigm aims to learn a student network that is lightweight yet accurate. In this way, a portable student network with significantly fewer parameters can achieve a considerable accuracy which is comparable to that of teacher network. However, beyond accuracy, robustness of the learned student network against perturbation is also essential for practical uses. Existing teacher-student learning frameworks mainly focus on accuracy and compression ratios, but ignore the robustness. In this paper, we make the student network produce more confident predictions with the help of the teacher network, and analyze the lower bound of the perturbation that will destroy the confidence of the student network. Two important objectives regarding prediction scores and gradients of examples are developed to maximize this lower bound, so as to enhance the robustness of the student network without sacrificing the performance. Experiments on benchmark datasets demonstrate the efficiency of the proposed approach to learn robust student networks which have satisfying accuracy and compact sizes.



### ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design
- **Arxiv ID**: http://arxiv.org/abs/1807.11164v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11164v1)
- **Published**: 2018-07-30 04:18:25+00:00
- **Updated**: 2018-07-30 04:18:25+00:00
- **Authors**: Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Currently, the neural network architecture design is mostly guided by the \emph{indirect} metric of computation complexity, i.e., FLOPs. However, the \emph{direct} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \emph{guidelines} for efficient network design. Accordingly, a new architecture is presented, called \emph{ShuffleNet V2}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff.



### Human Motion Analysis with Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.11176v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11176v2)
- **Published**: 2018-07-30 05:12:09+00:00
- **Updated**: 2018-08-06 00:13:42+00:00
- **Authors**: Huseyin Coskun, David Joseph Tan, Sailesh Conjeti, Nassir Navab, Federico Tombari
- **Comment**: To appear in ECCV 2018
- **Journal**: None
- **Summary**: Effectively measuring the similarity between two human motions is necessary for several computer vision tasks such as gait analysis, person identi- fication and action retrieval. Nevertheless, we believe that traditional approaches such as L2 distance or Dynamic Time Warping based on hand-crafted local pose metrics fail to appropriately capture the semantic relationship across motions and, as such, are not suitable for being employed as metrics within these tasks. This work addresses this limitation by means of a triplet-based deep metric learning specifically tailored to deal with human motion data, in particular with the prob- lem of varying input size and computationally expensive hard negative mining due to motion pair alignment. Specifically, we propose (1) a novel metric learn- ing objective based on a triplet architecture and Maximum Mean Discrepancy; as well as, (2) a novel deep architecture based on attentive recurrent neural networks. One benefit of our objective function is that it enforces a better separation within the learned embedding space of the different motion categories by means of the associated distribution moments. At the same time, our attentive recurrent neural network allows processing varying input sizes to a fixed size of embedding while learning to focus on those motion parts that are semantically distinctive. Our ex- periments on two different datasets demonstrate significant improvements over conventional human motion metrics.



### Deep Group-shuffling Random Walk for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1807.11178v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11178v1)
- **Published**: 2018-07-30 05:35:38+00:00
- **Updated**: 2018-07-30 05:35:38+00:00
- **Authors**: Yantao Shen, Hongsheng Li, Tong Xiao, Shuai Yi, Dapeng Chen, Xiaogang Wang
- **Comment**: CVPR 2018 poster
- **Journal**: None
- **Summary**: Person re-identification aims at finding a person of interest in an image gallery by comparing the probe image of this person with all the gallery images. It is generally treated as a retrieval problem, where the affinities between the probe image and gallery images (P2G affinities) are used to rank the retrieved gallery images. However, most existing methods only consider P2G affinities but ignore the affinities between all the gallery images (G2G affinity). Some frameworks incorporated G2G affinities into the testing process, which is not end-to-end trainable for deep neural networks. In this paper, we propose a novel group-shuffling random walk network for fully utilizing the affinity information between gallery images in both the training and testing processes. The proposed approach aims at end-to-end refining the P2G affinities based on G2G affinity information with a simple yet effective matrix operation, which can be integrated into deep neural networks. Feature grouping and group shuffle are also proposed to apply rich supervisions for learning better person features. The proposed approach outperforms state-of-the-art methods on the Market-1501, CUHK03, and DukeMTMC datasets by large margins, which demonstrate the effectiveness of our approach.



### End-to-End Deep Kronecker-Product Matching for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1807.11182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11182v1)
- **Published**: 2018-07-30 05:56:47+00:00
- **Updated**: 2018-07-30 05:56:47+00:00
- **Authors**: Yantao Shen, Tong Xiao, Hongsheng Li, Shuai Yi, Xiaogang Wang
- **Comment**: CVPR 2018 poster
- **Journal**: None
- **Summary**: Person re-identification aims to robustly measure similarities between person images. The significant variation of person poses and viewing angles challenges for accurate person re-identification. The spatial layout and correspondences between query person images are vital information for tackling this problem but are ignored by most state-of-the-art methods. In this paper, we propose a novel Kronecker Product Matching module to match feature maps of different persons in an end-to-end trainable deep neural network. A novel feature soft warping scheme is designed for aligning the feature maps based on matching results, which is shown to be crucial for achieving superior accuracy. The multi-scale features based on hourglass-like networks and self-residual attention are also exploited to further boost the re-identification performance. The proposed approach outperforms state-of-the-art methods on the Market-1501, CUHK03, and DukeMTMC datasets, which demonstrates the effectiveness and generalization ability of our proposed approach.



### Multi-Fiber Networks for Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.11195v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11195v3)
- **Published**: 2018-07-30 07:08:29+00:00
- **Updated**: 2018-09-18 07:17:44+00:00
- **Authors**: Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, Jiashi Feng
- **Comment**: ECCV 2018, Code is on GitHub
- **Journal**: None
- **Summary**: In this paper, we aim to reduce the computational cost of spatio-temporal deep neural networks, making them run as fast as their 2D counterparts while preserving state-of-the-art accuracy on video recognition benchmarks. To this end, we present the novel Multi-Fiber architecture that slices a complex neural network into an ensemble of lightweight networks or fibers that run through the network. To facilitate information flow between fibers we further incorporate multiplexer modules and end up with an architecture that reduces the computational cost of 3D networks by an order of magnitude, while increasing recognition performance at the same time. Extensive experimental results show that our multi-fiber architecture significantly boosts the efficiency of existing convolution networks for both image and video recognition tasks, achieving state-of-the-art performance on UCF-101, HMDB-51 and Kinetics datasets. Our proposed model requires over 9x and 13x less computations than the I3D and R(2+1)D models, respectively, yet providing higher accuracy.



### Hard-Aware Point-to-Set Deep Metric for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1807.11206v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11206v1)
- **Published**: 2018-07-30 07:41:34+00:00
- **Updated**: 2018-07-30 07:41:34+00:00
- **Authors**: Rui Yu, Zhiyong Dou, Song Bai, Zhaoxiang Zhang, Yongchao Xu, Xiang Bai
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: Person re-identification (re-ID) is a highly challenging task due to large variations of pose, viewpoint, illumination, and occlusion. Deep metric learning provides a satisfactory solution to person re-ID by training a deep network under supervision of metric loss, e.g., triplet loss. However, the performance of deep metric learning is greatly limited by traditional sampling methods. To solve this problem, we propose a Hard-Aware Point-to-Set (HAP2S) loss with a soft hard-mining scheme. Based on the point-to-set triplet loss framework, the HAP2S loss adaptively assigns greater weights to harder samples. Several advantageous properties are observed when compared with other state-of-the-art loss functions: 1) Accuracy: HAP2S loss consistently achieves higher re-ID accuracies than other alternatives on three large-scale benchmark datasets; 2) Robustness: HAP2S loss is more robust to outliers than other losses; 3) Flexibility: HAP2S loss does not rely on a specific weight function, i.e., different instantiations of HAP2S loss are equally effective. 4) Generality: In addition to person re-ID, we apply the proposed method to generic deep metric learning benchmarks including CUB-200-2011 and Cars196, and also achieve state-of-the-art results.



### Persistence Atlas for Critical Point Variability in Ensembles
- **Arxiv ID**: http://arxiv.org/abs/1807.11212v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CG, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1807.11212v1)
- **Published**: 2018-07-30 07:46:27+00:00
- **Updated**: 2018-07-30 07:46:27+00:00
- **Authors**: Guillaume Favelier, Noura Faraj, Brian Summa, Julien Tierny
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new approach for the visualization and analysis of the spatial variability of features of interest represented by critical points in ensemble data. Our framework, called Persistence Atlas, enables the visualization of the dominant spatial patterns of critical points, along with statistics regarding their occurrence in the ensemble. The persistence atlas represents in the geometrical domain each dominant pattern in the form of a confidence map for the appearance of critical points. As a by-product, our method also provides 2-dimensional layouts of the entire ensemble, highlighting the main trends at a global level. Our approach is based on the new notion of Persistence Map, a measure of the geometrical density in critical points which leverages the robustness to noise of topological persistence to better emphasize salient features. We show how to leverage spectral embedding to represent the ensemble members as points in a low-dimensional Euclidean space, where distances between points measure the dissimilarities between critical point layouts and where statistical tasks, such as clustering, can be easily carried out. Further, we show how the notion of mandatory critical point can be leveraged to evaluate for each cluster confidence regions for the appearance of critical points. Most of the steps of this framework can be trivially parallelized and we show how to efficiently implement them. Extensive experiments demonstrate the relevance of our approach. The accuracy of the confidence regions provided by the persistence atlas is quantitatively evaluated and compared to a baseline strategy using an off-the-shelf clustering approach. We illustrate the importance of the persistence atlas in a variety of real-life datasets, where clear trends in feature layouts are identified and analyzed.



### CAKE: Compact and Accurate K-dimensional representation of Emotion
- **Arxiv ID**: http://arxiv.org/abs/1807.11215v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1807.11215v2)
- **Published**: 2018-07-30 08:03:09+00:00
- **Updated**: 2018-08-03 08:07:32+00:00
- **Authors**: Corentin Kervadec, Valentin Vielzeuf, Stéphane Pateux, Alexis Lechervy, Frédéric Jurie
- **Comment**: None
- **Journal**: Image Analysis for Human Facial and Activity Recognition (BMVC
  Workshop), Sep 2018, Newcastle, United Kingdom.
  http://juz-dev.myweb.port.ac.uk/BMVCWorkshop/index.html
- **Summary**: Numerous models describing the human emotional states have been built by the psychology community. Alongside, Deep Neural Networks (DNN) are reaching excellent performances and are becoming interesting features extraction tools in many computer vision tasks.Inspired by works from the psychology community, we first study the link between the compact two-dimensional representation of the emotion known as arousal-valence, and discrete emotion classes (e.g. anger, happiness, sadness, etc.) used in the computer vision community. It enables to assess the benefits -- in terms of discrete emotion inference -- of adding an extra dimension to arousal-valence (usually named dominance). Building on these observations, we propose CAKE, a 3-dimensional representation of emotion learned in a multi-domain fashion, achieving accurate emotion recognition on several public datasets. Moreover, we visualize how emotions boundaries are organized inside DNN representations and show that DNNs are implicitly learning arousal-valence-like descriptions of emotions. Finally, we use the CAKE representation to compare the quality of the annotations of different public datasets.



### Deep Hybrid Real and Synthetic Training for Intrinsic Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1807.11226v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11226v1)
- **Published**: 2018-07-30 08:25:54+00:00
- **Updated**: 2018-07-30 08:25:54+00:00
- **Authors**: Sai Bi, Nima Khademi Kalantari, Ravi Ramamoorthi
- **Comment**: Accepted to EGSR 2018
- **Journal**: None
- **Summary**: Intrinsic image decomposition is the process of separating the reflectance and shading layers of an image, which is a challenging and underdetermined problem. In this paper, we propose to systematically address this problem using a deep convolutional neural network (CNN). Although deep learning (DL) has been recently used to handle this application, the current DL methods train the network only on synthetic images as obtaining ground truth reflectance and shading for real images is difficult. Therefore, these methods fail to produce reasonable results on real images and often perform worse than the non-DL techniques. We overcome this limitation by proposing a novel hybrid approach to train our network on both synthetic and real images. Specifically, in addition to directly supervising the network using synthetic images, we train the network by enforcing it to produce the same reflectance for a pair of images of the same real-world scene with different illuminations. Furthermore, we improve the results by incorporating a bilateral solver layer into our system during both training and test stages. Experimental results show that our approach produces better results than the state-of-the-art DL and non-DL methods on various synthetic and real datasets both visually and numerically.



### Predicting Conversion of Mild Cognitive Impairments to Alzheimer's Disease and Exploring Impact of Neuroimaging
- **Arxiv ID**: http://arxiv.org/abs/1807.11228v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.11228v1)
- **Published**: 2018-07-30 08:39:47+00:00
- **Updated**: 2018-07-30 08:39:47+00:00
- **Authors**: Yaroslav Shmulev, Mikhail Belyaev
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, a lot of scientific efforts are concentrated on the diagnosis of Alzheimer's Disease (AD) applying deep learning methods to neuroimaging data. Even for 2017, there were published more than a hundred papers dedicated to AD diagnosis, whereas only a few works considered a problem of mild cognitive impairments (MCI) conversion to the AD. However, the conversion prediction is an important problem since approximately 15% of patients with MCI converges to the AD every year. In the current work, we are focusing on the conversion prediction using brain Magnetic Resonance Imaging and clinical data, such as demographics, cognitive assessments, genetic, and biochemical markers. First of all, we applied state-of-the-art deep learning algorithms on the neuroimaging data and compared these results with two machine learning algorithms that we fit using the clinical data. As a result, the models trained on the clinical data outperform the deep learning algorithms applied to the MR images. To explore the impact of neuroimaging further, we trained a deep feed-forward embedding using similarity learning with Histogram loss on all available MRIs and obtained 64-dimensional vector representation of neuroimaging data. The use of learned representation from the deep embedding allowed to increase the quality of prediction based on the neuroimaging. Finally, the current results on this dataset show that the neuroimaging does affect conversion prediction, however, cannot noticeably increase the quality of the prediction. The best results of predicting MCI-to-AD conversion are provided by XGBoost algorithm trained on the clinical and embedding data. The resulting accuracy is 0.76 +- 0.01 and the area under the ROC curve - 0.86 +- 0.01.



### Improving Electron Micrograph Signal-to-Noise with an Atrous Convolutional Encoder-Decoder
- **Arxiv ID**: http://arxiv.org/abs/1807.11234v2
- **DOI**: 10.1016/j.ultramic.2019.03.017
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11234v2)
- **Published**: 2018-07-30 08:48:32+00:00
- **Updated**: 2018-11-02 10:37:59+00:00
- **Authors**: Jeffrey M. Ede
- **Comment**: 15 pages, 10 figures, 1 table
- **Journal**: Ultramicroscopy 202 (2019), pp.18-25
- **Summary**: We present an atrous convolutional encoder-decoder trained to denoise 512$\times$512 crops from electron micrographs. It consists of a modified Xception backbone, atrous convoltional spatial pyramid pooling module and a multi-stage decoder. Our neural network was trained end-to-end to remove Poisson noise applied to low-dose ($\ll$ 300 counts ppx) micrographs created from a new dataset of 17267 2048$\times$2048 high-dose ($>$ 2500 counts ppx) micrographs and then fine-tuned for ordinary doses (200-2500 counts ppx). Its performance is benchmarked against bilateral, non-local means, total variation, wavelet, Wiener and other restoration methods with their default parameters. Our network outperforms their best mean squared error and structural similarity index performances by 24.6% and 9.6% for low doses and by 43.7% and 5.5% for ordinary doses. In both cases, our network's mean squared error has the lowest variance. Source code and links to our new high-quality dataset and trained network have been made publicly available at https://github.com/Jeffrey-Ede/Electron-Micrograph-Denoiser



### Semantic Labeling in Very High Resolution Images via a Self-Cascaded Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1807.11236v1
- **DOI**: 10.1016/j.isprsjprs.2017.12.007
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.11236v1)
- **Published**: 2018-07-30 08:49:25+00:00
- **Updated**: 2018-07-30 08:49:25+00:00
- **Authors**: Yongcheng Liu, Bin Fan, Lingfeng Wang, Jun Bai, Shiming Xiang, Chunhong Pan
- **Comment**: accepted by ISPRS Journal of Photogrammetry and Remote Senseing 2017
- **Journal**: None
- **Summary**: Semantic labeling for very high resolution (VHR) images in urban areas, is of significant importance in a wide range of remote sensing applications. However, many confusing manmade objects and intricate fine-structured objects make it very difficult to obtain both coherent and accurate labeling results. For this challenging task, we propose a novel deep model with convolutional neural networks (CNNs), i.e., an end-to-end self-cascaded network (ScasNet). Specifically, for confusing manmade objects, ScasNet improves the labeling coherence with sequential global-to-local contexts aggregation. Technically, multi-scale contexts are captured on the output of a CNN encoder, and then they are successively aggregated in a self-cascaded manner. Meanwhile, for fine-structured objects, ScasNet boosts the labeling accuracy with a coarse-to-fine refinement strategy. It progressively refines the target objects using the low-level features learned by CNN's shallow layers. In addition, to correct the latent fitting residual caused by multi-feature fusion inside ScasNet, a dedicated residual correction scheme is proposed. It greatly improves the effectiveness of ScasNet. Extensive experimental results on three public datasets, including two challenging benchmarks, show that ScasNet achieves the state-of-the-art performance.



### Recurrently Exploring Class-wise Attention in A Hybrid Convolutional and Bidirectional LSTM Network for Multi-label Aerial Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1807.11245v2
- **DOI**: 10.1016/j.isprsjprs.2019.01.015
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11245v2)
- **Published**: 2018-07-30 09:14:15+00:00
- **Updated**: 2019-12-13 16:08:31+00:00
- **Authors**: Yuansheng Hua, Lichao Mou, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Aerial image classification is of great significance in remote sensing community, and many researches have been conducted over the past few years. Among these studies, most of them focus on categorizing an image into one semantic label, while in the real world, an aerial image is often associated with multiple labels, e.g., multiple object-level labels in our case. Besides, a comprehensive picture of present objects in a given high resolution aerial image can provide more in-depth understanding of the studied region. For these reasons, aerial image multi-label classification has been attracting increasing attention. However, one common limitation shared by existing methods in the community is that the co-occurrence relationship of various classes, so called class dependency, is underexplored and leads to an inconsiderate decision. In this paper, we propose a novel end-to-end network, namely class-wise attention-based convolutional and bidirectional LSTM network (CA-Conv-BiLSTM), for this task. The proposed network consists of three indispensable components: 1) a feature extraction module, 2) a class attention learning layer, and 3) a bidirectional LSTM-based sub-network. Particularly, the feature extraction module is designed for extracting fine-grained semantic feature maps, while the class attention learning layer aims at capturing discriminative class-specific features. As the most important part, the bidirectional LSTM-based sub-network models the underlying class dependency in both directions and produce structured multiple object labels. Experimental results on UCM multi-label dataset and DFC15 multi-label dataset validate the effectiveness of our model quantitatively and qualitatively.



### Modular Sensor Fusion for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.11249v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.11249v1)
- **Published**: 2018-07-30 09:27:14+00:00
- **Updated**: 2018-07-30 09:27:14+00:00
- **Authors**: Hermann Blum, Abel Gawel, Roland Siegwart, Cesar Cadena
- **Comment**: preprint of a paper presented at the IEEE International Conference on
  Intelligent Robots and Systems 2018
- **Journal**: None
- **Summary**: Sensor fusion is a fundamental process in robotic systems as it extends the perceptual range and increases robustness in real-world operations. Current multi-sensor deep learning based semantic segmentation approaches do not provide robustness to under-performing classes in one modality, or require a specific architecture with access to the full aligned multi-sensor training data. In this work, we analyze statistical fusion approaches for semantic segmentation that overcome these drawbacks while keeping a competitive performance. The studied approaches are modular by construction, allowing to have different training sets per modality and only a much smaller subset is needed to calibrate the statistical models. We evaluate a range of statistical fusion approaches and report their performance against state-of-the-art baselines on both real-world and simulated data. In our experiments, the approach improves performance in IoU over the best single modality segmentation results by up to 5%. We make all implementations and configurations publicly available.



### Extreme Network Compression via Filter Group Approximation
- **Arxiv ID**: http://arxiv.org/abs/1807.11254v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11254v2)
- **Published**: 2018-07-30 09:42:05+00:00
- **Updated**: 2018-07-31 05:03:55+00:00
- **Authors**: Bo Peng, Wenming Tan, Zheyang Li, Shun Zhang, Di Xie, Shiliang Pu
- **Comment**: Accepted by ECCV2018
- **Journal**: None
- **Summary**: In this paper we propose a novel decomposition method based on filter group approximation, which can significantly reduce the redundancy of deep convolutional neural networks (CNNs) while maintaining the majority of feature representation. Unlike other low-rank decomposition algorithms which operate on spatial or channel dimension of filters, our proposed method mainly focuses on exploiting the filter group structure for each layer. For several commonly used CNN models, including VGG and ResNet, our method can reduce over 80% floating-point operations (FLOPs) with less accuracy drop than state-of-the-art methods on various image classification datasets. Besides, experiments demonstrate that our method is conducive to alleviating degeneracy of the compressed network, which hurts the convergence and performance of the network.



### Uncertainty Quantification in CNN-Based Surface Prediction Using Shape Priors
- **Arxiv ID**: http://arxiv.org/abs/1807.11272v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.11272v1)
- **Published**: 2018-07-30 10:24:26+00:00
- **Updated**: 2018-07-30 10:24:26+00:00
- **Authors**: Katarína Tóthová, Sarah Parisot, Matthew C. H. Lee, Esther Puyol-Antón, Lisa M. Koch, Andrew P. King, Ender Konukoglu, Marc Pollefeys
- **Comment**: Accepted to ShapeMI MICCAI 2018: Workshop on Shape in Medical Imaging
- **Journal**: None
- **Summary**: Surface reconstruction is a vital tool in a wide range of areas of medical image analysis and clinical research. Despite the fact that many methods have proposed solutions to the reconstruction problem, most, due to their deterministic nature, do not directly address the issue of quantifying uncertainty associated with their predictions. We remedy this by proposing a novel probabilistic deep learning approach capable of simultaneous surface reconstruction and associated uncertainty prediction. The method incorporates prior shape information in the form of a principal component analysis (PCA) model. Experiments using the UK Biobank data show that our probabilistic approach outperforms an analogous deterministic PCA-based method in the task of 2D organ delineation and quantifies uncertainty by formulating distributions over predicted surface vertex positions.



### Self-Calibration of Cameras with Euclidean Image Plane in Case of Two Views and Known Relative Rotation Angle
- **Arxiv ID**: http://arxiv.org/abs/1807.11279v1
- **DOI**: 10.1007/978-3-030-01225-0_26
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11279v1)
- **Published**: 2018-07-30 10:48:56+00:00
- **Updated**: 2018-07-30 10:48:56+00:00
- **Authors**: Evgeniy Martyushev
- **Comment**: 13 pages, 7 eps-figures
- **Journal**: ECCV 2018. Lecture Notes in Computer Science, vol 11208. Springer
- **Summary**: The internal calibration of a pinhole camera is given by five parameters that are combined into an upper-triangular $3\times 3$ calibration matrix. If the skew parameter is zero and the aspect ratio is equal to one, then the camera is said to have Euclidean image plane. In this paper, we propose a non-iterative self-calibration algorithm for a camera with Euclidean image plane in case the remaining three internal parameters --- the focal length and the principal point coordinates --- are fixed but unknown. The algorithm requires a set of $N \geq 7$ point correspondences in two views and also the measured relative rotation angle between the views. We show that the problem generically has six solutions (including complex ones).   The algorithm has been implemented and tested both on synthetic data and on publicly available real dataset. The experiments demonstrate that the method is correct, numerically stable and robust.



### Improving Spatiotemporal Self-Supervision by Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.11293v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11293v1)
- **Published**: 2018-07-30 11:26:49+00:00
- **Updated**: 2018-07-30 11:26:49+00:00
- **Authors**: Uta Büchler, Biagio Brattoli, Björn Ommer
- **Comment**: Accepted for publication at ECCV 2018
- **Journal**: None
- **Summary**: Self-supervised learning of convolutional neural networks can harness large amounts of cheap unlabeled data to train powerful feature representations. As surrogate task, we jointly address ordering of visual data in the spatial and temporal domain. The permutations of training samples, which are at the core of self-supervision by ordering, have so far been sampled randomly from a fixed preselected set. Based on deep reinforcement learning we propose a sampling policy that adapts to the state of the network, which is being trained. Therefore, new permutations are sampled according to their expected utility for updating the convolutional feature representation. Experimental evaluation on unsupervised and transfer learning tasks demonstrates competitive performance on standard benchmarks for image and video classification and nearest neighbor retrieval.



### Action Detection from a Robot-Car Perspective
- **Arxiv ID**: http://arxiv.org/abs/1807.11332v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MA, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.11332v1)
- **Published**: 2018-07-30 13:11:21+00:00
- **Updated**: 2018-07-30 13:11:21+00:00
- **Authors**: Valentina Fontana, Gurkirt Singh, Stephen Akrigg, Manuele Di Maio, Suman Saha, Fabio Cuzzolin
- **Comment**: intial version, more to come - soon
- **Journal**: None
- **Summary**: We present the new Road Event and Activity Detection (READ) dataset, designed and created from an autonomous vehicle perspective to take action detection challenges to autonomous driving. READ will give scholars in computer vision, smart cars and machine learning at large the opportunity to conduct research into exciting new problems such as understanding complex (road) activities, discerning the behaviour of sentient agents, and predicting both the label and the location of future actions and events, with the final goal of supporting autonomous decision making.



### Unsupervised Domain Adaptive Re-Identification: Theory and Practice
- **Arxiv ID**: http://arxiv.org/abs/1807.11334v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11334v1)
- **Published**: 2018-07-30 13:18:31+00:00
- **Updated**: 2018-07-30 13:18:31+00:00
- **Authors**: Liangchen Song, Cheng Wang, Lefei Zhang, Bo Du, Qian Zhang, Chang Huang, Xinggang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of unsupervised domain adaptive re-identification (re-ID) which is an active topic in computer vision but lacks a theoretical foundation. We first extend existing unsupervised domain adaptive classification theories to re-ID tasks. Concretely, we introduce some assumptions on the extracted feature space and then derive several loss functions guided by these assumptions. To optimize them, a novel self-training scheme for unsupervised domain adaptive re-ID tasks is proposed. It iteratively makes guesses for unlabeled target data based on an encoder and trains the encoder based on the guessed labels. Extensive experiments on unsupervised domain adaptive person re-ID and vehicle re-ID tasks with comparisons to the state-of-the-arts confirm the effectiveness of the proposed theories and self-training framework. Our code is available at \url{https://github.com/LcDog/DomainAdaptiveReID}.



### Learning Adaptive Discriminative Correlation Filters via Temporal Consistency Preserving Spatial Feature Selection for Robust Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1807.11348v3
- **DOI**: 10.1109/TIP.2019.2919201
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11348v3)
- **Published**: 2018-07-30 13:46:46+00:00
- **Updated**: 2019-06-19 16:49:16+00:00
- **Authors**: Tianyang Xu, Zhen-Hua Feng, Xiao-Jun Wu, Josef Kittler
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, 2019
- **Summary**: With efficient appearance learning models, Discriminative Correlation Filter (DCF) has been proven to be very successful in recent video object tracking benchmarks and competitions. However, the existing DCF paradigm suffers from two major issues, i.e., spatial boundary effect and temporal filter degradation. To mitigate these challenges, we propose a new DCF-based tracking method. The key innovations of the proposed method include adaptive spatial feature selection and temporal consistent constraints, with which the new tracker enables joint spatial-temporal filter learning in a lower dimensional discriminative manifold. More specifically, we apply structured spatial sparsity constraints to multi-channel filers. Consequently, the process of learning spatial filters can be approximated by the lasso regularisation. To encourage temporal consistency, the filter model is restricted to lie around its historical value and updated locally to preserve the global structure in the manifold. Last, a unified optimisation framework is proposed to jointly select temporal consistency preserving spatial features and learn discriminative filters with the augmented Lagrangian method. Qualitative and quantitative evaluations have been conducted on a number of well-known benchmarking datasets such as OTB2013, OTB50, OTB100, Temple-Colour, UAV123 and VOT2018. The experimental results demonstrate the superiority of the proposed method over the state-of-the-art approaches.



### Small Organ Segmentation in Whole-body MRI using a Two-stage FCN and Weighting Schemes
- **Arxiv ID**: http://arxiv.org/abs/1807.11368v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11368v1)
- **Published**: 2018-07-30 14:35:02+00:00
- **Updated**: 2018-07-30 14:35:02+00:00
- **Authors**: Vanya V. Valindria, Ioannis Lavdas, Juan Cerrolaza, Eric O. Aboagye, Andrea G. Rockall, Daniel Rueckert, Ben Glocker
- **Comment**: Accepted at the MICCAI Workshop on Machine Learning in Medical
  Imaging (MLMI) 2018
- **Journal**: None
- **Summary**: Accurate and robust segmentation of small organs in whole-body MRI is difficult due to anatomical variation and class imbalance. Recent deep network based approaches have demonstrated promising performance on abdominal multi-organ segmentations. However, the performance on small organs is still suboptimal as these occupy only small regions of the whole-body volumes with unclear boundaries and variable shapes. A coarse-to-fine, hierarchical strategy is a common approach to alleviate this problem, however, this might miss useful contextual information. We propose a two-stage approach with weighting schemes based on auto-context and spatial atlas priors. Our experiments show that the proposed approach can boost the segmentation accuracy of multiple small organs in whole-body MRI scans.



### Multi-bin Trainable Linear Unit for Fast Image Restoration Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.11389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11389v1)
- **Published**: 2018-07-30 15:11:47+00:00
- **Updated**: 2018-07-30 15:11:47+00:00
- **Authors**: Shuhang Gu, Radu Timofte, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Tremendous advances in image restoration tasks such as denoising and super-resolution have been achieved using neural networks. Such approaches generally employ very deep architectures, large number of parameters, large receptive fields and high nonlinear modeling capacity. In order to obtain efficient and fast image restoration networks one should improve upon the above mentioned requirements.   In this paper we propose a novel activation function, the multi-bin trainable linear unit (MTLU), for increasing the nonlinear modeling capacity together with lighter and shallower networks. We validate the proposed fast image restoration networks for image denoising (FDnet) and super-resolution (FSRnet) on standard benchmarks. We achieve large improvements in both memory and runtime over current state-of-the-art for comparable or better PSNR accuracies.



### A Non-structural Representation Scheme for Articulated Shapes
- **Arxiv ID**: http://arxiv.org/abs/1807.11411v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11411v1)
- **Published**: 2018-07-30 16:04:49+00:00
- **Updated**: 2018-07-30 16:04:49+00:00
- **Authors**: Asli Genctav, Sibel Tari
- **Comment**: None
- **Journal**: None
- **Summary**: For representing articulated shapes, as an alternative to the structured models based on graphs representing part hierarchy, we propose a pixel-based distinctness measure. Its spatial distribution yields a partitioning of the shape into a set of regions each of which is represented via size normalized probability distribution of the distinctness. Without imposing any structural relation among parts, pairwise shape similarity is formulated as the cost of an optimal assignment between respective regions. The matching is performed via Hungarian algorithm permitting some unmatched regions. The proposed similarity measure is employed in the context of clustering a set of shapes. The clustering results obtained on three articulated shape datasets show that our method performs comparable to state of the art methods utilizing component graphs or trees even though we are not explicitly modeling component relations.



### REFUGE CHALLENGE 2018-Task 2:Deep Optic Disc and Cup Segmentation in Fundus Images Using U-Net and Multi-scale Feature Matching Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.11433v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11433v1)
- **Published**: 2018-07-30 16:43:25+00:00
- **Updated**: 2018-07-30 16:43:25+00:00
- **Authors**: Vivek Kumar Singh, Hatem A. Rashwan, Adel Saleh, Farhan Akram, Md Mostafa Kamal Sarker, Nidhi Pandey, Saddam Abdulwahab
- **Comment**: EYE REFUGE CHALLENGE 2018, submitted 7 Pages
- **Journal**: None
- **Summary**: In this paper, an optic disc and cup segmentation method is proposed using U-Net followed by a multi-scale feature matching network. The proposed method targets task 2 of the REFUGE challenge 2018. In order to solve the segmentation problem of task 2, we firstly crop the input image using single shot multibox detector (SSD). The cropped image is then passed to an encoder-decoder network with skip connections also known as generator. Afterwards, both the ground truth and generated images are fed to a convolution neural network (CNN) to extract their multi-level features. A dice loss function is then used to match the features of the two images by minimizing the error at each layer. The aggregation of error from each layer is back-propagated through the generator network to enforce it to generate a segmented image closer to the ground truth. The CNN network improves the performance of the generator network without increasing the complexity of the model.



### Leveraging Motion Priors in Videos for Improving Human Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.11436v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.11436v1)
- **Published**: 2018-07-30 16:52:04+00:00
- **Updated**: 2018-07-30 16:52:04+00:00
- **Authors**: Yu-Ting Chen, Wen-Yen Chang, Hai-Lun Lu, Tingfan Wu, Min Sun
- **Comment**: ECCV2018
- **Journal**: None
- **Summary**: Despite many advances in deep-learning based semantic segmentation, performance drop due to distribution mismatch is often encountered in the real world. Recently, a few domain adaptation and active learning approaches have been proposed to mitigate the performance drop. However, very little attention has been made toward leveraging information in videos which are naturally captured in most camera systems. In this work, we propose to leverage "motion prior" in videos for improving human segmentation in a weakly-supervised active learning setting. By extracting motion information using optical flow in videos, we can extract candidate foreground motion segments (referred to as motion prior) potentially corresponding to human segments. We propose to learn a memory-network-based policy model to select strong candidate segments (referred to as strong motion prior) through reinforcement learning. The selected segments have high precision and are directly used to finetune the model. In a newly collected surveillance camera dataset and a publicly available UrbanStreet dataset, our proposed method improves the performance of human segmentation across multiple scenes and modalities (i.e., RGB to Infrared (IR)). Last but not least, our method is empirically complementary to existing domain adaptation approaches such that additional performance gain is achieved by combining our weakly-supervised active learning approach with domain adaptation approaches.



### Comparator Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.11440v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.11440v1)
- **Published**: 2018-07-30 16:54:21+00:00
- **Updated**: 2018-07-30 16:54:21+00:00
- **Authors**: Weidi Xie, Li Shen, Andrew Zisserman
- **Comment**: To appear in ECCV 2018
- **Journal**: None
- **Summary**: The objective of this work is set-based verification, e.g. to decide if two sets of images of a face are of the same person or not. The traditional approach to this problem is to learn to generate a feature vector per image, aggregate them into one vector to represent the set, and then compute the cosine similarity between sets. Instead, we design a neural network architecture that can directly learn set-wise verification. Our contributions are: (i) We propose a Deep Comparator Network (DCN) that can ingest a pair of sets (each may contain a variable number of images) as inputs, and compute a similarity between the pair--this involves attending to multiple discriminative local regions (landmarks), and comparing local descriptors between pairs of faces; (ii) To encourage high-quality representations for each set, internal competition is introduced for recalibration based on the landmark score; (iii) Inspired by image retrieval, a novel hard sample mining regime is proposed to control the sampling process, such that the DCN is complementary to the standard image classification models. Evaluations on the IARPA Janus face recognition benchmarks show that the comparator networks outperform the previous state-of-the-art results by a large margin.



### Factor analysis of dynamic PET images: beyond Gaussian noise
- **Arxiv ID**: http://arxiv.org/abs/1807.11455v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.data-an, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.11455v2)
- **Published**: 2018-07-30 17:23:50+00:00
- **Updated**: 2019-03-26 13:58:34+00:00
- **Authors**: Yanna Cruz Cavalcanti, Thomas Oberlin, Nicolas Dobigeon, Cédric Févotte, Simon Stute, Maria-Joao Ribeiro, Clovis Tauber
- **Comment**: This manuscript has been accepted for publication in IEEE Trans.
  Medical Imaging
- **Journal**: None
- **Summary**: Factor analysis has proven to be a relevant tool for extracting tissue time-activity curves (TACs) in dynamic PET images, since it allows for an unsupervised analysis of the data. Reliable and interpretable results are possible only if considered with respect to suitable noise statistics. However, the noise in reconstructed dynamic PET images is very difficult to characterize, despite the Poissonian nature of the count-rates. Rather than explicitly modeling the noise distribution, this work proposes to study the relevance of several divergence measures to be used within a factor analysis framework. To this end, the $\beta$-divergence, widely used in other applicative domains, is considered to design the data-fitting term involved in three different factor models. The performances of the resulting algorithms are evaluated for different values of $\beta$, in a range covering Gaussian, Poissonian and Gamma-distributed noises. The results obtained on two different types of synthetic images and one real image show the interest of applying non-standard values of $\beta$ to improve factor analysis.



### To learn image super-resolution, use a GAN to learn how to do image degradation first
- **Arxiv ID**: http://arxiv.org/abs/1807.11458v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11458v1)
- **Published**: 2018-07-30 17:28:39+00:00
- **Updated**: 2018-07-30 17:28:39+00:00
- **Authors**: Adrian Bulat, Jing Yang, Georgios Tzimiropoulos
- **Comment**: Accepted to ECCV18
- **Journal**: None
- **Summary**: This paper is on image and face super-resolution. The vast majority of prior work for this problem focus on how to increase the resolution of low-resolution images which are artificially generated by simple bilinear down-sampling (or in a few cases by blurring followed by down-sampling).We show that such methods fail to produce good results when applied to real-world low-resolution, low quality images. To circumvent this problem, we propose a two-stage process which firstly trains a High-to-Low Generative Adversarial Network (GAN) to learn how to degrade and downsample high-resolution images requiring, during training, only unpaired high and low-resolution images. Once this is achieved, the output of this network is used to train a Low-to-High GAN for image super-resolution using this time paired low- and high-resolution images. Our main result is that this network can be now used to efectively increase the quality of real-world low-resolution images. We have applied the proposed pipeline for the problem of face super-resolution where we report large improvement over baselines and prior work although the proposed method is potentially applicable to other object categories.



### Improving Transferability of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.11459v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.11459v1)
- **Published**: 2018-07-30 17:34:24+00:00
- **Updated**: 2018-07-30 17:34:24+00:00
- **Authors**: Parijat Dube, Bishwaranjan Bhattacharjee, Elisabeth Petit-Bois, Matthew Hill
- **Comment**: 15 pages, 11 figures, 2 tables, Workshop on Domain Adaptation for
  Visual Understanding (Joint IJCAI/ECAI/AAMAS/ICML 2018 Workshop) Keywords:
  deep learning, transfer learning, finetuning, deep neural network,
  experimental
- **Journal**: None
- **Summary**: Learning from small amounts of labeled data is a challenge in the area of deep learning. This is currently addressed by Transfer Learning where one learns the small data set as a transfer task from a larger source dataset. Transfer Learning can deliver higher accuracy if the hyperparameters and source dataset are chosen well. One of the important parameters is the learning rate for the layers of the neural network. We show through experiments on the ImageNet22k and Oxford Flowers datasets that improvements in accuracy in range of 127% can be obtained by proper choice of learning rates. We also show that the images/label parameter for a dataset can potentially be used to determine optimal learning rates for the layers to get the best overall accuracy. We additionally validate this method on a sample of real-world image classification tasks from a public visual recognition API.



### A Restricted-Domain Dual Formulation for Two-Phase Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.11534v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.11534v1)
- **Published**: 2018-07-30 19:15:38+00:00
- **Updated**: 2018-07-30 19:15:38+00:00
- **Authors**: Jack Spencer
- **Comment**: None
- **Journal**: Irish Machine Vision and Image Processing Conference Proceedings,
  pp. 139-146, 2017
- **Summary**: In two-phase image segmentation, convex relaxation has allowed global minimisers to be computed for a variety of data fitting terms. Many efficient approaches exist to compute a solution quickly. However, we consider whether the nature of the data fitting in this formulation allows for reasonable assumptions to be made about the solution that can improve the computational performance further. In particular, we employ a well known dual formulation of this problem and solve the corresponding equations in a restricted domain. We present experimental results that explore the dependence of the solution on this restriction and quantify imrovements in the computational performance. This approach can be extended to analogous methods simply and could provide an efficient alternative for problems of this type.



### Markerless Visual Robot Programming by Demonstration
- **Arxiv ID**: http://arxiv.org/abs/1807.11541v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.11541v1)
- **Published**: 2018-07-30 19:33:00+00:00
- **Updated**: 2018-07-30 19:33:00+00:00
- **Authors**: Raphael Memmesheimer, Ivanna Mykhalchyshyna, Viktor Seib, Nick Theisen, Dietrich Paulus
- **Comment**: 6 pages, 5 figures, 3rd BAILAR workshop
- **Journal**: None
- **Summary**: In this paper we present an approach for learning to imitate human behavior on a semantic level by markerless visual observation. We analyze a set of spatial constraints on human pose data extracted using convolutional pose machines and object informations extracted from 2D image sequences. A scene analysis, based on an ontology of objects and affordances, is combined with continuous human pose estimation and spatial object relations. Using a set of constraints we associate the observed human actions with a set of executable robot commands. We demonstrate our approach in a kitchen task, where the robot learns to prepare a meal.



### Textual Explanations for Self-Driving Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1807.11546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11546v1)
- **Published**: 2018-07-30 19:38:24+00:00
- **Updated**: 2018-07-30 19:38:24+00:00
- **Authors**: Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, Zeynep Akata
- **Comment**: Accepted to ECCV 2018
- **Journal**: European Conference on Computer Vision (ECCV), 2018
- **Summary**: Deep neural perception and control networks have become key components of self-driving vehicles. User acceptance is likely to benefit from easy-to-interpret textual explanations which allow end-users to understand what triggered a particular behavior. Explanations may be triggered by the neural controller, namely introspective explanations, or informed by the neural controller's output, namely rationalizations. We propose a new approach to introspective explanations which consists of two parts. First, we use a visual (spatial) attention model to train a convolutional network end-to-end from images to the vehicle control commands, i.e., acceleration and change of course. The controller's attention identifies image regions that potentially influence the network's output. Second, we use an attention-based video-to-text model to produce textual explanations of model actions. The attention maps of controller and explanation model are aligned so that explanations are grounded in the parts of the scene that mattered to the controller. We explore two approaches to attention alignment, strong- and weak-alignment. Finally, we explore a version of our model that generates rationalizations, and compare with introspective explanations on the same video segments. We evaluate these models on a novel driving dataset with ground-truth human explanations, the Berkeley DeepDrive eXplanation (BDD-X) dataset. Code is available at https://github.com/JinkyuKimUCB/explainable-deep-driving.



### Testing the Efficient Network TRaining (ENTR) Hypothesis: initially reducing training image size makes Convolutional Neural Network training for image recognition tasks more efficient
- **Arxiv ID**: http://arxiv.org/abs/1807.11583v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1807.11583v1)
- **Published**: 2018-07-30 21:10:25+00:00
- **Updated**: 2018-07-30 21:10:25+00:00
- **Authors**: Thomas Cherico Wanger, Peter Frohn
- **Comment**: 12 pages, 5 figures, 1 table +++ Keywords: Image recognition,
  Efficient Network Training hypothesis, image size increase, network
  efficiency, ResNet models, Google Colaboratory, free cloud GPU, material
  science, geoscience, environmental science, convolutional neural networks,
  regularization
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) for image recognition tasks are seeing rapid advances in the available architectures and how networks are trained based on large computational infrastructure and standard datasets with millions of images. In contrast, performance and time constraints for example, of small devices and free cloud GPUs necessitate efficient network training (i.e., highest accuracy in the shortest inference time possible), often on small datasets. Here, we hypothesize that initially decreasing image size during training makes the training process more efficient, because pre-shaping weights with small images and later utilizing these weights with larger images reduces initial network parameters and total inference time. We test this Efficient Network TRaining (ENTR) Hypothesis by training pre-trained Residual Network (ResNet) models (ResNet18, 34, & 50) on three small datasets (steel microstructures, bee images, and geographic aerial images) with a free cloud GPU. Based on three training regimes of i) not, ii) gradually or iii) in one step increasing image size over the training process, we show that initially reducing image size increases training efficiency consistently across datasets and networks. We interpret these results mechanistically in the framework of regularization theory. Support for the ENTR hypothesis is an important contribution, because network efficiency improvements for image recognition tasks are needed for practical applications. In the future, it will be exciting to see how the ENTR hypothesis holds for large standard datasets like ImageNet or CIFAR, to better understand the underlying mechanisms, and how these results compare to other fields such as structural learning.



### Acquisition of Localization Confidence for Accurate Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.11590v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11590v1)
- **Published**: 2018-07-30 21:36:20+00:00
- **Updated**: 2018-07-30 21:36:20+00:00
- **Authors**: Borui Jiang, Ruixuan Luo, Jiayuan Mao, Tete Xiao, Yuning Jiang
- **Comment**: Accepted to European Conference on Computer Vision (ECCV) 2018
- **Journal**: None
- **Summary**: Modern CNN-based object detectors rely on bounding box regression and non-maximum suppression to localize objects. While the probabilities for class labels naturally reflect classification confidence, localization confidence is absent. This makes properly localized bounding boxes degenerate during iterative regression or even suppressed during NMS. In the paper we propose IoU-Net learning to predict the IoU between each detected bounding box and the matched ground-truth. The network acquires this confidence of localization, which improves the NMS procedure by preserving accurately localized bounding boxes. Furthermore, an optimization-based bounding box refinement method is proposed, where the predicted IoU is formulated as the objective. Extensive experiments on the MS-COCO dataset show the effectiveness of IoU-Net, as well as its compatibility with and adaptivity to several state-of-the-art object detectors.



### Pulse Sequence Resilient Fast Brain Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.11598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11598v1)
- **Published**: 2018-07-30 22:28:43+00:00
- **Updated**: 2018-07-30 22:28:43+00:00
- **Authors**: Amod Jog, Bruce Fischl
- **Comment**: Accepted at MICCAI 2018
- **Journal**: None
- **Summary**: Accurate automatic segmentation of brain anatomy from $T_1$-weighted~($T_1$-w) magnetic resonance images~(MRI) has been a computationally intensive bottleneck in neuroimaging pipelines, with state-of-the-art results obtained by unsupervised intensity modeling-based methods and multi-atlas registration and label fusion. With the advent of powerful supervised convolutional neural networks~(CNN)-based learning algorithms, it is now possible to produce a high quality brain segmentation within seconds. However, the very supervised nature of these methods makes it difficult to generalize them on data different from what they have been trained on. Modern neuroimaging studies are necessarily multi-center initiatives with a wide variety of acquisition protocols. Despite stringent protocol harmonization practices, it is not possible to standardize the whole gamut of MRI imaging parameters across scanners, field strengths, receive coils etc., that affect image contrast. In this paper we propose a CNN-based segmentation algorithm that, in addition to being highly accurate and fast, is also resilient to variation in the input $T_1$-w acquisition. Our approach relies on building approximate forward models of $T_1$-w pulse sequences that produce a typical test image. We use the forward models to augment the training data with test data specific training examples. These augmented data can be used to update and/or build a more robust segmentation model that is more attuned to the test data imaging properties. Our method generates highly accurate, state-of-the-art segmentation results~(overall Dice overlap=0.94), within seconds and is consistent across a wide-range of protocols.



### Fast and Robust Symmetric Image Registration Based on Distances Combining Intensity and Spatial Information
- **Arxiv ID**: http://arxiv.org/abs/1807.11599v2
- **DOI**: 10.1109/TIP.2019.2899947
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11599v2)
- **Published**: 2018-07-30 22:32:00+00:00
- **Updated**: 2019-02-21 09:49:28+00:00
- **Authors**: Johan Öfverstedt, Joakim Lindblad, Nataša Sladoje
- **Comment**: 14 pages, 4 tables, 7 figures
- **Journal**: None
- **Summary**: Intensity-based image registration approaches rely on similarity measures to guide the search for geometric correspondences with high affinity between images. The properties of the used measure are vital for the robustness and accuracy of the registration. In this study a symmetric, intensity interpolation-free, affine registration framework based on a combination of intensity and spatial information is proposed. The excellent performance of the framework is demonstrated on a combination of synthetic tests, recovering known transformations in the presence of noise, and real applications in biomedical and medical image registration, for both 2D and 3D images. The method exhibits greater robustness and higher accuracy than similarity measures in common use, when inserted into a standard gradient-based registration framework available as part of the open source Insight Segmentation and Registration Toolkit (ITK). The method is also empirically shown to have a low computational cost, making it practical for real applications. Source code is available.



