# Arxiv Papers in cs.CV on 2018-07-20
### Bounding Box Embedding for Single Shot Person Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.07674v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07674v1)
- **Published**: 2018-07-20 00:30:30+00:00
- **Updated**: 2018-07-20 00:30:30+00:00
- **Authors**: Jacob Richeimer, Jonathan Mitchell
- **Comment**: None
- **Journal**: None
- **Summary**: We present a bottom-up approach for the task of object instance segmentation using a single-shot model. The proposed model employs a fully convolutional network which is trained to predict class-wise segmentation masks as well as the bounding boxes of the object instances to which each pixel belongs. This allows us to group object pixels into individual instances. Our network architecture is based on the DeepLabv3+ model, and requires only minimal extra computation to achieve pixel-wise instance assignments. We apply our method to the task of person instance segmentation, a common task relevant to many applications. We train our model with COCO data and report competitive results for the person class in the COCO instance segmentation task.



### Toward Characteristic-Preserving Image-based Virtual Try-On Network
- **Arxiv ID**: http://arxiv.org/abs/1807.07688v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07688v3)
- **Published**: 2018-07-20 01:42:58+00:00
- **Updated**: 2018-09-12 08:19:42+00:00
- **Authors**: Bochao Wang, Huabin Zheng, Xiaodan Liang, Yimin Chen, Liang Lin, Meng Yang
- **Comment**: Accepted by ECCV 2018
- **Journal**: None
- **Summary**: Image-based virtual try-on systems for fitting new in-shop clothes into a person image have attracted increasing research attention, yet is still challenging. A desirable pipeline should not only transform the target clothes into the most fitting shape seamlessly but also preserve well the clothes identity in the generated image, that is, the key characteristics (e.g. texture, logo, embroidery) that depict the original clothes. However, previous image-conditioned generation works fail to meet these critical requirements towards the plausible virtual try-on performance since they fail to handle large spatial misalignment between the input image and target clothes. Prior work explicitly tackled spatial deformation using shape context matching, but failed to preserve clothing details due to its coarse-to-fine strategy. In this work, we propose a new fully-learnable Characteristic-Preserving Virtual Try-On Network(CP-VTON) for addressing all real-world challenges in this task. First, CP-VTON learns a thin-plate spline transformation for transforming the in-shop clothes into fitting the body shape of the target person via a new Geometric Matching Module (GMM) rather than computing correspondences of interest points as prior works did. Second, to alleviate boundary artifacts of warped clothes and make the results more realistic, we employ a Try-On Module that learns a composition mask to integrate the warped clothes and the rendered image to ensure smoothness. Extensive experiments on a fashion dataset demonstrate our CP-VTON achieves the state-of-the-art virtual try-on performance both qualitatively and quantitatively.



### Automatic Semantic Content Removal by Learning to Neglect
- **Arxiv ID**: http://arxiv.org/abs/1807.07696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07696v1)
- **Published**: 2018-07-20 02:17:33+00:00
- **Updated**: 2018-07-20 02:17:33+00:00
- **Authors**: Siyang Qin, Jiahui Wei, Roberto Manduchi
- **Comment**: Accepted to BMVC 2018 as an oral presentation
- **Journal**: None
- **Summary**: We introduce a new system for automatic image content removal and inpainting. Unlike traditional inpainting algorithms, which require advance knowledge of the region to be filled in, our system automatically detects the area to be removed and infilled. Region segmentation and inpainting are performed jointly in a single pass. In this way, potential segmentation errors are more naturally alleviated by the inpainting module. The system is implemented as an encoder-decoder architecture, with two decoder branches, one tasked with segmentation of the foreground region, the other with inpainting. The encoder and the two decoder branches are linked via neglect nodes, which guide the inpainting process in selecting which areas need reconstruction. The whole model is trained using a conditional GAN strategy. Comparative experiments show that our algorithm outperforms state-of-the-art inpainting techniques (which, unlike our system, do not segment the input image and thus must be aided by an external segmentation module.)



### Editable Generative Adversarial Networks: Generating and Editing Faces Simultaneously
- **Arxiv ID**: http://arxiv.org/abs/1807.07700v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07700v1)
- **Published**: 2018-07-20 03:13:16+00:00
- **Updated**: 2018-07-20 03:13:16+00:00
- **Authors**: Kyungjune Baek, Duhyeon Bang, Hyunjung Shim
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel framework for simultaneously generating and manipulating the face images with desired attributes. While the state-of-the-art attribute editing technique has achieved the impressive performance for creating realistic attribute effects, they only address the image editing problem, using the input image as the condition of model. Recently, several studies attempt to tackle both novel face generation and attribute editing problem using a single solution. However, their image quality is still unsatisfactory. Our goal is to develop a single unified model that can simultaneously create and edit high quality face images with desired attributes. A key idea of our work is that we decompose the image into the latent and attribute vector in low dimensional representation, and then utilize the GAN framework for mapping the low dimensional representation to the image. In this way, we can address both the generation and editing problem by learning the generator. For qualitative and quantitative evaluations, the proposed algorithm outperforms recent algorithms addressing the same problem. Also, we show that our model can achieve the competitive performance with the state-of-the-art attribute editing technique in terms of attribute editing quality.



### PhaseStain: Digital staining of label-free quantitative phase microscopy images using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1807.07701v1
- **DOI**: 10.1038/s41377-019-0129-y
- **Categories**: **eess.IV**, cs.CV, physics.med-ph, 68T01, 68T05, 68U10, 62M45, 78M32, 92C50, 92C55, 94A08, I.2; I.2.1; I.2.6; I.2.10; I.3; I.3.3; I.4.3; I.4.4; I.4.9; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1807.07701v1)
- **Published**: 2018-07-20 03:17:13+00:00
- **Updated**: 2018-07-20 03:17:13+00:00
- **Authors**: Yair Rivenson, Tairan Liu, Zhensong Wei, Yibo Zhang, Aydogan Ozcan
- **Comment**: None
- **Journal**: Light: Science and Applications, 8 (2019)
- **Summary**: Using a deep neural network, we demonstrate a digital staining technique, which we term PhaseStain, to transform quantitative phase images (QPI) of labelfree tissue sections into images that are equivalent to brightfield microscopy images of the same samples that are histochemically stained. Through pairs of image data (QPI and the corresponding brightfield images, acquired after staining) we train a generative adversarial network (GAN) and demonstrate the effectiveness of this virtual staining approach using sections of human skin, kidney and liver tissue, matching the brightfield microscopy images of the same samples stained with Hematoxylin and Eosin, Jones' stain, and Masson's trichrome stain, respectively. This digital staining framework might further strengthen various uses of labelfree QPI techniques in pathology applications and biomedical research in general, by eliminating the need for chemical staining, reducing sample preparation related costs and saving time. Our results provide a powerful example of some of the unique opportunities created by data driven image transformations enabled by deep learning.



### Alpha-rooting color image enhancement method by two-side 2-D quaternion discrete Fourier transform followed by spatial transformation
- **Arxiv ID**: http://arxiv.org/abs/1807.07960v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.07960v1)
- **Published**: 2018-07-20 03:17:43+00:00
- **Updated**: 2018-07-20 03:17:43+00:00
- **Authors**: Artyom M. Grigoryan, Aparna John, Sos S. Agaian
- **Comment**: 21 pages
- **Journal**: International Journal of Applied Control, Electrical and
  Electronics Engineering (IJACEEE) Vol 6, No. 1, February 2018
- **Summary**: In this paper a quaternion approach of enhancement method is proposed in which color in the image is considered as a single entity. This new method is referred as the alpha-rooting method of color image enhancement by the two-dimensional quaternion discrete Fourier transform (2-D QDFT) followed by a spatial transformation. The results of the proposed color image enhancement method are compared with its counterpart channel-by-channel enhancement algorithm by the 2-D DFT. The image enhancements are quantified to the enhancement measure that is based on visual perception referred as the color enhancement measure estimation (CEME). The preliminary experiment results show that the quaternion approach of image enhancement is an effective color image enhancement technique.



### A Novel Color Image Enhancement Method by the Transformation of Color Images to 2-D Grayscale Images
- **Arxiv ID**: http://arxiv.org/abs/1807.07962v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.07962v1)
- **Published**: 2018-07-20 04:18:21+00:00
- **Updated**: 2018-07-20 04:18:21+00:00
- **Authors**: Artyom M Grigoryan, Aparna John, Sos S Agaian
- **Comment**: 18 pages
- **Journal**: Int J Signal Process Anal 2017, 2:002
- **Summary**: A novel method of color image enhancement is proposed, in which three or four color channels of the image are transformed to one channel 2-D grayscale image. This paper describes different models of such transformations in the RGB and other color models. Color image enhancement is achieved by enhancing first the transformed grayscale image and, then, transforming back the grayscale image into the colors. The color image enhancement is done on the transformed 2-D grayscale image rather than on the color image. New algorithms of color image enhancement are described in both frequency and time domains. The enhancement by this novel method shows good results. The enhancement of the image is measured with respect to the metric referred to as the Color Enhancement Measure Estimation (CEME).



### Brain Tumor Segmentation and Tractographic Feature Extraction from Structural MR Images for Overall Survival Prediction
- **Arxiv ID**: http://arxiv.org/abs/1807.07716v3
- **DOI**: 10.1007/978-3-030-11726-9_12
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.07716v3)
- **Published**: 2018-07-20 07:10:24+00:00
- **Updated**: 2018-11-12 23:20:28+00:00
- **Authors**: Po-Yu Kao, Thuyen Ngo, Angela Zhang, Jefferson W. Chen, B. S. Manjunath
- **Comment**: 14 pages, 5 figures, 4 tables, accepted by BrainLes 2018 MICCAI
  workshop
- **Journal**: 4th International Workshop, BrainLes 2018, Held in Conjunction
  with MICCAI 2018
- **Summary**: This paper introduces a novel methodology to integrate human brain connectomics and parcellation for brain tumor segmentation and survival prediction. For segmentation, we utilize an existing brain parcellation atlas in the MNI152 1mm space and map this parcellation to each individual subject data. We use deep neural network architectures together with hard negative mining to achieve the final voxel level classification. For survival prediction, we present a new method for combining features from connectomics data, brain parcellation information, and the brain tumor mask. We leverage the average connectome information from the Human Connectome Project and map each subject brain volume onto this common connectome space. From this, we compute tractographic features that describe potential neural disruptions due to the brain tumor. These features are then used to predict the overall survival of the subjects. The main novelty in the proposed methods is the use of normalized brain parcellation data and tractography data from the human connectome project for analyzing MR images for segmentation and survival prediction. Experimental results are reported on the BraTS2018 dataset.



### Efficient Facial Representations for Age, Gender and Identity Recognition in Organizing Photo Albums using Multi-output CNN
- **Arxiv ID**: http://arxiv.org/abs/1807.07718v3
- **DOI**: 10.7717/peerj-cs.197
- **Categories**: **cs.CV**, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1807.07718v3)
- **Published**: 2018-07-20 07:12:36+00:00
- **Updated**: 2019-06-13 07:59:42+00:00
- **Authors**: Andrey V. Savchenko
- **Comment**: 19 pages, 2 figures, 8 tables
- **Journal**: PeerJ Computer Science 5:e197 (2019)
- **Summary**: This paper is focused on the automatic extraction of persons and their attributes (gender, year of born) from album of photos and videos. We propose the two-stage approach, in which, firstly, the convolutional neural network simultaneously predicts age/gender from all photos and additionally extracts facial representations suitable for face identification. We modified the MobileNet, which is preliminarily trained to perform face recognition, in order to additionally recognize age and gender. In the second stage of our approach, extracted faces are grouped using hierarchical agglomerative clustering techniques. The born year and gender of a person in each cluster are estimated using aggregation of predictions for individual photos. We experimentally demonstrated that our facial clustering quality is competitive with the state-of-the-art neural networks, though our implementation is much computationally cheaper. Moreover, our approach is characterized by more accurate video-based age/gender recognition when compared to the publicly available models.



### Improving Image Clustering With Multiple Pretrained CNN Feature Extractors
- **Arxiv ID**: http://arxiv.org/abs/1807.07760v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07760v1)
- **Published**: 2018-07-20 09:46:55+00:00
- **Updated**: 2018-07-20 09:46:55+00:00
- **Authors**: Joris Guérin, Byron Boots
- **Comment**: 13 pages, 3 figures, 4 tables. Poster presentation at BMVC 2018
  (29.9% acceptance)
- **Journal**: None
- **Summary**: For many image clustering problems, replacing raw image data with features extracted by a pretrained convolutional neural network (CNN), leads to better clustering performance. However, the specific features extracted, and, by extension, the selected CNN architecture, can have a major impact on the clustering results. In practice, this crucial design choice is often decided arbitrarily due to the impossibility of using cross-validation with unsupervised learning problems. However, information contained in the different pretrained CNN architectures may be complementary, even when pretrained on the same data. To improve clustering performance, we rephrase the image clustering problem as a multi-view clustering (MVC) problem that considers multiple different pretrained feature extractors as different "views" of the same data. We then propose a multi-input neural network architecture that is trained end-to-end to solve the MVC problem effectively. Our experimental results, conducted on three different natural image datasets, show that: 1. using multiple pretrained CNNs jointly as feature extractors improves image clustering; 2. using an end-to-end approach improves MVC; and 3. combining both produces state-of-the-art results for the problem of image clustering.



### An Efficient End-to-End Neural Model for Handwritten Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.07965v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.07965v2)
- **Published**: 2018-07-20 09:55:09+00:00
- **Updated**: 2018-07-26 13:31:24+00:00
- **Authors**: Arindam Chowdhury, Lovekesh Vig
- **Comment**: Accepted at British Machine Vision Conference 2018
- **Journal**: None
- **Summary**: Offline handwritten text recognition from images is an important problem for enterprises attempting to digitize large volumes of handmarked scanned documents/reports. Deep recurrent models such as Multi-dimensional LSTMs have been shown to yield superior performance over traditional Hidden Markov Model based approaches that suffer from the Markov assumption and therefore lack the representational power of RNNs. In this paper we introduce a novel approach that combines a deep convolutional network with a recurrent Encoder-Decoder network to map an image to a sequence of characters corresponding to the text present in the image. The entire model is trained end-to-end using Focal Loss, an improvement over the standard Cross-Entropy loss that addresses the class imbalance problem, inherent to text recognition. To enhance the decoding capacity of the model, Beam Search algorithm is employed which searches for the best sequence out of a set of hypotheses based on a joint distribution of individual characters. Our model takes as input a downsampled version of the original image thereby making it both computationally and memory efficient. The experimental results were benchmarked against two publicly available datasets, IAM and RIMES. We surpass the state-of-the-art word level accuracy on the evaluation set of both datasets by 3.5% & 1.1%, respectively.



### Physical Adversarial Examples for Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/1807.07769v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.07769v2)
- **Published**: 2018-07-20 10:14:27+00:00
- **Updated**: 2018-10-05 18:07:23+00:00
- **Authors**: Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Florian Tramer, Atul Prakash, Tadayoshi Kohno, Dawn Song
- **Comment**: This paper is the extended version of the USENIX WOOT 2018 version
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are vulnerable to adversarial examples-maliciously crafted inputs that cause DNNs to make incorrect predictions. Recent work has shown that these attacks generalize to the physical domain, to create perturbations on physical objects that fool image classifiers under a variety of real-world conditions. Such attacks pose a risk to deep learning models used in safety-critical cyber-physical systems. In this work, we extend physical attacks to more challenging object detection models, a broader class of deep learning algorithms widely used to detect and label multiple objects within a scene. Improving upon a previous physical attack on image classifiers, we create perturbed physical objects that are either ignored or mislabeled by object detection models. We implement a Disappearance Attack, in which we cause a Stop sign to "disappear" according to the detector-either by covering thesign with an adversarial Stop sign poster, or by adding adversarial stickers onto the sign. In a video recorded in a controlled lab environment, the state-of-the-art YOLOv2 detector failed to recognize these adversarial Stop signs in over 85% of the video frames. In an outdoor experiment, YOLO was fooled by the poster and sticker attacks in 72.5% and 63.5% of the video frames respectively. We also use Faster R-CNN, a different object detection model, to demonstrate the transferability of our adversarial perturbations. The created poster perturbation is able to fool Faster R-CNN in 85.9% of the video frames in a controlled lab environment, and 40.2% of the video frames in an outdoor environment. Finally, we present preliminary results with a new Creation Attack, where in innocuous physical stickers fool a model into detecting nonexistent objects.



### Dialectical GAN for SAR Image Translation: From Sentinel-1 to TerraSAR-X
- **Arxiv ID**: http://arxiv.org/abs/1807.07778v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.07778v1)
- **Published**: 2018-07-20 10:26:32+00:00
- **Updated**: 2018-07-20 10:26:32+00:00
- **Authors**: Dongyang Ao, Corneliu Octavian Dumitru, Gottfried Schwarz, Mihai Datcu
- **Comment**: 22 pages, 15 figures
- **Journal**: None
- **Summary**: Contrary to optical images, Synthetic Aperture Radar (SAR) images are in different electromagnetic spectrum where the human visual system is not accustomed to. Thus, with more and more SAR applications, the demand for enhanced high-quality SAR images has increased considerably. However, high-quality SAR images entail high costs due to the limitations of current SAR devices and their image processing resources. To improve the quality of SAR images and to reduce the costs of their generation, we propose a Dialectical Generative Adversarial Network (Dialectical GAN) to generate high-quality SAR images. This method is based on the analysis of hierarchical SAR information and the "dialectical" structure of GAN frameworks. As a demonstration, a typical example will be shown where a low-resolution SAR image (e.g., a Sentinel-1 image) with large ground coverage is translated into a high-resolution SAR image (e.g., a TerraSAR-X image). Three traditional algorithms are compared, and a new algorithm is proposed based on a network framework by combining conditional WGAN-GP (Wasserstein Generative Adversarial Network - Gradient Penalty) loss functions and Spatial Gram matrices under the rule of dialectics. Experimental results show that the SAR image translation works very well when we compare the results of our proposed method with the selected traditional methods.



### Model Agnostic Saliency for Weakly Supervised Lesion Detection from Breast DCE-MRI
- **Arxiv ID**: http://arxiv.org/abs/1807.07784v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07784v3)
- **Published**: 2018-07-20 10:48:18+00:00
- **Updated**: 2019-02-04 05:22:40+00:00
- **Authors**: Gabriel Maicas, Gerard Snaauw, Andrew P. Bradley, Ian Reid, Gustavo Carneiro
- **Comment**: None
- **Journal**: None
- **Summary**: There is a heated debate on how to interpret the decisions provided by deep learning models (DLM), where the main approaches rely on the visualization of salient regions to interpret the DLM classification process. However, these approaches generally fail to satisfy three conditions for the problem of lesion detection from medical images: 1) for images with lesions, all salient regions should represent lesions, 2) for images containing no lesions, no salient region should be produced,and 3) lesions are generally small with relatively smooth borders. We propose a new model-agnostic paradigm to interpret DLM classification decisions supported by a novel definition of saliency that incorporates the conditions above. Our model-agnostic 1-class saliency detector (MASD) is tested on weakly supervised breast lesion detection from DCE-MRI, achieving state-of-the-art detection accuracy when compared to current visualization methods.



### 3D-LMNet: Latent Embedding Matching for Accurate and Diverse 3D Point Cloud Reconstruction from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1807.07796v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07796v2)
- **Published**: 2018-07-20 11:32:02+00:00
- **Updated**: 2019-03-26 06:49:01+00:00
- **Authors**: Priyanka Mandikal, K L Navaneet, Mayank Agarwal, R. Venkatesh Babu
- **Comment**: Accepted at BMVC 2018; Codes are available at
  https://github.com/val-iisc/3d-lmnet
- **Journal**: None
- **Summary**: 3D reconstruction from single view images is an ill-posed problem. Inferring the hidden regions from self-occluded images is both challenging and ambiguous. We propose a two-pronged approach to address these issues. To better incorporate the data prior and generate meaningful reconstructions, we propose 3D-LMNet, a latent embedding matching approach for 3D reconstruction. We first train a 3D point cloud auto-encoder and then learn a mapping from the 2D image to the corresponding learnt embedding. To tackle the issue of uncertainty in the reconstruction, we predict multiple reconstructions that are consistent with the input view. This is achieved by learning a probablistic latent space with a novel view-specific diversity loss. Thorough quantitative and qualitative analysis is performed to highlight the significance of the proposed approach. We outperform state-of-the-art approaches on the task of single-view 3D reconstruction on both real and synthetic datasets while generating multiple plausible reconstructions, demonstrating the generalizability and utility of our approach.



### Competition vs. Concatenation in Skip Connections of Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.07803v1
- **DOI**: 10.1007/978-3-030-00919-9_25
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.07803v1)
- **Published**: 2018-07-20 12:06:06+00:00
- **Updated**: 2018-07-20 12:06:06+00:00
- **Authors**: Santiago Estrada, Sailesh Conjeti, Muneer Ahmad, Nassir Navab, Martin Reuter
- **Comment**: Paper accepted on MICCAI-MLMI 2018 workshop
- **Journal**: None
- **Summary**: Increased information sharing through short and long-range skip connections between layers in fully convolutional networks have demonstrated significant improvement in performance for semantic segmentation. In this paper, we propose Competitive Dense Fully Convolutional Networks (CDFNet) by introducing competitive maxout activations in place of naive feature concatenation for inducing competition amongst layers. Within CDFNet, we propose two architectural contributions, namely competitive dense block (CDB) and competitive unpooling block (CUB) to induce competition at local and global scales for short and long-range skip connections respectively. This extension is demonstrated to boost learning of specialized sub-networks targeted at segmenting specific anatomies, which in turn eases the training of complex tasks. We present the proof-of-concept on the challenging task of whole body segmentation in the publicly available VISCERAL benchmark and demonstrate improved performance over multiple learning and registration based state-of-the-art methods.



### Rank Minimization for Snapshot Compressive Imaging
- **Arxiv ID**: http://arxiv.org/abs/1807.07837v1
- **DOI**: 10.1109/TPAMI.2018.2873587
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07837v1)
- **Published**: 2018-07-20 13:44:37+00:00
- **Updated**: 2018-07-20 13:44:37+00:00
- **Authors**: Yang Liu, Xin Yuan, Jinli Suo, David J. Brady, Qionghai Dai
- **Comment**: 18 pages, 21 figures, and 2 tables. Code available at
  https://github.com/liuyang12/DeSCI
- **Journal**: None
- **Summary**: Snapshot compressive imaging (SCI) refers to compressive imaging systems where multiple frames are mapped into a single measurement, with video compressive imaging and hyperspectral compressive imaging as two representative applications. Though exciting results of high-speed videos and hyperspectral images have been demonstrated, the poor reconstruction quality precludes SCI from wide applications.This paper aims to boost the reconstruction quality of SCI via exploiting the high-dimensional structure in the desired signal. We build a joint model to integrate the nonlocal self-similarity of video/hyperspectral frames and the rank minimization approach with the SCI sensing process. Following this, an alternating minimization algorithm is developed to solve this non-convex problem. We further investigate the special structure of the sampling process in SCI to tackle the computational workload and memory issues in SCI reconstruction. Both simulation and real data (captured by four different SCI cameras) results demonstrate that our proposed algorithm leads to significant improvements compared with current state-of-the-art algorithms. We hope our results will encourage the researchers and engineers to pursue further in compressive imaging for real applications.



### Surgical Phase Recognition of Short Video Shots Based on Temporal Modeling of Deep Features
- **Arxiv ID**: http://arxiv.org/abs/1807.07853v4
- **DOI**: 10.5220/0007352000210029
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07853v4)
- **Published**: 2018-07-20 14:10:32+00:00
- **Updated**: 2018-12-07 08:00:17+00:00
- **Authors**: Constantinos Loukas
- **Comment**: 6 pages, 4 figures, 6 tables
- **Journal**: None
- **Summary**: Recognizing the phases of a laparoscopic surgery (LS) operation form its video constitutes a fundamental step for efficient content representation, indexing and retrieval in surgical video databases. In the literature, most techniques focus on phase segmentation of the entire LS video using hand-crafted visual features, instrument usage signals, and recently convolutional neural networks (CNNs). In this paper we address the problem of phase recognition of short video shots (10s) of the operation, without utilizing information about the preceding/forthcoming video frames, their phase labels or the instruments used. We investigate four state-of-the-art CNN architectures (Alexnet, VGG19, GoogleNet, and ResNet101), for feature extraction via transfer learning. Visual saliency was employed for selecting the most informative region of the image as input to the CNN. Video shot representation was based on two temporal pooling mechanisms. Most importantly, we investigate the role of 'elapsed time' (from the beginning of the operation), and we show that inclusion of this feature can increase performance dramatically (69% vs. 75% mean accuracy). Finally, a long short-term memory (LSTM) network was trained for video shot classification based on the fusion of CNN features with 'elapsed time', increasing the accuracy to 86%. Our results highlight the prominent role of visual saliency, long-range temporal recursion and 'elapsed time' (a feature so far ignored), for surgical phase recognition.



### Talking Face Generation by Adversarially Disentangled Audio-Visual Representation
- **Arxiv ID**: http://arxiv.org/abs/1807.07860v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.07860v2)
- **Published**: 2018-07-20 14:26:32+00:00
- **Updated**: 2019-04-23 16:40:06+00:00
- **Authors**: Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, Xiaogang Wang
- **Comment**: AAAI Conference on Artificial Intelligence (AAAI 2019) Oral
  Presentation. Code, models, and video results are available on our webpage:
  https://liuziwei7.github.io/projects/TalkingFace.html
- **Journal**: None
- **Summary**: Talking face generation aims to synthesize a sequence of face images that correspond to a clip of speech. This is a challenging task because face appearance variation and semantics of speech are coupled together in the subtle movements of the talking face regions. Existing works either construct specific face appearance model on specific subjects or model the transformation between lip motion and speech. In this work, we integrate both aspects and enable arbitrary-subject talking face generation by learning disentangled audio-visual representation. We find that the talking face sequence is actually a composition of both subject-related information and speech-related information. These two spaces are then explicitly disentangled through a novel associative-and-adversarial training process. This disentangled representation has an advantage where both audio and video can serve as inputs for generation. Extensive experiments show that the proposed approach generates realistic talking face sequences on arbitrary subjects with much clearer lip motion patterns than previous work. We also demonstrate the learned audio-visual representation is extremely useful for the tasks of automatic lip reading and audio-video retrieval.



### From Face Recognition to Models of Identity: A Bayesian Approach to Learning about Unknown Identities from Unsupervised Data
- **Arxiv ID**: http://arxiv.org/abs/1807.07872v1
- **DOI**: 10.1007/978-3-030-01216-8_46
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07872v1)
- **Published**: 2018-07-20 14:40:10+00:00
- **Updated**: 2018-07-20 14:40:10+00:00
- **Authors**: Daniel C. Castro, Sebastian Nowozin
- **Comment**: Accepted for publication at ECCV 2018
- **Journal**: None
- **Summary**: Current face recognition systems robustly recognize identities across a wide variety of imaging conditions. In these systems recognition is performed via classification into known identities obtained from supervised identity annotations. There are two problems with this current paradigm: (1) current systems are unable to benefit from unlabelled data which may be available in large quantities; and (2) current systems equate successful recognition with labelling a given input image. Humans, on the other hand, regularly perform identification of individuals completely unsupervised, recognising the identity of someone they have seen before even without being able to name that individual. How can we go beyond the current classification paradigm towards a more human understanding of identities? We propose an integrated Bayesian model that coherently reasons about the observed images, identities, partial knowledge about names, and the situational context of each observation. While our model achieves good recognition performance against known identities, it can also discover new identities from unsupervised data and learns to associate identities with different contexts depending on which identities tend to be observed together. In addition, the proposed semi-supervised component is able to handle not only acquaintances, whose names are known, but also unlabelled familiar faces and complete strangers in a unified framework.



### Perceptual Video Super Resolution with Enhanced Temporal Consistency
- **Arxiv ID**: http://arxiv.org/abs/1807.07930v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07930v2)
- **Published**: 2018-07-20 16:39:15+00:00
- **Updated**: 2019-05-02 16:59:15+00:00
- **Authors**: Eduardo Pérez-Pellitero, Mehdi S. M. Sajjadi, Michael Hirsch, Bernhard Schölkopf
- **Comment**: Major revision and improvement of the manuscript: New network
  architecture, new loss function and extended experiments
- **Journal**: None
- **Summary**: With the advent of perceptual loss functions, new possibilities in super-resolution have emerged, and we currently have models that successfully generate near-photorealistic high-resolution images from their low-resolution observations. Up to now, however, such approaches have been exclusively limited to single image super-resolution. The application of perceptual loss functions on video processing still entails several challenges, mostly related to the lack of temporal consistency of the generated images, i.e., flickering artifacts. In this work, we present a novel adversarial recurrent network for video upscaling that is able to produce realistic textures in a temporally consistent way. The proposed architecture naturally leverages information from previous frames due to its recurrent architecture, i.e. the input to the generator is composed of the low-resolution image and, additionally, the warped output of the network at the previous step. Together with a video discriminator, we also propose additional loss functions to further reinforce temporal consistency in the generated sequences. The experimental validation of our algorithm shows the effectiveness of our approach which obtains images with high perceptual quality and improved temporal consistency.



### Large scale evaluation of local image feature detectors on homography datasets
- **Arxiv ID**: http://arxiv.org/abs/1807.07939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07939v1)
- **Published**: 2018-07-20 16:54:27+00:00
- **Updated**: 2018-07-20 16:54:27+00:00
- **Authors**: Karel Lenc, Andrea Vedaldi
- **Comment**: Accepted to BMVC 2018
- **Journal**: None
- **Summary**: We present a large scale benchmark for the evaluation of local feature detectors. Our key innovation is the introduction of a new evaluation protocol which extends and improves the standard detection repeatability measure. The new protocol is better for assessment on a large number of images and reduces the dependency of the results on unwanted distractors such as the number of detected features and the feature magnification factor. Additionally, our protocol provides a comprehensive assessment of the expected performance of detectors under several practical scenarios. Using images from the recently-introduced HPatches dataset, we evaluate a range of state-of-the-art local feature detectors on two main tasks: viewpoint and illumination invariant detection. Contrary to previous detector evaluations, our study contains an order of magnitude more image sequences, resulting in a quantitative evaluation significantly more robust to over-fitting. We also show that traditional detectors are still very competitive when compared to recent deep-learning alternatives.



### Future Semantic Segmentation with Convolutional LSTM
- **Arxiv ID**: http://arxiv.org/abs/1807.07946v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.07946v1)
- **Published**: 2018-07-20 17:31:06+00:00
- **Updated**: 2018-07-20 17:31:06+00:00
- **Authors**: Seyed shahabeddin Nabavi, Mrigank Rochan, Yang, Wang
- **Comment**: Accepted to BMVC 2018
- **Journal**: None
- **Summary**: We consider the problem of predicting semantic segmentation of future frames in a video. Given several observed frames in a video, our goal is to predict the semantic segmentation map of future frames that are not yet observed. A reliable solution to this problem is useful in many applications that require real-time decision making, such as autonomous driving. We propose a novel model that uses convolutional LSTM (ConvLSTM) to encode the spatiotemporal information of observed frames for future prediction. We also extend our model to use bidirectional ConvLSTM to capture temporal information in both directions. Our proposed approach outperforms other state-of-the-art methods on the benchmark dataset.



### Optimize Deep Convolutional Neural Network with Ternarized Weights and High Accuracy
- **Arxiv ID**: http://arxiv.org/abs/1807.07948v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.07948v1)
- **Published**: 2018-07-20 17:36:05+00:00
- **Updated**: 2018-07-20 17:36:05+00:00
- **Authors**: Zhezhi He, Boqing Gong, Deliang Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolution neural network has achieved great success in many artificial intelligence applications. However, its enormous model size and massive computation cost have become the main obstacle for deployment of such powerful algorithm in the low power and resource-limited embedded systems. As the countermeasure to this problem, in this work, we propose statistical weight scaling and residual expansion methods to reduce the bit-width of the whole network weight parameters to ternary values (i.e. -1, 0, +1), with the objectives to greatly reduce model size, computation cost and accuracy degradation caused by the model compression. With about 16x model compression rate, our ternarized ResNet-32/44/56 could outperform full-precision counterparts by 0.12%, 0.24% and 0.18% on CIFAR- 10 dataset. We also test our ternarization method with AlexNet and ResNet-18 on ImageNet dataset, which both achieve the best top-1 accuracy compared to recent similar works, with the same 16x compression rate. If further incorporating our residual expansion method, compared to the full-precision counterpart, our ternarized ResNet-18 even improves the top-5 accuracy by 0.61% and merely degrades the top-1 accuracy only by 0.42% for the ImageNet dataset, with 8x model compression rate. It outperforms the recent ABC-Net by 1.03% in top-1 accuracy and 1.78% in top-5 accuracy, with around 1.25x higher compression rate and more than 6x computation reduction due to the weight sparsity.



### Ensemble of Deep Learned Features for Melanoma Classification
- **Arxiv ID**: http://arxiv.org/abs/1807.08008v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08008v1)
- **Published**: 2018-07-20 19:25:22+00:00
- **Updated**: 2018-07-20 19:25:22+00:00
- **Authors**: Loris Nanni, Alessandra Lumini, Stefano Ghidoni
- **Comment**: None
- **Journal**: None
- **Summary**: The aim of this work is to propose an ensemble of descriptors for Melanoma Classification, whose performance has been evaluated on validation and test datasets of the melanoma challenge 2018. The system proposed here achieves a strong discriminative power thanks to the combination of multiple descriptors. The proposed system represents a very simple yet effective way of boosting the performance of trained CNNs by composing multiple CNNs into an ensemble and combining scores by sum rule. Several types of ensembles are considered, with different CNN architectures along with different learning parameter sets. Moreover CNN are used as feature extractors: an input image is processed by a trained CNN and the response of a particular layer (usually the classification layer, but also internal layers can be employed) is treated as a descriptor for the image and used for training a set of Support Vector Machines (SVM).



### Explaining Image Classifiers by Counterfactual Generation
- **Arxiv ID**: http://arxiv.org/abs/1807.08024v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08024v3)
- **Published**: 2018-07-20 20:48:44+00:00
- **Updated**: 2019-02-25 16:22:07+00:00
- **Authors**: Chun-Hao Chang, Elliot Creager, Anna Goldenberg, David Duvenaud
- **Comment**: ICLR 2019 Camera Ready
- **Journal**: None
- **Summary**: When an image classifier makes a prediction, which parts of the image are relevant and why? We can rephrase this question to ask: which parts of the image, if they were not seen by the classifier, would most change its decision? Producing an answer requires marginalizing over images that could have been seen but weren't. We can sample plausible image in-fills by conditioning a generative model on the rest of the image. We then optimize to find the image regions that most change the classifier's decision after in-fill. Our approach contrasts with ad-hoc in-filling approaches, such as blurring or injecting noise, which generate inputs far from the data distribution, and ignore informative relationships between different parts of the image. Our method produces more compact and relevant saliency maps, with fewer artifacts compared to previous methods.



### Filter Distillation for Network Compression
- **Arxiv ID**: http://arxiv.org/abs/1807.10585v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10585v4)
- **Published**: 2018-07-20 23:36:11+00:00
- **Updated**: 2019-12-11 13:43:48+00:00
- **Authors**: Xavier Suau, Luca Zappella, Nicholas Apostoloff
- **Comment**: 10 pages, 3 figures, Deep neural network compression, spectral
  analysis, machine learning
- **Journal**: WACV 2020
- **Summary**: In this paper we introduce Principal Filter Analysis (PFA), an easy to use and effective method for neural network compression. PFA exploits the correlation between filter responses within network layers to recommend a smaller network that maintain as much as possible the accuracy of the full model. We propose two algorithms: the first allows users to target compression to specific network property, such as number of trainable variable (footprint), and produces a compressed model that satisfies the requested property while preserving the maximum amount of spectral energy in the responses of each layer, while the second is a parameter-free heuristic that selects the compression used at each layer by trying to mimic an ideal set of uncorrelated responses. Since PFA compresses networks based on the correlation of their responses we show in our experiments that it gains the additional flexibility of adapting each architecture to a specific domain while compressing. PFA is evaluated against several architectures and datasets, and shows considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. Our tests show that PFA is competitive with state-of-the-art approaches while removing adoption barriers thanks to its practical implementation, intuitive philosophy and ease of use.



