# Arxiv Papers in cs.CV on 2018-07-21
### S3D: Single Shot multi-Span Detector via Fully 3D Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.08069v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08069v2)
- **Published**: 2018-07-21 02:34:57+00:00
- **Updated**: 2018-08-07 18:33:06+00:00
- **Authors**: Da Zhang, Xiyang Dai, Xin Wang, Yuan-Fang Wang
- **Comment**: BMVC 2018 Oral
- **Journal**: None
- **Summary**: In this paper, we present a novel Single Shot multi-Span Detector for temporal activity detection in long, untrimmed videos using a simple end-to-end fully three-dimensional convolutional (Conv3D) network. Our architecture, named S3D, encodes the entire video stream and discretizes the output space of temporal activity spans into a set of default spans over different temporal locations and scales. At prediction time, S3D predicts scores for the presence of activity categories in each default span and produces temporal adjustments relative to the span location to predict the precise activity duration. Unlike many state-of-the-art systems that require a separate proposal and classification stage, our S3D is intrinsically simple and dedicatedly designed for single-shot, end-to-end temporal activity detection. When evaluating on THUMOS'14 detection benchmark, S3D achieves state-of-the-art performance and is very efficient and can operate at 1271 FPS.



### Conditional Infilling GANs for Data Augmentation in Mammogram Classification
- **Arxiv ID**: http://arxiv.org/abs/1807.08093v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08093v2)
- **Published**: 2018-07-21 06:29:10+00:00
- **Updated**: 2018-08-24 16:57:16+00:00
- **Authors**: Eric Wu, Kevin Wu, David Cox, William Lotter
- **Comment**: To appear in MICCAI 2018, Breast Image Analysis Workshop
- **Journal**: None
- **Summary**: Deep learning approaches to breast cancer detection in mammograms have recently shown promising results. However, such models are constrained by the limited size of publicly available mammography datasets, in large part due to privacy concerns and the high cost of generating expert annotations. Limited dataset size is further exacerbated by substantial class imbalance since "normal" images dramatically outnumber those with findings. Given the rapid progress of generative models in synthesizing realistic images, and the known effectiveness of simple data augmentation techniques (e.g. horizontal flipping), we ask if it is possible to synthetically augment mammogram datasets using generative adversarial networks (GANs). We train a class-conditional GAN to perform contextual in-filling, which we then use to synthesize lesions onto healthy screening mammograms. First, we show that GANs are capable of generating high-resolution synthetic mammogram patches. Next, we experimentally evaluate using the augmented dataset to improve breast cancer classification performance. We observe that a ResNet-50 classifier trained with GAN-augmented training data produces a higher AUROC compared to the same model trained only on traditionally augmented data, demonstrating the potential of our approach.



### Person Search via A Mask-Guided Two-Stream CNN Model
- **Arxiv ID**: http://arxiv.org/abs/1807.08107v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08107v1)
- **Published**: 2018-07-21 08:23:38+00:00
- **Updated**: 2018-07-21 08:23:38+00:00
- **Authors**: Di Chen, Shanshan Zhang, Wanli Ouyang, Jian Yang, Ying Tai
- **Comment**: accepted as poster to ECCV 2018
- **Journal**: None
- **Summary**: In this work, we tackle the problem of person search, which is a challenging task consisted of pedestrian detection and person re-identification~(re-ID). Instead of sharing representations in a single joint model, we find that separating detector and re-ID feature extraction yields better performance. In order to extract more representative features for each identity, we segment out the foreground person from the original image patch. We propose a simple yet effective re-ID method, which models foreground person and original image patches individually, and obtains enriched representations from two separate CNN streams. From the experiments on two standard person search benchmarks of CUHK-SYSU and PRW, we achieve mAP of $83.0\%$ and $32.6\%$ respectively, surpassing the state of the art by a large margin (more than 5pp).



### Simultaneous Adversarial Training - Learn from Others Mistakes
- **Arxiv ID**: http://arxiv.org/abs/1807.08108v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.08108v3)
- **Published**: 2018-07-21 08:28:21+00:00
- **Updated**: 2018-09-10 02:13:28+00:00
- **Authors**: Zukang Liao
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial examples are maliciously tweaked images that can easily fool machine learning techniques, such as neural networks, but they are normally not visually distinguishable for human beings. One of the main approaches to solve this problem is to retrain the networks using those adversarial examples, namely adversarial training. However, standard adversarial training might not actually change the decision boundaries but cause the problem of gradient masking, resulting in a weaker ability to generate adversarial examples. Therefore, it cannot alleviate the problem of black-box attacks, where adversarial examples generated from other networks can transfer to the targeted one. In order to reduce the problem of black-box attacks, we propose a novel method that allows two networks to learn from each others' adversarial examples and become resilient to black-box attacks. We also combine this method with a simple domain adaptation to further improve the performance.



### Generic Camera Attribute Control using Bayesian Optimization
- **Arxiv ID**: http://arxiv.org/abs/1807.10596v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.10596v3)
- **Published**: 2018-07-21 09:09:27+00:00
- **Updated**: 2018-08-14 10:43:34+00:00
- **Authors**: Joowan Kim, Younggun Cho, Ayoung Kim
- **Comment**: 8 pages, 11 figures, conference
- **Journal**: None
- **Summary**: Cameras are the most widely exploited sensor in both robotics and computer vision communities. Despite their popularity, two dominant attributes (i.e., gain and exposure time) have been determined empirically and images are captured in very passive manner. In this paper, we present an active and generic camera attribute control scheme using Bayesian optimization. We extend from our previous work [1] in two aspects. First, we propose a method that jointly controls camera gain and exposure time. Secondly, to speed up the Bayesian optimization process, we introduce image synthesis using the camera response function (CRF). These synthesized images allowed us to diminish the image acquisition time during the Bayesian optimization phase, substantially improving overall control performance. The proposed method is validated both in an indoor and an outdoor environment where light condition rapidly changes. Supplementary material is available at https://youtu.be/XTYR_Mih3OU .



### Multiple Convolutional Neural Network for Skin Dermoscopic Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1807.08114v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08114v2)
- **Published**: 2018-07-21 09:42:12+00:00
- **Updated**: 2018-07-26 03:17:47+00:00
- **Authors**: Yanhui Guo, Amira S. Ashour
- **Comment**: 9 pages, ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection
- **Journal**: None
- **Summary**: Melanoma classification is a serious stage to identify the skin disease. It is considered a challenging process due to the intra-class discrepancy of melanomas, skin lesions low contrast, and the artifacts in the dermoscopy images, including noise, existence of hair, air bubbles, and the similarity between melanoma and non-melanoma cases. To solve these problems, we propose a novel multiple convolution neural network model (MCNN) to classify different seven disease types in dermoscopic images, where several models were trained separately using an additive sample learning strategy. The MCNN model is trained and tested using the training and validation sets from the International Skin Imaging Collaboration (ISIC 2018), respectively. The receiver operating characteristic (ROC) curve is used to evaluate the performance of the proposed method. The values of AUC (the area under the ROC curve) were used to evaluate the performance of the MCNN.



### Coupled dictionary learning for unsupervised change detection between multi-sensor remote sensing images
- **Arxiv ID**: http://arxiv.org/abs/1807.08118v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1807.08118v2)
- **Published**: 2018-07-21 10:02:34+00:00
- **Updated**: 2019-09-02 13:44:07+00:00
- **Authors**: Vinicius Ferraris, Nicolas Dobigeon, Yanna Cavalcanti, Thomas Oberlin, Marie Chabert
- **Comment**: Submitted manuscript under consideration at Computer Vision and Image
  Understanding
- **Journal**: None
- **Summary**: Archetypal scenarios for change detection generally consider two images acquired through sensors of the same modality. However, in some specific cases such as emergency situations, the only images available may be those acquired through sensors of different modalities. This paper addresses the problem of unsupervisedly detecting changes between two observed images acquired by sensors of different modalities with possibly different resolutions. These sensor dissimilarities introduce additional issues in the context of operational change detection that are not addressed by most of the classical methods. This paper introduces a novel framework to effectively exploit the available information by modelling the two observed images as a sparse linear combination of atoms belonging to a pair of coupled overcomplete dictionaries learnt from each observed image. As they cover the same geographical location, codes are expected to be globally similar, except for possible changes in sparse spatial locations. Thus, the change detection task is envisioned through a dual code estimation which enforces spatial sparsity in the difference between the estimated codes associated with each image. This problem is formulated as an inverse problem which is iteratively solved using an efficient proximal alternating minimization algorithm accounting for nonsmooth and nonconvex functions. The proposed method is applied to real images with simulated yet realistic and real changes. A comparison with state-of-the-art change detection methods evidences the accuracy of the proposed strategy.



### Integrating Feature and Image Pyramid: A Lung Nodule Detector Learned in Curriculum Fashion
- **Arxiv ID**: http://arxiv.org/abs/1807.08135v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08135v2)
- **Published**: 2018-07-21 12:02:19+00:00
- **Updated**: 2018-08-01 07:22:38+00:00
- **Authors**: Benyuan Sun, Zhen Zhou, Fandong Zhang, Xiuli Li, Yizhou Wang
- **Comment**: Nodule Detection, Deep Learning, Curriculum Learning
- **Journal**: None
- **Summary**: Lung nodules suffer large variation in size and appearance in CT images. Nodules less than 10mm can easily lose information after down-sampling in convolutional neural networks, which results in low sensitivity. In this paper, a combination of 3D image and feature pyramid is exploited to integrate lower-level texture features with high-level semantic features, thus leading to a higher recall. However, 3D operations are time and memory consuming, which aggravates the situation with the explosive growth of medical images. To tackle this problem, we propose a general curriculum training strategy to speed up training. An dynamic sampling method is designed to pick up partial samples which give the best contribution to network training, thus leading to much less time consuming. In experiments, we demonstrate that the proposed network outperforms previous state-of-the-art methods. Meanwhile, our sampling strategy halves the training time of the proposal network on LUNA16.



### A post-processing method to improve the white matter hyperintensity segmentation accuracy for randomly-initialized U-net
- **Arxiv ID**: http://arxiv.org/abs/1807.10600v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.10600v1)
- **Published**: 2018-07-21 12:45:45+00:00
- **Updated**: 2018-07-21 12:45:45+00:00
- **Authors**: Yue Zhang, Wanli Chen, Yifan Chen, Xiaoying Tang
- **Comment**: None
- **Journal**: None
- **Summary**: White matter hyperintensity (WMH) is commonly found in elder individuals and appears to be associated with brain diseases. U-net is a convolutional network that has been widely used for biomedical image segmentation. Recently, U-net has been successfully applied to WMH segmentation. Random initialization is usally used to initialize the model weights in the U-net. However, the model may coverage to different local optima with different randomly initialized weights. We find a combination of thresholding and averaging the outputs of U-nets with different random initializations can largely improve the WMH segmentation accuracy. Based on this observation, we propose a post-processing technique concerning the way how averaging and thresholding are conducted. Specifically, we first transfer the score maps from three U-nets to binary masks via thresholding and then average those binary masks to obtain the final WMH segmentation. Both quantitative analysis (via the Dice similarity coefficient) and qualitative analysis (via visual examinations) reveal the superior performance of the proposed method. This post-processing technique is independent of the model used. As such, it can also be applied to situations where other deep learning models are employed, especially when random initialization is adopted and pre-training is unavailable.



### Spatial Correlation and Value Prediction in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.10598v2
- **DOI**: 10.1109/LCA.2018.2890236
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10598v2)
- **Published**: 2018-07-21 14:56:30+00:00
- **Updated**: 2019-01-01 20:10:32+00:00
- **Authors**: Gil Shomron, Uri Weiser
- **Comment**: This paper has been accepted to IEEE Computer Architecture Letters
  (https://ieeexplore.ieee.org/document/8594568)
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) are a widely used form of deep neural networks, introducing state-of-the-art results for different problems such as image classification, computer vision tasks, and speech recognition. However, CNNs are compute intensive, requiring billions of multiply-accumulate (MAC) operations per input. To reduce the number of MACs in CNNs, we propose a value prediction method that exploits the spatial correlation of zero-valued activations within the CNN output feature maps, thereby saving convolution operations. Our method reduces the number of MAC operations by 30.4%, averaged on three modern CNNs for ImageNet, with top-1 accuracy degradation of 1.7%, and top-5 accuracy degradation of 1.1%.



### Inductive Visual Localisation: Factorised Training for Superior Generalisation
- **Arxiv ID**: http://arxiv.org/abs/1807.08179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08179v1)
- **Published**: 2018-07-21 17:09:16+00:00
- **Updated**: 2018-07-21 17:09:16+00:00
- **Authors**: Ankush Gupta, Andrea Vedaldi, Andrew Zisserman
- **Comment**: In BMVC 2018 (spotlight)
- **Journal**: None
- **Summary**: End-to-end trained Recurrent Neural Networks (RNNs) have been successfully applied to numerous problems that require processing sequences, such as image captioning, machine translation, and text recognition. However, RNNs often struggle to generalise to sequences longer than the ones encountered during training. In this work, we propose to optimise neural networks explicitly for induction. The idea is to first decompose the problem in a sequence of inductive steps and then to explicitly train the RNN to reproduce such steps. Generalisation is achieved as the RNN is not allowed to learn an arbitrary internal state; instead, it is tasked with mimicking the evolution of a valid state. In particular, the state is restricted to a spatial memory map that tracks parts of the input image which have been accounted for in previous steps. The RNN is trained for single inductive steps, where it produces updates to the memory in addition to the desired output. We evaluate our method on two different visual recognition problems involving visual sequences: (1) text spotting, i.e. joint localisation and reading of text in images containing multiple lines (or a block) of text, and (2) sequential counting of objects in aerial images. We show that inductive training of recurrent models enhances their generalisation ability on challenging image datasets.



### Decouple Learning for Parameterized Image Operators
- **Arxiv ID**: http://arxiv.org/abs/1807.08186v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08186v2)
- **Published**: 2018-07-21 17:58:54+00:00
- **Updated**: 2018-08-09 17:08:45+00:00
- **Authors**: Qingnan Fan, Dongdong Chen, Lu Yuan, Gang Hua, Nenghai Yu, Baoquan Chen
- **Comment**: Accepted by ECCV2018
- **Journal**: None
- **Summary**: Many different deep networks have been used to approximate, accelerate or improve traditional image operators, such as image smoothing, super-resolution and denoising. Among these traditional operators, many contain parameters which need to be tweaked to obtain the satisfactory results, which we refer to as "parameterized image operators". However, most existing deep networks trained for these operators are only designed for one specific parameter configuration, which does not meet the needs of real scenarios that usually require flexible parameters settings. To overcome this limitation, we propose a new decouple learning algorithm to learn from the operator parameters to dynamically adjust the weights of a deep network for image operators, denoted as the base network. The learned algorithm is formed as another network, namely the weight learning network, which can be end-to-end jointly trained with the base network. Experiments demonstrate that the proposed framework can be successfully applied to many traditional parameterized image operators. We provide more analysis to better understand the proposed framework, which may inspire more promising research in this direction. Our codes and models have been released in https://github.com/fqnchina/DecoupleLearning



### Equal But Not The Same: Understanding the Implicit Relationship Between Persuasive Images and Text
- **Arxiv ID**: http://arxiv.org/abs/1807.08205v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08205v1)
- **Published**: 2018-07-21 20:53:39+00:00
- **Updated**: 2018-07-21 20:53:39+00:00
- **Authors**: Mingda Zhang, Rebecca Hwa, Adriana Kovashka
- **Comment**: To appear in BMVC2018
- **Journal**: None
- **Summary**: Images and text in advertisements interact in complex, non-literal ways. The two channels are usually complementary, with each channel telling a different part of the story. Current approaches, such as image captioning methods, only examine literal, redundant relationships, where image and text show exactly the same content. To understand more complex relationships, we first collect a dataset of advertisement interpretations for whether the image and slogan in the same visual advertisement form a parallel (conveying the same message without literally saying the same thing) or non-parallel relationship, with the help of workers recruited on Amazon Mechanical Turk. We develop a variety of features that capture the creativity of images and the specificity or ambiguity of text, as well as methods that analyze the semantics within and across channels. We show that our method outperforms standard image-text alignment approaches on predicting the parallel/non-parallel relationship between image and text.



