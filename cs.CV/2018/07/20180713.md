# Arxiv Papers in cs.CV on 2018-07-13
### Extracting Contact and Motion from Manipulation Videos
- **Arxiv ID**: http://arxiv.org/abs/1807.04870v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.04870v3)
- **Published**: 2018-07-13 00:15:39+00:00
- **Updated**: 2019-02-03 01:40:26+00:00
- **Authors**: Konstantinos Zampogiannis, Kanishka Ganguly, Cornelia Fermuller, Yiannis Aloimonos
- **Comment**: None
- **Journal**: None
- **Summary**: When we physically interact with our environment using our hands, we touch objects and force them to move: contact and motion are defining properties of manipulation. In this paper, we present an active, bottom-up method for the detection of actor-object contacts and the extraction of moved objects and their motions in RGBD videos of manipulation actions. At the core of our approach lies non-rigid registration: we continuously warp a point cloud model of the observed scene to the current video frame, generating a set of dense 3D point trajectories. Under loose assumptions, we employ simple point cloud segmentation techniques to extract the actor and subsequently detect actor-environment contacts based on the estimated trajectories. For each such interaction, using the detected contact as an attention mechanism, we obtain an initial motion segment for the manipulated object by clustering trajectories in the contact area vicinity and then we jointly refine the object segment and estimate its 6DOF pose in all observed frames. Because of its generality and the fundamental, yet highly informative, nature of its outputs, our approach is applicable to a wide range of perception and planning tasks. We qualitatively evaluate our method on a number of input sequences and present a comprehensive robot imitation learning example, in which we demonstrate the crucial role of our outputs in developing action representations/plans from observation.



### Effective Occlusion Handling for Fast Correlation Filter-based Trackers
- **Arxiv ID**: http://arxiv.org/abs/1807.04880v3
- **DOI**: 10.22161/eec.562
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04880v3)
- **Published**: 2018-07-13 01:23:24+00:00
- **Updated**: 2021-01-01 13:19:18+00:00
- **Authors**: Zheng Zhang, T. T. Wong
- **Comment**: www.eecjournal.com/ (Vol-5,Issue-6,November - December 2020)
- **Journal**: None
- **Summary**: Correlation filter-based trackers heavily suffer from the problem of multiple peaks in their response maps incurred by occlusions. Moreover, the whole tracking pipeline may break down due to the uncertainties brought by shifting among peaks, which will further lead to the degraded correlation filter model. To alleviate the drift problem caused by occlusions, we propose a novel scheme to choose the specific filter model according to different scenarios. Specifically, an effective measurement function is designed to evaluate the quality of filter response. A sophisticated strategy is employed to judge whether occlusions occur, and then decide how to update the filter models. In addition, we take advantage of both log-polar method and pyramid-like approach to estimate the best scale of the target. We evaluate our proposed approach on VOT2018 challenge and OTB100 dataset, whose experimental result shows that the proposed tracker achieves the promising performance compared against the state-of-the-art trackers.



### Utilizing Smartphone-Based Machine Learning in Medical Monitor Data Collection: Seven Segment Digit Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.04888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04888v1)
- **Published**: 2018-07-13 02:12:21+00:00
- **Updated**: 2018-07-13 02:12:21+00:00
- **Authors**: Varun N. Shenoy, Oliver O. Aalami
- **Comment**: Accepted for publication in AMIA 2017 Annual Symposium
- **Journal**: Shenoy VN, Aalami OO. Utilizing Smartphone-Based Machine Learning
  in Medical Monitor Data Collection: Seven Segment Digit Recognition. AMIA
  Annual Symposium Proceedings. 2017;2017:1564-1570
- **Summary**: Biometric measurements captured from medical devices, such as blood pressure gauges, glucose monitors, and weighing scales, are essential to tracking a patient's health. Trends in these measurements can accurately track diabetes, cardiovascular issues, and assist medication management for patients. Currently, patients record their results and data of measurement in a physical notebook. It may be weeks before a doctor sees a patient's records and can assess the health of the patient. With a predicted 6.8 billion smartphones in the world by 2022, health monitoring platforms, such as Apple's HealthKit, can be leveraged to provide the right care at the right time. This research presents a mobile application that enables users to capture medical monitor data and send it to their doctor swiftly. A key contribution of this paper is a robust engine that can recognize digits from medical monitors with an accuracy of 98.2%.



### Optical Flow Based Real-time Moving Object Detection in Unconstrained Scenes
- **Arxiv ID**: http://arxiv.org/abs/1807.04890v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04890v1)
- **Published**: 2018-07-13 02:15:39+00:00
- **Updated**: 2018-07-13 02:15:39+00:00
- **Authors**: Junjie Huang, Wei Zou, Jiagang Zhu, Zheng Zhu
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: Real-time moving object detection in unconstrained scenes is a difficult task due to dynamic background, changing foreground appearance and limited computational resource. In this paper, an optical flow based moving object detection framework is proposed to address this problem. We utilize homography matrixes to online construct a background model in the form of optical flow. When judging out moving foregrounds from scenes, a dual-mode judge mechanism is designed to heighten the system's adaptation to challenging situations. In experiment part, two evaluation metrics are redefined for more properly reflecting the performance of methods. We quantitatively and qualitatively validate the effectiveness and feasibility of our method with videos in various scene conditions. The experimental results show that our method adapts itself to different situations and outperforms the state-of-the-art methods, indicating the advantages of optical flow based methods.



### Computer Analysis of Architecture Using Automatic Image Understanding
- **Arxiv ID**: http://arxiv.org/abs/1807.04892v4
- **DOI**: 10.46298/jdmdh.4683
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04892v4)
- **Published**: 2018-07-13 02:25:28+00:00
- **Updated**: 2020-07-09 16:31:32+00:00
- **Authors**: Fan Wei, Yuan Li, Lior Shamir
- **Comment**: None
- **Journal**: Journal of Data Mining & Digital Humanities, 2018 (January 22,
  2019) jdmdh:4683
- **Summary**: In the past few years, computer vision and pattern recognition systems have been becoming increasingly more powerful, expanding the range of automatic tasks enabled by machine vision. Here we show that computer analysis of building images can perform quantitative analysis of architecture, and quantify similarities between city architectural styles in a quantitative fashion. Images of buildings from 18 cities and three countries were acquired using Google StreetView, and were used to train a machine vision system to automatically identify the location of the imaged building based on the image visual content. Experimental results show that the automatic computer analysis can automatically identify the geographical location of the StreetView image. More importantly, the algorithm was able to group the cities and countries and provide a phylogeny of the similarities between architectural styles as captured by StreetView images. These results demonstrate that computer vision and pattern recognition algorithms can perform the complex cognitive task of analyzing images of buildings, and can be used to measure and quantify visual similarities and differences between different styles of architectures. This experiment provides a new paradigm for studying architecture, based on a quantitative approach that can enhance the traditional manual observation and analysis. The source code used for the analysis is open and publicly available.



### Automatic segmentation of skin lesions using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1807.04893v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04893v1)
- **Published**: 2018-07-13 02:34:24+00:00
- **Updated**: 2018-07-13 02:34:24+00:00
- **Authors**: Joshua Peter Ebenezer, Jagath C. Rajapakse
- **Comment**: 4 pages, 1 figure, extended abstract of submission to ISIC 2018 skin
  analysis challenge
- **Journal**: None
- **Summary**: This paper summarizes the method used in our submission to Task 1 of the International Skin Imaging Collaboration's (ISIC) Skin Lesion Analysis Towards Melanoma Detection challenge held in 2018. We used a fully automated method to accurately segment lesion boundaries from dermoscopic images. A U-net deep learning network is trained on publicly available data from ISIC. We introduce the use of intensity, color, and texture enhancement operations as pre-processing steps and morphological operations and contour identification as post-processing steps.



### TS2C: Tight Box Mining with Surrounding Segmentation Context for Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.04897v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04897v1)
- **Published**: 2018-07-13 03:38:49+00:00
- **Updated**: 2018-07-13 03:38:49+00:00
- **Authors**: Yunchao Wei, Zhiqiang Shen, Bowen Cheng, Honghui Shi, Jinjun Xiong, Jiashi Feng, Thomas Huang
- **Comment**: ECCV2018
- **Journal**: None
- **Summary**: This work provides a simple approach to discover tight object bounding boxes with only image-level supervision, called Tight box mining with Surrounding Segmentation Context (TS2C). We observe that object candidates mined through current multiple instance learning methods are usually trapped to discriminative object parts, rather than the entire object. TS2C leverages surrounding segmentation context derived from weakly-supervised segmentation to suppress such low-quality distracting candidates and boost the high-quality ones. Specifically, TS2C is developed based on two key properties of desirable bounding boxes: 1) high purity, meaning most pixels in the box are with high object response, and 2) high completeness, meaning the box covers high object response pixels comprehensively. With such novel and computable criteria, more tight candidates can be discovered for learning a better object detector. With TS2C, we obtain 48.0% and 44.4% mAP scores on VOC 2007 and 2012 benchmarks, which are the new state-of-the-arts.



### Analysis Dictionary Learning based Classification: Structure for Robustness
- **Arxiv ID**: http://arxiv.org/abs/1807.04899v2
- **DOI**: 10.1109/TIP.2019.2919409
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04899v2)
- **Published**: 2018-07-13 03:47:38+00:00
- **Updated**: 2019-06-27 02:01:40+00:00
- **Authors**: Wen Tang, Ashkan Panahi, Hamid Krim, Liyi Dai
- **Comment**: This manuscript has been accepted and published to IEEE Transactions
  on Image Processing on June 2019
- **Journal**: None
- **Summary**: A discriminative structured analysis dictionary is proposed for the classification task. A structure of the union of subspaces (UoS) is integrated into the conventional analysis dictionary learning to enhance the capability of discrimination. A simple classifier is also simultaneously included into the formulated functional to ensure a more complete consistent classification. The solution of the algorithm is efficiently obtained by the linearized alternating direction method of multipliers. Moreover, a distributed structured analysis dictionary learning is also presented to address large scale datasets. It can group-(class-) independently train the structured analysis dictionaries by different machines/cores/threads, and therefore avoid a high computational cost. A consensus structured analysis dictionary and a global classifier are jointly learned in the distributed approach to safeguard the discriminative power and the efficiency of classification. Experiments demonstrate that our method achieves a comparable or better performance than the state-of-the-art algorithms in a variety of visual classification tasks. In addition, the training and testing computational complexity are also greatly reduced.



### Performance of Image Registration Tools on High-Resolution 3D Brain Images
- **Arxiv ID**: http://arxiv.org/abs/1807.04917v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04917v1)
- **Published**: 2018-07-13 05:13:50+00:00
- **Updated**: 2018-07-13 05:13:50+00:00
- **Authors**: Abdullah Nazib, James Galloway, Clinton Fookes, Dimitri Perrin
- **Comment**: None
- **Journal**: None
- **Summary**: Recent progress in tissue clearing has allowed for the imaging of entire organs at single-cell resolution. These methods produce very large 3D images (several gigabytes for a whole mouse brain). A necessary step in analysing these images is registration across samples. Existing methods of registration were developed for lower resolution image modalities (e.g. MRI) and it is unclear whether their performance and accuracy is satisfactory at this larger scale. In this study, we used data from different mouse brains cleared with the CUBIC protocol to evaluate five freely available image registration tools. We used several performance metrics to assess accuracy, and completion time as a measure of efficiency. The results of this evaluation suggest that the ANTS registration tool provides the best registration accuracy while Elastix has the highest computational efficiency among the methods with an acceptable accuracy. The results also highlight the need to develop new registration methods optimised for these high-resolution 3D images.



### TequilaGAN: How to easily identify GAN samples
- **Arxiv ID**: http://arxiv.org/abs/1807.04919v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.04919v1)
- **Published**: 2018-07-13 05:25:54+00:00
- **Updated**: 2018-07-13 05:25:54+00:00
- **Authors**: Rafael Valle, Wilson Cai, Anish Doshi
- **Comment**: 10 pages, 16 figures
- **Journal**: None
- **Summary**: In this paper we show strategies to easily identify fake samples generated with the Generative Adversarial Network framework. One strategy is based on the statistical analysis and comparison of raw pixel values and features extracted from them. The other strategy learns formal specifications from the real data and shows that fake samples violate the specifications of the real data. We show that fake samples produced with GANs have a universal signature that can be used to identify fake samples. We provide results on MNIST, CIFAR10, music and speech data.



### Deep Learning in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1807.04950v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.04950v1)
- **Published**: 2018-07-13 07:22:45+00:00
- **Updated**: 2018-07-13 07:22:45+00:00
- **Authors**: Thilo Stadelmann, Mohammadreza Amirian, Ismail Arabaci, Marek Arnold, Gilbert François Duivesteijn, Ismail Elezi, Melanie Geiger, Stefan Lörwald, Benjamin Bruno Meier, Katharina Rombach, Lukas Tuggener
- **Comment**: Invited paper on ANNPR 2018
- **Journal**: None
- **Summary**: Deep learning with neural networks is applied by an increasing number of people outside of classic research environments, due to the vast success of the methodology on a wide range of machine perception tasks. While this interest is fueled by beautiful success stories, practical work in deep learning on novel tasks without existing baselines remains challenging. This paper explores the specific challenges arising in the realm of real world tasks, based on case studies from research \& development in conjunction with industry, and extracts lessons learned from them. It thus fills a gap between the publication of latest algorithmic and methodical developments, and the usually omitted nitty-gritty of how to make them work. Specifically, we give insight into deep learning projects on face matching, print media monitoring, industrial quality control, music scanning, strategy game playing, and automated machine learning, thereby providing best practices for deep learning in practice.



### Single Bitmap Block Truncation Coding of Color Images Using Hill Climbing Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1807.04960v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04960v1)
- **Published**: 2018-07-13 07:57:22+00:00
- **Updated**: 2018-07-13 07:57:22+00:00
- **Authors**: Lige Zhang, Xiaolin Qin, Qing Li, Haoyue Peng, Yu Hou
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, the use of digital images in various fields is increasing rapidly. To increase the number of images stored and get faster transmission of them, it is necessary to reduce the size of these images. Single bitmap block truncation coding (SBBTC) schemes are compression techniques, which are used to generate a common bitmap to quantize the R, G and B planes in color image. As one of the traditional SBBTC schemes, weighted plane (W-plane) method is famous for its simplicity and low time consumption. However, the W-plane method also has poor performance in visual quality. This paper proposes an improved SBBTC scheme based on W-plane method using parallel computing and hill climbing algorithm. Compared with various schemes, the simulation results of the proposed scheme are better than that of the reference schemes in visual quality and time consumption.



### Recognition in Terra Incognita
- **Arxiv ID**: http://arxiv.org/abs/1807.04975v2
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.PE
- **Links**: [PDF](http://arxiv.org/pdf/1807.04975v2)
- **Published**: 2018-07-13 08:47:59+00:00
- **Updated**: 2018-07-25 01:59:00+00:00
- **Authors**: Sara Beery, Grant van Horn, Pietro Perona
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: It is desirable for detection and classification algorithms to generalize to unfamiliar environments, but suitable benchmarks for quantitatively studying this phenomenon are not yet available. We present a dataset designed to measure recognition generalization to novel environments. The images in our dataset are harvested from twenty camera traps deployed to monitor animal populations. Camera traps are fixed at one location, hence the background changes little across images; capture is triggered automatically, hence there is no human bias. The challenge is learning recognition in a handful of locations, and generalizing animal detection and classification to new locations where no training data is available. In our experiments state-of-the-art algorithms show excellent performance when tested at the same location where they were trained. However, we find that generalization to new locations is poor, especially for classification systems.



### Zoom-Net: Mining Deep Feature Interactions for Visual Relationship Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.04979v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.04979v1)
- **Published**: 2018-07-13 09:20:39+00:00
- **Updated**: 2018-07-13 09:20:39+00:00
- **Authors**: Guojun Yin, Lu Sheng, Bin Liu, Nenghai Yu, Xiaogang Wang, Jing Shao, Chen Change Loy
- **Comment**: 22 pages, 9 figures, accepted by ECCV 2018, the source code will be
  released on https://github.com/gjyin91/ZoomNet
- **Journal**: None
- **Summary**: Recognizing visual relationships <subject-predicate-object> among any pair of localized objects is pivotal for image understanding. Previous studies have shown remarkable progress in exploiting linguistic priors or external textual information to improve the performance. In this work, we investigate an orthogonal perspective based on feature interactions. We show that by encouraging deep message propagation and interactions between local object features and global predicate features, one can achieve compelling performance in recognizing complex relationships without using any linguistic priors. To this end, we present two new pooling cells to encourage feature interactions: (i) Contrastive ROI Pooling Cell, which has a unique deROI pooling that inversely pools local object features to the corresponding area of global predicate features. (ii) Pyramid ROI Pooling Cell, which broadcasts global predicate features to reinforce local object features.The two cells constitute a Spatiality-Context-Appearance Module (SCA-M), which can be further stacked consecutively to form our final Zoom-Net.We further shed light on how one could resolve ambiguous and noisy object and predicate annotations by Intra-Hierarchical trees (IH-tree). Extensive experiments conducted on Visual Genome dataset demonstrate the effectiveness of our feature-oriented approach compared to state-of-the-art methods (Acc@1 11.42% from 8.16%) that depend on explicit modeling of linguistic interactions. We further show that SCA-M can be incorporated seamlessly into existing approaches to improve the performance by a large margin. The source code will be released on https://github.com/gjyin91/ZoomNet.



### A comprehensive study of sparse representation techniques for offline signature verification
- **Arxiv ID**: http://arxiv.org/abs/1807.05039v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05039v4)
- **Published**: 2018-07-13 12:38:54+00:00
- **Updated**: 2019-01-23 14:28:43+00:00
- **Authors**: Elias N. Zois, Dimitrios Tsourounis, Ilias Theodorakopoulos, Anastasios Kesidis, George Economou
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In this work, a feature extraction method for offline signature verification is presented that harnesses the power of sparse representation in order to deliver state-of-the-art verification performance in several signature datasets like CEDAR, MCYT-75, GPDS and UTSIG. Beyond the accuracy improvements, several major parameters associated with sparse representation; such as selected configuration, dictionary size, sparsity level and positivity priors are investigated. Besides, it is evinced that 2nd order statistics of the sparse codes is a powerful pooling function for the formation of the global signature descriptor. Also, a thorough evaluation of the effects of preprocessing is introduced by an automated algorithm in order to select the optimum thinning level. Finally, a segmentation strategy which employs a special form of spatial pyramid tailored to the problem of sparse representation is presented along with the enhancing of the produced descriptor on meaningful areas of the signature as emerged from the BRISK key-point detection mechanism. The obtained state-of-the-art results on the most challenging signature datasets provide a strong indication towards the benefits of learned features, even in writer dependent (WD) scenarios with a unique model for each writer and only a few available reference samples of him/her.



### CascadeCNN: Pushing the Performance Limits of Quantisation in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.05053v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.05053v1)
- **Published**: 2018-07-13 13:21:18+00:00
- **Updated**: 2018-07-13 13:21:18+00:00
- **Authors**: Alexandros Kouris, Stylianos I. Venieris, Christos-Savvas Bouganis
- **Comment**: Accepted for publication at the 28th International Conference on
  Field Programmable Logic & Applications (FPL), 2018
- **Journal**: None
- **Summary**: This work presents CascadeCNN, an automated toolflow that pushes the quantisation limits of any given CNN model, aiming to perform high-throughput inference. A two-stage architecture tailored for any given CNN-FPGA pair is generated, consisting of a low- and high-precision unit in a cascade. A confidence evaluation unit is employed to identify misclassified cases from the excessively low-precision unit and forward them to the high-precision unit for re-processing. Experiments demonstrate that the proposed toolflow can achieve a performance boost up to 55% for VGG-16 and 48% for AlexNet over the baseline design for the same resource budget and accuracy, without the need of retraining the model or accessing the training data.



### Newton-Krylov PDE-constrained LDDMM in the space of band-limited vector fields
- **Arxiv ID**: http://arxiv.org/abs/1807.05117v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05117v1)
- **Published**: 2018-07-13 14:59:01+00:00
- **Updated**: 2018-07-13 14:59:01+00:00
- **Authors**: Monica Hernandez
- **Comment**: None
- **Journal**: None
- **Summary**: PDE-constrained Large Deformation Diffeomorphic Metric Mapping is a particularly interesting framework of physically meaningful diffeomorphic registration methods. Newton-Krylov optimization has shown an excellent numerical accuracy and an extraordinarily fast convergence rate in this framework. However, the most significant limitation of PDE-constrained LDDMM is the huge computational complexity, that hinders the extensive use in Computational Anatomy applications. In this work, we propose two PDE-constrained LDDMM methods parameterized in the space of band-limited vector fields and we evaluate their performance with respect to the most related state of the art methods. The parameterization in the space of band-limited vector fields dramatically alleviates the computational burden avoiding the computation of the high-frequency components of the velocity fields that would be suppressed by the action of the low-pass filters involved in the computation of the gradient and the Hessian-vector products. Besides, the proposed methods have shown an improved accuracy with respect to the benchmark methods.



### Learning-based Natural Geometric Matching with Homography Prior
- **Arxiv ID**: http://arxiv.org/abs/1807.05119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05119v1)
- **Published**: 2018-07-13 15:01:02+00:00
- **Updated**: 2018-07-13 15:01:02+00:00
- **Authors**: Yifang Xu, Tianli Liao, Jing Chen
- **Comment**: 13 pages,4 figures
- **Journal**: None
- **Summary**: Geometric matching is a key step in computer vision tasks. Previous learning-based methods for geometric matching concentrate more on improving alignment quality, while we argue the importance of naturalness issue simultaneously. To deal with this, firstly, Pearson correlation is applied to handle large intra-class variations of features in feature matching stage. Then, we parametrize homography transformation with 9 parameters in full connected layer of our network, to better characterize large viewpoint variations compared with affine transformation. Furthermore, a novel loss function with Gaussian weights guarantees the model accuracy and efficiency in training procedure. Finally, we provide two choices for different purposes in geometric matching. When compositing homography with affine transformation, the alignment accuracy improves and all lines are preserved, which results in a more natural transformed image. When compositing homography with non-rigid thin-plate-spline transformation, the alignment accuracy further improves. Experimental results on Proposal Flow dataset show that our method outperforms state-of-the-art methods, both in terms of alignment accuracy and naturalness.



### Multi-Scale Convolutional-Stack Aggregation for Robust White Matter Hyperintensities Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.05153v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05153v3)
- **Published**: 2018-07-13 15:56:20+00:00
- **Updated**: 2019-02-27 14:57:19+00:00
- **Authors**: Hongwei Li, Jianguo Zhang, Mark Muehlau, Jan Kirschke, Bjoern Menze
- **Comment**: accepted by MICCAI brain lesion workshop
- **Journal**: None
- **Summary**: Segmentation of both large and small white matter hyperintensities/lesions in brain MR images is a challenging task which has drawn much attention in recent years. We propose a multi-scale aggregation model framework to deal with volume-varied lesions. Firstly, we present a specifically-designed network for small lesion segmentation called Stack-Net, in which multiple convolutional layers are connected, aiming to preserve rich local spatial information of small lesions before the sub-sampling layer. Secondly, we aggregate multi-scale Stack-Nets with different receptive fields to learn multi-scale contextual information of both large and small lesions. Our model is evaluated on recent MICCAI WMH Challenge Dataset and outperforms the state-of-the-art on lesion recall and lesion F1-score under 5-fold cross validation. In addition, we further test our pre-trained models on a Multiple Sclerosis lesion dataset with 30 subjects under cross-center evaluation. Results show that the aggregation model is effective in learning multi-scale spatial information.It claimed the first place on the hidden test set after independent evaluation by the challenge organizer. In addition, we further test our pre-trained models on a Multiple Sclerosis lesion dataset with 30 subjects under cross-center evaluation. Results show that the aggregation model is effective in learning multi-scale spatial information.



### Large-Scale Visual Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.05162v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.05162v3)
- **Published**: 2018-07-13 16:21:34+00:00
- **Updated**: 2018-10-01 11:23:03+00:00
- **Authors**: Brendan Shillingford, Yannis Assael, Matthew W. Hoffman, Thomas Paine, Cían Hughes, Utsav Prabhu, Hank Liao, Hasim Sak, Kanishka Rao, Lorrayne Bennett, Marie Mulville, Ben Coppin, Ben Laurie, Andrew Senior, Nando de Freitas
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents a scalable solution to open-vocabulary visual speech recognition. To achieve this, we constructed the largest existing visual speech recognition dataset, consisting of pairs of text and video clips of faces speaking (3,886 hours of video). In tandem, we designed and trained an integrated lipreading system, consisting of a video processing pipeline that maps raw video to stable videos of lips and sequences of phonemes, a scalable deep neural network that maps the lip videos to sequences of phoneme distributions, and a production-level speech decoder that outputs sequences of words. The proposed system achieves a word error rate (WER) of 40.9% as measured on a held-out set. In comparison, professional lipreaders achieve either 86.4% or 92.9% WER on the same dataset when having access to additional types of contextual information. Our approach significantly improves on other lipreading approaches, including variants of LipNet and of Watch, Attend, and Spell (WAS), which are only capable of 89.8% and 76.8% WER respectively.



### Image Classification for Arabic: Assessing the Accuracy of Direct English to Arabic Translations
- **Arxiv ID**: http://arxiv.org/abs/1807.05206v2
- **DOI**: 10.1109/ACCESS.2019.2926924
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1807.05206v2)
- **Published**: 2018-07-13 17:44:20+00:00
- **Updated**: 2019-07-07 11:38:21+00:00
- **Authors**: Abdulkareem Alsudais
- **Comment**: None
- **Journal**: IEEE Access 2019
- **Summary**: Image classification is an ongoing research challenge. Most of the available research focuses on image classification for the English language, however there is very little research on image classification for the Arabic language. Expanding image classification to Arabic has several applications. The present study investigated a method for generating Arabic labels for images of objects. The method used in this study involved a direct English to Arabic translation of the labels that are currently available on ImageNet, a database commonly used in image classification research. The purpose of this study was to test the accuracy of this method. In this study, 2,887 labeled images were randomly selected from ImageNet. All of the labels were translated from English to Arabic using Google Translate. The accuracy of the translations was evaluated. Results indicated that that 65.6% of the Arabic labels were accurate. This study makes three important contributions to the image classification literature: (1) it determined the baseline level of accuracy for algorithms that provide Arabic labels for images, (2) it provided 1,895 images that are tagged with accurate Arabic labels, and (3) provided the accuracy of translations of image labels from English to Arabic.



### Performance of Humans in Iris Recognition: The Impact of Iris Condition and Annotation-driven Verification
- **Arxiv ID**: http://arxiv.org/abs/1807.05245v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05245v2)
- **Published**: 2018-07-13 18:36:54+00:00
- **Updated**: 2018-11-20 16:50:34+00:00
- **Authors**: Daniel Moreira, Mateusz Trokielewicz, Adam Czajka, Kevin W. Bowyer, Patrick J. Flynn
- **Comment**: Paper accepted for WACV 2019, Hawaii, USA
- **Journal**: None
- **Summary**: This paper advances the state of the art in human examination of iris images by (1) assessing the impact of different iris conditions in identity verification, and (2) introducing an annotation step that improves the accuracy of people's decisions. In a first experimental session, 114 subjects were asked to decide if pairs of iris images depict the same eye (genuine pairs) or two distinct eyes (impostor pairs). The image pairs sampled six conditions: (1) easy for algorithms to classify, (2) difficult for algorithms to classify, (3) large difference in pupil dilation, (4) disease-affected eyes, (5) identical twins, and (6) post-mortem samples. In a second session, 85 of the 114 subjects were asked to annotate matching and non-matching regions that supported their decisions. Subjects were allowed to change their initial classification as a result of the annotation process. Results suggest that: (a) people improve their identity verification accuracy when asked to annotate matching and non-matching regions between the pair of images, (b) images depicting the same eye with large difference in pupil dilation were the most challenging to subjects, but benefited well from the annotation-driven classification, (c) humans performed better than iris recognition algorithms when verifying genuine pairs of post-mortem and disease-affected eyes (i.e., samples showing deformations that go beyond the distortions of a healthy iris due to pupil dilation), and (d) annotation does not improve accuracy of analyzing images from identical twins, which remain confusing for people.



### Domain-Specific Human-Inspired Binarized Statistical Image Features for Iris Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.05248v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05248v2)
- **Published**: 2018-07-13 18:57:18+00:00
- **Updated**: 2018-11-18 03:48:18+00:00
- **Authors**: Adam Czajka, Daniel Moreira, Kevin W. Bowyer, Patrick J. Flynn
- **Comment**: Paper accepted for WACV 2019, Hawaii, USA
- **Journal**: None
- **Summary**: Binarized statistical image features (BSIF) have been successfully used for texture analysis in many computer vision tasks, including iris recognition and biometric presentation attack detection. One important point is that all applications of BSIF in iris recognition have used the original BSIF filters, which were trained on image patches extracted from natural images. This paper tests the question of whether domain-specific BSIF can give better performance than the default BSIF. The second important point is in the selection of image patches to use in training for BSIF. Can image patches derived from eye-tracking experiments, in which humans perform an iris recognition task, give better performance than random patches? Our results say that (1) domain-specific BSIF features can out-perform the default BSIF features, and (2) selecting image patches in a task-specific manner guided by human performance can out-perform selecting random patches. These results are important because BSIF is often regarded as a generic texture tool that does not need any domain adaptation, and human-task-guided selection of patches for training has never (to our knowledge) been done. This paper follows the reproducible research requirements, and the new iris-domain-specific BSIF filters, the patches used in filter training, the database used in testing and the source codes of the designed iris recognition method are made available along with this paper to facilitate applications of this concept.



### Survey on Deep Learning Techniques for Person Re-Identification Task
- **Arxiv ID**: http://arxiv.org/abs/1807.05284v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05284v3)
- **Published**: 2018-07-13 21:18:54+00:00
- **Updated**: 2018-07-19 20:58:50+00:00
- **Authors**: Bahram Lavi, Mehdi Fatan Serj, Ihsan Ullah
- **Comment**: None
- **Journal**: None
- **Summary**: Intelligent video-surveillance is currently an active research field in computer vision and machine learning techniques. It provides useful tools for surveillance operators and forensic video investigators. Person re-identification (PReID) is one among these tools. It consists of recognizing whether an individual has already been observed over a camera in a network or not. This tool can also be employed in various possible applications such as off-line retrieval of all the video-sequences showing an individual of interest whose image is given a query, and online pedestrian tracking over multiple camera views. To this aim, many techniques have been proposed to increase the performance of PReID. Among the systems, many researchers utilized deep neural networks (DNNs) because of their better performance and fast execution at test time. Our objective is to provide for future researchers the work being done on PReID to date. Therefore, we summarized state-of-the-art DNN models being used for this task. A brief description of each model along with their evaluation on a set of benchmark datasets is given. Finally, a detailed comparison is provided among these models followed by some limitations that can work as guidelines for future research.



