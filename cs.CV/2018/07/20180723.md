# Arxiv Papers in cs.CV on 2018-07-23
### Improving Automatic Skin Lesion Segmentation using Adversarial Learning based Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.08392v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08392v2)
- **Published**: 2018-07-23 00:42:25+00:00
- **Updated**: 2018-07-31 01:30:04+00:00
- **Authors**: Lei Bi, Dagan Feng, Jinman Kim
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Segmentation of skin lesions is considered as an important step in computer aided diagnosis (CAD) for automated melanoma diagnosis. In recent years, segmentation methods based on fully convolutional networks (FCN) have achieved great success in general images. This success is primarily due to the leveraging of large labelled datasets to learn features that correspond to the shallow appearance as well as the deep semantics of the images. However, the dependence on large dataset does not translate well into medical images. To improve the FCN performance for skin lesion segmentations, researchers attempted to use specific cost functions or add post-processing algorithms to refine the coarse boundaries of the FCN results. However, the performance of these methods is heavily reliant on the tuning of many parameters and post-processing techniques. In this paper, we leverage the state-of-the-art image feature learning method of generative adversarial network (GAN) for its inherent ability to produce consistent and realistic image features by using deep neural networks and adversarial learning concept. We improve upon GAN such that skin lesion features can be learned at different level of complexities, in a controlled manner. The outputs from our method is then augmented to the existing FCN training data, thus increasing the overall feature diversity. We evaluated our method on the ISIC 2018 skin lesion segmentation challenge dataset and showed that it was more accurate and robust when compared to the existing skin lesion segmentation methods.



### Visual Mesh: Real-time Object Detection Using Constant Sample Density
- **Arxiv ID**: http://arxiv.org/abs/1807.08405v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CG, cs.LG, cs.RO, 68T45, 68T40, I.2.10; I.2.6; I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/1807.08405v1)
- **Published**: 2018-07-23 02:21:31+00:00
- **Updated**: 2018-07-23 02:21:31+00:00
- **Authors**: Trent Houliston, Stephan K. Chalup
- **Comment**: 12 pages, 6 figures, RoboCup International Symposium 2018
- **Journal**: None
- **Summary**: This paper proposes an enhancement of convolutional neural networks for object detection in resource-constrained robotics through a geometric input transformation called Visual Mesh. It uses object geometry to create a graph in vision space, reducing computational complexity by normalizing the pixel and feature density of objects. The experiments compare the Visual Mesh with several other fast convolutional neural networks. The results demonstrate execution times sixteen times quicker than the fastest competitor tested, while achieving outstanding accuracy.



### Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd
- **Arxiv ID**: http://arxiv.org/abs/1807.08407v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08407v1)
- **Published**: 2018-07-23 02:36:03+00:00
- **Updated**: 2018-07-23 02:36:03+00:00
- **Authors**: Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, Stan Z. Li
- **Comment**: Accepted by ECCV 2018
- **Journal**: None
- **Summary**: Pedestrian detection in crowded scenes is a challenging problem since the pedestrians often gather together and occlude each other. In this paper, we propose a new occlusion-aware R-CNN (OR-CNN) to improve the detection accuracy in the crowd. Specifically, we design a new aggregation loss to enforce proposals to be close and locate compactly to the corresponding objects. Meanwhile, we use a new part occlusion-aware region of interest (PORoI) pooling unit to replace the RoI pooling layer in order to integrate the prior structure information of human body with visibility prediction into the network to handle occlusion. Our detector is trained in an end-to-end fashion, which achieves state-of-the-art results on three pedestrian detection datasets, i.e., CityPersons, ETH, and INRIA, and performs on-pair with the state-of-the-arts on Caltech.



### Actor-Action Semantic Segmentation with Region Masks
- **Arxiv ID**: http://arxiv.org/abs/1807.08430v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1807.08430v1)
- **Published**: 2018-07-23 05:11:23+00:00
- **Updated**: 2018-07-23 05:11:23+00:00
- **Authors**: Kang Dang, Chunluan Zhou, Zhigang Tu, Michael Hoy, Justin Dauwels, Junsong Yuan
- **Comment**: Accepted by BMVC 2018
- **Journal**: None
- **Summary**: In this paper, we study the actor-action semantic segmentation problem, which requires joint labeling of both actor and action categories in video frames. One major challenge for this task is that when an actor performs an action, different body parts of the actor provide different types of cues for the action category and may receive inconsistent action labeling when they are labeled independently. To address this issue, we propose an end-to-end region-based actor-action segmentation approach which relies on region masks from an instance segmentation algorithm. Our main novelty is to avoid labeling pixels in a region mask independently - instead we assign a single action label to these pixels to achieve consistent action labeling. When a pixel belongs to multiple region masks, max pooling is applied to resolve labeling conflicts. Our approach uses a two-stream network as the front-end (which learns features capturing both appearance and motion information), and uses two region-based segmentation networks as the back-end (which takes the fused features from the two-stream network as the input and predicts actor-action labeling). Experiments on the A2D dataset demonstrate that both the region-based segmentation strategy and the fused features from the two-stream network contribute to the performance improvements. The proposed approach outperforms the state-of-the-art results by more than 8% in mean class accuracy, and more than 5% in mean class IOU, which validates its effectiveness.



### Question Relevance in Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1807.08435v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1807.08435v1)
- **Published**: 2018-07-23 06:01:44+00:00
- **Updated**: 2018-07-23 06:01:44+00:00
- **Authors**: Prakruthi Prabhakar, Nitish Kulkarni, Linghao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Free-form and open-ended Visual Question Answering systems solve the problem of providing an accurate natural language answer to a question pertaining to an image. Current VQA systems do not evaluate if the posed question is relevant to the input image and hence provide nonsensical answers when posed with irrelevant questions to an image. In this paper, we solve the problem of identifying the relevance of the posed question to an image. We address the problem as two sub-problems. We first identify if the question is visual or not. If the question is visual, we then determine if it's relevant to the image or not. For the second problem, we generate a large dataset from existing visual question answering datasets in order to enable the training of complex architectures and model the relevance of a visual question to an image. We also compare the results of our Long Short-Term Memory Recurrent Neural Network based models to Logistic Regression, XGBoost and multi-layer perceptron based approaches to the problem.



### Zero-shot keyword spotting for visual speech recognition in-the-wild
- **Arxiv ID**: http://arxiv.org/abs/1807.08469v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08469v2)
- **Published**: 2018-07-23 08:06:08+00:00
- **Updated**: 2018-07-26 03:41:31+00:00
- **Authors**: Themos Stafylakis, Georgios Tzimiropoulos
- **Comment**: Accepted at ECCV-2018
- **Journal**: None
- **Summary**: Visual keyword spotting (KWS) is the problem of estimating whether a text query occurs in a given recording using only video information. This paper focuses on visual KWS for words unseen during training, a real-world, practical setting which so far has received no attention by the community. To this end, we devise an end-to-end architecture comprising (a) a state-of-the-art visual feature extractor based on spatiotemporal Residual Networks, (b) a grapheme-to-phoneme model based on sequence-to-sequence neural networks, and (c) a stack of recurrent neural networks which learn how to correlate visual features with the keyword representation. Different to prior works on KWS, which try to learn word representations merely from sequences of graphemes (i.e. letters), we propose the use of a grapheme-to-phoneme encoder-decoder model which learns how to map words to their pronunciation. We demonstrate that our system obtains very promising visual-only KWS results on the challenging LRS2 database, for keywords unseen during training. We also show that our system outperforms a baseline which addresses KWS via automatic speech recognition (ASR), while it drastically improves over other recently proposed ASR-free KWS methods.



### Deep attention-guided fusion network for lesion segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.08471v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08471v2)
- **Published**: 2018-07-23 08:14:36+00:00
- **Updated**: 2018-07-25 09:31:46+00:00
- **Authors**: Hengliang Zhu, Yangyang Hao, Lizhuang Ma, Ruixing Li, Hua Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We participated the Task 1: Lesion Segmentation. The paper describes our algorithm and the final result of validation set for the ISIC Challenge 2018 - Skin Lesion Analysis Towards Melanoma Detection.



### DASN:Data-Aware Skilled Network for Accurate MR Brain Tissue Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.08473v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08473v2)
- **Published**: 2018-07-23 08:19:19+00:00
- **Updated**: 2018-07-24 06:35:05+00:00
- **Authors**: Yang Deng, Yao Sun, Yongpei Zhu, Shuo Zhang, Mingwang Zhu, Kehong Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate segmentation of MR brain tissue is a crucial step for diagnosis, surgical planning, and treatment of brain abnormalities. Automatic and reliable segmenta-tion methods are required to assist doctor. Over the last few years, deep learning especially deep convolutional neural networks (CNNs) have emerged as one of the most prominent approaches for image recognition problems in various do-mains. But the improvement of deep networks always needs inspiration, which is rare for the ordinary. Until now,there have been reasonable MR brain tissue segmentation methods,all of which can achieve promising performance. These different methods have their own characteristic and are distinctive for data sets. In other words, different models performance vary widely on the same data sets and each model has what it is skilled in. It is on the basis of this, we propose a judgement to distinguish data sets that different models are good at. With our method, the segmentation accuracy can be improved easily based on the existing models, neither without increasing training data nor improving the network. We validate our method on the widely used IBSR 18 dataset and obtain average dice ratio of 88.06%,while it is 85.82% and 86.92% when only using separate one model respectively.



### Human peripheral blur is optimal for object recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.08476v3
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.08476v3)
- **Published**: 2018-07-23 08:25:35+00:00
- **Updated**: 2020-05-13 21:06:08+00:00
- **Authors**: R. T. Pramod, Harish Katti, S. P. Arun
- **Comment**: 24 pages, 6 figures, 1 table
- **Journal**: None
- **Summary**: Our vision is sharpest at the center of our gaze and becomes progressively blurry into the periphery. It is widely believed that this high foveal resolution evolved at the expense of peripheral acuity. But what if this sampling scheme is actually optimal for object recognition? To test this hypothesis, we trained deep neural networks on 'foveated' images with high resolution near objects and increasingly sparse sampling into the periphery. Neural networks trained using a blur profile matching the human eye yielded the best performance compared to shallower and steeper blur profiles. Even in humans, categorization accuracy deteriorated only for steeper blur profiles. Thus, our blurry peripheral vision may have evolved to optimize object recognition rather than merely due to wiring constraints.



### Domain Generalization via Conditional Invariant Representation
- **Arxiv ID**: http://arxiv.org/abs/1807.08479v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.08479v1)
- **Published**: 2018-07-23 08:33:46+00:00
- **Updated**: 2018-07-23 08:33:46+00:00
- **Authors**: Ya Li, Mingming Gong, Xinmei Tian, Tongliang Liu, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Domain generalization aims to apply knowledge gained from multiple labeled source domains to unseen target domains. The main difficulty comes from the dataset bias: training data and test data have different distributions, and the training set contains heterogeneous samples from different distributions. Let $X$ denote the features, and $Y$ be the class labels. Existing domain generalization methods address the dataset bias problem by learning a domain-invariant representation $h(X)$ that has the same marginal distribution $\mathbb{P}(h(X))$ across multiple source domains. The functional relationship encoded in $\mathbb{P}(Y|X)$ is usually assumed to be stable across domains such that $\mathbb{P}(Y|h(X))$ is also invariant. However, it is unclear whether this assumption holds in practical problems. In this paper, we consider the general situation where both $\mathbb{P}(X)$ and $\mathbb{P}(Y|X)$ can change across all domains. We propose to learn a feature representation which has domain-invariant class conditional distributions $\mathbb{P}(h(X)|Y)$. With the conditional invariant representation, the invariance of the joint distribution $\mathbb{P}(h(X),Y)$ can be guaranteed if the class prior $\mathbb{P}(Y)$ does not change across training and test domains. Extensive experiments on both synthetic and real data demonstrate the effectiveness of the proposed method.



### Learning 3D Shapes as Multi-Layered Height-maps using 2D Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.08485v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08485v2)
- **Published**: 2018-07-23 08:58:39+00:00
- **Updated**: 2018-07-26 11:40:12+00:00
- **Authors**: Kripasindhu Sarkar, Basavaraj Hampiholi, Kiran Varanasi, Didier Stricker
- **Comment**: ECCV 2018 camera ready
- **Journal**: None
- **Summary**: We present a novel global representation of 3D shapes, suitable for the application of 2D CNNs. We represent 3D shapes as multi-layered height-maps (MLH) where at each grid location, we store multiple instances of height maps, thereby representing 3D shape detail that is hidden behind several layers of occlusion. We provide a novel view merging method for combining view dependent information (Eg. MLH descriptors) from multiple views. Because of the ability of using 2D CNNs, our method is highly memory efficient in terms of input resolution compared to the voxel based input. Together with MLH descriptors and our multi view merging, we achieve the state-of-the-art result in classification on ModelNet dataset.



### A Multi-Level Deep Ensemble Model for Skin Lesion Classification in Dermoscopy Images
- **Arxiv ID**: http://arxiv.org/abs/1807.08488v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08488v1)
- **Published**: 2018-07-23 09:02:22+00:00
- **Updated**: 2018-07-23 09:02:22+00:00
- **Authors**: Yutong Xie, Jianpeng Zhang, Yong Xia
- **Comment**: 4 pages, 2 figures, ISIC2018
- **Journal**: None
- **Summary**: A multi-level deep ensemble (MLDE) model that can be trained in an 'end to end' manner is proposed for skin lesion classification in dermoscopy images. In this model, four pre-trained ResNet-50 networks are used to characterize the multiscale information of skin lesions and are combined by using an adaptive weighting scheme that can be learned during the error back propagation. The proposed MLDE model achieved an average AUC value of 86.5% on the ISIC-skin 2018 official validation dataset, which is substantially higher than the average AUC values achieved by each of four ResNet-50 networks.



### Git Loss for Deep Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.08512v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08512v4)
- **Published**: 2018-07-23 10:20:29+00:00
- **Updated**: 2018-07-28 17:29:04+00:00
- **Authors**: Alessandro Calefati, Muhammad Kamran Janjua, Shah Nawaz, Ignazio Gallo
- **Comment**: 12 pages. Accepted at BMVC2018
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have been widely used in computer vision tasks, such as face recognition and verification, and have achieved state-of-the-art results due to their ability to capture discriminative deep features. Conventionally, CNNs have been trained with softmax as supervision signal to penalize the classification loss. In order to further enhance the discriminative capability of deep features, we introduce a joint supervision signal, Git loss, which leverages on softmax and center loss functions. The aim of our loss function is to minimize the intra-class variations as well as maximize the inter-class distances. Such minimization and maximization of deep features are considered ideal for face recognition task. We perform experiments on two popular face recognition benchmarks datasets and show that our proposed loss function achieves maximum separability between deep face features of different identities and achieves state-of-the-art accuracy on two major face recognition benchmark datasets: Labeled Faces in the Wild (LFW) and YouTube Faces (YTF). However, it should be noted that the major objective of Git loss is to achieve maximum separability between deep features of divergent identities.



### A Capsule Network for Traffic Speed Prediction in Complex Road Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.10603v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10603v2)
- **Published**: 2018-07-23 10:40:22+00:00
- **Updated**: 2018-09-21 15:32:01+00:00
- **Authors**: Youngjoo Kim, Peng Wang, Yifei Zhu, Lyudmila Mihaylova
- **Comment**: To be presented in 2018 Sensor Data Fusion: Trends, Solutions,
  Applications (SDF), 10 Oct 2018, in Bonn, Germany
- **Journal**: None
- **Summary**: This paper proposes a deep learning approach for traffic flow prediction in complex road networks. Traffic flow data from induction loop sensors are essentially a time series, which is also spatially related to traffic in different road segments. The spatio-temporal traffic data can be converted into an image where the traffic data are expressed in a 3D space with respect to space and time axes. Although convolutional neural networks (CNNs) have been showing surprising performance in understanding images, they have a major drawback. In the max pooling operation, CNNs are losing important information by locally taking the highest activation values. The inter-relationship in traffic data measured by sparsely located sensors in different time intervals should not be neglected in order to obtain accurate predictions. Thus, we propose a neural network with capsules that replaces max pooling by dynamic routing. This is the first approach that employs the capsule network on a time series forecasting problem, to our best knowledge. Moreover, an experiment on real traffic speed data measured in the Santander city of Spain demonstrates the proposed method outperforms the state-of-the-art method based on a CNN by 13.1% in terms of root mean squared error.



### Improving Deep Models of Person Re-identification for Cross-Dataset Usage
- **Arxiv ID**: http://arxiv.org/abs/1807.08526v1
- **DOI**: 10.1007/978-3-319-92007-8_7
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1807.08526v1)
- **Published**: 2018-07-23 10:57:54+00:00
- **Updated**: 2018-07-23 10:57:54+00:00
- **Authors**: Sergey Rodionov, Alexey Potapov, Hugo Latapie, Enzo Fenoglio, Maxim Peterson
- **Comment**: AIAI 2018 (14th International Conference on Artificial Intelligence
  Applications and Innovations) proceeding. The final publication is available
  at link.springer.com
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) is the task of matching humans across cameras with non-overlapping views that has important applications in visual surveillance. Like other computer vision tasks, this task has gained much with the utilization of deep learning methods. However, existing solutions based on deep learning are usually trained and tested on samples taken from same datasets, while in practice one need to deploy Re-ID systems for new sets of cameras for which labeled data is unavailable. Here, we mitigate this problem for one state-of-the-art model, namely, metric embedding trained with the use of the triplet loss function, although our results can be extended to other models. The contribution of our work consists in developing a method of training the model on multiple datasets, and a method for its online practically unsupervised fine-tuning. These methods yield up to 19.1% improvement in Rank-1 score in the cross-dataset evaluation.



### Unsupervised Image-to-Image Translation with Stacked Cycle-Consistent Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.08536v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08536v2)
- **Published**: 2018-07-23 11:15:56+00:00
- **Updated**: 2018-07-28 09:06:34+00:00
- **Authors**: Minjun Li, Haozhi Huang, Lin Ma, Wei Liu, Tong Zhang, Yu-Gang Jiang
- **Comment**: To appear in ECCV 2018
- **Journal**: None
- **Summary**: Recent studies on unsupervised image-to-image translation have made a remarkable progress by training a pair of generative adversarial networks with a cycle-consistent loss. However, such unsupervised methods may generate inferior results when the image resolution is high or the two image domains are of significant appearance differences, such as the translations between semantic layouts and natural images in the Cityscapes dataset. In this paper, we propose novel Stacked Cycle-Consistent Adversarial Networks (SCANs) by decomposing a single translation into multi-stage transformations, which not only boost the image translation quality but also enable higher resolution image-to-image translations in a coarse-to-fine manner. Moreover, to properly exploit the information from the previous stage, an adaptive fusion block is devised to learn a dynamic integration of the current stage's output and the previous stage's output. Experiments on multiple datasets demonstrate that our proposed approach can improve the translation quality compared with previous single-stage unsupervised methods.



### Iterative Interaction Training for Segmentation Editing Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.08555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08555v1)
- **Published**: 2018-07-23 12:13:35+00:00
- **Updated**: 2018-07-23 12:13:35+00:00
- **Authors**: Gustav Bredell, Christine Tanner, Ender Konukoglu
- **Comment**: 8 pages, 4 figures, To appear in the Proceedings of the 21.
  International Conference On Medical Image Computing & Computer Assisted
  Intervention, Machine Learning in Medical Imaging workshop, 16-20 September
  2018, Granada, Spain
- **Journal**: None
- **Summary**: Automatic segmentation has great potential to facilitate morphological measurements while simultaneously increasing efficiency. Nevertheless often users want to edit the segmentation to their own needs and will need different tools for this. There has been methods developed to edit segmentations of automatic methods based on the user input, primarily for binary segmentations. Here however, we present an unique training strategy for convolutional neural networks (CNNs) trained on top of an automatic method to enable interactive segmentation editing that is not limited to binary segmentation. By utilizing a robot-user during training, we closely mimic realistic use cases to achieve optimal editing performance. In addition, we show that an increase of the iterative interactions during the training process up to ten improves the segmentation editing performance substantially. Furthermore, we compare our segmentation editing CNN (interCNN) to state-of-the-art interactive segmentation algorithms and show a superior or on par performance.



### Explainable Neural Computation via Stack Neural Module Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.08556v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08556v3)
- **Published**: 2018-07-23 12:18:18+00:00
- **Updated**: 2019-03-07 03:38:51+00:00
- **Authors**: Ronghang Hu, Jacob Andreas, Trevor Darrell, Kate Saenko
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: In complex inferential tasks like question answering, machine learning models must confront two challenges: the need to implement a compositional reasoning process, and, in many applications, the need for this reasoning process to be interpretable to assist users in both development and prediction. Existing models designed to produce interpretable traces of their decision-making process typically require these traces to be supervised at training time. In this paper, we present a novel neural modular approach that performs compositional reasoning by automatically inducing a desired sub-task decomposition without relying on strong supervision. Our model allows linking different reasoning tasks though shared modules that handle common routines across tasks. Experiments show that the model is more interpretable to human evaluators compared to other state-of-the-art models: users can better understand the model's underlying reasoning procedure and predict when it will succeed or fail based on observing its intermediate outputs.



### MVDepthNet: Real-time Multiview Depth Estimation Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1807.08563v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.08563v1)
- **Published**: 2018-07-23 12:37:13+00:00
- **Updated**: 2018-07-23 12:37:13+00:00
- **Authors**: Kaixuan Wang, Shaojie Shen
- **Comment**: This paper is accepted by 3DV 2018
- **Journal**: None
- **Summary**: Although deep neural networks have been widely applied to computer vision problems, extending them into multiview depth estimation is non-trivial. In this paper, we present MVDepthNet, a convolutional network to solve the depth estimation problem given several image-pose pairs from a localized monocular camera in neighbor viewpoints. Multiview observations are encoded in a cost volume and then combined with the reference image to estimate the depth map using an encoder-decoder network. By encoding the information from multiview observations into the cost volume, our method achieves real-time performance and the flexibility of traditional methods that can be applied regardless of the camera intrinsic parameters and the number of images. Geometric data augmentation is used to train MVDepthNet. We further apply MVDepthNet in a monocular dense mapping system that continuously estimates depth maps using a single localized moving camera. Experiments show that our method can generate depth maps efficiently and precisely.



### Invisible Steganography via Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.08571v3
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.08571v3)
- **Published**: 2018-07-23 12:53:29+00:00
- **Updated**: 2018-10-10 15:02:45+00:00
- **Authors**: Ru Zhang, Shiqi Dong, Jianyi Liu
- **Comment**: 13 pages, 7 figures
- **Journal**: None
- **Summary**: Nowadays, there are plenty of works introducing convolutional neural networks (CNNs) to the steganalysis and exceeding conventional steganalysis algorithms. These works have shown the improving potential of deep learning in information hiding domain. There are also several works based on deep learning to do image steganography, but these works still have problems in capacity, invisibility and security. In this paper, we propose a novel CNN architecture named as \isgan to conceal a secret gray image into a color cover image on the sender side and exactly extract the secret image out on the receiver side. There are three contributions in our work: (i) we improve the invisibility by hiding the secret image only in the Y channel of the cover image; (ii) We introduce the generative adversarial networks to strengthen the security by minimizing the divergence between the empirical probability distributions of stego images and natural images. (iii) In order to associate with the human visual system better, we construct a mixed loss function which is more appropriate for steganography to generate more realistic stego images and reveal out more better secret images. Experiment results show that ISGAN can achieve start-of-art performances on LFW, Pascal VOC2012 and ImageNet datasets.



### Person Search by Multi-Scale Matching
- **Arxiv ID**: http://arxiv.org/abs/1807.08582v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08582v1)
- **Published**: 2018-07-23 13:05:38+00:00
- **Updated**: 2018-07-23 13:05:38+00:00
- **Authors**: Xu Lan, Xiatian Zhu, Shaogang Gong
- **Comment**: to Appear in European Conference on Computer Vision 2018
- **Journal**: None
- **Summary**: We consider the problem of person search in unconstrained scene images. Existing methods usually focus on improving the person detection accuracy to mitigate negative effects imposed by misalignment, mis-detections, and false alarms resulted from noisy people auto-detection. In contrast to previous studies, we show that sufficiently reliable person instance cropping is achievable by slightly improved state-of-the-art deep learning object detectors (e.g. Faster-RCNN), and the under-studied multi-scale matching problem in person search is a more severe barrier. In this work, we address this multi-scale person search challenge by proposing a Cross-Level Semantic Alignment (CLSA) deep learning approach capable of learning more discriminative identity feature representations in a unified end-to-end model. This is realised by exploiting the in-network feature pyramid structure of a deep neural network enhanced by a novel cross pyramid-level semantic alignment loss function. This favourably eliminates the need for constructing a computationally expensive image pyramid and a complex multi-branch network architecture. Extensive experiments show the modelling advantages and performance superiority of CLSA over the state-of-the-art person search and multi-scale matching methods on two large person search benchmarking datasets: CUHK-SYSU and PRW.



### 3D Convolutional Neural Networks for Tumor Segmentation using Long-range 2D Context
- **Arxiv ID**: http://arxiv.org/abs/1807.08599v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.08599v1)
- **Published**: 2018-07-23 13:31:51+00:00
- **Updated**: 2018-07-23 13:31:51+00:00
- **Authors**: Pawel Mlynarski, Hervé Delingette, Antonio Criminisi, Nicholas Ayache
- **Comment**: Submitted to the journal Computerized Medical Imaging and Graphics
- **Journal**: None
- **Summary**: We present an efficient deep learning approach for the challenging task of tumor segmentation in multisequence MR images. In recent years, Convolutional Neural Networks (CNN) have achieved state-of-the-art performances in a large variety of recognition tasks in medical imaging. Because of the considerable computational cost of CNNs, large volumes such as MRI are typically processed by subvolumes, for instance slices (axial, coronal, sagittal) or small 3D patches. In this paper we introduce a CNN-based model which efficiently combines the advantages of the short-range 3D context and the long-range 2D context. To overcome the limitations of specific choices of neural network architectures, we also propose to merge outputs of several cascaded 2D-3D models by a voxelwise voting strategy. Furthermore, we propose a network architecture in which the different MR sequences are processed by separate subnetworks in order to be more robust to the problem of missing MR sequences. Finally, a simple and efficient algorithm for training large CNN models is introduced. We evaluate our method on the public benchmark of the BRATS 2017 challenge on the task of multiclass segmentation of malignant brain tumors. Our method achieves good performances and produces accurate segmentations with median Dice scores of 0.918 (whole tumor), 0.883 (tumor core) and 0.854 (enhancing core). Our approach can be naturally applied to various tasks involving segmentation of lesions or organs.



### Deep Learning from Label Proportions for Emphysema Quantification
- **Arxiv ID**: http://arxiv.org/abs/1807.08601v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08601v1)
- **Published**: 2018-07-23 13:34:01+00:00
- **Updated**: 2018-07-23 13:34:01+00:00
- **Authors**: Gerda Bortsova, Florian Dubost, Silas Ørting, Ioannis Katramados, Laurens Hogeweg, Laura Thomsen, Mathilde Wille, Marleen de Bruijne
- **Comment**: Accepted to MICCAI 2018
- **Journal**: None
- **Summary**: We propose an end-to-end deep learning method that learns to estimate emphysema extent from proportions of the diseased tissue. These proportions were visually estimated by experts using a standard grading system, in which grades correspond to intervals (label example: 1-5% of diseased tissue). The proposed architecture encodes the knowledge that the labels represent a volumetric proportion. A custom loss is designed to learn with intervals. Thus, during training, our network learns to segment the diseased tissue such that its proportions fit the ground truth intervals. Our architecture and loss combined improve the performance substantially (8% ICC) compared to a more conventional regression network. We outperform traditional lung densitometry and two recently published methods for emphysema quantification by a large margin (at least 7% AUC and 15% ICC), and achieve near-human-level performance. Moreover, our method generates emphysema segmentations that predict the spatial distribution of emphysema at human level.



### From Volcano to Toyshop: Adaptive Discriminative Region Discovery for Scene Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.08624v2
- **DOI**: 10.1145/3240508.3240698
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08624v2)
- **Published**: 2018-07-23 13:57:40+00:00
- **Updated**: 2018-08-15 14:35:23+00:00
- **Authors**: Zhengyu Zhao, Martha Larson
- **Comment**: To appear at the ACM International Conference on Multimedia (ACM MM
  2018). Code available at https://github.com/ZhengyuZhao/Adi-Red-Scene
- **Journal**: None
- **Summary**: As deep learning approaches to scene recognition emerge, they have continued to leverage discriminative regions at multiple scales, building on practices established by conventional image classification research. However, approaches remain largely generic, and do not carefully consider the special properties of scenes. In this paper, inspired by the intuitive differences between scenes and objects, we propose Adi-Red, an adaptive approach to discriminative region discovery for scene recognition. Adi-Red uses a CNN classifier, which was pre-trained using only image-level scene labels, to discover discriminative image regions directly. These regions are then used as a source of features to perform scene recognition. The use of the CNN classifier makes it possible to adapt the number of discriminative regions per image using a simple, yet elegant, threshold, at relatively low computational cost. Experimental results on the scene recognition benchmark dataset SUN397 demonstrate the ability of Adi-Red to outperform the state of the art. Additional experimental analysis on the Places dataset reveals the advantages of Adi-Red, and highlight how they are specific to scenes. We attribute the effectiveness of Adi-Red to the ability of adaptive region discovery to avoid introducing noise, while also not missing out on important information.



### Region Convolutional Features for Multi-Label Remote Sensing Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1807.08634v1
- **DOI**: 10.1109/JSTARS.2019.2961634
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08634v1)
- **Published**: 2018-07-23 14:04:18+00:00
- **Updated**: 2018-07-23 14:04:18+00:00
- **Authors**: Weixun Zhou, Xueqing Deng, Zhenfeng Shao
- **Comment**: 8 pages
- **Journal**: IEEE J-STARS, 13 (2020): 318-328
- **Summary**: Conventional remote sensing image retrieval (RSIR) systems usually perform single-label retrieval where each image is annotated by a single label representing the most significant semantic content of the image. This assumption, however, ignores the complexity of remote sensing images, where an image might have multiple classes (i.e., multiple labels), thus resulting in worse retrieval performance. We therefore propose a novel multi-label RSIR approach with fully convolutional networks (FCN). In our approach, we first train a FCN model using a pixel-wise labeled dataset,and the trained FCN is then used to predict the segmentation maps of each image in the considered archive. We finally extract region convolutional features of each image based on its segmentation map.The region features can be either used to perform region-based retrieval or further post-processed to obtain a feature vector for similarity measure. The experimental results show that our approach achieves state-of-the-art performance in contrast to conventional single-label and recent multi-label RSIR approaches.



### Joint Anchor-Feature Refinement for Real-Time Accurate Object Detection in Images and Videos
- **Arxiv ID**: http://arxiv.org/abs/1807.08638v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.08638v6)
- **Published**: 2018-07-23 14:29:27+00:00
- **Updated**: 2020-03-13 15:41:01+00:00
- **Authors**: Xingyu Chen, Junzhi Yu, Shihan Kong, Zhengxing Wu, Li Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection has been vigorously investigated for years but fast accurate detection for real-world scenes remains a very challenging problem. Overcoming drawbacks of single-stage detectors, we take aim at precisely detecting objects for static and temporal scenes in real time. Firstly, as a dual refinement mechanism, a novel anchor-offset detection is designed, which includes an anchor refinement, a feature location refinement, and a deformable detection head. This new detection mode is able to simultaneously perform two-step regression and capture accurate object features. Based on the anchor-offset detection, a dual refinement network (DRNet) is developed for high-performance static detection, where a multi-deformable head is further designed to leverage contextual information for describing objects. As for temporal detection in videos, temporal refinement networks (TRNet) and temporal dual refinement networks (TDRNet) are developed by propagating the refinement information across time. We also propose a soft refinement strategy to temporally match object motion with the previous refinement. Our proposed methods are evaluated on PASCAL VOC, COCO, and ImageNet VID datasets. Extensive comparisons on static and temporal detection verify the superiority of DRNet, TRNet, and TDRNet. Consequently, our developed approaches run in a fairly fast speed, and in the meantime achieve a significantly enhanced detection accuracy, i.e., 84.4% mAP on VOC 2007, 83.6% mAP on VOC 2012, 69.4% mAP on VID 2017, and 42.4% AP on COCO. Ultimately, producing encouraging results, our methods are applied to online underwater object detection and grasping with an autonomous system. Codes are publicly available at https://github.com/SeanChenxy/TDRN.



### Hybrid Diffusion: Spectral-Temporal Graph Filtering for Manifold Ranking
- **Arxiv ID**: http://arxiv.org/abs/1807.08692v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08692v2)
- **Published**: 2018-07-23 16:07:29+00:00
- **Updated**: 2018-11-22 16:43:29+00:00
- **Authors**: Ahmet Iscen, Yannis Avrithis, Giorgos Tolias, Teddy Furon, Ondrej Chum
- **Comment**: None
- **Journal**: None
- **Summary**: State of the art image retrieval performance is achieved with CNN features and manifold ranking using a k-NN similarity graph that is pre-computed off-line. The two most successful existing approaches are temporal filtering, where manifold ranking amounts to solving a sparse linear system online, and spectral filtering, where eigen-decomposition of the adjacency matrix is performed off-line and then manifold ranking amounts to dot-product search online. The former suffers from expensive queries and the latter from significant space overhead. Here we introduce a novel, theoretically well-founded hybrid filtering approach allowing full control of the space-time trade-off between these two extremes. Experimentally, we verify that our hybrid method delivers results on par with the state of the art, with lower memory demands compared to spectral filtering approaches and faster compared to temporal filtering.



### PS-FCN: A Flexible Learning Framework for Photometric Stereo
- **Arxiv ID**: http://arxiv.org/abs/1807.08696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08696v1)
- **Published**: 2018-07-23 16:13:27+00:00
- **Updated**: 2018-07-23 16:13:27+00:00
- **Authors**: Guanying Chen, Kai Han, Kwan-Yee K. Wong
- **Comment**: ECCV 2018: https://guanyingc.github.io/PS-FCN
- **Journal**: None
- **Summary**: This paper addresses the problem of photometric stereo for non-Lambertian surfaces. Existing approaches often adopt simplified reflectance models to make the problem more tractable, but this greatly hinders their applications on real-world objects. In this paper, we propose a deep fully convolutional network, called PS-FCN, that takes an arbitrary number of images of a static object captured under different light directions with a fixed camera as input, and predicts a normal map of the object in a fast feed-forward pass. Unlike the recently proposed learning based method, PS-FCN does not require a pre-defined set of light directions during training and testing, and can handle multiple images and light directions in an order-agnostic manner. Although we train PS-FCN on synthetic data, it can generalize well on real datasets. We further show that PS-FCN can be easily extended to handle the problem of uncalibrated photometric stereo.Extensive experiments on public real datasets show that PS-FCN outperforms existing approaches in calibrated photometric stereo, and promising results are achieved in uncalibrated scenario, clearly demonstrating its effectiveness.



### Clearing noisy annotations for computed tomography imaging
- **Arxiv ID**: http://arxiv.org/abs/1807.09151v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.09151v1)
- **Published**: 2018-07-23 16:42:57+00:00
- **Updated**: 2018-07-23 16:42:57+00:00
- **Authors**: Roman Khudorozhkov, Alexander Koryagin, Alexey Kozhevin
- **Comment**: None
- **Journal**: None
- **Summary**: One of the problems on the way to successful implementation of neural networks is the quality of annotation. For instance, different annotators can annotate images in a different way and very often their decisions do not match exactly and in extreme cases are even mutually exclusive which results in noisy annotations and, consequently, inaccurate predictions.   To avoid that problem in the task of computed tomography (CT) imaging segmentation we propose a clearing algorithm for annotations. It consists of 3 stages:   - annotators scoring, which assigns a higher confidence level to better annotators;   - nodules scoring, which assigns a higher confidence level to nodules confirmed by good annotators;   - nodules merging, which aggregates annotations according to nodules confidence.   In general, the algorithm can be applied to many different tasks (namely, binary and multi-class semantic segmentation, and also with trivial adjustments to classification and regression) where there are several annotators labeling each image.



### Identity Preserving Face Completion for Large Ocular Region Occlusion
- **Arxiv ID**: http://arxiv.org/abs/1807.08772v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08772v1)
- **Published**: 2018-07-23 18:13:16+00:00
- **Updated**: 2018-07-23 18:13:16+00:00
- **Authors**: Yajie Zhao, Weikai Chen, Jun Xing, Xiaoming Li, Zach Bessinger, Fuchang Liu, Wangmeng Zuo, Ruigang Yang
- **Comment**: 12 pages,9 figures
- **Journal**: The British Machine Vision Conference (BMVC) 2018
- **Summary**: We present a novel deep learning approach to synthesize complete face images in the presence of large ocular region occlusions. This is motivated by recent surge of VR/AR displays that hinder face-to-face communications. Different from the state-of-the-art face inpainting methods that have no control over the synthesized content and can only handle frontal face pose, our approach can faithfully recover the missing content under various head poses while preserving the identity. At the core of our method is a novel generative network with dedicated constraints to regularize the synthesis process. To preserve the identity, our network takes an arbitrary occlusion-free image of the target identity to infer the missing content, and its high-level CNN features as an identity prior to regularize the searching space of generator. Since the input reference image may have a different pose, a pose map and a novel pose discriminator are further adopted to supervise the learning of implicit pose transformations. Our method is capable of generating coherent facial inpainting with consistent identity over videos with large variations of head motions. Experiments on both synthesized and real data demonstrate that our method greatly outperforms the state-of-the-art methods in terms of both synthesis quality and robustness.



### Peeking Behind Objects: Layered Depth Prediction from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1807.08776v1
- **DOI**: 10.1016/j.patrec.2019.05.007
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08776v1)
- **Published**: 2018-07-23 18:23:53+00:00
- **Updated**: 2018-07-23 18:23:53+00:00
- **Authors**: Helisa Dhamo, Keisuke Tateno, Iro Laina, Nassir Navab, Federico Tombari
- **Comment**: None
- **Journal**: None
- **Summary**: While conventional depth estimation can infer the geometry of a scene from a single RGB image, it fails to estimate scene regions that are occluded by foreground objects. This limits the use of depth prediction in augmented and virtual reality applications, that aim at scene exploration by synthesizing the scene from a different vantage point, or at diminished reality. To address this issue, we shift the focus from conventional depth map prediction to the regression of a specific data representation called Layered Depth Image (LDI), which contains information about the occluded regions in the reference frame and can fill in occlusion gaps in case of small view changes. We propose a novel approach based on Convolutional Neural Networks (CNNs) to jointly predict depth maps and foreground separation masks used to condition Generative Adversarial Networks (GANs) for hallucinating plausible color and depths in the initially occluded areas. We demonstrate the effectiveness of our approach for novel scene view synthesis from a single image.



### Fast Vessel Segmentation and Tracking in Ultra High-Frequency Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/1807.08784v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08784v1)
- **Published**: 2018-07-23 18:54:31+00:00
- **Updated**: 2018-07-23 18:54:31+00:00
- **Authors**: Tejas Sudharshan Mathai, Lingbo Jin, Vijay Gorantla, John Galeotti
- **Comment**: Accepted for presentation at MICCAI 2018. 8 pages, and 3 figures
- **Journal**: None
- **Summary**: Ultra High Frequency Ultrasound (UHFUS) enables the visualization of highly deformable small and medium vessels in the hand. Intricate vessel-based measurements, such as intimal wall thickness and vessel wall compliance, require sub-millimeter vessel tracking between B-scans. Our fast GPU-based approach combines the advantages of local phase analysis, a distance-regularized level set, and an Extended Kalman Filter (EKF), to rapidly segment and track the deforming vessel contour. We validated on 35 UHFUS sequences of vessels in the hand, and we show the transferability of the approach to 5 more diverse datasets acquired by a traditional High Frequency Ultrasound (HFUS) machine. To the best of our knowledge, this is the first algorithm capable of rapidly segmenting and tracking deformable vessel contours in 2D UHFUS images. It is also the fastest and most accurate system for 2D HFUS images.



### Lesion segmentation using U-Net network
- **Arxiv ID**: http://arxiv.org/abs/1807.08844v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, 62P10
- **Links**: [PDF](http://arxiv.org/pdf/1807.08844v1)
- **Published**: 2018-07-23 21:54:35+00:00
- **Updated**: 2018-07-23 21:54:35+00:00
- **Authors**: Adrien Motsch, Sebastien Motsch, Thibaut Saguet
- **Comment**: 4 pages, ISIC 2018
- **Journal**: None
- **Summary**: This paper explains the method used in the segmentation challenge (Task 1) in the International Skin Imaging Collaboration's (ISIC) Skin Lesion Analysis Towards Melanoma Detection challenge held in 2018. We have trained a U-Net network to perform the segmentation. The key elements for the training were first to adjust the loss function to incorporate unbalanced proportion of background and second to perform post-processing operation to adjust the contour of the prediction.



