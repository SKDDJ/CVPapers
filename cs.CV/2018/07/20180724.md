# Arxiv Papers in cs.CV on 2018-07-24
### StereoNet: Guided Hierarchical Refinement for Real-Time Edge-Aware Depth Prediction
- **Arxiv ID**: http://arxiv.org/abs/1807.08865v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08865v1)
- **Published**: 2018-07-24 00:45:36+00:00
- **Updated**: 2018-07-24 00:45:36+00:00
- **Authors**: Sameh Khamis, Sean Fanello, Christoph Rhemann, Adarsh Kowdle, Julien Valentin, Shahram Izadi
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: This paper presents StereoNet, the first end-to-end deep architecture for real-time stereo matching that runs at 60 fps on an NVidia Titan X, producing high-quality, edge-preserved, quantization-free disparity maps. A key insight of this paper is that the network achieves a sub-pixel matching precision than is a magnitude higher than those of traditional stereo matching approaches. This allows us to achieve real-time performance by using a very low resolution cost volume that encodes all the information needed to achieve high disparity precision. Spatial precision is achieved by employing a learned edge-aware upsampling function. Our model uses a Siamese network to extract features from the left and right image. A first estimate of the disparity is computed in a very low resolution cost volume, then hierarchically the model re-introduces high-frequency details through a learned upsampling function that uses compact pixel-to-pixel refinement networks. Leveraging color input as a guide, this function is capable of producing high-quality edge-aware output. We achieve compelling results on multiple benchmarks, showing how the proposed method offers extreme flexibility at an acceptable computational budget.



### Top-Down Feedback for Crowd Counting Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1807.08881v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08881v2)
- **Published**: 2018-07-24 02:16:14+00:00
- **Updated**: 2018-07-27 07:42:11+00:00
- **Authors**: Deepak Babu Sam, R. Venkatesh Babu
- **Comment**: AAAI 2018
- **Journal**: None
- **Summary**: Counting people in dense crowds is a demanding task even for humans. This is primarily due to the large variability in appearance of people. Often people are only seen as a bunch of blobs. Occlusions, pose variations and background clutter further compound the difficulty. In this scenario, identifying a person requires larger spatial context and semantics of the scene. But the current state-of-the-art CNN regressors for crowd counting are feedforward and use only limited spatial context to detect people. They look for local crowd patterns to regress the crowd density map, resulting in false predictions. Hence, we propose top-down feedback to correct the initial prediction of the CNN. Our architecture consists of a bottom-up CNN along with a separate top-down CNN to generate feedback. The bottom-up network, which regresses the crowd density map, has two columns of CNN with different receptive fields. Features from various layers of the bottom-up CNN are fed to the top-down network. The feedback, thus generated, is applied on the lower layers of the bottom-up network in the form of multiplicative gating. This masking weighs activations of the bottom-up network at spatial as well as feature levels to correct the density prediction. We evaluate the performance of our model on all major crowd datasets and show the effectiveness of top-down feedback.



### SafeDrive: Enhancing Lane Appearance for Autonomous and Assisted Driving Under Limited Visibility
- **Arxiv ID**: http://arxiv.org/abs/1807.11575v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11575v1)
- **Published**: 2018-07-24 02:20:08+00:00
- **Updated**: 2018-07-24 02:20:08+00:00
- **Authors**: Jiawei Mo, Junaed Sattar
- **Comment**: arXiv admin note: text overlap with arXiv:1701.08449
- **Journal**: None
- **Summary**: Autonomous detection of lane markers improves road safety, and purely visual tracking is desirable for widespread vehicle compatibility and reducing sensor intrusion, cost, and energy consumption. However, visual approaches are often ineffective because of a number of factors; e.g., occlusion, poor weather conditions, and paint wear-off. We present an approach to enhance lane marker appearance for assisted and autonomous driving, particularly under poor visibility. Our method, named SafeDrive, attempts to improve visual lane detection approaches in drastically degraded visual conditions. SafeDrive finds lane markers in alternate imagery of the road at the vehicle's location and reconstructs a sparse 3D model of the surroundings. By estimating the geometric relationship between this 3D model and the current view, the lane markers are projected onto the visual scene; any lane detection algorithm can be subsequently used to detect lanes in the resulting image. SafeDrive does not require additional sensors other than vision and location data. We demonstrate the effectiveness of our approach on a number of test cases obtained from actual driving data recorded in urban settings.



### Skin Lesion Segmentation Using Atrous Convolution via DeepLab v3
- **Arxiv ID**: http://arxiv.org/abs/1807.08891v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08891v1)
- **Published**: 2018-07-24 03:09:23+00:00
- **Updated**: 2018-07-24 03:09:23+00:00
- **Authors**: Yujie Wang, Simon Sun, Jahow Yu, Dr. Limin Yu
- **Comment**: 4 pages, 1 figure, ISIC challenge 2018
- **Journal**: None
- **Summary**: As melanoma diagnoses increase across the US, automated efforts to identify malignant lesions become increasingly of interest to the research community. Segmentation of dermoscopic images is the first step in this process, thus accuracy is crucial. Although techniques utilizing convolutional neural networks have been used in the past for lesion segmentation, we present a solution employing the recently published DeepLab 3, an atrous convolution method for image segmentation. Although the results produced by this run are not ideal, with a mean Jaccard index of 0.498, we believe that with further adjustments and modifications to the compatibility with the DeepLab code and with training on more powerful processing units, this method may achieve better results in future trials.



### Multimodal Classification with Deep Convolutional-Recurrent Neural Networks for Electroencephalography
- **Arxiv ID**: http://arxiv.org/abs/1807.10641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10641v1)
- **Published**: 2018-07-24 03:33:43+00:00
- **Updated**: 2018-07-24 03:33:43+00:00
- **Authors**: Chuanqi Tan, Fuchun Sun, Wenchang Zhang, Jianhua Chen, Chunfang Liu
- **Comment**: 10 pages, 6 figures
- **Journal**: Neural Information Processing. 2017:767-776
- **Summary**: Electroencephalography (EEG) has become the most significant input signal for brain computer interface (BCI) based systems. However, it is very difficult to obtain satisfactory classification accuracy due to traditional methods can not fully exploit multimodal information. Herein, we propose a novel approach to modeling cognitive events from EEG data by reducing it to a video classification problem, which is designed to preserve the multimodal information of EEG. In addition, optical flow is introduced to represent the variant information of EEG. We train a deep neural network (DNN) with convolutional neural network (CNN) and recurrent neural network (RNN) for the EEG classification task by using EEG video and optical flow. The experiments demonstrate that our approach has many advantages, such as more robustness and more accuracy in EEG classification tasks. According to our approach, we designed a mixed BCI-based rehabilitation support system to help stroke patients perform some basic operations.



### ClusterNet: 3D Instance Segmentation in RGB-D Images
- **Arxiv ID**: http://arxiv.org/abs/1807.08894v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.08894v2)
- **Published**: 2018-07-24 03:42:53+00:00
- **Updated**: 2018-09-19 05:23:11+00:00
- **Authors**: Lin Shao, Ye Tian, Jeannette Bohg
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method for instance-level segmentation that uses RGB-D data as input and provides detailed information about the location, geometry and number of individual objects in the scene. This level of understanding is fundamental for autonomous robots. It enables safe and robust decision-making under the large uncertainty of the real-world. In our model, we propose to use the first and second order moments of the object occupancy function to represent an object instance. We train an hourglass Deep Neural Network (DNN) where each pixel in the output votes for the 3D position of the corresponding object center and for the object's size and pose. The final instance segmentation is achieved through clustering in the space of moments. The object-centric training loss is defined on the output of the clustering. Our method outperforms the state-of-the-art instance segmentation method on our synthesized dataset. We show that our method generalizes well on real-world data achieving visually better segmentation results.



### Self-produced Guidance for Weakly-supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1807.08902v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08902v2)
- **Published**: 2018-07-24 04:35:44+00:00
- **Updated**: 2018-08-05 20:22:34+00:00
- **Authors**: Xiaolin Zhang, Yunchao Wei, Guoliang Kang, Yi Yang, Thomas Huang
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Weakly supervised methods usually generate localization results based on attention maps produced by classification networks. However, the attention maps exhibit the most discriminative parts of the object which are small and sparse. We propose to generate Self-produced Guidance (SPG) masks which separate the foreground, the object of interest, from the background to provide the classification networks with spatial correlation information of pixels. A stagewise approach is proposed to incorporate high confident object regions to learn the SPG masks. The high confident regions within attention maps are utilized to progressively learn the SPG masks. The masks are then used as an auxiliary pixel-level supervision to facilitate the training of classification networks. Extensive experiments on ILSVRC demonstrate that SPG is effective in producing high-quality object localizations maps. Particularly, the proposed SPG achieves the Top-1 localization error rate of 43.83% on the ILSVRC validation set, which is a new state-of-the-art error rate.



### Panchromatic Sharpening of Remote Sensing Images Using a Multi-scale Approach
- **Arxiv ID**: http://arxiv.org/abs/1807.08917v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08917v1)
- **Published**: 2018-07-24 06:02:19+00:00
- **Updated**: 2018-07-24 06:02:19+00:00
- **Authors**: Hamid Reza Shahdoosti
- **Comment**: 11 pages, 7 figures, journal paper
- **Journal**: None
- **Summary**: An ideal fusion method preserves the Spectral information in fused image and adds spatial information to it with no spectral distortion. Recently wavelet kalman filter method is proposed which uses ARSIS concept to fuses MS and PAN images. This method is applied in a multiscale version, i.e. the variable index is scale instead of time. With the aim of fusion we present a more detailed study on this model and discuss about rationality of its assumptions such as first order markov model and Gaussian distribution of the posterior density. Finally, we propose a method using wavelet Kalman Particle filter to improve the spectral and spatial quality of the fused image. We show that our model is more consistent with natural MS and PAN images. Visual and statistical analyzes show that the proposed algorithm clearly improves the fusion quality in terms of: correlation coefficient, ERGAS, UIQI, and Q4; compared to other methods including IHS, HMP, PCA, A`trous, udWI, udWPC, Adaptive IHS, Improved Adaptive PCA and wavelet kalman filter.



### Competitive Inner-Imaging Squeeze and Excitation for Residual Network
- **Arxiv ID**: http://arxiv.org/abs/1807.08920v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.08920v4)
- **Published**: 2018-07-24 06:13:25+00:00
- **Updated**: 2018-12-23 02:56:45+00:00
- **Authors**: Yang Hu, Guihua Wen, Mingnan Luo, Dan Dai, Jiajiong Ma, Zhiwen Yu
- **Comment**: Code is available at
  https://github.com/scut-aitcm/Competitive-Inner-Imaging-SENet
- **Journal**: None
- **Summary**: Residual networks, which use a residual unit to supplement the identity mappings, enable very deep convolutional architecture to operate well, however, the residual architecture has been proved to be diverse and redundant, which may leads to low-efficient modeling. In this work, we propose a competitive squeeze-excitation (SE) mechanism for the residual network. Re-scaling the value for each channel in this structure will be determined by the residual and identity mappings jointly, and this design enables us to expand the meaning of channel relationship modeling in residual blocks. Modeling of the competition between residual and identity mappings cause the identity flow to control the complement of the residual feature maps for itself. Furthermore, we design a novel inner-imaging competitive SE block to shrink the consumption and re-image the global features of intermediate network structure, by using the inner-imaging mechanism, we can model the channel-wise relations with convolution in spatial. We carry out experiments on the CIFAR, SVHN, and ImageNet datasets, and the proposed method can challenge state-of-the-art results.



### Improved Adaptive Brovey as a New Method for Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/1807.09610v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09610v1)
- **Published**: 2018-07-24 06:35:55+00:00
- **Updated**: 2018-07-24 06:35:55+00:00
- **Authors**: Hamid Reza Shahdoosti
- **Comment**: 6 pages, 3 figures, journal paper. arXiv admin note: text overlap
  with arXiv:1701.01996, arXiv:1807.08917
- **Journal**: None
- **Summary**: An ideal fusion method preserves the Spectral information in fused image and adds spatial information to it with no spectral distortion. Among the existing fusion algorithms, the contourlet-based fusion method is the most frequently discussed one in recent publications, because the contourlet has the ability to capture and link the point of discontinuities to form a linear structure. The Brovey is a popular pan-sharpening method owing to its efficiency and high spatial resolution. This method can be explained by mathematical model of optical remote sensing sensors. This study presents a new fusion approach that integrates the advantages of both the Brovey and the cotourlet techniques to reduce the color distortion of fusion results. Visual and statistical analyzes show that the proposed algorithm clearly improves the merging quality in terms of: correlation coefficient, ERGAS, UIQI, and Q4; compared to fusion methods including IHS, PCA, Adaptive IHS, and Improved Adaptive PCA.



### CReaM: Condensed Real-time Models for Depth Prediction using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.08931v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08931v1)
- **Published**: 2018-07-24 07:16:54+00:00
- **Updated**: 2018-07-24 07:16:54+00:00
- **Authors**: Andrew Spek, Thanuja Dharmasiri, Tom Drummond
- **Comment**: None
- **Journal**: None
- **Summary**: Since the resurgence of CNNs the robotic vision community has developed a range of algorithms that perform classification, semantic segmentation and structure prediction (depths, normals, surface curvature) using neural networks. While some of these models achieve state-of-the art results and super human level performance, deploying these models in a time critical robotic environment remains an ongoing challenge. Real-time frameworks are of paramount importance to build a robotic society where humans and robots integrate seamlessly. To this end, we present a novel real-time structure prediction framework that predicts depth at 30fps on an NVIDIA-TX2. At the time of writing, this is the first piece of work to showcase such a capability on a mobile platform. We also demonstrate with extensive experiments that neural networks with very large model capacities can be leveraged in order to train accurate condensed model architectures in a "from teacher to student" style knowledge transfer.



### Combining Heterogeneously Labeled Datasets For Training Segmentation Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.08935v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08935v1)
- **Published**: 2018-07-24 07:43:23+00:00
- **Updated**: 2018-07-24 07:43:23+00:00
- **Authors**: Jana Kemnitz, Christian F. Baumgartner, Wolfgang Wirth, Felix Eckstein, Sebastian K. Eder, Ender Konukoglu
- **Comment**: Accepted for presentation at 9th International Conference on Machine
  Learning in Medical Imaging (MLMI 2018)
- **Journal**: None
- **Summary**: Accurate segmentation of medical images is an important step towards analyzing and tracking disease related morphological alterations in the anatomy. Convolutional neural networks (CNNs) have recently emerged as a powerful tool for many segmentation tasks in medical imaging. The performance of CNNs strongly depends on the size of the training data and combining data from different sources is an effective strategy for obtaining larger training datasets. However, this is often challenged by heterogeneous labeling of the datasets. For instance, one of the dataset may be missing labels or a number of labels may have been combined into a super label. In this work we propose a cost function which allows integration of multiple datasets with heterogeneous label subsets into a joint training. We evaluated the performance of this strategy on thigh MR and a cardiac MR datasets in which we artificially merged labels for half of the data. We found the proposed cost function substantially outperforms a naive masking approach, obtaining results very close to using the full annotations.



### Example Mining for Incremental Learning in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/1807.08942v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08942v1)
- **Published**: 2018-07-24 07:50:18+00:00
- **Updated**: 2018-07-24 07:50:18+00:00
- **Authors**: Pratyush Kumar, Muktabh Mayank Srivastava
- **Comment**: None
- **Journal**: None
- **Summary**: Incremental Learning is well known machine learning approach wherein the weights of the learned model are dynamically and gradually updated to generalize on new unseen data without forgetting the existing knowledge. Incremental learning proves to be time as well as resource-efficient solution for deployment of deep learning algorithms in real world as the model can automatically and dynamically adapt to new data as and when annotated data becomes available. The development and deployment of Computer Aided Diagnosis (CAD) tools in medical domain is another scenario, where incremental learning becomes very crucial as collection and annotation of a comprehensive dataset spanning over multiple pathologies and imaging machines might take years. However, not much has so far been explored in this direction. In the current work, we propose a robust and efficient method for incremental learning in medical imaging domain. Our approach makes use of Hard Example Mining technique (which is commonly used as a solution to heavy class imbalance) to automatically select a subset of dataset to fine-tune the existing network weights such that it adapts to new data while retaining existing knowledge. We develop our approach for incremental learning of our already under test model for detecting dental caries. Further, we apply our approach to one publicly available dataset and demonstrate that our approach reaches the accuracy of training on entire dataset at once, while availing the benefits of incremental learning scenario.



### Hyperspectral Images Classification Using Energy Profiles of Spatial and Spectral Features
- **Arxiv ID**: http://arxiv.org/abs/1807.08943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08943v1)
- **Published**: 2018-07-24 07:52:25+00:00
- **Updated**: 2018-07-24 07:52:25+00:00
- **Authors**: Hamid Reza Shahdoosti
- **Comment**: 7 pages, 3 figures, journal paper
- **Journal**: None
- **Summary**: This paper proposes a spatial feature extraction method based on energy of the features for classification of the hyperspectral data. A proposed orthogonal filter set extracts spatial features with maximum energy from the principal components and then, a profile is constructed based on these features. The important characteristic of the proposed approach is that the filter sets coefficients are extracted from statistical properties of data, thus they are more consistent with the type and texture of the remotely sensed images compared with those of other filters such as Gabor. To assess the performance of the proposed feature extraction method, the extracted features are fed into a support vector machine (SVM) classifier. Experiments on the widely used hyperspectral images namely, Indian Pines, and Salinas data sets reveal that the proposed approach improves the classification results in comparison with some recent spectral spatial classification methods.



### Dermoscopic Image Analysis for ISIC Challenge 2018
- **Arxiv ID**: http://arxiv.org/abs/1807.08948v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08948v1)
- **Published**: 2018-07-24 08:12:54+00:00
- **Updated**: 2018-07-24 08:12:54+00:00
- **Authors**: Jinyi Zou, Xiao Ma, Cheng Zhong, Yao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This short paper reports the algorithms we used and the evaluation performances for ISIC Challenge 2018. Our team participates in all the tasks in this challenge. In lesion segmentation task, the pyramid scene parsing network (PSPNet) is modified to segment the lesions. In lesion attribute detection task, the modified PSPNet is also adopted in a multi-label way. In disease classification task, the DenseNet-169 is adopted for multi-class classification.



### The Double Sphere Camera Model
- **Arxiv ID**: http://arxiv.org/abs/1807.08957v2
- **DOI**: 10.1109/3DV.2018.00069
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.08957v2)
- **Published**: 2018-07-24 08:42:00+00:00
- **Updated**: 2018-10-29 16:37:37+00:00
- **Authors**: Vladyslav Usenko, Nikolaus Demmel, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: Vision-based motion estimation and 3D reconstruction, which have numerous applications (e.g., autonomous driving, navigation systems for airborne devices and augmented reality) are receiving significant research attention. To increase the accuracy and robustness, several researchers have recently demonstrated the benefit of using large field-of-view cameras for such applications. In this paper, we provide an extensive review of existing models for large field-of-view cameras. For each model we provide projection and unprojection functions and the subspace of points that result in valid projection. Then, we propose the Double Sphere camera model that well fits with large field-of-view lenses, is computationally inexpensive and has a closed-form inverse. We evaluate the model using a calibration dataset with several different lenses and compare the models using the metrics that are relevant for Visual Odometry, i.e., reprojection error, as well as computation time for projection and unprojection functions and their Jacobians. We also provide qualitative results and discuss the performance of all models.



### Multi-Scale Gradual Integration CNN for False Positive Reduction in Pulmonary Nodule Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.10581v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.10581v1)
- **Published**: 2018-07-24 10:08:12+00:00
- **Updated**: 2018-07-24 10:08:12+00:00
- **Authors**: Bum-Chae Kim, Jun-Sik Choi, Heung-Il Suk
- **Comment**: 11 pages, 6 figures, 5 tables
- **Journal**: None
- **Summary**: Lung cancer is a global and dangerous disease, and its early detection is crucial to reducing the risks of mortality. In this regard, it has been of great interest in developing a computer-aided system for pulmonary nodules detection as early as possible on thoracic CT scans. In general, a nodule detection system involves two steps: (i) candidate nodule detection at a high sensitivity, which captures many false positives and (ii) false positive reduction from candidates. However, due to the high variation of nodule morphological characteristics and the possibility of mistaking them for neighboring organs, candidate nodule detection remains a challenge. In this study, we propose a novel Multi-scale Gradual Integration Convolutional Neural Network (MGI-CNN), designed with three main strategies: (1) to use multi-scale inputs with different levels of contextual information, (2) to use abstract information inherent in different input scales with gradual integration, and (3) to learn multi-stream feature integration in an end-to-end manner. To verify the efficacy of the proposed network, we conducted exhaustive experiments on the LUNA16 challenge datasets by comparing the performance of the proposed method with state-of-the-art methods in the literature. On two candidate subsets of the LUNA16 dataset, i.e., V1 and V2, our method achieved an average CPM of 0.908 (V1) and 0.942 (V2), outperforming comparable methods by a large margin. Our MGI-CNN is implemented in Python using TensorFlow and the source code is available from 'https://github.com/ku-milab/MGICNN.'



### CaricatureShop: Personalized and Photorealistic Caricature Sketching
- **Arxiv ID**: http://arxiv.org/abs/1807.09064v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1807.09064v1)
- **Published**: 2018-07-24 12:26:57+00:00
- **Updated**: 2018-07-24 12:26:57+00:00
- **Authors**: Xiaoguang Han, Kangcheng Hou, Dong Du, Yuda Qiu, Yizhou Yu, Kun Zhou, Shuguang Cui
- **Comment**: 12 pages,16 figures,submitted to IEEE TVCG
- **Journal**: None
- **Summary**: In this paper, we propose the first sketching system for interactively personalized and photorealistic face caricaturing. Input an image of a human face, the users can create caricature photos by manipulating its facial feature curves. Our system firstly performs exaggeration on the recovered 3D face model according to the edited sketches, which is conducted by assigning the laplacian of each vertex a scaling factor. To construct the mapping between 2D sketches and a vertex-wise scaling field, a novel deep learning architecture is developed. With the obtained 3D caricature model, two images are generated, one obtained by applying 2D warping guided by the underlying 3D mesh deformation and the other obtained by re-rendering the deformed 3D textured model. These two images are then seamlessly integrated to produce our final output. Due to the severely stretching of meshes, the rendered texture is of blurry appearances. A deep learning approach is exploited to infer the missing details for enhancing these blurry regions. Moreover, a relighting operation is invented to further improve the photorealism of the result. Both quantitative and qualitative experiment results validated the efficiency of our sketching system and the superiority of our proposed techniques against existing methods.



### Feature Fusion through Multitask CNN for Large-scale Remote Sensing Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.09072v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09072v1)
- **Published**: 2018-07-24 12:48:15+00:00
- **Updated**: 2018-07-24 12:48:15+00:00
- **Authors**: Shihao Sun, Lei Yang, Wenjie Liu, Ruirui Li
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, Fully Convolutional Networks (FCN) has been widely used in various semantic segmentation tasks, including multi-modal remote sensing imagery. How to fuse multi-modal data to improve the segmentation performance has always been a research hotspot. In this paper, a novel end-toend fully convolutional neural network is proposed for semantic segmentation of natural color, infrared imagery and Digital Surface Models (DSM). It is based on a modified DeepUNet and perform the segmentation in a multi-task way. The channels are clustered into groups and processed on different task pipelines. After a series of segmentation and fusion, their shared features and private features are successfully merged together. Experiment results show that the feature fusion network is efficient. And our approach achieves good performance in ISPRS Semantic Labeling Contest (2D).



### Learning Human Poses from Actions
- **Arxiv ID**: http://arxiv.org/abs/1807.09075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09075v1)
- **Published**: 2018-07-24 12:58:58+00:00
- **Updated**: 2018-07-24 12:58:58+00:00
- **Authors**: Aditya Arun, C. V. Jawahar, M. Pawan Kumar
- **Comment**: Accepted at BMVC 2018
- **Journal**: None
- **Summary**: We consider the task of learning to estimate human pose in still images. In order to avoid the high cost of full supervision, we propose to use a diverse data set, which consists of two types of annotations: (i) a small number of images are labeled using the expensive ground-truth pose; and (ii) other images are labeled using the inexpensive action label. As action information helps narrow down the pose of a human, we argue that this approach can help reduce the cost of training without significantly affecting the accuracy. To demonstrate this we design a probabilistic framework that employs two distributions: (i) a conditional distribution to model the uncertainty over the human pose given the image and the action; and (ii) a prediction distribution, which provides the pose of an image without using any action information. We jointly estimate the parameters of the two aforementioned distributions by minimizing their dissimilarity coefficient, as measured by a task-specific loss function. During both training and testing, we only require an efficient sampling strategy for both the aforementioned distributions. This allows us to use deep probabilistic networks that are capable of providing accurate pose estimates for previously unseen images. Using the MPII data set, we show that our approach outperforms baseline methods that either do not use the diverse annotations or rely on pointwise estimates of the pose.



### Sokoto Coventry Fingerprint Dataset
- **Arxiv ID**: http://arxiv.org/abs/1807.10609v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10609v1)
- **Published**: 2018-07-24 13:14:11+00:00
- **Updated**: 2018-07-24 13:14:11+00:00
- **Authors**: Yahaya Isah Shehu, Ariel Ruiz-Garcia, Vasile Palade, Anne James
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents the Sokoto Coventry Fingerprint Dataset (SOCOFing), a biometric fingerprint database designed for academic research purposes. SOCOFing is made up of 6,000 fingerprint images from 600 African subjects. SOCOFing contains unique attributes such as labels for gender, hand and finger name as well as synthetically altered versions with three different levels of alteration for obliteration, central rotation, and z-cut. The dataset is freely available for noncommercial research purposes at: https://www.kaggle.com/ruizgara/socofing



### ISIC 2017 Skin Lesion Segmentation Using Deep Encoder-Decoder Network
- **Arxiv ID**: http://arxiv.org/abs/1807.09083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09083v1)
- **Published**: 2018-07-24 13:17:55+00:00
- **Updated**: 2018-07-24 13:17:55+00:00
- **Authors**: Ngoc-Quang Nguyen
- **Comment**: ISIC 2018
- **Journal**: None
- **Summary**: This paper summarizes our method and validation results for part 1 of the ISBI Challenge 2018. Our algorithm makes use of deep encoder-decoder network and novel skin lesion data augmentation to segment the challenge objective. Besides, we also propose an effective testing strategy by applying multi-model comparison.



### Learning Class Prototypes via Structure Alignment for Zero-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.09123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09123v1)
- **Published**: 2018-07-24 14:01:04+00:00
- **Updated**: 2018-07-24 14:01:04+00:00
- **Authors**: Huajie Jiang, Ruiping Wang, Shiguang Shan, Xilin Chen
- **Comment**: To appear in ECCV 2018
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) aims to recognize objects of novel classes without any training samples of specific classes, which is achieved by exploiting the semantic information and auxiliary datasets. Recently most ZSL approaches focus on learning visual-semantic embeddings to transfer knowledge from the auxiliary datasets to the novel classes. However, few works study whether the semantic information is discriminative or not for the recognition task. To tackle such problem, we propose a coupled dictionary learning approach to align the visual-semantic structures using the class prototypes, where the discriminative information lying in the visual space is utilized to improve the less discriminative semantic space. Then, zero-shot recognition can be performed in different spaces by the simple nearest neighbor approach using the learned class prototypes. Extensive experiments on four benchmark datasets show the effectiveness of the proposed approach.



### Non-local Low-rank Cube-based Tensor Factorization for Spectral CT Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1807.10610v3
- **DOI**: 10.1109/TMI.2018.2878226
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10610v3)
- **Published**: 2018-07-24 14:15:03+00:00
- **Updated**: 2018-10-28 05:31:46+00:00
- **Authors**: Weiwen Wu, Fenglin Liu, Yanbo Zhang, Qian Wang, Hengyong Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Spectral computed tomography (CT) reconstructs material-dependent attenuation images with the projections of multiple narrow energy windows, it is meaningful for material identification and decomposition. Unfortunately, the multi-energy projection dataset always contains strong complicated noise and result in the projections has a lower signal-noise-ratio (SNR). Very recently, the spatial-spectral cube matching frame (SSCMF) was proposed to explore the non-local spatial-spectrum similarities for spectral CT. The method constructs such a group by clustering up a series of non-local spatial-spectrum cubes. The small size of spatial patch for such a group make SSCMF fails to encode the sparsity and low-rank properties. In addition, the hard-thresholding and collaboration filtering operation in the SSCMF are also rough to recover the image features and spatial edges. While for all steps are operated on 4-D group, we may not afford such huge computational and memory load in practical. To avoid the above limitation and further improve image quality, we first formulate a non-local cube-based tensor instead of the group to encode the sparsity and low-rank properties. Then, as a new regularizer, Kronecker-Basis-Representation (KBR) tensor factorization is employed into a basic spectral CT reconstruction model to enhance the ability of extracting image features and protecting spatial edges, generating the non-local low-rank cube-based tensor factorization (NLCTF) method. Finally, the split-Bregman strategy is adopted to solve the NLCTF model. Both numerical simulations and realistic preclinical mouse studies are performed to validate and assess the NLCTF algorithm. The results show that the NLCTF method outperforms the other competitors.



### Residual Network based Aggregation Model for Skin Lesion Classification
- **Arxiv ID**: http://arxiv.org/abs/1807.09150v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09150v1)
- **Published**: 2018-07-24 14:33:48+00:00
- **Updated**: 2018-07-24 14:33:48+00:00
- **Authors**: Yongsheng Pan, Yong Xia
- **Comment**: ISIC2018 task3
- **Journal**: None
- **Summary**: We recognize that the skin lesion diagnosis is an essential and challenging sub-task in Image classification, in which the Fisher vector (FV) encoding algorithm and deep convolutional neural network (DCNN) are two of the most successful techniques. Since the joint use of FV and DCNN has demonstrated proven success, the joint techniques could have discriminatory power on skin lesion diagnosis as well. To this hypothesis, we propose the aggregation algorithm for skin lesion diagnosis that utilize the residual network to extract the local features and the Fisher vector method to aggregate the local features to image-level representation. We applied our algorithm on the International Skin Imaging Collaboration 2018 (ISIC2018) challenge and only focus on the third task, i.e., the disease classification.



### QUEST: Quadriletral Senary bit Pattern for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.09154v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09154v1)
- **Published**: 2018-07-24 14:39:48+00:00
- **Updated**: 2018-07-24 14:39:48+00:00
- **Authors**: Monu Verma, Prafulla Saxena, Santosh. K. Vipparthi, Gridhari Singh
- **Comment**: 7 pages, 7 tables, 6 Figures
- **Journal**: None
- **Summary**: Facial expression has a significant role in analyzing human cognitive state. Deriving an accurate facial appearance representation is a critical task for an automatic facial expression recognition application. This paper provides a new feature descriptor named as Quadrilateral Senary bit Pattern for facial expression recognition. The QUEST pattern encoded the intensity changes by emphasizing the relationship between neighboring and reference pixels by dividing them into two quadrilaterals in a local neighborhood. Thus, the resultant gradient edges reveal the transitional variation information, that improves the classification rate by discriminating expression classes. Moreover, it also enhances the capability of the descriptor to deal with viewpoint variations and illumination changes. The trine relationship in a quadrilateral structure helps to extract the expressive edges and suppressing noise elements to enhance the robustness to noisy conditions. The QUEST pattern generates a six-bit compact code, which improves the efficiency of the FER system with more discriminability. The effectiveness of the proposed method is evaluated by conducting several experiments on four benchmark datasets: MMI, GEMEP-FERA, OULU-CASIA, and ISED. The experimental results show better performance of the proposed method as compared to existing state-art-the approaches.



### Partial Person Re-identification with Alignment and Hallucination
- **Arxiv ID**: http://arxiv.org/abs/1807.09162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09162v1)
- **Published**: 2018-07-24 14:48:26+00:00
- **Updated**: 2018-07-24 14:48:26+00:00
- **Authors**: Sara Iodice, Krystian Mikolajczyk
- **Comment**: None
- **Journal**: None
- **Summary**: Partial person re-identification involves matching pedestrian frames where only a part of a body is visible in corresponding images. This reflects practical CCTV surveillance scenario, where full person views are often not available. Missing body parts make the comparison very challenging due to significant misalignment and varying scale of the views. We propose Partial Matching Net (PMN) that detects body joints, aligns partial views and hallucinates the missing parts based on the information present in the frame and a learned model of a person. The aligned and reconstructed views are then combined into a joint representation and used for matching images. We evaluate our approach and compare to other methods on three different datasets, demonstrating significant improvements.



### Skin disease identification from dermoscopy images using deep convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1807.09163v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09163v1)
- **Published**: 2018-07-24 14:48:57+00:00
- **Updated**: 2018-07-24 14:48:57+00:00
- **Authors**: Anabik Pal, Sounak Ray, Utpal Garain
- **Comment**: Challenge Participation in ISIC 2018: Skin Lesion Analysis Towards
  Melanoma Detection
- **Journal**: None
- **Summary**: In this paper, a deep neural network based ensemble method is experimented for automatic identification of skin disease from dermoscopic images. The developed algorithm is applied on the task3 of the ISIC 2018 challenge dataset (Skin Lesion Analysis Towards Melanoma Detection).



### Convolutional Simplex Projection Network (CSPN) for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.09169v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09169v1)
- **Published**: 2018-07-24 15:06:59+00:00
- **Updated**: 2018-07-24 15:06:59+00:00
- **Authors**: Rania Briq, Michael Moeller, Juergen Gall
- **Comment**: BMVC 2018
- **Journal**: None
- **Summary**: Weakly supervised semantic segmentation has been a subject of increased interest due to the scarcity of fully annotated images. We introduce a new approach for solving weakly supervised semantic segmentation with deep Convolutional Neural Networks (CNNs). The method introduces a novel layer which applies simplex projection on the output of a neural network using area constraints of class objects. The proposed method is general and can be seamlessly integrated into any CNN architecture. Moreover, the projection layer allows strongly supervised models to be adapted to weakly supervised models effortlessly by substituting ground truth labels. Our experiments have shown that applying such an operation on the output of a CNN improves the accuracy of semantic segmentation in a weakly supervised setting with image-level labels.



### PReMVOS: Proposal-generation, Refinement and Merging for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.09190v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09190v2)
- **Published**: 2018-07-24 15:42:45+00:00
- **Updated**: 2018-11-03 17:35:06+00:00
- **Authors**: Jonathon Luiten, Paul Voigtlaender, Bastian Leibe
- **Comment**: Accepted for publication in ACCV18
- **Journal**: None
- **Summary**: We address semi-supervised video object segmentation, the task of automatically generating accurate and consistent pixel masks for objects in a video sequence, given the first-frame ground truth annotations. Towards this goal, we present the PReMVOS algorithm (Proposal-generation, Refinement and Merging for Video Object Segmentation). Our method separates this problem into two steps, first generating a set of accurate object segmentation mask proposals for each video frame and then selecting and merging these proposals into accurate and temporally consistent pixel-wise object tracks over a video sequence in a way which is designed to specifically tackle the difficult challenges involved with segmenting multiple objects across a video sequence. Our approach surpasses all previous state-of-the-art results on the DAVIS 2017 video object segmentation benchmark with a J & F mean score of 71.6 on the test-dev dataset, and achieves first place in both the DAVIS 2018 Video Object Segmentation Challenge and the YouTube-VOS 1st Large-scale Video Object Segmentation Challenge.



### Multicolumn Networks for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.09192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09192v1)
- **Published**: 2018-07-24 15:45:58+00:00
- **Updated**: 2018-07-24 15:45:58+00:00
- **Authors**: Weidi Xie, Andrew Zisserman
- **Comment**: To appear in BMVC2018
- **Journal**: None
- **Summary**: The objective of this work is set-based face recognition, i.e. to decide if two sets of images of a face are of the same person or not. Conventionally, the set-wise feature descriptor is computed as an average of the descriptors from individual face images within the set. In this paper, we design a neural network architecture that learns to aggregate based on both "visual" quality (resolution, illumination), and "content" quality (relative importance for discriminative classification). To this end, we propose a Multicolumn Network (MN) that takes a set of images (the number in the set can vary) as input, and learns to compute a fix-sized feature descriptor for the entire set. To encourage high-quality representations, each individual input image is first weighted by its "visual" quality, determined by a self-quality assessment module, and followed by a dynamic recalibration based on "content" qualities relative to the other images within the set. Both of these qualities are learnt implicitly during training for set-wise classification. Comparing with the previous state-of-the-art architectures trained with the same dataset (VGGFace2), our Multicolumn Networks show an improvement of between 2-6% on the IARPA IJB face recognition benchmarks, and exceed the state of the art for all methods on these benchmarks.



### Self-Paced Learning with Adaptive Deep Visual Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1807.09200v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.09200v1)
- **Published**: 2018-07-24 16:01:00+00:00
- **Updated**: 2018-07-24 16:01:00+00:00
- **Authors**: Vithursan Thangarasa, Graham W. Taylor
- **Comment**: Published as a conference paper at BMVC 2018
- **Journal**: None
- **Summary**: Selecting the most appropriate data examples to present a deep neural network (DNN) at different stages of training is an unsolved challenge. Though practitioners typically ignore this problem, a non-trivial data scheduling method may result in a significant improvement in both convergence and generalization performance. In this paper, we introduce Self-Paced Learning with Adaptive Deep Visual Embeddings (SPL-ADVisE), a novel end-to-end training protocol that unites self-paced learning (SPL) and deep metric learning (DML). We leverage the Magnet Loss to train an embedding convolutional neural network (CNN) to learn a salient representation space. The student CNN classifier dynamically selects similar instance-level training examples to form a mini-batch, where the easiness from the cross-entropy loss and the true diverseness of examples from the learned metric space serve as sample importance priors. To demonstrate the effectiveness of SPL-ADVisE, we use deep CNN architectures for the task of supervised image classification on several coarse- and fine-grained visual recognition datasets. Results show that, across all datasets, the proposed method converges faster and reaches a higher final accuracy than other SPL variants, particularly on fine-grained classes.



### Face Mask Extraction in Video Sequence
- **Arxiv ID**: http://arxiv.org/abs/1807.09207v3
- **DOI**: 10.1007/s11263-018-1130-2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09207v3)
- **Published**: 2018-07-24 16:09:32+00:00
- **Updated**: 2021-02-27 17:42:27+00:00
- **Authors**: Yujiang Wang, Bingnan Luo, Jie Shen, Maja Pantic
- **Comment**: 300VW-Mask dataset is available at:
  https://github.com/mapleandfire/300VW-Mask
- **Journal**: International Journal of Computer Vision, 127(6), 2019, 625-641
- **Summary**: Inspired by the recent development of deep network-based methods in semantic image segmentation, we introduce an end-to-end trainable model for face mask extraction in video sequence. Comparing to landmark-based sparse face shape representation, our method can produce the segmentation masks of individual facial components, which can better reflect their detailed shape variations. By integrating Convolutional LSTM (ConvLSTM) algorithm with Fully Convolutional Networks (FCN), our new ConvLSTM-FCN model works on a per-sequence basis and takes advantage of the temporal correlation in video clips. In addition, we also propose a novel loss function, called Segmentation Loss, to directly optimise the Intersection over Union (IoU) performances. In practice, to further increase segmentation accuracy, one primary model and two additional models were trained to focus on the face, eyes, and mouth regions, respectively. Our experiment shows the proposed method has achieved a 16.99% relative improvement (from 54.50% to 63.76% mean IoU) over the baseline FCN model on the 300 Videos in the Wild (300VW) dataset.



### Deterministic Fitting of Multiple Structures using Iterative MaxFS with Inlier Scale Estimation and Subset Updating
- **Arxiv ID**: http://arxiv.org/abs/1807.09210v1
- **DOI**: 10.1109/ICCV.2013.12
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09210v1)
- **Published**: 2018-07-24 16:16:41+00:00
- **Updated**: 2018-07-24 16:16:41+00:00
- **Authors**: Kwang Hee Lee, Sang Wook Lee
- **Comment**: An extended version of our ICCV 2013 paper
- **Journal**: None
- **Summary**: We present an efficient deterministic hypothesis generation algorithm for robust fitting of multiple structures based on the maximum feasible subsystem (MaxFS) framework. Despite its advantage, a global optimization method such as MaxFS has two main limitations for geometric model fitting. First, its performance is much influenced by the user-specified inlier scale. Second, it is computationally inefficient for large data. The presented MaxFS-based algorithm iteratively estimates model parameters and inlier scale and also overcomes the second limitation by reducing data for the MaxFS problem. Further it generates hypotheses only with top-n ranked subsets based on matching scores and data fitting residuals. This reduction of data for the MaxFS problem makes the algorithm computationally realistic. Our method, called iterative MaxFS with inlier scale estimation and subset updating (IMaxFS-ISE-SU) in this paper, performs hypothesis generation and fitting alternately until all of true structures are found. The IMaxFS-ISE-SU algorithm generates substantially more reliable hypotheses than random sampling-based methods especially as (pseudo-)outlier ratios increase. Experimental results demonstrate that our method can generate more reliable and consistent hypotheses than random sampling-based methods for estimating multiple structures from data with many outliers.



### Multi-Class Lesion Diagnosis with Pixel-wise Classification Network
- **Arxiv ID**: http://arxiv.org/abs/1807.09227v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09227v1)
- **Published**: 2018-07-24 16:45:49+00:00
- **Updated**: 2018-07-24 16:45:49+00:00
- **Authors**: Manu Goyal, Jiahua Ng, Moi Hoon Yap
- **Comment**: 6 pages, 4 figures and 2 tables
- **Journal**: None
- **Summary**: Lesion diagnosis of skin lesions is a very challenging task due to high inter-class similarities and intra-class variations in terms of color, size, site and appearance among different skin lesions. With the emergence of computer vision especially deep learning algorithms, lesion diagnosis is made possible using these algorithms trained on dermoscopic images. Usually, deep classification networks are used for the lesion diagnosis to determine different types of skin lesions. In this work, we used pixel-wise classification network to provide lesion diagnosis rather than classification network. We propose to use DeeplabV3+ for multi-class lesion diagnosis in dermoscopic images of Task 3 of ISIC Challenge 2018. We used various post-processing methods with DeeplabV3+ to determine the lesion diagnosis in this challenge and submitted the test results.



### Deep Learning on Retina Images as Screening Tool for Diagnostic Decision Support
- **Arxiv ID**: http://arxiv.org/abs/1807.09232v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1807.09232v1)
- **Published**: 2018-07-24 16:59:06+00:00
- **Updated**: 2018-07-24 16:59:06+00:00
- **Authors**: Maria Camila Alvarez Trivino, Jeremie Despraz, Jesus Alfonso Lopez Sotelo, Carlos Andres Pena
- **Comment**: None
- **Journal**: None
- **Summary**: In this project, we developed a deep learning system applied to human retina images for medical diagnostic decision support. The retina images were provided by EyePACS. These images were used in the framework of a Kaggle contest, whose purpose to identify diabetic retinopathy signs through an automatic detection system. Using as inspiration one of the solutions proposed in the contest, we implemented a model that successfully detects diabetic retinopathy from retina images. After a carefully designed preprocessing, the images were used as input to a deep convolutional neural network (CNN). The CNN performed a feature extraction process followed by a classification stage, which allowed the system to differentiate between healthy and ill patients using five categories. Our model was able to identify diabetic retinopathy in the patients with an agreement rate of 76.73% with respect to the medical expert's labels for the test data.



### Unsupervised Learning of Latent Physical Properties Using Perception-Prediction Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.09244v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.09244v2)
- **Published**: 2018-07-24 17:28:27+00:00
- **Updated**: 2018-07-25 19:03:07+00:00
- **Authors**: David Zheng, Vinson Luo, Jiajun Wu, Joshua B. Tenenbaum
- **Comment**: UAI 2018 (oral)
- **Journal**: None
- **Summary**: We propose a framework for the completely unsupervised learning of latent object properties from their interactions: the perception-prediction network (PPN). Consisting of a perception module that extracts representations of latent object properties and a prediction module that uses those extracted properties to simulate system dynamics, the PPN can be trained in an end-to-end fashion purely from samples of object dynamics. The representations of latent object properties learned by PPNs not only are sufficient to accurately simulate the dynamics of systems comprised of previously unseen objects, but also can be translated directly into human-interpretable properties (e.g., mass, coefficient of restitution) in an entirely unsupervised manner. Crucially, PPNs also generalize to novel scenarios: their gradient-based training can be applied to many dynamical systems and their graph-based structure functions over systems comprised of different numbers of objects. Our results demonstrate the efficacy of graph-based neural architectures in object-centric inference and prediction tasks, and our model has the potential to discover relevant object properties in systems that are not yet well understood.



### Visual Dynamics: Stochastic Future Generation via Layered Cross Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.09245v3
- **DOI**: 10.1109/TPAMI.2018.2854726
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.09245v3)
- **Published**: 2018-07-24 17:28:31+00:00
- **Updated**: 2019-08-09 23:11:54+00:00
- **Authors**: Tianfan Xue, Jiajun Wu, Katherine L. Bouman, William T. Freeman
- **Comment**: Journal preprint of arXiv:1607.02586 (IEEE TPAMI, 2019). The first
  two authors contributed equally to this work. Project page:
  http://visualdynamics.csail.mit.edu
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI), vol. 41, no. 9, pp. 2236-2250, 2019
- **Summary**: We study the problem of synthesizing a number of likely future frames from a single input image. In contrast to traditional methods that have tackled this problem in a deterministic or non-parametric way, we propose to model future frames in a probabilistic manner. Our probabilistic model makes it possible for us to sample and synthesize many possible future frames from a single input image. To synthesize realistic movement of objects, we propose a novel network structure, namely a Cross Convolutional Network; this network encodes image and motion information as feature maps and convolutional kernels, respectively. In experiments, our model performs well on synthetic data, such as 2D shapes and animated game sprites, and on real-world video frames. We present analyses of the learned network representations, showing it is implicitly learning a compact encoding of object appearance and motion. We also demonstrate a few of its applications, including visual analogy-making and video extrapolation.



### GANimation: Anatomically-aware Facial Animation from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1807.09251v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09251v2)
- **Published**: 2018-07-24 17:47:09+00:00
- **Updated**: 2018-08-28 23:46:23+00:00
- **Authors**: Albert Pumarola, Antonio Agudo, Aleix M. Martinez, Alberto Sanfeliu, Francesc Moreno-Noguer
- **Comment**: Accepted as oral at ECCV 2018. Code available at
  https://github.com/albertpumarola/GANimation. Added minor updates
- **Journal**: None
- **Summary**: Recent advances in Generative Adversarial Networks (GANs) have shown impressive results for task of facial expression synthesis. The most successful architecture is StarGAN, that conditions GANs generation process with images of a specific domain, namely a set of images of persons sharing the same expression. While effective, this approach can only generate a discrete number of expressions, determined by the content of the dataset. To address this limitation, in this paper, we introduce a novel GAN conditioning scheme based on Action Units (AU) annotations, which describes in a continuous manifold the anatomical facial movements defining a human expression. Our approach allows controlling the magnitude of activation of each AU and combine several of them. Additionally, we propose a fully unsupervised strategy to train the model, that only requires images annotated with their activated AUs, and exploit attention mechanisms that make our network robust to changing backgrounds and lighting conditions. Extensive evaluation show that our approach goes beyond competing conditional generators both in the capability to synthesize a much wider range of expressions ruled by anatomically feasible muscle movements, as in the capacity of dealing with images in the wild.



### Learning to Generate and Reconstruct 3D Meshes with only 2D Supervision
- **Arxiv ID**: http://arxiv.org/abs/1807.09259v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.09259v3)
- **Published**: 2018-07-24 17:54:51+00:00
- **Updated**: 2018-11-15 18:59:50+00:00
- **Authors**: Paul Henderson, Vittorio Ferrari
- **Comment**: BMVC 2018 (Oral). Differentiable renderer available at
  https://github.com/pmh47/dirt
- **Journal**: None
- **Summary**: We present a unified framework tackling two problems: class-specific 3D reconstruction from a single image, and generation of new 3D shape samples. These tasks have received considerable attention recently; however, existing approaches rely on 3D supervision, annotation of 2D images with keypoints or poses, and/or training with multiple views of each object instance. Our framework is very general: it can be trained in similar settings to these existing approaches, while also supporting weaker supervision scenarios. Importantly, it can be trained purely from 2D images, without ground-truth pose annotations, and with a single view per instance. We employ meshes as an output representation, instead of voxels used in most prior work. This allows us to exploit shading information during training, which previous 2D-supervised methods cannot. Thus, our method can learn to generate and reconstruct concave object classes. We evaluate our approach on synthetic data in various settings, showing that (i) it learns to disentangle shape from pose; (ii) using shading in the loss improves performance; (iii) our model is comparable or superior to state-of-the-art voxel-based approaches on quantitative metrics, while producing results that are visually more pleasing; (iv) it still performs well when given supervision weaker than in prior works.



### Ensemble of Multi-sized FCNs to Improve White Matter Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.09298v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09298v1)
- **Published**: 2018-07-24 18:29:20+00:00
- **Updated**: 2018-07-24 18:29:20+00:00
- **Authors**: Zhewei Wang, Charles D. Smith, Jundong Liu
- **Comment**: Accepted to MLMI 2018
- **Journal**: None
- **Summary**: In this paper, we develop a two-stage neural network solution for the challenging task of white-matter lesion segmentation. To cope with the vast vari- ability in lesion sizes, we sample brain MR scans with patches at three differ- ent dimensions and feed them into separate fully convolutional neural networks (FCNs). In the second stage, we process large and small lesion separately, and use ensemble-nets to combine the segmentation results generated from the FCNs. A novel activation function is adopted in the ensemble-nets to improve the segmen- tation accuracy measured by Dice Similarity Coefficient. Experiments on MICCAI 2017 White Matter Hyperintensities (WMH) Segmentation Challenge data demonstrate that our two-stage-multi-sized FCN approach, as well as the new activation function, are effective in capturing white-matter lesions in MR images.



### User Loss -- A Forced-Choice-Inspired Approach to Train Neural Networks directly by User Interaction
- **Arxiv ID**: http://arxiv.org/abs/1807.09303v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09303v2)
- **Published**: 2018-07-24 18:43:36+00:00
- **Updated**: 2018-12-21 13:39:21+00:00
- **Authors**: Shahab Zarei, Bernhard Stimpel, Christopher Syben, Andreas Maier
- **Comment**: Accepted on BVM 2019; Extended ArXiv Version with additional figures
  and details
- **Journal**: None
- **Summary**: In this paper, we investigate whether is it possible to train a neural network directly from user inputs. We consider this approach to be highly relevant for applications in which the point of optimality is not well-defined and user-dependent. Our application is medical image denoising which is essential in fluoroscopy imaging. In this field every user, i.e. physician, has a different flavor and image quality needs to be tailored towards each individual.   To address this important problem, we propose to construct a loss function derived from a forced-choice experiment. In order to make the learning problem feasible, we operate in the domain of precision learning, i.e., we inspire the network architecture by traditional signal processing methods in order to reduce the number of trainable parameters. The algorithm that was used for this is a Laplacian pyramid with only six trainable parameters.   In the experimental results, we demonstrate that two image experts who prefer different filter characteristics between sharpness and de-noising can be created using our approach. Also models trained for a specific user perform best on this users test data. This approach opens the way towards implementation of direct user feedback in deep learning and is applicable for a wide range of application.



### Encoderless Gimbal Calibration of Dynamic Multi-Camera Clusters
- **Arxiv ID**: http://arxiv.org/abs/1807.09304v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09304v1)
- **Published**: 2018-07-24 18:45:52+00:00
- **Updated**: 2018-07-24 18:45:52+00:00
- **Authors**: Christopher L. Choi, Jason Rebello, Leonid Koppel, Pranav Ganti, Arun Das, Steven L. Waslander
- **Comment**: ICRA 2018
- **Journal**: None
- **Summary**: Dynamic Camera Clusters (DCCs) are multi-camera systems where one or more cameras are mounted on actuated mechanisms such as a gimbal. Existing methods for DCC calibration rely on joint angle measurements to resolve the time-varying transformation between the dynamic and static camera. This information is usually provided by motor encoders, however, joint angle measurements are not always readily available on off-the-shelf mechanisms. In this paper, we present an encoderless approach for DCC calibration which simultaneously estimates the kinematic parameters of the transformation chain as well as the unknown joint angles. We also demonstrate the integration of an encoderless gimbal mechanism with a state-of-the art VIO algorithm, and show the extensions required in order to perform simultaneous online estimation of the joint angles and vehicle localization state. The proposed calibration approach is validated both in simulation and on a physical DCC composed of a 2-DOF gimbal mounted on a UAV. Finally, we show the experimental results of the calibrated mechanism integrated into the OKVIS VIO package, and demonstrate successful online joint angle estimation while maintaining localization accuracy that is comparable to a standard static multi-camera configuration.



### A Simple Probabilistic Model for Uncertainty Estimation
- **Arxiv ID**: http://arxiv.org/abs/1807.09312v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.09312v1)
- **Published**: 2018-07-24 19:14:29+00:00
- **Updated**: 2018-07-24 19:14:29+00:00
- **Authors**: Alexander Kuvaev, Roman Khudorozhkov
- **Comment**: None
- **Journal**: None
- **Summary**: The article focuses on determining the predictive uncertainty of a model on the example of atrial fibrillation detection problem by a single-lead ECG signal. To this end, the model predicts parameters of the beta distribution over class probabilities instead of these probabilities themselves. It was shown that the described approach allows to detect atypical recordings and significantly improve the quality of the algorithm on confident predictions.



### Handwritten Digit Recognition by Elastic Matching
- **Arxiv ID**: http://arxiv.org/abs/1807.09324v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09324v1)
- **Published**: 2018-07-24 19:59:47+00:00
- **Updated**: 2018-07-24 19:59:47+00:00
- **Authors**: Sagnik Majumder, C. von der Malsburg, Aashish Richhariya, Surekha Bhanot
- **Comment**: 8 pages, 1 figure, 1 table, journal
- **Journal**: None
- **Summary**: A simple model of MNIST handwritten digit recognition is presented here. The model is an adaptation of a previous theory of face recognition. It realizes translation and rotation invariance in a principled way instead of being based on extensive learning from large masses of sample data. The presented recognition rates fall short of other publications, but due to its inspectability and conceptual and numerical simplicity, our system commends itself as a basis for further development.



### Learning Plannable Representations with Causal InfoGAN
- **Arxiv ID**: http://arxiv.org/abs/1807.09341v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.09341v1)
- **Published**: 2018-07-24 20:46:05+00:00
- **Updated**: 2018-07-24 20:46:05+00:00
- **Authors**: Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart Russell, Pieter Abbeel
- **Comment**: ICML / IJCAI / AAMAS 2018 Workshop on Planning and Learning (PAL-18)
- **Journal**: None
- **Summary**: In recent years, deep generative models have been shown to 'imagine' convincing high-dimensional observations such as images, audio, and even video, learning directly from raw data. In this work, we ask how to imagine goal-directed visual plans -- a plausible sequence of observations that transition a dynamical system from its current configuration to a desired goal state, which can later be used as a reference trajectory for control. We focus on systems with high-dimensional observations, such as images, and propose an approach that naturally combines representation learning and planning. Our framework learns a generative model of sequential observations, where the generative process is induced by a transition in a low-dimensional planning model, and an additional noise. By maximizing the mutual information between the generated observations and the transition in the planning model, we obtain a low-dimensional representation that best explains the causal nature of the data. We structure the planning model to be compatible with efficient planning algorithms, and we propose several such models based on either discrete or continuous states. Finally, to generate a visual plan, we project the current and goal observations onto their respective states in the planning model, plan a trajectory, and then use the generative model to transform the trajectory to a sequence of observations. We demonstrate our method on imagining plausible visual plans of rope manipulation.



### A Synchronized Stereo and Plenoptic Visual Odometry Dataset
- **Arxiv ID**: http://arxiv.org/abs/1807.09372v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09372v2)
- **Published**: 2018-07-24 21:59:01+00:00
- **Updated**: 2018-08-26 20:43:35+00:00
- **Authors**: Niclas Zeller, Franz Quint, Uwe Stilla
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new dataset to evaluate monocular, stereo, and plenoptic camera based visual odometry algorithms. The dataset comprises a set of synchronized image sequences recorded by a micro lens array (MLA) based plenoptic camera and a stereo camera system. For this, the stereo cameras and the plenoptic camera were assembled on a common hand-held platform. All sequences are recorded in a very large loop, where beginning and end show the same scene. Therefore, the tracking accuracy of a visual odometry algorithm can be measured from the drift between beginning and end of the sequence. For both, the plenoptic camera and the stereo system, we supply full intrinsic camera models, as well as vignetting data. The dataset consists of 11 sequences which were recorded in challenging indoor and outdoor scenarios. We present, by way of example, the results achieved by state-of-the-art algorithms.



### Contrastive Video Representation Learning via Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/1807.09380v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09380v3)
- **Published**: 2018-07-24 22:46:42+00:00
- **Updated**: 2020-04-16 00:03:53+00:00
- **Authors**: Jue Wang, Anoop Cherian
- **Comment**: Revised version of ECCV 2018 Paper: Learning Discriminative Video
  Representations Using Adversarial Perturbations
- **Journal**: None
- **Summary**: Adversarial perturbations are noise-like patterns that can subtly change the data, while failing an otherwise accurate classifier. In this paper, we propose to use such perturbations within a novel contrastive learning setup to build negative samples, which are then used to produce improved video representations. To this end, given a well-trained deep model for per-frame video recognition, we first generate adversarial noise adapted to this model. Positive and negative bags are produced using the original data features from the full video sequence and their perturbed counterparts, respectively. Unlike the classic contrastive learning methods, we develop a binary classification problem that learns a set of discriminative hyperplanes -- as a subspace -- that will separate the two bags from each other. This subspace is then used as a descriptor for the video, dubbed \emph{discriminative subspace pooling}. As the perturbed features belong to data classes that are likely to be confused with the original features, the discriminative subspace will characterize parts of the feature space that are more representative of the original data, and thus may provide robust video representations. To learn such descriptors, we formulate a subspace learning objective on the Stiefel manifold and resort to Riemannian optimization methods for solving it efficiently. We provide experiments on several video datasets and demonstrate state-of-the-art results.



### Domain Stylization: A Strong, Simple Baseline for Synthetic to Real Image Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1807.09384v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.09384v1)
- **Published**: 2018-07-24 23:06:49+00:00
- **Updated**: 2018-07-24 23:06:49+00:00
- **Authors**: Aysegul Dundar, Ming-Yu Liu, Ting-Chun Wang, John Zedlewski, Jan Kautz
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have largely failed to effectively utilize synthetic data when applied to real images due to the covariate shift problem. In this paper, we show that by applying a straightforward modification to an existing photorealistic style transfer algorithm, we achieve state-of-the-art synthetic-to-real domain adaptation results. We conduct extensive experimental validations on four synthetic-to-real tasks for semantic segmentation and object detection, and show that our approach exceeds the performance of any current state-of-the-art GAN-based image translation approach as measured by segmentation and object detection metrics. Furthermore we offer a distance based analysis of our method which shows a dramatic reduction in Frechet Inception distance between the source and target domains, offering a quantitative metric that demonstrates the effectiveness of our algorithm in bridging the synthetic-to-real gap.



### LAPRAN: A Scalable Laplacian Pyramid Reconstructive Adversarial Network for Flexible Compressive Sensing Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1807.09388v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.09388v3)
- **Published**: 2018-07-24 23:28:17+00:00
- **Updated**: 2018-11-03 18:36:57+00:00
- **Authors**: Kai Xu, Zhikang Zhang, Fengbo Ren
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: This paper addresses the single-image compressive sensing (CS) and reconstruction problem. We propose a scalable Laplacian pyramid reconstructive adversarial network (LAPRAN) that enables high-fidelity, flexible and fast CS images reconstruction. LAPRAN progressively reconstructs an image following the concept of Laplacian pyramid through multiple stages of reconstructive adversarial networks (RANs). At each pyramid level, CS measurements are fused with a contextual latent vector to generate a high-frequency image residual. Consequently, LAPRAN can produce hierarchies of reconstructed images and each with an incremental resolution and improved quality. The scalable pyramid structure of LAPRAN enables high-fidelity CS reconstruction with a flexible resolution that is adaptive to a wide range of compression ratios (CRs), which is infeasible with existing methods. Experimental results on multiple public datasets show that LAPRAN offers an average 7.47dB and 5.98dB PSNR, and an average 57.93% and 33.20% SSIM improvement compared to model-based and data-driven baselines, respectively.



