# Arxiv Papers in cs.CV on 2018-07-31
### MnasNet: Platform-Aware Neural Architecture Search for Mobile
- **Arxiv ID**: http://arxiv.org/abs/1807.11626v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.11626v3)
- **Published**: 2018-07-31 01:34:21+00:00
- **Updated**: 2019-05-29 01:30:05+00:00
- **Authors**: Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le
- **Comment**: Published in CVPR 2019
- **Journal**: CVPR 2019
- **Summary**: Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than NASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet



### DFTerNet: Towards 2-bit Dynamic Fusion Networks for Accurate Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1808.04228v2
- **DOI**: 10.1109/ACCESS.2018.2873315
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.HC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.04228v2)
- **Published**: 2018-07-31 02:33:34+00:00
- **Updated**: 2018-09-29 04:47:24+00:00
- **Authors**: Zhan Yang, Osolo Ian Raymond, ChengYuan Zhang, Ying Wan, Jun Long
- **Comment**: 19 pages, 5 figures, 6 tables, accepted by IEEE Access
- **Journal**: IEEE ACCESS, vol. 6, pp. 56750-56764, 2018
- **Summary**: Deep Convolutional Neural Networks (DCNNs) are currently popular in human activity recognition applications. However, in the face of modern artificial intelligence sensor-based games, many research achievements cannot be practically applied on portable devices. DCNNs are typically resource-intensive and too large to be deployed on portable devices, thus this limits the practical application of complex activity detection. In addition, since portable devices do not possess high-performance Graphic Processing Units (GPUs), there is hardly any improvement in Action Game (ACT) experience. Besides, in order to deal with multi-sensor collaboration, all previous human activity recognition models typically treated the representations from different sensor signal sources equally. However, distinct types of activities should adopt different fusion strategies. In this paper, a novel scheme is proposed. This scheme is used to train 2-bit Convolutional Neural Networks with weights and activations constrained to {-0.5,0,0.5}. It takes into account the correlation between different sensor signal sources and the activity types. This model, which we refer to as DFTerNet, aims at producing a more reliable inference and better trade-offs for practical applications. Our basic idea is to exploit quantization of weights and activations directly in pre-trained filter banks and adopt dynamic fusion strategies for different activity types. Experiments demonstrate that by using dynamic fusion strategy can exceed the baseline model performance by up to ~5% on activity recognition like OPPORTUNITY and PAMAP2 datasets. Using the quantization method proposed, we were able to achieve performances closer to that of full-precision counterpart. These results were also verified using the UniMiB-SHAR dataset. In addition, the proposed method can achieve ~9x acceleration on CPUs and ~11x memory saving.



### Deep Graph Laplacian Regularization for Robust Denoising of Real Images
- **Arxiv ID**: http://arxiv.org/abs/1807.11637v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11637v3)
- **Published**: 2018-07-31 02:44:34+00:00
- **Updated**: 2019-05-03 12:41:25+00:00
- **Authors**: Jin Zeng, Jiahao Pang, Wenxiu Sun, Gene Cheung
- **Comment**: None
- **Journal**: None
- **Summary**: Recent developments in deep learning have revolutionized the paradigm of image restoration. However, its applications on real image denoising are still limited, due to its sensitivity to training data and the complex nature of real image noise. In this work, we combine the robustness merit of model-based approaches and the learning power of data-driven approaches for real image denoising. Specifically, by integrating graph Laplacian regularization as a trainable module into a deep learning framework, we are less susceptible to overfitting than pure CNN-based approaches, achieving higher robustness to small datasets and cross-domain denoising. First, a sparse neighborhood graph is built from the output of a convolutional neural network (CNN). Then the image is restored by solving an unconstrained quadratic programming problem, using a corresponding graph Laplacian regularizer as a prior term. The proposed restoration pipeline is fully differentiable and hence can be end-to-end trained. Experimental results demonstrate that our work is less prone to overfitting given small training data. It is also endowed with strong cross-domain generalization power, outperforming the state-of-the-art approaches by a remarkable margin.



### Brain MRI Image Super Resolution using Phase Stretch Transform and Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.11643v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11643v1)
- **Published**: 2018-07-31 02:51:21+00:00
- **Updated**: 2018-07-31 02:51:21+00:00
- **Authors**: Sifeng He, Bahram Jalali
- **Comment**: None
- **Journal**: None
- **Summary**: A hallucination-free and computationally efficient algorithm for enhancing the resolution of brain MRI images is demonstrated.



### The Devil of Face Recognition is in the Noise
- **Arxiv ID**: http://arxiv.org/abs/1807.11649v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11649v1)
- **Published**: 2018-07-31 03:43:11+00:00
- **Updated**: 2018-07-31 03:43:11+00:00
- **Authors**: Fei Wang, Liren Chen, Cheng Li, Shiyao Huang, Yanjie Chen, Chen Qian, Chen Change Loy
- **Comment**: accepted to ECCV'18
- **Journal**: None
- **Summary**: The growing scale of face recognition datasets empowers us to train strong convolutional networks for face recognition. While a variety of architectures and loss functions have been devised, we still have a limited understanding of the source and consequence of label noise inherent in existing datasets. We make the following contributions: 1) We contribute cleaned subsets of popular face databases, i.e., MegaFace and MS-Celeb-1M datasets, and build a new large-scale noise-controlled IMDb-Face dataset. 2) With the original datasets and cleaned subsets, we profile and analyze label noise properties of MegaFace and MS-Celeb-1M. We show that a few orders more samples are needed to achieve the same accuracy yielded by a clean subset. 3) We study the association between different types of noise, i.e., label flips and outliers, with the accuracy of face recognition models. 4) We investigate ways to improve data cleanliness, including a comprehensive user study on the influence of data labeling strategies to annotation accuracy. The IMDb-Face dataset has been released on https://github.com/fwang91/IMDb-Face.



### Improving the Annotation of DeepFashion Images for Fine-grained Attribute Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.11674v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11674v1)
- **Published**: 2018-07-31 06:03:00+00:00
- **Updated**: 2018-07-31 06:03:00+00:00
- **Authors**: Roshanak Zakizadeh, Michele Sasdelli, Yu Qian, Eduard Vazquez
- **Comment**: None
- **Journal**: None
- **Summary**: DeepFashion is a widely used clothing dataset with 50 categories and more than overall 200k images where each image is annotated with fine-grained attributes. This dataset is often used for clothes recognition and although it provides comprehensive annotations, the attributes distribution is unbalanced and repetitive specially for training fine-grained attribute recognition models. In this work, we tailored DeepFashion for fine-grained attribute recognition task by focusing on each category separately. After selecting categories with sufficient number of images for training, we remove very scarce attributes and merge the duplicate ones in each category, then we clean the dataset based on the new list of attributes. We use a bilinear convolutional neural network with pairwise ranking loss function for multi-label fine-grained attribute recognition and show that the new annotations improve the results for such a task. The detailed annotations for each of the selected categories are provided for public use.



### Leveraging Unlabeled Whole-Slide-Images for Mitosis Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.11677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11677v1)
- **Published**: 2018-07-31 06:19:19+00:00
- **Updated**: 2018-07-31 06:19:19+00:00
- **Authors**: Saad Ullah Akram, Talha Qaiser, Simon Graham, Juho Kannala, Janne Heikkilä, Nasir Rajpoot
- **Comment**: Accepted for MICCAI COMPAY 2018 Workshop
- **Journal**: None
- **Summary**: Mitosis count is an important biomarker for prognosis of various cancers. At present, pathologists typically perform manual counting on a few selected regions of interest in breast whole-slide-images (WSIs) of patient biopsies. This task is very time-consuming, tedious and subjective. Automated mitosis detection methods have made great advances in recent years. However, these methods require exhaustive labeling of a large number of selected regions of interest. This task is very expensive because expert pathologists are needed for reliable and accurate annotations. In this paper, we present a semi-supervised mitosis detection method which is designed to leverage a large number of unlabeled breast cancer WSIs. As a result, our method capitalizes on the growing number of digitized histology images, without relying on exhaustive annotations, subsequently improving mitosis detection. Our method first learns a mitosis detector from labeled data, uses this detector to mine additional mitosis samples from unlabeled WSIs, and then trains the final model using this larger and diverse set of mitosis samples. The use of unlabeled data improves F1-score by $\sim$5\% compared to our best performing fully-supervised model on the TUPAC validation set. Our submission (single model) to TUPAC challenge ranks highly on the leaderboard with an F1-score of 0.64.



### Deep Cross Modal Learning for Caricature Verification and Identification(CaVINet)
- **Arxiv ID**: http://arxiv.org/abs/1807.11688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11688v1)
- **Published**: 2018-07-31 07:19:14+00:00
- **Updated**: 2018-07-31 07:19:14+00:00
- **Authors**: Jatin Garg, Skand Vishwanath Peri, Himanshu Tolani, Narayanan C Krishnan
- **Comment**: None
- **Journal**: None
- **Summary**: Learning from different modalities is a challenging task. In this paper, we look at the challenging problem of cross modal face verification and recognition between caricature and visual image modalities. Caricature have exaggerations of facial features of a person. Due to the significant variations in the caricatures, building vision models for recognizing and verifying data from this modality is an extremely challenging task. Visual images with significantly lesser amount of distortions can act as a bridge for the analysis of caricature modality. We introduce a publicly available large Caricature-VIsual dataset [CaVI] with images from both the modalities that captures the rich variations in the caricature of an identity. This paper presents the first cross modal architecture that handles extreme distortions of caricatures using a deep learning network that learns similar representations across the modalities. We use two convolutional networks along with transformations that are subjected to orthogonality constraints to capture the shared and modality specific representations. In contrast to prior research, our approach neither depends on manually extracted facial landmarks for learning the representations, nor on the identities of the person for performing verification. The learned shared representation achieves 91% accuracy for verifying unseen images and 75% accuracy on unseen identities. Further, recognizing the identity in the image by knowledge transfer using a combination of shared and modality specific representations, resulted in an unprecedented performance of 85% rank-1 accuracy for caricatures and 95% rank-1 accuracy for visual images.



### A recurrent multi-scale approach to RBG-D Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1808.01357v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1808.01357v3)
- **Published**: 2018-07-31 08:15:06+00:00
- **Updated**: 2018-09-05 16:50:09+00:00
- **Authors**: Mirco Planamente, Mohammad Reza Loghmani, Barbara Caputo
- **Comment**: Master thesis extracted from the paper arXiv:1806.01673 submitted to
  accv 2018
- **Journal**: None
- **Summary**: Technological development aims to produce generations of increasingly efficient robots able to perform complex tasks. This requires considerable efforts, from the scientific community, to find new algorithms that solve computer vision problems, such as object recognition. The diffusion of RGB-D cameras directed the study towards the research of new architectures able to exploit the RGB and Depth information. The project that is developed in this thesis concerns the realization of a new end-to-end architecture for the recognition of RGB-D objects called RCFusion. Our method generates compact and highly discriminative multi-modal features by combining complementary RGB and depth information representing different levels of abstraction. We evaluate our method on standard object recognition datasets, RGB-D Object Dataset and JHUIT-50. The experiments performed show that our method outperforms the existing approaches and establishes new state-of-the-art results for both datasets.



### SegStereo: Exploiting Semantic Information for Disparity Estimation
- **Arxiv ID**: http://arxiv.org/abs/1807.11699v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11699v1)
- **Published**: 2018-07-31 08:24:36+00:00
- **Updated**: 2018-07-31 08:24:36+00:00
- **Authors**: Guorun Yang, Hengshuang Zhao, Jianping Shi, Zhidong Deng, Jiaya Jia
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: Disparity estimation for binocular stereo images finds a wide range of applications. Traditional algorithms may fail on featureless regions, which could be handled by high-level clues such as semantic segments. In this paper, we suggest that appropriate incorporation of semantic cues can greatly rectify prediction in commonly-used disparity estimation frameworks. Our method conducts semantic feature embedding and regularizes semantic cues as the loss term to improve learning disparity. Our unified model SegStereo employs semantic features from segmentation and introduces semantic softmax loss, which helps improve the prediction accuracy of disparity maps. The semantic cues work well in both unsupervised and supervised manners. SegStereo achieves state-of-the-art results on KITTI Stereo benchmark and produces decent prediction on both CityScapes and FlyingThings3D datasets.



### Learning Collaborative Generation Correction Modules for Blind Image Deblurring and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1807.11706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11706v1)
- **Published**: 2018-07-31 08:55:11+00:00
- **Updated**: 2018-07-31 08:55:11+00:00
- **Authors**: Risheng Liu, Yi He, Shichao Cheng, Xin Fan, Zhongxuan Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Blind image deblurring plays a very important role in many vision and multimedia applications. Most existing works tend to introduce complex priors to estimate the sharp image structures for blur kernel estimation. However, it has been verified that directly optimizing these models is challenging and easy to fall into degenerate solutions. Although several experience-based heuristic inference strategies, including trained networks and designed iterations, have been developed, it is still hard to obtain theoretically guaranteed accurate solutions. In this work, a collaborative learning framework is established to address the above issues. Specifically, we first design two modules, named Generator and Corrector, to extract the intrinsic image structures from the data-driven and knowledge-based perspectives, respectively. By introducing a collaborative methodology to cascade these modules, we can strictly prove the convergence of our image propagations to a deblurring-related optimal solution. As a nontrivial byproduct, we also apply the proposed method to address other related tasks, such as image interpolation and edge-preserved smoothing. Plenty of experiments demonstrate that our method can outperform the state-of-the-art approaches on both synthetic and real datasets.



### Design Flow of Accelerating Hybrid Extremely Low Bit-width Neural Network in Embedded FPGA
- **Arxiv ID**: http://arxiv.org/abs/1808.04311v2
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.04311v2)
- **Published**: 2018-07-31 09:24:57+00:00
- **Updated**: 2018-10-25 18:16:49+00:00
- **Authors**: Junsong Wang, Qiuwen Lou, Xiaofan Zhang, Chao Zhu, Yonghua Lin, Deming Chen
- **Comment**: Accepted by International Conference on Field-Programmable Logic and
  Applications (FPL'2018)
- **Journal**: None
- **Summary**: Neural network accelerators with low latency and low energy consumption are desirable for edge computing. To create such accelerators, we propose a design flow for accelerating the extremely low bit-width neural network (ELB-NN) in embedded FPGAs with hybrid quantization schemes. This flow covers both network training and FPGA-based network deployment, which facilitates the design space exploration and simplifies the tradeoff between network accuracy and computation efficiency. Using this flow helps hardware designers to deliver a network accelerator in edge devices under strict resource and power constraints. We present the proposed flow by supporting hybrid ELB settings within a neural network. Results show that our design can deliver very high performance peaking at 10.3 TOPS and classify up to 325.3 image/s/watt while running large-scale neural networks for less than 5W using embedded FPGA. To the best of our knowledge, it is the most energy efficient solution in comparison to GPU or other FPGA implementations reported so far in the literature.



### A Two-Stream Mutual Attention Network for Semi-supervised Biomedical Segmentation with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1807.11719v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11719v3)
- **Published**: 2018-07-31 09:34:16+00:00
- **Updated**: 2018-12-26 13:21:52+00:00
- **Authors**: Shaobo Min, Xuejin Chen, Zheng-Jun Zha, Feng Wu, Yongdong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: \begin{abstract} Learning-based methods suffer from a deficiency of clean annotations, especially in biomedical segmentation. Although many semi-supervised methods have been proposed to provide extra training data, automatically generated labels are usually too noisy to retrain models effectively. In this paper, we propose a Two-Stream Mutual Attention Network (TSMAN) that weakens the influence of back-propagated gradients caused by incorrect labels, thereby rendering the network robust to unclean data. The proposed TSMAN consists of two sub-networks that are connected by three types of attention models in different layers. The target of each attention model is to indicate potentially incorrect gradients in a certain layer for both sub-networks by analyzing their inferred features using the same input. In order to achieve this purpose, the attention models are designed based on the propagation analysis of noisy gradients at different layers. This allows the attention models to effectively discover incorrect labels and weaken their influence during the parameter updating process. By exchanging multi-level features within the two-stream architecture, the effects of noisy labels in each sub-network are reduced by decreasing the updating gradients. Furthermore, a hierarchical distillation is developed to provide more reliable pseudo labels for unlabelded data, which further boosts the performance of our retrained TSMAN. The experiments using both the HVSMR 2016 and BRATS 2015 benchmarks demonstrate that our semi-supervised learning framework surpasses the state-of-the-art fully-supervised results.



### Regional Multi-scale Approach for Visually Pleasing Explanations of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.11720v2
- **DOI**: 10.1109/ACCESS.2019.2963055
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11720v2)
- **Published**: 2018-07-31 09:37:39+00:00
- **Updated**: 2018-08-01 09:01:35+00:00
- **Authors**: Dasom Seo, Kanghan Oh, Il-Seok Oh
- **Comment**: 9 pages, 5 figures, submitted on NIPS 2018
- **Journal**: None
- **Summary**: Recently, many methods to interpret and visualize deep neural network predictions have been proposed and significant progress has been made. However, a more class-discriminative and visually pleasing explanation is required. Thus, this paper proposes a region-based approach that estimates feature importance in terms of appropriately segmented regions. By fusing the saliency maps generated from multi-scale segmentations, a more class-discriminative and visually pleasing map is obtained. We incorporate this regional multi-scale concept into a prediction difference method that is model-agnostic. An input image is segmented in several scales using the super-pixel method, and exclusion of a region is simulated by sampling a normal distribution constructed using the boundary prior. The experimental results demonstrate that the regional multi-scale method produces much more class-discriminative and visually pleasing saliency maps.



### A Zero-Shot Framework for Sketch-based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1807.11724v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11724v1)
- **Published**: 2018-07-31 09:42:16+00:00
- **Updated**: 2018-07-31 09:42:16+00:00
- **Authors**: Sasi Kiran Yelamarthi, Shiva Krishna Reddy, Ashish Mishra, Anurag Mittal
- **Comment**: Accepted in ECCV 2018, Munich Germany
- **Journal**: None
- **Summary**: Sketch-based image retrieval (SBIR) is the task of retrieving images from a natural image database that correspond to a given hand-drawn sketch. Ideally, an SBIR model should learn to associate components in the sketch (say, feet, tail, etc.) with the corresponding components in the image having similar shape characteristics. However, current evaluation methods simply focus only on coarse-grained evaluation where the focus is on retrieving images which belong to the same class as the sketch but not necessarily having the same shape characteristics as in the sketch. As a result, existing methods simply learn to associate sketches with classes seen during training and hence fail to generalize to unseen classes. In this paper, we propose a new benchmark for zero-shot SBIR where the model is evaluated in novel classes that are not seen during training. We show through extensive experiments that existing models for SBIR that are trained in a discriminative setting learn only class specific mappings and fail to generalize to the proposed zero-shot setting. To circumvent this, we propose a generative approach for the SBIR task by proposing deep conditional generative models that take the sketch as an input and fill the missing information stochastically. Experiments on this new benchmark created from the "Sketchy" dataset, which is a large-scale database of sketch-photo pairs demonstrate that the performance of these generative models is significantly better than several state-of-the-art approaches in the proposed zero-shot framework of the coarse-grained SBIR task.



### Deep Visual Odometry Methods for Mobile Robots
- **Arxiv ID**: http://arxiv.org/abs/1807.11745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11745v1)
- **Published**: 2018-07-31 10:20:19+00:00
- **Updated**: 2018-07-31 10:20:19+00:00
- **Authors**: Jahanzaib Shabbir, Thomas Kruezer
- **Comment**: None
- **Journal**: None
- **Summary**: Technology has made navigation in 3D real time possible and this has made possible what seemed impossible. This paper explores the aspect of deep visual odometry methods for mobile robots. Visual odometry has been instrumental in making this navigation successful. Noticeable challenges in mobile robots including the inability to attain Simultaneous Localization and Mapping have been solved by visual odometry through its cameras which are suitable for human environments. More intuitive, precise and accurate detection have been made possible by visual odometry in mobile robots. Another challenge in the mobile robot world is the 3D map reconstruction for exploration. A dense map in mobile robots can facilitate for localization and more accurate findings.



### Deep Learning-Based Multiple Object Visual Tracking on Embedded System for IoT and Mobile Edge Computing Applications
- **Arxiv ID**: http://arxiv.org/abs/1808.01356v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.01356v1)
- **Published**: 2018-07-31 10:33:09+00:00
- **Updated**: 2018-07-31 10:33:09+00:00
- **Authors**: Beatriz Blanco-Filgueira, Daniel García-Lesta, Mauro Fernández-Sanjurjo, Víctor M. Brea, Paula López
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Compute and memory demands of state-of-the-art deep learning methods are still a shortcoming that must be addressed to make them useful at IoT end-nodes. In particular, recent results depict a hopeful prospect for image processing using Convolutional Neural Netwoks, CNNs, but the gap between software and hardware implementations is already considerable for IoT and mobile edge computing applications due to their high power consumption. This proposal performs low-power and real time deep learning-based multiple object visual tracking implemented on an NVIDIA Jetson TX2 development kit. It includes a camera and wireless connection capability and it is battery powered for mobile and outdoor applications. A collection of representative sequences captured with the on-board camera, dETRUSC video dataset, is used to exemplify the performance of the proposed algorithm and to facilitate benchmarking. The results in terms of power consumption and frame rate demonstrate the feasibility of deep learning algorithms on embedded platforms although more effort to joint algorithm and hardware design of CNNs is needed.



### Remote sensing image regression for heterogeneous change detection
- **Arxiv ID**: http://arxiv.org/abs/1807.11766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11766v1)
- **Published**: 2018-07-31 11:28:52+00:00
- **Updated**: 2018-07-31 11:28:52+00:00
- **Authors**: Luigi T. Luppino, Filippo M. Bianchi, Gabriele Moser, Stian N. Anfinsen
- **Comment**: Accepted to Machine Learning for Signal Processing 2018
- **Journal**: None
- **Summary**: Change detection in heterogeneous multitemporal satellite images is an emerging topic in remote sensing. In this paper we propose a framework, based on image regression, to perform change detection in heterogeneous multitemporal satellite images, which has become a main topic in remote sensing. Our method learns a transformation to map the first image to the domain of the other image, and vice versa. Four regression methods are selected to carry out the transformation: Gaussian processes, support vector machines, random forests, and a recently proposed kernel regression method called homogeneous pixel transformation. To evaluate not only potentials and limitations of our framework, but also the pros and cons of each regression method, we perform experiments on two data sets. The results indicates that random forests achieve good performance, are fast and robust to hyperparameters, whereas the homogeneous pixel transformation method can achieve better accuracy at the cost of a higher complexity.



### Scale equivariance in CNNs with vector fields
- **Arxiv ID**: http://arxiv.org/abs/1807.11783v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11783v1)
- **Published**: 2018-07-31 12:14:45+00:00
- **Updated**: 2018-07-31 12:14:45+00:00
- **Authors**: Diego Marcos, Benjamin Kellenberger, Sylvain Lobry, Devis Tuia
- **Comment**: ICML/FAIM 2018 workshop on Towards learning with limited labels:
  Equivariance, Invariance, and Beyond (oral presentation)
- **Journal**: None
- **Summary**: We study the effect of injecting local scale equivariance into Convolutional Neural Networks. This is done by applying each convolutional filter at multiple scales. The output is a vector field encoding for the maximally activating scale and the scale itself, which is further processed by the following convolutional layers. This allows all the intermediate representations to be locally scale equivariant. We show that this improves the performance of the model by over $20\%$ in the scale equivariant task of regressing the scaling factor applied to randomly scaled MNIST digits. Furthermore, we find it also useful for scale invariant tasks, such as the actual classification of randomly scaled digits. This highlights the usefulness of allowing for a compact representation that can also learn relationships between different local scales by keeping internal scale equivariance.



### Attention is All We Need: Nailing Down Object-centric Attention for Egocentric Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1807.11794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11794v1)
- **Published**: 2018-07-31 12:54:06+00:00
- **Updated**: 2018-07-31 12:54:06+00:00
- **Authors**: Swathikiran Sudhakaran, Oswald Lanz
- **Comment**: Accepted to BMVC 2018
- **Journal**: None
- **Summary**: In this paper we propose an end-to-end trainable deep neural network model for egocentric activity recognition. Our model is built on the observation that egocentric activities are highly characterized by the objects and their locations in the video. Based on this, we develop a spatial attention mechanism that enables the network to attend to regions containing objects that are correlated with the activity under consideration. We learn highly specialized attention maps for each frame using class-specific activations from a CNN pre-trained for generic image recognition, and use them for spatio-temporal encoding of the video with a convolutional LSTM. Our model is trained in a weakly supervised setting using raw video-level activity-class labels. Nonetheless, on standard egocentric activity benchmarks our model surpasses by up to +6% points recognition accuracy the currently best performing method that leverages hand segmentation and object location strong supervision for training. We visually analyze attention maps generated by the network, revealing that the network successfully identifies the relevant objects present in the video frames which may explain the strong recognition performance. We also discuss an extensive ablation analysis regarding the design choices.



### Disaster Monitoring using Unmanned Aerial Vehicles and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.11805v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.11805v2)
- **Published**: 2018-07-31 13:24:31+00:00
- **Updated**: 2018-08-08 09:29:37+00:00
- **Authors**: Andreas Kamilaris, Francesc X. Prenafeta-Boldú
- **Comment**: Disaster Management for Resilience and Public Safety Workshop, Proc.
  of EnviroInfo 2017
- **Journal**: None
- **Summary**: Monitoring of disasters is crucial for mitigating their effects on the environment and human population, and can be facilitated by the use of unmanned aerial vehicles (UAV), equipped with camera sensors that produce aerial photos of the areas of interest. A modern technique for recognition of events based on aerial photos is deep learning. In this paper, we present the state of the art work related to the use of deep learning techniques for disaster identification. We demonstrate the potential of this technique in identifying disasters with high accuracy, by means of a relatively simple deep learning model. Based on a dataset of 544 images (containing disaster images such as fires, earthquakes, collapsed buildings, tsunami and flooding, as well as non-disaster scenes), our results show an accuracy of 91% achieved, indicating that deep learning, combined with UAV equipped with camera sensors, have the potential to predict disasters with high accuracy.



### Deep learning in agriculture: A survey
- **Arxiv ID**: http://arxiv.org/abs/1807.11809v1
- **DOI**: 10.1016/j.compag.2018.02.016
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.11809v1)
- **Published**: 2018-07-31 13:30:03+00:00
- **Updated**: 2018-07-31 13:30:03+00:00
- **Authors**: Andreas Kamilaris, Francesc X. Prenafeta-Boldu
- **Comment**: None
- **Journal**: Computers and Electronics in Agriculture International Journal,
  2018
- **Summary**: Deep learning constitutes a recent, modern technique for image processing and data analysis, with promising results and large potential. As deep learning has been successfully applied in various domains, it has recently entered also the domain of agriculture. In this paper, we perform a survey of 40 research efforts that employ deep learning techniques, applied to various agricultural and food production challenges. We examine the particular agricultural problems under study, the specific models and frameworks employed, the sources, nature and pre-processing of data used, and the overall performance achieved according to the metrics used at each work under study. Moreover, we study comparisons of deep learning with other existing popular techniques, in respect to differences in classification or regression performance. Our findings indicate that deep learning provides high accuracy, outperforming existing commonly used image processing techniques.



### Joint Learning of Intrinsic Images and Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.11857v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11857v1)
- **Published**: 2018-07-31 15:08:25+00:00
- **Updated**: 2018-07-31 15:08:25+00:00
- **Authors**: Anil S. Baslamisli, Thomas T. Groenestege, Partha Das, Hoang-An Le, Sezer Karaoglu, Theo Gevers
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Semantic segmentation of outdoor scenes is problematic when there are variations in imaging conditions. It is known that albedo (reflectance) is invariant to all kinds of illumination effects. Thus, using reflectance images for semantic segmentation task can be favorable. Additionally, not only segmentation may benefit from reflectance, but also segmentation may be useful for reflectance computation. Therefore, in this paper, the tasks of semantic segmentation and intrinsic image decomposition are considered as a combined process by exploring their mutual relationship in a joint fashion. To that end, we propose a supervised end-to-end CNN architecture to jointly learn intrinsic image decomposition and semantic segmentation. We analyze the gains of addressing those two problems jointly. Moreover, new cascade CNN architectures for intrinsic-for-segmentation and segmentation-for-intrinsic are proposed as single tasks. Furthermore, a dataset of 35K synthetic images of natural environments is created with corresponding albedo and shading (intrinsics), as well as semantic labels (segmentation) assigned to each object/scene. The experiments show that joint learning of intrinsic image decomposition and semantic segmentation is beneficial for both tasks for natural scenes. Dataset and models are available at: https://ivi.fnwi.uva.nl/cv/intrinseg



### Deep Dual Pyramid Network for Barcode Segmentation using Barcode-30k Database
- **Arxiv ID**: http://arxiv.org/abs/1807.11886v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11886v1)
- **Published**: 2018-07-31 15:59:11+00:00
- **Updated**: 2018-07-31 15:59:11+00:00
- **Authors**: Qijie Zhao, Feng Ni, Yang Song, Yongtao Wang, Zhi Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Digital signs(such as barcode or QR code) are widely used in our daily life, and for many applications, we need to localize them on images. However, difficult cases such as targets with small scales, half-occlusion, shape deformation and large illumination changes cause challenges for conventional methods. In this paper, we address this problem by producing a large-scale dataset and adopting a deep learning based semantic segmentation approach. Specifically, a synthesizing method was proposed to generate well-annotated images containing barcode and QR code labels, which contributes to largely decrease the annotation time. Through the synthesis strategy, we introduce a dataset that contains 30000 images with Barcode and QR code - Barcode-30k. Moreover, we further propose a dual pyramid structure based segmentation network - BarcodeNet, which is mainly formed with two novel modules, Prior Pyramid Pooling Module(P3M) and Pyramid Refine Module(PRM). We validate the effectiveness of BarcodeNet on the proposed synthetic dataset, and it yields the result of mIoU accuracy 95.36\% on validation set. Additional segmentation results of real images have shown that accurate segmentation performance is achieved.



### Deep End-to-end Fingerprint Denoising and Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1807.11888v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11888v3)
- **Published**: 2018-07-31 16:01:23+00:00
- **Updated**: 2018-09-13 14:28:28+00:00
- **Authors**: Youness Mansar
- **Comment**: Winning solution to the Chalearn LAP In-painting Competition Track 3
  / Accepted in the 2018 Chalearn Looking at People Satellite Workshop ECCV
- **Journal**: None
- **Summary**: This work describes our winning solution for the Chalearn LAP In-painting Competition Track 3 - Fingerprint Denoising and In-painting. The objective of this competition is to reduce noise, remove the background pattern and replace missing parts of fingerprint images in order to simplify the verification made by humans or third-party software. In this paper, we use a U-Net like CNN model that performs all those steps end-to-end after being trained on the competition data in a fully supervised way. This architecture and training procedure achieved the best results on all three metrics of the competition.



### End-to-End Physics Event Classification with CMS Open Data: Applying Image-Based Deep Learning to Detector Data for the Direct Classification of Collision Events at the LHC
- **Arxiv ID**: http://arxiv.org/abs/1807.11916v3
- **DOI**: 10.1007/s41781-020-00038-8
- **Categories**: **physics.data-an**, cs.CV, cs.LG, hep-ex
- **Links**: [PDF](http://arxiv.org/pdf/1807.11916v3)
- **Published**: 2018-07-31 16:52:07+00:00
- **Updated**: 2020-10-23 23:14:06+00:00
- **Authors**: Michael Andrews, Manfred Paulini, Sergei Gleyzer, Barnabas Poczos
- **Comment**: 14 pages, 5 figures; v3: published version
- **Journal**: Comput Softw Big Sci 4, 6 (2020)
- **Summary**: This paper describes the construction of novel end-to-end image-based classifiers that directly leverage low-level simulated detector data to discriminate signal and background processes in pp collision events at the Large Hadron Collider at CERN. To better understand what end-to-end classifiers are capable of learning from the data and to address a number of associated challenges, we distinguish the decay of the standard model Higgs boson into two photons from its leading background sources using high-fidelity simulated CMS Open Data. We demonstrate the ability of end-to-end classifiers to learn from the angular distribution of the photons recorded as electromagnetic showers, their intrinsic shapes, and the energy of their constituent hits, even when the underlying particles are not fully resolved, delivering a clear advantage in such cases over purely kinematics-based classifiers.



### What am I Searching for: Zero-shot Target Identity Inference in Visual Search
- **Arxiv ID**: http://arxiv.org/abs/1807.11926v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1807.11926v2)
- **Published**: 2018-07-31 17:15:11+00:00
- **Updated**: 2020-06-02 01:17:22+00:00
- **Authors**: Mengmi Zhang, Gabriel Kreiman
- **Comment**: Accepted for presentation at EPIC@CVPR2020 workshop
- **Journal**: None
- **Summary**: Can we infer intentions from a person's actions? As an example problem, here we consider how to decipher what a person is searching for by decoding their eye movement behavior. We conducted two psychophysics experiments where we monitored eye movements while subjects searched for a target object. We defined the fixations falling on non-target objects as "error fixations". Using those error fixations, we developed a model (InferNet) to infer what the target was. InferNet uses a pre-trained convolutional neural network to extract features from the error fixations and computes a similarity map between the error fixations and all locations across the search image. The model consolidates the similarity maps across layers and integrates these maps across all error fixations. InferNet successfully identifies the subject's goal and outperforms competitive null models, even without any object-specific training on the inference task.



### Egocentric Spatial Memory
- **Arxiv ID**: http://arxiv.org/abs/1807.11929v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.11929v1)
- **Published**: 2018-07-31 17:27:19+00:00
- **Updated**: 2018-07-31 17:27:19+00:00
- **Authors**: Mengmi Zhang, Keng Teck Ma, Shih-Cheng Yen, Joo Hwee Lim, Qi Zhao, Jiashi Feng
- **Comment**: 8 pages, 6 figures, accepted in IROS 2018
- **Journal**: None
- **Summary**: Egocentric spatial memory (ESM) defines a memory system with encoding, storing, recognizing and recalling the spatial information about the environment from an egocentric perspective. We introduce an integrated deep neural network architecture for modeling ESM. It learns to estimate the occupancy state of the world and progressively construct top-down 2D global maps from egocentric views in a spatially extended environment. During the exploration, our proposed ESM model updates belief of the global map based on local observations using a recurrent neural network. It also augments the local mapping with a novel external memory to encode and store latent representations of the visited places over long-term exploration in large environments which enables agents to perform place recognition and hence, loop closure. Our proposed ESM network contributes in the following aspects: (1) without feature engineering, our model predicts free space based on egocentric views efficiently in an end-to-end manner; (2) different from other deep learning-based mapping system, ESMN deals with continuous actions and states which is vitally important for robotic control in real applications. In the experiments, we demonstrate its accurate and robust global mapping capacities in 3D virtual mazes and realistic indoor environments by comparing with several competitive baselines.



### Gender Privacy: An Ensemble of Semi Adversarial Networks for Confounding Arbitrary Gender Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1807.11936v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.11936v1)
- **Published**: 2018-07-31 17:53:07+00:00
- **Updated**: 2018-07-31 17:53:07+00:00
- **Authors**: Vahid Mirjalili, Sebastian Raschka, Arun Ross
- **Comment**: Published in Proc. of IEEE 9th International Conference on
  Biometrics: Theory, Applications and Systems (BTAS), (Los Angeles, CA),
  October 2018
- **Journal**: None
- **Summary**: Recent research has proposed the use of Semi Adversarial Networks (SAN) for imparting privacy to face images. SANs are convolutional autoencoders that perturb face images such that the perturbed images cannot be reliably used by an attribute classifier (e.g., a gender classifier) but can still be used by a face matcher for matching purposes. However, the generalizability of SANs across multiple arbitrary gender classifiers has not been demonstrated in the literature. In this work, we tackle the generalization issue by designing an ensemble SAN model that generates a diverse set of perturbed outputs for a given input face image. This is accomplished by enforcing diversity among the individual models in the ensemble through the use of different data augmentation techniques. The goal is to ensure that at least one of the perturbed output faces will confound an arbitrary, previously unseen gender classifier. Extensive experiments using different unseen gender classifiers and face matchers are performed to demonstrate the efficacy of the proposed paradigm in imparting gender privacy to face images.



### Analyzing Human-Human Interactions: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1808.00022v2
- **DOI**: 10.1016/j.cviu.2019.102799
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1808.00022v2)
- **Published**: 2018-07-31 18:37:41+00:00
- **Updated**: 2019-08-17 09:24:43+00:00
- **Authors**: Alexandros Stergiou, Ronald Poppe
- **Comment**: None
- **Journal**: None
- **Summary**: Many videos depict people, and it is their interactions that inform us of their activities, relation to one another and the cultural and social setting. With advances in human action recognition, researchers have begun to address the automated recognition of these human-human interactions from video. The main challenges stem from dealing with the considerable variation in recording setting, the appearance of the people depicted and the coordinated performance of their interaction. This survey provides a summary of these challenges and datasets to address these, followed by an in-depth discussion of relevant vision-based recognition and detection methods. We focus on recent, promising work based on deep learning and convolutional neural networks (CNNs). Finally, we outline directions to overcome the limitations of the current state-of-the-art to analyze and, eventually, understand social human actions.



### ID Preserving Generative Adversarial Network for Partial Latent Fingerprint Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1808.00035v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00035v1)
- **Published**: 2018-07-31 19:20:49+00:00
- **Updated**: 2018-07-31 19:20:49+00:00
- **Authors**: Ali Dabouei, Sobhan Soleymani, Hadi Kazemi, Seyed Mehdi Iranmanesh, Jeremy Dawson, Nasser M. Nasrabadi
- **Comment**: Accepted in BTAS 2018
- **Journal**: None
- **Summary**: Performing recognition tasks using latent fingerprint samples is often challenging for automated identification systems due to poor quality, distortion, and partially missing information from the input samples. We propose a direct latent fingerprint reconstruction model based on conditional generative adversarial networks (cGANs). Two modifications are applied to the cGAN to adapt it for the task of latent fingerprint reconstruction. First, the model is forced to generate three additional maps to the ridge map to ensure that the orientation and frequency information is considered in the generation process, and prevent the model from filling large missing areas and generating erroneous minutiae. Second, a perceptual ID preservation approach is developed to force the generator to preserve the ID information during the reconstruction process. Using a synthetically generated database of latent fingerprints, the deep network learns to predict missing information from the input latent samples. We evaluate the proposed method in combination with two different fingerprint matching algorithms on several publicly available latent fingerprint datasets. We achieved the rank-10 accuracy of 88.02\% on the IIIT-Delhi latent fingerprint database for the task of latent-to-latent matching and rank-50 accuracy of 70.89\% on the IIIT-Delhi MOLF database for the task of latent-to-sensor matching. Experimental results of matching reconstructed samples in both latent-to-sensor and latent-to-latent frameworks indicate that the proposed method significantly increases the matching accuracy of the fingerprint recognition systems for the latent samples.



### Prosodic-Enhanced Siamese Convolutional Neural Networks for Cross-Device Text-Independent Speaker Verification
- **Arxiv ID**: http://arxiv.org/abs/1808.01026v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1808.01026v1)
- **Published**: 2018-07-31 19:21:59+00:00
- **Updated**: 2018-07-31 19:21:59+00:00
- **Authors**: Sobhan Soleymani, Ali Dabouei, Seyed Mehdi Iranmanesh, Hadi Kazemi, Jeremy Dawson, Nasser M. Nasrabadi
- **Comment**: Accepted in 9th IEEE International Conference on Biometrics: Theory,
  Applications, and Systems (BTAS 2018)
- **Journal**: None
- **Summary**: In this paper a novel cross-device text-independent speaker verification architecture is proposed. Majority of the state-of-the-art deep architectures that are used for speaker verification tasks consider Mel-frequency cepstral coefficients. In contrast, our proposed Siamese convolutional neural network architecture uses Mel-frequency spectrogram coefficients to benefit from the dependency of the adjacent spectro-temporal features. Moreover, although spectro-temporal features have proved to be highly reliable in speaker verification models, they only represent some aspects of short-term acoustic level traits of the speaker's voice. However, the human voice consists of several linguistic levels such as acoustic, lexicon, prosody, and phonetics, that can be utilized in speaker verification models. To compensate for these inherited shortcomings in spectro-temporal features, we propose to enhance the proposed Siamese convolutional neural network architecture by deploying a multilayer perceptron network to incorporate the prosodic, jitter, and shimmer features. The proposed end-to-end verification architecture performs feature extraction and verification simultaneously. This proposed architecture displays significant improvement over classical signal processing approaches and deep algorithms for forensic cross-device speaker verification.



### The Unreasonable Effectiveness of Texture Transfer for Single Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1808.00043v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00043v1)
- **Published**: 2018-07-31 19:43:24+00:00
- **Updated**: 2018-07-31 19:43:24+00:00
- **Authors**: Muhammad Waleed Gondal, Bernhard Schölkopf, Michael Hirsch
- **Comment**: 19 pages, 14 figures
- **Journal**: None
- **Summary**: While implicit generative models such as GANs have shown impressive results in high quality image reconstruction and manipulation using a combination of various losses, we consider a simpler approach leading to surprisingly strong results. We show that texture loss alone allows the generation of perceptually high quality images. We provide a better understanding of texture constraining mechanism and develop a novel semantically guided texture constraining method for further improvement. Using a recently developed perceptual metric employing "deep features" and termed LPIPS, the method obtains state-of-the-art results. Moreover, we show that a texture representation of those deep features better capture the perceptual quality of an image than the original deep features. Using texture information, off-the-shelf deep classification networks (without training) perform as well as the best performing (tuned and calibrated) LPIPS metrics. The code is publicly available.



### Lip-Reading Driven Deep Learning Approach for Speech Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1808.00046v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS, I.4; I.5; I.2
- **Links**: [PDF](http://arxiv.org/pdf/1808.00046v1)
- **Published**: 2018-07-31 19:50:13+00:00
- **Updated**: 2018-07-31 19:50:13+00:00
- **Authors**: Ahsan Adeel, Mandar Gogate, Amir Hussain, William M. Whitmer
- **Comment**: 11 pages, 13 figures
- **Journal**: None
- **Summary**: This paper proposes a novel lip-reading driven deep learning framework for speech enhancement. The proposed approach leverages the complementary strengths of both deep learning and analytical acoustic modelling (filtering based approach) as compared to recently published, comparatively simpler benchmark approaches that rely only on deep learning. The proposed audio-visual (AV) speech enhancement framework operates at two levels. In the first level, a novel deep learning-based lip-reading regression model is employed. In the second level, lip-reading approximated clean-audio features are exploited, using an enhanced, visually-derived Wiener filter (EVWF), for the clean audio power spectrum estimation. Specifically, a stacked long-short-term memory (LSTM) based lip-reading regression model is designed for clean audio features estimation using only temporal visual features considering different number of prior visual frames. For clean speech spectrum estimation, a new filterbank-domain EVWF is formulated, which exploits estimated speech features. The proposed EVWF is compared with conventional Spectral Subtraction and Log-Minimum Mean-Square Error methods using both ideal AV mapping and LSTM driven AV mapping. The potential of the proposed speech enhancement framework is evaluated under different dynamic real-world commercially-motivated scenarios (e.g. cafe, public transport, pedestrian area) at different SNR levels (ranging from low to high SNRs) using benchmark Grid and ChiME3 corpora. For objective testing, perceptual evaluation of speech quality is used to evaluate the quality of restored speech. For subjective testing, the standard mean-opinion-score method is used with inferential statistics. Comparative simulation results demonstrate significant lip-reading and speech enhancement improvement in terms of both speech quality and speech intelligibility.



### Learning to See Forces: Surgical Force Prediction with RGB-Point Cloud Temporal Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1808.00057v1
- **DOI**: 10.1007/978-3-030-01201-4_14
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1808.00057v1)
- **Published**: 2018-07-31 20:04:54+00:00
- **Updated**: 2018-07-31 20:04:54+00:00
- **Authors**: Cong Gao, Xingtong Liu, Michael Peven, Mathias Unberath, Austin Reiter
- **Comment**: MICCAI 2018 workshop, CARE(Computer Assisted and Robotic Endoscopy)
- **Journal**: None
- **Summary**: Robotic surgery has been proven to offer clear advantages during surgical procedures, however, one of the major limitations is obtaining haptic feedback. Since it is often challenging to devise a hardware solution with accurate force feedback, we propose the use of "visual cues" to infer forces from tissue deformation. Endoscopic video is a passive sensor that is freely available, in the sense that any minimally-invasive procedure already utilizes it. To this end, we employ deep learning to infer forces from video as an attractive low-cost and accurate alternative to typically complex and expensive hardware solutions. First, we demonstrate our approach in a phantom setting using the da Vinci Surgical System affixed with an OptoForce sensor. Second, we then validate our method on an ex vivo liver organ. Our method results in a mean absolute error of 0.814 N in the ex vivo study, suggesting that it may be a promising alternative to hardware based surgical force feedback in endoscopic procedures.



### Deep Sketch-Photo Face Recognition Assisted by Facial Attributes
- **Arxiv ID**: http://arxiv.org/abs/1808.00059v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1808.00059v1)
- **Published**: 2018-07-31 20:10:01+00:00
- **Updated**: 2018-07-31 20:10:01+00:00
- **Authors**: Seyed Mehdi Iranmanesh, Hadi Kazemi, Sobhan Soleymani, Ali Dabouei, Nasser M. Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a deep coupled framework to address the problem of matching sketch image against a gallery of mugshots. Face sketches have the essential in- formation about the spatial topology and geometric details of faces while missing some important facial attributes such as ethnicity, hair, eye, and skin color. We propose a cou- pled deep neural network architecture which utilizes facial attributes in order to improve the sketch-photo recognition performance. The proposed Attribute-Assisted Deep Con- volutional Neural Network (AADCNN) method exploits the facial attributes and leverages the loss functions from the facial attributes identification and face verification tasks in order to learn rich discriminative features in a common em- bedding subspace. The facial attribute identification task increases the inter-personal variations by pushing apart the embedded features extracted from individuals with differ- ent facial attributes, while the verification task reduces the intra-personal variations by pulling together all the fea- tures that are related to one person. The learned discrim- inative features can be well generalized to new identities not seen in the training data. The proposed architecture is able to make full use of the sketch and complementary fa- cial attribute information to train a deep model compared to the conventional sketch-photo recognition methods. Exten- sive experiments are performed on composite (E-PRIP) and semi-forensic (IIIT-D semi-forensic) datasets. The results show the superiority of our method compared to the state- of-the-art models in sketch-photo recognition algorithms



### DNN driven Speaker Independent Audio-Visual Mask Estimation for Speech Separation
- **Arxiv ID**: http://arxiv.org/abs/1808.00060v1
- **DOI**: 10.21437/Interspeech.2018-2516
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS, I.5; I.4; I.2
- **Links**: [PDF](http://arxiv.org/pdf/1808.00060v1)
- **Published**: 2018-07-31 20:12:15+00:00
- **Updated**: 2018-07-31 20:12:15+00:00
- **Authors**: Mandar Gogate, Ahsan Adeel, Ricard Marxer, Jon Barker, Amir Hussain
- **Comment**: Accepted for Interspeech 2018, 5 pages, 4 figures
- **Journal**: None
- **Summary**: Human auditory cortex excels at selectively suppressing background noise to focus on a target speaker. The process of selective attention in the brain is known to contextually exploit the available audio and visual cues to better focus on target speaker while filtering out other noises. In this study, we propose a novel deep neural network (DNN) based audiovisual (AV) mask estimation model. The proposed AV mask estimation model contextually integrates the temporal dynamics of both audio and noise-immune visual features for improved mask estimation and speech separation. For optimal AV features extraction and ideal binary mask (IBM) estimation, a hybrid DNN architecture is exploited to leverages the complementary strengths of a stacked long short term memory (LSTM) and convolution LSTM network. The comparative simulation results in terms of speech quality and intelligibility demonstrate significant performance improvement of our proposed AV mask estimation model as compared to audio-only and visual-only mask estimation approaches for both speaker dependent and independent scenarios.



