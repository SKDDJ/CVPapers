# Arxiv Papers in cs.CV on 2018-07-07
### Synthetic Sampling for Multi-Class Malignancy Prediction
- **Arxiv ID**: http://arxiv.org/abs/1807.02608v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.02608v1)
- **Published**: 2018-07-07 03:23:08+00:00
- **Updated**: 2018-07-07 03:23:08+00:00
- **Authors**: Matthew Yung, Eli T. Brown, Alexander Rasin, Jacob D. Furst, Daniela S. Raicu
- **Comment**: 5 pages, 3 figures, 4 Tables, KDD MLMH'18 Workshop
- **Journal**: None
- **Summary**: We explore several oversampling techniques for an imbalanced multi-label classification problem, a setting often encountered when developing models for Computer-Aided Diagnosis (CADx) systems. While most CADx systems aim to optimize classifiers for overall accuracy without considering the relative distribution of each class, we look into using synthetic sampling to increase per-class performance when predicting the degree of malignancy. Using low-level image features and a random forest classifier, we show that using synthetic oversampling techniques increases the sensitivity of the minority classes by an average of 7.22% points, with as much as a 19.88% point increase in sensitivity for a particular minority class. Furthermore, the analysis of low-level image feature distributions for the synthetic nodules reveals that these nodules can provide insights on how to preprocess image data for better classification performance or how to supplement the original datasets when more data acquisition is feasible.



### Representing a Partially Observed Non-Rigid 3D Human Using Eigen-Texture and Eigen-Deformation
- **Arxiv ID**: http://arxiv.org/abs/1807.02632v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02632v1)
- **Published**: 2018-07-07 08:18:31+00:00
- **Updated**: 2018-07-07 08:18:31+00:00
- **Authors**: Ryosuke Kimura, Akihiko Sayo, Fabian Lorenzo Dayrit, Yuta Nakashima, Hiroshi Kawasaki, Ambrosio Blanco, Katsushi Ikeuchi
- **Comment**: 6pages, accepted to ICPR
- **Journal**: None
- **Summary**: Reconstruction of the shape and motion of humans from RGB-D is a challenging problem, receiving much attention in recent years. Recent approaches for full-body reconstruction use a statistic shape model, which is built upon accurate full-body scans of people in skin-tight clothes, to complete invisible parts due to occlusion. Such a statistic model may still be fit to an RGB-D measurement with loose clothes but cannot describe its deformations, such as clothing wrinkles. Observed surfaces may be reconstructed precisely from actual measurements, while we have no cues for unobserved surfaces. For full-body reconstruction with loose clothes, we propose to use lower dimensional embeddings of texture and deformation referred to as eigen-texturing and eigen-deformation, to reproduce views of even unobserved surfaces. Provided a full-body reconstruction from a sequence of partial measurements as 3D meshes, the texture and deformation of each triangle are then embedded using eigen-decomposition. Combined with neural-network-based coefficient regression, our method synthesizes the texture and deformation from arbitrary viewpoints. We evaluate our method using simulated data and visually demonstrate how our method works on real data.



### Video Prediction with Appearance and Motion Conditions
- **Arxiv ID**: http://arxiv.org/abs/1807.02635v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02635v1)
- **Published**: 2018-07-07 08:55:10+00:00
- **Updated**: 2018-07-07 08:55:10+00:00
- **Authors**: Yunseok Jang, Gunhee Kim, Yale Song
- **Comment**: Accepted paper at ICML 2018. Project page:
  http://vision.snu.ac.kr/projects/amc-gan
- **Journal**: None
- **Summary**: Video prediction aims to generate realistic future frames by learning dynamic visual patterns. One fundamental challenge is to deal with future uncertainty: How should a model behave when there are multiple correct, equally probable future? We propose an Appearance-Motion Conditional GAN to address this challenge. We provide appearance and motion information as conditions that specify how the future may look like, reducing the level of uncertainty. Our model consists of a generator, two discriminators taking charge of appearance and motion pathways, and a perceptual ranking module that encourages videos of similar conditions to look similar. To train our model, we develop a novel conditioning scheme that consists of different combinations of appearance and motion conditions. We evaluate our model using facial expression and human action datasets and report favorable results compared to existing methods.



### One-shot Texture Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1807.02654v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02654v1)
- **Published**: 2018-07-07 12:23:38+00:00
- **Updated**: 2018-07-07 12:23:38+00:00
- **Authors**: Ivan Ustyuzhaninov, Claudio Michaelis, Wieland Brendel, Matthias Bethge
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce one-shot texture segmentation: the task of segmenting an input image containing multiple textures given a patch of a reference texture. This task is designed to turn the problem of texture-based perceptual grouping into an objective benchmark. We show that it is straight-forward to generate large synthetic data sets for this task from a relatively small number of natural textures. In particular, this task can be cast as a self-supervised problem thereby alleviating the need for massive amounts of manually annotated data necessary for traditional segmentation tasks. In this paper we introduce and study two concrete data sets: a dense collage of textures (CollTex) and a cluttered texturized Omniglot data set. We show that a baseline model trained on these synthesized data is able to generalize to natural images and videos without further fine-tuning, suggesting that the learned image representations are useful for higher-level vision tasks.



### Tournament Based Ranking CNN for the Cataract grading
- **Arxiv ID**: http://arxiv.org/abs/1807.02657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02657v1)
- **Published**: 2018-07-07 12:40:32+00:00
- **Updated**: 2018-07-07 12:40:32+00:00
- **Authors**: Dohyeun Kim, Tae Joon Jun, Daeyoung Kim, Youngsub Eom
- **Comment**: Submitted to ACCV 2018
- **Journal**: None
- **Summary**: Solving the classification problem, unbalanced number of dataset among the classes often causes performance degradation. Especially when some classes dominate the other classes with its large number of datasets, trained model shows low performance in identifying the dominated classes. This is common case when it comes to medical dataset. Because the case with a serious degree is not quite usual, there are imbalance in number of dataset between severe case and normal cases of diseases. Also, there is difficulty in precisely identifying grade of medical data because of vagueness between them. To solve these problems, we propose new architecture of convolutional neural network named Tournament based Ranking CNN which shows remarkable performance gain in identifying dominated classes while trading off very small accuracy loss in dominating classes. Our Approach complemented problems that occur when method of Ranking CNN that aggregates outputs of multiple binary neural network models is applied to medical data. By having tournament structure in aggregating method and using very deep pretrained binary models, our proposed model recorded 68.36% of exact match accuracy, while Ranking CNN recorded 53.40%, pretrained Resnet recorded 56.12% and CNN with linear regression recorded 57.48%. As a result, our proposed method is applied efficiently to cataract grading which have ordinal labels with imbalanced number of data among classes, also can be applied further to medical problems which have similar features to cataract and similar dataset configuration.



### Towards Multi-class Object Detection in Unconstrained Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/1807.02700v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.02700v3)
- **Published**: 2018-07-07 17:48:57+00:00
- **Updated**: 2018-11-01 12:52:41+00:00
- **Authors**: Seyed Majid Azimi, Eleonora Vig, Reza Bahmanyar, Marco KÃ¶rner, Peter Reinartz
- **Comment**: ACCV 2018
- **Journal**: None
- **Summary**: Automatic multi-class object detection in remote sensing images in unconstrained scenarios is of high interest for several applications including traffic monitoring and disaster management. The huge variation in object scale, orientation, category, and complex backgrounds, as well as the different camera sensors pose great challenges for current algorithms. In this work, we propose a new method consisting of a novel joint image cascade and feature pyramid network with multi-size convolution kernels to extract multi-scale strong and weak semantic features. These features are fed into rotation-based region proposal and region of interest networks to produce object detections. Finally, rotational non-maximum suppression is applied to remove redundant detections. During training, we minimize joint horizontal and oriented bounding box loss functions, as well as a novel loss that enforces oriented boxes to be rectangular. Our method achieves 68.16% mAP on horizontal and 72.45% mAP on oriented bounding box detection tasks on the challenging DOTA dataset, outperforming all published methods by a large margin (+6% and +12% absolute improvement, respectively). Furthermore, it generalizes to two other datasets, NWPU VHR-10 and UCAS-AOD, and achieves competitive results with the baselines even when trained on DOTA. Our method can be deployed in multi-class object detection applications, regardless of the image and object scales and orientations, making it a great choice for unconstrained aerial and satellite imagery.



### DeepSource: Point Source Detection using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.02701v1
- **DOI**: 10.1093/mnras/stz131
- **Categories**: **astro-ph.IM**, cs.CV, cs.LG, hep-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.02701v1)
- **Published**: 2018-07-07 18:00:07+00:00
- **Updated**: 2018-07-07 18:00:07+00:00
- **Authors**: A. Vafaei Sadr, Etienne. E. Vos, Bruce A. Bassett, Zafiirah Hosenie, N. Oozeer, Michelle Lochner
- **Comment**: 15 pages, 13 figures, submitted to MNRAS
- **Journal**: MNRAS, Volume 484, Issue 2, April 2019, Pages 2793-2806
- **Summary**: Point source detection at low signal-to-noise is challenging for astronomical surveys, particularly in radio interferometry images where the noise is correlated. Machine learning is a promising solution, allowing the development of algorithms tailored to specific telescope arrays and science cases. We present DeepSource - a deep learning solution - that uses convolutional neural networks to achieve these goals. DeepSource enhances the Signal-to-Noise Ratio (SNR) of the original map and then uses dynamic blob detection to detect sources. Trained and tested on two sets of 500 simulated 1 deg x 1 deg MeerKAT images with a total of 300,000 sources, DeepSource is essentially perfect in both purity and completeness down to SNR = 4 and outperforms PyBDSF in all metrics. For uniformly-weighted images it achieves a Purity x Completeness (PC) score at SNR = 3 of 0.73, compared to 0.31 for the best PyBDSF model. For natural-weighting we find a smaller improvement of ~40% in the PC score at SNR = 3. If instead we ask where either of the purity or completeness first drop to 90%, we find that DeepSource reaches this value at SNR = 3.6 compared to the 4.3 of PyBDSF (natural-weighting). A key advantage of DeepSource is that it can learn to optimally trade off purity and completeness for any science case under consideration. Our results show that deep learning is a promising approach to point source detection in astronomical images.



### A Deep-Learning-Based Geological Parameterization for History Matching Complex Models
- **Arxiv ID**: http://arxiv.org/abs/1807.02716v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/1807.02716v1)
- **Published**: 2018-07-07 20:34:04+00:00
- **Updated**: 2018-07-07 20:34:04+00:00
- **Authors**: Yimin Liu, Wenyue Sun, Louis J. Durlofsky
- **Comment**: None
- **Journal**: None
- **Summary**: A new low-dimensional parameterization based on principal component analysis (PCA) and convolutional neural networks (CNN) is developed to represent complex geological models. The CNN-PCA method is inspired by recent developments in computer vision using deep learning. CNN-PCA can be viewed as a generalization of an existing optimization-based PCA (O-PCA) method. Both CNN-PCA and O-PCA entail post-processing a PCA model to better honor complex geological features. In CNN-PCA, rather than use a histogram-based regularization as in O-PCA, a new regularization involving a set of metrics for multipoint statistics is introduced. The metrics are based on summary statistics of the nonlinear filter responses of geological models to a pre-trained deep CNN. In addition, in the CNN-PCA formulation presented here, a convolutional neural network is trained as an explicit transform function that can post-process PCA models quickly. CNN-PCA is shown to provide both unconditional and conditional realizations that honor the geological features present in reference SGeMS geostatistical realizations for a binary channelized system. Flow statistics obtained through simulation of random CNN-PCA models closely match results for random SGeMS models for a demanding case in which O-PCA models lead to significant discrepancies. Results for history matching are also presented. In this assessment CNN-PCA is applied with derivative-free optimization, and a subspace randomized maximum likelihood method is used to provide multiple posterior models. Data assimilation and significant uncertainty reduction are achieved for existing wells, and physically reasonable predictions are also obtained for new wells. Finally, the CNN-PCA method is extended to a more complex non-stationary bimodal deltaic fan system, and is shown to provide high-quality realizations for this challenging example.



