# Arxiv Papers in cs.CV on 2018-07-25
### Deterministic Hypothesis Generation for Robust Fitting of Multiple Structures
- **Arxiv ID**: http://arxiv.org/abs/1807.09408v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09408v1)
- **Published**: 2018-07-25 01:28:28+00:00
- **Updated**: 2018-07-25 01:28:28+00:00
- **Authors**: Kwang Hee Lee, Chanki Yu, Sang Wook Lee
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel algorithm for generating robust and consistent hypotheses for multiple-structure model fitting. Most of the existing methods utilize random sampling which produce varying results especially when outlier ratio is high. For a structure where a model is fitted, the inliers of other structures are regarded as outliers when multiple structures are present. Global optimization has recently been investigated to provide stable and unique solutions, but the computational cost of the algorithms is prohibitively high for most image data with reasonable sizes. The algorithm presented in this paper uses a maximum feasible subsystem (MaxFS) algorithm to generate consistent initial hypotheses only from partial datasets in spatially overlapping local image regions. Our assumption is that each genuine structure will exist as a dominant structure in at least one of the local regions. To refine initial hypotheses estimated from partial datasets and to remove residual tolerance dependency of the MaxFS algorithm, iterative re-weighted L1 (IRL1) minimization is performed for all the image data. Initial weights of IRL1 framework are determined from the initial hypotheses generated in local regions. Our approach is significantly more efficient than those that use only global optimization for all the image data. Experimental results demonstrate that the presented method can generate more reliable and consistent hypotheses than random-sampling methods for estimating single and multiple structures from data with a large amount of outliers. We clearly expose the influence of algorithm parameter settings on the results in our experiments.



### 3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/1807.09413v1
- **DOI**: 10.1007/978-3-030-01267-0_37
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09413v1)
- **Published**: 2018-07-25 01:56:21+00:00
- **Updated**: 2018-07-25 01:56:21+00:00
- **Authors**: Zi Jian Yew, Gim Hee Lee
- **Comment**: 17 pages, 6 figures. Accepted in ECCV 2018
- **Journal**: None
- **Summary**: In this paper, we propose the 3DFeat-Net which learns both 3D feature detector and descriptor for point cloud matching using weak supervision. Unlike many existing works, we do not require manual annotation of matching point clusters. Instead, we leverage on alignment and attention mechanisms to learn feature correspondences from GPS/INS tagged 3D point clouds without explicitly specifying them. We create training and benchmark outdoor Lidar datasets, and experiments show that 3DFeat-Net obtains state-of-the-art performance on these gravity-aligned datasets.



### Video Storytelling: Textual Summaries for Events
- **Arxiv ID**: http://arxiv.org/abs/1807.09418v3
- **DOI**: 10.1109/TMM.2019.2930041
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.09418v3)
- **Published**: 2018-07-25 02:43:19+00:00
- **Updated**: 2020-05-14 12:39:48+00:00
- **Authors**: Junnan Li, Yongkang Wong, Qi Zhao, Mohan S. Kankanhalli
- **Comment**: Published in IEEE Transactions on Multimedia
- **Journal**: J. Li, Y. Wong, Q. Zhao and M. S. Kankanhalli, "Video
  Storytelling: Textual Summaries for Events," in IEEE Transactions on
  Multimedia, 2019
- **Summary**: Bridging vision and natural language is a longstanding goal in computer vision and multimedia research. While earlier works focus on generating a single-sentence description for visual content, recent works have studied paragraph generation. In this work, we introduce the problem of video storytelling, which aims at generating coherent and succinct stories for long videos. Video storytelling introduces new challenges, mainly due to the diversity of the story and the length and complexity of the video. We propose novel methods to address the challenges. First, we propose a context-aware framework for multimodal embedding learning, where we design a Residual Bidirectional Recurrent Neural Network to leverage contextual information from past and future. Second, we propose a Narrator model to discover the underlying storyline. The Narrator is formulated as a reinforcement learning agent which is trained by directly optimizing the textual metric of the generated story. We evaluate our method on the Video Story dataset, a new dataset that we have collected to enable the study. We compare our method with multiple state-of-the-art baselines, and show that our method achieves better performance, in terms of quantitative measures and user study.



### Semantics Meet Saliency: Exploring Domain Affinity and Models for Dual-Task Prediction
- **Arxiv ID**: http://arxiv.org/abs/1807.09430v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09430v1)
- **Published**: 2018-07-25 04:07:17+00:00
- **Updated**: 2018-07-25 04:07:17+00:00
- **Authors**: Md Amirul Islam, Mahmoud Kalash, Neil D. B. Bruce
- **Comment**: BMVC 2018
- **Journal**: None
- **Summary**: Much research has examined models for prediction of semantic labels or instances including dense pixel-wise prediction. The problem of predicting salient objects or regions of an image has also been examined in a similar light. With that said, there is an apparent relationship between these two problem domains in that the composition of a scene and associated semantic categories is certain to play into what is deemed salient. In this paper, we explore the relationship between these two problem domains. This is carried out in constructing deep neural networks that perform both predictions together albeit with different configurations for flow of conceptual information related to each distinct problem. This is accompanied by a detailed analysis of object co-occurrences that shed light on dataset bias and semantic precedence specific to individual categories.



### Distinctive-attribute Extraction for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1807.09434v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1807.09434v1)
- **Published**: 2018-07-25 04:34:17+00:00
- **Updated**: 2018-07-25 04:34:17+00:00
- **Authors**: Boeun Kim, Young Han Lee, Hyedong Jung, Choongsang Cho
- **Comment**: 14 main pages, 4 supplementary pages
- **Journal**: None
- **Summary**: Image captioning, an open research issue, has been evolved with the progress of deep neural networks. Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) are employed to compute image features and generate natural language descriptions in the research. In previous works, a caption involving semantic description can be generated by applying additional information into the RNNs. In this approach, we propose a distinctive-attribute extraction (DaE) which explicitly encourages significant meanings to generate an accurate caption describing the overall meaning of the image with their unique situation. Specifically, the captions of training images are analyzed by term frequency-inverse document frequency (TF-IDF), and the analyzed semantic information is trained to extract distinctive-attributes for inferring captions. The proposed scheme is evaluated on a challenge data, and it improves an objective performance while describing images in more detail.



### Deterministic consensus maximization with biconvex programming
- **Arxiv ID**: http://arxiv.org/abs/1807.09436v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09436v3)
- **Published**: 2018-07-25 04:54:57+00:00
- **Updated**: 2018-12-03 01:59:01+00:00
- **Authors**: Zhipeng Cai, Tat-Jun Chin, Huu Le, David Suter
- **Comment**: European Conference on Computer Vision (ECCV) 2018, oral presentation
- **Journal**: None
- **Summary**: Consensus maximization is one of the most widely used robust fitting paradigms in computer vision, and the development of algorithms for consensus maximization is an active research topic. In this paper, we propose an efficient deterministic optimization algorithm for consensus maximization. Given an initial solution, our method conducts a deterministic search that forcibly increases the consensus of the initial solution. We show how each iteration of the update can be formulated as an instance of biconvex programming, which we solve efficiently using a novel biconvex optimization algorithm. In contrast to our algorithm, previous consensus improvement techniques rely on random sampling or relaxations of the objective function, which reduce their ability to significantly improve the initial consensus. In fact, on challenging instances, the previous techniques may even return a worse off solution. Comprehensive experiments show that our algorithm can consistently and greatly improve the quality of the initial solution, without substantial cost.



### Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net
- **Arxiv ID**: http://arxiv.org/abs/1807.09441v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09441v3)
- **Published**: 2018-07-25 05:51:15+00:00
- **Updated**: 2020-03-23 03:31:11+00:00
- **Authors**: Xingang Pan, Ping Luo, Jianping Shi, Xiaoou Tang
- **Comment**: Accepted for publication at ECCV 2018
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have achieved great successes in many computer vision problems. Unlike existing works that designed CNN architectures to improve performance on a single task of a single domain and not generalizable, we present IBN-Net, a novel convolutional architecture, which remarkably enhances a CNN's modeling ability on one domain (e.g. Cityscapes) as well as its generalization capacity on another domain (e.g. GTA5) without finetuning. IBN-Net carefully integrates Instance Normalization (IN) and Batch Normalization (BN) as building blocks, and can be wrapped into many advanced deep networks to improve their performances. This work has three key contributions. (1) By delving into IN and BN, we disclose that IN learns features that are invariant to appearance changes, such as colors, styles, and virtuality/reality, while BN is essential for preserving content related information. (2) IBN-Net can be applied to many advanced deep architectures, such as DenseNet, ResNet, ResNeXt, and SENet, and consistently improve their performance without increasing computational cost. (3) When applying the trained networks to new domains, e.g. from GTA5 to Cityscapes, IBN-Net achieves comparable improvements as domain adaptation methods, even without using data from the target domain. With IBN-Net, we won the 1st place on the WAD 2018 Challenge Drivable Area track, with an mIoU of 86.18%.



### Multi-view Reconstructive Preserving Embedding for Dimension Reduction
- **Arxiv ID**: http://arxiv.org/abs/1807.10614v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10614v1)
- **Published**: 2018-07-25 06:42:58+00:00
- **Updated**: 2018-07-25 06:42:58+00:00
- **Authors**: Huibing Wang, Lin Feng, Adong Kong, Bo Jin
- **Comment**: 17 pages, 6 figures
- **Journal**: None
- **Summary**: With the development of feature extraction technique, one sample always can be represented by multiple features which locate in high-dimensional space. Multiple features can re ect various perspectives of one same sample, so there must be compatible and complementary information among the multiple views. Therefore, it's natural to integrate multiple features together to obtain better performance. However, most multi-view dimension reduction methods cannot handle multiple features from nonlinear space with high dimensions. To address this problem, we propose a novel multi-view dimension reduction method named Multi-view Reconstructive Preserving Embedding (MRPE) in this paper. MRPE reconstructs each sample by utilizing its k nearest neighbors. The similarities between each sample and its neighbors are primely mapped into lower-dimensional space in order to preserve the underlying neighborhood structure of the original manifold. MRPE fully exploits correlations between each sample and its neighbors from multiple views by linear reconstruction. Furthermore, MRPE constructs an optimization problem and derives an iterative procedure to obtain the low-dimensional embedding. Various evaluations based on the applications of document classification, face recognition and image retrieval demonstrate the effectiveness of our proposed approach on multi-view dimension reduction.



### Attention Mechanisms for Object Recognition with Event-Based Cameras
- **Arxiv ID**: http://arxiv.org/abs/1807.09480v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09480v2)
- **Published**: 2018-07-25 08:39:25+00:00
- **Updated**: 2018-11-18 16:29:21+00:00
- **Authors**: Marco Cannici, Marco Ciccone, Andrea Romanoni, Matteo Matteucci
- **Comment**: WACV2019 camera-ready submission
- **Journal**: None
- **Summary**: Event-based cameras are neuromorphic sensors capable of efficiently encoding visual information in the form of sparse sequences of events. Being biologically inspired, they are commonly used to exploit some of the computational and power consumption benefits of biological vision. In this paper we focus on a specific feature of vision: visual attention. We propose two attentive models for event based vision: an algorithm that tracks events activity within the field of view to locate regions of interest and a fully-differentiable attention procedure based on DRAW neural model. We highlight the strengths and weaknesses of the proposed methods on four datasets, the Shifted N-MNIST, Shifted MNIST-DVS, CIFAR10-DVS and N-Caltech101 collections, using the Phased LSTM recognition network as a baseline reference model obtaining improvements in terms of both translation and scale invariance.



### How good is my GAN?
- **Arxiv ID**: http://arxiv.org/abs/1807.09499v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.09499v1)
- **Published**: 2018-07-25 09:31:17+00:00
- **Updated**: 2018-07-25 09:31:17+00:00
- **Authors**: Konstantin Shmelkov, Cordelia Schmid, Karteek Alahari
- **Comment**: Accepted to ECCV2018
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) are one of the most popular methods for generating images today. While impressive results have been validated by visual inspection, a number of quantitative criteria have emerged only recently. We argue here that the existing ones are insufficient and need to be in adequation with the task at hand. In this paper we introduce two measures based on image classification---GAN-train and GAN-test, which approximate the recall (diversity) and precision (quality of the image) of GANs respectively. We evaluate a number of recent GAN approaches based on these two measures and demonstrate a clear difference in performance. Furthermore, we observe that the increasing difficulty of the dataset, from CIFAR10 over CIFAR100 to ImageNet, shows an inverse correlation with the quality of the GANs, as clearly evident from our measures.



### Toward Scale-Invariance and Position-Sensitive Region Proposal Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.09528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09528v1)
- **Published**: 2018-07-25 11:01:47+00:00
- **Updated**: 2018-07-25 11:01:47+00:00
- **Authors**: Hsueh-Fu Lu, Xiaofei Du, Ping-Lin Chang
- **Comment**: 22 pages, 10 figures, accepted by ECCV2018
- **Journal**: None
- **Summary**: Accurately localising object proposals is an important precondition for high detection rate for the state-of-the-art object detection frameworks. The accuracy of an object detection method has been shown highly related to the average recall (AR) of the proposals. In this work, we propose an advanced object proposal network in favour of translation-invariance for objectness classification, translation-variance for bounding box regression, large effective receptive fields for capturing global context and scale-invariance for dealing with a range of object sizes from extremely small to large. The design of the network architecture aims to be simple while being effective and with real time performance. Without bells and whistles the proposed object proposal network significantly improves the AR at 1,000 proposals by $35\%$ and $45\%$ on PASCAL VOC and COCO dataset respectively and has a fast inference time of 44.8 ms for input image size of $640^{2}$. Empirical studies have also shown that the proposed method is class-agnostic to be generalised for general object proposal.



### Aerial Imagery for Roof Segmentation: A Large-Scale Dataset towards Automatic Mapping of Buildings
- **Arxiv ID**: http://arxiv.org/abs/1807.09532v2
- **DOI**: 10.1016/j.isprsjprs.2018.11.011
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09532v2)
- **Published**: 2018-07-25 11:23:45+00:00
- **Updated**: 2018-07-27 07:58:23+00:00
- **Authors**: Qi Chen, Lei Wang, Yifan Wu, Guangming Wu, Zhiling Guo, Steven L. Waslander
- **Comment**: arXiv admin note: This version has been removed as the user did not
  have the right to agree to the license at the time of submission
- **Journal**: None
- **Summary**: arXiv admin note: This version has been removed as the user did not have the right to agree to the license at the time of submission



### Conditional Information Gain Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.09534v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.09534v1)
- **Published**: 2018-07-25 11:26:46+00:00
- **Updated**: 2018-07-25 11:26:46+00:00
- **Authors**: Ufuk Can Biçici, Cem Keskin, Lale Akarun
- **Comment**: ICPR 2018 Paper
- **Journal**: None
- **Summary**: Deep neural network models owe their representational power to the high number of learnable parameters. It is often infeasible to run these largely parametrized deep models in limited resource environments, like mobile phones. Network models employing conditional computing are able to reduce computational requirements while achieving high representational power, with their ability to model hierarchies. We propose Conditional Information Gain Networks, which allow the feed forward deep neural networks to execute conditionally, skipping parts of the model based on the sample and the decision mechanisms inserted in the architecture. These decision mechanisms are trained using cost functions based on differentiable Information Gain, inspired by the training procedures of decision trees. These information gain based decision mechanisms are differentiable and can be trained end-to-end using a unified framework with a general cost function, covering both classification and decision losses. We test the effectiveness of the proposed method on MNIST and recently introduced Fashion MNIST datasets and show that our information gain based conditional execution approach can achieve better or comparable classification results using significantly fewer parameters, compared to standard convolutional neural network baselines.



### End-to-End Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.09536v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09536v2)
- **Published**: 2018-07-25 11:38:25+00:00
- **Updated**: 2018-09-03 07:51:16+00:00
- **Authors**: Francisco M. Castro, Manuel J. Marín-Jiménez, Nicolás Guil, Cordelia Schmid, Karteek Alahari
- **Comment**: To appear in ECCV 2018
- **Journal**: None
- **Summary**: Although deep learning approaches have stood out in recent years due to their state-of-the-art results, they continue to suffer from catastrophic forgetting, a dramatic decrease in overall performance when training with new classes added incrementally. This is due to current neural network architectures requiring the entire dataset, consisting of all the samples from the old as well as the new classes, to update the model -a requirement that becomes easily unsustainable as the number of classes grows. We address this issue with our approach to learn deep neural networks incrementally, using new data and only a small exemplar set corresponding to samples from the old classes. This is based on a loss composed of a distillation measure to retain the knowledge acquired from the old classes, and a cross-entropy loss to learn the new classes. Our incremental training is achieved while keeping the entire framework end-to-end, i.e., learning the data representation and the classifier jointly, unlike recent methods with no such guarantees. We evaluate our method extensively on the CIFAR-100 and ImageNet (ILSVRC 2012) image classification datasets, and show state-of-the-art performance.



### Patch-based Evaluation of Dense Image Matching Quality
- **Arxiv ID**: http://arxiv.org/abs/1807.09546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09546v1)
- **Published**: 2018-07-25 12:17:40+00:00
- **Updated**: 2018-07-25 12:17:40+00:00
- **Authors**: Zhenchao Zhang, Markus Gerke, George Vosselman, Michael Ying Yang
- **Comment**: 16 pages
- **Journal**: International Journal of Applied Earth Observation and
  Geoinformation, 2018
- **Summary**: Airborne laser scanning and photogrammetry are two main techniques to obtain 3D data representing the object surface. Due to the high cost of laser scanning, we want to explore the potential of using point clouds derived by dense image matching (DIM), as effective alternatives to laser scanning data. We present a framework to evaluate point clouds from dense image matching and derived Digital Surface Models (DSM) based on automatically extracted sample patches. Dense matching error and noise level are evaluated quantitatively at both the local level and whole block level. Experiments show that the optimal vertical accuracy achieved by dense matching is as follows: the mean offset to the reference data is 0.1 Ground Sampling Distance (GSD); the maximum offset goes up to 1.0 GSD. When additional oblique images are used in dense matching, the mean deviation, the variation of mean deviation and the level of random noise all get improved. We also detect a bias between the point cloud and DSM from a single photogrammetric workflow. This framework also allows to reveal inhomogeneity in the distribution of the dense matching errors due to over-fitted BBA network. Meanwhile, suggestions are given on the photogrammetric quality control.



### Change Detection between Multimodal Remote Sensing Data Using Siamese CNN
- **Arxiv ID**: http://arxiv.org/abs/1807.09562v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09562v1)
- **Published**: 2018-07-25 13:00:42+00:00
- **Updated**: 2018-07-25 13:00:42+00:00
- **Authors**: Zhenchao Zhang, George Vosselman, Markus Gerke, Devis Tuia, Michael Ying Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting topographic changes in the urban environment has always been an important task for urban planning and monitoring. In practice, remote sensing data are often available in different modalities and at different time epochs. Change detection between multimodal data can be very challenging since the data show different characteristics. Given 3D laser scanning point clouds and 2D imagery from different epochs, this paper presents a framework to detect building and tree changes. First, the 2D and 3D data are transformed to image patches, respectively. A Siamese CNN is then employed to detect candidate changes between the two epochs. Finally, the candidate patch-based changes are grouped and verified as individual object changes. Experiments on the urban data show that 86.4\% of patch pairs can be correctly classified by the model.



### Linear Span Network for Object Skeleton Detection
- **Arxiv ID**: http://arxiv.org/abs/1807.09601v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09601v1)
- **Published**: 2018-07-25 13:45:29+00:00
- **Updated**: 2018-07-25 13:45:29+00:00
- **Authors**: Chang Liu, Wei Ke, Fei Qin, Qixiang Ye
- **Comment**: Accepted by ECCV2018
- **Journal**: None
- **Summary**: Robust object skeleton detection requires to explore rich representative visual features and effective feature fusion strategies. In this paper, we first re-visit the implementation of HED, the essential principle of which can be ideally described with a linear reconstruction model. Hinted by this, we formalize a Linear Span framework, and propose Linear Span Network (LSN) modified by Linear Span Units (LSUs), which minimize the reconstruction error of convolutional network. LSN further utilizes subspace linear span beside the feature linear span to increase the independence of convolutional features and the efficiency of feature integration, which enlarges the capability of fitting complex ground-truth. As a result, LSN can effectively suppress the cluttered backgrounds and reconstruct object skeletons. Experimental results validate the state-of-the-art performance of the proposed LSN.



### Multi-Resolution Networks for Semantic Segmentation in Whole Slide Images
- **Arxiv ID**: http://arxiv.org/abs/1807.09607v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09607v1)
- **Published**: 2018-07-25 13:54:11+00:00
- **Updated**: 2018-07-25 13:54:11+00:00
- **Authors**: Feng Gu, Nikolay Burlutskiy, Mats Andersson, Lena Kajland Wilen
- **Comment**: Accepted by MICCAI COMPAY 2018 Workshop
- **Journal**: None
- **Summary**: Digital pathology provides an excellent opportunity for applying fully convolutional networks (FCNs) to tasks, such as semantic segmentation of whole slide images (WSIs). However, standard FCNs face challenges with respect to multi-resolution, inherited from the pyramid arrangement of WSIs. As a result, networks specifically designed to learn and aggregate information at different levels are desired. In this paper, we propose two novel multi-resolution networks based on the popular `U-Net' architecture, which are evaluated on a benchmark dataset for binary semantic segmentation in WSIs. The proposed methods outperform the U-Net, demonstrating superior learning and generalization capabilities.



### Multiple sclerosis lesion enhancement and white matter region estimation using hyperintensities in FLAIR images
- **Arxiv ID**: http://arxiv.org/abs/1807.09619v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09619v1)
- **Published**: 2018-07-25 14:06:03+00:00
- **Updated**: 2018-07-25 14:06:03+00:00
- **Authors**: Paulo G. L. Freire, Ricardo J. Ferrari
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple sclerosis (MS) is a demyelinating disease that affects more than 2 million people worldwide. The most used imaging technique to help in its diagnosis and follow-up is magnetic resonance imaging (MRI). Fluid Attenuated Inversion Recovery (FLAIR) images are usually acquired in the context of MS because lesions often appear hyperintense in this particular image weight, making it easier for physicians to identify them. Though lesions have a bright intensity profile, it may overlap with white matter (WM) and gray matter (GM) tissues, posing difficulties to be accurately segmented. In this sense, we propose a lesion enhancement technique to dim down WM and GM regions and highlight hyperintensities, making them much more distinguishable than other tissues. We applied our technique to the ISBI 2015 MS Lesion Segmentation Challenge and took the average gray level intensity of MS lesions, WM and GM on FLAIR and enhanced images. The lesion intensity profile in FLAIR was on average 25% and 19% brighter than white matter and gray matter, respectively; comparatively, the same profile in our enhanced images was on average 444% and 264% brighter. Such results mean a significant improvement on the intensity distinction among these three clusters, which may come as aid both for experts and automated techniques. Moreover, a byproduct of our proposal is that the enhancement can be used to automatically estimate a mask encompassing WM and MS lesions, which may be useful for brain tissue volume assessment and improve MS lesion segmentation accuracy in future works.



### OmniDepth: Dense Depth Estimation for Indoors Spherical Panoramas
- **Arxiv ID**: http://arxiv.org/abs/1807.09620v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09620v1)
- **Published**: 2018-07-25 14:06:10+00:00
- **Updated**: 2018-07-25 14:06:10+00:00
- **Authors**: Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas, Petros Daras
- **Comment**: Pre-print to appear in ECCV18
- **Journal**: None
- **Summary**: Recent work on depth estimation up to now has only focused on projective images ignoring 360 content which is now increasingly and more easily produced. We show that monocular depth estimation models trained on traditional images produce sub-optimal results on omnidirectional images, showcasing the need for training directly on 360 datasets, which however, are hard to acquire. In this work, we circumvent the challenges associated with acquiring high quality 360 datasets with ground truth depth annotations, by re-using recently released large scale 3D datasets and re-purposing them to 360 via rendering. This dataset, which is considerably larger than similar projective datasets, is publicly offered to the community to enable future research in this direction. We use this dataset to learn in an end-to-end fashion the task of depth estimation from 360 images. We show promising results in our synthesized data as well as in unseen realistic images.



### Video-based computer aided arthroscopy for patient specific reconstruction of the Anterior Cruciate Ligament
- **Arxiv ID**: http://arxiv.org/abs/1807.09627v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09627v1)
- **Published**: 2018-07-25 14:22:38+00:00
- **Updated**: 2018-07-25 14:22:38+00:00
- **Authors**: Carolina Raposo, Cristovao Sousa, Luis Ribeiro, Rui Melo, Joao P. Barreto, Joao Oliveira, Pedro Marques, Fernando Fonseca
- **Comment**: None
- **Journal**: None
- **Summary**: The Anterior Cruciate Ligament (ACL) tear is a common medical condition that is treated using arthroscopy by pulling a tissue graft through a tunnel opened with a drill. The correct anatomical position and orientation of this tunnel is crucial for knee stability, and drilling an adequate bone tunnel is the most technically challenging part of the procedure. This paper presents, for the first time, a guidance system based solely on intra-operative video for guiding the drilling of the tunnel. Our solution uses small, easily recognizable visual markers that are attached to the bone and tools for estimating their relative pose. A recent registration algorithm is employed for aligning a pre-operative image of the patient's anatomy with a set of contours reconstructed by touching the bone surface with an instrumented tool. Experimental validation using ex-vivo data shows that the method enables the accurate registration of the pre-operative model with the bone, providing useful information for guiding the surgeon during the medical procedure.



### A Surprising Linear Relationship Predicts Test Performance in Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1807.09659v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1807.09659v1)
- **Published**: 2018-07-25 15:20:02+00:00
- **Updated**: 2018-07-25 15:20:02+00:00
- **Authors**: Qianli Liao, Brando Miranda, Andrzej Banburski, Jack Hidary, Tomaso Poggio
- **Comment**: None
- **Journal**: None
- **Summary**: Given two networks with the same training loss on a dataset, when would they have drastically different test losses and errors? Better understanding of this question of generalization may improve practical applications of deep networks. In this paper we show that with cross-entropy loss it is surprisingly simple to induce significantly different generalization performances for two networks that have the same architecture, the same meta parameters and the same training error: one can either pretrain the networks with different levels of "corrupted" data or simply initialize the networks with weights of different Gaussian standard deviations. A corollary of recent theoretical results on overfitting shows that these effects are due to an intrinsic problem of measuring test performance with a cross-entropy/exponential-type loss, which can be decomposed into two components both minimized by SGD -- one of which is not related to expected classification performance. However, if we factor out this component of the loss, a linear relationship emerges between training and test losses. Under this transformation, classical generalization bounds are surprisingly tight: the empirical/training loss is very close to the expected/test loss. Furthermore, the empirical relation between classification error and normalized cross-entropy loss seem to be approximately monotonic



### Attend Before you Act: Leveraging human visual attention for continual learning
- **Arxiv ID**: http://arxiv.org/abs/1807.09664v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1807.09664v1)
- **Published**: 2018-07-25 15:23:44+00:00
- **Updated**: 2018-07-25 15:23:44+00:00
- **Authors**: Khimya Khetarpal, Doina Precup
- **Comment**: Lifelong Learning: A Reinforcement Learning Approach (LLARLA)
  Workshop, ICML 2018
- **Journal**: None
- **Summary**: When humans perform a task, such as playing a game, they selectively pay attention to certain parts of the visual input, gathering relevant information and sequentially combining it to build a representation from the sensory data. In this work, we explore leveraging where humans look in an image as an implicit indication of what is salient for decision making. We build on top of the UNREAL architecture in DeepMind Lab's 3D navigation maze environment. We train the agent both with original images and foveated images, which were generated by overlaying the original images with saliency maps generated using a real-time spectral residual technique. We investigate the effectiveness of this approach in transfer learning by measuring performance in the context of noise in the environment.



### Person re-identification across different datasets with multi-task learning
- **Arxiv ID**: http://arxiv.org/abs/1807.09666v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09666v1)
- **Published**: 2018-07-25 15:27:32+00:00
- **Updated**: 2018-07-25 15:27:32+00:00
- **Authors**: Matthieu Ospici, Antoine Cecchi
- **Comment**: 17 pages, 3 figures
- **Journal**: None
- **Summary**: This paper presents an approach to tackle the re-identification problem. This is a challenging problem due to the large variation of pose, illumination or camera view. More and more datasets are available to train machine learning models for person re-identification. These datasets vary in conditions: cameras numbers, camera positions, location, season, in size, i.e. number of images, number of different identities. Finally in labeling: there are datasets annotated with attributes while others are not. To deal with this variety of datasets we present in this paper an approach to take information from different datasets to build a system which performs well on all of them. Our model is based on a Convolutional Neural Network (CNN) and trained using multitask learning. Several losses are used to extract the different information available in the different datasets. Our main task is learned with a classification loss. To reduce the intra-class variation we experiment with the center loss. Our paper ends with a performance evaluation in which we discuss the influence of the different losses on the global re-identification performance. We show that with our method, we are able to build a system that performs well on different datasets and simultaneously extracts attributes. We also show that our system outperforms recent re-identification works on two datasets.



### Grounding Visual Explanations
- **Arxiv ID**: http://arxiv.org/abs/1807.09685v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09685v2)
- **Published**: 2018-07-25 16:03:35+00:00
- **Updated**: 2018-08-02 04:53:30+00:00
- **Authors**: Lisa Anne Hendricks, Ronghang Hu, Trevor Darrell, Zeynep Akata
- **Comment**: Accepted to ECCV 2018
- **Journal**: European Conference on Computer Vision (ECCV), 2018
- **Summary**: Existing visual explanation generating agents learn to fluently justify a class prediction. However, they may mention visual attributes which reflect a strong class prior, although the evidence may not actually be in the image. This is particularly concerning as ultimately such agents fail in building trust with human users. To overcome this limitation, we propose a phrase-critic model to refine generated candidate explanations augmented with flipped phrases which we use as negative examples while training. At inference time, our phrase-critic model takes an image and a candidate explanation as input and outputs a score indicating how well the candidate explanation is grounded in the image. Our explainable AI agent is capable of providing counter arguments for an alternative prediction, i.e. counterfactuals, along with explanations that justify the correct classification decisions. Our model improves the textual explanation quality of fine-grained classification decisions on the CUB dataset by mentioning phrases that are grounded in the image. Moreover, on the FOIL tasks, our agent detects when there is a mistake in the sentence, grounds the incorrect phrase and corrects it significantly better than other models.



### The Time-SIFT method : detecting 3-D changes from archival photogrammetric analysis with almost exclusively image information
- **Arxiv ID**: http://arxiv.org/abs/1807.09700v1
- **DOI**: 10.1016/j.isprsjprs.2018.10.016
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09700v1)
- **Published**: 2018-07-25 16:28:05+00:00
- **Updated**: 2018-07-25 16:28:05+00:00
- **Authors**: Denis Feurer, Fabrice Vinatier
- **Comment**: None
- **Journal**: ISPRS journal of photogrammetry and remote sensing 146 (2018):
  495-506
- **Summary**: Archival aerial imagery is a source of worldwide very high resolution data for documenting paste 3-D changes. However, external information is required so that accurate 3-D models can be computed from archival aerial imagery. In this research, we propose and test a new method, termed Time-SIFT (Scale Invariant Feature Transform), which allows for computing coherent multi-temporal Digital Elevation Models (DEMs) with almost exclusively image information. This method is based on the invariance properties of the SIFT-like methods which are at the root of the Structure from Motion (SfM) algorithms. On a test site of 170 km2, we applied SfM algorithms to a unique image block with all the images of four different dates covering forty years. We compared this method to more classical methods based on the use of affordable additional data such as ground control points collected in recent orthophotos. We did extensive tests to determine which processing choices were most impacting on the final result. With these tests, we aimed at evaluating the potential of the proposed Time-SIFT method for the detection and mapping of 3-D changes. Our study showed that the Time-SIFT method was the prime criteria that allowed for computing informative DEMs of difference with almost exclusively image information and limited photogrammetric expertise and human intervention. Due to the fact that the proposed Time-SIFT method can be automatically applied with exclusively image information, our results pave the way to a systematic processing of the archival aerial imagery on very large spatio-temporal windows, and should hence greatly help the unlocking of archival aerial imagery for the documenting of past 3-D changes.



### Asynchronous, Photometric Feature Tracking using Events and Frames
- **Arxiv ID**: http://arxiv.org/abs/1807.09713v1
- **DOI**: 10.1007/978-3-030-01258-8_46
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.09713v1)
- **Published**: 2018-07-25 16:40:05+00:00
- **Updated**: 2018-07-25 16:40:05+00:00
- **Authors**: Daniel Gehrig, Henri Rebecq, Guillermo Gallego, Davide Scaramuzza
- **Comment**: 22 pages, 15 figures, Video: https://youtu.be/A7UfeUnG6c4
- **Journal**: European Conference on Computer Vision (ECCV), Munich, 2018
- **Summary**: We present a method that leverages the complementarity of event cameras and standard cameras to track visual features with low-latency. Event cameras are novel sensors that output pixel-level brightness changes, called "events". They offer significant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. However, because the same scene pattern can produce different events depending on the motion direction, establishing event correspondences across time is challenging. By contrast, standard cameras provide intensity measurements (frames) that do not depend on motion direction. Our method extracts features on frames and subsequently tracks them asynchronously using events, thereby exploiting the best of both types of data: the frames provide a photometric representation that does not depend on motion direction and the events provide low-latency updates. In contrast to previous works, which are based on heuristics, this is the first principled method that uses raw intensity measurements directly, based on a generative event model within a maximum-likelihood framework. As a result, our method produces feature tracks that are both more accurate (subpixel accuracy) and longer than the state of the art, across a wide variety of scenes.



### Deep Unsupervised Multi-View Detection of Video Game Stream Highlights
- **Arxiv ID**: http://arxiv.org/abs/1807.09715v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09715v1)
- **Published**: 2018-07-25 16:41:12+00:00
- **Updated**: 2018-07-25 16:41:12+00:00
- **Authors**: Charles Ringer, Mihalis A. Nicolaou
- **Comment**: Foundation of Digital Games 2018, 6 pages
- **Journal**: None
- **Summary**: We consider the problem of automatic highlight-detection in video game streams. Currently, the vast majority of highlight-detection systems for games are triggered by the occurrence of hard-coded game events (e.g., score change, end-game), while most advanced tools and techniques are based on detection of highlights via visual analysis of game footage. We argue that in the context of game streaming, events that may constitute highlights are not only dependent on game footage, but also on social signals that are conveyed by the streamer during the play session (e.g., when interacting with viewers, or when commenting and reacting to the game). In this light, we present a multi-view unsupervised deep learning methodology for novelty-based highlight detection. The method jointly analyses both game footage and social signals such as the players facial expressions and speech, and shows promising results for generating highlights on streams of popular games such as Player Unknown's Battlegrounds.



### Flow-Grounded Spatial-Temporal Video Prediction from Still Images
- **Arxiv ID**: http://arxiv.org/abs/1807.09755v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09755v2)
- **Published**: 2018-07-25 17:56:33+00:00
- **Updated**: 2018-08-26 04:35:38+00:00
- **Authors**: Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, Ming-Hsuan Yang
- **Comment**: Accepted by ECCV 2018
- **Journal**: None
- **Summary**: Existing video prediction methods mainly rely on observing multiple historical frames or focus on predicting the next one-frame. In this work, we study the problem of generating consecutive multiple future frames by observing one single still image only. We formulate the multi-frame prediction task as a multiple time step flow (multi-flow) prediction phase followed by a flow-to-frame synthesis phase. The multi-flow prediction is modeled in a variational probabilistic manner with spatial-temporal relationships learned through 3D convolutions. The flow-to-frame synthesis is modeled as a generative process in order to keep the predicted results lying closer to the manifold shape of real video sequence. Such a two-phase design prevents the model from directly looking at the high-dimensional pixel space of the frame sequence and is demonstrated to be more effective in predicting better and diverse results. Extensive experimental results on videos with different types of motion show that the proposed algorithm performs favorably against existing methods in terms of quality, diversity and human perceptual evaluation.



### Coreset-Based Neural Network Compression
- **Arxiv ID**: http://arxiv.org/abs/1807.09810v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1807.09810v1)
- **Published**: 2018-07-25 18:26:49+00:00
- **Updated**: 2018-07-25 18:26:49+00:00
- **Authors**: Abhimanyu Dubey, Moitreya Chatterjee, Narendra Ahuja
- **Comment**: Camera-Ready version for ECCV 2018
- **Journal**: None
- **Summary**: We propose a novel Convolutional Neural Network (CNN) compression algorithm based on coreset representations of filters. We exploit the redundancies extant in the space of CNN weights and neuronal activations (across samples) in order to obtain compression. Our method requires no retraining, is easy to implement, and obtains state-of-the-art compression performance across a wide variety of CNN architectures. Coupled with quantization and Huffman coding, we create networks that provide AlexNet-like accuracy, with a memory footprint that is $832\times$ smaller than the original AlexNet, while also introducing significant reductions in inference time as well. Additionally these compressed networks when fine-tuned, successfully generalize to other domains as well.



### ADVIO: An authentic dataset for visual-inertial odometry
- **Arxiv ID**: http://arxiv.org/abs/1807.09828v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09828v1)
- **Published**: 2018-07-25 19:13:58+00:00
- **Updated**: 2018-07-25 19:13:58+00:00
- **Authors**: Santiago Cortés, Arno Solin, Esa Rahtu, Juho Kannala
- **Comment**: To appear in European Conference on Computer Vision (ECCV)
- **Journal**: None
- **Summary**: The lack of realistic and open benchmarking datasets for pedestrian visual-inertial odometry has made it hard to pinpoint differences in published methods. Existing datasets either lack a full six degree-of-freedom ground-truth or are limited to small spaces with optical tracking systems. We take advantage of advances in pure inertial navigation, and develop a set of versatile and challenging real-world computer vision benchmark sets for visual-inertial odometry. For this purpose, we have built a test rig equipped with an iPhone, a Google Pixel Android phone, and a Google Tango device. We provide a wide range of raw sensor data that is accessible on almost any modern-day smartphone together with a high-quality ground-truth track. We also compare resulting visual-inertial tracks from Google Tango, ARCore, and Apple ARKit with two recent methods published in academic forums. The data sets cover both indoor and outdoor cases, with stairs, escalators, elevators, office environments, a shopping mall, and metro station.



### Local Orthogonal-Group Testing
- **Arxiv ID**: http://arxiv.org/abs/1807.09848v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09848v2)
- **Published**: 2018-07-25 20:49:34+00:00
- **Updated**: 2018-09-20 11:03:43+00:00
- **Authors**: Ahmet Iscen, Ondrej Chum
- **Comment**: None
- **Journal**: None
- **Summary**: This work addresses approximate nearest neighbor search applied in the domain of large-scale image retrieval. Within the group testing framework we propose an efficient off-line construction of the search structures. The linear-time complexity orthogonal grouping increases the probability that at most one element from each group is matching to a given query. Non-maxima suppression with each group efficiently reduces the number of false positive results at no extra cost. Unlike in other well-performing approaches, all processing is local, fast, and suitable to process data in batches and in parallel. We experimentally show that the proposed method achieves search accuracy of the exhaustive search with significant reduction in the search complexity. The method can be naturally combined with existing embedding methods.



### Where are the Blobs: Counting by Localization with Point Supervision
- **Arxiv ID**: http://arxiv.org/abs/1807.09856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09856v1)
- **Published**: 2018-07-25 21:00:09+00:00
- **Updated**: 2018-07-25 21:00:09+00:00
- **Authors**: Issam H. Laradji, Negar Rostamzadeh, Pedro O. Pinheiro, David Vazquez, Mark Schmidt
- **Comment**: None
- **Journal**: None
- **Summary**: Object counting is an important task in computer vision due to its growing demand in applications such as surveillance, traffic monitoring, and counting everyday objects. State-of-the-art methods use regression-based optimization where they explicitly learn to count the objects of interest. These often perform better than detection-based methods that need to learn the more difficult task of predicting the location, size, and shape of each object. However, we propose a detection-based method that does not need to estimate the size and shape of the objects and that outperforms regression-based methods. Our contributions are three-fold: (1) we propose a novel loss function that encourages the network to output a single blob per object instance using point-level annotations only; (2) we design two methods for splitting large predicted blobs between object instances; and (3) we show that our method achieves new state-of-the-art results on several challenging datasets including the Pascal VOC and the Penguins dataset. Our method even outperforms those that use stronger supervision such as depth features, multi-point annotations, and bounding-box labels.



### Crossbar-aware neural network pruning
- **Arxiv ID**: http://arxiv.org/abs/1807.10816v3
- **DOI**: 10.1109/ACCESS.2018.2874823
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10816v3)
- **Published**: 2018-07-25 21:08:35+00:00
- **Updated**: 2018-12-06 03:08:39+00:00
- **Authors**: Ling Liang, Lei Deng, Yueling Zeng, Xing Hu, Yu Ji, Xin Ma, Guoqi Li, Yuan Xie
- **Comment**: None
- **Journal**: IEEE Access 6 (2018): 58324-58337
- **Summary**: Crossbar architecture based devices have been widely adopted in neural network accelerators by taking advantage of the high efficiency on vector-matrix multiplication (VMM) operations. However, in the case of convolutional neural networks (CNNs), the efficiency is compromised dramatically due to the large amounts of data reuse. Although some mapping methods have been designed to achieve a balance between the execution throughput and resource overhead, the resource consumption cost is still huge while maintaining the throughput.   Network pruning is a promising and widely studied leverage to shrink the model size. Whereas, previous work didn`t consider the crossbar architecture and the corresponding mapping method, which cannot be directly utilized by crossbar-based neural network accelerators. Tightly combining the crossbar structure and its mapping, this paper proposes a crossbar-aware pruning framework based on a formulated L0-norm constrained optimization problem. Specifically, we design an L0-norm constrained gradient descent (LGD) with relaxant probabilistic projection (RPP) to solve this problem. Two grains of sparsity are successfully achieved: i) intuitive crossbar-grain sparsity and ii) column-grain sparsity with output recombination, based on which we further propose an input feature maps (FMs) reorder method to improve the model accuracy. We evaluate our crossbar-aware pruning framework on median-scale CIFAR10 dataset and large-scale ImageNet dataset with VGG and ResNet models. Our method is able to reduce the crossbar overhead by 44%-72% with little accuracy degradation. This work greatly saves the resource and the related energy cost, which provides a new co-design solution for mapping CNNs onto various crossbar devices with significantly higher efficiency.



### Persuasive Faces: Generating Faces in Advertisements
- **Arxiv ID**: http://arxiv.org/abs/1807.09882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.09882v1)
- **Published**: 2018-07-25 22:21:53+00:00
- **Updated**: 2018-07-25 22:21:53+00:00
- **Authors**: Christopher Thomas, Adriana Kovashka
- **Comment**: None
- **Journal**: In British Machine Vision Conference (BMVC), Newcastle upon Tyne,
  UK, September 2018
- **Summary**: In this paper, we examine the visual variability of objects across different ad categories, i.e. what causes an advertisement to be visually persuasive. We focus on modeling and generating faces which appear to come from different types of ads. For example, if faces in beauty ads tend to be women wearing lipstick, a generative model should portray this distinct visual appearance. Training generative models which capture such category-specific differences is challenging because of the highly diverse appearance of faces in ads and the relatively limited amount of available training data. To address these problems, we propose a conditional variational autoencoder which makes use of predicted semantic attributes and facial expressions as a supervisory signal when training. We show how our model can be used to produce visually distinct faces which appear to be from a fixed ad topic category. Our human studies and quantitative and qualitative experiments confirm that our method greatly outperforms a variety of baselines, including two variations of a state-of-the-art generative adversarial network, for transforming faces to be more ad-category appropriate. Finally, we show preliminary generation results for other types of objects, conditioned on an ad topic.



### End-to-End Learning via a Convolutional Neural Network for Cancer Cell Line Classification
- **Arxiv ID**: http://arxiv.org/abs/1807.10638v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.10638v1)
- **Published**: 2018-07-25 22:45:22+00:00
- **Updated**: 2018-07-25 22:45:22+00:00
- **Authors**: Darlington Ahiale Akogo, Xavier-Lewis Palmer
- **Comment**: arXiv admin note: text overlap with arXiv:1805.08702
- **Journal**: None
- **Summary**: Computer Vision for automated analysis of cells and tissues usually include extracting features from images before analyzing such features via various Machine Learning and Machine Vision algorithms. We developed a Convolutional Neural Network model that classifies MDA-MB-468 and MCF7 breast cancer cells via brightfield microscopy images without the need of any prior feature extraction. Our 6-layer Convolutional Neural Network is directly trained, validated and tested on 1,241 images of MDA-MB-468 and MCF7 breast cancer cell line in an end-to-end fashion, allowing a system to distinguish between different cancer cell types. The model takes in as input imaged breast cancer cell line and then outputs the cell line type (MDA-MB-468 or MCF7) as predicted probabilities between the two classes. Our model scored a 99% Accuracy.



