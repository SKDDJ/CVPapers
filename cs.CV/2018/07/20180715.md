# Arxiv Papers in cs.CV on 2018-07-15
### A salt and pepper noise image denoising method based on the generative classification
- **Arxiv ID**: http://arxiv.org/abs/1807.05478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05478v1)
- **Published**: 2018-07-15 02:28:46+00:00
- **Updated**: 2018-07-15 02:28:46+00:00
- **Authors**: Bo Fu, Xiao-Yang Zhao, Yong-Gong Ren, Xi-Ming Li, Xiang-Hai Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, an image denoising algorithm is proposed for salt and pepper noise. First, a generative model is built on a patch as a basic unit and then the algorithm locates the image noise within that patch in order to better describe the patch and obtain better subsequent clustering. Second, the algorithm classifies patches using a generative clustering method, thus providing additional similarity information for noise repair and suppressing the interference of noise, abandoning those categories that consist of a smaller number of patches. Finally, the algorithm builds a non-local switching filter to remove the salt and pepper noise. Simulation results show that the proposed algorithm effectively denoises salt and pepper noise of various densities. It obtains a better visual quality and higher peak signal-to-noise ratio score than several state-of-the-art algorithms. In short, our algorithm uses a noisy patch as the basic unit, a patch clustering method to optimize the repair data set as well as obtain a better denoising effect, and provides a guideline for future denoising and repair methods.



### Near Real-time Hippocampus Segmentation Using Patch-based Canonical Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1807.05482v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05482v1)
- **Published**: 2018-07-15 03:23:28+00:00
- **Updated**: 2018-07-15 03:23:28+00:00
- **Authors**: Zhongliu Xie, Duncan Gillies
- **Comment**: None
- **Journal**: None
- **Summary**: Over the past decades, state-of-the-art medical image segmentation has heavily rested on signal processing paradigms, most notably registration-based label propagation and pair-wise patch comparison, which are generally slow despite a high segmentation accuracy. In recent years, deep learning has revolutionalized computer vision with many practices outperforming prior art, in particular the convolutional neural network (CNN) studies on image classification. Deep CNN has also started being applied to medical image segmentation lately, but generally involves long training and demanding memory requirements, achieving limited success. We propose a patch-based deep learning framework based on a revisit to the classic neural network model with substantial modernization, including the use of Rectified Linear Unit (ReLU) activation, dropout layers, 2.5D tri-planar patch multi-pathway settings. In a test application to hippocampus segmentation using 100 brain MR images from the ADNI database, our approach significantly outperformed prior art in terms of both segmentation accuracy and speed: scoring a median Dice score up to 90.98% on a near real-time performance (<1s).



### The Globally Optimal Reparameterization Algorithm: an Alternative to Fast Dynamic Time Warping for Action Recognition in Video Sequences
- **Arxiv ID**: http://arxiv.org/abs/1807.05485v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05485v1)
- **Published**: 2018-07-15 04:18:48+00:00
- **Updated**: 2018-07-15 04:18:48+00:00
- **Authors**: Thomas Mitchel, Sipu Ruan, Yixin Gao, Gregory S. Chirikjian
- **Comment**: ICARCV 2018, initial submission
- **Journal**: None
- **Summary**: Signal alignment has become a popular problem in robotics due in part to its fundamental role in action recognition. Currently, the most successful algorithms for signal alignment are Dynamic Time Warping (DTW) and its variant 'Fast' Dynamic Time Warping (FastDTW). Here we introduce a new framework for signal alignment, namely the Globally Optimal Reparameterization Algorithm (GORA). We review the algorithm's mathematical foundation and provide a numerical verification of its theoretical basis. We compare the performance of GORA with that of the DTW and FastDTW algorithms, in terms of computational efficiency and accuracy in matching signals. Our results show a significant improvement in both speed and accuracy over the DTW and FastDTW algorithms and suggest that GORA has the potential to provide a highly effective framework for signal alignment and action recognition.



### Deep neural network ensemble by data augmentation and bagging for skin lesion classification
- **Arxiv ID**: http://arxiv.org/abs/1807.05496v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05496v2)
- **Published**: 2018-07-15 06:31:41+00:00
- **Updated**: 2018-07-24 08:08:20+00:00
- **Authors**: Manik Goyal, Jagath C. Rajapakse
- **Comment**: 4 pages, 1 figure. ISIC 2018 challenge
- **Journal**: None
- **Summary**: This work summarizes our submission for the Task 3: Disease Classification of ISIC 2018 challenge in Skin Lesion Analysis Towards Melanoma Detection. We use a novel deep neural network (DNN) ensemble architecture introduced by us that can effectively classify skin lesions by using data-augmentation and bagging to address paucity of data and prevent over-fitting. The ensemble is composed of two DNN architectures: Inception-v4 and Inception-Resnet-v2. The DNN architectures are combined in to an ensemble by using a $1\times1$ convolution for fusion in a meta-learning layer.



### Object Detection with Deep Learning: A Review
- **Arxiv ID**: http://arxiv.org/abs/1807.05511v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05511v2)
- **Published**: 2018-07-15 08:16:03+00:00
- **Updated**: 2019-04-16 09:24:59+00:00
- **Authors**: Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, Xindong Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles which combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy and optimization function, etc. In this paper, we provide a review on deep learning based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely Convolutional Neural Network (CNN). Then we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network based learning systems.



### Deep Clustering for Unsupervised Learning of Visual Features
- **Arxiv ID**: http://arxiv.org/abs/1807.05520v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05520v2)
- **Published**: 2018-07-15 09:41:39+00:00
- **Updated**: 2019-03-18 16:20:30+00:00
- **Authors**: Mathilde Caron, Piotr Bojanowski, Armand Joulin, Matthijs Douze
- **Comment**: Accepted at ECCV 2018
- **Journal**: None
- **Summary**: Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, k-means, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a significant margin on all the standard benchmarks.



### Is the Pedestrian going to Cross? Answering by 2D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1807.10580v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1807.10580v1)
- **Published**: 2018-07-15 17:57:54+00:00
- **Updated**: 2018-07-15 17:57:54+00:00
- **Authors**: Zhijie Fang, Antonio M. LÃ³pez
- **Comment**: This is a paper presented in IEEE Intelligent Vehicles Symposium
  (IEEE IV 2018)
- **Journal**: None
- **Summary**: Our recent work suggests that, thanks to nowadays powerful CNNs, image-based 2D pose estimation is a promising cue for determining pedestrian intentions such as crossing the road in the path of the ego-vehicle, stopping before entering the road, and starting to walk or bending towards the road. This statement is based on the results obtained on non-naturalistic sequences (Daimler dataset), i.e. in sequences choreographed specifically for performing the study. Fortunately, a new publicly available dataset (JAAD) has appeared recently to allow developing methods for detecting pedestrian intentions in naturalistic driving conditions; more specifically, for addressing the relevant question is the pedestrian going to cross? Accordingly, in this paper we use JAAD to assess the usefulness of 2D pose estimation for answering such a question. We combine CNN-based pedestrian detection, tracking and pose estimation to predict the crossing action from monocular images. Overall, the proposed pipeline provides new state-of-the-art results.



### Deep Learning for Semantic Segmentation on Minimal Hardware
- **Arxiv ID**: http://arxiv.org/abs/1807.05597v1
- **DOI**: 10.1007/978-3-030-27544-0_29
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML, I.2.9; I.2.6; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1807.05597v1)
- **Published**: 2018-07-15 19:15:41+00:00
- **Updated**: 2018-07-15 19:15:41+00:00
- **Authors**: Sander G. van Dijk, Marcus M. Scheunemann
- **Comment**: 12 pages, 5 figures, RoboCup International Symposium 2018
- **Journal**: None
- **Summary**: Deep learning has revolutionised many fields, but it is still challenging to transfer its success to small mobile robots with minimal hardware. Specifically, some work has been done to this effect in the RoboCup humanoid football domain, but results that are performant and efficient and still generally applicable outside of this domain are lacking. We propose an approach conceptually different from those taken previously. It is based on semantic segmentation and does achieve these desired properties. In detail, it is being able to process full VGA images in real-time on a low-power mobile processor. It can further handle multiple image dimensions without retraining, it does not require specific domain knowledge for achieving a high frame rate and it is applicable on a minimal mobile hardware.



### Improved Person Re-Identification Based on Saliency and Semantic Parsing with Deep Neural Network Models
- **Arxiv ID**: http://arxiv.org/abs/1807.05618v1
- **DOI**: 10.1016/j.imavis.2019.07.009
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1807.05618v1)
- **Published**: 2018-07-15 21:40:00+00:00
- **Updated**: 2018-07-15 21:40:00+00:00
- **Authors**: Rodolfo Quispe, Helio Pedrini
- **Comment**: person re-identification
- **Journal**: Image and Vision Computing 2019
- **Summary**: Given a video or an image of a person acquired from a camera, person re-identification is the process of retrieving all instances of the same person from videos or images taken from a different camera with non-overlapping view. This task has applications in various fields, such as surveillance, forensics, robotics, multimedia. In this paper, we present a novel framework, named Saliency-Semantic Parsing Re-Identification (SSP-ReID), for taking advantage of the capabilities of both clues: saliency and semantic parsing maps, to guide a backbone convolutional neural network (CNN) to learn complementary representations that improves the results over the original backbones. The insight of fusing multiple clues is based on specific scenarios in which one response is better than another, thus favoring the combination of them to increase performance. Due to its definition, our framework can be easily applied to a wide variety of networks and, in contrast to other competitive methods, our training process follows simple and standard protocols. We present extensive evaluation of our approach through five backbones and three benchmarks. Experimental results demonstrate the effectiveness of our person re-identification framework. In addition, we combine our framework with re-ranking techniques to achieve state-of-the-art results on three benchmarks.



### Cross Pixel Optical Flow Similarity for Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1807.05636v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1807.05636v1)
- **Published**: 2018-07-15 23:48:59+00:00
- **Updated**: 2018-07-15 23:48:59+00:00
- **Authors**: Aravindh Mahendran, James Thewlis, Andrea Vedaldi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel method for learning convolutional neural image representations without manual supervision. We use motion cues in the form of optical flow, to supervise representations of static images. The obvious approach of training a network to predict flow from a single image can be needlessly difficult due to intrinsic ambiguities in this prediction task. We instead propose a much simpler learning goal: embed pixels such that the similarity between their embeddings matches that between their optical flow vectors. At test time, the learned deep network can be used without access to video or flow information and transferred to tasks such as image classification, detection, and segmentation. Our method, which significantly simplifies previous attempts at using motion for self-supervision, achieves state-of-the-art results in self-supervision using motion cues, competitive results for self-supervision in general, and is overall state of the art in self-supervised pretraining for semantic image segmentation, as demonstrated on standard benchmarks.



