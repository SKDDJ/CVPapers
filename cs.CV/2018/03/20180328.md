# Arxiv Papers in cs.CV on 2018-03-28
### Referring Relationships
- **Arxiv ID**: http://arxiv.org/abs/1803.10362v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10362v2)
- **Published**: 2018-03-28 00:32:57+00:00
- **Updated**: 2018-03-29 05:37:25+00:00
- **Authors**: Ranjay Krishna, Ines Chami, Michael Bernstein, Li Fei-Fei
- **Comment**: CVPR 2018, 19 pages, 12 figures, includes supplementary material
- **Journal**: None
- **Summary**: Images are not simply sets of objects: each image represents a web of interconnected relationships. These relationships between entities carry semantic meaning and help a viewer differentiate between instances of an entity. For example, in an image of a soccer match, there may be multiple persons present, but each participates in different relationships: one is kicking the ball, and the other is guarding the goal. In this paper, we formulate the task of utilizing these "referring relationships" to disambiguate between entities of the same category. We introduce an iterative model that localizes the two entities in the referring relationship, conditioned on one another. We formulate the cyclic condition between the entities in a relationship by modelling predicates that connect the entities as shifts in attention from one entity to another. We demonstrate that our model can not only outperform existing approaches on three datasets --- CLEVR, VRD and Visual Genome --- but also that it produces visually meaningful predicate shifts, as an instance of interpretable neural networks. Finally, we show that by modelling predicates as attention shifts, we can even localize entities in the absence of their category, allowing our model to find completely unseen categories.



### InLoc: Indoor Visual Localization with Dense Matching and View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1803.10368v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10368v2)
- **Published**: 2018-03-28 00:50:30+00:00
- **Updated**: 2018-04-08 15:08:20+00:00
- **Authors**: Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea Cimpoi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, Akihiko Torii
- **Comment**: None
- **Journal**: None
- **Summary**: We seek to predict the 6 degree-of-freedom (6DoF) pose of a query photograph with respect to a large indoor 3D map. The contributions of this work are three-fold. First, we develop a new large-scale visual localization method targeted for indoor environments. The method proceeds along three steps: (i) efficient retrieval of candidate poses that ensures scalability to large-scale environments, (ii) pose estimation using dense matching rather than local features to deal with textureless indoor scenes, and (iii) pose verification by virtual view synthesis to cope with significant changes in viewpoint, scene layout, and occluders. Second, we collect a new dataset with reference 6DoF poses for large-scale indoor localization. Query photographs are captured by mobile phones at a different time than the reference 3D map, thus presenting a realistic indoor localization scenario. Third, we demonstrate that our method significantly outperforms current state-of-the-art indoor localization approaches on this new challenging data.



### MicronNet: A Highly Compact Deep Convolutional Neural Network Architecture for Real-time Embedded Traffic Sign Classification
- **Arxiv ID**: http://arxiv.org/abs/1804.00497v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1804.00497v3)
- **Published**: 2018-03-28 01:32:59+00:00
- **Updated**: 2018-10-03 16:11:25+00:00
- **Authors**: Alexander Wong, Mohammad Javad Shafiee, Michael St. Jules
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Traffic sign recognition is a very important computer vision task for a number of real-world applications such as intelligent transportation surveillance and analysis. While deep neural networks have been demonstrated in recent years to provide state-of-the-art performance traffic sign recognition, a key challenge for enabling the widespread deployment of deep neural networks for embedded traffic sign recognition is the high computational and memory requirements of such networks. As a consequence, there are significant benefits in investigating compact deep neural network architectures for traffic sign recognition that are better suited for embedded devices. In this paper, we introduce MicronNet, a highly compact deep convolutional neural network for real-time embedded traffic sign recognition designed based on macroarchitecture design principles (e.g., spectral macroarchitecture augmentation, parameter precision optimization, etc.) as well as numerical microarchitecture optimization strategies. The resulting overall architecture of MicronNet is thus designed with as few parameters and computations as possible while maintaining recognition performance, leading to optimized information density of the proposed network. The resulting MicronNet possesses a model size of just ~1MB and ~510,000 parameters (~27x fewer parameters than state-of-the-art) while still achieving a human performance level top-1 accuracy of 98.9% on the German traffic sign recognition benchmark. Furthermore, MicronNet requires just ~10 million multiply-accumulate operations to perform inference, and has a time-to-compute of just 32.19 ms on a Cortex-A53 high efficiency processor. These experimental results show that highly compact, optimized deep neural network architectures can be designed for real-time traffic sign recognition that are well-suited for embedded scenarios.



### Automatic Stroke Lesions Segmentation in Diffusion-Weighted MRI
- **Arxiv ID**: http://arxiv.org/abs/1803.10385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10385v1)
- **Published**: 2018-03-28 02:13:11+00:00
- **Updated**: 2018-03-28 02:13:11+00:00
- **Authors**: Noranart Vesdapunt, Nongluk Covavisaruch
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion-Weighted Magnetic Resonance Imaging (DWI) is widely used for early cerebral infarct detection caused by ischemic stroke. Manual segmentation is done by a radiologist as a common clinical process, nonetheless, challenges of cerebral infarct segmentation come from low resolution and uncertain boundaries. Many segmentation techniques have been proposed and proved by manual segmentation as gold standard. In order to reduce human error in research operation and clinical process, we adopt a semi-automatic segmentation as gold standard using Fluid-Attenuated Inversion-Recovery (FLAIR) Magnetic Resonance Image (MRI) from the same patient under controlled environment. Extensive testing is performed on popular segmentation algorithms including Otsu method, Fuzzy C-means, Hill-climbing based segmentation, and Growcut. The selected segmentation techniques have been validated by accuracy, sensitivity, and specificity using leave-one-out cross-validation to determine the possibility of each techniques first then maximizes the accuracy from the training set. Our experimental results demonstrate the effectiveness of selected methods.



### Predictions of short-term driving intention using recurrent neural network on sequential data
- **Arxiv ID**: http://arxiv.org/abs/1804.00532v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.00532v1)
- **Published**: 2018-03-28 03:14:50+00:00
- **Updated**: 2018-03-28 03:14:50+00:00
- **Authors**: Zhou Xing, Fei Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: Predictions of driver's intentions and their behaviors using the road is of great importance for planning and decision making processes of autonomous driving vehicles. In particular, relatively short-term driving intentions are the fundamental units that constitute more sophisticated driving goals, behaviors, such as overtaking the slow vehicle in front, exit or merge onto a high way, etc. While it is not uncommon that most of the time human driver can rationalize, in advance, various on-road behaviors, intentions, as well as the associated risks, aggressiveness, reciprocity characteristics, etc., such reasoning skills can be challenging and difficult for an autonomous driving system to learn. In this article, we demonstrate a disciplined methodology that can be used to build and train a predictive drive system, therefore to learn the on-road characteristics aforementioned.



### Lip Movements Generation at a Glance
- **Arxiv ID**: http://arxiv.org/abs/1803.10404v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10404v3)
- **Published**: 2018-03-28 04:02:33+00:00
- **Updated**: 2018-05-21 22:23:50+00:00
- **Authors**: Lele Chen, Zhiheng Li, Ross K. Maddox, Zhiyao Duan, Chenliang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-modality generation is an emerging topic that aims to synthesize data in one modality based on information in a different modality. In this paper, we consider a task of such: given an arbitrary audio speech and one lip image of arbitrary target identity, generate synthesized lip movements of the target identity saying the speech. To perform well in this task, it inevitably requires a model to not only consider the retention of target identity, photo-realistic of synthesized images, consistency and smoothness of lip images in a sequence, but more importantly, learn the correlations between audio speech and lip movements. To solve the collective problems, we explore the best modeling of the audio-visual correlations in building and training a lip-movement generator network. Specifically, we devise a method to fuse audio and image embeddings to generate multiple lip images at once and propose a novel correlation loss to synchronize lip changes and speech changes. Our final model utilizes a combination of four losses for a comprehensive consideration in generating lip movements; it is trained in an end-to-end fashion and is robust to lip shapes, view angles and different facial characteristics. Thoughtful experiments on three datasets ranging from lab-recorded to lips in-the-wild show that our model significantly outperforms other state-of-the-art methods extended to this task.



### 3DMV: Joint 3D-Multi-View Prediction for 3D Semantic Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.10409v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10409v1)
- **Published**: 2018-03-28 04:22:13+00:00
- **Updated**: 2018-03-28 04:22:13+00:00
- **Authors**: Angela Dai, Matthias Nießner
- **Comment**: None
- **Journal**: None
- **Summary**: We present 3DMV, a novel method for 3D semantic scene segmentation of RGB-D scans in indoor environments using a joint 3D-multi-view prediction network. In contrast to existing methods that either use geometry or RGB data as input for this task, we combine both data modalities in a joint, end-to-end network architecture. Rather than simply projecting color data into a volumetric grid and operating solely in 3D -- which would result in insufficient detail -- we first extract feature maps from associated RGB images. These features are then mapped into the volumetric feature grid of a 3D network using a differentiable backprojection layer. Since our target is 3D scanning scenarios with possibly many frames, we use a multi-view pooling approach in order to handle a varying number of RGB input views. This learned combination of RGB and geometric features with our joint 2D-3D architecture achieves significantly better results than existing baselines. For instance, our final result on the ScanNet 3D segmentation benchmark increases from 52.8\% to 75\% accuracy compared to existing volumetric architectures.



### The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions
- **Arxiv ID**: http://arxiv.org/abs/1803.10417v3
- **DOI**: 10.1038/sdata.2018.161
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10417v3)
- **Published**: 2018-03-28 05:18:15+00:00
- **Updated**: 2018-11-25 10:18:03+00:00
- **Authors**: Philipp Tschandl, Cliff Rosendahl, Harald Kittler
- **Comment**: None
- **Journal**: Sci. Data 5, 180161 (2018)
- **Summary**: Training of neural networks for automated diagnosis of pigmented skin lesions is hampered by the small size and lack of diversity of available datasets of dermatoscopic images. We tackle this problem by releasing the HAM10000 ("Human Against Machine with 10000 training images") dataset. We collected dermatoscopic images from different populations acquired and stored by different modalities. Given this diversity we had to apply different acquisition and cleaning methods and developed semi-automatic workflows utilizing specifically trained neural networks. The final dataset consists of 10015 dermatoscopic images which are released as a training set for academic machine learning purposes and are publicly available through the ISIC archive. This benchmark dataset can be used for machine learning and for comparisons with human experts. Cases include a representative collection of all important diagnostic categories in the realm of pigmented lesions. More than 50% of lesions have been confirmed by pathology, while the ground truth for the rest of the cases was either follow-up, expert consensus, or confirmation by in-vivo confocal microscopy.



### The Effects of JPEG and JPEG2000 Compression on Attacks using Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1803.10418v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10418v2)
- **Published**: 2018-03-28 05:20:46+00:00
- **Updated**: 2018-03-31 14:42:27+00:00
- **Authors**: Ayse Elvan Aydemir, Alptekin Temizel, Tugba Taskaya Temizel
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial examples are known to have a negative effect on the performance of classifiers which have otherwise good performance on undisturbed images. These examples are generated by adding non-random noise to the testing samples in order to make classifier misclassify the given data. Adversarial attacks use these intentionally generated examples and they pose a security risk to the machine learning based systems. To be immune to such attacks, it is desirable to have a pre-processing mechanism which removes these effects causing misclassification while keeping the content of the image. JPEG and JPEG2000 are well-known image compression techniques which suppress the high-frequency content taking the human visual system into account. JPEG has been also shown to be an effective method for reducing adversarial noise. In this paper, we propose applying JPEG2000 compression as an alternative and systematically compare the classification performance of adversarial images compressed using JPEG and JPEG2000 at different target PSNR values and maximum compression levels. Our experiments show that JPEG2000 is more effective in reducing adversarial noise as it allows higher compression rates with less distortion and it does not introduce blocking artifacts.



### Eye movement simulation and detector creation to reduce laborious parameter adjustments
- **Arxiv ID**: http://arxiv.org/abs/1804.00970v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1804.00970v1)
- **Published**: 2018-03-28 06:48:37+00:00
- **Updated**: 2018-03-28 06:48:37+00:00
- **Authors**: Wolfgang Fuhl, Thiago Santini, Thomas Kuebler, Nora Castner, Wolfgang Rosenstiel, Enkelejda Kasneci
- **Comment**: None
- **Journal**: None
- **Summary**: Eye movements hold information about human perception, intention and cognitive state. Various algorithms have been proposed to identify and distinguish eye movements, particularly fixations, saccades, and smooth pursuits. A major drawback of existing algorithms is that they rely on accurate and constant sampling rates, impeding straightforward adaptation to new movements such as micro saccades. We propose a novel eye movement simulator that i) probabilistically simulates saccade movements as gamma distributions considering different peak velocities and ii) models smooth pursuit onsets with the sigmoid function. This simulator is combined with a machine learning approach to create detectors for general and specific velocity profiles. Additionally, our approach is capable of using any sampling rate, even with fluctuations. The machine learning approach consists of different binary patterns combined using conditional distributions. The simulation is evaluated against publicly available real data using a squared error, and the detectors are evaluated against state-of-the-art algorithms.



### Robust Video Content Alignment and Compensation for Rain Removal in a CNN Framework
- **Arxiv ID**: http://arxiv.org/abs/1803.10433v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10433v1)
- **Published**: 2018-03-28 07:22:10+00:00
- **Updated**: 2018-03-28 07:22:10+00:00
- **Authors**: Jie Chen, Cheen-Hau Tan, Junhui Hou, Lap-Pui Chau, He Li
- **Comment**: None
- **Journal**: None
- **Summary**: Rain removal is important for improving the robustness of outdoor vision based systems. Current rain removal methods show limitations either for complex dynamic scenes shot from fast moving cameras, or under torrential rain fall with opaque occlusions. We propose a novel derain algorithm, which applies superpixel (SP) segmentation to decompose the scene into depth consistent units. Alignment of scene contents are done at the SP level, which proves to be robust towards rain occlusion and fast camera motion. Two alignment output tensors, i.e., optimal temporal match tensor and sorted spatial-temporal match tensor, provide informative clues for rain streak location and occluded background contents to generate an intermediate derain output. These tensors will be subsequently prepared as input features for a convolutional neural network to restore high frequency details to the intermediate output for compensation of mis-alignment blur. Extensive evaluations show that up to 5 dB reconstruction PSNR advantage is achieved over state-of-the-art methods. Visual inspection shows that much cleaner rain removal is achieved especially for highly dynamic scenes with heavy and opaque rainfall from a fast moving camera.



### Exploiting Recurrent Neural Networks and Leap Motion Controller for Sign Language and Semaphoric Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.10435v1
- **DOI**: 10.1109/TMM.2018.2856094
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10435v1)
- **Published**: 2018-03-28 07:25:51+00:00
- **Updated**: 2018-03-28 07:25:51+00:00
- **Authors**: Danilo Avola, Marco Bernardi, Luigi Cinque, Gian Luca Foresti, Cristiano Massaroni
- **Comment**: None
- **Journal**: IEEE Transactions on Multimedia 21 (2019) 234-245
- **Summary**: In human interactions, hands are a powerful way of expressing information that, in some cases, can be used as a valid substitute for voice, as it happens in Sign Language. Hand gesture recognition has always been an interesting topic in the areas of computer vision and multimedia. These gestures can be represented as sets of feature vectors that change over time. Recurrent Neural Networks (RNNs) are suited to analyse this type of sets thanks to their ability to model the long term contextual information of temporal sequences. In this paper, a RNN is trained by using as features the angles formed by the finger bones of human hands. The selected features, acquired by a Leap Motion Controller (LMC) sensor, have been chosen because the majority of human gestures produce joint movements that generate truly characteristic corners. A challenging subset composed by a large number of gestures defined by the American Sign Language (ASL) is used to test the proposed solution and the effectiveness of the selected angles. Moreover, the proposed method has been compared to other state of the art works on the SHREC dataset, thus demonstrating its superiority in hand gesture recognition accuracy.



### Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.10464v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10464v2)
- **Published**: 2018-03-28 08:40:54+00:00
- **Updated**: 2018-04-09 05:13:41+00:00
- **Authors**: Jiwoon Ahn, Suha Kwak
- **Comment**: Accepted by CVPR 2018
- **Journal**: None
- **Summary**: The deficiency of segmentation labels is one of the main obstacles to semantic segmentation in the wild. To alleviate this issue, we present a novel framework that generates segmentation labels of images given their image-level class labels. In this weakly supervised setting, trained models have been known to segment local discriminative parts rather than the entire object area. Our solution is to propagate such local responses to nearby areas which belong to the same semantic entity. To this end, we propose a Deep Neural Network (DNN) called AffinityNet that predicts semantic affinity between a pair of adjacent image coordinates. The semantic propagation is then realized by random walk with the affinities predicted by AffinityNet. More importantly, the supervision employed to train AffinityNet is given by the initial discriminative part segmentation, which is incomplete as a segmentation annotation but sufficient for learning semantic affinities within small image areas. Thus the entire framework relies only on image-level class labels and does not require any extra data or annotations. On the PASCAL VOC 2012 dataset, a DNN learned with segmentation labels generated by our method outperforms previous models trained with the same level of supervision, and is even as competitive as those relying on stronger supervision.



### Objects Localisation from Motion with Constraints
- **Arxiv ID**: http://arxiv.org/abs/1803.10474v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10474v2)
- **Published**: 2018-03-28 09:14:33+00:00
- **Updated**: 2018-10-28 11:24:42+00:00
- **Authors**: Paul Gay, Alessio Del Bue
- **Comment**: 4 pages. This paper has been withdrawn as it has been merged with
  another submission: arXiv:1807.05933
- **Journal**: None
- **Summary**: This paper presents a method to estimate the 3D object position and occupancy given a set of object detections in multiple images and calibrated cameras. This problem is modelled as the estimation of a set of quadrics given 2D conics fit to the object bounding boxes. Although a closed form solution has been recently proposed, the resulting quadrics can be inaccurate or even be non valid ellipsoids in presence of noisy and inaccurate detections. This effect is especially important in case of small baselines, resulting in dramatic failures. To cope with this problem, we propose a set of linear constraints by matching the centres of the reprojected quadrics with the centres of the observed conics. These constraints can be solved with a linear system thus providing a more computationally efficient solution with respect to a non-linear alternative. Experiments on real data show that the proposed approach improves significantly the accuracy and the validity of the ellipsoids.



### Adversarial Spatio-Temporal Learning for Video Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1804.00533v2
- **DOI**: 10.1109/TIP.2018.2867733
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00533v2)
- **Published**: 2018-03-28 10:25:18+00:00
- **Updated**: 2018-08-22 04:14:18+00:00
- **Authors**: Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Wei Liu, Hongdong Li
- **Comment**: To appear in IEEE Transactions on Image Processing (TIP)
- **Journal**: None
- **Summary**: Camera shake or target movement often leads to undesired blur effects in videos captured by a hand-held camera. Despite significant efforts having been devoted to video-deblur research, two major challenges remain: 1) how to model the spatio-temporal characteristics across both the spatial domain (i.e., image plane) and temporal domain (i.e., neighboring frames), and 2) how to restore sharp image details w.r.t. the conventionally adopted metric of pixel-wise errors. In this paper, to address the first challenge, we propose a DeBLuRring Network (DBLRNet) for spatial-temporal learning by applying a modified 3D convolution to both spatial and temporal domains. Our DBLRNet is able to capture jointly spatial and temporal information encoded in neighboring frames, which directly contributes to improved video deblur performance. To tackle the second challenge, we leverage the developed DBLRNet as a generator in the GAN (generative adversarial network) architecture, and employ a content loss in addition to an adversarial loss for efficient adversarial training. The developed network, which we name as DeBLuRring Generative Adversarial Network (DBLRGAN), is tested on two standard benchmarks and achieves the state-of-the-art performance.



### Context-aware Deep Feature Compression for High-speed Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1803.10537v1
- **DOI**: 10.1109/CVPR.2018.00057
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10537v1)
- **Published**: 2018-03-28 11:38:12+00:00
- **Updated**: 2018-03-28 11:38:12+00:00
- **Authors**: Jongwon Choi, Hyung Jin Chang, Tobias Fischer, Sangdoo Yun, Kyuewang Lee, Jiyeoup Jeong, Yiannis Demiris, Jin Young Choi
- **Comment**: 9 pages, 6 figures, Accepted in CVPR2018 (IEEE conference on Computer
  Vision and Pattern Recognition)
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR2018)
- **Summary**: We propose a new context-aware correlation filter based tracking framework to achieve both high computational speed and state-of-the-art performance among real-time trackers. The major contribution to the high computational speed lies in the proposed deep feature compression that is achieved by a context-aware scheme utilizing multiple expert auto-encoders; a context in our framework refers to the coarse category of the tracking target according to appearance patterns. In the pre-training phase, one expert auto-encoder is trained per category. In the tracking phase, the best expert auto-encoder is selected for a given target, and only this auto-encoder is used. To achieve high tracking performance with the compressed feature map, we introduce extrinsic denoising processes and a new orthogonality loss term for pre-training and fine-tuning of the expert auto-encoders. We validate the proposed context-aware framework through a number of experiments, where our method achieves a comparable performance to state-of-the-art trackers which cannot run in real-time, while running at a significantly fast speed of over 100 fps.



### FPGA Implementations of 3D-SIMD Processor Architecture for Deep Neural Networks Using Relative Indexed Compressed Sparse Filter Encoding Format and Stacked Filters Stationary Flow
- **Arxiv ID**: http://arxiv.org/abs/1803.10548v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10548v3)
- **Published**: 2018-03-28 11:56:20+00:00
- **Updated**: 2018-04-12 07:42:42+00:00
- **Authors**: Yuechao Gao, Nianhong Liu, Sheng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: It is a challenging task to deploy computationally and memory intensive State-of-the-art deep neural networks (DNNs) on embedded systems with limited hardware resources and power budgets. Recently developed techniques like Deep Compression make it possible to fit large DNNs, such as AlexNet and VGGNet, fully in on-chip SRAM. But sparse networks compressed using existing encoding formats, like CSR or CSC, complex the computation at runtime due to their irregular memory access characteristics. In [1], we introduce a computation dataflow, stacked filters stationary dataflow (SFS), and a corresponding data encoding format, relative indexed compressed sparse filter format (CSF), to make the best of data sparsity, and simplify data handling at execution time. In this paper we present FPGA implementations of these methods. We implement several compact streaming fully connected (FC) and Convolutional (CONV) neural network processors to show their efficiency. Comparing with the state-of-the-art results [2,3,4], our methods achieve at least 2x improvement for computation efficiency per PE on most layers. Especially, our methods achieve 8x improvement on AlexNet layer CONV4 with 384 filters, and 11x improvement on VGG16 layer CONV5-3 with 512 filters.



### Normalization of Neural Networks using Analytic Variance Propagation
- **Arxiv ID**: http://arxiv.org/abs/1803.10560v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.10560v1)
- **Published**: 2018-03-28 12:37:27+00:00
- **Updated**: 2018-03-28 12:37:27+00:00
- **Authors**: Alexander Shekhovtsov, Boris Flach
- **Comment**: None
- **Journal**: In Proceedings of Computer Vision Winter Workshop 2018
- **Summary**: We address the problem of estimating statistics of hidden units in a neural network using a method of analytic moment propagation. These statistics are useful for approximate whitening of the inputs in front of saturating non-linearities such as a sigmoid function. This is important for initialization of training and for reducing the accumulated scale and bias dependencies (compensating covariate shift), which presumably eases the learning. In batch normalization, which is currently a very widely applied technique, sample estimates of statistics of hidden units over a batch are used. The proposed estimation uses an analytic propagation of mean and variance of the training set through the network. The result depends on the network structure and its current weights but not on the specific batch input. The estimates are suitable for initialization and normalization, efficient to compute and independent of the batch size. The experimental verification well supports these claims. However, the method does not share the generalization properties of BN, to which our experiments give some additional insight.



### ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes
- **Arxiv ID**: http://arxiv.org/abs/1803.10562v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10562v2)
- **Published**: 2018-03-28 12:39:03+00:00
- **Updated**: 2018-07-25 06:13:06+00:00
- **Authors**: Taihong Xiao, Jiapeng Hong, Jinwen Ma
- **Comment**: Github: https://github.com/Prinsphield/ELEGANT
- **Journal**: European Conference on Computer Vision 2018
- **Summary**: Recent studies on face attribute transfer have achieved great success. A lot of models are able to transfer face attributes with an input image. However, they suffer from three limitations: (1) incapability of generating image by exemplars; (2) being unable to transfer multiple face attributes simultaneously; (3) low quality of generated images, such as low-resolution or artifacts. To address these limitations, we propose a novel model which receives two images of opposite attributes as inputs. Our model can transfer exactly the same type of attributes from one image to another by exchanging certain part of their encodings. All the attributes are encoded in a disentangled manner in the latent space, which enables us to manipulate several attributes simultaneously. Besides, our model learns the residual images so as to facilitate training on higher resolution images. With the help of multi-scale discriminators for adversarial training, it can even generate high-quality images with finer details and less artifacts. We demonstrate the effectiveness of our model on overcoming the above three limitations by comparing with other methods on the CelebA face database. A pytorch implementation is available at https://github.com/Prinsphield/ELEGANT.



### Image Generation and Translation with Disentangled Representations
- **Arxiv ID**: http://arxiv.org/abs/1803.10567v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1803.10567v1)
- **Published**: 2018-03-28 12:53:01+00:00
- **Updated**: 2018-03-28 12:53:01+00:00
- **Authors**: Tobias Hinz, Stefan Wermter
- **Comment**: Accepted as a conference paper at the International Joint Conference
  on Neural Networks (IJCNN) 2018
- **Journal**: None
- **Summary**: Generative models have made significant progress in the tasks of modeling complex data distributions such as natural images. The introduction of Generative Adversarial Networks (GANs) and auto-encoders lead to the possibility of training on big data sets in an unsupervised manner. However, for many generative models it is not possible to specify what kind of image should be generated and it is not possible to translate existing images into new images of similar domains. Furthermore, models that can perform image-to-image translation often need distinct models for each domain, making it hard to scale these systems to multiple domain image-to-image translation. We introduce a model that can do both, controllable image generation and image-to-image translation between multiple domains. We split our image representation into two parts encoding unstructured and structured information respectively. The latter is designed in a disentangled manner, so that different parts encode different image characteristics. We train an encoder to encode images into these representations and use a small amount of labeled data to specify what kind of information should be encoded in the disentangled part. A generator is trained to generate images from these representations using the characteristics provided by the disentangled part of the representation. Through this we can control what kind of images the generator generates, translate images between different domains, and even learn unknown data-generating factors while only using one single model.



### Stochastic Variational Inference with Gradient Linearization
- **Arxiv ID**: http://arxiv.org/abs/1803.10586v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.10586v1)
- **Published**: 2018-03-28 13:22:57+00:00
- **Updated**: 2018-03-28 13:22:57+00:00
- **Authors**: Tobias Plötz, Anne S. Wannenwetsch, Stefan Roth
- **Comment**: To appear at CVPR 2018
- **Journal**: None
- **Summary**: Variational inference has experienced a recent surge in popularity owing to stochastic approaches, which have yielded practical tools for a wide range of model classes. A key benefit is that stochastic variational inference obviates the tedious process of deriving analytical expressions for closed-form variable updates. Instead, one simply needs to derive the gradient of the log-posterior, which is often much easier. Yet for certain model classes, the log-posterior itself is difficult to optimize using standard gradient techniques. One such example are random field models, where optimization based on gradient linearization has proven popular, since it speeds up convergence significantly and can avoid poor local optima. In this paper we propose stochastic variational inference with gradient linearization (SVIGL). It is similarly convenient as standard stochastic variational inference - all that is required is a local linearization of the energy gradient. Its benefit over stochastic variational inference with conventional gradient methods is a clear improvement in convergence speed, while yielding comparable or even better variational approximations in terms of KL divergence. We demonstrate the benefits of SVIGL in three applications: Optical flow estimation, Poisson-Gaussian denoising, and 3D surface reconstruction.



### Feed-forward Uncertainty Propagation in Belief and Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.10590v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1803.10590v2)
- **Published**: 2018-03-28 13:26:47+00:00
- **Updated**: 2018-11-01 17:02:02+00:00
- **Authors**: Alexander Shekhovtsov, Boris Flach, Michal Busta
- **Comment**: error corrections
- **Journal**: None
- **Summary**: We propose a feed-forward inference method applicable to belief and neural networks. In a belief network, the method estimates an approximate factorized posterior of all hidden units given the input. In neural networks the method propagates uncertainty of the input through all the layers. In neural networks with injected noise, the method analytically takes into account uncertainties resulting from this noise. Such feed-forward analytic propagation is differentiable in parameters and can be trained end-to-end. Compared to standard NN, which can be viewed as propagating only the means, we propagate the mean and variance. The method can be useful in all scenarios that require knowledge of the neuron statistics, e.g. when dealing with uncertain inputs, considering sigmoid activations as probabilities of Bernoulli units, training the models regularized by injected noise (dropout) or estimating activation statistics over the dataset (as needed for normalization methods). In the experiments we show the possible utility of the method in all these tasks as well as its current limitations.



### Motion Guided LIDAR-camera Self-calibration and Accelerated Depth Upsampling for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1803.10681v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10681v3)
- **Published**: 2018-03-28 15:26:23+00:00
- **Updated**: 2020-07-02 19:53:42+00:00
- **Authors**: Juan Castorena, Gint Puskorius, Gaurav Pandey
- **Comment**: None
- **Journal**: None
- **Summary**: This work proposes a novel motion guided method for target-less self-calibration of a LiDAR and camera and use the re-projection of LiDAR points onto the image reference frame for real-time depth upsampling. The calibration parameters are estimated by optimizing an objective function that penalizes distances between 2D and re-projected 3D motion vectors obtained from time-synchronized image and point cloud sequences. For upsampling, a simple, yet effective and time efficient formulation that minimizes depth gradients subject to an equality constraint involving the LiDAR measurements is proposed. Validation is performed on recorded real data from urban environments and demonstrations that our two methods are effective and suitable to mobile robotics and autonomous vehicle applications imposing real-time requirements is shown.



### Pose2Seg: Detection Free Human Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.10683v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10683v3)
- **Published**: 2018-03-28 15:33:56+00:00
- **Updated**: 2019-04-08 12:28:50+00:00
- **Authors**: Song-Hai Zhang, Ruilong Li, Xin Dong, Paul L. Rosin, Zixi Cai, Han Xi, Dingcheng Yang, Hao-Zhi Huang, Shi-Min Hu
- **Comment**: 8 pages
- **Journal**: CVPR 2019
- **Summary**: The standard approach to image instance segmentation is to perform the object detection first, and then segment the object from the detection bounding-box. More recently, deep learning methods like Mask R-CNN perform them jointly. However, little research takes into account the uniqueness of the "human" category, which can be well defined by the pose skeleton. Moreover, the human pose skeleton can be used to better distinguish instances with heavy occlusion than using bounding-boxes. In this paper, we present a brand new pose-based instance segmentation framework for humans which separates instances based on human pose, rather than proposal region detection. We demonstrate that our pose-based framework can achieve better accuracy than the state-of-art detection-based approach on the human instance segmentation problem, and can moreover better handle occlusion. Furthermore, there are few public datasets containing many heavily occluded humans along with comprehensive annotations, which makes this a challenging problem seldom noticed by researchers. Therefore, in this paper we introduce a new benchmark "Occluded Human (OCHuman)", which focuses on occluded humans with comprehensive annotations including bounding-box, human pose and instance masks. This dataset contains 8110 detailed annotated human instances within 4731 images. With an average 0.67 MaxIoU for each person, OCHuman is the most complex and challenging dataset related to human instance segmentation. Through this dataset, we want to emphasize occlusion as a challenging problem for researchers to study.



### Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment
- **Arxiv ID**: http://arxiv.org/abs/1803.10699v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10699v1)
- **Published**: 2018-03-28 15:58:23+00:00
- **Updated**: 2018-03-28 15:58:23+00:00
- **Authors**: Li Ding, Chenliang Xu
- **Comment**: CVPR 2018
- **Journal**: Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2018, pp. 6508-6516
- **Summary**: In this work, we address the task of weakly-supervised human action segmentation in long, untrimmed videos. Recent methods have relied on expensive learning models, such as Recurrent Neural Networks (RNN) and Hidden Markov Models (HMM). However, these methods suffer from expensive computational cost, thus are unable to be deployed in large scale. To overcome the limitations, the keys to our design are efficiency and scalability. We propose a novel action modeling framework, which consists of a new temporal convolutional network, named Temporal Convolutional Feature Pyramid Network (TCFPN), for predicting frame-wise action labels, and a novel training strategy for weakly-supervised sequence modeling, named Iterative Soft Boundary Assignment (ISBA), to align action sequences and update the network in an iterative fashion. The proposed framework is evaluated on two benchmark datasets, Breakfast and Hollywood Extended, with four different evaluation metrics. Extensive experimental results show that our methods achieve competitive or superior performance to state-of-the-art methods.



### End-to-End Multi-Task Learning with Attention
- **Arxiv ID**: http://arxiv.org/abs/1803.10704v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10704v2)
- **Published**: 2018-03-28 16:15:45+00:00
- **Updated**: 2019-04-05 05:57:50+00:00
- **Authors**: Shikun Liu, Edward Johns, Andrew J. Davison
- **Comment**: Accepted at Computer Vision and Pattern Recognition (CVPR), 2019
- **Journal**: None
- **Summary**: We propose a novel multi-task learning architecture, which allows learning of task-specific feature-level attention. Our design, the Multi-Task Attention Network (MTAN), consists of a single shared network containing a global feature pool, together with a soft-attention module for each task. These modules allow for learning of task-specific features from the global features, whilst simultaneously allowing for features to be shared across different tasks. The architecture can be trained end-to-end and can be built upon any feed-forward neural network, is simple to implement, and is parameter efficient. We evaluate our approach on a variety of datasets, across both image-to-image predictions and image classification tasks. We show that our architecture is state-of-the-art in multi-task learning compared to existing methods, and is also less sensitive to various weighting schemes in the multi-task loss function. Code is available at https://github.com/lorenmt/mtan.



### Asymmetric Loss Functions and Deep Densely Connected Networks for Highly Imbalanced Medical Image Segmentation: Application to Multiple Sclerosis Lesion Detection
- **Arxiv ID**: http://arxiv.org/abs/1803.11078v4
- **DOI**: 10.1109/ACCESS.2018.2886371
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11078v4)
- **Published**: 2018-03-28 16:29:54+00:00
- **Updated**: 2018-12-13 22:02:18+00:00
- **Authors**: Seyed Raein Hashemi, Seyed Sadegh Mohseni Salehi, Deniz Erdogmus, Sanjay P. Prabhu, Simon K. Warfield, Ali Gholipour
- **Comment**: IEEE Access
- **Journal**: None
- **Summary**: Fully convolutional deep neural networks have been asserted to be fast and precise frameworks with great potential in image segmentation. One of the major challenges in training such networks raises when data is unbalanced, which is common in many medical imaging applications such as lesion segmentation where lesion class voxels are often much lower in numbers than non-lesion voxels. A trained network with unbalanced data may make predictions with high precision and low recall, being severely biased towards the non-lesion class which is particularly undesired in most medical applications where FNs are more important than FPs. Various methods have been proposed to address this problem, more recently similarity loss functions and focal loss. In this work we trained fully convolutional deep neural networks using an asymmetric similarity loss function to mitigate the issue of data imbalance and achieve much better tradeoff between precision and recall. To this end, we developed a 3D FC-DenseNet with large overlapping image patches as input and an asymmetric similarity loss layer based on Tversky index (using Fbeta scores). We used large overlapping image patches as inputs for intrinsic and extrinsic data augmentation, a patch selection algorithm, and a patch prediction fusion strategy using B-spline weighted soft voting to account for the uncertainty of prediction in patch borders. We applied this method to MS lesion segmentation based on two different datasets of MSSEG and ISBI longitudinal MS lesion segmentation challenge, where we achieved top performance in both challenges. Our network trained with focal loss ranked first according to the ISBI challenge overall score and resulted in the lowest reported lesion false positive rate among all submitted methods. Our network trained with the asymmetric similarity loss led to the lowest surface distance and the best lesion true positive rate.



### Intertwiners between Induced Representations (with Applications to the Theory of Equivariant Neural Networks)
- **Arxiv ID**: http://arxiv.org/abs/1803.10743v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.10743v2)
- **Published**: 2018-03-28 17:30:26+00:00
- **Updated**: 2018-03-30 09:27:16+00:00
- **Authors**: Taco S. Cohen, Mario Geiger, Maurice Weiler
- **Comment**: None
- **Journal**: None
- **Summary**: Group equivariant and steerable convolutional neural networks (regular and steerable G-CNNs) have recently emerged as a very effective model class for learning from signal data such as 2D and 3D images, video, and other data where symmetries are present. In geometrical terms, regular G-CNNs represent data in terms of scalar fields ("feature channels"), whereas the steerable G-CNN can also use vector or tensor fields ("capsules") to represent data. In algebraic terms, the feature spaces in regular G-CNNs transform according to a regular representation of the group G, whereas the feature spaces in Steerable G-CNNs transform according to the more general induced representations of G. In order to make the network equivariant, each layer in a G-CNN is required to intertwine between the induced representations associated with its input and output space.   In this paper we present a general mathematical framework for G-CNNs on homogeneous spaces like Euclidean space or the sphere. We show, using elementary methods, that the layers of an equivariant network are convolutional if and only if the input and output feature spaces transform according to an induced representation. This result, which follows from G.W. Mackey's abstract theory on induced representations, establishes G-CNNs as a universal class of equivariant network architectures, and generalizes the important recent work of Kondor & Trivedi on the intertwiners between regular representations.



### Adversarial Network Compression
- **Arxiv ID**: http://arxiv.org/abs/1803.10750v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.10750v2)
- **Published**: 2018-03-28 17:38:20+00:00
- **Updated**: 2018-11-14 17:25:53+00:00
- **Authors**: Vasileios Belagiannis, Azade Farshad, Fabio Galasso
- **Comment**: 18 pages, 1 figure
- **Journal**: None
- **Summary**: Neural network compression has recently received much attention due to the computational requirements of modern deep models. In this work, our objective is to transfer knowledge from a deep and accurate model to a smaller one. Our contributions are threefold: (i) we propose an adversarial network compression approach to train the small student network to mimic the large teacher, without the need for labels during training; (ii) we introduce a regularization scheme to prevent a trivially-strong discriminator without reducing the network capacity and (iii) our approach generalizes on different teacher-student models.   In an extensive evaluation on five standard datasets, we show that our student has small accuracy drop, achieves better performance than other knowledge transfer approaches and it surpasses the performance of the same network trained with labels. In addition, we demonstrate state-of-the-art results compared to other compression strategies.



### TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1803.10794v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.10794v1)
- **Published**: 2018-03-28 18:30:17+00:00
- **Updated**: 2018-03-28 18:30:17+00:00
- **Authors**: Matthias Müller, Adel Bibi, Silvio Giancola, Salman Al-Subaihi, Bernard Ghanem
- **Comment**: preprint
- **Journal**: None
- **Summary**: Despite the numerous developments in object tracking, further development of current tracking algorithms is limited by small and mostly saturated datasets. As a matter of fact, data-hungry trackers based on deep-learning currently rely on object detection datasets due to the scarcity of dedicated large-scale tracking datasets. In this work, we present TrackingNet, the first large-scale dataset and benchmark for object tracking in the wild. We provide more than 30K videos with more than 14 million dense bounding box annotations. Our dataset covers a wide selection of object classes in broad and diverse context. By releasing such a large-scale dataset, we expect deep trackers to further improve and generalize. In addition, we introduce a new benchmark composed of 500 novel videos, modeled with a distribution similar to our training dataset. By sequestering the annotation of the test set and providing an online evaluation server, we provide a fair benchmark for future development of object trackers. Deep trackers fine-tuned on a fraction of our dataset improve their performance by up to 1.6% on OTB100 and up to 1.7% on TrackingNet Test. We provide an extensive benchmark on TrackingNet by evaluating more than 20 trackers. Our results suggest that object tracking in the wild is far from being solved.



### Learning to Become an Expert: Deep Networks Applied To Super-Resolution Microscopy
- **Arxiv ID**: http://arxiv.org/abs/1803.10806v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.10806v1)
- **Published**: 2018-03-28 19:01:45+00:00
- **Updated**: 2018-03-28 19:01:45+00:00
- **Authors**: Louis-Émile Robitaille, Audrey Durand, Marc-André Gardner, Christian Gagné, Paul De Koninck, Flavie Lavoie-Cardinal
- **Comment**: Accepted to the Thirtieth Innovative Applications of Artificial
  Intelligence Conference (IAAI), 2018
- **Journal**: None
- **Summary**: With super-resolution optical microscopy, it is now possible to observe molecular interactions in living cells. The obtained images have a very high spatial precision but their overall quality can vary a lot depending on the structure of interest and the imaging parameters. Moreover, evaluating this quality is often difficult for non-expert users. In this work, we tackle the problem of learning the quality function of super- resolution images from scores provided by experts. More specifically, we are proposing a system based on a deep neural network that can provide a quantitative quality measure of a STED image of neuronal structures given as input. We conduct a user study in order to evaluate the quality of the predictions of the neural network against those of a human expert. Results show the potential while highlighting some of the limits of the proposed approach.



### Who Let The Dogs Out? Modeling Dog Behavior From Visual Data
- **Arxiv ID**: http://arxiv.org/abs/1803.10827v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10827v2)
- **Published**: 2018-03-28 19:43:33+00:00
- **Updated**: 2018-05-17 20:00:03+00:00
- **Authors**: Kiana Ehsani, Hessam Bagherinezhad, Joseph Redmon, Roozbeh Mottaghi, Ali Farhadi
- **Comment**: Accepted to CVPR18
- **Journal**: None
- **Summary**: We introduce the task of directly modeling a visually intelligent agent. Computer vision typically focuses on solving various subtasks related to visual intelligence. We depart from this standard approach to computer vision; instead we directly model a visually intelligent agent. Our model takes visual information as input and directly predicts the actions of the agent. Toward this end we introduce DECADE, a large-scale dataset of ego-centric videos from a dog's perspective as well as her corresponding movements. Using this data we model how the dog acts and how the dog plans her movements. We show under a variety of metrics that given just visual input we can successfully model this intelligent agent in many situations. Moreover, the representation learned by our model encodes distinct information compared to representations trained on image classification, and our learned representation can generalize to other domains. In particular, we show strong results on the task of walkable surface estimation by using this dog modeling task as representation learning.



### Deep Learning Object Detection Methods for Ecological Camera Trap Data
- **Arxiv ID**: http://arxiv.org/abs/1803.10842v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10842v1)
- **Published**: 2018-03-28 20:30:39+00:00
- **Updated**: 2018-03-28 20:30:39+00:00
- **Authors**: Stefan Schneider, Graham W. Taylor, Stefan C. Kremer
- **Comment**: 8 pages, 6 figures, Conference of Computer and Robot Vision
- **Journal**: None
- **Summary**: Deep learning methods for computer vision tasks show promise for automating the data analysis of camera trap images. Ecological camera traps are a common approach for monitoring an ecosystem's animal population, as they provide continual insight into an environment without being intrusive. However, the analysis of camera trap images is expensive, labour intensive, and time consuming. Recent advances in the field of deep learning for object detection show promise towards automating the analysis of camera trap images. Here, we demonstrate their capabilities by training and comparing two deep learning object detection classifiers, Faster R-CNN and YOLO v2.0, to identify, quantify, and localize animal species within camera trap images using the Reconyx Camera Trap and the self-labeled Gold Standard Snapshot Serengeti data sets. When trained on large labeled datasets, object recognition methods have shown success. We demonstrate their use, in the context of realistically sized ecological data sets, by testing if object detection methods are applicable for ecological research scenarios when utilizing transfer learning. Faster R-CNN outperformed YOLO v2.0 with average accuracies of 93.0\% and 76.7\% on the two data sets, respectively. Our findings show promising steps towards the automation of the labourious task of labeling camera trap images, which can be used to improve our understanding of the population dynamics of ecosystems across the planet.



### Single Day Outdoor Photometric Stereo
- **Arxiv ID**: http://arxiv.org/abs/1803.10850v4
- **DOI**: 10.1109/TPAMI.2019.2962693
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10850v4)
- **Published**: 2018-03-28 20:55:42+00:00
- **Updated**: 2020-01-03 02:05:08+00:00
- **Authors**: Yannick Hold-Geoffroy, Paulo F. U. Gotardo, Jean-François Lalonde
- **Comment**: To appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence 2019, 0162-8828
- **Journal**: None
- **Summary**: Photometric Stereo (PS) under outdoor illumination remains a challenging, ill-posed problem due to insufficient variability in illumination. Months-long capture sessions are typically used in this setup, with little success on shorter, single-day time intervals. In this paper, we investigate the solution of outdoor PS over a single day, under different weather conditions. First, we investigate the relationship between weather and surface reconstructability in order to understand when natural lighting allows existing PS algorithms to work. Our analysis reveals that partially cloudy days improve the conditioning of the outdoor PS problem while sunny days do not allow the unambiguous recovery of surface normals from photometric cues alone. We demonstrate that calibrated PS algorithms can thus be employed to reconstruct Lambertian surfaces accurately under partially cloudy days. Second, we solve the ambiguity arising in clear days by combining photometric cues with prior knowledge on material properties, local surface geometry and the natural variations in outdoor lighting through a CNN-based, weakly-calibrated PS technique. Given a sequence of outdoor images captured during a single sunny day, our method robustly estimates the scene surface normals with unprecedented quality for the considered scenario. Our approach does not require precise geolocation and significantly outperforms several state-of-the-art methods on images with real lighting, showing that our CNN can combine efficiently learned priors and photometric cues available during a single sunny day.



### Features for Multi-Target Multi-Camera Tracking and Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1803.10859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10859v1)
- **Published**: 2018-03-28 21:23:23+00:00
- **Updated**: 2018-03-28 21:23:23+00:00
- **Authors**: Ergys Ristani, Carlo Tomasi
- **Comment**: Accepted as spotlight at CVPR 2018
- **Journal**: None
- **Summary**: Multi-Target Multi-Camera Tracking (MTMCT) tracks many people through video taken from several cameras. Person Re-Identification (Re-ID) retrieves from a gallery images of people similar to a person query image. We learn good features for both MTMCT and Re-ID with a convolutional neural network. Our contributions include an adaptive weighted triplet loss for training and a new technique for hard-identity mining. Our method outperforms the state of the art both on the DukeMTMC benchmarks for tracking, and on the Market-1501 and DukeMTMC-ReID benchmarks for Re-ID. We examine the correlation between good Re-ID and good MTMCT scores, and perform ablation studies to elucidate the contributions of the main components of our system. Code is available.



### Memory Warps for Learning Long-Term Online Video Representations
- **Arxiv ID**: http://arxiv.org/abs/1803.10861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10861v1)
- **Published**: 2018-03-28 21:36:40+00:00
- **Updated**: 2018-03-28 21:36:40+00:00
- **Authors**: Tuan-Hung Vu, Wongun Choi, Samuel Schulter, Manmohan Chandraker
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel memory-based online video representation that is efficient, accurate and predictive. This is in contrast to prior works that often rely on computationally heavy 3D convolutions, ignore actual motion when aligning features over time, or operate in an off-line mode to utilize future frames. In particular, our memory (i) holds the feature representation, (ii) is spatially warped over time to compensate for observer and scene motions, (iii) can carry long-term information, and (iv) enables predicting feature representations in future frames. By exploring a variant that operates at multiple temporal scales, we efficiently learn across even longer time horizons. We apply our online framework to object detection in videos, obtaining a large 2.3 times speed-up and losing only 0.9% mAP on ImageNet-VID dataset, compared to prior works that even use future frames. Finally, we demonstrate the predictive property of our representation in two novel detection setups, where features are propagated over time to (i) significantly enhance a real-time detector by more than 10% mAP in a multi-threaded online setup and to (ii) anticipate objects in future frames.



### A Survey on Deep Learning Methods for Robot Vision
- **Arxiv ID**: http://arxiv.org/abs/1803.10862v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1803.10862v1)
- **Published**: 2018-03-28 21:37:14+00:00
- **Updated**: 2018-03-28 21:37:14+00:00
- **Authors**: Javier Ruiz-del-Solar, Patricio Loncomilla, Naiomi Soto
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has allowed a paradigm shift in pattern recognition, from using hand-crafted features together with statistical classifiers to using general-purpose learning procedures for learning data-driven representations, features, and classifiers together. The application of this new paradigm has been particularly successful in computer vision, in which the development of deep learning methods for vision applications has become a hot research topic. Given that deep learning has already attracted the attention of the robot vision community, the main purpose of this survey is to address the use of deep learning in robot vision. To achieve this, a comprehensive overview of deep learning and its usage in computer vision is given, that includes a description of the most frequently used neural models and their main application areas. Then, the standard methodology and tools used for designing deep-learning based vision systems are presented. Afterwards, a review of the principal work using deep learning in robot vision is presented, as well as current and future trends related to the use of deep learning in robotics. This survey is intended to be a guide for the developers of robot vision systems.



### Human Emotional Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.10864v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10864v1)
- **Published**: 2018-03-28 21:44:21+00:00
- **Updated**: 2018-03-28 21:44:21+00:00
- **Authors**: Chendi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: An automatic Facial Expression Recognition (FER) model with Adaboost face detector, feature selection based on manifold learning and synergetic prototype based classifier has been proposed. Improved feature selection method and proposed classifier can achieve favorable effectiveness to performance FER in reasonable processing time.



### Learning to Look around Objects for Top-View Representations of Outdoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/1803.10870v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10870v1)
- **Published**: 2018-03-28 22:33:17+00:00
- **Updated**: 2018-03-28 22:33:17+00:00
- **Authors**: Samuel Schulter, Menghua Zhai, Nathan Jacobs, Manmohan Chandraker
- **Comment**: None
- **Journal**: None
- **Summary**: Given a single RGB image of a complex outdoor road scene in the perspective view, we address the novel problem of estimating an occlusion-reasoned semantic scene layout in the top-view. This challenging problem not only requires an accurate understanding of both the 3D geometry and the semantics of the visible scene, but also of occluded areas. We propose a convolutional neural network that learns to predict occluded portions of the scene layout by looking around foreground objects like cars or pedestrians. But instead of hallucinating RGB values, we show that directly predicting the semantics and depths in the occluded areas enables a better transformation into the top-view. We further show that this initial top-view representation can be significantly enhanced by learning priors and rules about typical road layouts from simulated or, if available, map data. Crucially, training our model does not require costly or subjective human annotations for occluded areas or the top-view, but rather uses readily available annotations for standard semantic segmentation. We extensively evaluate and analyze our approach on the KITTI and Cityscapes data sets.



