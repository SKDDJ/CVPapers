# Arxiv Papers in cs.CV on 2018-03-04
### Training Deep Learning Based Denoisers without Ground Truth Data
- **Arxiv ID**: http://arxiv.org/abs/1803.01314v4
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.01314v4)
- **Published**: 2018-03-04 07:55:40+00:00
- **Updated**: 2021-04-22 02:48:50+00:00
- **Authors**: Shakarim Soltanayev, Se Young Chun
- **Comment**: 12 pages, 10 figures, 7 tables, NeurIPS 2018, this is an extended
  version of it
- **Journal**: None
- **Summary**: Recently developed deep-learning-based denoisers often outperform state-of-the-art conventional denoisers such as the BM3D. They are typically trained to minimize the mean squared error (MSE) between the output image of a deep neural network (DNN) and a ground truth image. Thus, it is important for deep-learning-based denoisers to use high quality noiseless ground truth data for high performance. However, it is often challenging or even infeasible to obtain noiseless images in some applications. Here, we propose a method based on Stein's unbiased risk estimator (SURE) for training DNN denoisers based only on the use of noisy images in the training data with Gaussian noise. We demonstrate that our SURE-based method, without the use of ground truth data, is able to train DNN denoisers to yield performances close to those networks trained with ground truth for both grayscale and color images. We also propose a SURE-based refining method with a noisy test image for further performance improvement. Our quick refining method outperformed conventional BM3D, deep image prior, and often the networks trained with ground truth. Potential extension of our SURE-based methods to Poisson noise model was also investigated.



### Classification based Grasp Detection using Spatial Transformer Network
- **Arxiv ID**: http://arxiv.org/abs/1803.01356v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.01356v1)
- **Published**: 2018-03-04 14:02:47+00:00
- **Updated**: 2018-03-04 14:02:47+00:00
- **Authors**: Dongwon Park, Se Young Chun
- **Comment**: 6 pages, 10 figures, Under review
- **Journal**: None
- **Summary**: Robotic grasp detection task is still challenging, particularly for novel objects. With the recent advance of deep learning, there have been several works on detecting robotic grasp using neural networks. Typically, regression based grasp detection methods have outperformed classification based detection methods in computation complexity with excellent accuracy. However, classification based robotic grasp detection still seems to have merits such as intermediate step observability and straightforward back propagation routine for end-to-end training. In this work, we propose a novel classification based robotic grasp detection method with multiple-stage spatial transformer networks (STN). Our proposed method was able to achieve state-of-the-art performance in accuracy with real- time computation. Additionally, unlike other regression based grasp detection methods, our proposed method allows partial observation for intermediate results such as grasp location and orientation for a number of grasp configuration candidates.



### Accurate Facial Parts Localization and Deep Learning for 3D Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.05846v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1803.05846v1)
- **Published**: 2018-03-04 15:04:23+00:00
- **Updated**: 2018-03-04 15:04:23+00:00
- **Authors**: Asim Jan, Huaxiong Ding, Hongying Meng, Liming Chen, Huibin Li
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: Meaningful facial parts can convey key cues for both facial action unit detection and expression prediction. Textured 3D face scan can provide both detailed 3D geometric shape and 2D texture appearance cues of the face which are beneficial for Facial Expression Recognition (FER). However, accurate facial parts extraction as well as their fusion are challenging tasks. In this paper, a novel system for 3D FER is designed based on accurate facial parts extraction and deep feature fusion of facial parts. In particular, each textured 3D face scan is firstly represented as a 2D texture map and a depth map with one-to-one dense correspondence. Then, the facial parts of both texture map and depth map are extracted using a novel 4-stage process consists of facial landmark localization, facial rotation correction, facial resizing, facial parts bounding box extraction and post-processing procedures. Finally, deep fusion Convolutional Neural Networks (CNNs) features of all facial parts are learned from both texture maps and depth maps, respectively and nonlinear SVMs are used for expression prediction. Experiments are conducted on the BU-3DFE database, demonstrating the effectiveness of combing different facial parts, texture and depth cues and reporting the state-of-the-art results in comparison with all existing methods under the same setting.



### Egocentric Basketball Motion Planning from a Single First-Person Image
- **Arxiv ID**: http://arxiv.org/abs/1803.01413v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01413v1)
- **Published**: 2018-03-04 20:12:58+00:00
- **Updated**: 2018-03-04 20:12:58+00:00
- **Authors**: Gedas Bertasius, Aaron Chan, Jianbo Shi
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: We present a model that uses a single first-person image to generate an egocentric basketball motion sequence in the form of a 12D camera configuration trajectory, which encodes a player's 3D location and 3D head orientation throughout the sequence. To do this, we first introduce a future convolutional neural network (CNN) that predicts an initial sequence of 12D camera configurations, aiming to capture how real players move during a one-on-one basketball game. We also introduce a goal verifier network, which is trained to verify that a given camera configuration is consistent with the final goals of real one-on-one basketball players. Next, we propose an inverse synthesis procedure to synthesize a refined sequence of 12D camera configurations that (1) sufficiently matches the initial configurations predicted by the future CNN, while (2) maximizing the output of the goal verifier network. Finally, by following the trajectory resulting from the refined camera configuration sequence, we obtain the complete 12D motion sequence.   Our model generates realistic basketball motion sequences that capture the goals of real players, outperforming standard deep learning approaches such as recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and generative adversarial networks (GANs).



### Efficient and Accurate MRI Super-Resolution using a Generative Adversarial Network and 3D Multi-Level Densely Connected Network
- **Arxiv ID**: http://arxiv.org/abs/1803.01417v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1803.01417v3)
- **Published**: 2018-03-04 20:45:06+00:00
- **Updated**: 2018-06-09 06:14:37+00:00
- **Authors**: Yuhua Chen, Feng Shi, Anthony G. Christodoulou, Zhengwei Zhou, Yibin Xie, Debiao Li
- **Comment**: 10 pages, 2 figures, 2 tables. MICCAI 2018
- **Journal**: None
- **Summary**: High-resolution (HR) magnetic resonance images (MRI) provide detailed anatomical information important for clinical application and quantitative image analysis. However, HR MRI conventionally comes at the cost of longer scan time, smaller spatial coverage, and lower signal-to-noise ratio (SNR). Recent studies have shown that single image super-resolution (SISR), a technique to recover HR details from one single low-resolution (LR) input image, could provide high-quality image details with the help of advanced deep convolutional neural networks (CNN). However, deep neural networks consume memory heavily and run slowly, especially in 3D settings. In this paper, we propose a novel 3D neural network design, namely a multi-level densely connected super-resolution network (mDCSRN) with generative adversarial network (GAN)-guided training. The mDCSRN quickly trains and inferences and the GAN promotes realistic output hardly distinguishable from original HR images. Our results from experiments on a dataset with 1,113 subjects show that our new architecture beats other popular deep learning methods in recovering 4x resolution-downgraded im-ages and runs 6x faster.



