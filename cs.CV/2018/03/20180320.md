# Arxiv Papers in cs.CV on 2018-03-20
### DYAN: A Dynamical Atoms-Based Network for Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/1803.07201v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07201v2)
- **Published**: 2018-03-20 00:14:23+00:00
- **Updated**: 2018-09-14 21:25:54+00:00
- **Authors**: Wenqian Liu, Abhishek Sharma, Octavia Camps, Mario Sznaier
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to anticipate the future is essential when making real time critical decisions, provides valuable information to understand dynamic natural scenes, and can help unsupervised video representation learning. State-of-art video prediction is based on LSTM recursive networks and/or generative adversarial network learning. These are complex architectures that need to learn large numbers of parameters, are potentially hard to train, slow to run, and may produce blurry predictions. In this paper, we introduce DYAN, a novel network with very few parameters and easy to train, which produces accurate, high quality frame predictions, significantly faster than previous approaches. DYAN owes its good qualities to its encoder and decoder, which are designed following concepts from systems identification theory and exploit the dynamics-based invariants of the data. Extensive experiments using several standard video datasets show that DYAN is superior generating frames and that it generalizes well across domains.



### Real-time Burst Photo Selection Using a Light-Head Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1803.07212v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07212v1)
- **Published**: 2018-03-20 01:26:33+00:00
- **Updated**: 2018-03-20 01:26:33+00:00
- **Authors**: Baoyuan Wang, Noranart Vesdapunt, Utkarsh Sinha, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We present an automatic moment capture system that runs in real-time on mobile cameras. The system is designed to run in the viewfinder mode and capture a burst sequence of frames before and after the shutter is pressed. For each frame, the system predicts in real-time a "goodness" score, based on which the best moment in the burst can be selected immediately after the shutter is released, without any user interference. To solve the problem, we develop a highly efficient deep neural network ranking model, which implicitly learns a "latent relative attribute" space to capture subtle visual differences within a sequence of burst images. Then the overall goodness is computed as a linear aggregation of the goodnesses of all the latent attributes. The latent relative attributes and the aggregation function can be seamlessly integrated in one fully convolutional network and trained in an end-to-end fashion. To obtain a compact model which can run on mobile devices in real-time, we have explored and evaluated a wide range of network design choices, taking into account the constraints of model size, computational cost, and accuracy. Extensive studies show that the best frame predicted by our model hit users' top-1 (out of 11 on average) choice for $64.1\%$ cases and top-3 choices for $86.2\%$ cases. Moreover, the model(only 0.47M Bytes) can run in real time on mobile devices, e.g. only 13ms on iPhone 7 for one frame prediction.



### A Temporally-Aware Interpolation Network for Video Frame Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1803.07218v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07218v2)
- **Published**: 2018-03-20 02:01:37+00:00
- **Updated**: 2018-11-03 23:34:33+00:00
- **Authors**: Ximeng Sun, Ryan Szeto, Jason J. Corso
- **Comment**: None
- **Journal**: None
- **Summary**: We propose the first deep learning solution to video frame inpainting, a challenging instance of the general video inpainting problem with applications in video editing, manipulation, and forensics. Our task is less ambiguous than frame interpolation and video prediction because we have access to both the temporal context and a partial glimpse of the future, allowing us to better evaluate the quality of a model's predictions objectively. We devise a pipeline composed of two modules: a bidirectional video prediction module, and a temporally-aware frame interpolation module. The prediction module makes two intermediate predictions of the missing frames, one conditioned on the preceding frames and the other conditioned on the following frames, using a shared convolutional LSTM-based encoder-decoder. The interpolation module blends the intermediate predictions to form the final result. Specifically, it utilizes time information and hidden activations from the video prediction module to resolve disagreements between the predictions. Our experiments demonstrate that our approach produces more accurate and qualitatively satisfying results than a state-of-the-art video prediction method and many strong frame inpainting baselines.



### Learning the Hierarchical Parts of Objects by Deep Non-Smooth Nonnegative Matrix Factorization
- **Arxiv ID**: http://arxiv.org/abs/1803.07226v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DS, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/1803.07226v1)
- **Published**: 2018-03-20 02:39:44+00:00
- **Updated**: 2018-03-20 02:39:44+00:00
- **Authors**: Jinshi Yu, Guoxu Zhou, Andrzej Cichocki, Shengli Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Nonsmooth Nonnegative Matrix Factorization (nsNMF) is capable of producing more localized, less overlapped feature representations than other variants of NMF while keeping satisfactory fit to data. However, nsNMF as well as other existing NMF methods is incompetent to learn hierarchical features of complex data due to its shallow structure. To fill this gap, we propose a deep nsNMF method coined by the fact that it possesses a deeper architecture compared with standard nsNMF. The deep nsNMF not only gives parts-based features due to the nonnegativity constraints, but also creates higher-level, more abstract features by combing lower-level ones. The in-depth description of how deep architecture can help to efficiently discover abstract features in dnsNMF is presented. And we also show that the deep nsNMF has close relationship with the deep autoencoder, suggesting that the proposed model inherits the major advantages from both deep learning and NMF. Extensive experiments demonstrate the standout performance of the proposed method in clustering analysis.



### Hierarchical Metric Learning and Matching for 2D and 3D Geometric Correspondences
- **Arxiv ID**: http://arxiv.org/abs/1803.07231v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07231v3)
- **Published**: 2018-03-20 03:11:41+00:00
- **Updated**: 2018-08-02 03:12:43+00:00
- **Authors**: Mohammed E. Fathy, Quoc-Huy Tran, M. Zeeshan Zia, Paul Vernaza, Manmohan Chandraker
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: Interest point descriptors have fueled progress on almost every problem in computer vision. Recent advances in deep neural networks have enabled task-specific learned descriptors that outperform hand-crafted descriptors on many problems. We demonstrate that commonly used metric learning approaches do not optimally leverage the feature hierarchies learned in a Convolutional Neural Network (CNN), especially when applied to the task of geometric feature matching. While a metric loss applied to the deepest layer of a CNN, is often expected to yield ideal features irrespective of the task, in fact the growing receptive field as well as striding effects cause shallower features to be better at high precision matching tasks. We leverage this insight together with explicit supervision at multiple levels of the feature hierarchy for better regularization, to learn more effective descriptors in the context of geometric matching tasks. Further, we propose to use activation maps at different layers of a CNN, as an effective and principled replacement for the multi-resolution image pyramids often used for matching tasks. We propose concrete CNN architectures employing these ideas, and evaluate them on multiple datasets for 2D and 3D geometric matching as well as optical flow, demonstrating state-of-the-art results and generalization across datasets.



### SlideNet: Fast and Accurate Slide Quality Assessment Based on Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.07240v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07240v1)
- **Published**: 2018-03-20 03:29:03+00:00
- **Updated**: 2018-03-20 03:29:03+00:00
- **Authors**: Teng Zhang, Johanna Carvajal, Daniel F. Smith, Kun Zhao, Arnold Wiliem, Peter Hobson, Anthony Jennings, Brian C. Lovell
- **Comment**: None
- **Journal**: None
- **Summary**: This work tackles the automatic fine-grained slide quality assessment problem for digitized direct smears test using the Gram staining protocol. Automatic quality assessment can provide useful information for the pathologists and the whole digital pathology workflow. For instance, if the system found a slide to have a low staining quality, it could send a request to the automatic slide preparation system to remake the slide. If the system detects severe damage in the slides, it could notify the experts that manual microscope reading may be required. In order to address the quality assessment problem, we propose a deep neural network based framework to automatically assess the slide quality in a semantic way. Specifically, the first step of our framework is to perform dense fine-grained region classification on the whole slide and calculate the region distribution histogram. Next, our framework will generate assessments of the slide quality from various perspectives: staining quality, information density, damage level and which regions are more valuable for subsequent high-magnification analysis. To make the information more accessible, we present our results in the form of a heat map and text summaries. Additionally, in order to stimulate research in this direction, we propose a novel dataset for slide quality assessment. Experiments show that the proposed framework outperforms recent related works.



### 3D Point Cloud Denoising using Graph Laplacian Regularization of a Low Dimensional Manifold Model
- **Arxiv ID**: http://arxiv.org/abs/1803.07252v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07252v2)
- **Published**: 2018-03-20 04:29:46+00:00
- **Updated**: 2019-04-30 05:39:28+00:00
- **Authors**: Jin Zeng, Gene Cheung, Michael Ng, Jiahao Pang, Cheng Yang
- **Comment**: None
- **Journal**: None
- **Summary**: 3D point cloud - a new signal representation of volumetric objects - is a discrete collection of triples marking exterior object surface locations in 3D space. Conventional imperfect acquisition processes of 3D point cloud - e.g., stereo-matching from multiple viewpoint images or depth data acquired directly from active light sensors - imply non-negligible noise in the data. In this paper, we adopt a previously proposed low-dimensional manifold model for the surface patches in the point cloud and seek self-similar patches to denoise them simultaneously using the patch manifold prior. Due to discrete observations of the patches on the manifold, we approximate the manifold dimension computation defined in the continuous domain with a patch-based graph Laplacian regularizer and propose a new discrete patch distance measure to quantify the similarity between two same-sized surface patches for graph construction that is robust to noise. We show that our graph Laplacian regularizer has a natural graph spectral interpretation, and has desirable numerical stability properties via eigenanalysis. Extensive simulation results show that our proposed denoising scheme can outperform state-of-the-art methods in objective metrics and can better preserve visually salient structural features like edges.



### Transferring Rich Deep Features for Facial Beauty Prediction
- **Arxiv ID**: http://arxiv.org/abs/1803.07253v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07253v1)
- **Published**: 2018-03-20 04:39:28+00:00
- **Updated**: 2018-03-20 04:39:28+00:00
- **Authors**: Lu Xu, Jinhai Xiang, Xiaohui Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Feature extraction plays a significant part in computer vision tasks. In this paper, we propose a method which transfers rich deep features from a pretrained model on face verification task and feeds the features into Bayesian ridge regression algorithm for facial beauty prediction. We leverage the deep neural networks that extracts more abstract features from stacked layers. Through simple but effective feature fusion strategy, our method achieves improved or comparable performance on SCUT-FBP dataset and ECCV HotOrNot dataset. Our experiments demonstrate the effectiveness of the proposed method and clarify the inner interpretability of facial beauty perception.



### Learning Dynamic Memory Networks for Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1803.07268v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07268v2)
- **Published**: 2018-03-20 06:20:15+00:00
- **Updated**: 2018-09-02 05:09:02+00:00
- **Authors**: Tianyu Yang, Antoni B. Chan
- **Comment**: ECCV2018 camera ready. Code is available at
  https://github.com/skyoung/MemTrack
- **Journal**: None
- **Summary**: Template-matching methods for visual tracking have gained popularity recently due to their comparable performance and fast speed. However, they lack effective ways to adapt to changes in the target object's appearance, making their tracking accuracy still far from state-of-the-art. In this paper, we propose a dynamic memory network to adapt the template to the target's appearance variations during tracking. An LSTM is used as a memory controller, where the input is the search feature map and the outputs are the control signals for the reading and writing process of the memory block. As the location of the target is at first unknown in the search feature map, an attention mechanism is applied to concentrate the LSTM input on the potential target. To prevent aggressive model adaptivity, we apply gated residual template learning to control the amount of retrieved memory that is used to combine with the initial template. Unlike tracking-by-detection methods where the object's information is maintained by the weight parameters of neural networks, which requires expensive online fine-tuning to be adaptable, our tracker runs completely feed-forward and adapts to the target's appearance changes by updating the external memory. Moreover, unlike other tracking methods where the model capacity is fixed after offline training --- the capacity of our tracker can be easily enlarged as the memory requirements of a task increase, which is favorable for memorizing long-term object information. Extensive experiments on OTB and VOT demonstrates that our tracker MemTrack performs favorably against state-of-the-art tracking methods while retaining real-time speed of 50 fps.



### A Neural Markovian Multiresolution Image Labeling Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1804.04540v2
- **DOI**: 10.1007/978-3-030-52246-9_27
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.04540v2)
- **Published**: 2018-03-20 06:32:35+00:00
- **Updated**: 2020-08-17 07:53:03+00:00
- **Authors**: John Mashford, Brad Lane, Vic Ciesielski, Felix Lipkin
- **Comment**: replacement provides formal evaluation of MCV algorithm in published
  form
- **Journal**: Intelligent Computing. SAI 2020. Advances in Intelligent Systems
  and Computing, vol 1229. Springer, Cham
- **Summary**: This paper describes the results of formally evaluating the MCV (Markov concurrent vision) image labeling algorithm which is a (semi-) hierarchical algorithm commencing with a partition made up of single pixel regions and merging regions or subsets of regions using a Markov random field (MRF) image model. It is an example of a general approach to computer vision called concurrent vision in which the operations of image segmentation and image classification are carried out concurrently. While many image labeling algorithms output a single partition, or segmentation, the MCV algorithm outputs a sequence of partitions and this more elaborate structure may provide information that is valuable for higher level vision systems. With certain types of MRF the component of the system for image evaluation can be implemented as a hardwired feed forward neural network. While being applicable to images (i.e. 2D signals), the algorithm is equally applicable to 1D signals (e.g. speech) or 3D signals (e.g. video sequences) (though its performance in such domains remains to be tested). The algorithm is assessed using subjective and objective criteria with very good results.



### Text Detection and Recognition in images: A survey
- **Arxiv ID**: http://arxiv.org/abs/1803.07278v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07278v2)
- **Published**: 2018-03-20 07:36:48+00:00
- **Updated**: 2018-05-02 14:45:42+00:00
- **Authors**: Tanvi Goswami, Zankhana Barad, Prof. Nikita P. Desai
- **Comment**: The paper is found to be incomplete, it was just for assignment
  purpose so it was uploaded but some extra efforts are needed and supposed to
  have many changes because as a student we might not be aware of many
  things,so it might not be useful for others to refer it
- **Journal**: None
- **Summary**: Text Detection and recognition is a one of the important aspect of image processing. This paper analyzes and compares the methods to handle this task. It summarizes the fundamental problems and enumerates factors that need consideration when addressing these problems. Existing techniques are categorized as either stepwise or integrated and sub-problems are highlighted including digit localization, verification, segmentation and recognition. Special issues associated with the enhancement of degraded text and the processing of video text and multi-oriented text are also addressed. The categories and sub-categories of text are illustrated, benchmark datasets are enumerated, and the performance of the most representative approaches is compared. This review also provides a fundamental comparison and analysis of the remaining problems in the field.



### Face Recognition Techniques: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1803.07288v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07288v6)
- **Published**: 2018-03-20 08:15:54+00:00
- **Updated**: 2021-01-30 07:34:50+00:00
- **Authors**: Raunak Dave, Ankit Vyas, Nikita P Desai
- **Comment**: Work in progress
- **Journal**: None
- **Summary**: Nowadays research has expanded to extracting auxiliary information from various biometric techniques like fingerprints, face, iris, palm and voice . This information contains some major features like gender, age, beard, mustache, scars, height, hair, skin color, glasses, weight, facial marks and tattoos. All this information contributes strongly to identification of human. The major challenges that come across face recognition are to find age & gender of the person. This paper contributes a survey of various face recognition techniques for finding the age and gender. The existing techniques are discussed based on their performances. This paper also provides future directions for further research.



### Flex-Convolution (Million-Scale Point-Cloud Learning Beyond Grid-Worlds)
- **Arxiv ID**: http://arxiv.org/abs/1803.07289v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07289v4)
- **Published**: 2018-03-20 08:18:29+00:00
- **Updated**: 2020-04-15 07:22:36+00:00
- **Authors**: Fabian Groh, Patrick Wieschollek, Hendrik P. A. Lensch
- **Comment**: accepted at ACCV 2018
- **Journal**: None
- **Summary**: Traditional convolution layers are specifically designed to exploit the natural data representation of images -- a fixed and regular grid. However, unstructured data like 3D point clouds containing irregular neighborhoods constantly breaks the grid-based data assumption. Therefore applying best-practices and design choices from 2D-image learning methods towards processing point clouds are not readily possible. In this work, we introduce a natural generalization flex-convolution of the conventional convolution layer along with an efficient GPU implementation. We demonstrate competitive performance on rather small benchmark sets using fewer parameters and lower memory consumption and obtain significant improvements on a million-scale real-world dataset. Ours is the first which allows to efficiently process 7 million points concurrently.



### Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatial-Temporal Patterns
- **Arxiv ID**: http://arxiv.org/abs/1803.07293v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07293v1)
- **Published**: 2018-03-20 08:33:08+00:00
- **Updated**: 2018-03-20 08:33:08+00:00
- **Authors**: Jianming Lv, Weihang Chen, Qing Li, Can Yang
- **Comment**: Accepted by CVPR 2018
- **Journal**: None
- **Summary**: Most of the proposed person re-identification algorithms conduct supervised training and testing on single labeled datasets with small size, so directly deploying these trained models to a large-scale real-world camera network may lead to poor performance due to underfitting. It is challenging to incrementally optimize the models by using the abundant unlabeled data collected from the target domain. To address this challenge, we propose an unsupervised incremental learning algorithm, TFusion, which is aided by the transfer learning of the pedestrians' spatio-temporal patterns in the target domain. Specifically, the algorithm firstly transfers the visual classifier trained from small labeled source dataset to the unlabeled target dataset so as to learn the pedestrians' spatial-temporal patterns. Secondly, a Bayesian fusion model is proposed to combine the learned spatio-temporal patterns with visual features to achieve a significantly improved classifier. Finally, we propose a learning-to-rank based mutual promotion procedure to incrementally optimize the classifiers based on the unlabeled data in the target domain. Comprehensive experiments based on multiple real surveillance datasets are conducted, and the results show that our algorithm gains significant improvement compared with the state-of-art cross-dataset unsupervised person re-identification algorithms.



### Segmentation of histological images and fibrosis identification with a convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1803.07301v1
- **DOI**: 10.1016/j.compbiomed.2018.05.015
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07301v1)
- **Published**: 2018-03-20 08:48:24+00:00
- **Updated**: 2018-03-20 08:48:24+00:00
- **Authors**: Xiaohang Fu, Tong Liu, Zhaohan Xiong, Bruce H. Smaill, Martin K. Stiles, Jichao Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation of histological images is one of the most crucial tasks for many biomedical analyses including quantification of certain tissue type. However, challenges are posed by high variability and complexity of structural features in such images, in addition to imaging artifacts. Further, the conventional approach of manual thresholding is labor-intensive, and highly sensitive to inter- and intra-image intensity variations. An accurate and robust automated segmentation method is of high interest. We propose and evaluate an elegant convolutional neural network (CNN) designed for segmentation of histological images, particularly those with Masson's trichrome stain. The network comprises of 11 successive convolutional - rectified linear unit - batch normalization layers, and outperformed state-of-the-art CNNs on a dataset of cardiac histological images (labeling fibrosis, myocytes, and background) with a Dice similarity coefficient of 0.947. With 100 times fewer (only 300 thousand) trainable parameters, our CNN is less susceptible to overfitting, and is efficient. Additionally, it retains image resolution from input to output, captures fine-grained details, and can be trained end-to-end smoothly. To the best of our knowledge, this is the first deep CNN tailored for the problem of concern, and may be extended to solve similar segmentation tasks to facilitate investigations into pathology and clinical treatment.



### Progressive Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/1803.07349v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07349v2)
- **Published**: 2018-03-20 10:25:06+00:00
- **Updated**: 2018-07-10 21:25:53+00:00
- **Authors**: Alex Locher, Michal Havlena, Luc Van Gool
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: Structure from Motion or the sparse 3D reconstruction out of individual photos is a long studied topic in computer vision. Yet none of the existing reconstruction pipelines fully addresses a progressive scenario where images are only getting available during the reconstruction process and intermediate results are delivered to the user. Incremental pipelines are capable of growing a 3D model but often get stuck in local minima due to wrong (binding) decisions taken based on incomplete information. Global pipelines on the other hand need the access to the complete viewgraph and are not capable of delivering intermediate results. In this paper we propose a new reconstruction pipeline working in a progressive manner rather than in a batch processing scheme. The pipeline is able to recover from failed reconstructions in early stages, avoids to take binding decisions, delivers a progressive output and yet maintains the capabilities of existing pipelines. We demonstrate and evaluate our method on diverse challenging public and dedicated datasets including those with highly symmetric structures and compare to the state of the art.



### Discrete Potts Model for Generating Superpixels on Noisy Images
- **Arxiv ID**: http://arxiv.org/abs/1803.07351v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07351v1)
- **Published**: 2018-03-20 10:32:55+00:00
- **Updated**: 2018-03-20 10:32:55+00:00
- **Authors**: Ruobing Shen, Xiaoyu Chen, Xiangrui Zheng, Gerhard Reinelt
- **Comment**: 23 pages
- **Journal**: None
- **Summary**: Many computer vision applications, such as object recognition and segmentation, increasingly build on superpixels. However, there have been so far few superpixel algorithms that systematically deal with noisy images. We propose to first decompose the image into equal-sized rectangular patches, which also sets the maximum superpixel size. Within each patch, a Potts model for simultaneous segmentation and denoising is applied, that guarantees connected and non-overlapping superpixels and also produces a denoised image. The corresponding optimization problem is formulated as a mixed integer linear program (MILP), and solved by a commercial solver. Extensive experiments on the BSDS500 dataset images with noises are compared with other state-of-the-art superpixel methods. Our method achieves the best result in terms of a combined score (OP) composed of the under-segmentation error, boundary recall and compactness.



### Adaptive Co-weighting Deep Convolutional Features For Object Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1803.07360v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07360v1)
- **Published**: 2018-03-20 10:47:09+00:00
- **Updated**: 2018-03-20 10:47:09+00:00
- **Authors**: Jiaxing Wang, Jihua Zhu, Shanmin Pang, Zhongyu Li, Yaochen Li, Xueming Qian
- **Comment**: 6 pages,5 figures,ICME2018 poster
- **Journal**: None
- **Summary**: Aggregating deep convolutional features into a global image vector has attracted sustained attention in image retrieval. In this paper, we propose an efficient unsupervised aggregation method that uses an adaptive Gaussian filter and an elementvalue sensitive vector to co-weight deep features. Specifically, the Gaussian filter assigns large weights to features of region-of-interests (RoI) by adaptively determining the RoI's center, while the element-value sensitive channel vector suppresses burstiness phenomenon by assigning small weights to feature maps with large sum values of all locations. Experimental results on benchmark datasets validate the proposed two weighting schemes both effectively improve the discrimination power of image vectors. Furthermore, with the same experimental setting, our method outperforms other very recent aggregation approaches by a considerable margin.



### Are you eligible? Predicting adulthood from face images via class specific mean autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1803.07385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07385v1)
- **Published**: 2018-03-20 11:58:40+00:00
- **Updated**: 2018-03-20 11:58:40+00:00
- **Authors**: Maneet Singh, Shruti Nagpal, Mayank Vatsa, Richa Singh
- **Comment**: Accepted for publication in Pattern Recognition Letters
- **Journal**: None
- **Summary**: Predicting if a person is an adult or a minor has several applications such as inspecting underage driving, preventing purchase of alcohol and tobacco by minors, and granting restricted access. The challenging nature of this problem arises due to the complex and unique physiological changes that are observed with age progression. This paper presents a novel deep learning based formulation, termed as Class Specific Mean Autoencoder, to learn the intra-class similarity and extract class-specific features. We propose that the feature of a particular class if brought similar/closer to the mean feature of that class can help in learning class-specific representations. The proposed formulation is applied for the task of adulthood classification which predicts whether the given face image is of an adult or not. Experiments are performed on two large databases and the results show that the proposed algorithm yields higher classification accuracy compared to existing algorithms and a Commercial-Off-The-Shelf system.



### Residual Codean Autoencoder for Facial Attribute Analysis
- **Arxiv ID**: http://arxiv.org/abs/1803.07386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07386v1)
- **Published**: 2018-03-20 12:05:33+00:00
- **Updated**: 2018-03-20 12:05:33+00:00
- **Authors**: Akshay Sethi, Maneet Singh, Richa Singh, Mayank Vatsa
- **Comment**: Accepted in Pattern Recognition Letters
- **Journal**: None
- **Summary**: Facial attributes can provide rich ancillary information which can be utilized for different applications such as targeted marketing, human computer interaction, and law enforcement. This research focuses on facial attribute prediction using a novel deep learning formulation, termed as R-Codean autoencoder. The paper first presents Cosine similarity based loss function in an autoencoder which is then incorporated into the Euclidean distance based autoencoder to formulate R-Codean. The proposed loss function thus aims to incorporate both magnitude and direction of image vectors during feature learning. Further, inspired by the utility of shortcut connections in deep models to facilitate learning of optimal parameters, without incurring the problem of vanishing gradient, the proposed formulation is extended to incorporate shortcut connections in the architecture. The proposed R-Codean autoencoder is utilized in facial attribute prediction framework which incorporates patch-based weighting mechanism for assigning higher weights to relevant patches for each attribute. The experimental results on publicly available CelebA and LFWA datasets demonstrate the efficacy of the proposed approach in addressing this challenging problem.



### Patch-Based Image Inpainting with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.07422v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07422v1)
- **Published**: 2018-03-20 13:38:52+00:00
- **Updated**: 2018-03-20 13:38:52+00:00
- **Authors**: Ugur Demir, Gozde Unal
- **Comment**: None
- **Journal**: None
- **Summary**: Area of image inpainting over relatively large missing regions recently advanced substantially through adaptation of dedicated deep neural networks. However, current network solutions still introduce undesired artifacts and noise to the repaired regions. We present an image inpainting method that is based on the celebrated generative adversarial network (GAN) framework. The proposed PGGAN method includes a discriminator network that combines a global GAN (G-GAN) architecture with a patchGAN approach. PGGAN first shares network layers between G-GAN and patchGAN, then splits paths to produce two adversarial losses that feed the generator network in order to capture both local continuity of image texture and pervasive global features in images. The proposed framework is evaluated extensively, and the results including comparison to recent state-of-the-art demonstrate that it achieves considerable improvements on both visual and quantitative evaluations.



### A Distance Oriented Kalman Filter Particle Swarm Optimizer Applied to Multi-Modality Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1803.07423v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1803.07423v1)
- **Published**: 2018-03-20 13:40:36+00:00
- **Updated**: 2018-03-20 13:40:36+00:00
- **Authors**: Chengjia Wang, Keith A. Goatman, James Boardman, Erin Beveridge, David Newby, Scott Semple
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we describe improvements to the particle swarm optimizer (PSO) made by inclusion of an unscented Kalman filter to guide particle motion. We demonstrate the effectiveness of the unscented Kalman filter PSO by comparing it with the original PSO algorithm and its variants designed to improve performance. The PSOs were tested firstly on a number of common synthetic benchmarking functions, and secondly applied to a practical three-dimensional image registration problem. The proposed methods displayed better performances for 4 out of 8 benchmark functions, and reduced the target registration errors by at least 2mm when registering down-sampled benchmark brain images. Our methods also demonstrated an ability to align images featuring motion related artefacts which all other methods failed to register. These new PSO methods provide a novel, efficient mechanism to integrate prior knowledge into each iteration of the optimization process, which can enhance the accuracy and speed of convergence in the application of medical image registration.



### Ocean Eddy Identification and Tracking using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.07436v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07436v2)
- **Published**: 2018-03-20 13:48:40+00:00
- **Updated**: 2018-05-15 09:33:38+00:00
- **Authors**: Katharina Franz, Ribana Roscher, Andres Milioto, Susanne Wenzel, Jürgen Kusche
- **Comment**: accepted for International Geoscience and Remote Sensing Symposium
  2018
- **Journal**: None
- **Summary**: Global climate change plays an essential role in our daily life. Mesoscale ocean eddies have a significant impact on global warming, since they affect the ocean dynamics, the energy as well as the mass transports of ocean circulation. From satellite altimetry we can derive high-resolution, global maps containing ocean signals with dominating coherent eddy structures. The aim of this study is the development and evaluation of a deep-learning based approach for the analysis of eddies. In detail, we develop an eddy identification and tracking framework with two different approaches that are mainly based on feature learning with convolutional neural networks. Furthermore, state-of-the-art image processing tools and object tracking methods are used to support the eddy tracking. In contrast to previous methods, our framework is able to learn a representation of the data in which eddies can be detected and tracked in more objective and robust way. We show the detection and tracking results on sea level anomalies (SLA) data from the area of Australia and the East Australia current, and compare our two eddy detection and tracking approaches to identify the most robust and objective method.



### Semi-Blind Spatially-Variant Deconvolution in Optical Microscopy with Local Point Spread Function Estimation By Use Of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.07452v4
- **DOI**: 10.1109/ICIP.2018.8451736
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.07452v4)
- **Published**: 2018-03-20 14:29:12+00:00
- **Updated**: 2019-05-17 18:17:54+00:00
- **Authors**: Adrian Shajkofci, Michael Liebling
- **Comment**: 2018/02/11: submitted to IEEE ICIP 2018 - 2018/05/04: accepted to
  IEEE ICIP 2018
- **Journal**: 2018 25th IEEE International Conference on Image Processing (ICIP)
- **Summary**: We present a semi-blind, spatially-variant deconvolution technique aimed at optical microscopy that combines a local estimation step of the point spread function (PSF) and deconvolution using a spatially variant, regularized Richardson-Lucy algorithm. To find the local PSF map in a computationally tractable way, we train a convolutional neural network to perform regression of an optical parametric model on synthetically blurred image patches. We deconvolved both synthetic and experimentally-acquired data, and achieved an improvement of image SNR of 1.00 dB on average, compared to other deconvolution algorithms.



### Speech-Driven Facial Reenactment Using Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.07461v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07461v1)
- **Published**: 2018-03-20 14:42:49+00:00
- **Updated**: 2018-03-20 14:42:49+00:00
- **Authors**: Seyed Ali Jalalifar, Hosein Hasani, Hamid Aghajan
- **Comment**: Submitted for ECCV 2018
- **Journal**: None
- **Summary**: We present a novel approach to generating photo-realistic images of a face with accurate lip sync, given an audio input. By using a recurrent neural network, we achieved mouth landmarks based on audio features. We exploited the power of conditional generative adversarial networks to produce highly-realistic face conditioned on a set of landmarks. These two networks together are capable of producing a sequence of natural faces in sync with an input audio track.



### VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions
- **Arxiv ID**: http://arxiv.org/abs/1803.07464v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07464v2)
- **Published**: 2018-03-20 14:50:03+00:00
- **Updated**: 2018-08-25 04:07:24+00:00
- **Authors**: Qing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, Jiebo Luo
- **Comment**: ECCV 2018 Camera ready
- **Journal**: None
- **Summary**: Most existing works in visual question answering (VQA) are dedicated to improving the accuracy of predicted answers, while disregarding the explanations. We argue that the explanation for an answer is of the same or even more importance compared with the answer itself, since it makes the question and answering process more understandable and traceable. To this end, we propose a new task of VQA-E (VQA with Explanation), where the computational models are required to generate an explanation with the predicted answer. We first construct a new dataset, and then frame the VQA-E problem in a multi-task learning architecture. Our VQA-E dataset is automatically derived from the VQA v2 dataset by intelligently exploiting the available captions. We have conducted a user study to validate the quality of explanations synthesized by our method. We quantitatively show that the additional supervision from explanations can not only produce insightful textual sentences to justify the answers, but also improve the performance of answer prediction. Our model outperforms the state-of-the-art methods by a clear margin on the VQA v2 dataset.



### MAGSAC: marginalizing sample consensus
- **Arxiv ID**: http://arxiv.org/abs/1803.07469v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07469v2)
- **Published**: 2018-03-20 15:01:11+00:00
- **Updated**: 2019-06-04 23:06:14+00:00
- **Authors**: Daniel Barath, Jana Noskova, Jiri Matas
- **Comment**: None
- **Journal**: None
- **Summary**: A method called, sigma-consensus, is proposed to eliminate the need for a user-defined inlier-outlier threshold in RANSAC. Instead of estimating the noise sigma, it is marginalized over a range of noise scales. The optimized model is obtained by weighted least-squares fitting where the weights come from the marginalization over sigma of the point likelihoods of being inliers. A new quality function is proposed not requiring sigma and, thus, a set of inliers to determine the model quality. Also, a new termination criterion for RANSAC is built on the proposed marginalization approach. Applying sigma-consensus, MAGSAC is proposed with no need for a user-defined sigma and improving the accuracy of robust estimation significantly. It is superior to the state-of-the-art in terms of geometric accuracy on publicly available real-world datasets for epipolar geometry (F and E) and homography estimation. In addition, applying sigma-consensus only once as a post-processing step to the RANSAC output always improved the model quality on a wide range of vision problems without noticeable deterioration in processing time, adding a few milliseconds. The source code is at https://github.com/danini/magsac.



### An Improved Evaluation Framework for Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.07474v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07474v3)
- **Published**: 2018-03-20 15:09:09+00:00
- **Updated**: 2018-07-19 23:55:22+00:00
- **Authors**: Shaohui Liu, Yi Wei, Jiwen Lu, Jie Zhou
- **Comment**: 21 pages, 9 figures, 8 tables
- **Journal**: None
- **Summary**: In this paper, we propose an improved quantitative evaluation framework for Generative Adversarial Networks (GANs) on generating domain-specific images, where we improve conventional evaluation methods on two levels: the feature representation and the evaluation metric. Unlike most existing evaluation frameworks which transfer the representation of ImageNet inception model to map images onto the feature space, our framework uses a specialized encoder to acquire fine-grained domain-specific representation. Moreover, for datasets with multiple classes, we propose Class-Aware Frechet Distance (CAFD), which employs a Gaussian mixture model on the feature space to better fit the multi-manifold feature distribution. Experiments and analysis on both the feature level and the image level were conducted to demonstrate improvements of our proposed framework over the recently proposed state-of-the-art FID method. To our best knowledge, we are the first to provide counter examples where FID gives inconsistent results with human judgments. It is shown in the experiments that our framework is able to overcome the shortness of FID and improves robustness. Code will be made available.



### Actor and Action Video Segmentation from a Sentence
- **Arxiv ID**: http://arxiv.org/abs/1803.07485v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07485v1)
- **Published**: 2018-03-20 15:29:38+00:00
- **Updated**: 2018-03-20 15:29:38+00:00
- **Authors**: Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, Cees G. M. Snoek
- **Comment**: Accepted to CVPR 2018 as oral
- **Journal**: None
- **Summary**: This paper strives for pixel-level segmentation of actors and their actions in video content. Different from existing works, which all learn to segment from a fixed vocabulary of actor and action pairs, we infer the segmentation from a natural language input sentence. This allows to distinguish between fine-grained actors in the same super-category, identify actor and action instances, and segment pairs that are outside of the actor and action vocabulary. We propose a fully-convolutional model for pixel-level actor and action segmentation using an encoder-decoder architecture optimized for video. To show the potential of actor and action video segmentation from a sentence, we extend two popular actor and action datasets with more than 7,500 natural language descriptions. Experiments demonstrate the quality of the sentence-guided segmentations, the generalization ability of our model, and its advantage for traditional actor and action segmentation compared to the state-of-the-art.



### FastDeRain: A Novel Video Rain Streak Removal Method Using Directional Gradient Priors
- **Arxiv ID**: http://arxiv.org/abs/1803.07487v3
- **DOI**: 10.1109/TIP.2018.2880512
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07487v3)
- **Published**: 2018-03-20 15:32:54+00:00
- **Updated**: 2018-07-16 16:24:30+00:00
- **Authors**: Tai-Xiang Jiang, Ting-Zhu Huang, Xi-Le Zhao, Liang-Jian Deng, Yao Wang
- **Comment**: codes available at https://github.com/TaiXiangJiang/FastDeRain
- **Journal**: None
- **Summary**: Rain streak removal is an important issue in outdoor vision systems and has recently been investigated extensively. In this paper, we propose a novel video rain streak removal approach FastDeRain, which fully considers the discriminative characteristics of rain streaks and the clean video in the gradient domain. Specifically, on the one hand, rain streaks are sparse and smooth along the direction of the raindrops, whereas on the other hand, clean videos exhibit piecewise smoothness along the rain-perpendicular direction and continuity along the temporal direction. Theses smoothness and continuity results in the sparse distribution in the different directional gradient domain, respectively. Thus, we minimize 1) the $\ell_1$ norm to enhance the sparsity of the underlying rain streaks, 2) two $\ell_1$ norm of unidirectional Total Variation (TV) regularizers to guarantee the anisotropic spatial smoothness, and 3) an $\ell_1$ norm of the time-directional difference operator to characterize the temporal continuity. A split augmented Lagrangian shrinkage algorithm (SALSA) based algorithm is designed to solve the proposed minimization model. Experiments conducted on synthetic and real data demonstrate the effectiveness and efficiency of the proposed method. According to comprehensive quantitative performance measures, our approach outperforms other state-of-the-art methods especially on account of the running time. The code of FastDeRain can be downloaded at https://github.com/TaiXiangJiang/FastDeRain.



### Fusion of stereo and still monocular depth estimates in a self-supervised learning context
- **Arxiv ID**: http://arxiv.org/abs/1803.07512v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.07512v1)
- **Published**: 2018-03-20 16:24:21+00:00
- **Updated**: 2018-03-20 16:24:21+00:00
- **Authors**: Diogo Martins, Kevin van Hecke, Guido de Croon
- **Comment**: To be published at ICRA 2018, 8 pages, 8 figures
- **Journal**: None
- **Summary**: We study how autonomous robots can learn by themselves to improve their depth estimation capability. In particular, we investigate a self-supervised learning setup in which stereo vision depth estimates serve as targets for a convolutional neural network (CNN) that transforms a single still image to a dense depth map. After training, the stereo and mono estimates are fused with a novel fusion method that preserves high confidence stereo estimates, while leveraging the CNN estimates in the low-confidence regions. The main contribution of the article is that it is shown that the fused estimates lead to a higher performance than the stereo vision estimates alone. Experiments are performed on the KITTI dataset, and on board of a Parrot SLAMDunk, showing that even rather limited CNNs can help provide stereo vision equipped robots with more reliable depth maps for autonomous navigation.



### C3PO: Database and Benchmark for Early-stage Malicious Activity Detection in 3D Printing
- **Arxiv ID**: http://arxiv.org/abs/1803.07544v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07544v2)
- **Published**: 2018-03-20 17:41:50+00:00
- **Updated**: 2018-08-02 21:28:36+00:00
- **Authors**: Zhe Li, Xiaolong Ma, Hongjia Li, Qiyuan An, Aditya Singh Rathore, Qinru Qiu, Wenyao Xu, Yanzhi Wang
- **Comment**: The paper contains error, and the project is suspended, the results
  will be not updated in the near future, withdraw is better option than
  replace
- **Journal**: None
- **Summary**: Increasing malicious users have sought practices to leverage 3D printing technology to produce unlawful tools in criminal activities. Current regulations are inadequate to deal with the rapid growth of 3D printers. It is of vital importance to enable 3D printers to identify the objects to be printed, so that the manufacturing procedure of an illegal weapon can be terminated at the early stage. Deep learning yields significant rises in performance in the object recognition tasks. However, the lack of large-scale databases in 3D printing domain stalls the advancement of automatic illegal weapon recognition.   This paper presents a new 3D printing image database, namely C3PO, which compromises two subsets for the different system working scenarios. We extract images from the numerical control programming code files of 22 3D models, and then categorize the images into 10 distinct labels. The first set consists of 62,200 images which represent the object projections on the three planes in a Cartesian coordinate system. And the second sets consists of sequences of total 671,677 images to simulate the cameras' captures of the printed objects. Importantly, we demonstrate that the weapons can be recognized in either scenario using deep learning based approaches using our proposed database. % We also use the trained deep models to build a prototype of object-aware 3D printer. The quantitative results are promising, and the future exploration of the database and the crime prevention in 3D printing are demanding tasks.



### Learning Category-Specific Mesh Reconstruction from Image Collections
- **Arxiv ID**: http://arxiv.org/abs/1803.07549v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07549v2)
- **Published**: 2018-03-20 17:44:30+00:00
- **Updated**: 2018-07-30 18:22:59+00:00
- **Authors**: Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, Jitendra Malik
- **Comment**: Project URL: https://akanazawa.github.io/cmr/
- **Journal**: None
- **Summary**: We present a learning framework for recovering the 3D shape, camera, and texture of an object from a single image. The shape is represented as a deformable 3D mesh model of an object category where a shape is parameterized by a learned mean shape and per-instance predicted deformation. Our approach allows leveraging an annotated image collection for training, where the deformable model and the 3D prediction mechanism are learned without relying on ground-truth 3D or multi-view supervision. Our representation enables us to go beyond existing 3D prediction approaches by incorporating texture inference as prediction of an image in a canonical appearance space. Additionally, we show that semantic keypoints can be easily associated with the predicted shapes. We present qualitative and quantitative results of our approach on CUB and PASCAL3D datasets and show that we can learn to predict diverse shapes and textures across objects using only annotated image collections. The project website can be found at https://akanazawa.github.io/cmr/.



### Thermal to Visible Synthesis of Face Images using Multiple Regions
- **Arxiv ID**: http://arxiv.org/abs/1803.07599v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07599v1)
- **Published**: 2018-03-20 18:41:30+00:00
- **Updated**: 2018-03-20 18:41:30+00:00
- **Authors**: Benjamin S. Riggan, Nathaniel J. Short, Shuowen Hu
- **Comment**: Accepted IEEE WACV 2018, received best paper award
- **Journal**: None
- **Summary**: Synthesis of visible spectrum faces from thermal facial imagery is a promising approach for heterogeneous face recognition; enabling existing face recognition software trained on visible imagery to be leveraged, and allowing human analysts to verify cross-spectrum matches more effectively. We propose a new synthesis method to enhance the discriminative quality of synthesized visible face imagery by leveraging both global (e.g., entire face) and local regions (e.g., eyes, nose, and mouth). Here, each region provides (1) an independent representation for the corresponding area, and (2) additional regularization terms, which impact the overall quality of synthesized images. We analyze the effects of using multiple regions to synthesize a visible face image from a thermal face. We demonstrate that our approach improves cross-spectrum verification rates over recently published synthesis approaches. Moreover, using our synthesized imagery, we report the results on facial landmark detection-commonly used for image registration-which is a critical part of the face recognition process.



### A Survey of Deep Learning Techniques for Mobile Robot Applications
- **Arxiv ID**: http://arxiv.org/abs/1803.07608v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.07608v1)
- **Published**: 2018-03-20 19:12:05+00:00
- **Updated**: 2018-03-20 19:12:05+00:00
- **Authors**: Jahanzaib Shabbir, Tarique Anwer
- **Comment**: None
- **Journal**: None
- **Summary**: Advancements in deep learning over the years have attracted research into how deep artificial neural networks can be used in robotic systems. This research survey will present a summarization of the current research with a specific focus on the gains and obstacles for deep learning to be applied to mobile robotics.



### IntPhys: A Framework and Benchmark for Visual Intuitive Physics Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1803.07616v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.07616v3)
- **Published**: 2018-03-20 19:29:46+00:00
- **Updated**: 2020-02-11 10:05:20+00:00
- **Authors**: Ronan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, Véronique Izard, Emmanuel Dupoux
- **Comment**: None
- **Journal**: None
- **Summary**: In order to reach human performance on complexvisual tasks, artificial systems need to incorporate a sig-nificant amount of understanding of the world in termsof macroscopic objects, movements, forces, etc. Inspiredby work on intuitive physics in infants, we propose anevaluation benchmark which diagnoses how much a givensystem understands about physics by testing whether itcan tell apart well matched videos of possible versusimpossible events constructed with a game engine. Thetest requires systems to compute a physical plausibilityscore over an entire video. It is free of bias and cantest a range of basic physical reasoning concepts. Wethen describe two Deep Neural Networks systems aimedat learning intuitive physics in an unsupervised way,using only physically possible videos. The systems aretrained with a future semantic mask prediction objectiveand tested on the possible versus impossible discrimi-nation task. The analysis of their results compared tohuman data gives novel insights in the potentials andlimitations of next frame prediction architectures.



### Dynamic Filtering with Large Sampling Field for ConvNets
- **Arxiv ID**: http://arxiv.org/abs/1803.07624v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.07624v3)
- **Published**: 2018-03-20 19:52:16+00:00
- **Updated**: 2019-09-09 14:37:15+00:00
- **Authors**: Jialin Wu, Dai Li, Yu Yang, Chandrajit Bajaj, Xiangyang Ji
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: We propose a dynamic filtering strategy with large sampling field for ConvNets (LS-DFN), where the position-specific kernels learn from not only the identical position but also multiple sampled neighbor regions. During sampling, residual learning is introduced to ease training and an attention mechanism is applied to fuse features from different samples. Such multiple samples enlarge the kernels' receptive fields significantly without requiring more parameters. While LS-DFN inherits the advantages of DFN, namely avoiding feature map blurring by position-wise kernels while keeping translation invariance, it also efficiently alleviates the overfitting issue caused by much more parameters than normal CNNs. Our model is efficient and can be trained end-to-end via standard back-propagation. We demonstrate the merits of our LS-DFN on both sparse and dense prediction tasks involving object detection, semantic segmentation, and flow estimation. Our results show LS-DFN enjoys stronger recognition abilities in object detection and semantic segmentation tasks on VOC benchmark and sharper responses in flow estimation on FlyingChairs dataset compared to strong baselines.



### Product Characterisation towards Personalisation: Learning Attributes from Unstructured Data to Recommend Fashion Products
- **Arxiv ID**: http://arxiv.org/abs/1803.07679v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CL, cs.CV, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.07679v1)
- **Published**: 2018-03-20 22:25:29+00:00
- **Updated**: 2018-03-20 22:25:29+00:00
- **Authors**: Ângelo Cardoso, Fabio Daolio, Saúl Vargas
- **Comment**: Under submission
- **Journal**: None
- **Summary**: In this paper, we describe a solution to tackle a common set of challenges in e-commerce, which arise from the fact that new products are continually being added to the catalogue. The challenges involve properly personalising the customer experience, forecasting demand and planning the product range. We argue that the foundational piece to solve all of these problems is having consistent and detailed information about each product, information that is rarely available or consistent given the multitude of suppliers and types of products. We describe in detail the architecture and methodology implemented at ASOS, one of the world's largest fashion e-commerce retailers, to tackle this problem. We then show how this quantitative understanding of the products can be leveraged to improve recommendations in a hybrid recommender system approach.



### A Feature-Driven Active Framework for Ultrasound-Based Brain Shift Compensation
- **Arxiv ID**: http://arxiv.org/abs/1803.07682v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07682v1)
- **Published**: 2018-03-20 22:50:37+00:00
- **Updated**: 2018-03-20 22:50:37+00:00
- **Authors**: Jie Luo, Matt Toews, Ines Machado, Sarah Frisken, Miaomiao Zhang, Frank Preiswerk, Alireza Sedghi, Hongyi Ding, Steve Pieper, Polina Golland, Alexandra Golby, Masashi Sugiyama, William M. Wells III
- **Comment**: None
- **Journal**: None
- **Summary**: A reliable Ultrasound (US)-to-US registration method to compensate for brain shift would substantially improve Image-Guided Neurological Surgery. Developing such a registration method is very challenging, due to factors such as missing correspondence in images, the complexity of brain pathology and the demand for fast computation. We propose a novel feature-driven active framework. Here, landmarks and their displacement are first estimated from a pair of US images using corresponding local image features. Subsequently, a Gaussian Process (GP) model is used to interpolate a dense deformation field from the sparse landmarks. Kernels of the GP are estimated by using variograms and a discrete grid search method. If necessary, the user can actively add new landmarks based on the image context and visualization of the uncertainty measure provided by the GP to further improve the result. We retrospectively demonstrate our registration framework as a robust and accurate brain shift compensation solution on clinical data acquired during neurosurgery.



