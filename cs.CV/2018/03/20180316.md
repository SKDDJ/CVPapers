# Arxiv Papers in cs.CV on 2018-03-16
### Zero-Shot Object Detection: Learning to Simultaneously Recognize and Localize Novel Concepts
- **Arxiv ID**: http://arxiv.org/abs/1803.06049v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06049v1)
- **Published**: 2018-03-16 01:18:29+00:00
- **Updated**: 2018-03-16 01:18:29+00:00
- **Authors**: Shafin Rahman, Salman Khan, Fatih Porikli
- **Comment**: None
- **Journal**: ACCV 2018
- **Summary**: Current Zero-Shot Learning (ZSL) approaches are restricted to recognition of a single dominant unseen object category in a test image. We hypothesize that this setting is ill-suited for real-world applications where unseen objects appear only as a part of a complex scene, warranting both the `recognition' and `localization' of an unseen category. To address this limitation, we introduce a new \emph{`Zero-Shot Detection'} (ZSD) problem setting, which aims at simultaneously recognizing and locating object instances belonging to novel categories without any training examples. We also propose a new experimental protocol for ZSD based on the highly challenging ILSVRC dataset, adhering to practical issues, e.g., the rarity of unseen objects. To the best of our knowledge, this is the first end-to-end deep network for ZSD that jointly models the interplay between visual and semantic domain information. To overcome the noise in the automatically derived semantic descriptions, we utilize the concept of meta-classes to design an original loss function that achieves synergy between max-margin class separation and semantic space clustering. Furthermore, we present a baseline approach extended from recognition to detection setting. Our extensive experiments show significant performance boost over the baseline on the imperative yet difficult ZSD problem.



### Deep Multiple Instance Learning for Zero-shot Image Tagging
- **Arxiv ID**: http://arxiv.org/abs/1803.06051v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06051v1)
- **Published**: 2018-03-16 01:25:04+00:00
- **Updated**: 2018-03-16 01:25:04+00:00
- **Authors**: Shafin Rahman, Salman Khan
- **Comment**: None
- **Journal**: None
- **Summary**: In-line with the success of deep learning on traditional recognition problem, several end-to-end deep models for zero-shot recognition have been proposed in the literature. These models are successful to predict a single unseen label given an input image, but does not scale to cases where multiple unseen objects are present. In this paper, we model this problem within the framework of Multiple Instance Learning (MIL). To the best of our knowledge, we propose the first end-to-end trainable deep MIL framework for the multi-label zero-shot tagging problem. Due to its novel design, the proposed framework has several interesting features: (1) Unlike previous deep MIL models, it does not use any off-line procedure (e.g., Selective Search or EdgeBoxes) for bag generation. (2) During test time, it can process any number of unseen labels given their semantic embedding vectors. (3) Using only seen labels per image as weak annotation, it can produce a bounding box for each predicted labels. We experiment with the NUS-WIDE dataset and achieve superior performance across conventional, zero-shot and generalized zero-shot tagging tasks.



### Dynamic-structured Semantic Propagation Network
- **Arxiv ID**: http://arxiv.org/abs/1803.06067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06067v1)
- **Published**: 2018-03-16 03:28:22+00:00
- **Updated**: 2018-03-16 03:28:22+00:00
- **Authors**: Xiaodan Liang, Hongfei Zhou, Eric Xing
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Semantic concept hierarchy is still under-explored for semantic segmentation due to the inefficiency and complicated optimization of incorporating structural inference into dense prediction. This lack of modeling semantic correlations also makes prior works must tune highly-specified models for each task due to the label discrepancy across datasets. It severely limits the generalization capability of segmentation models for open set concept vocabulary and annotation utilization. In this paper, we propose a Dynamic-Structured Semantic Propagation Network (DSSPN) that builds a semantic neuron graph by explicitly incorporating the semantic concept hierarchy into network construction. Each neuron represents the instantiated module for recognizing a specific type of entity such as a super-class (e.g. food) or a specific concept (e.g. pizza). During training, DSSPN performs the dynamic-structured neuron computation graph by only activating a sub-graph of neurons for each image in a principled way. A dense semantic-enhanced neural block is proposed to propagate the learned knowledge of all ancestor neurons into each fine-grained child neuron for feature evolving. Another merit of such semantic explainable structure is the ability of learning a unified model concurrently on diverse datasets by selectively activating different neuron sub-graphs for each annotation at each step. Extensive experiments on four public semantic segmentation datasets (i.e. ADE20K, COCO-Stuff, Cityscape and Mapillary) demonstrate the superiority of our DSSPN over state-of-the-art segmentation models. Moreoever, we demonstrate a universal segmentation model that is jointly trained on diverse datasets can surpass the performance of the common fine-tuning scheme for exploiting multiple domain knowledge.



### Real-time Detection, Tracking, and Classification of Moving and Stationary Objects using Multiple Fisheye Images
- **Arxiv ID**: http://arxiv.org/abs/1803.06077v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06077v2)
- **Published**: 2018-03-16 05:29:12+00:00
- **Updated**: 2018-08-31 09:16:33+00:00
- **Authors**: Iljoo Baek, Albert Davies, Geng Yan, Ragunathan, Rajkumar
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to detect pedestrians and other moving objects is crucial for an autonomous vehicle. This must be done in real-time with minimum system overhead. This paper discusses the implementation of a surround view system to identify moving as well as static objects that are close to the ego vehicle. The algorithm works on 4 views captured by fisheye cameras which are merged into a single frame. The moving object detection and tracking solution uses minimal system overhead to isolate regions of interest (ROIs) containing moving objects. These ROIs are then analyzed using a deep neural network (DNN) to categorize the moving object. With deployment and testing on a real car in urban environments, we have demonstrated the practical feasibility of the solution. The video demos of our algorithm have been uploaded to Youtube: https://youtu.be/vpoCfC724iA, https://youtu.be/2X4aqH2bMBs



### Salient Objects in Clutter: Bringing Salient Object Detection to the Foreground
- **Arxiv ID**: http://arxiv.org/abs/1803.06091v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06091v2)
- **Published**: 2018-03-16 06:52:22+00:00
- **Updated**: 2018-07-22 10:46:02+00:00
- **Authors**: Deng-Ping Fan, Ming-Ming Cheng, Jiang-Jiang Liu, Shang-Hua Gao, Qibin Hou, Ali Borji
- **Comment**: ECCV 2018
- **Journal**: ECCV 2018
- **Summary**: We provide a comprehensive evaluation of salient object detection (SOD) models. Our analysis identifies a serious design bias of existing SOD datasets which assumes that each image contains at least one clearly outstanding salient object in low clutter. The design bias has led to a saturated high performance for state-of-the-art SOD models when evaluated on existing datasets. The models, however, still perform far from being satisfactory when applied to real-world daily scenes. Based on our analyses, we first identify 7 crucial aspects that a comprehensive and balanced dataset should fulfill. Then, we propose a new high quality dataset and update the previous saliency benchmark. Specifically, our SOC (Salient Objects in Clutter) dataset, includes images with salient and non-salient objects from daily object categories. Beyond object category annotations, each salient image is accompanied by attributes that reflect common challenges in real-world scenes. Finally, we report attribute-based performance assessment on our dataset.



### A Dataset and Architecture for Visual Reasoning with a Working Memory
- **Arxiv ID**: http://arxiv.org/abs/1803.06092v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.06092v2)
- **Published**: 2018-03-16 06:53:45+00:00
- **Updated**: 2018-07-20 14:12:49+00:00
- **Authors**: Guangyu Robert Yang, Igor Ganichev, Xiao-Jing Wang, Jonathon Shlens, David Sussillo
- **Comment**: None
- **Journal**: None
- **Summary**: A vexing problem in artificial intelligence is reasoning about events that occur in complex, changing visual stimuli such as in video analysis or game play. Inspired by a rich tradition of visual reasoning and memory in cognitive psychology and neuroscience, we developed an artificial, configurable visual question and answer dataset (COG) to parallel experiments in humans and animals. COG is much simpler than the general problem of video analysis, yet it addresses many of the problems relating to visual and logical reasoning and memory -- problems that remain challenging for modern deep learning architectures. We additionally propose a deep learning architecture that performs competitively on other diagnostic VQA datasets (i.e. CLEVR) as well as easy settings of the COG dataset. However, several settings of COG result in datasets that are progressively more challenging to learn. After training, the network can zero-shot generalize to many new tasks. Preliminary analyses of the network architectures trained on COG demonstrate that the network accomplishes the task in a manner interpretable to humans.



### Inverse Visual Question Answering: A New Benchmark and VQA Diagnosis Tool
- **Arxiv ID**: http://arxiv.org/abs/1803.06936v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06936v1)
- **Published**: 2018-03-16 07:58:21+00:00
- **Updated**: 2018-03-16 07:58:21+00:00
- **Authors**: Feng Liu, Tao Xiang, Timothy M. Hospedales, Wankou Yang, Changyin Sun
- **Comment**: arXiv admin note: text overlap with arXiv:1710.03370
- **Journal**: None
- **Summary**: In recent years, visual question answering (VQA) has become topical. The premise of VQA's significance as a benchmark in AI, is that both the image and textual question need to be well understood and mutually grounded in order to infer the correct answer. However, current VQA models perhaps `understand' less than initially hoped, and instead master the easier task of exploiting cues given away in the question and biases in the answer distribution. In this paper we propose the inverse problem of VQA (iVQA). The iVQA task is to generate a question that corresponds to a given image and answer pair. We propose a variational iVQA model that can generate diverse, grammatically correct and content correlated questions that match the given answer. Based on this model, we show that iVQA is an interesting benchmark for visuo-linguistic understanding, and a more challenging alternative to VQA because an iVQA model needs to understand the image better to be successful. As a second contribution, we show how to use iVQA in a novel reinforcement learning framework to diagnose any existing VQA model by way of exposing its belief set: the set of question-answer pairs that the VQA model would predict true for a given image. This provides a completely new window into what VQA models `believe' about images. We show that existing VQA models have more erroneous beliefs than previously thought, revealing their intrinsic weaknesses. Suggestions are then made on how to address these weaknesses going forward.



### Lipschitz Constrained GANs via Boundedness and Continuity
- **Arxiv ID**: http://arxiv.org/abs/1803.06107v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06107v3)
- **Published**: 2018-03-16 08:15:09+00:00
- **Updated**: 2020-12-23 00:49:13+00:00
- **Authors**: Kanglin Liu, Guoping Qiu
- **Comment**: 20 pages, 7 figures
- **Journal**: None
- **Summary**: One of the challenges in the study of Generative Adversarial Networks (GANs) is the difficulty of its performance control. Lipschitz constraint is essential in guaranteeing training stability for GANs. Although heuristic methods such as weight clipping, gradient penalty and spectral normalization have been proposed to enforce Lipschitz constraint, it is still difficult to achieve a solution that is both practically effective and theoretically provably satisfying a Lipschitz constraint. In this paper, we introduce the boundedness and continuity ($BC$) conditions to enforce the Lipschitz constraint on the discriminator functions of GANs. We prove theoretically that GANs with discriminators meeting the BC conditions satisfy the Lipschitz constraint. We present a practically very effective implementation of a GAN based on a convolutional neural network (CNN) by forcing the CNN to satisfy the $BC$ conditions (BC-GAN). We show that as compared to recent techniques including gradient penalty and spectral normalization, BC-GANs not only have better performances but also lower computational complexity.



### Towards Image Understanding from Deep Compression without Decoding
- **Arxiv ID**: http://arxiv.org/abs/1803.06131v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06131v1)
- **Published**: 2018-03-16 09:51:09+00:00
- **Updated**: 2018-03-16 09:51:09+00:00
- **Authors**: Robert Torfason, Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, Luc Van Gool
- **Comment**: ICLR 2018 conference paper
- **Journal**: None
- **Summary**: Motivated by recent work on deep neural network (DNN)-based image compression methods showing potential improvements in image quality, savings in storage, and bandwidth reduction, we propose to perform image understanding tasks such as classification and segmentation directly on the compressed representations produced by these compression methods. Since the encoders and decoders in DNN-based compression methods are neural networks with feature-maps as internal representations of the images, we directly integrate these with architectures for image understanding. This bypasses decoding of the compressed representation into RGB space and reduces computational cost. Our study shows that accuracies comparable to networks that operate on compressed RGB images can be achieved while reducing the computational complexity up to $2\times$. Furthermore, we show that synergies are obtained by jointly training compression networks with classification networks on the compressed representations, improving image quality, classification accuracy, and segmentation performance. We find that inference from compressed representations is particularly advantageous compared to inference from compressed RGB images for aggressive compression rates.



### Patchwise object tracking via structural local sparse appearance model
- **Arxiv ID**: http://arxiv.org/abs/1803.06141v1
- **DOI**: 10.1109/ICCKE.2017.8167940
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06141v1)
- **Published**: 2018-03-16 10:06:37+00:00
- **Updated**: 2018-03-16 10:06:37+00:00
- **Authors**: Hossein Kashiyani, Shahriar B. Shokouhi
- **Comment**: 6 pages, 3 figures, Accepted by ICCKE 2017
- **Journal**: None
- **Summary**: In this paper, we propose a robust visual tracking method which exploits the relationships of targets in adjacent frames using patchwise joint sparse representation. Two sets of overlapping patches with different sizes are extracted from target candidates to construct two dictionaries with consideration of joint sparse representation. By applying this representation into structural sparse appearance model, we can take two-fold advantages. First, the correlation of target patches over time is considered. Second, using this local appearance model with different patch sizes takes into account local features of target thoroughly. Furthermore, the position of candidate patches and their occlusion levels are utilized simultaneously to obtain the final likelihood of target candidates. Evaluations on recent challenging benchmark show that our tracking method outperforms the state-of-the-art trackers.



### Object Captioning and Retrieval with Natural Language
- **Arxiv ID**: http://arxiv.org/abs/1803.06152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06152v1)
- **Published**: 2018-03-16 10:32:25+00:00
- **Updated**: 2018-03-16 10:32:25+00:00
- **Authors**: Anh Nguyen, Thanh-Toan Do, Ian Reid, Darwin G. Caldwell, Nikos G. Tsagarakis
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: We address the problem of jointly learning vision and language to understand the object in a fine-grained manner. The key idea of our approach is the use of object descriptions to provide the detailed understanding of an object. Based on this idea, we propose two new architectures to solve two related problems: object captioning and natural language-based object retrieval. The goal of the object captioning task is to simultaneously detect the object and generate its associated description, while in the object retrieval task, the goal is to localize an object given an input query. We demonstrate that both problems can be solved effectively using hybrid end-to-end CNN-LSTM networks. The experimental results on our new challenging dataset show that our methods outperform recent methods by a fair margin, while providing a detailed understanding of the object and having fast inference time. The source code will be made available.



### Semantic Segmentation of Pathological Lung Tissue with Dilated Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.06167v1
- **DOI**: 10.1109/JBHI.2018.2818620
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06167v1)
- **Published**: 2018-03-16 11:23:38+00:00
- **Updated**: 2018-03-16 11:23:38+00:00
- **Authors**: Marios Anthimopoulos, Stergios Christodoulidis, Lukas Ebner, Thomas Geiser, Andreas Christe, Stavroula Mougiakakou
- **Comment**: None
- **Journal**: None
- **Summary**: Early and accurate diagnosis of interstitial lung diseases (ILDs) is crucial for making treatment decisions, but can be challenging even for experienced radiologists. The diagnostic procedure is based on the detection and recognition of the different ILD pathologies in thoracic CT scans, yet their manifestation often appears similar. In this study, we propose the use of a deep purely convolutional neural network for the semantic segmentation of ILD patterns, as the basic component of a computer aided diagnosis (CAD) system for ILDs. The proposed CNN, which consists of convolutional layers with dilated filters, takes as input a lung CT image of arbitrary size and outputs the corresponding label map. We trained and tested the network on a dataset of 172 sparsely annotated CT scans, within a cross-validation scheme. The training was performed in an end-to-end and semi-supervised fashion, utilizing both labeled and non-labeled image regions. The experimental results show significant performance improvement with respect to the state of the art.



### The ApolloScape Open Dataset for Autonomous Driving and its Application
- **Arxiv ID**: http://arxiv.org/abs/1803.06184v4
- **DOI**: 10.1109/TPAMI.2019.2926463
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06184v4)
- **Published**: 2018-03-16 12:15:58+00:00
- **Updated**: 2019-07-04 04:10:39+00:00
- **Authors**: Xinyu Huang, Peng Wang, Xinjing Cheng, Dingfu Zhou, Qichuan Geng, Ruigang Yang
- **Comment**: Version 4: Accepted by TPAMI. Version 3: 17 pages, 10 tables, 11
  figures, added the application (DeLS-3D) based on the ApolloScape Dataset.
  Version 2: 7 pages, 6 figures, added comparison with BDD100K dataset
- **Journal**: None
- **Summary**: Autonomous driving has attracted tremendous attention especially in the past few years. The key techniques for a self-driving car include solving tasks like 3D map construction, self-localization, parsing the driving road and understanding objects, which enable vehicles to reason and act. However, large scale data set for training and system evaluation is still a bottleneck for developing robust perception models. In this paper, we present the ApolloScape dataset [1] and its applications for autonomous driving. Compared with existing public datasets from real scenes, e.g. KITTI [2] or Cityscapes [3], ApolloScape contains much large and richer labelling including holistic semantic dense point cloud for each site, stereo, per-pixel semantic labelling, lanemark labelling, instance segmentation, 3D car instance, high accurate location for every frame in various driving videos from multiple sites, cities and daytimes. For each task, it contains at lease 15x larger amount of images than SOTA datasets. To label such a complete dataset, we develop various tools and algorithms specified for each task to accelerate the labelling process, such as 3D-2D segment labeling tools, active labelling in videos etc. Depend on ApolloScape, we are able to develop algorithms jointly consider the learning and inference of multiple tasks. In this paper, we provide a sensor fusion scheme integrating camera videos, consumer-grade motion sensors (GPS/IMU), and a 3D semantic map in order to achieve robust self-localization and semantic segmentation for autonomous driving. We show that practically, sensor fusion and joint learning of multiple tasks are beneficial to achieve a more robust and accurate system. We expect our dataset and proposed relevant algorithms can support and motivate researchers for further development of multi-sensor fusion and multi-task learning in the field of computer vision.



### Triplet-Center Loss for Multi-View 3D Object Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1803.06189v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06189v1)
- **Published**: 2018-03-16 12:31:24+00:00
- **Updated**: 2018-03-16 12:31:24+00:00
- **Authors**: Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai, Xiang Bai
- **Comment**: accepted by CVPR2018
- **Journal**: None
- **Summary**: Most existing 3D object recognition algorithms focus on leveraging the strong discriminative power of deep learning models with softmax loss for the classification of 3D data, while learning discriminative features with deep metric learning for 3D object retrieval is more or less neglected. In the paper, we study variants of deep metric learning losses for 3D object retrieval, which did not receive enough attention from this area. First , two kinds of representative losses, triplet loss and center loss, are introduced which could learn more discriminative features than traditional classification loss. Then, we propose a novel loss named triplet-center loss, which can further enhance the discriminative power of the features. The proposed triplet-center loss learns a center for each class and requires that the distances between samples and centers from the same class are closer than those from different classes. Extensive experimental results on two popular 3D object retrieval benchmarks and two widely-adopted sketch-based 3D shape retrieval benchmarks consistently demonstrate the effectiveness of our proposed loss, and significant improvements have been achieved compared with the state-of-the-arts.



### Monocular Fisheye Camera Depth Estimation Using Sparse LiDAR Supervision
- **Arxiv ID**: http://arxiv.org/abs/1803.06192v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06192v3)
- **Published**: 2018-03-16 12:40:18+00:00
- **Updated**: 2018-09-24 13:21:29+00:00
- **Authors**: Varun Ravi Kumar, Stefan Milz, Martin Simon, Christian Witt, Karl Amende, Johannes Petzold, Senthil Yogamani, Timo Pech
- **Comment**: None
- **Journal**: None
- **Summary**: Near field depth estimation around a self driving car is an important function that can be achieved by four wide angle fisheye cameras having a field of view of over 180. Depth estimation based on convolutional neural networks (CNNs) produce state of the art results, but progress is hindered because depth annotation cannot be obtained manually. Synthetic datasets are commonly used but they have limitations. For instance, they do not capture the extensive variability in the appearance of objects like vehicles present in real datasets. There is also a domain shift while performing inference on natural images illustrated by many attempts to handle the domain adaptation explicitly. In this work, we explore an alternate approach of training using sparse LiDAR data as ground truth for depth estimation for fisheye camera. We built our own dataset using our self driving car setup which has a 64 beam Velodyne LiDAR and four wide angle fisheye cameras. To handle the difference in view points of LiDAR and fisheye camera, an occlusion resolution mechanism was implemented. We started with Eigen's multiscale convolutional network architecture and improved by modifying activation function and optimizer. We obtained promising results on our dataset with RMSE errors comparable to the state of the art results obtained on KITTI.



### Complex-YOLO: Real-time 3D Object Detection on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1803.06199v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06199v2)
- **Published**: 2018-03-16 12:54:40+00:00
- **Updated**: 2018-09-24 09:27:33+00:00
- **Authors**: Martin Simon, Stefan Milz, Karl Amende, Horst-Michael Gross
- **Comment**: None
- **Journal**: None
- **Summary**: Lidar based 3D object detection is inevitable for autonomous driving, because it directly links to environmental understanding and therefore builds the base for prediction and motion planning. The capacity of inferencing highly sparse 3D data in real-time is an ill-posed problem for lots of other application areas besides automated vehicles, e.g. augmented reality, personal robotics or industrial automation. We introduce Complex-YOLO, a state of the art real-time 3D object detection network on point clouds only. In this work, we describe a network that expands YOLOv2, a fast 2D standard object detector for RGB images, by a specific complex regression strategy to estimate multi-class 3D boxes in Cartesian space. Thus, we propose a specific Euler-Region-Proposal Network (E-RPN) to estimate the pose of the object by adding an imaginary and a real fraction to the regression network. This ends up in a closed complex space and avoids singularities, which occur by single angle estimations. The E-RPN supports to generalize well during training. Our experiments on the KITTI benchmark suite show that we outperform current leading methods for 3D object detection specifically in terms of efficiency. We achieve state of the art results for cars, pedestrians and cyclists by being more than five times faster than the fastest competitor. Further, our model is capable of estimating all eight KITTI-classes, including Vans, Trucks or sitting pedestrians simultaneously with high accuracy.



### Joint Recognition of Handwritten Text and Named Entities with a Neural End-to-end Model
- **Arxiv ID**: http://arxiv.org/abs/1803.06252v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1803.06252v2)
- **Published**: 2018-03-16 14:47:58+00:00
- **Updated**: 2018-03-22 12:27:08+00:00
- **Authors**: Manuel Carbonell, Mauricio Villegas, Alicia Fornés, Josep Lladós
- **Comment**: To appear in IAPR International Workshop on Document Analysis Systems
  2018 (DAS 2018)
- **Journal**: None
- **Summary**: When extracting information from handwritten documents, text transcription and named entity recognition are usually faced as separate subsequent tasks. This has the disadvantage that errors in the first module affect heavily the performance of the second module. In this work we propose to do both tasks jointly, using a single neural network with a common architecture used for plain text recognition. Experimentally, the work has been tested on a collection of historical marriage records. Results of experiments are presented to show the effect on the performance for different configurations: different ways of encoding the information, doing or not transfer learning and processing at text line or multi-line region level. The results are comparable to state of the art reported in the ICDAR 2017 Information Extraction competition, even though the proposed technique does not use any dictionaries, language modeling or post processing.



### Land cover mapping at very high resolution with rotation equivariant CNNs: towards small yet accurate models
- **Arxiv ID**: http://arxiv.org/abs/1803.06253v1
- **DOI**: 10.1016/j.isprsjprs.2018.01.021
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06253v1)
- **Published**: 2018-03-16 14:48:58+00:00
- **Updated**: 2018-03-16 14:48:58+00:00
- **Authors**: Diego Marcos, Michele Volpi, Benjamin Kellenberger, Devis Tuia
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, 2018
- **Summary**: In remote sensing images, the absolute orientation of objects is arbitrary. Depending on an object's orientation and on a sensor's flight path, objects of the same semantic class can be observed in different orientations in the same image. Equivariance to rotation, in this context understood as responding with a rotated semantic label map when subject to a rotation of the input image, is therefore a very desirable feature, in particular for high capacity models, such as Convolutional Neural Networks (CNNs). If rotation equivariance is encoded in the network, the model is confronted with a simpler task and does not need to learn specific (and redundant) weights to address rotated versions of the same object class. In this work we propose a CNN architecture called Rotation Equivariant Vector Field Network (RotEqNet) to encode rotation equivariance in the network itself. By using rotating convolutions as building blocks and passing only the the values corresponding to the maximally activating orientation throughout the network in the form of orientation encoding vector fields, RotEqNet treats rotated versions of the same object with the same filter bank and therefore achieves state-of-the-art performances even when using very small architectures trained from scratch. We test RotEqNet in two challenging sub-decimeter resolution semantic labeling problems, and show that we can perform better than a standard CNN while requiring one order of magnitude less parameters.



### Consistent sets of lines with no colorful incidence
- **Arxiv ID**: http://arxiv.org/abs/1803.06267v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV, math.CO
- **Links**: [PDF](http://arxiv.org/pdf/1803.06267v1)
- **Published**: 2018-03-16 15:25:43+00:00
- **Updated**: 2018-03-16 15:25:43+00:00
- **Authors**: Boris Bukh, Xavier Goaoc, Alfredo Hubard, Matthew Trager
- **Comment**: 20 pages, 4 color figures
- **Journal**: None
- **Summary**: We consider incidences among colored sets of lines in $\mathbb{R}^d$ and examine whether the existence of certain concurrences between lines of $k$ colors force the existence of at least one concurrence between lines of $k+1$ colors. This question is relevant for problems in 3D reconstruction in computer vision.



### Improved Part Segmentation Performance by Optimising Realism of Synthetic Images using Cycle Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.06301v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06301v1)
- **Published**: 2018-03-16 16:36:43+00:00
- **Updated**: 2018-03-16 16:36:43+00:00
- **Authors**: Ruud Barth, Jochen Hemming, Eldert J. van Henten
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we report on improved part segmentation performance using convolutional neural networks to reduce the dependency on the large amount of manually annotated empirical images. This was achieved by optimising the visual realism of synthetic agricultural images.In Part I, a cycle consistent generative adversarial network was applied to synthetic and empirical images with the objective to generate more realistic synthetic images by translating them to the empirical domain. We first hypothesise and confirm that plant part image features such as color and texture become more similar to the empirical domain after translation of the synthetic images.Results confirm this with an improved mean color distribution correlation with the empirical data prior of 0.62 and post translation of 0.90. Furthermore, the mean image features of contrast, homogeneity, energy and entropy moved closer to the empirical mean, post translation. In Part II, 7 experiments were performed using convolutional neural networks with different combinations of synthetic, synthetic translated to empirical and empirical images. We hypothesised that the translated images can be used for (i) improved learning of empirical images, and (ii) that learning without any fine-tuning with empirical images is improved by bootstrapping with translated images over bootstrapping with synthetic images. Results confirm our second and third hypotheses. First a maximum intersection-over-union performance was achieved of 0.52 when bootstrapping with translated images and fine-tuning with empirical images; an 8% increase compared to only using synthetic images. Second, training without any empirical fine-tuning resulted in an average IOU of 0.31; a 55% performance increase over previous methods that only used synthetic images.



### EVA$^2$: Exploiting Temporal Redundancy in Live Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1803.06312v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1803.06312v2)
- **Published**: 2018-03-16 16:59:47+00:00
- **Updated**: 2018-04-17 02:26:35+00:00
- **Authors**: Mark Buckler, Philip Bedoukian, Suren Jayasuriya, Adrian Sampson
- **Comment**: Appears in ISCA 2018
- **Journal**: None
- **Summary**: Hardware support for deep convolutional neural networks (CNNs) is critical to advanced computer vision in mobile and embedded devices. Current designs, however, accelerate generic CNNs; they do not exploit the unique characteristics of real-time vision. We propose to use the temporal redundancy in natural video to avoid unnecessary computation on most frames. A new algorithm, activation motion compensation, detects changes in the visual input and incrementally updates a previously-computed output. The technique takes inspiration from video compression and applies well-known motion estimation techniques to adapt to visual changes. We use an adaptive key frame rate to control the trade-off between efficiency and vision quality as the input changes. We implement the technique in hardware as an extension to existing state-of-the-art CNN accelerator designs. The new unit reduces the average energy per frame by 54.2%, 61.7%, and 87.6% for three CNNs with less than 1% loss in vision accuracy.



### Temporal Gaussian Mixture Layer for Videos
- **Arxiv ID**: http://arxiv.org/abs/1803.06316v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06316v6)
- **Published**: 2018-03-16 17:09:54+00:00
- **Updated**: 2019-08-01 18:53:33+00:00
- **Authors**: AJ Piergiovanni, Michael S. Ryoo
- **Comment**: ICML 2019
- **Journal**: None
- **Summary**: We introduce a new convolutional layer named the Temporal Gaussian Mixture (TGM) layer and present how it can be used to efficiently capture longer-term temporal information in continuous activity videos. The TGM layer is a temporal convolutional layer governed by a much smaller set of parameters (e.g., location/variance of Gaussians) that are fully differentiable. We present our fully convolutional video models with multiple TGM layers for activity detection. The extensive experiments on multiple datasets, including Charades and MultiTHUMOS, confirm the effectiveness of TGM layers, significantly outperforming the state-of-the-arts.



### Synchronisation of Partial Multi-Matchings via Non-negative Factorisations
- **Arxiv ID**: http://arxiv.org/abs/1803.06320v3
- **DOI**: 10.1016/j.patcog.2019.03.021
- **Categories**: **cs.CV**, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.06320v3)
- **Published**: 2018-03-16 17:17:05+00:00
- **Updated**: 2019-03-25 13:33:00+00:00
- **Authors**: Florian Bernard, Johan Thunberg, Jorge Goncalves, Christian Theobalt
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we study permutation synchronisation for the challenging case of partial permutations, which plays an important role for the problem of matching multiple objects (e.g. images or shapes). The term synchronisation refers to the property that the set of pairwise matchings is cycle-consistent, i.e. in the full matching case all compositions of pairwise matchings over cycles must be equal to the identity. Motivated by clustering and matrix factorisation perspectives of cycle-consistency, we derive an algorithm to tackle the permutation synchronisation problem based on non-negative factorisations. In order to deal with the inherent non-convexity of the permutation synchronisation problem, we use an initialisation procedure based on a novel rotation scheme applied to the solution of the spectral relaxation. Moreover, this rotation scheme facilitates a convenient Euclidean projection to obtain a binary solution after solving our relaxed problem. In contrast to state-of-the-art methods, our approach is guaranteed to produce cycle-consistent results. We experimentally demonstrate the efficacy of our method and show that it achieves better results compared to existing methods.



### Learning deep structured active contours end-to-end
- **Arxiv ID**: http://arxiv.org/abs/1803.06329v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06329v1)
- **Published**: 2018-03-16 17:30:37+00:00
- **Updated**: 2018-03-16 17:30:37+00:00
- **Authors**: Diego Marcos, Devis Tuia, Benjamin Kellenberger, Lisa Zhang, Min Bai, Renjie Liao, Raquel Urtasun
- **Comment**: To appear, CVPR 2018
- **Journal**: None
- **Summary**: The world is covered with millions of buildings, and precisely knowing each instance's position and extents is vital to a multitude of applications. Recently, automated building footprint segmentation models have shown superior detection accuracy thanks to the usage of Convolutional Neural Networks (CNN). However, even the latest evolutions struggle to precisely delineating borders, which often leads to geometric distortions and inadvertent fusion of adjacent building instances. We propose to overcome this issue by exploiting the distinct geometric properties of buildings. To this end, we present Deep Structured Active Contours (DSAC), a novel framework that integrates priors and constraints into the segmentation process, such as continuous boundaries, smooth edges, and sharp corners. To do so, DSAC employs Active Contour Models (ACM), a family of constraint- and prior-based polygonal models. We learn ACM parameterizations per instance using a CNN, and show how to incorporate all components in a structured output model, making DSAC trainable end-to-end. We evaluate DSAC on three challenging building instance segmentation datasets, where it compares favorably against state-of-the-art. Code will be made available.



### Faces as Lighting Probes via Unsupervised Deep Highlight Extraction
- **Arxiv ID**: http://arxiv.org/abs/1803.06340v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06340v2)
- **Published**: 2018-03-16 17:53:06+00:00
- **Updated**: 2018-07-21 00:06:47+00:00
- **Authors**: Renjiao Yi, Chenyang Zhu, Ping Tan, Stephen Lin
- **Comment**: 42 pages, with supplementary material, to appear in ECCV 2018
- **Journal**: None
- **Summary**: We present a method for estimating detailed scene illumination using human faces in a single image. In contrast to previous works that estimate lighting in terms of low-order basis functions or distant point lights, our technique estimates illumination at a higher precision in the form of a non-parametric environment map. Based on the observation that faces can exhibit strong highlight reflections from a broad range of lighting directions, we propose a deep neural network for extracting highlights from faces, and then trace these reflections back to the scene to acquire the environment map. Since real training data for highlight extraction is very limited, we introduce an unsupervised scheme for finetuning the network on real images, based on the consistent diffuse chromaticity of a given face seen in multiple real images. In tracing the estimated highlights to the environment, we reduce the blurring effect of skin reflectance on reflected light through a deconvolution determined by prior knowledge on face material properties. Comparisons to previous techniques for highlight extraction and illumination estimation show the state-of-the-art performance of this approach on a variety of indoor and outdoor scenes.



### Semantic Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1804.00499v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.00499v1)
- **Published**: 2018-03-16 18:02:14+00:00
- **Updated**: 2018-03-16 18:02:14+00:00
- **Authors**: Hossein Hosseini, Radha Poovendran
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are known to be vulnerable to adversarial examples, i.e., images that are maliciously perturbed to fool the model. Generating adversarial examples has been mostly limited to finding small perturbations that maximize the model prediction error. Such images, however, contain artificial perturbations that make them somewhat distinguishable from natural images. This property is used by several defense methods to counter adversarial examples by applying denoising filters or training the model to be robust to small perturbations.   In this paper, we introduce a new class of adversarial examples, namely "Semantic Adversarial Examples," as images that are arbitrarily perturbed to fool the model, but in such a way that the modified image semantically represents the same object as the original image. We formulate the problem of generating such images as a constrained optimization problem and develop an adversarial transformation based on the shape bias property of human cognitive system. In our method, we generate adversarial images by first converting the RGB image into the HSV (Hue, Saturation and Value) color space and then randomly shifting the Hue and Saturation components, while keeping the Value component the same. Our experimental results on CIFAR10 dataset show that the accuracy of VGG16 network on adversarial color-shifted images is 5.7%.



### A Low-rank Tensor Regularization Strategy for Hyperspectral Unmixing
- **Arxiv ID**: http://arxiv.org/abs/1803.06355v1
- **DOI**: 10.1109/SSP.2018.8450853
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06355v1)
- **Published**: 2018-03-16 18:06:46+00:00
- **Updated**: 2018-03-16 18:06:46+00:00
- **Authors**: Tales Imbiriba, Ricardo Augusto Borsoi, José Carlos Moreira Bermudez
- **Comment**: None
- **Journal**: None
- **Summary**: Tensor-based methods have recently emerged as a more natural and effective formulation to address many problems in hyperspectral imaging. In hyperspectral unmixing (HU), low-rank constraints on the abundance maps have been shown to act as a regularization which adequately accounts for the multidimensional structure of the underlying signal. However, imposing a strict low-rank constraint for the abundance maps does not seem to be adequate, as important information that may be required to represent fine scale abundance behavior may be discarded. This paper introduces a new low-rank tensor regularization that adequately captures the low-rank structure underlying the abundance maps without hindering the flexibility of the solution. Simulation results with synthetic and real data show that the the extra flexibility introduced by the proposed regularization significantly improves the unmixing results.



### Deep Component Analysis via Alternating Direction Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.06407v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.06407v1)
- **Published**: 2018-03-16 21:40:02+00:00
- **Updated**: 2018-03-16 21:40:02+00:00
- **Authors**: Calvin Murdock, Ming-Fang Chang, Simon Lucey
- **Comment**: None
- **Journal**: None
- **Summary**: Despite a lack of theoretical understanding, deep neural networks have achieved unparalleled performance in a wide range of applications. On the other hand, shallow representation learning with component analysis is associated with rich intuition and theory, but smaller capacity often limits its usefulness. To bridge this gap, we introduce Deep Component Analysis (DeepCA), an expressive multilayer model formulation that enforces hierarchical structure through constraints on latent variables in each layer. For inference, we propose a differentiable optimization algorithm implemented using recurrent Alternating Direction Neural Networks (ADNNs) that enable parameter learning using standard backpropagation. By interpreting feed-forward networks as single-iteration approximations of inference in our model, we provide both a novel theoretical perspective for understanding them and a practical technique for constraining predictions with prior knowledge. Experimentally, we demonstrate performance improvements on a variety of tasks, including single-image depth prediction with sparse output constraints.



### Learning to Segment via Cut-and-Paste
- **Arxiv ID**: http://arxiv.org/abs/1803.06414v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06414v1)
- **Published**: 2018-03-16 21:58:51+00:00
- **Updated**: 2018-03-16 21:58:51+00:00
- **Authors**: Tal Remez, Jonathan Huang, Matthew Brown
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a weakly-supervised approach to object instance segmentation. Starting with known or predicted object bounding boxes, we learn object masks by playing a game of cut-and-paste in an adversarial learning setup. A mask generator takes a detection box and Faster R-CNN features, and constructs a segmentation mask that is used to cut-and-paste the object into a new image location. The discriminator tries to distinguish between real objects, and those cut and pasted via the generator, giving a learning signal that leads to improved object masks. We verify our method experimentally using Cityscapes, COCO, and aerial image datasets, learning to segment objects without ever having seen a mask in training. Our method exceeds the performance of existing weakly supervised methods, without requiring hand-tuned segment proposals, and reaches 90% of supervised performance.



