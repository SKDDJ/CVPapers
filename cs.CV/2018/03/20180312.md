# Arxiv Papers in cs.CV on 2018-03-12
### Full Reference Objective Quality Assessment for Reconstructed Background Images
- **Arxiv ID**: http://arxiv.org/abs/1803.04103v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04103v2)
- **Published**: 2018-03-12 03:26:56+00:00
- **Updated**: 2018-04-11 18:15:25+00:00
- **Authors**: Aditee Shrotre, Lina Karam
- **Comment**: Associated source code: https://github.com/ashrotre/RBQI, Associated
  Database:
  https://drive.google.com/drive/folders/1bg8YRPIBcxpKIF9BIPisULPBPcA5x-Bk?usp=sharing
  (Email for permissions at: ashrotre<at>asu<dot>edu)
- **Journal**: None
- **Summary**: With an increased interest in applications that require a clean background image, such as video surveillance, object tracking, street view imaging and location-based services on web-based maps, multiple algorithms have been developed to reconstruct a background image from cluttered scenes. Traditionally, statistical measures and existing image quality techniques have been applied for evaluating the quality of the reconstructed background images. Though these quality assessment methods have been widely used in the past, their performance in evaluating the perceived quality of the reconstructed background image has not been verified. In this work, we discuss the shortcomings in existing metrics and propose a full reference Reconstructed Background image Quality Index (RBQI) that combines color and structural information at multiple scales using a probability summation model to predict the perceived quality in the reconstructed background image given a reference image. To compare the performance of the proposed quality index with existing image quality assessment measures, we construct two different datasets consisting of reconstructed background images and corresponding subjective scores. The quality assessment measures are evaluated by correlating their objective scores with human subjective ratings. The correlation results show that the proposed RBQI outperforms all the existing approaches. Additionally, the constructed datasets and the corresponding subjective scores provide a benchmark to evaluate the performance of future metrics that are developed to evaluate the perceived quality of reconstructed background images.



### Style Aggregated Network for Facial Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/1803.04108v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04108v4)
- **Published**: 2018-03-12 03:46:12+00:00
- **Updated**: 2018-03-22 12:26:08+00:00
- **Authors**: Xuanyi Dong, Yan Yan, Wanli Ouyang, Yi Yang
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: Recent advances in facial landmark detection achieve success by learning discriminative features from rich deformation of face shapes and poses. Besides the variance of faces themselves, the intrinsic variance of image styles, e.g., grayscale vs. color images, light vs. dark, intense vs. dull, and so on, has constantly been overlooked. This issue becomes inevitable as increasing web images are collected from various sources for training neural networks. In this work, we propose a style-aggregated approach to deal with the large intrinsic variance of image styles for facial landmark detection. Our method transforms original face images to style-aggregated images by a generative adversarial module. The proposed scheme uses the style-aggregated image to maintain face images that are more robust to environmental changes. Then the original face images accompanying with style-aggregated ones play a duet to train a landmark detector which is complementary to each other. In this way, for each face, our method takes two images as input, i.e., one in its original style and the other in the aggregated style. In experiments, we observe that the large variance of image styles would degenerate the performance of facial landmark detectors. Moreover, we show the robustness of our method to the large variance of image styles by comparing to a variant of our approach, in which the generative adversarial module is removed, and no style-aggregated images are used. Our approach is demonstrated to perform well when compared with state-of-the-art algorithms on benchmark datasets AFLW and 300-W. Code is publicly available on GitHub: https://github.com/D-X-Y/SAN



### Innovative Texture Database Collecting Approach and Feature Extraction Method based on Combination of Gray Tone Difference Matrixes, Local Binary Patterns,and K-means Clustering
- **Arxiv ID**: http://arxiv.org/abs/1803.04125v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04125v1)
- **Published**: 2018-03-12 06:15:49+00:00
- **Updated**: 2018-03-12 06:15:49+00:00
- **Authors**: Shervan Fekri-Ershad
- **Comment**: First International Conference on Computer, Information Technology
  and Communications (CCITC), 2014
- **Journal**: None
- **Summary**: Texture analysis and classification are some of the problems which have been paid much attention by image processing scientists since late 80s. If texture analysis is done accurately, it can be used in many cases such as object tracking, visual pattern recognition, and face recognition.Since now, so many methods are offered to solve this problem. Against their technical differences, all of them used same popular databases to evaluate their performance such asBrodatz or Outex, which may be made their performance biased on these databases. In this paper, an approach is proposed to collect more efficient databases of texture images. The proposed approach is included two stages. The first one is developing feature representation based on gray tone difference matrixes and local binary patterns features and the next one is consisted an innovative algorithm which is based on K-means clustering to collect images based on evaluated features. In order to evaluate the performance of the proposed approach, a texture database is collected and fisher rate is computed for collected one and well known databases. Also, texture classification is evaluated based on offered feature extraction and the accuracy is compared by some state of the art texture classification methods.



### Deep Class-Wise Hashing: Semantics-Preserving Hashing via Class-wise Loss
- **Arxiv ID**: http://arxiv.org/abs/1803.04137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04137v1)
- **Published**: 2018-03-12 07:19:04+00:00
- **Updated**: 2018-03-12 07:19:04+00:00
- **Authors**: Xuefei Zhe, Shifeng Chen, Hong Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep supervised hashing has emerged as an influential solution to large-scale semantic image retrieval problems in computer vision. In the light of recent progress, convolutional neural network based hashing methods typically seek pair-wise or triplet labels to conduct the similarity preserving learning. However, complex semantic concepts of visual contents are hard to capture by similar/dissimilar labels, which limits the retrieval performance. Generally, pair-wise or triplet losses not only suffer from expensive training costs but also lack in extracting sufficient semantic information. In this regard, we propose a novel deep supervised hashing model to learn more compact class-level similarity preserving binary codes. Our deep learning based model is motivated by deep metric learning that directly takes semantic labels as supervised information in training and generates corresponding discriminant hashing code. Specifically, a novel cubic constraint loss function based on Gaussian distribution is proposed, which preserves semantic variations while penalizes the overlap part of different classes in the embedding space. To address the discrete optimization problem introduced by binary codes, a two-step optimization strategy is proposed to provide efficient training and avoid the problem of gradient vanishing. Extensive experiments on four large-scale benchmark databases show that our model can achieve the state-of-the-art retrieval performance. Moreover, when training samples are limited, our method surpasses other supervised deep hashing methods with non-negligible margins.



### Noise2Noise: Learning Image Restoration without Clean Data
- **Arxiv ID**: http://arxiv.org/abs/1803.04189v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.04189v3)
- **Published**: 2018-03-12 11:07:58+00:00
- **Updated**: 2018-10-29 10:29:23+00:00
- **Authors**: Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, Timo Aila
- **Comment**: Added link to official implementation and updated MRI results to
  match it
- **Journal**: None
- **Summary**: We apply basic statistical reasoning to signal reconstruction by machine learning -- learning to map corrupted observations to clean signals -- with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean data, without explicit image priors or likelihood models of the corruption. In practice, we show that a single model learns photographic noise removal, denoising synthetic Monte Carlo images, and reconstruction of undersampled MRI scans -- all corrupted by different processes -- based on noisy data only.



### Automated detection and segmentation of non-mass enhancing breast tumors with dynamic contrast-enhanced magnetic resonance imaging
- **Arxiv ID**: http://arxiv.org/abs/1803.04200v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.04200v2)
- **Published**: 2018-03-12 11:26:43+00:00
- **Updated**: 2018-09-26 11:55:18+00:00
- **Authors**: Ignacio Alvarez Illan, Javier Ramirez, Juan M. Gorriz, Maria Adele Marino, Daly Avenda√±o, Thomas Helbich, Pascal Baltzer, Katja Pinker, Anke Meyer-Baese
- **Comment**: 20 pages, 9 figures, Contrast Media and Molecular Imaging, in press
- **Journal**: None
- **Summary**: Non-mass enhancing lesions (NME) constitute a diagnostic challenge in dynamic contrast enhanced magnetic resonance imaging (DCE-MRI) of the breast. Computer Aided Diagnosis (CAD) systems provide physicians with advanced tools for analysis, assessment and evaluation that have a significant impact on the diagnostic performance. Here, we propose a new approach to address the challenge of NME detection and segmentation, taking advantage of independent component analysis (ICA) to extract data-driven dynamic lesion characterizations. A set of independent sources was obtained from DCE-MRI dataset of breast patients, and the dynamic behavior of the different tissues was described by multiple dynamic curves, together with a set of eigenimages describing the scores for each voxel. A new test image is projected onto the independent source space using the unmixing matrix, and each voxel is classified by a support vector machine (SVM) that has already been trained with manually delineated data. A solution to the high false positive rate problem is proposed by controlling the SVM hyperplane location, outperforming previously published approaches.



### Omnidirectional CNN for Visual Place Recognition and Navigation
- **Arxiv ID**: http://arxiv.org/abs/1803.04228v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1803.04228v1)
- **Published**: 2018-03-12 12:49:50+00:00
- **Updated**: 2018-03-12 12:49:50+00:00
- **Authors**: Tsun-Hsuan Wang, Hung-Jui Huang, Juan-Ting Lin, Chan-Wei Hu, Kuo-Hao Zeng, Min Sun
- **Comment**: 8 pages. 6 figures. Accepted to 2018 IEEE International Conference on
  Robotics and Automation (ICRA 2018)
- **Journal**: None
- **Summary**: $ $Visual place recognition is challenging, especially when only a few place exemplars are given. To mitigate the challenge, we consider place recognition method using omnidirectional cameras and propose a novel Omnidirectional Convolutional Neural Network (O-CNN) to handle severe camera pose variation. Given a visual input, the task of the O-CNN is not to retrieve the matched place exemplar, but to retrieve the closest place exemplar and estimate the relative distance between the input and the closest place. With the ability to estimate relative distance, a heuristic policy is proposed to navigate a robot to the retrieved closest place. Note that the network is designed to take advantage of the omnidirectional view by incorporating circular padding and rotation invariance. To train a powerful O-CNN, we build a virtual world for training on a large scale. We also propose a continuous lifted structured feature embedding loss to learn the concept of distance efficiently. Finally, our experimental results confirm that our method achieves state-of-the-art accuracy and speed with both the virtual world and real-world datasets.



### Video Object Segmentation with Joint Re-identification and Attention-Aware Mask Propagation
- **Arxiv ID**: http://arxiv.org/abs/1803.04242v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04242v2)
- **Published**: 2018-03-12 13:25:19+00:00
- **Updated**: 2018-03-14 12:37:50+00:00
- **Authors**: Xiaoxiao Li, Chen Change Loy
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of video object segmentation can become extremely challenging when multiple instances co-exist. While each instance may exhibit large scale and pose variations, the problem is compounded when instances occlude each other causing failures in tracking. In this study, we formulate a deep recurrent network that is capable of segmenting and tracking objects in video simultaneously by their temporal continuity, yet able to re-identify them when they re-appear after a prolonged occlusion. We combine both temporal propagation and re-identification functionalities into a single framework that can be trained end-to-end. In particular, we present a re-identification module with template expansion to retrieve missing objects despite their large appearance changes. In addition, we contribute a new attention-based recurrent mask propagation approach that is robust to distractors not belonging to the target segment. Our approach achieves a new state-of-the-art global mean (Region Jaccard and Boundary F measure) of 68.2 on the challenging DAVIS 2017 benchmark (test-dev set), outperforming the winning solution which achieves a global mean of 66.1 on the same partition.



### SO-Net: Self-Organizing Network for Point Cloud Analysis
- **Arxiv ID**: http://arxiv.org/abs/1803.04249v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04249v4)
- **Published**: 2018-03-12 13:49:14+00:00
- **Updated**: 2018-03-27 02:59:48+00:00
- **Authors**: Jiaxin Li, Ben M. Chen, Gim Hee Lee
- **Comment**: 17 pages, CVPR 2018
- **Journal**: None
- **Summary**: This paper presents SO-Net, a permutation invariant architecture for deep learning with orderless point clouds. The SO-Net models the spatial distribution of point cloud by building a Self-Organizing Map (SOM). Based on the SOM, SO-Net performs hierarchical feature extraction on individual points and SOM nodes, and ultimately represents the input point cloud by a single feature vector. The receptive field of the network can be systematically adjusted by conducting point-to-node k nearest neighbor search. In recognition tasks such as point cloud reconstruction, classification, object part segmentation and shape retrieval, our proposed network demonstrates performance that is similar with or better than state-of-the-art approaches. In addition, the training speed is significantly faster than existing point cloud recognition networks because of the parallelizability and simplicity of the proposed architecture. Our code is available at the project website. https://github.com/lijx10/SO-Net



### Super-resolution of Sentinel-2 images: Learning a globally applicable deep neural network
- **Arxiv ID**: http://arxiv.org/abs/1803.04271v2
- **DOI**: 10.1016/j.isprsjprs.2018.09.018
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.04271v2)
- **Published**: 2018-03-12 14:15:07+00:00
- **Updated**: 2018-10-01 15:54:43+00:00
- **Authors**: Charis Lanaras, Jos√© Bioucas-Dias, Silvano Galliani, Emmanuel Baltsavias, Konrad Schindler
- **Comment**: 19 pages, 11 figures
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, 146 (2018),
  pp. 305-319
- **Summary**: The Sentinel-2 satellite mission delivers multi-spectral imagery with 13 spectral bands, acquired at three different spatial resolutions. The aim of this research is to super-resolve the lower-resolution (20 m and 60 m Ground Sampling Distance - GSD) bands to 10 m GSD, so as to obtain a complete data cube at the maximal sensor resolution. We employ a state-of-the-art convolutional neural network (CNN) to perform end-to-end upsampling, which is trained with data at lower resolution, i.e., from 40->20 m, respectively 360->60 m GSD. In this way, one has access to a virtually infinite amount of training data, by downsampling real Sentinel-2 images. We use data sampled globally over a wide range of geographical locations, to obtain a network that generalises across different climate zones and land-cover types, and can super-resolve arbitrary Sentinel-2 images without the need of retraining. In quantitative evaluations (at lower scale, where ground truth is available), our network, which we call DSen2, outperforms the best competing approach by almost 50% in RMSE, while better preserving the spectral characteristics. It also delivers visually convincing results at the full 10 m GSD. The code is available at https://github.com/lanha/DSen2



### In-depth Assessment of an Interactive Graph-based Approach for the Segmentation for Pancreatic Metastasis in Ultrasound Acquisitions of the Liver with two Specialists in Internal Medicine
- **Arxiv ID**: http://arxiv.org/abs/1803.04279v1
- **DOI**: 10.1109/BMEiCON.2017.8229099
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04279v1)
- **Published**: 2018-03-12 14:24:39+00:00
- **Updated**: 2018-03-12 14:24:39+00:00
- **Authors**: Jan Egger, Xiaojun Chen, Lucas Bettac, Mark H√§nle, Tilmann Gr√§ter, Wolfram Zoller, Dieter Schmalstieg, Alexander Hann
- **Comment**: 5 pages, 3 Figures, 1 Table, The 2017 Biomedical Engineering
  International Conference (BMEiCON-2017)
- **Journal**: None
- **Summary**: The manual outlining of hepatic metastasis in (US) ultrasound acquisitions from patients suffering from pancreatic cancer is common practice. However, such pure manual measurements are often very time consuming, and the results repeatedly differ between the raters. In this contribution, we study the in-depth assessment of an interactive graph-based approach for the segmentation for pancreatic metastasis in US images of the liver with two specialists in Internal Medicine. Thereby, evaluating the approach with over one hundred different acquisitions of metastases. The two physicians or the algorithm had never assessed the acquisitions before the evaluation. In summary, the physicians first performed a pure manual outlining followed by an algorithmic segmentation over one month later. As a result, the experts satisfied in up to ninety percent of algorithmic segmentation results. Furthermore, the algorithmic segmentation was much faster than manual outlining and achieved a median Dice Similarity Coefficient (DSC) of over eighty percent. Ultimately, the algorithm enables a fast and accurate segmentation of liver metastasis in clinical US images, which can support the manual outlining in daily practice.



### Replication study: Development and validation of deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs
- **Arxiv ID**: http://arxiv.org/abs/1803.04337v3
- **DOI**: 10.1371/journal.pone.0217541
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04337v3)
- **Published**: 2018-03-12 16:04:28+00:00
- **Updated**: 2018-08-30 00:28:12+00:00
- **Authors**: Mike Voets, Kajsa M√∏llersen, Lars Ailo Bongo
- **Comment**: The third version of this paper includes results from replication
  after certain hyper-parameters were published in later article. 16 pages, 6
  figures, 1 table, presented at NOBIM 2018
- **Journal**: None
- **Summary**: Replication studies are essential for validation of new methods, and are crucial to maintain the high standards of scientific publications, and to use the results in practice. We have attempted to replicate the main method in 'Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs' published in JAMA 2016; 316(22). We re-implemented the method since the source code is not available, and we used publicly available data sets. The original study used non-public fundus images from EyePACS and three hospitals in India for training. We used a different EyePACS data set from Kaggle. The original study used the benchmark data set Messidor-2 to evaluate the algorithm's performance. We used the same data set. In the original study, ophthalmologists re-graded all images for diabetic retinopathy, macular edema, and image gradability. There was one diabetic retinopathy grade per image for our data sets, and we assessed image gradability ourselves. Hyper-parameter settings were not described in the original study. But some of these were later published. We were not able to replicate the original study. Our algorithm's area under the receiver operating curve (AUC) of 0.94 on the Kaggle EyePACS test set and 0.80 on Messidor-2 did not come close to the reported AUC of 0.99 in the original study. This may be caused by the use of a single grade per image, different data, or different not described hyper-parameter settings. This study shows the challenges of replicating deep learning, and the need for more replication studies to validate deep learning methods, especially for medical image analysis.   Our source code and instructions are available at: https://github.com/mikevoets/jama16-retina-replication



### Classifying Online Dating Profiles on Tinder using FaceNet Facial Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1803.04347v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SI, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.04347v1)
- **Published**: 2018-03-12 16:14:24+00:00
- **Updated**: 2018-03-12 16:14:24+00:00
- **Authors**: Charles F Jekel, Raphael T. Haftka
- **Comment**: 6 pages, 7 figures
- **Journal**: None
- **Summary**: A method to produce personalized classification models to automatically review online dating profiles on Tinder is proposed, based on the user's historical preference. The method takes advantage of a FaceNet facial classification model to extract features which may be related to facial attractiveness. The embeddings from a FaceNet model were used as the features to describe an individual's face. A user reviewed 8,545 online dating profiles. For each reviewed online dating profile, a feature set was constructed from the profile images which contained just one face. Two approaches are presented to go from the set of features for each face, to a set of profile features. A simple logistic regression trained on the embeddings from just 20 profiles could obtain a 65% validation accuracy. A point of diminishing marginal returns was identified to occur around 80 profiles, at which the model accuracy of 73% would only improve marginally after reviewing a significant number of additional profiles.



### idtracker.ai: Tracking all individuals in large collectives of unmarked animals
- **Arxiv ID**: http://arxiv.org/abs/1803.04351v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04351v1)
- **Published**: 2018-03-12 16:21:19+00:00
- **Updated**: 2018-03-12 16:21:19+00:00
- **Authors**: Francisco Romero-Ferrero, Mattia G. Bergomi, Robert Hinz, Francisco J. H. Heras, Gonzalo G. de Polavieja
- **Comment**: 44 pages, 1 main figure, 13 supplementary figures, 6 tables
- **Journal**: None
- **Summary**: Our understanding of collective animal behavior is limited by our ability to track each of the individuals. We describe an algorithm and software, idtracker.ai, that extracts from video all trajectories with correct identities at a high accuracy for collectives of up to 100 individuals. It uses two deep networks, one detecting when animals touch or cross and another one for animal identification, trained adaptively to conditions and difficulty of the video.



### Beyond Gr√∂bner Bases: Basis Selection for Minimal Solvers
- **Arxiv ID**: http://arxiv.org/abs/1803.04360v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04360v1)
- **Published**: 2018-03-12 16:36:13+00:00
- **Updated**: 2018-03-12 16:36:13+00:00
- **Authors**: Viktor Larsson, Magnus Oskarsson, Kalle √Östr√∂m, Alge Wallis, Zuzana Kukelova, Tomas Pajdla
- **Comment**: None
- **Journal**: None
- **Summary**: Many computer vision applications require robust estimation of the underlying geometry, in terms of camera motion and 3D structure of the scene. These robust methods often rely on running minimal solvers in a RANSAC framework. In this paper we show how we can make polynomial solvers based on the action matrix method faster, by careful selection of the monomial bases. These monomial bases have traditionally been based on a Gr\"obner basis for the polynomial ideal. Here we describe how we can enumerate all such bases in an efficient way. We also show that going beyond Gr\"obner bases leads to more efficient solvers in many cases. We present a novel basis sampling scheme that we evaluate on a number of problems.



### Discriminability objective for training descriptive captions
- **Arxiv ID**: http://arxiv.org/abs/1803.04376v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04376v2)
- **Published**: 2018-03-12 17:09:26+00:00
- **Updated**: 2018-06-08 18:09:36+00:00
- **Authors**: Ruotian Luo, Brian Price, Scott Cohen, Gregory Shakhnarovich
- **Comment**: CVPR2018
- **Journal**: None
- **Summary**: One property that remains lacking in image captions generated by contemporary methods is discriminability: being able to tell two images apart given the caption for one of them. We propose a way to improve this aspect of caption generation. By incorporating into the captioning training objective a loss component directly related to ability (by a machine) to disambiguate image/caption matches, we obtain systems that produce much more discriminative caption, according to human evaluation. Remarkably, our approach leads to improvement in other aspects of generated captions, reflected by a battery of standard scores such as BLEU, SPICE etc. Our approach is modular and can be applied to a variety of model/loss combinations commonly proposed for image captioning.



### Extended Affinity Propagation: Global Discovery and Local Insights
- **Arxiv ID**: http://arxiv.org/abs/1803.04459v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1803.04459v2)
- **Published**: 2018-03-12 18:56:38+00:00
- **Updated**: 2019-04-15 18:43:44+00:00
- **Authors**: Rayyan Ahmad Khan, Rana Ali Amjad, Martin Kleinsteuber
- **Comment**: Submitted to TKDE
- **Journal**: None
- **Summary**: We propose a new clustering algorithm, Extended Affinity Propagation, based on pairwise similarities. Extended Affinity Propagation is developed by modifying Affinity Propagation such that the desirable features of Affinity Propagation, e.g., exemplars, reasonable computational complexity and no need to specify number of clusters, are preserved while the shortcomings, e.g., the lack of global structure discovery, that limit the applicability of Affinity Propagation are overcome. Extended Affinity Propagation succeeds not only in achieving this goal but can also provide various additional insights into the internal structure of the individual clusters, e.g., refined confidence values, relative cluster densities and local cluster strength in different regions of a cluster, which are valuable for an analyst. We briefly discuss how these insights can help in easily tuning the hyperparameters. We also illustrate these desirable features and the performance of Extended Affinity Propagation on various synthetic and real world datasets.



### Dissimilarity-based representation for radiomics applications
- **Arxiv ID**: http://arxiv.org/abs/1803.04460v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04460v1)
- **Published**: 2018-03-12 18:56:47+00:00
- **Updated**: 2018-03-12 18:56:47+00:00
- **Authors**: Hongliu Cao, Simon Bernard, Laurent Heutte, Robert Sabourin
- **Comment**: conference, 6 pages, 2 figures
- **Journal**: None
- **Summary**: Radiomics is a term which refers to the analysis of the large amount of quantitative tumor features extracted from medical images to find useful predictive, diagnostic or prognostic information. Many recent studies have proved that radiomics can offer a lot of useful information that physicians cannot extract from the medical images and can be associated with other information like gene or protein data. However, most of the classification studies in radiomics report the use of feature selection methods without identifying the machine learning challenges behind radiomics. In this paper, we first show that the radiomics problem should be viewed as an high dimensional, low sample size, multi view learning problem, then we compare different solutions proposed in multi view learning for classifying radiomics data. Our experiments, conducted on several real world multi view datasets, show that the intermediate integration methods work significantly better than filter and embedded feature selection methods commonly used in radiomics.



### An Introduction to Image Synthesis with Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/1803.04469v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04469v2)
- **Published**: 2018-03-12 19:14:35+00:00
- **Updated**: 2018-11-17 15:25:11+00:00
- **Authors**: He Huang, Philip S. Yu, Changhu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: There has been a drastic growth of research in Generative Adversarial Nets (GANs) in the past few years. Proposed in 2014, GAN has been applied to various applications such as computer vision and natural language processing, and achieves impressive performance. Among the many applications of GAN, image synthesis is the most well-studied one, and research in this area has already demonstrated the great potential of using GAN in image synthesis. In this paper, we provide a taxonomy of methods used in image synthesis, review different models for text-to-image synthesis and image-to-image translation, and discuss some evaluation metrics as well as possible future research directions in image synthesis with GAN.



### Correction by Projection: Denoising Images with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.04477v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04477v1)
- **Published**: 2018-03-12 19:30:01+00:00
- **Updated**: 2018-03-12 19:30:01+00:00
- **Authors**: Subarna Tripathi, Zachary C. Lipton, Truong Q. Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) transform low-dimensional latent vectors into visually plausible images. If the real dataset contains only clean images, then ostensibly, the manifold learned by the GAN should contain only clean images. In this paper, we propose to denoise corrupted images by finding the nearest point on the GAN manifold, recovering latent vectors by minimizing distances in image space. We first demonstrate that given a corrupted version of an image that truly lies on the GAN manifold, we can approximately recover the latent vector and denoise the image, obtaining significantly higher quality, comparing with BM3D. Next, we demonstrate that latent vectors recovered from noisy images exhibit a consistent bias. By subtracting this bias before projecting back to image space, we improve denoising results even further. Finally, even for unseen images, our method performs better at denoising better than BM3D. Notably, the basic version of our method (without bias correction) requires no prior knowledge on the noise variance. To achieve the highest possible denoising quality, the best performing signal processing based methods, such as BM3D, require an estimate of the blur kernel.



### Event-based Moving Object Detection and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1803.04523v3
- **DOI**: 10.1109/IROS.2018.8593805
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04523v3)
- **Published**: 2018-03-12 20:43:59+00:00
- **Updated**: 2020-01-12 23:55:30+00:00
- **Authors**: Anton Mitrokhin, Cornelia Fermuller, Chethan Parameshwara, Yiannis Aloimonos
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: Event-based vision sensors, such as the Dynamic Vision Sensor (DVS), are ideally suited for real-time motion analysis. The unique properties encompassed in the readings of such sensors provide high temporal resolution, superior sensitivity to light and low latency. These properties provide the grounds to estimate motion extremely reliably in the most sophisticated scenarios but they come at a price - modern event-based vision sensors have extremely low resolution and produce a lot of noise. Moreover, the asynchronous nature of the event stream calls for novel algorithms.   This paper presents a new, efficient approach to object tracking with asynchronous cameras. We present a novel event stream representation which enables us to utilize information about the dynamic (temporal) component of the event stream, and not only the spatial component, at every moment of time. This is done by approximating the 3D geometry of the event stream with a parametric model; as a result, the algorithm is capable of producing the motion-compensated event stream (effectively approximating egomotion), and without using any form of external sensors in extremely low-light and noisy conditions without any form of feature tracking or explicit optical flow computation. We demonstrate our framework on the task of independent motion detection and tracking, where we use the temporal model inconsistencies to locate differently moving objects in challenging situations of very fast motion.



### Measuring Conflict in a Multi-Source Environment as a Normal Measure
- **Arxiv ID**: http://arxiv.org/abs/1803.04556v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.AI, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1803.04556v1)
- **Published**: 2018-03-12 22:27:11+00:00
- **Updated**: 2018-03-12 22:27:11+00:00
- **Authors**: Pan Wei, John E. Ball, Derek T. Anderson, Archit Harsh, Christopher Archibald
- **Comment**: 4 pages, 8 figures, conference paper
- **Journal**: IEEE International Workshop on Computational Advances in
  Multi-Sensor Adaptive Processing (CAMSAP), December, 2015
- **Summary**: In a multi-source environment, each source has its own credibility. If there is no external knowledge about credibility then we can use the information provided by the sources to assess their credibility. In this paper, we propose a way to measure conflict in a multi-source environment as a normal measure. We examine our algorithm using three simulated examples of increasing conflict and one experimental example. The results demonstrate that the proposed measure can represent conflict in a meaningful way similar to what a human might expect and from it we can identify conflict within our sources.



### Onion-Peeling Outlier Detection in 2-D data Sets
- **Arxiv ID**: http://arxiv.org/abs/1803.04964v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1803.04964v1)
- **Published**: 2018-03-12 22:37:14+00:00
- **Updated**: 2018-03-12 22:37:14+00:00
- **Authors**: Archit Harsh, John E. Ball, Pan Wei
- **Comment**: 6 pages, 4 figures, journal paper
- **Journal**: International Journal of Computer Application, Vol.139 (3),
  pp.26-31, April, 2016
- **Summary**: Outlier Detection is a critical and cardinal research task due its array of applications in variety of domains ranging from data mining, clustering, statistical analysis, fraud detection, network intrusion detection and diagnosis of diseases etc. Over the last few decades, distance-based outlier detection algorithms have gained significant reputation as a viable alternative to the more traditional statistical approaches due to their scalable, non-parametric and simple implementation. In this paper, we present a modified onion peeling (Convex hull) genetic algorithm to detect outliers in a Gaussian 2-D point data set. We present three different scenarios of outlier detection using a) Euclidean Distance Metric b) Standardized Euclidean Distance Metric and c) Mahalanobis Distance Metric. Finally, we analyze the performance and evaluate the results.



### Learning to recognize Abnormalities in Chest X-Rays with Location-Aware Dense Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.04565v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1803.04565v1)
- **Published**: 2018-03-12 22:57:18+00:00
- **Updated**: 2018-03-12 22:57:18+00:00
- **Authors**: Sebastian Guendel, Sasa Grbic, Bogdan Georgescu, Kevin Zhou, Ludwig Ritschl, Andreas Meier, Dorin Comaniciu
- **Comment**: None
- **Journal**: None
- **Summary**: Chest X-ray is the most common medical imaging exam used to assess multiple pathologies. Automated algorithms and tools have the potential to support the reading workflow, improve efficiency, and reduce reading errors. With the availability of large scale data sets, several methods have been proposed to classify pathologies on chest X-ray images. However, most methods report performance based on random image based splitting, ignoring the high probability of the same patient appearing in both training and test set. In addition, most methods fail to explicitly incorporate the spatial information of abnormalities or utilize the high resolution images. We propose a novel approach based on location aware Dense Networks (DNetLoc), whereby we incorporate both high-resolution image data and spatial information for abnormality classification. We evaluate our method on the largest data set reported in the community, containing a total of 86,876 patients and 297,541 chest X-ray images. We achieve (i) the best average AUC score for published training and test splits on the single benchmarking data set (ChestX-Ray14), and (ii) improved AUC scores when the pathology location information is explicitly used. To foster future research we demonstrate the limitations of the current benchmarking setup and provide new reference patient-wise splits for the used data sets. This could support consistent and meaningful benchmarking of future methods on the largest publicly available data sets.



