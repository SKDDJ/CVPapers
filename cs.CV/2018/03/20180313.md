# Arxiv Papers in cs.CV on 2018-03-13
### Target Driven Instance Detection
- **Arxiv ID**: http://arxiv.org/abs/1803.04610v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04610v6)
- **Published**: 2018-03-13 03:56:36+00:00
- **Updated**: 2019-10-01 15:32:03+00:00
- **Authors**: Phil Ammirato, Cheng-Yang Fu, Mykhailo Shvets, Jana Kosecka, Alexander C. Berg
- **Comment**: None
- **Journal**: None
- **Summary**: While state-of-the-art general object detectors are getting better and better, there are not many systems specifically designed to take advantage of the instance detection problem. For many applications, such as household robotics, a system may need to recognize a few very specific instances at a time. Speed can be critical in these applications, as can the need to recognize previously unseen instances. We introduce a Target Driven Instance Detector(TDID), which modifies existing general object detectors for the instance recognition setting. TDID not only improves performance on instances seen during training, with a fast runtime, but is also able to generalize to detect novel instances.



### Image Segmentation and Processing for Efficient Parking Space Analysis
- **Arxiv ID**: http://arxiv.org/abs/1803.04620v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.04620v1)
- **Published**: 2018-03-13 05:05:35+00:00
- **Updated**: 2018-03-13 05:05:35+00:00
- **Authors**: Chetan Sai Tutika, Charan Vallapaneni, Karthik R, Bharath KP, N Ruban Rajesh Kumar Muthu
- **Comment**: 6 pages, 2018 International Conference on Informatics Computing in
  Engineering Systems (ICICES)
- **Journal**: None
- **Summary**: In this paper, we develop a method to detect vacant parking spaces in an environment with unclear segments and contours with the help of MATLAB image processing capabilities. Due to the anomalies present in the parking spaces, such as uneven illumination, distorted slot lines and overlapping of cars. The present-day conventional algorithms have difficulties processing the image for accurate results. The algorithm proposed uses a combination of image pre-processing and false contour detection techniques to improve the detection efficiency. The proposed method also eliminates the need to employ individual sensors to detect a car, instead uses real-time static images to consider a group of slots together, instead of the usual single slot method. This greatly decreases the expenses required to design an efficient parking system. We compare the performance of our algorithm to that of other techniques. These comparisons show that the proposed algorithm can detect the vacancies in the parking spots while ignoring the false data and other distortions.



### Maintaining Natural Image Statistics with the Contextual Loss
- **Arxiv ID**: http://arxiv.org/abs/1803.04626v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04626v3)
- **Published**: 2018-03-13 05:19:26+00:00
- **Updated**: 2018-07-18 12:32:51+00:00
- **Authors**: Roey Mechrez, Itamar Talmi, Firas Shama, Lihi Zelnik-Manor
- **Comment**: None
- **Journal**: None
- **Summary**: Maintaining natural image statistics is a crucial factor in restoration and generation of realistic looking images. When training CNNs, photorealism is usually attempted by adversarial training (GAN), that pushes the output images to lie on the manifold of natural images. GANs are very powerful, but not perfect. They are hard to train and the results still often suffer from artifacts. In this paper we propose a complementary approach, that could be applied with or without GAN, whose goal is to train a feed-forward CNN to maintain natural internal statistics. We look explicitly at the distribution of features in an image and train the network to generate images with natural feature distributions. Our approach reduces by orders of magnitude the number of images required for training and achieves state-of-the-art results on both single-image super-resolution, and high-resolution surface normal estimation.



### TOM-Net: Learning Transparent Object Matting from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1803.04636v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04636v3)
- **Published**: 2018-03-13 06:03:42+00:00
- **Updated**: 2018-03-29 02:41:59+00:00
- **Authors**: Guanying Chen, Kai Han, Kwan-Yee K. Wong
- **Comment**: CVPR 2018. Project Page: https://guanyingc.github.io/TOM-Net
- **Journal**: None
- **Summary**: This paper addresses the problem of transparent object matting. Existing image matting approaches for transparent objects often require tedious capturing procedures and long processing time, which limit their practical use. In this paper, we first formulate transparent object matting as a refractive flow estimation problem. We then propose a deep learning framework, called TOM-Net, for learning the refractive flow. Our framework comprises two parts, namely a multi-scale encoder-decoder network for producing a coarse prediction, and a residual network for refinement. At test time, TOM-Net takes a single image as input, and outputs a matte (consisting of an object mask, an attenuation mask and a refractive flow field) in a fast feed-forward pass. As no off-the-shelf dataset is available for transparent object matting, we create a large-scale synthetic dataset consisting of 158K images of transparent objects rendered in front of images sampled from the Microsoft COCO dataset. We also collect a real dataset consisting of 876 samples using 14 transparent objects and 60 background images. Promising experimental results have been achieved on both synthetic and real data, which clearly demonstrate the effectiveness of our approach.



### Dynamic Vision Sensors for Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.04667v1
- **DOI**: 10.1109/ACPR.2017.136
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04667v1)
- **Published**: 2018-03-13 07:49:06+00:00
- **Updated**: 2018-03-13 07:49:06+00:00
- **Authors**: Stefanie Anna Baby, Bimal Vinod, Chaitanya Chinni, Kaushik Mitra
- **Comment**: 6 pages, 9 figures, accepted at the 4th Asian Conference on Pattern
  Recognition (ACPR) 2017
- **Journal**: None
- **Summary**: Unlike conventional cameras which capture video at a fixed frame rate, Dynamic Vision Sensors (DVS) record only changes in pixel intensity values. The output of DVS is simply a stream of discrete ON/OFF events based on the polarity of change in its pixel values. DVS has many attractive features such as low power consumption, high temporal resolution, high dynamic range and fewer storage requirements. All these make DVS a very promising camera for potential applications in wearable platforms where power consumption is a major concern.   In this paper, we explore the feasibility of using DVS for Human Activity Recognition (HAR). We propose to use the various slices (such as $x-y$, $x-t$, and $y-t$) of the DVS video as a feature map for HAR and denote them as Motion Maps. We show that fusing motion maps with Motion Boundary Histogram (MBH) give good performance on the benchmark DVS dataset as well as on a real DVS gesture dataset collected by us. Interestingly, the performance of DVS is comparable to that of conventional videos although DVS captures only sparse motion information.



### Multi-Frame Quality Enhancement for Compressed Video
- **Arxiv ID**: http://arxiv.org/abs/1803.04680v4
- **DOI**: 10.1109/CVPR.2018.00697
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1803.04680v4)
- **Published**: 2018-03-13 08:40:15+00:00
- **Updated**: 2018-03-16 02:09:36+00:00
- **Authors**: Ren Yang, Mai Xu, Zulin Wang, Tianyi Li
- **Comment**: to appear in CVPR 2018
- **Journal**: None
- **Summary**: The past few years have witnessed great success in applying deep learning to enhance the quality of compressed image/video. The existing approaches mainly focus on enhancing the quality of a single frame, ignoring the similarity between consecutive frames. In this paper, we investigate that heavy quality fluctuation exists across compressed video frames, and thus low quality frames can be enhanced using the neighboring high quality frames, seen as Multi-Frame Quality Enhancement (MFQE). Accordingly, this paper proposes an MFQE approach for compressed video, as a first attempt in this direction. In our approach, we firstly develop a Support Vector Machine (SVM) based detector to locate Peak Quality Frames (PQFs) in compressed video. Then, a novel Multi-Frame Convolutional Neural Network (MF-CNN) is designed to enhance the quality of compressed video, in which the non-PQF and its nearest two PQFs are as the input. The MF-CNN compensates motion between the non-PQF and PQFs through the Motion Compensation subnet (MC-subnet). Subsequently, the Quality Enhancement subnet (QE-subnet) reduces compression artifacts of the non-PQF with the help of its nearest PQFs. Finally, the experiments validate the effectiveness and generality of our MFQE approach in advancing the state-of-the-art quality enhancement of compressed video. The code of our MFQE approach is available at https://github.com/ryangBUAA/MFQE.git



### Multimodal Recurrent Neural Networks with Information Transfer Layers for Indoor Scene Labeling
- **Arxiv ID**: http://arxiv.org/abs/1803.04687v1
- **DOI**: 10.1109/TMM.2017.2774007
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04687v1)
- **Published**: 2018-03-13 09:08:49+00:00
- **Updated**: 2018-03-13 09:08:49+00:00
- **Authors**: Abrar H. Abdulnabi, Bing Shuai, Zhen Zuo, Lap-Pui Chau, Gang Wang
- **Comment**: 15 pages, 13 figures, IEEE TMM 2017
- **Journal**: IEEE Transactions on Multimedia 2017
- **Summary**: This paper proposes a new method called Multimodal RNNs for RGB-D scene semantic segmentation. It is optimized to classify image pixels given two input sources: RGB color channels and Depth maps. It simultaneously performs training of two recurrent neural networks (RNNs) that are crossly connected through information transfer layers, which are learnt to adaptively extract relevant cross-modality features. Each RNN model learns its representations from its own previous hidden states and transferred patterns from the other RNNs previous hidden states; thus, both model-specific and crossmodality features are retained. We exploit the structure of quad-directional 2D-RNNs to model the short and long range contextual information in the 2D input image. We carefully designed various baselines to efficiently examine our proposed model structure. We test our Multimodal RNNs method on popular RGB-D benchmarks and show how it outperforms previous methods significantly and achieves competitive results with other state-of-the-art works.



### Face Spoofing Detection by Fusing Binocular Depth and Spatial Pyramid Coding Micro-Texture Features
- **Arxiv ID**: http://arxiv.org/abs/1803.04722v1
- **DOI**: 10.1109/ICIP.2017.8296250
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04722v1)
- **Published**: 2018-03-13 10:49:45+00:00
- **Updated**: 2018-03-13 10:49:45+00:00
- **Authors**: Xiao Song, Xu Zhao, Tianwei Lin
- **Comment**: 5 pages, 2 figures, accepted by 2017 IEEE International Conference on
  Image Processing (ICIP)
- **Journal**: None
- **Summary**: Robust features are of vital importance to face spoofing detection, because various situations make feature space extremely complicated to partition. Thus in this paper, two novel and robust features for anti-spoofing are proposed. The first one is a binocular camera based depth feature called Template Face Matched Binocular Depth (TFBD) feature. The second one is a high-level micro-texture based feature called Spatial Pyramid Coding Micro-Texture (SPMT) feature. Novel template face registration algorithm and spatial pyramid coding algorithm are also introduced along with the two novel features. Multi-modal face spoofing detection is implemented based on these two robust features. Experiments are conducted on a widely used dataset and a comprehensive dataset constructed by ourselves. The results reveal that face spoofing detection with the fusion of our proposed features is of strong robustness and time efficiency, meanwhile outperforming other state-of-the-art traditional methods.



### Feature Selective Small Object Detection via Knowledge-based Recurrent Attentive Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1803.05263v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1803.05263v4)
- **Published**: 2018-03-13 12:45:55+00:00
- **Updated**: 2019-04-20 08:31:05+00:00
- **Authors**: Kai Yi, Zhiqiang Jian, Shitao Chen, Nanning Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: At present, the performance of deep neural network in general object detection is comparable to or even surpasses that of human beings. However, due to the limitations of deep learning itself, the small proportion of feature pixels, and the occurence of blur and occlusion, the detection of small objects in complex scenes is still an open question. But we can not deny that real-time and accurate object detection is fundamental to automatic perception and subsequent perception-based decision-making and planning tasks of autonomous driving.   Considering the characteristics of small objects in autonomous driving scene, we proposed a novel method named KB-RANN, which based on domain knowledge, intuitive experience and feature attentive selection. It can focus on particular parts of image features, and then it tries to stress the importance of these features and strengthenes the learning parameters of them. Our comparative experiments on KITTI and COCO datasets show that our proposed method can achieve considerable results both in speed and accuracy, and can improve the effect of small object detection through self-selection of important features and continuous enhancement of proposed method, and deployed it in our self-developed autonomous driving car.



### Video Based Reconstruction of 3D People Models
- **Arxiv ID**: http://arxiv.org/abs/1803.04758v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04758v3)
- **Published**: 2018-03-13 12:56:28+00:00
- **Updated**: 2018-04-16 10:08:19+00:00
- **Authors**: Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, Gerard Pons-Moll
- **Comment**: CVPR 2018 Spotlight, IEEE Conference on Computer Vision and Pattern
  Recognition 2018 (CVPR)
- **Journal**: None
- **Summary**: This paper describes how to obtain accurate 3D body models and texture of arbitrary people from a single, monocular video in which a person is moving. Based on a parametric body model, we present a robust processing pipeline achieving 3D model fits with 5mm accuracy also for clothed people. Our main contribution is a method to nonrigidly deform the silhouette cones corresponding to the dynamic human silhouettes, resulting in a visual hull in a common reference frame that enables surface reconstruction. This enables efficient estimation of a consensus 3D shape, texture and implanted animation skeleton based on a large number of frames. We present evaluation results for a number of test subjects and analyze overall performance. Requiring only a smartphone or webcam, our method enables everyone to create their own fully animatable digital double, e.g., for social VR applications or virtual try-on for online fashion shopping.



### Learning Monocular 3D Human Pose Estimation from Multi-view Images
- **Arxiv ID**: http://arxiv.org/abs/1803.04775v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04775v2)
- **Published**: 2018-03-13 13:14:47+00:00
- **Updated**: 2018-03-24 17:24:45+00:00
- **Authors**: Helge Rhodin, Jörg Spörri, Isinsu Katircioglu, Victor Constantin, Frédéric Meyer, Erich Müller, Mathieu Salzmann, Pascal Fua
- **Comment**: CVPR 2018, Ski-Pose PTZ-Camera Dataset available
- **Journal**: None
- **Summary**: Accurate 3D human pose estimation from single images is possible with sophisticated deep-net architectures that have been trained on very large datasets. However, this still leaves open the problem of capturing motions for which no such database exists. Manual annotation is tedious, slow, and error-prone. In this paper, we propose to replace most of the annotations by the use of multiple views, at training time only. Specifically, we train the system to predict the same pose in all views. Such a consistency constraint is necessary but not sufficient to predict accurate poses. We therefore complement it with a supervised loss aiming to predict the correct pose in a small set of labeled images, and with a regularization term that penalizes drift from initial predictions. Furthermore, we propose a method to estimate camera pose jointly with human pose, which lets us utilize multi-view footage where calibration is difficult, e.g., for pan-tilt or moving handheld cameras. We demonstrate the effectiveness of our approach on established benchmarks, as well as on a new Ski dataset with rotating cameras and expert ski motion, for which annotations are truly hard to obtain.



### A Learning-Based Visual Saliency Fusion Model for High Dynamic Range Video (LBVS-HDR)
- **Arxiv ID**: http://arxiv.org/abs/1803.04827v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04827v1)
- **Published**: 2018-03-13 14:21:09+00:00
- **Updated**: 2018-03-13 14:21:09+00:00
- **Authors**: Amin Banitalebi-Dehkordi, Yuanyuan Dong, Mahsa T. Pourazad, Panos Nasiopoulos
- **Comment**: None
- **Journal**: EUSIPCO, 2015
- **Summary**: Saliency prediction for Standard Dynamic Range (SDR) videos has been well explored in the last decade. However, limited studies are available on High Dynamic Range (HDR) Visual Attention Models (VAMs). Considering that the characteristic of HDR content in terms of dynamic range and color gamut is quite different than those of SDR content, it is essential to identify the importance of different saliency attributes of HDR videos for designing a VAM and understand how to combine these features. To this end we propose a learning-based visual saliency fusion method for HDR content (LVBS-HDR) to combine various visual saliency features. In our approach various conspicuity maps are extracted from HDR data, and then for fusing conspicuity maps, a Random Forests algorithm is used to train a model based on the collected data from an eye-tracking experiment. Performance evaluations demonstrate the superiority of the proposed fusion method against other existing fusion methods.



### Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN
- **Arxiv ID**: http://arxiv.org/abs/1803.04831v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.04831v3)
- **Published**: 2018-03-13 14:27:42+00:00
- **Updated**: 2018-05-22 11:54:28+00:00
- **Authors**: Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, Yanbo Gao
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Recurrent neural networks (RNNs) have been widely used for processing sequential data. However, RNNs are commonly difficult to train due to the well-known gradient vanishing and exploding problems and hard to learn long-term patterns. Long short-term memory (LSTM) and gated recurrent unit (GRU) were developed to address these problems, but the use of hyperbolic tangent and the sigmoid action functions results in gradient decay over layers. Consequently, construction of an efficiently trainable deep network is challenging. In addition, all the neurons in an RNN layer are entangled together and their behaviour is hard to interpret. To address these problems, a new type of RNN, referred to as independently recurrent neural network (IndRNN), is proposed in this paper, where neurons in the same layer are independent of each other and they are connected across layers. We have shown that an IndRNN can be easily regulated to prevent the gradient exploding and vanishing problems while allowing the network to learn long-term dependencies. Moreover, an IndRNN can work with non-saturated activation functions such as relu (rectified linear unit) and be still trained robustly. Multiple IndRNNs can be stacked to construct a network that is deeper than the existing RNNs. Experimental results have shown that the proposed IndRNN is able to process very long sequences (over 5000 time steps), can be used to construct very deep networks (21 layers used in the experiment) and still be trained robustly. Better performances have been achieved on various tasks by using IndRNNs compared with the traditional RNN and LSTM. The code is available at https://github.com/Sunnydreamrain/IndRNN_Theano_Lasagne.



### 3D Video Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1803.04836v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04836v1)
- **Published**: 2018-03-13 14:31:15+00:00
- **Updated**: 2018-03-13 14:31:15+00:00
- **Authors**: Amin Banitalebi Dehkordi
- **Comment**: PhD Thesis, UBC, 2015
- **Journal**: None
- **Summary**: A key factor in designing 3D systems is to understand how different visual cues and distortions affect the perceptual quality of 3D video. The ultimate way to assess video quality is through subjective tests. However, subjective evaluation is time consuming, expensive, and in most cases not even possible. An alternative solution is objective quality metrics, which attempt to model the Human Visual System (HVS) in order to assess the perceptual quality. The potential of 3D technology to significantly improve the immersiveness of video content has been hampered by the difficulty of objectively assessing Quality of Experience (QoE). A no-reference (NR) objective 3D quality metric, which could help determine capturing parameters and improve playback perceptual quality, would be welcomed by camera and display manufactures. Network providers would embrace a full-reference (FR) 3D quality metric, as they could use it to ensure efficient QoE-based resource management during compression and Quality of Service (QoS) during transmission.



### Resource aware design of a deep convolutional-recurrent neural network for speech recognition through audio-visual sensor fusion
- **Arxiv ID**: http://arxiv.org/abs/1803.04840v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04840v1)
- **Published**: 2018-03-13 14:35:00+00:00
- **Updated**: 2018-03-13 14:35:00+00:00
- **Authors**: Matthijs Van keirsbilck, Bert Moons, Marian Verhelst
- **Comment**: Tech. report
- **Journal**: None
- **Summary**: Today's Automatic Speech Recognition systems only rely on acoustic signals and often don't perform well under noisy conditions. Performing multi-modal speech recognition - processing acoustic speech signals and lip-reading video simultaneously - significantly enhances the performance of such systems, especially in noisy environments. This work presents the design of such an audio-visual system for Automated Speech Recognition, taking memory and computation requirements into account. First, a Long-Short-Term-Memory neural network for acoustic speech recognition is designed. Second, Convolutional Neural Networks are used to model lip-reading features. These are combined with an LSTM network to model temporal dependencies and perform automatic lip-reading on video. Finally, acoustic-speech and visual lip-reading networks are combined to process acoustic and visual features simultaneously. An attention mechanism ensures performance of the model in noisy environments. This system is evaluated on the TCD-TIMIT 'lipspeaker' dataset for audio-visual phoneme recognition with clean audio and with additive white noise at an SNR of 0dB. It achieves 75.70% and 58.55% phoneme accuracy respectively, over 14 percentage points better than the state-of-the-art for all noise levels.



### A Learning-Based Visual Saliency Prediction Model for Stereoscopic 3D Video (LBVS-3D)
- **Arxiv ID**: http://arxiv.org/abs/1803.04842v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04842v1)
- **Published**: 2018-03-13 14:37:58+00:00
- **Updated**: 2018-03-13 14:37:58+00:00
- **Authors**: Amin Banitalebi-Dehkordi, Mahsa T. Pourazad, Panos Nasiopoulos
- **Comment**: None
- **Journal**: Multimedia Tools and Applications, 2016
- **Summary**: Over the past decade, many computational saliency prediction models have been proposed for 2D images and videos. Considering that the human visual system has evolved in a natural 3D environment, it is only natural to want to design visual attention models for 3D content. Existing monocular saliency models are not able to accurately predict the attentive regions when applied to 3D image/video content, as they do not incorporate depth information. This paper explores stereoscopic video saliency prediction by exploiting both low-level attributes such as brightness, color, texture, orientation, motion, and depth, as well as high-level cues such as face, person, vehicle, animal, text, and horizon. Our model starts with a rough segmentation and quantifies several intuitive observations such as the effects of visual discomfort level, depth abruptness, motion acceleration, elements of surprise, size and compactness of the salient regions, and emphasizing only a few salient objects in a scene. A new fovea-based model of spatial distance between the image regions is adopted for considering local and global feature calculations. To efficiently fuse the conspicuity maps generated by our method to one single saliency map that is highly correlated with the eye-fixation data, a random forest based algorithm is utilized. The performance of the proposed saliency model is evaluated against the results of an eye-tracking experiment, which involved 24 subjects and an in-house database of 61 captured stereoscopic videos. Our stereo video database as well as the eye-tracking data are publicly available along with this paper. Experiment results show that the proposed saliency prediction method achieves competitive performance compared to the state-of-the-art approaches.



### Expert identification of visual primitives used by CNNs during mammogram classification
- **Arxiv ID**: http://arxiv.org/abs/1803.04858v1
- **DOI**: 10.1117/12.2293890
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04858v1)
- **Published**: 2018-03-13 14:54:38+00:00
- **Updated**: 2018-03-13 14:54:38+00:00
- **Authors**: Jimmy Wu, Diondra Peck, Scott Hsieh, Vandana Dialani, Constance D. Lehman, Bolei Zhou, Vasilis Syrgkanis, Lester Mackey, Genevieve Patterson
- **Comment**: None
- **Journal**: Medical Imaging 2018: Computer-Aided Diagnosis, Proc. of SPIE Vol.
  10575, 105752T
- **Summary**: This work interprets the internal representations of deep neural networks trained for classification of diseased tissue in 2D mammograms. We propose an expert-in-the-loop interpretation method to label the behavior of internal units in convolutional neural networks (CNNs). Expert radiologists identify that the visual patterns detected by the units are correlated with meaningful medical phenomena such as mass tissue and calcificated vessels. We demonstrate that several trained CNN models are able to produce explanatory descriptions to support the final classification decisions. We view this as an important first step toward interpreting the internal representations of medical classification CNNs and explaining their predictions.



### Using Convolutional Neural Networks for Determining Reticulocyte Percentage in Cats
- **Arxiv ID**: http://arxiv.org/abs/1803.04873v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04873v2)
- **Published**: 2018-03-13 15:17:30+00:00
- **Updated**: 2018-03-14 15:30:00+00:00
- **Authors**: Krunoslav Vinicki, Pierluigi Ferrari, Maja Belic, Romana Turk
- **Comment**: 10 pages, 2 figures
- **Journal**: None
- **Summary**: Recent advances in artificial intelligence (AI), specifically in computer vision (CV) and deep learning (DL), have created opportunities for novel systems in many fields. In the last few years, deep learning applications have demonstrated impressive results not only in fields such as autonomous driving and robotics, but also in the field of medicine, where they have, in some cases, even exceeded human-level performance. However, despite the huge potential, adoption of deep learning-based methods is still slow in many areas, especially in veterinary medicine, where we haven't been able to find any research papers using modern convolutional neural networks (CNNs) in medical image processing. We believe that using deep learning-based medical imaging can enable more accurate, faster and less expensive diagnoses in veterinary medicine. In order to do so, however, these methods have to be accessible to everyone in this field, not just to computer scientists. To show the potential of this technology, we present results on a real-world task in veterinary medicine that is usually done manually: feline reticulocyte percentage. Using an open source Keras implementation of the Single-Shot MultiBox Detector (SSD) model architecture and training it on only 800 labeled images, we achieve an accuracy of 98.7% at predicting the correct number of aggregate reticulocytes in microscope images of cat blood smears. The main motivation behind this paper is to show not only that deep learning can approach or even exceed human-level performance on a task like this, but also that anyone in the field can implement it, even without a background in computer science.



### Quantization of Fully Convolutional Networks for Accurate Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.04907v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04907v1)
- **Published**: 2018-03-13 16:06:13+00:00
- **Updated**: 2018-03-13 16:06:13+00:00
- **Authors**: Xiaowei Xu, Qing Lu, Yu Hu, Lin Yang, Sharon Hu, Danny Chen, Yiyu Shi
- **Comment**: 9 pages, 11 Figs, 1 Table, Accepted by CVPR
- **Journal**: None
- **Summary**: With pervasive applications of medical imaging in health-care, biomedical image segmentation plays a central role in quantitative analysis, clinical diagno- sis, and medical intervention. Since manual anno- tation su ers limited reproducibility, arduous e orts, and excessive time, automatic segmentation is desired to process increasingly larger scale histopathological data. Recently, deep neural networks (DNNs), par- ticularly fully convolutional networks (FCNs), have been widely applied to biomedical image segmenta- tion, attaining much improved performance. At the same time, quantization of DNNs has become an ac- tive research topic, which aims to represent weights with less memory (precision) to considerably reduce memory and computation requirements of DNNs while maintaining acceptable accuracy. In this paper, we apply quantization techniques to FCNs for accurate biomedical image segmentation. Unlike existing litera- ture on quantization which primarily targets memory and computation complexity reduction, we apply quan- tization as a method to reduce over tting in FCNs for better accuracy. Speci cally, we focus on a state-of- the-art segmentation framework, suggestive annotation [22], which judiciously extracts representative annota- tion samples from the original training dataset, obtain- ing an e ective small-sized balanced training dataset. We develop two new quantization processes for this framework: (1) suggestive annotation with quantiza- tion for highly representative training samples, and (2) network training with quantization for high accuracy. Extensive experiments on the MICCAI Gland dataset show that both quantization processes can improve the segmentation performance, and our proposed method exceeds the current state-of-the-art performance by up to 1%. In addition, our method has a reduction of up to 6.4x on memory usage.



### A Framework for Video-Driven Crowd Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1803.04969v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04969v1)
- **Published**: 2018-03-13 16:58:27+00:00
- **Updated**: 2018-03-13 16:58:27+00:00
- **Authors**: Jordan Stadler, Faisal Z. Qureshi
- **Comment**: None
- **Journal**: None
- **Summary**: We present a framework for video-driven crowd synthesis. Motion vectors extracted from input crowd video are processed to compute global motion paths. These paths encode the dominant motions observed in the input video. These paths are then fed into a behavior-based crowd simulation framework, which is responsible for synthesizing crowd animations that respect the motion patterns observed in the video. Our system synthesizes 3D virtual crowds by animating virtual humans along the trajectories returned by the crowd simulation framework. We also propose a new metric for comparing the "visual similarity" between the synthesized crowd and exemplar crowd. We demonstrate the proposed approach on crowd videos collected under different settings.



### Automatic Pixelwise Object Labeling for Aerial Imagery Using Stacked U-Nets
- **Arxiv ID**: http://arxiv.org/abs/1803.04953v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04953v1)
- **Published**: 2018-03-13 17:39:01+00:00
- **Updated**: 2018-03-13 17:39:01+00:00
- **Authors**: Andrew Khalel, Motaz El-Saban
- **Comment**: None
- **Journal**: None
- **Summary**: Automation of objects labeling in aerial imagery is a computer vision task with numerous practical applications. Fields like energy exploration require an automated method to process a continuous stream of imagery on a daily basis. In this paper we propose a pipeline to tackle this problem using a stack of convolutional neural networks (U-Net architecture) arranged end-to-end. Each network works as post-processor to the previous one. Our model outperforms current state-of-the-art on two different datasets: Inria Aerial Image Labeling dataset and Massachusetts Buildings dataset each with different characteristics such as spatial resolution, object shapes and scales. Moreover, we experimentally validate computation time savings by processing sub-sampled images and later upsampling pixelwise labeling. These savings come at a negligible degradation in segmentation quality. Though the conducted experiments in this paper cover only aerial imagery, the technique presented is general and can handle other types of images.



### Development and Validation of Deep Learning Algorithms for Detection of Critical Findings in Head CT Scans
- **Arxiv ID**: http://arxiv.org/abs/1803.05854v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05854v2)
- **Published**: 2018-03-13 17:43:30+00:00
- **Updated**: 2018-04-12 06:32:23+00:00
- **Authors**: Sasank Chilamkurthy, Rohit Ghosh, Swetha Tanamala, Mustafa Biviji, Norbert G. Campeau, Vasantha Kumar Venugopal, Vidur Mahajan, Pooja Rao, Prashant Warier
- **Comment**: Improved operating points, updated link to CQ500 dataset:
  headctstudy.qure.ai/dataset
- **Journal**: None
- **Summary**: Importance: Non-contrast head CT scan is the current standard for initial imaging of patients with head trauma or stroke symptoms.   Objective: To develop and validate a set of deep learning algorithms for automated detection of following key findings from non-contrast head CT scans: intracranial hemorrhage (ICH) and its types, intraparenchymal (IPH), intraventricular (IVH), subdural (SDH), extradural (EDH) and subarachnoid (SAH) hemorrhages, calvarial fractures, midline shift and mass effect.   Design and Settings: We retrospectively collected a dataset containing 313,318 head CT scans along with their clinical reports from various centers. A part of this dataset (Qure25k dataset) was used to validate and the rest to develop algorithms. Additionally, a dataset (CQ500 dataset) was collected from different centers in two batches B1 & B2 to clinically validate the algorithms.   Main Outcomes and Measures: Original clinical radiology report and consensus of three independent radiologists were considered as gold standard for Qure25k and CQ500 datasets respectively. Area under receiver operating characteristics curve (AUC) for each finding was primarily used to evaluate the algorithms.   Results: Qure25k dataset contained 21,095 scans (mean age 43.31; 42.87% female) while batches B1 and B2 of CQ500 dataset consisted of 214 (mean age 43.40; 43.92% female) and 277 (mean age 51.70; 30.31% female) scans respectively. On Qure25k dataset, the algorithms achieved AUCs of 0.9194, 0.8977, 0.9559, 0.9161, 0.9288 and 0.9044 for detecting ICH, IPH, IVH, SDH, EDH and SAH respectively. AUCs for the same on CQ500 dataset were 0.9419, 0.9544, 0.9310, 0.9521, 0.9731 and 0.9574 respectively. For detecting calvarial fractures, midline shift and mass effect, AUCs on Qure25k dataset were 0.9244, 0.9276 and 0.8583 respectively, while AUCs on CQ500 dataset were 0.9624, 0.9697 and 0.9216 respectively.



### LCANet: End-to-End Lipreading with Cascaded Attention-CTC
- **Arxiv ID**: http://arxiv.org/abs/1803.04988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04988v1)
- **Published**: 2018-03-13 18:04:10+00:00
- **Updated**: 2018-03-13 18:04:10+00:00
- **Authors**: Kai Xu, Dawei Li, Nick Cassimatis, Xiaolong Wang
- **Comment**: FG 2018
- **Journal**: None
- **Summary**: Machine lipreading is a special type of automatic speech recognition (ASR) which transcribes human speech by visually interpreting the movement of related face regions including lips, face, and tongue. Recently, deep neural network based lipreading methods show great potential and have exceeded the accuracy of experienced human lipreaders in some benchmark datasets. However, lipreading is still far from being solved, and existing methods tend to have high error rates on the wild data. In this paper, we propose LCANet, an end-to-end deep neural network based lipreading system. LCANet encodes input video frames using a stacked 3D convolutional neural network (CNN), highway network and bidirectional GRU network. The encoder effectively captures both short-term and long-term spatio-temporal information. More importantly, LCANet incorporates a cascaded attention-CTC decoder to generate output texts. By cascading CTC with attention, it partially eliminates the defect of the conditional independence assumption of CTC within the hidden neural layers, and this yields notably performance improvement as well as faster convergence. The experimental results show the proposed system achieves a 1.3% CER and 3.0% WER on the GRID corpus database, leading to a 12.3% improvement compared to the state-of-the-art methods.



### A Probabilistic Disease Progression Model for Predicting Future Clinical Outcome
- **Arxiv ID**: http://arxiv.org/abs/1803.05011v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.05011v1)
- **Published**: 2018-03-13 19:05:08+00:00
- **Updated**: 2018-03-13 19:05:08+00:00
- **Authors**: Yingying Zhu, Mert R. Sabuncu
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we consider the problem of predicting the course of a progressive disease, such as cancer or Alzheimer's. Progressive diseases often start with mild symptoms that might precede a diagnosis, and each patient follows their own trajectory. Patient trajectories exhibit wild variability, which can be associated with many factors such as genotype, age, or sex. An additional layer of complexity is that, in real life, the amount and type of data available for each patient can differ significantly. For example, for one patient we might have no prior history, whereas for another patient we might have detailed clinical assessments obtained at multiple prior time-points. This paper presents a probabilistic model that can handle multiple modalities (including images and clinical assessments) and variable patient histories with irregular timings and missing entries, to predict clinical scores at future time-points. We use a sigmoidal function to model latent disease progression, which gives rise to clinical observations in our generative model. We implemented an approximate Bayesian inference strategy on the proposed model to estimate the parameters on data from a large population of subjects. Furthermore, the Bayesian framework enables the model to automatically fine-tune its predictions based on historical observations that might be available on the test subject. We applied our method to a longitudinal Alzheimer's disease dataset with more than 3000 subjects [23] and present a detailed empirical analysis of prediction performance under different scenarios, with comparisons against several benchmarks. We also demonstrate how the proposed model can be interrogated to glean insights about temporal dynamics in Alzheimer's disease.



### Principal Component Analysis with Tensor Train Subspace
- **Arxiv ID**: http://arxiv.org/abs/1803.05026v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, cs.NA, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1803.05026v1)
- **Published**: 2018-03-13 19:58:46+00:00
- **Updated**: 2018-03-13 19:58:46+00:00
- **Authors**: Wenqi Wang, Vaneet Aggarwal, Shuchin Aeron
- **Comment**: None
- **Journal**: None
- **Summary**: Tensor train is a hierarchical tensor network structure that helps alleviate the curse of dimensionality by parameterizing large-scale multidimensional data via a set of network of low-rank tensors. Associated with such a construction is a notion of Tensor Train subspace and in this paper we propose a TT-PCA algorithm for estimating this structured subspace from the given data. By maintaining low rank tensor structure, TT-PCA is more robust to noise comparing with PCA or Tucker-PCA. This is borne out numerically by testing the proposed approach on the Extended YaleFace Dataset B.



### SAF- BAGE: Salient Approach for Facial Soft-Biometric Classification - Age, Gender, and Facial Expression
- **Arxiv ID**: http://arxiv.org/abs/1803.05719v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05719v2)
- **Published**: 2018-03-13 21:11:11+00:00
- **Updated**: 2018-11-21 07:50:13+00:00
- **Authors**: Ayesha Gurnani, Kenil Shah, Vandit Gajjar, Viraj Mavani, Yash Khandhediya
- **Comment**: 9 Pages, 4 Figures, 6 Tables; Accepted for publication in IEEE Winter
  Conference on Application of Computer Vision (WACV) 2019, Hawaii, USA
- **Journal**: None
- **Summary**: How can we improve the facial soft-biometric classification with help of the human visual system? This paper explores the use of saliency which is equivalent to the human visual system to classify Age, Gender and Facial Expression soft-biometric for facial images. Using the Deep Multi-level Network (ML-Net) [1] and off-the-shelf face detector [2], we propose our approach - SAF-BAGE, which first detects the face in the test image, increases the Bounding Box (B-Box) margin by 30%, finds the saliency map using ML-Net, with 30% reweighted ratio of saliency map, it multiplies with the input cropped face and extracts the Convolutional Neural Networks (CNN) predictions on the multiplied reweighted salient face. Our CNN uses the model AlexNet [3], which is pre-trained on ImageNet. The proposed approach surpasses the performance of other approaches, increasing the state-of-the-art by approximately 0.8% on the widely-used Adience [28] dataset for Age and Gender classification and by nearly 3% on the recent AffectNet [36] dataset for Facial Expression classification. We hope our simple, reproducible and effective approach will help ease future research in facial soft-biometric classification using saliency.



### A Review on Image Texture Analysis Methods
- **Arxiv ID**: http://arxiv.org/abs/1804.00494v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00494v1)
- **Published**: 2018-03-13 23:04:12+00:00
- **Updated**: 2018-03-13 23:04:12+00:00
- **Authors**: Shervan Fekri-Ershad
- **Comment**: in Persian
- **Journal**: International Online Journal of Image Processing and Pattern
  Recognition, Vol. 1, No. 1, pp. 1-63, 2018
- **Summary**: Texture classification is an active topic in image processing which plays an important role in many applications such as image retrieval, inspection systems, face recognition, medical image processing, etc. There are many approaches extracting texture features in gray-level images such as local binary patterns, gray level co-occurrence matrices, statistical features, skeleton, scale invariant feature transform, etc. The texture analysis methods can be categorized in 4 groups titles: statistical methods, structural methods, filter-based and model based approaches. In many related researches, authors have tried to extract color and texture features jointly. In this respect, combined methods are considered as efficient image analysis descriptors. Mostly important challenges in image texture analysis are rotation sensitivity, gray scale variations, noise sensitivity, illumination and brightness conditions, etc. In this paper, we review most efficient and state-of-the-art image texture analysis methods. Also, some texture classification approaches are survived.



### A Multi-Modal Approach to Infer Image Affect
- **Arxiv ID**: http://arxiv.org/abs/1803.05070v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.05070v1)
- **Published**: 2018-03-13 23:07:45+00:00
- **Updated**: 2018-03-13 23:07:45+00:00
- **Authors**: Ashok Sundaresan, Sugumar Murugesan, Sean Davis, Karthik Kappaganthu, ZhongYi Jin, Divya Jain, Anurag Maunder
- **Comment**: None
- **Journal**: None
- **Summary**: The group affect or emotion in an image of people can be inferred by extracting features about both the people in the picture and the overall makeup of the scene. The state-of-the-art on this problem investigates a combination of facial features, scene extraction and even audio tonality. This paper combines three additional modalities, namely, human pose, text-based tagging and CNN extracted features / predictions. To the best of our knowledge, this is the first time all of the modalities were extracted using deep neural networks. We evaluate the performance of our approach against baselines and identify insights throughout this paper.



