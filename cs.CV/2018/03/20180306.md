# Arxiv Papers in cs.CV on 2018-03-06
### Learning Scene Gist with Convolutional Neural Networks to Improve Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.01967v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01967v2)
- **Published**: 2018-03-06 00:45:33+00:00
- **Updated**: 2018-06-09 04:45:00+00:00
- **Authors**: Kevin Wu, Eric Wu, Gabriel Kreiman
- **Comment**: None
- **Journal**: None
- **Summary**: Advancements in convolutional neural networks (CNNs) have made significant strides toward achieving high performance levels on multiple object recognition tasks. While some approaches utilize information from the entire scene to propose regions of interest, the task of interpreting a particular region or object is still performed independently of other objects and features in the image. Here we demonstrate that a scene's 'gist' can significantly contribute to how well humans can recognize objects. These findings are consistent with the notion that humans foveate on an object and incorporate information from the periphery to aid in recognition. We use a biologically inspired two-part convolutional neural network ('GistNet') that models the fovea and periphery to provide a proof-of-principle demonstration that computational object recognition can significantly benefit from the gist of the scene as contextual information. Our model yields accuracy improvements of up to 50% in certain object categories when incorporating contextual gist, while only increasing the original model size by 5%. This proposed model mirrors our intuition about how the human visual system recognizes objects, suggesting specific biologically plausible constraints to improve machine vision and building initial steps towards the challenge of scene understanding.



### CNN-Based Automatic Urinary Particles Recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.02699v1
- **DOI**: 10.1007/s10916-018-1014-6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02699v1)
- **Published**: 2018-03-06 02:26:43+00:00
- **Updated**: 2018-03-06 02:26:43+00:00
- **Authors**: Rui Kang, Yixiong Liang, Chunyan Lian, Yuan Mao
- **Comment**: The manuscript has been submitted to Journal of Medical Systems on
  Jul 02. 2017
- **Journal**: None
- **Summary**: The urine sediment analysis of particles in microscopic images can assist physicians in evaluating patients with renal and urinary tract diseases. Manual urine sediment examination is labor-intensive, subjective and time-consuming, and the traditional automatic algorithms often extract the hand-crafted features for recognition. Instead of using the hand-crafted features, in this paper, we exploit CNN to learn features in an end-to-end manner to recognize the urine particles. We treat the urine particles recognition as object detection and exploit two state-of-the-art CNN-based object detection methods, Faster R-CNN and SSD, as well as their variants for urine particles recognition. We further investigate different factors involving these CNN-based object detection methods for urine particles recognition. We comprehensively evaluate these methods on a dataset consisting of 5,376 annotated images corresponding to 7 categories of urine particles, i.e., erythrocyte, leukocyte, epithelial cell, crystal, cast, mycete, epithelial nuclei, and obtain a best mAP (mean average precision) of 84.1% while taking only 72 ms per image on a NVIDIA Titan X GPU.



### Occupancy Map Prediction Using Generative and Fully Convolutional Networks for Vehicle Navigation
- **Arxiv ID**: http://arxiv.org/abs/1803.02007v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.02007v1)
- **Published**: 2018-03-06 04:01:37+00:00
- **Updated**: 2018-03-06 04:01:37+00:00
- **Authors**: Kapil Katyal, Katie Popek, Chris Paxton, Joseph Moore, Kevin Wolfe, Philippe Burlina, Gregory D. Hager
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Fast, collision-free motion through unknown environments remains a challenging problem for robotic systems. In these situations, the robot's ability to reason about its future motion is often severely limited by sensor field of view (FOV). By contrast, biological systems routinely make decisions by taking into consideration what might exist beyond their FOV based on prior experience. In this paper, we present an approach for predicting occupancy map representations of sensor data for future robot motions using deep neural networks. We evaluate several deep network architectures, including purely generative and adversarial models. Testing on both simulated and real environments we demonstrated performance both qualitatively and quantitatively, with SSIM similarity measure up to 0.899. We showed that it is possible to make predictions about occupied space beyond the physical robot's FOV from simulated training data. In the future, this method will allow robots to navigate through unknown environments in a faster, safer manner.



### MIS-SLAM: Real-time Large Scale Dense Deformable SLAM System in Minimal Invasive Surgery Based on Heterogeneous Computing
- **Arxiv ID**: http://arxiv.org/abs/1803.02009v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02009v2)
- **Published**: 2018-03-06 04:19:24+00:00
- **Updated**: 2018-03-27 04:13:56+00:00
- **Authors**: Jingwei Song, Jun Wang, Liang Zhao, Shoudong Huang, Gamini Dissanayake
- **Comment**: Submitted to conference
- **Journal**: None
- **Summary**: Real-time simultaneously localization and dense mapping is very helpful for providing Virtual Reality and Augmented Reality for surgeons or even surgical robots. In this paper, we propose MIS-SLAM: a complete real-time large scale dense deformable SLAM system with stereoscope in Minimal Invasive Surgery based on heterogeneous computing by making full use of CPU and GPU. Idled CPU is used to perform ORB- SLAM for providing robust global pose. Strategies are taken to integrate modules from CPU and GPU. We solved the key problem raised in previous work, that is, fast movement of scope and blurry images make the scope tracking fail. Benefiting from improved localization, MIS-SLAM can achieve large scale scope localizing and dense mapping in real-time. It transforms and deforms current model and incrementally fuses new observation while keeping vivid texture. In-vivo experiments conducted on publicly available datasets presented in the form of videos demonstrate the feasibility and practicality of MIS-SLAM for potential clinical purpose.



### Automated Detection of Acute Leukemia using K-mean Clustering Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1803.08544v1
- **DOI**: 10.1007/978-981-10-3773-3_64
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08544v1)
- **Published**: 2018-03-06 06:24:01+00:00
- **Updated**: 2018-03-06 06:24:01+00:00
- **Authors**: Sachin Kumar, Sumita Mishra, Pallavi Asthana, Pragya
- **Comment**: Presented in ICCCCS 2016
- **Journal**: Advances in Intelligent Systems and Computing, vol 554. Springer,
  2018
- **Summary**: Leukemia is a hematologic cancer which develops in blood tissue and triggers rapid production of immature and abnormal shaped white blood cells. Based on statistics it is found that the leukemia is one of the leading causes of death in men and women alike. Microscopic examination of blood sample or bone marrow smear is the most effective technique for diagnosis of leukemia. Pathologists analyze microscopic samples to make diagnostic assessments on the basis of characteristic cell features. Recently, computerized methods for cancer detection have been explored towards minimizing human intervention and providing accurate clinical information. This paper presents an algorithm for automated image based acute leukemia detection systems. The method implemented uses basic enhancement, morphology, filtering and segmenting technique to extract region of interest using k-means clustering algorithm. The proposed algorithm achieved an accuracy of 92.8% and is tested with Nearest Neighbor (KNN) and Naive Bayes Classifier on the data-set of 60 samples.



### The Earth ain't Flat: Monocular Reconstruction of Vehicles on Steep and Graded Roads from a Moving Camera
- **Arxiv ID**: http://arxiv.org/abs/1803.02057v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.02057v1)
- **Published**: 2018-03-06 08:28:18+00:00
- **Updated**: 2018-03-06 08:28:18+00:00
- **Authors**: Junaid Ahmed Ansari, Sarthak Sharma, Anshuman Majumdar, J. Krishna Murthy, K. Madhava Krishna
- **Comment**: Submitted to IROS 2018
- **Journal**: None
- **Summary**: Accurate localization of other traffic participants is a vital task in autonomous driving systems. State-of-the-art systems employ a combination of sensing modalities such as RGB cameras and LiDARs for localizing traffic participants, but most such demonstrations have been confined to plain roads. We demonstrate, to the best of our knowledge, the first results for monocular object localization and shape estimation on surfaces that do not share the same plane with the moving monocular camera. We approximate road surfaces by local planar patches and use semantic cues from vehicles in the scene to initialize a local bundle-adjustment like procedure that simultaneously estimates the pose and shape of the vehicles, and the orientation of the local ground plane on which the vehicle stands as well. We evaluate the proposed approach on the KITTI and SYNTHIA-SF benchmarks, for a variety of road plane configurations. The proposed approach significantly improves the state-of-the-art for monocular object localization on arbitrarily-shaped roads.



### The Contextual Loss for Image Transformation with Non-Aligned Data
- **Arxiv ID**: http://arxiv.org/abs/1803.02077v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.02077v4)
- **Published**: 2018-03-06 09:43:25+00:00
- **Updated**: 2018-07-18 09:58:50+00:00
- **Authors**: Roey Mechrez, Itamar Talmi, Lihi Zelnik-Manor
- **Comment**: ECCV Oral. Paper web page:
  http://cgm.technion.ac.il/Computer-Graphics-Multimedia/Software/contextual/
- **Journal**: None
- **Summary**: Feed-forward CNNs trained for image transformation problems rely on loss functions that measure the similarity between the generated image and a target image. Most of the common loss functions assume that these images are spatially aligned and compare pixels at corresponding locations. However, for many tasks, aligned training pairs of images will not be available. We present an alternative loss function that does not require alignment, thus providing an effective and simple solution for a new space of problems. Our loss is based on both context and semantics -- it compares regions with similar semantic meaning, while considering the context of the entire image. Hence, for example, when transferring the style of one face to another, it will translate eyes-to-eyes and mouth-to-mouth. Our code can be found at https://www.github.com/roimehrez/contextualLoss



### Where is my Device? - Detecting the Smart Device's Wearing Location in the Context of Active Safety for Vulnerable Road Users
- **Arxiv ID**: http://arxiv.org/abs/1803.02097v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.02097v1)
- **Published**: 2018-03-06 10:34:47+00:00
- **Updated**: 2018-03-06 10:34:47+00:00
- **Authors**: Maarten Bieshaar
- **Comment**: 10 pages, 3 figures, accepted for publication in Organic Computing:
  Doctoral Dissertation Colloquium 2017. Volume 11 of Intelligent Embedded
  Systems. Kassel University Press, Kassel
- **Journal**: None
- **Summary**: This article describes an approach to detect the wearing location of smart devices worn by pedestrians and cyclists. The detection, which is based solely on the sensors of the smart devices, is important context-information which can be used to parametrize subsequent algorithms, e.g. for dead reckoning or intention detection to improve the safety of vulnerable road users. The wearing location recognition can in terms of Organic Computing (OC) be seen as a step towards self-awareness and self-adaptation. For the wearing location detection a two-stage process is presented. It is subdivided into moving detection followed by the wearing location classification. Finally, the approach is evaluated on a real world dataset consisting of pedestrians and cyclists.



### Nonlocality-Reinforced Convolutional Neural Networks for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1803.02112v2
- **DOI**: 10.1109/LSP.2018.2850222
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.02112v2)
- **Published**: 2018-03-06 11:11:38+00:00
- **Updated**: 2018-06-21 12:07:27+00:00
- **Authors**: Cristóvão Cruz, Alessandro Foi, Vladimir Katkovnik, Karen Egiazarian
- **Comment**: Accepted for publication in IEEE SPL
- **Journal**: None
- **Summary**: We introduce a paradigm for nonlocal sparsity reinforced deep convolutional neural network denoising. It is a combination of a local multiscale denoising by a convolutional neural network (CNN) based denoiser and a nonlocal denoising based on a nonlocal filter (NLF) exploiting the mutual similarities between groups of patches. CNN models are leveraged with noise levels that progressively decrease at every iteration of our framework, while their output is regularized by a nonlocal prior implicit within the NLF. Unlike complicated neural networks that embed the nonlocality prior within the layers of the network, our framework is modular, it uses standard pre-trained CNNs together with standard nonlocal filters. An instance of the proposed framework, called NN3D, is evaluated over large grayscale image datasets showing state-of-the-art performance.



### A Non-Technical Survey on Deep Convolutional Neural Network Architectures
- **Arxiv ID**: http://arxiv.org/abs/1803.02129v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02129v1)
- **Published**: 2018-03-06 11:40:46+00:00
- **Updated**: 2018-03-06 11:40:46+00:00
- **Authors**: Felix Altenberger, Claus Lenz
- **Comment**: 17 pages (incl. references), 23 Postscript figures, uses IEEEtran
- **Journal**: None
- **Summary**: Artificial neural networks have recently shown great results in many disciplines and a variety of applications, including natural language understanding, speech processing, games and image data generation. One particular application in which the strong performance of artificial neural networks was demonstrated is the recognition of objects in images, where deep convolutional neural networks are commonly applied. In this survey, we give a comprehensive introduction to this topic (object recognition with deep convolutional neural networks), with a strong focus on the evolution of network architectures. Therefore, we aim to compress the most important concepts in this field in a simple and non-technical manner to allow for future researchers to have a quick general understanding.   This work is structured as follows:   1. We will explain the basic ideas of (convolutional) neural networks and deep learning and examine their usage for three object recognition tasks: image classification, object localization and object detection.   2. We give a review on the evolution of deep convolutional neural networks by providing an extensive overview of the most important network architectures presented in chronological order of their appearances.



### Conceptualization of Object Compositions Using Persistent Homology
- **Arxiv ID**: http://arxiv.org/abs/1803.02140v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.02140v3)
- **Published**: 2018-03-06 12:31:43+00:00
- **Updated**: 2018-11-20 22:20:51+00:00
- **Authors**: Christian A. Mueller, Andreas Birk
- **Comment**: revised version; this work is accepted for IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS) 2018
- **Journal**: None
- **Summary**: A topological shape analysis is proposed and utilized to learn concepts that reflect shape commonalities. Our approach is two-fold: i) a spatial topology analysis of point cloud segment constellations within objects. Therein constellations are decomposed and described in an hierarchical manner - from single segments to segment groups until a single group reflects an entire object. ii) a topology analysis of the description space in which segment decompositions are exposed in. Inspired by Persistent Homology, hidden groups of shape commonalities are revealed from object segment decompositions. Experiments show that extracted persistent groups of commonalities can represent semantically meaningful shape concepts. We also show the generalization capability of the proposed approach considering samples of external datasets.



### Fully Convolutional Grasp Detection Network with Oriented Anchor Box
- **Arxiv ID**: http://arxiv.org/abs/1803.02209v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.02209v1)
- **Published**: 2018-03-06 14:21:55+00:00
- **Updated**: 2018-03-06 14:21:55+00:00
- **Authors**: Xinwen Zhou, Xuguang Lan, Hanbo Zhang, Zhiqiang Tian, Yang Zhang, Nanning Zheng
- **Comment**: 8 pages 6 figures
- **Journal**: None
- **Summary**: In this paper, we present a real-time approach to predict multiple grasping poses for a parallel-plate robotic gripper using RGB images. A model with oriented anchor box mechanism is proposed and a new matching strategy is used during the training process. An end-to-end fully convolutional neural network is employed in our work. The network consists of two parts: the feature extractor and multi-grasp predictor. The feature extractor is a deep convolutional neural network. The multi-grasp predictor regresses grasp rectangles from predefined oriented rectangles, called oriented anchor boxes, and classifies the rectangles into graspable and ungraspable. On the standard Cornell Grasp Dataset, our model achieves an accuracy of 97.74% and 96.61% on image-wise split and object-wise split respectively, and outperforms the latest state-of-the-art approach by 1.74% on image-wise split and 0.51% on object-wise split.



### Multi-class Active Learning: A Hybrid Informative and Representative Criterion Inspired Approach
- **Arxiv ID**: http://arxiv.org/abs/1803.02222v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.02222v1)
- **Published**: 2018-03-06 14:32:15+00:00
- **Updated**: 2018-03-06 14:32:15+00:00
- **Authors**: Xi Fang, Zengmao Wang, Xinyao Tang, Chen Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Labeling each instance in a large dataset is extremely labor- and time- consuming . One way to alleviate this problem is active learning, which aims to which discover the most valuable instances for labeling to construct a powerful classifier. Considering both informativeness and representativeness provides a promising way to design a practical active learning. However, most existing active learning methods select instances favoring either informativeness or representativeness. Meanwhile, many are designed based on the binary class, so that they may present suboptimal solutions on the datasets with multiple classes. In this paper, a hybrid informative and representative criterion based multi-class active learning approach is proposed. We combine the informative informativeness and representativeness into one formula, which can be solved under a unified framework. The informativeness is measured by the margin minimum while the representative information is measured by the maximum mean discrepancy. By minimizing the upper bound for the true risk, we generalize the empirical risk minimization principle to the active learning setting. Simultaneously, our proposed method makes full use of the label information, and the proposed active learning is designed based on multiple classes. So the proposed method is not suitable to the binary class but also the multiple classes. We conduct our experiments on twelve benchmark UCI data sets, and the experimental results demonstrate that the proposed method performs better than some state-of-the-art methods.



### Early Start Intention Detection of Cyclists Using Motion History Images and a Deep Residual Network
- **Arxiv ID**: http://arxiv.org/abs/1803.02242v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.02242v1)
- **Published**: 2018-03-06 15:12:27+00:00
- **Updated**: 2018-03-06 15:12:27+00:00
- **Authors**: Stefan Zernetsch, Viktor Kress, Bernhard Sick, Konrad Doll
- **Comment**: None
- **Journal**: None
- **Summary**: In this article, we present a novel approach to detect starting motions of cyclists in real world traffic scenarios based on Motion History Images (MHIs). The method uses a deep Convolutional Neural Network (CNN) with a residual network architecture (ResNet), which is commonly used in image classification and detection tasks. By combining MHIs with a ResNet classifier and performing a frame by frame classification of the MHIs, we are able to detect starting motions in image sequences. The detection is performed using a wide angle stereo camera system at an urban intersection. We compare our algorithm to an existing method to detect movement transitions of pedestrians that uses MHIs in combination with a Histograms of Oriented Gradients (HOG) like descriptor and a Support Vector Machine (SVM), which we adapted to cyclists. To train and evaluate the methods a dataset containing MHIs of 394 cyclist starting motions was created. The results show that both methods can be used to detect starting motions of cyclists. Using the SVM approach, we were able to safely detect starting motions 0.506 s on average after the bicycle starts moving with an F1-score of 97.7%. The ResNet approach achieved an F1-score of 100% at an average detection time of 0.144 s. The ResNet approach outperformed the SVM approach in both robustness against false positive detections and detection time.



### Methodology to analyze the accuracy of 3D objects reconstructed with collaborative robot based monocular LSD-SLAM
- **Arxiv ID**: http://arxiv.org/abs/1803.02257v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.02257v1)
- **Published**: 2018-03-06 15:33:15+00:00
- **Updated**: 2018-03-06 15:33:15+00:00
- **Authors**: Sergey Triputen, Atmaraaj Gopal, Thomas Weber, Christian Hofert, Kristiaan Schreve, Matthias Ratsch
- **Comment**: 5 pages, 5 figures, 2018 International Conference on Intelligent
  Autonomous Systems (ICoIAS 2018)
- **Journal**: None
- **Summary**: SLAM systems are mainly applied for robot navigation while research on feasibility for motion planning with SLAM for tasks like bin-picking, is scarce. Accurate 3D reconstruction of objects and environments is important for planning motion and computing optimal gripper pose to grasp objects. In this work, we propose the methods to analyze the accuracy of a 3D environment reconstructed using a LSD-SLAM system with a monocular camera mounted onto the gripper of a collaborative robot. We discuss and propose a solution to the pose space conversion problem. Finally, we present several criteria to analyze the 3D reconstruction accuracy. These could be used as guidelines to improve the accuracy of 3D reconstructions with monocular LSD-SLAM and other SLAM based solutions.



### ExpandNet: A Deep Convolutional Neural Network for High Dynamic Range Expansion from Low Dynamic Range Content
- **Arxiv ID**: http://arxiv.org/abs/1803.02266v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1803.02266v2)
- **Published**: 2018-03-06 15:56:51+00:00
- **Updated**: 2019-09-04 08:10:25+00:00
- **Authors**: Demetris Marnerides, Thomas Bashford-Rogers, Jonathan Hatchett, Kurt Debattista
- **Comment**: Eurographics 2018
- **Journal**: None
- **Summary**: High dynamic range (HDR) imaging provides the capability of handling real world lighting as opposed to the traditional low dynamic range (LDR) which struggles to accurately represent images with higher dynamic range. However, most imaging content is still available only in LDR. This paper presents a method for generating HDR content from LDR content based on deep Convolutional Neural Networks (CNNs) termed ExpandNet. ExpandNet accepts LDR images as input and generates images with an expanded range in an end-to-end fashion. The model attempts to reconstruct missing information that was lost from the original signal due to quantization, clipping, tone mapping or gamma correction. The added information is reconstructed from learned features, as the network is trained in a supervised fashion using a dataset of HDR images. The approach is fully automatic and data driven; it does not require any heuristics or human expertise. ExpandNet uses a multiscale architecture which avoids the use of upsampling layers to improve image quality. The method performs well compared to expansion/inverse tone mapping operators quantitatively on multiple metrics, even for badly exposed inputs.



### Personalized Exposure Control Using Adaptive Metering and Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1803.02269v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02269v3)
- **Published**: 2018-03-06 16:00:54+00:00
- **Updated**: 2018-08-05 08:05:27+00:00
- **Authors**: Huan Yang, Baoyuan Wang, Noranart Vesdapunt, Minyi Guo, Sing Bing Kang
- **Comment**: 17 pages, 20 figures
- **Journal**: None
- **Summary**: We propose a reinforcement learning approach for real-time exposure control of a mobile camera that is personalizable. Our approach is based on Markov Decision Process (MDP). In the camera viewfinder or live preview mode, given the current frame, our system predicts the change in exposure so as to optimize the trade-off among image quality, fast convergence, and minimal temporal oscillation. We model the exposure prediction function as a fully convolutional neural network that can be trained through Gaussian policy gradient in an end-to-end fashion. As a result, our system can associate scene semantics with exposure values; it can also be extended to personalize the exposure adjustments for a user and device. We improve the learning performance by incorporating an adaptive metering module that links semantics with exposure. This adaptive metering module generalizes the conventional spot or matrix metering techniques. We validate our system using the MIT FiveK and our own datasets captured using iPhone 7 and Google Pixel. Experimental results show that our system exhibits stable real-time behavior while improving visual quality compared to what is achieved through native camera control.



### GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose
- **Arxiv ID**: http://arxiv.org/abs/1803.02276v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02276v2)
- **Published**: 2018-03-06 16:09:21+00:00
- **Updated**: 2018-03-12 03:02:07+00:00
- **Authors**: Zhichao Yin, Jianping Shi
- **Comment**: Accepted to CVPR 2018; Code will be made available at
  https://github.com/yzcjtr/GeoNet
- **Journal**: None
- **Summary**: We propose GeoNet, a jointly unsupervised learning framework for monocular depth, optical flow and ego-motion estimation from videos. The three components are coupled by the nature of 3D scene geometry, jointly learned by our framework in an end-to-end manner. Specifically, geometric relationships are extracted over the predictions of individual modules and then combined as an image reconstruction loss, reasoning about static and dynamic scene parts separately. Furthermore, we propose an adaptive geometric consistency loss to increase robustness towards outliers and non-Lambertian regions, which resolves occlusions and texture ambiguities effectively. Experimentation on the KITTI driving dataset reveals that our scheme achieves state-of-the-art results in all of the three tasks, performing better than previously unsupervised methods and comparably with supervised ones.



### Zero-Shot Sketch-Image Hashing
- **Arxiv ID**: http://arxiv.org/abs/1803.02284v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02284v1)
- **Published**: 2018-03-06 16:23:44+00:00
- **Updated**: 2018-03-06 16:23:44+00:00
- **Authors**: Yuming Shen, Li Liu, Fumin Shen, Ling Shao
- **Comment**: Accepted as spotlight at CVPR 2018
- **Journal**: None
- **Summary**: Recent studies show that large-scale sketch-based image retrieval (SBIR) can be efficiently tackled by cross-modal binary representation learning methods, where Hamming distance matching significantly speeds up the process of similarity search. Providing training and test data subjected to a fixed set of pre-defined categories, the cutting-edge SBIR and cross-modal hashing works obtain acceptable retrieval performance. However, most of the existing methods fail when the categories of query sketches have never been seen during training. In this paper, the above problem is briefed as a novel but realistic zero-shot SBIR hashing task. We elaborate the challenges of this special task and accordingly propose a zero-shot sketch-image hashing (ZSIH) model. An end-to-end three-network architecture is built, two of which are treated as the binary encoders. The third network mitigates the sketch-image heterogeneity and enhances the semantic relations among data by utilizing the Kronecker fusion layer and graph convolution, respectively. As an important part of ZSIH, we formulate a generative hashing scheme in reconstructing semantic knowledge representations for zero-shot retrieval. To the best of our knowledge, ZSIH is the first zero-shot hashing work suitable for SBIR and cross-modal search. Comprehensive experiments are conducted on two extended datasets, i.e., Sketchy and TU-Berlin with a novel zero-shot train-test split. The proposed model remarkably outperforms related works.



### Hybrid Multi-camera Visual Servoing to Moving Target
- **Arxiv ID**: http://arxiv.org/abs/1803.02285v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.02285v2)
- **Published**: 2018-03-06 16:25:15+00:00
- **Updated**: 2018-08-02 08:59:14+00:00
- **Authors**: Hanz Cuevas-Velasquez, Nanbo Li, Radim Tylecek, Marcelo Saval-Calvo, Robert B. Fisher
- **Comment**: 6 pages, Published in IROS 2018
- **Journal**: None
- **Summary**: Visual servoing is a well-known task in robotics. However, there are still challenges when multiple visual sources are combined to accurately guide the robot or occlusions appear. In this paper we present a novel visual servoing approach using hybrid multi-camera input data to lead a robot arm accurately to dynamically moving target points in the presence of partial occlusions. The approach uses four RGBD sensors as Eye-to-Hand (EtoH) visual input, and an arm-mounted stereo camera as Eye-in-Hand (EinH). A Master supervisor task selects between using the EtoH or the EinH, depending on the distance between the robot and target. The Master also selects the subset of EtoH cameras that best perceive the target. When the EinH sensor is used, if the target becomes occluded or goes out of the sensor's view-frustum, the Master switches back to the EtoH sensors to re-track the object. Using this adaptive visual input data, the robot is then controlled using an iterative planner that uses position, orientation and joint configuration to estimate the trajectory. Since the target is dynamic, this trajectory is updated every time-step. Experiments show good performance in four different situations: tracking a ball, targeting a bulls-eye, guiding a straw to a mouth and delivering an item to a moving hand. The experiments cover both simple situations such as a ball that is mostly visible from all cameras, and more complex situations such as the mouth which is partially occluded from some of the sensors.



### Learning monocular visual odometry with dense 3D mapping from dense 3D flow
- **Arxiv ID**: http://arxiv.org/abs/1803.02286v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.02286v2)
- **Published**: 2018-03-06 16:26:06+00:00
- **Updated**: 2018-07-25 15:40:15+00:00
- **Authors**: Cheng Zhao, Li Sun, Pulak Purkait, Tom Duckett, Rustam Stolkin
- **Comment**: International Conference on Intelligent Robots and Systems(IROS 2018)
- **Journal**: International Conference on Intelligent Robots and Systems(IROS
  2018)
- **Summary**: This paper introduces a fully deep learning approach to monocular SLAM, which can perform simultaneous localization using a neural network for learning visual odometry (L-VO) and dense 3D mapping. Dense 2D flow and a depth image are generated from monocular images by sub-networks, which are then used by a 3D flow associated layer in the L-VO network to generate dense 3D flow. Given this 3D flow, the dual-stream L-VO network can then predict the 6DOF relative pose and furthermore reconstruct the vehicle trajectory. In order to learn the correlation between motion directions, the Bivariate Gaussian modelling is employed in the loss function. The L-VO network achieves an overall performance of 2.68% for average translational error and 0.0143 deg/m for average rotational error on the KITTI odometry benchmark. Moreover, the learned depth is fully leveraged to generate a dense 3D map. As a result, an entire visual SLAM system, that is, learning monocular odometry combined with dense 3D mapping, is achieved.



### Deep Thermal Imaging: Proximate Material Type Recognition in the Wild through Deep Learning of Spatial Surface Temperature Patterns
- **Arxiv ID**: http://arxiv.org/abs/1803.02310v1
- **DOI**: 10.1145/3173574.3173576
- **Categories**: **cs.CV**, cond-mat.mtrl-sci, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.02310v1)
- **Published**: 2018-03-06 17:29:08+00:00
- **Updated**: 2018-03-06 17:29:08+00:00
- **Authors**: Youngjun Cho, Nadia Bianchi-Berthouze, Nicolai Marquardt, Simon J. Julier
- **Comment**: Proceedings of the 2018 CHI Conference on Human Factors in Computing
  Systems
- **Journal**: None
- **Summary**: We introduce Deep Thermal Imaging, a new approach for close-range automatic recognition of materials to enhance the understanding of people and ubiquitous technologies of their proximal environment. Our approach uses a low-cost mobile thermal camera integrated into a smartphone to capture thermal textures. A deep neural network classifies these textures into material types. This approach works effectively without the need for ambient light sources or direct contact with materials. Furthermore, the use of a deep learning network removes the need to handcraft the set of features for different materials. We evaluated the performance of the system by training it to recognise 32 material types in both indoor and outdoor environments. Our approach produced recognition accuracies above 98% in 14,860 images of 15 indoor materials and above 89% in 26,584 images of 17 outdoor materials. We conclude by discussing its potentials for real-time use in HCI applications and future directions.



### Comparison of Deep Learning Approaches for Multi-Label Chest X-Ray Classification
- **Arxiv ID**: http://arxiv.org/abs/1803.02315v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02315v2)
- **Published**: 2018-03-06 18:04:25+00:00
- **Updated**: 2019-01-29 17:10:24+00:00
- **Authors**: Ivo M. Baltruschat, Hannes Nickisch, Michael Grass, Tobias Knopp, Axel Saalbach
- **Comment**: added official split, non-image feature investigation
- **Journal**: None
- **Summary**: The increased availability of X-ray image archives (e.g. the ChestX-ray14 dataset from the NIH Clinical Center) has triggered a growing interest in deep learning techniques. To provide better insight into the different approaches, and their applications to chest X-ray classification, we investigate a powerful network architecture in detail: the ResNet-50. Building on prior work in this domain, we consider transfer learning with and without fine-tuning as well as the training of a dedicated X-ray network from scratch. To leverage the high spatial resolution of X-ray data, we also include an extended ResNet-50 architecture, and a network integrating non-image data (patient age, gender and acquisition type) in the classification process. In a concluding experiment, we also investigate multiple ResNet depths (i.e. ResNet-38 and ResNet-101). In a systematic evaluation, using 5-fold re-sampling and a multi-label loss function, we compare the performance of the different approaches for pathology classification by ROC statistics and analyze differences between the classifiers using rank correlation. Overall, we observe a considerable spread in the achieved performance and conclude that the X-ray-specific ResNet-38, integrating non-image data yields the best overall results. Furthermore, class activation maps are used to understand the classification process, and a detailed analysis of the impact of non-image features is provided.



### Comparison of various image fusion methods for impervious surface classification from VNREDSat-1
- **Arxiv ID**: http://arxiv.org/abs/1803.02326v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02326v2)
- **Published**: 2018-03-06 18:29:45+00:00
- **Updated**: 2018-05-04 09:29:28+00:00
- **Authors**: Hung V. Luu, Manh V. Pham, Chuc D. Man, Hung Q. Bui, Thanh T. N. Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: Impervious surface is an important indicator for urban development monitoring. Accurate urban impervious surfaces mapping with VNREDSat-1 remains challenging due to their spectral diversity not captured by individual PAN image. In this artical, five multi-resolution image fusion techniques were compared for classification task of urban impervious surface. The result shows that for VNREDSat-1 dataset, UNB and Wavelet tranform methods are the best techniques reserving spatial and spectral information of original MS image, respectively. However, the UNB technique gives best results when it comes to impervious surface classification especially in the case of shadow area included in non-impervious surface group.



### Fast Cylinder and Plane Extraction from Depth Cameras for Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/1803.02380v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.02380v3)
- **Published**: 2018-03-06 19:07:45+00:00
- **Updated**: 2018-07-05 11:17:01+00:00
- **Authors**: Pedro F. Proença, Yang Gao
- **Comment**: Accepted to IROS 2018
- **Journal**: None
- **Summary**: This paper presents CAPE, a method to extract planes and cylinder segments from organized point clouds, which processes 640x480 depth images on a single CPU core at an average of 300 Hz, by operating on a grid of planar cells. While, compared to state-of-the-art plane extraction, the latency of CAPE is more consistent and 4-10 times faster, depending on the scene, we also demonstrate empirically that applying CAPE to visual odometry can improve trajectory estimation on scenes made of cylindrical surfaces (e.g. tunnels), whereas using a plane extraction approach that is not curve-aware deteriorates performance on these scenes. To use these geometric primitives in visual odometry, we propose extending a probabilistic RGB-D odometry framework based on points, lines and planes to cylinder primitives. Following this framework, CAPE runs on fused depth maps and the parameters of cylinders are modelled probabilistically to account for uncertainty and weight accordingly the pose optimization residuals.



### Trifo-VIO: Robust and Efficient Stereo Visual Inertial Odometry using Points and Lines
- **Arxiv ID**: http://arxiv.org/abs/1803.02403v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02403v2)
- **Published**: 2018-03-06 19:49:00+00:00
- **Updated**: 2018-09-18 23:05:27+00:00
- **Authors**: Feng Zheng, Grace Tsai, Zhe Zhang, Shaoshan Liu, Chen-Chi Chu, Hongbing Hu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present the Trifo Visual Inertial Odometry (Trifo-VIO), a tightly-coupled filtering-based stereo VIO system using both points and lines. Line features help improve system robustness in challenging scenarios when point features cannot be reliably detected or tracked, e.g. low-texture environment or lighting change. In addition, we propose a novel lightweight filtering-based loop closing technique to reduce accumulated drift without global bundle adjustment or pose graph optimization. We formulate loop closure as EKF updates to optimally relocate the current sliding window maintained by the filter to past keyframes. We also present the Trifo Ironsides dataset, a new visual-inertial dataset, featuring high-quality synchronized stereo camera and IMU data from the Ironsides sensor [3] with various motion types and textures and millimeter-accuracy groundtruth. To validate the performance of the proposed system, we conduct extensive comparison with state-of-the-art approaches (OKVIS, VINS-MONO and S-MSCKF) using both the public EuRoC dataset and the Trifo Ironsides dataset.



### Categorical Mixture Models on VGGNet activations
- **Arxiv ID**: http://arxiv.org/abs/1803.02446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02446v1)
- **Published**: 2018-03-06 22:20:10+00:00
- **Updated**: 2018-03-06 22:20:10+00:00
- **Authors**: Sean Billings
- **Comment**: None
- **Journal**: None
- **Summary**: In this project, I use unsupervised learning techniques in order to cluster a set of yelp restaurant photos under meaningful topics. In order to do this, I extract layer activations from a pre-trained implementation of the popular VGGNet convolutional neural network. First, I explore using LDA with the activations of convolutional layers as features. Secondly, I explore using the object-recognition powers of VGGNet trained on ImageNet in order to extract meaningful objects from the photos, and then perform LDA to group the photos under topic-archetypes. I find that this second approach finds meaningful archetypes, which match the human intuition for photo topics such as restaurant, food, and drinks. Furthermore, these clusters align well and distinctly with the actual yelp photo labels.



