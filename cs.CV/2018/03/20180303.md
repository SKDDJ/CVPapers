# Arxiv Papers in cs.CV on 2018-03-03
### Focal Loss Dense Detector for Vehicle Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1803.01114v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01114v1)
- **Published**: 2018-03-03 06:18:27+00:00
- **Updated**: 2018-03-03 06:18:27+00:00
- **Authors**: Xiaoliang Wang, Peng Cheng, Xinchuan Liu, Benedict Uzochukwu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has been widely recognized as a promising approach in different computer vision applications. Specifically, one-stage object detector and two-stage object detector are regarded as the most important two groups of Convolutional Neural Network based object detection methods. One-stage object detector could usually outperform two-stage object detector in speed; However, it normally trails in detection accuracy, compared with two-stage object detectors. In this study, focal loss based RetinaNet, which works as one-stage object detector, is utilized to be able to well match the speed of regular one-stage detectors and also defeat two-stage detectors in accuracy, for vehicle detection. State-of-the-art performance result has been showed on the DETRAC vehicle dataset.



### OIL: Observational Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/1803.01129v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.01129v3)
- **Published**: 2018-03-03 09:17:42+00:00
- **Updated**: 2019-05-23 11:24:12+00:00
- **Authors**: Guohao Li, Matthias MÃ¼ller, Vincent Casser, Neil Smith, Dominik L. Michels, Bernard Ghanem
- **Comment**: Accepted at RSS'19. First two authors contributed equally
- **Journal**: None
- **Summary**: Recent work has explored the problem of autonomous navigation by imitating a teacher and learning an end-to-end policy, which directly predicts controls from raw images. However, these approaches tend to be sensitive to mistakes by the teacher and do not scale well to other environments or vehicles. To this end, we propose Observational Imitation Learning (OIL), a novel imitation learning variant that supports online training and automatic selection of optimal behavior by observing multiple imperfect teachers. We apply our proposed methodology to the challenging problems of autonomous driving and UAV racing. For both tasks, we utilize the Sim4CV simulator that enables the generation of large amounts of synthetic training data and also allows for online learning and evaluation. We train a perception network to predict waypoints from raw image data and use OIL to train another network to predict controls from these waypoints. Extensive experiments demonstrate that our trained network outperforms its teachers, conventional imitation learning (IL) and reinforcement learning (RL) baselines and even humans in simulation. The project website is available at https://sites.google.com/kaust.edu.sa/oil/ and a video at https://youtu.be/_rhq8a0qgeg



### A Structural Correlation Filter Combined with A Multi-task Gaussian Particle Filter for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1803.05845v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05845v1)
- **Published**: 2018-03-03 11:11:17+00:00
- **Updated**: 2018-03-03 11:11:17+00:00
- **Authors**: Manna Dai, Shuying Cheng, Xiangjian He, Dadong Wang
- **Comment**: 10 pages. arXiv admin note: text overlap with arXiv:1703.05020 by
  other authors
- **Journal**: None
- **Summary**: In this paper, we propose a novel structural correlation filter combined with a multi-task Gaussian particle filter (KCF-GPF) model for robust visual tracking. We first present an assemble structure where several KCF trackers as weak experts provide a preliminary decision for a Gaussian particle filter to make a final decision. The proposed method is designed to exploit and complement the strength of a KCF and a Gaussian particle filter. Compared with the existing tracking methods based on correlation filters or particle filters, the proposed tracker has several advantages. First, it can detect the tracked target in a large-scale search scope via weak KCF trackers and evaluate the reliability of weak trackers\rq decisions for a Gaussian particle filter to make a strong decision, and hence it can tackle fast motions, appearance variations, occlusions and re-detections. Second, it can effectively handle large-scale variations via a Gaussian particle filter. Third, it can be amenable to fully parallel implementation using importance sampling without resampling, thereby it is convenient for VLSI implementation and can lower the computational costs. Extensive experiments on the OTB-2013 dataset containing 50 challenging sequences demonstrate that the proposed algorithm performs favourably against 16 state-of-the-art trackers.



### Depth Information Guided Crowd Counting for Complex Crowd Scenes
- **Arxiv ID**: http://arxiv.org/abs/1803.02256v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02256v2)
- **Published**: 2018-03-03 11:50:45+00:00
- **Updated**: 2018-04-23 10:30:16+00:00
- **Authors**: Mingliang Xu, Zhaoyang Ge, Xiaoheng Jiang, Gaoge Cui, Pei Lv, Bing Zhou, Changsheng Xu
- **Comment**: 9 pages, 8 figures. The paper is under consideration at Pattern
  Recognition Letters
- **Journal**: None
- **Summary**: It is important to monitor and analyze crowd events for the sake of city safety. In an EDOF (extended depth of field) image with a crowded scene, the distribution of people is highly imbalanced. People far away from the camera look much smaller and often occlude each other heavily, while people close to the camera look larger. In such a case, it is difficult to accurately estimate the number of people by using one technique. In this paper, we propose a Depth Information Guided Crowd Counting (DigCrowd) method to deal with crowded EDOF scenes. DigCrowd first uses the depth information of an image to segment the scene into a far-view region and a near-view region. Then Digcrowd maps the far-view region to its crowd density map and uses a detection method to count the people in the near-view region. In addition, we introduce a new crowd dataset that contains 1000 images. Experimental results demonstrate the effectiveness of our DigCrowd method



### Enhancement of land-use change modeling using convolutional neural networks and convolutional denoising autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1803.01159v1
- **DOI**: None
- **Categories**: **stat.AP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.01159v1)
- **Published**: 2018-03-03 13:28:51+00:00
- **Updated**: 2018-03-03 13:28:51+00:00
- **Authors**: Guodong Du, Liang Yuan, Kong Joo Shin, Shunsuke Managi
- **Comment**: 26 pages, 8 tables, 6 figures
- **Journal**: None
- **Summary**: The neighborhood effect is a key driving factor for the land-use change (LUC) process. This study applies convolutional neural networks (CNN) to capture neighborhood characteristics from satellite images and to enhance the performance of LUC modeling. We develop a hybrid CNN model (conv-net) to predict the LU transition probability by combining satellite images and geographical features. A spatial weight layer is designed to incorporate the distance-decay characteristics of neighborhood effect into conv-net. As an alternative model, we also develop a hybrid convolutional denoising autoencoder and multi-layer perceptron model (CDAE-net), which specifically learns latent representations from satellite images and denoises the image data. Finally, a DINAMICA-based cellular automata (CA) model simulates the LU pattern. The results show that the convolutional-based models improve the modeling performances compared with a model that accepts only the geographical features. Overall, conv-net outperforms CDAE-net in terms of LUC predictive performance. Nonetheless, CDAE-net performs better when the data are noisy.



### Real-Time Deep Learning Method for Abandoned Luggage Detection in Video
- **Arxiv ID**: http://arxiv.org/abs/1803.01160v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01160v3)
- **Published**: 2018-03-03 13:30:06+00:00
- **Updated**: 2018-06-15 15:01:43+00:00
- **Authors**: Sorina Smeureanu, Radu Tudor Ionescu
- **Comment**: Accepted at EUSIPCO 2018
- **Journal**: None
- **Summary**: Recent terrorist attacks in major cities around the world have brought many casualties among innocent citizens. One potential threat is represented by abandoned luggage items (that could contain bombs or biological warfare) in public areas. In this paper, we describe an approach for real-time automatic detection of abandoned luggage in video captured by surveillance cameras. The approach is comprised of two stages: (i) static object detection based on background subtraction and motion estimation and (ii) abandoned luggage recognition based on a cascade of convolutional neural networks (CNN). To train our neural networks we provide two types of examples: images collected from the Internet and realistic examples generated by imposing various suitcases and bags over the scene's background. We present empirical results demonstrating that our approach yields better performance than a strong CNN baseline method.



### The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches
- **Arxiv ID**: http://arxiv.org/abs/1803.01164v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01164v2)
- **Published**: 2018-03-03 13:46:40+00:00
- **Updated**: 2018-09-12 15:57:23+00:00
- **Authors**: Md Zahangir Alom, Tarek M. Taha, Christopher Yakopcic, Stefan Westberg, Paheding Sidike, Mst Shamima Nasrin, Brian C Van Esesn, Abdul A S. Awwal, Vijayan K. Asari
- **Comment**: 39 pages, 46 figures, 3 tables. arXiv admin note: text overlap with
  arXiv:1408.3264, arXiv:1411.4046
- **Journal**: None
- **Summary**: Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].



### Chest X-Ray Analysis of Tuberculosis by Deep Learning with Segmentation and Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.01199v1
- **DOI**: 10.1109/ELNANO.2018.8477564
- **Categories**: **cs.LG**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1803.01199v1)
- **Published**: 2018-03-03 16:42:19+00:00
- **Updated**: 2018-03-03 16:42:19+00:00
- **Authors**: Sergii Stirenko, Yuriy Kochura, Oleg Alienin, Oleksandr Rokovyi, Peng Gang, Wei Zeng, Yuri Gordienko
- **Comment**: 6 pages, 11 figures, 1 table
- **Journal**: 2018 IEEE 38th International Conference on Electronics and
  Nanotechnology (ELNANO), Kiev, 2018, pp. 422-428
- **Summary**: The results of chest X-ray (CXR) analysis of 2D images to get the statistically reliable predictions (availability of tuberculosis) by computer-aided diagnosis (CADx) on the basis of deep learning are presented. They demonstrate the efficiency of lung segmentation, lossless and lossy data augmentation for CADx of tuberculosis by deep convolutional neural network (CNN) applied to the small and not well-balanced dataset even. CNN demonstrates ability to train (despite overfitting) on the pre-processed dataset obtained after lung segmentation in contrast to the original not-segmented dataset. Lossless data augmentation of the segmented dataset leads to the lowest validation loss (without overfitting) and nearly the same accuracy (within the limits of standard deviation) in comparison to the original and other pre-processed datasets after lossy data augmentation. The additional limited lossy data augmentation results in the lower validation loss, but with a decrease of the validation accuracy. In conclusion, besides the more complex deep CNNs and bigger datasets, the better progress of CADx for the small and not well-balanced datasets even could be obtained by better segmentation, data augmentation, dataset stratification, and exclusion of non-evident outliers.



### Automatic Instrument Segmentation in Robot-Assisted Surgery Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1803.01207v2
- **DOI**: 10.1109/ICMLA.2018.00100
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01207v2)
- **Published**: 2018-03-03 17:41:55+00:00
- **Updated**: 2018-06-19 19:50:47+00:00
- **Authors**: Alexey Shvets, Alexander Rakhlin, Alexandr A. Kalinin, Vladimir Iglovikov
- **Comment**: 9 pages, 3 figures. arXiv admin note: substantial text overlap with
  arXiv:1804.08024
- **Journal**: 2018 17th IEEE International Conference on Machine Learning and
  Applications (ICMLA)
- **Summary**: Semantic segmentation of robotic instruments is an important problem for the robot-assisted surgery. One of the main challenges is to correctly detect an instrument's position for the tracking and pose estimation in the vicinity of surgical scenes. Accurate pixel-wise instrument segmentation is needed to address this challenge. In this paper we describe our winning solution for MICCAI 2017 Endoscopic Vision SubChallenge: Robotic Instrument Segmentation. Our approach demonstrates an improvement over the state-of-the-art results using several novel deep neural network architectures. It addressed the binary segmentation problem, where every pixel in an image is labeled as an instrument or background from the surgery video feed. In addition, we solve a multi-class segmentation problem, where we distinguish different instruments or different parts of an instrument from the background. In this setting, our approach outperforms other methods in every task subcategory for automatic instrument segmentation thereby providing state-of-the-art solution for this problem. The source code for our solution is made publicly available at https://github.com/ternaus/robot-surgery-segmentation



### Deep Bayesian Active Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1803.01216v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T10, 62H30, 62H35
- **Links**: [PDF](http://arxiv.org/pdf/1803.01216v1)
- **Published**: 2018-03-03 19:13:40+00:00
- **Updated**: 2018-03-03 19:13:40+00:00
- **Authors**: Matthias Rottmann, Karsten Kahl, Hanno Gottschalk
- **Comment**: None
- **Journal**: None
- **Summary**: In many applications the process of generating label information is expensive and time consuming. We present a new method that combines active and semi-supervised deep learning to achieve high generalization performance from a deep convolutional neural network with as few known labels as possible. In a setting where a small amount of labeled data as well as a large amount of unlabeled data is available, our method first learns the labeled data set. This initialization is followed by an expectation maximization algorithm, where further training reduces classification entropy on the unlabeled data by targeting a low entropy fit which is consistent with the labeled data. In addition the algorithm asks at a specified frequency an oracle for labels of data with entropy above a certain entropy quantile. Using this active learning component we obtain an agile labeling process that achieves high accuracy, but requires only a small amount of known labels. For the MNIST dataset we report an error rate of 2.06% using only 300 labels and 1.06% for 1000 labels. These results are obtained without employing any special network architecture or data augmentation.



### GAN-based Synthetic Medical Image Augmentation for increased CNN Performance in Liver Lesion Classification
- **Arxiv ID**: http://arxiv.org/abs/1803.01229v1
- **DOI**: 10.1016/j.neucom.2018.09.013
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.01229v1)
- **Published**: 2018-03-03 20:20:38+00:00
- **Updated**: 2018-03-03 20:20:38+00:00
- **Authors**: Maayan Frid-Adar, Idit Diamant, Eyal Klang, Michal Amitai, Jacob Goldberger, Hayit Greenspan
- **Comment**: Preprint submitted to Neurocomputing
- **Journal**: None
- **Summary**: Deep learning methods, and in particular convolutional neural networks (CNNs), have led to an enormous breakthrough in a wide range of computer vision tasks, primarily by using large-scale annotated datasets. However, obtaining such datasets in the medical domain remains a challenge. In this paper, we present methods for generating synthetic medical images using recently presented deep learning Generative Adversarial Networks (GANs). Furthermore, we show that generated medical images can be used for synthetic data augmentation, and improve the performance of CNN for medical image classification. Our novel method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). We first exploit GAN architectures for synthesizing high quality liver lesion ROIs. Then we present a novel scheme for liver lesion classification using CNN. Finally, we train the CNN using classic data augmentation and our synthetic data augmentation and compare performance. In addition, we explore the quality of our synthesized examples using visualization and expert assessment. The classification performance using only classic data augmentation yielded 78.6% sensitivity and 88.4% specificity. By adding the synthetic data augmentation the results increased to 85.7% sensitivity and 92.4% specificity. We believe that this approach to synthetic data augmentation can generalize to other medical classification applications and thus support radiologists' efforts to improve diagnosis.



### A Benchmark for Iris Location and a Deep Learning Detector Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1803.01250v5
- **DOI**: 10.1109/IJCNN.2018.8489638
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01250v5)
- **Published**: 2018-03-03 22:08:30+00:00
- **Updated**: 2018-04-30 05:02:39+00:00
- **Authors**: Evair Severo, Rayson Laroca, Cides S. Bezerra, Luiz A. Zanlorensi, Daniel Weingaertner, Gladston Moreira, David Menotti
- **Comment**: Accepted for presentation at the International Joint Conference on
  Neural Networks (IJCNN) 2018
- **Journal**: None
- **Summary**: The iris is considered as the biometric trait with the highest unique probability. The iris location is an important task for biometrics systems, affecting directly the results obtained in specific applications such as iris recognition, spoofing and contact lenses detection, among others. This work defines the iris location problem as the delimitation of the smallest squared window that encompasses the iris region. In order to build a benchmark for iris location we annotate (iris squared bounding boxes) four databases from different biometric applications and make them publicly available to the community. Besides these 4 annotated databases, we include 2 others from the literature. We perform experiments on these six databases, five obtained with near infra-red sensors and one with visible light sensor. We compare the classical and outstanding Daugman iris location approach with two window based detectors: 1) a sliding window detector based on features from Histogram of Oriented Gradients (HOG) and a linear Support Vector Machines (SVM) classifier; 2) a deep learning based detector fine-tuned from YOLO object detector. Experimental results showed that the deep learning based detector outperforms the other ones in terms of accuracy and runtime (GPUs version) and should be chosen whenever possible.



### Unsupervised Learning of Face Representations
- **Arxiv ID**: http://arxiv.org/abs/1803.01260v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01260v1)
- **Published**: 2018-03-03 23:20:52+00:00
- **Updated**: 2018-03-03 23:20:52+00:00
- **Authors**: Samyak Datta, Gaurav Sharma, C. V. Jawahar
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach for unsupervised training of CNNs in order to learn discriminative face representations. We mine supervised training data by noting that multiple faces in the same video frame must belong to different persons and the same face tracked across multiple frames must belong to the same person. We obtain millions of face pairs from hundreds of videos without using any manual supervision. Although faces extracted from videos have a lower spatial resolution than those which are available as part of standard supervised face datasets such as LFW and CASIA-WebFace, the former represent a much more realistic setting, e.g. in surveillance scenarios where most of the faces detected are very small. We train our CNNs with the relatively low resolution faces extracted from video frames collected, and achieve a higher verification accuracy on the benchmark LFW dataset cf. hand-crafted features such as LBPs, and even surpasses the performance of state-of-the-art deep networks such as VGG-Face, when they are made to work with low resolution input images.



