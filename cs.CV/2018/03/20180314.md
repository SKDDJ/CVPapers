# Arxiv Papers in cs.CV on 2018-03-14
### Revisiting Salient Object Detection: Simultaneous Detection, Ranking, and Subitizing of Multiple Salient Objects
- **Arxiv ID**: http://arxiv.org/abs/1803.05082v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05082v2)
- **Published**: 2018-03-14 00:20:28+00:00
- **Updated**: 2018-03-23 14:59:48+00:00
- **Authors**: Md Amirul Islam, Mahmoud Kalash, Neil D. B. Bruce
- **Comment**: To appear in CVPR 2018
- **Journal**: None
- **Summary**: Salient object detection is a problem that has been considered in detail and many solutions proposed. In this paper, we argue that work to date has addressed a problem that is relatively ill-posed. Specifically, there is not universal agreement about what constitutes a salient object when multiple observers are queried. This implies that some objects are more likely to be judged salient than others, and implies a relative rank exists on salient objects. The solution presented in this paper solves this more general problem that considers relative rank, and we propose data and metrics suitable to measuring success in a relative object saliency landscape. A novel deep learning solution is proposed based on a hierarchical representation of relative saliency and stage-wise refinement. We also show that the problem of salient object subitizing can be addressed with the same network, and our approach exceeds performance of any prior work across all metrics considered (both traditional and newly proposed).



### DeepN-JPEG: A Deep Neural Network Favorable JPEG-based Image Compression Framework
- **Arxiv ID**: http://arxiv.org/abs/1803.05788v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1803.05788v1)
- **Published**: 2018-03-14 02:18:55+00:00
- **Updated**: 2018-03-14 02:18:55+00:00
- **Authors**: Zihao Liu, Tao Liu, Wujie Wen, Lei Jiang, Jie Xu, Yanzhi Wang, Gang Quan
- **Comment**: 55th Design Automation Conference (DAC2018)
- **Journal**: None
- **Summary**: As one of most fascinating machine learning techniques, deep neural network (DNN) has demonstrated excellent performance in various intelligent tasks such as image classification. DNN achieves such performance, to a large extent, by performing expensive training over huge volumes of training data. To reduce the data storage and transfer overhead in smart resource-limited Internet-of-Thing (IoT) systems, effective data compression is a "must-have" feature before transferring real-time produced dataset for training or classification. While there have been many well-known image compression approaches (such as JPEG), we for the first time find that a human-visual based image compression approach such as JPEG compression is not an optimized solution for DNN systems, especially with high compression ratios. To this end, we develop an image compression framework tailored for DNN applications, named "DeepN-JPEG", to embrace the nature of deep cascaded information process mechanism of DNN architecture. Extensive experiments, based on "ImageNet" dataset with various state-of-the-art DNNs, show that "DeepN-JPEG" can achieve ~3.5x higher compression rate over the popular JPEG solution while maintaining the same accuracy level for image recognition, demonstrating its great potential of storage and power efficiency in DNN-based smart IoT system design.



### Feature Distillation: DNN-Oriented JPEG Compression Against Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1803.05787v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1803.05787v2)
- **Published**: 2018-03-14 02:24:59+00:00
- **Updated**: 2019-04-16 16:46:16+00:00
- **Authors**: Zihao Liu, Qi Liu, Tao Liu, Nuo Xu, Xue Lin, Yanzhi Wang, Wujie Wen
- **Comment**: 2019 Conference on Computer Vision and Pattern Recognition (CVPR
  2019)
- **Journal**: None
- **Summary**: Image compression-based approaches for defending against the adversarial-example attacks, which threaten the safety use of deep neural networks (DNN), have been investigated recently. However, prior works mainly rely on directly tuning parameters like compression rate, to blindly reduce image features, thereby lacking guarantee on both defense efficiency (i.e. accuracy of polluted images) and classification accuracy of benign images, after applying defense methods. To overcome these limitations, we propose a JPEG-based defensive compression framework, namely "feature distillation", to effectively rectify adversarial examples without impacting classification accuracy on benign data. Our framework significantly escalates the defense efficiency with marginal accuracy reduction using a two-step method: First, we maximize malicious features filtering of adversarial input perturbations by developing defensive quantization in frequency domain of JPEG compression or decompression, guided by a semi-analytical method; Second, we suppress the distortions of benign features to restore classification accuracy through a DNN-oriented quantization refine process. Our experimental results show that proposed "feature distillation" can significantly surpass the latest input-transformation based mitigations such as Quilting and TV Minimization in three aspects, including defense efficiency (improve classification accuracy from $\sim20\%$ to $\sim90\%$ on adversarial examples), accuracy of benign images after defense ($\le1\%$ accuracy degradation), and processing time per image ($\sim259\times$ Speedup). Moreover, our solution can also provide the best defense efficiency ($\sim60\%$ accuracy) against the recent adaptive attack with least accuracy reduction ($\sim1\%$) on benign images when compared with other input-transformation based defense methods.



### Topology guaranteed segmentation of the human retina from OCT using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1803.05120v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05120v1)
- **Published**: 2018-03-14 03:21:01+00:00
- **Updated**: 2018-03-14 03:21:01+00:00
- **Authors**: Yufan He, Aaron Carass, Bruno M. Jedynak, Sharon D. Solomon, Shiv Saidha, Peter A. Calabresi, Jerry L. Prince
- **Comment**: None
- **Journal**: None
- **Summary**: Optical coherence tomography (OCT) is a noninvasive imaging modality which can be used to obtain depth images of the retina. The changing layer thicknesses can thus be quantified by analyzing these OCT images, moreover these changes have been shown to correlate with disease progression in multiple sclerosis. Recent automated retinal layer segmentation tools use machine learning methods to perform pixel-wise labeling and graph methods to guarantee the layer hierarchy or topology. However, graph parameters like distance and smoothness constraints must be experimentally assigned by retinal region and pathology, thus degrading the flexibility and time efficiency of the whole framework. In this paper, we develop cascaded deep networks to provide a topologically correct segmentation of the retinal layers in a single feed forward propagation. The first network (S-Net) performs pixel-wise labeling and the second regression network (R-Net) takes the topologically unconstrained S-Net results and outputs layer thicknesses for each layer and each position. Relu activation is used as the final operation of the R-Net which guarantees non-negativity of the output layer thickness. Since the segmentation boundary position is acquired by summing up the corresponding non-negative layer thicknesses, the layer ordering (i.e., topology) of the reconstructed boundaries is guaranteed even at the fovea where the distances between boundaries can be zero. The R-Net is trained using simulated masks and thus can be generalized to provide topology guaranteed segmentation for other layered structures. This deep network has achieved comparable mean absolute boundary error (2.82 {\mu}m) to state-of-the-art graph methods (2.83 {\mu}m).



### Adversarial Data Programming: Using GANs to Relax the Bottleneck of Curated Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/1803.05137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05137v1)
- **Published**: 2018-03-14 04:54:02+00:00
- **Updated**: 2018-03-14 04:54:02+00:00
- **Authors**: Arghya Pal, Vineeth N Balasubramanian
- **Comment**: CVPR 2018 main conference paper
- **Journal**: None
- **Summary**: Paucity of large curated hand-labeled training data for every domain-of-interest forms a major bottleneck in the deployment of machine learning models in computer vision and other fields. Recent work (Data Programming) has shown how distant supervision signals in the form of labeling functions can be used to obtain labels for given data in near-constant time. In this work, we present Adversarial Data Programming (ADP), which presents an adversarial methodology to generate data as well as a curated aggregated label has given a set of weak labeling functions. We validated our method on the MNIST, Fashion MNIST, CIFAR 10 and SVHN datasets, and it outperformed many state-of-the-art models. We conducted extensive experiments to study its usefulness, as well as showed how the proposed ADP framework can be used for transfer learning as well as multi-task learning, where data from two domains are generated simultaneously using the framework along with the label information. Our future work will involve understanding the theoretical implications of this new framework from a game-theoretic perspective, as well as explore the performance of the method on more complex datasets.



### An application of cascaded 3D fully convolutional networks for medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.05431v2
- **DOI**: 10.1016/j.compmedimag.2018.03.001
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05431v2)
- **Published**: 2018-03-14 07:20:37+00:00
- **Updated**: 2018-03-20 05:22:07+00:00
- **Authors**: Holger R. Roth, Hirohisa Oda, Xiangrong Zhou, Natsuki Shimizu, Ying Yang, Yuichiro Hayashi, Masahiro Oda, Michitaka Fujiwara, Kazunari Misawa, Kensaku Mori
- **Comment**: Preprint accepted for publication in Computerized Medical Imaging and
  Graphics. Substantial extension of arXiv:1704.06382; Corrected references to
  figure numbers in this version
- **Journal**: Computerized Medical Imaging and Graphics, Elsevier, Volume 66,
  June 2018, Pages 90-99
- **Summary**: Recent advances in 3D fully convolutional networks (FCN) have made it feasible to produce dense voxel-wise predictions of volumetric images. In this work, we show that a multi-class 3D FCN trained on manually labeled CT scans of several anatomical structures (ranging from the large organs to thin vessels) can achieve competitive segmentation results, while avoiding the need for handcrafting features or training class-specific models.   To this end, we propose a two-stage, coarse-to-fine approach that will first use a 3D FCN to roughly define a candidate region, which will then be used as input to a second 3D FCN. This reduces the number of voxels the second FCN has to classify to ~10% and allows it to focus on more detailed segmentation of the organs and vessels.   We utilize training and validation sets consisting of 331 clinical CT images and test our models on a completely unseen data collection acquired at a different hospital that includes 150 CT scans, targeting three anatomical organs (liver, spleen, and pancreas). In challenging organs such as the pancreas, our cascaded approach improves the mean Dice score from 68.5 to 82.2%, achieving the highest reported average score on this dataset. We compare with a 2D FCN method on a separate dataset of 240 CT scans with 18 classes and achieve a significantly higher performance in small organs and vessels. Furthermore, we explore fine-tuning our models to different datasets.   Our experiments illustrate the promise and robustness of current 3D FCN based semantic segmentation of medical images, achieving state-of-the-art results. Our code and trained models are available for download: https://github.com/holgerroth/3Dunet_abdomen_cascade.



### Real-time Cardiovascular MR with Spatio-temporal Artifact Suppression using Deep Learning - Proof of Concept in Congenital Heart Disease
- **Arxiv ID**: http://arxiv.org/abs/1803.05192v3
- **DOI**: 10.1002/mrm.27480
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1803.05192v3)
- **Published**: 2018-03-14 10:25:35+00:00
- **Updated**: 2018-06-14 13:03:43+00:00
- **Authors**: Andreas Hauptmann, Simon Arridge, Felix Lucka, Vivek Muthurangu, Jennifer A. Steeden
- **Comment**: None
- **Journal**: None
- **Summary**: PURPOSE: Real-time assessment of ventricular volumes requires high acceleration factors. Residual convolutional neural networks (CNN) have shown potential for removing artifacts caused by data undersampling. In this study we investigated the effect of different radial sampling patterns on the accuracy of a CNN. We also acquired actual real-time undersampled radial data in patients with congenital heart disease (CHD), and compare CNN reconstruction to Compressed Sensing (CS).   METHODS: A 3D (2D plus time) CNN architecture was developed, and trained using 2276 gold-standard paired 3D data sets, with 14x radial undersampling. Four sampling schemes were tested, using 169 previously unseen 3D 'synthetic' test data sets. Actual real-time tiny Golden Angle (tGA) radial SSFP data was acquired in 10 new patients (122 3D data sets), and reconstructed using the 3D CNN as well as a CS algorithm; GRASP.   RESULTS: Sampling pattern was shown to be important for image quality, and accurate visualisation of cardiac structures. For actual real-time data, overall reconstruction time with CNN (including creation of aliased images) was shown to be more than 5x faster than GRASP. Additionally, CNN image quality and accuracy of biventricular volumes was observed to be superior to GRASP for the same raw data.   CONCLUSION: This paper has demonstrated the potential for the use of a 3D CNN for deep de-aliasing of real-time radial data, within the clinical setting. Clinical measures of ventricular volumes using real-time data with CNN reconstruction are not statistically significantly different from the gold-standard, cardiac gated, BH techniques.



### EdgeStereo: A Context Integrated Residual Pyramid Network for Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/1803.05196v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05196v3)
- **Published**: 2018-03-14 10:36:54+00:00
- **Updated**: 2018-09-23 14:36:00+00:00
- **Authors**: Xiao Song, Xu Zhao, Hanwen Hu, Liangji Fang
- **Comment**: Accepted by Asian Conference on Computer Vision (ACCV) 2018
- **Journal**: None
- **Summary**: Recent convolutional neural networks, especially end-to-end disparity estimation models, achieve remarkable performance on stereo matching task. However, existed methods, even with the complicated cascade structure, may fail in the regions of non-textures, boundaries and tiny details. Focus on these problems, we propose a multi-task network EdgeStereo that is composed of a backbone disparity network and an edge sub-network. Given a binocular image pair, our model enables end-to-end prediction of both disparity map and edge map. Basically, we design a context pyramid to encode multi-scale context information in disparity branch, followed by a compact residual pyramid for cascaded refinement. To further preserve subtle details, our EdgeStereo model integrates edge cues by feature embedding and edge-aware smoothness loss regularization. Comparative results demonstrates that stereo matching and edge detection can help each other in the unified model. Furthermore, our method achieves state-of-art performance on both KITTI Stereo and Scene Flow benchmarks, which proves the effectiveness of our design.



### Combining Multi-level Contexts of Superpixel using Convolutional Neural Networks to perform Natural Scene Labeling
- **Arxiv ID**: http://arxiv.org/abs/1803.05200v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05200v1)
- **Published**: 2018-03-14 10:56:40+00:00
- **Updated**: 2018-03-14 10:56:40+00:00
- **Authors**: Aritra Das, Swarnendu Ghosh, Ritesh Sarkhel, Sandipan Choudhuri, Nibaran Das, Mita Nasipuri
- **Comment**: Accepted for publication in the Proceedings of Second International
  Conference on Computing and Communication 2018, Sikkim Manipal Institute of
  Technology, Sikkim, India
- **Journal**: None
- **Summary**: Modern deep learning algorithms have triggered various image segmentation approaches. However most of them deal with pixel based segmentation. However, superpixels provide a certain degree of contextual information while reducing computation cost. In our approach, we have performed superpixel level semantic segmentation considering 3 various levels as neighbours for semantic contexts. Furthermore, we have enlisted a number of ensemble approaches like max-voting and weighted-average. We have also used the Dempster-Shafer theory of uncertainty to analyze confusion among various classes. Our method has proved to be superior to a number of different modern approaches on the same dataset.



### LivDet 2017 Fingerprint Liveness Detection Competition 2017
- **Arxiv ID**: http://arxiv.org/abs/1803.05210v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05210v1)
- **Published**: 2018-03-14 11:28:52+00:00
- **Updated**: 2018-03-14 11:28:52+00:00
- **Authors**: Valerio Mura, Giulia Orrù, Roberto Casula, Alessandra Sibiriu, Giulia Loi, Pierluigi Tuveri, Luca Ghiani, Gian Luca Marcialis
- **Comment**: presented at ICB 2018
- **Journal**: None
- **Summary**: Fingerprint Presentation Attack Detection (FPAD) deals with distinguishing images coming from artificial replicas of the fingerprint characteristic, made up of materials like silicone, gelatine or latex, and images coming from alive fingerprints. Images are captured by modern scanners, typically relying on solid-state or optical technologies. Since from 2009, the Fingerprint Liveness Detection Competition (LivDet) aims to assess the performance of the state-of-the-art algorithms according to a rigorous experimental protocol and, at the same time, a simple overview of the basic achievements. The competition is open to all academics research centers and all companies that work in this field. The positive, increasing trend of the participants number, which supports the success of this initiative, is confirmed even this year: 17 algorithms were submitted to the competition, with a larger involvement of companies and academies. This means that the topic is relevant for both sides, and points out that a lot of work must be done in terms of fundamental and applied research.



### Deep Image Demosaicking using a Cascade of Convolutional Residual Denoising Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.05215v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05215v4)
- **Published**: 2018-03-14 11:44:08+00:00
- **Updated**: 2018-07-12 07:32:27+00:00
- **Authors**: Filippos Kokkinos, Stamatios Lefkimmiatis
- **Comment**: Camera ready paper to appear in the Proceedings of ECCV 2018
- **Journal**: None
- **Summary**: Demosaicking and denoising are among the most crucial steps of modern digital camera pipelines and their joint treatment is a highly ill-posed inverse problem where at-least two-thirds of the information are missing and the rest are corrupted by noise. This poses a great challenge in obtaining meaningful reconstructions and a special care for the efficient treatment of the problem is required. While there are several machine learning approaches that have been recently introduced to deal with joint image demosaicking-denoising, in this work we propose a novel deep learning architecture which is inspired by powerful classical image regularization methods and large-scale convex optimization techniques. Consequently, our derived network is more transparent and has a clear interpretation compared to alternative competitive deep learning approaches. Our extensive experiments demonstrate that our network outperforms any previous approaches on both noisy and noise-free data. This improvement in reconstruction quality is attributed to the principled way we design our network architecture, which also requires fewer trainable parameters than the current state-of-the-art deep network solution. Finally, we show that our network has the ability to generalize well even when it is trained on small datasets, while keeping the overall number of trainable parameters low.



### Face-MagNet: Magnifying Feature Maps to Detect Small Faces
- **Arxiv ID**: http://arxiv.org/abs/1803.05258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05258v1)
- **Published**: 2018-03-14 13:22:05+00:00
- **Updated**: 2018-03-14 13:22:05+00:00
- **Authors**: Pouya Samangouei, Mahyar Najibi, Larry Davis, Rama Chellappa
- **Comment**: Accepted in WACV18
- **Journal**: None
- **Summary**: In this paper, we introduce the Face Magnifier Network (Face-MageNet), a face detector based on the Faster-RCNN framework which enables the flow of discriminative information of small scale faces to the classifier without any skip or residual connections. To achieve this, Face-MagNet deploys a set of ConvTranspose, also known as deconvolution, layers in the Region Proposal Network (RPN) and another set before the Region of Interest (RoI) pooling layer to facilitate detection of finer faces. In addition, we also design, train, and evaluate three other well-tuned architectures that represent the conventional solutions to the scale problem: context pooling, skip connections, and scale partitioning. Each of these three networks achieves comparable results to the state-of-the-art face detectors. With extensive experiments, we show that Face-MagNet based on a VGG16 architecture achieves better results than the recently proposed ResNet101-based HR method on the task of face detection on WIDER dataset and also achieves similar results on the hard set as our other method SSH.



### Rotation-Sensitive Regression for Oriented Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/1803.05265v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05265v1)
- **Published**: 2018-03-14 13:29:33+00:00
- **Updated**: 2018-03-14 13:29:33+00:00
- **Authors**: Minghui Liao, Zhen Zhu, Baoguang Shi, Gui-song Xia, Xiang Bai
- **Comment**: accepted by CVPR 2018
- **Journal**: None
- **Summary**: Text in natural images is of arbitrary orientations, requiring detection in terms of oriented bounding boxes. Normally, a multi-oriented text detector often involves two key tasks: 1) text presence detection, which is a classification problem disregarding text orientation; 2) oriented bounding box regression, which concerns about text orientation. Previous methods rely on shared features for both tasks, resulting in degraded performance due to the incompatibility of the two tasks. To address this issue, we propose to perform classification and regression on features of different characteristics, extracted by two network branches of different designs. Concretely, the regression branch extracts rotation-sensitive features by actively rotating the convolutional filters, while the classification branch extracts rotation-invariant features by pooling the rotation-sensitive features. The proposed method named Rotation-sensitive Regression Detector (RRD) achieves state-of-the-art performance on three oriented scene text benchmark datasets, including ICDAR 2015, MSRA-TD500, RCTW-17 and COCO-Text. Furthermore, RRD achieves a significant improvement on a ship collection dataset, demonstrating its generality on oriented object detection.



### On the Applicability of Registration Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1803.05266v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05266v3)
- **Published**: 2018-03-14 13:30:25+00:00
- **Updated**: 2020-04-23 03:02:12+00:00
- **Authors**: Jie Luo, Alireza Sedghi, Karteek Popuri, Dana Cobzas, Miaomiao Zhang, Frank Preiswerk, Matthew Toews, Alexandra Golby, Masashi Sugiyama, William M. Wells III, Sarah Frisken
- **Comment**: MICCAI 2019. arXiv admin note: text overlap with arXiv:1704.08121
- **Journal**: None
- **Summary**: Estimating the uncertainty in (probabilistic) image registration enables, e.g., surgeons to assess the operative risk based on the trustworthiness of the registered image data. If surgeons receive inaccurately calculated registration uncertainty and misplace unwarranted confidence in the alignment solutions, severe consequences may result. For probabilistic image registration (PIR), the predominant way to quantify the registration uncertainty is using summary statistics of the distribution of transformation parameters. The majority of existing research focuses on trying out different summary statistics as well as a means to exploit them. Distinctively, in this paper, we study two rarely examined topics: (1) whether those summary statistics of the transformation distribution most informatively represent the registration uncertainty; (2) Does utilizing the registration uncertainty always be beneficial. We show that there are two types of uncertainties: the transformation uncertainty, Ut, and label uncertainty Ul. The conventional way of using Ut to quantify Ul is inappropriate and can be misleading. By a real data experiment, we also share a potentially critical finding that making use of the registration uncertainty may not always be an improvement.



### Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1803.05268v2
- **DOI**: 10.1109/CVPR.2018.00519
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05268v2)
- **Published**: 2018-03-14 13:33:06+00:00
- **Updated**: 2018-07-02 18:48:31+00:00
- **Authors**: David Mascharka, Philip Tran, Ryan Soklaski, Arjun Majumdar
- **Comment**: CVPR 2018 pre-print
- **Journal**: None
- **Summary**: Visual question answering requires high-order reasoning about an image, which is a fundamental capability needed by machine systems to follow complex directives. Recently, modular networks have been shown to be an effective framework for performing visual reasoning tasks. While modular networks were initially designed with a degree of model transparency, their performance on complex visual reasoning benchmarks was lacking. Current state-of-the-art approaches do not provide an effective mechanism for understanding the reasoning process. In this paper, we close the performance gap between interpretable models and state-of-the-art visual reasoning methods. We propose a set of visual-reasoning primitives which, when composed, manifest as a model capable of performing complex reasoning tasks in an explicitly-interpretable manner. The fidelity and interpretability of the primitives' outputs enable an unparalleled ability to diagnose the strengths and weaknesses of the resulting model. Critically, we show that these primitives are highly performant, achieving state-of-the-art accuracy of 99.1% on the CLEVR dataset. We also show that our model is able to effectively learn generalized representations when provided a small amount of data containing novel object attributes. Using the CoGenT generalization task, we show more than a 20 percentage point improvement over the current state of the art.



### Illumination-aware Faster R-CNN for Robust Multispectral Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1803.05347v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05347v2)
- **Published**: 2018-03-14 15:15:58+00:00
- **Updated**: 2018-08-14 17:34:09+00:00
- **Authors**: Chengyang Li, Dan Song, Ruofeng Tong, Min Tang
- **Comment**: Accepted for Publication in Pattern Recognition
- **Journal**: None
- **Summary**: Multispectral images of color-thermal pairs have shown more effective than a single color channel for pedestrian detection, especially under challenging illumination conditions. However, there is still a lack of studies on how to fuse the two modalities effectively. In this paper, we deeply compare six different convolutional network fusion architectures and analyse their adaptations, enabling a vanilla architecture to obtain detection performances comparable to the state-of-the-art results. Further, we discover that pedestrian detection confidences from color or thermal images are correlated with illumination conditions. With this in mind, we propose an Illumination-aware Faster R-CNN (IAF RCNN). Specifically, an Illumination-aware Network is introduced to give an illumination measure of the input image. Then we adaptively merge color and thermal sub-networks via a gate function defined over the illumination value. The experimental results on KAIST Multispectral Pedestrian Benchmark validate the effectiveness of the proposed IAF R-CNN.



### Towards Monocular Digital Elevation Model (DEM) Estimation by Convolutional Neural Networks - Application on Synthetic Aperture Radar Images
- **Arxiv ID**: http://arxiv.org/abs/1803.05387v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.05387v1)
- **Published**: 2018-03-14 16:32:18+00:00
- **Updated**: 2018-03-14 16:32:18+00:00
- **Authors**: Gabriele Costante, Thomas A. Ciarfuglia, Filippo Biondi
- **Comment**: Accepted for publication in Proceedings of the 12th European
  Conference on Synthetic Aperture Radar
- **Journal**: None
- **Summary**: Synthetic aperture radar (SAR) interferometry (InSAR) is performed using repeat-pass geometry. InSAR technique is used to estimate the topographic reconstruction of the earth surface. The main problem of the range-Doppler focusing technique is the nature of the two-dimensional SAR result, affected by the layover indetermination. In order to resolve this problem, a minimum of two sensor acquisitions, separated by a baseline and extended in the cross-slant-range, are needed. However, given its multi-temporal nature, these techniques are vulnerable to atmosphere and Earth environment parameters variation in addition to physical platform instabilities. Furthermore, either two radars are needed or an interferometric cycle is required (that spans from days to weeks), which makes real time DEM estimation impossible. In this work, the authors propose a novel experimental alternative to the InSAR method that uses single-pass acquisitions, using a data driven approach implemented by Deep Neural Networks. We propose a fully Convolutional Neural Network (CNN) Encoder-Decoder architecture, training it on radar images in order to estimate DEMs from single pass image acquisitions. Our results on a set of Sentinel images show that this method is able to learn to some extent the statistical properties of the DEM. The results of this exploratory analysis are encouraging and open the way to the solution of single-pass DEM estimation problem with data driven approaches.



### Image Colorization with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.05400v5
- **DOI**: 10.1007/978-3-319-94544-6_9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05400v5)
- **Published**: 2018-03-14 16:53:03+00:00
- **Updated**: 2018-05-16 12:52:00+00:00
- **Authors**: Kamyar Nazeri, Eric Ng, Mehran Ebrahimi
- **Comment**: Lecture Notes in Computer Science, Proceedings of tenth international
  conference on Articulated Motion and Deformable Objects (AMDO), Palma,
  Mallorca, Spain, 12-13 July 2018
- **Journal**: LNCS 10945 (2018) 85-94
- **Summary**: Over the last decade, the process of automatic image colorization has been of significant interest for several application areas including restoration of aged or degraded images. This problem is highly ill-posed due to the large degrees of freedom during the assignment of color information. Many of the recent developments in automatic colorization involve images that contain a common theme or require highly processed data such as semantic maps as input. In our approach, we attempt to fully generalize the colorization procedure using a conditional Deep Convolutional Generative Adversarial Network (DCGAN), extend current methods to high-resolution images and suggest training strategies that speed up the process and greatly stabilize it. The network is trained over datasets that are publicly available such as CIFAR-10 and Places365. The results of the generative model and traditional deep neural networks are compared.



### Approximate Query Matching for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1803.05401v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1803.05401v1)
- **Published**: 2018-03-14 16:57:50+00:00
- **Updated**: 2018-03-14 16:57:50+00:00
- **Authors**: Abhijit Suprem, Polo Chau
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional image recognition involves identifying the key object in a portrait-type image with a single object focus (ILSVRC, AlexNet, and VGG). More recent approaches consider dense image recognition - segmenting an image with appropriate bounding boxes and performing image recognition within these bounding boxes (Semantic segmentation). The Visual Genome dataset [5] is an attempt to bridge these various approaches to a cohesive dataset for each subtask - bounding box generation, image recognition, captioning, and a new operation: scene graph generation. Our focus is on using such scene graphs to perform graph search on image databases to holistically retrieve images based on a search criteria. We develop a method to store scene graphs and metadata in graph databases (using Neo4J) and to perform fast approximate retrieval of images based on a graph search query. We process more complex queries than single object search, e.g. "girl eating cake" retrieves images that contain the specified relation as well as variations.



### Averaging Weights Leads to Wider Optima and Better Generalization
- **Arxiv ID**: http://arxiv.org/abs/1803.05407v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.05407v3)
- **Published**: 2018-03-14 17:09:27+00:00
- **Updated**: 2019-02-25 14:18:11+00:00
- **Authors**: Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson
- **Comment**: Appears at the Conference on Uncertainty in Artificial Intelligence
  (UAI), 2018
- **Journal**: None
- **Summary**: Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.



### Computer-aided diagnosis of lung carcinoma using deep learning - a pilot study
- **Arxiv ID**: http://arxiv.org/abs/1803.05471v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05471v1)
- **Published**: 2018-03-14 18:46:17+00:00
- **Updated**: 2018-03-14 18:46:17+00:00
- **Authors**: Zhang Li, Zheyu Hu, Jiaolong Xu, Tao Tan, Hui Chen, Zhi Duan, Ping Liu, Jun Tang, Guoping Cai, Quchang Ouyang, Yuling Tang, Geert Litjens, Qiang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Aim: Early detection and correct diagnosis of lung cancer are the most important steps in improving patient outcome. This study aims to assess which deep learning models perform best in lung cancer diagnosis. Methods: Non-small cell lung carcinoma and small cell lung carcinoma biopsy specimens were consecutively obtained and stained. The specimen slides were diagnosed by two experienced pathologists (over 20 years). Several deep learning models were trained to discriminate cancer and non-cancer biopsies. Result: Deep learning models give reasonable AUC from 0.8810 to 0.9119. Conclusion: The deep learning analysis could help to speed up the detection process for the whole-slide image (WSI) and keep the comparable detection rate with human observer.



### Targeted change detection in remote sensing images
- **Arxiv ID**: http://arxiv.org/abs/1803.05482v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1803.05482v1)
- **Published**: 2018-03-14 19:09:24+00:00
- **Updated**: 2018-03-14 19:09:24+00:00
- **Authors**: Vladimir Ignatiev, Alexey Trekin, Viktor Lobachev, Georgy Potapov, Evgeny Burnaev
- **Comment**: 10 pages, 1 figures, 1 table
- **Journal**: None
- **Summary**: Recent developments in the remote sensing systems and image processing made it possible to propose a new method of the object classification and detection of the specific changes in the series of satellite Earth images (so called targeted change detection). In this paper we propose a formal problem statement that allows to use effectively the deep learning approach to analyze time-dependent series of remote sensing images. We also introduce a new framework for the development of deep learning models for targeted change detection and demonstrate some cases of business applications it can be used for.



### Improving Object Counting with Heatmap Regulation
- **Arxiv ID**: http://arxiv.org/abs/1803.05494v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05494v2)
- **Published**: 2018-03-14 19:52:43+00:00
- **Updated**: 2018-05-23 21:43:47+00:00
- **Authors**: Shubhra Aich, Ian Stavness
- **Comment**: Code repository: https://github.com/littleaich/heatmap-regulation
- **Journal**: None
- **Summary**: In this paper, we propose a simple and effective way to improve one-look regression models for object counting from images. We use class activation map visualizations to illustrate the drawbacks of learning a pure one-look regression model for a counting task. Based on these insights, we enhance one-look regression counting models by regulating activation maps from the final convolution layer of the network with coarse ground-truth activation maps generated from simple dot annotations. We call this strategy heatmap regulation (HR). We show that this simple enhancement effectively suppresses false detections generated by the corresponding one-look baseline model and also improves the performance in terms of false negatives. Evaluations are performed on four different counting datasets --- two for car counting (CARPK, PUCPR+), one for crowd counting (WorldExpo) and another for biological cell counting (VGG-Cells). Adding HR to a simple VGG front-end improves performance on all these benchmarks compared to a simple one-look baseline model and results in state-of-the-art performance for car counting.



### Unpaired Image Captioning by Language Pivoting
- **Arxiv ID**: http://arxiv.org/abs/1803.05526v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05526v2)
- **Published**: 2018-03-14 22:41:35+00:00
- **Updated**: 2018-07-18 08:19:19+00:00
- **Authors**: Jiuxiang Gu, Shafiq Joty, Jianfei Cai, Gang Wang
- **Comment**: 17 pages, 4 figures, Accepted at ECCV 2018
- **Journal**: None
- **Summary**: Image captioning is a multimodal task involving computer vision and natural language processing, where the goal is to learn a mapping from the image to its natural language description. In general, the mapping function is learned from a training set of image-caption pairs. However, for some language, large scale image-caption paired corpus might not be available. We present an approach to this unpaired image captioning problem by language pivoting. Our method can effectively capture the characteristics of an image captioner from the pivot language (Chinese) and align it to the target language (English) using another pivot-target (Chinese-English) sentence parallel corpus. We evaluate our method on two image-to-English benchmark datasets: MSCOCO and Flickr30K. Quantitative comparisons against several baseline approaches demonstrate the effectiveness of our method.



### Self-Supervised Monocular Image Depth Learning and Confidence Estimation
- **Arxiv ID**: http://arxiv.org/abs/1803.05530v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05530v1)
- **Published**: 2018-03-14 22:59:01+00:00
- **Updated**: 2018-03-14 22:59:01+00:00
- **Authors**: Long Chen, Wen Tang, Nigel John
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) need large amounts of data with ground truth annotation, which is a challenging problem that has limited the development and fast deployment of CNNs for many computer vision tasks. We propose a novel framework for depth estimation from monocular images with corresponding confidence in a self-supervised manner. A fully differential patch-based cost function is proposed by using the Zero-Mean Normalized Cross Correlation (ZNCC) that takes multi-scale patches as a matching strategy. This approach greatly increases the accuracy and robustness of the depth learning. In addition, the proposed patch-based cost function can provide a 0 to 1 confidence, which is then used to supervise the training of a parallel network for confidence map learning and estimation. Evaluation on KITTI dataset shows that our method outperforms the state-of-the-art results.



### Evaluation of Dense 3D Reconstruction from 2D Face Images in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1803.05536v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05536v2)
- **Published**: 2018-03-14 23:12:12+00:00
- **Updated**: 2018-04-20 23:08:42+00:00
- **Authors**: Zhen-Hua Feng, Patrik Huber, Josef Kittler, Peter JB Hancock, Xiao-Jun Wu, Qijun Zhao, Paul Koppen, Matthias Rätsch
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: This paper investigates the evaluation of dense 3D face reconstruction from a single 2D image in the wild. To this end, we organise a competition that provides a new benchmark dataset that contains 2000 2D facial images of 135 subjects as well as their 3D ground truth face scans. In contrast to previous competitions or challenges, the aim of this new benchmark dataset is to evaluate the accuracy of a 3D dense face reconstruction algorithm using real, accurate and high-resolution 3D ground truth face scans. In addition to the dataset, we provide a standard protocol as well as a Python script for the evaluation. Last, we report the results obtained by three state-of-the-art 3D face reconstruction systems on the new benchmark dataset. The competition is organised along with the 2018 13th IEEE Conference on Automatic Face & Gesture Recognition.



### Context-Aware Mixed Reality: A Framework for Ubiquitous Interaction
- **Arxiv ID**: http://arxiv.org/abs/1803.05541v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05541v1)
- **Published**: 2018-03-14 23:38:54+00:00
- **Updated**: 2018-03-14 23:38:54+00:00
- **Authors**: Long Chen, Wen Tang, Nigel John, Tao Ruan Wan, Jian Jun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Mixed Reality (MR) is a powerful interactive technology that yields new types of user experience. We present a semantic based interactive MR framework that exceeds the current geometry level approaches, a step change in generating high-level context-aware interactions. Our key insight is to build semantic understanding in MR that not only can greatly enhance user experience through object-specific behaviours, but also pave the way for solving complex interaction design challenges. The framework generates semantic properties of the real world environment through dense scene reconstruction and deep image understanding. We demonstrate our approach with a material-aware prototype system for generating context-aware physical interactions between the real and the virtual objects. Quantitative and qualitative evaluations are carried out and the results show that the framework delivers accurate and fast semantic information in interactive MR environment, providing effective semantic level interactions.



