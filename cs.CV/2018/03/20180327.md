# Arxiv Papers in cs.CV on 2018-03-27
### Neural Baby Talk
- **Arxiv ID**: http://arxiv.org/abs/1803.09845v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1803.09845v1)
- **Published**: 2018-03-27 01:59:56+00:00
- **Updated**: 2018-03-27 01:59:56+00:00
- **Authors**: Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh
- **Comment**: 12 pages, 7 figures, CVPR 2018
- **Journal**: None
- **Summary**: We introduce a novel framework for image captioning that can produce natural language explicitly grounded in entities that object detectors find in the image. Our approach reconciles classical slot filling approaches (that are generally better grounded in images) with modern neural captioning approaches (that are generally more natural sounding and accurate). Our approach first generates a sentence `template' with slot locations explicitly tied to specific image regions. These slots are then filled in by visual concepts identified in the regions by object detectors. The entire architecture (sentence template generation and slot filling with object detectors) is end-to-end differentiable. We verify the effectiveness of our proposed model on different image captioning tasks. On standard image captioning and novel object captioning, our model reaches state-of-the-art on both COCO and Flickr30k datasets. We also demonstrate that our model has unique advantages when the train and test distributions of scene compositions -- and hence language priors of associated captions -- are different. Code has been made available at: https://github.com/jiasenlu/NeuralBabyTalk



### Attributes as Operators: Factorizing Unseen Attribute-Object Compositions
- **Arxiv ID**: http://arxiv.org/abs/1803.09851v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09851v2)
- **Published**: 2018-03-27 02:30:56+00:00
- **Updated**: 2018-08-27 19:33:30+00:00
- **Authors**: Tushar Nagarajan, Kristen Grauman
- **Comment**: European Conference on Computer Vision (ECCV) 2018
- **Journal**: None
- **Summary**: We present a new approach to modeling visual attributes. Prior work casts attributes in a similar role as objects, learning a latent representation where properties (e.g., sliced) are recognized by classifiers much in the way objects (e.g., apple) are. However, this common approach fails to separate the attributes observed during training from the objects with which they are composed, making it ineffectual when encountering new attribute-object compositions. Instead, we propose to model attributes as operators. Our approach learns a semantic embedding that explicitly factors out attributes from their accompanying objects, and also benefits from novel regularizers expressing attribute operators' effects (e.g., blunt should undo the effects of sharp). Not only does our approach align conceptually with the linguistic role of attributes as modifiers, but it also generalizes to recognize unseen compositions of objects and attributes. We validate our approach on two challenging datasets and demonstrate significant improvements over the state-of-the-art. In addition, we show that not only can our model recognize unseen compositions robustly in an open-world setting, it can also generalize to compositions where objects themselves were unseen during training.



### WebSeg: Learning Semantic Segmentation from Web Searches
- **Arxiv ID**: http://arxiv.org/abs/1803.09859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09859v1)
- **Published**: 2018-03-27 02:59:13+00:00
- **Updated**: 2018-03-27 02:59:13+00:00
- **Authors**: Qibin Hou, Ming-Ming Cheng, Jiangjiang Liu, Philip H. S. Torr
- **Comment**: Submitted to ECCV2018
- **Journal**: None
- **Summary**: In this paper, we improve semantic segmentation by automatically learning from Flickr images associated with a particular keyword, without relying on any explicit user annotations, thus substantially alleviating the dependence on accurate annotations when compared to previous weakly supervised methods.   To solve such a challenging problem, we leverage several low-level cues (such as saliency, edges, etc.) to help generate a proxy ground truth. Due to the diversity of web-crawled images, we anticipate a large amount of 'label noise' in which other objects might be present. We design an online noise filtering scheme which is able to deal with this label noise, especially in cluttered images. We use this filtering strategy as an auxiliary module to help assist the segmentation network in learning cleaner proxy annotations. Extensive experiments on the popular PASCAL VOC 2012 semantic segmentation benchmark show surprising good results in both our WebSeg (mIoU = 57.0%) and weakly supervised (mIoU = 63.3%) settings.



### Three Birds One Stone: A General Architecture for Salient Object Segmentation, Edge Detection and Skeleton Extraction
- **Arxiv ID**: http://arxiv.org/abs/1803.09860v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09860v2)
- **Published**: 2018-03-27 03:00:44+00:00
- **Updated**: 2019-04-06 02:31:04+00:00
- **Authors**: Qibin Hou, Jiang-Jiang Liu, Ming-Ming Cheng, Ali Borji, Philip H. S. Torr
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we aim at solving pixel-wise binary problems, including salient object segmentation, skeleton extraction, and edge detection, by introducing a unified architecture. Previous works have proposed tailored methods for solving each of the three tasks independently. Here, we show that these tasks share some similarities that can be exploited for developing a unified framework. In particular, we introduce a horizontal cascade, each component of which is densely connected to the outputs of previous component. Stringing these components together allows us to effectively exploit features across different levels hierarchically to effectively address the multiple pixel-wise binary regression tasks. To assess the performance of our proposed network on these tasks, we carry out exhaustive evaluations on multiple representative datasets. Although these tasks are inherently very different, we show that our unified approach performs very well on all of them and works far better than current single-purpose state-of-the-art methods. All the code in this paper will be publicly available.



### Towards Human-Machine Cooperation: Self-supervised Sample Mining for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1803.09867v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09867v2)
- **Published**: 2018-03-27 03:06:51+00:00
- **Updated**: 2018-05-24 11:59:38+00:00
- **Authors**: Keze Wang, Xiaopeng Yan, Dongyu Zhang, Lei Zhang, Liang Lin
- **Comment**: We enabled to mine from unlabeled or partially labeled data to boost
  object detection (Accepted by CVPR 2018) The source code is available at
  http://kezewang.com/codes/SSM_CVPR.zip
- **Journal**: None
- **Summary**: Though quite challenging, leveraging large-scale unlabeled or partially labeled images in a cost-effective way has increasingly attracted interests for its great importance to computer vision. To tackle this problem, many Active Learning (AL) methods have been developed. However, these methods mainly define their sample selection criteria within a single image context, leading to the suboptimal robustness and impractical solution for large-scale object detection. In this paper, aiming to remedy the drawbacks of existing AL methods, we present a principled Self-supervised Sample Mining (SSM) process accounting for the real challenges in object detection. Specifically, our SSM process concentrates on automatically discovering and pseudo-labeling reliable region proposals for enhancing the object detector via the introduced cross image validation, i.e., pasting these proposals into different labeled images to comprehensively measure their values under different image contexts. By resorting to the SSM process, we propose a new AL framework for gradually incorporating unlabeled or partially labeled data into the model learning while minimizing the annotating effort of users. Extensive experiments on two public benchmarks clearly demonstrate our proposed framework can achieve the comparable performance to the state-of-the-art methods with significantly fewer annotations.



### CompNet: Complementary Segmentation Network for Brain MRI Extraction
- **Arxiv ID**: http://arxiv.org/abs/1804.00521v2
- **DOI**: 10.1007/978-3-030-00931-1_72
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00521v2)
- **Published**: 2018-03-27 03:26:22+00:00
- **Updated**: 2018-06-17 04:28:20+00:00
- **Authors**: Raunak Dey, Yi Hong
- **Comment**: 8 pages, Accepted to MICCAI 2018
- **Journal**: None
- **Summary**: Brain extraction is a fundamental step for most brain imaging studies. In this paper, we investigate the problem of skull stripping and propose complementary segmentation networks (CompNets) to accurately extract the brain from T1-weighted MRI scans, for both normal and pathological brain images. The proposed networks are designed in the framework of encoder-decoder networks and have two pathways to learn features from both the brain tissue and its complementary part located outside of the brain. The complementary pathway extracts the features in the non-brain region and leads to a robust solution to brain extraction from MRIs with pathologies, which do not exist in our training dataset. We demonstrate the effectiveness of our networks by evaluating them on the OASIS dataset, resulting in the state of the art performance under the two-fold cross-validation setting. Moreover, the robustness of our networks is verified by testing on images with introduced pathologies and by showing its invariance to unseen brain pathologies. In addition, our complementary network design is general and can be extended to address other image segmentation problems with better generalization.



### Diversity Regularized Spatiotemporal Attention for Video-based Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1803.09882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09882v1)
- **Published**: 2018-03-27 03:47:53+00:00
- **Updated**: 2018-03-27 03:47:53+00:00
- **Authors**: Shuang Li, Slawomir Bak, Peter Carr, Xiaogang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Video-based person re-identification matches video clips of people across non-overlapping cameras. Most existing methods tackle this problem by encoding each video frame in its entirety and computing an aggregate representation across all frames. In practice, people are often partially occluded, which can corrupt the extracted features. Instead, we propose a new spatiotemporal attention model that automatically discovers a diverse set of distinctive body parts. This allows useful information to be extracted from all frames without succumbing to occlusions and misalignments. The network learns multiple spatial attention models and employs a diversity regularization term to ensure multiple models do not discover the same body part. Features extracted from local image regions are organized by spatial attention model and are combined using temporal attention. As a result, the network learns latent representations of the face, torso and other body parts using the best available image patches from the entire video sequence. Extensive evaluations on three datasets show that our framework outperforms the state-of-the-art approaches by large margins on multiple metrics.



### Multi-Scale Structure-Aware Network for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1803.09894v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09894v3)
- **Published**: 2018-03-27 04:37:20+00:00
- **Updated**: 2018-09-16 21:55:51+00:00
- **Authors**: Lipeng Ke, Ming-Ching Chang, Honggang Qi, Siwei Lyu
- **Comment**: Accepted by ECCV2018
- **Journal**: None
- **Summary**: We develop a robust multi-scale structure-aware neural network for human pose estimation. This method improves the recent deep conv-deconv hourglass models with four key improvements: (1) multi-scale supervision to strengthen contextual feature learning in matching body keypoints by combining feature heatmaps across scales, (2) multi-scale regression network at the end to globally optimize the structural matching of the multi-scale features, (3) structure-aware loss used in the intermediate supervision and at the regression to improve the matching of keypoints and respective neighbors to infer a higher-order matching configurations, and (4) a keypoint masking training scheme that can effectively fine-tune our network to robustly localize occluded keypoints via adjacent matches. Our method can effectively improve state-of-the-art pose estimation methods that suffer from difficulties in scale varieties, occlusions, and complex multi-person scenarios. This multi-scale supervision tightly integrates with the regression network to effectively (i) localize keypoints using the ensemble of multi-scale features, and (ii) infer global pose configuration by maximizing structural consistencies across multiple keypoints and scales. The keypoint masking training enhances these advantages to focus learning on hard occlusion samples. Our method achieves the leading position in the MPII challenge leaderboard among the state-of-the-art methods.



### Compassionately Conservative Balanced Cuts for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.09903v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09903v1)
- **Published**: 2018-03-27 05:42:47+00:00
- **Updated**: 2018-03-27 05:42:47+00:00
- **Authors**: Nathan D. Cahill, Tyler L. Hayes, Renee T. Meinhold, John F. Hamilton
- **Comment**: Long version of paper accepted at CVPR 2018
- **Journal**: None
- **Summary**: The Normalized Cut (NCut) objective function, widely used in data clustering and image segmentation, quantifies the cost of graph partitioning in a way that biases clusters or segments that are balanced towards having lower values than unbalanced partitionings. However, this bias is so strong that it avoids any singleton partitions, even when vertices are very weakly connected to the rest of the graph. Motivated by the B\"uhler-Hein family of balanced cut costs, we propose the family of Compassionately Conservative Balanced (CCB) Cut costs, which are indexed by a parameter that can be used to strike a compromise between the desire to avoid too many singleton partitions and the notion that all partitions should be balanced. We show that CCB-Cut minimization can be relaxed into an orthogonally constrained $\ell_{\tau}$-minimization problem that coincides with the problem of computing Piecewise Flat Embeddings (PFE) for one particular index value, and we present an algorithm for solving the relaxed problem by iteratively minimizing a sequence of reweighted Rayleigh quotients (IRRQ). Using images from the BSDS500 database, we show that image segmentation based on CCB-Cut minimization provides better accuracy with respect to ground truth and greater variability in region size than NCut-based image segmentation.



### A Divide-and-Conquer Approach to Compressed Sensing MRI
- **Arxiv ID**: http://arxiv.org/abs/1803.09909v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09909v1)
- **Published**: 2018-03-27 06:07:17+00:00
- **Updated**: 2018-03-27 06:07:17+00:00
- **Authors**: Liyan Sun, Zhiwen Fan, Xinghao Ding, Congbo Cai, Yue Huang, John Paisley
- **Comment**: 37 pages, 20 figures, 2 tables
- **Journal**: None
- **Summary**: Compressed sensing (CS) theory assures us that we can accurately reconstruct magnetic resonance images using fewer k-space measurements than the Nyquist sampling rate requires. In traditional CS-MRI inversion methods, the fact that the energy within the Fourier measurement domain is distributed non-uniformly is often neglected during reconstruction. As a result, more densely sampled low-frequency information tends to dominate penalization schemes for reconstructing MRI at the expense of high-frequency details. In this paper, we propose a new framework for CS-MRI inversion in which we decompose the observed k-space data into "subspaces" via sets of filters in a lossless way, and reconstruct the images in these various spaces individually using off-the-shelf algorithms. We then fuse the results to obtain the final reconstruction. In this way we are able to focus reconstruction on frequency information within the entire k-space more equally, preserving both high and low frequency details. We demonstrate that the proposed framework is competitive with state-of-the-art methods in CS-MRI in terms of quantitative performance, and often improves an algorithm's results qualitatively compared with it's direct application to k-space.



### Person re-identification with fusion of hand-crafted and deep pose-based body region features
- **Arxiv ID**: http://arxiv.org/abs/1803.10630v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10630v1)
- **Published**: 2018-03-27 06:09:36+00:00
- **Updated**: 2018-03-27 06:09:36+00:00
- **Authors**: Jubin Johnson, Shunsuke Yasugi, Yoichi Sugino, Sugiri Pranata, Shengmei Shen
- **Comment**: arXiv admin note: text overlap with arXiv:1711.08184,
  arXiv:1707.00798 by other authors
- **Journal**: None
- **Summary**: Person re-identification (re-ID) aims to accurately re- trieve a person from a large-scale database of images cap- tured across multiple cameras. Existing works learn deep representations using a large training subset of unique per- sons. However, identifying unseen persons is critical for a good re-ID algorithm. Moreover, the misalignment be- tween person crops to detection errors or pose variations leads to poor feature matching. In this work, we present a fusion of handcrafted features and deep feature representa- tion learned using multiple body parts to complement the global body features that achieves high performance on un- seen test images. Pose information is used to detect body regions that are passed through Convolutional Neural Net- works (CNN) to guide feature learning. Finally, a metric learning step enables robust distance matching on a dis- criminative subspace. Experimental results on 4 popular re-ID benchmark datasets namely VIPer, DukeMTMC-reID, Market-1501 and CUHK03 show that the proposed method achieves state-of-the-art performance in image-based per- son re-identification.



### Diagonalwise Refactorization: An Efficient Training Method for Depthwise Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1803.09926v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09926v1)
- **Published**: 2018-03-27 07:06:54+00:00
- **Updated**: 2018-03-27 07:06:54+00:00
- **Authors**: Zheng Qin, Zhaoning Zhang, Dongsheng Li, Yiming Zhang, Yuxing Peng
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Depthwise convolutions provide significant performance benefits owing to the reduction in both parameters and mult-adds. However, training depthwise convolution layers with GPUs is slow in current deep learning frameworks because their implementations cannot fully utilize the GPU capacity. To address this problem, in this paper we present an efficient method (called diagonalwise refactorization) for accelerating the training of depthwise convolution layers. Our key idea is to rearrange the weight vectors of a depthwise convolution into a large diagonal weight matrix so as to convert the depthwise convolution into one single standard convolution, which is well supported by the cuDNN library that is highly-optimized for GPU computations. We have implemented our training method in five popular deep learning frameworks. Evaluation results show that our proposed method gains $15.4\times$ training speedup on Darknet, $8.4\times$ on Caffe, $5.4\times$ on PyTorch, $3.5\times$ on MXNet, and $1.4\times$ on TensorFlow, compared to their original implementations of depthwise convolutions.



### Image Semantic Transformation: Faster, Lighter and Stronger
- **Arxiv ID**: http://arxiv.org/abs/1803.09932v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1803.09932v1)
- **Published**: 2018-03-27 07:20:46+00:00
- **Updated**: 2018-03-27 07:20:46+00:00
- **Authors**: Dasong Li, Jianbo Wang
- **Comment**: ECCV 2018 submission, 14 pages
- **Journal**: None
- **Summary**: We propose Image-Semantic-Transformation-Reconstruction-Circle(ISTRC) model, a novel and powerful method using facenet's Euclidean latent space to understand the images. As the name suggests, ISTRC construct the circle, able to perfectly reconstruct images. One powerful Euclidean latent space embedded in ISTRC is FaceNet's last layer with the power of distinguishing and understanding images. Our model will reconstruct the images and manipulate Euclidean latent vectors to achieve semantic transformations and semantic images arthimetic calculations. In this paper, we show that ISTRC performs 10 high-level semantic transformations like "Male and female","add smile","open mouth", "deduct beard or add mustache", "bigger/smaller nose", "make older and younger", "bigger lips", "bigger eyes", "bigger/smaller mouths" and "more attractive". It just takes 3 hours(GTX 1080) to train the models of 10 semantic transformations.



### Image-based deep learning for classification of noise transients in gravitational wave detectors
- **Arxiv ID**: http://arxiv.org/abs/1803.09933v1
- **DOI**: 10.1088/1361-6382/aab793
- **Categories**: **gr-qc**, astro-ph.IM, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.09933v1)
- **Published**: 2018-03-27 07:23:13+00:00
- **Updated**: 2018-03-27 07:23:13+00:00
- **Authors**: Massimiliano Razzano, Elena Cuoco
- **Comment**: 25 pages, 8 figures, accepted for publication in Classical and
  Quantum Gravity
- **Journal**: Razzano, M., and Cuoco, E., 2018, Classical and Quantum Gravity,
  Volume 35, Number 9, 2018
- **Summary**: The detection of gravitational waves has inaugurated the era of gravitational astronomy and opened new avenues for the multimessenger study of cosmic sources. Thanks to their sensitivity, the Advanced LIGO and Advanced Virgo interferometers will probe a much larger volume of space and expand the capability of discovering new gravitational wave emitters. The characterization of these detectors is a primary task in order to recognize the main sources of noise and optimize the sensitivity of interferometers. Glitches are transient noise events that can impact the data quality of the interferometers and their classification is an important task for detector characterization. Deep learning techniques are a promising tool for the recognition and classification of glitches. We present a classification pipeline that exploits convolutional neural networks to classify glitches starting from their time-frequency evolution represented as images. We evaluated the classification accuracy on simulated glitches, showing that the proposed algorithm can automatically classify glitches on very fast timescales and with high accuracy, thus providing a promising tool for online detector characterization.



### Dual Attention Matching Network for Context-Aware Feature Sequence based Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1803.09937v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09937v1)
- **Published**: 2018-03-27 07:41:18+00:00
- **Updated**: 2018-03-27 07:41:18+00:00
- **Authors**: Jianlou Si, Honggang Zhang, Chun-Guang Li, Jason Kuen, Xiangfei Kong, Alex C. Kot, Gang Wang
- **Comment**: 10 pages, 8 figures, 7 tables, accepted by CVPR 2018
- **Journal**: None
- **Summary**: Typical person re-identification (ReID) methods usually describe each pedestrian with a single feature vector and match them in a task-specific metric space. However, the methods based on a single feature vector are not sufficient enough to overcome visual ambiguity, which frequently occurs in real scenario. In this paper, we propose a novel end-to-end trainable framework, called Dual ATtention Matching network (DuATM), to learn context-aware feature sequences and perform attentive sequence comparison simultaneously. The core component of our DuATM framework is a dual attention mechanism, in which both intra-sequence and inter-sequence attention strategies are used for feature refinement and feature-pair alignment, respectively. Thus, detailed visual cues contained in the intermediate feature sequences can be automatically exploited and properly compared. We train the proposed DuATM network as a siamese network via a triplet loss assisted with a de-correlation loss and a cross-entropy loss. We conduct extensive experiments on both image and video based ReID benchmark datasets. Experimental results demonstrate the significant advantages of our approach compared to the state-of-the-art methods.



### Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1803.09956v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.09956v3)
- **Published**: 2018-03-27 08:31:28+00:00
- **Updated**: 2018-09-30 20:34:49+00:00
- **Authors**: Andy Zeng, Shuran Song, Stefan Welker, Johnny Lee, Alberto Rodriguez, Thomas Funkhouser
- **Comment**: To appear at the International Conference On Intelligent Robots and
  Systems (IROS) 2018. Project webpage: http://vpg.cs.princeton.edu Summary
  video: https://youtu.be/-OkyX7ZlhiU
- **Journal**: None
- **Summary**: Skilled robotic manipulation benefits from complex synergies between non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing can help rearrange cluttered objects to make space for arms and fingers; likewise, grasping can help displace objects to make pushing movements more precise and collision-free. In this work, we demonstrate that it is possible to discover and learn these synergies from scratch through model-free deep reinforcement learning. Our method involves training two fully convolutional networks that map from visual observations to actions: one infers the utility of pushes for a dense pixel-wise sampling of end effector orientations and locations, while the other does the same for grasping. Both networks are trained jointly in a Q-learning framework and are entirely self-supervised by trial and error, where rewards are provided from successful grasps. In this way, our policy learns pushing motions that enable future grasps, while learning grasps that can leverage past pushes. During picking experiments in both simulation and real-world scenarios, we find that our system quickly learns complex behaviors amid challenging cases of clutter, and achieves better grasping success rates and picking efficiencies than baseline alternatives after only a few hours of training. We further demonstrate that our method is capable of generalizing to novel objects. Qualitative results (videos), code, pre-trained models, and simulation environments are available at http://vpg.cs.princeton.edu



### Towards Highly Accurate Coral Texture Images Classification Using Deep Convolutional Neural Networks and Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.00516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00516v1)
- **Published**: 2018-03-27 12:05:12+00:00
- **Updated**: 2018-03-27 12:05:12+00:00
- **Authors**: Anabel Gómez-Ríos, Siham Tabik, Julián Luengo, ASM Shihavuddin, Bartosz Krawczyk, Francisco Herrera
- **Comment**: 22 pages, 10 figures
- **Journal**: None
- **Summary**: The recognition of coral species based on underwater texture images pose a significant difficulty for machine learning algorithms, due to the three following challenges embedded in the nature of this data: 1) datasets do not include information about the global structure of the coral; 2) several species of coral have very similar characteristics; and 3) defining the spatial borders between classes is difficult as many corals tend to appear together in groups. For this reason, the classification of coral species has always required an aid from a domain expert. The objective of this paper is to develop an accurate classification model for coral texture images. Current datasets contain a large number of imbalanced classes, while the images are subject to inter-class variation. We have analyzed 1) several Convolutional Neural Network (CNN) architectures, 2) data augmentation techniques and 3) transfer learning. We have achieved the state-of-the art accuracies using different variations of ResNet on the two current coral texture datasets, EILAT and RSMAS.



### Recent Developments from Attribute Profiles for Remote Sensing Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1803.10036v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10036v1)
- **Published**: 2018-03-27 12:22:49+00:00
- **Updated**: 2018-03-27 12:22:49+00:00
- **Authors**: Minh-Tan Pham, Sébastien Lefèvre, Erchan Aptoula, Lorenzo Bruzzone
- **Comment**: 6 pages; to appear in ICPRAI 2018
- **Journal**: None
- **Summary**: Morphological attribute profiles (APs) are among the most effective methods to model the spatial and contextual information for the analysis of remote sensing images, especially for classification task. Since their first introduction to this field in early 2010's, many research studies have been contributed not only to exploit and adapt their use to different applications, but also to extend and improve their performance for better dealing with more complex data. In this paper, we revisit and discuss different developments and extensions from APs which have drawn significant attention from researchers in the past few years. These studies are analyzed and gathered based on the concept of multi-stage AP construction. In our experiments, a comparative study on classification results of two remote sensing data is provided in order to show their significant improvements compared to the originally proposed APs.



### Learning Depth from Single Images with Deep Neural Network Embedding Focal Length
- **Arxiv ID**: http://arxiv.org/abs/1803.10039v1
- **DOI**: 10.1109/TIP.2018.2832296
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10039v1)
- **Published**: 2018-03-27 12:26:15+00:00
- **Updated**: 2018-03-27 12:26:15+00:00
- **Authors**: Lei He, Guanghui Wang, Zhanyi Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Learning depth from a single image, as an important issue in scene understanding, has attracted a lot of attention in the past decade. The accuracy of the depth estimation has been improved from conditional Markov random fields, non-parametric methods, to deep convolutional neural networks most recently. However, there exist inherent ambiguities in recovering 3D from a single 2D image. In this paper, we first prove the ambiguity between the focal length and monocular depth learning, and verify the result using experiments, showing that the focal length has a great influence on accurate depth recovery. In order to learn monocular depth by embedding the focal length, we propose a method to generate synthetic varying-focal-length dataset from fixed-focal-length datasets, and a simple and effective method is implemented to fill the holes in the newly generated images. For the sake of accurate depth recovery, we propose a novel deep neural network to infer depth through effectively fusing the middle-level information on the fixed-focal-length dataset, which outperforms the state-of-the-art methods built on pre-trained VGG. Furthermore, the newly generated varying-focal-length dataset is taken as input to the proposed network in both learning and inference phases. Extensive experiments on the fixed- and varying-focal-length datasets demonstrate that the learned monocular depth with embedded focal length is significantly improved compared to that without embedding the focal length information.



### Kinetic Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/1803.10045v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, physics.data-an, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.10045v1)
- **Published**: 2018-03-27 12:46:57+00:00
- **Updated**: 2018-03-27 12:46:57+00:00
- **Authors**: Michele Scipioni, Maria F. Santarelli, Luigi Landini, Ciprian Catana, Douglas N. Greve, Julie C. Price, Stefano Pedemonte
- **Comment**: 5 pages, 6 figures, Submitted to the Conference Record of "IEEE
  Nuclear Science Symposium and Medical Imaging Conference (IEEE NSS-MIC) 2017"
- **Journal**: None
- **Summary**: Parametric images provide insight into the spatial distribution of physiological parameters, but they are often extremely noisy, due to low SNR of tomographic data. Direct estimation from projections allows accurate noise modeling, improving the results of post-reconstruction fitting. We propose a method, which we name kinetic compressive sensing (KCS), based on a hierarchical Bayesian model and on a novel reconstruction algorithm, that encodes sparsity of kinetic parameters. Parametric maps are reconstructed by maximizing the joint probability, with an Iterated Conditional Modes (ICM) approach, alternating the optimization of activity time series (OS-MAP-OSL), and kinetic parameters (MAP-LM). We evaluated the proposed algorithm on a simulated dynamic phantom: a bias/variance study confirmed how direct estimates can improve the quality of parametric maps over a post-reconstruction fitting, and showed how the novel sparsity prior can further reduce their variance, without affecting bias. Real FDG PET human brain data (Siemens mMR, 40min) images were also processed. Results enforced how the proposed KCS-regularized direct method can produce spatially coherent images and parametric maps, with lower spatial noise and better tissue contrast. A GPU-based open source implementation of the algorithm is provided.



### Tensor graph convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1803.10071v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10071v1)
- **Published**: 2018-03-27 13:34:05+00:00
- **Updated**: 2018-03-27 13:34:05+00:00
- **Authors**: Tong Zhang, Wenming Zheng, Zhen Cui, Yang Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel tensor graph convolutional neural network (TGCNN) to conduct convolution on factorizable graphs, for which here two types of problems are focused, one is sequential dynamic graphs and the other is cross-attribute graphs. Especially, we propose a graph preserving layer to memorize salient nodes of those factorized subgraphs, i.e. cross graph convolution and graph pooling. For cross graph convolution, a parameterized Kronecker sum operation is proposed to generate a conjunctive adjacency matrix characterizing the relationship between every pair of nodes across two subgraphs. Taking this operation, then general graph convolution may be efficiently performed followed by the composition of small matrices, which thus reduces high memory and computational burden. Encapsuling sequence graphs into a recursive learning, the dynamics of graphs can be efficiently encoded as well as the spatial layout of graphs. To validate the proposed TGCNN, experiments are conducted on skeleton action datasets as well as matrix completion dataset. The experiment results demonstrate that our method can achieve more competitive performance with the state-of-the-art methods.



### A Framework for Evaluating 6-DOF Object Trackers
- **Arxiv ID**: http://arxiv.org/abs/1803.10075v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10075v3)
- **Published**: 2018-03-27 13:42:03+00:00
- **Updated**: 2018-09-06 20:21:41+00:00
- **Authors**: Mathieu Garon, Denis Laurendeau, Jean-François Lalonde
- **Comment**: Project website :
  http://vision.gel.ulaval.ca/~jflalonde/projects/6dofObjectTracking/index.html
- **Journal**: None
- **Summary**: We present a challenging and realistic novel dataset for evaluating 6-DOF object tracking algorithms. Existing datasets show serious limitations---notably, unrealistic synthetic data, or real data with large fiducial markers---preventing the community from obtaining an accurate picture of the state-of-the-art. Using a data acquisition pipeline based on a commercial motion capture system for acquiring accurate ground truth poses of real objects with respect to a Kinect V2 camera, we build a dataset which contains a total of 297 calibrated sequences. They are acquired in three different scenarios to evaluate the performance of trackers: stability, robustness to occlusion and accuracy during challenging interactions between a person and the object. We conduct an extensive study of a deep 6-DOF tracking architecture and determine a set of optimal parameters. We enhance the architecture and the training methodology to train a 6-DOF tracker that can robustly generalize to objects never seen during training, and demonstrate favorable performance compared to previous approaches trained specifically on the objects to track.



### DeepJDOT: Deep Joint Distribution Optimal Transport for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1803.10081v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1803.10081v3)
- **Published**: 2018-03-27 13:54:05+00:00
- **Updated**: 2018-09-05 09:57:46+00:00
- **Authors**: Bharath Bhushan Damodaran, Benjamin Kellenberger, Rémi Flamary, Devis Tuia, Nicolas Courty
- **Comment**: European Conference on Computer Vision 2018 (ECCV-2018)
- **Journal**: in Proceedings of European Conference on Computer Vision 2018
  (ECCV-2018)
- **Summary**: In computer vision, one is often confronted with problems of domain shifts, which occur when one applies a classifier trained on a source dataset to target data sharing similar characteristics (e.g. same classes), but also different latent data structures (e.g. different acquisition conditions). In such a situation, the model will perform poorly on the new data, since the classifier is specialized to recognize visual cues specific to the source domain. In this work we explore a solution, named DeepJDOT, to tackle this problem: through a measure of discrepancy on joint deep representations/labels based on optimal transport, we not only learn new data representations aligned between the source and target domain, but also simultaneously preserve the discriminative information used by the classifier. We applied DeepJDOT to a series of visual recognition tasks, where it compares favorably against state-of-the-art deep domain adaptation methods.



### Efficient parametrization of multi-domain deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1803.10082v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.10082v1)
- **Published**: 2018-03-27 13:55:56+00:00
- **Updated**: 2018-03-27 13:55:56+00:00
- **Authors**: Sylvestre-Alvise Rebuffi, Hakan Bilen, Andrea Vedaldi
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: A practical limitation of deep neural networks is their high degree of specialization to a single task and visual domain. Recently, inspired by the successes of transfer learning, several authors have proposed to learn instead universal, fixed feature extractors that, used as the first stage of any deep network, work well for several tasks and domains simultaneously. Nevertheless, such universal features are still somewhat inferior to specialized networks.   To overcome this limitation, in this paper we propose to consider instead universal parametric families of neural networks, which still contain specialized problem-specific models, but differing only by a small number of parameters. We study different designs for such parametrizations, including series and parallel residual adapters, joint adapter compression, and parameter allocations, and empirically identify the ones that yield the highest compression. We show that, in order to maximize performance, it is necessary to adapt both shallow and deep layers of a deep network, but the required changes are very small. We also show that these universal parametrization are very effective for transfer learning, where they outperform traditional fine-tuning techniques.



### Point Convolutional Neural Networks by Extension Operators
- **Arxiv ID**: http://arxiv.org/abs/1803.10091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10091v1)
- **Published**: 2018-03-27 14:06:16+00:00
- **Updated**: 2018-03-27 14:06:16+00:00
- **Authors**: Matan Atzmon, Haggai Maron, Yaron Lipman
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents Point Convolutional Neural Networks (PCNN): a novel framework for applying convolutional neural networks to point clouds. The framework consists of two operators: extension and restriction, mapping point cloud functions to volumetric functions and vise-versa. A point cloud convolution is defined by pull-back of the Euclidean volumetric convolution via an extension-restriction mechanism.   The point cloud convolution is computationally efficient, invariant to the order of points in the point cloud, robust to different samplings and varying densities, and translation invariant, that is the same convolution kernel is used at all points. PCNN generalizes image CNNs and allows readily adapting their architectures to the point cloud setting.   Evaluation of PCNN on three central point cloud learning benchmarks convincingly outperform competing point cloud learning methods, and the vast majority of methods working with more informative shape representations such as surfaces and/or normals.



### A New Target-specific Object Proposal Generation Method for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1803.10098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10098v1)
- **Published**: 2018-03-27 14:20:25+00:00
- **Updated**: 2018-03-27 14:20:25+00:00
- **Authors**: Guanjun Guo, Hanzi Wang, Yan Yan, Hong-Yuan Mark Liao, Bo Li
- **Comment**: 14pages,11figures, Submited to IEEE Transactions on Cybernetisc
- **Journal**: None
- **Summary**: Object proposal generation methods have been widely applied to many computer vision tasks. However, existing object proposal generation methods often suffer from the problems of motion blur, low contrast, deformation, etc., when they are applied to video related tasks. In this paper, we propose an effective and highly accurate target-specific object proposal generation (TOPG) method, which takes full advantage of the context information of a video to alleviate these problems. Specifically, we propose to generate target-specific object proposals by integrating the information of two important objectness cues: colors and edges, which are complementary to each other for different challenging environments in the process of generating object proposals. As a result, the recall of the proposed TOPG method is significantly increased. Furthermore, we propose an object proposal ranking strategy to increase the rank accuracy of the generated object proposals. The proposed TOPG method has yielded significant recall gain (about 20%-60% higher) compared with several state-of-the-art object proposal methods on several challenging visual tracking datasets. Then, we apply the proposed TOPG method to the task of visual tracking and propose a TOPG-based tracker (called as TOPGT), where TOPG is used as a sample selection strategy to select a small number of high-quality target candidates from the generated object proposals. Since the object proposals generated by the proposed TOPG cover many hard negative samples and positive samples, these object proposals can not only be used for training an effective classifier, but also be used as target candidates for visual tracking. Experimental results show the superior performance of TOPGT for visual tracking compared with several other state-of-the-art visual trackers (about 3%-11% higher than the winner of the VOT2015 challenge in term of distance precision).



### Random Polyhedral Scenes: An Image Generator for Active Vision System Experiments
- **Arxiv ID**: http://arxiv.org/abs/1803.10100v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10100v1)
- **Published**: 2018-03-27 14:22:42+00:00
- **Updated**: 2018-03-27 14:22:42+00:00
- **Authors**: Markus D. Solbach, Stephen Voland, Jeff Edmonds, John K. Tsotsos
- **Comment**: 18 pages, 9 Figures, 2 Listings, Technical Report
- **Journal**: None
- **Summary**: We present a Polyhedral Scene Generator system which creates a random scene based on a few user parameters, renders the scene from random view points and creates a dataset containing the renderings and corresponding annotation files. We hope that this generator will enable research on how a program could parse a scene if it had multiple viewpoints to consider. For ambiguous scenes, typically people move their head or change their position to see the scene from different angles as well as seeing how it changes while they move; this research field is called active perception. The random scene generator presented is designed to support research in this field by generating images of scenes with known complexity characteristics and with verifiable properties with respect to the distribution of features across a population. Thus, it is well-suited for research in active perception without the requirement of a live 3D environment and mobile sensing agent, including comparative performance evaluations. The system is publicly available at https://polyhedral.eecs.yorku.ca.



### A Fast Face Detection Method via Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1803.10103v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10103v1)
- **Published**: 2018-03-27 14:25:04+00:00
- **Updated**: 2018-03-27 14:25:04+00:00
- **Authors**: Guanjun Guo, Hanzi Wang, Yan Yan, Jin Zheng, Bo Li
- **Comment**: 11 figures, 30 pages, To appear in Neurocomputing
- **Journal**: None
- **Summary**: Current face or object detection methods via convolutional neural network (such as OverFeat, R-CNN and DenseNet) explicitly extract multi-scale features based on an image pyramid. However, such a strategy increases the computational burden for face detection. In this paper, we propose a fast face detection method based on discriminative complete features (DCFs) extracted by an elaborately designed convolutional neural network, where face detection is directly performed on the complete feature maps. DCFs have shown the ability of scale invariance, which is beneficial for face detection with high speed and promising performance. Therefore, extracting multi-scale features on an image pyramid employed in the conventional methods is not required in the proposed method, which can greatly improve its efficiency for face detection. Experimental results on several popular face detection datasets show the efficiency and the effectiveness of the proposed method for face detection.



### Event-based Face Detection and Tracking in the Blink of an Eye
- **Arxiv ID**: http://arxiv.org/abs/1803.10106v3
- **DOI**: 10.3389/fnins.2020.00587
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10106v3)
- **Published**: 2018-03-27 14:27:26+00:00
- **Updated**: 2019-04-02 19:05:55+00:00
- **Authors**: Gregor Lenz, Sio-Hoi Ieng, Ryad Benosman
- **Comment**: None
- **Journal**: Frontiers in Neuroscience 2020 volume 14
- **Summary**: We present the first purely event-based method for face detection using the high temporal resolution of an event-based camera. We will rely on a new feature that has never been used for such a task that relies on detecting eye blinks. Eye blinks are a unique natural dynamic signature of human faces that is captured well by event-based sensors that rely on relative changes of luminance. Although an eye blink can be captured with conventional cameras, we will show that the dynamics of eye blinks combined with the fact that two eyes act simultaneously allows to derive a robust methodology for face detection at a low computational cost and high temporal resolution. We show that eye blinks have a unique temporal signature over time that can be easily detected by correlating the acquired local activity with a generic temporal model of eye blinks that has been generated from a wide population of users. We furthermore show that once the face is reliably detected it is possible to apply a probabilistic framework to track the spatial position of a face for each incoming event while updating the position of trackers. Results are shown for several indoor and outdoor experiments. We will also release an annotated data set that can be used for future work on the topic.



### DeepScores -- A Dataset for Segmentation, Detection and Classification of Tiny Objects
- **Arxiv ID**: http://arxiv.org/abs/1804.00525v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.00525v2)
- **Published**: 2018-03-27 14:44:45+00:00
- **Updated**: 2018-05-26 21:12:59+00:00
- **Authors**: Lukas Tuggener, Ismail Elezi, Jürgen Schmidhuber, Marcello Pelillo, Thilo Stadelmann
- **Comment**: 6 pages, accepted on IEEE International Conference on Pattern
  Recognition 2018
- **Journal**: None
- **Summary**: We present the DeepScores dataset with the goal of advancing the state-of-the-art in small objects recognition, and by placing the question of object recognition in the context of scene understanding. DeepScores contains high quality images of musical scores, partitioned into 300,000 sheets of written music that contain symbols of different shapes and sizes. With close to a hundred millions of small objects, this makes our dataset not only unique, but also the largest public dataset. DeepScores comes with ground truth for object classification, detection and semantic segmentation. DeepScores thus poses a relevant challenge for computer vision in general, beyond the scope of optical music recognition (OMR) research. We present a detailed statistical analysis of the dataset, comparing it with other computer vision datasets like Caltech101/256, PASCAL VOC, SUN, SVHN, ImageNet, MS-COCO, smaller computer vision datasets, as well as with other OMR datasets. Finally, we provide baseline performances for object classification and give pointers to future research based on this dataset.



### Learning distributions of shape trajectories from longitudinal datasets: a hierarchical model on a manifold of diffeomorphisms
- **Arxiv ID**: http://arxiv.org/abs/1803.10119v2
- **DOI**: None
- **Categories**: **cs.CV**, math.DG, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/1803.10119v2)
- **Published**: 2018-03-27 15:03:51+00:00
- **Updated**: 2018-06-13 14:35:22+00:00
- **Authors**: Alexandre Bône, Olivier Colliot, Stanley Durrleman
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method to learn a distribution of shape trajectories from longitudinal data, i.e. the collection of individual objects repeatedly observed at multiple time-points. The method allows to compute an average spatiotemporal trajectory of shape changes at the group level, and the individual variations of this trajectory both in terms of geometry and time dynamics. First, we formulate a non-linear mixed-effects statistical model as the combination of a generic statistical model for manifold-valued longitudinal data, a deformation model defining shape trajectories via the action of a finite-dimensional set of diffeomorphisms with a manifold structure, and an efficient numerical scheme to compute parallel transport on this manifold. Second, we introduce a MCMC-SAEM algorithm with a specific approach to shape sampling, an adaptive scheme for proposal variances, and a log-likelihood tempering strategy to estimate our model. Third, we validate our algorithm on 2D simulated data, and then estimate a scenario of alteration of the shape of the hippocampus 3D brain structure during the course of Alzheimer's disease. The method shows for instance that hippocampal atrophy progresses more quickly in female subjects, and occurs earlier in APOE4 mutation carriers. We finally illustrate the potential of our method for classifying pathological trajectories versus normal ageing.



### End-to-End Learning of Driving Models with Surround-View Cameras and Route Planners
- **Arxiv ID**: http://arxiv.org/abs/1803.10158v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10158v2)
- **Published**: 2018-03-27 16:07:45+00:00
- **Updated**: 2018-08-06 14:54:01+00:00
- **Authors**: Simon Hecker, Dengxin Dai, Luc Van Gool
- **Comment**: to be published at ECCV 2018
- **Journal**: None
- **Summary**: For human drivers, having rear and side-view mirrors is vital for safe driving. They deliver a more complete view of what is happening around the car. Human drivers also heavily exploit their mental map for navigation. Nonetheless, several methods have been published that learn driving models with only a front-facing camera and without a route planner. This lack of information renders the self-driving task quite intractable. We investigate the problem in a more realistic setting, which consists of a surround-view camera system with eight cameras, a route planner, and a CAN bus reader. In particular, we develop a sensor setup that provides data for a 360-degree view of the area surrounding the vehicle, the driving route to the destination, and low-level driving maneuvers (e.g. steering angle and speed) by human drivers. With such a sensor setup we collect a new driving dataset, covering diverse driving scenarios and varying weather/illumination conditions. Finally, we learn a novel driving model by integrating information from the surround-view cameras and the route planner. Two route planners are exploited: 1) by representing the planned routes on OpenStreetMap as a stack of GPS coordinates, and 2) by rendering the planned routes on TomTom Go Mobile and recording the progression into a video. Our experiments show that: 1) 360-degree surround-view cameras help avoid failures made with a single front-view camera, in particular for city driving and intersection scenarios; and 2) route planners help the driving task significantly, especially for steering angle prediction.



### A Neuronal Planar Modeling for Handwriting Signature based on Automatic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1804.00527v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00527v1)
- **Published**: 2018-03-27 16:50:35+00:00
- **Updated**: 2018-03-27 16:50:35+00:00
- **Authors**: Imen Abroug Ben Abdelghani, Najwa Essoukri Ben Amara
- **Comment**: None
- **Journal**: None
- **Summary**: This paper deals with offline handwriting signature verification.We propose a planar neuronal model of signature image. Planarmodelsare generally based on delimiting homogenous zones ofimages; we propose in this paper an automatic segmentationapproach into bands of signature images. Signature image ismodeled by a planar neuronal model with horizontal secondarymodels and a verticalprincipal model. The proposed methodhas been tested on two databases. The first is the one we havecollected; it includes 6000 signaturescorresponding to 60writers. The second is the public GPDS-300 database including16200 signature corresponding to 300 persons. The achievedresults are promising.



### HDM-Net: Monocular Non-Rigid 3D Reconstruction with Learned Deformation Model
- **Arxiv ID**: http://arxiv.org/abs/1803.10193v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10193v2)
- **Published**: 2018-03-27 17:31:47+00:00
- **Updated**: 2019-08-05 18:56:15+00:00
- **Authors**: Vladislav Golyanik, Soshi Shimada, Kiran Varanasi, Didier Stricker
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: Monocular dense 3D reconstruction of deformable objects is a hard ill-posed problem in computer vision. Current techniques either require dense correspondences and rely on motion and deformation cues, or assume a highly accurate reconstruction (referred to as a template) of at least a single frame given in advance and operate in the manner of non-rigid tracking. Accurate computation of dense point tracks often requires multiple frames and might be computationally expensive. Availability of a template is a very strong prior which restricts system operation to a pre-defined environment and scenarios. In this work, we propose a new hybrid approach for monocular non-rigid reconstruction which we call Hybrid Deformation Model Network (HDM-Net). In our approach, deformation model is learned by a deep neural network, with a combination of domain-specific loss functions. We train the network with multiple states of a non-rigidly deforming structure with a known shape at rest. HDM-Net learns different reconstruction cues including texture-dependent surface deformations, shading and contours. We show generalisability of HDM-Net to states not presented in the training dataset, with unseen textures and under new illumination conditions. Experiments with noisy data and a comparison with other methods demonstrate robustness and accuracy of the proposed approach and suggest possible application scenarios of the new technique in interventional diagnostics and augmented reality.



### Multimodal Biometric Authentication Using Choquet Integral and Genetic Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1804.00528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00528v1)
- **Published**: 2018-03-27 18:03:22+00:00
- **Updated**: 2018-03-27 18:03:22+00:00
- **Authors**: Anouar Ben Khalifa, Sami Gazzah, Najoua Essoukri Ben Amara
- **Comment**: None
- **Journal**: None
- **Summary**: The Choquet integral is a tool for the information fusion that is very effective in the case where fuzzy measures associated with it are well chosen. In this paper,we propose a new approach for calculating fuzzy measures associated with the Choquet integral in a context of data fusion in multimodal biometrics. The proposed approach is based on genetic algorithms. It has been validated in two databases: the first base is relative to synthetic scores and the second one is biometrically relating to the face, fingerprintand palmprint. The results achieved attest the robustness of the proposed approach.



### Non-Linear Temporal Subspace Representations for Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.11064v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11064v1)
- **Published**: 2018-03-27 20:11:04+00:00
- **Updated**: 2018-03-27 20:11:04+00:00
- **Authors**: Anoop Cherian, Suvrit Sra, Stephen Gould, Richard Hartley
- **Comment**: Accepted at the IEEE International Conference on Computer Vision and
  Pattern Recognition, CVPR, 2018. arXiv admin note: substantial text overlap
  with arXiv:1705.08583
- **Journal**: None
- **Summary**: Representations that can compactly and effectively capture the temporal evolution of semantic content are important to computer vision and machine learning algorithms that operate on multi-variate time-series data. We investigate such representations motivated by the task of human action recognition. Here each data instance is encoded by a multivariate feature (such as via a deep CNN) where action dynamics are characterized by their variations in time. As these features are often non-linear, we propose a novel pooling method, kernelized rank pooling, that represents a given sequence compactly as the pre-image of the parameters of a hyperplane in a reproducing kernel Hilbert space, projections of data onto which captures their temporal order. We develop this idea further and show that such a pooling scheme can be cast as an order-constrained kernelized PCA objective. We then propose to use the parameters of a kernelized low-rank feature subspace as the representation of the sequences. We cast our formulation as an optimization problem on generalized Grassmann manifolds and then solve it efficiently using Riemannian optimization techniques. We present experiments on several action recognition datasets using diverse feature modalities and demonstrate state-of-the-art results.



### Adaptive Affinity Fields for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.10335v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10335v3)
- **Published**: 2018-03-27 21:21:57+00:00
- **Updated**: 2018-08-21 16:20:16+00:00
- **Authors**: Tsung-Wei Ke, Jyh-Jing Hwang, Ziwei Liu, Stella X. Yu
- **Comment**: To appear in European Conference on Computer Vision (ECCV) 2018
- **Journal**: None
- **Summary**: Semantic segmentation has made much progress with increasingly powerful pixel-wise classifiers and incorporating structural priors via Conditional Random Fields (CRF) or Generative Adversarial Networks (GAN). We propose a simpler alternative that learns to verify the spatial structure of segmentation during training only. Unlike existing approaches that enforce semantic labels on individual pixels and match labels between neighbouring pixels, we propose the concept of Adaptive Affinity Fields (AAF) to capture and match the semantic relations between neighbouring pixels in the label space. We use adversarial learning to select the optimal affinity field size for each semantic category. It is formulated as a minimax problem, optimizing our segmentation neural network in a best worst-case learning scenario. AAF is versatile for representing structures as a collection of pixel-centric relations, easier to train than GAN and more efficient than CRF without run-time inference. Our extensive evaluations on PASCAL VOC 2012, Cityscapes, and GTA5 datasets demonstrate its above-par segmentation performance and robust generalization across domains.



### Graph Convolutions on Spectral Embeddings: Learning of Cortical Surface Data
- **Arxiv ID**: http://arxiv.org/abs/1803.10336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10336v1)
- **Published**: 2018-03-27 21:25:12+00:00
- **Updated**: 2018-03-27 21:25:12+00:00
- **Authors**: Karthik Gopinath, Christian Desrosiers, Herve Lombaert
- **Comment**: 8 pages, Under review in MICCAI 2018
- **Journal**: None
- **Summary**: Neuronal cell bodies mostly reside in the cerebral cortex. The study of this thin and highly convoluted surface is essential for understanding how the brain works. The analysis of surface data is, however, challenging due to the high variability of the cortical geometry. This paper presents a novel approach for learning and exploiting surface data directly across surface domains. Current approaches rely on geometrical simplifications, such as spherical inflations, a popular but costly process. For instance, the widely used FreeSurfer takes about 3 hours to parcellate brain surfaces on a standard machine. Direct learning of surface data via graph convolutions would provide a new family of fast algorithms for processing brain surfaces. However, the current limitation of existing state-of-the-art approaches is their inability to compare surface data across different surface domains. Surface bases are indeed incompatible between brain geometries. This paper leverages recent advances in spectral graph matching to transfer surface data across aligned spectral domains. This novel approach enables a direct learning of surface data across compatible surface bases. It exploits spectral filters over intrinsic representations of surface neighborhoods. We illustrate the benefits of this approach with an application to brain parcellation. We validate the algorithm over 101 manually labeled brain surfaces. The results show a significant improvement in labeling accuracy over recent Euclidean approaches, while gaining a drastic speed improvement over conventional methods.



### Structural inpainting
- **Arxiv ID**: http://arxiv.org/abs/1803.10348v1
- **DOI**: 10.1145/3240508.3240678
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10348v1)
- **Published**: 2018-03-27 22:36:55+00:00
- **Updated**: 2018-03-27 22:36:55+00:00
- **Authors**: Huy V. Vo, Ngoc Q. K. Duong, Patrick Perez
- **Comment**: None
- **Journal**: None
- **Summary**: Scene-agnostic visual inpainting remains very challenging despite progress in patch-based methods. Recently, Pathak et al. 2016 have introduced convolutional "context encoders" (CEs) for unsupervised feature learning through image completion tasks. With the additional help of adversarial training, CEs turned out to be a promising tool to complete complex structures in real inpainting problems. In the present paper we propose to push further this key ability by relying on perceptual reconstruction losses at training time. We show on a wide variety of visual scenes the merit of the approach for structural inpainting, and confirm it through a user study. Combined with the optimization-based refinement of Yang et al. 2016 with neural patches, our context encoder opens up new opportunities for prior-free visual inpainting.



### ClickBAIT-v2: Training an Object Detector in Real-Time
- **Arxiv ID**: http://arxiv.org/abs/1803.10358v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.10358v1)
- **Published**: 2018-03-27 23:30:08+00:00
- **Updated**: 2018-03-27 23:30:08+00:00
- **Authors**: Ervin Teng, Rui Huang, Bob Iannucci
- **Comment**: 8 pages, 13 figures. For ClickBAIT-v1, see arXiv:1709.05021
- **Journal**: None
- **Summary**: Modern deep convolutional neural networks (CNNs) for image classification and object detection are often trained offline on large static datasets. Some applications, however, will require training in real-time on live video streams with a human-in-the-loop. We refer to this class of problem as time-ordered online training (ToOT). These problems will require a consideration of not only the quantity of incoming training data, but the human effort required to annotate and use it. We demonstrate and evaluate a system tailored to training an object detector on a live video stream with minimal input from a human operator. We show that we can obtain bounding box annotation from weakly-supervised single-point clicks through interactive segmentation. Furthermore, by exploiting the time-ordered nature of the video stream through object tracking, we can increase the average training benefit of human interactions by 3-4 times.



