# Arxiv Papers in cs.CV on 2018-03-17
### Constrained Deep Learning using Conditional Gradient and Applications in Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1803.06453v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.06453v1)
- **Published**: 2018-03-17 03:59:34+00:00
- **Updated**: 2018-03-17 03:59:34+00:00
- **Authors**: Sathya N. Ravi, Tuan Dinh, Vishnu Lokhande, Vikas Singh
- **Comment**: None
- **Journal**: None
- **Summary**: A number of results have recently demonstrated the benefits of incorporating various constraints when training deep architectures in vision and machine learning. The advantages range from guarantees for statistical generalization to better accuracy to compression. But support for general constraints within widely used libraries remains scarce and their broader deployment within many applications that can benefit from them remains under-explored. Part of the reason is that Stochastic gradient descent (SGD), the workhorse for training deep neural networks, does not natively deal with constraints with global scope very well. In this paper, we revisit a classical first order scheme from numerical optimization, Conditional Gradients (CG), that has, thus far had limited applicability in training deep models. We show via rigorous analysis how various constraints can be naturally handled by modifications of this algorithm. We provide convergence guarantees and show a suite of immediate benefits that are possible -- from training ResNets with fewer layers but better accuracy simply by substituting in our version of CG to faster training of GANs with 50% fewer epochs in image inpainting applications to provably better generalization guarantees using efficiently implementable forms of recently proposed regularizers.



### Learning to Cluster for Proposal-Free Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.06459v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.06459v1)
- **Published**: 2018-03-17 04:35:21+00:00
- **Updated**: 2018-03-17 04:35:21+00:00
- **Authors**: Yen-Chang Hsu, Zheng Xu, Zsolt Kira, Jiawei Huang
- **Comment**: None
- **Journal**: None
- **Summary**: This work proposed a novel learning objective to train a deep neural network to perform end-to-end image pixel clustering. We applied the approach to instance segmentation, which is at the intersection of image semantic segmentation and object detection. We utilize the most fundamental property of instance labeling -- the pairwise relationship between pixels -- as the supervision to formulate the learning objective, then apply it to train a fully convolutional network (FCN) for learning to perform pixel-wise clustering. The resulting clusters can be used as the instance labeling directly. To support labeling of an unlimited number of instance, we further formulate ideas from graph coloring theory into the proposed learning objective. The evaluation on the Cityscapes dataset demonstrates strong performance and therefore proof of the concept. Moreover, our approach won the second place in the lane detection competition of 2017 CVPR Autonomous Driving Challenge, and was the top performer without using external data.



### Queuing Theory Guided Intelligent Traffic Scheduling through Video Analysis using Dirichlet Process Mixture Model
- **Arxiv ID**: http://arxiv.org/abs/1803.06480v1
- **DOI**: 10.1016/j.eswa.2018.09.057
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1803.06480v1)
- **Published**: 2018-03-17 07:49:00+00:00
- **Updated**: 2018-03-17 07:49:00+00:00
- **Authors**: Santhosh Kelathodi Kumaran, Debi Prosad Dogra, Partha Pratim Roy
- **Comment**: None
- **Journal**: Expert Systems with Applications Volume 118, 15 March 2019, Pages
  169-181
- **Summary**: Accurate prediction of traffic signal duration for roadway junction is a challenging problem due to the dynamic nature of traffic flows. Though supervised learning can be used, parameters may vary across roadway junctions. In this paper, we present a computer vision guided expert system that can learn the departure rate of a given traffic junction modeled using traditional queuing theory. First, we temporally group the optical flow of the moving vehicles using Dirichlet Process Mixture Model (DPMM). These groups are referred to as tracklets or temporal clusters. Tracklet features are then used to learn the dynamic behavior of a traffic junction, especially during on/off cycles of a signal. The proposed queuing theory based approach can predict the signal open duration for the next cycle with higher accuracy when compared with other popular features used for tracking. The hypothesis has been verified on two publicly available video datasets. The results reveal that the DPMM based features are better than existing tracking frameworks to estimate $\mu$. Thus, signal duration prediction is more accurate when tested on these datasets.The method can be used for designing intelligent operator-independent traffic control systems for roadway junctions at cities and highways.



### Robust event-stream pattern tracking based on correlative filter
- **Arxiv ID**: http://arxiv.org/abs/1803.06490v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06490v1)
- **Published**: 2018-03-17 11:15:32+00:00
- **Updated**: 2018-03-17 11:15:32+00:00
- **Authors**: Hongmin Li, Luping Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Object tracking based on retina-inspired and event-based dynamic vision sensor (DVS) is challenging for the noise events, rapid change of event-stream shape, chaos of complex background textures, and occlusion. To address these challenges, this paper presents a robust event-stream pattern tracking method based on correlative filter mechanism. In the proposed method, rate coding is used to encode the event-stream object in each segment. Feature representations from hierarchical convolutional layers of a deep convolutional neural network (CNN) are used to represent the appearance of the rate encoded event-stream object. The results prove that our method not only achieves good tracking performance in many complicated scenes with noise events, complex background textures, occlusion, and intersected trajectories, but also is robust to variable scale, variable pose, and non-rigid deformations. In addition, this correlative filter based event-stream tracking has the advantage of high speed. The proposed approach will promote the potential applications of these event-based vision sensors in self-driving, robots and many other high-speed scenes.



### Evolving Deep Convolutional Neural Networks by Variable-length Particle Swarm Optimization for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1803.06492v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.06492v1)
- **Published**: 2018-03-17 11:47:43+00:00
- **Updated**: 2018-03-17 11:47:43+00:00
- **Authors**: Bin Wang, Yanan Sun, Bing Xue, Mengjie Zhang
- **Comment**: accepted by IEEE CEC 2018
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) are one of the most effective deep learning methods to solve image classification problems, but the best architecture of a CNN to solve a specific problem can be extremely complicated and hard to design. This paper focuses on utilising Particle Swarm Optimisation (PSO) to automatically search for the optimal architecture of CNNs without any manual work involved. In order to achieve the goal, three improvements are made based on traditional PSO. First, a novel encoding strategy inspired by computer networks which empowers particle vectors to easily encode CNN layers is proposed; Second, in order to allow the proposed method to learn variable-length CNN architectures, a Disabled layer is designed to hide some dimensions of the particle vector to achieve variable-length particles; Third, since the learning process on large data is slow, partial datasets are randomly picked for the evaluation to dramatically speed it up. The proposed algorithm is examined and compared with 12 existing algorithms including the state-of-art methods on three widely used image classification benchmark datasets. The experimental results show that the proposed algorithm is a strong competitor to the state-of-art algorithms in terms of classification error. This is the first work using PSO for automatically evolving the architectures of CNNs.



### Weakly Supervised Salient Object Detection Using Image Labels
- **Arxiv ID**: http://arxiv.org/abs/1803.06503v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06503v1)
- **Published**: 2018-03-17 13:40:00+00:00
- **Updated**: 2018-03-17 13:40:00+00:00
- **Authors**: Guanbin Li, Yuan Xie, Liang Lin
- **Comment**: Accept by AAAI2018
- **Journal**: None
- **Summary**: Deep learning based salient object detection has recently achieved great success with its performance greatly outperforms any other unsupervised methods. However, annotating per-pixel saliency masks is a tedious and inefficient procedure. In this paper, we note that superior salient object detection can be obtained by iteratively mining and correcting the labeling ambiguity on saliency maps from traditional unsupervised methods. We propose to use the combination of a coarse salient object activation map from the classification network and saliency maps generated from unsupervised methods as pixel-level annotation, and develop a simple yet very effective algorithm to train fully convolutional networks for salient object detection supervised by these noisy annotations. Our algorithm is based on alternately exploiting a graphical model and training a fully convolutional network for model updating. The graphical model corrects the internal labeling ambiguity through spatial consistency and structure preserving while the fully convolutional network helps to correct the cross-image semantic ambiguity and simultaneously update the coarse activation map for next iteration. Experimental results demonstrate that our proposed method greatly outperforms all state-of-the-art unsupervised saliency detection methods and can be comparable to the current best strongly-supervised methods training with thousands of pixel-level saliency map annotations on all public benchmarks.



### Learning Unsupervised Visual Grounding Through Semantic Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/1803.06506v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06506v3)
- **Published**: 2018-03-17 13:46:59+00:00
- **Updated**: 2018-11-16 21:25:43+00:00
- **Authors**: Syed Ashar Javed, Shreyas Saxena, Vineet Gandhi
- **Comment**: NIPS Workshop 2018
- **Journal**: None
- **Summary**: Localizing natural language phrases in images is a challenging problem that requires joint understanding of both the textual and visual modalities. In the unsupervised setting, lack of supervisory signals exacerbate this difficulty. In this paper, we propose a novel framework for unsupervised visual grounding which uses concept learning as a proxy task to obtain self-supervision. The simple intuition behind this idea is to encourage the model to localize to regions which can explain some semantic property in the data, in our case, the property being the presence of a concept in a set of images. We present thorough quantitative and qualitative experiments to demonstrate the efficacy of our approach and show a 5.6% improvement over the current state of the art on Visual Genome dataset, a 5.8% improvement on the ReferItGame dataset and comparable to state-of-art performance on the Flickr30k dataset.



### MergeNet: A Deep Net Architecture for Small Obstacle Discovery
- **Arxiv ID**: http://arxiv.org/abs/1803.06508v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.06508v1)
- **Published**: 2018-03-17 14:07:34+00:00
- **Updated**: 2018-03-17 14:07:34+00:00
- **Authors**: Krishnam Gupta, Syed Ashar Javed, Vineet Gandhi, K. Madhava Krishna
- **Comment**: None
- **Journal**: None
- **Summary**: We present here, a novel network architecture called MergeNet for discovering small obstacles for on-road scenes in the context of autonomous driving. The basis of the architecture rests on the central consideration of training with less amount of data since the physical setup and the annotation process for small obstacles is hard to scale. For making effective use of the limited data, we propose a multi-stage training procedure involving weight-sharing, separate learning of low and high level features from the RGBD input and a refining stage which learns to fuse the obtained complementary features. The model is trained and evaluated on the Lost and Found dataset and is able to achieve state-of-art results with just 135 images in comparison to the 1000 images used by the previous benchmark. Additionally, we also compare our results with recent methods trained on 6000 images and show that our method achieves comparable performance with only 1000 training samples.



### SeqFace: Make full use of sequence information for face recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.06524v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06524v2)
- **Published**: 2018-03-17 15:49:07+00:00
- **Updated**: 2018-03-24 02:55:59+00:00
- **Authors**: Wei Hu, Yangyu Huang, Fan Zhang, Ruirui Li, Wei Li, Guodong Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have greatly improved the Face Recognition (FR) performance in recent years. Almost all CNNs in FR are trained on the carefully labeled datasets containing plenty of identities. However, such high-quality datasets are very expensive to collect, which restricts many researchers to achieve state-of-the-art performance. In this paper, we propose a framework, called SeqFace, for learning discriminative face features. Besides a traditional identity training dataset, the designed SeqFace can train CNNs by using an additional dataset which includes a large number of face sequences collected from videos. Moreover, the label smoothing regularization (LSR) and a new proposed discriminative sequence agent (DSA) loss are employed to enhance discrimination power of deep face features via making full use of the sequence data. Our method achieves excellent performance on Labeled Faces in the Wild (LFW), YouTube Faces (YTF), only with a single ResNet. The code and models are publicly available on-line (https://github.com/huangyangyu/SeqFace).



### Adaptive strategy for superpixel-based region-growing image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.06541v1
- **DOI**: 10.1117/1.JEI.26.6.061605
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06541v1)
- **Published**: 2018-03-17 17:13:09+00:00
- **Updated**: 2018-03-17 17:13:09+00:00
- **Authors**: Mahaman Sani Chaibou, Pierre-Henri Conze, Karim Kalti, Basel Solaiman, Mohamed Ali Mahjoub
- **Comment**: None
- **Journal**: J. of Electronic Imaging, 26(6), 061605 (2017)
- **Summary**: This work presents a region-growing image segmentation approach based on superpixel decomposition. From an initial contour-constrained over-segmentation of the input image, the image segmentation is achieved by iteratively merging similar superpixels into regions. This approach raises two key issues: (1) how to compute the similarity between superpixels in order to perform accurate merging and (2) in which order those superpixels must be merged together. In this perspective, we firstly introduce a robust adaptive multi-scale superpixel similarity in which region comparisons are made both at content and common border level. Secondly, we propose a global merging strategy to efficiently guide the region merging process. Such strategy uses an adpative merging criterion to ensure that best region aggregations are given highest priorities. This allows to reach a final segmentation into consistent regions with strong boundary adherence. We perform experiments on the BSDS500 image dataset to highlight to which extent our method compares favorably against other well-known image segmentation algorithms. The obtained results demonstrate the promising potential of the proposed approach.



### Convolutional Point-set Representation: A Convolutional Bridge Between a Densely Annotated Image and 3D Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/1803.06542v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1803.06542v2)
- **Published**: 2018-03-17 17:20:25+00:00
- **Updated**: 2018-04-02 21:49:54+00:00
- **Authors**: Yuhang Wu, Le Anh Vu Ha, Xiang Xu, Ioannis A. Kakadiaris
- **Comment**: Preprint Submitted
- **Journal**: None
- **Summary**: We present a robust method for estimating the facial pose and shape information from a densely annotated facial image. The method relies on Convolutional Point-set Representation (CPR), a carefully designed matrix representation to summarize different layers of information encoded in the set of detected points in the annotated image. The CPR disentangles the dependencies of shape and different pose parameters and enables updating different parameters in a sequential manner via convolutional neural networks and recurrent layers. When updating the pose parameters, we sample reprojection errors along with a predicted direction and update the parameters based on the pattern of reprojection errors. This technique boosts the model's capability in searching a local minimum under challenging scenarios. We also demonstrate that annotation from different sources can be merged under the framework of CPR and contributes to outperforming the current state-of-the-art solutions for 3D face alignment. Experiments indicate the proposed CPRFA (CPR-based Face Alignment) significantly improves 3D alignment accuracy when the densely annotated image contains noise and missing values, which is common under "in-the-wild" acquisition scenarios.



### Fusion of an Ensemble of Augmented Image Detectors for Robust Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1803.06554v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1803.06554v1)
- **Published**: 2018-03-17 19:16:26+00:00
- **Updated**: 2018-03-17 19:16:26+00:00
- **Authors**: Pan Wei, John E. Ball, Derek T. Anderson
- **Comment**: 21 pages, 12 figures, journal paper, MDPI Sensors, 2018
- **Journal**: None
- **Summary**: A significant challenge in object detection is accurate identification of an object's position in image space, whereas one algorithm with one set of parameters is usually not enough, and the fusion of multiple algorithms and/or parameters can lead to more robust results. Herein, a new computational intelligence fusion approach based on the dynamic analysis of agreement among object detection outputs is proposed. Furthermore, we propose an online versus just in training image augmentation strategy. Experiments comparing the results both with and without fusion are presented. We demonstrate that the augmented and fused combination results are the best, with respect to higher accuracy rates and reduction of outlier influences. The approach is demonstrated in the context of cone, pedestrian and box detection for Advanced Driver Assistance Systems (ADAS) applications.



### A Multi-perspective Approach To Anomaly Detection For Self-aware Embodied Agents
- **Arxiv ID**: http://arxiv.org/abs/1803.06579v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06579v1)
- **Published**: 2018-03-17 22:00:08+00:00
- **Updated**: 2018-03-17 22:00:08+00:00
- **Authors**: Mohamad Baydoun, Mahdyar Ravanbakhsh, Damian Campo, Pablo Marin, David Martin, Lucio Marcenaro, Andrea Cavallaro, Carlo S. Regazzoni
- **Comment**: IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP) 2018
- **Journal**: None
- **Summary**: This paper focuses on multi-sensor anomaly detection for moving cognitive agents using both external and private first-person visual observations. Both observation types are used to characterize agents' motion in a given environment. The proposed method generates locally uniform motion models by dividing a Gaussian process that approximates agents' displacements on the scene and provides a Shared Level (SL) self-awareness based on Environment Centered (EC) models. Such models are then used to train in a semi-unsupervised way a set of Generative Adversarial Networks (GANs) that produce an estimation of external and internal parameters of moving agents. Obtained results exemplify the feasibility of using multi-perspective data for predicting and analyzing trajectory information.



### Depth Pooling Based Large-scale 3D Action Recognition with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1804.01194v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.01194v2)
- **Published**: 2018-03-17 23:30:34+00:00
- **Updated**: 2018-04-17 23:46:47+00:00
- **Authors**: Pichao Wang, Wanqing Li, Zhimin Gao, Chang Tang, Philip Ogunbona
- **Comment**: arXiv admin note: text overlap with arXiv:1701.01814,
  arXiv:1608.06338
- **Journal**: None
- **Summary**: This paper proposes three simple, compact yet effective representations of depth sequences, referred to respectively as Dynamic Depth Images (DDI), Dynamic Depth Normal Images (DDNI) and Dynamic Depth Motion Normal Images (DDMNI), for both isolated and continuous action recognition. These dynamic images are constructed from a segmented sequence of depth maps using hierarchical bidirectional rank pooling to effectively capture the spatial-temporal information. Specifically, DDI exploits the dynamics of postures over time and DDNI and DDMNI exploit the 3D structural information captured by depth maps. Upon the proposed representations, a ConvNet based method is developed for action recognition. The image-based representations enable us to fine-tune the existing Convolutional Neural Network (ConvNet) models trained on image data without training a large number of parameters from scratch. The proposed method achieved the state-of-art results on three large datasets, namely, the Large-scale Continuous Gesture Recognition Dataset (means Jaccard index 0.4109), the Large-scale Isolated Gesture Recognition Dataset (59.21%), and the NTU RGB+D Dataset (87.08% cross-subject and 84.22% cross-view) even though only the depth modality was used.



