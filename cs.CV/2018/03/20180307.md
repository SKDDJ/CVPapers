# Arxiv Papers in cs.CV on 2018-03-07
### Exponential Discriminative Metric Embedding in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1803.02504v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.02504v1)
- **Published**: 2018-03-07 02:39:34+00:00
- **Updated**: 2018-03-07 02:39:34+00:00
- **Authors**: Bowen Wu, Zhangling Chen, Jun Wang, Huaming Wu
- **Comment**: None
- **Journal**: None
- **Summary**: With the remarkable success achieved by the Convolutional Neural Networks (CNNs) in object recognition recently, deep learning is being widely used in the computer vision community. Deep Metric Learning (DML), integrating deep learning with conventional metric learning, has set new records in many fields, especially in classification task. In this paper, we propose a replicable DML method, called Include and Exclude (IE) loss, to force the distance between a sample and its designated class center away from the mean distance of this sample to other class centers with a large margin in the exponential feature projection space. With the supervision of IE loss, we can train CNNs to enhance the intra-class compactness and inter-class separability, leading to great improvements on several public datasets ranging from object recognition to face verification. We conduct a comparative study of our algorithm with several typical DML methods on three kinds of networks with different capacity. Extensive experiments on three object recognition datasets and two face recognition datasets demonstrate that IE loss is always superior to other mainstream DML methods and approach the state-of-the-art results.



### Rigid Point Registration with Expectation Conditional Maximization
- **Arxiv ID**: http://arxiv.org/abs/1803.02518v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02518v1)
- **Published**: 2018-03-07 03:59:05+00:00
- **Updated**: 2018-03-07 03:59:05+00:00
- **Authors**: Jing Wu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the issue of matching rigid 3D object points with 2D image points through point registration based on maximum likelihood principle in computer simulated images. Perspective projection is necessary when transforming 3D coordinate into 2D. The problem then recasts into a missing data framework where unknown correspondences are handled via mixture models. Adopting the Expectation Conditional Maximization for Point Registration (ECMPR), two different rotation and translation optimization algorithms are compared in this paper. We analyze in detail the associated consequences in terms of estimation of the registration parameters theoretically and experimentally.



### Sparse Adversarial Perturbations for Videos
- **Arxiv ID**: http://arxiv.org/abs/1803.02536v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02536v1)
- **Published**: 2018-03-07 06:28:43+00:00
- **Updated**: 2018-03-07 06:28:43+00:00
- **Authors**: Xingxing Wei, Jun Zhu, Hang Su
- **Comment**: None
- **Journal**: None
- **Summary**: Although adversarial samples of deep neural networks (DNNs) have been intensively studied on static images, their extensions in videos are never explored. Compared with images, attacking a video needs to consider not only spatial cues but also temporal cues. Moreover, to improve the imperceptibility as well as reduce the computation cost, perturbations should be added on as fewer frames as possible, i.e., adversarial perturbations are temporally sparse. This further motivates the propagation of perturbations, which denotes that perturbations added on the current frame can transfer to the next frames via their temporal interactions. Thus, no (or few) extra perturbations are needed for these frames to misclassify them. To this end, we propose an l2,1-norm based optimization algorithm to compute the sparse adversarial perturbations for videos. We choose the action recognition as the targeted task, and networks with a CNN+RNN architecture as threat models to verify our method. Thanks to the propagation, we can compute perturbations on a shortened version video, and then adapt them to the long version video to fool DNNs. Experimental results on the UCF101 dataset demonstrate that even only one frame in a video is perturbed, the fooling rate can still reach 59.7%.



### Visual Explanations From Deep 3D Convolutional Neural Networks for Alzheimer's Disease Classification
- **Arxiv ID**: http://arxiv.org/abs/1803.02544v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.02544v3)
- **Published**: 2018-03-07 07:07:39+00:00
- **Updated**: 2018-07-06 00:28:49+00:00
- **Authors**: Chengliang Yang, Anand Rangarajan, Sanjay Ranka
- **Comment**: Accepted by 2018 American Medical Informatics Association Annual
  Symposium (AMIA2018)
- **Journal**: None
- **Summary**: We develop three efficient approaches for generating visual explanations from 3D convolutional neural networks (3D-CNNs) for Alzheimer's disease classification. One approach conducts sensitivity analysis on hierarchical 3D image segmentation, and the other two visualize network activations on a spatial map. Visual checks and a quantitative localization benchmark indicate that all approaches identify important brain parts for Alzheimer's disease diagnosis. Comparative analysis show that the sensitivity analysis based approach has difficulty handling loosely distributed cerebral cortex, and approaches based on visualization of activations are constrained by the resolution of the convolutional layer. The complementarity of these methods improves the understanding of 3D-CNNs in Alzheimer's disease classification from different perspectives.



### Pyramid Person Matching Network for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1803.02547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02547v1)
- **Published**: 2018-03-07 07:21:44+00:00
- **Updated**: 2018-03-07 07:21:44+00:00
- **Authors**: Chaojie Mao, Yingming Li, Zhongfei Zhang, Yaqing Zhang, Xi Li
- **Comment**: 11pages, 3 figures, 4 tables and accepted by Proceedings of 9th Asian
  Conference on Machine Learning (ACML2017) JMLR Workshop and Conference
  Proceedings, vol. 77, 2017
- **Journal**: None
- **Summary**: In this work, we present a deep convolutional pyramid person matching network (PPMN) with specially designed Pyramid Matching Module to address the problem of person re-identification. The architecture takes a pair of RGB images as input, and outputs a similiarity value indicating whether the two input images represent the same person or not. Based on deep convolutional neural networks, our approach first learns the discriminative semantic representation with the semantic-component-aware features for persons and then employs the Pyramid Matching Module to match the common semantic-components of persons, which is robust to the variation of spatial scales and misalignment of locations posed by viewpoint changes. The above two processes are jointly optimized via a unified end-to-end deep learning scheme. Extensive experiments on several benchmark datasets demonstrate the effectiveness of our approach against the state-of-the-art approaches, especially on the rank-1 recognition rate.



### Object cosegmentation using deep Siamese network
- **Arxiv ID**: http://arxiv.org/abs/1803.02555v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02555v2)
- **Published**: 2018-03-07 07:49:29+00:00
- **Updated**: 2018-03-08 08:35:30+00:00
- **Authors**: Prerana Mukherjee, Brejesh Lall, Snehith Lattupally
- **Comment**: Appears in International Conference on Pattern Recognition and
  Artificial Intelligence (ICPRAI), 2018
- **Journal**: None
- **Summary**: Object cosegmentation addresses the problem of discovering similar objects from multiple images and segmenting them as foreground simultaneously. In this paper, we propose a novel end-to-end pipeline to segment the similar objects simultaneously from relevant set of images using supervised learning via deep-learning framework. We experiment with multiple set of object proposal generation techniques and perform extensive numerical evaluations by training the Siamese network with generated object proposals. Similar objects proposals for the test images are retrieved using the ANNOY (Approximate Nearest Neighbor) library and deep semantic segmentation is performed on them. Finally, we form a collage from the segmented similar objects based on the relative importance of the objects.



### Multi-Channel Pyramid Person Matching Network for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1803.02558v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02558v1)
- **Published**: 2018-03-07 08:10:26+00:00
- **Updated**: 2018-03-07 08:10:26+00:00
- **Authors**: Chaojie Mao, Yingming Li, Yaqing Zhang, Zhongfei Zhang, Xi Li
- **Comment**: 9 pages, 5 figures, 7 tables and accepted by the 32nd AAAI Conference
  on Artificial Intelligence
- **Journal**: None
- **Summary**: In this work, we present a Multi-Channel deep convolutional Pyramid Person Matching Network (MC-PPMN) based on the combination of the semantic-components and the color-texture distributions to address the problem of person re-identification. In particular, we learn separate deep representations for semantic-components and color-texture distributions from two person images and then employ pyramid person matching network (PPMN) to obtain correspondence representations. These correspondence representations are fused to perform the re-identification task. Further, the proposed framework is optimized via a unified end-to-end deep learning scheme. Extensive experiments on several benchmark datasets demonstrate the effectiveness of our approach against the state-of-the-art literature, especially on the rank-1 recognition rate.



### Decoupled Spatial Neural Attention for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.02563v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02563v1)
- **Published**: 2018-03-07 08:59:35+00:00
- **Updated**: 2018-03-07 08:59:35+00:00
- **Authors**: Tianyi Zhang, Guosheng Lin, Jianfei Cai, Tong Shen, Chunhua Shen, Alex C. Kot
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly supervised semantic segmentation receives much research attention since it alleviates the need to obtain a large amount of dense pixel-wise ground-truth annotations for the training images. Compared with other forms of weak supervision, image labels are quite efficient to obtain. In our work, we focus on the weakly supervised semantic segmentation with image label annotations. Recent progress for this task has been largely dependent on the quality of generated pseudo-annotations. In this work, inspired by spatial neural-attention for image captioning, we propose a decoupled spatial neural attention network for generating pseudo-annotations. Our decoupled attention structure could simultaneously identify the object regions and localize the discriminative parts which generates high-quality pseudo-annotations in one forward path. The generated pseudo-annotations lead to the segmentation results which achieve the state-of-the-art in weakly-supervised semantic segmentation.



### Generating Goal-Directed Visuomotor Plans Based on Learning Using a Predictive Coding-type Deep Visuomotor Recurrent Neural Network Model
- **Arxiv ID**: http://arxiv.org/abs/1803.02578v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02578v2)
- **Published**: 2018-03-07 10:21:08+00:00
- **Updated**: 2018-06-05 06:24:42+00:00
- **Authors**: Minkyu Choi, Takazumi Matsumoto, Minju Jung, Jun Tani
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: The current paper presents how a predictive coding type deep recurrent neural networks can generate vision-based goal-directed plans based on prior learning experience by examining experiment results using a real arm robot. The proposed deep recurrent neural network learns to predict visuo-proprioceptive sequences by extracting an adequate predictive model from various visuomotor experiences related to object-directed behaviors. The predictive model was developed in terms of mapping from intention state space to expected visuo-proprioceptive sequences space through iterative learning. Our arm robot experiments adopted with three different tasks with different levels of difficulty showed that the error minimization principle in the predictive coding framework applied to inference of the optimal intention states for given goal states can generate goal-directed plans even for unlearned goal states with generalization. It was, however, shown that sufficient generalization requires relatively large number of learning trajectories. The paper discusses possible countermeasure to overcome this problem.



### Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.02579v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02579v2)
- **Published**: 2018-03-07 10:22:06+00:00
- **Updated**: 2018-06-08 15:46:44+00:00
- **Authors**: Abhijit Guha Roy, Nassir Navab, Christian Wachinger
- **Comment**: Accepted at MICCAI 2018
- **Journal**: None
- **Summary**: Fully convolutional neural networks (F-CNNs) have set the state-of-the-art in image segmentation for a plethora of applications. Architectural innovations within F-CNNs have mainly focused on improving spatial encoding or network connectivity to aid gradient flow. In this paper, we explore an alternate direction of recalibrating the feature maps adaptively, to boost meaningful features, while suppressing weak ones. We draw inspiration from the recently proposed squeeze & excitation (SE) module for channel recalibration of feature maps for image classification. Towards this end, we introduce three variants of SE modules for image segmentation, (i) squeezing spatially and exciting channel-wise (cSE), (ii) squeezing channel-wise and exciting spatially (sSE) and (iii) concurrent spatial and channel squeeze & excitation (scSE). We effectively incorporate these SE modules within three different state-of-the-art F-CNNs (DenseNet, SD-Net, U-Net) and observe consistent improvement of performance across all architectures, while minimally effecting model complexity. Evaluations are performed on two challenging applications: whole brain segmentation on MRI scans (Multi-Atlas Labelling Challenge Dataset) and organ segmentation on whole body contrast enhanced CT scans (Visceral Dataset).



### Single View Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/1803.02612v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02612v2)
- **Published**: 2018-03-07 12:07:23+00:00
- **Updated**: 2018-03-09 07:47:18+00:00
- **Authors**: Yue Luo, Jimmy Ren, Mude Lin, Jiahao Pang, Wenxiu Sun, Hongsheng Li, Liang Lin
- **Comment**: Spotlight in IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2018
- **Journal**: None
- **Summary**: Previous monocular depth estimation methods take a single view and directly regress the expected results. Though recent advances are made by applying geometrically inspired loss functions during training, the inference procedure does not explicitly impose any geometrical constraint. Therefore these models purely rely on the quality of data and the effectiveness of learning to generalize. This either leads to suboptimal results or the demand of huge amount of expensive ground truth labelled data to generate reasonable results. In this paper, we show for the first time that the monocular depth estimation problem can be reformulated as two sub-problems, a view synthesis procedure followed by stereo matching, with two intriguing properties, namely i) geometrical constraints can be explicitly imposed during inference; ii) demand on labelled depth data can be greatly alleviated. We show that the whole pipeline can still be trained in an end-to-end fashion and this new formulation plays a critical role in advancing the performance. The resulting model outperforms all the previous monocular depth estimation methods as well as the stereo block matching method in the challenging KITTI dataset by only using a small number of real training data. The model also generalizes well to other monocular depth estimation benchmarks. We also discuss the implications and the advantages of solving monocular depth estimation using stereo methods.



### 3D Human Pose Estimation in RGBD Images for Robotic Task Learning
- **Arxiv ID**: http://arxiv.org/abs/1803.02622v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.02622v2)
- **Published**: 2018-03-07 12:46:18+00:00
- **Updated**: 2018-03-13 10:18:18+00:00
- **Authors**: Christian Zimmermann, Tim Welschehold, Christian Dornhege, Wolfram Burgard, Thomas Brox
- **Comment**: Accepted to ICRA 2018. Video and Code (ROS node) are available:
  http://lmb.informatik.uni-freiburg.de/projects/rgbd-pose3d/
- **Journal**: None
- **Summary**: We propose an approach to estimate 3D human pose in real world units from a single RGBD image and show that it exceeds performance of monocular 3D pose estimation approaches from color as well as pose estimation exclusively from depth. Our approach builds on robust human keypoint detectors for color images and incorporates depth for lifting into 3D. We combine the system with our learning from demonstration framework to instruct a service robot without the need of markers. Experiments in real world settings demonstrate that our approach enables a PR2 robot to imitate manipulation actions observed from a human teacher.



### TRLG: Fragile blind quad watermarking for image tamper detection and recovery by providing compact digests with quality optimized using LWT and GA
- **Arxiv ID**: http://arxiv.org/abs/1803.02623v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1803.02623v1)
- **Published**: 2018-03-07 12:47:18+00:00
- **Updated**: 2018-03-07 12:47:18+00:00
- **Authors**: Behrouz Bolourian Haghighi, Amir Hossein Taherinia, Amir Hossein Mohajerzadeh
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, an efficient fragile blind quad watermarking scheme for image tamper detection and recovery based on lifting wavelet transform and genetic algorithm is proposed. TRLG generates four compact digests with super quality based on lifting wavelet transform and halftoning technique by distinguishing the types of image blocks. In other words, for each 2*2 non-overlap blocks, four chances for recovering destroyed blocks are considered. A special parameter estimation technique based on genetic algorithm is performed to improve and optimize the quality of digests and watermarked image. Furthermore, CCS map is used to determine the mapping block for embedding information, encrypting and confusing the embedded information. In order to improve the recovery rate, Mirror-aside and Partner-block are proposed. The experiments that have been conducted to evaluate the performance of TRLG proved the superiority in terms of quality of the watermarked and recovered image, tamper localization and security compared with state-of-the-art methods. The results indicate that the PSNR and SSIM of the watermarked image are about 46 dB and approximately one, respectively. Also, the mean of PSNR and SSIM of several recovered images which has been destroyed about 90% is reached to 24 dB and 0.86, respectively.



### Inferencing Based on Unsupervised Learning of Disentangled Representations
- **Arxiv ID**: http://arxiv.org/abs/1803.02627v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1803.02627v1)
- **Published**: 2018-03-07 12:58:53+00:00
- **Updated**: 2018-03-07 12:58:53+00:00
- **Authors**: Tobias Hinz, Stefan Wermter
- **Comment**: Accepted as a conference paper at the European Symposium on
  Artificial Neural Networks, Computational Intelligence and Machine Learning
  (ESANN) 2018, 6 pages
- **Journal**: None
- **Summary**: Combining Generative Adversarial Networks (GANs) with encoders that learn to encode data points has shown promising results in learning data representations in an unsupervised way. We propose a framework that combines an encoder and a generator to learn disentangled representations which encode meaningful information about the data distribution without the need for any labels. While current approaches focus mostly on the generative aspects of GANs, our framework can be used to perform inference on both real and generated data points. Experiments on several data sets show that the encoder learns interpretable, disentangled representations which encode descriptive properties and can be used to sample images that exhibit specific characteristics.



### Learning Spectral-Spatial-Temporal Features via a Recurrent Convolutional Neural Network for Change Detection in Multispectral Imagery
- **Arxiv ID**: http://arxiv.org/abs/1803.02642v1
- **DOI**: 10.1109/TGRS.2018.2863224
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02642v1)
- **Published**: 2018-03-07 13:30:59+00:00
- **Updated**: 2018-03-07 13:30:59+00:00
- **Authors**: Lichao Mou, Lorenzo Bruzzone, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Change detection is one of the central problems in earth observation and was extensively investigated over recent decades. In this paper, we propose a novel recurrent convolutional neural network (ReCNN) architecture, which is trained to learn a joint spectral-spatial-temporal feature representation in a unified framework for change detection in multispectral images. To this end, we bring together a convolutional neural network (CNN) and a recurrent neural network (RNN) into one end-to-end network. The former is able to generate rich spectral-spatial feature representations, while the latter effectively analyzes temporal dependency in bi-temporal images. In comparison with previous approaches to change detection, the proposed network architecture possesses three distinctive properties: 1) It is end-to-end trainable, in contrast to most existing methods whose components are separately trained or computed; 2) it naturally harnesses spatial information that has been proven to be beneficial to change detection task; 3) it is capable of adaptively learning the temporal dependency between multitemporal images, unlike most of algorithms that use fairly simple operation like image differencing or stacking. As far as we know, this is the first time that a recurrent convolutional network architecture has been proposed for multitemporal remote sensing image analysis. The proposed network is validated on real multispectral data sets. Both visual and quantitative analysis of experimental results demonstrates competitive performance in the proposed mode.



### Deep Back-Projection Networks For Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1803.02735v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02735v1)
- **Published**: 2018-03-07 16:05:35+00:00
- **Updated**: 2018-03-07 16:05:35+00:00
- **Authors**: Muhammad Haris, Greg Shakhnarovich, Norimichi Ukita
- **Comment**: To appear in CVPR2018
- **Journal**: None
- **Summary**: The feed-forward architectures of recently proposed deep super-resolution networks learn representations of low-resolution inputs, and the non-linear mapping from those to high-resolution output. However, this approach does not fully address the mutual dependencies of low- and high-resolution images. We propose Deep Back-Projection Networks (DBPN), that exploit iterative up- and down-sampling layers, providing an error feedback mechanism for projection errors at each stage. We construct mutually-connected up- and down-sampling stages each of which represents different types of image degradation and high-resolution components. We show that extending this idea to allow concatenation of features across up- and down-sampling stages (Dense DBPN) allows us to reconstruct further improve super-resolution, yielding superior results and in particular establishing new state of the art results for large scaling factors such as 8x across multiple data sets.



### HENet:A Highly Efficient Convolutional Neural Networks Optimized for Accuracy, Speed and Storage
- **Arxiv ID**: http://arxiv.org/abs/1803.02742v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02742v2)
- **Published**: 2018-03-07 16:18:51+00:00
- **Updated**: 2018-03-15 15:18:24+00:00
- **Authors**: Qiuyu Zhu, Ruixin Zhang
- **Comment**: 11 pages,3 figures
- **Journal**: None
- **Summary**: In order to enhance the real-time performance of convolutional neural networks(CNNs), more and more researchers are focusing on improving the efficiency of CNN. Based on the analysis of some CNN architectures, such as ResNet, DenseNet, ShuffleNet and so on, we combined their advantages and proposed a very efficient model called Highly Efficient Networks(HENet). The new architecture uses an unusual way to combine group convolution and channel shuffle which was mentioned in ShuffleNet. Inspired by ResNet and DenseNet, we also proposed a new way to use element-wise addition and concatenation connection with each block. In order to make greater use of feature maps, pooling operations are removed from HENet. The experiments show that our model's efficiency is more than 1 times higher than ShuffleNet on many open source datasets, such as CIFAR-10/100 and SVHN.



### RTSeg: Real-time Semantic Segmentation Comparative Study
- **Arxiv ID**: http://arxiv.org/abs/1803.02758v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02758v5)
- **Published**: 2018-03-07 16:49:48+00:00
- **Updated**: 2020-05-16 15:11:54+00:00
- **Authors**: Mennatullah Siam, Mostafa Gamal, Moemen Abdel-Razek, Senthil Yogamani, Martin Jagersand
- **Comment**: Accepted in IEEE ICIP 2018. IEEE Copyrights: Personal use of this
  material is permitted. Permission from IEEE must be obtained for all other
  uses
- **Journal**: None
- **Summary**: Semantic segmentation benefits robotics related applications especially autonomous driving. Most of the research on semantic segmentation is only on increasing the accuracy of segmentation models with little attention to computationally efficient solutions. The few work conducted in this direction does not provide principled methods to evaluate the different design choices for segmentation. In this paper, we address this gap by presenting a real-time semantic segmentation benchmarking framework with a decoupled design for feature extraction and decoding methods. The framework is comprised of different network architectures for feature extraction such as VGG16, Resnet18, MobileNet, and ShuffleNet. It is also comprised of multiple meta-architectures for segmentation that define the decoding methodology. These include SkipNet, UNet, and Dilation Frontend. Experimental results are presented on the Cityscapes dataset for urban scenes. The modular design allows novel architectures to emerge, that lead to 143x GFLOPs reduction in comparison to SegNet. This benchmarking framework is publicly available at "https://github.com/MSiam/TFSegmentation".



### Fast and Accurate Semantic Mapping through Geometric-based Incremental Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.02784v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.02784v1)
- **Published**: 2018-03-07 17:36:34+00:00
- **Updated**: 2018-03-07 17:36:34+00:00
- **Authors**: Yoshikatsu Nakajima, Keisuke Tateno, Federico Tombari, Hideo Saito
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an efficient and scalable method for incrementally building a dense, semantically annotated 3D map in real-time. The proposed method assigns class probabilities to each region, not each element (e.g., surfel and voxel), of the 3D map which is built up through a robust SLAM framework and incrementally segmented with a geometric-based segmentation method. Differently from all other approaches, our method has a capability of running at over 30Hz while performing all processing components, including SLAM, segmentation, 2D recognition, and updating class probabilities of each segmentation label at every incoming frame, thanks to the high efficiency that characterizes the computationally intensive stages of our framework. By utilizing a specifically designed CNN to improve the frame-wise segmentation result, we can also achieve high accuracy. We validate our method on the NYUv2 dataset by comparing with the state of the art in terms of accuracy and computational efficiency, and by means of an analysis in terms of time and space complexity.



### A Deep Learning Algorithm for One-step Contour Aware Nuclei Segmentation of Histopathological Images
- **Arxiv ID**: http://arxiv.org/abs/1803.02786v1
- **DOI**: 10.1007/s11517-019-02008-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02786v1)
- **Published**: 2018-03-07 17:53:20+00:00
- **Updated**: 2018-03-07 17:53:20+00:00
- **Authors**: Yuxin Cui, Guiying Zhang, Zhonghao Liu, Zheng Xiong, Jianjun Hu
- **Comment**: 13 pages. 12 figures
- **Journal**: Med Biol Eng Comput 2019
- **Summary**: This paper addresses the task of nuclei segmentation in high-resolution histopathological images. We propose an auto- matic end-to-end deep neural network algorithm for segmenta- tion of individual nuclei. A nucleus-boundary model is introduced to predict nuclei and their boundaries simultaneously using a fully convolutional neural network. Given a color normalized image, the model directly outputs an estimated nuclei map and a boundary map. A simple, fast and parameter-free post-processing procedure is performed on the estimated nuclei map to produce the final segmented nuclei. An overlapped patch extraction and assembling method is also designed for seamless prediction of nuclei in large whole-slide images. We also show the effectiveness of data augmentation methods for nuclei segmentation task. Our experiments showed our method outperforms prior state-of-the- art methods. Moreover, it is efficient that one 1000X1000 image can be segmented in less than 5 seconds. This makes it possible to precisely segment the whole-slide image in acceptable time



