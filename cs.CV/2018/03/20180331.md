# Arxiv Papers in cs.CV on 2018-03-31
### FloorNet: A Unified Framework for Floorplan Reconstruction from 3D Scans
- **Arxiv ID**: http://arxiv.org/abs/1804.00090v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00090v1)
- **Published**: 2018-03-31 00:22:27+00:00
- **Updated**: 2018-03-31 00:22:27+00:00
- **Authors**: Chen Liu, Jiaye Wu, Yasutaka Furukawa
- **Comment**: None
- **Journal**: None
- **Summary**: The ultimate goal of this indoor mapping research is to automatically reconstruct a floorplan simply by walking through a house with a smartphone in a pocket. This paper tackles this problem by proposing FloorNet, a novel deep neural architecture. The challenge lies in the processing of RGBD streams spanning a large 3D space. FloorNet effectively processes the data through three neural network branches: 1) PointNet with 3D points, exploiting the 3D information; 2) CNN with a 2D point density image in a top-down view, enhancing the local spatial reasoning; and 3) CNN with RGB images, utilizing the full image information. FloorNet exchanges intermediate features across the branches to exploit the best of all the architectures. We have created a benchmark for floorplan reconstruction by acquiring RGBD video streams for 155 residential houses or apartments with Google Tango phones and annotating complete floorplan information. Our qualitative and quantitative evaluations demonstrate that the fusion of three branches effectively improves the reconstruction quality. We hope that the paper together with the benchmark will be an important step towards solving a challenging vector-graphics reconstruction problem. Code and data are available at https://github.com/art-programmer/FloorNet.



### Iterative Learning with Open-set Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1804.00092v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00092v1)
- **Published**: 2018-03-31 00:27:30+00:00
- **Updated**: 2018-03-31 00:27:30+00:00
- **Authors**: Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, Shu-Tao Xia
- **Comment**: CVPR2018-Spotlight
- **Journal**: None
- **Summary**: Large-scale datasets possessing clean label annotations are crucial for training Convolutional Neural Networks (CNNs). However, labeling large-scale data can be very costly and error-prone, and even high-quality datasets are likely to contain noisy (incorrect) labels. Existing works usually employ a closed-set assumption, whereby the samples associated with noisy labels possess a true class contained within the set of known classes in the training data. However, such an assumption is too restrictive for many applications, since samples associated with noisy labels might in fact possess a true class that is not present in the training data. We refer to this more complex scenario as the \textbf{open-set noisy label} problem and show that it is nontrivial in order to make accurate predictions. To address this problem, we propose a novel iterative learning framework for training CNNs on datasets with open-set noisy labels. Our approach detects noisy labels and learns deep discriminative features in an iterative fashion. To benefit from the noisy label detection, we design a Siamese network to encourage clean labels and noisy labels to be dissimilar. A reweighting module is also applied to simultaneously emphasize the learning from clean labels and reduce the effect caused by noisy labels. Experiments on CIFAR-10, ImageNet and real-world noisy (web-search) datasets demonstrate that our proposed model can robustly train CNNs in the presence of a high proportion of open-set as well as closed-set noisy labels.



### Adversarial Attacks and Defences Competition
- **Arxiv ID**: http://arxiv.org/abs/1804.00097v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1804.00097v1)
- **Published**: 2018-03-31 00:52:20+00:00
- **Updated**: 2018-03-31 00:52:20+00:00
- **Authors**: Alexey Kurakin, Ian Goodfellow, Samy Bengio, Yinpeng Dong, Fangzhou Liao, Ming Liang, Tianyu Pang, Jun Zhu, Xiaolin Hu, Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, Alan Yuille, Sangxia Huang, Yao Zhao, Yuzhe Zhao, Zhonglin Han, Junjiajia Long, Yerkebulan Berdibekov, Takuya Akiba, Seiya Tokui, Motoki Abe
- **Comment**: 36 pages, 10 figures
- **Journal**: None
- **Summary**: To accelerate research on adversarial examples and robustness of machine learning classifiers, Google Brain organized a NIPS 2017 competition that encouraged researchers to develop new methods to generate adversarial examples as well as to develop new ways to defend against them. In this chapter, we describe the structure and organization of the competition and the solutions developed by several of the top-placing teams.



### Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1804.00100v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00100v2)
- **Published**: 2018-03-31 01:08:33+00:00
- **Updated**: 2018-04-03 08:29:53+00:00
- **Authors**: Jingwen Wang, Wenhao Jiang, Lin Ma, Wei Liu, Yong Xu
- **Comment**: CVPR2018 spotlight paper
- **Journal**: None
- **Summary**: Dense video captioning is a newly emerging task that aims at both localizing and describing all events in a video. We identify and tackle two challenges on this task, namely, (1) how to utilize both past and future contexts for accurate event proposal predictions, and (2) how to construct informative input to the decoder for generating natural event descriptions. First, previous works predominantly generate temporal event proposals in the forward direction, which neglects future video context. We propose a bidirectional proposal method that effectively exploits both past and future contexts to make proposal predictions. Second, different events ending at (nearly) the same time are indistinguishable in the previous works, resulting in the same captions. We solve this problem by representing each event with an attentive fusion of hidden states from the proposal module and video contents (e.g., C3D features). We further propose a novel context gating mechanism to balance the contributions from the current event and its surrounding contexts dynamically. We empirically show that our attentively fused event representation is superior to the proposal hidden states or video contents alone. By coupling proposal and captioning modules into one unified framework, our model outperforms the state-of-the-arts on the ActivityNet Captions dataset with a relative gain of over 100% (Meteor score increases from 4.82 to 9.65).



### A LiDAR Point Cloud Generator: from a Virtual World to Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1804.00103v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00103v1)
- **Published**: 2018-03-31 01:32:11+00:00
- **Updated**: 2018-03-31 01:32:11+00:00
- **Authors**: Xiangyu Yue, Bichen Wu, Sanjit A. Seshia, Kurt Keutzer, Alberto L. Sangiovanni-Vincentelli
- **Comment**: None
- **Journal**: None
- **Summary**: 3D LiDAR scanners are playing an increasingly important role in autonomous driving as they can generate depth information of the environment. However, creating large 3D LiDAR point cloud datasets with point-level labels requires a significant amount of manual annotation. This jeopardizes the efficient development of supervised deep learning algorithms which are often data-hungry. We present a framework to rapidly create point clouds with accurate point-level labels from a computer game. The framework supports data collection from both auto-driving scenes and user-configured scenes. Point clouds from auto-driving scenes can be used as training data for deep learning algorithms, while point clouds from user-configured scenes can be used to systematically test the vulnerability of a neural network, and use the falsifying examples to make the neural network more robust through retraining. In addition, the scene images can be captured simultaneously in order for sensor fusion tasks, with a method proposed to do automatic calibration between the point clouds and captured scene images. We show a significant improvement in accuracy (+9%) in point cloud segmentation by augmenting the training dataset with the generated synthesized data. Our experiments also show by testing and retraining the network using point clouds from user-configured scenes, the weakness/blind spots of the neural network can be fixed.



### Visual Question Reasoning on General Dependency Tree
- **Arxiv ID**: http://arxiv.org/abs/1804.00105v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00105v1)
- **Published**: 2018-03-31 01:48:27+00:00
- **Updated**: 2018-03-31 01:48:27+00:00
- **Authors**: Qingxing Cao, Xiaodan Liang, Bailing Li, Guanbin Li, Liang Lin
- **Comment**: Accepted as spotlight at CVPR 2018
- **Journal**: None
- **Summary**: The collaborative reasoning for understanding each image-question pair is very critical but under-explored for an interpretable Visual Question Answering (VQA) system. Although very recent works also tried the explicit compositional processes to assemble multiple sub-tasks embedded in the questions, their models heavily rely on the annotations or hand-crafted rules to obtain valid reasoning layout, leading to either heavy labor or poor performance on composition reasoning. In this paper, to enable global context reasoning for better aligning image and language domains in diverse and unrestricted cases, we propose a novel reasoning network called Adversarial Composition Modular Network (ACMN). This network comprises of two collaborative modules: i) an adversarial attention module to exploit the local visual evidence for each word parsed from the question; ii) a residual composition module to compose the previously mined evidence. Given a dependency parse tree for each question, the adversarial attention module progressively discovers salient regions of one word by densely combining regions of child word nodes in an adversarial manner. Then residual composition module merges the hidden representations of an arbitrary number of children through sum pooling and residual connection. Our ACMN is thus capable of building an interpretable VQA system that gradually dives the image cues following a question-driven reasoning route and makes global reasoning by incorporating the learned knowledge of all attention modules in a principled manner. Experiments on relational datasets demonstrate the superiority of our ACMN and visualization results show the explainable capability of our reasoning system.



### Compare and Contrast: Learning Prominent Visual Differences
- **Arxiv ID**: http://arxiv.org/abs/1804.00112v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00112v2)
- **Published**: 2018-03-31 03:20:18+00:00
- **Updated**: 2018-04-13 18:44:09+00:00
- **Authors**: Steven Chen, Kristen Grauman
- **Comment**: In Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2018
- **Journal**: None
- **Summary**: Relative attribute models can compare images in terms of all detected properties or attributes, exhaustively predicting which image is fancier, more natural, and so on without any regard to ordering. However, when humans compare images, certain differences will naturally stick out and come to mind first. These most noticeable differences, or prominent differences, are likely to be described first. In addition, many differences, although present, may not be mentioned at all. In this work, we introduce and model prominent differences, a rich new functionality for comparing images. We collect instance-level annotations of most noticeable differences, and build a model trained on relative attribute features that predicts prominent differences for unseen pairs. We test our model on the challenging UT-Zap50K shoes and LFW10 faces datasets, and outperform an array of baseline methods. We then demonstrate how our prominence model improves two vision tasks, image search and description generation, enabling more natural communication between people and vision systems.



### Tagging like Humans: Diverse and Distinct Image Annotation
- **Arxiv ID**: http://arxiv.org/abs/1804.00113v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00113v1)
- **Published**: 2018-03-31 03:22:50+00:00
- **Updated**: 2018-03-31 03:22:50+00:00
- **Authors**: Baoyuan Wu, Weidong Chen, Peng Sun, Wei Liu, Bernard Ghanem, Siwei Lyu
- **Comment**: Accepted by CVPR 2018
- **Journal**: None
- **Summary**: In this work we propose a new automatic image annotation model, dubbed {\bf diverse and distinct image annotation} (D2IA). The generative model D2IA is inspired by the ensemble of human annotations, which create semantically relevant, yet distinct and diverse tags. In D2IA, we generate a relevant and distinct tag subset, in which the tags are relevant to the image contents and semantically distinct to each other, using sequential sampling from a determinantal point process (DPP) model. Multiple such tag subsets that cover diverse semantic aspects or diverse semantic levels of the image contents are generated by randomly perturbing the DPP sampling process. We leverage a generative adversarial network (GAN) model to train D2IA. Extensive experiments including quantitative and qualitative comparisons, as well as human subject studies, on two benchmark datasets demonstrate that the proposed model can produce more diverse and distinct tags than the state-of-the-arts.



### Multi-label Learning with Missing Labels using Mixed Dependency Graphs
- **Arxiv ID**: http://arxiv.org/abs/1804.00117v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.00117v1)
- **Published**: 2018-03-31 04:15:11+00:00
- **Updated**: 2018-03-31 04:15:11+00:00
- **Authors**: Baoyuan Wu, Fan Jia, Wei Liu, Bernard Ghanem, Siwei Lyu
- **Comment**: Published in International Journal of Computer Vision, 2018
- **Journal**: None
- **Summary**: This work focuses on the problem of multi-label learning with missing labels (MLML), which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels. The key point to handle missing labels is propagating the label information from provided labels to missing labels, through a dependency graph that each label of each instance is treated as a node. We build this graph by utilizing different types of label dependencies. Specifically, the instance-level similarity is served as undirected edges to connect the label nodes across different instances and the semantic label hierarchy is used as directed edges to connect different classes. This base graph is referred to as the mixed dependency graph, as it includes both undirected and directed edges. Furthermore, we present another two types of label dependencies to connect the label nodes across different classes. One is the class co-occurrence, which is also encoded as undirected edges. Combining with the base graph, we obtain a new mixed graph, called MG-CO (mixed graph with co-occurrence). The other is the sparse and low rank decomposition of the whole label matrix, to embed high-order dependencies over all labels. Combining with the base graph, the new mixed graph is called as MG-SL (mixed graph with sparse and low rank decomposition). Based on MG-CO and MG-SL, we propose two convex transductive formulations of the MLML problem, denoted as MLMG-CO and MLMG-SL, respectively. Two important applications, including image annotation and tag based image retrieval, can be jointly handled using our proposed methods. Experiments on benchmark datasets show that our methods give significant improvements in performance and robustness to missing labels over the state-of-the-art methods.



### Quantitative Evaluation of Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1804.00118v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00118v1)
- **Published**: 2018-03-31 04:25:05+00:00
- **Updated**: 2018-03-31 04:25:05+00:00
- **Authors**: Mao-Chuang Yeh, Shuai Tang, Anand Bhattad, D. A. Forsyth
- **Comment**: 30 pages, including supplementary
- **Journal**: None
- **Summary**: Style transfer methods produce a transferred image which is a rendering of a content image in the manner of a style image. There is a rich literature of variant methods. However, evaluation procedures are qualitative, mostly involving user studies. We describe a novel quantitative evaluation procedure. One plots effectiveness (a measure of the extent to which the style was transferred) against coherence (a measure of the extent to which the transferred image decomposes into objects in the same way that the content image does) to obtain an EC plot.   We construct EC plots comparing a number of recent style transfer methods. Most methods control within-layer gram matrices, but we also investigate a method that controls cross-layer gram matrices. These EC plots reveal a number of intriguing properties of recent style transfer methods. The style used has a strong effect on the outcome, for all methods. Using large style weights does not necessarily improve effectiveness, and can produce worse results. Cross-layer gram matrices easily beat all other methods, but some styles remain difficult for all methods. Ensemble methods show real promise. It is likely that, for current methods, each style requires a different choice of weights to obtain the best results, so that automated weight setting methods are desirable. Finally, we show evidence comparing our EC evaluations to human evaluations.



### Snap Angle Prediction for 360$^{\circ}$ Panoramas
- **Arxiv ID**: http://arxiv.org/abs/1804.00126v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00126v2)
- **Published**: 2018-03-31 06:09:30+00:00
- **Updated**: 2018-08-12 01:00:39+00:00
- **Authors**: Bo Xiong, Kristen Grauman
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: 360$^{\circ}$ panoramas are a rich medium, yet notoriously difficult to visualize in the 2D image plane. We explore how intelligent rotations of a spherical image may enable content-aware projection with fewer perceptible distortions. Whereas existing approaches assume the viewpoint is fixed, intuitively some viewing angles within the sphere preserve high-level objects better than others. To discover the relationship between these optimal snap angles and the spherical panorama's content, we develop a reinforcement learning approach for the cubemap projection model. Implemented as a deep recurrent neural network, our method selects a sequence of rotation actions and receives reward for avoiding cube boundaries that overlap with important foreground objects. We show our approach creates more visually pleasing panoramas while using 5x less computation than the baseline.



### A Subpixel Registration Algorithm for Low PSNR Images
- **Arxiv ID**: http://arxiv.org/abs/1804.00174v1
- **DOI**: 10.1109/ICACI.2012.6463241
- **Categories**: **cs.CV**, astro-ph.IM
- **Links**: [PDF](http://arxiv.org/pdf/1804.00174v1)
- **Published**: 2018-03-31 14:00:32+00:00
- **Updated**: 2018-03-31 14:00:32+00:00
- **Authors**: Song Feng, Linhua Deng, Guofeng Shu, Feng Wang, Hui Deng, Kaifan Ji
- **Comment**: in 2012 IEEE 5th Int. Conf. on Advanced Computational Intelligence
  (ICACI) (New York: IEEE), 626
- **Journal**: None
- **Summary**: This paper presents a fast algorithm for obtaining high-accuracy subpixel translation of low PSNR images. Instead of locating the maximum point on the upsampled images or fitting the peak of correlation surface, the proposed algorithm is based on the measurement of centroid on the cross correlation surface by Modified Moment method. Synthetic images, real solar images and standard testing images with white Gaussian noise added were tested, and the results show that the accuracies of our algorithm are comparable with other subpixel registration techniques and the processing speed is higher. The drawback is also discussed at the end of this paper.



### DeepIM: Deep Iterative Matching for 6D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1804.00175v4
- **DOI**: 10.1007/s11263-019-01250-9
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.00175v4)
- **Published**: 2018-03-31 14:02:25+00:00
- **Updated**: 2019-10-02 00:54:47+00:00
- **Authors**: Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, Dieter Fox
- **Comment**: submitted to IJCV, update results on YCB_Video, add depth-based
  results
- **Journal**: None
- **Summary**: Estimating the 6D pose of objects from images is an important problem in various applications such as robot manipulation and virtual reality. While direct regression of images to object poses has limited accuracy, matching rendered images of an object against the observed image can produce accurate results. In this work, we propose a novel deep neural network for 6D pose matching named DeepIM. Given an initial pose estimation, our network is able to iteratively refine the pose by matching the rendered image against the observed image. The network is trained to predict a relative pose transformation using an untangled representation of 3D location and 3D orientation and an iterative training process. Experiments on two commonly used benchmarks for 6D pose estimation demonstrate that DeepIM achieves large improvements over state-of-the-art methods. We furthermore show that DeepIM is able to match previously unseen objects.



### Webly Supervised Learning for Skin Lesion Classification
- **Arxiv ID**: http://arxiv.org/abs/1804.00177v2
- **DOI**: 10.1007/978-3-030-00934-2_45
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00177v2)
- **Published**: 2018-03-31 14:13:43+00:00
- **Updated**: 2019-05-31 08:14:55+00:00
- **Authors**: Fernando Navarro, Sailesh Conjeti, Federico Tombari, Nassir Navab
- **Comment**: Accepted to International Conference on Medical Image Computing and
  Computer-Assisted Intervention 2018 Added Acknowledgements section, rest is
  unchanged. In MICCAI 2018. Springer, Cham
- **Journal**: None
- **Summary**: Within medical imaging, manual curation of sufficient well-labeled samples is cost, time and scale-prohibitive. To improve the representativeness of the training dataset, for the first time, we present an approach to utilize large amounts of freely available web data through web-crawling. To handle noise and weak nature of web annotations, we propose a two-step transfer learning based training process with a robust loss function, termed as Webly Supervised Learning (WSL) to train deep models for the task. We also leverage search by image to improve the search specificity of our web-crawling and reduce cross-domain noise. Within WSL, we explicitly model the noise structure between classes and incorporate it to selectively distill knowledge from the web data during model training. To demonstrate improved performance due to WSL, we benchmarked on a publicly available 10-class fine-grained skin lesion classification dataset and report a significant improvement of top-1 classification accuracy from 71.25 % to 80.53 % due to the incorporation of web-supervision.



### Presentation Attack Detection for Iris Recognition: An Assessment of the State of the Art
- **Arxiv ID**: http://arxiv.org/abs/1804.00194v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00194v3)
- **Published**: 2018-03-31 17:17:50+00:00
- **Updated**: 2018-06-13 17:08:39+00:00
- **Authors**: Adam Czajka, Kevin W. Bowyer
- **Comment**: Pre-print accepted for publication in ACM Computing Surveys on June
  13, 2018
- **Journal**: None
- **Summary**: Iris recognition is increasingly used in large-scale applications. As a result, presentation attack detection for iris recognition takes on fundamental importance. This survey covers the diverse research literature on this topic. Different categories of presentation attack are described and placed in an application-relevant framework, and the state of the art in detecting each category of attack is summarized. One conclusion from this is that presentation attack detection for iris recognition is not yet a solved problem. Datasets available for research are described, research directions for the near- and medium-term future are outlined, and a short list of recommended readings are suggested.



### Gated Fusion Network for Single Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/1804.00213v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00213v1)
- **Published**: 2018-03-31 20:33:11+00:00
- **Updated**: 2018-03-31 20:33:11+00:00
- **Authors**: Wenqi Ren, Lin Ma, Jiawei Zhang, Jinshan Pan, Xiaochun Cao, Wei Liu, Ming-Hsuan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an efficient algorithm to directly restore a clear image from a hazy input. The proposed algorithm hinges on an end-to-end trainable neural network that consists of an encoder and a decoder. The encoder is exploited to capture the context of the derived input images, while the decoder is employed to estimate the contribution of each input to the final dehazed result using the learned representations attributed to the encoder. The constructed network adopts a novel fusion-based strategy which derives three inputs from an original hazy image by applying White Balance (WB), Contrast Enhancing (CE), and Gamma Correction (GC). We compute pixel-wise confidence maps based on the appearance differences between these different inputs to blend the information of the derived inputs and preserve the regions with pleasant visibility. The final dehazed image is yielded by gating the important features of the derived inputs. To train the network, we introduce a multi-scale approach such that the halo artifacts can be avoided. Extensive experimental results on both synthetic and real-world images demonstrate that the proposed algorithm performs favorably against the state-of-the-art algorithms.



### Human Semantic Parsing for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1804.00216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00216v1)
- **Published**: 2018-03-31 21:13:07+00:00
- **Updated**: 2018-03-31 21:13:07+00:00
- **Authors**: Mahdi M. Kalayeh, Emrah Basaran, Muhittin Gokmen, Mustafa E. Kamasak, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification is a challenging task mainly due to factors such as background clutter, pose, illumination and camera point of view variations. These elements hinder the process of extracting robust and discriminative representations, hence preventing different identities from being successfully distinguished. To improve the representation learning, usually, local features from human body parts are extracted. However, the common practice for such a process has been based on bounding box part detection. In this paper, we propose to adopt human semantic parsing which, due to its pixel-level accuracy and capability of modeling arbitrary contours, is naturally a better alternative. Our proposed SPReID integrates human semantic parsing in person re-identification and not only considerably outperforms its counter baseline, but achieves state-of-the-art performance. We also show that by employing a \textit{simple} yet effective training strategy, standard popular deep convolutional architectures such as Inception-V3 and ResNet-152, with no modification, while operating solely on full image, can dramatically outperform current state-of-the-art. Our proposed methods improve state-of-the-art person re-identification on: Market-1501 by ~17% in mAP and ~6% in rank-1, CUHK03 by ~4% in rank-1 and DukeMTMC-reID by ~24% in mAP and ~10% in rank-1.



### Bio-inspired digit recognition using reward-modulated spike-timing-dependent plasticity in deep convolutional networks
- **Arxiv ID**: http://arxiv.org/abs/1804.00227v3
- **DOI**: 10.1016/j.patcog.2019.05.015
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1804.00227v3)
- **Published**: 2018-03-31 23:35:12+00:00
- **Updated**: 2019-05-27 19:33:54+00:00
- **Authors**: Milad Mozafari, Mohammad Ganjtabesh, Abbas Nowzari-Dalini, Simon J. Thorpe, Timothée Masquelier
- **Comment**: Pattern Recognition (2019)
- **Journal**: None
- **Summary**: The primate visual system has inspired the development of deep artificial neural networks, which have revolutionized the computer vision domain. Yet these networks are much less energy-efficient than their biological counterparts, and they are typically trained with backpropagation, which is extremely data-hungry. To address these limitations, we used a deep convolutional spiking neural network (DCSNN) and a latency-coding scheme. We trained it using a combination of spike-timing-dependent plasticity (STDP) for the lower layers and reward-modulated STDP (R-STDP) for the higher ones. In short, with R-STDP a correct (resp. incorrect) decision leads to STDP (resp. anti-STDP). This approach led to an accuracy of $97.2\%$ on MNIST, without requiring an external classifier. In addition, we demonstrated that R-STDP extracts features that are diagnostic for the task at hand, and discards the other ones, whereas STDP extracts any feature that repeats. Finally, our approach is biologically plausible, hardware friendly, and energy-efficient.



