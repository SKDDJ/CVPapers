# Arxiv Papers in cs.CV on 2018-03-24
### Realtime Time Synchronized Event-based Stereo
- **Arxiv ID**: http://arxiv.org/abs/1803.09025v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09025v2)
- **Published**: 2018-03-24 01:00:49+00:00
- **Updated**: 2018-10-18 19:01:43+00:00
- **Authors**: Alex Zihao Zhu, Yibo Chen, Kostas Daniilidis
- **Comment**: 13 pages, 3 figures, 1 table. Video: https://youtu.be/4oa7e4hsrYo.
  Updated with final version with additional experiments
- **Journal**: European Conference on Computer Vision 2018
- **Summary**: In this work, we propose a novel event based stereo method which addresses the problem of motion blur for a moving event camera. Our method uses the velocity of the camera and a range of disparities to synchronize the positions of the events, as if they were captured at a single point in time. We represent these events using a pair of novel time synchronized event disparity volumes, which we show remove motion blur for pixels at the correct disparity in the volume, while further blurring pixels at the wrong disparity. We then apply a novel matching cost over these time synchronized event disparity volumes, which both rewards similarity between the volumes while penalizing blurriness. We show that our method outperforms more expensive, smoothing based event stereo methods, by evaluating on the Multi Vehicle Stereo Event Camera dataset.



### Design of a PCIe Interface Card Control Software Based on WDF
- **Arxiv ID**: http://arxiv.org/abs/1803.09052v3
- **DOI**: None
- **Categories**: **cs.CV**, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1803.09052v3)
- **Published**: 2018-03-24 04:04:20+00:00
- **Updated**: 2018-04-11 01:16:13+00:00
- **Authors**: Meng Shengwei, Lu Jianjie
- **Comment**: 4 pages, 13 figures
- **Journal**: None
- **Summary**: Based on a clear analysis of the latest Windows driver framework WDF, this paper has implemented a driver of the PCIe-SpaceWire interface card device and put forward a discussion about ensuring the stability of PCIe driver. At the same time, Qt and OpenGL are used to design the upper application. Finally, a functional verification of the control software is provided.



### A Single-shot-per-pose Camera-Projector Calibration System For Imperfect Planar Targets
- **Arxiv ID**: http://arxiv.org/abs/1803.09058v2
- **DOI**: 10.1109/ISMAR-Adjunct.2018.00023
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09058v2)
- **Published**: 2018-03-24 05:25:23+00:00
- **Updated**: 2018-10-17 19:43:56+00:00
- **Authors**: Bingyao Huang, Samed Ozdemir, Ying Tang, Chunyuan Liao, Haibin Ling
- **Comment**: Adjunct Proceedings of the IEEE International Symposium for Mixed and
  Augmented Reality 2018. Source code:
  https://github.com/BingyaoHuang/single-shot-pro-cam-calib/
- **Journal**: None
- **Summary**: Existing camera-projector calibration methods typically warp feature points from a camera image to a projector image using estimated homographies, and often suffer from errors in camera parameters and noise due to imperfect planarity of the calibration target. In this paper we propose a simple yet robust solution that explicitly deals with these challenges. Following the structured light (SL) camera-project calibration framework, a carefully designed correspondence algorithm is built on top of the De Bruijn patterns. Such correspondence is then used for initial camera-projector calibration. Then, to gain more robustness against noises, especially those from an imperfect planar calibration board, a bundle adjustment algorithm is developed to jointly optimize the estimated camera and projector models. Aside from the robustness, our solution requires only one shot of SL pattern for each calibration board pose, which is much more convenient than multi-shot solutions in practice. Data validations are conducted on both synthetic and real datasets, and our method shows clear advantages over existing methods in all experiments.



### Adversarial Framework for Unsupervised Learning of Motion Dynamics in Videos
- **Arxiv ID**: http://arxiv.org/abs/1803.09092v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09092v2)
- **Published**: 2018-03-24 11:17:41+00:00
- **Updated**: 2019-09-17 20:42:07+00:00
- **Authors**: C. Spampinato, S. Palazzo, P. D'Oro, D. Giordano, M. Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Human behavior understanding in videos is a complex, still unsolved problem and requires to accurately model motion at both the local (pixel-wise dense prediction) and global (aggregation of motion cues) levels. Current approaches based on supervised learning require large amounts of annotated data, whose scarce availability is one of the main limiting factors to the development of general solutions. Unsupervised learning can instead leverage the vast amount of videos available on the web and it is a promising solution for overcoming the existing limitations. In this paper, we propose an adversarial GAN-based framework that learns video representations and dynamics through a self-supervision mechanism in order to perform dense and global prediction in videos. Our approach synthesizes videos by 1) factorizing the process into the generation of static visual content and motion, 2) learning a suitable representation of a motion latent space in order to enforce spatio-temporal coherency of object trajectories, and 3) incorporating motion estimation and pixel-wise dense prediction into the training procedure. Self-supervision is enforced by using motion masks produced by the generator, as a co-product of its generation process, to supervise the discriminator network in performing dense prediction. Performance evaluation, carried out on standard benchmarks, shows that our approach is able to learn, in an unsupervised way, both local and global video dynamics. The learned representations, then, support the training of video object segmentation methods with sensibly less (about 50%) annotations, giving performance comparable to the state of the art. Furthermore, the proposed method achieves promising performance in generating realistic videos, outperforming state-of-the-art approaches especially on motion-related metrics.



### Comparing Generative Adversarial Network Techniques for Image Creation and Modification
- **Arxiv ID**: http://arxiv.org/abs/1803.09093v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.09093v1)
- **Published**: 2018-03-24 11:19:07+00:00
- **Updated**: 2018-03-24 11:19:07+00:00
- **Authors**: Mathijs Pieters, Marco Wiering
- **Comment**: 20 pages, 23 figures
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have demonstrated to be successful at generating realistic real-world images. In this paper we compare various GAN techniques, both supervised and unsupervised. The effects on training stability of different objective functions are compared. We add an encoder to the network, making it possible to encode images to the latent space of the GAN. The generator, discriminator and encoder are parameterized by deep convolutional neural networks. For the discriminator network we experimented with using the novel Capsule Network, a state-of-the-art technique for detecting global features in images. Experiments are performed using a digit and face dataset, with various visualizations illustrating the results. The results show that using the encoder network it is possible to reconstruct images. With the conditional GAN we can alter visual attributes of generated or encoded images. The experiments with the Capsule Network as discriminator result in generated images of a lower quality, compared to a standard convolutional neural network.



### Predicting Gaze in Egocentric Video by Learning Task-dependent Attention Transition
- **Arxiv ID**: http://arxiv.org/abs/1803.09125v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09125v3)
- **Published**: 2018-03-24 15:31:55+00:00
- **Updated**: 2018-12-04 12:23:07+00:00
- **Authors**: Yifei Huang, Minjie Cai, Zhenqiang Li, Yoichi Sato
- **Comment**: Accepted as oral presentation in ECCV 2018
- **Journal**: None
- **Summary**: We present a new computational model for gaze prediction in egocentric videos by exploring patterns in temporal shift of gaze fixations (attention transition) that are dependent on egocentric manipulation tasks. Our assumption is that the high-level context of how a task is completed in a certain way has a strong influence on attention transition and should be modeled for gaze prediction in natural dynamic scenes. Specifically, we propose a hybrid model based on deep neural networks which integrates task-dependent attention transition with bottom-up saliency prediction. In particular, the task-dependent attention transition is learned with a recurrent neural network to exploit the temporal context of gaze fixations, e.g. looking at a cup after moving gaze away from a grasped bottle. Experiments on public egocentric activity datasets show that our model significantly outperforms state-of-the-art gaze prediction methods and is able to learn meaningful transition of human attention.



### Merging and Evolution: Improving Convolutional Neural Networks for Mobile Applications
- **Arxiv ID**: http://arxiv.org/abs/1803.09127v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09127v1)
- **Published**: 2018-03-24 15:37:49+00:00
- **Updated**: 2018-03-24 15:37:49+00:00
- **Authors**: Zheng Qin, Zhaoning Zhang, Shiqing Zhang, Hao Yu, Yuxing Peng
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: Compact neural networks are inclined to exploit "sparsely-connected" convolutions such as depthwise convolution and group convolution for employment in mobile applications. Compared with standard "fully-connected" convolutions, these convolutions are more computationally economical. However, "sparsely-connected" convolutions block the inter-group information exchange, which induces severe performance degradation. To address this issue, we present two novel operations named merging and evolution to leverage the inter-group information. Our key idea is encoding the inter-group information with a narrow feature map, then combining the generated features with the original network for better representation. Taking advantage of the proposed operations, we then introduce the Merging-and-Evolution (ME) module, an architectural unit specifically designed for compact networks. Finally, we propose a family of compact neural networks called MENet based on ME modules. Extensive experiments on ILSVRC 2012 dataset and PASCAL VOC 2007 dataset demonstrate that MENet consistently outperforms other state-of-the-art compact networks under different computational budgets. For instance, under the computational budget of 140 MFLOPs, MENet surpasses ShuffleNet by 1% and MobileNet by 1.95% on ILSVRC 2012 top-1 accuracy, while by 2.3% and 4.1% on PASCAL VOC 2007 mAP, respectively.



### Multi-Level Factorisation Net for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1803.09132v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09132v2)
- **Published**: 2018-03-24 16:34:01+00:00
- **Updated**: 2018-04-17 21:47:56+00:00
- **Authors**: Xiaobin Chang, Timothy M. Hospedales, Tao Xiang
- **Comment**: To Appear at CVPR2018
- **Journal**: None
- **Summary**: Key to effective person re-identification (Re-ID) is modelling discriminative and view-invariant factors of person appearance at both high and low semantic levels. Recently developed deep Re-ID models either learn a holistic single semantic level feature representation and/or require laborious human annotation of these factors as attributes. We propose Multi-Level Factorisation Net (MLFN), a novel network architecture that factorises the visual appearance of a person into latent discriminative factors at multiple semantic levels without manual annotation. MLFN is composed of multiple stacked blocks. Each block contains multiple factor modules to model latent factors at a specific level, and factor selection modules that dynamically select the factor modules to interpret the content of each input image. The outputs of the factor selection modules also provide a compact latent factor descriptor that is complementary to the conventional deeply learned features. MLFN achieves state-of-the-art results on three Re-ID datasets, as well as compelling results on the general object categorisation CIFAR-100 dataset.



### Noise generation for compression algorithms
- **Arxiv ID**: http://arxiv.org/abs/1803.09165v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09165v1)
- **Published**: 2018-03-24 21:19:29+00:00
- **Updated**: 2018-03-24 21:19:29+00:00
- **Authors**: Renata Khasanova, Jan Wassenberg, Jyrki Alakuijala
- **Comment**: None
- **Journal**: None
- **Summary**: In various Computer Vision and Signal Processing applications, noise is typically perceived as a drawback of the image capturing system that ought to be removed. We, on the other hand, claim that image noise, just as texture, is important for visual perception and, therefore, critical for lossy compression algorithms that tend to make decompressed images look less realistic by removing small image details. In this paper we propose a physically and biologically inspired technique that learns a noise model at the encoding step of the compression algorithm and then generates the appropriate amount of additive noise at the decoding step. Our method can significantly increase the realism of the decompressed image at the cost of few bytes of additional memory space regardless of the original image size. The implementation of our method is open-sourced and available at https://github.com/google/pik.



### Multiple Sclerosis Lesion Segmentation from Brain MRI via Fully Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.09172v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09172v1)
- **Published**: 2018-03-24 22:43:16+00:00
- **Updated**: 2018-03-24 22:43:16+00:00
- **Authors**: Snehashis Roy, John A. Butman, Daniel S. Reich, Peter A. Calabresi, Dzung L. Pham
- **Comment**: Submitted to NeuroImage
- **Journal**: None
- **Summary**: Multiple Sclerosis (MS) is an autoimmune disease that leads to lesions in the central nervous system. Magnetic resonance (MR) images provide sufficient imaging contrast to visualize and detect lesions, particularly those in the white matter. Quantitative measures based on various features of lesions have been shown to be useful in clinical trials for evaluating therapies. Therefore robust and accurate segmentation of white matter lesions from MR images can provide important information about the disease status and progression. In this paper, we propose a fully convolutional neural network (CNN) based method to segment white matter lesions from multi-contrast MR images. The proposed CNN based method contains two convolutional pathways. The first pathway consists of multiple parallel convolutional filter banks catering to multiple MR modalities. In the second pathway, the outputs of the first one are concatenated and another set of convolutional filters are applied. The output of this last pathway produces a membership function for lesions that may be thresholded to obtain a binary segmentation. The proposed method is evaluated on a dataset of 100 MS patients, as well as the ISBI 2015 challenge data consisting of 14 patients. The comparison is performed against four publicly available MS lesion segmentation methods. Significant improvement in segmentation quality over the competing methods is demonstrated on various metrics, such as Dice and false positive ratio. While evaluating on the ISBI 2015 challenge data, our method produces a score of 90.48, where a score of 90 is considered to be comparable to a human rater.



### FaceForensics: A Large-scale Video Dataset for Forgery Detection in Human Faces
- **Arxiv ID**: http://arxiv.org/abs/1803.09179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09179v1)
- **Published**: 2018-03-24 23:12:44+00:00
- **Updated**: 2018-03-24 23:12:44+00:00
- **Authors**: Andreas Rössler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, Matthias Nießner
- **Comment**: Video: https://youtu.be/Tle7YaPkO_k
- **Journal**: None
- **Summary**: With recent advances in computer vision and graphics, it is now possible to generate videos with extremely realistic synthetic faces, even in real time. Countless applications are possible, some of which raise a legitimate alarm, calling for reliable detectors of fake videos. In fact, distinguishing between original and manipulated video can be a challenge for humans and computers alike, especially when the videos are compressed or have low resolution, as it often happens on social networks. Research on the detection of face manipulations has been seriously hampered by the lack of adequate datasets. To this end, we introduce a novel face manipulation dataset of about half a million edited images (from over 1000 videos). The manipulations have been generated with a state-of-the-art face editing approach. It exceeds all existing video manipulation datasets by at least an order of magnitude. Using our new dataset, we introduce benchmarks for classical image forensic tasks, including classification and segmentation, considering videos compressed at various quality levels. In addition, we introduce a benchmark evaluation for creating indistinguishable forgeries with known ground truth; for instance with generative refinement models.



### Unsupervised Domain Adaptation: from Simulation Engine to the RealWorld
- **Arxiv ID**: http://arxiv.org/abs/1803.09180v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.09180v1)
- **Published**: 2018-03-24 23:34:06+00:00
- **Updated**: 2018-03-24 23:34:06+00:00
- **Authors**: Sicheng Zhao, Bichen Wu, Joseph Gonzalez, Sanjit A. Seshia, Kurt Keutzer
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale labeled training datasets have enabled deep neural networks to excel on a wide range of benchmark vision tasks. However, in many applications it is prohibitively expensive or time-consuming to obtain large quantities of labeled data. To cope with limited labeled training data, many have attempted to directly apply models trained on a large-scale labeled source domain to another sparsely labeled target domain. Unfortunately, direct transfer across domains often performs poorly due to domain shift and dataset bias. Domain adaptation is the machine learning paradigm that aims to learn a model from a source domain that can perform well on a different (but related) target domain. In this paper, we summarize and compare the latest unsupervised domain adaptation methods in computer vision applications. We classify the non-deep approaches into sample re-weighting and intermediate subspace transformation categories, while the deep strategy includes discrepancy-based methods, adversarial generative models, adversarial discriminative models and reconstruction-based methods. We also discuss some potential directions.



