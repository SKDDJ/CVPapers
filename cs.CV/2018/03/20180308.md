# Arxiv Papers in cs.CV on 2018-03-08
### A framework with updateable joint images re-ranking for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1803.02983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02983v1)
- **Published**: 2018-03-08 06:51:45+00:00
- **Updated**: 2018-03-08 06:51:45+00:00
- **Authors**: Mingyue Yuan, Dong Yin, Jingwen Ding, Yuhao Luo, Zhipeng Zhou, Chengfeng Zhu, Rui Zhang
- **Comment**: 23 pages,5 figures. submitted to JVCI
- **Journal**: None
- **Summary**: Person re-identification plays an important role in realistic video surveillance with increasing demand for public safety. In this paper, we propose a novel framework with rules of updating images for person re-identification in real-world surveillance system. First, Image Pool is generated by using mean-shift tracking method to automatically select video frame fragments of the target person. Second, features extracted from Image Pool by convolutional network work together to re-rank original ranking list of the main image and matching results will be generated. In addition, updating rules are designed for replacing images in Image Pool when a new image satiating with our updating critical formula in video system. These rules fall into two categories: if the new image is from the same camera as the previous updated image, it will replace one of assist images; otherwise, it will replace the main image directly. Experiments are conduced on Market-1501, iLIDS-VID and PRID-2011 and our ITSD datasets to validate that our framework outperforms on rank-1 accuracy and mAP for person re-identification. Furthermore, the update ability of our framework provides consistently remarkable accuracy rate in real-world surveillance system.



### Improved Deep Hashing with Soft Pairwise Similarity for Multi-label Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1803.02987v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02987v3)
- **Published**: 2018-03-08 07:26:20+00:00
- **Updated**: 2019-10-17 02:00:49+00:00
- **Authors**: Zheng Zhang, Qin Zou, Yuewei Lin, Long Chen, Song Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Hash coding has been widely used in the approximate nearest neighbor search for large-scale image retrieval. Recently, many deep hashing methods have been proposed and shown largely improved performance over traditional feature-learning-based methods. Most of these methods examine the pairwise similarity on the semantic-level labels, where the pairwise similarity is generally defined in a hard-assignment way. That is, the pairwise similarity is '1' if they share no less than one class label and '0' if they do not share any. However, such similarity definition cannot reflect the similarity ranking for pairwise images that hold multiple labels. In this paper, a new deep hashing method is proposed for multi-label image retrieval by re-defining the pairwise similarity into an instance similarity, where the instance similarity is quantified into a percentage based on the normalized semantic labels. Based on the instance similarity, a weighted cross-entropy loss and a minimum mean square error loss are tailored for loss-function construction, and are efficiently used for simultaneous feature learning and hash coding. Experiments on three popular datasets demonstrate that, the proposed method outperforms the competing methods and achieves the state-of-the-art performance in multi-label image retrieval.



### Rethinking Feature Distribution for Loss Functions in Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1803.02988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02988v1)
- **Published**: 2018-03-08 07:28:55+00:00
- **Updated**: 2018-03-08 07:28:55+00:00
- **Authors**: Weitao Wan, Yuanyi Zhong, Tianpeng Li, Jiansheng Chen
- **Comment**: Accepted to CVPR 2018 as spotlight
- **Journal**: None
- **Summary**: We propose a large-margin Gaussian Mixture (L-GM) loss for deep neural networks in classification tasks. Different from the softmax cross-entropy loss, our proposal is established on the assumption that the deep features of the training set follow a Gaussian Mixture distribution. By involving a classification margin and a likelihood regularization, the L-GM loss facilitates both a high classification performance and an accurate modeling of the training feature distribution. As such, the L-GM loss is superior to the softmax loss and its major variants in the sense that besides classification, it can be readily used to distinguish abnormal inputs, such as the adversarial examples, based on their features' likelihood to the training feature distribution. Extensive experiments on various recognition benchmarks like MNIST, CIFAR, ImageNet and LFW, as well as on adversarial examples demonstrate the effectiveness of our proposal.



### Learning Effective Binary Visual Representations with Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.03004v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03004v1)
- **Published**: 2018-03-08 08:56:17+00:00
- **Updated**: 2018-03-08 08:56:17+00:00
- **Authors**: Jianxin Wu, Jian-Hao Luo
- **Comment**: 16 pages, 3 figures
- **Journal**: None
- **Summary**: Although traditionally binary visual representations are mainly designed to reduce computational and storage costs in the image retrieval research, this paper argues that binary visual representations can be applied to large scale recognition and detection problems in addition to hashing in retrieval. Furthermore, the binary nature may make it generalize better than its real-valued counterparts. Existing binary hashing methods are either two-stage or hinging on loss term regularization or saturated functions, hence converge slowly and only emit soft binary values. This paper proposes Approximately Binary Clamping (ABC), which is non-saturating, end-to-end trainable, with fast convergence and can output true binary visual representations. ABC achieves comparable accuracy in ImageNet classification as its real-valued counterpart, and even generalizes better in object detection. On benchmark image retrieval datasets, ABC also outperforms existing hashing methods.



### Insights into the robustness of control point configurations for homography and planar pose estimation
- **Arxiv ID**: http://arxiv.org/abs/1803.03025v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03025v2)
- **Published**: 2018-03-08 10:14:41+00:00
- **Updated**: 2019-01-09 11:01:24+00:00
- **Authors**: Raul Acuna, Volker Willert
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we investigate the influence of the spatial configuration of a number of $n \geq 4$ control points on the accuracy and robustness of space resection methods, e.g. used by a fiducial marker for pose estimation. We find robust configurations of control points by minimizing the first order perturbed solution of the DLT algorithm which is equivalent to minimizing the condition number of the data matrix. An empirical statistical evaluation is presented verifying that these optimized control point configurations not only increase the performance of the DLT homography estimation but also improve the performance of planar pose estimation methods like IPPE and EPnP, including the iterative minimization of the reprojection error which is the most accurate algorithm. We provide the characteristics of stable control point configurations for real-world noisy camera data that are practically independent on the camera pose and form certain symmetric patterns dependent on the number of points. Finally, we present a comparison of optimized configuration versus the number of control points.



### Preserving Semantic Relations for Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1803.03049v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03049v1)
- **Published**: 2018-03-08 11:39:30+00:00
- **Updated**: 2018-03-08 11:39:30+00:00
- **Authors**: Yashas Annadani, Soma Biswas
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Zero-shot learning has gained popularity due to its potential to scale recognition models without requiring additional training data. This is usually achieved by associating categories with their semantic information like attributes. However, we believe that the potential offered by this paradigm is not yet fully exploited. In this work, we propose to utilize the structure of the space spanned by the attributes using a set of relations. We devise objective functions to preserve these relations in the embedding space, thereby inducing semanticity to the embedding space. Through extensive experimental evaluation on five benchmark datasets, we demonstrate that inducing semanticity to the embedding space is beneficial for zero-shot learning. The proposed approach outperforms the state-of-the-art on the standard zero-shot setting as well as the more realistic generalized zero-shot setting. We also demonstrate how the proposed approach can be useful for making approximate semantic inferences about an image belonging to a category for which attribute information is not available.



### Vision-Aided Absolute Trajectory Estimation Using an Unsupervised Deep Network with Online Error Correction
- **Arxiv ID**: http://arxiv.org/abs/1803.05850v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.05850v1)
- **Published**: 2018-03-08 11:59:49+00:00
- **Updated**: 2018-03-08 11:59:49+00:00
- **Authors**: E. Jared Shamwell, Sarah Leung, William D. Nothwang
- **Comment**: Submitted to IROS 2018
- **Journal**: None
- **Summary**: We present an unsupervised deep neural network approach to the fusion of RGB-D imagery with inertial measurements for absolute trajectory estimation. Our network, dubbed the Visual-Inertial-Odometry Learner (VIOLearner), learns to perform visual-inertial odometry (VIO) without inertial measurement unit (IMU) intrinsic parameters (corresponding to gyroscope and accelerometer bias or white noise) or the extrinsic calibration between an IMU and camera. The network learns to integrate IMU measurements and generate hypothesis trajectories which are then corrected online according to the Jacobians of scaled image projection errors with respect to a spatial grid of pixel coordinates. We evaluate our network against state-of-the-art (SOA) visual-inertial odometry, visual odometry, and visual simultaneous localization and mapping (VSLAM) approaches on the KITTI Odometry dataset and demonstrate competitive odometry performance.



### Leveraging Unlabeled Data for Crowd Counting by Learning to Rank
- **Arxiv ID**: http://arxiv.org/abs/1803.03095v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03095v1)
- **Published**: 2018-03-08 14:10:56+00:00
- **Updated**: 2018-03-08 14:10:56+00:00
- **Authors**: Xialei Liu, Joost van de Weijer, Andrew D. Bagdanov
- **Comment**: Accepted by CVPR18
- **Journal**: None
- **Summary**: We propose a novel crowd counting approach that leverages abundantly available unlabeled crowd imagery in a learning-to-rank framework. To induce a ranking of cropped images , we use the observation that any sub-image of a crowded scene image is guaranteed to contain the same number or fewer persons than the super-image. This allows us to address the problem of limited size of existing datasets for crowd counting. We collect two crowd scene datasets from Google using keyword searches and query-by-example image retrieval, respectively. We demonstrate how to efficiently learn from these unlabeled datasets by incorporating learning-to-rank in a multi-task network which simultaneously ranks images and estimates crowd density maps. Experiments on two of the most challenging crowd counting datasets show that our approach obtains state-of-the-art results.



### Applicability and interpretation of the deterministic weighted cepstral distance
- **Arxiv ID**: http://arxiv.org/abs/1803.03104v1
- **DOI**: None
- **Categories**: **cs.SY**, cs.CV, math.DS, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.03104v1)
- **Published**: 2018-03-08 14:31:46+00:00
- **Updated**: 2018-03-08 14:31:46+00:00
- **Authors**: Oliver Lauwers, Bart De Moor
- **Comment**: 18 pages, 5 figures, submitted for review to Automatica
- **Journal**: None
- **Summary**: Quantifying similarity between data objects is an important part of modern data science. Deciding what similarity measure to use is very application dependent. In this paper, we combine insights from systems theory and machine learning, and investigate the weighted cepstral distance, which was previously defined for signals coming from ARMA models. We provide an extension of this distance to invertible deterministic linear time invariant single input single output models, and assess its applicability. We show that it can always be interpreted in terms of the poles and zeros of the underlying model, and that, in the case of stable, minimum-phase, or unstable, maximum-phase models, a geometrical interpretation in terms of subspace angles can be given. We then devise a method to assess stability and phase-type of the generating models, using only input/output signal information. In this way, we prove a connection between the extended weighted cepstral distance and a weighted cepstral model norm. In this way, we provide a purely data-driven way to assess different underlying dynamics of input/output signal pairs, without the need for any system identification step. This can be useful in machine learning tasks such as time series clustering. An iPython tutorial is published complementary to this paper, containing implementations of the various methods and algorithms presented here, as well as some numerical illustrations of the equivalences proven here.



### Towards Knowledge Discovery from the Vatican Secret Archives. In Codice Ratio -- Episode 1: Machine Transcription of the Manuscripts
- **Arxiv ID**: http://arxiv.org/abs/1803.03200v3
- **DOI**: 10.1145/3219819.3219879
- **Categories**: **cs.DL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.03200v3)
- **Published**: 2018-03-08 17:01:32+00:00
- **Updated**: 2018-09-12 12:49:24+00:00
- **Authors**: Donatella Firmani, Marco Maiorino, Paolo Merialdo, Elena Nieddu
- **Comment**: Donatella Firmani, Marco Maiorino, Paolo Merialdo, and Elena Nieddu.
  2018. Towards Knowledge Discovery from the Vatican Secret Archives. In Codice
  Ratio - Episode 1: Machine Transcription of the Manuscripts. In Proceedings
  of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data
  Mining (KDD '18). ACM, New York, NY, USA, 263-272
- **Journal**: None
- **Summary**: In Codice Ratio is a research project to study tools and techniques for analyzing the contents of historical documents conserved in the Vatican Secret Archives (VSA). In this paper, we present our efforts to develop a system to support the transcription of medieval manuscripts. The goal is to provide paleographers with a tool to reduce their efforts in transcribing large volumes, as those stored in the VSA, producing good transcriptions for significant portions of the manuscripts. We propose an original approach based on character segmentation. Our solution is able to deal with the dirty segmentation that inevitably occurs in handwritten documents. We use a convolutional neural network to recognize characters and language models to compose word transcriptions. Our approach requires minimal training efforts, making the transcription process more scalable as the production of training sets requires a few pages and can be easily crowdsourced. We have conducted experiments on manuscripts from the Vatican Registers, an unreleased corpus containing the correspondence of the popes. With training data produced by 120 high school students, our system has been able to produce good transcriptions that can be used by paleographers as a solid basis to speedup the transcription process at a large scale.



### Domain Adaptive Faster R-CNN for Object Detection in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1803.03243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03243v1)
- **Published**: 2018-03-08 18:36:22+00:00
- **Updated**: 2018-03-08 18:36:22+00:00
- **Authors**: Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, Luc Van Gool
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: Object detection typically assumes that training and test data are drawn from an identical distribution, which, however, does not always hold in practice. Such a distribution mismatch will lead to a significant performance drop. In this work, we aim to improve the cross-domain robustness of object detection. We tackle the domain shift on two levels: 1) the image-level shift, such as image style, illumination, etc, and 2) the instance-level shift, such as object appearance, size, etc. We build our approach based on the recent state-of-the-art Faster R-CNN model, and design two domain adaptation components, on image level and instance level, to reduce the domain discrepancy. The two domain adaptation components are based on H-divergence theory, and are implemented by learning a domain classifier in adversarial training manner. The domain classifiers on different levels are further reinforced with a consistency regularization to learn a domain-invariant region proposal network (RPN) in the Faster R-CNN model. We evaluate our newly proposed approach using multiple datasets including Cityscapes, KITTI, SIM10K, etc. The results demonstrate the effectiveness of our proposed approach for robust object detection in various domain shift scenarios.



### GONet: A Semi-Supervised Deep Learning Approach For Traversability Estimation
- **Arxiv ID**: http://arxiv.org/abs/1803.03254v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.03254v1)
- **Published**: 2018-03-08 18:52:03+00:00
- **Updated**: 2018-03-08 18:52:03+00:00
- **Authors**: Noriaki Hirose, Amir Sadeghian, Marynel Vázquez, Patrick Goebel, Silvio Savarese
- **Comment**: 8 pages, 7 figures, 3 tables
- **Journal**: None
- **Summary**: We present semi-supervised deep learning approaches for traversability estimation from fisheye images. Our method, GONet, and the proposed extensions leverage Generative Adversarial Networks (GANs) to effectively predict whether the area seen in the input image(s) is safe for a robot to traverse. These methods are trained with many positive images of traversable places, but just a small set of negative images depicting blocked and unsafe areas. This makes the proposed methods practical. Positive examples can be collected easily by simply operating a robot through traversable spaces, while obtaining negative examples is time consuming, costly, and potentially dangerous. Through extensive experiments and several demonstrations, we show that the proposed traversability estimation approaches are robust and can generalize to unseen scenarios. Further, we demonstrate that our methods are memory efficient and fast, allowing for real-time operation on a mobile robot with single or stereo fisheye cameras. As part of our contributions, we open-source two new datasets for traversability estimation. These datasets are composed of approximately 24h of videos from more than 25 indoor environments. Our methods outperform baseline approaches for traversability estimation on these new datasets.



### Generalization in Metric Learning: Should the Embedding Layer be the Embedding Layer?
- **Arxiv ID**: http://arxiv.org/abs/1803.03310v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03310v2)
- **Published**: 2018-03-08 21:29:38+00:00
- **Updated**: 2018-12-10 17:02:32+00:00
- **Authors**: Nam Vo, James Hays
- **Comment**: new version for WACV
- **Journal**: None
- **Summary**: This work studies deep metric learning under small to medium scale data as we believe that better generalization could be a contributing factor to the improvement of previous fine-grained image retrieval methods; it should be considered when designing future techniques. In particular, we investigate using other layers in a deep metric learning system (besides the embedding layer) for feature extraction and analyze how well they perform on training data and generalize to testing data. From this study, we suggest a new regularization practice where one can add or choose a more optimal layer for feature extraction. State-of-the-art performance is demonstrated on 3 fine-grained image retrieval benchmarks: Cars-196, CUB-200-2011, and Stanford Online Product.



### Analysis of Hand Segmentation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1803.03317v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03317v2)
- **Published**: 2018-03-08 21:45:07+00:00
- **Updated**: 2018-03-28 20:11:20+00:00
- **Authors**: Aisha Urooj Khan, Ali Borji
- **Comment**: Accepted at CVPR 2018
- **Journal**: None
- **Summary**: A large number of works in egocentric vision have concentrated on action and object recognition. Detection and segmentation of hands in first-person videos, however, has less been explored. For many applications in this domain, it is necessary to accurately segment not only hands of the camera wearer but also the hands of others with whom he is interacting. Here, we take an in-depth look at the hand segmentation problem. In the quest for robust hand segmentation methods, we evaluated the performance of the state of the art semantic segmentation methods, off the shelf and fine-tuned, on existing datasets. We fine-tune RefineNet, a leading semantic segmentation method, for hand segmentation and find that it does much better than the best contenders. Existing hand segmentation datasets are collected in the laboratory settings. To overcome this limitation, we contribute by collecting two new datasets: a) EgoYouTubeHands including egocentric videos containing hands in the wild, and b) HandOverFace to analyze the performance of our models in presence of similar appearance occlusions. We further explore whether conditional random fields can help refine generated hand segmentations. To demonstrate the benefit of accurate hand maps, we train a CNN for hand-based activity recognition and achieve higher accuracy when a CNN was trained using hand maps produced by the fine-tuned RefineNet. Finally, we annotate a subset of the EgoHands dataset for fine-grained action recognition and show that an accuracy of 58.6% can be achieved by just looking at a single hand pose which is much better than the chance level (12.5%).



### Motion deblurring of faces
- **Arxiv ID**: http://arxiv.org/abs/1803.03330v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03330v1)
- **Published**: 2018-03-08 23:07:22+00:00
- **Updated**: 2018-03-08 23:07:22+00:00
- **Authors**: Grigorios G. Chrysos, Paolo Favaro, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: Face analysis is a core part of computer vision, in which remarkable progress has been observed in the past decades. Current methods achieve recognition and tracking with invariance to fundamental modes of variation such as illumination, 3D pose, expressions. Notwithstanding, a much less standing mode of variation is motion deblurring, which however presents substantial challenges in face analysis. Recent approaches either make oversimplifying assumptions, e.g. in cases of joint optimization with other tasks, or fail to preserve the highly structured shape/identity information. Therefore, we propose a data-driven method that encourages identity preservation. The proposed model includes two parallel streams (sub-networks): the first deblurs the image, the second implicitly extracts and projects the identity of both the sharp and the blurred image in similar subspaces. We devise a method for creating realistic motion blur by averaging a variable number of frames to train our model. The averaged images originate from a 2MF2 dataset with 10 million facial frames, which we introduce for the task. Considering deblurring as an intermediate step, we utilize the deblurred outputs to conduct a thorough experimentation on high-level face analysis tasks, i.e. landmark localization and face verification. The experimental evaluation demonstrates the superiority of our method.



