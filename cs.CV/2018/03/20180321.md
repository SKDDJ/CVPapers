# Arxiv Papers in cs.CV on 2018-03-21
### Robust Depth Estimation from Auto Bracketed Images
- **Arxiv ID**: http://arxiv.org/abs/1803.07702v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07702v1)
- **Published**: 2018-03-21 00:35:43+00:00
- **Updated**: 2018-03-21 00:35:43+00:00
- **Authors**: Sunghoon Im, Hae-Gon Jeon, In So Kweon
- **Comment**: To appear in CVPR 2018. Total 9 pages
- **Journal**: None
- **Summary**: As demand for advanced photographic applications on hand-held devices grows, these electronics require the capture of high quality depth. However, under low-light conditions, most devices still suffer from low imaging quality and inaccurate depth acquisition. To address the problem, we present a robust depth estimation method from a short burst shot with varied intensity (i.e., Auto Bracketing) or strong noise (i.e., High ISO). We introduce a geometric transformation between flow and depth tailored for burst images, enabling our learning-based multi-view stereo matching to be performed effectively. We then describe our depth estimation pipeline that incorporates the geometric transformation into our residual-flow network. It allows our framework to produce an accurate depth map even with a bracketed image sequence. We demonstrate that our method outperforms state-of-the-art methods for various datasets captured by a smartphone and a DSLR camera. Moreover, we show that the estimated depth is applicable for image quality enhancement and photographic editing.



### Weakly Supervised Medical Diagnosis and Localization from Multiple Resolutions
- **Arxiv ID**: http://arxiv.org/abs/1803.07703v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07703v1)
- **Published**: 2018-03-21 00:40:57+00:00
- **Updated**: 2018-03-21 00:40:57+00:00
- **Authors**: Li Yao, Jordan Prosky, Eric Poblenz, Ben Covington, Kevin Lyman
- **Comment**: submitted to ECCV 2018
- **Journal**: None
- **Summary**: Diagnostic imaging often requires the simultaneous identification of a multitude of findings of varied size and appearance. Beyond global indication of said findings, the prediction and display of localization information improves trust in and understanding of results when augmenting clinical workflow. Medical training data rarely includes more than global image-level labels as segmentations are time-consuming and expensive to collect. We introduce an approach to managing these practical constraints by applying a novel architecture which learns at multiple resolutions while generating saliency maps with weak supervision. Further, we parameterize the Log-Sum-Exp pooling function with a learnable lower-bounded adaptation (LSE-LBA) to build in a sharpness prior and better handle localizing abnormalities of different sizes using only image-level labels. Applying this approach to interpreting chest x-rays, we set the state of the art on 9 abnormalities in the NIH's CXR14 dataset while generating saliency maps with the highest resolution to date.



### Generative Adversarial Talking Head: Bringing Portraits to Life with a Weakly Supervised Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1803.07716v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07716v2)
- **Published**: 2018-03-21 01:56:41+00:00
- **Updated**: 2018-03-28 04:42:06+00:00
- **Authors**: Hai X. Pham, Yuting Wang, Vladimir Pavlovic
- **Comment**: Fix typos, add youtube link of supplementary video
- **Journal**: None
- **Summary**: This paper presents Generative Adversarial Talking Head (GATH), a novel deep generative neural network that enables fully automatic facial expression synthesis of an arbitrary portrait with continuous action unit (AU) coefficients. Specifically, our model directly manipulates image pixels to make the unseen subject in the still photo express various emotions controlled by values of facial AU coefficients, while maintaining her personal characteristics, such as facial geometry, skin color and hair style, as well as the original surrounding background. In contrast to prior work, GATH is purely data-driven and it requires neither a statistical face model nor image processing tricks to enact facial deformations. Additionally, our model is trained from unpaired data, where the input image, with its auxiliary identity label taken from abundance of still photos in the wild, and the target frame are from different persons. In order to effectively learn such model, we propose a novel weakly supervised adversarial learning framework that consists of a generator, a discriminator, a classifier and an action unit estimator. Our work gives rise to template-and-target-free expression editing, where still faces can be effortlessly animated with arbitrary AU coefficients provided by the user.



### Modeling Camera Effects to Improve Visual Learning from Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/1803.07721v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07721v6)
- **Published**: 2018-03-21 02:36:11+00:00
- **Updated**: 2018-10-02 02:34:46+00:00
- **Authors**: Alexandra Carlson, Katherine A. Skinner, Ram Vasudevan, Matthew Johnson-Roberson
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work has focused on generating synthetic imagery to increase the size and variability of training data for learning visual tasks in urban scenes. This includes increasing the occurrence of occlusions or varying environmental and weather effects. However, few have addressed modeling variation in the sensor domain. Sensor effects can degrade real images, limiting generalizability of network performance on visual tasks trained on synthetic data and tested in real environments. This paper proposes an efficient, automatic, physically-based augmentation pipeline to vary sensor effects --chromatic aberration, blur, exposure, noise, and color cast-- for synthetic imagery. In particular, this paper illustrates that augmenting synthetic training datasets with the proposed pipeline reduces the domain gap between synthetic and real domains for the task of object detection in urban driving scenes.



### Attention on Attention: Architectures for Visual Question Answering (VQA)
- **Arxiv ID**: http://arxiv.org/abs/1803.07724v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, 68Txx
- **Links**: [PDF](http://arxiv.org/pdf/1803.07724v1)
- **Published**: 2018-03-21 03:05:58+00:00
- **Updated**: 2018-03-21 03:05:58+00:00
- **Authors**: Jasdeep Singh, Vincent Ying, Alex Nutkiewicz
- **Comment**: Visual Question Answering Project
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) is an increasingly popular topic in deep learning research, requiring coordination of natural language processing and computer vision modules into a single architecture. We build upon the model which placed first in the VQA Challenge by developing thirteen new attention mechanisms and introducing a simplified classifier. We performed 300 GPU hours of extensive hyperparameter and architecture searches and were able to achieve an evaluation score of 64.78%, outperforming the existing state-of-the-art single model's validation score of 63.15%.



### Unsupervised Representation Learning by Predicting Image Rotations
- **Arxiv ID**: http://arxiv.org/abs/1803.07728v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.07728v1)
- **Published**: 2018-03-21 03:21:14+00:00
- **Updated**: 2018-03-21 03:21:14+00:00
- **Authors**: Spyros Gidaris, Praveer Singh, Nikos Komodakis
- **Comment**: Accepted at ICLR2018. Code and models will be published on:
  https://github.com/gidariss/FeatureLearningRotNet
- **Journal**: None
- **Summary**: Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet .



### Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/1803.07729v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.07729v2)
- **Published**: 2018-03-21 03:21:38+00:00
- **Updated**: 2018-07-26 06:10:27+00:00
- **Authors**: Xin Wang, Wenhan Xiong, Hongmin Wang, William Yang Wang
- **Comment**: 21 pages, 7 figures, with supplementary material
- **Journal**: None
- **Summary**: Existing research studies on vision and language grounding for robot navigation focus on improving model-free deep reinforcement learning (DRL) models in synthetic environments. However, model-free DRL models do not consider the dynamics in the real-world environments, and they often fail to generalize to new scenes. In this paper, we take a radical approach to bridge the gap between synthetic studies and real-world practices---We propose a novel, planned-ahead hybrid reinforcement learning model that combines model-free and model-based reinforcement learning to solve a real-world vision-language navigation task. Our look-ahead module tightly integrates a look-ahead policy model with an environment model that predicts the next state and the reward. Experimental results suggest that our proposed method significantly outperforms the baselines and achieves the best on the real-world Room-to-Room dataset. Moreover, our scalable method is more generalizable when transferring to unseen environments.



### PyramidBox: A Context-assisted Single Shot Face Detector
- **Arxiv ID**: http://arxiv.org/abs/1803.07737v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07737v2)
- **Published**: 2018-03-21 03:50:53+00:00
- **Updated**: 2018-08-17 02:43:53+00:00
- **Authors**: Xu Tang, Daniel K. Du, Zeqiang He, Jingtuo Liu
- **Comment**: 21 pages, 12 figures
- **Journal**: ECCV2018
- **Summary**: Face detection has been well studied for many years and one of remaining challenges is to detect small, blurred and partially occluded faces in uncontrolled environment. This paper proposes a novel context-assisted single shot face detector, named \emph{PyramidBox} to handle the hard face detection problem. Observing the importance of the context, we improve the utilization of contextual information in the following three aspects. First, we design a novel context anchor to supervise high-level contextual feature learning by a semi-supervised method, which we call it PyramidAnchors. Second, we propose the Low-level Feature Pyramid Network to combine adequate high-level context semantic feature and Low-level facial feature together, which also allows the PyramidBox to predict faces of all scales in a single shot. Third, we introduce a context-sensitive structure to increase the capacity of prediction network to improve the final accuracy of output. In addition, we use the method of Data-anchor-sampling to augment the training samples across different scales, which increases the diversity of training data for smaller faces. By exploiting the value of context, PyramidBox achieves superior performance among the state-of-the-art over the two common face detection benchmarks, FDDB and WIDER FACE. Our code is available in PaddlePaddle: \href{https://github.com/PaddlePaddle/models/tree/develop/fluid/face_detection}{\url{https://github.com/PaddlePaddle/models/tree/develop/fluid/face_detection}}.



### Assessing Shape Bias Property of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.07739v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07739v1)
- **Published**: 2018-03-21 03:54:18+00:00
- **Updated**: 2018-03-21 03:54:18+00:00
- **Authors**: Hossein Hosseini, Baicen Xiao, Mayoore Jaiswal, Radha Poovendran
- **Comment**: None
- **Journal**: None
- **Summary**: It is known that humans display "shape bias" when classifying new items, i.e., they prefer to categorize objects based on their shape rather than color. Convolutional Neural Networks (CNNs) are also designed to take into account the spatial structure of image data. In fact, experiments on image datasets, consisting of triples of a probe image, a shape-match and a color-match, have shown that one-shot learning models display shape bias as well.   In this paper, we examine the shape bias property of CNNs. In order to conduct large scale experiments, we propose using the model accuracy on images with reversed brightness as a metric to evaluate the shape bias property. Such images, called negative images, contain objects that have the same shape as original images, but with different colors. Through extensive systematic experiments, we investigate the role of different factors, such as training data, model architecture, initialization and regularization techniques, on the shape bias property of CNNs. We show that it is possible to design different CNNs that achieve similar accuracy on original images, but perform significantly different on negative images, suggesting that CNNs do not intrinsically display shape bias. We then show that CNNs are able to learn and generalize the structures, when the model is properly initialized or data is properly augmented, and if batch normalization is used.



### Fast Semantic Segmentation on Video Using Block Motion-Based Feature Interpolation
- **Arxiv ID**: http://arxiv.org/abs/1803.07742v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07742v5)
- **Published**: 2018-03-21 04:05:45+00:00
- **Updated**: 2018-11-25 00:41:01+00:00
- **Authors**: Samvit Jain, Joseph E. Gonzalez
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Convolutional networks optimized for accuracy on challenging, dense prediction tasks are prohibitively slow to run on each frame in a video. The spatial similarity of nearby video frames, however, suggests opportunity to reuse computation. Existing work has explored basic feature reuse and feature warping based on optical flow, but has encountered limits to the speedup attainable with these techniques. In this paper, we present a new, two part approach to accelerating inference on video. First, we propose a fast feature propagation technique that utilizes the block motion vectors present in compressed video (e.g. H.264 codecs) to cheaply propagate features from frame to frame. Second, we develop a novel feature estimation scheme, termed feature interpolation, that fuses features propagated from enclosing keyframes to render accurate feature estimates, even at sparse keyframe frequencies. We evaluate our system on the Cityscapes and CamVid datasets, comparing to both a frame-by-frame baseline and related work. We find that we are able to substantially accelerate segmentation on video, achieving near real-time frame rates (20.1 frames per second) on large images (960 x 720 pixels), while maintaining competitive accuracy. This represents an improvement of almost 6x over the single-frame baseline and 2.5x over the fastest prior work.



### Learning and Recognizing Human Action from Skeleton Movement with Deep Residual Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.07780v1
- **DOI**: 10.1049/cp.2017.0154
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07780v1)
- **Published**: 2018-03-21 07:43:44+00:00
- **Updated**: 2018-03-21 07:43:44+00:00
- **Authors**: Huy-Hieu Pham, Louahdi Khoudour, Alain Crouzil, Pablo Zegers, Sergio A. Velastin
- **Comment**: The 8th International Conference of Pattern Recognition Systems
  (ICPRS 2017), Madrid, Spain
- **Journal**: None
- **Summary**: Automatic human action recognition is indispensable for almost artificial intelligent systems such as video surveillance, human-computer interfaces, video retrieval, etc. Despite a lot of progress, recognizing actions in an unknown video is still a challenging task in computer vision. Recently, deep learning algorithms have proved its great potential in many vision-related recognition tasks. In this paper, we propose the use of Deep Residual Neural Networks (ResNets) to learn and recognize human action from skeleton data provided by Kinect sensor. Firstly, the body joint coordinates are transformed into 3D-arrays and saved in RGB images space. Five different deep learning models based on ResNet have been designed to extract image features and classify them into classes. Experiments are conducted on two public video datasets for human action recognition containing various challenges. The results show that our method achieves the state-of-the-art performance comparing with existing approaches.



### Exploiting deep residual networks for human action recognition from skeletal data
- **Arxiv ID**: http://arxiv.org/abs/1803.07781v1
- **DOI**: 10.1016/j.cviu.2018.03.003
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07781v1)
- **Published**: 2018-03-21 07:43:53+00:00
- **Updated**: 2018-03-21 07:43:53+00:00
- **Authors**: Huy-Hieu Pham, Louahdi Khoudour, Alain Crouzil, Pablo Zegers, Sergio A. Velastin
- **Comment**: This version corresponds to the pre-print of the paper accepted for
  Computer Vision and Image Understanding (CVIU)
- **Journal**: None
- **Summary**: The computer vision community is currently focusing on solving action recognition problems in real videos, which contain thousands of samples with many challenges. In this process, Deep Convolutional Neural Networks (D-CNNs) have played a significant role in advancing the state-of-the-art in various vision-based action recognition systems. Recently, the introduction of residual connections in conjunction with a more traditional CNN model in a single architecture called Residual Network (ResNet) has shown impressive performance and great potential for image recognition tasks. In this paper, we investigate and apply deep ResNets for human action recognition using skeletal data provided by depth sensors. Firstly, the 3D coordinates of the human body joints carried in skeleton sequences are transformed into image-based representations and stored as RGB images. These color images are able to capture the spatial-temporal evolutions of 3D motions from skeleton sequences and can be efficiently learned by D-CNNs. We then propose a novel deep learning architecture based on ResNets to learn features from obtained color-based representations and classify them into action classes. The proposed method is evaluated on three challenging benchmark datasets including MSR Action 3D, KARD, and NTU-RGB+D datasets. Experimental results demonstrate that our method achieves state-of-the-art performance for all these benchmarks whilst requiring less computation resource. In particular, the proposed method surpasses previous approaches by a significant margin of 3.4% on MSR Action 3D dataset, 0.67% on KARD dataset, and 2.5% on NTU-RGB+D dataset.



### Domain Adaptation for Ear Recognition Using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.07801v1
- **DOI**: 10.1049/iet-bmt.2017.0209
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07801v1)
- **Published**: 2018-03-21 08:55:28+00:00
- **Updated**: 2018-03-21 08:55:28+00:00
- **Authors**: Fevziye Irem Eyiokur, Dogucan Yaman, Hazım Kemal Ekenel
- **Comment**: 12 pages, 7 figures, IET Biometrics
- **Journal**: None
- **Summary**: In this paper, we have extensively investigated the unconstrained ear recognition problem. We have first shown the importance of domain adaptation, when deep convolutional neural network models are used for ear recognition. To enable domain adaptation, we have collected a new ear dataset using the Multi-PIE face dataset, which we named as Multi-PIE ear dataset. To improve the performance further, we have combined different deep convolutional neural network models. We have analyzed in depth the effect of ear image quality, for example illumination and aspect ratio, on the classification performance. Finally, we have addressed the problem of dataset bias in the ear recognition field. Experiments on the UERC dataset have shown that domain adaptation leads to a significant performance improvement. For example, when VGG-16 model is used and the domain adaptation is applied, an absolute increase of around 10\% has been achieved. Combining different deep convolutional neural network models has further improved the accuracy by 4\%. It has also been observed that image quality has an influence on the results. In the experiments that we have conducted to examine the dataset bias, given an ear image, we were able to classify the dataset that it has come from with 99.71\% accuracy, which indicates a strong bias among the ear recognition datasets.



### Patch-based Fake Fingerprint Detection Using a Fully Convolutional Neural Network with a Small Number of Parameters and an Optimal Threshold
- **Arxiv ID**: http://arxiv.org/abs/1803.07817v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07817v1)
- **Published**: 2018-03-21 09:50:25+00:00
- **Updated**: 2018-03-21 09:50:25+00:00
- **Authors**: Eunsoo Park, Xuenan Cui, Weonjin Kim, Jinsong Liu, Hakil Kim
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Fingerprint authentication is widely used in biometrics due to its simple process, but it is vulnerable to fake fingerprints. This study proposes a patch-based fake fingerprint detection method using a fully convolutional neural network with a small number of parameters and an optimal threshold to solve the above-mentioned problem. Unlike the existing methods that classify a fingerprint as live or fake, the proposed method classifies fingerprints as fake, live, or background, so preprocessing methods such as segmentation are not needed. The proposed convolutional neural network (CNN) structure applies the Fire module of SqueezeNet, and the fewer parameters used require only 2.0 MB of memory. The network that has completed training is applied to the training data in a fully convolutional way, and the optimal threshold to distinguish fake fingerprints is determined, which is used in the final test. As a result of this study experiment, the proposed method showed an average classification error of 1.35%, demonstrating a fake fingerprint detection method using a high-performance CNN with a small number of parameters.



### End-to-End Fingerprints Liveness Detection using Convolutional Networks with Gram module
- **Arxiv ID**: http://arxiv.org/abs/1803.07830v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07830v1)
- **Published**: 2018-03-21 10:11:27+00:00
- **Updated**: 2018-03-21 10:11:27+00:00
- **Authors**: Eunsoo Park, Xuenan Cui, Weonjin Kim, Hakil Kim
- **Comment**: 15 pages, 7 figures
- **Journal**: None
- **Summary**: This paper proposes an end-to-end CNN(Convolutional Neural Networks) model that uses gram modules with parameters that are approximately 1.2MB in size to detect fake fingerprints. The proposed method assumes that texture is the most appropriate characteristic in fake fingerprint detection, and implements the gram module to extract textures from the CNN. The proposed CNN structure uses the fire module as the base model and uses the gram module for texture extraction. Tensors that passed the fire module will be joined with gram modules to create a gram matrix with the same spatial size. After 3 gram matrices extracted from different layers are combined with the channel axis, it becomes the basis for categorizing fake fingerprints. The experiment results had an average detection error of 2.61% from the LivDet 2011, 2013, 2015 data, proving that an end-to-end CNN structure with few parameters that is able to be used in fake fingerprint detection can be designed.



### Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network
- **Arxiv ID**: http://arxiv.org/abs/1803.07835v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1803.07835v1)
- **Published**: 2018-03-21 10:27:04+00:00
- **Updated**: 2018-03-21 10:27:04+00:00
- **Authors**: Yao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, Xi Zhou
- **Comment**: 18 pages, 10 figures
- **Journal**: None
- **Summary**: We propose a straightforward method that simultaneously reconstructs the 3D facial structure and provides dense alignment. To achieve this, we design a 2D representation called UV position map which records the 3D shape of a complete face in UV space, then train a simple Convolutional Neural Network to regress it from a single 2D image. We also integrate a weight mask into the loss function during training to improve the performance of the network. Our method does not rely on any prior face model, and can reconstruct full facial geometry along with semantic meaning. Meanwhile, our network is very light-weighted and spends only 9.8ms to process an image, which is extremely faster than previous works. Experiments on multiple challenging datasets show that our method surpasses other state-of-the-art methods on both reconstruction and alignment tasks by a large margin.



### HATS: Histograms of Averaged Time Surfaces for Robust Event-based Object Classification
- **Arxiv ID**: http://arxiv.org/abs/1803.07913v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07913v1)
- **Published**: 2018-03-21 13:37:24+00:00
- **Updated**: 2018-03-21 13:37:24+00:00
- **Authors**: Amos Sironi, Manuele Brambilla, Nicolas Bourdis, Xavier Lagorce, Ryad Benosman
- **Comment**: Accepted for publication at CVPR2018. Dataset available at
  http://www.prophesee.ai/dataset-n-cars/
- **Journal**: None
- **Summary**: Event-based cameras have recently drawn the attention of the Computer Vision community thanks to their advantages in terms of high temporal resolution, low power consumption and high dynamic range, compared to traditional frame-based cameras. These properties make event-based cameras an ideal choice for autonomous vehicles, robot navigation or UAV vision, among others. However, the accuracy of event-based object classification algorithms, which is of crucial importance for any reliable system working in real-world conditions, is still far behind their frame-based counterparts. Two main reasons for this performance gap are: 1. The lack of effective low-level representations and architectures for event-based object classification and 2. The absence of large real-world event-based datasets. In this paper we address both problems. First, we introduce a novel event-based feature representation together with a new machine learning architecture. Compared to previous approaches, we use local memory units to efficiently leverage past temporal information and build a robust event-based representation. Second, we release the first large real-world event-based dataset for object classification. We compare our method to the state-of-the-art with extensive experiments, showing better classification performance and real-time computation.



### Modelling the Influence of Cultural Information on Vision-Based Human Home Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.07915v1
- **DOI**: 10.1109/URAI.2017.7992880
- **Categories**: **cs.CV**, cs.CY, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.07915v1)
- **Published**: 2018-03-21 13:41:10+00:00
- **Updated**: 2018-03-21 13:41:10+00:00
- **Authors**: Roberto Menicatti, Barbara Bruno, Antonio Sgorbissa
- **Comment**: 7 pages, 4 figures, Proc. URAI2017, International Conference on
  Ubiquitous Robots and Ambient Intelligence, Maison Glad Jeju, Jeju, Korea
  from June 28-July 2017
- **Journal**: Proc. URAI2017, International Conference on Ubiquitous Robots and
  Ambient Intelligence, Maison Glad Jeju, Jeju, Korea from June 28-July 2017
- **Summary**: Daily life activities, such as eating and sleeping, are deeply influenced by a person's culture, hence generating differences in the way a same activity is performed by individuals belonging to different cultures. We argue that taking cultural information into account can improve the performance of systems for the automated recognition of human activities. We propose four different solutions to the problem and present a system which uses a Naive Bayes model to associate cultural information with semantic information extracted from still images. Preliminary experiments with a dataset of images of individuals lying on the floor, sleeping on a futon and sleeping on a bed suggest that: i) solutions explicitly taking cultural information into account are more accurate than culture-unaware solutions; and ii) the proposed system is a promising starting point for the development of culture-aware Human Activity Recognition methods.



### End-to-End Video Captioning with Multitask Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1803.07950v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07950v2)
- **Published**: 2018-03-21 14:51:17+00:00
- **Updated**: 2019-01-01 10:40:07+00:00
- **Authors**: Lijun Li, Boqing Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Although end-to-end (E2E) learning has led to impressive progress on a variety of visual understanding tasks, it is often impeded by hardware constraints (e.g., GPU memory) and is prone to overfitting. When it comes to video captioning, one of the most challenging benchmark tasks in computer vision, those limitations of E2E learning are especially amplified by the fact that both the input videos and output captions are lengthy sequences. Indeed, state-of-the-art methods for video captioning process video frames by convolutional neural networks and generate captions by unrolling recurrent neural networks. If we connect them in an E2E manner, the resulting model is both memory-consuming and data-hungry, making it extremely hard to train. In this paper, we propose a multitask reinforcement learning approach to training an E2E video captioning model. The main idea is to mine and construct as many effective tasks (e.g., attributes, rewards, and the captions) as possible from the human captioned videos such that they can jointly regulate the search space of the E2E neural network, from which an E2E video captioning model can be found and generalized to the testing phase. To the best of our knowledge, this is the first video captioning model that is trained end-to-end from the raw video input to the caption output. Experimental results show that such a model outperforms existing ones to a large margin on two benchmark video captioning datasets.



### A Cascaded Convolutional Neural Network for Single Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/1803.07955v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07955v1)
- **Published**: 2018-03-21 15:01:21+00:00
- **Updated**: 2018-03-21 15:01:21+00:00
- **Authors**: Chongyi Li, Jichang Guo, Fatih Porikli, Huazhu Fu, Yanwei Pang
- **Comment**: This manuscript is accepted by IEEE ACCESS
- **Journal**: None
- **Summary**: Images captured under outdoor scenes usually suffer from low contrast and limited visibility due to suspended atmospheric particles, which directly affects the quality of photos. Despite numerous image dehazing methods have been proposed, effective hazy image restoration remains a challenging problem. Existing learning-based methods usually predict the medium transmission by Convolutional Neural Networks (CNNs), but ignore the key global atmospheric light. Different from previous learning-based methods, we propose a flexible cascaded CNN for single hazy image restoration, which considers the medium transmission and global atmospheric light jointly by two task-driven subnetworks. Specifically, the medium transmission estimation subnetwork is inspired by the densely connected CNN while the global atmospheric light estimation subnetwork is a light-weight CNN. Besides, these two subnetworks are cascaded by sharing the common features. Finally, with the estimated model parameters, the haze-free image is obtained by the atmospheric scattering model inversion, which achieves more accurate and effective restoration performance. Qualitatively and quantitatively experimental results on the synthetic and real-world hazy images demonstrate that the proposed method effectively removes haze from such images, and outperforms several state-of-the-art dehazing methods.



### Non-rigid 3D Shape Registration using an Adaptive Template
- **Arxiv ID**: http://arxiv.org/abs/1803.07973v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07973v2)
- **Published**: 2018-03-21 15:44:36+00:00
- **Updated**: 2018-09-22 18:07:12+00:00
- **Authors**: Hang Dai, Nick Pears, William Smith
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new fully-automatic non-rigid 3D shape registration (morphing) framework comprising (1) a new 3D landmarking and pose normalisation method; (2) an adaptive shape template method to accelerate the convergence of registration algorithms and achieve a better final shape correspondence and (3) a new iterative registration method that combines Iterative Closest Points with Coherent Point Drift (CPD) to achieve a more stable and accurate correspondence establishment than standard CPD. We call this new morphing approach Iterative Coherent Point Drift (ICPD). Our proposed framework is evaluated qualitatively and quantitatively on three datasets and compared with several other methods. The proposed framework is shown to give state-of-the-art performance.



### BioTracker: An Open-Source Computer Vision Framework for Visual Animal Tracking
- **Arxiv ID**: http://arxiv.org/abs/1803.07985v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07985v1)
- **Published**: 2018-03-21 16:12:18+00:00
- **Updated**: 2018-03-21 16:12:18+00:00
- **Authors**: Hauke Jürgen Mönck, Andreas Jörg, Tobias von Falkenhausen, Julian Tanke, Benjamin Wild, David Dormagen, Jonas Piotrowski, Claudia Winklmayr, David Bierbach, Tim Landgraf
- **Comment**: 4 pages, 2 figures
- **Journal**: None
- **Summary**: The study of animal behavior increasingly relies on (semi-) automatic methods for the extraction of relevant behavioral features from video or picture data. To date, several specialized software products exist to detect and track animals' positions in simple (laboratory) environments. Tracking animals in their natural environments, however, often requires substantial customization of the image processing algorithms to the problem-specific image characteristics. Here we introduce BioTracker, an open-source computer vision framework, that provides programmers with core functionalities that are essential parts of a tracking software, such as video I/O, graphics overlays and mouse and keyboard interfaces. BioTracker additionally provides a number of different tracking algorithms suitable for a variety of image recording conditions. The main feature of BioTracker is however the straightforward implementation of new problem-specific tracking modules and vision algorithms that can build upon BioTracker's core functionalities. With this open-source framework the scientific community can accelerate their research and focus on the development of new vision algorithms.



### Quantification of Lung Abnormalities in Cystic Fibrosis using Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.07991v1
- **DOI**: 10.1117/12.2292188
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07991v1)
- **Published**: 2018-03-21 16:18:46+00:00
- **Updated**: 2018-03-21 16:18:46+00:00
- **Authors**: Filipe Marques, Florian Dubost, Mariette Kemner-van de Corput, Harm A. W. Tiddens, Marleen de Bruijne
- **Comment**: SPIE - Medical Imaging 2018: Image Processing
- **Journal**: Proc. SPIE 10574, Medical Imaging 2018: Image Processing, 105741G
  (2 March 2018)
- **Summary**: Cystic fibrosis is a genetic disease which may appear in early life with structural abnormalities in lung tissues. We propose to detect these abnormalities using a texture classification approach. Our method is a cascade of two convolutional neural networks. The first network detects the presence of abnormal tissues. The second network identifies the type of the structural abnormalities: bronchiectasis, atelectasis or mucus plugging.We also propose a network computing pixel-wise heatmaps of abnormality presence learning only from the patch-wise annotations. Our database consists of CT scans of 194 subjects. We use 154 subjects to train our algorithms and the 40 remaining ones as a test set. We compare our method with random forest and a single neural network approach. The first network reaches an accuracy of 0,94 for disease detection, 0,18 higher than the random forest classifier and 0,37 higher than the single neural network. Our cascade approach yields a final class-averaged F1-score of 0,33, outperforming the baseline method and the single network by 0,10 and 0,12.



### Adversarial Defense based on Structure-to-Signal Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1803.07994v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.07994v1)
- **Published**: 2018-03-21 16:25:26+00:00
- **Updated**: 2018-03-21 16:25:26+00:00
- **Authors**: Joachim Folz, Sebastian Palacio, Joern Hees, Damian Borth, Andreas Dengel
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial attack methods have demonstrated the fragility of deep neural networks. Their imperceptible perturbations are frequently able fool classifiers into potentially dangerous misclassifications. We propose a novel way to interpret adversarial perturbations in terms of the effective input signal that classifiers actually use. Based on this, we apply specially trained autoencoders, referred to as S2SNets, as defense mechanism. They follow a two-stage training scheme: first unsupervised, followed by a fine-tuning of the decoder, using gradients from an existing classifier. S2SNets induce a shift in the distribution of gradients propagated through them, stripping them from class-dependent signal. We analyze their robustness against several white-box and gray-box scenarios on the large ImageNet dataset. Our approach reaches comparable resilience in white-box attack scenarios as other state-of-the-art defenses in gray-box scenarios. We further analyze the relationships of AlexNet, VGG 16, ResNet 50 and Inception v3 in adversarial space, and found that VGG 16 is the easiest to fool, while perturbations from ResNet 50 are the most transferable.



### Video Object Segmentation with Language Referring Expressions
- **Arxiv ID**: http://arxiv.org/abs/1803.08006v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08006v3)
- **Published**: 2018-03-21 16:44:19+00:00
- **Updated**: 2019-02-05 10:15:25+00:00
- **Authors**: Anna Khoreva, Anna Rohrbach, Bernt Schiele
- **Comment**: ACCV 2018: 14th Asian Conference on Computer Vision
- **Journal**: None
- **Summary**: Most state-of-the-art semi-supervised video object segmentation methods rely on a pixel-accurate mask of a target object provided for the first frame of a video. However, obtaining a detailed segmentation mask is expensive and time-consuming. In this work we explore an alternative way of identifying a target object, namely by employing language referring expressions. Besides being a more practical and natural way of pointing out a target object, using language specifications can help to avoid drift as well as make the system more robust to complex dynamics and appearance variations. Leveraging recent advances of language grounding models designed for images, we propose an approach to extend them to video data, ensuring temporally coherent predictions. To evaluate our method we augment the popular video object segmentation benchmarks, DAVIS'16 and DAVIS'17 with language descriptions of target objects. We show that our language-supervised approach performs on par with the methods which have access to a pixel-level mask of the target object on DAVIS'16 and is competitive to methods using scribbles on the challenging DAVIS'17 dataset.



### Monocular Depth Estimation by Learning from Heterogeneous Datasets
- **Arxiv ID**: http://arxiv.org/abs/1803.08018v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.08018v2)
- **Published**: 2018-03-21 17:18:25+00:00
- **Updated**: 2018-09-12 17:40:58+00:00
- **Authors**: Akhil Gurram, Onay Urfalioglu, Ibrahim Halfaoui, Fahd Bouzaraa, Antonio M. Lopez
- **Comment**: Accepted in IEEE-Intelligent Vehicles Symposium, IV'2018
- **Journal**: None
- **Summary**: Depth estimation provides essential information to perform autonomous driving and driver assistance. Especially, Monocular Depth Estimation is interesting from a practical point of view, since using a single camera is cheaper than many other options and avoids the need for continuous calibration strategies as required by stereo-vision approaches. State-of-the-art methods for Monocular Depth Estimation are based on Convolutional Neural Networks (CNNs). A promising line of work consists of introducing additional semantic information about the traffic scene when training CNNs for depth estimation. In practice, this means that the depth data used for CNN training is complemented with images having pixel-wise semantic labels, which usually are difficult to annotate (e.g. crowded urban images). Moreover, so far it is common practice to assume that the same raw training data is associated with both types of ground truth, i.e., depth and semantic labels. The main contribution of this paper is to show that this hard constraint can be circumvented, i.e., that we can train CNNs for depth estimation by leveraging the depth and semantic information coming from heterogeneous datasets. In order to illustrate the benefits of our approach, we combine KITTI depth and Cityscapes semantic segmentation datasets, outperforming state-of-the-art results on Monocular Depth Estimation.



### Stacked Cross Attention for Image-Text Matching
- **Arxiv ID**: http://arxiv.org/abs/1803.08024v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.08024v2)
- **Published**: 2018-03-21 17:22:27+00:00
- **Updated**: 2018-07-23 04:41:57+00:00
- **Authors**: Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, Xiaodong He
- **Comment**: Accepted to ECCV 2018
- **Journal**: None
- **Summary**: In this paper, we study the problem of image-text matching. Inferring the latent semantic alignment between objects or other salient stuff (e.g. snow, sky, lawn) and the corresponding words in sentences allows to capture fine-grained interplay between vision and language, and makes image-text matching more interpretable. Prior work either simply aggregates the similarity of all possible pairs of regions and words without attending differentially to more and less important words or regions, or uses a multi-step attentional process to capture limited number of semantic alignments which is less interpretable. In this paper, we present Stacked Cross Attention to discover the full latent alignments using both image regions and words in a sentence as context and infer image-text similarity. Our approach achieves the state-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K, our approach outperforms the current best methods by 22.1% relatively in text retrieval from image query, and 18.2% relatively in image retrieval with text query (based on Recall@1). On MS-COCO, our approach improves sentence retrieval by 17.8% relatively and image retrieval by 16.6% relatively (based on Recall@1 using the 5K test set). Code has been made available at: https://github.com/kuanghuei/SCAN.



### Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs
- **Arxiv ID**: http://arxiv.org/abs/1803.08035v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1803.08035v2)
- **Published**: 2018-03-21 17:52:42+00:00
- **Updated**: 2018-04-08 18:53:39+00:00
- **Authors**: Xiaolong Wang, Yufei Ye, Abhinav Gupta
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: We consider the problem of zero-shot recognition: learning a visual classifier for a category with zero training examples, just using the word embedding of the category and its relationship to other categories, which visual data are provided. The key to dealing with the unfamiliar or novel category is to transfer knowledge obtained from familiar classes to describe the unfamiliar class. In this paper, we build upon the recently introduced Graph Convolutional Network (GCN) and propose an approach that uses both semantic embeddings and the categorical relationships to predict the classifiers. Given a learned knowledge graph (KG), our approach takes as input semantic embeddings for each node (representing visual category). After a series of graph convolutions, we predict the visual classifier for each category. During training, the visual classifiers for a few categories are given to learn the GCN parameters. At test time, these filters are used to predict the visual classifiers of unseen categories. We show that our approach is robust to noise in the KG. More importantly, our approach provides significant improvement in performance compared to the current state-of-the-art results (from 2 ~ 3% on some metrics to whopping 20% on a few).



### Eigendecomposition-free Training of Deep Networks with Zero Eigenvalue-based Losses
- **Arxiv ID**: http://arxiv.org/abs/1803.08071v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08071v2)
- **Published**: 2018-03-21 18:13:54+00:00
- **Updated**: 2018-03-26 12:36:08+00:00
- **Authors**: Zheng Dang, Kwang Moo Yi, Yinlin Hu, Fei Wang, Pascal Fua, Mathieu Salzmann
- **Comment**: 25 pages
- **Journal**: None
- **Summary**: Many classical Computer Vision problems, such as essential matrix computation and pose estimation from 3D to 2D correspondences, can be solved by finding the eigenvector corresponding to the smallest, or zero, eigenvalue of a matrix representing a linear system. Incorporating this in deep learning frameworks would allow us to explicitly encode known notions of geometry, instead of having the network implicitly learn them from data. However, performing eigendecomposition within a network requires the ability to differentiate this operation. Unfortunately, while theoretically doable, this introduces numerical instability in the optimization process in practice.   In this paper, we introduce an eigendecomposition-free approach to training a deep network whose loss depends on the eigenvector corresponding to a zero eigenvalue of a matrix predicted by the network. We demonstrate on several tasks, including keypoint matching and 3D pose estimation, that our approach is much more robust than explicit differentiation of the eigendecomposition, It has better convergence properties and yields state-of-the-art results on both tasks.



### Probabilistic Video Generation using Holistic Attribute Control
- **Arxiv ID**: http://arxiv.org/abs/1803.08085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08085v1)
- **Published**: 2018-03-21 18:39:47+00:00
- **Updated**: 2018-03-21 18:39:47+00:00
- **Authors**: Jiawei He, Andreas Lehrmann, Joseph Marino, Greg Mori, Leonid Sigal
- **Comment**: None
- **Journal**: None
- **Summary**: Videos express highly structured spatio-temporal patterns of visual data. A video can be thought of as being governed by two factors: (i) temporally invariant (e.g., person identity), or slowly varying (e.g., activity), attribute-induced appearance, encoding the persistent content of each frame, and (ii) an inter-frame motion or scene dynamics (e.g., encoding evolution of the person ex-ecuting the action). Based on this intuition, we propose a generative framework for video generation and future prediction. The proposed framework generates a video (short clip) by decoding samples sequentially drawn from a latent space distribution into full video frames. Variational Autoencoders (VAEs) are used as a means of encoding/decoding frames into/from the latent space and RNN as a wayto model the dynamics in the latent space. We improve the video generation consistency through temporally-conditional sampling and quality by structuring the latent space with attribute controls; ensuring that attributes can be both inferred and conditioned on during learning/generation. As a result, given attributes and/orthe first frame, our model is able to generate diverse but highly consistent sets ofvideo sequences, accounting for the inherent uncertainty in the prediction task. Experimental results on Chair CAD, Weizmann Human Action, and MIT-Flickr datasets, along with detailed comparison to the state-of-the-art, verify effectiveness of the framework.



### T-RECS: Training for Rate-Invariant Embeddings by Controlling Speed for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.08094v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08094v2)
- **Published**: 2018-03-21 19:05:15+00:00
- **Updated**: 2018-03-23 04:11:36+00:00
- **Authors**: Madan Ravi Ganesh, Eric Hofesmann, Byungsu Min, Nadha Gafoor, Jason J. Corso
- **Comment**: None
- **Journal**: None
- **Summary**: An action should remain identifiable when modifying its speed: consider the contrast between an expert chef and a novice chef each chopping an onion. Here, we expect the novice chef to have a relatively measured and slow approach to chopping when compared to the expert. In general, the speed at which actions are performed, whether slower or faster than average, should not dictate how they are recognized. We explore the erratic behavior caused by this phenomena on state-of-the-art deep network-based methods for action recognition in terms of maximum performance and stability in recognition accuracy across a range of input video speeds. By observing the trends in these metrics and summarizing them based on expected temporal behaviour w.r.t. variations in input video speeds, we find two distinct types of network architectures. In this paper, we propose a preprocessing method named T-RECS, as a way to extend deep-network-based methods for action recognition to explicitly account for speed variability in the data. We do so by adaptively resampling the inputs to a given model. T-RECS is agnostic to the specific deep-network model; we apply it to four state-of-the-art action recognition architectures, C3D, I3D, TSN, and ConvNet+LSTM. On HMDB51 and UCF101, T-RECS-based I3D models show a peak improvement of at least 2.9% in performance over the baseline while T-RECS-based C3D models achieve a maximum improvement in stability by 59% over the baseline, on the HMDB51 dataset.



### A Unified Framework for Multi-View Multi-Class Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1803.08103v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08103v2)
- **Published**: 2018-03-21 19:41:33+00:00
- **Updated**: 2018-10-06 16:57:11+00:00
- **Authors**: Chi Li, Jin Bai, Gregory D. Hager
- **Comment**: Accepted in ECCV2018
- **Journal**: None
- **Summary**: One core challenge in object pose estimation is to ensure accurate and robust performance for large numbers of diverse foreground objects amidst complex background clutter. In this work, we present a scalable framework for accurately inferring six Degree-of-Freedom (6-DoF) pose for a large number of object classes from single or multiple views. To learn discriminative pose features, we integrate three new capabilities into a deep Convolutional Neural Network (CNN): an inference scheme that combines both classification and pose regression based on a uniform tessellation of the Special Euclidean group in three dimensions (SE(3)), the fusion of class priors into the training process via a tiled class map, and an additional regularization using deep supervision with an object mask. Further, an efficient multi-view framework is formulated to address single-view ambiguity. We show that this framework consistently improves the performance of the single-view network. We evaluate our method on three large-scale benchmarks: YCB-Video, JHUScene-50 and ObjectNet-3D. Our approach achieves competitive or superior performance over the current state-of-the-art methods.



### Task dependent Deep LDA pruning of neural networks
- **Arxiv ID**: http://arxiv.org/abs/1803.08134v6
- **DOI**: 10.1016/j.cviu.2020.103154
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08134v6)
- **Published**: 2018-03-21 20:52:32+00:00
- **Updated**: 2020-11-24 05:56:56+00:00
- **Authors**: Qing Tian, Tal Arbel, James J. Clark
- **Comment**: None
- **Journal**: Computer Vision and Image Understanding 203C (2021) 103154
- **Summary**: With deep learning's success, a limited number of popular deep nets have been widely adopted for various vision tasks. However, this usually results in unnecessarily high complexities and possibly many features of low task utility. In this paper, we address this problem by introducing a task-dependent deep pruning framework based on Fisher's Linear Discriminant Analysis (LDA). The approach can be applied to convolutional, fully-connected, and module-based deep network structures, in all cases leveraging the high decorrelation of neuron motifs found in the pre-decision space and cross-layer deconv dependency. Moreover, we examine our approach's potential in network architecture search for specific tasks and analyze the influence of our pruning on model robustness to noises and adversarial attacks. Experimental results on datasets of generic objects (ImageNet, CIFAR100) as well as domain specific tasks (Adience, and LFWA) illustrate our framework's superior performance over state-of-the-art pruning approaches and fixed compact nets (e.g. SqueezeNet, MobileNet). The proposed method successfully maintains comparable accuracies even after discarding most parameters (98%-99% for VGG16, up to 82% for the already compact InceptionNet) and with significant FLOP reductions (83% for VGG16, up to 64% for InceptionNet). Through pruning, we can also derive smaller, but more accurate and more robust models suitable for the task.



### Robust Blind Deconvolution via Mirror Descent
- **Arxiv ID**: http://arxiv.org/abs/1803.08137v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.08137v1)
- **Published**: 2018-03-21 20:55:26+00:00
- **Updated**: 2018-03-21 20:55:26+00:00
- **Authors**: Sathya N. Ravi, Ronak Mehta, Vikas Singh
- **Comment**: None
- **Journal**: None
- **Summary**: We revisit the Blind Deconvolution problem with a focus on understanding its robustness and convergence properties. Provable robustness to noise and other perturbations is receiving recent interest in vision, from obtaining immunity to adversarial attacks to assessing and describing failure modes of algorithms in mission critical applications. Further, many blind deconvolution methods based on deep architectures internally make use of or optimize the basic formulation, so a clearer understanding of how this sub-module behaves, when it can be solved, and what noise injection it can tolerate is a first order requirement. We derive new insights into the theoretical underpinnings of blind deconvolution. The algorithm that emerges has nice convergence guarantees and is provably robust in a sense we formalize in the paper. Interestingly, these technical results play out very well in practice, where on standard datasets our algorithm yields results competitive with or superior to the state of the art. Keywords: blind deconvolution, robust continuous optimization



### Extended depth-of-field in holographic image reconstruction using deep learning based auto-focusing and phase-recovery
- **Arxiv ID**: http://arxiv.org/abs/1803.08138v1
- **DOI**: 10.1364/OPTICA.5.000704
- **Categories**: **cs.CV**, cs.LG, physics.optics, 68T01, 68T05, 68U10, 62M45, 78M32, 92C55, 94A08, I.2, I.2.1, I.2.6, I.2.10, I.4.5, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1803.08138v1)
- **Published**: 2018-03-21 20:59:33+00:00
- **Updated**: 2018-03-21 20:59:33+00:00
- **Authors**: Yichen Wu, Yair Rivenson, Yibo Zhang, Zhensong Wei, Harun Gunaydin, Xing Lin, Aydogan Ozcan
- **Comment**: None
- **Journal**: Optica, Vol. 5, Issue 6, pp. 704-710 (2018)
- **Summary**: Holography encodes the three dimensional (3D) information of a sample in the form of an intensity-only recording. However, to decode the original sample image from its hologram(s), auto-focusing and phase-recovery are needed, which are in general cumbersome and time-consuming to digitally perform. Here we demonstrate a convolutional neural network (CNN) based approach that simultaneously performs auto-focusing and phase-recovery to significantly extend the depth-of-field (DOF) in holographic image reconstruction. For this, a CNN is trained by using pairs of randomly de-focused back-propagated holograms and their corresponding in-focus phase-recovered images. After this training phase, the CNN takes a single back-propagated hologram of a 3D sample as input to rapidly achieve phase-recovery and reconstruct an in focus image of the sample over a significantly extended DOF. This deep learning based DOF extension method is non-iterative, and significantly improves the algorithm time-complexity of holographic image reconstruction from O(nm) to O(1), where n refers to the number of individual object points or particles within the sample volume, and m represents the focusing search space within which each object point or particle needs to be individually focused. These results highlight some of the unique opportunities created by data-enabled statistical image reconstruction methods powered by machine learning, and we believe that the presented approach can be broadly applicable to computationally extend the DOF of other imaging modalities.



