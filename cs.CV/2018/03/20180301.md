# Arxiv Papers in cs.CV on 2018-03-01
### A Class-Incremental Learning Method Based on One Class Support Vector Machine
- **Arxiv ID**: http://arxiv.org/abs/1803.00159v1
- **DOI**: 10.1088/1742-6596/1267/1/012007
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00159v1)
- **Published**: 2018-03-01 01:50:42+00:00
- **Updated**: 2018-03-01 01:50:42+00:00
- **Authors**: Chengfei Yao, Jie Zou, Yanan Luo, Tao Li, Gang Bai
- **Comment**: 6 pages, 4 figures, Under review as a conference paper at
  International Conference on Pattern Recognition 2018
- **Journal**: None
- **Summary**: A method based on one class support vector machine (OCSVM) is proposed for class incremental learning. Several OCSVM models divide the input space into several parts. Then, the 1VS1 classifiers are constructed for the confuse part by using the support vectors. During the class incremental learning process, the OCSVM of the new class is trained at first. Then the support vectors of the old classes and the support vectors of the new class are reused to train 1VS1 classifiers for the confuse part. In order to bring more information to the certain support vectors, the support vectors are at the boundary of the distribution of samples as much as possible when the OCSVM is built. Compared with the traditional methods, the proposed method retains the original model and thus reduces memory consumption and training time cost. Various experiments on different datasets also verify the efficiency of the proposed method.



### Facial Expression Recognition Based on Complexity Perception Classification Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1803.00185v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1803.00185v1)
- **Published**: 2018-03-01 03:05:50+00:00
- **Updated**: 2018-03-01 03:05:50+00:00
- **Authors**: Tianyuan Chang, Guihua Wen, Yang Hu, JiaJiong Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Facial expression recognition (FER) has always been a challenging issue in computer vision. The different expressions of emotion and uncontrolled environmental factors lead to inconsistencies in the complexity of FER and variability of between expression categories, which is often overlooked in most facial expression recognition systems. In order to solve this problem effectively, we presented a simple and efficient CNN model to extract facial features, and proposed a complexity perception classification (CPC) algorithm for FER. The CPC algorithm divided the dataset into an easy classification sample subspace and a complex classification sample subspace by evaluating the complexity of facial features that are suitable for classification. The experimental results of our proposed algorithm on Fer2013 and CK-plus datasets demonstrated the algorithm's effectiveness and superiority over other state-of-the-art approaches.



### Temporally Identity-Aware SSD with Attentional LSTM
- **Arxiv ID**: http://arxiv.org/abs/1803.00197v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.00197v4)
- **Published**: 2018-03-01 03:48:07+00:00
- **Updated**: 2020-03-25 08:14:32+00:00
- **Authors**: Xingyu Chen, Junzhi Yu, Zhengxing Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal object detection has attracted significant attention, but most popular detection methods cannot leverage rich temporal information in videos. Very recently, many algorithms have been developed for video detection task, yet very few approaches can achieve \emph{real-time online} object detection in videos. In this paper, based on attention mechanism and convolutional long short-term memory (ConvLSTM), we propose a temporal single-shot detector (TSSD) for real-world detection. Distinct from previous methods, we take aim at temporally integrating pyramidal feature hierarchy using ConvLSTM, and design a novel structure including a low-level temporal unit as well as a high-level one (LH-TU) for multi-scale feature maps. Moreover, we develop a creative temporal analysis unit, namely, attentional ConvLSTM (AC-LSTM), in which a temporal attention mechanism is specially tailored for background suppression and scale suppression while a ConvLSTM integrates attention-aware features across time. An association loss and a multi-step training are designed for temporal coherence. Besides, an online tubelet analysis (OTA) is exploited for identification. Our framework is evaluated on ImageNet VID dataset and 2DMOT15 dataset. Extensive comparisons on the detection and tracking capability validate the superiority of the proposed approach. Consequently, the developed TSSD-OTA achieves a fast speed and an overall competitive performance in terms of detection and tracking. Finally, a real-world maneuver is conducted for underwater object grasping. The source code is publicly available at https://github.com/SeanChenxy/TSSD-OTA.



### Multi-Instance Dynamic Ordinal Random Fields for Weakly-supervised Facial Behavior Analysis
- **Arxiv ID**: http://arxiv.org/abs/1803.00907v1
- **DOI**: 10.1109/TIP.2018.2830189
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1803.00907v1)
- **Published**: 2018-03-01 04:13:47+00:00
- **Updated**: 2018-03-01 04:13:47+00:00
- **Authors**: Adria Ruiz, Ognjen Rudovic, Xavier Binefa, Maja Pantic
- **Comment**: submitted TIP (June 2017). arXiv admin note: text overlap with
  arXiv:1609.01465
- **Journal**: None
- **Summary**: We propose a Multi-Instance-Learning (MIL) approach for weakly-supervised learning problems, where a training set is formed by bags (sets of feature vectors or instances) and only labels at bag-level are provided. Specifically, we consider the Multi-Instance Dynamic-Ordinal-Regression (MI-DOR) setting, where the instance labels are naturally represented as ordinal variables and bags are structured as temporal sequences. To this end, we propose Multi-Instance Dynamic Ordinal Random Fields (MI-DORF). In this framework, we treat instance-labels as temporally-dependent latent variables in an Undirected Graphical Model. Different MIL assumptions are modelled via newly introduced high-order potentials relating bag and instance-labels within the energy function of the model. We also extend our framework to address the Partially-Observed MI-DOR problems, where a subset of instance labels are available during training. We show on the tasks of weakly-supervised facial behavior analysis, Facial Action Unit (DISFA dataset) and Pain (UNBC dataset) Intensity estimation, that the proposed framework outperforms alternative learning approaches. Furthermore, we show that MIDORF can be employed to reduce the data annotation efforts in this context by large-scale.



### Tongue image constitution recognition based on Complexity Perception method
- **Arxiv ID**: http://arxiv.org/abs/1803.00219v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1803.00219v1)
- **Published**: 2018-03-01 05:32:43+00:00
- **Updated**: 2018-03-01 05:32:43+00:00
- **Authors**: Jiajiong Ma, Guihua Wen, Yang Hu, Tianyuan Chang, Haibin Zeng, Lijun Jiang, Jianzeng Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Background and Object: In China, body constitution is highly related to physiological and pathological functions of human body and determines the tendency of the disease, which is of great importance for treatment in clinical medicine. Tongue diagnosis, as a key part of Traditional Chinese Medicine inspection, is an important way to recognize the type of constitution.In order to deploy tongue image constitution recognition system on non-invasive mobile device to achieve fast, efficient and accurate constitution recognition, an efficient method is required to deal with the challenge of this kind of complex environment. Methods: In this work, we perform the tongue area detection, tongue area calibration and constitution classification using methods which are based on deep convolutional neural network. Subject to the variation of inconstant environmental condition, the distribution of the picture is uneven, which has a bad effect on classification performance. To solve this problem, we propose a method based on the complexity of individual instances to divide dataset into two subsets and classify them separately, which is capable of improving classification accuracy. To evaluate the performance of our proposed method, we conduct experiments on three sizes of tongue datasets, in which deep convolutional neural network method and traditional digital image analysis method are respectively applied to extract features for tongue images. The proposed method is combined with the base classifier Softmax, SVM, and DecisionTree respectively. Results: As the experiments results shown, our proposed method improves the classification accuracy by 1.135% on average and achieves 59.99% constitution classification accuracy. Conclusions: Experimental results on three datasets show that our proposed method can effectively improve the classification accuracy of tongue constitution recognition.



### WRPN & Apprentice: Methods for Training and Inference using Low-Precision Numerics
- **Arxiv ID**: http://arxiv.org/abs/1803.00227v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1803.00227v1)
- **Published**: 2018-03-01 06:22:37+00:00
- **Updated**: 2018-03-01 06:22:37+00:00
- **Authors**: Asit Mishra, Debbie Marr
- **Comment**: Tech report
- **Journal**: None
- **Summary**: Today's high performance deep learning architectures involve large models with numerous parameters. Low precision numerics has emerged as a popular technique to reduce both the compute and memory requirements of these large models. However, lowering precision often leads to accuracy degradation. We describe three schemes whereby one can both train and do efficient inference using low precision numerics without hurting accuracy. Finally, we describe an efficient hardware accelerator that can take advantage of the proposed low precision numerics.



### DRUNET: A Dilated-Residual U-Net Deep Learning Network to Digitally Stain Optic Nerve Head Tissues in Optical Coherence Tomography Images
- **Arxiv ID**: http://arxiv.org/abs/1803.00232v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.00232v1)
- **Published**: 2018-03-01 06:37:30+00:00
- **Updated**: 2018-03-01 06:37:30+00:00
- **Authors**: Sripad Krishna Devalla, Prajwal K. Renukanand, Bharathwaj K. Sreedhar, Shamira Perera, Jean-Martial Mari, Khai Sing Chin, Tin A. Tun, Nicholas G. Strouthidis, Tin Aung, Alexandre H. Thiery, Michael J. A. Girard
- **Comment**: None
- **Journal**: None
- **Summary**: Given that the neural and connective tissues of the optic nerve head (ONH) exhibit complex morphological changes with the development and progression of glaucoma, their simultaneous isolation from optical coherence tomography (OCT) images may be of great interest for the clinical diagnosis and management of this pathology. A deep learning algorithm was designed and trained to digitally stain (i.e. highlight) 6 ONH tissue layers by capturing both the local (tissue texture) and contextual information (spatial arrangement of tissues). The overall dice coefficient (mean of all tissues) was $0.91 \pm 0.05$ when assessed against manual segmentations performed by an expert observer. We offer here a robust segmentation framework that could be extended for the automated parametric study of the ONH tissues.



### Scalable Dense Non-rigid Structure-from-Motion: A Grassmannian Perspective
- **Arxiv ID**: http://arxiv.org/abs/1803.00233v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00233v2)
- **Published**: 2018-03-01 07:08:39+00:00
- **Updated**: 2018-03-23 22:19:20+00:00
- **Authors**: Suryansh Kumar, Anoop Cherian, Yuchao Dai, Hongdong Li
- **Comment**: 10 pages, 7 figure, 4 tables. Accepted for publication in Conference
  on Computer Vision and Pattern Recognition (CVPR), 2018, typos fixed and
  acknowledgement added
- **Journal**: None
- **Summary**: This paper addresses the task of dense non-rigid structure-from-motion (NRSfM) using multiple images. State-of-the-art methods to this problem are often hurdled by scalability, expensive computations, and noisy measurements. Further, recent methods to NRSfM usually either assume a small number of sparse feature points or ignore local non-linearities of shape deformations, and thus cannot reliably model complex non-rigid deformations. To address these issues, in this paper, we propose a new approach for dense NRSfM by modeling the problem on a Grassmann manifold. Specifically, we assume the complex non-rigid deformations lie on a union of local linear subspaces both spatially and temporally. This naturally allows for a compact representation of the complex non-rigid deformation over frames. We provide experimental results on several synthetic and real benchmark datasets. The procured results clearly demonstrate that our method, apart from being scalable and more accurate than state-of-the-art methods, is also more robust to noise and generalizes to highly non-linear deformations.



### Monocular Depth Estimation using Multi-Scale Continuous CRFs as Sequential Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.00891v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00891v1)
- **Published**: 2018-03-01 07:33:19+00:00
- **Updated**: 2018-03-01 07:33:19+00:00
- **Authors**: Dan Xu, Elisa Ricci, Wanli Ouyang, Xiaogang Wang, Nicu Sebe
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1704.02157
- **Journal**: None
- **Summary**: Depth cues have been proved very useful in various computer vision and robotic tasks. This paper addresses the problem of monocular depth estimation from a single still image. Inspired by the effectiveness of recent works on multi-scale convolutional neural networks (CNN), we propose a deep model which fuses complementary information derived from multiple CNN side outputs. Different from previous methods using concatenation or weighted average schemes, the integration is obtained by means of continuous Conditional Random Fields (CRFs). In particular, we propose two different variations, one based on a cascade of multiple CRFs, the other on a unified graphical model. By designing a novel CNN implementation of mean-field updates for continuous CRFs, we show that both proposed models can be regarded as sequential deep networks and that training can be performed end-to-end. Through an extensive experimental evaluation, we demonstrate the effectiveness of the proposed approach and establish new state of the art results for the monocular depth estimation task on three publicly available datasets, i.e. NYUD-V2, Make3D and KITTI.



### Five-point Fundamental Matrix Estimation for Uncalibrated Cameras
- **Arxiv ID**: http://arxiv.org/abs/1803.00260v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1803.00260v1)
- **Published**: 2018-03-01 09:08:18+00:00
- **Updated**: 2018-03-01 09:08:18+00:00
- **Authors**: Daniel Barath
- **Comment**: None
- **Journal**: None
- **Summary**: We aim at estimating the fundamental matrix in two views from five correspondences of rotation invariant features obtained by e.g.\ the SIFT detector. The proposed minimal solver first estimates a homography from three correspondences assuming that they are co-planar and exploiting their rotational components. Then the fundamental matrix is obtained from the homography and two additional point pairs in general position. The proposed approach, combined with robust estimators like Graph-Cut RANSAC, is superior to other state-of-the-art algorithms both in terms of accuracy and number of iterations required. This is validated on synthesized data and $561$ real image pairs. Moreover, the tests show that requiring three points on a plane is not too restrictive in urban environment and locally optimized robust estimators lead to accurate estimates even if the points are not entirely co-planar. As a potential application, we show that using the proposed method makes two-view multi-motion estimation more accurate.



### A Deep Learning Approach for Multimodal Deception Detection
- **Arxiv ID**: http://arxiv.org/abs/1803.00344v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.00344v1)
- **Published**: 2018-03-01 12:38:13+00:00
- **Updated**: 2018-03-01 12:38:13+00:00
- **Authors**: Gangeshwar Krishnamurthy, Navonil Majumder, Soujanya Poria, Erik Cambria
- **Comment**: Accepted at the 19th International Conference on Computational
  Linguistics and Intelligent Text Processing (CICLing), 2018
- **Journal**: None
- **Summary**: Automatic deception detection is an important task that has gained momentum in computational linguistics due to its potential applications. In this paper, we propose a simple yet tough to beat multi-modal neural model for deception detection. By combining features from different modalities such as video, audio, and text along with Micro-Expression features, we show that detecting deception in real life videos can be more accurate. Experimental results on a dataset of real-life deception videos show that our model outperforms existing techniques for deception detection with an accuracy of 96.14% and ROC-AUC of 0.9799.



### Poisson Image Denoising Using Best Linear Prediction: A Post-processing Framework
- **Arxiv ID**: http://arxiv.org/abs/1803.00389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00389v1)
- **Published**: 2018-03-01 14:42:24+00:00
- **Updated**: 2018-03-01 14:42:24+00:00
- **Authors**: Milad Niknejad, Mario A. T. Figueiredo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of denoising images degraded by Poisson noise. We propose a new patch-based approach based on best linear prediction to estimate the underlying clean image. A simplified prediction formula is derived for Poisson observations, which requires the covariance matrix of the underlying clean patch. We use the assumption that similar patches in a neighborhood share the same covariance matrix, and we use off-the-shelf Poisson denoising methods in order to obtain an initial estimate of the covariance matrices. Our method can be seen as a post-processing step for Poisson denoising methods and the results show that it improves upon several Poisson denoising methods by relevant margins.



### Knowledge Transfer with Jacobian Matching
- **Arxiv ID**: http://arxiv.org/abs/1803.00443v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.00443v1)
- **Published**: 2018-03-01 15:31:26+00:00
- **Updated**: 2018-03-01 15:31:26+00:00
- **Authors**: Suraj Srinivas, Francois Fleuret
- **Comment**: None
- **Journal**: None
- **Summary**: Classical distillation methods transfer representations from a "teacher" neural network to a "student" network by matching their output activations. Recent methods also match the Jacobians, or the gradient of output activations with the input. However, this involves making some ad hoc decisions, in particular, the choice of the loss function.   In this paper, we first establish an equivalence between Jacobian matching and distillation with input noise, from which we derive appropriate loss functions for Jacobian matching. We then rely on this analysis to apply Jacobian matching to transfer learning by establishing equivalence of a recent transfer learning procedure to distillation.   We then show experimentally on standard image datasets that Jacobian-based penalties improve distillation, robustness to noisy inputs, and transfer learning.



### LCR-Net++: Multi-person 2D and 3D Pose Detection in Natural Images
- **Arxiv ID**: http://arxiv.org/abs/1803.00455v3
- **DOI**: 10.1109/TPAMI.2019.2892985
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00455v3)
- **Published**: 2018-03-01 15:39:38+00:00
- **Updated**: 2019-01-13 19:33:24+00:00
- **Authors**: Gregory Rogez, Philippe Weinzaepfel, Cordelia Schmid
- **Comment**: journal version of the CVPR 2017 paper, accepted to appear in IEEE
  Trans. PAMI
- **Journal**: None
- **Summary**: We propose an end-to-end architecture for joint 2D and 3D human pose estimation in natural images. Key to our approach is the generation and scoring of a number of pose proposals per image, which allows us to predict 2D and 3D poses of multiple people simultaneously. Hence, our approach does not require an approximate localization of the humans for initialization. Our Localization-Classification-Regression architecture, named LCR-Net, contains 3 main components: 1) the pose proposal generator that suggests candidate poses at different locations in the image; 2) a classifier that scores the different pose proposals; and 3) a regressor that refines pose proposals both in 2D and 3D. All three stages share the convolutional feature layers and are trained jointly. The final pose estimation is obtained by integrating over neighboring pose hypotheses, which is shown to improve over a standard non maximum suppression algorithm. Our method recovers full-body 2D and 3D poses, hallucinating plausible body parts when the persons are partially occluded or truncated by the image boundary. Our approach significantly outperforms the state of the art in 3D pose estimation on Human3.6M, a controlled environment. Moreover, it shows promising results on real images for both single and multi-person subsets of the MPII 2D pose benchmark and demonstrates satisfying 3D pose results even for multi-person images.



### The 2018 DAVIS Challenge on Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.00557v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00557v2)
- **Published**: 2018-03-01 18:56:36+00:00
- **Updated**: 2018-03-27 10:02:26+00:00
- **Authors**: Sergi Caelles, Alberto Montes, Kevis-Kokitsi Maninis, Yuhua Chen, Luc Van Gool, Federico Perazzi, Jordi Pont-Tuset
- **Comment**: Challenge website: http://davischallenge.org/
- **Journal**: None
- **Summary**: We present the 2018 DAVIS Challenge on Video Object Segmentation, a public competition specifically designed for the task of video object segmentation. It builds upon the DAVIS 2017 dataset, which was presented in the previous edition of the DAVIS Challenge, and added 100 videos with multiple objects per sequence to the original DAVIS 2016 dataset. Motivated by the analysis of the results of the 2017 edition, the main track of the competition will be the same than in the previous edition (segmentation given the full mask of the objects in the first frame -- semi-supervised scenario). This edition, however, also adds an interactive segmentation teaser track, where the participants will interact with a web service simulating the input of a human that provides scribbles to iteratively improve the result.



### Fast and accurate computation of orthogonal moments for texture analysis
- **Arxiv ID**: http://arxiv.org/abs/1803.00638v2
- **DOI**: 10.1016/j.patcog.2018.06.012
- **Categories**: **math.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.00638v2)
- **Published**: 2018-03-01 21:40:42+00:00
- **Updated**: 2018-05-02 14:51:05+00:00
- **Authors**: C. Di Ruberto, L. Putzu, G. Rodriguez
- **Comment**: 29 pages, 9 figures
- **Journal**: Pattern Recongnit. 83 (2018) 498-510
- **Summary**: In this work we describe a fast and stable algorithm for the computation of the orthogonal moments of an image. Indeed, orthogonal moments are characterized by a high discriminative power, but some of their possible formulations are characterized by a large computational complexity, which limits their real-time application. This paper describes in detail an approach based on recurrence relations, and proposes an optimized Matlab implementation of the corresponding computational procedure, aiming to solve the above limitations and put at the community's disposal an efficient and easy to use software. In our experiments we evaluate the effectiveness of the recurrence formulation, as well as its performance for the reconstruction task, in comparison to the closed form representation, often used in the literature. The results show a sensible reduction in the computational complexity, together with a greater accuracy in reconstruction. In order to assess and compare the accuracy of the computed moments in texture analysis, we perform classification experiments on six well-known databases of texture images. Again, the recurrence formulation performs better in classification than the closed form representation. More importantly, if computed from the GLCM of the image using the proposed stable procedure, the orthogonal moments outperform in some situations some of the most diffused state-of-the-art descriptors for texture classification.



### Resampling Forgery Detection Using Deep Learning and A-Contrario Analysis
- **Arxiv ID**: http://arxiv.org/abs/1803.01711v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01711v1)
- **Published**: 2018-03-01 21:59:04+00:00
- **Updated**: 2018-03-01 21:59:04+00:00
- **Authors**: Arjuna Flenner, Lawrence Peterson, Jason Bunk, Tajuddin Manhar Mohammed, Lakshmanan Nataraj, B. S. Manjunath
- **Comment**: arXiv admin note: text overlap with arXiv:1802.03154
- **Journal**: None
- **Summary**: The amount of digital imagery recorded has recently grown exponentially, and with the advancement of software, such as Photoshop or Gimp, it has become easier to manipulate images. However, most images on the internet have not been manipulated and any automated manipulation detection algorithm must carefully control the false alarm rate. In this paper we discuss a method to automatically detect local resampling using deep learning while controlling the false alarm rate using a-contrario analysis. The automated procedure consists of three primary steps. First, resampling features are calculated for image blocks. A deep learning classifier is then used to generate a heatmap that indicates if the image block has been resampled. We expect some of these blocks to be falsely identified as resampled. We use a-contrario hypothesis testing to both identify if the patterns of the manipulated blocks indicate if the image has been tampered with and to localize the manipulation. We demonstrate that this strategy is effective in indicating if an image has been manipulated and localizing the manipulations.



### Semi-parametric Topological Memory for Navigation
- **Arxiv ID**: http://arxiv.org/abs/1803.00653v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.00653v1)
- **Published**: 2018-03-01 22:50:35+00:00
- **Updated**: 2018-03-01 22:50:35+00:00
- **Authors**: Nikolay Savinov, Alexey Dosovitskiy, Vladlen Koltun
- **Comment**: Published at International Conference on Learning Representations
  (ICLR) 2018. Project website at https://sites.google.com/view/SPTM
- **Journal**: None
- **Summary**: We introduce a new memory architecture for navigation in previously unseen environments, inspired by landmark-based navigation in animals. The proposed semi-parametric topological memory (SPTM) consists of a (non-parametric) graph with nodes corresponding to locations in the environment and a (parametric) deep network capable of retrieving nodes from the graph based on observations. The graph stores no metric information, only connectivity of locations corresponding to the nodes. We use SPTM as a planning module in a navigation system. Given only 5 minutes of footage of a previously unseen maze, an SPTM-based navigation agent can build a topological map of the environment and use it to confidently navigate towards goals. The average success rate of the SPTM agent in goal-directed navigation across test environments is higher than the best-performing baseline by a factor of three. A video of the agent is available at https://youtu.be/vRF7f4lhswo



### SD-CNN: a Shallow-Deep CNN for Improved Breast Cancer Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1803.00663v2
- **DOI**: 10.1016/j.compmedimag.2018.09.004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.00663v2)
- **Published**: 2018-03-01 23:59:20+00:00
- **Updated**: 2018-10-26 17:48:24+00:00
- **Authors**: Fei Gao, Teresa Wu, Jing Li, Bin Zheng, Lingxiang Ruan, Desheng Shang, Bhavika Patel
- **Comment**: None
- **Journal**: Computerized Medical Imaging and Graphics (2018) 70 53-62
- **Summary**: Breast cancer is the second leading cause of cancer death among women worldwide. Nevertheless, it is also one of the most treatable malignances if detected early. Screening for breast cancer with digital mammography (DM) has been widely used. However it demonstrates limited sensitivity for women with dense breasts. An emerging technology in the field is contrast-enhanced digital mammography (CEDM), which includes a low energy (LE) image similar to DM, and a recombined image leveraging tumor neoangiogenesis similar to breast magnetic resonance imaging (MRI). CEDM has shown better diagnostic accuracy than DM. While promising, CEDM is not yet widely available across medical centers. In this research, we propose a Shallow-Deep Convolutional Neural Network (SD-CNN) where a shallow CNN is developed to derive "virtual" recombined images from LE images, and a deep CNN is employed to extract novel features from LE, recombined or "virtual" recombined images for ensemble models to classify the cases as benign vs. cancer. To evaluate the validity of our approach, we first develop a deep-CNN using 49 CEDM cases collected from Mayo Clinic to prove the contributions from recombined images for improved breast cancer diagnosis (0.86 in accuracy using LE imaging vs. 0.90 in accuracy using both LE and recombined imaging). We then develop a shallow-CNN using the same 49 CEDM cases to learn the nonlinear mapping from LE to recombined images. Next, we use 69 DM cases collected from the hospital located at Zhejiang University, China to generate "virtual" recombined images. Using DM alone provides 0.91 in accuracy, whereas SD-CNN improves the diagnostic accuracy to 0.95.



