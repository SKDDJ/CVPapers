# Arxiv Papers in cs.CV on 2018-03-19
### White matter hyperintensity segmentation from T1 and FLAIR images using fully convolutional neural networks enhanced with residual connections
- **Arxiv ID**: http://arxiv.org/abs/1803.06782v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06782v1)
- **Published**: 2018-03-19 02:07:00+00:00
- **Updated**: 2018-03-19 02:07:00+00:00
- **Authors**: Dakai Jin, Ziyue Xu, Adam P. Harrison, Daniel J. Mollura
- **Comment**: IEEE International Symposium on Biomedical Imaging (ISBI)
- **Journal**: None
- **Summary**: Segmentation and quantification of white matter hyperintensities (WMHs) are of great importance in studying and understanding various neurological and geriatric disorders. Although automatic methods have been proposed for WMH segmentation on magnetic resonance imaging (MRI), manual corrections are often necessary to achieve clinically practical results. Major challenges for WMH segmentation stem from their inhomogeneous MRI intensities, random location and size distributions, and MRI noise. The presence of other brain anatomies or diseases with enhanced intensities adds further difficulties. To cope with these challenges, we present a specifically designed fully convolutional neural network (FCN) with residual connections to segment WMHs by using combined T1 and fluid-attenuated inversion recovery (FLAIR) images. Our customized FCN is designed to be straightforward and generalizable, providing efficient end-to-end training due to its enhanced information propagation. We tested our method on the open WMH Segmentation Challenge MICCAI2017 dataset, and, despite our method's relative simplicity, results show that it performs amongst the leading techniques across five metrics. More importantly, our method achieves the best score for hausdorff distance and average volume difference in testing datasets from two MRI scanners that were not included in training, demonstrating better generalization ability of our proposed method over its competitors.



### TOMAAT: volumetric medical image analysis as a cloud service
- **Arxiv ID**: http://arxiv.org/abs/1803.06784v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1803.06784v2)
- **Published**: 2018-03-19 02:21:36+00:00
- **Updated**: 2018-04-25 09:19:03+00:00
- **Authors**: Fausto Milletari, Johann Frei, Seyed-Ahmad Ahmadi
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has been recently applied to a multitude of computer vision and medical image analysis problems. Although recent research efforts have improved the state of the art, most of the methods cannot be easily accessed, compared or used by either researchers or the general public. Researchers often publish their code and trained models on the internet, but this does not always enable these approaches to be easily used or integrated in stand-alone applications and existing workflows. In this paper we propose a framework which allows easy deployment and access of deep learning methods for segmentation through a cloud-based architecture. Our approach comprises three parts: a server, which wraps trained deep learning models and their pre- and post-processing data pipelines and makes them available on the cloud; a client which interfaces with the server to obtain predictions on user data; a service registry that informs clients about available prediction endpoints that are available in the cloud. These three parts constitute the open-source TOMAAT framework.



### Multimodal Sentiment Analysis: Addressing Key Issues and Setting up the Baselines
- **Arxiv ID**: http://arxiv.org/abs/1803.07427v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1803.07427v2)
- **Published**: 2018-03-19 02:23:30+00:00
- **Updated**: 2019-02-12 02:42:19+00:00
- **Authors**: Soujanya Poria, Navonil Majumder, Devamanyu Hazarika, Erik Cambria, Alexander Gelbukh, Amir Hussain
- **Comment**: IEEE Intelligence Systems. arXiv admin note: substantial text overlap
  with arXiv:1707.09538
- **Journal**: None
- **Summary**: We compile baselines, along with dataset split, for multimodal sentiment analysis. In this paper, we explore three different deep-learning based architectures for multimodal sentiment classification, each improving upon the previous. Further, we evaluate these architectures with multiple datasets with fixed train/test partition. We also discuss some major issues, frequently ignored in multimodal sentiment analysis research, e.g., role of speaker-exclusive models, importance of different modalities, and generalizability. This framework illustrates the different facets of analysis to be considered while performing multimodal sentiment analysis and, hence, serves as a new benchmark for future research in this emerging field.



### Depth-aware CNN for RGB-D Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.06791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06791v1)
- **Published**: 2018-03-19 03:09:42+00:00
- **Updated**: 2018-03-19 03:09:42+00:00
- **Authors**: Weiyue Wang, Ulrich Neumann
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN) are limited by the lack of capability to handle geometric information due to the fixed grid kernel structure. The availability of depth data enables progress in RGB-D semantic segmentation with CNNs. State-of-the-art methods either use depth as additional images or process spatial information in 3D volumes or point clouds. These methods suffer from high computation and memory cost. To address these issues, we present Depth-aware CNN by introducing two intuitive, flexible and effective operations: depth-aware convolution and depth-aware average pooling. By leveraging depth similarity between pixels in the process of information propagation, geometry is seamlessly incorporated into CNN. Without introducing any additional parameters, both operators can be easily integrated into existing CNNs. Extensive experiments and ablation studies on challenging RGB-D semantic segmentation benchmarks validate the effectiveness and flexibility of our approach.



### Nonlocal Low-Rank Tensor Factor Analysis for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1803.06795v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.06795v1)
- **Published**: 2018-03-19 03:48:14+00:00
- **Updated**: 2018-03-19 03:48:14+00:00
- **Authors**: Xinyuan Zhang, Xin Yuan, Lawrence Carin
- **Comment**: None
- **Journal**: None
- **Summary**: Low-rank signal modeling has been widely leveraged to capture non-local correlation in image processing applications. We propose a new method that employs low-rank tensor factor analysis for tensors generated by grouped image patches. The low-rank tensors are fed into the alternative direction multiplier method (ADMM) to further improve image reconstruction. The motivating application is compressive sensing (CS), and a deep convolutional architecture is adopted to approximate the expensive matrix inversion in CS applications. An iterative algorithm based on this low-rank tensor factorization strategy, called NLR-TFA, is presented in detail. Experimental results on noiseless and noisy CS measurements demonstrate the superiority of the proposed approach, especially at low CS sampling rates.



### Attention-GAN for Object Transfiguration in Wild Images
- **Arxiv ID**: http://arxiv.org/abs/1803.06798v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06798v1)
- **Published**: 2018-03-19 04:00:13+00:00
- **Updated**: 2018-03-19 04:00:13+00:00
- **Authors**: Xinyuan Chen, Chang Xu, Xiaokang Yang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies the object transfiguration problem in wild images. The generative network in classical GANs for object transfiguration often undertakes a dual responsibility: to detect the objects of interests and to convert the object from source domain to target domain. In contrast, we decompose the generative network into two separat networks, each of which is only dedicated to one particular sub-task. The attention network predicts spatial attention maps of images, and the transformation network focuses on translating objects. Attention maps produced by attention network are encouraged to be sparse, so that major attention can be paid to objects of interests. No matter before or after object transfiguration, attention maps should remain constant. In addition, learning attention network can receive more instructions, given the available segmentation annotations of images. Experimental results demonstrate the necessity of investigating attention in object transfiguration, and that the proposed algorithm can learn accurate attention to improve quality of generated images.



### Revisiting RCNN: On Awakening the Classification Power of Faster RCNN
- **Arxiv ID**: http://arxiv.org/abs/1803.06799v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06799v3)
- **Published**: 2018-03-19 04:13:06+00:00
- **Updated**: 2018-07-14 04:50:20+00:00
- **Authors**: Bowen Cheng, Yunchao Wei, Honghui Shi, Rogerio Feris, Jinjun Xiong, Thomas Huang
- **Comment**: ECCV 2018
- **Journal**: None
- **Summary**: Recent region-based object detectors are usually built with separate classification and localization branches on top of shared feature extraction networks. In this paper, we analyze failure cases of state-of-the-art detectors and observe that most hard false positives result from classification instead of localization. We conjecture that: (1) Shared feature representation is not optimal due to the mismatched goals of feature learning for classification and localization; (2) multi-task learning helps, yet optimization of the multi-task loss may result in sub-optimal for individual tasks; (3) large receptive field for different scales leads to redundant context information for small objects.We demonstrate the potential of detector classification power by a simple, effective, and widely-applicable Decoupled Classification Refinement (DCR) network. DCR samples hard false positives from the base classifier in Faster RCNN and trains a RCNN-styled strong classifier. Experiments show new state-of-the-art results on PASCAL VOC and COCO without any bells and whistles.



### Alive Caricature from 2D to 3D
- **Arxiv ID**: http://arxiv.org/abs/1803.06802v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06802v3)
- **Published**: 2018-03-19 04:47:16+00:00
- **Updated**: 2018-03-27 06:38:08+00:00
- **Authors**: Qianyi Wu, Juyong Zhang, Yu-Kun Lai, Jianmin Zheng, Jianfei Cai
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: Caricature is an art form that expresses subjects in abstract, simple and exaggerated view. While many caricatures are 2D images, this paper presents an algorithm for creating expressive 3D caricatures from 2D caricature images with a minimum of user interaction. The key idea of our approach is to introduce an intrinsic deformation representation that has a capacity of extrapolation enabling us to create a deformation space from standard face dataset, which maintains face constraints and meanwhile is sufficiently large for producing exaggerated face models. Built upon the proposed deformation representation, an optimization model is formulated to find the 3D caricature that captures the style of the 2D caricature image automatically. The experiments show that our approach has better capability in expressing caricatures than those fitting approaches directly using classical parametric face models such as 3DMM and FaceWareHouse. Moreover, our approach is based on standard face datasets and avoids constructing complicated 3D caricature training set, which provides great flexibility in real applications.



### Learnable Image Encryption
- **Arxiv ID**: http://arxiv.org/abs/1804.00490v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.00490v1)
- **Published**: 2018-03-19 05:44:53+00:00
- **Updated**: 2018-03-19 05:44:53+00:00
- **Authors**: Masayuki Tanaka
- **Comment**: 2 pages
- **Journal**: None
- **Summary**: The network-based machine learning algorithm is very powerful tools. However, it requires huge training dataset. Researchers often meet privacy issues when they collect image dataset especially for surveillance applications. A learnable image encryption scheme is introduced. The key idea of this scheme is to encrypt images, so that human cannot understand images but the network can be train with encrypted images. This scheme allows us to train the network without the privacy issues. In this paper, a simple learnable image encryption algorithm is proposed. Then, the proposed algorithm is validated with cifar dataset.



### Weakly Supervised Object Localization on grocery shelves using simple FCN and Synthetic Dataset
- **Arxiv ID**: http://arxiv.org/abs/1803.06813v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06813v2)
- **Published**: 2018-03-19 06:34:29+00:00
- **Updated**: 2019-01-09 09:54:08+00:00
- **Authors**: Srikrishna Varadarajan, Muktabh Mayank Srivastava
- **Comment**: Published at The Indian Conference on Computer Vision, Graphics and
  Image Processing (ICVGIP) 2018. ( https://cvit.iiit.ac.in/icvgip18/ )
- **Journal**: None
- **Summary**: We propose a weakly supervised method using two algorithms to predict object bounding boxes given only an image classification dataset. First algorithm is a simple Fully Convolutional Network (FCN) trained to classify object instances. We use the property of FCN to return a mask for images larger than training images to get a primary output segmentation mask during test time by passing an image pyramid to it. We enhance the FCN output mask into final output bounding boxes by a Convolutional Encoder-Decoder (ConvAE) viz. the second algorithm. ConvAE is trained to localize objects on an artificially generated dataset of output segmentation masks. We demonstrate the effectiveness of this method in localizing objects in grocery shelves where annotating data for object detection is hard due to variety of objects. This method can be extended to any problem domain where collecting images of objects is easy and annotating their coordinates is hard.



### ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.06815v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06815v3)
- **Published**: 2018-03-19 06:42:47+00:00
- **Updated**: 2018-07-25 00:45:02+00:00
- **Authors**: Sachin Mehta, Mohammad Rastegari, Anat Caspi, Linda Shapiro, Hannaneh Hajishirzi
- **Comment**: Accepted at ECCV'18
- **Journal**: None
- **Summary**: We introduce a fast and efficient convolutional neural network, ESPNet, for semantic segmentation of high resolution images under resource constraints. ESPNet is based on a new convolutional module, efficient spatial pyramid (ESP), which is efficient in terms of computation, memory, and power. ESPNet is 22 times faster (on a standard GPU) and 180 times smaller than the state-of-the-art semantic segmentation network PSPNet, while its category-wise accuracy is only 8% less. We evaluated ESPNet on a variety of semantic segmentation datasets including Cityscapes, PASCAL VOC, and a breast biopsy whole slide image dataset. Under the same constraints on memory and computation, ESPNet outperforms all the current efficient CNN networks such as MobileNet, ShuffleNet, and ENet on both standard metrics and our newly introduced performance metrics that measure efficiency on edge devices. Our network can process high resolution images at a rate of 112 and 9 frames per second on a standard GPU and edge device, respectively.



### A Mixture of Views Network with Applications to the Classification of Breast Microcalcifications
- **Arxiv ID**: http://arxiv.org/abs/1803.06898v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.06898v1)
- **Published**: 2018-03-19 13:11:10+00:00
- **Updated**: 2018-03-19 13:11:10+00:00
- **Authors**: Yaniv Shachor, Hayit Greenspan, Jacob Goldberger
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we examine data fusion methods for multi-view data classification. We present a decision concept which explicitly takes into account the input multi-view structure, where for each case there is a different subset of relevant views. This data fusion concept, which we dub Mixture of Views, is implemented by a special purpose neural network architecture. It is demonstrated on the task of classifying breast microcalcifications as benign or malignant based on CC and MLO mammography views. The single view decisions are combined by a data-driven decision, according to the relevance of each view in a given case, into a global decision. The method is evaluated on a large multi-view dataset extracted from the standardized digital database for screening mammography (DDSM). The experimental results show that our method outperforms previously suggested fusion methods.



### Aerial LaneNet: Lane Marking Semantic Segmentation in Aerial Imagery using Wavelet-Enhanced Cost-sensitive Symmetric Fully Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.06904v2
- **DOI**: 10.1109/TGRS.2018.2878510
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06904v2)
- **Published**: 2018-03-19 13:32:27+00:00
- **Updated**: 2018-11-01 16:16:51+00:00
- **Authors**: Seyed Majid Azimi, Peter Fischer, Marco Körner, Peter Reinartz
- **Comment**: IEEE TGRS 2018 - Accepted
- **Journal**: None
- **Summary**: The knowledge about the placement and appearance of lane markings is a prerequisite for the creation of maps with high precision, necessary for autonomous driving, infrastructure monitoring, lane-wise traffic management, and urban planning. Lane markings are one of the important components of such maps. Lane markings convey the rules of roads to drivers. While these rules are learned by humans, an autonomous driving vehicle should be taught to learn them to localize itself. Therefore, accurate and reliable lane marking semantic segmentation in the imagery of roads and highways is needed to achieve such goals. We use airborne imagery which can capture a large area in a short period of time by introducing an aerial lane marking dataset. In this work, we propose a Symmetric Fully Convolutional Neural Network enhanced by Wavelet Transform in order to automatically carry out lane marking segmentation in aerial imagery. Due to a heavily unbalanced problem in terms of number of lane marking pixels compared with background pixels, we use a customized loss function as well as a new type of data augmentation step. We achieve a very high accuracy in pixel-wise localization of lane markings without using 3rd-party information. In this work, we introduce the first high-quality dataset used within our experiments which contains a broad range of situations and classes of lane markings representative of current transportation systems. This dataset will be publicly available and hence, it can be used as the benchmark dataset for future algorithms within this domain.



### Unsupervised Semantic Deep Hashing
- **Arxiv ID**: http://arxiv.org/abs/1803.06911v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06911v1)
- **Published**: 2018-03-19 13:42:23+00:00
- **Updated**: 2018-03-19 13:42:23+00:00
- **Authors**: Sheng Jin
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep hashing methods have been proved to be efficient since it employs convolutional neural network to learn features and hashing codes simultaneously. However, these methods are mostly supervised. In real-world application, it is a time-consuming and overloaded task for annotating a large number of images. In this paper, we propose a novel unsupervised deep hashing method for large-scale image retrieval. Our method, namely unsupervised semantic deep hashing (\textbf{USDH}), uses semantic information preserved in the CNN feature layer to guide the training of network. We enforce four criteria on hashing codes learning based on VGG-19 model: 1) preserving relevant information of feature space in hashing space; 2) minimizing quantization loss between binary-like codes and hashing codes; 3) improving the usage of each bit in hashing codes by using maximum information entropy, and 4) invariant to image rotation. Extensive experiments on CIFAR-10, NUSWIDE have demonstrated that \textbf{USDH} outperforms several state-of-the-art unsupervised hashing methods for image retrieval. We also conduct experiments on Oxford 17 datasets for fine-grained classification to verify its efficiency for other computer vision tasks.



### Deja Vu: Motion Prediction in Static Images
- **Arxiv ID**: http://arxiv.org/abs/1803.06951v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06951v2)
- **Published**: 2018-03-19 14:32:39+00:00
- **Updated**: 2018-03-21 09:48:41+00:00
- **Authors**: Silvia L. Pintea, Jan C. van Gemert, Arnold W. M. Smeulders
- **Comment**: Published in the proceedings of the European Conference on Computer
  Vision (ECCV), 2014
- **Journal**: None
- **Summary**: This paper proposes motion prediction in single still images by learning it from a set of videos. The building assumption is that similar motion is characterized by similar appearance. The proposed method learns local motion patterns given a specific appearance and adds the predicted motion in a number of applications. This work (i) introduces a novel method to predict motion from appearance in a single static image, (ii) to that end, extends of the Structured Random Forest with regression derived from first principles, and (iii) shows the value of adding motion predictions in different tasks such as: weak frame-proposals containing unexpected events, action recognition, motion saliency. Illustrative results indicate that motion prediction is not only feasible, but also provides valuable information for a number of applications.



### Asymmetric kernel in Gaussian Processes for learning target variance
- **Arxiv ID**: http://arxiv.org/abs/1803.06952v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.06952v1)
- **Published**: 2018-03-19 14:34:35+00:00
- **Updated**: 2018-03-19 14:34:35+00:00
- **Authors**: Silvia L. Pintea, Jan C. van Gemert, Arnold W. M. Smeulders
- **Comment**: Accepted in Pattern Recognition Letters, 2018
- **Journal**: None
- **Summary**: This work incorporates the multi-modality of the data distribution into a Gaussian Process regression model. We approach the problem from a discriminative perspective by learning, jointly over the training data, the target space variance in the neighborhood of a certain sample through metric learning. We start by using data centers rather than all training samples. Subsequently, each center selects an individualized kernel metric. This enables each center to adjust the kernel space in its vicinity in correspondence with the topology of the targets --- a multi-modal approach. We additionally add descriptiveness by allowing each center to learn a precision matrix. We demonstrate empirically the reliability of the model.



### Featureless: Bypassing feature extraction in action categorization
- **Arxiv ID**: http://arxiv.org/abs/1803.06962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06962v1)
- **Published**: 2018-03-19 14:47:09+00:00
- **Updated**: 2018-03-19 14:47:09+00:00
- **Authors**: Silvia L. Pintea, Pascal S. Mettes, Jan C. van Gemert, Arnold W. M. Smeulders
- **Comment**: Published in the proceedings of the International Conference on Image
  Processing (ICIP), 2016
- **Journal**: None
- **Summary**: This method introduces an efficient manner of learning action categories without the need of feature estimation. The approach starts from low-level values, in a similar style to the successful CNN methods. However, rather than extracting general image features, we learn to predict specific video representations from raw video data. The benefit of such an approach is that at the same computational expense it can predict 2 D video representations as well as 3 D ones, based on motion. The proposed model relies on discriminative Waldboost, which we enhance to a multiclass formulation for the purpose of learning video representations. The suitability of the proposed approach as well as its time efficiency are tested on the UCF11 action recognition dataset.



### Improving Transferability of Adversarial Examples with Input Diversity
- **Arxiv ID**: http://arxiv.org/abs/1803.06978v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.06978v4)
- **Published**: 2018-03-19 15:07:51+00:00
- **Updated**: 2019-06-01 17:12:24+00:00
- **Authors**: Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, Alan Yuille
- **Comment**: CVPR 2019, code is available at:
  https://github.com/cihangxie/DI-2-FGSM
- **Journal**: None
- **Summary**: Though CNNs have achieved the state-of-the-art performance on various vision tasks, they are vulnerable to adversarial examples --- crafted by adding human-imperceptible perturbations to clean images. However, most of the existing adversarial attacks only achieve relatively low success rates under the challenging black-box setting, where the attackers have no knowledge of the model structure and parameters. To this end, we propose to improve the transferability of adversarial examples by creating diverse input patterns. Instead of only using the original images to generate adversarial examples, our method applies random transformations to the input images at each iteration. Extensive experiments on ImageNet show that the proposed attack method can generate adversarial examples that transfer much better to different networks than existing baselines. By evaluating our method against top defense solutions and official baselines from NIPS 2017 adversarial competition, the enhanced attack reaches an average success rate of 73.0%, which outperforms the top-1 attack submission in the NIPS competition by a large margin of 6.6%. We hope that our proposed attack strategy can serve as a strong benchmark baseline for evaluating the robustness of networks to adversaries and the effectiveness of different defense methods in the future. Code is available at https://github.com/cihangxie/DI-2-FGSM.



### Live Target Detection with Deep Learning Neural Network and Unmanned Aerial Vehicle on Android Mobile Device
- **Arxiv ID**: http://arxiv.org/abs/1803.07015v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07015v2)
- **Published**: 2018-03-19 16:15:22+00:00
- **Updated**: 2018-03-22 17:57:00+00:00
- **Authors**: Ali Canberk Anar, Erkan Bostanci, Mehmet Serdar Guzel
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: This paper describes the stages faced during the development of an Android program which obtains and decodes live images from DJI Phantom 3 Professional Drone and implements certain features of the TensorFlow Android Camera Demo application. Test runs were made and outputs of the application were noted. A lake was classified as seashore, breakwater and pier with the proximities of 24.44%, 21.16% and 12.96% respectfully. The joystick of the UAV controller and laptop keyboard was classified with the proximities of 19.10% and 13.96% respectfully. The laptop monitor was classified as screen, monitor and television with the proximities of 18.77%, 14.76% and 14.00% respectfully. The computer used during the development of this study was classified as notebook and laptop with the proximities of 20.04% and 11.68% respectfully. A tractor parked at a parking lot was classified with the proximity of 12.88%. A group of cars in the same parking lot were classified as sports car, racer and convertible with the proximities of 31.75%, 18.64% and 13.45% respectfully at an inference time of 851ms.



### Factorised spatial representation learning: application in semi-supervised myocardial segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.07031v2
- **DOI**: 10.1007/978-3-030-00934-2_55
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07031v2)
- **Published**: 2018-03-19 16:44:00+00:00
- **Updated**: 2018-11-02 14:37:12+00:00
- **Authors**: Agisilaos Chartsias, Thomas Joyce, Giorgos Papanastasiou, Scott Semple, Michelle Williams, David Newby, Rohan Dharmakumar, Sotirios A. Tsaftaris
- **Comment**: Accepted in MICCAI 2018
- **Journal**: International Conference on Medical Image Computing and Computer
  Assisted Intervention (2018) pp. 490-498. Springer, Cham
- **Summary**: The success and generalisation of deep learning algorithms heavily depend on learning good feature representations. In medical imaging this entails representing anatomical information, as well as properties related to the specific imaging setting. Anatomical information is required to perform further analysis, whereas imaging information is key to disentangle scanner variability and potential artefacts. The ability to factorise these would allow for training algorithms only on the relevant information according to the task. To date, such factorisation has not been attempted. In this paper, we propose a methodology of latent space factorisation relying on the cycle-consistency principle. As an example application, we consider cardiac MR segmentation, where we separate information related to the myocardium from other features related to imaging and surrounding substructures. We demonstrate the proposed method's utility in a semi-supervised setting: we use very few labelled images together with many unlabelled images to train a myocardium segmentation neural network. Specifically, we achieve comparable performance to fully supervised networks using a fraction of labelled images in experiments on ACDC and a dataset from Edinburgh Imaging Facility QMRI. Code will be made available at https://github.com/agis85/spatial_factorisation.



### Towards Explanation of DNN-based Prediction with Guided Feature Inversion
- **Arxiv ID**: http://arxiv.org/abs/1804.00506v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1804.00506v2)
- **Published**: 2018-03-19 17:35:26+00:00
- **Updated**: 2018-05-26 04:47:32+00:00
- **Authors**: Mengnan Du, Ninghao Liu, Qingquan Song, Xia Hu
- **Comment**: None
- **Journal**: None
- **Summary**: While deep neural networks (DNN) have become an effective computational tool, the prediction results are often criticized by the lack of interpretability, which is essential in many real-world applications such as health informatics. Existing attempts based on local interpretations aim to identify relevant features contributing the most to the prediction of DNN by monitoring the neighborhood of a given input. They usually simply ignore the intermediate layers of the DNN that might contain rich information for interpretation. To bridge the gap, in this paper, we propose to investigate a guided feature inversion framework for taking advantage of the deep architectures towards effective interpretation. The proposed framework not only determines the contribution of each feature in the input but also provides insights into the decision-making process of DNN models. By further interacting with the neuron of the target category at the output layer of the DNN, we enforce the interpretation result to be class-discriminative. We apply the proposed interpretation model to different CNN architectures to provide explanations for image data and conduct extensive experiments on ImageNet and PASCAL VOC07 datasets. The interpretation results demonstrate the effectiveness of our proposed framework in providing class-discriminative interpretation for DNN-based prediction.



### Learning Region Features for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1803.07066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07066v1)
- **Published**: 2018-03-19 17:58:50+00:00
- **Updated**: 2018-03-19 17:58:50+00:00
- **Authors**: Jiayuan Gu, Han Hu, Liwei Wang, Yichen Wei, Jifeng Dai
- **Comment**: None
- **Journal**: None
- **Summary**: While most steps in the modern object detection methods are learnable, the region feature extraction step remains largely hand-crafted, featured by RoI pooling methods. This work proposes a general viewpoint that unifies existing region feature extraction methods and a novel method that is end-to-end learnable. The proposed method removes most heuristic choices and outperforms its RoI pooling counterparts. It moves further towards fully learnable object detection.



### VGAN-Based Image Representation Learning for Privacy-Preserving Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.07100v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07100v2)
- **Published**: 2018-03-19 18:14:35+00:00
- **Updated**: 2018-09-07 14:28:15+00:00
- **Authors**: Jiawei Chen, Janusz Konrad, Prakash Ishwar
- **Comment**: None
- **Journal**: None
- **Summary**: Reliable facial expression recognition plays a critical role in human-machine interactions. However, most of the facial expression analysis methodologies proposed to date pay little or no attention to the protection of a user's privacy. In this paper, we propose a Privacy-Preserving Representation-Learning Variational Generative Adversarial Network (PPRL-VGAN) to learn an image representation that is explicitly disentangled from the identity information. At the same time, this representation is discriminative from the standpoint of facial expression recognition and generative as it allows expression-equivalent face image synthesis. We evaluate the proposed model on two public datasets under various threat scenarios. Quantitative and qualitative results demonstrate that our approach strikes a balance between the preservation of privacy and data utility. We further demonstrate that our model can be effectively applied to other tasks such as expression morphing and image completion.



### Zero-Shot Detection
- **Arxiv ID**: http://arxiv.org/abs/1803.07113v2
- **DOI**: 10.1109/TCSVT.2019.2899569
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07113v2)
- **Published**: 2018-03-19 18:48:41+00:00
- **Updated**: 2019-03-19 16:38:24+00:00
- **Authors**: Pengkai Zhu, Hanxiao Wang, Venkatesh Saligrama
- **Comment**: Published in IEEE Transactions on Circuits and Systems for Video
  Technology
- **Journal**: None
- **Summary**: As we move towards large-scale object detection, it is unrealistic to expect annotated training data, in the form of bounding box annotations around objects, for all object classes at sufficient scale, and so methods capable of unseen object detection are required. We propose a novel zero-shot method based on training an end-to-end model that fuses semantic attribute prediction with visual features to propose object bounding boxes for seen and unseen classes. While we utilize semantic features during training, our method is agnostic to semantic information for unseen classes at test-time. Our method retains the efficiency and effectiveness of YOLOv2 for objects seen during training, while improving its performance for novel and unseen objects. The ability of state-of-art detection methods to learn discriminative object features to reject background proposals also limits their performance for unseen objects. We posit that, to detect unseen objects, we must incorporate semantic information into the visual domain so that the learned visual features reflect this information and leads to improved recall rates for unseen objects. We test our method on PASCAL VOC and MS COCO dataset and observed significant improvements on the average precision of unseen classes.



### Local Binary Pattern Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.07125v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07125v2)
- **Published**: 2018-03-19 19:12:19+00:00
- **Updated**: 2018-03-22 03:06:13+00:00
- **Authors**: Jeng-Hau Lin, Yunfan Yang, Rajesh Gupta, Zhuowen Tu
- **Comment**: 14 pages, 10 figures, 6 tables
- **Journal**: None
- **Summary**: Memory and computation efficient deep learning architec- tures are crucial to continued proliferation of machine learning capabili- ties to new platforms and systems. Binarization of operations in convo- lutional neural networks has shown promising results in reducing model size and computing efficiency. In this paper, we tackle the problem us- ing a strategy different from the existing literature by proposing local binary pattern networks or LBPNet, that is able to learn and perform binary operations in an end-to-end fashion. LBPNet1 uses local binary comparisons and random projection in place of conventional convolu- tion (or approximation of convolution) operations. These operations can be implemented efficiently on different platforms including direct hard- ware implementation. We applied LBPNet and its variants on standard benchmarks. The results are promising across benchmarks while provid- ing an important means to improve memory and speed efficiency that is particularly suited for small footprint devices and hardware accelerators.



### Visual Psychophysics for Making Face Recognition Algorithms More Explainable
- **Arxiv ID**: http://arxiv.org/abs/1803.07140v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07140v2)
- **Published**: 2018-03-19 19:50:54+00:00
- **Updated**: 2018-07-19 18:07:53+00:00
- **Authors**: Brandon RichardWebster, So Yon Kwon, Christopher Clarizio, Samuel E. Anthony, Walter J. Scheirer
- **Comment**: 20 pages, 5 figures. To appear Proceedings of the European Conference
  on Computer Vision (ECCV). For supplemental material see
  http://bjrichardwebster.com/papers/menagerie/supp
- **Journal**: None
- **Summary**: Scientific fields that are interested in faces have developed their own sets of concepts and procedures for understanding how a target model system (be it a person or algorithm) perceives a face under varying conditions. In computer vision, this has largely been in the form of dataset evaluation for recognition tasks where summary statistics are used to measure progress. While aggregate performance has continued to improve, understanding individual causes of failure has been difficult, as it is not always clear why a particular face fails to be recognized, or why an impostor is recognized by an algorithm. Importantly, other fields studying vision have addressed this via the use of visual psychophysics: the controlled manipulation of stimuli and careful study of the responses they evoke in a model system. In this paper, we suggest that visual psychophysics is a viable methodology for making face recognition algorithms more explainable. A comprehensive set of procedures is developed for assessing face recognition algorithm behavior, which is then deployed over state-of-the-art convolutional neural networks and more basic, yet still widely used, shallow and handcrafted feature-based approaches.



### Attention-based Temporal Weighted Convolutional Neural Network for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.07179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07179v1)
- **Published**: 2018-03-19 22:09:49+00:00
- **Updated**: 2018-03-19 22:09:49+00:00
- **Authors**: Jinliang Zang, Le Wang, Ziyi Liu, Qilin Zhang, Zhenxing Niu, Gang Hua, Nanning Zheng
- **Comment**: 14th International Conference on Artificial Intelligence Applications
  and Innovations (AIAI 2018), May 25-27, 2018, Rhodes, Greece
- **Journal**: None
- **Summary**: Research in human action recognition has accelerated significantly since the introduction of powerful machine learning tools such as Convolutional Neural Networks (CNNs). However, effective and efficient methods for incorporation of temporal information into CNNs are still being actively explored in the recent literature. Motivated by the popular recurrent attention models in the research area of natural language processing, we propose the Attention-based Temporal Weighted CNN (ATW), which embeds a visual attention model into a temporal weighted multi-stream CNN. This attention model is simply implemented as temporal weighting yet it effectively boosts the recognition performance of video representations. Besides, each stream in the proposed ATW framework is capable of end-to-end training, with both network parameters and temporal weights optimized by stochastic gradient descent (SGD) with backpropagation. Our experiments show that the proposed attention mechanism contributes substantially to the performance gains with the more discriminative snippets by focusing on more relevant video segments.



### Unveiling the invisible - mathematical methods for restoring and interpreting illuminated manuscripts
- **Arxiv ID**: http://arxiv.org/abs/1803.07187v1
- **DOI**: 10.1186/s40494-018-0216-z
- **Categories**: **cs.CV**, eess.IV, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1803.07187v1)
- **Published**: 2018-03-19 22:50:53+00:00
- **Updated**: 2018-03-19 22:50:53+00:00
- **Authors**: Luca Calatroni, Marie d'Autume, Rob Hocking, Stella Panayotova, Simone Parisotto, Paola Ricciardi, Carola-Bibiane Schönlieb
- **Comment**: None
- **Journal**: None
- **Summary**: The last fifty years have seen an impressive development of mathematical methods for the analysis and processing of digital images, mostly in the context of photography, biomedical imaging and various forms of engineering. The arts have been mostly overlooked in this process, apart from a few exceptional works in the last ten years. With the rapid emergence of digitisation in the arts, however, the arts domain is becoming increasingly receptive to digital image processing methods and the importance of paying attention to this therefore increases. In this paper we discuss a range of mathematical methods for digital image restoration and digital visualisation for illuminated manuscripts. The latter provide an interesting opportunity for digital manipulation because they traditionally remain physically untouched. At the same time they also serve as an example for the possibilities mathematics and digital restoration offer as a generic and objective toolkit for the arts.



### A Minimalist Approach to Type-Agnostic Detection of Quadrics in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1803.07191v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.07191v1)
- **Published**: 2018-03-19 23:01:01+00:00
- **Updated**: 2018-03-19 23:01:01+00:00
- **Authors**: Tolga Birdal, Benjamin Busam, Nassir Navab, Slobodan Ilic, Peter Sturm
- **Comment**: Accepted for publication at CVPR 2018
- **Journal**: None
- **Summary**: This paper proposes a segmentation-free, automatic and efficient procedure to detect general geometric quadric forms in point clouds, where clutter and occlusions are inevitable. Our everyday world is dominated by man-made objects which are designed using 3D primitives (such as planes, cones, spheres, cylinders, etc.). These objects are also omnipresent in industrial environments. This gives rise to the possibility of abstracting 3D scenes through primitives, thereby positions these geometric forms as an integral part of perception and high level 3D scene understanding.   As opposed to state-of-the-art, where a tailored algorithm treats each primitive type separately, we propose to encapsulate all types in a single robust detection procedure. At the center of our approach lies a closed form 3D quadric fit, operating in both primal & dual spaces and requiring as low as 4 oriented-points. Around this fit, we design a novel, local null-space voting strategy to reduce the 4-point case to 3. Voting is coupled with the famous RANSAC and makes our algorithm orders of magnitude faster than its conventional counterparts. This is the first method capable of performing a generic cross-type multi-object primitive detection in difficult scenes. Results on synthetic and real datasets support the validity of our method.



### Diagnostic Classification Of Lung Nodules Using 3D Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.07192v1
- **DOI**: 10.1109/ISBI.2018.8363687
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.07192v1)
- **Published**: 2018-03-19 23:02:37+00:00
- **Updated**: 2018-03-19 23:02:37+00:00
- **Authors**: Raunak Dey, Zhongjie Lu, Yi Hong
- **Comment**: Accepted for publication in IEEE International Symposium on
  Biomedical Imaging (ISBI) 2018 Copyright c 2018 IEEE
- **Journal**: None
- **Summary**: Lung cancer is the leading cause of cancer-related death worldwide. Early diagnosis of pulmonary nodules in Computed Tomography (CT) chest scans provides an opportunity for designing effective treatment and making financial and care plans. In this paper, we consider the problem of diagnostic classification between benign and malignant lung nodules in CT images, which aims to learn a direct mapping from 3D images to class labels. To achieve this goal, four two-pathway Convolutional Neural Networks (CNN) are proposed, including a basic 3D CNN, a novel multi-output network, a 3D DenseNet, and an augmented 3D DenseNet with multi-outputs. These four networks are evaluated on the public LIDC-IDRI dataset and outperform most existing methods. In particular, the 3D multi-output DenseNet (MoDenseNet) achieves the state-of-the-art classification accuracy on the task of end-to-end lung nodule diagnosis. In addition, the networks pretrained on the LIDC-IDRI dataset can be further extended to handle smaller datasets using transfer learning. This is demonstrated on our dataset with encouraging prediction accuracy in lung nodule classification.



### Adaptive Polar Active Contour for Segmentation and Tracking in Ultrasound Videos
- **Arxiv ID**: http://arxiv.org/abs/1803.07195v1
- **DOI**: 10.1109/TCSVT.2018.2818072
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.07195v1)
- **Published**: 2018-03-19 23:30:06+00:00
- **Updated**: 2018-03-19 23:30:06+00:00
- **Authors**: Ebrahim Karami, Mohamed Shehata, Andrew Smith
- **Comment**: Accepted for publication in IEEE Transactions on Circuit and Systems
  for Video Technology
- **Journal**: None
- **Summary**: Detection of relative changes in circulating blood volume is important to guide resuscitation and manage a variety of medical conditions including sepsis, trauma, dialysis and congestive heart failure. Recent studies have shown that estimates of circulating blood volume can be obtained from the cross-sectional area (CSA) of the internal jugular vein (IJV) from ultrasound images. However, accurate segmentation and tracking of the IJV in ultrasound imaging is a challenging task and is significantly influenced by a number of parameters such as the image quality, shape, and temporal variation. In this paper, we propose a novel adaptive polar active contour (Ad-PAC) algorithm for the segmentation and tracking of the IJV in ultrasound videos. In the proposed algorithm, the parameters of the Ad-PAC algorithm are adapted based on the results of segmentation in previous frames. The Ad-PAC algorithm is applied to 65 ultrasound videos captured from 13 healthy subjects, with each video containing 450 frames. The results show that spatial and temporal adaptation of the energy function significantly improves segmentation performance when compared to current state-of-the-art active contour algorithms.



