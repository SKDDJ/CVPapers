# Arxiv Papers in cs.CV on 2018-03-09
### Adversarial Training for Adverse Conditions: Robust Metric Localisation using Appearance Transfer
- **Arxiv ID**: http://arxiv.org/abs/1803.03341v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03341v1)
- **Published**: 2018-03-09 00:45:08+00:00
- **Updated**: 2018-03-09 00:45:08+00:00
- **Authors**: Horia Porav, Will Maddern, Paul Newman
- **Comment**: Accepted at ICRA2018
- **Journal**: None
- **Summary**: We present a method of improving visual place recognition and metric localisation under very strong appear- ance change. We learn an invertable generator that can trans- form the conditions of images, e.g. from day to night, summer to winter etc. This image transforming filter is explicitly designed to aid and abet feature-matching using a new loss based on SURF detector and dense descriptor maps. A network is trained to output synthetic images optimised for feature matching given only an input RGB image, and these generated images are used to localize the robot against a previously built map using traditional sparse matching approaches. We benchmark our results using multiple traversals of the Oxford RobotCar Dataset over a year-long period, using one traversal as a map and the other to localise. We show that this method significantly improves place recognition and localisation under changing and adverse conditions, while reducing the number of mapping runs needed to successfully achieve reliable localisation.



### Deep Semantic Face Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1803.03345v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03345v2)
- **Published**: 2018-03-09 01:11:20+00:00
- **Updated**: 2018-03-16 07:32:57+00:00
- **Authors**: Ziyi Shen, Wei-Sheng Lai, Tingfa Xu, Jan Kautz, Ming-Hsuan Yang
- **Comment**: This work is accepted in CVPR 2018. The project website is on
  https://sites.google.com/site/ziyishenmi/cvpr18_face_deblur
- **Journal**: None
- **Summary**: In this paper, we present an effective and efficient face deblurring algorithm by exploiting semantic cues via deep convolutional neural networks (CNNs). As face images are highly structured and share several key semantic components (e.g., eyes and mouths), the semantic information of a face provides a strong prior for restoration. As such, we propose to incorporate global semantic priors as input and impose local structure losses to regularize the output within a multi-scale deep CNN. We train the network with perceptual and adversarial losses to generate photo-realistic results and develop an incremental training strategy to handle random blur kernels in the wild. Quantitative and qualitative evaluations demonstrate that the proposed face deblurring algorithm restores sharp images with more facial details and performs favorably against state-of-the-art methods in terms of restoration quality, face recognition and execution speed.



### Tracking by Prediction: A Deep Generative Model for Mutli-Person localisation and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1803.03347v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03347v1)
- **Published**: 2018-03-09 01:14:30+00:00
- **Updated**: 2018-03-09 01:14:30+00:00
- **Authors**: Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: To appear in IEEE Winter Conference on Applications of Computer
  Vision (WACV), 2018
- **Journal**: None
- **Summary**: Current multi-person localisation and tracking systems have an over reliance on the use of appearance models for target re-identification and almost no approaches employ a complete deep learning solution for both objectives. We present a novel, complete deep learning framework for multi-person localisation and tracking. In this context we first introduce a light weight sequential Generative Adversarial Network architecture for person localisation, which overcomes issues related to occlusions and noisy detections, typically found in a multi person environment. In the proposed tracking framework we build upon recent advances in pedestrian trajectory prediction approaches and propose a novel data association scheme based on predicted trajectories. This removes the need for computationally expensive person re-identification systems based on appearance features and generates human like trajectories with minimal fragmentation. The proposed method is evaluated on multiple public benchmarks including both static and dynamic cameras and is capable of generating outstanding performance, especially among other recently proposed deep neural network based approaches.



### Indoor Scene Understanding in 2.5/3D for Autonomous Agents: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1803.03352v2
- **DOI**: 10.1109/ACCESS.2018.2886133
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03352v2)
- **Published**: 2018-03-09 02:02:26+00:00
- **Updated**: 2019-01-10 07:56:08+00:00
- **Authors**: Muzammal Naseer, Salman H Khan, Fatih Porikli
- **Comment**: IEEE Access
- **Journal**: Year: DECEMBER 2019, Volume: 7, Issue:1, Page(s): 1859-1887,
- **Summary**: With the availability of low-cost and compact 2.5/3D visual sensing devices, computer vision community is experiencing a growing interest in visual scene understanding of indoor environments. This survey paper provides a comprehensive background to this research topic. We begin with a historical perspective, followed by popular 3D data representations and a comparative analysis of available datasets. Before delving into the application specific details, this survey provides a succinct introduction to the core technologies that are the underlying methods extensively used in the literature. Afterwards, we review the developed techniques according to a taxonomy based on the scene understanding tasks. This covers holistic indoor scene understanding as well as subtasks such as scene classification, object detection, pose estimation, semantic segmentation, 3D reconstruction, saliency detection, physics-based reasoning and affordance prediction. Later on, we summarize the performance metrics used for evaluation in different tasks and a quantitative comparison among the recent state-of-the-art techniques. We conclude this review with the current challenges and an outlook towards the open research problems requiring further investigation.



### Task Specific Visual Saliency Prediction with Memory Augmented Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.03354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03354v1)
- **Published**: 2018-03-09 02:08:09+00:00
- **Updated**: 2018-03-09 02:08:09+00:00
- **Authors**: Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: To appear in IEEE Winter Conference on Applications of Computer
  Vision (WACV), 2018
- **Journal**: None
- **Summary**: Visual saliency patterns are the result of a variety of factors aside from the image being parsed, however existing approaches have ignored these. To address this limitation, we propose a novel saliency estimation model which leverages the semantic modelling power of conditional generative adversarial networks together with memory architectures which capture the subject's behavioural patterns and task dependent factors. We make contributions aiming to bridge the gap between bottom-up feature learning capabilities in modern deep learning architectures and traditional top-down hand-crafted features based methods for task specific saliency modelling. The conditional nature of the proposed framework enables us to learn contextual semantics and relationships among different tasks together, instead of learning them separately for each task. Our studies not only shed light on a novel application area for generative adversarial networks, but also emphasise the importance of task specific saliency modelling and demonstrate the plausibility of fully capturing this context via an augmented memory architecture.



### Learning a Discriminative Prior for Blind Image Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1803.03363v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03363v2)
- **Published**: 2018-03-09 02:48:10+00:00
- **Updated**: 2018-04-04 18:46:53+00:00
- **Authors**: Lerenhan Li, Jinshan Pan, Wei-Sheng Lai, Changxin Gao, Nong Sang, Ming-Hsuan Yang
- **Comment**: This paper is accepted by CVPR2018 as poster
- **Journal**: None
- **Summary**: We present an effective blind image deblurring method based on a data-driven discriminative prior.Our work is motivated by the fact that a good image prior should favor clear images over blurred images.In this work, we formulate the image prior as a binary classifier which can be achieved by a deep convolutional neural network (CNN).The learned prior is able to distinguish whether an input image is clear or not.Embedded into the maximum a posterior (MAP) framework, it helps blind deblurring in various scenarios, including natural, face, text, and low-illumination images.However, it is difficult to optimize the deblurring method with the learned image prior as it involves a non-linear CNN.Therefore, we develop an efficient numerical approach based on the half-quadratic splitting method and gradient decent algorithm to solve the proposed model.Furthermore, the proposed model can be easily extended to non-uniform deblurring.Both qualitative and quantitative experimental results show that our method performs favorably against state-of-the-art algorithms as well as domain-specific image deblurring approaches.



### Review of Visual Saliency Detection with Comprehensive Information
- **Arxiv ID**: http://arxiv.org/abs/1803.03391v2
- **DOI**: 10.1109/TCSVT.2018.2870832
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03391v2)
- **Published**: 2018-03-09 05:55:33+00:00
- **Updated**: 2018-09-14 04:29:18+00:00
- **Authors**: Runmin Cong, Jianjun Lei, Huazhu Fu, Ming-Ming Cheng, Weisi Lin, Qingming Huang
- **Comment**: 18 pages, 11 figures, 7 tables, Accepted by IEEE Transactions on
  Circuits and Systems for Video Technology 2018, https://rmcong.github.io/
- **Journal**: IEEE TCSVT 2018
- **Summary**: Visual saliency detection model simulates the human visual system to perceive the scene, and has been widely used in many vision tasks. With the acquisition technology development, more comprehensive information, such as depth cue, inter-image correspondence, or temporal relationship, is available to extend image saliency detection to RGBD saliency detection, co-saliency detection, or video saliency detection. RGBD saliency detection model focuses on extracting the salient regions from RGBD images by combining the depth information. Co-saliency detection model introduces the inter-image correspondence constraint to discover the common salient object in an image group. The goal of video saliency detection model is to locate the motion-related salient object in video sequences, which considers the motion cue and spatiotemporal constraint jointly. In this paper, we review different types of saliency detection algorithms, summarize the important issues of the existing methods, and discuss the existent problems and future works. Moreover, the evaluation datasets and quantitative measurements are briefly introduced, and the experimental analysis and discission are conducted to provide a holistic overview of different saliency detection methods.



### Image Registration Based Flicker Solving in Video Face Replacement and Analysis Based Sub-pixel Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1803.05851v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05851v1)
- **Published**: 2018-03-09 06:37:53+00:00
- **Updated**: 2018-03-09 06:37:53+00:00
- **Authors**: Xiaofang Wang, Guoqiang Xiang, Xinyue Zhang, Wei Wei
- **Comment**: 9 Pages, 10 figures, partly accepted by 2017 Intelligent Computing
  and Information Systems (ICIS2017)
- **Journal**: None
- **Summary**: In this paper, a framework of video face replacement is proposed and it deals with the flicker of swapped face in video sequence. This framework contains two main innovations: 1) the technique of image registration is exploited to align the source and target video faces for eliminating the flicker or jitter of the segmented video face sequence; 2) a fast subpixel image registration method is proposed for farther accuracy and efficiency. Unlike the priori works, it minimizes the overlapping region and takes spatiotemporal coherence into account. Flicker in resulted videos is usually caused by the frequently changed bound of the blending target face and unregistered faces between and along video sequences. The subpixel image registration method is proposed to solve the flicker problem. During the alignment process, integer pixel registration is formulated by maximizing the similarity of images with down sampling strategy speeding up the process and sub-pixel image registration is a single-step image match via analytic method. Experimental results show the proposed algorithm reduces the computation time and gets a high accuracy when conducting experiments on different data sets.



### Cross-View Image Synthesis using Conditional GANs
- **Arxiv ID**: http://arxiv.org/abs/1803.03396v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03396v2)
- **Published**: 2018-03-09 06:53:36+00:00
- **Updated**: 2018-03-29 17:18:49+00:00
- **Authors**: Krishna Regmi, Ali Borji
- **Comment**: Accepted at CVPR 2018
- **Journal**: None
- **Summary**: Learning to generate natural scenes has always been a challenging task in computer vision. It is even more painstaking when the generation is conditioned on images with drastically different views. This is mainly because understanding, corresponding, and transforming appearance and semantic information across the views is not trivial. In this paper, we attempt to solve the novel problem of cross-view image synthesis, aerial to street-view and vice versa, using conditional generative adversarial networks (cGAN). Two new architectures called Crossview Fork (X-Fork) and Crossview Sequential (X-Seq) are proposed to generate scenes with resolutions of 64x64 and 256x256 pixels. X-Fork architecture has a single discriminator and a single generator. The generator hallucinates both the image and its semantic segmentation in the target view. X-Seq architecture utilizes two cGANs. The first one generates the target image which is subsequently fed to the second cGAN for generating its corresponding semantic segmentation map. The feedback from the second cGAN helps the first cGAN generate sharper images. Both of our proposed architectures learn to generate natural images as well as their semantic segmentation maps. The proposed methods show that they are able to capture and maintain the true semantics of objects in source and target views better than the traditional image-to-image translation method which considers only the visual appearance of the scene. Extensive qualitative and quantitative evaluations support the effectiveness of our frameworks, compared to two state of the art methods, for natural scene generation across drastically different views.



### Fusing Hierarchical Convolutional Features for Human Body Segmentation and Clothing Fashion Classification
- **Arxiv ID**: http://arxiv.org/abs/1803.03415v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03415v2)
- **Published**: 2018-03-09 08:46:21+00:00
- **Updated**: 2018-03-13 02:39:09+00:00
- **Authors**: Zheng Zhang, Chengfang Song, Qin Zou
- **Comment**: None
- **Journal**: None
- **Summary**: The clothing fashion reflects the common aesthetics that people share with each other in dressing. To recognize the fashion time of a clothing is meaningful for both an individual and the industry. In this paper, under the assumption that the clothing fashion changes year by year, the fashion-time recognition problem is mapped into a clothing-fashion classification problem. Specifically, a novel deep neural network is proposed which achieves accurate human body segmentation by fusing multi-scale convolutional features in a fully convolutional network, and then feature learning and fashion classification are performed on the segmented parts avoiding the influence of image background. In the experiments, 9,339 fashion images from 8 continuous years are collected for performance evaluation. The results demonstrate the effectiveness of the proposed body segmentation and fashion classification methods.



### Robust Landmark Detection for Alignment of Mouse Brain Section Images
- **Arxiv ID**: http://arxiv.org/abs/1803.03420v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03420v1)
- **Published**: 2018-03-09 08:58:18+00:00
- **Updated**: 2018-03-09 08:58:18+00:00
- **Authors**: Yuncong Chen, David Kleinfeld, Martyn Goulding, Yoav Freund
- **Comment**: Submitted to MICCAI 2015
- **Journal**: None
- **Summary**: Brightfield and fluorescent imaging of whole brain sections are funda- mental tools of research in mouse brain study. As sectioning and imaging become more efficient, there is an increasing need to automate the post-processing of sec- tions for alignment and three dimensional visualization. There is a further need to facilitate the development of a digital atlas, i.e. a brain-wide map annotated with cell type and tract tracing data, which would allow the automatic registra- tion of images stacks to a common coordinate system. Currently, registration of slices requires manual identification of landmarks. In this work we describe the first steps in developing a semi-automated system to construct a histology at- las of mouse brainstem that combines atlas-guided annotation, landmark-based registration and atlas generation in an iterative framework. We describe an unsu- pervised approach for identifying and matching region and boundary landmarks, based on modelling texture. Experiments show that the detected landmarks corre- spond well with brain structures, and matching is robust under distortion. These results will serve as the basis for registration and atlas building.



### Solving Fourier ptychographic imaging problems via neural network modeling and TensorFlow
- **Arxiv ID**: http://arxiv.org/abs/1803.03434v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1803.03434v1)
- **Published**: 2018-03-09 09:38:32+00:00
- **Updated**: 2018-03-09 09:38:32+00:00
- **Authors**: Shaowei Jiang, Kaikai Guo, Jun Liao, Guoan Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Fourier ptychography is a recently developed imaging approach for large field-of-view and high-resolution microscopy. Here we model the Fourier ptychographic forward imaging process using a convolution neural network (CNN) and recover the complex object information in the network training process. In this approach, the input of the network is the point spread function in the spatial domain or the coherent transfer function in the Fourier domain. The object is treated as 2D learnable weights of a convolution or a multiplication layer. The output of the network is modeled as the loss function we aim to minimize. The batch size of the network corresponds to the number of captured low-resolution images in one forward / backward pass. We use a popular open-source machine learning library, TensorFlow, for setting up the network and conducting the optimization process. We analyze the performance of different learning rates, different solvers, and different batch sizes. It is shown that a large batch size with the Adam optimizer achieves the best performance in general. To accelerate the phase retrieval process, we also discuss a strategy to implement Fourier-magnitude projection using a multiplication neural network model. Since convolution and multiplication are the two most-common operations in imaging modeling, the reported approach may provide a new perspective to examine many coherent and incoherent systems. As a demonstration, we discuss the extensions of the reported networks for modeling single-pixel imaging and structured illumination microscopy (SIM). 4-frame resolution doubling is demonstrated using a neural network for SIM. We have made our implementation code open-source for the broad research community.



### An end-to-end TextSpotter with Explicit Alignment and Attention
- **Arxiv ID**: http://arxiv.org/abs/1803.03474v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03474v3)
- **Published**: 2018-03-09 11:30:51+00:00
- **Updated**: 2018-03-23 02:49:42+00:00
- **Authors**: Tong He, Zhi Tian, Weilin Huang, Chunhua Shen, Yu Qiao, Changming Sun
- **Comment**: Accepted to IEEE Conf. Computer Vision and Pattern Recognition (CVPR)
  2018. Code is available at: https://github.com/tonghe90/textspotter
- **Journal**: None
- **Summary**: Text detection and recognition in natural images have long been considered as two separate tasks that are processed sequentially. Training of two tasks in a unified framework is non-trivial due to significant dif- ferences in optimisation difficulties. In this work, we present a conceptually simple yet efficient framework that simultaneously processes the two tasks in one shot. Our main contributions are three-fold: 1) we propose a novel text-alignment layer that allows it to precisely compute convolutional features of a text instance in ar- bitrary orientation, which is the key to boost the per- formance; 2) a character attention mechanism is introduced by using character spatial information as explicit supervision, leading to large improvements in recognition; 3) two technologies, together with a new RNN branch for word recognition, are integrated seamlessly into a single model which is end-to-end trainable. This allows the two tasks to work collaboratively by shar- ing convolutional features, which is critical to identify challenging text instances. Our model achieves impressive results in end-to-end recognition on the ICDAR2015 dataset, significantly advancing most recent results, with improvements of F-measure from (0.54, 0.51, 0.47) to (0.82, 0.77, 0.63), by using a strong, weak and generic lexicon respectively. Thanks to joint training, our method can also serve as a good detec- tor by achieving a new state-of-the-art detection performance on two datasets.



### Cooperative Starting Movement Detection of Cyclists Using Convolutional Neural Networks and a Boosted Stacking Ensemble
- **Arxiv ID**: http://arxiv.org/abs/1803.03487v2
- **DOI**: 10.1109/TIV.2018.2873900
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1803.03487v2)
- **Published**: 2018-03-09 12:27:14+00:00
- **Updated**: 2018-10-09 13:05:43+00:00
- **Authors**: Maarten Bieshaar, Stefan Zernetsch, Andreas Hubert, Bernhard Sick, Konrad Doll
- **Comment**: 10 Pages, 22 figures, accepted for Special Issue of IEEE Transactions
  on Intelligent Vehicles
- **Journal**: IEEE Transactions on Intelligent Vehicles 3 (2018), Nr. 4
- **Summary**: In future, vehicles and other traffic participants will be interconnected and equipped with various types of sensors, allowing for cooperation on different levels, such as situation prediction or intention detection. In this article we present a cooperative approach for starting movement detection of cyclists using a boosted stacking ensemble approach realizing feature- and decision level cooperation. We introduce a novel method based on a 3D Convolutional Neural Network (CNN) to detect starting motions on image sequences by learning spatio-temporal features. The CNN is complemented by a smart device based starting movement detection originating from smart devices carried by the cyclist. Both model outputs are combined in a stacking ensemble approach using an extreme gradient boosting classifier resulting in a fast and yet robust cooperative starting movement detector. We evaluate our cooperative approach on real-world data originating from experiments with 49 test subjects consisting of 84 starting motions.



### An Integrated Inverse Space Sparse Representation Framework for Tumor Classification
- **Arxiv ID**: http://arxiv.org/abs/1803.03562v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03562v4)
- **Published**: 2018-03-09 15:29:20+00:00
- **Updated**: 2019-02-27 22:35:11+00:00
- **Authors**: Xiaohui Yang, Wenming Wu, Yunmei Chen, Xianqi Li, Juan Zhang, Dan Long, Lijun Yang
- **Comment**: 40 pages, 18 figures, 13 tables
- **Journal**: None
- **Summary**: Microarray gene expression data-based tumor classification is an active and challenging issue. In this paper, an integrated tumor classification framework is presented, which aims to exploit information in existing available samples, and focuses on the small sample problem and unbalanced classification problem. Firstly, an inverse space sparse representation based classification (ISSRC) model is proposed by considering the characteristics of gene-based tumor data, such as sparsity and a small number of training samples. A decision information factors (DIF)-based gene selection method is constructed to enhance the representation ability of the ISSRC. It is worth noting that the DIF is established from reducing clinical misdiagnosis rate and dimension of small sample data. For further improving the representation ability and classification stability of the ISSRC, feature learning is conducted on the selected gene subset. The feature learning method is constructed by complementing the advantages of non-negative matrix factorization (NMF) and deep learning. Without confusion, the ISSRC combined with gene selection and feature learning is called the integrated ISSRC, whose stability, optimization and the corresponding convergence are analyzed. Extensive experiments on six public microarray gene expression datasets show the integrated ISSRC-based tumor classification framework is superior to classical and state-of-the-art methods. There are significant improvements in classification accuracy, specificity and sensitivity, whether there is a tumor in the early diagnosis, what kind of tumor, or whether metastasis occurs after tumor surgery.



### Intentions of Vulnerable Road Users - Detection and Forecasting by Means of Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1803.03577v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03577v1)
- **Published**: 2018-03-09 15:49:07+00:00
- **Updated**: 2018-03-09 15:49:07+00:00
- **Authors**: Michael Goldhammer, Sebastian Köhler, Stefan Zernetsch, Konrad Doll, Bernhard Sick, Klaus Dietmayer
- **Comment**: None
- **Journal**: None
- **Summary**: Avoiding collisions with vulnerable road users (VRUs) using sensor-based early recognition of critical situations is one of the manifold opportunities provided by the current development in the field of intelligent vehicles. As especially pedestrians and cyclists are very agile and have a variety of movement options, modeling their behavior in traffic scenes is a challenging task. In this article we propose movement models based on machine learning methods, in particular artificial neural networks, in order to classify the current motion state and to predict the future trajectory of VRUs. Both model types are also combined to enable the application of specifically trained motion predictors based on a continuously updated pseudo probabilistic state classification. Furthermore, the architecture is used to evaluate motion-specific physical models for starting and stopping and video-based pedestrian motion classification. A comprehensive dataset consisting of 1068 pedestrian and 494 cyclist scenes acquired at an urban intersection is used for optimization, training, and evaluation of the different models. The results show substantial higher classification rates and the ability to earlier recognize motion state changes with the machine learning approaches compared to interacting multiple model (IMM) Kalman Filtering. The trajectory prediction quality is also improved for all kinds of test scenes, especially when starting and stopping motions are included. Here, 37\% and 41\% lower position errors were achieved on average, respectively.



### Local Kernels that Approximate Bayesian Regularization and Proximal Operators
- **Arxiv ID**: http://arxiv.org/abs/1803.03711v1
- **DOI**: 10.1109/TIP.2019.2893071
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03711v1)
- **Published**: 2018-03-09 22:19:38+00:00
- **Updated**: 2018-03-09 22:19:38+00:00
- **Authors**: Frank Ong, Peyman Milanfar, Pascal Getreuer
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we broadly connect kernel-based filtering (e.g. approaches such as the bilateral filters and nonlocal means, but also many more) with general variational formulations of Bayesian regularized least squares, and the related concept of proximal operators. The latter set of variational/Bayesian/proximal formulations often result in optimization problems that do not have closed-form solutions, and therefore typically require global iterative solutions. Our main contribution here is to establish how one can approximate the solution of the resulting global optimization problems with use of locally adaptive filters with specific kernels. Our results are valid for small regularization strength but the approach is powerful enough to be useful for a wide range of applications because we expose how to derive a "kernelized" solution to these problems that approximates the global solution in one-shot, using only local operations. As another side benefit in the reverse direction, given a local data-adaptive filter constructed with a particular choice of kernel, we enable the interpretation of such filters in the variational/Bayesian/proximal framework.



