# Arxiv Papers in cs.CV on 2018-03-29
### Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.10892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10892v1)
- **Published**: 2018-03-29 01:24:02+00:00
- **Updated**: 2018-03-29 01:24:02+00:00
- **Authors**: Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, Alexandre Alahi
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding human motion behavior is critical for autonomous moving platforms (like self-driving cars and social robots) if they are to navigate human-centric environments. This is challenging because human motion is inherently multimodal: given a history of human motion paths, there are many socially plausible ways that people could move in the future. We tackle this problem by combining tools from sequence prediction and generative adversarial networks: a recurrent sequence-to-sequence model observes motion histories and predicts future behavior, using a novel pooling mechanism to aggregate information across people. We predict socially plausible futures by training adversarially against a recurrent discriminator, and encourage diverse predictions with a novel variety loss. Through experiments on several datasets we demonstrate that our approach outperforms prior work in terms of accuracy, variety, collision avoidance, and computational complexity.



### Deep Texture Manifold for Ground Terrain Recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.10896v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10896v2)
- **Published**: 2018-03-29 01:56:54+00:00
- **Updated**: 2018-04-03 01:09:04+00:00
- **Authors**: Jia Xue, Hang Zhang, Kristin Dana
- **Comment**: None
- **Journal**: None
- **Summary**: We present a texture network called Deep Encoding Pooling Network (DEP) for the task of ground terrain recognition. Recognition of ground terrain is an important task in establishing robot or vehicular control parameters, as well as for localization within an outdoor environment. The architecture of DEP integrates orderless texture details and local spatial information and the performance of DEP surpasses state-of-the-art methods for this task. The GTOS database (comprised of over 30,000 images of 40 classes of ground terrain in outdoor scenes) enables supervised recognition. For evaluation under realistic conditions, we use test images that are not from the existing GTOS dataset, but are instead from hand-held mobile phone videos of similar terrain. This new evaluation dataset, GTOS-mobile, consists of 81 videos of 31 classes of ground terrain such as grass, gravel, asphalt and sand. The resultant network shows excellent performance not only for GTOS-mobile, but also for more general databases (MINC and DTD). Leveraging the discriminant features learned from this network, we build a new texture manifold called DEP-manifold. We learn a parametric distribution in feature space in a fully supervised manner, which gives the distance relationship among classes and provides a means to implicitly represent ambiguous class boundaries. The source code and database are publicly available.



### Motion-Appearance Co-Memory Networks for Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1803.10906v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10906v1)
- **Published**: 2018-03-29 02:47:49+00:00
- **Updated**: 2018-03-29 02:47:49+00:00
- **Authors**: Jiyang Gao, Runzhou Ge, Kan Chen, Ram Nevatia
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Video Question Answering (QA) is an important task in understanding video temporal structure. We observe that there are three unique attributes of video QA compared with image QA: (1) it deals with long sequences of images containing richer information not only in quantity but also in variety; (2) motion and appearance information are usually correlated with each other and able to provide useful attention cues to the other; (3) different questions require different number of frames to infer the answer. Based these observations, we propose a motion-appearance comemory network for video QA. Our networks are built on concepts from Dynamic Memory Network (DMN) and introduces new mechanisms for video QA. Specifically, there are three salient aspects: (1) a co-memory attention mechanism that utilizes cues from both motion and appearance to generate attention; (2) a temporal conv-deconv network to generate multi-level contextual facts; (3) a dynamic fact ensemble method to construct temporal representation dynamically for different questions. We evaluate our method on TGIF-QA dataset, and the results outperform state-of-the-art significantly on all four tasks of TGIF-QA.



### Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling Perspective
- **Arxiv ID**: http://arxiv.org/abs/1803.10910v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10910v1)
- **Published**: 2018-03-29 03:05:28+00:00
- **Updated**: 2018-03-29 03:05:28+00:00
- **Authors**: Jing Zhang, Tong Zhang, Yuchao Dai, Mehrtash Harandi, Richard Hartley
- **Comment**: Accepted by IEEE/CVF CVPR 2018 as Spotlight
- **Journal**: None
- **Summary**: The success of current deep saliency detection methods heavily depends on the availability of large-scale supervision in the form of per-pixel labeling. Such supervision, while labor-intensive and not always possible, tends to hinder the generalization ability of the learned models. By contrast, traditional handcrafted features based unsupervised saliency detection methods, even though have been surpassed by the deep supervised methods, are generally dataset-independent and could be applied in the wild. This raises a natural question that "Is it possible to learn saliency maps without using labeled data while improving the generalization ability?". To this end, we present a novel perspective to unsupervised saliency detection through learning from multiple noisy labeling generated by "weak" and "noisy" unsupervised handcrafted saliency methods. Our end-to-end deep learning framework for unsupervised saliency detection consists of a latent saliency prediction module and a noise modeling module that work collaboratively and are optimized jointly. Explicit noise modeling enables us to deal with noisy saliency maps in a probabilistic way. Extensive experimental results on various benchmarking datasets show that our model not only outperforms all the unsupervised saliency methods with a large margin but also achieves comparable performance with the recent state-of-the-art supervised deep saliency methods.



### Adversarial Binary Coding for Efficient Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1803.10914v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10914v3)
- **Published**: 2018-03-29 03:24:06+00:00
- **Updated**: 2018-04-06 03:24:35+00:00
- **Authors**: Zheng Liu, Jie Qin, Annan Li, Yunhong Wang, Luc Van Gool
- **Comment**: 17 pages, 6 figures, 8 tables. Codes:
  https://github.com/dongb5/AdversarialBinaryCoding4ReID
- **Journal**: None
- **Summary**: Person re-identification (ReID) aims at matching persons across different views/scenes. In addition to accuracy, the matching efficiency has received more and more attention because of demanding applications using large-scale data. Several binary coding based methods have been proposed for efficient ReID, which either learn projections to map high-dimensional features to compact binary codes, or directly adopt deep neural networks by simply inserting an additional fully-connected layer with tanh-like activations. However, the former approach requires time-consuming hand-crafted feature extraction and complicated (discrete) optimizations; the latter lacks the necessary discriminative information greatly due to the straightforward activation functions. In this paper, we propose a simple yet effective framework for efficient ReID inspired by the recent advances in adversarial learning. Specifically, instead of learning explicit projections or adding fully-connected mapping layers, the proposed Adversarial Binary Coding (ABC) framework guides the extraction of binary codes implicitly and effectively. The discriminability of the extracted codes is further enhanced by equipping the ABC with a deep triplet network for the ReID task. More importantly, the ABC and triplet network are simultaneously optimized in an end-to-end manner. Extensive experiments on three large-scale ReID benchmarks demonstrate the superiority of our approach over the state-of-the-art methods.



### B-DCGAN:Evaluation of Binarized DCGAN for FPGA
- **Arxiv ID**: http://arxiv.org/abs/1803.10930v2
- **DOI**: 10.1007/978-3-030-36708-4_5
- **Categories**: **cs.CV**, cs.LG, 00A99
- **Links**: [PDF](http://arxiv.org/pdf/1803.10930v2)
- **Published**: 2018-03-29 05:43:23+00:00
- **Updated**: 2019-11-06 08:57:41+00:00
- **Authors**: Hideo Terada, Hayaru Shouno
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We are trying to implement deep neural networks in the edge computing environment for real-world applications such as the IoT(Internet of Things), the FinTech etc., for the purpose of utilizing the significant achievement of Deep Learning in recent years. Especially, we now focus algorithm implementation on FPGA, because FPGA is one of the promising devices for low-cost and low-power implementation of the edge computer. In this work, we introduce Binary-DCGAN(B-DCGAN) - Deep Convolutional GAN model with binary weights and activations, and with using integer-valued operations in forward pass(train-time and run-time). And we show how to implement B-DCGAN on FPGA(Xilinx Zynq). Using the B-DCGAN, we do feasibility study of FPGA's characteristic and performance for Deep Learning. Because the binarization and using integer-valued operation reduce the memory capacity and the number of the circuit gates, it is very effective for FPGA implementation. On the other hand, the quality of generated data from the model will be decreased by these reductions. So we investigate the influence of these reductions.



### Learning Free-Form Deformations for 3D Object Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1803.10932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10932v1)
- **Published**: 2018-03-29 05:56:15+00:00
- **Updated**: 2018-03-29 05:56:15+00:00
- **Authors**: Dominic Jack, Jhony K. Pontes, Sridha Sridharan, Clinton Fookes, Sareh Shirazi, Frederic Maire, Anders Eriksson
- **Comment**: 16 pages, 7 figures, 3 tables
- **Journal**: Asian Conference on Computer Vision (ACCV) 2018
- **Summary**: Representing 3D shape in deep learning frameworks in an accurate, efficient and compact manner still remains an open challenge. Most existing work addresses this issue by employing voxel-based representations. While these approaches benefit greatly from advances in computer vision by generalizing 2D convolutions to the 3D setting, they also have several considerable drawbacks. The computational complexity of voxel-encodings grows cubically with the resolution thus limiting such representations to low-resolution 3D reconstruction. In an attempt to solve this problem, point cloud representations have been proposed. Although point clouds are more efficient than voxel representations as they only cover surfaces rather than volumes, they do not encode detailed geometric information about relationships between points. In this paper we propose a method to learn free-form deformations (FFD) for the task of 3D reconstruction from a single image. By learning to deform points sampled from a high-quality mesh, our trained model can be used to produce arbitrarily dense point clouds or meshes with fine-grained geometry. We evaluate our proposed framework on both synthetic and real-world data and achieve state-of-the-art results on point-cloud and volumetric metrics. Additionally, we qualitatively demonstrate its applicability to label transferring for 3D semantic segmentation.



### Context-aware Synthesis for Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/1803.10967v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10967v1)
- **Published**: 2018-03-29 08:56:14+00:00
- **Updated**: 2018-03-29 08:56:14+00:00
- **Authors**: Simon Niklaus, Feng Liu
- **Comment**: CVPR 2018, http://graphics.cs.pdx.edu/project/ctxsyn
- **Journal**: None
- **Summary**: Video frame interpolation algorithms typically estimate optical flow or its variations and then use it to guide the synthesis of an intermediate frame between two consecutive original frames. To handle challenges like occlusion, bidirectional flow between the two input frames is often estimated and used to warp and blend the input frames. However, how to effectively blend the two warped frames still remains a challenging problem. This paper presents a context-aware synthesis approach that warps not only the input frames but also their pixel-wise contextual information and uses them to interpolate a high-quality intermediate frame. Specifically, we first use a pre-trained neural network to extract per-pixel contextual information for input frames. We then employ a state-of-the-art optical flow algorithm to estimate bidirectional flow between them and pre-warp both input frames and their context maps. Finally, unlike common approaches that blend the pre-warped frames, our method feeds them and their context maps to a video frame synthesis neural network to produce the interpolated frame in a context-aware fashion. Our neural network is fully convolutional and is trained end to end. Our experiments show that our method can handle challenging scenarios such as occlusion and large motion and outperforms representative state-of-the-art approaches.



### A real-time warning system for rear-end collision based on random forest classifier
- **Arxiv ID**: http://arxiv.org/abs/1803.10988v1
- **DOI**: 10.22115/scce.2020.217605.1172
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10988v1)
- **Published**: 2018-03-29 09:51:26+00:00
- **Updated**: 2018-03-29 09:51:26+00:00
- **Authors**: Fateme Teimouri, Mehdi Ghatee
- **Comment**: 26 Pages, 7 Tables, 16 Figures, Extracted from an MSc project with
  Department of Computer Science, Amirkabir University of Technology, Tehran,
  Iran
- **Journal**: Journal of Soft Computing in Civil Engineering, 4(1), pp. 49-71,
  2020
- **Summary**: Rear-end collision warning system has a great role to enhance the driving safety. In this system some measures are used to estimate the dangers and the system warns drivers to be more cautious. The real-time processes should be executed in such system, to remain enough time and distance to avoid collision with the front vehicle. To this end, in this paper a new system is developed by using random forest classifier. To evaluate the performance of the proposed system, vehicles trajectory data of 100 car's database from Virginia tech transportation institute are used and the methods are compared based on their accuracy and their processing time. By using TOPSIS multi-criteria selection method, we show that the results of the implemented classifier is better than the results of different classifiers including Bayesian network, naive Bayes, MLP neural network, support vector machine, nearest neighbor, rule-based methods and decision tree. The presented experiments reveals that the random forest is an acceptable algorithm for the proposed driver assistant system with 88.4% accuracy for detecting warning situations and 94.7% for detecting safe situations.



### Structured Attention Guided Convolutional Neural Fields for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1803.11029v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11029v1)
- **Published**: 2018-03-29 12:27:34+00:00
- **Updated**: 2018-03-29 12:27:34+00:00
- **Authors**: Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, Elisa Ricci
- **Comment**: Accepted at CVPR 2018 as a spotlight
- **Journal**: None
- **Summary**: Recent works have shown the benefit of integrating Conditional Random Fields (CRFs) models into deep architectures for improving pixel-level prediction tasks. Following this line of research, in this paper we introduce a novel approach for monocular depth estimation. Similarly to previous works, our method employs a continuous CRF to fuse multi-scale information derived from different layers of a front-end Convolutional Neural Network (CNN). Differently from past works, our approach benefits from a structured attention model which automatically regulates the amount of information transferred between corresponding features at different scales. Importantly, the proposed attention model is seamlessly integrated into the CRF, allowing end-to-end training of the entire architecture. Our extensive experimental evaluation demonstrates the effectiveness of the proposed method which is competitive with previous methods on the KITTI benchmark and outperforms the state of the art on the NYU Depth V2 dataset.



### 3D Consistent Biventricular Myocardial Segmentation Using Deep Learning for Mesh Generation
- **Arxiv ID**: http://arxiv.org/abs/1803.11080v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.11080v1)
- **Published**: 2018-03-29 14:08:12+00:00
- **Updated**: 2018-03-29 14:08:12+00:00
- **Authors**: Qiao Zheng, Hervé Delingette, Nicolas Duchateau, Nicholas Ayache
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel automated method to segment the myocardium of both left and right ventricles in MRI volumes. The segmentation is consistent in 3D across the slices such that it can be directly used for mesh generation. Two specific neural networks with multi-scale coarse-to-fine prediction structure are proposed to cope with the small training dataset and trained using an original loss function. The former segments a slice in the middle of the volume. Then the latter iteratively propagates the slice segmentations towards the base and the apex, in a spatially consistent way. We perform 5-fold cross-validation on the 15 cases from STACOM to validate the method. For training, we use real cases and their synthetic variants generated by combining motion simulation and image synthesis. Accurate and consistent testing results are obtained.



### Webcam-based Eye Gaze Tracking under Natural Head Movement
- **Arxiv ID**: http://arxiv.org/abs/1803.11088v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1803.11088v1)
- **Published**: 2018-03-29 14:16:00+00:00
- **Updated**: 2018-03-29 14:16:00+00:00
- **Authors**: Kalin Stefanov
- **Comment**: MSc Thesis in Artificial Intelligence
- **Journal**: None
- **Summary**: This manuscript investigates and proposes a visual gaze tracker that tackles the problem using only an ordinary web camera and no prior knowledge in any sense (scene set-up, camera intrinsic and/or extrinsic parameters). The tracker we propose is based on the observation that our desire to grant the freedom of natural head movement to the user requires 3D modeling of the scene set-up. Although, using a single low resolution web camera bounds us in dimensions (no depth can be recovered), we propose ways to cope with this drawback and model the scene in front of the user. We tackle this three-dimensional problem by realizing that it can be viewed as series of two-dimensional special cases. Then, we propose a procedure that treats each movement of the user's head as a special two-dimensional case, hence reducing the complexity of the problem back to two dimensions. Furthermore, the proposed tracker is calibration free and discards this tedious part of all previously mentioned trackers.   Experimental results show that the proposed tracker achieves good results, given the restrictions on it. We can report that the tracker commits a mean error of (56.95, 70.82) pixels in x and y direction, respectively, when the user's head is as static as possible (no chin-rests are used). Furthermore, we can report that the proposed tracker commits a mean error of (87.18, 103.86) pixels in x and y direction, respectively, under natural head movement.



### Mining on Manifolds: Metric Learning without Labels
- **Arxiv ID**: http://arxiv.org/abs/1803.11095v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11095v1)
- **Published**: 2018-03-29 14:29:46+00:00
- **Updated**: 2018-03-29 14:29:46+00:00
- **Authors**: Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Ondrej Chum
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we present a novel unsupervised framework for hard training example mining. The only input to the method is a collection of images relevant to the target application and a meaningful initial representation, provided e.g. by pre-trained CNN. Positive examples are distant points on a single manifold, while negative examples are nearby points on different manifolds. Both types of examples are revealed by disagreements between Euclidean and manifold similarities. The discovered examples can be used in training with any discriminative loss. The method is applied to unsupervised fine-tuning of pre-trained networks for fine-grained classification and particular object retrieval. Our models are on par or are outperforming prior models that are fully or partially supervised.



### Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision
- **Arxiv ID**: http://arxiv.org/abs/1803.11097v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11097v1)
- **Published**: 2018-03-29 14:33:21+00:00
- **Updated**: 2018-03-29 14:33:21+00:00
- **Authors**: Yaojie Liu, Amin Jourabloo, Xiaoming Liu
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Face anti-spoofing is the crucial step to prevent face recognition systems from a security breach. Previous deep learning approaches formulate face anti-spoofing as a binary classification problem. Many of them struggle to grasp adequate spoofing cues and generalize poorly. In this paper, we argue the importance of auxiliary supervision to guide the learning toward discriminative and generalizable cues. A CNN-RNN model is learned to estimate the face depth with pixel-wise supervision, and to estimate rPPG signals with sequence-wise supervision. Then we fuse the estimated depth and rPPG to distinguish live vs. spoof faces. In addition, we introduce a new face anti-spoofing database that covers a large range of illumination, subject, and pose variations. Experimental results show that our model achieves the state-of-the-art performance on both intra-database and cross-database testing.



### Bag of Recurrence Patterns Representation for Time-Series Classification
- **Arxiv ID**: http://arxiv.org/abs/1803.11111v1
- **DOI**: 10.1007/s10044-018-0703-6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11111v1)
- **Published**: 2018-03-29 15:16:45+00:00
- **Updated**: 2018-03-29 15:16:45+00:00
- **Authors**: Nima Hatami, Yann Gavet, Johan Debayle
- **Comment**: None
- **Journal**: Pattern Analysis and Applications Journal, 2018
- **Summary**: Time-Series Classification (TSC) has attracted a lot of attention in pattern recognition, because wide range of applications from different domains such as finance and health informatics deal with time-series signals. Bag of Features (BoF) model has achieved a great success in TSC task by summarizing signals according to the frequencies of "feature words" of a data-learned dictionary. This paper proposes embedding the Recurrence Plots (RP), a visualization technique for analysis of dynamic systems, in the BoF model for TSC. While the traditional BoF approach extracts features from 1D signal segments, this paper uses the RP to transform time-series into 2D texture images and then applies the BoF on them. Image representation of time-series enables us to explore different visual descriptors that are not available for 1D signals and to treats TSC task as a texture recognition problem. Experimental results on the UCI time-series classification archive demonstrates a significant accuracy boost by the proposed Bag of Recurrence patterns (BoR), compared not only to the existing BoF models, but also to the state-of-the art algorithms.



### Detection of Structural Change in Geographic Regions of Interest by Self Organized Mapping: Las Vegas City and Lake Mead across the Years
- **Arxiv ID**: http://arxiv.org/abs/1803.11125v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.11125v1)
- **Published**: 2018-03-29 15:56:05+00:00
- **Updated**: 2018-03-29 15:56:05+00:00
- **Authors**: John M. Wandeto, Henry O. Nyongesa, Birgitta Dresp-Langley
- **Comment**: None
- **Journal**: None
- **Summary**: Time-series of satellite images may reveal important data about changes in environmental conditions and natural or urban landscape structures that are of potential interest to citizens, historians, or policymakers. We applied a fast method of image analysis using Self Organized Maps (SOM) and, more specifically, the quantization error (QE), for the visualization of critical changes in satellite images of Las Vegas, generated across the years 1984-2008, a period of major restructuration of the urban landscape. As shown in our previous work, the QE from the SOM output is a reliable measure of variability in local image contents. In the present work, we use statistical trend analysis to show how the QE from SOM run on specific geographic regions of interest extracted from satellite images can be exploited to detect both the magnitude and the direction of structural change across time at a glance. Significantly correlated demographic data for the same reference time period are highlighted. The approach is fast and reliable, and can be implemented for the rapid detection of potentially critical changes in time series of large bodies of image data.



### Learning Kinematic Descriptions using SPARE: Simulated and Physical ARticulated Extendable dataset
- **Arxiv ID**: http://arxiv.org/abs/1803.11147v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.11147v1)
- **Published**: 2018-03-29 16:49:14+00:00
- **Updated**: 2018-03-29 16:49:14+00:00
- **Authors**: Abhishek Venkataraman, Brent Griffin, Jason J. Corso
- **Comment**: None
- **Journal**: None
- **Summary**: Next generation robots will need to understand intricate and articulated objects as they cooperate in human environments. To do so, these robots will need to move beyond their current abilities--- working with relatively simple objects in a task-indifferent manner--- toward more sophisticated abilities that dynamically estimate the properties of complex, articulated objects. To that end, we make two compelling contributions toward general articulated (physical) object understanding in this paper. First, we introduce a new dataset, SPARE: Simulated and Physical ARticulated Extendable dataset. SPARE is an extendable open-source dataset providing equivalent simulated and physical instances of articulated objects (kinematic chains), providing the greater research community with a training and evaluation tool for methods generating kinematic descriptions of articulated objects. To the best of our knowledge, this is the first joint visual and physical (3D-printable) dataset for the Vision community. Second, we present a deep neural network that can predit the number of links and the length of the links of an articulated object. These new ideas outperform classical approaches to understanding kinematic chains, such tracking-based methods, which fail in the case of occlusion and do not leverage multiple views when available.



### Security Consideration For Deep Learning-Based Image Forensics
- **Arxiv ID**: http://arxiv.org/abs/1803.11157v2
- **DOI**: 10.1587/transinf.2018EDL8091
- **Categories**: **cs.CV**, cs.LG, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.11157v2)
- **Published**: 2018-03-29 17:06:00+00:00
- **Updated**: 2018-04-03 09:54:20+00:00
- **Authors**: Wei Zhao, Pengpeng Yang, Rongrong Ni, Yao Zhao, Haorui Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, image forensics community has paied attention to the research on the design of effective algorithms based on deep learning technology and facts proved that combining the domain knowledge of image forensics and deep learning would achieve more robust and better performance than the traditional schemes. Instead of improving it, in this paper, the safety of deep learning based methods in the field of image forensics is taken into account. To the best of our knowledge, this is a first work focusing on this topic. Specifically, we experimentally find that the method using deep learning would fail when adding the slight noise into the images (adversarial images). Furthermore, two kinds of strategys are proposed to enforce security of deep learning-based method. Firstly, an extra penalty term to the loss function is added, which is referred to the 2-norm of the gradient of the loss with respect to the input images, and then an novel training method are adopt to train the model by fusing the normal and adversarial images. Experimental results show that the proposed algorithm can achieve good performance even in the case of adversarial images and provide a safety consideration for deep learning-based image forensics



### Towards Open-Set Identity Preserving Face Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1803.11182v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11182v2)
- **Published**: 2018-03-29 17:54:51+00:00
- **Updated**: 2018-08-09 06:55:19+00:00
- **Authors**: Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, Gang Hua
- **Comment**: None
- **Journal**: 2018 IEEE Conference on Computer Vision and Pattern
  Recognition(CVPR 2018)
- **Summary**: We propose a framework based on Generative Adversarial Networks to disentangle the identity and attributes of faces, such that we can conveniently recombine different identities and attributes for identity preserving face synthesis in open domains. Previous identity preserving face synthesis processes are largely confined to synthesizing faces with known identities that are already in the training dataset. To synthesize a face with identity outside the training dataset, our framework requires one input image of that subject to produce an identity vector, and any other input face image to extract an attribute vector capturing, e.g., pose, emotion, illumination, and even the background. We then recombine the identity vector and the attribute vector to synthesize a new face of the subject with the extracted attribute. Our proposed framework does not need to annotate the attributes of faces in any way. It is trained with an asymmetric loss function to better preserve the identity and stabilize the training process. It can also effectively leverage large amounts of unlabeled training face images to further improve the fidelity of the synthesized faces for subjects that are not presented in the labeled training face dataset. Our experiments demonstrate the efficacy of the proposed framework. We also present its usage in a much broader set of applications including face frontalization, face attribute morphing, and face adversarial example detection.



### Unsupervised Textual Grounding: Linking Words to Image Concepts
- **Arxiv ID**: http://arxiv.org/abs/1803.11185v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11185v1)
- **Published**: 2018-03-29 17:58:43+00:00
- **Updated**: 2018-03-29 17:58:43+00:00
- **Authors**: Raymond A. Yeh, Minh N. Do, Alexander G. Schwing
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: Textual grounding, i.e., linking words to objects in images, is a challenging but important task for robotics and human-computer interaction. Existing techniques benefit from recent progress in deep learning and generally formulate the task as a supervised learning problem, selecting a bounding box from a set of possible options. To train these deep net based approaches, access to a large-scale datasets is required, however, constructing such a dataset is time-consuming and expensive. Therefore, we develop a completely unsupervised mechanism for textual grounding using hypothesis testing as a mechanism to link words to detected image concepts. We demonstrate our approach on the ReferIt Game dataset and the Flickr30k data, outperforming baselines by 7.98% and 6.96% respectively.



### Two can play this Game: Visual Dialog with Discriminative Question Generation and Answering
- **Arxiv ID**: http://arxiv.org/abs/1803.11186v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1803.11186v1)
- **Published**: 2018-03-29 17:58:43+00:00
- **Updated**: 2018-03-29 17:58:43+00:00
- **Authors**: Unnat Jain, Svetlana Lazebnik, Alexander Schwing
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: Human conversation is a complex mechanism with subtle nuances. It is hence an ambitious goal to develop artificial intelligence agents that can participate fluently in a conversation. While we are still far from achieving this goal, recent progress in visual question answering, image captioning, and visual question generation shows that dialog systems may be realizable in the not too distant future. To this end, a novel dataset was introduced recently and encouraging results were demonstrated, particularly for question answering. In this paper, we demonstrate a simple symmetric discriminative baseline, that can be applied to both predicting an answer as well as predicting a question. We show that this method performs on par with the state of the art, even memory net based methods. In addition, for the first time on the visual dialog dataset, we assess the performance of a system asking questions, and demonstrate how visual dialog can be generated from discriminative question generation and question answering.



### MaskRNN: Instance Level Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.11187v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11187v1)
- **Published**: 2018-03-29 17:58:44+00:00
- **Updated**: 2018-03-29 17:58:44+00:00
- **Authors**: Yuan-Ting Hu, Jia-Bin Huang, Alexander G. Schwing
- **Comment**: Accepted to NIPS 2017
- **Journal**: None
- **Summary**: Instance level video object segmentation is an important technique for video editing and compression. To capture the temporal coherence, in this paper, we develop MaskRNN, a recurrent neural net approach which fuses in each frame the output of two deep nets for each object instance -- a binary segmentation net providing a mask and a localization net providing a bounding box. Due to the recurrent component and the localization component, our method is able to take advantage of long-term temporal structures of the video data as well as rejecting outliers. We validate the proposed algorithm on three challenging benchmark datasets, the DAVIS-2016 dataset, the DAVIS-2017 dataset, and the Segtrack v2 dataset, achieving state-of-the-art performance on all of them.



### Generative Modeling using the Sliced Wasserstein Distance
- **Arxiv ID**: http://arxiv.org/abs/1803.11188v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11188v1)
- **Published**: 2018-03-29 17:58:44+00:00
- **Updated**: 2018-03-29 17:58:44+00:00
- **Authors**: Ishan Deshpande, Ziyu Zhang, Alexander Schwing
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: Generative Adversarial Nets (GANs) are very successful at modeling distributions from given samples, even in the high-dimensional case. However, their formulation is also known to be hard to optimize and often not stable. While this is particularly true for early GAN formulations, there has been significant empirically motivated and theoretically founded progress to improve stability, for instance, by using the Wasserstein distance rather than the Jenson-Shannon divergence. Here, we consider an alternative formulation for generative modeling based on random projections which, in its simplest form, results in a single objective rather than a saddle-point formulation. By augmenting this approach with a discriminator we improve its accuracy. We found our approach to be significantly more stable compared to even the improved Wasserstein GAN. Further, unlike the traditional GAN loss, the loss formulated in our method is a good measure of the actual distance between the distributions and, for the first time for GAN training, we are able to show estimates for the same.



### Iterative Visual Reasoning Beyond Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1803.11189v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11189v1)
- **Published**: 2018-03-29 17:59:03+00:00
- **Updated**: 2018-03-29 17:59:03+00:00
- **Authors**: Xinlei Chen, Li-Jia Li, Li Fei-Fei, Abhinav Gupta
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: We present a novel framework for iterative visual reasoning. Our framework goes beyond current recognition systems that lack the capability to reason beyond stack of convolutions. The framework consists of two core modules: a local module that uses spatial memory to store previous beliefs with parallel updates; and a global graph-reasoning module. Our graph module has three components: a) a knowledge graph where we represent classes as nodes and build edges to encode different types of semantic relationships between them; b) a region graph of the current image where regions in the image are nodes and spatial relationships between these regions are edges; c) an assignment graph that assigns regions to classes. Both the local module and the global module roll-out iteratively and cross-feed predictions to each other to refine estimates. The final predictions are made by combining the best of both modules with an attention mechanism. We show strong performance over plain ConvNets, \eg achieving an $8.4\%$ absolute improvement on ADE measured by per-class average precision. Analysis also shows that the framework is resilient to missing regions for reasoning.



### Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts
- **Arxiv ID**: http://arxiv.org/abs/1803.11209v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11209v1)
- **Published**: 2018-03-29 18:14:19+00:00
- **Updated**: 2018-03-29 18:14:19+00:00
- **Authors**: Raymond A. Yeh, Jinjun Xiong, Wen-mei W. Hwu, Minh N. Do, Alexander G. Schwing
- **Comment**: Accepted to NIPS 2017
- **Journal**: None
- **Summary**: Textual grounding is an important but challenging task for human-computer interaction, robotics and knowledge mining. Existing algorithms generally formulate the task as selection from a set of bounding box proposals obtained from deep net based systems. In this work, we demonstrate that we can cast the problem of textual grounding into a unified framework that permits efficient search over all possible bounding boxes. Hence, the method is able to consider significantly more proposals and doesn't rely on a successful first stage hypothesizing bounding box proposals. Beyond, we demonstrate that the trained parameters of our model can be used as word-embeddings which capture spatial-image relationships and provide interpretability. Lastly, at the time of submission, our approach outperformed the current state-of-the-art methods on the Flickr 30k Entities and the ReferItGame dataset by 3.08% and 7.77% respectively.



### Joint Person Segmentation and Identification in Synchronized First- and Third-person Videos
- **Arxiv ID**: http://arxiv.org/abs/1803.11217v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11217v2)
- **Published**: 2018-03-29 18:46:03+00:00
- **Updated**: 2018-07-25 21:53:37+00:00
- **Authors**: Mingze Xu, Chenyou Fan, Yuchen Wang, Michael S Ryoo, David J Crandall
- **Comment**: To appear in ECCV 2018
- **Journal**: None
- **Summary**: In a world of pervasive cameras, public spaces are often captured from multiple perspectives by cameras of different types, both fixed and mobile. An important problem is to organize these heterogeneous collections of videos by finding connections between them, such as identifying correspondences between the people appearing in the videos and the people holding or wearing the cameras. In this paper, we wish to solve two specific problems: (1) given two or more synchronized third-person videos of a scene, produce a pixel-level segmentation of each visible person and identify corresponding people across different views (i.e., determine who in camera A corresponds with whom in camera B), and (2) given one or more synchronized third-person videos as well as a first-person video taken by a mobile or wearable camera, segment and identify the camera wearer in the third-person videos. Unlike previous work which requires ground truth bounding boxes to estimate the correspondences, we perform person segmentation and identification jointly. We find that solving these two problems simultaneously is mutually beneficial, because better fine-grained segmentation allows us to better perform matching across views, and information from multiple views helps us perform more accurate segmentation. We evaluate our approach on two challenging datasets of interacting people captured from multiple wearable cameras, and show that our proposed method performs significantly better than the state-of-the-art on both person segmentation and identification.



### AI Blue Book: Vehicle Price Prediction using Visual Features
- **Arxiv ID**: http://arxiv.org/abs/1803.11227v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11227v2)
- **Published**: 2018-03-29 19:22:23+00:00
- **Updated**: 2018-10-18 17:04:30+00:00
- **Authors**: Richard R. Yang, Steven Chen, Edward Chou
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we build a series of machine learning models to predict the price of a product given its image, and visualize the features that result in higher or lower price predictions. We collect two novel datasets of product images and their MSRP prices for this purpose: a bicycle dataset and a car dataset. We set baselines for price regression using linear regression on histogram of oriented gradients (HOG) and convolutional neural network (CNN) features, and a baseline for price segment classification using a multiclass SVM. For our main models, we train several deep CNNs using both transfer learning and our own architectures, for both regression and classification. We achieve strong results on both datasets, with deep CNNs significantly outperforming other models in a variety of metrics. Finally, we use several recently-developed methods to visualize the image features that result in higher or lower prices.



### Euphrates: Algorithm-SoC Co-Design for Low-Power Mobile Continuous Vision
- **Arxiv ID**: http://arxiv.org/abs/1803.11232v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11232v1)
- **Published**: 2018-03-29 19:34:17+00:00
- **Updated**: 2018-03-29 19:34:17+00:00
- **Authors**: Yuhao Zhu, Anand Samajdar, Matthew Mattina, Paul Whatmough
- **Comment**: None
- **Journal**: None
- **Summary**: Continuous computer vision (CV) tasks increasingly rely on convolutional neural networks (CNN). However, CNNs have massive compute demands that far exceed the performance and energy constraints of mobile devices. In this paper, we propose and develop an algorithm-architecture co-designed system, Euphrates, that simultaneously improves the energy-efficiency and performance of continuous vision tasks.   Our key observation is that changes in pixel data between consecutive frames represents visual motion. We first propose an algorithm that leverages this motion information to relax the number of expensive CNN inferences required by continuous vision applications. We co-design a mobile System-on-a-Chip (SoC) architecture to maximize the efficiency of the new algorithm. The key to our architectural augmentation is to co-optimize different SoC IP blocks in the vision pipeline collectively. Specifically, we propose to expose the motion data that is naturally generated by the Image Signal Processor (ISP) early in the vision pipeline to the CNN engine. Measurement and synthesis results show that Euphrates achieves up to 66% SoC-level energy savings (4 times for the vision computations), with only 1% accuracy loss.



### Improve the performance of transfer learning without fine-tuning using dissimilarity-based multi-view learning for breast cancer histology images
- **Arxiv ID**: http://arxiv.org/abs/1803.11241v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11241v1)
- **Published**: 2018-03-29 20:14:07+00:00
- **Updated**: 2018-03-29 20:14:07+00:00
- **Authors**: Hongliu Cao, Simon Bernard, Laurent Heutte, Robert Sabourin
- **Comment**: None
- **Journal**: None
- **Summary**: Breast cancer is one of the most common types of cancer and leading cancer-related death causes for women. In the context of ICIAR 2018 Grand Challenge on Breast Cancer Histology Images, we compare one handcrafted feature extractor and five transfer learning feature extractors based on deep learning. We find out that the deep learning networks pretrained on ImageNet have better performance than the popular handcrafted features used for breast cancer histology images. The best feature extractor achieves an average accuracy of 79.30%. To improve the classification performance, a random forest dissimilarity based integration method is used to combine different feature groups together. When the five deep learning feature groups are combined, the average accuracy is improved to 82.90% (best accuracy 85.00%). When handcrafted features are combined with the five deep learning feature groups, the average accuracy is improved to 87.10% (best accuracy 93.00%).



### Detection, localisation and tracking of pallets using machine learning techniques and 2D range data
- **Arxiv ID**: http://arxiv.org/abs/1803.11254v3
- **DOI**: 10.1007/s00521-019-04352-0
- **Categories**: **cs.RO**, cs.CV, 68T40
- **Links**: [PDF](http://arxiv.org/pdf/1803.11254v3)
- **Published**: 2018-03-29 21:00:56+00:00
- **Updated**: 2019-04-28 16:28:15+00:00
- **Authors**: Ihab S. Mohamed, Alessio Capitanelli, Fulvio Mastrogiovanni, Stefano Rovetta, Renato Zaccaria
- **Comment**: This paper has been submitted to Neural Computing and Applications
  (NCAA). 23 pages, 7 figures
- **Journal**: None
- **Summary**: The problem of autonomous transportation in industrial scenarios is receiving a renewed interest due to the way it can revolutionise internal logistics, especially in unstructured environments. This paper presents a novel architecture allowing a robot to detect, localise, and track (possibly multiple) pallets using machine learning techniques based on an on-board 2D laser rangefinder only. The architecture is composed of two main components: the first stage is a pallet detector employing a Faster Region-based Convolutional Neural Network (Faster R-CNN) detector cascaded with a CNN-based classifier; the second stage is a Kalman filter for localising and tracking detected pallets, which we also use to defer commitment to a pallet detected in the first stage until sufficient confidence has been acquired via a sequential data acquisition process. For fine-tuning the CNNs, the architecture has been systematically evaluated using a real-world dataset containing 340 labeled 2D scans, which have been made freely available in an online repository. Detection performance has been assessed on the basis of the average accuracy over k-fold cross-validation, and it scored 99.58% in our tests. Concerning pallet localisation and tracking, experiments have been performed in a scenario where the robot is approaching the pallet to fork. Although data have been originally acquired by considering only one pallet as per specification of the use case we consider, artificial data have been generated as well to mimic the presence of multiple pallets in the robot workspace. Our experimental results confirm that the system is capable of identifying, localising and tracking pallets with a high success rate while being robust to false positives.



### DIY Human Action Data Set Generation
- **Arxiv ID**: http://arxiv.org/abs/1803.11264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11264v1)
- **Published**: 2018-03-29 21:30:19+00:00
- **Updated**: 2018-03-29 21:30:19+00:00
- **Authors**: Mehran Khodabandeh, Hamid Reza Vaezi Joze, Ilya Zharkov, Vivek Pradeep
- **Comment**: None
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  Workshop (CVPRW), 2018
- **Summary**: The recent successes in applying deep learning techniques to solve standard computer vision problems has aspired researchers to propose new computer vision problems in different domains. As previously established in the field, training data itself plays a significant role in the machine learning process, especially deep learning approaches which are data hungry. In order to solve each new problem and get a decent performance, a large amount of data needs to be captured which may in many cases pose logistical difficulties. Therefore, the ability to generate de novo data or expand an existing data set, however small, in order to satisfy data requirement of current networks may be invaluable. Herein, we introduce a novel way to partition an action video clip into action, subject and context. Each part is manipulated separately and reassembled with our proposed video generation technique. Furthermore, our novel human skeleton trajectory generation along with our proposed video generation technique, enables us to generate unlimited action recognition training data. These techniques enables us to generate video action clips from an small set without costly and time-consuming data acquisition. Lastly, we prove through extensive set of experiments on two small human action recognition data sets, that this new data generation technique can improve the performance of current action recognition neural nets.



### Two-Stream Neural Networks for Tampered Face Detection
- **Arxiv ID**: http://arxiv.org/abs/1803.11276v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11276v1)
- **Published**: 2018-03-29 22:36:11+00:00
- **Updated**: 2018-03-29 22:36:11+00:00
- **Authors**: Peng Zhou, Xintong Han, Vlad I. Morariu, Larry S. Davis
- **Comment**: None
- **Journal**: 2017 CVPR workshop
- **Summary**: We propose a two-stream network for face tampering detection. We train GoogLeNet to detect tampering artifacts in a face classification stream, and train a patch based triplet network to leverage features capturing local noise residuals and camera characteristics as a second stream. In addition, we use two different online face swapping applications to create a new dataset that consists of 2010 tampered images, each of which contains a tampered face. We evaluate the proposed two-stream network on our newly collected dataset. Experimental results demonstrate the effectiveness of our method.



### Revisiting Oxford and Paris: Large-Scale Image Retrieval Benchmarking
- **Arxiv ID**: http://arxiv.org/abs/1803.11285v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.11285v1)
- **Published**: 2018-03-29 23:22:31+00:00
- **Updated**: 2018-03-29 23:22:31+00:00
- **Authors**: Filip Radenović, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Ondřej Chum
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: In this paper we address issues with image retrieval benchmarking on standard and popular Oxford 5k and Paris 6k datasets. In particular, annotation errors, the size of the dataset, and the level of challenge are addressed: new annotation for both datasets is created with an extra attention to the reliability of the ground truth. Three new protocols of varying difficulty are introduced. The protocols allow fair comparison between different methods, including those using a dataset pre-processing stage. For each dataset, 15 new challenging queries are introduced. Finally, a new set of 1M hard, semi-automatically cleaned distractors is selected.   An extensive comparison of the state-of-the-art methods is performed on the new benchmark. Different types of methods are evaluated, ranging from local-feature-based to modern CNN based methods. The best results are achieved by taking the best of the two worlds. Most importantly, image retrieval appears far from being solved.



### FutureMapping: The Computational Structure of Spatial AI Systems
- **Arxiv ID**: http://arxiv.org/abs/1803.11288v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.11288v1)
- **Published**: 2018-03-29 23:46:34+00:00
- **Updated**: 2018-03-29 23:46:34+00:00
- **Authors**: Andrew J. Davison
- **Comment**: None
- **Journal**: None
- **Summary**: We discuss and predict the evolution of Simultaneous Localisation and Mapping (SLAM) into a general geometric and semantic `Spatial AI' perception capability for intelligent embodied devices. A big gap remains between the visual perception performance that devices such as augmented reality eyewear or comsumer robots will require and what is possible within the constraints imposed by real products. Co-design of algorithms, processors and sensors will be needed. We explore the computational structure of current and future Spatial AI algorithms and consider this within the landscape of ongoing hardware developments.



