# Arxiv Papers in cs.CV on 2018-03-10
### Contour Parametrization via Anisotropic Mean Curvature Flows
- **Arxiv ID**: http://arxiv.org/abs/1803.03724v1
- **DOI**: None
- **Categories**: **math.DG**, cs.CG, cs.CV, math.AP, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1803.03724v1)
- **Published**: 2018-03-10 00:05:01+00:00
- **Updated**: 2018-03-10 00:05:01+00:00
- **Authors**: P. Suárez-Serrato, E. I. Velázquez Richards
- **Comment**: 30 pages, 20 images, source code for our numerical implementation is
  available in this URL https://github.com/V3du4rd0/AMCF
- **Journal**: None
- **Summary**: We present a new implementation of anisotropic mean curvature flow for contour recognition. Our procedure couples the mean curvature flow of planar closed smooth curves, with an external field from a potential of point-wise charges. This coupling constrains the motion when the curve matches a picture placed as background. We include a stability criteria for our numerical approximation.



### A Large-Scale Multi-Institutional Evaluation of Advanced Discrimination Algorithms for Buried Threat Detection in Ground Penetrating Radar
- **Arxiv ID**: http://arxiv.org/abs/1803.03729v2
- **DOI**: 10.1109/TGRS.2019.2909665
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03729v2)
- **Published**: 2018-03-10 00:37:44+00:00
- **Updated**: 2018-06-07 18:49:29+00:00
- **Authors**: Jordan M. Malof, Daniel Reichman, Andrew Karem, Hichem Frigui, Dominic K. C. Ho, Joseph N. Wilson, Wen-Hsiung Lee, William Cummings, Leslie M. Collins
- **Comment**: IEEE Transactions on Geoscience and Remote Sensing (2019)
- **Journal**: None
- **Summary**: In this paper we consider the development of algorithms for the automatic detection of buried threats using ground penetrating radar (GPR) measurements. GPR is one of the most studied and successful modalities for automatic buried threat detection (BTD), and a large variety of BTD algorithms have been proposed for it. Despite this, large-scale comparisons of GPR-based BTD algorithms are rare in the literature. In this work we report the results of a multi-institutional effort to develop advanced buried threat detection algorithms for a real-world GPR BTD system. The effort involved five institutions with substantial experience with the development of GPR-based BTD algorithms. In this paper we report the technical details of the advanced algorithms submitted by each institution, representing their latest technical advances, and many state-of-the-art GPR-based BTD algorithms. We also report the results of evaluating the algorithms from each institution on the large experimental dataset used for development. The experimental dataset comprised 120,000 m^2 of GPR data using surface area, from 13 different lanes across two US test sites. The data was collected using a vehicle-mounted GPR system, the variants of which have supplied data for numerous publications. Using these results, we identify the most successful and common processing strategies among the submitted algorithms, and make recommendations for GPR-based BTD algorithm design.



### Low Rank Variation Dictionary and Inverse Projection Group Sparse Representation Model for Breast Tumor Classification
- **Arxiv ID**: http://arxiv.org/abs/1803.04793v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04793v1)
- **Published**: 2018-03-10 03:59:13+00:00
- **Updated**: 2018-03-10 03:59:13+00:00
- **Authors**: Xiaohui Yang, Xiaoying Jiang, Wenming Wu, Juan Zhang, Dan Long, Funa Zhou, Yiming Xu
- **Comment**: 31 pages, 14 figures, 12 tables. arXiv admin note: text overlap with
  arXiv:1803.03562
- **Journal**: None
- **Summary**: Sparse representation classification achieves good results by addressing recognition problem with sufficient training samples per subject. However, SRC performs not very well for small sample data. In this paper, an inverse-projection group sparse representation model is presented for breast tumor classification, which is based on constructing low-rank variation dictionary. The proposed low-rank variation dictionary tackles tumor recognition problem from the viewpoint of detecting and using variations in gene expression profiles of normal and patients, rather than directly using these samples. The inverse projection group sparsity representation model is constructed based on taking full using of exist samples and group effect of microarray gene data. Extensive experiments on public breast tumor microarray gene expression datasets demonstrate the proposed technique is competitive with state-of-the-art methods. The results of Breast-1, Breast-2 and Breast-3 databases are 80.81%, 89.10% and 100% respectively, which are better than the latest literature.



### Driving Scene Perception Network: Real-time Joint Detection, Depth Estimation and Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.03778v1
- **DOI**: 10.1109/WACV.2018.00145
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03778v1)
- **Published**: 2018-03-10 08:55:46+00:00
- **Updated**: 2018-03-10 08:55:46+00:00
- **Authors**: Liangfu Chen, Zeng Yang, Jianjun Ma, Zheng Luo
- **Comment**: 9 pages, 7 figures, WACV'18
- **Journal**: None
- **Summary**: As the demand for enabling high-level autonomous driving has increased in recent years and visual perception is one of the critical features to enable fully autonomous driving, in this paper, we introduce an efficient approach for simultaneous object detection, depth estimation and pixel-level semantic segmentation using a shared convolutional architecture. The proposed network model, which we named Driving Scene Perception Network (DSPNet), uses multi-level feature maps and multi-task learning to improve the accuracy and efficiency of object detection, depth estimation and image segmentation tasks from a single input image. Hence, the resulting network model uses less than 850 MiB of GPU memory and achieves 14.0 fps on NVIDIA GeForce GTX 1080 with a 1024x512 input image, and both precision and efficiency have been improved over combination of single tasks.



### ShuffleSeg: Real-time Semantic Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/1803.03816v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03816v2)
- **Published**: 2018-03-10 14:28:45+00:00
- **Updated**: 2018-03-15 10:08:00+00:00
- **Authors**: Mostafa Gamal, Mennatullah Siam, Moemen Abdel-Razek
- **Comment**: 6 pages, under review by ICIP 2018
- **Journal**: None
- **Summary**: Real-time semantic segmentation is of significant importance for mobile and robotics related applications. We propose a computationally efficient segmentation network which we term as ShuffleSeg. The proposed architecture is based on grouped convolution and channel shuffling in its encoder for improving the performance. An ablation study of different decoding methods is compared including Skip architecture, UNet, and Dilation Frontend. Interesting insights on the speed and accuracy tradeoff is discussed. It is shown that skip architecture in the decoding method provides the best compromise for the goal of real-time performance, while it provides adequate accuracy by utilizing higher resolution feature maps for a more accurate segmentation. ShuffleSeg is evaluated on CityScapes and compared against the state of the art real-time segmentation networks. It achieves 2x GFLOPs reduction, while it provides on par mean intersection over union of 58.3% on CityScapes test set. ShuffleSeg runs at 15.7 frames per second on NVIDIA Jetson TX2, which makes it of great potential for real-time applications.



### Face2Text: Collecting an Annotated Image Description Corpus for the Generation of Rich Face Descriptions
- **Arxiv ID**: http://arxiv.org/abs/1803.03827v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.03827v2)
- **Published**: 2018-03-10 15:52:08+00:00
- **Updated**: 2021-03-05 07:32:51+00:00
- **Authors**: Albert Gatt, Marc Tanti, Adrian Muscat, Patrizia Paggio, Reuben A. Farrugia, Claudia Borg, Kenneth P. Camilleri, Mike Rosner, Lonneke van der Plas
- **Comment**: Proceedings of the 11th edition of the Language Resources and
  Evaluation Conference (LREC'18)
- **Journal**: None
- **Summary**: The past few years have witnessed renewed interest in NLP tasks at the interface between vision and language. One intensively-studied problem is that of automatically generating text from images. In this paper, we extend this problem to the more specific domain of face description. Unlike scene descriptions, face descriptions are more fine-grained and rely on attributes extracted from the image, rather than objects and relations. Given that no data exists for this task, we present an ongoing crowdsourcing study to collect a corpus of descriptions of face images taken `in the wild'. To gain a better understanding of the variation we find in face description and the possible issues that this may raise, we also conducted an annotation study on a subset of the corpus. Primarily, we found descriptions to refer to a mixture of attributes, not only physical, but also emotional and inferential, which is bound to create further challenges for current image-to-text methods.



### Fire detection in a still image using colour information
- **Arxiv ID**: http://arxiv.org/abs/1803.03828v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.03828v1)
- **Published**: 2018-03-10 15:52:46+00:00
- **Updated**: 2018-03-10 15:52:46+00:00
- **Authors**: Oluwarotimi Giwa, Abdsamad Benkrid
- **Comment**: None
- **Journal**: None
- **Summary**: Colour analysis is a crucial step in image-based fire detection algorithms. Many of the proposed fire detection algorithms in a still image are prone to false alarms caused by objects with a colour similar to fire. To design a colour-based system with a better false alarm rate, a new colour-differentiating conversion matrix, efficient on images of high colour complexity, is proposed. The elements of this conversion matrix are obtained by performing K-medoids clustering and Particle Swarm Optimisation procedures on a fire sample image with a background of high fire-colour similarity. The proposed conversion matrix is then used to construct two new fire colour detection frameworks. The first detection method is a two-stage non-linear image transformation framework, while the second is a direct transformation of an image with the proposed conversion matrix. A performance comparison of the proposed methods with alternate methods in the literature was carried out. Experimental results indicate that the linear image transformation method outperforms other methods regarding false alarm rate while the non-linear two-stage image transformation method has the best performance on the F-score metric and provides a better trade-off between missed detection and false alarm rate.



### Sample-Relaxed Two-Dimensional Color Principal Component Analysis for Face Recognition and Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1803.03837v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03837v1)
- **Published**: 2018-03-10 16:49:54+00:00
- **Updated**: 2018-03-10 16:49:54+00:00
- **Authors**: Meixiang Zhao, Zhigang Jia, Dunwei Gong
- **Comment**: 18 pages, 7 figures
- **Journal**: None
- **Summary**: A sample-relaxed two-dimensional color principal component analysis (SR-2DCPCA) approach is presented for face recognition and image reconstruction based on quaternion models. A relaxation vector is automatically generated according to the variances of training color face images with the same label. A sample-relaxed, low-dimensional covariance matrix is constructed based on all the training samples relaxed by a relaxation vector, and its eigenvectors corresponding to the $r$ largest eigenvalues are defined as the optimal projection. The SR-2DCPCA aims to enlarge the global variance rather than to maximize the variance of the projected training samples. The numerical results based on real face data sets validate that SR-2DCPCA has a higher recognition rate than state-of-the-art methods and is efficient in image reconstruction.



### Learning to Localize Sound Source in Visual Scenes
- **Arxiv ID**: http://arxiv.org/abs/1803.03849v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1803.03849v1)
- **Published**: 2018-03-10 18:19:02+00:00
- **Updated**: 2018-03-10 18:19:02+00:00
- **Authors**: Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, In So Kweon
- **Comment**: To appear in CVPR 2018. Total 9 pages
- **Journal**: None
- **Summary**: Visual events are usually accompanied by sounds in our daily lives. We pose the question: Can the machine learn the correspondence between visual scene and the sound, and localize the sound source only by observing sound and visual scene pairs like human? In this paper, we propose a novel unsupervised algorithm to address the problem of localizing the sound source in visual scenes. A two-stream network structure which handles each modality, with attention mechanism is developed for sound source localization. Moreover, although our network is formulated within the unsupervised learning framework, it can be extended to a unified architecture with a simple modification for the supervised and semi-supervised learning settings as well. Meanwhile, a new sound source dataset is developed for performance evaluation. Our empirical evaluation shows that the unsupervised method eventually go through false conclusion in some cases. We show that even with a few supervision, false conclusion is able to be corrected and the source of sound in a visual scene can be localized effectively.



### Revisiting Decomposable Submodular Function Minimization with Incidence Relations
- **Arxiv ID**: http://arxiv.org/abs/1803.03851v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DM
- **Links**: [PDF](http://arxiv.org/pdf/1803.03851v3)
- **Published**: 2018-03-10 18:40:07+00:00
- **Updated**: 2018-09-24 22:35:05+00:00
- **Authors**: Pan Li, Olgica Milenkovic
- **Comment**: A part of this work will be presented in NIPS2018
- **Journal**: None
- **Summary**: We introduce a new approach to decomposable submodular function minimization (DSFM) that exploits incidence relations. Incidence relations describe which variables effectively influence the component functions, and when properly utilized, they allow for improving the convergence rates of DSFM solvers. Our main results include the precise parametrization of the DSFM problem based on incidence relations, the development of new scalable alternative projections and parallel coordinate descent methods and an accompanying rigorous analysis of their convergence rates.



### A Deep Learning Approach for Pose Estimation from Volumetric OCT Data
- **Arxiv ID**: http://arxiv.org/abs/1803.03852v1
- **DOI**: 10.1016/j.media.2018.03.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03852v1)
- **Published**: 2018-03-10 18:48:18+00:00
- **Updated**: 2018-03-10 18:48:18+00:00
- **Authors**: Nils Gessert, Matthias Schlüter, Alexander Schlaefer
- **Comment**: https://doi.org/10.1016/j.media.2018.03.002
- **Journal**: None
- **Summary**: Tracking the pose of instruments is a central problem in image-guided surgery. For microscopic scenarios, optical coherence tomography (OCT) is increasingly used as an imaging modality. OCT is suitable for accurate pose estimation due to its micrometer range resolution and volumetric field of view. However, OCT image processing is challenging due to speckle noise and reflection artifacts in addition to the images' 3D nature. We address pose estimation from OCT volume data with a new deep learning-based tracking framework. For this purpose, we design a new 3D convolutional neural network (CNN) architecture to directly predict the 6D pose of a small marker geometry from OCT volumes. We use a hexapod robot to automatically acquire labeled data points which we use to train 3D CNN architectures for multi-output regression. We use this setup to provide an in-depth analysis on deep learning-based pose estimation from volumes. Specifically, we demonstrate that exploiting volume information for pose estimation yields higher accuracy than relying on 2D representations with depth information. Supporting this observation, we provide quantitative and qualitative results that 3D CNNs effectively exploit the depth structure of marker objects. Regarding the deep learning aspect, we present efficient design principles for 3D CNNs, making use of insights from the 2D deep learning community. In particular, we present Inception3D as a new architecture which performs best for our application. We show that our deep learning approach reaches errors at our ground-truth label's resolution. We achieve a mean average error of $\SI{14.89 \pm 9.3}{\micro\metre}$ and $\SI{0.096 \pm 0.072}{\degree}$ for position and orientation learning, respectively.



### Learning from Noisy Web Data with Category-level Supervision
- **Arxiv ID**: http://arxiv.org/abs/1803.03857v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03857v3)
- **Published**: 2018-03-10 19:30:43+00:00
- **Updated**: 2018-05-24 05:41:17+00:00
- **Authors**: Li Niu, Qingtao Tang, Ashok Veeraraghavan, Ashu Sabharwal
- **Comment**: None
- **Journal**: None
- **Summary**: As tons of photos are being uploaded to public websites (e.g., Flickr, Bing, and Google) every day, learning from web data has become an increasingly popular research direction because of freely available web resources, which is also referred to as webly supervised learning. Nevertheless, the performance gap between webly supervised learning and traditional supervised learning is still very large, owning to the label noise of web data. To be exact, the labels of images crawled from public websites are very noisy and often inaccurate. Some existing works tend to facilitate learning from web data with the aid of extra information, such as augmenting or purifying web data by virtue of instance-level supervision, which is usually in demand of heavy manual annotation. Instead, we propose to tackle the label noise by leveraging more accessible category-level supervision. In particular, we build our method upon variational autoencoder (VAE), in which the classification network is attached on the hidden layer of VAE in a way that the classification network and VAE can jointly leverage the category-level hybrid semantic information. The effectiveness of our proposed method is clearly demonstrated by extensive experiments on three benchmark datasets.



### Testing Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.04792v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/1803.04792v4)
- **Published**: 2018-03-10 23:19:13+00:00
- **Updated**: 2019-04-15 16:49:14+00:00
- **Authors**: Youcheng Sun, Xiaowei Huang, Daniel Kroening, James Sharp, Matthew Hill, Rob Ashmore
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have a wide range of applications, and software employing them must be thoroughly tested, especially in safety-critical domains. However, traditional software test coverage metrics cannot be applied directly to DNNs. In this paper, inspired by the MC/DC coverage criterion, we propose a family of four novel test criteria that are tailored to structural features of DNNs and their semantics. We validate the criteria by demonstrating that the generated test inputs guided via our proposed coverage criteria are able to capture undesired behaviours in a DNN. Test cases are generated using a symbolic approach and a gradient-based heuristic search. By comparing them with existing methods, we show that our criteria achieve a balance between their ability to find bugs (proxied using adversarial examples) and the computational cost of test case generation. Our experiments are conducted on state-of-the-art DNNs obtained using popular open source datasets, including MNIST, CIFAR-10 and ImageNet.



