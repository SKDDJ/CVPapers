# Arxiv Papers in cs.CV on 2018-03-05
### Deep Continuous Clustering
- **Arxiv ID**: http://arxiv.org/abs/1803.01449v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.01449v1)
- **Published**: 2018-03-05 01:15:38+00:00
- **Updated**: 2018-03-05 01:15:38+00:00
- **Authors**: Sohil Atul Shah, Vladlen Koltun
- **Comment**: The code is available at http://github.com/shahsohil/DCC
- **Journal**: None
- **Summary**: Clustering high-dimensional datasets is hard because interpoint distances become less informative in high-dimensional spaces. We present a clustering algorithm that performs nonlinear dimensionality reduction and clustering jointly. The data is embedded into a lower-dimensional space by a deep autoencoder. The autoencoder is optimized as part of the clustering process. The resulting network produces clustered data. The presented approach does not rely on prior knowledge of the number of ground-truth clusters. Joint nonlinear dimensionality reduction and clustering are formulated as optimization of a global continuous objective. We thus avoid discrete reconfigurations of the objective that characterize prior clustering algorithms. Experiments on datasets from multiple domains demonstrate that the presented algorithm outperforms state-of-the-art clustering schemes, including recent methods that use deep networks.



### Less Is More: Picking Informative Frames for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1803.01457v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01457v1)
- **Published**: 2018-03-05 01:57:49+00:00
- **Updated**: 2018-03-05 01:57:49+00:00
- **Authors**: Yangyu Chen, Shuhui Wang, Weigang Zhang, Qingming Huang
- **Comment**: 10 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: In video captioning task, the best practice has been achieved by attention-based models which associate salient visual components with sentences in the video. However, existing study follows a common procedure which includes a frame-level appearance modeling and motion modeling on equal interval frame sampling, which may bring about redundant visual information, sensitivity to content noise and unnecessary computation cost.   We propose a plug-and-play PickNet to perform informative frame picking in video captioning. Based on a standard Encoder-Decoder framework, we develop a reinforcement-learning-based procedure to train the network sequentially, where the reward of each frame picking action is designed by maximizing visual diversity and minimizing textual discrepancy. If the candidate is rewarded, it will be selected and the corresponding latent representation of Encoder-Decoder will be updated for future trials. This procedure goes on until the end of the video sequence. Consequently, a compact frame subset can be selected to represent the visual information and perform video captioning without performance degradation. Experiment results shows that our model can use 6-8 frames to achieve competitive performance across popular benchmarks.



### Totally Looks Like - How Humans Compare, Compared to Machines
- **Arxiv ID**: http://arxiv.org/abs/1803.01485v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.01485v3)
- **Published**: 2018-03-05 03:43:20+00:00
- **Updated**: 2018-10-18 18:31:00+00:00
- **Authors**: Amir Rosenfeld, Markus D. Solbach, John K. Tsotsos
- **Comment**: ACCV 2018. Project website:
  https://sites.google.com/view/totally-looks-like-dataset
- **Journal**: None
- **Summary**: Perceptual judgment of image similarity by humans relies on rich internal representations ranging from low-level features to high-level concepts, scene properties and even cultural associations. However, existing methods and datasets attempting to explain perceived similarity use stimuli which arguably do not cover the full breadth of factors that affect human similarity judgments, even those geared toward this goal. We introduce a new dataset dubbed Totally-Looks-Like (TLL) after a popular entertainment website, which contains images paired by humans as being visually similar. The dataset contains 6016 image-pairs from the wild, shedding light upon a rich and diverse set of criteria employed by human beings. We conduct experiments to try to reproduce the pairings via features extracted from state-of-the-art deep convolutional neural networks, as well as additional human experiments to verify the consistency of the collected data. Though we create conditions to artificially make the matching task increasingly easier, we show that machine-extracted representations perform very poorly in terms of reproducing the matching selected by humans. We discuss and analyze these results, suggesting future directions for improvement of learned image representations.



### Cross-Paced Representation Learning with Partial Curricula for Sketch-based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1803.01504v1
- **DOI**: 10.1109/TIP.2018.2837381
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01504v1)
- **Published**: 2018-03-05 05:30:08+00:00
- **Updated**: 2018-03-05 05:30:08+00:00
- **Authors**: Dan Xu, Xavier Alameda-Pineda, Jingkuan Song, Elisa Ricci, Nicu Sebe
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we address the problem of learning robust cross-domain representations for sketch-based image retrieval (SBIR). While most SBIR approaches focus on extracting low- and mid-level descriptors for direct feature matching, recent works have shown the benefit of learning coupled feature representations to describe data from two related sources. However, cross-domain representation learning methods are typically cast into non-convex minimization problems that are difficult to optimize, leading to unsatisfactory performance. Inspired by self-paced learning, a learning methodology designed to overcome convergence issues related to local optima by exploiting the samples in a meaningful order (i.e. easy to hard), we introduce the cross-paced partial curriculum learning (CPPCL) framework. Compared with existing self-paced learning methods which only consider a single modality and cannot deal with prior knowledge, CPPCL is specifically designed to assess the learning pace by jointly handling data from dual sources and modality-specific prior information provided in the form of partial curricula. Additionally, thanks to the learned dictionaries, we demonstrate that the proposed CPPCL embeds robust coupled representations for SBIR. Our approach is extensively evaluated on four publicly available datasets (i.e. CUFS, Flickr15K, QueenMary SBIR and TU-Berlin Extension datasets), showing superior performance over competing SBIR methods.



### A new stereo formulation not using pixel and disparity models
- **Arxiv ID**: http://arxiv.org/abs/1803.01516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01516v1)
- **Published**: 2018-03-05 06:50:23+00:00
- **Updated**: 2018-03-05 06:50:23+00:00
- **Authors**: Kiyoshi Oguri, Yuichiro Shibata
- **Comment**: 7 pages, 10 figures
- **Journal**: None
- **Summary**: We introduce a new stereo formulation which does not use pixel and disparity models. Many problems in vision are treated as assigning each pixel a label. Disparities are labels for stereo. Such pixel-labeling problems are naturally represented in terms of energy minimization, where the energy function has two terms: one term penalizes solutions that inconsistent with the observed data, the other term enforces spatial smoothness. Graph cuts are one of the effi- cient methods for solving energy minimization. However, exact minimization of multi labeling problems can be performed by graph cuts only for the case with convex smoothness terms. In pixel-disparity formulation, convex smoothness terms do not generate well reconstructed 3D results. Thus, truncated linear or quadratic smoothness terms, etc. are used, where approximate energy minimization is necessary. In this paper, we introduce a new site-labeling formulation, where the sites are not pixels but lines in 3D space, labels are not disparities but depth numbers. For this formulation, visibility reasoning is naturally included in the energy function. In addition, this formulation allows us to use a small smoothness term, which does not affect the 3D results much. This makes the optimization step very simple, so we could develop an approximation method for graph cut itself (not for energy minimization) and a high performance GPU graph cut program. For Tsukuba stereo pair in Middlebury data set, we got the result in 5ms using GTX1080GPU, 19ms using GTX660GPU.



### LSTD: A Low-Shot Transfer Detector for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1803.01529v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01529v1)
- **Published**: 2018-03-05 07:30:58+00:00
- **Updated**: 2018-03-05 07:30:58+00:00
- **Authors**: Hao Chen, Yali Wang, Guoyou Wang, Yu Qiao
- **Comment**: Accepted by AAAI2018
- **Journal**: None
- **Summary**: Recent advances in object detection are mainly driven by deep learning with large-scale detection benchmarks. However, the fully-annotated training set is often limited for a target detection task, which may deteriorate the performance of deep detectors. To address this challenge, we propose a novel low-shot transfer detector (LSTD) in this paper, where we leverage rich source-domain knowledge to construct an effective target-domain detector with very few training examples. The main contributions are described as follows. First, we design a flexible deep architecture of LSTD to alleviate transfer difficulties in low-shot detection. This architecture can integrate the advantages of both SSD and Faster RCNN in a unified deep framework. Second, we introduce a novel regularized transfer learning framework for low-shot detection, where the transfer knowledge (TK) and background depression (BD) regularizations are proposed to leverage object knowledge respectively from source and target domains, in order to further enhance fine-tuning with a few target images. Finally, we examine our LSTD on a number of challenging low-shot detection experiments, where LSTD outperforms other state-of-the-art approaches. The results demonstrate that LSTD is a preferable deep detector for low-shot scenarios.



### Learning-Based Dequantization For Image Restoration Against Extremely Poor Illumination
- **Arxiv ID**: http://arxiv.org/abs/1803.01532v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01532v2)
- **Published**: 2018-03-05 07:39:29+00:00
- **Updated**: 2018-03-20 18:29:28+00:00
- **Authors**: Chang Liu, Xiaolin Wu, Xiao Shu
- **Comment**: None
- **Journal**: None
- **Summary**: All existing image enhancement methods, such as HDR tone mapping, cannot recover A/D quantization losses due to insufficient or excessive lighting, (underflow and overflow problems). The loss of image details due to A/D quantization is complete and it cannot be recovered by traditional image processing methods, but the modern data-driven machine learning approach offers a much needed cure to the problem. In this work we propose a novel approach to restore and enhance images acquired in low and uneven lighting. First, the ill illumination is algorithmically compensated by emulating the effects of artificial supplementary lighting. Then a DCNN trained using only synthetic data recovers the missing detail caused by quantization.



### Path Aggregation Network for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.01534v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01534v4)
- **Published**: 2018-03-05 07:46:36+00:00
- **Updated**: 2018-09-18 04:26:54+00:00
- **Authors**: Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, Jiaya Jia
- **Comment**: Accepted to CVPR 2018
- **Journal**: None
- **Summary**: The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction. These improvements are simple to implement, with subtle extra computational overhead. Our PANet reaches the 1st place in the COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. It is also state-of-the-art on MVD and Cityscapes. Code is available at https://github.com/ShuLiu1993/PANet



### Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect
- **Arxiv ID**: http://arxiv.org/abs/1803.01541v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.01541v1)
- **Published**: 2018-03-05 08:00:39+00:00
- **Updated**: 2018-03-05 08:00:39+00:00
- **Authors**: Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, Liqiang Wang
- **Comment**: Accepted as a conference paper in International Conference on
  Learning Representation(ICLR). Xiang Wei and Boqing Gong contributed equally
  in this work
- **Journal**: None
- **Summary**: Despite being impactful on a variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, called Wasserstein GAN (WGAN), hinges on the 1-Lipschitz continuity of the discriminator. In this paper, we propose a novel approach to enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning methods. As a result, it gives rise to not only better photo-realistic samples than the previous methods but also state-of-the-art semi-supervised learning results. In particular, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR-10 images and is the first that exceeds the accuracy of 90% on the CIFAR-10 dataset using only 4,000 labeled images, to the best of our knowledge.



### Relocalization, Global Optimization and Map Merging for Monocular Visual-Inertial SLAM
- **Arxiv ID**: http://arxiv.org/abs/1803.01549v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01549v1)
- **Published**: 2018-03-05 08:13:42+00:00
- **Updated**: 2018-03-05 08:13:42+00:00
- **Authors**: Tong Qin, Perliang Li, Shaojie Shen
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: The monocular visual-inertial system (VINS), which consists one camera and one low-cost inertial measurement unit (IMU), is a popular approach to achieve accurate 6-DOF state estimation. However, such locally accurate visual-inertial odometry is prone to drift and cannot provide absolute pose estimation. Leveraging history information to relocalize and correct drift has become a hot topic. In this paper, we propose a monocular visual-inertial SLAM system, which can relocalize camera and get the absolute pose in a previous-built map. Then 4-DOF pose graph optimization is performed to correct drifts and achieve global consistent. The 4-DOF contains x, y, z, and yaw angle, which is the actual drifted direction in the visual-inertial system. Furthermore, the proposed system can reuse a map by saving and loading it in an efficient way. Current map and previous map can be merged together by the global pose graph optimization. We validate the accuracy of our system on public datasets and compare against other state-of-the-art algorithms. We also evaluate the map merging ability of our system in the large-scale outdoor environment. The source code of map reuse is integrated into our public code, VINS-Mono.



### Beyond Context: Exploring Semantic Similarity for Tiny Face Detection
- **Arxiv ID**: http://arxiv.org/abs/1803.01555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01555v1)
- **Published**: 2018-03-05 08:29:35+00:00
- **Updated**: 2018-03-05 08:29:35+00:00
- **Authors**: Yue Xi, Jiangbin Zheng, Xiangjian He, Wenjing Jia, Hanhui Li
- **Comment**: None
- **Journal**: None
- **Summary**: Tiny face detection aims to find faces with high degrees of variability in scale, resolution and occlusion in cluttered scenes. Due to the very little information available on tiny faces, it is not sufficient to detect them merely based on the information presented inside the tiny bounding boxes or their context. In this paper, we propose to exploit the semantic similarity among all predicted targets in each image to boost current face detectors. To this end, we present a novel framework to model semantic similarity as pairwise constraints within the metric learning scheme, and then refine our predictions with the semantic similarity by utilizing the graph cut techniques. Experiments conducted on three widely-used benchmark datasets have demonstrated the improvement over the-state-of-the-arts gained by applying this idea.



### Local Distance Metric Learning for Nearest Neighbor Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1803.01562v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.01562v2)
- **Published**: 2018-03-05 08:45:47+00:00
- **Updated**: 2018-03-15 20:21:22+00:00
- **Authors**: Hossein Rajabzadeh, Mansoor Zolghadri Jahromi, Mohammad Sadegh Zare, Mostafa Fakhrahmad
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Distance metric learning is a successful way to enhance the performance of the nearest neighbor classifier. In most cases, however, the distribution of data does not obey a regular form and may change in different parts of the feature space. Regarding that, this paper proposes a novel local distance metric learning method, namely Local Mahalanobis Distance Learning (LMDL), in order to enhance the performance of the nearest neighbor classifier. LMDL considers the neighborhood influence and learns multiple distance metrics for a reduced set of input samples. The reduced set is called as prototypes which try to preserve local discriminative information as much as possible. The proposed LMDL can be kernelized very easily, which is significantly desirable in the case of highly nonlinear data. The quality as well as the efficiency of the proposed method assesses through a set of different experiments on various datasets and the obtained results show that LDML as well as the kernelized version is superior to the other related state-of-the-art methods.



### Predicting Out-of-View Feature Points for Model-Based Camera Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1803.01577v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1803.01577v1)
- **Published**: 2018-03-05 10:03:17+00:00
- **Updated**: 2018-03-05 10:03:17+00:00
- **Authors**: Oliver Moolan-Feroze, Andrew Calway
- **Comment**: Submitted to IROS 2018
- **Journal**: None
- **Summary**: In this work we present a novel framework that uses deep learning to predict object feature points that are out-of-view in the input image. This system was developed with the application of model-based tracking in mind, particularly in the case of autonomous inspection robots, where only partial views of the object are available. Out-of-view prediction is enabled by applying scaling to the feature point labels during network training. This is combined with a recurrent neural network architecture designed to provide the final prediction layers with rich feature information from across the spatial extent of the input image. To show the versatility of these out-of-view predictions, we describe how to integrate them in both a particle filter tracker and an optimisation based tracker. To evaluate our work we compared our framework with one that predicts only points inside the image. We show that as the amount of the object in view decreases, being able to predict outside the image bounds adds robustness to the final pose estimation.



### Spectral reflectance estimation from one RGB image using self-interreflections in a concave object
- **Arxiv ID**: http://arxiv.org/abs/1803.01595v1
- **DOI**: 10.1364/AO.57.004918
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01595v1)
- **Published**: 2018-03-05 10:43:50+00:00
- **Updated**: 2018-03-05 10:43:50+00:00
- **Authors**: Rada Deeb, Damien Muselet, Mathieu Hebert, Alain Tremeau
- **Comment**: submitted to Applied Optics
- **Journal**: None
- **Summary**: Light interreflections occurring in a concave object generate a color gradient which is characteristic of the object's spectral reflectance. In this paper, we use this property in order to estimate the spectral reflectance of matte, uniformly colored, V-shaped surfaces from a single RGB image taken under directional lighting. First, simulations show that using one image of the concave object is equivalent to, and can even outperform, the state of the art approaches based on three images taken under three lightings with different colors. Experiments on real images of folded papers were performed under unmeasured direct sunlight. The results show that our interreflection-based approach outperforms existing approaches even when the latter are improved by a calibration step. The mathematical solution for the interreflection equation and the effect of surface parameters on the performance of the method are also discussed in this paper.



### AdaDepth: Unsupervised Content Congruent Adaptation for Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1803.01599v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01599v2)
- **Published**: 2018-03-05 10:55:58+00:00
- **Updated**: 2018-06-07 11:06:01+00:00
- **Authors**: Jogendra Nath Kundu, Phani Krishna Uppala, Anuj Pahuja, R. Venkatesh Babu
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Supervised deep learning methods have shown promising results for the task of monocular depth estimation; but acquiring ground truth is costly, and prone to noise as well as inaccuracies. While synthetic datasets have been used to circumvent above problems, the resultant models do not generalize well to natural scenes due to the inherent domain shift. Recent adversarial approaches for domain adaption have performed well in mitigating the differences between the source and target domains. But these methods are mostly limited to a classification setup and do not scale well for fully-convolutional architectures. In this work, we propose AdaDepth - an unsupervised domain adaptation strategy for the pixel-wise regression task of monocular depth estimation. The proposed approach is devoid of above limitations through a) adversarial learning and b) explicit imposition of content consistency on the adapted target representation. Our unsupervised approach performs competitively with other established approaches on depth estimation tasks and achieves state-of-the-art results in a semi-supervised setting.



### 2^B3^C: 2 Box 3 Crop of Facial Image for Gender Classification with Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.02181v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02181v1)
- **Published**: 2018-03-05 11:25:14+00:00
- **Updated**: 2018-03-05 11:25:14+00:00
- **Authors**: Vandit Gajjar
- **Comment**: 8 Pages, 7 Figures, Submitted to IEEE Computer Society Biometrics
  2018 workshop in conjuction with CVPR 2018
- **Journal**: None
- **Summary**: In this paper, we tackle the classification of gender in facial images with deep learning. Our convolutional neural networks (CNN) use the VGG-16 architecture [1] and are pretrained on ImageNet for image classification. Our proposed method (2^B3^C) first detects the face in the facial image, increases the margin of a detected face by 50%, cropping the face with two boxes three crop schemes (Left, Middle, and Right crop) and extracts the CNN predictions on the cropped schemes. The CNNs of our method is fine-tuned on the Adience and LFW with gender annotations. We show the effectiveness of our method by achieving 90.8% classification on Adience and achieving competitive 95.3% classification accuracy on LFW dataset. In addition, to check the true ability of our method, our gender classification system has a frame rate of 7-10 fps (frames per seconds) on a GPU considering real-time scenarios.



### I Know What You See: Power Side-Channel Attack on Convolutional Neural Network Accelerators
- **Arxiv ID**: http://arxiv.org/abs/1803.05847v2
- **DOI**: 10.1145/3274694.3274696
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.05847v2)
- **Published**: 2018-03-05 11:35:14+00:00
- **Updated**: 2019-11-29 15:11:58+00:00
- **Authors**: Lingxiao Wei, Bo Luo, Yu Li, Yannan Liu, Qiang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has become the de-facto computational paradigm for various kinds of perception problems, including many privacy-sensitive applications such as online medical image analysis. No doubt to say, the data privacy of these deep learning systems is a serious concern. Different from previous research focusing on exploiting privacy leakage from deep learning models, in this paper, we present the first attack on the implementation of deep learning models. To be specific, we perform the attack on an FPGA-based convolutional neural network accelerator and we manage to recover the input image from the collected power traces without knowing the detailed parameters in the neural network. For the MNIST dataset, our power side-channel attack is able to achieve up to 89% recognition accuracy.



### Affine Differential Invariants for Invariant Feature Point Detection
- **Arxiv ID**: http://arxiv.org/abs/1803.01669v2
- **DOI**: None
- **Categories**: **cs.CV**, math.GR
- **Links**: [PDF](http://arxiv.org/pdf/1803.01669v2)
- **Published**: 2018-03-05 14:14:13+00:00
- **Updated**: 2018-03-12 17:02:18+00:00
- **Authors**: Stanley L. Tuznik, Peter J. Olver, Allen Tannenbaum
- **Comment**: None
- **Journal**: None
- **Summary**: Image feature points are detected as pixels which locally maximize a detector function, two commonly used examples of which are the (Euclidean) image gradient and the Harris-Stephens corner detector. A major limitation of these feature detectors are that they are only Euclidean-invariant. In this work we demonstrate the application of a 2D affine-invariant image feature point detector based on differential invariants as derived through the equivariant method of moving frames. The fundamental equi-affine differential invariants for 3D image volumes are also computed.



### Towards Clinical Diagnosis: Automated Stroke Lesion Segmentation on Multimodal MR Image Using Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1803.05848v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.05848v1)
- **Published**: 2018-03-05 14:27:41+00:00
- **Updated**: 2018-03-05 14:27:41+00:00
- **Authors**: Zhiyang Liu, Chen Cao, Shuxue Ding, Tong Han, Hong Wu, Sheng Liu
- **Comment**: submitted to Neuroimage: Clinical
- **Journal**: None
- **Summary**: The patient with ischemic stroke can benefit most from the earliest possible definitive diagnosis. While the high quality medical resources are quite scarce across the globe, an automated diagnostic tool is expected in analyzing the magnetic resonance (MR) images to provide reference in clinical diagnosis. In this paper, we propose a deep learning method to automatically segment ischemic stroke lesions from multi-modal MR images. By using atrous convolution and global convolution network, our proposed residual-structured fully convolutional network (Res-FCN) is able to capture features from large receptive fields. The network architecture is validated on a large dataset of 212 clinically acquired multi-modal MR images, which is shown to achieve a mean dice coefficient of 0.645 with a mean number of false negative lesions of 1.515. The false negatives can reach a value that close to a common medical image doctor, making it exceptive for a real clinical application.



### XNORBIN: A 95 TOp/s/W Hardware Accelerator for Binary Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.05849v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.AR, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1803.05849v1)
- **Published**: 2018-03-05 15:41:28+00:00
- **Updated**: 2018-03-05 15:41:28+00:00
- **Authors**: Andrawes Al Bahou, Geethan Karunaratne, Renzo Andri, Lukas Cavigelli, Luca Benini
- **Comment**: None
- **Journal**: None
- **Summary**: Deploying state-of-the-art CNNs requires power-hungry processors and off-chip memory. This precludes the implementation of CNNs in low-power embedded systems. Recent research shows CNNs sustain extreme quantization, binarizing their weights and intermediate feature maps, thereby saving 8-32\x memory and collapsing energy-intensive sum-of-products into XNOR-and-popcount operations.   We present XNORBIN, an accelerator for binary CNNs with computation tightly coupled to memory for aggressive data reuse. Implemented in UMC 65nm technology XNORBIN achieves an energy efficiency of 95 TOp/s/W and an area efficiency of 2.0 TOp/s/MGE at 0.8 V.



### A generalized parametric 3D shape representation for articulated pose estimation
- **Arxiv ID**: http://arxiv.org/abs/1803.01780v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01780v1)
- **Published**: 2018-03-05 17:12:36+00:00
- **Updated**: 2018-03-05 17:12:36+00:00
- **Authors**: Meng Ding, Guoliang Fan
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: We present a novel parametric 3D shape representation, Generalized sum of Gaussians (G-SoG), which is particularly suitable for pose estimation of articulated objects. Compared with the original sum-of-Gaussians (SoG), G-SoG can handle both isotropic and anisotropic Gaussians, leading to a more flexible and adaptable shape representation yet with much fewer anisotropic Gaussians involved. An articulated shape template can be developed by embedding G-SoG in a tree-structured skeleton model to represent an articulated object. We further derive a differentiable similarity function between G-SoG (the template) and SoG (observed data) that can be optimized analytically for efficient pose estimation. The experimental results on a standard human pose estimation dataset show the effectiveness and advantages of G-SoG over the original SoG as well as the promise compared with the recent algorithms that use more complicated shape models.



### DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild
- **Arxiv ID**: http://arxiv.org/abs/1803.02188v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.02188v2)
- **Published**: 2018-03-05 17:21:35+00:00
- **Updated**: 2018-03-11 20:24:33+00:00
- **Authors**: Riza Alp Guler, Yuxiang Zhou, George Trigeorgis, Epameinondas Antonakos, Patrick Snape, Stefanos Zafeiriou, Iasonas Kokkinos
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1612.01202
- **Journal**: None
- **Summary**: In this work we use deep learning to establish dense correspondences between a 3D object model and an image "in the wild". We introduce "DenseReg", a fully-convolutional neural network (F-CNN) that densely regresses at every foreground pixel a pair of U-V template coordinates in a single feedforward pass. To train DenseReg we construct a supervision signal by combining 3D deformable model fitting and 2D landmark annotations. We define the regression task in terms of the intrinsic, U-V coordinates of a 3D deformable model that is brought into correspondence with image instances at training time. A host of other object-related tasks (e.g. part segmentation, landmark localization) are shown to be by-products of this task, and to largely improve thanks to its introduction. We obtain highly-accurate regression results by combining ideas from semantic segmentation with regression networks, yielding a 'quantized regression' architecture that first obtains a quantized estimate of position through classification, and refines it through regression of the residual. We show that such networks can boost the performance of existing state-of-the-art systems for pose estimation. Firstly, we show that our system can serve as an initialization for Statistical Deformable Models, as well as an element of cascaded architectures that jointly localize landmarks and estimate dense correspondences. We also show that the obtained dense correspondence can act as a source of 'privileged information' that complements and extends the pure landmark-level annotations, accelerating and improving the training of pose estimation networks. We report state-of-the-art performance on the challenging 300W benchmark for facial landmark localization and on the MPII and LSP datasets for human pose estimation.



### Hyperdrive: A Multi-Chip Systolically Scalable Binary-Weight CNN Inference Engine
- **Arxiv ID**: http://arxiv.org/abs/1804.00623v3
- **DOI**: None
- **Categories**: **cs.DC**, cs.AR, cs.CV, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1804.00623v3)
- **Published**: 2018-03-05 17:35:42+00:00
- **Updated**: 2019-03-14 11:15:37+00:00
- **Authors**: Renzo Andri, Lukas Cavigelli, Davide Rossi, Luca Benini
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have achieved impressive results in computer vision and machine learning. Unfortunately, state-of-the-art networks are extremely compute and memory intensive which makes them unsuitable for mW-devices such as IoT end-nodes. Aggressive quantization of these networks dramatically reduces the computation and memory footprint. Binary-weight neural networks (BWNs) follow this trend, pushing weight quantization to the limit. Hardware accelerators for BWNs presented up to now have focused on core efficiency, disregarding I/O bandwidth and system-level efficiency that are crucial for deployment of accelerators in ultra-low power devices. We present Hyperdrive: a BWN accelerator dramatically reducing the I/O bandwidth exploiting a novel binary-weight streaming approach, which can be used for arbitrarily sized convolutional neural network architecture and input resolution by exploiting the natural scalability of the compute units both at chip-level and system-level by arranging Hyperdrive chips systolically in a 2D mesh while processing the entire feature map together in parallel. Hyperdrive achieves 4.3 TOp/s/W system-level efficiency (i.e., including I/Os)---3.1x higher than state-of-the-art BWN accelerators, even if its core uses resource-intensive FP16 arithmetic for increased robustness.



### ST-GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing
- **Arxiv ID**: http://arxiv.org/abs/1803.01837v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.01837v1)
- **Published**: 2018-03-05 18:59:01+00:00
- **Updated**: 2018-03-05 18:59:01+00:00
- **Authors**: Chen-Hsuan Lin, Ersin Yumer, Oliver Wang, Eli Shechtman, Simon Lucey
- **Comment**: Accepted to CVPR 2018 (website & code:
  https://chenhsuanlin.bitbucket.io/spatial-transformer-GAN/)
- **Journal**: None
- **Summary**: We address the problem of finding realistic geometric corrections to a foreground object such that it appears natural when composited into a background image. To achieve this, we propose a novel Generative Adversarial Network (GAN) architecture that utilizes Spatial Transformer Networks (STNs) as the generator, which we call Spatial Transformer GANs (ST-GANs). ST-GANs seek image realism by operating in the geometric warp parameter space. In particular, we exploit an iterative STN warping scheme and propose a sequential training strategy that achieves better results compared to naive training of a single generator. One of the key advantages of ST-GAN is its applicability to high-resolution images indirectly since the predicted warp parameters are transferable between reference frames. We demonstrate our approach in two applications: (1) visualizing how indoor furniture (e.g. from product images) might be perceived in a room, (2) hallucinating how accessories like glasses would look when matched with real portraits.



### Abnormality Detection in Mammography using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.01906v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01906v1)
- **Published**: 2018-03-05 20:04:56+00:00
- **Updated**: 2018-03-05 20:04:56+00:00
- **Authors**: Pengcheng Xi, Chang Shu, Rafik Goubran
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Breast cancer is the most common cancer in women worldwide. The most common screening technology is mammography. To reduce the cost and workload of radiologists, we propose a computer aided detection approach for classifying and localizing calcifications and masses in mammogram images. To improve on conventional approaches, we apply deep convolutional neural networks (CNN) for automatic feature learning and classifier building. In computer-aided mammography, deep CNN classifiers cannot be trained directly on full mammogram images because of the loss of image details from resizing at input layers. Instead, our classifiers are trained on labelled image patches and then adapted to work on full mammogram images for localizing the abnormalities. State-of-the-art deep convolutional neural networks are compared on their performance of classifying the abnormalities. Experimental results indicate that VGGNet receives the best overall accuracy at 92.53\% in classifications. For localizing abnormalities, ResNet is selected for computing class activation maps because it is ready to be deployed without structural change or further training. Our approach demonstrates that deep convolutional neural network classifiers have remarkable localization capabilities despite no supervision on the location of abnormalities is provided.



### M3Fusion: A Deep Learning Architecture for Multi-{Scale/Modal/Temporal} satellite data fusion
- **Arxiv ID**: http://arxiv.org/abs/1803.01945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01945v1)
- **Published**: 2018-03-05 21:59:42+00:00
- **Updated**: 2018-03-05 21:59:42+00:00
- **Authors**: P. Benedetti, D. Ienco, R. Gaetano, K. Osé, R. Pensa, S. Dupuy
- **Comment**: None
- **Journal**: None
- **Summary**: Modern Earth Observation systems provide sensing data at different temporal and spatial resolutions. Among optical sensors, today the Sentinel-2 program supplies high-resolution temporal (every 5 days) and high spatial resolution (10m) images that can be useful to monitor land cover dynamics. On the other hand, Very High Spatial Resolution images (VHSR) are still an essential tool to figure out land cover mapping characterized by fine spatial patterns. Understand how to efficiently leverage these complementary sources of information together to deal with land cover mapping is still challenging. With the aim to tackle land cover mapping through the fusion of multi-temporal High Spatial Resolution and Very High Spatial Resolution satellite images, we propose an End-to-End Deep Learning framework, named M3Fusion, able to leverage simultaneously the temporal knowledge contained in time series data as well as the fine spatial information available in VHSR information. Experiments carried out on the Reunion Island study area asses the quality of our proposal considering both quantitative and qualitative aspects.



### Segmentation of Drosophila Heart in Optical Coherence Microscopy Images Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.01947v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.01947v2)
- **Published**: 2018-03-05 22:03:52+00:00
- **Updated**: 2018-07-21 01:55:59+00:00
- **Authors**: Lian Duan, Xi Qin, Yuanhao He, Xialin Sang, Jinda Pan, Tao Xu, Jing Men, Rudolph E. Tanzi, Airong Li, Yutao Ma, Chao Zhou
- **Comment**: 7 figures
- **Journal**: None
- **Summary**: Convolutional neural networks are powerful tools for image segmentation and classification. Here, we use this method to identify and mark the heart region of Drosophila at different developmental stages in the cross-sectional images acquired by a custom optical coherence microscopy (OCM) system. With our well-trained convolutional neural network model, the heart regions through multiple heartbeat cycles can be marked with an intersection over union (IOU) of ~86%. Various morphological and dynamical cardiac parameters can be quantified accurately with automatically segmented heart regions. This study demonstrates an efficient heart segmentation method to analyze OCM images of the beating heart in Drosophila.



