# Arxiv Papers in cs.CV on 2018-03-11
### Calculating the Midsagittal Plane for Symmetrical Bilateral Shapes: Applications to Clinical Facial Surgical Planning
- **Arxiv ID**: http://arxiv.org/abs/1803.05853v1
- **DOI**: None
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1803.05853v1)
- **Published**: 2018-03-11 01:31:15+00:00
- **Updated**: 2018-03-11 01:31:15+00:00
- **Authors**: Aarti Jajoo, Matthew Nicol, Jaime Gateno, Ken-Chung Chen, Zhen Tang, Tasadduk Chowdhury, Jainfu Li, Steve Goufang Shen, James J. Xia
- **Comment**: None
- **Journal**: None
- **Summary**: It is difficult to estimate the midsagittal plane of human subjects with craniomaxillofacial (CMF) deformities. We have developed a LAndmark GEometric Routine (LAGER), which automatically estimates a midsagittal plane for such subjects. The LAGER algorithm was based on the assumption that the optimal midsagittal plane of a patient with a deformity is the premorbid midsagittal plane of the patient (i.e. hypothetically normal without deformity). The LAGER algorithm consists of three steps. The first step quantifies the asymmetry of the landmarks using a Euclidean distance matrix analysis and ranks the landmarks according to their degree of asymmetry. The second step uses a recursive algorithm to drop outlier landmarks. The third step inputs the remaining landmarks into an optimization algorithm to determine an optimal midsaggital plane. We validate LAGER on 20 synthetic models mimicking the skulls of real patients with CMF deformities. The results indicated that all the LAGER algorithm-generated midsagittal planes met clinical criteria. Thus it can be used clinically to determine the midsagittal plane for patients with CMF deformities.



### Knowledge Aided Consistency for Weakly Supervised Phrase Grounding
- **Arxiv ID**: http://arxiv.org/abs/1803.03879v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03879v1)
- **Published**: 2018-03-11 02:00:24+00:00
- **Updated**: 2018-03-11 02:00:24+00:00
- **Authors**: Kan Chen, Jiyang Gao, Ram Nevatia
- **Comment**: CVPR 2018 conference paper
- **Journal**: None
- **Summary**: Given a natural language query, a phrase grounding system aims to localize mentioned objects in an image. In weakly supervised scenario, mapping between image regions (i.e., proposals) and language is not available in the training set. Previous methods address this deficiency by training a grounding system via learning to reconstruct language information contained in input queries from predicted proposals. However, the optimization is solely guided by the reconstruction loss from the language modality, and ignores rich visual information contained in proposals and useful cues from external knowledge. In this paper, we explore the consistency contained in both visual and language modalities, and leverage complementary external knowledge to facilitate weakly supervised grounding. We propose a novel Knowledge Aided Consistency Network (KAC Net) which is optimized by reconstructing input query and proposal's information. To leverage complementary knowledge contained in the visual features, we introduce a Knowledge Based Pooling (KBP) gate to focus on query-related proposals. Experiments show that KAC Net provides a significant improvement on two popular datasets.



### Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1803.03893v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03893v3)
- **Published**: 2018-03-11 03:56:29+00:00
- **Updated**: 2018-04-05 01:12:43+00:00
- **Authors**: Huangying Zhan, Ravi Garg, Chamara Saroj Weerasekera, Kejie Li, Harsh Agarwal, Ian Reid
- **Comment**: 8 pages, 6 figures, CVPR 2018
- **Journal**: None
- **Summary**: Despite learning based methods showing promising results in single view depth estimation and visual odometry, most existing approaches treat the tasks in a supervised manner. Recent approaches to single view depth estimation explore the possibility of learning without full supervision via minimizing photometric error. In this paper, we explore the use of stereo sequences for learning depth and visual odometry. The use of stereo sequences enables the use of both spatial (between left-right pairs) and temporal (forward backward) photometric warp error, and constrains the scene depth and camera motion to be in a common, real-world scale. At test time our framework is able to estimate single view depth and two-view odometry from a monocular sequence. We also show how we can improve on a standard photometric warp loss by considering a warp of deep features. We show through extensive experiments that: (i) jointly training for single view depth and visual odometry improves depth prediction because of the additional constraint imposed on depths and achieves competitive results for visual odometry; (ii) deep feature-based warping loss improves upon simple photometric warp loss for both single view depth estimation and visual odometry. Our method outperforms existing learning based methods on the KITTI driving dataset in both tasks. The source code is available at https://github.com/Huangying-Zhan/Depth-VO-Feat



### Adaptive Kernel Estimation of the Spectral Density with Boundary Kernel Analysis
- **Arxiv ID**: http://arxiv.org/abs/1803.03906v1
- **DOI**: None
- **Categories**: **stat.ME**, cs.CV, eess.AS, eess.SP, math.ST, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1803.03906v1)
- **Published**: 2018-03-11 05:14:42+00:00
- **Updated**: 2018-03-11 05:14:42+00:00
- **Authors**: Alexander Sidorenko, Kurt S. Riedel
- **Comment**: None
- **Journal**: Approximation Theory VIII: Approximation And Interpolation, pg
  519-528, edited by Chui, Schumaker, 1995 World Scientific
- **Summary**: A hybrid estimator of the log-spectral density of a stationary time series is proposed. First, a multiple taper estimate is performed, followed by kernel smoothing the log-multitaper estimate. This procedure reduces the expected mean square error by $({\pi^2 \over 4})^{.8}$ over simply smoothing the log tapered periodogram. The optimal number of tapers is $O(N^{8/15})$. A data adaptive implementation of a variable bandwidth kernel smoother is given. When the spectral density is discontinuous, one sided smoothing estimates are used.



### Cubic Range Error Model for Stereo Vision with Illuminators
- **Arxiv ID**: http://arxiv.org/abs/1803.03932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03932v1)
- **Published**: 2018-03-11 10:23:25+00:00
- **Updated**: 2018-03-11 10:23:25+00:00
- **Authors**: Marius Huber, Timo Hinzmann, Roland Siegwart, Larry H. Matthies
- **Comment**: 6 pages, to be published at ICRA 2018
- **Journal**: None
- **Summary**: Use of low-cost depth sensors, such as a stereo camera setup with illuminators, is of particular interest for numerous applications ranging from robotics and transportation to mixed and augmented reality. The ability to quantify noise is crucial for these applications, e.g., when the sensor is used for map generation or to develop a sensor scheduling policy in a multi-sensor setup. Range error models provide uncertainty estimates and help weigh the data correctly in instances where range measurements are taken from different vantage points or with different sensors. The weighing is important to fuse range data into a map in a meaningful way, i.e., the high confidence data is relied on most heavily. Such a model is derived in this work. We show that the range error for stereo systems with integrated illuminators is cubic and validate the proposed model experimentally with an off-the-shelf structured light stereo system. The experiments confirm the validity of the model and simplify the application of this type of sensor in robotics. The proposed error model is relevant to any stereo system with low ambient light where the main light source is located at the camera system. Among others, this is the case for structured light stereo systems and night stereo systems with headlights. In this work, we propose that the range error is cubic in range for stereo systems with integrated illuminators. Experimental validation with an off-the-shelf structured light stereo system shows that the exponent is between 2.4 and 2.6. The deviation is attributed to our model considering only shot noise.



### BTS-DSN: Deeply Supervised Neural Network with Short Connections for Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.03963v2
- **DOI**: 10.1016/j.ijmedinf.2019.03.015
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.03963v2)
- **Published**: 2018-03-11 14:10:28+00:00
- **Updated**: 2019-09-23 02:06:55+00:00
- **Authors**: Song Guo, Kai Wang, Hong Kang, Yujun Zhang, Yingqi Gao, Tao Li
- **Comment**: None
- **Journal**: International Journal of Medical Informatics 126, 105-113 (Jun,
  2019)
- **Summary**: Background and Objective: The condition of vessel of the human eye is an important factor for the diagnosis of ophthalmological diseases. Vessel segmentation in fundus images is a challenging task due to complex vessel structure, the presence of similar structures such as microaneurysms and hemorrhages, micro-vessel with only one to several pixels wide, and requirements for finer results. Methods:In this paper, we present a multi-scale deeply supervised network with short connections (BTS-DSN) for vessel segmentation. We used short connections to transfer semantic information between side-output layers. Bottom-top short connections pass low level semantic information to high level for refining results in high-level side-outputs, and top-bottom short connection passes much structural information to low level for reducing noises in low-level side-outputs. In addition, we employ cross-training to show that our model is suitable for real world fundus images. Results: The proposed BTS-DSN has been verified on DRIVE, STARE and CHASE_DB1 datasets, and showed competitive performance over other state-of-the-art methods. Specially, with patch level input, the network achieved 0.7891/0.8212 sensitivity, 0.9804/0.9843 specificity, 0.9806/0.9859 AUC, and 0.8249/0.8421 F1-score on DRIVE and STARE, respectively. Moreover, our model behaves better than other methods in cross-training experiments. Conclusions: BTS-DSN achieves competitive performance in vessel segmentation task on three public datasets. It is suitable for vessel segmentation. The source code of our method is available at https://github.com/guomugong/BTS-DSN.



### Deep Dictionary Learning: A PARametric NETwork Approach
- **Arxiv ID**: http://arxiv.org/abs/1803.04022v1
- **DOI**: 10.1109/TIP.2019.2914376
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04022v1)
- **Published**: 2018-03-11 19:29:56+00:00
- **Updated**: 2018-03-11 19:29:56+00:00
- **Authors**: Shahin Mahdizadehaghdam, Ashkan Panahi, Hamid Krim, Liyi Dai
- **Comment**: None
- **Journal**: None
- **Summary**: Deep dictionary learning seeks multiple dictionaries at different image scales to capture complementary coherent characteristics. We propose a method for learning a hierarchy of synthesis dictionaries with an image classification goal. The dictionaries and classification parameters are trained by a classification objective, and the sparse features are extracted by reducing a reconstruction loss in each layer. The reconstruction objectives in some sense regularize the classification problem and inject source signal information in the extracted features. The performance of the proposed hierarchical method increases by adding more layers, which consequently makes this model easier to tune and adapt. The proposed algorithm furthermore, shows remarkably lower fooling rate in presence of adversarial perturbation. The validation of the proposed approach is based on its classification performance using four benchmark datasets and is compared to a CNN of similar size.



### Cascade context encoder for improved inpainting
- **Arxiv ID**: http://arxiv.org/abs/1803.04033v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04033v1)
- **Published**: 2018-03-11 20:31:39+00:00
- **Updated**: 2018-03-11 20:31:39+00:00
- **Authors**: Bartosz Zieliński, Łukasz Struski, Marek Śmieja, Jacek Tabor
- **Comment**: Supplemental materials are available at
  http://www.ii.uj.edu.pl/~zielinsb
- **Journal**: None
- **Summary**: In this paper, we analyze if cascade usage of the context encoder with increasing input can improve the results of the inpainting. For this purpose, we train context encoder for 64x64 pixels images in a standard way and use its resized output to fill in the missing input region of the 128x128 context encoder, both in training and evaluation phase. As the result, the inpainting is visibly more plausible. In order to thoroughly verify the results, we introduce normalized squared-distortion, a measure for quantitative inpainting evaluation, and we provide its mathematical explanation. This is the first attempt to formalize the inpainting measure, which is based on the properties of latent feature representation, instead of L2 reconstruction loss.



### Multiple Instance Choquet Integral Classifier Fusion and Regression for Remote Sensing Applications
- **Arxiv ID**: http://arxiv.org/abs/1803.04048v2
- **DOI**: 10.1109/TGRS.2018.2876687
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04048v2)
- **Published**: 2018-03-11 21:39:45+00:00
- **Updated**: 2019-02-18 17:34:57+00:00
- **Authors**: Xiaoxiao Du, Alina Zare
- **Comment**: None
- **Journal**: None
- **Summary**: In classifier (or regression) fusion the aim is to combine the outputs of several algorithms to boost overall performance. Standard supervised fusion algorithms often require accurate and precise training labels. However, accurate labels may be difficult to obtain in many remote sensing applications. This paper proposes novel classification and regression fusion models that can be trained given ambiguosly and imprecisely labeled training data in which training labels are associated with sets of data points (i.e., "bags") instead of individual data points (i.e., "instances") following a multiple instance learning framework. Experiments were conducted based on the proposed algorithms on both synthetic data and applications such as target detection and crop yield prediction given remote sensing data. The proposed algorithms show effective classification and regression performance.



### Learning Local Distortion Visibility From Image Quality Data-sets
- **Arxiv ID**: http://arxiv.org/abs/1803.04053v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.04053v1)
- **Published**: 2018-03-11 22:01:37+00:00
- **Updated**: 2018-03-11 22:01:37+00:00
- **Authors**: Navaneeth Kamballur Kottayil, Giuseppe Valenzise, Frederic Dufaux, Irene Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate prediction of local distortion visibility thresholds is critical in many image and video processing applications. Existing methods require an accurate modeling of the human visual system, and are derived through pshycophysical experiments with simple, artificial stimuli. These approaches, however, are difficult to generalize to natural images with complex types of distortion. In this paper, we explore a different perspective, and we investigate whether it is possible to learn local distortion visibility from image quality scores. We propose a convolutional neural network based optimization framework to infer local detection thresholds in a distorted image. Our model is trained on multiple quality datasets, and the results are correlated with empirical visibility thresholds collected on complex stimuli in a recent study. Our results are comparable to state-of-the-art mathematical models that were trained on phsycovisual data directly. This suggests that it is possible to predict psychophysical phenomena from visibility information embedded in image quality scores.



### Two-Stage Convolutional Neural Network for Breast Cancer Histology Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1803.04054v2
- **DOI**: 10.1007/978-3-319-93000-8_81
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.04054v2)
- **Published**: 2018-03-11 22:05:33+00:00
- **Updated**: 2018-04-01 16:37:19+00:00
- **Authors**: Kamyar Nazeri, Azad Aminpour, Mehran Ebrahimi
- **Comment**: 10 pages, 5 figures, ICIAR 2018 conference
- **Journal**: LNCS 10882 (2018) 717-726
- **Summary**: This paper explores the problem of breast tissue classification of microscopy images. Based on the predominant cancer type the goal is to classify images into four categories of normal, benign, in situ carcinoma, and invasive carcinoma. Given a suitable training dataset, we utilize deep learning techniques to address the classification problem. Due to the large size of each image in the training dataset, we propose a patch-based technique which consists of two consecutive convolutional neural networks. The first "patch-wise" network acts as an auto-encoder that extracts the most salient features of image patches while the second "image-wise" network performs classification of the whole image. The first network is pre-trained and aimed at extracting local information while the second network obtains global information of an input image. We trained the networks using the ICIAR 2018 grand challenge on BreAst Cancer Histology (BACH) dataset. The proposed method yields 95 % accuracy on the validation set compared to previously reported 77 % accuracy rates in the literature. Our code is publicly available at https://github.com/ImagingLab/ICIAR2018



