# Arxiv Papers in cs.CV on 2018-03-15
### Object Detection in Video with Spatiotemporal Sampling Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.05549v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05549v2)
- **Published**: 2018-03-15 00:23:22+00:00
- **Updated**: 2018-07-24 04:34:29+00:00
- **Authors**: Gedas Bertasius, Lorenzo Torresani, Jianbo Shi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a Spatiotemporal Sampling Network (STSN) that uses deformable convolutions across time for object detection in videos. Our STSN performs object detection in a video frame by learning to spatially sample features from the adjacent frames. This naturally renders the approach robust to occlusion or motion blur in individual frames. Our framework does not require additional supervision, as it optimizes sampling locations directly with respect to object detection performance. Our STSN outperforms the state-of-the-art on the ImageNet VID dataset and compared to prior video object detection methods it uses a simpler design, and does not require optical flow data for training.



### Facelet-Bank for Fast Portrait Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1803.05576v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05576v3)
- **Published**: 2018-03-15 02:48:55+00:00
- **Updated**: 2018-03-30 08:42:55+00:00
- **Authors**: Ying-Cong Chen, Huaijia Lin, Michelle Shu, Ruiyu Li, Xin Tao, Yangang Ye, Xiaoyong Shen, Jiaya Jia
- **Comment**: Accepted by CVPR 2018. Code is available on
  https://github.com/yingcong/Facelet_Bank
- **Journal**: None
- **Summary**: Digital face manipulation has become a popular and fascinating way to touch images with the prevalence of smartphones and social networks. With a wide variety of user preferences, facial expressions, and accessories, a general and flexible model is necessary to accommodate different types of facial editing. In this paper, we propose a model to achieve this goal based on an end-to-end convolutional neural network that supports fast inference, edit-effect control, and quick partial-model update. In addition, this model learns from unpaired image sets with different attributes. Experimental results show that our framework can handle a wide range of expressions, accessories, and makeup effects. It produces high-resolution and high-quality results in fast speed.



### Deep Adaptive Attention for Joint Facial Action Unit Detection and Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/1803.05588v2
- **DOI**: 10.1007/978-3-030-01261-8_43
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05588v2)
- **Published**: 2018-03-15 04:24:02+00:00
- **Updated**: 2018-07-24 09:05:07+00:00
- **Authors**: Zhiwen Shao, Zhilei Liu, Jianfei Cai, Lizhuang Ma
- **Comment**: This paper has been accepted by ECCV 2018
- **Journal**: None
- **Summary**: Facial action unit (AU) detection and face alignment are two highly correlated tasks since facial landmarks can provide precise AU locations to facilitate the extraction of meaningful local features for AU detection. Most existing AU detection works often treat face alignment as a preprocessing and handle the two tasks independently. In this paper, we propose a novel end-to-end deep learning framework for joint AU detection and face alignment, which has not been explored before. In particular, multi-scale shared features are learned firstly, and high-level features of face alignment are fed into AU detection. Moreover, to extract precise local features, we propose an adaptive attention learning module to refine the attention map of each AU adaptively. Finally, the assembled local features are integrated with face alignment features and global features for AU detection. Experiments on BP4D and DISFA benchmarks demonstrate that our framework significantly outperforms the state-of-the-art methods for AU detection.



### Fast End-to-End Trainable Guided Filter
- **Arxiv ID**: http://arxiv.org/abs/1803.05619v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05619v2)
- **Published**: 2018-03-15 07:31:24+00:00
- **Updated**: 2019-09-25 07:58:10+00:00
- **Authors**: Huikai Wu, Shuai Zheng, Junge Zhang, Kaiqi Huang
- **Comment**: Accepted by CVPR 2018. GitHub:
  https://github.com/wuhuikai/DeepGuidedFilter
- **Journal**: None
- **Summary**: Dense pixel-wise image prediction has been advanced by harnessing the capabilities of Fully Convolutional Networks (FCNs). One central issue of FCNs is the limited capacity to handle joint upsampling. To address the problem, we present a novel building block for FCNs, namely guided filtering layer, which is designed for efficiently generating a high-resolution output given the corresponding low-resolution one and a high-resolution guidance map. Such a layer contains learnable parameters, which can be integrated with FCNs and jointly optimized through end-to-end training. To further take advantage of end-to-end training, we plug in a trainable transformation function for generating the task-specific guidance map. Based on the proposed layer, we present a general framework for pixel-wise image prediction, named deep guided filtering network (DGF). The proposed network is evaluated on five image processing tasks. Experiments on MIT-Adobe FiveK Dataset demonstrate that DGF runs 10-100 times faster and achieves the state-of-the-art performance. We also show that DGF helps to improve the performance of multiple computer vision tasks.



### LEGO: Learning Edge with Geometry all at Once by Watching Videos
- **Arxiv ID**: http://arxiv.org/abs/1803.05648v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05648v2)
- **Published**: 2018-03-15 09:14:11+00:00
- **Updated**: 2018-03-24 00:31:11+00:00
- **Authors**: Zhenheng Yang, Peng Wang, Yang Wang, Wei Xu, Ram Nevatia
- **Comment**: Accepted to CVPR 2018 as spotlight; Camera ready plus supplementary
  material. Code will come
- **Journal**: None
- **Summary**: Learning to estimate 3D geometry in a single image by watching unlabeled videos via deep convolutional network is attracting significant attention. In this paper, we introduce a "3D as-smooth-as-possible (3D-ASAP)" prior inside the pipeline, which enables joint estimation of edges and 3D scene, yielding results with significant improvement in accuracy for fine detailed structures. Specifically, we define the 3D-ASAP prior by requiring that any two points recovered in 3D from an image should lie on an existing planar surface if no other cues provided. We design an unsupervised framework that Learns Edges and Geometry (depth, normal) all at Once (LEGO). The predicted edges are embedded into depth and surface normal smoothness terms, where pixels without edges in-between are constrained to satisfy the prior. In our framework, the predicted depths, normals and edges are forced to be consistent all the time. We conduct experiments on KITTI to evaluate our estimated geometry and CityScapes to perform edge evaluation. We show that in all of the tasks, i.e.depth, normal and edge, our algorithm vastly outperforms other state-of-the-art (SOTA) algorithms, demonstrating the benefits of our approach.



### Fast Subspace Clustering Based on the Kronecker Product
- **Arxiv ID**: http://arxiv.org/abs/1803.05657v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.05657v1)
- **Published**: 2018-03-15 09:31:44+00:00
- **Updated**: 2018-03-15 09:31:44+00:00
- **Authors**: Lei Zhou, Xiao Bai, Xianglong Liu, Jun Zhou, Hancock Edwin
- **Comment**: 16 pages, 2 figures
- **Journal**: None
- **Summary**: Subspace clustering is a useful technique for many computer vision applications in which the intrinsic dimension of high-dimensional data is often smaller than the ambient dimension. Spectral clustering, as one of the main approaches to subspace clustering, often takes on a sparse representation or a low-rank representation to learn a block diagonal self-representation matrix for subspace generation. However, existing methods require solving a large scale convex optimization problem with a large set of data, with computational complexity reaches O(N^3) for N data points. Therefore, the efficiency and scalability of traditional spectral clustering methods can not be guaranteed for large scale datasets. In this paper, we propose a subspace clustering model based on the Kronecker product. Due to the property that the Kronecker product of a block diagonal matrix with any other matrix is still a block diagonal matrix, we can efficiently learn the representation matrix which is formed by the Kronecker product of k smaller matrices. By doing so, our model significantly reduces the computational complexity to O(kN^{3/k}). Furthermore, our model is general in nature, and can be adapted to different regularization based subspace clustering methods. Experimental results on two public datasets show that our model significantly improves the efficiency compared with several state-of-the-art methods. Moreover, we have conducted experiments on synthetic data to verify the scalability of our model for large scale datasets.



### Training of Convolutional Networks on Multiple Heterogeneous Datasets for Street Scene Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.05675v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.05675v2)
- **Published**: 2018-03-15 10:24:43+00:00
- **Updated**: 2018-07-08 14:04:13+00:00
- **Authors**: Panagiotis Meletis, Gijs Dubbelman
- **Comment**: IEEE Intelligent Vehicles 2018
- **Journal**: None
- **Summary**: We propose a convolutional network with hierarchical classifiers for per-pixel semantic segmentation, which is able to be trained on multiple, heterogeneous datasets and exploit their semantic hierarchy. Our network is the first to be simultaneously trained on three different datasets from the intelligent vehicles domain, i.e. Cityscapes, GTSDB and Mapillary Vistas, and is able to handle different semantic level-of-detail, class imbalances, and different annotation types, i.e. dense per-pixel and sparse bounding-box labels. We assess our hierarchical approach, by comparing against flat, non-hierarchical classifiers and we show improvements in mean pixel accuracy of 13.0% for Cityscapes classes and 2.4% for Vistas classes and 32.3% for GTSDB classes. Our implementation achieves inference rates of 17 fps at a resolution of 520x706 for 108 classes running on a GPU.



### Exploring Linear Relationship in Feature Map Subspace for ConvNets Compression
- **Arxiv ID**: http://arxiv.org/abs/1803.05729v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05729v1)
- **Published**: 2018-03-15 13:20:16+00:00
- **Updated**: 2018-03-15 13:20:16+00:00
- **Authors**: Dong Wang, Lei Zhou, Xueni Zhang, Xiao Bai, Jun Zhou
- **Comment**: 17 pages, 4 figures
- **Journal**: None
- **Summary**: While the research on convolutional neural networks (CNNs) is progressing quickly, the real-world deployment of these models is often limited by computing resources and memory constraints. In this paper, we address this issue by proposing a novel filter pruning method to compress and accelerate CNNs. Our work is based on the linear relationship identified in different feature map subspaces via visualization of feature maps. Such linear relationship implies that the information in CNNs is redundant. Our method eliminates the redundancy in convolutional filters by applying subspace clustering to feature maps. In this way, most of the representative information in the network can be retained in each cluster. Therefore, our method provides an effective solution to filter pruning for which most existing methods directly remove filters based on simple heuristics. The proposed method is independent of the network structure, thus it can be adopted by any off-the-shelf deep learning libraries. Experiments on different networks and tasks show that our method outperforms existing techniques before fine-tuning, and achieves the state-of-the-art results after fine-tuning.



### Diverse M-Best Solutions by Dynamic Programming
- **Arxiv ID**: http://arxiv.org/abs/1803.05748v1
- **DOI**: 10.1007/978-3-319-66709-6_21
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05748v1)
- **Published**: 2018-03-15 13:52:01+00:00
- **Updated**: 2018-03-15 13:52:01+00:00
- **Authors**: Carsten Haubold, Virginie Uhlmann, Michael Unser, Fred A. Hamprecht
- **Comment**: Includes supplementary and corrigendum
- **Journal**: Haubold, C., Uhlmann, V., Unser, M., Hamprecht, F. A.: Diverse
  M-best solutions by dynamic programming. In: Roth, V., Vetter, T. (eds.)
  Pattern Recognition. GCPR 2017. vol 10496, pp. 255-267. Springer, Cham (2017)
- **Summary**: Many computer vision pipelines involve dynamic programming primitives such as finding a shortest path or the minimum energy solution in a tree-shaped probabilistic graphical model. In such cases, extracting not merely the best, but the set of M-best solutions is useful to generate a rich collection of candidate proposals that can be used in downstream processing. In this work, we show how M-best solutions of tree-shaped graphical models can be obtained by dynamic programming on a special graph with M layers. The proposed multi-layer concept is optimal for searching M-best solutions, and so flexible that it can also approximate M-best diverse solutions. We illustrate the usefulness with applications to object detection, panorama stitching and centerline extraction.   Note: We have observed that an assumption in section 4 of our paper is not always fulfilled, see the attached corrigendum for details.



### What Catches the Eye? Visualizing and Understanding Deep Saliency Models
- **Arxiv ID**: http://arxiv.org/abs/1803.05753v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05753v3)
- **Published**: 2018-03-15 14:00:28+00:00
- **Updated**: 2018-03-22 09:26:00+00:00
- **Authors**: Sen He, Ali Borji, Yang Mi, Nicolas Pugeault
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks have demonstrated high performances for fixation prediction in recent years. How they achieve this, however, is less explored and they remain to be black box models. Here, we attempt to shed light on the internal structure of deep saliency models and study what features they extract for fixation prediction. Specifically, we use a simple yet powerful architecture, consisting of only one CNN and a single resolution input, combined with a new loss function for pixel-wise fixation prediction during free viewing of natural scenes. We show that our simple method is on par or better than state-of-the-art complicated saliency models. Furthermore, we propose a method, related to saliency model evaluation metrics, to visualize deep models for fixation prediction. Our method reveals the inner representations of deep models for fixation prediction and provides evidence that saliency, as experienced by humans, is likely to involve high-level semantic knowledge in addition to low-level perceptual cues. Our results can be useful to measure the gap between current saliency models and the human inter-observer model and to build new models to close this gap.



### Salient Region Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.05759v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05759v1)
- **Published**: 2018-03-15 14:09:47+00:00
- **Updated**: 2018-03-15 14:09:47+00:00
- **Authors**: Sen He, Nicolas Pugeault
- **Comment**: None
- **Journal**: None
- **Summary**: Saliency prediction is a well studied problem in computer vision. Early saliency models were based on low-level hand-crafted feature derived from insights gained in neuroscience and psychophysics. In the wake of deep learning breakthrough, a new cohort of models were proposed based on neural network architectures, allowing significantly higher gaze prediction than previous shallow models, on all metrics.   However, most models treat the saliency prediction as a \textit{regression} problem, and accurate regression of high-dimensional data is known to be a hard problem. Furthermore, it is unclear that intermediate levels of saliency (ie, neither very high, nor very low) are meaningful: Something is either salient, or it is not.   Drawing from those two observations, we reformulate the saliency prediction problem as a salient region \textit{segmentation} problem. We demonstrate that the reformulation allows for faster convergence than the classical regression problem, while performance is comparable to state-of-the-art.   We also visualise the general features learned by the model, which are showed to be consistent with insights from psychophysics.



### Aggregated Sparse Attention for Steering Angle Prediction
- **Arxiv ID**: http://arxiv.org/abs/1803.05785v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05785v1)
- **Published**: 2018-03-15 14:48:47+00:00
- **Updated**: 2018-03-15 14:48:47+00:00
- **Authors**: Sen He, Dmitry Kangin, Yang Mi, Nicolas Pugeault
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we apply the attention mechanism to autonomous driving for steering angle prediction. We propose the first model, applying the recently introduced sparse attention mechanism to visual domain, as well as the aggregated extension for this model. We show the improvement of the proposed method, comparing to no attention as well as to different types of attention.



### Temporal Human Action Segmentation via Dynamic Clustering
- **Arxiv ID**: http://arxiv.org/abs/1803.05790v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05790v2)
- **Published**: 2018-03-15 14:55:22+00:00
- **Updated**: 2018-03-18 23:26:10+00:00
- **Authors**: Yan Zhang, He Sun, Siyu Tang, Heiko Neumann
- **Comment**: comparing with the 1st version, only corrected typos
- **Journal**: None
- **Summary**: We present an effective dynamic clustering algorithm for the task of temporal human action segmentation, which has comprehensive applications such as robotics, motion analysis, and patient monitoring. Our proposed algorithm is unsupervised, fast, generic to process various types of features, and applicable in both the online and offline settings. We perform extensive experiments of processing data streams, and show that our algorithm achieves the state-of-the-art results for both online and offline settings.



### 2D Reconstruction of Small Intestine's Interior Wall
- **Arxiv ID**: http://arxiv.org/abs/1803.05817v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1803.05817v1)
- **Published**: 2018-03-15 15:46:00+00:00
- **Updated**: 2018-03-15 15:46:00+00:00
- **Authors**: Rahman Attar, Xiang Xie, Zhihua Wang, Shigang Yue
- **Comment**: Journal draft
- **Journal**: None
- **Summary**: Examining and interpreting of a large number of wireless endoscopic images from the gastrointestinal tract is a tiresome task for physicians. A practical solution is to automatically construct a two dimensional representation of the gastrointestinal tract for easy inspection. However, little has been done on wireless endoscopic image stitching, let alone systematic investigation. The proposed new wireless endoscopic image stitching method consists of two main steps to improve the accuracy and efficiency of image registration. First, the keypoints are extracted by Principle Component Analysis and Scale Invariant Feature Transform (PCA-SIFT) algorithm and refined with Maximum Likelihood Estimation SAmple Consensus (MLESAC) outlier removal to find the most reliable keypoints. Second, the optimal transformation parameters obtained from first step are fed to the Normalised Mutual Information (NMI) algorithm as an initial solution. With modified Marquardt-Levenberg search strategy in a multiscale framework, the NMI can find the optimal transformation parameters in the shortest time. The proposed methodology has been tested on two different datasets - one with real wireless endoscopic images and another with images obtained from Micro-Ball (a new wireless cubic endoscopy system with six image sensors). The results have demonstrated the accuracy and robustness of the proposed methodology both visually and quantitatively.



### Local Spectral Graph Convolution for Point Set Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/1803.05827v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.05827v1)
- **Published**: 2018-03-15 16:00:50+00:00
- **Updated**: 2018-03-15 16:00:50+00:00
- **Authors**: Chu Wang, Babak Samari, Kaleem Siddiqi
- **Comment**: None
- **Journal**: None
- **Summary**: Feature learning on point clouds has shown great promise, with the introduction of effective and generalizable deep learning frameworks such as pointnet++. Thus far, however, point features have been abstracted in an independent and isolated manner, ignoring the relative layout of neighboring points as well as their features. In the present article, we propose to overcome this limitation by using spectral graph convolution on a local graph, combined with a novel graph pooling strategy. In our approach, graph convolution is carried out on a nearest neighbor graph constructed from a point's neighborhood, such that features are jointly learned. We replace the standard max pooling step with a recursive clustering and pooling strategy, devised to aggregate information from within clusters of nodes that are close to one another in their spectral coordinates, leading to richer overall feature descriptors. Through extensive experiments on diverse datasets, we show a consistent demonstrable advantage for the tasks of both point set classification and segmentation.



### Pseudo Mask Augmented Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1803.05858v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05858v2)
- **Published**: 2018-03-15 16:51:57+00:00
- **Updated**: 2018-03-27 02:36:38+00:00
- **Authors**: Xiangyun Zhao, Shuang Liang, Yichen Wei
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a novel and effective framework to facilitate object detection with the instance-level segmentation information that is only supervised by bounding box annotation. Starting from the joint object detection and instance segmentation network, we propose to recursively estimate the pseudo ground-truth object masks from the instance-level object segmentation network training, and then enhance the detection network with top-down segmentation feedbacks. The pseudo ground truth mask and network parameters are optimized alternatively to mutually benefit each other. To obtain the promising pseudo masks in each iteration, we embed a graphical inference that incorporates the low-level image appearance consistency and the bounding box annotations to refine the segmentation masks predicted by the segmentation network. Our approach progressively improves the object detection performance by incorporating the detailed pixel-wise information learned from the weakly-supervised segmentation network. Extensive evaluation on the detection task in PASCAL VOC 2007 and 2012 [12] verifies that the proposed approach is effective.



### Learned Neural Iterative Decoding for Lossy Image Compression Systems
- **Arxiv ID**: http://arxiv.org/abs/1803.05863v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05863v3)
- **Published**: 2018-03-15 16:58:45+00:00
- **Updated**: 2018-11-10 06:24:57+00:00
- **Authors**: Alexander G. Ororbia, Ankur Mali, Jian Wu, Scott O'Connell, David Miller, C. Lee Giles
- **Comment**: Vastly updated version, now includes JP2
- **Journal**: None
- **Summary**: For lossy image compression systems, we develop an algorithm, iterative refinement, to improve the decoder's reconstruction compared to standard decoding techniques. Specifically, we propose a recurrent neural network approach for nonlinear, iterative decoding. Our decoder, which works with any encoder, employs self-connected memory units that make use of causal and non-causal spatial context information to progressively reduce reconstruction error over a fixed number of steps. We experiment with variants of our estimator and find that iterative refinement consistently creates lower distortion images of higher perceptual quality compared to other approaches. Specifically, on the Kodak Lossless True Color Image Suite, we observe as much as a 0.871 decibel (dB) gain over JPEG, a 1.095 dB gain over JPEG 2000, and a 0.971 dB gain over a competitive neural model.



### Virtual CNN Branching: Efficient Feature Ensemble for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1803.05872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05872v1)
- **Published**: 2018-03-15 17:11:07+00:00
- **Updated**: 2018-03-15 17:11:07+00:00
- **Authors**: Albert Gong, Qiang Qiu, Guillermo Sapiro
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce an ensemble method for convolutional neural network (CNN), called "virtual branching," which can be implemented with nearly no additional parameters and computation on top of standard CNNs. We propose our method in the context of person re-identification (re-ID). Our CNN model consists of shared bottom layers, followed by "virtual" branches, where neurons from a block of regular convolutional and fully-connected layers are partitioned into multiple sets. Each virtual branch is trained with different data to specialize in different aspects, e.g., a specific body region or pose orientation. In this way, robust ensemble representations are obtained against human body misalignment, deformations, or variations in viewing angles, at nearly no any additional cost. The proposed method achieves competitive performance on multiple person re-ID benchmark datasets, including Market-1501, CUHK03, and DukeMTMC-reID.



### Deep Structure Inference Network for Facial Action Unit Recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.05873v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05873v2)
- **Published**: 2018-03-15 17:14:35+00:00
- **Updated**: 2018-03-23 11:17:17+00:00
- **Authors**: Ciprian A. Corneanu, Meysam Madadi, Sergio Escalera
- **Comment**: None
- **Journal**: None
- **Summary**: Facial expressions are combinations of basic components called Action Units (AU). Recognizing AUs is key for developing general facial expression analysis. In recent years, most efforts in automatic AU recognition have been dedicated to learning combinations of local features and to exploiting correlations between Action Units. In this paper, we propose a deep neural architecture that tackles both problems by combining learned local and global features in its initial stages and replicating a message passing algorithm between classes similar to a graphical model inference approach in later stages. We show that by training the model end-to-end with increased supervision we improve state-of-the-art by 5.3% and 8.2% performance on BP4D and DISFA datasets, respectively.



### Toolflows for Mapping Convolutional Neural Networks on FPGAs: A Survey and Future Directions
- **Arxiv ID**: http://arxiv.org/abs/1803.05900v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.05900v1)
- **Published**: 2018-03-15 17:58:19+00:00
- **Updated**: 2018-03-15 17:58:19+00:00
- **Authors**: Stylianos I. Venieris, Alexandros Kouris, Christos-Savvas Bouganis
- **Comment**: Accepted for publication at the ACM Computing Surveys (CSUR) journal,
  2018
- **Journal**: None
- **Summary**: In the past decade, Convolutional Neural Networks (CNNs) have demonstrated state-of-the-art performance in various Artificial Intelligence tasks. To accelerate the experimentation and development of CNNs, several software frameworks have been released, primarily targeting power-hungry CPUs and GPUs. In this context, reconfigurable hardware in the form of FPGAs constitutes a potential alternative platform that can be integrated in the existing deep learning ecosystem to provide a tunable balance between performance, power consumption and programmability. In this paper, a survey of the existing CNN-to-FPGA toolflows is presented, comprising a comparative study of their key characteristics which include the supported applications, architectural choices, design space exploration methods and achieved performance. Moreover, major challenges and objectives introduced by the latest trends in CNN algorithmic research are identified and presented. Finally, a uniform evaluation methodology is proposed, aiming at the comprehensive, complete and in-depth evaluation of CNN-to-FPGA toolflows.



### Smartphone picture organization: A hierarchical approach
- **Arxiv ID**: http://arxiv.org/abs/1803.05940v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05940v2)
- **Published**: 2018-03-15 18:37:50+00:00
- **Updated**: 2019-09-06 19:16:10+00:00
- **Authors**: Stefan Lonn, Petia Radeva, Mariella Dimiccoli
- **Comment**: None
- **Journal**: Computer Vision and Image Understanding (CVIU), Volume 187,
  October 2019, 102789
- **Summary**: We live in a society where the large majority of the population has a camera-equipped smartphone. In addition, hard drives and cloud storage are getting cheaper and cheaper, leading to a tremendous growth in stored personal photos. Unlike photo collections captured by a digital camera, which typically are pre-processed by the user who organizes them into event-related folders, smartphone pictures are automatically stored in the cloud. As a consequence, photo collections captured by a smartphone are highly unstructured and because smartphones are ubiquitous, they present a larger variability compared to pictures captured by a digital camera. To solve the need of organizing large smartphone photo collections automatically, we propose here a new methodology for hierarchical photo organization into topics and topic-related categories. Our approach successfully estimates latent topics in the pictures by applying probabilistic Latent Semantic Analysis, and automatically assigns a name to each topic by relying on a lexical database. Topic-related categories are then estimated by using a set of topic-specific Convolutional Neuronal Networks. To validate our approach, we ensemble and make public a large dataset of more than 8,000 smartphone pictures from 10 persons. Experimental results demonstrate better user satisfaction with respect to state of the art solutions in terms of organization.



### Mo2Cap2: Real-time Mobile 3D Motion Capture with a Cap-mounted Fisheye Camera
- **Arxiv ID**: http://arxiv.org/abs/1803.05959v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05959v2)
- **Published**: 2018-03-15 19:20:12+00:00
- **Updated**: 2019-01-23 10:30:21+00:00
- **Authors**: Weipeng Xu, Avishek Chatterjee, Michael Zollhoefer, Helge Rhodin, Pascal Fua, Hans-Peter Seidel, Christian Theobalt
- **Comment**: IEEE TVCG Proc. VR 2019. Webpage:
  http://gvv.mpi-inf.mpg.de/projects/wxu/Mo2Cap2/
- **Journal**: None
- **Summary**: We propose the first real-time approach for the egocentric estimation of 3D human body pose in a wide range of unconstrained everyday activities. This setting has a unique set of challenges, such as mobility of the hardware setup, and robustness to long capture sessions with fast recovery from tracking failures. We tackle these challenges based on a novel lightweight setup that converts a standard baseball cap to a device for high-quality pose estimation based on a single cap-mounted fisheye camera. From the captured egocentric live stream, our CNN based 3D pose estimation approach runs at 60Hz on a consumer-level GPU. In addition to the novel hardware setup, our other main contributions are: 1) a large ground truth training corpus of top-down fisheye images and 2) a novel disentangled 3D pose estimation approach that takes the unique properties of the egocentric viewpoint into account. As shown by our evaluation, we achieve lower 3D joint error as well as better 2D overlay than the existing baselines.



### Studying Invariances of Trained Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.05963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05963v1)
- **Published**: 2018-03-15 19:27:41+00:00
- **Updated**: 2018-03-15 19:27:41+00:00
- **Authors**: Charlotte Bunne, Lukas Rahmann, Thomas Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) define an exceptionally powerful class of models for image classification, but the theoretical background and the understanding of how invariances to certain transformations are learned is limited. In a large scale screening with images modified by different affine and nonaffine transformations of varying magnitude, we analyzed the behavior of the CNN architectures AlexNet and ResNet. If the magnitude of different transformations does not exceed a class- and transformation dependent threshold, both architectures show invariant behavior. In this work we furthermore introduce a new learnable module, the Invariant Transformer Net, which enables us to learn differentiable parameters for a set of affine transformations. This allows us to extract the space of transformations to which the CNN is invariant and its class prediction robust.



### Real-time Deep Pose Estimation with Geodesic Loss for Image-to-Template Rigid Registration
- **Arxiv ID**: http://arxiv.org/abs/1803.05982v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05982v4)
- **Published**: 2018-03-15 20:07:59+00:00
- **Updated**: 2018-08-18 19:59:46+00:00
- **Authors**: Seyed Sadegh Mohseni Salehi, Shadab Khan, Deniz Erdogmus, Ali Gholipour
- **Comment**: This work has been submitted to TMI
- **Journal**: None
- **Summary**: With an aim to increase the capture range and accelerate the performance of state-of-the-art inter-subject and subject-to-template 3D registration, we propose deep learning-based methods that are trained to find the 3D position of arbitrarily oriented subjects or anatomy based on slices or volumes of medical images. For this, we propose regression CNNs that learn to predict the angle-axis representation of 3D rotations and translations using image features. We use and compare mean square error and geodesic loss to train regression CNNs for 3D pose estimation used in two different scenarios: slice-to-volume registration and volume-to-volume registration. Our results show that in such registration applications that are amendable to learning, the proposed deep learning methods with geodesic loss minimization can achieve accurate results with a wide capture range in real-time (<100ms). We also tested the generalization capability of the trained CNNs on an expanded age range and on images of newborn subjects with similar and different MR image contrasts. We trained our models on T2-weighted fetal brain MRI scans and used them to predict the 3D pose of newborn brains based on T1-weighted MRI scans. We showed that the trained models generalized well for the new domain when we performed image contrast transfer through a conditional generative adversarial network. This indicates that the domain of application of the trained deep regression CNNs can be further expanded to image modalities and contrasts other than those used in training. A combination of our proposed methods with accelerated optimization-based registration algorithms can dramatically enhance the performance of automatic imaging devices and image processing methods of the future.



### Deep Co-Training for Semi-Supervised Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.05984v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05984v1)
- **Published**: 2018-03-15 20:13:07+00:00
- **Updated**: 2018-03-15 20:13:07+00:00
- **Authors**: Siyuan Qiao, Wei Shen, Zhishuai Zhang, Bo Wang, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the problem of semi-supervised image recognition, which is to learn classifiers using both labeled and unlabeled images. We present Deep Co-Training, a deep learning based method inspired by the Co-Training framework. The original Co-Training learns two classifiers on two views which are data from different sources that describe the same instances. To extend this concept to deep learning, Deep Co-Training trains multiple deep neural networks to be the different views and exploits adversarial examples to encourage view difference, in order to prevent the networks from collapsing into each other. As a result, the co-trained networks provide different and complementary information about the data, which is necessary for the Co-Training framework to achieve good results. We test our method on SVHN, CIFAR-10/100 and ImageNet datasets, and our method outperforms the previous state-of-the-art methods by a large margin.



### Efficient Hardware Realization of Convolutional Neural Networks using Intra-Kernel Regular Pruning
- **Arxiv ID**: http://arxiv.org/abs/1803.05909v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.05909v1)
- **Published**: 2018-03-15 21:00:17+00:00
- **Updated**: 2018-03-15 21:00:17+00:00
- **Authors**: Maurice Yang, Mahmoud Faraj, Assem Hussein, Vincent Gaudet
- **Comment**: 6 pages, 8 figures, ISMVL 2018
- **Journal**: None
- **Summary**: The recent trend toward increasingly deep convolutional neural networks (CNNs) leads to a higher demand of computational power and memory storage. Consequently, the deployment of CNNs in hardware has become more challenging. In this paper, we propose an Intra-Kernel Regular (IKR) pruning scheme to reduce the size and computational complexity of the CNNs by removing redundant weights at a fine-grained level. Unlike other pruning methods such as Fine-Grained pruning, IKR pruning maintains regular kernel structures that are exploitable in a hardware accelerator. Experimental results demonstrate up to 10x parameter reduction and 7x computational reduction at a cost of less than 1% degradation in accuracy versus the un-pruned case.



### Transferable Pedestrian Motion Prediction Models at Intersections
- **Arxiv ID**: http://arxiv.org/abs/1804.00495v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1804.00495v2)
- **Published**: 2018-03-15 23:58:19+00:00
- **Updated**: 2019-09-18 23:51:54+00:00
- **Authors**: Macheng Shen, Golnaz Habibi, Jonathan P. How
- **Comment**: None
- **Journal**: None
- **Summary**: One desirable capability of autonomous cars is to accurately predict the pedestrian motion near intersections for safe and efficient trajectory planning. We are interested in developing transfer learning algorithms that can be trained on the pedestrian trajectories collected at one intersection and yet still provide accurate predictions of the trajectories at another, previously unseen intersection. We first discussed the feature selection for transferable pedestrian motion models in general. Following this discussion, we developed one transferable pedestrian motion prediction algorithm based on Inverse Reinforcement Learning (IRL) that infers pedestrian intentions and predicts future trajectories based on observed trajectory. We evaluated our algorithm on a dataset collected at two intersections, trained at one intersection and tested at the other intersection. We used the accuracy of augmented semi-nonnegative sparse coding (ASNSC), trained and tested at the same intersection as a baseline. The result shows that the proposed algorithm improves the baseline accuracy by 40% in the non-transfer task, and 16% in the transfer task.



