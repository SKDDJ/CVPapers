# Arxiv Papers in cs.CV on 2018-03-22
### CalibNet: Geometrically Supervised Extrinsic Calibration using 3D Spatial Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.08181v2
- **DOI**: 10.1109/IROS.2018.8593693
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.08181v2)
- **Published**: 2018-03-22 00:24:04+00:00
- **Updated**: 2019-08-04 10:40:14+00:00
- **Authors**: Ganesh Iyer, R. Karnik Ram., J. Krishna Murthy, K. Madhava Krishna
- **Comment**: Appeared in the proccedings of the IEEE International Conference on
  Intelligent Robots and Systems (IROS) 2018
- **Journal**: None
- **Summary**: 3D LiDARs and 2D cameras are increasingly being used alongside each other in sensor rigs for perception tasks. Before these sensors can be used to gather meaningful data, however, their extrinsics (and intrinsics) need to be accurately calibrated, as the performance of the sensor rig is extremely sensitive to these calibration parameters. A vast majority of existing calibration techniques require significant amounts of data and/or calibration targets and human effort, severely impacting their applicability in large-scale production systems. We address this gap with CalibNet: a self-supervised deep network capable of automatically estimating the 6-DoF rigid body transformation between a 3D LiDAR and a 2D camera in real-time. CalibNet alleviates the need for calibration targets, thereby resulting in significant savings in calibration efforts. During training, the network only takes as input a LiDAR point cloud, the corresponding monocular image, and the camera calibration matrix K. At train time, we do not impose direct supervision (i.e., we do not directly regress to the calibration parameters, for example). Instead, we train the network to predict calibration parameters that maximize the geometric and photometric consistency of the input images and point clouds. CalibNet learns to iteratively solve the underlying geometric problem and accurately predicts extrinsic calibration parameters for a wide range of mis-calibrations, without requiring retraining or domain adaptation. The project page is hosted at https://epiception.github.io/CalibNet



### Deep Pose Consensus Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.08190v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08190v2)
- **Published**: 2018-03-22 01:22:50+00:00
- **Updated**: 2019-10-07 04:44:03+00:00
- **Authors**: Geonho Cha, Minsik Lee, Jungchan Cho, Songhwai Oh
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of estimating a 3D human pose from a single image, which is important but difficult to solve due to many reasons, such as self-occlusions, wild appearance changes, and inherent ambiguities of 3D estimation from a 2D cue. These difficulties make the problem ill-posed, which have become requiring increasingly complex estimators to enhance the performance. On the other hand, most existing methods try to handle this problem based on a single complex estimator, which might not be good solutions. In this paper, to resolve this issue, we propose a multiple-partial-hypothesis-based framework for the problem of estimating 3D human pose from a single image, which can be fine-tuned in an end-to-end fashion. We first select several joint groups from a human joint model using the proposed sampling scheme, and estimate the 3D poses of each joint group separately based on deep neural networks. After that, they are aggregated to obtain the final 3D poses using the proposed robust optimization formula. The overall procedure can be fine-tuned in an end-to-end fashion, resulting in better performance. In the experiments, the proposed framework shows the state-of-the-art performances on popular benchmark data sets, namely Human3.6M and HumanEva, which demonstrate the effectiveness of the proposed framework.



### Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs
- **Arxiv ID**: http://arxiv.org/abs/1803.08207v2
- **DOI**: 10.1038/s41592-019-0575-8
- **Categories**: **q-bio.QM**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.08207v2)
- **Published**: 2018-03-22 02:24:22+00:00
- **Updated**: 2018-10-08 19:18:18+00:00
- **Authors**: Tristan Bepler, Andrew Morin, Julia Brasch, Lawrence Shapiro, Alex J. Noble, Bonnie Berger
- **Comment**: 43 pages, 5 main figures, 6 supplemental figures
- **Journal**: Nature Methods (2019)
- **Summary**: Cryo-electron microscopy (cryoEM) is an increasingly popular method for protein structure determination. However, identifying a sufficient number of particles for analysis (often >100,000) can take months of manual effort. Current computational approaches are limited by high false positive rates and require significant ad-hoc post-processing, especially for unusually shaped particles. To address this shortcoming, we develop Topaz, an efficient and accurate particle picking pipeline using neural networks trained with few labeled particles by newly leveraging the remaining unlabeled particles through the framework of positive-unlabeled (PU) learning. Remarkably, despite using minimal labeled particles, Topaz allows us to improve reconstruction resolution by up to 0.15 {\AA} over published particles on three public cryoEM datasets without any post-processing. Furthermore, we show that our novel generalized-expectation criteria approach to PU learning outperforms existing general PU learning approaches when applied to particle detection, especially for challenging datasets of non-globular proteins. We expect Topaz to be an essential component of cryoEM analysis.



### Single-Shot Bidirectional Pyramid Networks for High-Quality Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1803.08208v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08208v1)
- **Published**: 2018-03-22 02:25:54+00:00
- **Updated**: 2018-03-22 02:25:54+00:00
- **Authors**: Xiongwei Wu, Daoxin Zhang, Jianke Zhu, Steven C. H. Hoi
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have witnessed many exciting achievements for object detection using deep learning techniques. Despite achieving significant progresses, most existing detectors are designed to detect objects with relatively low-quality prediction of locations, i.e., often trained with the threshold of Intersection over Union (IoU) set to 0.5 by default, which can yield low-quality or even noisy detections. It remains an open challenge for how to devise and train a high-quality detector that can achieve more precise localization (i.e., IoU$>$0.5) without sacrificing the detection performance. In this paper, we propose a novel single-shot detection framework of Bidirectional Pyramid Networks (BPN) towards high-quality object detection, which consists of two novel components: (i) a Bidirectional Feature Pyramid structure for more effective and robust feature representations; and (ii) a Cascade Anchor Refinement to gradually refine the quality of predesigned anchors for more effective training. Our experiments showed that the proposed BPN achieves the best performances among all the single-stage object detectors on both PASCAL VOC and MS COCO datasets, especially for high-quality detections.



### PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model
- **Arxiv ID**: http://arxiv.org/abs/1803.08225v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08225v1)
- **Published**: 2018-03-22 04:31:02+00:00
- **Updated**: 2018-03-22 04:31:02+00:00
- **Authors**: George Papandreou, Tyler Zhu, Liang-Chieh Chen, Spyros Gidaris, Jonathan Tompson, Kevin Murphy
- **Comment**: Person detection and pose estimation, segmentation and grouping
- **Journal**: None
- **Summary**: We present a box-free bottom-up approach for the tasks of pose estimation and instance segmentation of people in multi-person images using an efficient single-shot model. The proposed PersonLab model tackles both semantic-level reasoning and object-part associations using part-based modeling. Our model employs a convolutional network which learns to detect individual keypoints and predict their relative displacements, allowing us to group keypoints into person pose instances. Further, we propose a part-induced geometric embedding descriptor which allows us to associate semantic person pixels with their corresponding person instance, delivering instance-level person segmentations. Our system is based on a fully-convolutional architecture and allows for efficient inference, with runtime essentially independent of the number of people present in the scene. Trained on COCO data alone, our system achieves COCO test-dev keypoint average precision of 0.665 using single-scale inference and 0.687 using multi-scale inference, significantly outperforming all previous bottom-up pose estimation systems. We are also the first bottom-up method to report competitive results for the person class in the COCO instance segmentation task, achieving a person category average precision of 0.417.



### Entrenamiento de una red neuronal para el reconocimiento de imagenes de lengua de senas capturadas con sensores de profundidad
- **Arxiv ID**: http://arxiv.org/abs/1804.00508v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1804.00508v1)
- **Published**: 2018-03-22 05:40:28+00:00
- **Updated**: 2018-03-22 05:40:28+00:00
- **Authors**: Rivas P. Pedro E., Velarde-Anaya Omar, Gonzalez-Lopez Samuel, Rivas P. Pablo, Alvarez-Torres Norma Angelica
- **Comment**: 5 pages, in Spanish, 10 figures, 1 table
- **Journal**: None
- **Summary**: Due to the growth of the population with hearing problems, devices have been developed that facilitate the inclusion of deaf people in society, using technology as a communication tool, such as vision systems. Then, a solution to this problem is presented using neural networks and autoencoders for the classification of American Sign Language images. As a result, 99.5% accuracy and an error of 0.01684 were obtained for image classification



### Unsupervised Adversarial Learning of 3D Human Pose from 2D Joint Locations
- **Arxiv ID**: http://arxiv.org/abs/1803.08244v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08244v1)
- **Published**: 2018-03-22 06:41:23+00:00
- **Updated**: 2018-03-22 06:41:23+00:00
- **Authors**: Yasunori Kudo, Keisuke Ogaki, Yusuke Matsui, Yuri Odagiri
- **Comment**: None
- **Journal**: None
- **Summary**: The task of three-dimensional (3D) human pose estimation from a single image can be divided into two parts: (1) Two-dimensional (2D) human joint detection from the image and (2) estimating a 3D pose from the 2D joints. Herein, we focus on the second part, i.e., a 3D pose estimation from 2D joint locations. The problem with existing methods is that they require either (1) a 3D pose dataset or (2) 2D joint locations in consecutive frames taken from a video sequence. We aim to solve these problems. For the first time, we propose a method that learns a 3D human pose without any 3D datasets. Our method can predict a 3D pose from 2D joint locations in a single image. Our system is based on the generative adversarial networks, and the networks are trained in an unsupervised manner. Our primary idea is that, if the network can predict a 3D human pose correctly, the 3D pose that is projected onto a 2D plane should not collapse even if it is rotated perpendicularly. We evaluated the performance of our method using Human3.6M and the MPII dataset and showed that our network can predict a 3D pose well even if the 3D dataset is not available during training.



### Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/1803.08314v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08314v3)
- **Published**: 2018-03-22 11:52:10+00:00
- **Updated**: 2018-07-23 01:35:21+00:00
- **Authors**: Xihui Liu, Hongsheng Li, Jing Shao, Dapeng Chen, Xiaogang Wang
- **Comment**: Accepted by ECCV 2018
- **Journal**: None
- **Summary**: The aim of image captioning is to generate captions by machine to describe image contents. Despite many efforts, generating discriminative captions for images remains non-trivial. Most traditional approaches imitate the language structure patterns, thus tend to fall into a stereotype of replicating frequent phrases or sentences and neglect unique aspects of each image. In this work, we propose an image captioning framework with a self-retrieval module as training guidance, which encourages generating discriminative captions. It brings unique advantages: (1) the self-retrieval guidance can act as a metric and an evaluator of caption discriminativeness to assure the quality of generated captions. (2) The correspondence between generated captions and images are naturally incorporated in the generation process without human annotations, and hence our approach could utilize a large amount of unlabeled images to boost captioning performance with no additional laborious annotations. We demonstrate the effectiveness of the proposed retrieval-guided method on COCO and Flickr30k captioning datasets, and show its superior captioning performance with more discriminative captions.



### Learning to Detect and Track Visible and Occluded Body Joints in a Virtual World
- **Arxiv ID**: http://arxiv.org/abs/1803.08319v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08319v3)
- **Published**: 2018-03-22 12:03:19+00:00
- **Updated**: 2018-09-18 14:34:14+00:00
- **Authors**: Matteo Fabbri, Fabio Lanzi, Simone Calderara, Andrea Palazzi, Roberto Vezzani, Rita Cucchiara
- **Comment**: Accepted at ECCV 2018
- **Journal**: None
- **Summary**: Multi-People Tracking in an open-world setting requires a special effort in precise detection. Moreover, temporal continuity in the detection phase gains more importance when scene cluttering introduces the challenging problems of occluded targets. For the purpose, we propose a deep network architecture that jointly extracts people body parts and associates them across short temporal spans. Our model explicitly deals with occluded body parts, by hallucinating plausible solutions of not visible joints. We propose a new end-to-end architecture composed by four branches (visible heatmaps, occluded heatmaps, part affinity fields and temporal affinity fields) fed by a time linker feature extractor. To overcome the lack of surveillance data with tracking, body part and occlusion annotations we created the vastest Computer Graphics dataset for people tracking in urban scenarios by exploiting a photorealistic videogame. It is up to now the vastest dataset (about 500.000 frames, almost 10 million body poses) of human body parts for people tracking in urban scenarios. Our architecture trained on virtual data exhibits good generalization capabilities also on public real tracking benchmarks, when image resolution and sharpness are high enough, producing reliable tracklets useful for further batch data association or re-id modules.



### Prioritized Multi-View Stereo Depth Map Generation Using Confidence Prediction
- **Arxiv ID**: http://arxiv.org/abs/1803.08323v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08323v1)
- **Published**: 2018-03-22 12:21:21+00:00
- **Updated**: 2018-03-22 12:21:21+00:00
- **Authors**: Christian Mostegel, Friedrich Fraundorfer, Horst Bischof
- **Comment**: This paper was accepted to ISPRS Journal of Photogrammetry and Remote
  Sensing
  (https://www.journals.elsevier.com/isprs-journal-of-photogrammetry-and-remote-sensing)
  on March 21, 2018. The official version will be made available on
  ScienceDirect (https://www.sciencedirect.com)
- **Journal**: None
- **Summary**: In this work, we propose a novel approach to prioritize the depth map computation of multi-view stereo (MVS) to obtain compact 3D point clouds of high quality and completeness at low computational cost. Our prioritization approach operates before the MVS algorithm is executed and consists of two steps. In the first step, we aim to find a good set of matching partners for each view. In the second step, we rank the resulting view clusters (i.e. key views with matching partners) according to their impact on the fulfillment of desired quality parameters such as completeness, ground resolution and accuracy. Additional to geometric analysis, we use a novel machine learning technique for training a confidence predictor. The purpose of this confidence predictor is to estimate the chances of a successful depth reconstruction for each pixel in each image for one specific MVS algorithm based on the RGB images and the image constellation. The underlying machine learning technique does not require any ground truth or manually labeled data for training, but instead adapts ideas from depth map fusion for providing a supervision signal. The trained confidence predictor allows us to evaluate the quality of image constellations and their potential impact to the resulting 3D reconstruction and thus builds a solid foundation for our prioritization approach. In our experiments, we are thus able to reach more than 70% of the maximal reachable quality fulfillment using only 5% of the available images as key views. For evaluating our approach within and across different domains, we use two completely different scenarios, i.e. cultural heritage preservation and reconstruction of single family houses.



### Revisiting Gray Pixel for Statistical Illumination Estimation
- **Arxiv ID**: http://arxiv.org/abs/1803.08326v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08326v4)
- **Published**: 2018-03-22 12:45:36+00:00
- **Updated**: 2019-01-09 14:19:24+00:00
- **Authors**: Yanlin Qian, Said Pertuz, Jarno Nikkanen, Joni-Kristian Kämäräinen, Jiri Matas
- **Comment**: updated and will appear in VISSAP 2019 (long paper)
- **Journal**: None
- **Summary**: We present a statistical color constancy method that relies on novel gray pixel detection and mean shift clustering. The method, called Mean Shifted Grey Pixel -- MSGP, is based on the observation: true-gray pixels are aligned towards one single direction. Our solution is compact, easy to compute and requires no training. Experiments on two real-world benchmarks show that the proposed approach outperforms state-of-the-art methods in the camera-agnostic scenario. In the setting where the camera is known, MSGP outperforms all statistical methods.



### What do Deep Networks Like to See?
- **Arxiv ID**: http://arxiv.org/abs/1803.08337v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.08337v1)
- **Published**: 2018-03-22 13:10:47+00:00
- **Updated**: 2018-03-22 13:10:47+00:00
- **Authors**: Sebastian Palacio, Joachim Folz, Jörn Hees, Federico Raue, Damian Borth, Andreas Dengel
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel way to measure and understand convolutional neural networks by quantifying the amount of input signal they let in. To do this, an autoencoder (AE) was fine-tuned on gradients from a pre-trained classifier with fixed parameters. We compared the reconstructed samples from AEs that were fine-tuned on a set of image classifiers (AlexNet, VGG16, ResNet-50, and Inception~v3) and found substantial differences. The AE learns which aspects of the input space to preserve and which ones to ignore, based on the information encoded in the backpropagated gradients. Measuring the changes in accuracy when the signal of one classifier is used by a second one, a relation of total order emerges. This order depends directly on each classifier's input signal but it does not correlate with classification accuracy or network size. Further evidence of this phenomenon is provided by measuring the normalized mutual information between original images and auto-encoded reconstructions from different fine-tuned AEs. These findings break new ground in the area of neural network understanding, opening a new way to reason, debug, and interpret their results. We present four concrete examples in the literature where observations can now be explained in terms of the input signal that a model uses.



### Deep Learning using Rectified Linear Units (ReLU)
- **Arxiv ID**: http://arxiv.org/abs/1803.08375v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.08375v2)
- **Published**: 2018-03-22 14:30:17+00:00
- **Updated**: 2019-02-07 06:13:13+00:00
- **Authors**: Abien Fred Agarap
- **Comment**: 7 pages, 11 figures, 9 tables
- **Journal**: None
- **Summary**: We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer $h_{n - 1}$ in a neural network, then multiply it by weight parameters $\theta$ to get the raw scores $o_{i}$. Afterwards, we threshold the raw scores $o_{i}$ by $0$, i.e. $f(o) = \max(0, o_{i})$, where $f(o)$ is the ReLU function. We provide class predictions $\hat{y}$ through argmax function, i.e. argmax $f(x)$.



### Found a good match: should I keep searching? - Accuracy and Performance in Iris Matching Using 1-to-First Search
- **Arxiv ID**: http://arxiv.org/abs/1803.08394v1
- **DOI**: 10.1016/j.imavis.2018.03.003
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08394v1)
- **Published**: 2018-03-22 15:07:53+00:00
- **Updated**: 2018-03-22 15:07:53+00:00
- **Authors**: Andrey Kuehlkamp, Kevin Bowyer
- **Comment**: None
- **Journal**: Image and Vision Computing vol 73, May 2018, pp. 17-27
- **Summary**: Iris recognition is used in many applications around the world, with enrollment sizes as large as over one billion persons in India's Aadhaar program. Large enrollment sizes can require special optimizations in order to achieve fast database searches. One such optimization that has been used in some operational scenarios is 1:First search. In this approach, instead of scanning the entire database, the search is terminated when the first sufficiently good match is found. This saves time, but ignores potentially better matches that may exist in the unexamined portion of the enrollments. At least one prominent and successful border-crossing program used this approach for nearly a decade, in order to allow users a fast "token-free" search. Our work investigates the search accuracy of 1:First and compares it to the traditional 1:N search. Several different scenarios are considered trying to emulate real environments as best as possible: a range of enrollment sizes, closed- and open-set configurations, two iris matchers, and different permutations of the galleries. Results confirm the expected accuracy degradation using 1:First search, and also allow us to identify acceptable working parameters where significant search time reduction is achieved, while maintaining accuracy similar to 1:N search.



### Densely Connected Pyramid Dehazing Network
- **Arxiv ID**: http://arxiv.org/abs/1803.08396v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1803.08396v1)
- **Published**: 2018-03-22 15:09:53+00:00
- **Updated**: 2018-03-22 15:09:53+00:00
- **Authors**: He Zhang, Vishal M. Patel
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new end-to-end single image dehazing method, called Densely Connected Pyramid Dehazing Network (DCPDN), which can jointly learn the transmission map, atmospheric light and dehazing all together. The end-to-end learning is achieved by directly embedding the atmospheric scattering model into the network, thereby ensuring that the proposed method strictly follows the physics-driven scattering model for dehazing. Inspired by the dense network that can maximize the information flow along features from different levels, we propose a new edge-preserving densely connected encoder-decoder structure with multi-level pyramid pooling module for estimating the transmission map. This network is optimized using a newly introduced edge-preserving loss function. To further incorporate the mutual structural information between the estimated transmission map and the dehazed result, we propose a joint-discriminator based on generative adversarial network framework to decide whether the corresponding dehazed image and the estimated transmission map are real or fake. An ablation study is conducted to demonstrate the effectiveness of each module evaluated at both estimated transmission map and dehazed result. Extensive experiments demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods. Code will be made available at: https://github.com/hezhangsprinter



### PlaneMatch: Patch Coplanarity Prediction for Robust RGB-D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1803.08407v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08407v3)
- **Published**: 2018-03-22 15:29:21+00:00
- **Updated**: 2018-07-27 04:43:15+00:00
- **Authors**: Yifei Shi, Kai Xu, Matthias Niessner, Szymon Rusinkiewicz, Thomas Funkhouser
- **Comment**: ECCV 2018 oral paper; Supplemental material included
- **Journal**: ECCV 2018
- **Summary**: We introduce a novel RGB-D patch descriptor designed for detecting coplanar surfaces in SLAM reconstruction. The core of our method is a deep convolutional neural net that takes in RGB, depth, and normal information of a planar patch in an image and outputs a descriptor that can be used to find coplanar patches from other images.We train the network on 10 million triplets of coplanar and non-coplanar patches, and evaluate on a new coplanarity benchmark created from commodity RGB-D scans. Experiments show that our learned descriptor outperforms alternatives extended for this new task by a significant margin. In addition, we demonstrate the benefits of coplanarity matching in a robust RGBD reconstruction formulation.We find that coplanarity constraints detected with our method are sufficient to get reconstruction results comparable to state-of-the-art frameworks on most scenes, but outperform other methods on standard benchmarks when combined with a simple keypoint method.



### A Smoke Removal Method for Laparoscopic Images
- **Arxiv ID**: http://arxiv.org/abs/1803.08410v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08410v1)
- **Published**: 2018-03-22 15:34:40+00:00
- **Updated**: 2018-03-22 15:34:40+00:00
- **Authors**: Congcong Wang, Faouzi Alaya Cheikh, Mounir Kaaniche, Ole Jacob Elle
- **Comment**: None
- **Journal**: None
- **Summary**: In laparoscopic surgery, image quality can be severely degraded by surgical smoke, which not only introduces error for the image processing (used in image guided surgery), but also reduces the visibility of the surgeons. In this paper, we propose to enhance the laparoscopic images by decomposing them into unwanted smoke part and enhanced part using a variational approach. The proposed method relies on the observation that smoke has low contrast and low inter-channel differences. A cost function is defined based on this prior knowledge and is solved using an augmented Lagrangian method. The obtained unwanted smoke component is then subtracted from the original degraded image, resulting in the enhanced image. The obtained quantitative scores in terms of FADE, JNBM and RE metrics show that our proposed method performs rather well. Furthermore, the qualitative visual inspection of the results show that it removes smoke effectively from the laparoscopic images.



### Group Sparsity Residual with Non-Local Samples for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1803.08412v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08412v1)
- **Published**: 2018-03-22 15:37:17+00:00
- **Updated**: 2018-03-22 15:37:17+00:00
- **Authors**: Zhiyuan Zha, Xinggan Zhang, Qiong Wang, Yechao Bai, Lan Tang, Xin Yuan
- **Comment**: None
- **Journal**: International Conference on Acoustics, Speech and Signal
  Processing 2018
- **Summary**: Inspired by group-based sparse coding, recently proposed group sparsity residual (GSR) scheme has demonstrated superior performance in image processing. However, one challenge in GSR is to estimate the residual by using a proper reference of the group-based sparse coding (GSC), which is desired to be as close to the truth as possible. Previous researches utilized the estimations from other algorithms (i.e., GMM or BM3D), which are either not accurate or too slow. In this paper, we propose to use the Non-Local Samples (NLS) as reference in the GSR regime for image denoising, thus termed GSR-NLS. More specifically, we first obtain a good estimation of the group sparse coefficients by the image nonlocal self-similarity, and then solve the GSR model by an effective iterative shrinkage algorithm. Experimental results demonstrate that the proposed GSR-NLS not only outperforms many state-of-the-art methods, but also delivers the competitive advantage of speed.



### Buried object detection from B-scan ground penetrating radar data using Faster-RCNN
- **Arxiv ID**: http://arxiv.org/abs/1803.08414v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08414v1)
- **Published**: 2018-03-22 15:41:43+00:00
- **Updated**: 2018-03-22 15:41:43+00:00
- **Authors**: Minh-Tan Pham, Sébastien Lefèvre
- **Comment**: 4 pages, to appear in IGARSS 2018
- **Journal**: None
- **Summary**: In this paper, we adapt the Faster-RCNN framework for the detection of underground buried objects (i.e. hyperbola reflections) in B-scan ground penetrating radar (GPR) images. Due to the lack of real data for training, we propose to incorporate more simulated radargrams generated from different configurations using the gprMax toolbox. Our designed CNN is first pre-trained on the grayscale Cifar-10 database. Then, the Faster-RCNN framework based on the pre-trained CNN is trained and fine-tuned on both real and simulated GPR data. Preliminary detection results show that the proposed technique can provide significant improvements compared to classical computer vision methods and hence becomes quite promising to deal with this kind of specific GPR data even with few training samples.



### Incremental Color Quantization for Color-Vision-Deficient Observers Using Mobile Gaming Data
- **Arxiv ID**: http://arxiv.org/abs/1803.08420v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1803.08420v1)
- **Published**: 2018-03-22 15:54:43+00:00
- **Updated**: 2018-03-22 15:54:43+00:00
- **Authors**: Jose Cambronero, Phillip Stanley-Marbell, Martin Rinard
- **Comment**: None
- **Journal**: None
- **Summary**: The sizes of compressed images depend on their spatial resolution (number of pixels) and on their color resolution (number of color quantization levels). We introduce DaltonQuant, a new color quantization technique for image compression that cloud services can apply to images destined for a specific user with known color vision deficiencies. DaltonQuant improves compression in a user-specific but reversible manner thereby improving a user's network bandwidth and data storage efficiency. DaltonQuant quantizes image data to account for user-specific color perception anomalies, using a new method for incremental color quantization based on a large corpus of color vision acuity data obtained from a popular mobile game. Servers that host images can revert DaltonQuant's image requantization and compression when those images must be transmitted to a different user, making the technique practical to deploy on a large scale. We evaluate DaltonQuant's compression performance on the Kodak PC reference image set and show that it improves compression by an additional 22%-29% over the state-of-the-art compressors TinyPNG and pngquant.



### Guided Image Inpainting: Replacing an Image Region by Pulling Content from Another Image
- **Arxiv ID**: http://arxiv.org/abs/1803.08435v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08435v1)
- **Published**: 2018-03-22 16:20:45+00:00
- **Updated**: 2018-03-22 16:20:45+00:00
- **Authors**: Yinan Zhao, Brian Price, Scott Cohen, Danna Gurari
- **Comment**: None
- **Journal**: None
- **Summary**: Deep generative models have shown success in automatically synthesizing missing image regions using surrounding context. However, users cannot directly decide what content to synthesize with such approaches. We propose an end-to-end network for image inpainting that uses a different image to guide the synthesis of new content to fill the hole. A key challenge addressed by our approach is synthesizing new content in regions where the guidance image and the context of the original image are inconsistent. We conduct four studies that demonstrate our results yield more realistic image inpainting results over seven baselines.



### A Comprehensive Analysis of Deep Regression
- **Arxiv ID**: http://arxiv.org/abs/1803.08450v3
- **DOI**: 10.1109/TPAMI.2019.2910523
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08450v3)
- **Published**: 2018-03-22 16:46:39+00:00
- **Updated**: 2020-09-24 15:10:03+00:00
- **Authors**: Stéphane Lathuilière, Pablo Mesejo, Xavier Alameda-Pineda, Radu Horaud
- **Comment**: Published in IEEE TPAMI
- **Journal**: IEEE TPAMI Volume: 42 , Issue: 9 , Sept. 1 2020
- **Summary**: Deep learning revolutionized data science, and recently its popularity has grown exponentially, as did the amount of papers employing deep networks. Vision tasks, such as human pose estimation, did not escape from this trend. There is a large number of deep models, where small changes in the network architecture, or in the data pre-processing, together with the stochastic nature of the optimization procedures, produce notably different results, making extremely difficult to sift methods that significantly outperform others. This situation motivates the current study, in which we perform a systematic evaluation and statistical analysis of vanilla deep regression, i.e. convolutional neural networks with a linear regression top layer. This is the first comprehensive analysis of deep regression techniques. We perform experiments on four vision problems, and report confidence intervals for the median performance as well as the statistical significance of the results, if any. Surprisingly, the variability due to different data pre-processing procedures generally eclipses the variability due to modifications in the network architecture. Our results reinforce the hypothesis according to which, in general, a general-purpose network (e.g. VGG-16 or ResNet-50) adequately tuned can yield results close to the state-of-the-art without having to resort to more complex and ad-hoc regression models.



### Clustering-driven Deep Embedding with Pairwise Constraints
- **Arxiv ID**: http://arxiv.org/abs/1803.08457v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08457v5)
- **Published**: 2018-03-22 16:58:06+00:00
- **Updated**: 2018-10-19 15:29:28+00:00
- **Authors**: Sharon Fogel, Hadar Averbuch-Elor, Jacov Goldberger, Daniel Cohen-Or
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, there has been increasing interest to leverage the competence of neural networks to analyze data. In particular, new clustering methods that employ deep embeddings have been presented. In this paper, we depart from centroid-based models and suggest a new framework, called Clustering-driven deep embedding with PAirwise Constraints (CPAC), for non-parametric clustering using a neural network. We present a clustering-driven embedding based on a Siamese network that encourages pairs of data points to output similar representations in the latent space. Our pair-based model allows augmenting the information with labeled pairs to constitute a semi-supervised framework. Our approach is based on analyzing the losses associated with each pair to refine the set of constraints. We show that clustering performance increases when using this scheme, even with a limited amount of user queries. We demonstrate how our architecture is adapted for various types of data and present the first deep framework to cluster 3D shapes.



### Towards Universal Representation for Unseen Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.08460v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1803.08460v1)
- **Published**: 2018-03-22 17:02:45+00:00
- **Updated**: 2018-03-22 17:02:45+00:00
- **Authors**: Yi Zhu, Yang Long, Yu Guan, Shawn Newsam, Ling Shao
- **Comment**: Accepted at CVPR 2018
- **Journal**: None
- **Summary**: Unseen Action Recognition (UAR) aims to recognise novel action categories without training examples. While previous methods focus on inner-dataset seen/unseen splits, this paper proposes a pipeline using a large-scale training source to achieve a Universal Representation (UR) that can generalise to a more realistic Cross-Dataset UAR (CD-UAR) scenario. We first address UAR as a Generalised Multiple-Instance Learning (GMIL) problem and discover 'building-blocks' from the large-scale ActivityNet dataset using distribution kernels. Essential visual and semantic components are preserved in a shared space to achieve the UR that can efficiently generalise to new datasets. Predicted UR exemplars can be improved by a simple semantic adaptation, and then an unseen action can be directly recognised using UR during the test. Without further training, extensive experiments manifest significant improvements over the UCF101 and HMDB51 benchmarks.



### BSD-GAN: Branched Generative Adversarial Network for Scale-Disentangled Representation Learning and Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1803.08467v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08467v5)
- **Published**: 2018-03-22 17:07:32+00:00
- **Updated**: 2020-08-04 02:17:13+00:00
- **Authors**: Zili Yi, Zhiqin Chen, Hao Cai, Wendong Mao, Minglun Gong, Hao Zhang
- **Comment**: 12 pages, 20 figures, accepted to IEEE Transaction on Image
  Processing
- **Journal**: None
- **Summary**: We introduce BSD-GAN, a novel multi-branch and scale-disentangled training method which enables unconditional Generative Adversarial Networks (GANs) to learn image representations at multiple scales, benefiting a wide range of generation and editing tasks. The key feature of BSD-GAN is that it is trained in multiple branches, progressively covering both the breadth and depth of the network, as resolutions of the training images increase to reveal finer-scale features. Specifically, each noise vector, as input to the generator network of BSD-GAN, is deliberately split into several sub-vectors, each corresponding to, and is trained to learn, image representations at a particular scale. During training, we progressively "de-freeze" the sub-vectors, one at a time, as a new set of higher-resolution images is employed for training and more network layers are added. A consequence of such an explicit sub-vector designation is that we can directly manipulate and even combine latent (sub-vector) codes which model different feature scales.Extensive experiments demonstrate the effectiveness of our training method in scale-disentangled learning of image representations and synthesis of novel image contents, without any extra labels and without compromising quality of the synthesized high-resolution images. We further demonstrate several image generation and manipulation applications enabled or improved by BSD-GAN. Source codes are available at https://github.com/duxingren14/BSD-GAN.



### KonIQ-10k: Towards an ecologically valid and large-scale IQA database
- **Arxiv ID**: http://arxiv.org/abs/1803.08489v1
- **DOI**: 10.1109/TIP.2020.2967829
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1803.08489v1)
- **Published**: 2018-03-22 17:50:05+00:00
- **Updated**: 2018-03-22 17:50:05+00:00
- **Authors**: Hanhe Lin, Vlad Hosu, Dietmar Saupe
- **Comment**: Image database, image quality assessment, diversity sampling,
  crowdsourcing
- **Journal**: None
- **Summary**: The main challenge in applying state-of-the-art deep learning methods to predict image quality in-the-wild is the relatively small size of existing quality scored datasets. The reason for the lack of larger datasets is the massive resources required in generating diverse and publishable content. We present a new systematic and scalable approach to create large-scale, authentic and diverse image datasets for Image Quality Assessment (IQA). We show how we built an IQA database, KonIQ-10k, consisting of 10,073 images, on which we performed very large scale crowdsourcing experiments in order to obtain reliable quality ratings from 1,467 crowd workers (1.2 million ratings). We argue for its ecological validity by analyzing the diversity of the dataset, by comparing it to state-of-the-art IQA databases, and by checking the reliability of our user studies.



### Group Normalization
- **Arxiv ID**: http://arxiv.org/abs/1803.08494v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.08494v3)
- **Published**: 2018-03-22 17:57:16+00:00
- **Updated**: 2018-06-11 22:48:02+00:00
- **Authors**: Yuxin Wu, Kaiming He
- **Comment**: v3: Update trained-from-scratch results in COCO to 41.0AP. Code and
  models at
  https://github.com/facebookresearch/Detectron/blob/master/projects/GN
- **Journal**: None
- **Summary**: Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.



### Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1803.08495v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.08495v1)
- **Published**: 2018-03-22 17:57:47+00:00
- **Updated**: 2018-03-22 17:57:47+00:00
- **Authors**: Kevin Chen, Christopher B. Choy, Manolis Savva, Angel X. Chang, Thomas Funkhouser, Silvio Savarese
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for generating colored 3D shapes from natural language. To this end, we first learn joint embeddings of freeform text descriptions and colored 3D shapes. Our model combines and extends learning by association and metric learning approaches to learn implicit cross-modal connections, and produces a joint representation that captures the many-to-many relations between language and physical properties of 3D shapes such as color and shape. To evaluate our approach, we collect a large dataset of natural language descriptions for physical 3D objects in the ShapeNet dataset. With this learned joint embedding we demonstrate text-to-shape retrieval that outperforms baseline approaches. Using our embeddings with a novel conditional Wasserstein GAN framework, we generate colored 3D shapes from text. Our method is the first to connect natural language text with realistic 3D objects exhibiting rich variations in color, texture, and shape detail. See video at https://youtu.be/zraPvRdl13Q



### Generalized Scene Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1803.08496v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08496v3)
- **Published**: 2018-03-22 17:59:19+00:00
- **Updated**: 2018-05-24 20:47:35+00:00
- **Authors**: John K. Leffingwell, Donald J. Meagher, Khan W. Mahmud, Scott Ackerson
- **Comment**: 14 pages, 21 figures
- **Journal**: None
- **Summary**: A new passive approach called Generalized Scene Reconstruction (GSR) enables "generalized scenes" to be effectively reconstructed. Generalized scenes are defined to be "boundless" spaces that include non-Lambertian, partially transmissive, textureless and finely-structured matter. A new data structure called a plenoptic octree is introduced to enable efficient (database-like) light and matter field reconstruction in devices such as mobile phones, augmented reality (AR) glasses and drones. To satisfy threshold requirements for GSR accuracy, scenes are represented as systems of partially polarized light, radiometrically interacting with matter. To demonstrate GSR, a prototype imaging polarimeter is used to reconstruct (in generalized light fields) highly reflective, hail-damaged automobile body panels. Follow-on GSR experiments are described.



### Aligning Across Large Gaps in Time
- **Arxiv ID**: http://arxiv.org/abs/1803.08542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08542v1)
- **Published**: 2018-03-22 18:48:56+00:00
- **Updated**: 2018-03-22 18:48:56+00:00
- **Authors**: Hunter Goforth, Simon Lucey
- **Comment**: 15 pages, 10 figures
- **Journal**: None
- **Summary**: We present a method of temporally-invariant image registration for outdoor scenes, with invariance across time of day, across seasonal variations, and across decade-long periods, for low- and high-texture scenes. Our method can be useful for applications in remote sensing, GPS-denied UAV localization, 3D reconstruction, and many others. Our method leverages a recently proposed approach to image registration, where fully-convolutional neural networks are used to create feature maps which can be registered using the Inverse-Composition Lucas-Kanade algorithm (ICLK). We show that invariance that is learned from satellite imagery can be transferable to time-lapse data captured by webcams mounted on buildings near ground-level.



### Weighted Bilinear Coding over Salient Body Parts for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1803.08580v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08580v3)
- **Published**: 2018-03-22 20:51:26+00:00
- **Updated**: 2020-01-08 14:39:21+00:00
- **Authors**: Zhigang Chang, Qin Zhou, Heng Fan, Hang Su, Hua Yang, Shibao Zheng, Haibin Ling
- **Comment**: 22 pages
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have demonstrated dominant performance in person re-identification (Re-ID). Existing CNN based methods utilize global average pooling (GAP) to aggregate intermediate convolutional features for Re-ID. However, this strategy only considers the first-order statistics of local features and treats local features at different locations equally important, leading to sub-optimal feature representation. To deal with these issues, we propose a novel weighted bilinear coding (WBC) framework for local feature aggregation in CNN networks to pursue more representative and discriminative feature representations, which can adapt to other state-of-the-art methods and improve their performance. In specific, bilinear coding is used to encode the channel-wise feature correlations to capture richer feature interactions. Meanwhile, a weighting scheme is applied on the bilinear coding to adaptively adjust the weights of local features at different locations based on their importance in recognition, further improving the discriminability of feature aggregation. To handle the spatial misalignment issue, we use a salient part net (spatial attention module) to derive salient body parts, and apply the WBC model on each part. The final representation, formed by concatenating the WBC encoded features of each part, is both discriminative and resistant to spatial misalignment. Experiments on three benchmarks including Market-1501, DukeMTMC-reID and CUHK03 evidence the favorable performance of our method against other outstanding methods.



### Maximum Consensus Parameter Estimation by Reweighted $\ell_1$ Methods
- **Arxiv ID**: http://arxiv.org/abs/1803.08602v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08602v1)
- **Published**: 2018-03-22 22:35:48+00:00
- **Updated**: 2018-03-22 22:35:48+00:00
- **Authors**: Pulak Purkait, Christopher Zach, Anders Eriksson
- **Comment**: EMMCVPR 2017
- **Journal**: None
- **Summary**: Robust parameter estimation in computer vision is frequently accomplished by solving the maximum consensus (MaxCon) problem. Widely used randomized methods for MaxCon, however, can only produce {random} approximate solutions, while global methods are too slow to exercise on realistic problem sizes. Here we analyse MaxCon as iterative reweighted algorithms on the data residuals. We propose a smooth surrogate function, the minimization of which leads to an extremely simple iteratively reweighted algorithm for MaxCon. We show that our algorithm is very efficient and in many cases, yields the global solution. This makes it an attractive alternative for randomized methods and global optimizers. The convergence analysis of our method and its fundamental differences from the other iteratively reweighted methods are also presented.



### DeepDRR -- A Catalyst for Machine Learning in Fluoroscopy-guided Procedures
- **Arxiv ID**: http://arxiv.org/abs/1803.08606v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.08606v1)
- **Published**: 2018-03-22 23:04:16+00:00
- **Updated**: 2018-03-22 23:04:16+00:00
- **Authors**: Mathias Unberath, Jan-Nico Zaech, Sing Chun Lee, Bastian Bier, Javad Fotouhi, Mehran Armand, Nassir Navab
- **Comment**: MU and JNZ have contributed equally
- **Journal**: None
- **Summary**: Machine learning-based approaches outperform competing methods in most disciplines relevant to diagnostic radiology. Interventional radiology, however, has not yet benefited substantially from the advent of deep learning, in particular because of two reasons: 1) Most images acquired during the procedure are never archived and are thus not available for learning, and 2) even if they were available, annotations would be a severe challenge due to the vast amounts of data. When considering fluoroscopy-guided procedures, an interesting alternative to true interventional fluoroscopy is in silico simulation of the procedure from 3D diagnostic CT. In this case, labeling is comparably easy and potentially readily available, yet, the appropriateness of resulting synthetic data is dependent on the forward model. In this work, we propose DeepDRR, a framework for fast and realistic simulation of fluoroscopy and digital radiography from CT scans, tightly integrated with the software platforms native to deep learning. We use machine learning for material decomposition and scatter estimation in 3D and 2D, respectively, combined with analytic forward projection and noise injection to achieve the required performance. On the example of anatomical landmark detection in X-ray images of the pelvis, we demonstrate that machine learning models trained on DeepDRRs generalize to unseen clinically acquired data without the need for re-training or domain adaptation. Our results are promising and promote the establishment of machine learning in fluoroscopy-guided procedures.



### A Quantization-Friendly Separable Convolution for MobileNets
- **Arxiv ID**: http://arxiv.org/abs/1803.08607v3
- **DOI**: 10.1109/EMC2.2018.00011
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08607v3)
- **Published**: 2018-03-22 23:06:38+00:00
- **Updated**: 2019-03-12 17:58:19+00:00
- **Authors**: Tao Sheng, Chen Feng, Shaojie Zhuo, Xiaopeng Zhang, Liang Shen, Mickey Aleksic
- **Comment**: Accepted At THE 1ST WORKSHOP ON ENERGY EFFICIENT MACHINE LEARNING AND
  COGNITIVE COMPUTING FOR EMBEDDED APPLICATIONS (EMC^2 2018)
- **Journal**: https://ieeexplore.ieee.org/document/8524017, 2018
- **Summary**: As deep learning (DL) is being rapidly pushed to edge computing, researchers invented various ways to make inference computation more efficient on mobile/IoT devices, such as network pruning, parameter compression, and etc. Quantization, as one of the key approaches, can effectively offload GPU, and make it possible to deploy DL on fixed-point pipeline. Unfortunately, not all existing networks design are friendly to quantization. For example, the popular lightweight MobileNetV1, while it successfully reduces parameter size and computation latency with separable convolution, our experiment shows its quantized models have large accuracy gap against its float point models. To resolve this, we analyzed the root cause of quantization loss and proposed a quantization-friendly separable convolution architecture. By evaluating the image classification task on ImageNet2012 dataset, our modified MobileNetV1 model can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the gap to the float pipeline.



### X-ray-transform Invariant Anatomical Landmark Detection for Pelvic Trauma Surgery
- **Arxiv ID**: http://arxiv.org/abs/1803.08608v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08608v1)
- **Published**: 2018-03-22 23:09:50+00:00
- **Updated**: 2018-03-22 23:09:50+00:00
- **Authors**: Bastian Bier, Mathias Unberath, Jan-Nico Zaech, Javad Fotouhi, Mehran Armand, Greg Osgood, Nassir Navab, Andreas Maier
- **Comment**: BB and MU have contributed equally and are listed alphabetically
- **Journal**: None
- **Summary**: X-ray image guidance enables percutaneous alternatives to complex procedures. Unfortunately, the indirect view onto the anatomy in addition to projective simplification substantially increase the task-load for the surgeon. Additional 3D information such as knowledge of anatomical landmarks can benefit surgical decision making in complicated scenarios. Automatic detection of these landmarks in transmission imaging is challenging since image-domain features characteristic to a certain landmark change substantially depending on the viewing direction. Consequently and to the best of our knowledge, the above problem has not yet been addressed. In this work, we present a method to automatically detect anatomical landmarks in X-ray images independent of the viewing direction. To this end, a sequential prediction framework based on convolutional layers is trained on synthetically generated data of the pelvic anatomy to predict 23 landmarks in single X-ray images. View independence is contingent on training conditions and, here, is achieved on a spherical segment covering (120 x 90) degrees in LAO/RAO and CRAN/CAUD, respectively, centered around AP. On synthetic data, the proposed approach achieves a mean prediction error of 5.6 +- 4.5 mm. We demonstrate that the proposed network is immediately applicable to clinically acquired data of the pelvis. In particular, we show that our intra-operative landmark detection together with pre-operative CT enables X-ray pose estimation which, ultimately, benefits initialization of image-based 2D/3D registration.



### Closing the Calibration Loop: An Inside-out-tracking Paradigm for Augmented Reality in Orthopedic Surgery
- **Arxiv ID**: http://arxiv.org/abs/1803.08610v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08610v1)
- **Published**: 2018-03-22 23:15:59+00:00
- **Updated**: 2018-03-22 23:15:59+00:00
- **Authors**: Jonas Hajek, Mathias Unberath, Javad Fotouhi, Bastian Bier, Sing Chun Lee, Greg Osgood, Andreas Maier, Mehran Armand, Nassir Navab
- **Comment**: JH, MU, and JF have contributed equally
- **Journal**: None
- **Summary**: In percutaneous orthopedic interventions the surgeon attempts to reduce and fixate fractures in bony structures. The complexity of these interventions arises when the surgeon performs the challenging task of navigating surgical tools percutaneously only under the guidance of 2D interventional X-ray imaging. Moreover, the intra-operatively acquired data is only visualized indirectly on external displays. In this work, we propose a flexible Augmented Reality (AR) paradigm using optical see-through head mounted displays. The key technical contribution of this work includes the marker-less and dynamic tracking concept which closes the calibration loop between patient, C-arm and the surgeon. This calibration is enabled using Simultaneous Localization and Mapping of the environment of the operating theater. In return, the proposed solution provides in situ visualization of pre- and intra-operative 3D medical data directly at the surgical site. We demonstrate pre-clinical evaluation of a prototype system, and report errors for calibration and target registration. Finally, we demonstrate the usefulness of the proposed inside-out tracking system in achieving "bull's eye" view for C-arm-guided punctures. This AR solution provides an intuitive visualization of the anatomy and can simplify the hand-eye coordination for the orthopedic surgeon.



