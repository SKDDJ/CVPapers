# Arxiv Papers in cs.CV on 2018-03-18
### Efficient and accurate inversion of multiple scattering with deep learning
- **Arxiv ID**: http://arxiv.org/abs/1803.06594v2
- **DOI**: 10.1364/OE.26.014678
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06594v2)
- **Published**: 2018-03-18 03:11:48+00:00
- **Updated**: 2018-04-05 17:02:41+00:00
- **Authors**: Yu Sun, Zhihao Xia, Ulugbek S. Kamilov
- **Comment**: None
- **Journal**: None
- **Summary**: Image reconstruction under multiple light scattering is crucial in a number of applications such as diffraction tomography. The reconstruction problem is often formulated as a nonconvex optimization, where a nonlinear measurement model is used to account for multiple scattering and regularization is used to enforce prior constraints on the object. In this paper, we propose a powerful alternative to this optimization-based view of image reconstruction by designing and training a deep convolutional neural network that can invert multiple scattered measurements to produce a high-quality image of the refractive index. Our results on both simulated and experimental datasets show that the proposed approach is substantially faster and achieves higher imaging quality compared to the state-of-the-art methods based on optimization.



### Facial Landmarks Detection by Self-Iterative Regression based Landmarks-Attention Network
- **Arxiv ID**: http://arxiv.org/abs/1803.06598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06598v1)
- **Published**: 2018-03-18 03:32:35+00:00
- **Updated**: 2018-03-18 03:32:35+00:00
- **Authors**: Tao Hu, Honggang Qi, Jizheng Xu, Qingming Huang
- **Comment**: Accepted in AAAI 2018
- **Journal**: None
- **Summary**: Cascaded Regression (CR) based methods have been proposed to solve facial landmarks detection problem, which learn a series of descent directions by multiple cascaded regressors separately trained in coarse and fine stages. They outperform the traditional gradient descent based methods in both accuracy and running speed. However, cascaded regression is not robust enough because each regressor's training data comes from the output of previous regressor. Moreover, training multiple regressors requires lots of computing resources, especially for deep learning based methods. In this paper, we develop a Self-Iterative Regression (SIR) framework to improve the model efficiency. Only one self-iterative regressor is trained to learn the descent directions for samples from coarse stages to fine stages, and parameters are iteratively updated by the same regressor. Specifically, we proposed Landmarks-Attention Network (LAN) as our regressor, which concurrently learns features around each landmark and obtains the holistic location increment. By doing so, not only the rest of regressors are removed to simplify the training process, but the number of model parameters is significantly decreased. The experiments demonstrate that with only 3.72M model parameters, our proposed method achieves the state-of-the-art performance.



### Trajectory-based Scene Understanding using Dirichlet Process Mixture Model
- **Arxiv ID**: http://arxiv.org/abs/1803.06613v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06613v3)
- **Published**: 2018-03-18 06:40:34+00:00
- **Updated**: 2019-06-16 14:30:11+00:00
- **Authors**: Santhosh Kelathodi Kumaran, Debi Prosad Dogra, Partha Pratim Roy, Bidyut Baran Chaudhuri
- **Comment**: 14 pages, 27 figures
- **Journal**: None
- **Summary**: Appropriate modeling of a surveillance scene is essential for detection of anomalies in road traffic. Learning usual paths can provide valuable insight into road traffic conditions and thus can help in identifying unusual routes taken by commuters/vehicles. If usual traffic paths are learned in a nonparametric way, manual interventions in road marking road can be avoided. In this paper, we propose an unsupervised and nonparametric method to learn frequently used paths from the tracks of moving objects in $\Theta(kn)$ time, where $k$ denotes the number of paths and $n$ represents the number of tracks. In the proposed method, temporal dependencies of the moving objects are considered to make the clustering meaningful using Temporally Incremental Gravity Model (TIGM). In addition, the distance-based scene learning makes it intuitive to estimate the model parameters. Further, we have extended TIGM hierarchically as Dynamically Evolving Model (DEM) to represent notable traffic dynamics of a scene. Experimental validation reveals that the proposed method can learn a scene quickly without prior knowledge about the number of paths ($k$). We have compared the results with various state-of-the-art methods. We have also highlighted the advantages of the proposed method over existing techniques popularly used for designing traffic monitoring applications. It can be used for administrative decision making to control traffic at junctions or crowded places and generate alarm signals, if necessary.



### The Automatic Identification of Butterfly Species
- **Arxiv ID**: http://arxiv.org/abs/1803.06626v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1803.06626v1)
- **Published**: 2018-03-18 08:47:20+00:00
- **Updated**: 2018-03-18 08:47:20+00:00
- **Authors**: Juanying Xie, Qi Hou, Yinghuan Shi, Lv Peng, Liping Jing, Fuzhen Zhuang, Junping Zhang, Xiaoyang Tang, Shengquan Xu
- **Comment**: 9 pages, in Chinese, 8 figures
- **Journal**: None
- **Summary**: The available butterfly data sets comprise a few limited species, and the images in the data sets are always standard patterns without the images of butterflies in their living environment. To overcome the aforementioned limitations in the butterfly data sets, we build a butterfly data set composed of all species of butterflies in China with 4270 standard pattern images of 1176 butterfly species, and 1425 images from living environment of 111 species. We propose to use the deep learning technique Faster-Rcnn to train an automatic butterfly identification system including butterfly position detection and species recognition. We delete those species with only one living environment image from data set, then partition the rest images from living environment into two subsets, one used as test subset, the other as training subset respectively combined with all standard pattern butterfly images or the standard pattern butterfly images with the same species of the images from living environment. In order to construct the training subset for FasterRcnn, nine methods were adopted to amplifying the images in the training subset including the turning of up and down, and left and right, rotation with different angles, adding noises, blurring, and contrast ratio adjusting etc. Three prediction models were trained. The mAP (Mean Average prediction) criterion was used to evaluate the performance of the prediction model. The experimental results demonstrate that our Faster-Rcnn based butterfly automatic identification system performed well, and its worst mAP is up to 60%, and can simultaneously detect the positions of more than one butterflies in one images from living environment and recognize the species of those butterflies as well.



### Cross-modality image synthesis from unpaired data using CycleGAN: Effects of gradient consistency loss and training data size
- **Arxiv ID**: http://arxiv.org/abs/1803.06629v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06629v3)
- **Published**: 2018-03-18 09:13:51+00:00
- **Updated**: 2018-07-31 07:21:17+00:00
- **Authors**: Yuta Hiasa, Yoshito Otake, Masaki Takao, Takumi Matsuoka, Kazuma Takashima, Jerry L. Prince, Nobuhiko Sugano, Yoshinobu Sato
- **Comment**: 10 pages, 7 figures, MICCAI 2018 Workshop on Simulation and Synthesis
  in Medical Imaging
- **Journal**: None
- **Summary**: CT is commonly used in orthopedic procedures. MRI is used along with CT to identify muscle structures and diagnose osteonecrosis due to its superior soft tissue contrast. However, MRI has poor contrast for bone structures. Clearly, it would be helpful if a corresponding CT were available, as bone boundaries are more clearly seen and CT has standardized (i.e., Hounsfield) units. Therefore, we aim at MR-to-CT synthesis. The CycleGAN was successfully applied to unpaired CT and MR images of the head, these images do not have as much variation of intensity pairs as do images in the pelvic region due to the presence of joints and muscles. In this paper, we extended the CycleGAN approach by adding the gradient consistency loss to improve the accuracy at the boundaries. We conducted two experiments. To evaluate image synthesis, we investigated dependency of image synthesis accuracy on 1) the number of training data and 2) the gradient consistency loss. To demonstrate the applicability of our method, we also investigated a segmentation accuracy on synthesized images.



### Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains
- **Arxiv ID**: http://arxiv.org/abs/1803.06641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06641v1)
- **Published**: 2018-03-18 11:11:10+00:00
- **Updated**: 2018-03-18 11:11:10+00:00
- **Authors**: Jiahao Pang, Wenxiu Sun, Chengxi Yang, Jimmy Ren, Ruichao Xiao, Jin Zeng, Liang Lin
- **Comment**: Accepted at CVPR 2018
- **Journal**: None
- **Summary**: Despite the recent success of stereo matching with convolutional neural networks (CNNs), it remains arduous to generalize a pre-trained deep stereo model to a novel domain. A major difficulty is to collect accurate ground-truth disparities for stereo pairs in the target domain. In this work, we propose a self-adaptation approach for CNN training, utilizing both synthetic training data (with ground-truth disparities) and stereo pairs in the new domain (without ground-truths). Our method is driven by two empirical observations. By feeding real stereo pairs of different domains to stereo models pre-trained with synthetic data, we see that: i) a pre-trained model does not generalize well to the new domain, producing artifacts at boundaries and ill-posed regions; however, ii) feeding an up-sampled stereo pair leads to a disparity map with extra details. To avoid i) while exploiting ii), we formulate an iterative optimization problem with graph Laplacian regularization. At each iteration, the CNN adapts itself better to the new domain: we let the CNN learn its own higher-resolution output; at the meanwhile, a graph Laplacian regularization is imposed to discriminatively keep the desired edges while smoothing out the artifacts. We demonstrate the effectiveness of our method in two domains: daily scenes collected by smartphone cameras, and street views captured in a driving car.



### Line Artist: A Multiple Style Sketch to Painting Synthesis Scheme
- **Arxiv ID**: http://arxiv.org/abs/1803.06647v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06647v1)
- **Published**: 2018-03-18 11:54:22+00:00
- **Updated**: 2018-03-18 11:54:22+00:00
- **Authors**: Jinning Li, Siqi Liu, Mengyao Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Drawing a beautiful painting is a dream of many people since childhood. In this paper, we propose a novel scheme, Line Artist, to synthesize artistic style paintings with freehand sketch images, leveraging the power of deep learning and advanced algorithms. Our scheme includes three models. The Sketch Image Extraction (SIE) model is applied to generate the training data. It includes smoothing reality images and pencil sketch extraction. The Detailed Image Synthesis (DIS) model trains a conditional generative adversarial network to generate detailed real-world information. The Adaptively Weighted Artistic Style Transfer (AWAST) model is capable to combine multiple style images with a content with the VGG19 network and PageRank algorithm. The appealing artistic images are then generated by optimization iterations. Experiments are operated on the Kaggle Cats dataset and The Oxford Buildings Dataset. Our synthesis results are proved to be artistic, beautiful and robust.



### DUGMA: Dynamic Uncertainty-Based Gaussian Mixture Alignment
- **Arxiv ID**: http://arxiv.org/abs/1803.07426v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.07426v2)
- **Published**: 2018-03-18 13:06:24+00:00
- **Updated**: 2018-08-02 10:25:53+00:00
- **Authors**: Can Pu, Nanbo Li, Radim Tylecek, Robert B Fisher
- **Comment**: Accepted by 3DV 2018. 9 pages. arXiv admin note: text overlap with
  arXiv:1707.08626
- **Journal**: None
- **Summary**: Registering accurately point clouds from a cheap low-resolution sensor is a challenging task. Existing rigid registration methods failed to use the physical 3D uncertainty distribution of each point from a real sensor in the dynamic alignment process mainly because the uncertainty model for a point is static and invariant and it is hard to describe the change of these physical uncertainty models in the registration process. Additionally, the existing Gaussian mixture alignment architecture cannot be efficiently implement these dynamic changes.   This paper proposes a simple architecture combining error estimation from sample covariances and dual dynamic global probability alignment using the convolution of uncertainty-based Gaussian Mixture Models (GMM) from point clouds. Firstly, we propose an efficient way to describe the change of each 3D uncertainty model, which represents the structure of the point cloud much better. Unlike the invariant GMM (representing a fixed point cloud) in traditional Gaussian mixture alignment, we use two uncertainty-based GMMs that change and interact with each other in each iteration. In order to have a wider basin of convergence than other local algorithms, we design a more robust energy function by convolving efficiently the two GMMs over the whole 3D space.   Tens of thousands of trials have been conducted on hundreds of models from multiple datasets to demonstrate the proposed method's superior performance compared with the current state-of-the-art methods. The new dataset and code is available from https://github.com/Canpu999



### Ratio-Preserving Half-Cylindrical Warps for Natural Image Stitching
- **Arxiv ID**: http://arxiv.org/abs/1803.06655v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06655v1)
- **Published**: 2018-03-18 13:13:30+00:00
- **Updated**: 2018-03-18 13:13:30+00:00
- **Authors**: Yifang Xu, Jing Chen, Tianli Liao
- **Comment**: 3 pages, 5 figures
- **Journal**: None
- **Summary**: A novel warp for natural image stitching is proposed that utilizes the property of cylindrical warp and a horizontal pixel selection strategy. The proposed ratio-preserving half-cylindrical warp is a combination of homography and cylindrical warps which guarantees alignment by homography and possesses less projective distortion by cylindrical warp. Unlike previous approaches applying cylindrical warp before homography, we use partition lines to divide the image into different parts and apply homography in the overlapping region while a composition of homography and cylindrical warps in the non-overlapping region. The pixel selection strategy then samples the points in horizontal and reconstructs the image via interpolation to further reduce horizontal distortion by maintaining the ratio as similarity. With applying half-cylindrical warp and horizontal pixel selection, the projective distortion in vertical and horizontal is mitigated simultaneously. Experiments show that our warp is efficient and produces a more natural-looking stitched result than previous methods.



### Sdf-GAN: Semi-supervised Depth Fusion with Multi-scale Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.06657v3
- **DOI**: 10.3390/rs11050487
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06657v3)
- **Published**: 2018-03-18 13:17:16+00:00
- **Updated**: 2019-05-06 15:48:51+00:00
- **Authors**: Can Pu, Runzi Song, Radim Tylecek, Nanbo Li, Robert B Fisher
- **Comment**: This is our draft and accepted by the journal Remote Sensing. There
  is a little difference between the title on Arxiv and that on Remote Sensing.
  Two small corrections have been made in "Performance on Kitti2015 Dataset" in
  this latest version (which is slightly different from the previous version in
  Remote Sensing)
- **Journal**: Remote Sens. 2019, 11, 487
- **Summary**: Refining raw disparity maps from different algorithms to exploit their complementary advantages is still challenging. Uncertainty estimation and complex disparity relationships among pixels limit the accuracy and robustness of existing methods and there is no standard method for fusion of different kinds of depth data. In this paper, we introduce a new method to fuse disparity maps from different sources, while incorporating supplementary information (intensity, gradient, etc.) into a refiner network to better refine raw disparity inputs. A discriminator network classifies disparities at different receptive fields and scales. Assuming a Markov Random Field for the refined disparity map produces better estimates of the true disparity distribution. Both fully supervised and semi-supervised versions of the algorithm are proposed. The approach includes a more robust loss function to inpaint invalid disparity values and requires much less labeled data to train in the semi-supervised learning mode. The algorithm can be generalized to fuse depths from different kinds of depth sources. Experiments explored different fusion opportunities: stereo-monocular fusion, stereo-ToF fusion and stereo-stereo fusion. The experiments show the superiority of the proposed algorithm compared with the most recent algorithms on public synthetic datasets (Scene Flow, SYNTH3, our synthetic garden dataset) and real datasets (Kitti2015 dataset and Trimbot2020 Garden dataset).



### Land use mapping in the Three Gorges Reservoir Area based on semantic segmentation deep learning method
- **Arxiv ID**: http://arxiv.org/abs/1804.00498v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00498v1)
- **Published**: 2018-03-18 13:30:49+00:00
- **Updated**: 2018-03-18 13:30:49+00:00
- **Authors**: Xin Zhang, Bingfang Wu, Liang Zhu, Fuyou Tian, Miao Zhang, Yuanzeng
- **Comment**: None
- **Journal**: None
- **Summary**: The Three Gorges Dam, a massive cross-century project spans the Yangtze River by the town of Sandouping, located in Yichang, Hubei province, China, was built to provide great power, improve the River shipping, control floods in the upper reaches of the Yangtze River, and increase the dry season flow in the middle and lower reaches of the Yangtze River. Benefits are enormous and comprehensive. However, the social and environmental impacts are also immense and far-reaching to its surrounding areas. Mapping land use /land cover changed (LUCC) is critical for tracking the impacts. Remote sensing has been proved to be an effective way to map and monitor land use change in real time and in large areas such as the Three Gorges Reservoir Area(TGRA) by using pixel based or oriented based classifier in different resolution. In this paper, we first test the state of the art semantic segmentation deep learning classifiers for LUCC mapping with 7 categories in the TGRA area with rapideye 5m resolution data. The topographic information was also added for better accuracy in mountain area. By compared with the pixel-based classifier, the semantic segmentation deep learning method has better accuracy and robustness at 5m resolution level.



### Discriminative Learning of Latent Features for Zero-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.06731v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.06731v1)
- **Published**: 2018-03-18 20:18:48+00:00
- **Updated**: 2018-03-18 20:18:48+00:00
- **Authors**: Yan Li, Junge Zhang, Jianguo Zhang, Kaiqi Huang
- **Comment**: CVPR 2018 (Oral)
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) aims to recognize unseen image categories by learning an embedding space between image and semantic representations. For years, among existing works, it has been the center task to learn the proper mapping matrices aligning the visual and semantic space, whilst the importance to learn discriminative representations for ZSL is ignored. In this work, we retrospect existing methods and demonstrate the necessity to learn discriminative representations for both visual and semantic instances of ZSL. We propose an end-to-end network that is capable of 1) automatically discovering discriminative regions by a zoom network; and 2) learning discriminative semantic representations in an augmented space introduced for both user-defined and latent attributes. Our proposed method is tested extensively on two challenging ZSL datasets, and the experiment results show that the proposed method significantly outperforms state-of-the-art methods.



### Fast Neural Architecture Construction using EnvelopeNets
- **Arxiv ID**: http://arxiv.org/abs/1803.06744v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.06744v3)
- **Published**: 2018-03-18 21:28:03+00:00
- **Updated**: 2018-12-14 00:34:40+00:00
- **Authors**: Purushotham Kamath, Abhishek Singh, Debo Dutta
- **Comment**: A shorter version of this paper appeared in the Workshop on
  MetaLearning 2018 (MetaLearning 2018 at NeurIPS 2018)
- **Journal**: None
- **Summary**: Fast Neural Architecture Construction (NAC) is a method to construct deep network architectures by pruning and expansion of a base network. In recent years, several automated search methods for neural network architectures have been proposed using methods such as evolutionary algorithms and reinforcement learning. These methods use a single scalar objective function (usually accuracy) that is evaluated after a full training and evaluation cycle. In contrast NAC directly compares the utility of different filters using statistics derived from filter featuremaps reach a state where the utility of different filters within a network can be compared and hence can be used to construct networks. The training epochs needed for filters within a network to reach this state is much less than the training epochs needed for the accuracy of a network to stabilize. NAC exploits this finding to construct convolutional neural nets (CNNs) with close to state of the art accuracy, in < 1 GPU day, faster than most of the current neural architecture search methods. The constructed networks show close to state of the art performance on the image classification problem on well known datasets (CIFAR-10, ImageNet) and consistently show better performance than hand constructed and randomly generated networks of the same depth, operators and approximately the same number of parameters.



