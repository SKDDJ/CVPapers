# Arxiv Papers in cs.CV on 2018-03-25
### Scene Graph Parsing as Dependency Parsing
- **Arxiv ID**: http://arxiv.org/abs/1803.09189v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.09189v1)
- **Published**: 2018-03-25 01:53:29+00:00
- **Updated**: 2018-03-25 01:53:29+00:00
- **Authors**: Yu-Siang Wang, Chenxi Liu, Xiaohui Zeng, Alan Yuille
- **Comment**: To appear in NAACL 2018 as oral. Code is available at
  https://github.com/Yusics/bist-parser/tree/sgparser
- **Journal**: None
- **Summary**: In this paper, we study the problem of parsing structured knowledge graphs from textual descriptions. In particular, we consider the scene graph representation that considers objects together with their attributes and relations: this representation has been proved useful across a variety of vision and language applications. We begin by introducing an alternative but equivalent edge-centric view of scene graphs that connect to dependency parses. Together with a careful redesign of label and action space, we combine the two-stage pipeline used in prior work (generic dependency parsing followed by simple post-processing) into one, enabling end-to-end training. The scene graphs generated by our learned neural dependency parser achieve an F-score similarity of 49.67% to ground truth graphs on our evaluation set, surpassing best previous approaches by 5%. We further demonstrate the effectiveness of our learned parser on image retrieval applications.



### Learning Type-Aware Embeddings for Fashion Compatibility
- **Arxiv ID**: http://arxiv.org/abs/1803.09196v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09196v2)
- **Published**: 2018-03-25 02:50:58+00:00
- **Updated**: 2018-07-27 20:27:54+00:00
- **Authors**: Mariya I. Vasileva, Bryan A. Plummer, Krishna Dusad, Shreya Rajpal, Ranjitha Kumar, David Forsyth
- **Comment**: Accepted at ECCV 2018
- **Journal**: None
- **Summary**: Outfits in online fashion data are composed of items of many different types (e.g. top, bottom, shoes) that share some stylistic relationship with one another. A representation for building outfits requires a method that can learn both notions of similarity (for example, when two tops are interchangeable) and compatibility (items of possibly different type that can go together in an outfit). This paper presents an approach to learning an image embedding that respects item type, and jointly learns notions of item similarity and compatibility in an end-to-end model. To evaluate the learned representation, we crawled 68,306 outfits created by users on the Polyvore website. Our approach obtains 3-5% improvement over the state-of-the-art on outfit compatibility prediction and fill-in-the-blank tasks using our dataset, as well as an established smaller dataset, while supporting a variety of useful queries.



### Unsupervised Depth Estimation, 3D Face Rotation and Replacement
- **Arxiv ID**: http://arxiv.org/abs/1803.09202v5
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.09202v5)
- **Published**: 2018-03-25 05:07:11+00:00
- **Updated**: 2018-12-24 01:51:13+00:00
- **Authors**: Joel Ruben Antony Moniz, Christopher Beckham, Simon Rajotte, Sina Honari, Christopher Pal
- **Comment**: Depth Estimation, Face Rotation, Face Swap, 32nd Conference on Neural
  Information Processing Systems (NIPS 2018)
- **Journal**: None
- **Summary**: We present an unsupervised approach for learning to estimate three dimensional (3D) facial structure from a single image while also predicting 3D viewpoint transformations that match a desired pose and facial geometry. We achieve this by inferring the depth of facial keypoints of an input image in an unsupervised manner, without using any form of ground-truth depth information. We show how it is possible to use these depths as intermediate computations within a new backpropable loss to predict the parameters of a 3D affine transformation matrix that maps inferred 3D keypoints of an input face to the corresponding 2D keypoints on a desired target facial geometry or pose. Our resulting approach, called DepthNets, can therefore be used to infer plausible 3D transformations from one face pose to another, allowing faces to be frontalized, transformed into 3D models or even warped to another pose and facial geometry. Lastly, we identify certain shortcomings with our formulation, and explore adversarial image translation techniques as a post-processing step to re-synthesize complete head shots for faces re-targeted to different poses or identities.



### Unsupervised Domain Adaptation: A Multi-task Learning-based Method
- **Arxiv ID**: http://arxiv.org/abs/1803.09208v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09208v1)
- **Published**: 2018-03-25 06:20:00+00:00
- **Updated**: 2018-03-25 06:20:00+00:00
- **Authors**: Jing Zhang, Wanqing Li, Philip Ogunbona
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel multi-task learning-based method for unsupervised domain adaptation. Specifically, the source and target domain classifiers are jointly learned by considering the geometry of target domain and the divergence between the source and target domains based on the concept of multi-task learning. Two novel algorithms are proposed upon the method using Regularized Least Squares and Support Vector Machines respectively. Experiments on both synthetic and real world cross domain recognition tasks have shown that the proposed methods outperform several state-of-the-art domain adaptation methods.



### Importance Weighted Adversarial Nets for Partial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1803.09210v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09210v2)
- **Published**: 2018-03-25 06:47:22+00:00
- **Updated**: 2018-03-28 21:04:48+00:00
- **Authors**: Jing Zhang, Zewei Ding, Wanqing Li, Philip Ogunbona
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an importance weighted adversarial nets-based method for unsupervised domain adaptation, specific for partial domain adaptation where the target domain has less number of classes compared to the source domain. Previous domain adaptation methods generally assume the identical label spaces, such that reducing the distribution divergence leads to feasible knowledge transfer. However, such an assumption is no longer valid in a more realistic scenario that requires adaptation from a larger and more diverse source domain to a smaller target domain with less number of classes. This paper extends the adversarial nets-based domain adaptation and proposes a novel adversarial nets-based partial domain adaptation method to identify the source samples that are potentially from the outlier classes and, at the same time, reduce the shift of shared classes between domains.



### Image Recognition Using Scale Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.09218v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09218v1)
- **Published**: 2018-03-25 09:16:55+00:00
- **Updated**: 2018-03-25 09:16:55+00:00
- **Authors**: Dong-Qing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Network(CNN) has been widely used for image recognition with great success. However, there are a number of limitations of the current CNN based image recognition paradigm. First, the receptive field of CNN is generally fixed, which limits its recognition capacity when the input image is very large. Second, it lacks the computational scalability for dealing with images with different sizes. Third, it is quite different from human visual system for image recognition, which involves both feadforward and recurrent proprocessing. This paper proposes a different paradigm of image recognition, which can take advantages of variable scales of the input images, has more computational scalabilities, and is more similar to image recognition by human visual system. It is based on recurrent neural network (RNN) defined on image scale with an embeded base CNN, which is named Scale Recurrent Neural Network(SRNN). This RNN based approach makes it easier to deal with images with variable sizes, and allows us to borrow existing RNN techniques, such as LSTM and GRU, to further enhance the recognition accuracy. Our experiments show that the recognition accuracy of a base CNN can be significantly boosted using the proposed SRNN models. It also significantly outperforms the scale ensemble method, which integrate the results of performing CNN to the input image at different scales, although the computational overhead of using SRNN is negligible.



### Detecting Heads using Feature Refine Net and Cascaded Multi-Scale Architecture
- **Arxiv ID**: http://arxiv.org/abs/1803.09256v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09256v4)
- **Published**: 2018-03-25 14:02:51+00:00
- **Updated**: 2018-11-24 01:53:48+00:00
- **Authors**: Dezhi Peng, Zikai Sun, Zirong Chen, Zirui Cai, Lele Xie, Lianwen Jin
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a method that can accurately detect heads especially small heads under the indoor scene. To achieve this, we propose a novel method, Feature Refine Net (FRN), and a cascaded multi-scale architecture. FRN exploits the multi-scale hierarchical features created by deep convolutional neural networks. The proposed channel weighting method enables FRN to make use of features alternatively and effectively. To improve the performance of small head detection, we propose a cascaded multi-scale architecture which has two detectors. One called global detector is responsible for detecting large objects and acquiring the global distribution information. The other called local detector is designed for small objects detection and makes use of the information provided by global detector. Due to the lack of head detection datasets, we have collected and labeled a new large dataset named SCUT-HEAD which includes 4405 images with 111251 heads annotated. Experiments show that our method has achieved state-of-the-art performance on SCUT-HEAD.



### P2P-NET: Bidirectional Point Displacement Net for Shape Transform
- **Arxiv ID**: http://arxiv.org/abs/1803.09263v4
- **DOI**: 10.1145/3197517.3201288
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.09263v4)
- **Published**: 2018-03-25 14:30:51+00:00
- **Updated**: 2018-05-15 08:14:30+00:00
- **Authors**: Kangxue Yin, Hui Huang, Daniel Cohen-Or, Hao Zhang
- **Comment**: siggraph revision is done. 13 pages
- **Journal**: ACM Transactions on Graphics(Proc. of SIGGRAPH), 37(4),
  152:1-152:13, 2018
- **Summary**: We introduce P2P-NET, a general-purpose deep neural network which learns geometric transformations between point-based shape representations from two domains, e.g., meso-skeletons and surfaces, partial and complete scans, etc. The architecture of the P2P-NET is that of a bi-directional point displacement network, which transforms a source point set to a target point set with the same cardinality, and vice versa, by applying point-wise displacement vectors learned from data. P2P-NET is trained on paired shapes from the source and target domains, but without relying on point-to-point correspondences between the source and target point sets. The training loss combines two uni-directional geometric losses, each enforcing a shape-wise similarity between the predicted and the target point sets, and a cross-regularization term to encourage consistency between displacement vectors going in opposite directions. We develop and present several different applications enabled by our general-purpose bidirectional P2P-NET to highlight the effectiveness, versatility, and potential of our network in solving a variety of point-based shape transformation problems.



### Deep Depth Completion of a Single RGB-D Image
- **Arxiv ID**: http://arxiv.org/abs/1803.09326v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09326v2)
- **Published**: 2018-03-25 20:07:10+00:00
- **Updated**: 2018-05-02 03:40:10+00:00
- **Authors**: Yinda Zhang, Thomas Funkhouser
- **Comment**: Accepted by CVPR2018 (Spotlight). Project webpage:
  http://deepcompletion.cs.princeton.edu/ This version includes supplementary
  materials which provide more implementation details, quantitative evaluation,
  and qualitative results. Due to file size limit, please check project website
  for high-res paper
- **Journal**: None
- **Summary**: The goal of our work is to complete the depth channel of an RGB-D image. Commodity-grade depth cameras often fail to sense depth for shiny, bright, transparent, and distant surfaces. To address this problem, we train a deep network that takes an RGB image as input and predicts dense surface normals and occlusion boundaries. Those predictions are then combined with raw depth observations provided by the RGB-D camera to solve for depths for all pixels, including those missing in the original observation. This method was chosen over others (e.g., inpainting depths directly) as the result of extensive experiments with a new depth completion benchmark dataset, where holes are filled in training data through the rendering of surface reconstructions created from multiview RGB-D scans. Experiments with different network inputs, depth representations, loss functions, optimization methods, inpainting methods, and deep depth estimation networks show that our proposed approach provides better depth completions than these alternatives.



### StarMap for Category-Agnostic Keypoint and Viewpoint Estimation
- **Arxiv ID**: http://arxiv.org/abs/1803.09331v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09331v2)
- **Published**: 2018-03-25 20:28:53+00:00
- **Updated**: 2018-07-26 04:31:28+00:00
- **Authors**: Xingyi Zhou, Arjun Karpur, Linjie Luo, Qixing Huang
- **Comment**: ECCV 2018. Supplementary material with more qualitative results and
  higher resolution is available on the code page
- **Journal**: None
- **Summary**: Semantic keypoints provide concise abstractions for a variety of visual understanding tasks. Existing methods define semantic keypoints separately for each category with a fixed number of semantic labels in fixed indices. As a result, this keypoint representation is in-feasible when objects have a varying number of parts, e.g. chairs with varying number of legs. We propose a category-agnostic keypoint representation, which combines a multi-peak heatmap (StarMap) for all the keypoints and their corresponding features as 3D locations in the canonical viewpoint (CanViewFeature) defined for each instance. Our intuition is that the 3D locations of the keypoints in canonical object views contain rich semantic and compositional information. Using our flexible representation, we demonstrate competitive performance in keypoint detection and localization compared to category-specific state-of-the-art methods. Moreover, we show that when augmented with an additional depth channel (DepthMap) to lift the 2D keypoints to 3D, our representation can achieve state-of-the-art results in viewpoint estimation. Finally, we show that our category-agnostic keypoint representation can be generalized to novel categories.



### DeepVesselNet: Vessel Segmentation, Centerline Prediction, and Bifurcation Detection in 3-D Angiographic Volumes
- **Arxiv ID**: http://arxiv.org/abs/1803.09340v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09340v3)
- **Published**: 2018-03-25 21:09:36+00:00
- **Updated**: 2019-08-13 12:50:47+00:00
- **Authors**: Giles Tetteh, Velizar Efremov, Nils D. Forkert, Matthias Schneider, Jan Kirschke, Bruno Weber, Claus Zimmer, Marie Piraud, Bjoern H. Menze
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: We present DeepVesselNet, an architecture tailored to the challenges faced when extracting vessel networks or trees and corresponding features in 3-D angiographic volumes using deep learning. We discuss the problems of low execution speed and high memory requirements associated with full 3-D convolutional networks, high-class imbalance arising from the low percentage of vessel voxels, and unavailability of accurately annotated training data - and offer solutions as the building blocks of DeepVesselNet.   First, we formulate 2-D orthogonal cross-hair filters which make use of 3-D context information at a reduced computational burden. Second, we introduce a class balancing cross-entropy loss function with false positive rate correction to handle the high-class imbalance and high false positive rate problems associated with existing loss functions. Finally, we generate synthetic dataset using a computational angiogenesis model capable of generating vascular trees under physiological constraints on local network structure and topology and use these data for transfer learning.   DeepVesselNet is optimized for segmenting and analyzing vessels, and we test the performance on a range of angiographic volumes including clinical MRA data of the human brain, as well as X-ray tomographic microscopy scans of the rat brain. Our experiments show that, by replacing 3-D filters with cross-hair filters in our network, we achieve over 23% improvement in speed, lower memory footprint, lower network complexity which prevents overfitting and comparable accuracy (with a Cox-Wilcoxon paired sample significance test p-value of 0.07 when compared to full 3-D filters). Our class balancing metric is crucial for training the network and transfer learning with synthetic data is an efficient, robust, and very generalizable approach leading to a network that excels in a variety of angiography segmentation tasks.



### Learning-Based Quality Control for Cardiac MR Images
- **Arxiv ID**: http://arxiv.org/abs/1803.09354v2
- **DOI**: 10.1109/TMI.2018.2878509
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09354v2)
- **Published**: 2018-03-25 21:49:55+00:00
- **Updated**: 2018-09-15 09:36:00+00:00
- **Authors**: Giacomo Tarroni, Ozan Oktay, Wenjia Bai, Andreas Schuh, Hideaki Suzuki, Jonathan Passerat-Palmbach, Antonio de Marvao, Declan P. O'Regan, Stuart Cook, Ben Glocker, Paul M. Matthews, Daniel Rueckert
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging Nov 2018
- **Summary**: The effectiveness of a cardiovascular magnetic resonance (CMR) scan depends on the ability of the operator to correctly tune the acquisition parameters to the subject being scanned and on the potential occurrence of imaging artefacts such as cardiac and respiratory motion. In the clinical practice, a quality control step is performed by visual assessment of the acquired images: however, this procedure is strongly operator-dependent, cumbersome and sometimes incompatible with the time constraints in clinical settings and large-scale studies. We propose a fast, fully-automated, learning-based quality control pipeline for CMR images, specifically for short-axis image stacks. Our pipeline performs three important quality checks: 1) heart coverage estimation, 2) inter-slice motion detection, 3) image contrast estimation in the cardiac region. The pipeline uses a hybrid decision forest method - integrating both regression and structured classification models - to extract landmarks as well as probabilistic segmentation maps from both long- and short-axis images as a basis to perform the quality checks. The technique was tested on up to 3000 cases from the UK Biobank as well as on 100 cases from the UK Digital Heart Project, and validated against manual annotations and visual inspections performed by expert interpreters. The results show the capability of the proposed pipeline to correctly detect incomplete or corrupted scans (e.g. on UK Biobank, sensitivity and specificity respectively 88% and 99% for heart coverage estimation, 85% and 95% for motion detection), allowing their exclusion from the analysed dataset or the triggering of a new acquisition.



### A Face Recognition Signature Combining Patch-based Features with Soft Facial Attributes
- **Arxiv ID**: http://arxiv.org/abs/1803.09359v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09359v1)
- **Published**: 2018-03-25 22:32:18+00:00
- **Updated**: 2018-03-25 22:32:18+00:00
- **Authors**: Lingfeng Zhang, Pengfei Dou, Ioannis A. Kakadiaris
- **Comment**: None
- **Journal**: None
- **Summary**: This paper focuses on improving face recognition performance with a new signature combining implicit facial features with explicit soft facial attributes. This signature has two components: the existing patch-based features and the soft facial attributes. A deep convolutional neural network adapted from state-of-the-art networks is used to learn the soft facial attributes. Then, a signature matcher is introduced that merges the contributions of both patch-based features and the facial attributes. In this matcher, the matching scores computed from patch-based features and the facial attributes are combined to obtain a final matching score. The matcher is also extended so that different weights are assigned to different facial attributes. The proposed signature and matcher have been evaluated with the UR2D system on the UHDB31 and IJB-A datasets. The experimental results indicate that the proposed signature achieve better performance than using only patch-based features. The Rank-1 accuracy is improved significantly by 4% and 0.37% on the two datasets when compared with the UR2D system.



