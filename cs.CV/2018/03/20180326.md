# Arxiv Papers in cs.CV on 2018-03-26
### Generalized Hadamard-Product Fusion Operators for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1803.09374v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.09374v2)
- **Published**: 2018-03-26 00:30:34+00:00
- **Updated**: 2018-04-06 15:18:26+00:00
- **Authors**: Brendan Duke, Graham W. Taylor
- **Comment**: 8 pages, 3 figures. To appear in CRV, 2018, 15th Canadian Conference
  on Computer and Robot Vision
- **Journal**: None
- **Summary**: We propose a generalized class of multimodal fusion operators for the task of visual question answering (VQA). We identify generalizations of existing multimodal fusion operators based on the Hadamard product, and show that specific non-trivial instantiations of this generalized fusion operator exhibit superior performance in terms of OpenEnded accuracy on the VQA task. In particular, we introduce Nonlinearity Ensembling, Feature Gating, and post-fusion neural network layers as fusion operator components, culminating in an absolute percentage point improvement of $1.1\%$ on the VQA 2.0 test-dev set over baseline fusion operators, which use the same features as input. We use our findings as evidence that our generalized class of fusion operators could lead to the discovery of even superior task-specific operators when used as a search space in an architecture search over fusion operators.



### Correcting differences in multi-site neuroimaging data using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1803.09375v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09375v2)
- **Published**: 2018-03-26 00:49:50+00:00
- **Updated**: 2018-04-12 06:01:50+00:00
- **Authors**: Harrison Nguyen, Richard W. Morris, Anthony W. Harris, Mayuresh S. Korgoankar, Fabio Ramos
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic Resonance Imaging (MRI) of the brain has been used to investigate a wide range of neurological disorders, but data acquisition can be expensive, time-consuming, and inconvenient. Multi-site studies present a valuable opportunity to advance research by pooling data in order to increase sensitivity and statistical power. However images derived from MRI are susceptible to both obvious and non-obvious differences between sites which can introduce bias and subject variance, and so reduce statistical power. To rectify these differences, we propose a data driven approach using a deep learning architecture known as generative adversarial networks (GANs). GANs learn to estimate two distributions, and can then be used to transform examples from one distribution into the other distribution. Here we transform T1-weighted brain images collected from two different sites into MR images from the same site. We evaluate whether our model can reduce site-specific differences without loss of information related to gender (male, female) or clinical diagnosis (schizophrenia, bipolar disorder, healthy). When trained appropriately, our model is able to normalise imaging sets to a common scanner set with less information loss compared to current approaches. An important advantage is our method can be treated as a black box that does not require any knowledge of the sources of bias but only needs at least two distinct imaging sets.



### A Systematic Comparison of Deep Learning Architectures in an Autonomous Vehicle
- **Arxiv ID**: http://arxiv.org/abs/1803.09386v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.09386v2)
- **Published**: 2018-03-26 01:58:07+00:00
- **Updated**: 2018-10-13 00:04:29+00:00
- **Authors**: Michael Teti, William Edward Hahn, Shawn Martin, Christopher Teti, Elan Barenholtz
- **Comment**: 16 pages, 14 figures, 2 tables
- **Journal**: None
- **Summary**: Self-driving technology is advancing rapidly --- albeit with significant challenges and limitations. This progress is largely due to recent developments in deep learning algorithms. To date, however, there has been no systematic comparison of how different deep learning architectures perform at such tasks, or an attempt to determine a correlation between classification performance and performance in an actual vehicle, a potentially critical factor in developing self-driving systems. Here, we introduce the first controlled comparison of multiple deep-learning architectures in an end-to-end autonomous driving task across multiple testing conditions. We compared performance, under identical driving conditions, across seven architectures including a fully-connected network, a simple 2 layer CNN, AlexNet, VGG-16, Inception-V3, ResNet, and an LSTM by assessing the number of laps each model was able to successfully complete without crashing while traversing an indoor racetrack. We compared performance across models when the conditions exactly matched those in training as well as when the local environment and track were configured differently and objects that were not included in the training dataset were placed on the track in various positions. In addition, we considered performance using several different data types for training and testing including single grayscale and color frames, and multiple grayscale frames stacked together in sequence. With the exception of a fully-connected network, all models performed reasonably well (around or above 80\%) and most very well (~95\%) on at least one input type but with considerable variation across models and inputs. Overall, AlexNet, operating on single color frames as input, achieved the best level of performance (100\% success rate in phase one and 55\% in phase two) while VGG-16 performed well most consistently across image types.



### Precision Sugarcane Monitoring Using SVM Classifier
- **Arxiv ID**: http://arxiv.org/abs/1803.09413v1
- **DOI**: 10.1016/j.procs.2017.11.450
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09413v1)
- **Published**: 2018-03-26 05:05:25+00:00
- **Updated**: 2018-03-26 05:05:25+00:00
- **Authors**: Sachin Kumar, Sumita Mishra, Pooja Khanna, Pragya
- **Comment**: This is a pre-print of an article published in [Procedia Computer
  Science 2017]
- **Journal**: Procedia Computer Science,2017,vol.122,pp. 881-887
- **Summary**: India is agriculture based economy and sugarcane is one of the major crops produced in northern India. Productivity of sugarcane decreases due to inappropriate soil conditions and infections caused by various types of diseases , timely and accurate disease diagnosis, plays an important role towards optimizing crop yield. This paper presents a system model for monitoring of sugarcane crop, the proposed model continuously monitor parameters (temperature, humidity and moisture) responsible for healthy growth of the crop in addition KNN clustering along with SVM classifier is utilized for infection identification if any through images obtained at regular intervals. The data has been transmitted wirelessly from the site to the control unit. Model achieves an accuracy of 96% on a sample of 200 images, the model was tested at Lolai, near Malhaur, Gomti Nagar Extension.



### Multi-scale Processing of Noisy Images using Edge Preservation Losses
- **Arxiv ID**: http://arxiv.org/abs/1803.09420v5
- **DOI**: 10.1109/ICPR48806.2021.9413325
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09420v5)
- **Published**: 2018-03-26 05:39:31+00:00
- **Updated**: 2019-03-21 14:40:45+00:00
- **Authors**: Nati Ofir, Yosi Keller
- **Comment**: None
- **Journal**: None
- **Summary**: Noisy images processing is a fundamental task of computer vision. The first example is the detection of faint edges in noisy images, a challenging problem studied in the last decades. A recent study introduced a fast method to detect faint edges in the highest accuracy among all the existing approaches. Their complexity is nearly linear in the image's pixels and their runtime is seconds for a noisy image. Their approach utilizes a multi-scale binary partitioning of the image. By utilizing the multi-scale U-net architecture, we show in this paper that their method can be dramatically improved in both aspects of run time and accuracy. By training the network on a dataset of binary images, we developed an approach for faint edge detection that works in a linear complexity. Our runtime of a noisy image is milliseconds on a GPU. Even though our method is orders of magnitude faster, we still achieve higher accuracy of detection under many challenging scenarios. In addition, we show that our approach to performing multi-scale preprocessing of noisy images using U-net improves the ability to perform other vision tasks under the presence of noise. We prove it on the problems of noisy objects classification and classical image denoising. We show that multi-scale denoising can be carried out by a novel edge preservation loss. As our experiments show, we achieve high-quality results in the three aspects of faint edge detection, noisy image classification and natural image denoising.



### Cascaded multi-scale and multi-dimension convolutional neural network for stereo matching
- **Arxiv ID**: http://arxiv.org/abs/1803.09437v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09437v2)
- **Published**: 2018-03-26 06:51:46+00:00
- **Updated**: 2018-04-17 06:35:42+00:00
- **Authors**: Haihua Lu, Hai Xu, Li Zhang, Yong Zhao
- **Comment**: 13 pages, 2 figures, 8 tables
- **Journal**: None
- **Summary**: Convolutional neural networks(CNN) have been shown to perform better than the conventional stereo algorithms for stereo estimation. Numerous efforts focus on the pixel-wise matching cost computation, which is the important building block for many start-of-the-art algorithms. However, those architectures are limited to small and single scale receptive fields and use traditional methods for cost aggregation or even ignore cost aggregation. Differently we take them both into consideration. Firstly, we propose a new multi-scale matching cost computation sub-network, in which two different sizes of receptive fields are implemented parallelly. In this way, the network can make the best use of both variants and balance the trade-off between the increase of receptive field and the loss of detail. Furthermore, we show that our multi-dimension aggregation sub-network which containing 2D convolution and 3D convolution operations can provide rich context and semantic information for estimating an accurate initial disparity. Finally, experiments on challenging stereo benchmark KITTI demonstrate that the proposed method can achieve competitive results even without any additional post-processing.



### REST: Real-to-Synthetic Transform for Illumination Invariant Camera Localization
- **Arxiv ID**: http://arxiv.org/abs/1803.09448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09448v1)
- **Published**: 2018-03-26 07:36:11+00:00
- **Updated**: 2018-03-26 07:36:11+00:00
- **Authors**: Sota Shoman, Tomohiro Mashita, Alexander Plopski, Photchara Ratsamee, Yuki Uranishi, Haruo Takemura
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate camera localization is an essential part of tracking systems. However, localization results are greatly affected by illumination. Including data collected under various lighting conditions can improve the robustness of the localization algorithm to lighting variation. However, this is very tedious and time consuming. By using synthesized images it is possible to easily accumulate a large variety of views under varying illumination and weather conditions. Despite continuously improving processing power and rendering algorithms, synthesized images do not perfectly match real images of the same scene, i.e. there exists a gap between real and synthesized images that also affects the accuracy of camera localization. To reduce the impact of this gap, we introduce "REal-to-Synthetic Transform (REST)." REST is an autoencoder-like network that converts real features to their synthetic counterpart. The converted features can then be matched against the accumulated database for robust camera localization. In our experiments REST improved feature matching accuracy under variable lighting conditions by approximately 30%. Moreover, our system outperforms state of the art CNN-based camera localization methods trained with synthetic images. We believe our method could be used to initialize local tracking and to simplify data accumulation for lighting robust localization.



### CNN in MRF: Video Object Segmentation via Inference in A CNN-Based Higher-Order Spatio-Temporal MRF
- **Arxiv ID**: http://arxiv.org/abs/1803.09453v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09453v1)
- **Published**: 2018-03-26 07:56:43+00:00
- **Updated**: 2018-03-26 07:56:43+00:00
- **Authors**: Linchao Bao, Baoyuan Wu, Wei Liu
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: This paper addresses the problem of video object segmentation, where the initial object mask is given in the first frame of an input video. We propose a novel spatio-temporal Markov Random Field (MRF) model defined over pixels to handle this problem. Unlike conventional MRF models, the spatial dependencies among pixels in our model are encoded by a Convolutional Neural Network (CNN). Specifically, for a given object, the probability of a labeling to a set of spatially neighboring pixels can be predicted by a CNN trained for this specific object. As a result, higher-order, richer dependencies among pixels in the set can be implicitly modeled by the CNN. With temporal dependencies established by optical flow, the resulting MRF model combines both spatial and temporal cues for tackling video object segmentation. However, performing inference in the MRF model is very difficult due to the very high-order dependencies. To this end, we propose a novel CNN-embedded algorithm to perform approximate inference in the MRF. This algorithm proceeds by alternating between a temporal fusion step and a feed-forward CNN step. When initialized with an appearance-based one-shot segmentation CNN, our model outperforms the winning entries of the DAVIS 2017 Challenge, without resorting to model ensembling or any dedicated detectors.



### Fast and Accurate Single Image Super-Resolution via Information Distillation Network
- **Arxiv ID**: http://arxiv.org/abs/1803.09454v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09454v1)
- **Published**: 2018-03-26 07:56:54+00:00
- **Updated**: 2018-03-26 07:56:54+00:00
- **Authors**: Zheng Hui, Xiumei Wang, Xinbo Gao
- **Comment**: To appear in CVPR2018
- **Journal**: None
- **Summary**: Recently, deep convolutional neural networks (CNNs) have been demonstrated remarkable progress on single image super-resolution. However, as the depth and width of the networks increase, CNN-based super-resolution methods have been faced with the challenges of computational complexity and memory consumption in practice. In order to solve the above questions, we propose a deep but compact convolutional network to directly reconstruct the high resolution image from the original low resolution image. In general, the proposed model consists of three parts, which are feature extraction block, stacked information distillation blocks and reconstruction block respectively. By combining an enhancement unit with a compression unit into a distillation block, the local long and short-path features can be effectively extracted. Specifically, the proposed enhancement unit mixes together two different types of features and the compression unit distills more useful information for the sequential blocks. In addition, the proposed network has the advantage of fast execution due to the comparatively few numbers of filters per layer and the use of group convolution. Experimental results demonstrate that the proposed method is superior to the state-of-the-art methods, especially in terms of time performance.



### Regularizing Deep Hashing Networks Using GAN Generated Fake Images
- **Arxiv ID**: http://arxiv.org/abs/1803.09466v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09466v2)
- **Published**: 2018-03-26 08:30:18+00:00
- **Updated**: 2018-09-02 16:24:44+00:00
- **Authors**: Libing Geng, Yan Pan, Jikai Chen, Hanjiang Lai
- **Comment**: I need to modify the experiments
- **Journal**: None
- **Summary**: Recently, deep-networks-based hashing (deep hashing) has become a leading approach for large-scale image retrieval. It aims to learn a compact bitwise representation for images via deep networks, so that similar images are mapped to nearby hash codes. Since a deep network model usually has a large number of parameters, it may probably be too complicated for the training data we have, leading to model over-fitting. To address this issue, in this paper, we propose a simple two-stage pipeline to learn deep hashing models, by regularizing the deep hashing networks using fake images. The first stage is to generate fake images from the original training set without extra data, via a generative adversarial network (GAN). In the second stage, we propose a deep architec- ture to learn hash functions, in which we use a maximum-entropy based loss to incorporate the newly created fake images by the GAN. We show that this loss acts as a strong regularizer of the deep architecture, by penalizing low-entropy output hash codes. This loss can also be interpreted as a model ensemble by simultaneously training many network models with massive weight sharing but over different training sets. Empirical evaluation results on several benchmark datasets show that the proposed method has superior performance gains over state-of-the-art hashing methods.



### Real Time Surveillance for Low Resolution and Limited-Data Scenarios: An Image Set Classification Approach
- **Arxiv ID**: http://arxiv.org/abs/1803.09470v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09470v2)
- **Published**: 2018-03-26 08:52:26+00:00
- **Updated**: 2019-03-03 18:09:17+00:00
- **Authors**: Uzair Nadeem, Syed Afaq Ali Shah, Mohammed Bennamoun, Roberto Togneri, Ferdous Sohel
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel image set classification technique based on the concept of linear regression. Unlike most other approaches, the proposed technique does not involve any training or feature extraction. The gallery image sets are represented as subspaces in a high dimensional space. Class specific gallery subspaces are used to estimate regression models for each image of the test image set. Images of the test set are then projected on the gallery subspaces. Residuals, calculated using the Euclidean distance between the original and the projected test images, are used as the distance metric. Three different strategies are devised to decide on the final class of the test image set. We performed extensive evaluations of the proposed technique under the challenges of low resolution, noise and less gallery data for the tasks of surveillance, video-based face recognition and object recognition. Experiments show that the proposed technique achieves a better classification accuracy and a faster execution time compared to existing techniques especially under the challenging conditions of low resolution and small gallery and test data.



### Semantic See-Through Rendering on Light Fields
- **Arxiv ID**: http://arxiv.org/abs/1803.09474v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09474v1)
- **Published**: 2018-03-26 09:06:12+00:00
- **Updated**: 2018-03-26 09:06:12+00:00
- **Authors**: Huangjie Yu, Guli Zhang, Yuanxi Ma, Yingliang Zhang, Jingyi Yu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel semantic light field (LF) refocusing technique that can achieve unprecedented see-through quality. Different from prior art, our semantic see-through (SST) differentiates rays in their semantic meaning and depth. Specifically, we combine deep learning and stereo matching to provide each ray a semantic label. We then design tailored weighting schemes for blending the rays. Although simple, our solution can effectively remove foreground residues when focusing on the background. At the same time, SST maintains smooth transitions in varying focal depths. Comprehensive experiments on synthetic and new real indoor and outdoor datasets demonstrate the effectiveness and usefulness of our technique.



### Unsupervised Learning and Segmentation of Complex Activities from Video
- **Arxiv ID**: http://arxiv.org/abs/1803.09490v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09490v1)
- **Published**: 2018-03-26 09:47:26+00:00
- **Updated**: 2018-03-26 09:47:26+00:00
- **Authors**: Fadime Sener, Angela Yao
- **Comment**: CVPR 2018 Accepted Manuscript
- **Journal**: None
- **Summary**: This paper presents a new method for unsupervised segmentation of complex activities from video into multiple steps, or sub-activities, without any textual input. We propose an iterative discriminative-generative approach which alternates between discriminatively learning the appearance of sub-activities from the videos' visual features to sub-activity labels and generatively modelling the temporal structure of sub-activities using a Generalized Mallows Model. In addition, we introduce a model for background to account for frames unrelated to the actual activities. Our approach is validated on the challenging Breakfast Actions and Inria Instructional Videos datasets and outperforms both unsupervised and weakly-supervised state of the art.



### Latency and Throughput Characterization of Convolutional Neural Networks for Mobile Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1803.09492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09492v1)
- **Published**: 2018-03-26 09:49:03+00:00
- **Updated**: 2018-03-26 09:49:03+00:00
- **Authors**: Jussi Hanhirova, Teemu Kämäräinen, Sipi Seppälä, Matti Siekkinen, Vesa Hirvisalo, Antti Ylä-Jääski
- **Comment**: 13 pages, 18 figures
- **Journal**: None
- **Summary**: We study performance characteristics of convolutional neural networks (CNN) for mobile computer vision systems. CNNs have proven to be a powerful and efficient approach to implement such systems. However, the system performance depends largely on the utilization of hardware accelerators, which are able to speed up the execution of the underlying mathematical operations tremendously through massive parallelism. Our contribution is performance characterization of multiple CNN-based models for object recognition and detection with several different hardware platforms and software frameworks, using both local (on-device) and remote (network-side server) computation. The measurements are conducted using real workloads and real processing platforms. On the platform side, we concentrate especially on TensorFlow and TensorRT. Our measurements include embedded processors found on mobile devices and high-performance processors that can be used on the network side of mobile systems. We show that there exists significant latency--throughput trade-offs but the behavior is very complex. We demonstrate and discuss several factors that affect the performance and yield this complex behavior.



### Long-term Tracking in the Wild: A Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1803.09502v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09502v3)
- **Published**: 2018-03-26 10:43:45+00:00
- **Updated**: 2018-08-10 16:03:38+00:00
- **Authors**: Jack Valmadre, Luca Bertinetto, João F. Henriques, Ran Tao, Andrea Vedaldi, Arnold Smeulders, Philip Torr, Efstratios Gavves
- **Comment**: To appear at ECCV 2018
- **Journal**: None
- **Summary**: We introduce the OxUvA dataset and benchmark for evaluating single-object tracking algorithms. Benchmarks have enabled great strides in the field of object tracking by defining standardized evaluations on large sets of diverse videos. However, these works have focused exclusively on sequences that are just tens of seconds in length and in which the target is always visible. Consequently, most researchers have designed methods tailored to this "short-term" scenario, which is poorly representative of practitioners' needs. Aiming to address this disparity, we compile a long-term, large-scale tracking dataset of sequences with average length greater than two minutes and with frequent target object disappearance. The OxUvA dataset is much larger than the object tracking datasets of recent years: it comprises 366 sequences spanning 14 hours of video. We assess the performance of several algorithms, considering both the ability to locate the target and to determine whether it is present or absent. Our goal is to offer the community a large and diverse benchmark to enable the design and evaluation of tracking methods ready to be used "in the wild". The project website is http://oxuva.net



### On Regularized Losses for Weakly-supervised CNN Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.09569v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09569v2)
- **Published**: 2018-03-26 13:14:58+00:00
- **Updated**: 2018-04-10 21:59:43+00:00
- **Authors**: Meng Tang, Federico Perazzi, Abdelaziz Djelouah, Ismail Ben Ayed, Christopher Schroers, Yuri Boykov
- **Comment**: None
- **Journal**: None
- **Summary**: Minimization of regularized losses is a principled approach to weak supervision well-established in deep learning, in general. However, it is largely overlooked in semantic segmentation currently dominated by methods mimicking full supervision via "fake" fully-labeled training masks (proposals) generated from available partial input. To obtain such full masks the typical methods explicitly use standard regularization techniques for "shallow" segmentation, e.g. graph cuts or dense CRFs. In contrast, we integrate such standard regularizers directly into the loss functions over partial input. This approach simplifies weakly-supervised training by avoiding extra MRF/CRF inference steps or layers explicitly generating full masks, while improving both the quality and efficiency of training. This paper proposes and experimentally compares different losses integrating MRF/CRF regularization terms. We juxtapose our regularized losses with earlier proposal-generation methods using explicit regularization steps or layers. Our approach achieves state-of-the-art accuracy in semantic segmentation with near full-supervision quality.



### Efficient Image Dataset Classification Difficulty Estimation for Predicting Deep-Learning Accuracy
- **Arxiv ID**: http://arxiv.org/abs/1803.09588v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09588v1)
- **Published**: 2018-03-26 13:46:54+00:00
- **Updated**: 2018-03-26 13:46:54+00:00
- **Authors**: Florian Scheidegger, Roxana Istrate, Giovanni Mariani, Luca Benini, Costas Bekas, Cristiano Malossi
- **Comment**: None
- **Journal**: None
- **Summary**: In the deep-learning community new algorithms are published at an incredible pace. Therefore, solving an image classification problem for new datasets becomes a challenging task, as it requires to re-evaluate published algorithms and their different configurations in order to find a close to optimal classifier. To facilitate this process, before biasing our decision towards a class of neural networks or running an expensive search over the network space, we propose to estimate the classification difficulty of the dataset. Our method computes a single number that characterizes the dataset difficulty 27x faster than training state-of-the-art networks. The proposed method can be used in combination with network topology and hyper-parameter search optimizers to efficiently drive the search towards promising neural-network configurations.



### One-Shot Segmentation in Clutter
- **Arxiv ID**: http://arxiv.org/abs/1803.09597v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09597v2)
- **Published**: 2018-03-26 14:07:20+00:00
- **Updated**: 2018-06-13 12:01:10+00:00
- **Authors**: Claudio Michaelis, Matthias Bethge, Alexander S. Ecker
- **Comment**: To appaer in: $\textit{Proceedings of the $\mathit{35}^{th}$
  International Conference on Machine Learning}$, Stockholm, Sweden, PMLR 80,
  2018
- **Journal**: None
- **Summary**: We tackle the problem of one-shot segmentation: finding and segmenting a previously unseen object in a cluttered scene based on a single instruction example. We propose a novel dataset, which we call $\textit{cluttered Omniglot}$. Using a baseline architecture combining a Siamese embedding for detection with a U-net for segmentation we show that increasing levels of clutter make the task progressively harder. Using oracle models with access to various amounts of ground-truth information, we evaluate different aspects of the problem and show that in this kind of visual search task, detection and segmentation are two intertwined problems, the solution to each of which helps solving the other. We therefore introduce $\textit{MaskNet}$, an improved model that attends to multiple candidate locations, generates segmentation proposals to mask out background clutter and selects among the segmented objects. Our findings suggest that such image recognition models based on an iterative refinement of object detection and foreground segmentation may provide a way to deal with highly cluttered scenes.



### Metric Learning with Dynamically Generated Pairwise Constraints for Ear Recognition
- **Arxiv ID**: http://arxiv.org/abs/1803.09630v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.09630v1)
- **Published**: 2018-03-26 14:45:36+00:00
- **Updated**: 2018-03-26 14:45:36+00:00
- **Authors**: Ibrahim Omara, Hongzhi Zhang, Faqiang Wang, Wangmeng Zuo
- **Comment**: 17 pages, 3 figures
- **Journal**: None
- **Summary**: Ear recognition task is known as predicting whether two ear images belong to the same person or not. In this paper, we present a novel metric learning method for ear recognition. This method is formulated as a pairwise constrained optimization problem. In each training cycle, this method selects the nearest similar and dissimilar neighbors of each sample to construct the pairwise constraints, and then solve the optimization problem by the iterated Bregman projections. Experiments are conducted on AMI, USTB II and WPUT databases. The results show that the proposed approach can achieve promising recognition rates in ear recognition, and its training process is much more efficient than the other competing metric learning methods.



### On the Limitation of Local Intrinsic Dimensionality for Characterizing the Subspaces of Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1803.09638v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.09638v1)
- **Published**: 2018-03-26 14:56:28+00:00
- **Updated**: 2018-03-26 14:56:28+00:00
- **Authors**: Pei-Hsuan Lu, Pin-Yu Chen, Chia-Mu Yu
- **Comment**: Accepted to ICLR 2018 Worshop
- **Journal**: None
- **Summary**: Understanding and characterizing the subspaces of adversarial examples aid in studying the robustness of deep neural networks (DNNs) to adversarial perturbations. Very recently, Ma et al. (ICLR 2018) proposed to use local intrinsic dimensionality (LID) in layer-wise hidden representations of DNNs to study adversarial subspaces. It was demonstrated that LID can be used to characterize the adversarial subspaces associated with different attack methods, e.g., the Carlini and Wagner's (C&W) attack and the fast gradient sign attack.   In this paper, we use MNIST and CIFAR-10 to conduct two new sets of experiments that are absent in existing LID analysis and report the limitation of LID in characterizing the corresponding adversarial subspaces, which are (i) oblivious attacks and LID analysis using adversarial examples with different confidence levels; and (ii) black-box transfer attacks. For (i), we find that the performance of LID is very sensitive to the confidence parameter deployed by an attack, and the LID learned from ensembles of adversarial examples with varying confidence levels surprisingly gives poor performance. For (ii), we find that when adversarial examples are crafted from another DNN model, LID is ineffective in characterizing their adversarial subspaces. These two findings together suggest the limited capability of LID in characterizing the subspaces of adversarial examples.



### The Unmanned Aerial Vehicle Benchmark: Object Detection and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1804.00518v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00518v1)
- **Published**: 2018-03-26 15:07:09+00:00
- **Updated**: 2018-03-26 15:07:09+00:00
- **Authors**: Dawei Du, Yuankai Qi, Hongyang Yu, Yifan Yang, Kaiwen Duan, Guorong Li, Weigang Zhang, Qingming Huang, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: With the advantage of high mobility, Unmanned Aerial Vehicles (UAVs) are used to fuel numerous important applications in computer vision, delivering more efficiency and convenience than surveillance cameras with fixed camera angle, scale and view. However, very limited UAV datasets are proposed, and they focus only on a specific task such as visual tracking or object detection in relatively constrained scenarios. Consequently, it is of great importance to develop an unconstrained UAV benchmark to boost related researches. In this paper, we construct a new UAV benchmark focusing on complex scenarios with new level challenges. Selected from 10 hours raw videos, about 80,000 representative frames are fully annotated with bounding boxes as well as up to 14 kinds of attributes (e.g., weather condition, flying altitude, camera view, vehicle category, and occlusion) for three fundamental computer vision tasks: object detection, single object tracking, and multiple object tracking. Then, a detailed quantitative study is performed using most recent state-of-the-art algorithms for each task. Experimental results show that the current state-of-the-art methods perform relative worse on our dataset, due to the new challenges appeared in UAV based real scenes, e.g., high density, small object, and camera motion. To our knowledge, our work is the first time to explore such issues in unconstrained scenes comprehensively.



### BAGAN: Data Augmentation with Balancing GAN
- **Arxiv ID**: http://arxiv.org/abs/1803.09655v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.09655v2)
- **Published**: 2018-03-26 15:20:56+00:00
- **Updated**: 2018-06-05 08:07:30+00:00
- **Authors**: Giovanni Mariani, Florian Scheidegger, Roxana Istrate, Costas Bekas, Cristiano Malossi
- **Comment**: None
- **Journal**: None
- **Summary**: Image classification datasets are often imbalanced, characteristic that negatively affects the accuracy of deep-learning classifiers. In this work we propose balancing GAN (BAGAN) as an augmentation tool to restore balance in imbalanced datasets. This is challenging because the few minority-class images may not be enough to train a GAN. We overcome this issue by including during the adversarial training all available images of majority and minority classes. The generative model learns useful features from majority classes and uses these to generate images for minority classes. We apply class conditioning in the latent space to drive the generation process towards a target class. The generator in the GAN is initialized with the encoder module of an autoencoder that enables us to learn an accurate class-conditioning in the latent space. We compare the proposed methodology with state-of-the-art GANs and demonstrate that BAGAN generates images of superior quality when trained with an imbalanced dataset.



### A multilayer backpropagation saliency detection algorithm and its applications
- **Arxiv ID**: http://arxiv.org/abs/1803.09659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09659v1)
- **Published**: 2018-03-26 15:26:21+00:00
- **Updated**: 2018-03-26 15:26:21+00:00
- **Authors**: Chunbiao Zhu, Ge Li
- **Comment**: Publish version can be downloaded in
  https://link.springer.com/article/10.1007/s11042-018-5780-4 . Source code can
  be downloaded in https://github.com/ChunbiaoZhu/CAIP2017
- **Journal**: None
- **Summary**: Saliency detection is an active topic in the multimedia field. Most previous works on saliency detection focus on 2D images. However, these methods are not robust against complex scenes which contain multiple objects or complex backgrounds. Recently, depth information supplies a powerful cue for saliency detection. In this paper, we propose a multilayer backpropagation saliency detection algorithm based on depth mining by which we exploit depth cue from three different layers of images. The proposed algorithm shows a good performance and maintains the robustness in complex situations. Experiments' results show that the proposed framework is superior to other existing saliency approaches. Besides, we give two innovative applications by this algorithm, such as scene reconstruction from multiple images and small target object detection in video.



### On the Intrinsic Dimensionality of Image Representations
- **Arxiv ID**: http://arxiv.org/abs/1803.09672v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.09672v2)
- **Published**: 2018-03-26 15:38:27+00:00
- **Updated**: 2019-04-11 01:04:41+00:00
- **Authors**: Sixue Gong, Vishnu Naresh Boddeti, Anil K. Jain
- **Comment**: Accepted for publication at CVPR 2019
- **Journal**: None
- **Summary**: This paper addresses the following questions pertaining to the intrinsic dimensionality of any given image representation: (i) estimate its intrinsic dimensionality, (ii) develop a deep neural network based non-linear mapping, dubbed DeepMDS, that transforms the ambient representation to the minimal intrinsic space, and (iii) validate the veracity of the mapping through image matching in the intrinsic space. Experiments on benchmark image datasets (LFW, IJB-C and ImageNet-100) reveal that the intrinsic dimensionality of deep neural network representations is significantly lower than the dimensionality of the ambient features. For instance, SphereFace's 512-dim face representation and ResNet's 512-dim image representation have an intrinsic dimensionality of 16 and 19 respectively. Further, the DeepMDS mapping is able to obtain a representation of significantly lower dimensionality while maintaining discriminative ability to a large extent, 59.75% TAR @ 0.1% FAR in 16-dim vs 71.26% TAR in 512-dim on IJB-C and a Top-1 accuracy of 77.0% at 19-dim vs 83.4% at 512-dim on ImageNet-100.



### Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++
- **Arxiv ID**: http://arxiv.org/abs/1803.09693v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09693v1)
- **Published**: 2018-03-26 16:14:36+00:00
- **Updated**: 2018-03-26 16:14:36+00:00
- **Authors**: David Acuna, Huan Ling, Amlan Kar, Sanja Fidler
- **Comment**: Accepted to CVPR 2018 (http://www.cs.toronto.edu/polyrnn/)
- **Journal**: None
- **Summary**: Manually labeling datasets with object masks is extremely time consuming. In this work, we follow the idea of Polygon-RNN to produce polygonal annotations of objects interactively using humans-in-the-loop. We introduce several important improvements to the model: 1) we design a new CNN encoder architecture, 2) show how to effectively train the model with Reinforcement Learning, and 3) significantly increase the output resolution using a Graph Neural Network, allowing the model to accurately annotate high-resolution objects in images. Extensive evaluation on the Cityscapes dataset shows that our model, which we refer to as Polygon-RNN++, significantly outperforms the original model in both automatic (10% absolute and 16% relative improvement in mean IoU) and interactive modes (requiring 50% fewer clicks by annotators). We further analyze the cross-domain scenario in which our model is trained on one dataset, and used out of the box on datasets from varying domains. The results show that Polygon-RNN++ exhibits powerful generalization capabilities, achieving significant improvements over existing pixel-wise methods. Using simple online fine-tuning we further achieve a high reduction in annotation time for new datasets, moving a step closer towards an interactive annotation tool to be used in practice.



### On the Importance of Stereo for Accurate Depth Estimation: An Efficient Semi-Supervised Deep Neural Network Approach
- **Arxiv ID**: http://arxiv.org/abs/1803.09719v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09719v4)
- **Published**: 2018-03-26 17:19:40+00:00
- **Updated**: 2020-07-08 02:14:14+00:00
- **Authors**: Nikolai Smolyanskiy, Alexey Kamenev, Stan Birchfield
- **Comment**: CVPR 2018 Workshop on Autonomous Driving. For video, see
  https://youtu.be/0FPQdVOYoAU
- **Journal**: None
- **Summary**: We revisit the problem of visual depth estimation in the context of autonomous vehicles. Despite the progress on monocular depth estimation in recent years, we show that the gap between monocular and stereo depth accuracy remains large$-$a particularly relevant result due to the prevalent reliance upon monocular cameras by vehicles that are expected to be self-driving. We argue that the challenges of removing this gap are significant, owing to fundamental limitations of monocular vision. As a result, we focus our efforts on depth estimation by stereo. We propose a novel semi-supervised learning approach to training a deep stereo neural network, along with a novel architecture containing a machine-learned argmax layer and a custom runtime (that will be shared publicly) that enables a smaller version of our stereo DNN to run on an embedded GPU. Competitive results are shown on the KITTI 2015 stereo dataset. We also evaluate the recent progress of stereo algorithms by measuring the impact upon accuracy of various design criteria.



### 3D Human Pose Estimation in the Wild by Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1803.09722v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09722v2)
- **Published**: 2018-03-26 17:24:40+00:00
- **Updated**: 2018-04-16 14:20:04+00:00
- **Authors**: Wei Yang, Wanli Ouyang, Xiaolong Wang, Jimmy Ren, Hongsheng Li, Xiaogang Wang
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Recently, remarkable advances have been achieved in 3D human pose estimation from monocular images because of the powerful Deep Convolutional Neural Networks (DCNNs). Despite their success on large-scale datasets collected in the constrained lab environment, it is difficult to obtain the 3D pose annotations for in-the-wild images. Therefore, 3D human pose estimation in the wild is still a challenge. In this paper, we propose an adversarial learning framework, which distills the 3D human pose structures learned from the fully annotated dataset to in-the-wild images with only 2D pose annotations. Instead of defining hard-coded rules to constrain the pose estimation results, we design a novel multi-source discriminator to distinguish the predicted 3D poses from the ground-truth, which helps to enforce the pose estimator to generate anthropometrically valid poses even with images in the wild. We also observe that a carefully designed information source for the discriminator is essential to boost the performance. Thus, we design a geometric descriptor, which computes the pairwise relative locations and distances between body joints, as a new information source for the discriminator. The efficacy of our adversarial learning framework with the new geometric descriptor has been demonstrated through extensive experiments on widely used public benchmarks. Our approach significantly improves the performance compared with previous state-of-the-art approaches.



### Predicting the Future with Transformational States
- **Arxiv ID**: http://arxiv.org/abs/1803.09760v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1803.09760v1)
- **Published**: 2018-03-26 18:00:07+00:00
- **Updated**: 2018-03-26 18:00:07+00:00
- **Authors**: Andrew Jaegle, Oleh Rybkin, Konstantinos G. Derpanis, Kostas Daniilidis
- **Comment**: 24 pages, including supplement
- **Journal**: None
- **Summary**: An intelligent observer looks at the world and sees not only what is, but what is moving and what can be moved. In other words, the observer sees how the present state of the world can transform in the future. We propose a model that predicts future images by learning to represent the present state and its transformation given only a sequence of images. To do so, we introduce an architecture with a latent state composed of two components designed to capture (i) the present image state and (ii) the transformation between present and future states, respectively. We couple this latent state with a recurrent neural network (RNN) core that predicts future frames by transforming past states into future states by applying the accumulated state transformation with a learned operator. We describe how this model can be integrated into an encoder-decoder convolutional neural network (CNN) architecture that uses weighted residual connections to integrate representations of the past with representations of the future. Qualitatively, our approach generates image sequences that are stable and capture realistic motion over multiple predicted frames, without requiring adversarial training. Quantitatively, our method achieves prediction results comparable to state-of-the-art results on standard image prediction benchmarks (Moving MNIST, KTH, and UCF101).



### Video Representation Learning Using Discriminative Pooling
- **Arxiv ID**: http://arxiv.org/abs/1803.10628v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.10628v2)
- **Published**: 2018-03-26 18:32:04+00:00
- **Updated**: 2018-03-29 15:38:14+00:00
- **Authors**: Jue Wang, Anoop Cherian, Fatih Porikli, Stephen Gould
- **Comment**: 8 pages, 7 figures, Accepted in CVPR2018. arXiv admin note:
  substantial text overlap with arXiv:1704.01716
- **Journal**: None
- **Summary**: Popular deep models for action recognition in videos generate independent predictions for short clips, which are then pooled heuristically to assign an action label to the full video segment. As not all frames may characterize the underlying action---indeed, many are common across multiple actions---pooling schemes that impose equal importance on all frames might be unfavorable. In an attempt to tackle this problem, we propose discriminative pooling, based on the notion that among the deep features generated on all short clips, there is at least one that characterizes the action. To this end, we learn a (nonlinear) hyperplane that separates this unknown, yet discriminative, feature from the rest. Applying multiple instance learning in a large-margin setup, we use the parameters of this separating hyperplane as a descriptor for the full video segment. Since these parameters are directly related to the support vectors in a max-margin framework, they serve as robust representations for pooling of the features. We formulate a joint objective and an efficient solver that learns these hyperplanes per video and the corresponding action classifiers over the hyperplanes. Our pooling scheme is end-to-end trainable within a deep framework. We report results from experiments on three benchmark datasets spanning a variety of challenges and demonstrate state-of-the-art performance across these tasks.



### Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1803.09786v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09786v1)
- **Published**: 2018-03-26 18:47:55+00:00
- **Updated**: 2018-03-26 18:47:55+00:00
- **Authors**: Jingya Wang, Xiatian Zhu, Shaogang Gong, Wei Li
- **Comment**: Accepted at CVPR 2018
- **Journal**: None
- **Summary**: Most existing person re-identification (re-id) methods require supervised model learning from a separate large set of pairwise labelled training data for every single camera pair. This significantly limits their scalability and usability in real-world large scale deployments with the need for performing re-id across many camera views. To address this scalability problem, we develop a novel deep learning method for transferring the labelled information of an existing dataset to a new unseen (unlabelled) target domain for person re-id without any supervised learning in the target domain. Specifically, we introduce an Transferable Joint Attribute-Identity Deep Learning (TJ-AIDL) for simultaneously learning an attribute-semantic and identitydiscriminative feature representation space transferrable to any new (unseen) target domain for re-id tasks without the need for collecting new labelled training data from the target domain (i.e. unsupervised learning in the target domain). Extensive comparative evaluations validate the superiority of this new TJ-AIDL model for unsupervised person re-id over a wide range of state-of-the-art methods on four challenging benchmarks including VIPeR, PRID, Market-1501, and DukeMTMC-ReID.



### Women also Snowboard: Overcoming Bias in Captioning Models
- **Arxiv ID**: http://arxiv.org/abs/1803.09797v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09797v4)
- **Published**: 2018-03-26 19:07:08+00:00
- **Updated**: 2019-03-13 21:32:00+00:00
- **Authors**: Kaylee Burns, Lisa Anne Hendricks, Kate Saenko, Trevor Darrell, Anna Rohrbach
- **Comment**: 22 pages, 6 figures, Burns and Hendricks contributed equally
- **Journal**: None
- **Summary**: Most machine learning methods are known to capture and exploit biases of the training data. While some biases are beneficial for learning, others are harmful. Specifically, image captioning models tend to exaggerate biases present in training data (e.g., if a word is present in 60% of training sentences, it might be predicted in 70% of sentences at test time). This can lead to incorrect captions in domains where unbiased captions are desired, or required, due to over-reliance on the learned prior and image context. In this work we investigate generation of gender-specific caption words (e.g. man, woman) based on the person's appearance or the image context. We introduce a new Equalizer model that ensures equal gender probability when gender evidence is occluded in a scene and confident predictions when gender evidence is present. The resulting model is forced to look at a person rather than use contextual cues to make a gender-specific predictions. The losses that comprise our model, the Appearance Confusion Loss and the Confident Loss, are general, and can be added to any description model in order to mitigate impacts of unwanted bias in a description dataset. Our proposed model has lower error than prior work when describing images with people and mentioning their gender and more closely matches the ground truth ratio of sentences including women to sentences including men. We also show that unlike other approaches, our model is indeed more often looking at people when predicting their gender.



### Generating Talking Face Landmarks from Speech
- **Arxiv ID**: http://arxiv.org/abs/1803.09803v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09803v2)
- **Published**: 2018-03-26 19:25:02+00:00
- **Updated**: 2018-04-23 15:08:51+00:00
- **Authors**: Sefik Emre Eskimez, Ross K Maddox, Chenliang Xu, Zhiyao Duan
- **Comment**: To Appear in LVA ICA 2018. Please see the following link:
  http://www2.ece.rochester.edu/projects/air/projects/talkingface.html
- **Journal**: None
- **Summary**: The presence of a corresponding talking face has been shown to significantly improve speech intelligibility in noisy conditions and for hearing impaired population. In this paper, we present a system that can generate landmark points of a talking face from an acoustic speech in real time. The system uses a long short-term memory (LSTM) network and is trained on frontal videos of 27 different speakers with automatically extracted face landmarks. After training, it can produce talking face landmarks from the acoustic speech of unseen speakers and utterances. The training phase contains three key steps. We first transform landmarks of the first video frame to pin the two eye points into two predefined locations and apply the same transformation on all of the following video frames. We then remove the identity information by transforming the landmarks into a mean face shape across the entire training dataset. Finally, we train an LSTM network that takes the first- and second-order temporal differences of the log-mel spectrogram as input to predict face landmarks in each frame. We evaluate our system using the mean-squared error (MSE) loss of landmarks of lips between predicted and ground-truth landmarks as well as their first- and second-order temporal differences. We further evaluate our system by conducting subjective tests, where the subjects try to distinguish the real and fake videos of talking face landmarks. Both tests show promising results.



### A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay
- **Arxiv ID**: http://arxiv.org/abs/1803.09820v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.09820v2)
- **Published**: 2018-03-26 20:05:59+00:00
- **Updated**: 2018-04-24 17:43:51+00:00
- **Authors**: Leslie N. Smith
- **Comment**: Files to help replicate the results reported here are available on
  Github
- **Journal**: None
- **Summary**: Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation/test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentums. Files to help replicate the results reported here are available.



### Low-Shot Learning for the Semantic Segmentation of Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/1803.09824v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.09824v1)
- **Published**: 2018-03-26 20:17:19+00:00
- **Updated**: 2018-03-26 20:17:19+00:00
- **Authors**: Ronald Kemker, Ryan Luu, Christopher Kanan
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in computer vision using deep learning with RGB imagery (e.g., object recognition and detection) have been made possible thanks to the development of large annotated RGB image datasets. In contrast, multispectral image (MSI) and hyperspectral image (HSI) datasets contain far fewer labeled images, in part due to the wide variety of sensors used. These annotations are especially limited for semantic segmentation, or pixel-wise classification, of remote sensing imagery because it is labor intensive to generate image annotations. Low-shot learning algorithms can make effective inferences despite smaller amounts of annotated data. In this paper, we study low-shot learning using self-taught feature learning for semantic segmentation. We introduce 1) an improved self-taught feature learning framework for HSI and MSI data and 2) a semi-supervised classification algorithm. When these are combined, they achieve state-of-the-art performance on remote sensing datasets that have little annotated training data available. These low-shot learning frameworks will reduce the manual image annotation burden and improve semantic segmentation performance for remote sensing imagery.



