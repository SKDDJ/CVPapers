# Arxiv Papers in cs.CV on 2018-03-23
### Classification of simulated radio signals using Wide Residual Networks for use in the search for extra-terrestrial intelligence
- **Arxiv ID**: http://arxiv.org/abs/1803.08624v1
- **DOI**: None
- **Categories**: **cs.LG**, astro-ph.IM, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1803.08624v1)
- **Published**: 2018-03-23 00:56:13+00:00
- **Updated**: 2018-03-23 00:56:13+00:00
- **Authors**: G. A. Cox, S. Egly, G. R. Harp, J. Richards, S. Vinodababu, J. Voien
- **Comment**: 16 pages, 8 figures
- **Journal**: None
- **Summary**: We describe a new approach and algorithm for the detection of artificial signals and their classification in the search for extraterrestrial intelligence (SETI). The characteristics of radio signals observed during SETI research are often most apparent when those signals are represented as spectrograms. Additionally, many observed signals tend to share the same characteristics, allowing for sorting of the signals into different classes. For this work, complex-valued time-series data were simulated to produce a corpus of 140,000 signals from seven different signal classes. A wide residual neural network was then trained to classify these signal types using the gray-scale 2D spectrogram representation of those signals. An average $F_1$ score of 95.11\% was attained when tested on previously unobserved simulated signals. We also report on the performance of the model across a range of signal amplitudes.



### Hardware based Spatio-Temporal Neural Processing Backend for Imaging Sensors: Towards a Smart Camera
- **Arxiv ID**: http://arxiv.org/abs/1803.08635v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.ET, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1803.08635v1)
- **Published**: 2018-03-23 01:57:49+00:00
- **Updated**: 2018-03-23 01:57:49+00:00
- **Authors**: Samiran Ganguly, Yunfei Gu, Mircea R. Stan, Avik W. Ghosh
- **Comment**: 11 pages, 5 figures. To be presented in SPIE DCS 2018: Image Sensing
  Technologies: Materials, Devices, Systems, and Applications V
- **Journal**: None
- **Summary**: In this work we show how we can build a technology platform for cognitive imaging sensors using recent advances in recurrent neural network architectures and training methods inspired from biology. We demonstrate learning and processing tasks specific to imaging sensors, including enhancement of sensitivity and signal-to-noise ratio (SNR) purely through neural filtering beyond the fundamental limits sensor materials, and inferencing and spatio-temporal pattern recognition capabilities of these networks with applications in object detection, motion tracking and prediction. We then show designs of unit hardware cells built using complementary metal-oxide semiconductor (CMOS) and emerging materials technologies for ultra-compact and energy-efficient embedded neural processors for smart cameras.



### PDNet: Prior-model Guided Depth-enhanced Network for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1803.08636v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1803.08636v2)
- **Published**: 2018-03-23 02:04:47+00:00
- **Updated**: 2018-10-13 13:53:32+00:00
- **Authors**: Chunbiao Zhu, Xing Cai, Kan Huang, Thomas H Li, Ge Li
- **Comment**: This paper is under review. Project website:
  https://github.com/ChunbiaoZhu/PDNet/
- **Journal**: None
- **Summary**: Fully convolutional neural networks (FCNs) have shown outstanding performance in many computer vision tasks including salient object detection. However, there still remains two issues needed to be addressed in deep learning based saliency detection. One is the lack of tremendous amount of annotated data to train a network. The other is the lack of robustness for extracting salient objects in images containing complex scenes. In this paper, we present a new architecture$ - $PDNet, a robust prior-model guided depth-enhanced network for RGB-D salient object detection. In contrast to existing works, in which RGB-D values of image pixels are fed directly to a network, the proposed architecture is composed of a master network for processing RGB values, and a sub-network making full use of depth cues and incorporate depth-based features into the master network. To overcome the limited size of the labeled RGB-D dataset for training, we employ a large conventional RGB dataset to pre-train the master network, which proves to contribute largely to the final accuracy. Extensive evaluations over five benchmark datasets demonstrate that our proposed method performs favorably against the state-of-the-art approaches.



### Fictitious GAN: Training GANs with Historical Models
- **Arxiv ID**: http://arxiv.org/abs/1803.08647v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.08647v2)
- **Published**: 2018-03-23 03:46:12+00:00
- **Updated**: 2018-07-11 18:50:03+00:00
- **Authors**: Hao Ge, Yin Xia, Xu Chen, Randall Berry, Ying Wu
- **Comment**: 19 pages. First three authors have equal contributions
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) are powerful tools for learning generative models. In practice, the training may suffer from lack of convergence. GANs are commonly viewed as a two-player zero-sum game between two neural networks. Here, we leverage this game theoretic view to study the convergence behavior of the training process. Inspired by the fictitious play learning process, a novel training method, referred to as Fictitious GAN, is introduced. Fictitious GAN trains the deep neural networks using a mixture of historical models. Specifically, the discriminator (resp. generator) is updated according to the best-response to the mixture outputs from a sequence of previously trained generators (resp. discriminators). It is shown that Fictitious GAN can effectively resolve some convergence issues that cannot be resolved by the standard training approach. It is proved that asymptotically the average of the generator outputs has the same distribution as the data samples.



### Lifting Layers: Analysis and Applications
- **Arxiv ID**: http://arxiv.org/abs/1803.08660v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1803.08660v1)
- **Published**: 2018-03-23 05:47:50+00:00
- **Updated**: 2018-03-23 05:47:50+00:00
- **Authors**: Peter Ochs, Tim Meinhardt, Laura Leal-Taixe, Michael Moeller
- **Comment**: None
- **Journal**: None
- **Summary**: The great advances of learning-based approaches in image processing and computer vision are largely based on deeply nested networks that compose linear transfer functions with suitable non-linearities. Interestingly, the most frequently used non-linearities in imaging applications (variants of the rectified linear unit) are uncommon in low dimensional approximation problems. In this paper we propose a novel non-linear transfer function, called lifting, which is motivated from a related technique in convex optimization. A lifting layer increases the dimensionality of the input, naturally yields a linear spline when combined with a fully connected layer, and therefore closes the gap between low and high dimensional approximation problems. Moreover, applying the lifting operation to the loss layer of the network allows us to handle non-convex and flat (zero-gradient) cost functions. We analyze the proposed lifting theoretically, exemplify interesting properties in synthetic experiments and demonstrate its effectiveness in deep learning approaches to image classification and denoising.



### Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network
- **Arxiv ID**: http://arxiv.org/abs/1803.08664v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08664v5)
- **Published**: 2018-03-23 06:07:20+00:00
- **Updated**: 2018-10-04 21:47:19+00:00
- **Authors**: Namhyuk Ahn, Byungkon Kang, Kyung-Ah Sohn
- **Comment**: European Conference on Computer Vision (ECCV), 2018
- **Journal**: None
- **Summary**: In recent years, deep learning methods have been successfully applied to single-image super-resolution tasks. Despite their great performances, deep learning methods cannot be easily applied to real-world applications due to the requirement of heavy computation. In this paper, we address this issue by proposing an accurate and lightweight deep network for image super-resolution. In detail, we design an architecture that implements a cascading mechanism upon a residual network. We also present variant models of the proposed cascading residual network to further improve efficiency. Our extensive experiments show that even with much fewer parameters and operations, our models achieve performance comparable to that of state-of-the-art methods.



### Pyramid Stereo Matching Network
- **Arxiv ID**: http://arxiv.org/abs/1803.08669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08669v1)
- **Published**: 2018-03-23 06:40:09+00:00
- **Updated**: 2018-03-23 06:40:09+00:00
- **Authors**: Jia-Ren Chang, Yong-Sheng Chen
- **Comment**: CVPR 2018
- **Journal**: None
- **Summary**: Recent work has shown that depth estimation from a stereo pair of images can be formulated as a supervised learning task to be resolved with convolutional neural networks (CNNs). However, current architectures rely on patch-based Siamese networks, lacking the means to exploit context information for finding correspondence in illposed regions. To tackle this problem, we propose PSMNet, a pyramid stereo matching network consisting of two main modules: spatial pyramid pooling and 3D CNN. The spatial pyramid pooling module takes advantage of the capacity of global context information by aggregating context in different scales and locations to form a cost volume. The 3D CNN learns to regularize cost volume using stacked multiple hourglass networks in conjunction with intermediate supervision. The proposed approach was evaluated on several benchmark datasets. Our method ranked first in the KITTI 2012 and 2015 leaderboards before March 18, 2018. The codes of PSMNet are available at: https://github.com/JiaRenChang/PSMNet.



### Object Detection for Comics using Manga109 Annotations
- **Arxiv ID**: http://arxiv.org/abs/1803.08670v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1803.08670v2)
- **Published**: 2018-03-23 06:54:48+00:00
- **Updated**: 2018-03-26 05:35:40+00:00
- **Authors**: Toru Ogawa, Atsushi Otsubo, Rei Narita, Yusuke Matsui, Toshihiko Yamasaki, Kiyoharu Aizawa
- **Comment**: http://www.manga109.org/en/
- **Journal**: None
- **Summary**: With the growth of digitized comics, image understanding techniques are becoming important. In this paper, we focus on object detection, which is a fundamental task of image understanding. Although convolutional neural networks (CNN)-based methods archived good performance in object detection for naturalistic images, there are two problems in applying these methods to the comic object detection task. First, there is no large-scale annotated comics dataset. The CNN-based methods require large-scale annotations for training. Secondly, the objects in comics are highly overlapped compared to naturalistic images. This overlap causes the assignment problem in the existing CNN-based methods. To solve these problems, we proposed a new annotation dataset and a new CNN model. We annotated an existing image dataset of comics and created the largest annotation dataset, named Manga109-annotations. For the assignment problem, we proposed a new CNN-based detector, SSD300-fork. We compared SSD300-fork with other detection methods using Manga109-annotations and confirmed that our model outperformed them based on the mAP score.



### Revisiting Single Image Depth Estimation: Toward Higher Resolution Maps with Accurate Object Boundaries
- **Arxiv ID**: http://arxiv.org/abs/1803.08673v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08673v2)
- **Published**: 2018-03-23 07:16:33+00:00
- **Updated**: 2018-09-22 07:30:39+00:00
- **Authors**: Junjie Hu, Mete Ozay, Yan Zhang, Takayuki Okatani
- **Comment**: None
- **Journal**: None
- **Summary**: This paper considers the problem of single image depth estimation. The employment of convolutional neural networks (CNNs) has recently brought about significant advancements in the research of this problem. However, most existing methods suffer from loss of spatial resolution in the estimated depth maps; a typical symptom is distorted and blurry reconstruction of object boundaries. In this paper, toward more accurate estimation with a focus on depth maps with higher spatial resolution, we propose two improvements to existing approaches. One is about the strategy of fusing features extracted at different scales, for which we propose an improved network architecture consisting of four modules: an encoder, decoder, multi-scale feature fusion module, and refinement module. The other is about loss functions for measuring inference errors used in training. We show that three loss terms, which measure errors in depth, gradients and surface normals, respectively, contribute to improvement of accuracy in an complementary fashion. Experimental results show that these two improvements enable to attain higher accuracy than the current state-of-the-arts, which is given by finer resolution reconstruction, for example, with small objects and object boundaries.



### Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1803.08679v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08679v1)
- **Published**: 2018-03-23 07:55:24+00:00
- **Updated**: 2018-03-23 07:55:24+00:00
- **Authors**: Feng Li, Cheng Tian, Wangmeng Zuo, Lei Zhang, Ming-Hsuan Yang
- **Comment**: Accepted at CVPR 2018
- **Journal**: None
- **Summary**: Discriminative Correlation Filters (DCF) are efficient in visual tracking but suffer from unwanted boundary effects. Spatially Regularized DCF (SRDCF) has been suggested to resolve this issue by enforcing spatial penalty on DCF coefficients, which, inevitably, improves the tracking performance at the price of increasing complexity. To tackle online updating, SRDCF formulates its model on multiple training images, further adding difficulties in improving efficiency. In this work, by introducing temporal regularization to SRDCF with single sample, we present our spatial-temporal regularized correlation filters (STRCF). Motivated by online Passive-Agressive (PA) algorithm, we introduce the temporal regularization to SRDCF with single sample, thus resulting in our spatial-temporal regularized correlation filters (STRCF). The STRCF formulation can not only serve as a reasonable approximation to SRDCF with multiple training samples, but also provide a more robust appearance model than SRDCF in the case of large appearance variations. Besides, it can be efficiently solved via the alternating direction method of multipliers (ADMM). By incorporating both temporal and spatial regularization, our STRCF can handle boundary effects without much loss in efficiency and achieve superior performance over SRDCF in terms of accuracy and speed. Experiments are conducted on three benchmark datasets: OTB-2015, Temple-Color, and VOT-2016. Compared with SRDCF, STRCF with hand-crafted features provides a 5 times speedup and achieves a gain of 5.4% and 3.6% AUC score on OTB-2015 and Temple-Color, respectively. Moreover, STRCF combined with CNN features also performs favorably against state-of-the-art CNN-based trackers and achieves an AUC score of 68.3% on OTB-2015.



### Improving DNN Robustness to Adversarial Attacks using Jacobian Regularization
- **Arxiv ID**: http://arxiv.org/abs/1803.08680v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.08680v4)
- **Published**: 2018-03-23 07:57:04+00:00
- **Updated**: 2019-05-28 09:48:05+00:00
- **Authors**: Daniel Jakubovitz, Raja Giryes
- **Comment**: ECCV 2018 Conference Paper
- **Journal**: None
- **Summary**: Deep neural networks have lately shown tremendous performance in various applications including vision and speech processing tasks. However, alongside their ability to perform these tasks with such high accuracy, it has been shown that they are highly susceptible to adversarial attacks: a small change in the input would cause the network to err with high confidence. This phenomenon exposes an inherent fault in these networks and their ability to generalize well. For this reason, providing robustness to adversarial attacks is an important challenge in networks training, which has led to extensive research. In this work, we suggest a theoretically inspired novel approach to improve the networks' robustness. Our method applies regularization using the Frobenius norm of the Jacobian of the network, which is applied as post-processing, after regular training has finished. We demonstrate empirically that it leads to enhanced robustness results with a minimal change in the original network's accuracy.



### Region-filtering Correlation Tracking
- **Arxiv ID**: http://arxiv.org/abs/1803.08687v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08687v1)
- **Published**: 2018-03-23 08:37:43+00:00
- **Updated**: 2018-03-23 08:37:43+00:00
- **Authors**: Nana Fan, Zhenyu He
- **Comment**: 16 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: Recently, correlation filters have demonstrated the excellent performance in visual tracking. However, the base training sample region is larger than the object region,including the Interference Region(IR). The IRs in training samples from cyclic shifts of the base training sample severely degrade the quality of a tracking model. In this paper, we propose the novel Region-filtering Correlation Tracking (RFCT) to address this problem. We immediately filter training samples by introducing a spatial map into the standard CF formulation. Compared with existing correlation filter trackers, our proposed tracker has the following advantages: (1) The correlation filter can be learned on a larger search region without the interference of the IR by a spatial map. (2) Due to processing training samples by a spatial map, it is more general way to control background information and target information in training samples. The values of the spatial map are not restricted, then a better spatial map can be explored. (3) The weight proportions of accurate filters are increased to alleviate model corruption. Experiments are performed on two benchmark datasets: OTB-2013 and OTB-2015. Quantitative evaluations on these benchmarks demonstrate that the proposed RFCT algorithm performs favorably against several state-of-the-art methods.



### Deep learning and its application to medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.08691v1
- **DOI**: 10.11409/mit.36.63
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08691v1)
- **Published**: 2018-03-23 08:55:10+00:00
- **Updated**: 2018-03-23 08:55:10+00:00
- **Authors**: Holger R. Roth, Chen Shen, Hirohisa Oda, Masahiro Oda, Yuichiro Hayashi, Kazunari Misawa, Kensaku Mori
- **Comment**: Accepted for publication in the journal of the Japanese Society of
  Medical Imaging Technology (JAMIT)
- **Journal**: Medical Imaging Technology, Volume 36 (2018), Issue 2, p. 63-71
- **Summary**: One of the most common tasks in medical imaging is semantic segmentation. Achieving this segmentation automatically has been an active area of research, but the task has been proven very challenging due to the large variation of anatomy across different patients. However, recent advances in deep learning have made it possible to significantly improve the performance of image recognition and semantic segmentation methods in the field of computer vision. Due to the data driven approaches of hierarchical feature learning in deep learning frameworks, these advances can be translated to medical images without much difficulty. Several variations of deep convolutional neural networks have been successfully applied to medical images. Especially fully convolutional architectures have been proven efficient for segmentation of 3D medical images. In this article, we describe how to build a 3D fully convolutional network (FCN) that can process 3D images in order to produce automatic semantic segmentations. The model is trained and evaluated on a clinical computed tomography (CT) dataset and shows state-of-the-art performance in multi-organ segmentation.



### An Incremental Boolean Tensor Factorization approach to model Change Patterns of Objects in Images
- **Arxiv ID**: http://arxiv.org/abs/1803.08696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08696v1)
- **Published**: 2018-03-23 09:08:35+00:00
- **Updated**: 2018-03-23 09:08:35+00:00
- **Authors**: S Saritha, G Santhosh Kumar
- **Comment**: This work is not submitted to any journals/conferences
- **Journal**: None
- **Summary**: Change detection process has recently progressed from a post-classification method to an expert knowledge interpretation process of the time-series data. The technique finds applications mainly in remote sensing images and can be utilized to analyze urbanization and monitor forest regions. In this paper, a framework to perform a knowledge based interpretation of the changes/no changes observed in a spatiotemporal domain using tensor based approaches is presented. An incremental approach to Boolean Tensor Factorization method is proposed in this work, which is adopted to model the change patterns of objects/classes as well as their associated features. The framework is evaluated under different datasets to visualize the performance for the dependency factors. The algorithm is also validated in comparison with the tradition Boolean Tensor Factorization method and the results are substantial.



### Optimizing the Trade-off between Single-Stage and Two-Stage Object Detectors using Image Difficulty Prediction
- **Arxiv ID**: http://arxiv.org/abs/1803.08707v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08707v3)
- **Published**: 2018-03-23 09:35:05+00:00
- **Updated**: 2018-08-31 13:13:53+00:00
- **Authors**: Petru Soviany, Radu Tudor Ionescu
- **Comment**: Accepted at SYNASC 2018
- **Journal**: None
- **Summary**: There are mainly two types of state-of-the-art object detectors. On one hand, we have two-stage detectors, such as Faster R-CNN (Region-based Convolutional Neural Networks) or Mask R-CNN, that (i) use a Region Proposal Network to generate regions of interests in the first stage and (ii) send the region proposals down the pipeline for object classification and bounding-box regression. Such models reach the highest accuracy rates, but are typically slower. On the other hand, we have single-stage detectors, such as YOLO (You Only Look Once) and SSD (Singe Shot MultiBox Detector), that treat object detection as a simple regression problem by taking an input image and learning the class probabilities and bounding box coordinates. Such models reach lower accuracy rates, but are much faster than two-stage object detectors. In this paper, we propose to use an image difficulty predictor to achieve an optimal trade-off between accuracy and speed in object detection. The image difficulty predictor is applied on the test images to split them into easy versus hard images. Once separated, the easy images are sent to the faster single-stage detector, while the hard images are sent to the more accurate two-stage detector. Our experiments on PASCAL VOC 2007 show that using image difficulty compares favorably to a random split of the images. Our method is flexible, in that it allows to choose a desired threshold for splitting the images into easy versus hard.



### Pose-Driven Deep Models for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1803.08709v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08709v1)
- **Published**: 2018-03-23 09:38:42+00:00
- **Updated**: 2018-03-23 09:38:42+00:00
- **Authors**: Andreas Eberle
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (re-id) is the task of recognizing and matching persons at different locations recorded by cameras with non-overlapping views. One of the main challenges of re-id is the large variance in person poses and camera angles since neither of them can be influenced by the re-id system. In this work, an effective approach to integrate coarse camera view information as well as fine-grained pose information into a convolutional neural network (CNN) model for learning discriminative re-id embeddings is introduced. In most recent work pose information is either explicitly modeled within the re-id system or explicitly used for pre-processing, for example by pose-normalizing person images. In contrast, the proposed approach shows that a direct use of camera view as well as the detected body joint locations into a standard CNN can be used to significantly improve the robustness of learned re-id embeddings. On four challenging surveillance and video re-id datasets significant improvements over the current state of the art have been achieved. Furthermore, a novel reordering of the MARS dataset, called X-MARS is introduced to allow cross-validation of models trained for single-image re-id on tracklet data.



### Expanding a robot's life: Low power object recognition via FPGA-based DCNN deployment
- **Arxiv ID**: http://arxiv.org/abs/1804.00512v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1804.00512v1)
- **Published**: 2018-03-23 09:44:44+00:00
- **Updated**: 2018-03-23 09:44:44+00:00
- **Authors**: Panagiotis G. Mousouliotis, Konstantinos L. Panayiotou, Emmanouil G. Tsardoulias, Loukas P. Petrou, Andreas L. Symeonidis
- **Comment**: Accepted in MOCAST 2018
- **Journal**: None
- **Summary**: FPGAs are commonly used to accelerate domain-specific algorithmic implementations, as they can achieve impressive performance boosts, are reprogrammable and exhibit minimal power consumption. In this work, the SqueezeNet DCNN is accelerated using an SoC FPGA in order for the offered object recognition resource to be employed in a robotic application. Experiments are conducted to investigate the performance and power consumption of the implementation in comparison to deployment on other widely-used computational systems.



### CSfM: Community-based Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/1803.08716v1
- **DOI**: 10.1109/ICIP.2017.8297137
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08716v1)
- **Published**: 2018-03-23 10:27:01+00:00
- **Updated**: 2018-03-23 10:27:01+00:00
- **Authors**: Hainan Cui, Shuhan Shen, Xiang Gao, Zhanyi Hu
- **Comment**: Our paper has been published in ICIP2017
- **Journal**: 2017 IEEE International Conference on Image Processing (ICIP2017)
- **Summary**: Structure-from-Motion approaches could be broadly divided into two classes: incremental and global. While incremental manner is robust to outliers, it suffers from error accumulation and heavy computation load. The global manner has the advantage of simultaneously estimating all camera poses, but it is usually sensitive to epipolar geometry outliers. In this paper, we propose an adaptive community-based SfM (CSfM) method which takes both robustness and efficiency into consideration. First, the epipolar geometry graph is partitioned into separate communities. Then, the reconstruction problem is solved for each community in parallel. Finally, the reconstruction results are merged by a novel global similarity averaging method, which solves three convex $L1$ optimization problems. Experimental results show that our method performs better than many of the state-of-the-art global SfM approaches in terms of computational efficiency, while achieves similar or better reconstruction accuracy and robustness than many of the state-of-the-art incremental SfM approaches.



### Generalizability vs. Robustness: Adversarial Examples for Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/1804.00504v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1804.00504v1)
- **Published**: 2018-03-23 10:43:16+00:00
- **Updated**: 2018-03-23 10:43:16+00:00
- **Authors**: Magdalini Paschali, Sailesh Conjeti, Fernando Navarro, Nassir Navab
- **Comment**: Under Review for MICCAI 2018
- **Journal**: None
- **Summary**: In this paper, for the first time, we propose an evaluation method for deep learning models that assesses the performance of a model not only in an unseen test scenario, but also in extreme cases of noise, outliers and ambiguous input data. To this end, we utilize adversarial examples, images that fool machine learning models, while looking imperceptibly different from original data, as a measure to evaluate the robustness of a variety of medical imaging models. Through extensive experiments on skin lesion classification and whole brain segmentation with state-of-the-art networks such as Inception and UNet, we show that models that achieve comparable performance regarding generalizability may have significant variations in their perception of the underlying data manifold, leading to an extensive performance gap in their robustness.



### Speeding-up Object Detection Training for Robotics with FALKON
- **Arxiv ID**: http://arxiv.org/abs/1803.08740v2
- **DOI**: 10.1109/IROS.2018.8593990
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1803.08740v2)
- **Published**: 2018-03-23 11:13:29+00:00
- **Updated**: 2018-08-27 15:19:38+00:00
- **Authors**: Elisa Maiettini, Giulia Pasquale, Lorenzo Rosasco, Lorenzo Natale
- **Comment**: None
- **Journal**: IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), 2018
- **Summary**: Latest deep learning methods for object detection provide remarkable performance, but have limits when used in robotic applications. One of the most relevant issues is the long training time, which is due to the large size and imbalance of the associated training sets, characterized by few positive and a large number of negative examples (i.e. background). Proposed approaches are based on end-to-end learning by back-propagation [22] or kernel methods trained with Hard Negatives Mining on top of deep features [8]. These solutions are effective, but prohibitively slow for on-line applications. In this paper we propose a novel pipeline for object detection that overcomes this problem and provides comparable performance, with a 60x training speedup. Our pipeline combines (i) the Region Proposal Network and the deep feature extractor from [22] to efficiently select candidate RoIs and encode them into powerful representations, with (ii) the FALKON [23] algorithm, a novel kernel-based method that allows fast training on large scale problems (millions of points). We address the size and imbalance of training data by exploiting the stochastic subsampling intrinsic into the method and a novel, fast, bootstrapping approach. We assess the effectiveness of the approach on a standard Computer Vision dataset (PASCAL VOC 2007 [5]) and demonstrate its applicability to a real robotic scenario with the iCubWorld Transformations [18] dataset.



### A Deep Error Correction Network for Compressed Sensing MRI
- **Arxiv ID**: http://arxiv.org/abs/1803.08763v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08763v1)
- **Published**: 2018-03-23 12:18:25+00:00
- **Updated**: 2018-03-23 12:18:25+00:00
- **Authors**: Liyan Sun, Zhiwen Fan, Yue Huang, Xinghao Ding, John Paisley
- **Comment**: 7 pages, 7 figures, 2 tables
- **Journal**: None
- **Summary**: Compressed sensing for magnetic resonance imaging (CS-MRI) exploits image sparsity properties to reconstruct MRI from very few Fourier k-space measurements. The goal is to minimize any structural errors in the reconstruction that could have a negative impact on its diagnostic quality. To this end, we propose a deep error correction network (DECN) for CS-MRI. The DECN model consists of three parts, which we refer to as modules: a guide, or template, module, an error correction module, and a data fidelity module. Existing CS-MRI algorithms can serve as the template module for guiding the reconstruction. Using this template as a guide, the error correction module learns a convolutional neural network (CNN) to map the k-space data in a way that adjusts for the reconstruction error of the template image. Our experimental results show the proposed DECN CS-MRI reconstruction framework can considerably improve upon existing inversion algorithms by supplementing with an error-correcting CNN.



### Learning Deep Context-Network Architectures for Image Annotation
- **Arxiv ID**: http://arxiv.org/abs/1803.08794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08794v1)
- **Published**: 2018-03-23 13:58:21+00:00
- **Updated**: 2018-03-23 13:58:21+00:00
- **Authors**: Mingyuan Jiu, Hichem Sahbi
- **Comment**: None
- **Journal**: None
- **Summary**: Context plays an important role in visual pattern recognition as it provides complementary clues for different learning tasks including image classification and annotation. In the particular scenario of kernel learning, the general recipe of context-based kernel design consists in learning positive semi-definite similarity functions that return high values not only when data share similar content but also similar context. However, in spite of having a positive impact on performance, the use of context in these kernel design methods has not been fully explored; indeed, context has been handcrafted instead of being learned. In this paper, we introduce a novel context-aware kernel design framework based on deep learning. Our method discriminatively learns spatial geometric context as the weights of a deep network (DN). The architecture of this network is fully determined by the solution of an objective function that mixes content, context and regularization, while the parameters of this network determine the most relevant (discriminant) parts of the learned context. We apply this context and kernel learning framework to image classification using the challenging ImageCLEF Photo Annotation benchmark; the latter shows that our deep context learning provides highly effective kernels for image classification as corroborated through extensive experiments.



### Geometric and Physical Constraints for Drone-Based Head Plane Crowd Density Estimation
- **Arxiv ID**: http://arxiv.org/abs/1803.08805v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08805v3)
- **Published**: 2018-03-23 14:19:13+00:00
- **Updated**: 2019-07-18 09:05:50+00:00
- **Authors**: Weizhe Liu, Krzysztof Lis, Mathieu Salzmann, Pascal Fua
- **Comment**: IROS 2019
- **Journal**: None
- **Summary**: State-of-the-art methods for counting people in crowded scenes rely on deep networks to estimate crowd density in the image plane. While useful for this purpose, this image-plane density has no immediate physical meaning because it is subject to perspective distortion. This is a concern in sequences acquired by drones because the viewpoint changes often. This distortion is usually handled implicitly by either learning scale-invariant features or estimating density in patches of different sizes, neither of which accounts for the fact that scale changes must be consistent over the whole scene.   In this paper, we explicitly model the scale changes and reason in terms of people per square-meter. We show that feeding the perspective model to the network allows us to enforce global scale consistency and that this model can be obtained on the fly from the drone sensors. In addition, it also enables us to enforce physically-inspired temporal consistency constraints that do not have to be learned. This yields an algorithm that outperforms state-of-the-art methods in inferring crowd density from a moving drone camera especially when perspective effects are strong.



### What Do We Understand About Convolutional Networks?
- **Arxiv ID**: http://arxiv.org/abs/1803.08834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08834v1)
- **Published**: 2018-03-23 15:22:01+00:00
- **Updated**: 2018-03-23 15:22:01+00:00
- **Authors**: Isma Hadji, Richard P. Wildes
- **Comment**: None
- **Journal**: None
- **Summary**: This document will review the most prominent proposals using multilayer convolutional architectures. Importantly, the various components of a typical convolutional network will be discussed through a review of different approaches that base their design decisions on biological findings and/or sound theoretical bases. In addition, the different attempts at understanding ConvNets via visualizations and empirical studies will be reviewed. The ultimate goal is to shed light on the role of each layer of processing involved in a ConvNet architecture, distill what we currently understand about ConvNets and highlight critical open problems.



### Effective deep learning training for single-image super-resolution in endomicroscopy exploiting video-registration-based reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1803.08840v1
- **DOI**: 10.1007/s11548-018-1764-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08840v1)
- **Published**: 2018-03-23 15:31:40+00:00
- **Updated**: 2018-03-23 15:31:40+00:00
- **Authors**: Daniele Ravì, Agnieszka Barbara Szczotka, Dzhoshkun Ismail Shakir, Stephen P Pereira, Tom Vercauteren
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Probe-based Confocal Laser Endomicroscopy (pCLE) is a recent imaging modality that allows performing in vivo optical biopsies. The design of pCLE hardware, and its reliance on an optical fibre bundle, fundamentally limits the image quality with a few tens of thousands fibres, each acting as the equivalent of a single-pixel detector, assembled into a single fibre bundle. Video-registration techniques can be used to estimate high-resolution (HR) images by exploiting the temporal information contained in a sequence of low-resolution (LR) images. However, the alignment of LR frames, required for the fusion, is computationally demanding and prone to artefacts. Methods: In this work, we propose a novel synthetic data generation approach to train exemplar-based Deep Neural Networks (DNNs). HR pCLE images with enhanced quality are recovered by the models trained on pairs of estimated HR images (generated by the video-registration algorithm) and realistic synthetic LR images. Performance of three different state-of-the-art DNNs techniques were analysed on a Smart Atlas database of 8806 images from 238 pCLE video sequences. The results were validated through an extensive Image Quality Assessment (IQA) that takes into account different quality scores, including a Mean Opinion Score (MOS). Results: Results indicate that the proposed solution produces an effective improvement in the quality of the obtained reconstructed image. Conclusion: The proposed training strategy and associated DNNs allows us to perform convincing super-resolution of pCLE images.



### Audio-Visual Event Localization in Unconstrained Videos
- **Arxiv ID**: http://arxiv.org/abs/1803.08842v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08842v1)
- **Published**: 2018-03-23 15:34:03+00:00
- **Updated**: 2018-03-23 15:34:03+00:00
- **Authors**: Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, Chenliang Xu
- **Comment**: 23 pages, 7 figures
- **Journal**: None
- **Summary**: In this paper, we introduce a novel problem of audio-visual event localization in unconstrained videos. We define an audio-visual event as an event that is both visible and audible in a video segment. We collect an Audio-Visual Event(AVE) dataset to systemically investigate three temporal localization tasks: supervised and weakly-supervised audio-visual event localization, and cross-modality localization. We develop an audio-guided visual attention mechanism to explore audio-visual correlations, propose a dual multimodal residual network (DMRN) to fuse information over the two modalities, and introduce an audio-visual distance learning network to handle the cross-modality localization. Our experiments support the following findings: joint modeling of auditory and visual modalities outperforms independent modeling, the learned attention can capture semantics of sounding objects, temporal alignment is important for audio-visual fusion, the proposed DMRN is effective in fusing audio-visual features, and strong correlations between the two modalities enable cross-modality localization.



### Dist-GAN: An Improved GAN using Distance Constraints
- **Arxiv ID**: http://arxiv.org/abs/1803.08887v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08887v3)
- **Published**: 2018-03-23 17:06:26+00:00
- **Updated**: 2018-12-15 08:32:35+00:00
- **Authors**: Ngoc-Trung Tran, Tuan-Anh Bui, Ngai-Man Cheung
- **Comment**: Published as a conference paper at ECCV 2018
- **Journal**: None
- **Summary**: We introduce effective training algorithms for Generative Adversarial Networks (GAN) to alleviate mode collapse and gradient vanishing. In our system, we constrain the generator by an Autoencoder (AE). We propose a formulation to consider the reconstructed samples from AE as "real" samples for the discriminator. This couples the convergence of the AE with that of the discriminator, effectively slowing down the convergence of discriminator and reducing gradient vanishing. Importantly, we propose two novel distance constraints to improve the generator. First, we propose a latent-data distance constraint to enforce compatibility between the latent sample distances and the corresponding data sample distances. We use this constraint to explicitly prevent the generator from mode collapse. Second, we propose a discriminator-score distance constraint to align the distribution of the generated samples with that of the real samples through the discriminator score. We use this constraint to guide the generator to synthesize samples that resemble the real ones. Our proposed GAN using these distance constraints, namely Dist-GAN, can achieve better results than state-of-the-art methods across benchmark datasets: synthetic, MNIST, MNIST-1K, CelebA, CIFAR-10 and STL-10 datasets. Our code is published here (https://github.com/tntrung/gan) for research.



### Explicit Reasoning over End-to-End Neural Architectures for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1803.08896v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1803.08896v1)
- **Published**: 2018-03-23 17:17:16+00:00
- **Updated**: 2018-03-23 17:17:16+00:00
- **Authors**: Somak Aditya, Yezhou Yang, Chitta Baral
- **Comment**: 9 pages, 3 figures, AAAI 2018
- **Journal**: None
- **Summary**: Many vision and language tasks require commonsense reasoning beyond data-driven image and natural language processing. Here we adopt Visual Question Answering (VQA) as an example task, where a system is expected to answer a question in natural language about an image. Current state-of-the-art systems attempted to solve the task using deep neural architectures and achieved promising performance. However, the resulting systems are generally opaque and they struggle in understanding questions for which extra knowledge is required. In this paper, we present an explicit reasoning layer on top of a set of penultimate neural network based systems. The reasoning layer enables reasoning and answering questions where additional knowledge is required, and at the same time provides an interpretable interface to the end users. Specifically, the reasoning layer adopts a Probabilistic Soft Logic (PSL) based engine to reason over a basket of inputs: visual relations, the semantic parse of the question, and background ontological knowledge from word2vec and ConceptNet. Experimental analysis of the answers and the key evidential predicates generated on the VQA dataset validate our approach.



### Context Encoding for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1803.08904v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08904v1)
- **Published**: 2018-03-23 17:34:21+00:00
- **Updated**: 2018-03-23 17:34:21+00:00
- **Authors**: Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, Amit Agrawal
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2018
- **Journal**: None
- **Summary**: Recent work has made significant progress in improving spatial resolution for pixelwise labeling with Fully Convolutional Network (FCN) framework by employing Dilated/Atrous convolution, utilizing multi-scale features and refining boundaries. In this paper, we explore the impact of global contextual information in semantic segmentation by introducing the Context Encoding Module, which captures the semantic context of scenes and selectively highlights class-dependent featuremaps. The proposed Context Encoding Module significantly improves semantic segmentation results with only marginal extra computation cost over FCN. Our approach has achieved new state-of-the-art results 51.7% mIoU on PASCAL-Context, 85.9% mIoU on PASCAL VOC 2012. Our single model achieves a final score of 0.5567 on ADE20K test set, which surpass the winning entry of COCO-Place Challenge in 2017. In addition, we also explore how the Context Encoding Module can improve the feature representation of relatively shallow networks for the image classification on CIFAR-10 dataset. Our 14 layer network has achieved an error rate of 3.45%, which is comparable with state-of-the-art approaches with over 10 times more layers. The source code for the complete system are publicly available.



### Learning to Reconstruct Texture-less Deformable Surfaces from a Single View
- **Arxiv ID**: http://arxiv.org/abs/1803.08908v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08908v2)
- **Published**: 2018-03-23 17:46:20+00:00
- **Updated**: 2018-07-27 14:11:25+00:00
- **Authors**: Jan Bednařík, Pascal Fua, Mathieu Salzmann
- **Comment**: Accepted to 3DV 2018
- **Journal**: None
- **Summary**: Recent years have seen the development of mature solutions for reconstructing deformable surfaces from a single image, provided that they are relatively well-textured. By contrast, recovering the 3D shape of texture-less surfaces remains an open problem, and essentially relates to Shape-from-Shading. In this paper, we introduce a data-driven approach to this problem. We introduce a general framework that can predict diverse 3D representations, such as meshes, normals, and depth maps. Our experiments show that meshes are ill-suited to handle texture-less 3D reconstruction in our context. Furthermore, we demonstrate that our approach generalizes well to unseen objects, and that it yields higher-quality reconstructions than a state-of-the-art SfS technique, particularly in terms of normal estimates. Our reconstructions accurately model the fine details of the surfaces, such as the creases of a T-Shirt worn by a person.



### Image Inpainting using Block-wise Procedural Training with Annealed Adversarial Counterpart
- **Arxiv ID**: http://arxiv.org/abs/1803.08943v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08943v2)
- **Published**: 2018-03-23 18:45:12+00:00
- **Updated**: 2018-03-28 02:28:01+00:00
- **Authors**: Chao Yang, Yuhang Song, Xiaofeng Liu, Qingming Tang, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in deep generative models have shown promising potential in image inpanting, which refers to the task of predicting missing pixel values of an incomplete image using the known context. However, existing methods can be slow or generate unsatisfying results with easily detectable flaws. In addition, there is often perceivable discontinuity near the holes and require further post-processing to blend the results. We present a new approach to address the difficulty of training a very deep generative model to synthesize high-quality photo-realistic inpainting. Our model uses conditional generative adversarial networks (conditional GANs) as the backbone, and we introduce a novel block-wise procedural training scheme to stabilize the training while we increase the network depth. We also propose a new strategy called adversarial loss annealing to reduce the artifacts. We further describe several losses specifically designed for inpainting and show their effectiveness. Extensive experiments and user-study show that our approach outperforms existing methods in several tasks such as inpainting, face completion and image harmonization. Finally, we show our framework can be easily used as a tool for interactive guided inpainting, demonstrating its practical value to solve common real-world challenges.



### Deep Convolutional Compressed Sensing for LiDAR Depth Completion
- **Arxiv ID**: http://arxiv.org/abs/1803.08949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08949v1)
- **Published**: 2018-03-23 19:18:01+00:00
- **Updated**: 2018-03-23 19:18:01+00:00
- **Authors**: Nathaniel Chodosh, Chaoyang Wang, Simon Lucey
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we consider the problem of estimating a dense depth map from a set of sparse LiDAR points. We use techniques from compressed sensing and the recently developed Alternating Direction Neural Networks (ADNNs) to create a deep recurrent auto-encoder for this task. Our architecture internally performs an algorithm for extracting multi-level convolutional sparse codes from the input which are then used to make a prediction. Our results demonstrate that with only two layers and 1800 parameters we are able to out perform all previously published results, including deep networks with orders of magnitude more parameters.



### Iterative Low-Rank Approximation for CNN Compression
- **Arxiv ID**: http://arxiv.org/abs/1803.08995v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.08995v2)
- **Published**: 2018-03-23 22:04:48+00:00
- **Updated**: 2019-11-15 12:27:42+00:00
- **Authors**: Maksym Kholiavchenko
- **Comment**: Updated paper: arXiv:1903.09973
- **Journal**: None
- **Summary**: Deep convolutional neural networks contain tens of millions of parameters, making them impossible to work efficiently on embedded devices. We propose iterative approach of applying low-rank approximation to compress deep convolutional neural networks. Since classification and object detection are the most favored tasks for embedded devices, we demonstrate the effectiveness of our approach by compressing AlexNet, VGG-16, YOLOv2 and Tiny YOLO networks. Our results show the superiority of the proposed method compared to non-repetitive ones. We demonstrate higher compression ratio providing less accuracy loss.



### Pattern Analysis with Layered Self-Organizing Maps
- **Arxiv ID**: http://arxiv.org/abs/1803.08996v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1803.08996v2)
- **Published**: 2018-03-23 22:07:52+00:00
- **Updated**: 2018-03-28 17:07:18+00:00
- **Authors**: David Friedlander
- **Comment**: 16 pages, 21 color figures, DRAFT
- **Journal**: None
- **Summary**: This paper defines a new learning architecture, Layered Self-Organizing Maps (LSOMs), that uses the SOM and supervised-SOM learning algorithms. The architecture is validated with the MNIST database of hand-written digit images. LSOMs are similar to convolutional neural nets (covnets) in the way they sample data, but different in the way they represent features and learn. LSOMs analyze (or generate) image patches with maps of exemplars determined by the SOM learning algorithm rather than feature maps from filter-banks learned via backprop.   LSOMs provide an alternative to features derived from covnets. Multi-layer LSOMs are trained bottom-up, without the use of backprop and therefore may be of interest as a model of the visual cortex. The results show organization at multiple levels. The algorithm appears to be resource efficient in learning, classifying and generating images. Although LSOMs can be used for classification, their validation accuracy for these exploratory runs was well below the state of the art. The goal of this article is to define the architecture and display the structures resulting from its application to the MNIST images.



### LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/1803.08999v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1803.08999v1)
- **Published**: 2018-03-23 22:25:52+00:00
- **Updated**: 2018-03-23 22:25:52+00:00
- **Authors**: Chuhang Zou, Alex Colburn, Qi Shan, Derek Hoiem
- **Comment**: CVPR2018
- **Journal**: None
- **Summary**: We propose an algorithm to predict room layout from a single image that generalizes across panoramas and perspective images, cuboid layouts and more general layouts (e.g. L-shape room). Our method operates directly on the panoramic image, rather than decomposing into perspective images as do recent works. Our network architecture is similar to that of RoomNet, but we show improvements due to aligning the image based on vanishing points, predicting multiple layout elements (corners, boundaries, size and translation), and fitting a constrained Manhattan layout to the resulting predictions. Our method compares well in speed and accuracy to other existing work on panoramas, achieves among the best accuracy for perspective images, and can handle both cuboid-shaped and more general Manhattan layouts.



### Face Recognition with Hybrid Efficient Convolution Algorithms on FPGAs
- **Arxiv ID**: http://arxiv.org/abs/1803.09004v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1803.09004v1)
- **Published**: 2018-03-23 22:42:57+00:00
- **Updated**: 2018-03-23 22:42:57+00:00
- **Authors**: Chuanhao Zhuge, Xinheng Liu, Xiaofan Zhang, Sudeep Gummadi, Jinjun Xiong, Deming Chen
- **Comment**: This paper is accepted in GLSVLSI'18
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks have become a Swiss knife in solving critical artificial intelligence tasks. However, deploying deep CNN models for latency-critical tasks remains to be challenging because of the complex nature of CNNs. Recently, FPGA has become a favorable device to accelerate deep CNNs thanks to its high parallel processing capability and energy efficiency. In this work, we explore different fast convolution algorithms including Winograd and Fast Fourier Transform (FFT), and find an optimal strategy to apply them together on different types of convolutions. We also propose an optimization scheme to exploit parallelism on novel CNN architectures such as Inception modules in GoogLeNet. We implement a configurable IP-based face recognition acceleration system based on FaceNet using High-Level Synthesis. Our implementation on a Xilinx Ultrascale device achieves 3.75x latency speedup compared to a high-end NVIDIA GPU and surpasses previous FPGA results significantly.



### Feature Transfer Learning for Deep Face Recognition with Under-Represented Data
- **Arxiv ID**: http://arxiv.org/abs/1803.09014v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1803.09014v2)
- **Published**: 2018-03-23 23:37:31+00:00
- **Updated**: 2019-08-19 05:52:42+00:00
- **Authors**: Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, Manmohan Chandraker
- **Comment**: CVPR2019
- **Journal**: None
- **Summary**: Despite the large volume of face recognition datasets, there is a significant portion of subjects, of which the samples are insufficient and thus under-represented. Ignoring such significant portion results in insufficient training data. Training with under-represented data leads to biased classifiers in conventionally-trained deep networks. In this paper, we propose a center-based feature transfer framework to augment the feature space of under-represented subjects from the regular subjects that have sufficiently diverse samples. A Gaussian prior of the variance is assumed across all subjects and the variance from regular ones are transferred to the under-represented ones. This encourages the under-represented distribution to be closer to the regular distribution. Further, an alternating training regimen is proposed to simultaneously achieve less biased classifiers and a more discriminative feature representation. We conduct ablative study to mimic the under-represented datasets by varying the portion of under-represented classes on the MS-Celeb-1M dataset. Advantageous results on LFW, IJB-A and MS-Celeb-1M demonstrate the effectiveness of our feature transfer and training strategy, compared to both general baselines and state-of-the-art methods. Moreover, our feature transfer successfully presents smooth visual interpolation, which conducts disentanglement to preserve identity of a class while augmenting its feature space with non-identity variations such as pose and lighting.



