# Arxiv Papers in cs.CV on 2018-12-13
### Design Pseudo Ground Truth with Motion Cue for Unsupervised Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1812.05206v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05206v1)
- **Published**: 2018-12-13 00:13:29+00:00
- **Updated**: 2018-12-13 00:13:29+00:00
- **Authors**: Ye Wang, Jongmoo Choi, Yueru Chen, Qin Huang, Siyang Li, Ming-Sui Lee, C. -C. Jay Kuo
- **Comment**: 16 pages, 7 figures, 6 tables, conference
- **Journal**: None
- **Summary**: One major technique debt in video object segmentation is to label the object masks for training instances. As a result, we propose to prepare inexpensive, yet high quality pseudo ground truth corrected with motion cue for video object segmentation training. Our method conducts semantic segmentation using instance segmentation networks and, then, selects the segmented object of interest as the pseudo ground truth based on the motion information. Afterwards, the pseudo ground truth is exploited to finetune the pretrained objectness network to facilitate object segmentation in the remaining frames of the video. We show that the pseudo ground truth could effectively improve the segmentation performance. This straightforward unsupervised video object segmentation method is more efficient than existing methods. Experimental results on DAVIS and FBMS show that the proposed method outperforms state-of-the-art unsupervised segmentation methods on various benchmark datasets. And the category-agnostic pseudo ground truth has great potential to extend to multiple arbitrary object tracking.



### Learning to Learn from Noisy Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/1812.05214v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.05214v2)
- **Published**: 2018-12-13 00:58:05+00:00
- **Updated**: 2019-04-12 12:28:54+00:00
- **Authors**: Junnan Li, Yongkang Wong, Qi Zhao, Mohan Kankanhalli
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the success of deep neural networks (DNNs) in image classification tasks, the human-level performance relies on massive training data with high-quality manual annotations, which are expensive and time-consuming to collect. There exist many inexpensive data sources on the web, but they tend to contain inaccurate labels. Training on noisy labeled datasets causes performance degradation because DNNs can easily overfit to the label noise. To overcome this problem, we propose a noise-tolerant training algorithm, where a meta-learning update is performed prior to conventional gradient update. The proposed meta-learning method simulates actual training by generating synthetic noisy labels, and train the model such that after one gradient update using each set of synthetic noisy labels, the model does not overfit to the specific noise. We conduct extensive experiments on the noisy CIFAR-10 dataset and the Clothing1M dataset. The results demonstrate the advantageous performance of the proposed method compared to several state-of-the-art baselines.



### End to End Video Segmentation for Driving : Lane Detection For Autonomous Car
- **Arxiv ID**: http://arxiv.org/abs/1812.05914v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05914v1)
- **Published**: 2018-12-13 01:17:57+00:00
- **Updated**: 2018-12-13 01:17:57+00:00
- **Authors**: Wenhui Zhang, Tejas Mahale
- **Comment**: arXiv admin note: text overlap with arXiv:1806.07226 by other authors
- **Journal**: None
- **Summary**: Safety and decline of road traffic accidents remain important issues of autonomous driving. Statistics show that unintended lane departure is a leading cause of worldwide motor vehicle collisions, making lane detection the most promising and challenge task for self-driving. Today, numerous groups are combining deep learning techniques with computer vision problems to solve self-driving problems. In this paper, a Global Convolution Networks (GCN) model is used to address both classification and localization issues for semantic segmentation of lane. We are using color-based segmentation is presented and the usability of the model is evaluated. A residual-based boundary refinement and Adam optimization is also used to achieve state-of-art performance. As normal cars could not afford GPUs on the car, and training session for a particular road could be shared by several cars. We propose a framework to get it work in real world. We build a real time video transfer system to get video from the car, get the model trained in edge server (which is equipped with GPUs), and send the trained model back to the car.



### Advances of Scene Text Datasets
- **Arxiv ID**: http://arxiv.org/abs/1812.05219v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05219v1)
- **Published**: 2018-12-13 01:36:36+00:00
- **Updated**: 2018-12-13 01:36:36+00:00
- **Authors**: Masakazu Iwamura
- **Comment**: None
- **Journal**: None
- **Summary**: This article introduces publicly available datasets in scene text detection and recognition. The information is as of 2017.



### Nrityantar: Pose oblivious Indian classical dance sequence classification system
- **Arxiv ID**: http://arxiv.org/abs/1812.05231v1
- **DOI**: 10.1145/3293353.3293419
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05231v1)
- **Published**: 2018-12-13 02:16:38+00:00
- **Updated**: 2018-12-13 02:16:38+00:00
- **Authors**: Vinay Kaushik, Prerana Mukherjee, Brejesh Lall
- **Comment**: Eleventh Indian Conference on Computer Vision, Graphics and Image
  Processing 2018
- **Journal**: None
- **Summary**: In this paper, we attempt to advance the research work done in human action recognition to a rather specialized application namely Indian Classical Dance (ICD) classification. The variation in such dance forms in terms of hand and body postures, facial expressions or emotions and head orientation makes pose estimation an extremely challenging task. To circumvent this problem, we construct a pose-oblivious shape signature which is fed to a sequence learning framework. The pose signature representation is done in two-fold process. First, we represent person-pose in first frame of a dance video using symmetric Spatial Transformer Networks (STN) to extract good person object proposals and CNN-based parallel single person pose estimator (SPPE). Next, the pose basis are converted to pose flows by assigning a similarity score between successive poses followed by non-maximal suppression. Instead of feeding a simple chain of joints in the sequence learner which generally hinders the network performance we constitute a feature vector of the normalized distance vectors, flow, angles between anchor joints which captures the adjacency configuration in the skeletal pattern. Thus, the kinematic relationship amongst the body joints across the frames using pose estimation helps in better establishing the spatio-temporal dependencies. We present an exhaustive empirical evaluation of state-of-the-art deep network based methods for dance classification on ICD dataset.



### MetaStyle: Three-Way Trade-Off Among Speed, Flexibility, and Quality in Neural Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1812.05233v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.05233v3)
- **Published**: 2018-12-13 02:25:10+00:00
- **Updated**: 2019-03-07 03:44:16+00:00
- **Authors**: Chi Zhang, Yixin Zhu, Song-Chun Zhu
- **Comment**: AAAI 2019 spotlight. Supplementary:
  http://wellyzhang.github.io/attach/aaai19zhang_supp.pdf GitHub:
  https://github.com/WellyZhang/MetaStyle Project:
  http://wellyzhang.github.io/project/metastyle.html
- **Journal**: None
- **Summary**: An unprecedented booming has been witnessed in the research area of artistic style transfer ever since Gatys et al. introduced the neural method. One of the remaining challenges is to balance a trade-off among three critical aspects---speed, flexibility, and quality: (i) the vanilla optimization-based algorithm produces impressive results for arbitrary styles, but is unsatisfyingly slow due to its iterative nature, (ii) the fast approximation methods based on feed-forward neural networks generate satisfactory artistic effects but bound to only a limited number of styles, and (iii) feature-matching methods like AdaIN achieve arbitrary style transfer in a real-time manner but at a cost of the compromised quality. We find it considerably difficult to balance the trade-off well merely using a single feed-forward step and ask, instead, whether there exists an algorithm that could adapt quickly to any style, while the adapted model maintains high efficiency and good image quality. Motivated by this idea, we propose a novel method, coined MetaStyle, which formulates the neural style transfer as a bilevel optimization problem and combines learning with only a few post-processing update steps to adapt to a fast approximation model with satisfying artistic effects, comparable to the optimization-based methods for an arbitrary style. The qualitative and quantitative analysis in the experiments demonstrates that the proposed approach achieves high-quality arbitrary artistic style transfer effectively, with a good trade-off among speed, flexibility, and quality.



### Dynamic Fusion with Intra- and Inter- Modality Attention Flow for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1812.05252v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1812.05252v4)
- **Published**: 2018-12-13 03:41:18+00:00
- **Updated**: 2019-08-23 19:25:25+00:00
- **Authors**: Gao Peng, Zhengkai Jiang, Haoxuan You, Pan Lu, Steven Hoi, Xiaogang Wang, Hongsheng Li
- **Comment**: CVPR 2019 ORAL
- **Journal**: None
- **Summary**: Learning effective fusion of multi-modality features is at the heart of visual question answering. We propose a novel method of dynamically fusing multi-modal features with intra- and inter-modality information flow, which alternatively pass dynamic information between and across the visual and language modalities. It can robustly capture the high-level interactions between language and vision domains, thus significantly improves the performance of visual question answering. We also show that the proposed dynamic intra-modality attention flow conditioned on the other modality can dynamically modulate the intra-modality attention of the target modality, which is vital for multimodality feature fusion. Experimental evaluations on the VQA 2.0 dataset show that the proposed method achieves state-of-the-art VQA performance. Extensive ablation studies are carried out for the comprehensive analysis of the proposed method.



### ELASTIC: Improving CNNs with Dynamic Scaling Policies
- **Arxiv ID**: http://arxiv.org/abs/1812.05262v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05262v2)
- **Published**: 2018-12-13 05:00:31+00:00
- **Updated**: 2019-04-08 18:29:53+00:00
- **Authors**: Huiyu Wang, Aniruddha Kembhavi, Ali Farhadi, Alan Yuille, Mohammad Rastegari
- **Comment**: CVPR 2019 oral, code available https://github.com/allenai/elastic
- **Journal**: None
- **Summary**: Scale variation has been a challenge from traditional to modern approaches in computer vision. Most solutions to scale issues have a similar theme: a set of intuitive and manually designed policies that are generic and fixed (e.g. SIFT or feature pyramid). We argue that the scaling policy should be learned from data. In this paper, we introduce ELASTIC, a simple, efficient and yet very effective approach to learn a dynamic scale policy from data. We formulate the scaling policy as a non-linear function inside the network's structure that (a) is learned from data, (b) is instance specific, (c) does not add extra computation, and (d) can be applied on any network architecture. We applied ELASTIC to several state-of-the-art network architectures and showed consistent improvement without extra (sometimes even lower) computation on ImageNet classification, MSCOCO multi-label classification, and PASCAL VOC semantic segmentation. Our results show major improvement for images with scale challenges. Our code is available here: https://github.com/allenai/elastic



### IPOD: Intensive Point-based Object Detector for Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1812.05276v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05276v1)
- **Published**: 2018-12-13 05:48:49+00:00
- **Updated**: 2018-12-13 05:48:49+00:00
- **Authors**: Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, Jiaya Jia
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel 3D object detection framework, named IPOD, based on raw point cloud. It seeds object proposal for each point, which is the basic element. This paradigm provides us with high recall and high fidelity of information, leading to a suitable way to process point cloud data. We design an end-to-end trainable architecture, where features of all points within a proposal are extracted from the backbone network and achieve a proposal feature for final bounding inference. These features with both context information and precise point cloud coordinates yield improved performance. We conduct experiments on KITTI dataset, evaluating our performance in terms of 3D object detection, Bird's Eye View (BEV) detection and 2D object detection. Our method accomplishes new state-of-the-art , showing great advantage on the hard set.



### IRLAS: Inverse Reinforcement Learning for Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1812.05285v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1812.05285v5)
- **Published**: 2018-12-13 06:53:36+00:00
- **Updated**: 2019-11-06 02:30:08+00:00
- **Authors**: Minghao Guo, Zhao Zhong, Wei Wu, Dahua Lin, Junjie Yan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an inverse reinforcement learning method for architecture search (IRLAS), which trains an agent to learn to search network structures that are topologically inspired by human-designed network. Most existing architecture search approaches totally neglect the topological characteristics of architectures, which results in complicated architecture with a high inference latency. Motivated by the fact that human-designed networks are elegant in topology with a fast inference speed, we propose a mirror stimuli function inspired by biological cognition theory to extract the abstract topological knowledge of an expert human-design network (ResNeXt). To avoid raising a too strong prior over the search space, we introduce inverse reinforcement learning to train the mirror stimuli function and exploit it as a heuristic guidance for architecture search, easily generalized to different architecture search algorithms. On CIFAR-10, the best architecture searched by our proposed IRLAS achieves 2.60% error rate. For ImageNet mobile setting, our model achieves a state-of-the-art top-1 accuracy 75.28%, while being 2~4x faster than most auto-generated architectures. A fast version of this model achieves 10% faster than MobileNetV2, while maintaining a higher accuracy.



### FDFNet : A Secure Cancelable Deep Finger Dorsal Template Generation Network Secured via. Bio-Hashing
- **Arxiv ID**: http://arxiv.org/abs/1812.05308v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05308v1)
- **Published**: 2018-12-13 08:10:56+00:00
- **Updated**: 2018-12-13 08:10:56+00:00
- **Authors**: Avantika Singh, Ashish Arora, Shreya Hasmukh Patel, Gaurav Jaswal, Aditya Nigam
- **Comment**: Accepted in ISBA 2019: International Conference on Identity, Security
  and Behavior Analysis
- **Journal**: None
- **Summary**: Present world has already been consistently exploring the fine edges of online and digital world by imposing multiple challenging problems/scenarios. Similar to physical world, personal identity management is very crucial in-order to provide any secure online system. Last decade has seen a lot of work in this area using biometrics such as face, fingerprint, iris etc. Still there exist several vulnerabilities and one should have to address the problem of compromised biometrics much more seriously, since they cannot be modified easily once compromised. In this work, we have proposed a secure cancelable finger dorsal template generation network (learning domain specific features) secured via. Bio-Hashing. Proposed system effectively protects the original finger dorsal images by withdrawing compromised template and reassigning the new one. A novel Finger-Dorsal Feature Extraction Net (FDFNet) has been proposed for extracting the discriminative features. This network is exclusively trained on trait specific features without using any kind of pre-trained architecture. Later Bio-Hashing, a technique based on assigning a tokenized random number to each user, has been used to hash the features extracted from FDFNet. To test the performance of the proposed architecture, we have tested it over two benchmark public finger knuckle datasets: PolyU FKP and PolyU Contactless FKI. The experimental results shows the effectiveness of the proposed system in terms of security and accuracy.



### Visual Social Relationship Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.05917v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05917v1)
- **Published**: 2018-12-13 08:18:52+00:00
- **Updated**: 2018-12-13 08:18:52+00:00
- **Authors**: Junnan Li, Yongkang Wong, Qi Zhao, Mohan S. Kankanhalli
- **Comment**: arXiv admin note: text overlap with arXiv:1708.00634
- **Journal**: None
- **Summary**: Social relationships form the basis of social structure of humans. Developing computational models to understand social relationships from visual data is essential for building intelligent machines that can better interact with humans in a social environment. In this work, we study the problem of visual social relationship recognition in images. We propose a Dual-Glance model for social relationship recognition, where the first glance fixates at the person of interest and the second glance deploys attention mechanism to exploit contextual cues. To enable this study, we curated a large scale People in Social Context (PISC) dataset, which comprises of 23,311 images and 79,244 person pairs with annotated social relationships. Since visually identifying social relationship bears certain degree of uncertainty, we further propose an Adaptive Focal Loss to leverage the ambiguous annotations for more effective learning. We conduct extensive experiments to quantitatively and qualitatively demonstrate the efficacy of our proposed method, which yields state-of-the-art performance on social relationship recognition.



### When Semi-Supervised Learning Meets Transfer Learning: Training Strategies, Models and Datasets
- **Arxiv ID**: http://arxiv.org/abs/1812.05313v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05313v1)
- **Published**: 2018-12-13 08:46:42+00:00
- **Updated**: 2018-12-13 08:46:42+00:00
- **Authors**: Hong-Yu Zhou, Avital Oliver, Jianxin Wu, Yefeng Zheng
- **Comment**: Technical report
- **Journal**: None
- **Summary**: Semi-Supervised Learning (SSL) has been proved to be an effective way to leverage both labeled and unlabeled data at the same time. Recent semi-supervised approaches focus on deep neural networks and have achieved promising results on several benchmarks: CIFAR10, CIFAR100 and SVHN. However, most of their experiments are based on models trained from scratch instead of pre-trained models. On the other hand, transfer learning has demonstrated its value when the target domain has limited labeled data. Here comes the intuitive question: is it possible to incorporate SSL when fine-tuning a pre-trained model? We comprehensively study how SSL methods starting from pretrained models perform under varying conditions, including training strategies, architecture choice and datasets. From this study, we obtain several interesting and useful observations.   While practitioners have had an intuitive understanding of these observations, we do a comprehensive emperical analysis and demonstrate that: (1) the gains from SSL techniques over a fully-supervised baseline are smaller when trained from a pre-trained model than when trained from random initialization, (2) when the domain of the source data used to train the pre-trained model differs significantly from the domain of the target task, the gains from SSL are significantly higher and (3) some SSL methods are able to advance fully-supervised baselines (like Pseudo-Label).   We hope our studies can deepen the understanding of SSL research and facilitate the process of developing more effective SSL methods to utilize pre-trained models. Code is now available at github.



### Omni-directional Feature Learning for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1812.05319v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05319v1)
- **Published**: 2018-12-13 09:05:11+00:00
- **Updated**: 2018-12-13 09:05:11+00:00
- **Authors**: Di Wu, Hong-Wei Yang, De-Shuang Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (PReID) has received increasing attention due to it is an important part in intelligent surveillance. Recently, many state-of-the-art methods on PReID are part-based deep models. Most of them focus on learning the part feature representation of person body in horizontal direction. However, the feature representation of body in vertical direction is usually ignored. Besides, the spatial information between these part features and the different feature channels is not considered. In this study, we introduce a multi-branches deep model for PReID. Specifically, the model consists of five branches. Among the five branches, two of them learn the local feature with spatial information from horizontal or vertical orientations, respectively. The other one aims to learn interdependencies knowledge between different feature channels generated by the last convolution layer. The remains of two other branches are identification and triplet sub-networks, in which the discriminative global feature and a corresponding measurement can be learned simultaneously. All the five branches can improve the representation learning. We conduct extensive comparative experiments on three PReID benchmarks including CUHK03, Market-1501 and DukeMTMC-reID. The proposed deep framework outperforms many state-of-the-art in most cases.



### Wider Channel Attention Network for Remote Sensing Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1812.05329v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1812.05329v2)
- **Published**: 2018-12-13 09:27:12+00:00
- **Updated**: 2019-01-02 08:51:09+00:00
- **Authors**: Jun Gu, Guangluan Xu, Yue Zhang, Xian Sun, Ran Wen, Lei Wang
- **Comment**: This work is proposed for remote sensing images, but the idea of the
  whole paper do not foucs on the characteristics of remote sensing images. The
  content of the article does not match the title. In this case, we want to do
  some experiments on the natural images to verify the three tricks in our work
- **Journal**: None
- **Summary**: Recently, deep convolutional neural networks (CNNs) have obtained promising results in image processing tasks including super-resolution (SR). However, most CNN-based SR methods treat low-resolution (LR) inputs and features equally across channels, rarely notice the loss of information flow caused by the activation function and fail to leverage the representation ability of CNNs. In this letter, we propose a novel single-image super-resolution (SISR) algorithm named Wider Channel Attention Network (WCAN) for remote sensing images. Firstly, the channel attention mechanism is used to adaptively recalibrate the importance of each channel at the middle of the wider attention block (WAB). Secondly, we propose the Local Memory Connection (LMC) to enhance the information flow. Finally, the features within each WAB are fused to take advantage of the network's representation capability and further improve information and gradient flow. Analytic experiments on a public remote sensing data set (UC Merced) show that our WCAN achieves better accuracy and visual improvements against most state-of-the-art methods.



### Geometrical Stem Detection from Image Data for Precision Agriculture
- **Arxiv ID**: http://arxiv.org/abs/1812.05415v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1812.05415v1)
- **Published**: 2018-12-13 13:28:27+00:00
- **Updated**: 2018-12-13 13:28:27+00:00
- **Authors**: F. Langer, L. Mandtler, A. Milioto, E. Palazzolo, C. Stachniss
- **Comment**: Note that this work has been published without author's consent by
  WSEAS TRANSACTIONS on SYSTEMS, so please cite this arxiv paper if you want to
  reference to our work
- **Journal**: None
- **Summary**: High efficiency in precision farming depends on accurate tools to perform weed detection and mapping of crops. This allows for precise removal of harmful weeds with a lower amount of pesticides, as well as increase of the harvest's yield by providing the farmer with valuable information. In this paper, we address the problem of fully automatic stem detection from image data for this purpose. Our approach runs on mobile agricultural robots taking RGB images. After processing the images to obtain a vegetation mask, our approach separates each plant into its individual leaves and later estimates a precise stem position. This allows an upstream mapping algorithm to add the high-resolution stem positions as a semantic aggregate to the global map of the robot, which can be used for weeding and for analyzing crop statistics. We implemented our approach and thoroughly tested it on three different datasets with vegetation masks and stem position ground truth. The experiments presented in this paper conclude that our module is able to detect leaves and estimate the stem's position at a rate of 56 Hz on a single CPU. We furthermore provide the software to the community.



### DLOW: Domain Flow for Adaptation and Generalization
- **Arxiv ID**: http://arxiv.org/abs/1812.05418v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05418v2)
- **Published**: 2018-12-13 13:31:59+00:00
- **Updated**: 2019-05-13 14:04:36+00:00
- **Authors**: Rui Gong, Wen Li, Yuhua Chen, Luc Van Gool
- **Comment**: Accepted to CVPR 2019 (oral)
- **Journal**: None
- **Summary**: In this work, we present a domain flow generation(DLOW) model to bridge two different domains by generating a continuous sequence of intermediate domains flowing from one domain to the other. The benefits of our DLOW model are two-fold. First, it is able to transfer source images into different styles in the intermediate domains. The transferred images smoothly bridge the gap between source and target domains, thus easing the domain adaptation task. Second, when multiple target domains are provided for training, our DLOW model is also able to generate new styles of images that are unseen in the training data. We implement our DLOW model based on CycleGAN. A domainness variable is introduced to guide the model to generate the desired intermediate domain images. In the inference phase, a flow of various styles of images can be obtained by varying the domainness variable. We demonstrate the effectiveness of our model for both cross-domain semantic segmentation and the style generalization tasks on benchmark datasets. Our implementation is available at https://github.com/ETHRuiGong/DLOW.



### Generating Hard Examples for Pixel-wise Classification
- **Arxiv ID**: http://arxiv.org/abs/1812.05447v3
- **DOI**: 10.1109/JSTARS.2021.3112924
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05447v3)
- **Published**: 2018-12-13 14:29:25+00:00
- **Updated**: 2022-04-07 10:13:26+00:00
- **Authors**: Hyungtae Lee, Heesung Kwon, Wonkook Kim
- **Comment**: IEEE Journal of Selected Topics in Applied Earth Observations and
  Remote Sensing (JSTARS)
- **Journal**: in IEEE Journal of Selected Topics in Applied Earth Observations
  and Remote Sensing, vol. 14, pp. 9504-9517, 2021
- **Summary**: Pixel-wise classification in remote sensing identifies entities in large-scale satellite-based images at the pixel level. Few fully annotated large-scale datasets for pixel-wise classification exist due to the challenges of annotating individual pixels. Training data scarcity inevitably ensues from the annotation challenge, leading to overfitting classifiers and degraded classification performance. The lack of annotated pixels also necessarily results in few hard examples of various entities critical for generating a robust classification hyperplane. To overcome the problem of the data scarcity and lack of hard examples in training, we introduce a two-step hard example generation (HEG) approach that first generates hard example candidates and then mines actual hard examples. In the first step, a generator that creates hard example candidates is learned via the adversarial learning framework by fooling a discriminator and a pixel-wise classification model at the same time. In the second step, mining is performed to build a fixed number of hard examples from a large pool of real and artificially generated examples. To evaluate the effectiveness of the proposed HEG approach, we design a 9-layer fully convolutional network suitable for pixel-wise classification. Experiments show that using generated hard examples from the proposed HEG approach improves the pixel-wise classification model's accuracy on red tide detection and hyperspectral image classification tasks.



### Using Motion and Internal Supervision in Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.05455v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05455v1)
- **Published**: 2018-12-13 14:33:31+00:00
- **Updated**: 2018-12-13 14:33:31+00:00
- **Authors**: Daniel Harari
- **Comment**: PhD dissertation, 87 pages, 51 figures, 7 tables
- **Journal**: None
- **Summary**: In this thesis we address two related aspects of visual object recognition: the use of motion information, and the use of internal supervision, to help unsupervised learning. These two aspects are inter-related in the current study, since image motion is used for internal supervision, via the detection of spatiotemporal events of active-motion and the use of tracking. Most current work in object recognition deals with static images during both learning and recognition. In contrast, we are interested in a dynamic scene where visual processes, such as detecting motion events and tracking, contribute spatiotemporal information, which is useful for object attention, motion segmentation, 3-D understanding and object interactions. We explore the use of these sources of information in both learning and recognition processes. In the first part of the work, we demonstrate how motion can be used for adaptive detection of object-parts in dynamic environments, while automatically learning new object appearances and poses. In the second and main part of the study we develop methods for using specific types of visual motion to solve two difficult problems in unsupervised visual learning: learning to recognize hands by their appearance and by context, and learning to extract direction of gaze. We use our conclusions in this part to propose a model for several aspects of learning by human infants from their visual environment.



### Gaussian Process Deep Belief Networks: A Smooth Generative Model of Shape with Uncertainty Propagation
- **Arxiv ID**: http://arxiv.org/abs/1812.05477v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.05477v1)
- **Published**: 2018-12-13 15:25:40+00:00
- **Updated**: 2018-12-13 15:25:40+00:00
- **Authors**: Alessandro Di Martino, Erik Bodin, Carl Henrik Ek, Neill D. F. Campbell
- **Comment**: None
- **Journal**: None
- **Summary**: The shape of an object is an important characteristic for many vision problems such as segmentation, detection and tracking. Being independent of appearance, it is possible to generalize to a large range of objects from only small amounts of data. However, shapes represented as silhouette images are challenging to model due to complicated likelihood functions leading to intractable posteriors. In this paper we present a generative model of shapes which provides a low dimensional latent encoding which importantly resides on a smooth manifold with respect to the silhouette images. The proposed model propagates uncertainty in a principled manner allowing it to learn from small amounts of data and providing predictions with associated uncertainty. We provide experiments that show how our proposed model provides favorable quantitative results compared with the state-of-the-art while simultaneously providing a representation that resides on a low-dimensional interpretable manifold.



### Human Motion Prediction via Spatio-Temporal Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1812.05478v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05478v2)
- **Published**: 2018-12-13 15:27:15+00:00
- **Updated**: 2019-10-29 03:41:27+00:00
- **Authors**: Alejandro Hernandez Ruiz, Juergen Gall, Francesc Moreno-Noguer
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: We propose a Generative Adversarial Network (GAN) to forecast 3D human motion given a sequence of past 3D skeleton poses. While recent GANs have shown promising results, they can only forecast plausible motion over relatively short periods of time (few hundred milliseconds) and typically ignore the absolute position of the skeleton w.r.t. the camera. Our scheme provides long term predictions (two seconds or more) for both the body pose and its absolute position. Our approach builds upon three main contributions. First, we represent the data using a spatio-temporal tensor of 3D skeleton coordinates which allows formulating the prediction problem as an inpainting one, for which GANs work particularly well. Secondly, we design an architecture to learn the joint distribution of body poses and global motion, capable to hypothesize large chunks of the input 3D tensor with missing data. And finally, we argue that the L2 metric, considered so far by most approaches, fails to capture the actual distribution of long-term human motion. We propose two alternative metrics, based on the distribution of frequencies, that are able to capture more realistic motion patterns. Extensive experiments demonstrate our approach to significantly improve the state of the art, while also handling situations in which past observations are corrupted by occlusions, noise and missing frames.



### Unsupervised Image Decomposition in Vector Layers
- **Arxiv ID**: http://arxiv.org/abs/1812.05484v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05484v2)
- **Published**: 2018-12-13 15:41:19+00:00
- **Updated**: 2019-07-07 22:11:18+00:00
- **Authors**: Othman Sbai, Camille Couprie, Mathieu Aubry
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: Deep image generation is becoming a tool to enhance artists and designers creativity potential. In this paper, we aim at making the generation process more structured and easier to interact with. Inspired by vector graphics systems, we propose a new deep image reconstruction paradigm where the outputs are composed from simple layers, defined by their color and a vector transparency mask. This presents a number of advantages compared to the commonly used convolutional network architectures. In particular, our layered decomposition allows simple user interaction, for example to update a given mask, or change the color of a selected layer. From a compact code, our architecture also generates vector images with a virtually infinite resolution, the color at each point in an image being a parametric function of its coordinates. We validate the efficiency of our approach by comparing reconstructions with state-of-the-art baselines given similar memory resources on CelebA and ImageNet datasets. Most importantly, we demonstrate several applications of our new image representation obtained in an unsupervised manner, including editing, vectorization and image search.



### Deep Face Image Retrieval: a Comparative Study with Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1812.05490v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05490v1)
- **Published**: 2018-12-13 15:58:02+00:00
- **Updated**: 2018-12-13 15:58:02+00:00
- **Authors**: Ahmad S. Tarawneh, Ahmad B. A. Hassanat, Ceyhun Celik, Dmitry Chetverikov, M. Sohel Rahman, Chaman Verma
- **Comment**: None
- **Journal**: None
- **Summary**: Facial image retrieval is a challenging task since faces have many similar features (areas), which makes it difficult for the retrieval systems to distinguish faces of different people. With the advent of deep learning, deep networks are often applied to extract powerful features that are used in many areas of computer vision. This paper investigates the application of different deep learning models for face image retrieval, namely, Alexlayer6, Alexlayer7, VGG16layer6, VGG16layer7, VGG19layer6, and VGG19layer7, with two types of dictionary learning techniques, namely $K$-means and $K$-SVD. We also investigate some coefficient learning techniques such as the Homotopy, Lasso, Elastic Net and SSF and their effect on the face retrieval system. The comparative results of the experiments conducted on three standard face image datasets show that the best performers for face image retrieval are Alexlayer7 with $K$-means and SSF, Alexlayer6 with $K$-SVD and SSF, and Alexlayer6 with $K$-means and SSF. The APR and ARR of these methods were further compared to some of the state of the art methods based on local descriptors. The experimental results show that deep learning outperforms most of those methods and therefore can be recommended for use in practice of face image retrieval



### Combining Sentinel-1 and Sentinel-2 Time Series via RNN for object-based land cover classification
- **Arxiv ID**: http://arxiv.org/abs/1812.05530v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05530v1)
- **Published**: 2018-12-13 17:24:58+00:00
- **Updated**: 2018-12-13 17:24:58+00:00
- **Authors**: Dino Ienco, Raffaele Gaetano, Roberto Interdonato, Kenji Ose, Dinh Ho Tong Minh
- **Comment**: None
- **Journal**: None
- **Summary**: Radar and Optical Satellite Image Time Series (SITS) are sources of information that are commonly employed to monitor earth surfaces for tasks related to ecology, agriculture, mobility, land management planning and land cover monitoring. Many studies have been conducted using one of the two sources, but how to smartly combine the complementary information provided by radar and optical SITS is still an open challenge. In this context, we propose a new neural architecture for the combination of Sentinel-1 (S1) and Sentinel-2 (S2) imagery at object level, applied to a real-world land cover classification task. Experiments carried out on the Reunion Island, a overseas department of France in the Indian Ocean, demonstrate the significance of our proposal.



### Stochastic Image Deformation in Frequency Domain and Parameter Estimation using Moment Evolutions
- **Arxiv ID**: http://arxiv.org/abs/1812.05537v1
- **DOI**: None
- **Categories**: **math.ST**, cs.CV, stat.TH, 53A35, 62M99, 68U10, 60H15
- **Links**: [PDF](http://arxiv.org/pdf/1812.05537v1)
- **Published**: 2018-12-13 17:46:06+00:00
- **Updated**: 2018-12-13 17:46:06+00:00
- **Authors**: Line Kühnel, Alexis Arnaudon, Tom Fletcher, Stefan Sommer
- **Comment**: None
- **Journal**: None
- **Summary**: Modelling deformation of anatomical objects observed in medical images can help describe disease progression patterns and variations in anatomy across populations. We apply a stochastic generalisation of the Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework to model differences in the evolution of anatomical objects detected in populations of image data. The computational challenges that are prevalent even in the deterministic LDDMM setting are handled by extending the FLASH LDDMM representation to the stochastic setting keeping a finite discretisation of the infinite dimensional space of image deformations. In this computationally efficient setting, we perform estimation to infer parameters for noise correlations and local variability in datasets of images. Fundamental for the optimisation procedure is using the finite dimensional Fourier representation to derive approximations of the evolution of moments for the stochastic warps. Particularly, the first moment allows us to infer deformation mean trajectories. The second moment encodes variation around the mean, and thus provides information on the noise correlation. We show on simulated datasets of 2D MR brain images that the estimation algorithm can successfully recover parameters of the stochastic model.



### The Pros and Cons: Rank-aware Temporal Attention for Skill Determination in Long Videos
- **Arxiv ID**: http://arxiv.org/abs/1812.05538v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05538v2)
- **Published**: 2018-12-13 17:46:23+00:00
- **Updated**: 2019-04-10 15:17:19+00:00
- **Authors**: Hazel Doughty, Walterio Mayol-Cuevas, Dima Damen
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We present a new model to determine relative skill from long videos, through learnable temporal attention modules. Skill determination is formulated as a ranking problem, making it suitable for common and generic tasks. However, for long videos, parts of the video are irrelevant for assessing skill, and there may be variability in the skill exhibited throughout a video. We therefore propose a method which assesses the relative overall level of skill in a long video by attending to its skill-relevant parts. Our approach trains temporal attention modules, learned with only video-level supervision, using a novel rank-aware loss function. In addition to attending to task relevant video parts, our proposed loss jointly trains two attention modules to separately attend to video parts which are indicative of higher (pros) and lower (cons) skill. We evaluate our approach on the EPIC-Skills dataset and additionally annotate a larger dataset from YouTube videos for skill determination with five previously unexplored tasks. Our method outperforms previous approaches and classic softmax attention on both datasets by over 4% pairwise accuracy, and as much as 12% on individual tasks. We also demonstrate our model's ability to attend to rank-aware parts of the video.



### Benchmark Dataset for Automatic Damaged Building Detection from Post-Hurricane Remotely Sensed Imagery
- **Arxiv ID**: http://arxiv.org/abs/1812.05581v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05581v1)
- **Published**: 2018-12-13 18:52:40+00:00
- **Updated**: 2018-12-13 18:52:40+00:00
- **Authors**: Sean Andrew Chen, Andrew Escay, Christopher Haberland, Tessa Schneider, Valentina Staneva, Youngjun Choe
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: Rapid damage assessment is of crucial importance to emergency responders during hurricane events, however, the evaluation process is often slow, labor-intensive, costly, and error-prone. New advances in computer vision and remote sensing open possibilities to observe the Earth at a different scale. However, substantial pre-processing work is still required in order to apply state-of-the-art methodology for emergency response. To enable the comparison of methods for automatic detection of damaged buildings from post-hurricane remote sensing imagery taken from both airborne and satellite sensors, this paper presents the development of benchmark datasets from publicly available data. The major contributions of this work include (1) a scalable framework for creating benchmark datasets of hurricane-damaged buildings and (2) public sharing of the resulting benchmark datasets for Greater Houston area after Hurricane Harvey in 2017. The proposed approach can be used to build other hurricane-damaged building datasets on which researchers can train and test object detection models to automatically identify damaged buildings.



### Scene Recomposition by Learning-based ICP
- **Arxiv ID**: http://arxiv.org/abs/1812.05583v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1812.05583v2)
- **Published**: 2018-12-13 18:54:14+00:00
- **Updated**: 2020-04-07 08:09:26+00:00
- **Authors**: Hamid Izadinia, Steven M. Seitz
- **Comment**: To appear at CVPR 2020
- **Journal**: None
- **Summary**: By moving a depth sensor around a room, we compute a 3D CAD model of the environment, capturing the room shape and contents such as chairs, desks, sofas, and tables. Rather than reconstructing geometry, we match, place, and align each object in the scene to thousands of CAD models of objects. In addition to the fully automatic system, the key technical contribution is a novel approach for aligning CAD models to 3D scans, based on deep reinforcement learning. This approach, which we call Learning-based ICP, outperforms prior ICP methods in the literature, by learning the best points to match and conditioning on object viewpoint. LICP learns to align using only synthetic data and does not require ground truth annotation of object pose or keypoint pair matching in real scene scans. While LICP is trained on synthetic data and without 3D real scene annotations, it outperforms both learned local deep feature matching and geometric based alignment methods in real scenes. The proposed method is evaluated on real scenes datasets of SceneNN and ScanNet as well as synthetic scenes of SUNCG. High quality results are demonstrated on a range of real world scenes, with robustness to clutter, viewpoint, and occlusion.



### FA-RPN: Floating Region Proposals for Face Detection
- **Arxiv ID**: http://arxiv.org/abs/1812.05586v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05586v1)
- **Published**: 2018-12-13 18:55:17+00:00
- **Updated**: 2018-12-13 18:55:17+00:00
- **Authors**: Mahyar Najibi, Bharat Singh, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel approach for generating region proposals for performing face-detection. Instead of classifying anchor boxes using features from a pixel in the convolutional feature map, we adopt a pooling-based approach for generating region proposals. However, pooling hundreds of thousands of anchors which are evaluated for generating proposals becomes a computational bottleneck during inference. To this end, an efficient anchor placement strategy for reducing the number of anchor-boxes is proposed. We then show that proposals generated by our network (Floating Anchor Region Proposal Network, FA-RPN) are better than RPN for generating region proposals for face detection. We discuss several beneficial features of FA-RPN proposals like iterative refinement, placement of fractional anchors and changing anchors which can be enabled without making any changes to the trained model. Our face detector based on FA-RPN obtains 89.4% mAP with a ResNet-50 backbone on the WIDER dataset.



### Adversarial Inference for Multi-Sentence Video Description
- **Arxiv ID**: http://arxiv.org/abs/1812.05634v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1812.05634v2)
- **Published**: 2018-12-13 19:07:17+00:00
- **Updated**: 2019-04-16 02:04:44+00:00
- **Authors**: Jae Sung Park, Marcus Rohrbach, Trevor Darrell, Anna Rohrbach
- **Comment**: Accepted to Computer Vision and Pattern Recognition (CVPR) 2019
- **Journal**: None
- **Summary**: While significant progress has been made in the image captioning task, video description is still in its infancy due to the complex nature of video data. Generating multi-sentence descriptions for long videos is even more challenging. Among the main issues are the fluency and coherence of the generated descriptions, and their relevance to the video. Recently, reinforcement and adversarial learning based methods have been explored to improve the image captioning models; however, both types of methods suffer from a number of issues, e.g. poor readability and high redundancy for RL and stability issues for GANs. In this work, we instead propose to apply adversarial techniques during inference, designing a discriminator which encourages better multi-sentence video description. In addition, we find that a multi-discriminator "hybrid" design, where each discriminator targets one aspect of a description, leads to the best results. Specifically, we decouple the discriminator to evaluate on three criteria: 1) visual relevance to the video, 2) language diversity and fluency, and 3) coherence across sentences. Our approach results in more accurate, diverse, and coherent multi-sentence video descriptions, as shown by automatic as well as human evaluation on the popular ActivityNet Captions dataset.



### Dynamic Graph Modules for Modeling Object-Object Interactions in Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1812.05637v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05637v3)
- **Published**: 2018-12-13 19:11:55+00:00
- **Updated**: 2019-05-07 17:02:47+00:00
- **Authors**: Hao Huang, Luowei Zhou, Wei Zhang, Jason J. Corso, Chenliang Xu
- **Comment**: 14 pages, 5 figures
- **Journal**: None
- **Summary**: Video action recognition, a critical problem in video understanding, has been gaining increasing attention. To identify actions induced by complex object-object interactions, we need to consider not only spatial relations among objects in a single frame, but also temporal relations among different or the same objects across multiple frames. However, existing approaches that model video representations and non-local features are either incapable of explicitly modeling relations at the object-object level or unable to handle streaming videos. In this paper, we propose a novel dynamic hidden graph module to model complex object-object interactions in videos, of which two instantiations are considered: a visual graph that captures appearance/motion changes among objects and a location graph that captures relative spatiotemporal position changes among objects. Additionally, the proposed graph module allows us to process streaming videos, setting it apart from existing methods. Experimental results on benchmark datasets, Something-Something and ActivityNet, show the competitive performance of our method.



### SIGNet: Semantic Instance Aided Unsupervised 3D Geometry Perception
- **Arxiv ID**: http://arxiv.org/abs/1812.05642v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05642v2)
- **Published**: 2018-12-13 19:21:44+00:00
- **Updated**: 2019-04-05 02:58:11+00:00
- **Authors**: Yue Meng, Yongxi Lu, Aman Raj, Samuel Sunarjo, Rui Guo, Tara Javidi, Gaurav Bansal, Dinesh Bharadia
- **Comment**: To appear at CVPR 2019
- **Journal**: None
- **Summary**: Unsupervised learning for geometric perception (depth, optical flow, etc.) is of great interest to autonomous systems. Recent works on unsupervised learning have made considerable progress on perceiving geometry; however, they usually ignore the coherence of objects and perform poorly under scenarios with dark and noisy environments. In contrast, supervised learning algorithms, which are robust, require large labeled geometric dataset. This paper introduces SIGNet, a novel framework that provides robust geometry perception without requiring geometrically informative labels. Specifically, SIGNet integrates semantic information to make depth and flow predictions consistent with objects and robust to low lighting conditions. SIGNet is shown to improve upon the state-of-the-art unsupervised learning for depth prediction by 30% (in squared relative error). In particular, SIGNet improves the dynamic object class performance by 39% in depth prediction and 29% in flow prediction. Our code will be made available at https://github.com/mengyuest/SIGNet



### Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem
- **Arxiv ID**: http://arxiv.org/abs/1812.05720v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1812.05720v2)
- **Published**: 2018-12-13 22:52:42+00:00
- **Updated**: 2019-05-07 06:57:43+00:00
- **Authors**: Matthias Hein, Maksym Andriushchenko, Julian Bitterwolf
- **Comment**: Slight update of the CVPR 2019 final version [accepted with an oral
  presentation]
- **Journal**: None
- **Summary**: Classifiers used in the wild, in particular for safety-critical systems, should not only have good generalization properties but also should know when they don't know, in particular make low confidence predictions far away from the training data. We show that ReLU type neural networks which yield a piecewise linear classifier function fail in this regard as they produce almost always high confidence predictions far away from the training data. For bounded domains like images we propose a new robust optimization technique similar to adversarial training which enforces low confidence predictions far away from the training data. We show that this technique is surprisingly effective in reducing the confidence of predictions far away from the training data while maintaining high confidence predictions and test error on the original classification task compared to standard training.



### Detecting unseen visual relations using analogies
- **Arxiv ID**: http://arxiv.org/abs/1812.05736v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1812.05736v3)
- **Published**: 2018-12-13 23:56:24+00:00
- **Updated**: 2019-09-22 18:09:10+00:00
- **Authors**: Julia Peyre, Ivan Laptev, Cordelia Schmid, Josef Sivic
- **Comment**: None
- **Journal**: None
- **Summary**: We seek to detect visual relations in images of the form of triplets t = (subject, predicate, object), such as "person riding dog", where training examples of the individual entities are available but their combinations are unseen at training. This is an important set-up due to the combinatorial nature of visual relations : collecting sufficient training data for all possible triplets would be very hard. The contributions of this work are three-fold. First, we learn a representation of visual relations that combines (i) individual embeddings for subject, object and predicate together with (ii) a visual phrase embedding that represents the relation triplet. Second, we learn how to transfer visual phrase embeddings from existing training triplets to unseen test triplets using analogies between relations that involve similar objects. Third, we demonstrate the benefits of our approach on three challenging datasets : on HICO-DET, our model achieves significant improvement over a strong baseline for both frequent and unseen triplets, and we observe similar improvement for the retrieval of unseen triplets with out-of-vocabulary predicates on the COCO-a dataset as well as the challenging unusual triplets in the UnRel dataset.



